{"title": "Dream to Drive: Model-Based Vehicle Control Using Analytic World Models", "authors": ["Asen Nachkov\u00b9", "Danda Pani Paudel\u00b9", "Jan-Nico Zaech\u00b9", "Davide Scaramuzza2", "Luc Van Gool1,3"], "abstract": "Differentiable simulators have recently shown great promise for training autonomous vehicle controllers. Being able to backpropagate through them, they can be placed into an end-to-end training loop where their known dynamics turn into useful priors for the policy to learn, removing the typical black box assumption of the environment. So far, these systems have only been used to train policies. However, this is not the end of the story in terms of what they can offer. Here, for the first time, we use them to train world models. Specifically, we present three new task setups that allow us to learn next state predictors, optimal planners, and optimal inverse states. Unlike analytic policy gradients (APG), which requires the gradient of the next simulator state with respect to the current actions, our proposed setups rely on the gradient of the next state with respect to the current state. We call this approach Analytic World Models (AWMs) and showcase its applications, including how to use it for planning in the Waymax simulator. Apart from pushing the limits of what is possible with such simulators, we offer an improved training recipe that increases performance on the large-scale Waymo Open Motion dataset by up to 12% compared to baselines at essentially no additional cost.", "sections": [{"title": "I. INTRODUCTION", "content": "Differentiable simulation has emerged as a powerful tool to train controllers and predictors across different domains like physics [22, 55, 31], graphics [26, 24, 65], and robotics [23, 8, 4, 50]. Within the field of autonomous vehicles (AVs), it was recently shown that differentiable motion dynamics can serve as a useful stepping stone for training robust and realistic vehicle policies [39]. The framework is straightforward and bears similarity to backpropagation through time \u2013 it involves rolling out a trajectory and supervising it with a ground-truth (GT) expert one. The process is sample-efficient because the gradients of the dynamics automatically guide the policy toward optimality and there is no search involved, unlike when the environment is treated as black box.\nYet, this has only been explored for single policies, which are reactive [53] in their nature \u2013 at test-time they simply associate an action with each observation without providing any guarantee for their expected performance. Unlike them, model-based methods use planning at test time [44], which guarantees to maximize the estimated reward. They are considered more interpretable compared to model-free methods, due to the simulated world dynamics, and more amenable to conditioning, which makes them potentially safer [42]. They are also sample-efficient due to the self-supervised training [5].\nThus, we are motivated to understand the kinds of tasks solvable in a differentiable simulator for vehicle motion. Policy learning with differentiable simulation is called Analytic Pol-icy Gradients (APG). Similarly, we call the proposed approach Analytic World Models (AWMs). It is intuitive, yet distinct from APG because APG requires the gradients of the next state with respect to the current actions, while AWM requires the gradients of the next state with respect to the current state.\nThe benefit of using a differentiable environment for solving these tasks is twofold. First, by not assuming black box dynamics, one can avoid searching for the solution, which improves sample efficiency. Second, when the simulator is put into an end-to-end training loop, the differentiable dynamics serve to better condition the predictions, leading to more physically-consistent representations, as evidenced from other works. This happens because the gradients of the dynamics get mixed together with the gradients of the predictors. More gen-erally, similar to how differentiable rendering [26, 24] allows us to solve inverse graphics tasks, we believe that differentiable simulation allows us to solve new inverse dynamics tasks.\nHaving established the world modeling tasks, we use them for planning at test time. Specifically, we formulate a method based on model-predictive control (MPC), which allows the agent to autoregressively imagine future trajectories and apply the proposed world modeling predictors on their imagined states. Thus, the agent can efficiently compose these predictors in time. Note that this requires another world modeling pre-dictor \u2013 one that predicts the next state latent sensory features from the current ones. This is the de facto standard world model [15]. Its necessity results only from the architectural design, to drive the autoregressive generation. Contrary to it, the predictions from our three proposed tasks are interpretable and meaningful, and represent a step towards the goal of having robust accurate planning for driving.\nWe use Waymax [14] as our simulator of choice, due to it being fully differentiable and data-driven. The scenarios are instantiated from the large-scale Waymo Open Motion Dataset (WOMD) [11] and are realistic in terms of roadgraph layouts and traffic actors. Apart from establishing new ways of using the differentiable simulator, we offer an improved training recipe over the previous APG work, resulting in up to 12% improvement in the average displacement error on WOMD while using an equal or smaller amount of compute. This improvement results from shorter training to limit any possible overfitting to the training trajectories and a regularization term that encourages the entropy of the policy. Additionally, we substitute the non-differentiable computation for collision detection in Waymax with a differentiable approximation based on Gaussian overlap. This allows us to differentiate not only through the dynamics, but also through the agent poses themselves. The learning signal from this overlap is sparse, yet conceptually useful to make the training setup complete.\nContributions. Our contributions are the following:\n\u2022 In Sec. III-B to III-D we present three world modeling tasks, solvable within a differentiable simulator - relative odometry, state planning, and inverse state estimation.\n\u2022 In Sec. III-E and III-F we implement predictors for these tasks and present a planning agent that uses them.\n\u2022 We train and evaluate our proposed setups in the Way-max simulator [14], and further study them in multiple settings and conditions. In the process, we introduce technical modifications, such as differentiable overlap, for improved training relative to the baseline APG method."}, {"title": "II. RELATED WORK", "content": "Differentiable simulation. The most relevant work is An-alytic Policy Gradients (APG) applied for vehicle motion [39]. It trains near-optimal policies in a supervised manner, relying on the differentiability of the Waymax simulator [14]. The authors present a recurrent model that selects actions autoregressively from the observed agent locations, nearest roadgraph points, traffic lights, and goal heading. At training time the model learns to select those actions that would bring the simulated trajectory as close as possible to the log-trajectory. By adopting a GRU architecture, the derivatives of the dynamics from each timestep mix with those of the RNN hidden state and propagate backwards until the start of the trajectory. Compared to this APG model, we aim to introduce planning in the differentiable environment.\nMore generally, differentiable simulators have grown in popularity because they allow one to solve ill-posed inverse problems related to the dynamics. As examples, an object's physical parameters like mass, friction, and elasticity could be estimated directly from videos and real-world experiments [7, 38, 13], or simulations of soft material cutting could enable precise calibration and policy learning [21]. Simulations can be parallelized across accelerators to enable efficient scaling of problem and experiment sizes [61, 33, 12].\nWithin the field of robotics, differentiable simulation is used extensively, especially for training robotic policies in physically-realistic settings [41, 32, 54, 45, 22]. The focus has often been on object manipulation [29, 60, 62, 30] which requires having differentiable contact models for detecting col-lisions - something lacking in the Waymax dynamics. Analytic policy gradients (APG) has been used to train policies for trajectory tracking and navigation in quadrotors and fixed-wing drones [58], quadruped locomotion [51], and for quadrotor control from visual features [20].\nDriving simulators like CARLA [10, 34] have been used in autonomous driving [6, 64], with less focus on differentiable ones [27]. Some are differentiable but lack expert actions [27], others are lacking acceleration support, which is crucial for large scale training [52, 28, 57]. Waymax [14] was introduced recently as a data-driven simulator for vehicle motion. It represents vehicles as 2D boxes and supports acceleration.\nPlanning. Planning using world models [47, 15, 35] is a classic topic with two main approaches: model-predictive control (MPC) [2] and Dyna-style imagination [53]. With MPC [1, 46, 25], one starts with a random policy from which actions are sampled and evaluated. Then, the policy is repeatedly refit on only the best trajectories, from which new trajectories are sampled. Eventually, an aggregated action from the best trajectories is selected and executed. Being closed-loop, this strategy is repeated at every timestep. Dyna-style planning relies on the agent simulating entire trajectories and using them to update its policy [17, 18, 19]. Compared to"}, {"title": "III. METHOD", "content": "In this section we introduce different task setups showcasing the usage of a differentiable simulator and the benefits it offers. Subsequently, in Sec. III-E and III-F, we introduce the agent architecture and the planning at test time.\nNotation. In all that follows we represent the current simulator state with st, the current action with at, the log (expert) state with \u015dt, the log action with \u00e2t. The simulator is a function Sim: S \u00d7 A \u2192 S, with Sim(st, at) \u2192 St+1, where the set of all states is S and that of the actions A.\nA. Preliminaries - APG\nIt is shown in [39] that a differentiable simulator can turn the search problem of optimal policy learning into a supervised one. Here the policy \u03c0\u03b8 produces an action at from the current state, which is executed in the environment to obtain the next state st+1. Comparing it to the log-trajectory \u015dt+1 produces a loss, whose gradient is backpropagated through the simulator and back to the policy:\n$\\min\\limits_{a_t} ||Sim (s_t, \\pi_\\theta (S_t)) - \\hat{S}_{t+1}||^2$. \n(1)\nThe key gradient here is that of the next state with respect to the current agent actions $\\frac{\\partial S_{t+1}}{\\partial a_t}$. The loss is minimized whenever the policy outputs an action equal to the inverse kinematics InvKin(st, \u015dt+1). To obtain similar supervision without access to a differentiable simulator, one would need to supervise the policy with the inverse kinematic actions, which are unavailable if the environment is considered a black box. Hence, this is an example of an inverse dynamics problem that is not efficiently solvable without access to a known environment, in this case to provide inverse kinematics.\nB. Relative odometry\nIn this simple setting a world model fo : S\u00d7A \u2192 S predicts the next state st+1 from the current state-action pair (st, at). Here, a differentiable simulator is not needed to learn a good predictor. One can obtain (st, at, St+1) tuples simply by rolling out a random policy and then supervising the predictions with the next state st+1. Nonetheless, we provide a formulation for bringing the simulator into the training loop of this task:\n$\\min || Sim^{-1} (f_\\theta(s_t, a_t), a_t) - s_t||^2$.\n(2)\nHere, the world model fe takes (st, at) and returns a next-state estimate St+1. We then feed it into an inverse simulator Sim\u22121 which is a function with the property that Sim\u22121(Sim(st, at), at) = st. This output is compared with the current st. The loss is minimized when fe predicts exactly St+1, thus becoming a predictor of the next state.\nWe implement the inverse simulator for the bicycle dynam-ics in Waymax [14], however observe that it is problematic in the following sense. The velocities Vr and Vy are tied to the yaw angle of the agent through the relationship vx = v cos \u03c6 and vy = v sind, where \u03c6 is the yaw angle and v is the current speed. However, at the first simulation step, due to the WOM dataset [11] being collected with noisy estimates of the agent state parameters, the relationships between vx, Uy, and \u03c6 do not hold. Thus, the inverse simulator produces incorrect results for the first timestep.\nFor this reason, we provide another formulation for the problem that only requires access to a forward simulator:\n$\\min\\limits_\\theta||Sim (s_{t+1} - f_\\theta (s_t, a_t), a_t) - S_{t+1}||^2$ \n(3)\nHere, fe predicts the relative state difference that executing at will bring to the agent. One can verify that the loss is minimized if and only if the prediction is equal to st+1 \u2212 St. This can still be interpreted as a world model where fe learns to estimate how an action would change its relative state.\nSince the time-varying elements of the agent state consist of (x, y, vx, Uy, \u03c6), this world model has a clear relative odometric interpretation. Learning such a predictor without a differentiable simulator will prevent the gradients of the environment dynamics from mixing with those of the network.\nInverse dynamics and inverse kinematics. Given a tuple (st, at, St+1), one can learn inverse dynamics (St+1, at) \u2192 St and inverse kinematics (st, St+1) \u2192 at without a differentiable simulator, which is useful for exploration [43]. Formulations that involve the simulator are also possible. We do not list them here because they are similar to Eqn. 3.\nC. Optimal planners\nWe call the network fo: S \u2192 S with St \\rightarrow St+1 a planner because it plans out the next state to visit from the current one. Unlike a policy, which selects an action without explicitly knowing the next state, the planner does not execute"}, {"title": "D. Inverse optimal state estimation", "content": "any actions. Hence, until an action is executed, its output is inconsequential. We consider the problem of learning an optimal planner. With a differentiable simulator, as long as we have access to the inverse kinematics, we can formulate the optimisation as:\n$\\min||Sim (s_t, InvKin(s_t, s_t + f_\\theta(s_t))) - \\hat{s}_{t+1}||^2$ \n(4)\nHere, fo predicts the next state to visit as an offset to the current one. The action that reaches it is obtained using the inverse kinematics. After executing the action we directly supervise with the optimal next state. The gradient of the loss goes through the simulator, then through the inverse kinematics, and finally through the state planner network. Note that with a black box environment we can still supervise the planner directly with \u015dt+1, but a black box does not provide any inverse kinematics, hence there is no way to perform trajectory rollouts, unless with a separate behavioral policy.\nWe now consider the following task \"Given (st, at), find an alternative state for the current timestep t where taking action at will lead to an optimal next state \u015dt+1\". We formulate the problem as\n$\\min||Sim (s_t + f_\\theta(s_t, a_t), a_t) - \\hat{s}_{t+1}||^2$ \n(5)\nHere fe needs to estimate the effect of the action at and predict a new state \u0161t, relatively to the current state st, such that after executing at in it, the agent reaches \u015dt+1. The loss is minimized if fo predicts \u0161t \u2212 st. The key gradient, as in Eqns. 2, 3, and 4, is that of the next state with respect to the current state. Given the design of the Waymax simulator [14], these gradients are readily-available.\nConsider solving this task with a black box environment. To do so, one would need to supervise the prediction fo(st, at) with the state \u0161t \u2212 st, with \u0161t being unknown. By definition \u0160t = Sim\u22121(\u015dt+1, at), which is unobtainable since under a black box environment assumption, Sim\u22121 is unavailable. Hence, this is another inverse problem which is not solvable unless we are given more information about the environment, here specifically its inverse function.\nThe utility of this task is in providing a \u201cconfidence\u201d measure to an action. If the prediction of fe is close to 0, then the agent is relatively certain that the action at is close to optimal. Likewise, a large prediction from fe indicates that the action at is believed to be optimal for a different state. The prediction units are also directly interpretable."}, {"title": "E. Architecture", "content": "Having described the world modeling tasks, we now present the architecture that implements predictors for them.\nNetworks. Figure 3 shows our setup. We follow [39] and extract observations for each modality \u2013 roadgraph, agent locations, traffic lights, and any form of route conditioning and process them into a unified world state representing the current situation around the ego-vehicle. To capture temporal information, we use an RNN to evolve a hidden state according to the observed features. A world model with multiple heads predict the next unified world state, a reward, and the estimates for the three tasks introduced previously \u2013 relative odometry, state planning, and inverse state estimation.\nLosses. We use four main losses \u2013 one for the control task, which drives the behavioral policy, and three additional losses for the relative odometry, state planning, and inverse state tasks. Each of these leverages the differentiability of the environment. The inputs to the world model are detached (denoted with sg[]) so the world modeling losses do not impact the behavioral policy. This greatly improves policy stability and makes it so one does not need to weigh the modeling losses relative to the control loss.\nFor extended functionality, our agent requires three addi-tional auxiliary losses. The first trains the world model to predict the next world state in latent space, which is needed to be able to predict autoregressively arbitrarily long future sequences Zt, Zt+1, .... It also allows us to use the AWM task heads on those imagined trajectories, similar to [17, 59]. The second is a reward loss so the world model can predict rewards. We use a standard reward defined as rt = =||St+1 - $t+1||2. The third loss is an object collision loss, which is sparse and penalizes the ego-vehicle from colliding with other agents. It is described in Sec. III-G"}, {"title": "F. Planning at test time", "content": "Compared to the APG method in [39], our world modeling predictors enable planning at test time. The relevant workflow is shown in Fig. 4. Specifically, we adopt a model-predictive control (MPC) algorithm where we first repeatedly compose the world models and the policy to simulate N future trajecto-ries branching out from the current state st. Subsequently, the reward model evaluates them and the rewards along each one are summed to yield a single scalar, representing the value of this trajectory. Finally, we select the top-k trajectories in terms of value, and average their first actions. This aggregate action is the one to be executed by the agent at the current step. The planning loop is repeated at every timestep, leading to adaptive, closed loop behavior.\nSince the architecture relies on a recurrent model to process the incoming observations, we adapt the planning algorithm to evolve the RNN hidden state within the imagined trajectories. The pseudocode is shown in Algorithm 1."}, {"title": "G. Additional improvements", "content": "We have now explained the world modeling tasks, the agent architecture, and the planning. Here we focus on additional practical improvements relevant to our experiments.\nPolicy generalization. Compared to the APG model in [39], our strategy is to improve the generalization capability of the agent by increasing the diversity of the sampled trajectories. To that end, we first limit the number of training epochs to reduce any possible overfitting, and second, we explicitly add entropy regularization on the actions sampled from the policy [16]. Since the distribution of the actions \u2013 steering and acceleration - is modeled as a Gaussian mixture (GM), this regularization is added only for the mixing distribution, which is categorical. To prevent the individual Gaussian distributions from degenerating into deterministic values, we clip their variances to a minimal value.\nDifferentiable overlap. Being able to detect collisions in Waymax [14] is of crucial importance. The default algorithm there is based on the separation axis theorem and only tells us whether two objects overlap, without telling us how much they overlap. We are interested in obtaining a differentiable approximation for the overlap between two rotated 2D boxes, so that we can differentiate not only through the dynamics, but also through the scene configuration itself.\nWe approximate box overlap as the overlap of 2D Gaus-sians. Specifically, from a vehicle pose (x, y, \u03b8, w, h), we build a 2D Gaussian with parameters \u03bc and \u2211 given by:\n$\\mu = \\begin{bmatrix} x\\\\ y \\end{bmatrix}$\n(6)\n$\\Sigma = R \\begin{bmatrix} w^2/25 & 0\\\\ 0 & h^2/25 \\end{bmatrix} R^T$\n(7)\n$R = \\begin{bmatrix} cos \\theta & -sin \\theta\\\\ sin \\theta & cos \\theta \\end{bmatrix}$\n(8)\nThe division by 25 ensures that the density of the Gaussian covers the box relatively well. Then, the overlap between two boxes N(\u03bc1, \u03a31) and N(\u03bc2, \u03a32) is computed in closed form as another Gaussian density:\n$\\frac{1}{2\\pi det(\\Sigma_1 + \\Sigma_2)} e^{-(\\mu_1-\\mu_2)^T (\\Sigma_1+\\Sigma_2)^{-1} (\\mu_1-\\mu_2)}.$ \n(9)\nThis overlap provides a very sparse training signal, as the ego-vehicle does not collide all the time. Yet, we include it to make the setup complete in terms of differentiating through both the dynamics and the agent interactions."}, {"title": "IV. EXPERIMENTS", "content": "Setup. We follow the settings of [39]. We use the Waymax simulator, whose initial simulator states are built over the WOMD [11]. We measure the quality of a simulated trajectory using the average displacement error (ADE) compared to the expert (log) one. Since the policy is stochastic, we realize multiple trajectories and report the ADE of the best one, leading to minADE. We also report the minimum overlap rate, which represents collisions, and the minimum offroad rate, which represents agents navigating beyond the drivable regions. These rates represent the proportion of scenarios, in which at least one collision/offroad event occurs. All the following results are on the WOMD validation set."}, {"title": "A. Evaluating the analytic predictors", "content": "Here we present independent, isolated evaluations for each of the four tasks described in Sec. III.\nOptimal control. Optimal control in a differentiable envi-ronment can be solved using APG. Hence, we evaluate the reactive performance of our policies similarly to how it was evaluated in the baseline APG method [39]. Table I shows the details. Naturally, the trajectories obtained from rolling out a reactive policy trained with APG are accurate. Increasing the number of rollouts improves performance in terms of min ADE, min overlap, and min offroad by up to 12%.\nCompared to the baseline [39], we use a more efficient training strategy that trains for a smaller number of epochs to limit possible overfitting. Additionally, we aim to increase the diversity of the policy as follows. As previously state, the policy parametrizes a Gaussian mixture with 6 components. We add a regularization term that maximizes the categorical entropy of the mixture distribution (not the Gaussians them-selves). To prevent the Gaussians from degenerating into single values, we clip the predicted variances.\nRelative odometry. To evaluate the next state prediction we first produce qualitative results that demonstrate the con-trollability of the learned odometry predictor. Specifically, we condition the agent, for example, to intentionally commit to a turn over a long time frame. Concurrently, the odometry, along with the latent state predictors are used to imagine the next second of the planned motion, conditional on the actions. We judge the imagined trajectory to be accurate if the imagination precisely aligns with the realized trajectory. Fig. 5 shows an example. Overall, we observe accurate controllability \u2013 if we condition the agent to turn left/right, accelerate/decelerate, the imagined trajectories also represent similar motion.\nManually conditioning the predicted odometry on a de-sired action sequence could easily lead to out-of-distribution state-action sequences. For example, driving offroad, mak-ing sudden sharp U-turns, or maximally accelerating can be considered rare events within the expert distribution. The accurate alignment between the imagined trajectory and the executed one shows that the network learns to generalize effectively. Nonetheless, the complexity of the scene and the autoregressive prediction length do limit the accuracy of the imagined trajectories. Finally, for in-distribution sequences and for shorter future horizons the odometry is very accurate, as shown in Table II."}, {"title": "B. Planning", "content": "Here we assess the performance of the planning agent, which uses the analytic predictors. Evaluating with the best of multiple trajectory realizations is common practice in the literature [11, 36, 3], yet is unrealistic because in the real world the agent can execute only one trajectory. Planning at test time is a considerable improvement to this setup because it allows the agent to simulate multiple virtual trajectories while only executing a single real one."}, {"title": "C. Additional experiments", "content": "Route conditioning. To obtain good performance in terms of ADE the agent needs to know both where to go, and how to get there. Route conditioning can be used to provide more information to the agent, effectively narrowing down the possible directions that could be taken. We explore two forms of route conditioning \u2013 heading, where the agent receives the heading angle to keep in order to reach a future expert state, and waypoint, where the agent observes the (x, y) location of the last expert state. Both are used in the ego reference frame.\nTable VI shows that adding route conditioning helps notice-ably. Using the waypoint is the most useful because to reach the expert location, the agent needs to adapt both its steering and acceleration. The heading conditioning only provides information about the steering, not the acceleration. In general, as more conditioning is added, the task becomes closer and closer to trajectory following. The heading conditioning is the default setting we use in the other experiments.\nDifferentiable overlap. We also provide experiments re-lated to the differentiable overlap approximation used by our method. Table VII shows that the differentiable overlap has a small effect \u2013 it hurts the ADE, but improves the overlap metric. The differentiable overlap loss pushes the agent in the direction of maximally decreasing overlap when there is a collision. This direction may be different than the one towards the expert state (see Fig. 8). Thus, we recognize a trade-off. To limit the effect of conflicting gradients, we set the weight of the differentiable loss term to a small number."}, {"title": "D. Implementation details", "content": "Training details. We use Adam for the optimizer and a cosine schedule for the learning rate. Training lasts 40 epochs and is done on 8 A100 GPUs, with a total batch size of 256 samples. Each sample here refers to a full WOMD scenario of length 9 seconds. The agent is lightweight with 6M parameters altogether, similar to other models working over intermediate-level traffic scenario representations [36] (roadgraph, 2D boxes). Inference speed is almost 23K timesteps processed per second on a single GPU. Further details, including how the prediction tasks are formulated for the specific dynamics in Waymax [14], are available in the supplementary materials."}, {"title": "V. LIMITATIONS", "content": "While our setup yields strong results, it has limitations. Camera tokens. Raw camera images are unavailable in WOMD [11]. Recently however, camera token features have"}, {"title": "APPENDIX A", "content": "IMPLEMENTATION DETAILS\nThe task formulations presented in Section III are con-ceptual. We now discuss their concrete formulation in the Waymax simulator [14] for the bicycle dynamics, where the agent predicts acceleration a and steering \u03ba.\nBicycle model. In the bicycle model the agent state is given by (x, y, 0, vx, Vy), containing the (x, y) location in global coordinates, the yaw angle 0, and the velocities (vx, Vy) over the two global axes. The actions a = (\u0430, \u043a) consist of acceleration and steering. Given an action, the agent state evolves to its new values (x', y', \u03b8', \u03c5, \u03bd) as follows:\nx' = x + vx\u2206t + $\\frac{1}{2}$a $\\cos \\theta$ t\u00b2\ny' = y + vy\u2206t + $\\frac{1}{2}$a $\\sin \\theta$ t\u00b2\n$\\theta$' = $\\theta$ + \u043a [\u221a+At+ $\\frac{1}{2}$\u039e\u03b1 \u0394\u03b52]\nv' = \u221a2 + v + at\nv = v' $\\cos \\theta'$\nv = v' $\\sin \\theta'$.\n(10)\nWe study the dynamics by fixing a single transition from a trajectory and exploring the behavior of a predictor being trained with gradient descent to overfit on this transition.\nRelative odometry. For the relative odometry task in III-B, one can notice that, while st+1 \u2212 st is a guaranteed optimal, where the loss function attains a value of zero, it is not the only optimal point. Indeed, our results suggest that if the network f\u03b8 predicts a full agent state \u0161\u0165 = (x, y, 0, vx, vy), then there are many states \u0161\u0165 from which taking an action at will bring us to st+1 = (x',y',\u03b8', \u03c5, \u03c5\u2081). Intuitively, if (x,y) is predicted closer to (x', y'), then (vx, Vy) can also be predicted smaller. Under the fixed action, we do not need a large initial velocity to reach a point that is relatively close. However, what can happen is that (x, y) is predicted to be far from (x', y'), and (vx, vy) are also predicted to be large. From a more distant point the agent needs to have a larger initial velocity to reach the target point, under a fixed action at = (\u0430, \u043a).\nEmpirically, this means that even though gradient descent perfectly minimizes the loss, it may not learn to predict st+1 \u2212 st. Since it is useful to have precisely this difference learned by the network, and since at test time it is mostly the (x, y, 0) variables that matter for our planning, we limit our dynamics predictor f\u00f8 to predict only (\u2206x, \u2206y, \u2206\u03b8), and take the velocities to be fixed at their current values in st. This guarantees the prediction convergence to st+1 \u2212 st. Yet, we can supervise all five variables (x, y, \u03b8, vx, vy) of the simulated state, because the relative odometry task uses only transitions collected from the behavioral policy.\nIn brief, the concrete relative odometry task we solve is: given action (\u0430, \u043a), predict (\u2206x, \u0394y, \u2206\u03b8) such that the state (xt + \u2206x,yt + \u2206y,\u03b8t + \u0394\u03b8, vx,t, Vy,t) evolves to (xt+1, Yt+1, 0t+1, Ux,t+1, Vy,t+1). Formulated in this way, there is a unique global minimizer.\nOptimal inverse state. The setting is similar for the in-verse optimal state estimation, presented in Section III-D. One can either predict the full state (x, y, 0, vx, Vy), ob-taining one of possibly many solutions, or one can predict only the elements (x, y, 0), in which case whatever pre-diction gradient descent converges to will be the unique prediction. We opt for the latter. Our final prediction prob-lem is: given action (\u0430, \u043a), predict (\u2206x, \u0394y, \u2206\u03b8) such that the state (xt + \u2206x, yt + \u2206y,0t + \u22060,vx,t, Uy,t) evolves to (Xt+1, Yt+1, Ot+1, \u00dbx,t+1, \u00dby,t+1), where the target is, this time, the expert state. We apply the loss only over (x, y) elements because the target combination of (\u00eet+1, \u0177t+1, \u00dbx,t+1, Vy,t+1) may be, in general, unreachable from the current state.\nOptimal planning. For the optimal planner task in III-C, we study the inverse kinematics equations:\na = [\u221a\uc774\uae00+\uacf5-\u221a\ub8cc+0] /\u2206t\n$\\kappa$= (arctan\n-0)/[\u221a2+At+$\\frac{1}{2}$\\Xi a $\\Delta$$ t^2$\n(11)\nHere, it is enough to predict only (v, v\u00fd) in order to determine the action that brings the agent to the next state. In this setting, we have also found it useful to remove the typical action clipping applied, because in the cases where (u, v) are too large, the action obtained from the inverse kinematics will be clipped, which prevents gradients from flowing back and updating the model weights. To make this task have a unique minimizer for each transition, we predict (v, v\u00fd) and supervise only the (x, y, 0) elements.\nIn summary, these tasks require inverting the bicycle dynam-ics, which depending on the formulation, may not always be one-to-one. We need to condition the prediction on additional variables, so that the minimizers are unique. This requirement stems from the underlying dynamics model itself.\nCoordinate systems. Even though we phrase the world modeling tasks using actual simulator states, e.g. St, a \u2192 St+1, the precise inputs to our world modeling predictors fe are al-ways in the local time-dependent ego-vehicle reference frame. Additionally, one needs to take care to make sure that the state additions and subtractions in Eqns. 3, 4, and 5 are all done in the same coordinate frame."}]}