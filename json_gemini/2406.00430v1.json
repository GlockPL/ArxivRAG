{"title": "Evaluating Uncertainty-based Failure Detection for Closed-Loop LLM Planners", "authors": ["Zhi Zheng", "Qian Feng", "Hang Li", "Alois Knoll", "Jianxiang Feng"], "abstract": "Recently, Large Language Models (LLMs) have witnessed remarkable performance as zero-shot task planners for robotic manipulation tasks. However, the open-loop nature of previous works makes LLM-based planning error-prone and fragile. On the other hand, failure detection approaches for closed-loop planning are often limited by task-specific heuristics or following an unrealistic assumption that the prediction is trustworthy all the time. As a general-purpose resoning machine, LLMs or Multimodal Large Language Models (MLLMs) are promising for detecting failures. However, the appropriateness of the aforementioned assumption diminishes due to the notorious hullucination problem. In this work, we attempt to mitigate these issues by introducing a framework for closed-loop LLM-based planning called KnowLoop, backed by an uncertainty-based MLLMs failure detector, which is agnostic to any used MLLMs or LLMs. Specifically, we evaluate three different ways for quantifying the uncertainty of MLLMs, namely token probability, entropy, and self-explained confidence as primary metrics based on three carefully designed representative prompting strategies. With a self-collected dataset including various manipulation tasks and an LLM-based robot system, our experiments demonstrate that token probability and entropy are more reflective compared to self-explained confidence. By setting an appropriate threshold to filter out uncertain predictions and seek human help actively, the accuracy of failure detection can be significantly enhanced. This improvement boosts the effectiveness of closed-loop planning and the overall success rate of tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Pre-trained Large language models (LLMs) have shown superior generalization capability as zero-shot task planners in robot learning by transforming high-level language in-structions into low-level action plans [1]-[5]. However, these planners predominantly employ open-loop control, rigidly following initial plans without incorporating environmental feedback. Consequently, execution errors or plan deficiencies remain unaddressed, potentially leading to task failure.\nRecent works on facilitating closed-loop planning for LLM planners tend to be less attentive toward the indispens-able first step in closed-loop planning - failure detection [6]\u2013[9]. They usually adopt either LLMs [6]-[8] or Multimodal Large Language Models (MLLMs) [9] by assuming their predictions to be trustworthy all the time.\nThis simplified assumption might not hold due to the noto-rious hallucination problem of LLMs/MLLMs [10], meaning the inconsistencies between the generated contents and the factual truth or user input contexts. On the other hand, some heuristic-based failure detection [11] for LLM planners"}, {"title": "II. RELATED WORK", "content": "To lay the groundwork for a comprehensive understanding, we review the literature in the following relevant areas:\n1) Uncertainty Estimation in Robotics: Uncertainty esti-mation has a longstanding tradition in robotics, dating back to the era of probabilistic probiotics [20] and robotic intro-spection [21]. With the advent of deep learning, uncertainty estimation is gaining more and more attention in achieving trustworthy learning-enabled robotics due to the black-box nature of neural networks [12], [22], [23]. The spectrum of applying uncertainty estimation to empower robots spans from learning-based perception [15]-[17] to control [24]. Moreover, the significance of uncertainty estimation is in-creasingly prominent in the age of foundational models [14]. When it comes to LLMs for robotics, there is only a limited amount of work, such as KnowNo [25], which proposes a method to estimate the uncertainty of an LLM planer. It is done by generating multiple options as the next steps and analyzing their corresponding token probabilities with conformal prediction. However, they do not consider the failure of task execution itself.\n2) Uncertainty Estimation in LLMs: LLMs have shown an inevitable tendency to create hallucinations, which refers to the inconsistencies between the generated contents and the real-world truth or user inputs [10]. The model's uncertainty is demonstrated to be related to the occurrence of LLM hallucinations [26]. Using uncertainty estimation to predict LLM hallucination as a zero-resource setting prevents the need for external knowledge resources. [27] attempt to use Prompt Engineering to have the LLM output the confidence level of the answer simultaneously when speaking it out, as well as obtaining the model's confidence level based on the Consistency base method. [18] propose an LLM self-evaluation method in the form of multiple-choice questions or true/false statements to obtain a quality-calibrated confi-dence score at the token level. [27] introduce a confidence elicitation framework consisting of prompting, sampling, and aggregation strategies. The framework is evaluated with confidence calibration and failure prediction tasks. Their focuses are both on the general question-answer settings instead of robotic applications.\n3) Closed-loop planning with LLMs: LLMs can be har-nessed to translate high-level, abstract task instructions into actionable, step-by-step sequences for execution by agents. Recent studies [11], [28]-[31] have showcased the capa-bilities of LLMs for self-reflection and correction in re-sponse to environmental feedback. Here they often assume a ground truth environmental feedback. A prior work similar to ours [8] utilizes multisensory data and hierarchical summary to detect, reason, and correct failures in closed-loop robotic task planning in real-world scenarios. They rely on dedicated hand-crafted external failure detectors but we propose to estimate the uncertainty from a multimodal Large Language Model (MLLM) for more general failure detection, providing a more versatile and adaptable solution."}, {"title": "III. THE PROPOSED CLOSED-LOOP PLANNER", "content": "In this chapter, we outline KnowLoop, the proposed closed-loop control system's framework. Our implementation of an interactive failure detector is based on uncertainty derived from the MLLM. Specifically, we utilize open-source MLLMs such as LLaVA [19] and ChatGPT-4V for failure detection within an LLM planner based on ChatGPT-4. Note that the LLMs/MLLMs used in this work can be merged into one in principle, which we leave for future work. The procedure entails a decomposition of a language instruction into a series of sub-goals. After executing each sub-goal, the failure detector is activated to collect feedback from environment images and determine whether the sub-goal has been achieved. In the following, we will articulate the major components of the proposed uncertainty-based failure detec-tor, including the prompting strategies to activate the MLLM failure detector and approaches for uncertainty quantification in LLMs. Finally, we integrate the uncertainty-based failure detector into an LLM-based planning framework for closed-loop planning."}, {"title": "A. Uncertainty-based Failure Detection", "content": "To trigger an MLLM failure detector, a proper and infor-mative prompt is necessary. For this purpose, we specifically design two representative categories of prompting strategies, namely a direct one and an indirect one. Direct prompting strategy entails directly the question of success or failure, while the indirect one does not encompace this kind of direct question of failure or success in the context. Furthermore, to tackle the improper assumption of the failure detector, we consequently estimate the uncertainty of each response to these prompts. Based on the uncertainty estimates and a predetermined threshold, the reliability of each response can be evaluated quantitatively in a generic way. Responses with excessively high uncertainty will be filtered out, and instead, human feedback will be requested to assess the current state, as shown in Algorithm 1. More noteworthy, the proposed way to detect failure cases is agnostic to LLM used in the planner. Therefore, it enjoys the merit of general applicability when compared with other task-specific or heuristic approaches.\n1) Prompting Strategy: The impact of utilizing diverse prompts for an LLM is substantial, as the chosen prompt di-rectly shapes the model's output in manifold ways. To attain improved detection outcomes and reasonable uncertainty, it is crucial to meticulously select the prompts employed in the detection process. Toward this objective, we have carefully designed three distinct prompt strategies, articulated as follows.\nDirect Prompts via Subgoal State Comparison (SSC). Given only background information and environmental im-ages, inferring the state description upon completing an action is challenging for LLMs. Moreover, comparing this with the environmental images to determine if the current state aligns with the subgoal poses additional challenges. Consequently, we propose to succinctly outline each subgoal or action, accompanied by an expected state description. The MLLM will then compare this description against the current environmental image to ascertain whether the present state matches the anticipated outcome. This indicates the successful achievement of the subgoal.\nDirect Prompts via Spatial Relationship Analysis (SRA). The \"Chain of Thought\u201d (CoT) [32] reasoning in LLMs epitomizes a sophisticated strategy for tackling complex problem-solving tasks. This technique capitalizes on the model's capacity to generate intermediate steps or rationales leading to a conclusive answer, thereby emulating a process akin to human reasoning. In the context of robot manipulation tasks, the most profound transformations are observed in the dynamics between the gripper and the spatial positioning of objects following each action. Rather than directly evaluating the outcome as success or failure based on state descriptions and images, the MLLM should incorporate an intermediate cognitive process. This analyzes the current spatial positions and conditions of objects within the environment, comparing these with the anticipated states and subsequently adjudicating the action's success or failure.\nIndirect Prompts via Next Action Prediction(NAC). To distinguish this strategy from the preceding ones, success is no longer binary (a simple yes or no). Instead, upon completing each action, we present all high-level plans and the corresponding executable actions to the MLLM. Then, images are supplied to illustrate the current state of the environment. Based on the current state, the MLLM is asked to select the subsequent action from the presented options. This approach reformulates the detection problem into a multiple-choice scenario. Success is achieved when the MLLM's selection aligns with the subsequent step of the predefined plan. Conversely, the selection of any alternative action is deemed an execution failure.\n2) Uncertainty Quantification: The precision of the fail-ure detector's judgments is pivotal for the success of the entire closed-loop control system. Should a true negative scenario arise, it may cause the system to miss errors during execution, persisting with the initial plan and leading to task failure. In contrast, a false negative can interrupt the correct sequence of operations, leading to erroneous states that hinder the task's completion. Hence, the greater the failure detector's accuracy, the more effective the system. Nonethe-less, even the most advanced MLLMs, like ChatGPT-4V, fail to achieve flawless accuracy and may produce hallucinations, generating responses arbitrarily. We aim to quantify the uncertainty in the model's responses to better gauge its confidence in its answers. When the model is uncertain, expressing \"I don't know\" is preferable to providing an arbitrary response.\nToken probability. Prior work [25] suggests that, by for-mulating multiple-choice questions and answering (MCQA) and having LLMs' responses to them, it is feasible to deduce the occurrence probability of each option token P(xi). This probability is interpreted as the likelihood of the correspond-ing option being correct. In the context of failure detection, the issue can be conceptualized as a binary question: whether the outcome of the most recent action was successful or if the current state aligns with the anticipated state. The response sought is either 'Yes' or 'No'. In a similar vein, deriving the token probability from the MLLM output can act as an indicator of the probability of affirmative or negative responses.\nEntropy. In information theory, entropy represents a mea-sure of uncertainty or disorder. For binary classification prob-lems, the entropy of a model's predictions can be calculated using the formula:\n$H(X) = - \\sum P(x_i) \\log P(x_i)$   (1)\nwhere P(xi) is the probability of class i (in case of failure detection, \"Yes\" or \"No\"). Within the output layer of the MLLM, each token is assigned a score. When subjected to a softmax transformation, these scores yield the probability associated with each token. The probability of a token can thus be interpreted as the probability of a given classifi-cation. The entropy derived from these token probabilities may reflect the model's uncertainty about its response to a certain extent. Higher entropy values suggest greater model uncertainty. This relationship between entropy and model uncertainty will be further explored and discussed in the subsequent sections.\nSelf-explained Confidence. In certain instances, LLMs can be induced to articulate their confidence levels explicitly. For example, the model might be prompted to deliver re-sponses in the format: \"I am X% certain that the answer is Y,\u201d wherein X represents the model's self-evaluated confidence level. This approach is contingent upon meticulous prompt engineering. Given that it hinges on the model's inherent processes for comprehending and generating confidence-related responses, it may not consistently produce precise confidence estimations."}, {"title": "B. Closed-loop Planning", "content": "In the final proposed closed-loop control framework, ChatGPT-4 is used as the high-level task planner. The LLM planner understands language instructions and decomposes them into multiple subtasks. GPT's code generation function will be called for each subtask, and combined with the robotic arm motion API, it will generate corresponding Python code for each action. Subsequently, the robotic arm will execute actions one by one according to the resultant task plan. After each action execution, the detector will be called to assess the state of the environment and detect the failure. If the assessment indicates successful execution, the robotic arm will proceed with the following action as planned.\nIf the assessment indicates failure, the robotic arm will stop the current action and re-execute all actions as shown in Algorithm 2."}, {"title": "IV. EXPERIMENT", "content": "We first show our experimental setup including the self-collected dataset and implementation details of the system and algorithm. Then we discuss the experiment results. We first evaluate the performance of three different uncertainty estimation methods along with different promoting strategies based on a self-collected dataset.\nSecondly, based on the results obtained in the uncertainty evaluation part, we select the combination with the most encouraging performance for closed-loop LLM planners on real hardware. To note that the test set includes slightly different tasks\u00b9 but with similar items. The LLM initiates high-level planning upon receiving language instructions, decomposing the tasks into several subtasks. In scenarios devoid of external interference, the robotic arm's adherence to the high-level action plan results in a success rate ranging between 70-80% for these tasks. To illustrate the significance of closed-loop control more effectively, each experiment will"}, {"title": "A. Dataset", "content": "Valid uncertainty implies that higher model uncertainty correlates with decreased prediction accuracy. Conversely, as the model's confidence increases, its accuracy significantly improves. Additionally, the applicability of this uncertainty in robotic manipulation tasks remains uncertain. To investigate this more efficiently, we collected 142 samples representing different execution states across five tasks\u00b2 in Table I. The primary failures encompass detection and operational errors, explicitly excluding planning errors. The tasks primarily involve actions such as grasping, placing, and pushing."}, {"title": "B. Evaluation Metrics", "content": "In this subsection, we explain the metrics used for evalu-ating different variants in the experiment.\na) Uncertainty-Acuracy Curve and Area Under Curve (AUC) (Calibration-AUC): The uncertainty-accuracy curve [18] depicts accuracy as a function of the abstention uncertainty threshold. This threshold determines the point at which samples are excluded from consideration if their uncertainty surpasses (1 - threshold). Initially, no samples are omitted when the uncertainty threshold is set to zero, allowing the accuracy measurement to reflect the entire dataset's performance. As the uncertainty threshold escalates, samples exhibiting uncertainty above (1-threshold) are selectively discarded. Subsequently, the accuracy metric is recalculated to reflect the composition of the residual dataset. Should uncertainty serve as a reliable indicator of the model's uncertainty, discarding samples marked by higher uncertainty should theoretically enhance the quality of the retained samples. Consequently, this selective exclusion is expected to improve the model's overall accuracy.\nb) Selective Generation Curve and AUC (Selective-AUC): The selective generation curve [18] depits accuracy as a function of the abstention rate, denoted as a. This approach involves ordering the samples by their uncertainty and then abstaining from the top a% of samples based on their uncertainty values. No samples are abstained at a = 0%, meaning the curve initiates at the conventional accuracy metric. Consequently, an increase in the curve is anticipated as a escalates.\nc) Success Rate: For each real-world task, it is consid-ered a success if the instruction requirements are eventually met under our closed-loop control. If the task remains incomplete after reaching the maximum number of attempts,"}, {"title": "D. Results", "content": "1) Uncertainty Estimation for MLLM Failure Detecor: We first evaluate the MLLM failure detector based on LLaVA, as we need token probability for uncertainty cal-culation which is not supported by ChatGPT-4V API at the moment. We document each sample's prediction outcomes and token probability for each prompting strategy. Upon quantification, the utility of LLaVA's entropy, as derived through the techniques mentioned above, in differentiating between high-quality and low-quality predictions remains ambiguous. Consequently, we employ the following criteria to assess the correlation between the uncertainty derived from various prompting and quantification methods and the accuracy of the predictions"}, {"title": "V. CONCLUSION", "content": "In this study, we have demonstrated that uncertainty quan-tified by token probability, and entropy is able to reflect the predictive quality of the MLLM LLaVA. We further integrate this into an MLLM failure detector within a zero-shot LLM planner to facilitate closed-loop planning \u2013 a framework dubbed KnowLoop. With this, lower-quality responses can be effectively identified using a threshold. Our proposed framework can mitigate model illusions or overconfidence, thereby enhancing model accuracy. Such improvements ren-der it suitable for the detection phase of closed-loop con-trol systems. For future work, it would be more resource-efficient to use the MLLM to substitute the LLM in the task planner. Moreover, conformal prediction can be employed to determine the threshold in the framework with a statistical guarantee [25]. In addition, investigating how uncertainty estimation can be utilized for failure reasoning and correction is an important next step in setting up a robust and effective closed-loop system."}]}