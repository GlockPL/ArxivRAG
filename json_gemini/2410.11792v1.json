{"title": "OKAMI: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation", "authors": ["Jinhan Li", "Yifeng Zhu", "Yuqi Xie", "Zhenyu Jiang", "Mingyo Seo", "Georgios Pavlakos", "Yuke Zhu"], "abstract": "We study the problem of teaching humanoid robots manipulation skills by imitating from single video demonstrations. We introduce OKAMI, a method that generates a manipulation plan from a single RGB-D video and derives a policy for execution. At the heart of our approach is object-aware retargeting, which enables the humanoid robot to mimic the human motions in an RGB-D video while adjusting to different object locations during deployment. OKAMI uses open-world vision models to identify task-relevant objects and retarget the body motions and hand poses separately. Our experiments show that OKAMI achieves strong generalizations across varying visual and spatial conditions, out-performing the state-of-the-art baseline on open-world imitation from observation. Furthermore, OKAMI rollout trajectories are leveraged to train closed-loop visuomotor policies, which achieve an average success rate of 79.2% without the need for labor-intensive teleoperation. More videos can be found on our website https://ut-austin-rpl.github.io/OKAMI/.", "sections": [{"title": "1 Introduction", "content": "Deploying generalist robots to assist with everyday tasks requires them to operate autonomously in natural environments. With recent advances in hardware designs and increased commercial availability, humanoid robots emerge as a promising platform to deploy in our living and working spaces. Despite their great potential, they still struggle to operate autonomously and deploy robustly in the unstructured world. A burgeoning line of work has resorted to deep imitation learning methods for humanoid manipulation [1-3]. However, they rely on large amounts of demonstrations through whole-body teleoperation, requiring domain expertise and strenuous efforts. In contrast, humans have the innate ability to watch their peers do a task once and mimic the behaviors. Equipping robots with the ability to imitate from visual observations will move us closer to the goal of training robotic foundation models from Internet-scale human activity videos.\nWe explore teaching humanoid robots to manipulate objects by watching humans. We consider a problem setting recently formulated as \"open-world imitation from observation,\" where a robot im-itates a manipulation skill from a single video of human demonstration [4-6]. This setting would facilitate users in effortlessly demonstrating tasks and enable a humanoid robot to acquire new skills quickly. Enabling humanoids to imitate from single videos presents a significant challenge the video does not have action labels, but yet the robot has to learn to perform tasks in new situations beyond what's demonstrated in the video. Prior works on one-shot video learning have attempted to optimize robot actions to reconstruct the future object motion trajectories [4, 5]. However, they have been applied to single-arm manipulators and are computationally prohibitive for humanoid robots due to their high degrees of freedom and joint redundancy [7]. Meanwhile, the similar kinematic structure shared by humans and humanoids makes directly retargeting human motions to robots fea-sible [8, 9]. Nonetheless, existing retargeting techniques focus on free-space body motions [10-14], lacking the contextual awareness of objects and interactions needed for manipulation. To address this shortcoming, we introduce the concept of \u201cobject-aware retargeting\". By incorporating object contextual information into the retargeting process, the resulting humanoid motions can be effi-ciently adapted to the locations of objects in open-ended environments.\nTo this end, we introduce OKAMI (Object-aware Kinematic retArgeting for humanoid Imitation), an object-aware retargeting method that enables a bimanual humanoid with two dexterous hands to imitate manipulation behaviors from a single RGB-D video demonstration. OKAMI uses a two-stage process to retarget the human motions to the humanoid robot to accomplish the task across varying initial conditions. The first stage processes the video to generate a reference manipulation plan. The second stage uses this plan to synthesize the humanoid motions through motion retargeting that adapts to the object locations in target environments.\nOKAMI consists of two key designs. The first design is an open-world vision pipeline that iden-tifies task-relevant objects, reconstructs human motions from the video, and localizes task-relevant objects during evaluation. Localizing objects at test time also enables motion retargeting to adapt to different backgrounds or new object instances of the same categories. The second design is the factorized process for retargeting, where we retarget the body motions and hand poses separately. We first retarget the body motions from the reference plan in the task space, and then warp the retar-geted trajectory given the location of task-relevant objects. The trajectory of body joints is obtained through inverse kinematics. The joint angles of fingers are mapped from the plan onto the dexterous hands, reproducing hand-object interaction. With object-aware retargeting, OKAMI policies sys-tematically generalize across various spatial layouts of objects and scene clutters. Finally, we train visuomotor policies on the rollout trajectories from OKAMI through behavioral cloning to obtain vision-based manipulation skills.\nWe evaluate OKAMI on human video demonstrations of diverse tasks that cover rich object inter-actions, such as picking, placing, pushing, and pouring. We show that its object-aware retargeting achieves 71.7% task success rates averaged across all tasks and outperforms the ORION [4] baseline by 58.3%. We then train closed-loop visuomotor policies on the trajectories generated by OKAMI, achieving an average success rate of 79.2%. Our contributions of OKAMI are three-fold:\n1. OKAMI enables a humanoid robot to mimic human behaviors from a single video for dexterous manipulation. Its object-aware retargeting process generates feasible motions of the humanoid robot while adapting the motions to target object locations at test time;\n2. OKAMI uses vision foundation models [15, 16] to identify task-relevant objects with-out additional human inputs. Their common-sense reasoning ability helps recognize task-"}, {"title": "2 Related Work", "content": "Humanoid Robot Control. Methods like motion planning and optimal control have been devel-oped for humanoid locomotion and manipulation [10, 12, 17]. These model-based approaches rely on precise physical modeling and expensive computation [11, 12, 18]. To mitigate the stringent requirements, researchers have explored policy training in simulation and sim-to-real transfer [10, 19]. However, these methods still require a significant amount of labor and expertise in designing simulation tasks and reward functions, limiting their successes to locomotion domains. In parallel to automated methods, a variety of human control mechanisms and devices have been developed for humanoid teleoperation using motion capture suits [9, 12, 20-24], telexistence cockpits [25-29], VR devices [1, 30, 31], or videos that track human bodies [17, 32]. While these systems can control the robots to generate diverse behaviors, they require real-time human input that poses significant cognitive and physical burdens. In contrast, OKAMI only requires single RGB-D human videos to teach the humanoid robot new skills, significantly reducing the human cost.\nImitation Learning for Robot Manipulation. Imitation Learning has significantly advanced vision-based robot manipulation with high sample efficiency [33-44]. Prior works have shown that robots can learn visuomotor policies to complete various tasks with just dozens of demonstrations, ranging from long-horizon manipulation [34-36] to dexterous manipulation [37-39]. However, collecting demonstrations often requires domain expertise and high costs, creating challenges to scale. Another line of work focuses on one-shot imitation learning [40-44], yet they demand excessive data collection for meta-training tasks. Recently, researchers have looked into a new problem setting of imitating from a single video demonstration [4-6], referred to as \"open-world imitation from observation\" [4]. Unlike prior works that abstract away embodiment motions due to kinematic differences between the robot and the human, we exploit embodiment motion information owing to the kinematic similarity between humans and humanoids. Specifically, we introduce object-aware retargeting that adapts human motions to humanoid robots.\nMotion Retargeting. Motion retargeting has wide applications in computer graphics and 3D vision [8], where extensive literature studies how to adapt human motions to digital avatars [45-47]. This technique has been adopted in robotics for recreating human-like motions on humanoid or anthropomorphic robots through various retargeting methods, including optimization-based ap-proaches [11, 12, 20, 48], geometric-based methods [49], and learning-based techniques [10, 13, 17]. However, in manipulation tasks, these retargeting methods have been used within teleoperation systems, lacking a vision pipeline for automatic adaptation to object locations. OKAMI integrates the retargeting process with open-world vision, endowing it with object awareness so that the robot can mimic human motions from video demonstrations and adapt to object locations at test time."}, {"title": "3 OKAMI", "content": "In this work, we introduce OKAMI, a two-staged method that tackles open-world imitation from observation for humanoid robots. OKAMI first generates a reference plan using the object locations and reconstructed human motions from a given RGB-D video. Then, it retargets the human motion trajectories to the humanoid robot while adapting the trajectories based on new locations of the objects. Figure 2 illustrates the whole pipeline.\nProblem Formulation We formulate a humanoid manipulation task as a discrete-time Markov Decision Process defined by a tuple: $M = (S, A, P, R, \\gamma, \\mu)$, where S is the state space, A is the action space, $P(. s, a)$ is the transition probability, R(s) is the reward function, $\\gamma \\in [0,1)$ is the discount factor, and u is the initial state distribution. In our context, S is the space of raw RGB-D observations that capture both the robot and object states, A is the space of the motion commands for the humanoid robot, R is the sparse reward function that returns 1 when a task is complete. The objective of solving a task is to find a policy \u3160 that maximizes the expected task success rates from a wide range of initial configurations drawn from \u00b5 at test time.\nWe consider the setting of \"open-world imitation from observation\" [4], where the robot system takes a recorded RGB-D human video, V as input, and returns a humanoid manipulation policy \u03c0 that completes the task as demonstrated in V. This setting is \"open-world\" as the robot does not have prior knowledge or ground-truth access to the categories or physical states of objects involved in the task, and it is \"from observation\" in the sense that video V does not come with any ground-truth robot actions. A policy execution is considered successful if the state matches the state of the final frame from V. The success conditions of all tested tasks are described in Appendix B.1. Notably, two assumptions are made about V in this paper: all the image frames in V capture the human bodies, and the camera view of shooting V is static throughout the recording."}, {"title": "3.1 Reference Plan Generation", "content": "To enable object-aware retargeting, OKAMI first generates a reference plan for the humanoid robot to follow. Plan generation involves understanding what task-relevant objects are and how humans manipulate them.\nIdentifying and Localizing Task-Relevant Objects. To imitate manipulation tasks from videos V, OKAMI must identify the task-relevant objects to interact with. While prior methods rely on un-supervised approaches with simple backgrounds or require additional human annotations [50-53], OKAMI uses an off-the-shelf Vision-Language Models (VLMs), GPT-4V, to identify task-relevant objects in V by leveraging the commonsense knowledge internalized in the model. Concretely, OKAMI obtains the names of task-relevant objects by sampling RGB frames from the video demonstration V and prompting GPT-4V with the concatenation of these images (details in Appendix A.2). Using these object names, OKAMI employs Grounded-SAM [16] to segment the objects in the first frame and track their locations throughout the video using a Vidoe Object"}, {"title": "3.2 Object-Aware Retargeting", "content": "Given a reference plan from the video demonstration, OKAMI enables the humanoid robot to imi-tate the task in V. The robot follows each step li in the plan by localizing task-relevant objects and retargeting the SMPL-H trajectory segment onto the humanoid. The retargeted trajectories are then converted into joint commands through inverse kinematics. This process repeats until all the steps are executed, and success is evaluated based on task-specific conditions (see Appendix B.1).\nLocalizing Objects at Test Time. To execute the plan in the test-time environment, OKAMI must localize the task-relevant objects in the robot's observations, extracting 3D point clouds to track object locations. By attending to task-relevant objects, OKAMI policies generalize across various visual conditions, including different backgrounds or the presence of novel instances of task-relevant objects.\nRetargeting Human Motions to the Humanoid. The key aspect of object-awareness is adapting motions to new object locations. After localizing the objects, we employ a factorized retargeting process that synthesizes arm and hand motions separately. OKAMI first adapts the arm motions to the object locations so that the fingers of the hands are placed within the object-centric coordinate frame. Then OKAMI only needs to retarget fingers in the joint configuration to mimic how the demonstrator interacts with objects with their hands.\nConcretely, we first map human body motions to the task space of the humanoid, scaling and adjust-ing trajectories to account for differences in size and proportion. OKAMI then warps the retargeted trajectory so that the robot's arm reaches the new object locations (More details in Appendix A.5). We consider two cases in trajectory warping when the relational state between target and refer-ence objects is unchanged and when it changes, adjusting the warping accordingly. In the first case,"}, {"title": "4 Experiments", "content": "Our experiments are designed to answer the following research question: 1) Is OKAMI effective for a humanoid robot to imitate diverse manipulation tasks from single videos of human demonstra-tion? 2) Is it critical in OKAMI to retarget the body motions of demonstrators to the humanoid robot instead of only retargeting based on object locations? 3) Can OKAMI retain its performances con-sistently on videos demonstrated by humans of diverse demographics? 4) Can the rollouts generated by OKAMI be used for training closed-loop visuomotor policies?"}, {"title": "4.1 Experimental Setup", "content": "Task Designs. We describe the six tasks we use in the experiments: 1) Plush-toy-in-basket: placing a plush toy in the basket; 2) Sprinkle-salt: sprinkling a bit of salt into the bowl; 3) Close-the-drawer: pushing the drawer in to close it; 4) Close-the-laptop: closing the lid of the laptop; 5) Place-snacks-on-plate: placing a bag of snacks on the plate. 6) Bagging: placing a chip bag into a shopping bag. We select these six tasks that cover a diverse range of manipulation behaviors: Plush-toy-in-basket and Place-snacks-on-plate require pick-and-place behaviors of daily objects; Sprinkle-salt is the task that covers pour-ing behavior; Close-the-drawer and Close-the-laptop require the humanoid to interact with articulated objects, a prevalent type of interaction in daily environments; Bagging involves dexterous, bimanual manipulation and includes multiple subgoals. While we mainly focus on real\nHardware Setup. We use a Fourier GR1 robot as our hardware platform, equipped with two 6-DoF Inspire dexterous hands and a D435i Intel RealSense camera for video recording and test-time obser-vation. We implement a joint position controller that operates at 400Hz. To avoid jerky movements, we compute joint position commands at 40Hz and interpolate the commands to 400Hz trajectories.\nEvaluation Protocol. We run 12 trials for each task. The locations of the objects are randomly initialized within the intersection of the robot camera's view and the humanoid arms' reachable range. The tasks are evaluated on a tabletop workspace with multiple objects, including both task-relevant objects and various other objects. Further, we test new object generalization on Place-snacks-on-plate, Plush-toy-in-basket, and Sprinkle-salt tasks, chang-ing the involved plate, snack bag, plush toy, and bowl to other instances of the same type.\nBaselines. We compare our result with a baseline ORION [4]. Since ORION was proposed for parallel-jaw grippers, it is not directly applicable in our experiments and we adopt it with minimal modifications: we estimate the palm trajectory using the SMPL-H trajectories, and warp the tra-jectory conditioning on the new object locations. The warped trajectory is used in the subsequent inverse kinematics for computing robot joint configurations."}, {"title": "4.2 Quantitative Results", "content": "To answer question (1), we evaluate the policies of OKAMI across all the tasks, covering diverse behaviors such as daily pick-place, pouring, and manipulation of articulated objects. The results are presented in Figure 4(a). In our experiment, we randomly initialize the object locations so that the robot needs to adapt to the locations of the objects. This result shows the effectiveness of OKAMI in generalizing over different visual and spatial conditions.\nTo answer question (2), we compare OKAMI against ORION on two representative tasks, Place-snacks-on-plate and Close-the-laptop. In the comparison experiment, OKAMI differs from ORION in that ORION does not condition on the human body poses. OKAMI achieves 75.0% and 83.3% success rates, respectively, while ORION only achieves 0.0% and 41.2%, respectively. Additionally, we compare OKAMI against ORION on the two simulated versions of Sprinkle-salt and Close-the-drawer tasks. In simulation, OKAMI achieves 82.0% and 84.0% success rates in two tasks while ORION only achieves 0.0% and 10.0%. Most failures of ORION policies are due to failing to approach objects with reliable grasping poses (e.g., in Place-snacks-on-plate task, ORION tries to grasp the snack from the sides instead of the top-down grasp in human video), and failing to rotate the wrist fully to achieve behaviors such as pouring. These behaviors originate from the fact that ORION ignores the embodiment information, thus falling short in performance compared to OKAMI. The superior performance of OKAMI sug-gests the importance of retargeting the body motion of the human demonstrators onto the humanoid when imitating from human videos.\nTo answer question (3), we conduct a controlled experiment of recording videos of different demonstrators and test if OKAMI policies maintain strong performance across the video inputs."}, {"title": "4.3 Learning Visuomotor Policy With OKAMI Rollout Data", "content": "We address question (4) by training neural visuomotor policies on OKAMI rollouts. We first run OKAMI over randomly initialized object layouts to generate multiple rollouts and collect a dataset of successful trajectories while discarding the failed ones. We train neural network policies on this dataset through a behavioral cloning algorithm. Since smooth execution is critical for humanoid manipu-lation, we implement the behavioral cloning with ACT [61], which predicts smooth actions via its temporal ensemble design, a trajectory smoothing component (more implementation details in Appendix B.5). We train visuomotor policies for Sprinkle-salt and Bagging. Figure 5 illustrates the success rates of these policies, demonstrating that OKAMI rollouts are effective data sources for training. We also show that the learned policies improve as more rollouts are collected. These results hold the promise of scaling up data collection for learning humanoid manipulation skills without laborious teleoperation."}, {"title": "5 Conclusion", "content": "This paper introduces OKAMI that enables a humanoid robot to imitate a single RGB-D human video demonstration. At the core of OKAMI is object-aware retargeting, which retargets the human motions onto the humanoid robot and adapts the motions to the target object locations. OKAMI consists of two stages to realize object-aware retargeting. The first stage is generating a reference plan for manipulation from the video. The second stage is used for retargeting, where OKAMI retargets the arm motions in the task space and the finger motions in the joint configuration space. Our experiments validate the design of OKAMI, showing the systematic generalization of OKAMI policies. OKAMI enables efficient collection of trajectory data based on a single human video demonstration. OKAMI-based data collection significantly reduces the human cost for policy train-ing compared to that required by teleoperation.\nLimitations and Future Work. The current focus of OKAMI is on the upper body motion retargeting of humanoid robots, particularly for manipulation tasks within tabletop workspaces. A promising future direction is to include lower body retargeting that enables locomotion behaviors during video imitation. To enable full-body loco-manipulation, a whole-body motion controller needs to be implemented as opposed to the joint position controller used in OKAMI. Additionally, we rely on RGB-D videos in OKAMI, which limits us from using in-the-wild Internet videos recorded in RGB. Extending OKAMI to use web videos will be another promising direction for future works. At last, the current implementation of retargeting has limited robustness against large variations in object shapes. A future improvement would be integrating more powerful foundation models that endow the robot with a general understanding of how to interact with a class of objects in spite of their large shape changes."}, {"title": "A Implementation Details", "content": "A.1 Human Reconstruction From Videos\nMethod. For the 3D human reconstruction, we start by tracking the person in the video and getting an initial estimate of their 3D body pose using 4D Humans [62]. This body reconstruction cannot capture the hand pose details (i.e., the hands are flat). Therefore, for each detection of the person in the video, we detect the two hands using ViTPose [63], and for each hand, we apply HaMeR [57] to get an estimate of the 3D hand pose. However, the hands reconstructed by HaMeR can be incon-sistent with the arms from the body reconstruction (e.g., different wrist orientation and location). To address this, we apply an optimization refinement to make the body and the hands consistent in each frame, and encourage that the holistic body and hands motion is smooth over time. This op-timization is similar to SLAHMR [55], with the difference that besides the body pose and location of the SMPL+H model [56], we also optimize the hand poses. We initialize the procedure using the 3D body pose estimate from 4D Humans and the 3D hand poses from HaMeR. Moreover, we use the 2D projection of the 3D hands predicted by HaMeR to constrain the projection of the 3D hand keypoints of the holistic model using a reprojection loss. Finally, we can jointly optimize all the parameters (body location, body pose, hand poses) over the duration of the video, as described in SLAHMR [55].\nOur modified SLAHMR incorporates the SMPL-H model [56] to include hand poses in the hu-man motion reconstruction. We initialize hand poses in each frame using 3D hand estimates from HaMeR [57]. The optimization process then jointly refines body locations, body poses, and hand poses over the video sequence. This joint optimization allows for accurate modeling of how hands interact with objects, which is crucial for manipulation tasks.\nThe optimization minimizes the error between the 2D projections of the 3D joints from the SMPL-H model and the detected 2D joint locations from the video. We use standard parameters and settings as described in SLAHMR [55], adapting them to accommodate the SMPL-H model.\nInference Requirements. The model of human reconstruction we use is large and needs to be run on a computer with sufficiently good computation speed. Here we provide details about the runtime performance of the human reconstruction model. We use a desktop that comes with a GPU RTX3090 that has the size of the memory 24 GB. For a 10 seconds video with fps 30, it processes 10 minutes."}, {"title": "A.2 Prompts of Using GPT4V", "content": "In order to use GPT4V in OKAMI, we need GPT4V's output to be in a typed format so that the rest of the programs can parse the result. Moreover, in order for the prompts to be general across a diverse set of tasks, our prompt does not leak any task information to the model. Here we describe the three different prompts in OKAMI for using GPT4V.\nIdentify Task-relevant Objects. OKAMI uses the following prompt to invoke GPT4V so that it can identify the task-relevant objects from a provided human video:\nPrompt: You need to analyze what the human is doing in the images, then tell me: 1. All the objects in front scene (mostly on the table). You should ignore the background objects. 2. The objects of interest. They should be a subset of your answer to the first question. They are likely the objects manipulated by human or near human. Note that there are irrelevant objects in the scene, such as objects that does not move at all. You should ignore the irelevant objects.\nYour output format is:\n```json\n{\n \"object\": [\"OBJECT1\", \"OBJECT2\", ...],\n```\nEnsure the response can be parsed by Python 'json.loads', e.g.: no trailing commas, no single quotes, etc. You should output the names of objects of interest in a list [\u201cOBJECT1\u201d, \u201c\u041e\u0412-JECT2\", ...] that can be easily parsed by Python. The name is a string, e.g., \"apple\", \"pen\u201d, \"keyboard\", etc.\nIdentify Target Objects. OKAMI uses the following prompt to identify the target object of each step in the reference plan:\nPrompt: The following images shows a manipulation motion, where the human is manipulating an object.\nYour task is to determine which object is being manipulated in the images below. You need to choose from the following objects: {a list of task-relevant objects}.\nTips: the manipulated object is the object that the human is interacting with, such as picking up, moving, or pressing, and it is in contact with the human's {the major moving arm in this step} hand.\nYour output format is:\n```json\n{\n  \"manipulate_object_name\": \"MANIPULATE_OBJECT_NAME\",\n```\nEnsure the response can be parsed by Python 'json.loads', e.g.: no trailing commas, no single quotes, etc.\nIdentify Reference Objects. Here is the prompt that asks GPT4V to identify the reference object of each step in the reference plan:\nPrompt: The following images shows a manipulation motion, where the human is manipulating the object {manipulate_object_name}.\nPlease identify the reference object in the image below, which could be an object on which to place {manipulate_object_name}, or an object that {manipulate_object_name} is interacting with. Note that there may not necessarily have an reference object, as sometimes human may just playing with the object itself, like throwing it, or spinning it around. You need to first identify whether there is a reference object. If so, you need to output the reference object's name chosen from the following objects: {a list of task-relevant objects}.\nYour output format is:\n```json\n{\n  \"reference_object_name\": \"REFERENCE_OBJECT_NAME\" or \"None\",\n```\nEnsure the response can be parsed by Python 'json.loads', e.g.: no trailing commas, no single quotes, etc.\""}, {"title": "A.3 Details on Factorized Process for Retargeting", "content": "Body Motion Retarget. To retarget body motions from the SMPL-H representation to the hu-manoid, we extract the shoulder, elbow, and wrist poses from the SMPL-H models. We then use inverse kinematics to solve the body joints on the humanoid, ensuring they produce similar shoulder and elbow orientations and similar wrist poses. The inverse kinematics is implemented using an open-sourced library Pink [64]. The IK weights we use for shoulder orientation, elbow orientation, wrist orientation, and wrist position are 0.04, 0.04, 0.08, and 1.0, respectively.\nHand Pose Mapping. As we describe in the method section, we first retarget the hands from SMPL-H models to the humanoid's dexterous hands using a hybrid implementation of inverse kine-matics and angle mapping. Here are the details of how this mapping is performed. Once we obtain the SMPL-H models from a video demonstration, we can obtain the locations of 3D joints from the hand mesh models from SMPL-H. Subsequently, we can compute the rotating angles of each joint that correspond to certain hand poses. Then we apply the computed joint angles to the hand meshes of a canonical SMPL-H model, which is pre-defined to have the same size as the humanoid robot hardware. From this canonical SMPL-H model, we can get the 3D keypoints of hand joints and use an existing package, dex-retarget, an off-the-shelf optimization package to directly compute the hand joint angles of the robot [65].\nInverse Kinematics. After warping the arm trajectory, we use inverse kinematics to compute the robot's joint configurations. We assign weights of 1.0 to hand position and 0.08 to hand rotation, prioritizing accurate hand placement while allowing the arms to maintain natural postures.\nFor retargeting human hand poses to the robot, we map the human hand joint angles to the corre-sponding joints in the robot's hand. This enables the robot to replicate fine-grained manipulations demonstrated by the human, such as grasping and object interaction. Our implementation ensures that the retargeted motions are physically feasible for the robot and that overall execution appears natural and effective for the task at hand."}, {"title": "A.4 Additional Details of Plan Generation", "content": "For temporal segmentation, we sample keypoints from the segmented objects in the first frame and track them across the video using CoTracker [58]. We compute the average velocity of these keypoints at each frame and apply an unsupervised changepoint detection algorithm [66] to detect significant changes in motion, identifying keyframes that correspond to subgoal states.\nTo determine contact between objects, we compute the relative spatial locations and distances be-tween the point clouds of objects. If the distance between objects falls below a predefined threshold, we consider them to be in contact. For non-contact relations that are difficult to infer geometri-cally-such as a cup in a pouring task-we use GPT4V to predict semantic relations based on the visual context. GPT4V can infer that the cup is the recipient in a pouring action even if there is no direct contact."}, {"title": "A.5 Trajectory Warping", "content": "Here, we mathematically describe the process of trajectory warping. We denote the trajectory for robot as $T^{robot}_{ti}$ retargeted from $T^{SMPL}_{ti}$ in the generated plan. Denote the starting point and end point of robot as $p_{start}, p_{end}$, respectively. Note that all points along the trajectory are represented in SE(3) space.\nEach point pt on the original retargetd trajectory can be described by the following function:\n$p_t = p_{start} + (T^{robot}(t) - p_{start})$\nwhere $t \\in \\{t_i,..., t_{i+1}\\}, T^{robot}(t_i) = p_{start}, T^{robot}(t_{i+1}) = p_{end}$.\nWhen warping the trajectory, we either only needs to adapt the trajectory to the new target object location, or adapt the trajectory to the new locations of both the target and the reference objects,\nas described in Section 3.2. Without loss of generality, we denote the SE(3) transformation for the starting point is $T_{start}$, and the SE(3) transformation for the end point is $T_{end}$. Now the warped trajectory can be described by the following function:\n$p_t = T_{start} \\cdot p_{start} + (T^{robot}(t) - T_{start} \\cdot p_{start})$\nwhere $T^{robot}(t) = \\frac{T^{robot}(t)-p_{start}}{p_{end}-p_{start}} (T_{end} p_{end} - T_{start} p_{start}) +T_{start} p_{start}, t \\in \\{t_i,...,t_{i+1}\\}$. In this way, we have $T^{robot}(t_i) = T_{start} \\cdot p_{start}, T^{robot}(t_{i+1}) = T_{end} \\cdot p_{end}$. Note that this trajectory warping assumes the end point of a trajectory is not the same as the starting point, which is a common assumption for most of the manipulation behaviors."}, {"title": "B Additional Experimental Details", "content": "B.1 Success Conditions\nWe describe the success conditions we use to evaluate if a task rollout is successful or not.\n\u2022 Sprinkle-salt: The salt bottle reaches a position where the salt is poured out into the bowl.\n\u2022 Plush-toy-in-basket: The plush toy is put inside the container, with more than 50% of the toy inside the container.\n\u2022 Close-the-laptop: The display is lowered towards the base until the two parts meet at the hinge (aka the laptop is closed).\n\u2022 Close-the-drawer: The drawer is pushed back to the containing region, either it's a drawer or a layer of a cabinet.\n\u2022 Place-snacks-on-plate: The snack is placed on top of the plate, with more than 50% of the snack package on the plate.\n\u2022 Bagging: The chip bag is put into the shopping bag which is initially closed."}, {"title": "B.2 Implementation of Baseline", "content": "We implement the baseline ORION [4] with minimal modifications to apply it to our humanoid setting. First, we estimate the palm trajectory from SMPL-H trajectories by using the center point of the reconstructed fingers as the palm position at each time step. Next, we warp the palm trajectory based on the test-time objects' locations. Finally, we use inverse kinematics to solve for the robot's body joints, with the warped trajectory serving as the target palm position."}, {"title": "B.3 Details on Different Demonstrators", "content": "shows the videos of three different human demonstrators performing Place-snacks-on-plate and Close-the-laptop tasks. We calculate the success rates of imitating different videos, and the results are shown in Figure 4(b."}, {"title": "B.4 Simulation Evaluation", "content": "For easy reproducibility, we replicate two tasks, Sprinkle-salt and Close-the-drawer, in simulation (Figure 7). We implement these tasks using robosuite [60], which recently provided cross-embodiment support, including humanoid manipulation. We use \u201cGR1FixedLowerBody\" as the robot embodiment in these two tasks.\nNote that for the policy of each task, we use the same human video as the ones used in real robot experiments. We compare three methods in simulation: OKAMI (w/vision), OKAMI (w/o vision), and ORION. OKAMI (w/vision) the same method we use in our real robot experiments. OKAMI (w/o vision) is the simplified version of OKAMI where we assume the model directly gets the"}, {"title": "B.5 Visuomotor Policy Details", "content": "We choose ACT [61] in our experiments for behavioral cloning, an algorithm that has been shown effective in learning humanoid manipulation policies [67]. Notably, we choose pretrained Di-noV2 [68, 69] as the visual backbone of a policy. The policy takes a single RGB image and 26-dimension joint positions as input and outputs the action of the 26-dimension absolute joint position for the robot to reach. In Table 2, we show the hyperparameters used for behavioral cloning."}]}