{"title": "Robust 3D Point Clouds Classification based on Declarative Defenders", "authors": ["Kaidong Li", "Tianxiao Zhang", "Chuncong Zhong", "Ziming Zhang", "Guanghui Wang"], "abstract": "3D point cloud classification requires distinct models from 2D image classifica-\ntion due to the divergent characteristics of the respective input data. While 3D\npoint clouds are unstructured and sparse, 2D images are structured and dense.\nBridging the domain gap between these two data types is a non-trivial challenge\nto enable model interchangeability. Recent research using Lattice Point Classi-\nfier (LPC) highlights the feasibility of cross-domain applicability. However, the\nlattice projection operation in LPC generates 2D images with disconnected pro-\njected pixels. In this paper, we explore three distinct algorithms for mapping\n3D point clouds into 2D images. Through extensive experiments, we thoroughly\nexamine and analyze their performance and defense mechanisms. Leveraging\ncurrent large foundation models, we scrutinize the feature disparities between reg-\nular 2D images and projected 2D images. The proposed approaches demonstrate\nsuperior accuracy and robustness against adversarial attacks. The generative\nmodel-based mapping algorithms yield regular 2D images, further minimizing the\ndomain gap from regular 2D classification tasks. The source code is available at\nhttps://github.com/KaidongLi/pytorch-LatticePointClassifier.git.", "sections": [{"title": "1 Introduction", "content": "Point clouds consist of data points sampled through light detection and ranging\n(LiDAR) sensors, playing a crucial role in various 3D vision tasks. LiDAR is widely\nemployed in applications like autonomous driving because it provides accurate 3D\nmeasurements. With the advancements in 2D image tasks [1-4], 3D point clouds also\nwitnessed rapid development, primarily driven by the integration of deep neural net-\nworks (DNNs). However, significant work is required to design modules to adapt 3D\ndata for DNNS.\nIn recent years, several approaches have been proposed for 3D classification. Multi-\nview-based methods [5, 6] projects 3D objects into multiple 2D data, which can either\nbe 2D images or 2D features from different views. Although this approach can directly\napply 2D models, it inevitably loses depth information. Moreover, the generated 2D\nimages are usually scattered pixels, deviating from normal 2D images. Graph-based\nmethods [7, 8] employs graph representation to preserve spatial information like depth\nin 3D point cloud. Then graph convolution networks (GCNs) are utilized for end-\nto-end training. Voxel-based methods [9, 10] creates 3D voxels by dividing the space\ninto occupancy grids. The features extracted in individual voxels are then gathered\nusing 3D sparse convolutions. However, due to its computation complexity, its res-\nolution is limited [11]. Point-based methods [12-14] extract point-wise features and\naggregate them using customized modules. To effectively merge the traditionally sep-\narate branches of DNNs, it is essential to explore mapping algorithms that enable a\nseamless integration of 2D models.\nRecent work proposed by Li et al. [15] showed that existing DNNs are vulnerable\nto adversarial attacks and proposed a new approach to 3D point cloud classification.\nExploring correct attack algorithms [16, 17], perfect attack results can be achieved on\nalmost all of the state-of-the-art classifiers. Since most 3D classifier applications are\nsafety-critical, like autonomous driving, it is imperative to adopt a robust classifier\nunder adversarial attacks. In this paper, we extend the lattice point classifier (LPC) in\n[15] and propose three additional mapping algorithms as shown in Figure 1. We design\na suite of experiments to evaluate and analyze their performance and defense abilities.\nDirect projection. Inspired by LPC [15] which projects the 3D point cloud onto a\nlattice hyperplane, we introduce a simpler version to analyze the performance. Like\nLPC, both algorithms project 3D points onto a hyperplane and generate sparse pix-\nels on the 2D plane. However, these geometric projection methods will lose depth\ninformation. Graph drawing. Lyu et al. [18] proposed a point cloud part segmenta-\ntion model based on an efficient hierarchical graph-drawing algorithm to represent\n3D point clouds using 2D grids. The spatial relationship can be preserved by graphs.\nWe extend the algorithm to further improve its classifier performance and analyze its\ndefense mechanisms.\nThese mapping algorithms are inspired by multi-view and graph methods. The\ngenerated 2D images contain sparse projected pixels with the majority of the images\nfilled with dark backgrounds, as shown in Fig. 1. To further align the generated 2D\nimages with regular 2D images, we also introduce a generative model-based mapping\nalgorithm to reduce the domain gap.\nThe main contributions are summarized below."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 3D Point Cloud Classifications", "content": "Popular 3D point cloud classifications can be classified into four categories. Multi-\nview-based networks [5, 19, 20] project 3D point clouds onto one or multiple 2D\nhyperplanes to generate one or multiple 2D images to feed to a regular 2D neural\nnetworks. Then pooling algorithms can be applied to aggregate features. Voxel-based"}, {"title": "2.2 3D Point Cloud Attacks and Defenses", "content": "Deep learning-based models are proven to be vulnerable to attacks [16, 24, 25]. Adver-\nsarial attacks aim to trick the DNN models by slightly modifying the input data, in a\nway that is indiscernible by human eyes. Many gradient-based methods can be applied\nto 3D point clouds. We categorize attacks based on how the input point clouds are\nmodified. Point perturbation [16, 17, 26, 27] is the most common type. It attacks the\npoint clouds by moving all (or part of) the points by a small amount. The attack effi-\nciency can be demonstrated by the amount of perturbation, which is often measured\nby perturbation L2 norm. Advanced versions [16, 26] will constrain the number of\nperturbations in the optimization process. Point addition [16, 28] tries to trick mod-\nels by adding extra points. These added points can be independent or sampled from\ntargeted class objects. Then, slight perturbations, scaling and rotations are added to\nthe extra points to increase attack efficiency. In contrast, point dropping [287, 29]\nis a technique where adversarial attackers select critical points from each input point\ncloud and remove them to deceive the classifier.\nNevertheless, this operation lacks differentiability. A significant portion of research\nendeavors is dedicated to transforming point-dropping into an alternative action that\nfacilitates gradient propagation. For example, Zheng et al. [29] proposed to view it as\nrelocating points to the cloud center. Other categories of attacks also have seen great\nsuccess. LG-GAN [30] is based on generative adversarial networks (GANs). Backdoor\nattacks [31, 32] poison the model training process by inserting a backdoor pattern\nso that the victim models will be tricked if the backdoor pattern is presented during\ninference. [33, 34] proposed attacks that force constraints on isometric property.\nAdversarial defense in 3D point clouds is important because many applications\nof 3D models are safety-critical. The defense approaches can be summarized into the\nfollowing categories. (1) Statistical Outlier Removal (SOR) [35] tries to defend by\nremoving points to make a smooth surface. In addition, SOR is non-differentiable,\nobfuscating gradients for defenders. DUP-Net [36] and IF-Defense [37] both employ\nSOR and append a geometric-aware network to refine the point clouds. (2) Random\nsampling exhibits robustness against adversarial attacks [38]. PointGuard [39] intro-\nduced a classification method by randomly sub-sampling point clouds multiple times,\nand making predictions based on the majority votes. (3) Data augmentation emerges"}, {"title": "2.3 Graph Drawing", "content": "The goal of graph drawing is to represent original 3D point clouds using graph\nG = (V,E) and draw the graph on a low-dimensional space [18, 44, 45]. By different\ngraph properties (e.g., spatial relationships among vertices and edges), graph represen-\ntations can be divided into many families. For example, k-planar drawing [46] limits\nthe number of edge crossings, and RAC drawings [47] will only have perpendicular\ncrossings on their edges. Lyu et al. [18] proposed a Delaunary triangulation-based [48]\ngraph drawing method for 3D segmentation. To directly apply normal 2D networks,\na novel hierarchical approximation algorithm is designed to generate normal images\nfrom graph representation while preserving local information in point clouds."}, {"title": "2.4 Reconstruction and Rendering", "content": "Reconstruction surface from point clouds is an important and long-standing research\ndirection in computer graphics, bridging the gap between points and images. A\ncommon approach is to form triangulations to compute a volume tetrahedralization\n[49, 50]. But these methods tend to create undesirable holes especially when points get\nnoisy [51]. Implicit surface reconstruction [52, 53] can handle noise better at the cost of\nrequiring more computational resources. Katz et al. [54] proposed a method to recon-\nstruct the surface only from one single view using hidden point removal. The rapid\ndevelopment of DNNs shines light on the potential of bypassing manually designed\npriors. Hanocka et al. [55] proposed a DNN-based method to deform an initial mesh to"}, {"title": "2.5 Multi-modal Large Language Models", "content": "Multi-modal Large Language Models (MLLMs) are designed to achieve a broad under-\nstanding across various modalities, encompassing audio [58], image [59], point cloud\n[60], and more. The underlying idea is that incorporating language modality can\nenhance a model's ability to comprehend high-level interactions within diverse input\ndata. Within this domain, one category of MLLMs employs language as a means to\ninteract with other modalities [61, 62]. These models showcase versatile capabilities,\nparticularly when provided with text inputs as prompts. Another category of MLLMs\nadopts individual encoders for each modality, aligning features from diverse modali-\nties [59, 63?]. These models demonstrate notable zero-shot generalization capabilities.\nNoteworthy examples include CLIP [59], which has inspired a myriad of novel appli-\ncations. For instance, PointCLIP [64] leverages CLIP for zero-shot 3D multi-view\nclassification, while Xue et al. [60] utilize CLIP's multi-modal features to supervise a\n3D point cloud encoder, aligning three modalities. Given CLIP's proven robustness\nacross multiple tasks, we leverage CLIP to assess the domain gap of generated 2D\nimages when compared to typical 2D images."}, {"title": "3 Mapping Algorithms", "content": "The section describes the three mapping algorithms that can be used to map 3D point\nclouds into 2D space."}, {"title": "3.1 Basic Projection Classifier", "content": "The Lattice Point Classifier (LPC) was introduced as an implementation of a robust\ndeclarative classifier (RDC) with robust defense capabilities against gradient-based\nattacks [15]. However, LPC involves a sophisticated mapping algorithm. It first\nprojects 3D point clouds onto a lattice hyperplane [66], which is populated with a\ntriangular lattice. Subsequently, each projected point is splatted onto the three enclos-\ning triangular lattice vertices. In this context, we delve into a more straightforward\nmapping algorithm for 3D-to-2D projection by simply dropping one dimension of the\ndata as shown in Figure 2. This basic projection serves as a baseline for performance\ncomparison, and the defensive behavior of RDC is scrutinized in conjunction with this\nbasic projection."}, {"title": "3.2 Graph Drawing Classifier", "content": "Both basic projection and permutohedral lattice projection unavoidably lead to the\nloss of depth information perpendicular to the 2D hyperplanes. In contrast, graph\nrepresentations showcase the capability to preserve 3D spatial information, rendering\nthem an optimal choice for the mapping algorithm.\nGiven the graph G = (V,E) = h(X), where X = {xi \u2208 R\u00b3} denotes input point\ncloud, the function h : X \u2192 G transform input point clouds to graph representations.\nGraph drawing is originally employed to transform a graph G into continuous drawings.\nTo facilitate the utilization of standard 2D neural networks, we want to find a function\nf: G \u2192 Z\u00b2 that converts graphs to 2D integer coordinates. Inspired by the graph\ndrawing segmentation model [18], we adopt an efficient graph drawing classifier. This"}, {"title": "3.3 Rendering Classifier", "content": "As shown in Fig. 1, the images generated from the above mapping algorithm still\nhave distinctive visual artifacts. The rendering-based method can generate realistic\n2D images by reconstructing object surfaces, which can further reduce the domain gap\nfrom regular 2D tasks. Z2P [51] is a lightweight DNN-based rendering algorithm. The\npipeline of the mapping is shown in Fig. 4. It first generates a z-buffer projection from\nthe original point cloud, and then renders the 2D image through a modified U-Net.\nz-buffer Projection. Like the basic projection method, it projects points onto a\n2D hyperplane, obtaining a pixel-point pair (xk, I(i, j)), where xk is a 3D point and\nI(i, j) is the corresponding pixel value at image coordinate (i, j). The pixel value is\ndetermined by the distance of the 3D point from the image plane d. The pixel value\ncan be calculated by I(i, j) = e\u2212(d-a)/\u03b2, meaning the farther the points are, the darker\nthey appear on the depth image. The pixel value is shared within a 3 \u00d7 3 pixel window\ncentered at I(i, j). The pixels without a corresponding point are given a value 0.\nAdaptive Instance Normalization (AdaIN). To enhance control over the colors\nand shadows of the rendered images, we replace the original normalization layers with\nAdaIN [65]. In addition to the normal DNN features f of size H\u00d7W\u00d7C, AdaIN takes a\nscene-control vector w as input. Each AdaIN layer contains two affine transformations,\ndenoted as As and Ab, which are utilized to compute styles ys = Asw and yb = Abw.\nThe output of AdaIN is determined by\nAdaIN(fi, w) = Ys,i(fi - \u03bc(fi))/(\u03c3(fi))+ Yb,i (1)\nwhere \u00b5(fi) and o(fi) are the mean and standard deviation (std) at each feature\nchannel fi."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset", "content": "ModelNet40 [9]: The dataset comprises clean synthetic 3D objects generated from\nCAD models, encompassing 40 distinct categories, with a total of 12,311 objects."}, {"title": "4.2 Training Settings", "content": "We perform classification training on the ModelNet40 dataset. For all training tasks,\nwe run 200 training epochs using Adam optimizer [68] with 0.0001 decay rate and\n(0.9, 0.999) beta values. A simple step learning rate scheduler is used with a step size of\n20 and a starting learning rate of 0.001 unless otherwise specified. The point clouds are\nevenly sampled from the object surface following the practice in [12]. During training,\nthe points are augmented by random point dropout, scale, and shift. The max dropout\nrate is 0.875. The random scale rate is set from 0.8 to 1.0. The max random shift\ndistance is 0.1. When random rotation is enabled, the max random rotation angle is\n\u03c0. No augmentation techniques are applied during the inference.\nBasic Projection Classifier. We scale the point clouds to generate 456 \u00d7 456 images.\nDuring data augmentation, points projected outside the image will be set to pixel\nI(0,0).\nGraph Drawing Classifier. Using the balanced KMeans [18], the original input\npoint clouds are divided into K = 32 clusters. The balanced KMeans clustering algo-\nrithm initially generates unbalanced clusters h, and then starts to reduce the number\nof points in oversize clusters. A cluster is considered oversized when the number of\npoints |h| > \u03b1\u00b7|X|/K where |X| is the total number of points in input point clouds,\nand a is set to 1.2. For the graph drawing, both the lower-level and top-level grid sizes\nare 16 \u00d7 16, making the final graph drawing output to be 256 \u00d7 256.\nRendering Classifier. The classifier undergoes a two-step training process. In the\nfirst step, the rendering model is trained for 10 epochs with a learning rate of 3 \u00d7 10-4.\nThroughout the z-buffer projection, the splat window size is set to 3. During inference\nfor scene control, the generated object color RGB values are fixed at (255, 255, 255),\nand lighting is positioned at the origin point for simplicity. The resolution of the\ngenerated rendered 2D image is set to 313 \u00d7 313. The rendering training data is\ngenerated following [51] using [69]."}, {"title": "4.3 Domain Gap", "content": "A crucial factor for the application of regular 2D classifiers is minimizing domain\ngaps between generated and regular images. To quantify this domain gap, we lever-\nage well-established foundation models [59, 70] that have been trained on extensive\ngeneral domain data with over 15 million images. These models have demonstrated\nnotable zero-shot classification accuracy across diverse datasets. For instance, CLIP\n[59] achieves a 76.2% zero-shot classification accuracy on the extensive ImageNet\ndataset [71]. The inherent alignment of text-image features in these pre-trained models\nsimplifies their adaptation to diverse classification tasks across datasets with varying\ncategories.\nWe use CLIP [59] to run zero-shot classification on the generated 2D image from\nLPC [15] and our three declarative classifiers. The image backbone is ViT-B/32\n[72] which takes images of resolution 224\u00d7224. We directly utilize CLIP's image\nnormalization mean and std, (0.4814, 0.4578, 0.4082) and (0.2686, 0.2613, 0.2758), for"}, {"title": "4.3.1 CLIP Zero-shot Accuracy", "content": "The zero-shot classification results are shown in Table 1. We have three observations\nfrom the results. (i) Graph drawing 2D images have the lowest accuracy, hovering\naround the same level of total random prediction accuracy  NUM_CLASS = 1/40 = 2.5%. It is not surprising when we take a look at the visualization of graph draw-\ning images depicted in Fig. 6. However, considering that the primary goal of graph\nrepresentation is to encode point clouds into latent features rather than human-\nunderstandable images, this approach remains promising, as evidenced by the 85.58%\nclassification accuracy. (ii) The two projection-based methods (LPC and basic projec-\ntion classifier) still demonstrate a substantial domain gap. The sparse enabled pixels\nin Fig. 6 are different from regular dense images. However, the accuracy difference\nbetween LPC (2.64%) and basic projection (5.17%) indicates that point of view can\nsignificantly affect classifier performance. This point is further validated in subsequent\nexperiments on rendering classification in the ablation study. (iii) Despite rendering\nclassifier 2D images achieving a noteworthy reduction in domain gap compared to\nother mapping algorithms, other classifiers do not achieve satisfactory CLIP zero-shot\nperformance as shown in Table 1."}, {"title": "4.4 Classification Accuracy", "content": "For all four classification models, we employ EfficientNet-B5 [74] as the 2D backbone\nnetworks, and the results are summarized in Table 2. The basic projection model\nachieves the highest test accuracy at 91.02%, outperforming LPC [15]. As visual-\nized in Fig. 6, LPC and basic projection essentially represent high-to-low-dimensional\nprojections from different viewpoints, which significantly influences the classification\nperformance. This observation is further validated in subsequent ablation studies. The"}, {"title": "4.5 Ablation Study", "content": "Basic Projection Classifier. We ran experiments with different settings, as shown in\nTable 4. Comparing the first two experiments, we observe that additional linear layers\nimprove classification accuracy. Another observation, from comparing the second and\nthird rows, is that higher 2D resolution does not necessarily lead to better performance.\nIncreasing the number of pixels in the converted 2D images results in more sparsity,\nwhich slightly reduces accuracy for the ViT-L backbone.\nRendering Classifier. We evaluate the contribution of different setups on our render-\ning classifier. Many applications have proven the power of foundation models trained\non large amounts of data [59, 75-77]. We employ two different CLIP [59] image\nencoders (ViT [72] and ResNet50 [1]), which produce 768 channel and 1024 chan-\nnel image features. We append 3 fully connected layers after the foundation image\nencoders, with 4096 as the fully connected layers' hidden channel dimension. The\nablation results can be found in Table 3."}, {"title": "4.5.1 Learnable Backbone Weights", "content": "For the foundation model encoders, we run training with the encoder weights frozen\nor trainable to study the effects. From the foundation ViT experiments (1 and 2) and\nResNet50 experiments (2 and 3), we can see making the foundation encoder learnable\nwill reduce the accuracy. Specifically, the accuracy decreases by 2% to 76.53% for ViT\nand 10% to 56.81% for ResNet50. Examining the training data accuracy, we observe\nthat the frozen and learnable backbones using ResNet50 have 58.93% and 56.38%"}, {"title": "4.5.2 Normalization", "content": "We conducted a comparison between different normalizations: CIFAR-100 normal-\nization and rendered image normalization with mean values of (0.1136, 0.1135,\n0.1128) and standard deviations of (0.2729, 0.2719, 0.2761) during training. Utilizing\nEfficientNet-B5 [74], we observed an improvement in accuracy by 0.64% and 0.29%.\nHowever, this conclusion does not extend to foundation encoders. Specifically, with\nthe foundation ViT backbone, the calculated normalization results in a decrease in\naccuracy by 1.07%. We suggest that foundation models might have a sufficient volume\nof data, allowing them to adapt to various data distributions."}, {"title": "4.5.3 Observation View Point", "content": "As discussed in Section 4.3.1, the viewpoint might have a significant impact on classi-\nfication accuracy. As shown in Table 3, rotating object upright significantly improves\nclassification accuracy. When CIFAR-100 normalization is used, the inference accuracy\nis boosted by 2.96%, and with accurate normalization, the improvement is 2.61%."}, {"title": "4.5.4 Random Rotation", "content": "The rendering classifier training is carried out in three steps: initially to train the\nrendering model, then to generate rendered images, and finally training on 2D images\nto save time. Consequently, during the training of the 2D image classifier, it only\nhas exposure to the 3D object from a single viewpoint. To enhance the dataset, four\nadditional sets of rendered images are created through the rotation of the original input\npoint clouds. Specifically, we rotate along x-axis and \u00b1 along z-axis separately to\ngenerate the four sets of rendered images. This data augmentation strategy contributes\nto an increase in inference accuracy by 1.4%."}, {"title": "4.6 Learning Rate", "content": "We also conducted experiments to show the sensitivity of our models to different\nlearning rates, as shown in the following Table 5.\nIn all experiments, we used the Adam optimizer with a step learning rate scheduler\nand a step size of 20. For the rendering classifier, the model quickly diverged during\ntraining with a learning rate of 0.01, while a learning rate of 0.0001 was too small,\ncausing the model to get stuck in local minima."}, {"title": "4.7 Defense Experiment Settings", "content": "To test the defense performance of each mapping algorithm, we run FGSM [78] similar\nto LPC [15] under various gradient-based attacks. The FGSM attack is applied on the\nentire test set from ModelNet40 [9]. The attack learning rate of FGSM is set to 0.1."}, {"title": "4.8 Defense Performance", "content": "The attack results are listed in Table 6. From the results, we can observe: (i) Basic\nprojection and rendering classifier exhibit similar defense behavior as LPC [15]. They\nare all robust under gradient-based attackers. (ii) Not all robust declarative classifiers\nare guaranteed to be robust under attackers. Graph drawing classifier is still vulnerable\nto adversarial attacks.\nWe designed more experiments to further investigate the factors that influence\ndefense performance. An illustration of the original declarative classifier can be found\nin Fig. 7 without the orange connection. The classifier can be formulated as\ny = g(h(x); w) (2)\nwhere h(x) = arg minzez f(x, z; 0), g(x;w) is the network backbone, which takes\nthe generated images x h(x) as input, and h(x) is the declar ative defender. In\na gradient-based attack, the attack gradient can be easily back-propagated through"}, {"title": "5 Conclusion", "content": "In this paper, we have investigated three distinct mapping algorithms as declarative\ndefenders for 3D point cloud classification, demonstrating the robust performance of\ndeclarative nodes through diverse 3D-to-2D mapping techniques. The majority of the\nproposed declarative defenders exhibit resilience against adversarial attacks, with the\nbasic projection classifier achieves the best performance. Through the use of a ren-\ndering classifier, we discover that the observation viewpoint and random rotation can\nsubstantially impact classification accuracy. Notably, upon observing the diminish-\ning defense capability in the graph drawing classifier, we conclude that the presence\nof a skip connection circumvents implicit gradients, thereby defeating the defense\ncapability.\nThere is room for improvement in the rendering classifier. Currently, the scene\ncontrol parameters are manually set, mainly due to the scarcity of training data with\nactual object textures. A potential avenue for future work could involve designing a\nmodule to learn texture priors from the point cloud. This approach aims to generate\nmore realistic 2D images by incorporating learned texture information."}]}