{"title": "Induced Modularity and Community Detection for Functionally Interpretable Reinforcement Learning", "authors": ["Anna Soligo", "Pietro Ferraro", "David Boyle"], "abstract": "Interpretability in reinforcement learning is\ncrucial for ensuring AI systems align with hu-\nman values and fulfill the diverse related re-\nquirements including safety, robustness and\nfairness. Building on recent approaches to\nencouraging sparsity and locality in neural\nnetworks, we demonstrate how the penalisa-\ntion of non-local weights leads to the emer-\ngence of functionally independent modules in\nthe policy network of a reinforcement learn-\ning agent. To illustrate this, we demonstrate\nthe emergence of two parallel modules for as-\nsessment of movement along the X and Y\naxes in a stochastic Minigrid environment.\nThrough the novel application of commu-\nnity detection algorithms, we show how these\nmodules can be automatically identified and\ntheir functional roles verified through direct\nintervention on the network weights prior to\ninference. This establishes a scalable frame-\nwork for reinforcement learning interpretabil-\nity through functional modularity, addressing\nchallenges regarding the trade-off between\ncompleteness and cognitive tractability of re-\ninforcement learning explanations.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep reinforcement learning (RL) has emerged as a\npowerful approach for improving performance in com-\nplex decision making domains. Learning policies di-\nrectly from interactions can offer increased flexibility\nand improved performance, whilst avoiding the chal-\nlenges of designing effective cost functions and plan-\nning strategies faced by classical model-based control\napproaches (Song et al., 2023). A growing body of re-\nsearch is demonstrating the potential for RL to have\npositive impacts in diverse real-world domains, from\nbattery manufacturing (Lu et al., 2020) to the design\nof medical treatment regimes (Coronato et al., 2020):\napplications which directly implicate on critical issues\nsuch as climate-change and human-health. However,\nsuch broad implications also introduce societal risks,\nand the application of RL raises wide-ranging concerns\nrelated to topics of safety, reliability, privacy and bias,\namong others. For this reason, it becomes crucial that\nthe behaviour of RL agents can be properly charac-\nterised, to the extent that it can be reasonably veri-\nfied that their impacts will positively align with human\nvalues. As reflected in the EU's AI ethics guidelines:\nsystems must allow for human oversight, accountabil-\nity and transparency (European Comission and High-\nLevel Expert Group on AI, 2019).\nHowever, there are fundamental challenges to achiev-\ning this, and current RL systems rarely afford suffi-\ncient levels of interpretability to fulfill these three re-\nquirements. Central to this is the ambiguity regard-\ning what constitutes an acceptable 'explanation' of a\nmodel. A balance must be struck between the scope\nand detail of an explanation and its suitability for a\nhuman audience. Human understanding of an expla-\nnation must be considered to ensure its utility, whilst\nalso avoiding incompleteness that risks leaving room\nfor subjective interpretations. Lipton (2016) consid-\ners this concept of a tractable explanation under the\nterm of 'simulatability': the idea that a human can, in\nreasonable time, take a model input and explanation\nand predict its output. He pairs this with the par-\nallel notion of 'decomposability', which suggests that\nthe constituent components of a model should be in-\ndividually interpretable. These notions are echoed by\nDoshi-Velez and Kim (2017) through the concept of\n'cognitive chunks', emphasising their relevance to hu-\nman processing. Questions arise regarding how the\nsize, number, organisation and interactions of these\nrelate to whether a model or explanation can be con-\nsidered understandable.\nIn this work we address these challenges by taking a\nmodular perspective to interpretability, drawing inspi-"}, {"title": "1.1 Contributions", "content": "Leveraging this concept of interpretability at the level\nof functional modules, our work makes the following\ncontributions:\n\u2022 We build on recent bio-inspired algorithms for en-\ncouraging locality in neural networks (Achterberg\net al., 2023; Liu et al., 2023; Margalit et al., 2024)\nand demonstrate that the penalisation of non-\nlocal weights encourages the emergence of semi-\nencapsulated functional modules in the policy net-\nwork of an RL agent: a level of decomposition\nwhich we suggest strongly aligns with human de-\ncision making frameworks.\n\u2022 We show how the application of community de-\ntection methods, based on the Louvain algorithm\nand spectral analysis of the adjacency matrix,\nallows for the automatic detection of modules\nwithin neural networks. This demonstrates the\npotential to automate and scale modular inter-\npretability.\n\u2022 Finally, drawing on techniques from mechanistic\ninterpretability, we demonstrate how direct inter-\nvention on network weights prior to inference en-\nables the characterisation of detected modules, of-\nfering a verifiable interpretation of their function-\nality.\nWe show interactive graphs of modular RL networks\nand animations of their functional behavior on the\nproject page:\nhttps://sites.google.com/view/\nfunctionally-interpretable-rl"}, {"title": "2 BACKGROUND AND RELATED\nWORKS", "content": "Following Glanois et al. (2024), and to avoid confu-\nsion arising from the inconsistent use of terms in the\nliterature, we define interpretability as a passive model\nquality denoting the extent to which a model's inner\nworkings can be examined and understood. We distin-\nguish this from explainability, which we define as an\nexternal understanding of model behaviour that arises\nfrom active, generally post-hoc, attempts at explain-\ning the decision making process. Interpretability and\nexplainability thus present two different paths to ob-\ntaining information that can be used to generate ex-\nplanations for model behaviour. Although post-hoc\nexplainability methods can offer improved flexibility\nand scalability compared to intrinsic interpretability\napproaches, their lack of grounding in model inter-\nnals raises concerns regarding the potentially mislead-\ning and subjective nature of the resulting explanations\n(Adebayo et al., 2018; Atrey et al., 2019).\nIn this work we take a direct interpretability approach\nby aiming to learn an intrinsically more interpretable\nmodel architecture. Decision trees are a prevalent ex-\nisting approach to achieving this and can be used to\nlearn Q-values or policies. Although their classical im-\nplementation using boolean decision variables is not\ndifferentiable, recent work on 'soft decision trees' en-\nables efficient reinforcement learning through gradient\ndescent (Silva et al., 2020). Symbolic equations of-\nfer an alternative intrinsically interpretable architec-\nture, and diverse methods exist to generate effective\nRL policies in this form. For instance, using genetic\nprogramming to efficiently search a space of function\ntrees (Hein et al., 2017) or learning a recurrent neural\nnetwork (RNN) to directly generate policy equations\n(Landajuela et al., 2021). Beyond mathematical oper-\nators, policies have been learnt as weighted combina-\ntions of first order logic rules, from which natural lan-\nguage explanations can be extracted (Jiang and Luo,\n2019).\nThese approaches demonstrate a reliance on making\nfundamental model architecture changes to improve\ninterpretability, which leads to issues regarding scala-\nbility and performance. Although the components of\nthe described methods, such as formulaic polices, can\ntheoretically scale to complex scenarios, this rapidly\nbecomes computationally prohibitive, even when clas-\nsically discrete approaches are relaxed to be continuous\nor an indirect policy distillation approach is adopted\n(Glanois et al., 2024). Moreover, scaling these archi-\ntectures to improve performance compromises their\ninterpretability: a decision tree with an intractable\nnumber of nodes, for example, may be no more inter-\npretable than a network with an intractable number\nof neurons.\nTangentially, the field of mechanistic interpretability\ntakes a bottom up approach to reverse-engineering net-\nworks, particularly large language models. Notably\nthis can involve the identification of 'circuits': compu-\ntational sub graphs within a network which perform"}, {"title": "Anna Soligo, Pietro Ferraro, David Boyle", "content": "specific tasks (Wang et al., 2022). We share this task-\nlevel focus, but rather than attempting interpretability\nat the level of computations, as per the aforementioned\napproaches, we instead characterise higher-level collec-\ntions of neurons.\nThese collections of neurons, or 'modules', can be con-\nsidered in terms of both their topological and func-\ntional modularity. Topological (or structural) mod-\nules are communities of neurons that are densely in-\nterconnected by network weights, while being sparsely\nconnected to external neurons. Conversely, functional\nmodules are sets of neurons which together perform\nspecific tasks with a level of independence from the\nremainder of the network. A number of neural net-\nwork architectures exhibit principles of modularity. At\nthe data level, manual or learnt decomposition of the\ndomain can parallelise processing, for example by par-\ntitioning regions of an image to be independently pro-\ncessed by CNNs (Zhang et al., 2014). Within net-\nworks, sequential structural modularity can be seen\nin the repeated-block structure of transformers, which\nis additionally parallelised in a functional manner in\nmulti-modal architectures (He et al., 2021).\nFor the purpose of interpretability, we are interested in\nfunctional modularity, for which topological modular-\nity is a necessary but not sufficient condition (Amer\nand Maul, 2019). This is approached by policy tree\napproaches to hierarchical RL, which decompose deci-\nsion making into sub policies (Pateria et al., 2021).\nHowever, these are limited to a predefined level of\ndecomposition and only afford interpretability when\ndiscernible sub-behaviours, such as motor primitives\n(Merel et al., 2018), are explicitly learnt based on prior\nknowledge.\nFunctional modularity in the brain arises alongside its\n'small-world' architecture: a combination of high clus-\ntering and short average path length hypothesised to\nhave evolved to satisfy spatial and energy constraints\n(Margalit et al., 2024). Recently, several works have\ninvestigated the impacts of applying analogous con-\nstraints to neural networks. Achterberg et al. (2023)\nspatially embed an RNN and penalise its connections\nrelative to their length, demonstrating high energy effi-\nciency and a level of functional clustering in a one-step\ninference task. Concurrently, Margalit et al. (2024)\nintroduced a training loss to encourage correlation be-\ntween local activations and applied this to convolu-\ntional layers projected onto simulated cortical sheets.\nWhile these studies apply bio-inspired training for\nthe purpose of advancing neuro-scientific understand-\ning and modelling, Liu et al. (2023) employ similar\ntechniques to improve the interpretability of neural\nnetwork visualisations. They demonstrate that their\n'brain inspired modular training' approach, which cou-\nples length-relative weight penalisation with neuron\nrelocation during training, reveals structures within\ntasks such as regression to symbolic mathematical for-\nmulae. Building on these findings, we extend this\n'small-world' approach to an RL context to encourage\nfunctional modularity. We further propose methods\nfor automatic detection and characterisation of the re-\nsulting modules, enabling scalable interpretability in a\ndecision making context."}, {"title": "3 METHODS", "content": "We use the standard RL formalism for an agent in-\nteracting with a fully observable environment. The\nenvironment is described by the set of states S, set of\ndiscrete actions A, reward functionr: S \u00d7 A \u2192 R,\ndiscount factor \u03b3 \u2208 [0,1], and transition probabilities\np(St+1 St, at). The policy \u03c0\u03b8 : S \u2192 A generates a\nprobability distribution over actions, from which an\naction at is sampled at each time step t. Each action\nresults in a reward rt = r(st, at) and an updated envi-\nronment state st+1. The return of a state-action pair\nis defined as the discounted sum of future rewards,\n$Rt = \\sum_{i=t}^{\\infty} \\gamma^{i-t}r_i$, and the expected return is given by\nthe action-value function $Q^{\\pi}(St, at) = E[Rt|St, At]$.\nWe apply PPO (Schulman et al., 2017) due to its sta-\nbility and simplicity. As a policy-gradient method,\nPPO directly learns the parameterised policy function,\nthe actor, by estimating the gradient of the expected\nreturn with the respect to the policy parameters. The\ngeneralised advantage estimator (GAE) is used to de-\ntermine the advantage of taking an action at over alter-\nnative actions in state st. This advantage is defined as\n$A(St, at) = Q(St, at) \u2013 V (st)$, where $V^{\\pi}(st) = E[Rt|St]$:\nthe state-value function approximated by a critic net-\nwork. The GAE value is clipped to constrain the mag-\nnitude of the policy change at each learning step and\nprevent large changes which may lead to performance\ncollapse."}, {"title": "3.2 Spatially Aware Regularisation", "content": "L1 regularisation encourages sparsity by penalising the\nabsolute values of the model parameters. Following\nAchterberg et al. (2023); Liu et al. (2023), we extend\nthis to encourage local connectivity by projecting the\nneural network into Euclidian space and scaling L1\nweight penalties by the distance between the neurons\nthey connect.\nFor a network with L weight layers and L + 1 neu-\nron layers, we denote neuron layers as Ni where\nl\u2208 {0,1,...,L} and weight layers as W\u2081 where"}, {"title": "Induced Modularity and Community Detection for Functionally Interpretable RL", "content": "ij\nl\u2208 {1,...,L}. For our fully connected networks,\n$Wi\u2208 R^{N_{i-1}\u00d7\\tilde{N_i}}$ and each weight $w_{ij}^{l}$ connects the $i^{th}$\nneuron in $N_{i-1}$ with the $j^{th}$ neuron in Ni. The di-\nmension of the neuron layer Ni is ni, such that no and\nNL represent the dimensions of the input features and\ndiscrete action space respectively.\nAll neurons ni N\u012b share a y coordinate: $yi := y\u0131, i \u2208$\n{1,...}. Initial x coordinates are defined as uni-\nformly spaced along this axis such that $x = A$. A\nweighting factor, Acc, scales the connection cost loss in\neach layer to give the total network connection cost,\n$L_{cc}$ defined in Equation 1.\n$L_{cc} = A_{cc} \\sum_{l=1}^{L} \\sum_{i=1}^{n_{i-1}} \\sum_{j=1}^{n_i} ||p_{i-1}^{l-1} - p_{i}^{l}|| ||w_{ij}^{l}|| \\hspace{1cm} (1)$\nwhere\n$p_{i}^{l} = [x_{i}^{l}, y_{i}]$ .Note that if all neuron pairs are equidistant (i.e.,\n$||p_{i-1}^{l-1} - p_{i}^{l}||$ is a constant) Lcc reduces to L1 sparsity.\nTo further encourage locality, neurons positions are\nsystematically relocated during training in a manner\nthat minimises the total connection cost Lcc of the net-\nwork, as proposed by Liu et al. (2023). At a fixed up-\ndate interval and for every hidden layer, the weighted\ndegree of each neuron is calculated as the sum of its\nincoming and outgoing weights:\n$w(n) = \\sum|w(n)_{in}| + \\sum|w(n)_{out}| \\hspace{2cm} (2)$\nThe k = 10 neurons with highest w(n) are selected for\nposition optimization. For each candidate neuron nc,\nthe relocation algorithm:\n1. Computes the baseline connection cost CCo for\nthe current configuration, based on Equation 1.\n2. For every other neuron ni in the same layer as\nnc, calculates an updated connection cost CCi ob-\ntained by swapping the positions of ne and ni\n3. Identifies the swap partner ns that yields\n$min(CC_{i})$\n4. If $min(CC_{i}) < CCo$, executes the position swap\nbetween ne and ns"}, {"title": "3.3 Minigrid", "content": "We conduct experiments using the dynamic obstacles\nMinigrid environment shown in Figure 1 (Chevalier-\nBoisvert et al., 2023). This consists of an agent, goal\nand three 'ball' obstacle entities randomly initialised in\na 4x4 grid, which is encoded into a symbolic observa-\ntion of entity coordinates relative to the agent. The ac-\ntion space consists of left, right, up and down steps and"}, {"title": "4 EXPERIMENTS", "content": "We implement the PPO actor and critic as MLPs\nwith the architecture described in Table 1. Hyper-\nparameters were selected based on a grid search in the\nregion of the baseline presented in the NAVIX Mini-\ngrid implementation (Pignatelli et al., 2024)."}, {"title": "4.1 Emergent Modularity", "content": "Figure 3 shows the emergence of two distinct mod-\nules within the actor network of a dynamic-obstacles\nMinigrid agent. As the connection cost weighting fac-\ntor (Acc) is increased, independence emerges in the\nsecond and third weight layers, with feature-sharing\npersisting in the first. The neuron relocation results\nin the input features and output actions reordering\nin a manner that reflects their relevance, with feature\nx and y coordinates positioned on the same sides as\nmovements on the x and y axes respectively. Figure\n4 shows the necessity of both the connection cost loss\nand neuron relocation to achieve this result. When L1\nsparsity is applied in isolation or with only one of these\nmethods, the same visual clarity is not achieved."}, {"title": "4.2 Module Detection", "content": "Modularity within a network can be quantified as\nthe ratio of intra-community links to inter-community\nlinks.\n$Q = \\frac{1}{2m} \\sum_{ij} (A_{ij} - \\frac{k_i k_j}{2m} ) \u03b4(C_i, C_j) \\hspace{3cm} (3)$\nwhere m is the sum of all edge weights ($m = \\sum_{ij} w_{ij}$),\nA is the adjacency matrix of the network, ki is the\nweighted degree of node i in a null model of the net-\nwork, and d(ci, Cj) is a binary variable with a value of\n1 when i and j belong to the same community. This\nequation forms the basis of the heuristic Louvain ap-\nproach to community detection, which iteratively opti-\nmises Q through hierarchical local node reassignments\n(Blondel et al., 2008). The resolution parameter y\ninfluences the size of the detected communities by ad-\njusting the level of connection considered to constitute\nnodes within the same community.\nWe apply Louvain clustering to the adjacency matrices\nof the actor networks in order to automate the identi-"}, {"title": "Anna Soligo, Pietro Ferraro, David Boyle", "content": "fication of modules and quantify the networks overall\nmodularity. Figure 5a shows the increase in modular-\nity (Q) as the connection cost increases. Empirically,\nwe observe that Q values in the region of 0.4 and above\ncorrespond to a visual separation of two modules in the\nnetwork graph. This begins to occur from Acc values\nof 0.0004, and becomes consistent across all random\nseeds for Ace values greater than 0.0014. With a Acc\nof 0.0014 the mean modularity represents a 340% in-\ncrease on the baseline value of 0.12 observed with a\nAcc of 0. The induced sparsity does, however, impact\non performance, as shown in Figure 5b. The decrease\nin return ranges from 13.7% when modules initially\nemerge in some seeds to a mean of 20.6% at the stage\nwhere modularity occurs consistently. The reduction\nin performance plateaus between Acc values of 0.0016\nand 0.0026, corresponding to plateaus in both the con-\nnection cost loss and the modularity value. This sug-\ngests that there is a maximum level of sparsity that\ncan be achieved without a collapse in performance, as\nbegins to occur at Acc = 0.0028.\nWhile there is an impact on performance when con-\nsidering the complete model, the sparsity reduces the\nproportion of significant weights in the model, conse-\nquently allowing for significant pruning. We find that\nwe can remove 85% of the sparse models' parameters\nwithout reducing mean performance, compared to 10%\nin the vanilla PPO-Clip implementation. This may of-\nfer further interpretability benefits in addition to sig-\nnificantly reduced computational overhead during in-\nference, as discussed in Appendix B.\nClustering results of the Louvain method are shown\nin Figure 6 for 4 randomly selected networks with Acc\nvalues between 0.0004 and 0.0008. The Louvain al-\ngorithm (Equation 3) does not rely on a predefined\nnumber of communities, which offers advantages for\nscalability to module detection within large networks\nwith an unknown number of modules. However, we ob-\nserve that the resulting modules do not strongly align\nwith the visually apparent modularity of the networks.\nConsequently, we also apply a spectral clustering ap-\nproach to community detection (von Luxburg, 2007).\nWhile spectral clustering techniques are commonly ap-\nplied to forms of the graph Laplacian, E. Crisostomi\nand Shorten (2011) demonstrate how clustering of the\nvalues of the second eigenvector can be used to iden-\ntify communities based on the transition matrix of a\nroad network. Following this, we calculate the second\neigenvector of the adjacency matrix of the policy net-\nwork and demonstrate that this shows clustering corre-\nsponding to the network modules. Figure 7 shows the\nresult of classifying nodes by partitioning the eigen-\nvector at its largest value separation and shows clear\nalignment with the visually apparent modularity."}, {"title": "4.3 Module Intervention", "content": "The automatic classification of neurons into commu-\nnities enables direct intervention on their behaviour.\nWe achieve this through modification of network pa-\nrameters prior to inference, which we demonstrate on\nthe eigenvector detected modules of the policy network\nshown in Figure 8.\nTable 2 gives statistics on the actions taken and their\noutcomes (Failure, Success or Continuation of the\nepisode) for three scenarios: the original network and\ntwo modified versions where all weights and biases in\nthe targeted community are masked with a value of"}, {"title": "Anna Soligo, Pietro Ferraro, David Boyle", "content": "-5. This effectively disables the module and the re-\nsults show that intervening on community 2 eliminates\nup and down actions, while the same intervention on\ncommunity 1 eliminates left and right. Notably, while\nthe success rate drops significantly when a module is\nmasked, the proportion of actions resulting in failures\ndoes not increase. This demonstrates that we are able\nto disable a module while retaining the ability of the\nother to select the optimal action from its correspond-\ning outputs.\nIn this example, the module intervention results intu-"}, {"title": "5 CONCLUSIONS", "content": "Our results demonstrate that functional modularity\ncan be explicitly encouraged in reinforcement learn-\ning through the use training modifications to promote\nsparsity and locality of connections. To enable scal-\nability to complex decision making applications and\nmodel architectures, we propose methods for the auto-\nmatic detection and characterisation of modules. This\nfunctional decomposition enhances interpretability by\ndecomposing the network into distinct, meaningful\nunits which provide insights into the agent's decision-\nmaking process while maintaining a level of abstrac-\ntion that aligns with human-level understanding.\nSeveral further areas of investigation offer potential to\nimprove the performance and utility of this approach."}, {"title": "Induced Modularity and Community Detection for Functionally Interpretable RL", "content": "Our approaches to module detection present oppor-\ntunities for refinement, particularly considering scal-\nability to more complex applications. The Louvain\napproach could be modified to account for the sequen-\ntially constrained architecture of neural networks or\nthe spectral clustering approach could be extended\nto multiple eigenvectors to better capture higher di-\nmensional clusters (von Luxburg, 2007). Alternative\nmethods also merit being explored, such as grouping\nneurons based on the concurrency of their activations\nin order to directly capture functional as opposed to\nstructural modularity. In this work, we characterise\nmodules using parameter modification prior to infer-\nence due to its ease of implementation. However, mod-\nification of activation values may offer greater insights,\nparticularly for models exhibiting more complex struc-\ntural modularity, as it offers the potential to intervene\non a module while preserving a natural distribution of\nactivation values. The current trade-off between in-\nterpretability and performance, while common among\n'white-box' approaches, is undesirable and a barrier\nto the adoption of interpretable systems. We expect\nthe specific trade-off between modularity, sparsity, and\nperformance observed here to vary across tasks, but it\nmay be possible to mitigate performance losses, for ex-\nample by encouraging modularity only in fine-tuning.\nCurrently, the lack of formal metrics for interpretabil-\nity make it challenging to comparatively evaluate the\nutility of differing interpretability approaches, particu-\nlarly those which approach interpretability at varying\nlevels of abstraction. In the absence of these, we dis-\ncuss performance, human understanding and scope of\nour approach, and anticipate evaluating against firm\nbenchmarks as these emerge."}, {"title": "APPENDIX", "content": "We show further examples of increasing the connection cost over 5 random seeds to demonstrate the consistency\nwith which modularity emerges, and to show the impact on the model architecture when a performance collapse\noccurs. This can be seen to occur with a connection cost weighting factor, CC A, of 0.004."}, {"title": "B Pruning Results", "content": "We implement parameter pruning based on magnitude by zeroing out a specified fraction of weights and biases\nwith the lowest absolute values in each layer prior to inference. This is done in a layer-wise manner due to the\ndiffering distributions of weights and biases across the layers. As illustrated in Figure A.2, models trained with\nL1 sparsity or our connection cost protocol are highly resilient to pruning and up to 90% of model parameters\ncan be zeroed without a reduction in return. In contrast, models trained with the vanilla PPO implementation\nexhibit performance degradation when pruning is increased beyond 10%, and by 90% pruning, the performance\nobserved is equivalent to that when actions are selected randomly. This high degree of achievable sparsity\npresents opportunities for reducing the model memory requirements and increasing the computational efficiency\nduring inference.\nWe also note that the reduction in performance observed as we increase the weighting of the connection cost\nloss closely resembles that seen when L1 sparsity is increased, as shown in Figure A.3. Given that L1 sparsity is\nan established regularisation technique to prevent overfitting, we theorise that our modularity inducing training\nprotocol may offer similar benefits in certain applications."}]}