{"title": "RURANET++: An Unsupervised Learning Method for\nDiabetic Macular Edema Based on SCSE Attention\nMechanisms and Dynamic Multi-Projection Head\nClustering", "authors": ["Wei Yang", "Yiran Zhu", "Jiayu Shen", "Yuhan Tang", "Chengchang Pan", "Hui He", "Yan Su", "Honggang Qi"], "abstract": "Diabetic Macular Edema (DME), a prevalent complication among di-\nabetic patients, constitutes a major cause of visual impairment and blindness. Alt-\nhough deep learning has achieved remarkable progress in medical image analy-\nsis, traditional DME diagnosis still relies on extensive annotated data and subjec-\ntive ophthalmologist assessments, limiting practical applications. To address this,\nwe present RURANET++, an unsupervised learning-based automated DME di-\nagnostic system. This framework incorporates an optimized U-Net architecture\nwith embedded Spatial and Channel Squeeze & Excitation (SCSE) attention\nmechanisms to enhance lesion feature extraction. During feature processing, a\npre-trained GoogLeNet model extracts deep features from retinal images, fol-\nlowed by PCA-based dimensionality reduction to 50 dimensions for computa-\ntional efficiency. Notably, we introduce a novel clustering algorithm employing\nmulti-projection heads to explicitly control cluster diversity while dynamically\nadjusting similarity thresholds, thereby optimizing intra-class consistency and in-\nter-class discrimination. Experimental results demonstrate superior performance\nacross multiple metrics, achieving maximum accuracy (0.8411), precision\n(0.8593), recall (0.8411), and F1-score (0.8390), with exceptional clustering\nquality. This work provides an efficient unsupervised solution for DME diagno-\nsis with significant clinical implications.", "sections": [{"title": "1 Introduction", "content": "The World Health Organization's 2024 report reveals a dramatic surge in global diabe-\ntes prevalence from 7% to 14% among adults between 1990-2022, affecting over 800\nmillion individuals with approximately 59% lacking effective treatment [1-2]. As a\ncommon diabetic complication, Diabetic Macular Edema (DME) represents a leading\ncause of irreversible vision loss [3]. Early detection and intervention could substantially\nmitigate visual deterioration, yet conventional diagnosis remains constrained by sub-\njective ophthalmologist evaluations and operational inefficiencies.\nWhile deep learning has advanced medical image analysis [4-5], current methodolo-\ngies face dual challenges: Supervised approaches (e.g., lesion segmentation [6-8], self-\nsupervised diagnosis [9-10]) require labor-intensive annotated data constrained by med-\nical expertise and costs. Simultaneously, existing methods suffer from insufficient dis-\ncriminative power and noise sensitivity during feature extraction and clustering, limit-\ning lesion differentiation accuracy.\nTo address these limitations, we propose an unsupervised diagnostic framework for\ndiabetic macular edema (DME) that simultaneously optimizes image segmentation and\nfeature clustering through joint learning. Departing from conventional approaches, our\nmethod introduces a dynamic multi-projection clustering mechanism with adaptive\nsimilarity thresholding to maintain cluster diversity.\nOur principal contributions are threefold: (1) We develop an unsupervised DME di-\nagnostic system that reduces annotation dependency and improves diagnostic accuracy;\n(2) We integrate SCSE attention mechanisms into the U-Net architecture and identify\nan optimal PCA dimensionality (50 dimensions) that balances computational efficiency\nwith critical feature retention; (3) Implementation of multi-projection head clustering\nthat ensures cluster stability and accuracy through diversity control and dynamic simi-\nlarity threshold adaptation."}, {"title": "2 Methodology", "content": null}, {"title": "2.1 Hierarchical Lesion Segmentation Network", "content": "We propose a multi-scale segmentation model based on an enhanced U-Net architecture\nto improve encoder performance. Candidate models are constructed by integrating di-\nverse backbones into the U-Net decoder, with optimal backbones selected per lesion\ntype through comparative experiments. An SCSE module [12] is embedded to enhance\nfeature discriminability for subtle lesions (detailed in Section 3.2, Fig. 1a)."}, {"title": "Lesion-Specific Backbone Network Selection.", "content": "To address DME lesion heterogeneity,\nwe propose an adaptive backbone selection strategy for U-Net encoder, selecting opti-\nmal backbones independently for each of four target lesions. Ten mainstream back-\nbones (e.g., VGG19) were evaluated, with optimal backbones determined by individual\nlesion segmentation performance (IoU, accuracy, precision) (detailed in Section 3.2)."}, {"title": "SCSE Attention Mechanism Integration in U-Net.", "content": "The SCSE module enhances fea-\nture representation by jointly optimizing spatial and channel-wise feature responses\n(Fig. 1b). Embedded in U-Net skip connections, it processes encoder features $F_e \\in$\n$R^{H\\times W\\times C}$ and decoder features $F_d \\in R^{H\\times W\\times c}$ as follows: 1) Concatenates $F_e$ and $F_d$ to\nform $F_{cat} \\in R^{H\\times W\\times 2C}$; 2) Computes spatial attention $S \\in R^{H\\times W\\times 1}$ and channel atten-\ntion $C \\in R^{1\\times 1\\times C}$; 3) Recalibrates features via S and C to output $F_{out} \\in R^{H\\times W\\times C}$."}, {"title": "Loss Function Design.", "content": "To address class imbalance (dominant background pixels vs.\nsparse lesion pixels), a hybrid CombinedLoss (Fig. 1c) integrating Dice Loss and Focal\nLoss (1) is proposed. This dual-objective function balances global segmentation accu-\nracy and sensitivity to small lesions:\n$FocalLoss = -a_1(1 \u2212 p_t)^{\\gamma}log(p_t)$ (1)\nTraining employs the Adam optimizer (learning rate = 1e-4, \u03b2\u2081 =0.9, \u03b22 =0.999) with\nearly stopping based on validation Dice coefficient."}, {"title": "2.2 Multi-projection Clustering Framework", "content": "Following lesion segmentation, we employ a pre-trained GoogLeNet model to extract\nfeatures from retinal images and implement a dynamic multi-projection head clustering\nalgorithm for disease image classification (Fig. 2)."}, {"title": "Multi-lesion Feature Weighting Strategy.", "content": "To quantify lesion-specific contributions\nto diabetic retinopathy grading, learnable weights $w_i \\in [1.0,10.0]$ are assigned to four\nlesion features (hard exudates, microaneurysms, disk hemorrhages, edema). Optimal\nweights are determined via grid search over {1.0,5.0,10.0}4, maximizing clustering ac-\ncuracy (2). Aggregated features are computed as:\n$F_{global} = \\frac{1}{4} \\sum_{i=1}^{4} w_i F_i (i = 1,2,3,4)$ (2)"}, {"title": "Deep Feature Extraction.", "content": "We utilize an ImageNet-pretrained GoogLeNet (Inception-\nv1) as the feature extractor, removing the original classification layer while retaining\nthe feature extraction backbone. This leverages ImageNet-pretrained weights for gen-\neralization and captures semantic features for simplified clustering."}, {"title": "Dynamic Multi-projection Head Clustering.", "content": "Building upon the multi-lesion feature\nweighting strategy, we design a dynamic multi-projection head clustering algorithm to\nenhance the diversity of clustering results across distinct lesions and guide the model\nto focus on lesion-specific pathological characteristics. Specifically, after unified fea-\nture extraction through the backbone network of the clustering model, lesion-specific\nfeatures are processed by dedicated projection heads to generate discriminative repre-\nsentations tailored to each lesion type. These refined features undergo weighted aggre-\ngation followed by K-Medoids clustering. Recognizing the varying clinical signifi-\ncance of different lesions in diabetic macular edema (DME) diagnosis[19-20], we em-\nploy a weight exploration network to evaluate the impact of different weighting\nschemes on clustering performance, thereby quantifying the model's attention alloca-\ntion to specific lesions.\nDuring training, a dual-term objective function optimizes clustering quality and le-\nsion-specific diversity:\n$L_{total} = L_{div} + \\lambda L_{cluster}$ (3)"}, {"title": "Multidimensional Metric-based Feature Dimension Reduction Optimization.", "content": "To\nbalance feature expressiveness and computational efficiency, we propose a PCA-based\nreduction strategy optimized via multidimensional metric evaluation:\nCumulative variance contribution ratio. Evaluates global information preservation by\nthe first k principal components. Experimental results show a cumulative ratio of\n94.32% at k = 50.\nReconstruction error quantification. Measures information loss through projection-in-\nverse projection :\n$ReconstructionError = \\frac{1}{N} \\sum_{i=1}^{N}||x^{(i)} \u2212 W_kW_k^Tx^{(i)}||_2^2$ (6)\nwhere $W_k \\in R^{d\\times k}$ represents the projection matrix. At k = 50, the reconstruction error\nreaches 0.0568, balancing computational efficiency and information preservation."}, {"title": "2.3 Evaluation Protocol", "content": "To comprehensively assess the model's performance in fundus lesion segmentation and\nclustering tasks, we establish a multidimensional evaluation framework spanning su-\npervised segmentation and unsupervised clustering metrics.\nSegmentation Metrics. Accuracy, Precision, Recall, and IoU.\nClustering Metrics. A dual-metric system combining the Calinski-Harabasz (CH) In-\ndex and Davis-Bouldin (DB) Index is implemented.\nThe orthogonal dual-metric framework mitigates biases and robustly evaluates the\napplicability of unsupervised clustering in fundus image analysis."}, {"title": "3 Experiments", "content": null}, {"title": "3.1 Data and Preprocessing", "content": "This study employs a multi-source fundus image dataset, derived from six publicly\navailable resources: Retinal Lesions[13]\u3001Retinal Vessel Segmentation[14]\u3001Retinal"}, {"title": "3.2 Comparison of Backbone Networks in the Segmentation Module", "content": "The section compares different backbone networks to selecte optimal backbone net-\nworks for each lesion type to improve segmentation accuracy (Table 1). Through com-\nprehensive comparisons, these are final feature extraction networks (Table 2)."}, {"title": "3.3 Clustering Algorithms", "content": "Supervised Binary Classification with Different Backbone Networks. This section\ndemonstrates the differences in lesion recognition when using different backbone net-\nworks. We select optimal models by monitoring validation performance (Table 3).\nComparison of Different Backbone Networks and Clustering Methods under Su-\npervised Learning. Based on Section 3.3.1 results, backbone networks significantly\nimpact model performance. Therefore, this section compares eight backbone networks\nand four clustering methods-as detailed in Table 4, with DMH + GoogLeNet demon-\nstrated excellent performance in accuracy (0.8411), precision (0.8593), recall (0.8411),\nF1-score (0.8390) and high clustering quality, so it is the final model."}, {"title": "3.4 Ablation Studies", "content": "Our work incorporates three major contributions: a pre-clustering segmentation mech-\nanism, an embedded SCSE module in the segmentation network, and a dynamic multi-\nprojection head clustering algorithm (DMH) . Ablation studies analyze each compo-\nnent's impact, starting from K-Medoids baseline, incrementally adding components,\nand finally removing PCA. Results are in Table 5.\nBaseline Model (K-Medoids). As shown in Table 5a, the original K-Medoids algo-\nrithm without feature engineering achieves low accuracy (0.5430) and F1-score\n(0.5767), with high recall (0.6225) but low precision (0.5371), indicating many false\npositives. The Davies-Bouldin index (0.3350) also shows poor separability.\nSegmentation Mechanism. As shown in Table 5b, introducing a GoogLeNet-based\nsegmentation network (without SCSE) improves accuracy to 0.8400 and F1-score to\n0.8397. The DB improves to 2.0853, confirming enhanced intra-class consistency.\nSCSE Mechanism in the Segmentation Network. As shown in Table 5c, integrating\nthe SCSE module (Table 5c) leads to qualitative improvements: accuracy surpasses the\n0.86 threshold (+2.38%), and the F1-score increases by 2.41%.\nDynamic Multi-Projection Head Clustering Algorithm. As shown in Table 5d, the\nfull experiment incorporates our novel DMH algorithm. Results show excellent perfor-\nmance across all metrics: accuracy (0.8411), precision (0.8593), recall (0.8411), and\nF1-score (0.8390). In clustering quality, the CH (142.8781) increases by 2.93-fold, and\nthe DB (1.3839) decreases by 26.2%, indicating enhanced structural separability in the\nfeature space, effectively capturing the heterogeneity of diabetic macular edema.\nRemoval of PCA Dimensionality Reduction from the Full Model. To evaluate the\nbalance between computational efficiency and performance, we compare the full model\nwith a PCA-free version. As shown in Table 5e, quantitative analysis reveals that PCA\nretains critical classification information while optimizing computation: accuracy\n(0.8377 vs. 0.8411) and F1-score (0.8355 vs. 0.8390) differ by less than 0.5%."}, {"title": "4 Discussion and Conclusion", "content": "The unsupervised learning framework, RURANET++, proposed in this study, signifi-\ncantly reduces the reliance on annotated data for the diagnosis of Diabetic Macular\nEdema (DME) by integrating an optimized U-Net segmentation network, the SCSE\nattention mechanism, and a dynamic multi-projection head clustering algorithm. The\nSCSE module effectively enhances the feature capture capability for minute lesions,\nwhile PCA dimensionality reduction optimizes computational efficiency by retaining\n94.32% of the feature information. Additionally, the dynamic clustering mechanism\nimproves the stability and accuracy of results through adaptive threshold adjustments.\nThe results show that the model's metrics in both segmentation and clustering tasks\nsignificantly outperform existing methods, validating the potential of unsupervised\nstrategies in medical image analysis. This framework provides an efficient solution for\nearly screening of DME in resource-limited settings. Future work will extend this re-\nsearch to include multi-modal data fusion and cross-disease generalization studies.\nHowever, despite achieving favorable results across multiple performance metrics,\nthe study still encounters certain computational complexities when handling high-di-\nmensional data. To further enhance computational efficiency and model scalability, fu-\nture research could explore more efficient feature dimensionality reduction techniques\nand optimize clustering algorithms to reduce computational overhead. Moreover, as the\ndataset size increases, the model's generalization ability will require further validation."}]}