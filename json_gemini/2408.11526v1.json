{"title": "RConE: Rough Cone Embedding for Multi-Hop Logical Query Answering on Multi-Modal Knowledge Graphs", "authors": ["Mayank Kharbanda", "Rajiv Ratn Shah", "Raghava Mutharaju"], "abstract": "Multi-hop query answering over a Knowledge Graph (KG) involves traversing one or more hops from the start node to answer a query. Path-based and logic-based methods are state-of-the-art for multi-hop question answering. The former is used in link prediction tasks. The latter is for answering complex logical queries. The logical multi-hop querying technique embeds the KG and queries in the same embedding space. The existing work incorporates First Order Logic (FOL) operators, such as conjunction (\\land), disjunction (\\lor), and negation (\\neg). Though current models have most of the building blocks to execute the FOL queries, they cannot use the dense information of multi-modal entities in the case of Multi-Modal Knowledge Graphs (MMKGs). We propose RConE, an embedding method to capture the multi-modal information needed to answer a query. The model first shortlists candidate (multi-modal) entities containing the answer. It then finds the solution (sub-entities) within those entities. Several existing works tackle path-based question-answering in MMKGs. However, to our knowledge, we are the first to introduce logical constructs in querying MMKGS and to answer queries that involve sub-entities of multi-modal entities as the answer. Extensive evaluation of four publicly available MMKGs indicates that RConE outperforms the current state-of-the-art. The source code and datasets are available at https://github.com/kracr/rcone-qa-mmkg.", "sections": [{"title": "I. INTRODUCTION", "content": "Knowledge Graph (KG) [1] is a directed graph with a set of entities (nodes) and directed relations among those entities (edges). KGs are an excellent tool for representing data in graph topology. They are used in applications such as question-answering and recommendation systems in diverse fields like biomedicine, physics, and geoscience [2].\nMulti-hop logical query answering over KGs has gained attention recently [3]. Various neural methods are proposed to answer a logical query. It involves traversing one or more hops over a KG to reach the answers. The query generally consists of First Order Logic (FOL) operators, such as existential quantification (\\exists), conjunction (\\land), disjunction (\\lor), and negation (\\neg).\nCurrent State-of-the-Art (SOTA). There are two major challenges to consider while handling logical query answering over KGs [4]. First, long and complex queries on large KGs incur exponential computational costs. Second, a robust model for handling missing edges in the graphs. Several methods have been proposed to embed the KG and queries in the same space to tackle these issues [5]. These methods iteratively pro-gressed to incorporate different FOL operators in the queries. Geometric [4], [6], and probabilistic [7] methods embed the queries as geometrical shapes and probabilistic distributions, respectively. These methods are scalable and do not keep track of the intermediate nodes.\nShortcomings of SOTA Methods. [RQ1] Multi-Modal Knowledge Graphs (MMKGs) are KGs with multiple modali-ties, such as images, texts, and videos, as entities [8]. Though the current approaches can handle all the FOL operators well, they cannot incorporate the rich information of multi-modal entities. The node\u2019s embedding, in these models, is based on the relations with its neighbors. They do not consider the entity\u2019s features, which may lead to the loss of critical information in the case of multi-modal entities. [RQ2] There can be multiple subjects in a single multi-modal entity, and it might be that all the subjects are not answers to a query. One of the goals of this work is to get those individual subjects as answers. [RQ3] One way to tackle [RQ1] and [RQ2] is to generate a sub-KG for each multi-modal entity before training (offline). Convert the original MMKG to a non-multi-modal KG by merging all sub-KGs with the MMKG. However, for large MMKGs, constructing the sub-KGs for each multi-modal entity will incorporate high pre-processing and space overhead.\nTo address these challenges, we propose RConE, an em-bedding method for answering logical queries on MMKGs. In this work, we focus on entity/relational labels and images as the modalities\u00b9. We summarize our contributions as follows.\nContribution I: Logical Query Answering on MMKGs at a finer granularity. We propose a novel problem of query answering using FOL constructs on MMKGs. In this problem, the answers might not be complete entities but some part (sub-entities) of the multi-modal entities. To our knowledge, we are the first to handle such queries. Consider the MMKG in Figure 1. For the query, \u201cShirt color of the actor not wearing brown shoes in the Toy Story movie,\u201d the answer is Green. The related computational graph is in Figure 2. In the MMKG, Green (shirt), is in the Toy Story (Poster) entity (sub-entity of Toy Story (Poster) entity), but there is no entity Green in the MMKG. To cater to these kinds of queries, we propose our model RConE.\nContribution II: RConE \u2013 A novel perspective of han-dling the query answering on MMKGs. We extend one of"}, {"title": "III. PRELIMINARIES", "content": "MMKG. A Multi-Modal Knowledge Graph $G(V,R,U,M)$ is a directed graph where V and R are entity and relation sets, respectively. U is the triplet set, represented as\n$U = \\{(e_s,r,e_d) \\mid e_s,e_d \\in V,r \\in R\\}$\nThere are $|M| = k$ modalities in the graph, such that\n$\\gamma(e_i) \\in M = \\{M_1, M_2, ..., m_k\\}, e_i \\in V$\nwhere $m_1$ is the generic entity label. In Figure 1, the entities {Tom Hanks, Walt Disney} $\\in m_1$ (generic) and {Toy Story (Poster), Walt Disney (Logo)} $\\in m_2$ (images). We consider $k = 2$, in this work.\nSub-Entity Knowledge Graph (Scene Graph). For each node, $e_j$, such that $\\gamma(e_j) = m_2$, we define a sub-entity KG or a scene graph as $SG_j(V_j, R_j,U_j)$. $V_j$, $R_j$, and $U_j$ are sub-entity, sub-relation, and sub-triplet sets, respectively. The $SGs$ are the KGs describing each multi-modal entity in $G$.\nFOL Queries. First Order Logic (FOL) queries $q \\in Q = \\{Q_{train}, Q_{test}\\}$ consists of logical operators such as conjunc-tion ($\\land$), disjunction ($\\lor$), existential quantification ($\\exists$), and negation ($\\neg$). We are not including universal quantification ($\\forall$), similar to [6], [7], as its applications in real-world KGs are rare. Queries are expected to be in Disjunctive Normal Form (DNF). This enables us to handle the union operator at the end, which helps the model to be scalable for long queries. An FOL query $q \\in Q$, with respect to an MMKG, comprises a non-variable anchor entity set\n$V_a \\subseteq V_u = \\bigcup_{j=1}^{\\nu} V_j: \\bigcup_{V_j \\mid \\gamma(e_j) = m_2}$"}, {"title": "IV. METHOD", "content": "Figure 4 presents the flow diagram of our model RConE. It comprises two modules, the RConE Engine (RCE) and the Sub-Entity Prediction module. The RCE traverses query and outputs two entity sets 1) Answer entities in the rigid region (John Lassester in Q2). 2) Multi-modal entities consisting of the answers in the fuzzy region (Toy Story (Poster) in Q1). For each candidate multi-modal entity in the fuzzy region (here, Toy Story (Poster)), the Sub-Entity Prediction module initially generates a sub-entity KG (scene graph). Following this, it embeds the scene graph. Finally, the module transforms the sub-entities to the RConE's embedding space. The transforma-tion is such that the answer sub-entities (Green in Q1) belong in the rigid region and other sub-entities outside the RConE embedding. In the following subsections, we describe each module in detail. A brief overview of the steps is presented in Algorithm 1 in the supplementary material.\nA. RConE Engine (RCE)\nAs shown in Figure 4, RCE takes MMKG and queries as input and embeds them in the RConE embedding space. It traverses the query computational graph to generate the embedding. During traversal, RCE handles different FOL operators in a query. We first discuss the embedding procedure for queries and entities. Following it are the details about the transformation in embeddings based on the FOL operators.\nEntity and Query Embedding. Let $A(q)$ be the entity set satisfying the query q. The embedding for $A(q)$ is the cartesian product of d-ary (fuzzy-rigid) sector cones in $K^d$ embedding space (Figure 3). The embedding is presented as $V_q = (\\theta_{ax}, \\theta_{ri}, \\theta_{fu})$, where $\\theta_{ax} \\in [-\\pi,\\pi)^d$ is the semantic axis. $\\theta_{ri} \\in [0, 2\\pi]^d$ is the rigid aperture enclosing the region A around it. $\\theta_{fu} \\in [0, 2\\pi - \\theta_{ri}]^d$ is the fuzzy aperture enclosing region B around the rigid cone.\nAll entities belonging to the answer set (John Lasseter in Q2) will have embeddings in the region A. The multi-modal entities in G, which have some part in the answer (Toy Story (Poster) in Q1), will have embeddings in region B.\nThe embedding of node $v \\in V_u$ is represented as $v = (\\theta_{ax}, 0, 0)$. It can be considered as an answer set consisting of a single answer v. The embedding consists of no rigid and fuzzy areas (blue vector in Figure 3).\nThe reasons we are using the rough sets (fuzzy (boundary) region) as an extension to the original ConE [6] model are\n$\\bullet$ The multi-modal entities should operate in the same space as the unimodal entities as we are traversing the query in that space.\n$\\bullet$ The multi-modal entities containing the answer would be semantically similar to other answers. So, their embed-ding should also be closer. Hence, they would belong to the boundary region with partial membership (Equation 7c) to the answer set.\nLogical Operators. The query computational graph consists of different logical operators (Intersection, Union, and Com-plement) along with the relation projection operator. The $A(q)$'s RConE embedding resembles the answer set that satisfies the processed query at any moment through this traversal. Figure 5 consists of the visual representation of how all the operations work on 1-ary RConE embedding. We present the modeling of each operator below. Each operator has been extended to accomodate the fuzzy boundary part in ConE [6].\nProjection. Projection is a relation-dependent transforma-tion of answer distribution ($A(q)$), with the transformed em-bedding in the same space as the original one. We define projection mapping using the following function.\n$P_r: V_q \\rightarrow V_{q'}, \\forall V_{q}, V_{q'} \\in K^d$"}, {"title": "V. EXPERIMENTS", "content": "A. Experimental Setup\nDatasets and Evaluation Metrics. For evaluation, we use multi-modal datasets FB15k, FB15k-(237), YAGO15k, and DB15k [31]. We also train on FB15k (NMM) [22] (uni-modal dataset) to check whether the performance of our proposed model degrades compared to its non-multi-modal counterpart (ConE). Table II contains statistics of all the datasets. We use the average Mean Reciprocal Rank (MRR) and average HITS scores of all the answers to a query. We use these metrics for all three goals (1a, 1b, 2) described in Section IV-C.\nBaselines. Since RConE is the first multi-modal logical query-answering algorithm, we choose non-multi-modal models Be-taE [7] and ConE [6] as baselines. BetaE embeds the query answer distribution as d-dimensional beta distribution. The method was the first to handle the negation (\u00ac) operator. ConE"}, {"title": "D. Fuzzy (Boundary) Region Integration Analysis", "content": "In this sub-section, we check the impact of adding the fuzzy region to the original ConE embedding. For this, we compare the neural implementation of the projection operator with the actual set operator. However, the neural method may not precisely imitate the set operator. The goal is to check the performance using the fuzzy region compared to other components and baselines. We use the model trained on the FB15k dataset.\nWe randomly generate 8000 pairs of RConE embeddings $(A_i, B_i)$, such that $A_i \\subseteq B_i$. The goal is to check if, after the projection operation, the final embeddings $A_{pi}$ and $B_{pi}$ are still holding the relation $A_{pi} \\subseteq B_{pi}$. We select an arbitrary relation, $r_i$, from the relation set in the FB15k dataset for the projection. We check the intersection area by calculating the score as $s_c = Ar(A_{pi} \\cap B_{pi})/Ar(A_{pi})$, where $Ar()$ is the area of the sector cone. The score for the rigid region is 41%, while for the fuzzy region, it is 57%. The score for the ConE model is 46%. For the actual projection operator, the rigid region slightly drops in the score if we compare it with the baseline, and the score of the fuzzy part lies in the same range (better in this case). Hence, the fuzzy region performs on par with our model's other neural structures and baselines. Therefore, the component addition is reasonable."}, {"title": "E. Time Complexity Analysis", "content": "[RQ3] Efficiency and scalability of RConE.\nEfficiency. Generating scene graphs for each multi-modal entity before training, even if there is no query related to it, will incur extra overhead. To counter this, in RConE, we extract the scene graph for the multi-modal entities belonging to the fuzzy region only, which substantially decreases the computational cost.\nScalability. For the RCE module, we are adding a parameter, $\\theta_{fu}$, in the original ConE architecture. Its space complexity depends on the number of relations in the MMKG, while its time complexity for each operator in RCE is on par with Ori. Hence, it would not have much impact on the computation cost as we scale (compared to ConE). The Sub-Entity Prediction module uses three architectures: Scene Graph Generation, Scene Graph Embedding, and Graph Transformation. We train the Scene Graph Generation module to generate graphs to a dataset (Visual Genome) which is independent of the MMKG size. The Scene Graph embedding module's time complexity depends on the scene graph's size and is independent of the size of the KG. For the Graph Transformation module, we have neighbors of the multi-modal entity for external context. In real-world scale-free networks, the degree distribution follows near power law, with few nodes having a substantial number of neighbors, so it would have little impact as we scale. Hence, we can say that RConE is scalable."}, {"title": "VI. CONCLUSION AND FUTURE WORK", "content": "In this work, we proposed a novel method, RConE, for extracting information from multi-modal entities to resolve logical queries. Our model is the first to answer logical queries related to multi-modal entities and can handle queries with sub-entities as answers. Our proposed model can also generalize well in handling multi-modal and non-multi-modal answer based queries. It performs very efficiently for multi-modal query answers, with minimal impact on non-multi-modal answers. We also provide a novel query generation module for MMKGs, which would be helpful for the research community in generating queries with sub-entity answers.\nThe future goal is to explore the idea of inductive question answering and to get an answer by fusing information from multiple entities of different modalities for better context while answering a query. Apart from images, we will also consider working on other modalities for query answering."}, {"title": "APPENDIX", "content": "A. Algorithm\nAlgorithm 1 provides the pseudo-code for RConE. The model first initializes embeddings for all the entities and relations. It also pre-trains the scene graph generation module (Lines 1-2). For each training iteration, it selects a random query $q_i \\in Q_{train}$ (Lines 3-4). The model picks the start nodes from the computational graph of query $q_i$ (Line 5). It then traverses the MMKG based on the query and executes FOL operators until it reaches the end of the computational graph (Lines 6-7). This gives $V_q$, as query embedding, an answer set for Type II answers, and candidate multi-modal entities. We then calculate the loss $L_c$, $L_{mm}$ (Equation 31) (Line 9). The model generates a scene graph for each candidate multi-modal entity containing the answer. It then embeds the scene graph and transforms it to the query embedding space (Lines 11-13). Loss $L_{se}$ is calculated for the sub-entity answer (Equation 31) (Line 15).\nB. Dataset Description\n$\\bullet$ Freebase-15k (FB15k (NMM)) [22] is a subset of the original Freebase dataset of general facts. The dataset consists of 14,951 entities and 1,345 relations. Both entities and relations have at least 100 mentions. $\\bullet$ Freebase-15k Multi-Modal (FB15k) [31] is a multi-modal extension of the original FB15k (NMM) dataset. It consists, on average, 55.8 scaled images for each entity present in the KG. $\\bullet$ Freebase-15k (237) Multi-Modal (FB15k-(237)) [31] is the FB15k dataset with 237 relations in it. The original non-multi-modal dataset was proposed to counter the problem of inverse relation test leakage [35]. $\\bullet$ YAGO Multi-Modal (YAGO15k) [31] Similar to FB15k, this is a multi-modal extension of Yet Another Great Ontology (YAGO) dataset [36]. The YAGO dataset extends the information of Wordnet by merging general knowledge from Wikipedia. $\\bullet$ DBPedia Multi-Modal (DB15k) [31] DB15k is an extension of the original DBpedia dataset [37], with images as entities included in it.\nC. Query Generation\nFigure 8 represents the query generation module."}]}