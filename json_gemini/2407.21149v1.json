{"title": "Domain Shift Analysis in Chest Radiographs Classification in a Veterans Healthcare Administration Population", "authors": ["Mayanka Chandrashekar", "Ian Goethert", "Md Inzamam Ul Haque", "Benjamin McMahon", "Sayera Dhaubhadel", "Kathryn Knight", "Joseph Erdos", "Donna Reagan", "Caroline Taylor", "Peter Kuzmak", "John Michael Gaziano", "Eileen McAllister", "Lauren Costa", "Yuk-Lam Ho", "Kelly Cho", "Suzanne Tamang", "Samah Fodeh-Jarad", "Olga S. Ovchinnikova", "Amy C. Justice", "Jacob Hinkle", "Ioana Danciu"], "abstract": "Objectives: This study aims to assess the impact of domain shift on chest X-ray classification accuracy and to analyze the influence of ground truth label quality and demographic factors such as age group, sex, and study year.\nMaterials and Methods: We used a DenseNet121 model pretrained MIMIC-CXR dataset for deep learning-based multilabel classification using ground truth labels from radiology reports extracted using the CheXpert and CheXbert Labeler. We compared the performance of the 14 chest X-ray labels on the MIMIC-CXR and Veterans Healthcare Administration chest X-ray dataset (VA-CXR). The VA-CXR dataset comprises over 259k chest X-ray images spanning between the years 2010 and 2022.\nResults: The validation of ground truth and the assessment of multi-label classification performance across various NLP extraction tools revealed that the VA-CXR dataset exhibited lower disagreement rates than the MIMIC-CXR datasets. Additionally, there were notable differences in AUC scores between models utilizing CheXpert and CheXbert. When evaluating multi-label classification performance across different datasets, minimal domain shift was observed in unseen datasets, except for the label \"Enlarged Cardiomediastinum.\" The study year's subgroup analyses exhibited the most significant variations in multi-label classification model performance. These findings underscore the importance of considering domain shift in chest X-ray classification tasks, particularly concerning study years.\nConclusion: Our study reveals the significant impact of domain shift and demographic factors on chest X-ray classification, emphasizing the need for improved transfer learning and equitable model development. Addressing these challenges is crucial for advancing medical imaging and enhancing patient care.", "sections": [{"title": "1 Introduction", "content": "Chest radiography is the first-line imaging test for respiratory disease and some forms of cardiac disease. Chest X-ray abnormalities detection has been automated by recent advanced artificial intelligence techniques. Accurate chest X-ray classification has played an important role in many biomedical applications to accelerate the diagnosis and treatment of conditions like pneumonia, heart failure, rib trauma, pulmonary fibrosis, etc. Though there has been an increase in the integration of machine learning models to enhance diagnostic promise, the efficacy of these models hinges on the availability and quality of training data.\nRecent years have seen a wave of artificial intelligence (AI) and machine learning models for clinical applications from large research medical centers or based on limited de-identified datasets. The introduction of transfer learning has resulted in AI models being accessible to researchers with fewer computational resources and less data, allowing them to leverage existing knowledge. Transfer learning is the ability to apply a model trained on a dataset to another dataset of interest. A major consideration for transfer learning is domain shift, the dissimilarity between data distributions from the source used for training and the population to which the models are applied. This divergence between publicly available datasets and private, institution-specific datasets can introduce substantial bias and hinder the generalization of machine learning models to real-world scenarios.\nIn this study, we quantify the domain shift between the public domain (MIMIC-CXR) and a private dataset (VA-CXR). The efficacy of the transfer learning approach for chest X-ray classification and the subsequent impact on classification accuracy when dealing with domain shift form the main focus of this study. The domain shift is traditionally viewed only based on the model and its performance, ignoring the multi-fold causes leading to the shift. We comprehensively address the effects of domain shift in this three-stage study: 1) Compare the performance between the source domain and target domain accuracy on the chest X-ray classification. 2) Quantify the quality of the ground truth extracted from the radiology reports, as supervised learning models are heavily dependent on the quality of the labels. 3) Analyze the relationship between demographic factors and classification accuracy, as domain mismatches often originate in demographic mismatches.\nBy addressing the critical interplay of domain shift and demographic factors, this study not only provides insights into the technical challenges of adapting machine learning models to private datasets but also underscores the broader implications of these challenges in the context of healthcare. Our research contributes to developing more accurate, robust, and generalizable chest X-ray classification models by creating a systematic approach for using an existing model and understanding the nuances of a model's performance before applying it to a new population."}, {"title": "2 Methods", "content": ""}, {"title": "2.1 Dataset", "content": "MIMIC-CXR is a publicly available dataset of 377,110 chest X-rays associated with 227,827 imaging studies from 65,379 patients. MIMIC-CXR is collected from the inpatient setting of Beth Israel Deaconess, a Boston hospital.  We also use the test-split of this dataset (Test-split MIMIC-CXR) as the hold-out set analysis, which consists of 5159 studies with 293 patients [6].\nVA-CXR is a private dataset of 259,361 chest X-rays associated with 91,020 imaging studies from 35,771 patients. VA-CXR is collected in the outpatient setting of the Boston Veterans Healthcare Administration station (refer to Table 1). The ground truth labels were extracted from the VA's corporate data warehouse (CDW) and joined with images using patient information from DICOM headers as published in Knight et al. [9]."}, {"title": "2.2 Ground truth Label Extraction", "content": "We used the CheXbert labelers [13] to expertly assign labels to 14 specific labels (Atelectasis, Cardiomegaly, Consolidation, Edema, Enlarged Cardiomediastinum, Fracture, Lung Lesion, Lung Opacity, No Finding, Pleural Effusion, Pleural Other, Pneumonia, Pneumothorax, Support Devices) associated with different chest conditions from radiology reports. CheXbert is a BERT-based approach that automates the detection of these observations, effectively streamlining the process of annotating medical images and reports. The NLP label extraction outputs scores for four classes: positive, negative, blank, and uncertain, associated with each of the 14 labels. As the class names indicate, for"}, {"title": "2.3 Label Validation", "content": "Because ground truth assignment ultimately determines the accuracy of the imaging classifiers, we developed an image evaluation procedure in two steps. First, we evaluated the agreement between the NLP label extraction tool, CheXbert, and its precursor, CheXpert [5], a rule-based tool. We focused on positive class agreements for our evaluation, using only positive classes for classification, and have combined uncertain/negative class agreements. The disagreement between NLP label extraction tools indicates ambiguity/ less confidence on the labels, ultimately creating an unreliable ground truth. The agreement is measured for both MIMIC-CXR and VA-CXR datasets; Of note that neither of the datasets was used to train the NLP extraction tools."}, {"title": "2.3.1 Relation to Diagnoses Codes", "content": "To validate the ground extracted from chexpert-labeler, we analyzed the relationship of specific ground truth labels to ICD codes in the patient's electronic health record (EHR) extracted from the VA's Corporate Data Warehouse (CDW).The assignment of ICD-9 and ICD-10 diagnosis codes associated with each condition was exploratory and not extensively optimized.\nStarting concepts were retrieved from the Chexpert-labeler github repository, where phrases they used to search notes can be found at https://github.com/stanfordmlgroup. These phrases, along with our own expertise, were used to identify diagnosis codes and cross-reference radiology reports with diagnoses. For example: Pneumonia was identified by chexpert-labeler as indicated by pneumonia, infection, infected process, and infectious; Edema was indicated by terms edema, heart failure, chf, vascular congestion, pulmonary congestion, indistinctness, and vascular prominence; Fracture was indicated solely by the word fracture; and Pneumothorax was identified by either pneumothorax or pneumothoraces.\nThis method enabled us to validate the ground truth labels by correlating them with the relevant ICD codes in the patients' EHRs, ensuring accurate cross-referencing of radiology reports with diagnoses.\nFor instance, the ICD-9 codes we used to indicate a pneumonia diagnosis in the outpatient diagnosis tables ranged from 480 to 486 and included 487.0. These codes encompass viral, bacterial, and other types of pneumonia, as well as pneumonia caused by unspecified pathogens. A similar approach was applied to ICD-10 codes for pneumonia and other conditions. The specific ICD codes used for each condition are detailed in Appendix Table A.2."}, {"title": "2.4 Multi-label Image Classification", "content": "Using the 14 labels extracted from the corresponding radiology reports with CheXbert, we created a multi-label image classification model from X-ray images. We used a pre-trained DenseNet model[3] as the core framework, removed the top classification layer, and integrated a custom classification layer for multi-label output. Previous work shows the effectiveness of different resolutions of DenseNet-121 based multi-label classification chest X-ray model on MIMIC-CXR dataset [2]. This work uses the MIMIC-CXR trained DenseNet-121 Model on Chest X-ray pre-processed into 256x256 JPG images [6, 7]."}, {"title": "2.5 Metrics", "content": "We evaluated our models using the Area Under the Curve (AUC). The AUC score was calculated separately for each of the 14 labels, indicating the separability measure for a given chest X-ray label. We also analyzed the difference in AUC scores between MIMIC-CXR and VA-CXR, and the prevalence for each label was calculated as the number of studies with positive results for a given label divided by the total number of labels, indicating the label's presence in the given cohort."}, {"title": "2.6 Domain Shift Analysis", "content": "We compared our source and target datasets along different dimensions: 1) Demographic Details: Age At Time of Imaging Study, Sex; 2) Imaging Study Details: Study Year, View Point (Lateral view (Lat), erect anteroposterior (AP), posteroanterior (PA)) ; 3) Ground truth Labels: 14 labels. All the"}, {"title": "3 Results", "content": ""}, {"title": "3.1 Ground truth Label Validation", "content": "Table 2 presents a comprehensive analysis of agreement and disagreement rates between Chexpert and Chexbert on the MIMIC-CXR and VA-CXR datasets across the 14 labels. The results shed light on the performance and consistency of Chexpert and Chexbert and the uncertainty of the ground truth labels. Notably, in MIMIC-CXR, Atelectasis was identified in 19.5% of cases with a disagreement rate of 10.1%, whereas in VA-CXR, the identification rate was lower at 9.8% with a disagreement rate of only 0.7%. This discrepancy in positive identification and disagreement rates is further exemplified in conditions like Cardiomegaly, where MIMIC-CXR reported positive identification in 15.7% of cases with a significant disagreement rate of 39.1%, contrasting with VA-CXR's 9.3% positive identification rate and 13.7% disagreement rate."}, {"title": "3.2 Comparison with Diagnosis Codes", "content": "While the X-ray classification accuracy was evaluated against the label extracted using NLP of the appropriate radiology report, it is possible also to compare directly against diagnosis codes in the clinical record. However, interpreting this comparison is challenging due to two main issues: first, diagnoses are based on a broader range of information beyond just the X-ray, and second, patients may have multiple conditions, making it difficult to determine the specific reason each X-ray was ordered. Despite these challenges, we have conducted two useful comparisons of X-ray findings against diagnosis codes for nine categories within our dataset.\nThe top of Figure 2 shows the fraction where a diagnosis related to the label was recorded within a week (either before or after) of the X-ray, compared to the total number of patients, including those who never had the diagnosis. Patients who had the diagnosis but not within the one-week window were excluded from this calculation. This plot, labeled 'Sensitivity,' reflects the proportion of cases where a diagnosis occurs within a week of noting the condition in the X-ray. We observe a positive finding on an X-ray, as extracted from the radiology report, which is mostly associated with a specific diagnosis of pneumonia and edema and frequently associated with pleural effusion and pneumothorax. We also see that the ChexBert model is more frequently associated with a corresponding diagnosis than the ChexPert model.\nThe bottom of Figure 2 compares the factor by which the enrichment ratio of positive finding in the radiology report increases with a concurrent (within one week) diagnosis. Pleural effusion, for example, is seen 10 times more frequently in the radiology report when a concurrent diagnosis code"}, {"title": "3.3 Multi-label Image Classification on VA-CXR across NLP tools", "content": "Table 3 compares ground truth extraction techniques, specifically focusing on the label-wise performance metrics on the VA-CXR dataset. Two techniques, ChexPert and ChexBert, are evaluated based on their Area Under the Curve (AUC), prevalence, and count across 14 labels. Across the findings, both techniques generally demonstrate similar AUC values, indicating comparable performance in distinguishing positive cases. For instance, in identifying Atelectasis, both ChexPert and ChexBert exhibit AUC values around 0.80, with similar prevalence and count numbers. Notably, in the case of Cardiomegaly, ChexBert shows a notably higher AUC of 0.862 compared to ChexPert's 0.753, suggesting potentially superior performance in this specific finding. However, the prevalence and count metrics vary across the findings and between the two techniques. For example, ChexPert tends to have higher prevalence and count values for several findings like ECM (Enlarged Cardiomediastinum), while ChexBert shows higher values for others such as Pleural Effusion."}, {"title": "3.4 Multi-label Image Classification Performance across multiple datasets", "content": "Table 4 shows the comparison of MIMIC-CXR (Source dataset), Test Split MIMIC-CXR (Hold-out Source Dataset), and VA-CXR based on AUC on the 14 labels. The Hold-out Source dataset is the test split of MIMIC-CXR dataset, DenseNet-121 Model. The Test Split of MIMIC-CXR gives us a fair comparison to VA-CXR, the unseen target dataset. This can be observed based on the AUC drop from the overall MIMIC-CXR to test-split. In Table 4, the difference in AUC between Hold-out and Target indicates the performance variation between VA-CXR and Test Split of MIMIC. The Negative value of the difference in AUC indicates that the VA-CXR performs better than the Test Split MIMIC-CXR, and the positive value indicates that the Test Split performs better. The Enlarged Cardiomediastinum (ECM) label has the highest difference in AUC, indicating a huge performance drop in VA-CXR. This could directly impact the lack of a large number of image studies in ECM in the source dataset compared to the target."}, {"title": "3.5 Study Year-wise Performance on VA-CXR", "content": "Table 3 shows the label-wise distribution of the study years. The AUC systematically drops as for study year 2020 to 2022 for all labels except Consolidation and Pleural Other, which peaks at 2020. The prevalence increases over the years in VA-CXR for Atelectasis, Enlarged Cardiomediastinum, and Pleural Effusion."}, {"title": "3.6 Performance across datasets based on Sex", "content": "Figure 4 compares the label-wise AUC and prevalence of the two sexes across the Test Split MIMIC-CXR and VA-CXR. This comparison is essential as the female-male patient ratio in VA-CXR is higher than that of MIMIC-CXR; dashed lines in the figure 4 can observe this."}, {"title": "3.7 View Position-wise across datasets", "content": "Figure 5 compares labels across Test-Split MIMIC-CXR and VA-CXR. The VA-CXR doesn't contain any Lateral images; the figure shows only MIMIC-CXR performance on the Lateral. The prevalence and AUC of viewpoints vary based on the label, with ECM having the most difference between the datasets. Pleural Other has a drop in performance in VA-CXR in AP view position, potentially due to low prevalence in both datasets."}, {"title": "3.8 Age Group-wise comparison across datasets", "content": "Figure 6 shows the VA-CXR prevalence increase as the age increases across all labels except No Finding, Lung Lesion, and Pneumothorax. The highest performance drops of 0.15 to 0.2 AUC in VA-CXR compared to the Test Split of MIMIC-CXR can be observed in Enlarged Cardiomediastinum and Support Devices. In VA-CXR, it is interesting that the AUC performance across the age groups is more stable, i.e., there is not much change in AUC, with the exception of Atelectasis."}, {"title": "3.9 Summary", "content": "As seen from the results, the ground truth validation and multi-label classification performance across the NLP extraction tools showed that though the VA-CXR dataset has lesser disagreement rates than the MIMIC-CXR datasets, and there were AUC differences between models when using ChexPert and ChexBert (potentially propagated by distribution differences). When comparing the multi-label classification performance on different datasets, the unseen datasets didn't show domain shift other than a few labels such as Enlarged Cardiomediastinum. Among the different subgroup analyses, the study year had the most drastic differences in the performance of the multi-label classification model. These differences indicate that the domain shift is definitely of concern in study years."}, {"title": "4 Discussion", "content": "Our study quantified the domain shift between a publicly available critical care dataset (MIMIC-CXR) and an outpatient, private institutional dataset (VA-CXR), assessing the efficacy of transfer learning for chest X-ray classification. Because domain shift encompasses multifaceted factors, our comprehensive study was structured into three interrelated parts: the quality of ground truth, performance comparison, and subgroup analysis.\nQuality of Ground Truth Supervised learning models heavily rely on the quality of labels for training. Therefore, we quantified the ground truth quality extracted from radiology reports in both datasets. Despite being a source dataset, MIMIC-CXR exhibited significantly higher disagreement rates between the two ground truth information extraction NLP algorithms than the target, VA-CXR dataset. The potential performance drift of the target can be attributed to the mismatch of ground truth extraction methods. Our analysis underscored the importance of high-quality annotations in mitigating the effects of domain shift [12, 11] and improving model performance.\nPerformance Comparison: We compared the difference in classification performance between MIMIC-CXR and VA-CXR to understand the extent of domain shift and its implications for model generalization. Our findings revealed notable variability in classification accuracy, highlighting the challenges posed by domain shift. We observe that the prevalence and performance were directly associated; for example, MIMIC-CXR's enlarged cardio mediastinum low prevalence may be the reason for its low performance.\nSubgroup analysis Subgroup analysis using demographic factors is crucial as we expect our populations to have different demographic distributions [1]. We observed a decline in the performance of the VA-CXR dataset over time, particularly in the years following 2020. This drop may be due to differences in the study years between the source dataset (MIMIC-CXR) and the VA-CXR dataset, as well as potential impacts from the pandemic years (note: we did not evaluate additional labels related to the pandemic). We observed that both groups, though with different distributions, have aging populations across conditions of interest. Still, the prevalence of most conditions with age increases in the VA-CXR dataset, but it is highly variable in MIMIC-CXR. For sex subgroup analysis, the performance was similar across the female and male populations in the datasets. Though the VA-CXR population is skewed towards the male population, the model performed well on the female population. This can be attributed to the source domain (MIMIC-CXR) having a balanced male-to-female ratio. For the view-based subgroup, we observed high variability between the views. This behavior can be attributed to the necessity of specific views for accurately diagnosing certain diseases. For instance, conditions such as Pneumonia and ECM primarily rely on PA (Posteroanterior) or AP (Anteroposterior) views for diagnosis.\nChest radiography is a first-line imaging modality for assessing lung and heart conditions. With advanced artificial intelligence techniques, the automation of chest X-ray abnormality detection has seen significant progress [4]. This has accelerated the pace of diagnosis and treatment and opened avenues for exploring biomedical applications in various clinical settings. However, the efficacy of these machine learning models heavily relies on the quality and availability of training data [14].\nClinical relevance Despite the surge in artificial intelligence and machine learning applications in clinical settings, their adoption has been uneven, primarily due to disparities in funding[8], expertise, and availability of computing resources. While academic medical centers have been at the forefront of AI adoption, the accessibility of such resources remains a challenge for smaller research institutions. Introducing transfer learning has mitigated some of these challenges by enabling researchers to leverage pre-existing models trained on publicly available datasets for their specific applications. However, transfer learning introduces the concept of domain shift, which poses a significant obstacle"}, {"title": "5 Conclusion", "content": "In conclusion, our study sheds light on the critical interplay between domain shift, demographic factors, and the efficacy of transfer learning in chest X-ray classification. By quantifying the domain shift between a publicly available dataset and a private institutional dataset, we have identified disparities in classification accuracy and highlighted the challenges posed by transferring models across domains. Moreover, our analysis of demographic factors underscores the importance of considering population diversity in model development to ensure equitable healthcare outcomes. Moving forward, addressing these challenges will require concerted efforts to improve data quality, develop robust transfer learning techniques, and enhance the generalizability of machine learning models in clinical practice. Ultimately, by fostering a deeper understanding of these complexities, our research paves the way for the development of more accurate, reliable, and accessible chest X-ray classification models with the potential to transform patient care and advance the field of medical imaging."}]}