{"title": "MMPT: Multimodal Prompt Tuning for Zero-shot Instruction Learning", "authors": ["Taowen Wang", "Yiyang Liu", "James Chenhao Liang", "Junhan Zhao", "Yiming Cui", "Yuning Mao", "Shaoliang Nie", "Jiahao Liu", "Fuli Feng", "Zenglin Xu", "Cheng Han", "Lifu Huang", "Qifan Wang", "Dongfang Liu"], "abstract": "Multimodal Large Language Models (MLLMs) demonstrate remarkable performance across a wide range of domains, with increasing emphasis on enhancing their zero-shot generalization capabilities for unseen tasks across various modalities. Instruction tuning has emerged as an effective strategy for achieving zero-shot generalization by finetuning pretrained models on diverse multimodal tasks. As the scale of MLLMs continues to grow, parameter-efficient finetuning becomes increasingly critical. However, most existing parameter-efficient approaches focus only on single modalities and often overlook the multimodal characteristics during finetuning. In this work, we introduce a novel Multimodal Prompt Tuning (MMPT) approach for efficient instruction tuning of MLLMs. MMPT effectively integrates visual and textual prompts into the vision encoder and language processor respectively during finetuning, facilitating the extraction and alignment of features across modalities. Empirical results on various multimodal evaluation datasets demonstrate the superior performance of our approach compared to several state-of-the-art baselines. A comprehensive set of ablation studies validates the effectiveness of our prompt design and the efficiency of our approach.", "sections": [{"title": "1 Introduction", "content": "Human cognition intricately integrates various sensory modalities to perceive, interpret, and engage with the environment, fostering a comprehensive understanding of the surrounding world (Liu et al., 2023c; Dumas et al., 2009; Chen et al., 2024; Jin et al., 2024a,c). The development of Multimodal Large Language Models (MLLMs) (Alayrac et al., 2022; Yin et al., 2023; Han et al., 2024a,c; Jin et al., 2024b,d) marks a significant advancement in emulating this capability, playing a pivotal role in bridging the gap between human and machine intelligence. A key focus in advancing MLLMs is enhancing their zero-shot generalization to new multimodal tasks. In this pursuit, multimodal instruction tuning (Liu et al., 2023c,b; Xu et al., 2023), which finetunes pretrained models using diverse and instruction-based multimodal tasks, has proven effective in improving zero-shot generalization to unseen multimodal domains.\nDespite considerable advancements, finetuning MLLMs for specific domain knowledge poses significant challenges. As the scale and complexity of these models increase, the training overhead for downstream tasks grows exponentially (Wu et al., 2024a; Xu et al., 2024a; Zhang et al., 2024). These escalating demands render the current tuning schemes for MLLMs obsolete and unsustainable, impeding their widespread utility. A promising solution is the utilization of parameter-efficient finetuning (PEFT) approaches (Lester et al., 2021; Hu et al., 2022; Chowdhury et al., 2023; Dong et al., 2023b; Wang et al., 2023a), which have been widely applied and achieved notable success in natural language processing (Liu et al., 2021; Wang et al., 2022) and computer vision tasks (Jia et al., 2022; Han et al., 2023; Ju et al., 2022a). However, most existing PEFT approaches only focus on single modality tuning while overlooking multimodal instructing learning. A primary challenge is the intricate process of finetuning data of multiple modalities within a cohesive model, extracting and aligning feature representations across modalities. Additionally, there is a pressing need to enhance the zero-shot generalization capabilities for unseen multimodal tasks while minimizing training costs.\nIn light of this, we present a novel Multimodal Prompt Tuning (MMPT) approach with efficient and effective MLLM adaptation for zero-shot instruction learning. MMPT demonstrates competitive performance across a wide spectrum of tasks while tuning 0.09% of overall parameters. Specifically, we introduce two sets of soft prompts: visual prompts and textual prompts, which are prepended to the visual and instruction inputs, respectively. The learned embeddings of the visual prompts are projected into the embedding space of the textual prompts. The cross-modality interactions between the two sets of prompts are enforced during instruction tuning, facilitating the alignment and learning of the feature representation between the two modalities. In this way, MMPT provides explicit guidance and clear directives through instruction tuning, enabling models to understand context and reduce ambiguity in zero-shot settings.\nTo effectively assess our method, we conduct comprehensive experiments to evaluate its performance. In \u00a74.2, we demonstrate that MMPT surpasses several state-of-the-art PEFT techniques while tuning only 0.09% of the trainable parameters. We further conduct activation analysis to show the effectiveness of the learned prompts during the attention computation. In \u00a75, we provide various ablation analysis on the impact of model components, prompt length, prompt location, and data volumes in detail. Furthermore, we conduct case studies to better understand the advantage of MMPT and its limitations. We hope this work offers valuable insights into related fields. We summarize the main contributions as follows:\n\u2022 We present multimodal prompt tuning by introducing both visual and textual prompts into vision encoder and language processor respectively. These modality specific prompts play a crucial role in effectively guiding the model's fine-tuning process, enabling fast and accurate multimodal model adaptation.\n\u2022 We design the cross-modality interaction between the prompts from different modalities during the instruction tuning. By doing so, MMPT leverages the strengths of each modality, resulting in comprehensive and coherent learning results. This synergy empowers the model to perform complex tasks that require the integration from multimodal perspectives.\n\u2022 We conduct comprehensive experiments on various multimodal tasks in the zero-shot setting, demonstrating the effectiveness of the proposed approach over several state-of-the-art parameter efficient finetuning methods."}, {"title": "2 Related Work", "content": "Multimodal Large Language Models. MLLMs (Dai et al., 2023; Driess et al., 2023; Liu et al., 2023c; Yao et al., 2023; Sun et al., 2024a) integrate multimodal information (e.g., audio, image, video), extending beyond the textual semantic information processed by conventional Large Language Models (LLMs). A general structure of MLLMs includes three main components (Li et al., 2024a): a pre-trained modality encoder to encode multimodality data, a pre-trained LLM to reason encoded multimodal data and perform generation tasks, and a connection layer to project modality information into tokens. During the standard full finetuning process (Liu et al., 2023c), a substantial amount of weights within all intermediate layers and the pretrained LLM are continuously updated. Conceptually different, our approach can elegantly fine-tune the model with adjusting a minimum of weights.\nInstruction Tuning. To enhance the zero-shot and in-context learning (Brown et al., 2020; Li et al., 2024b) capabilities of large language models (LLMs) (Zhao et al., 2023), researchers have explored instruction tuning (Ouyang et al., 2022; Zhang et al., 2023), a technique that enables pre-trained LLMs to be more adaptable for intricate multimodal tasks. Specifically, instruction-tuning is a process that refines LLMs by finetuning them on meticulously curated instruction-following datasets encapsulating user intent and desired outputs (Ouyang et al., 2022). With the rapid advance-"}, {"title": "3 Methodology", "content": "3.1 Preliminaries\nMultimodal Large Language Models integrate visual and language processing capabilities, leveraging the power of LLMs to enhance the comprehension of multimodal information. A prevalent workflow of MLLMs begins with the utilization of pre-trained vision encoders \\(f_v\\) (e.g., LLaVA (Liu et al., 2023c) and its variants (Li et al., 2022; Sun et al., 2023)), encoding visual input \\(X_{im}\\) and extracting output \\(O_v = f_v(X_{im})\\) through remarkable vision understanding ability. Subsequently, the vision output is further projected into language space, aligning with the textual embedding and enabling the model to understand and respond to instructions effectively. Ultimately, the integrated LLM \\(f_{llm}\\) assimilates \\(O_v\\) and text embedding \\(O_t\\). Harnessing the extensive knowledge of LLM, integrating multimodal inputs to generate coherent and contextually relevant language response \\(y\\), represented as:\n\\[y = f_{llm}(O_v, O_t). \\]\nPrompt Tuning is a form of PEFT approach, demonstrating exceptional efficacy within single-modality settings under both visual (Han et al., 2023) and textual (Wang et al., 2023a) domains. It entails learnable continuous soft prompts into the input space while concurrently preserving the majority of parameters within the backbone frozen. Specifically, given a \\(K\\) layer transformer-based model \\(f\\), soft prompts \\(P^k\\) combined with the input of \\(k\\)-th layer to obtain the output \\(O^k\\) as:\n\\[\\begin{aligned}O^1 &= f^1(P^1, E) \\\\O^k &= f^k(P^k, O^{k-1}),\\end{aligned} \\]\nwhere \\(k \\in \\{2, 3, ..., K \\}\\). \\(\u2022\\) and \\(\u2022\\) indicate frozen and tunable parameter during finetuning, respectively. \\(E\\) is the embedded vector of initial inputs.\n3.2 Multimodal Prompt Tuning\nIn this section, we formally introduce MMPT, a novel multimodal prompt tuning approach for the effective and efficient finetuning of MLLMs. The overall model architecture is depicted in Figure 2. Fundamentally, our model necessitates the training of only three targeted components, while keeping the backbone parameters from both visual encoder and LLM frozen. Specifically, these components include \u2460 Visual Prompt (\\(P^v\\)), which is the learnable parameter (i.e., soft prompt) integrated into"}, {"title": "4 Experiments", "content": "4.1 Experiment Setup\nDatasets. For training, we conduct instruction tuning on Vision-Flan (Xu et al., 2024b), which is a human-annotated multimodal instruction tuning dataset with 191 diverse tasks. To reduce computational costs, we follow common practice (Shen et al., 2024) and employ a scaled-down version containing up to 1,000 instances per task, resulting in a total of 191, 105 instances. For zero-shot evaluation, we examine our approach on the comprehensive multimodal evaluation benchmark MME (Yin et al., 2023), measuring both perception and cognition abilities across 14 subtasks. We further evaluate the model's capabilities using 7 multimodal datasets. Specifically, for Optical Character Recognition, we utilize the Text-VQA (Singh et al., 2019), and for reasoning, we employ the Visual Spatial Reasoning (VSR) (Liu et al., 2023a). Following (Zhai et al., 2023; Shen et al., 2024), the perception capability is tested on CIFAR-10/100 (Krizhevsky et al., 2009) and MNIST (Deng, 2012). SNLI-VE (Xie et al., 2019) evaluates Visual Entailment capabilities, while the POPE (Li et al., 2023) dataset examines the tendency towards object hallucination. More details are provided in the Appendix S1.\nTraining Details. Following previous works (Han et al., 2023; Shen et al., 2024; Jia et al., 2022), we conduct grid search to match the best tuning hyperparameters, learning rate (i.e., [1e-3, 9e-4, 7e-4, 4e-4, 1e-4, 5e-5]), textual prompt length (i.e., [0, 5, 10, 20, 40]) and visual prompt length (i.e., [0, 5, 10, 20, 40]). For all models, the learning rate is scheduled following a cosine decay policy, the warm up ratio is set at 0.03 and trained for 3 epochs except in training epoch experiment. We follow the same batch size setting in (Shen et al., 2024) as 128 for instruction tuning. Further de-"}, {"title": "5 Analysis and Discussion", "content": "Attention Activation Pattern Analysis. Following common practice (Sun et al., 2024b), we extract and discuss the activation maps from the attention block of MLLMs, and investigate the influence of visual and textual prompts in Fig. 3. We randomly select two samples from the MME dataset and visualize their corresponding activation maps of the attention block in the last layers of both visual encoder (Fig. 3(a)) and LLM (Fig. 3(b)).\nTo analyze the impact of textual and visual prompts to the frozen components, we categorize them, according to LLaVa's model structure, into textual prompts, system text tokens, visual prompts, image tokens and instructions. Several findings can be observed. First, in the LLM attention activation map, we observe that the token regions corresponding to textual prompts exhibit elevated activation levels, indicating their significant role in shaping the model's responses. The activation levels of visual prompts within the LLM, while comparatively lower, remain notable relative to most other regions."}, {"title": "6 Conclusion", "content": "We introduce MMPT, a novel framework in multimodal prompt tuning for zero-shot instruction learning. Our framework offers several advantages: i) it introduces visual and textual prompts elegantly into vision encoder and LLM, respectively, enabling fast and accurate multimodal adaptation; ii) it synergizes modalities by cross-modality interaction, enjoying coherent integration from multimodal perspectives; and iii) it significantly reduces the number of trainable parameters compared to conventional finetuning methods, while maintaining robust performance across zero-shot tasks. As a whole, we conclude that the outcomes elucidated in this paper impart essential understandings and necessitate further exploration within this realm."}, {"title": "Limitations", "content": "For potential limitations, MMPT requires two hyperparameters on prompt length searching (i.e., Visual Prompt, Textual Prompt). Though in practice, we find both lengths vary into a relatively narrow range (see \u00a75), and are sufficient enough to outperform all current methods, there is still possible integration (He et al., 2022b) of a local searching network to generate optimal combinations of lengths. Another potential limitation is that MMPT, akin to other PEFT approaches (Han et al., 2023; Jia et al., 2022), lacks ad-hoc explainability (Biehl et al., 2016; Wang et al., 2023b). While in \u00a74, we demonstrates that activation maps from MLLMs are significantly influenced by visual and textual prompts, further research is necessary to elucidate the underlying nature of these prompts."}]}