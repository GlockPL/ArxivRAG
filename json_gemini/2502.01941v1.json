{"title": "Can LLMs Maintain Fundamental Abilities under KV Cache Compression?", "authors": ["Xiang Liu", "Zhenheng Tang", "Hong Chen", "Peijie Dong", "Zeyu Li", "Xiuze Zhou", "Bo Li", "Xuming Hu", "Xiaowen Chu"], "abstract": "This paper investigates an under-explored challenge in large language models (LLMs): the impact of KV cache compression methods on LLMs' fundamental capabilities. While existing methods achieve impressive compression ratios on long-context benchmarks, their effects on core model capabilities remain understudied. We present a comprehensive empirical study evaluating prominent KV cache compression methods across diverse tasks, spanning world knowledge, common-sense reasoning, arithmetic reasoning, code generation, safety, and long-context understanding and generation.Our analysis reveals that KV cache compression methods exhibit task-specific performance degradation. Arithmetic reasoning tasks prove particularly sensitive to aggressive compression, with different methods showing performance drops of 17.4%-43.3%. Notably, the DeepSeek R1 Distill model exhibits more robust compression tolerance compared to instruction-tuned models, showing only 9.67%-25.53% performance degradation. Based on our analysis of attention patterns and cross-task compression performance, we propose ShotKV, a novel compression approach that distinctly handles prefill and decoding phases while maintaining shot-level semantic coherence. Empirical results show that ShotKV achieves 9%-18% performance improvements on long-context generation tasks under aggressive compression ratios.", "sections": [{"title": "1. Introduction", "content": "The emergence of Key-Value (KV) cache compression techniques has become crucial for efficient LLM deployment,\nprimarily due to the increasing demands of memory management during inference. This need arises from the significant evolution of Large Language Models (LLMs), which now excel at processing extensive documents spanning thousands of tokens for tasks like question answering and summarization (Raffel et al., 2020; Brown et al., 2020; Chowdhery et al., 2022; Tay et al., 2022; Touvron et al., 2023a;b). The ability to handle such lengthy inputs has been transformed by two concurrent developments: breakthroughs in ML system architectures for processing long sequences (Dao et al., 2022; Dao, 2024; Jacobs et al., 2023; Xiao et al., 2024), and innovations in model design (Chen et al., 2023a; Xiong et al., 2024; Chen et al., 2023b; Peng et al., 2024).\nHowever, this enhanced capability comes with significant computational costs; as context lengths grow, the GPU memory requirements for inference operations increase substantially (AI21, 2024; X.AI, 2024; Reid et al., 2024; Anthropic, 2024; DeepSeek-AI, 2024; Liu et al., 2024a). This challenge has made the development of efficient KV cache compression strategies a critical focus in the field of LLM deployment and optimization.\nMultiple studies have proposed innovative approaches to address this challenge through selective token retention strategies (Xiao et al., 2024; Zhang et al., 2023; Li et al., 2024b; Ge et al., 2023; Cai et al., 2024; Fu et al., 2024; Yang et al., 2024; Adnan et al., 2024; Liu et al., 2024e; Tang et al., 2024; Liu et al., 2025). Pioneering works such as H2O (Zhang et al., 2023) and SnapKV (Li et al., 2024b) demonstrate that retaining approximately 50% of KV cache entries can maintain model performance while achieving substantial memory efficiency. However, current research primarily evaluates these methods on long-context scenarios like LongBench (Bai et al., 2023; 2025) and Need-In-A-Haystack (NIAH) (Kamradt, 2023), leaving their broader impact on fundamental LLM capabilities largely unexplored.\nOur preliminary analysis, as shown in Figure 1, reveals two critical findings: (a) arithmetic reasoning tasks suffer significantly higher performance degradation under compression compared to long-context tasks, and (b) attention patterns in long-context scenarios exhibit notably higher sparsity than arithmetic tasks. These observations suggest that current evaluation frameworks, which focus predominantly on long-context performance, may inadequately capture the full spectrum of impacts on model capabilities. Further analysis of attention patterns, visualized in Figure 2, reveals distinct task-specific behaviors: while world knowledge and commonsense reasoning tasks exhibit universal attention distributions, arithmetic reasoning and safety tasks demonstrate more specialized patterns. Specifically, arithmetic reasoning tasks display increased attention sparsity, indicating focused attention on individual prompt examples, while safety tasks show concentrated attention on the system prompt. In contrast, world knowledge and commonsense reasoning tasks demonstrate more uniform attention distribution across the entire prompt. These varying attention patterns\u2014visualized through colored squares representing system prompts, shot examples, and questions\u2014provide insights into task-specific context utilization and motivate our investigation into how compression affects various factors including model size, prompt length, and task type.\nMotivated by these initial findings, we conduct a systematic study to comprehensively evaluate KV cache compression methods across different tasks (detailed in Section 4). Our analysis reveals several key findings: (1) Performance degradation is highly task-dependent, with arithmetic reasoning tasks showing particular sensitivity to aggressive compression; (2) Multi-step reasoning LLMs (O1 and R1)"}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Key-value Cache Optimization Techniques", "content": "KV cache is the core component in LLM inference, which avoids repetitive computations by caching Key and Value vectors. However, the cost of caching KV increases exponentially with the expansions of the model size and context length (Pope et al., 2023). Some approaches have been published to alleviate the issue. For instance, KV Compression designs efficient content selection strategies to filter and manage tokens (Zhang et al., 2023; Adnan et al., 2024). Some methods identify important tokens by focusing on high attention allocation (Li et al., 2024b), while others optimize token selection by combining attention scores with value vector norms to improve importance evaluation (Guo et al., 2024). Techniques like PyramidInfer reduce critical contexts layer-by-layer based on the distribution of attention scores (Yang et al., 2024), and StreamingLLM preserves attention sinks to maintain stable performance in extended sequences (Xiao et al., 2024). Researchers reduce storage costs by merging similar context representations and solving input disturbances caused by compression (Zhang et al.). For instance, CaM (Zhang et al.) works by integrating the KV cache to be dropped into the retained cache in proportion to the attention weight. In addition, Yao et al. (2024) proposes CacheBlend to achieve selective KV recompute. Only partial KVs of crucial tokens are updated to reduce the delay in the prefill stage and increase the throughput. Besides, the dynamic budget allocation method is also used to optimize the KV Cache, which adjusts the resource allocation in real-time according to the importance of the context, providing a balance between performance and efficiency in multi-task inference scenarios (Cai et al., 2024; Feng et al., 2024; Liu et al., 2025)."}, {"title": "2.2. Evaluation of LLMs' Fundamental Abilities", "content": "Accurately evaluating the fundamental capabilities of large language models is crucial for understanding their true potential and limitations. The evaluation typically spans across several key dimensions: world knowledge tasks like MMLU (Hendrycks et al., 2020), BBH (Suzgun et al., 2022) assess models' grasp of diverse domains through multiple-choice questions; commonsense reasoning tasks such as CSQA (Talmor et al., 2019) evaluate inference and context understanding abilities; arithmetic reasoning benchmarks like GSM8K (Cobbe et al., 2021) test mathematical problem-solving capabilities through step-by-step reasoning; code generation tasks including HumanEval (Chen et al., 2021) measure the ability to generate functionally correct code; and safety evaluations using benchmarks like JailBreakV (Luo et al., 2024) assess models' robustness against harmful content generation. Additionally, long-context benchmarks such as LongBench (Bai et al., 2023; 2025) and Need-In-A-Haystack (NIAH) (Kamradt, 2023) aiming to evaluate models' long-context summarization and retrieval capabilities. Furthermore, LongGenBench (Liu et al., 2024d) evaluate models' abilities to process and generate responses for extended input sequences. And recently, many-shot in-context learning has been recognized as a long-context reasoning paradigm (Agarwal et al., 2024), which considers the number of shots as a critical factor in the performance of LLMs. While these tasks typically employ automatic evaluation metrics for standardization, KV cache compression may introduce unique challenges, particularly in tasks requiring complex reasoning chains or extensive knowledge retrieval."}, {"title": "3. Preliminary", "content": "In this section, we provide comprehensive preliminaries of KV cache compression and LLM evaluation.\nKey-Value Cache in LLMs With the increasing long-context capabilities of LLMs, the Key-Value (KV) cache has become crucial for improving inference efficiency. During LLM inference, the KV cache stores intermediate computation results to avoid redundant calculations. For a given input sequence $x = (x_1, x_2, ..., x_n)$, each transformer layer $l$ maintains its key cache $K^l = (k_1, k_2, ..., k_n)$ and value cache $V^l = (v_1, v_2, ..., v_n)$, where $k_i, v_i \\in R^d$ represent the key and value vectors for token $x_i$ at layer $l$.\nKV Cache Compression KV cache compression aims to reduce memory usage by selectively storing or merging cached vectors. A compression operation can be denoted as $C(K,V) = (K', V')$, where $K'$ and $V'$ are compressed caches with size $m < n$, where $C$ is the compression method, $m$ is the number of retained tokens, and $n$ is the original number of tokens. The core idea is token selection - identifying and retaining important tokens based on"}, {"title": "4. Experiments Design", "content": ""}, {"title": "4.1. Experimental Setups", "content": "In this section, we will introduce the experimental setups, including the datasets, models, and evaluation environment.\nDatasets To evaluate the performance of KV cache compression on LLMs' overarching capabilities, we assess five benchmark categories: World Knowledge using MMLU (Hendrycks et al., 2020), measured by accuracy; CommonSense Reasoning using CommonsenseQA (Talmor et al., 2019), evaluated through multiple-choice accuracy; Arithmetic Reasoning using GSM8K (Cobbe et al., 2021), assessed by solve rate; Code Generation using HumanEval (Chen et al., 2021), measured by pass@1 rate on test cases; and Safety using JailBreakV (Luo et al., 2024), evaluated by attack success rate. Furthermore, we test the performance of KV cache compression on LongGenBench (Liu et al., 2024d), a long-context generation benchmark.\nModels We conduct experiments on a series of LLMs, including LLaMA-3.1-8B, LLaMA-3.1-8B-Instruct (Dubey et al., 2024), and multi-step reasoning LLM DeepSeek-R1-Distill-Llama-8B (Guo et al., 2025).\nKV Cache Compression Methods To comprehensively investigate the potential impact on KV cache compression methods, we select the following methods: StreamingLLM (Xiao et al., 2024), SnapKV (Li et al., 2024b), H2O (Zhang et al., 2023), PyramidKV (Cai et al., 2024), PyramidInfer (Yang et al., 2024), and ChunkKV (Liu et al., 2025).\nHyperparameters For the experiments on observation 1, 2 we set the normal shot number as in the original paper. For the experiments on observation 3, 4, 5, the shot number are dependent on the experiment settings. More details are shown in Appendix B.1."}, {"title": "4.2. Results and Analysis", "content": "In this section, we will present the results and analysis of the experiments. For comprehensive results, please refer to Appendix B.2.\nObservation 1. KV cache compression methods show task-dependent performance degradation, with varying sensitivity thresholds across different benchmark categories. As demonstrated in Figure 3, all tasks maintain stable performance at compression ratios above 40%, but exhibit distinct degradation patterns below this threshold. GSM8K, HumanEval, and JailBreakV tasks demonstrate the highest compression sensitivity, characterized by precipitous performance declines. Figure 4 illustrates the detailed performance impact of various KV cache compression"}, {"title": "5. ShotKV", "content": "Based on our comprehensive analysis, we find that current unified KV cache compression methods exhibit significant performance degradation on Long-Context Generation tasks. In this section, we introduce ShotKV, a decoding-time compression method designed to mitigate this degradation. Our approach is motivated by two key insights: (1) Figure 2 demonstrates that n-shot example prompts receive substantial attention in reasoning benchmarks, and (2) Observation 4 reveals that chunk-level compression is particularly effective for long-context arithmetic reasoning tasks. Based on these findings, we hypothesize that each shot example represents a coherent chunk of information, leading us to design ShotKV to preserve shot examples intact during decoding time."}, {"title": "5.1. Implementation", "content": "The ShotKV (Prefill-Decoding Separated Shot-aware KV Cache Compression), which separates the compression strategy for prefill and decoding phases. The key insight is that the prefill phase KV cache, which contains crucial prompt information, should be compressed once and remain fixed, while the decoding phase KV cache can be dynamically compressed with different strategies.\nGiven a prompt with n shots and generated tokens, we define:\n$KV_{total} = KV_{prefill} \\cup KV_{decoding}$\nFor the prefill phase, we compute shot importance and preserve complete shot examples:\n$Score_{prefill}(S_i) = \\frac{1}{k_i} \\sum_{t \\in S_i} \\sum_{h=1}^{H} \\sum_{l=1}^{L} \\alpha_{t,h}^l$\n$KV_{prefill} = Compress({s_i|S_i \\in S_{preserved}})$ \nwhere $S_{preserved} = argmax_{S \\subset {$1,...,n$}} \\sum_{s_i \\in S} Score_{prefill}(S_i)$ \nsubject to: $\\sum_{S_i \\in S} k_i \\leq r_p \\times |KV_{prefill}|$ \nHere, $KV^C_{prefill}$ represents the compressed prefill KV cache, and $S_{preserved}$ represents the optimal subset of shots to be preserved after compression. The first equation aims to maximize the total importance score of selected shots, where {$1,..., $n} represents all available shots and $Score_{prefill}(S_i)$ is the importance score of shot $s_i$ computed using attention weights as defined earlier. The second equation enforces the memory constraint: the total number of tokens ($k_i$) in the selected shots must not exceed the allocated budget, which is determined by the prefill compression ratio $r_p$ multiplied by the original KV cache size.\nFor the decoding phase, we compute importance scores only for the tokens generated during decoding:\n$Score_{decoding}(t) = \\sum_{h=1}^{H} \\sum_{l=1}^{L} \\alpha_{t,h}^l$\nGiven a decoding compression ratio $r_d$, we select tokens with the highest scores to preserve. The compressed decoding KV cache $KV^C_{decoding}$ retains only the top-k tokens where k = $r_d \\times |KV_{decoding}|$, effectively maintaining the most influential context tokens while reducing memory usage:\n$KV^C_{decoding} = TopK(KV_{decoding}, Score_{decoding}, k = r_d \\times |KV_{decoding}|)$\nFinally, we combine the compressed prefill and decoding KV caches to form the total compressed KV cache:\n$KV_{total} = KV^C_{prefill} \\cup KV^C_{decoding}$"}, {"title": "5.2. Empirical Results", "content": "In this section, we evaluate ShotKV under two scenarios: many-shot GSM8K with multiple KV cache compression methods, and LongGenBench-GSM8K with three unified compression methods that optimize the KV cache during generation.\nBaseline. For LongGenBench-GSM8K evaluation, we employ three state-of-the-art unified compression methods as baselines: StreamingLLM (Xiao et al., 2024), H2O (Zhang et al., 2023), and PyramidInfer (Yang et al., 2024). We conduct experiments using LLaMA-3-8B-Instruct (Dubey et al., 2024) on the LongGenBench-GSM8K benchmark (Liu et al., 2024d), maintaining consistent parameters with Observation 6 (K = 35, T = 20). For many-shot GSM8K experiments, we follow the configuration detailed in Observation 4.\nMain results and analysis. From the Table 3, we can see that ShotKV achieves the best performance on LongGenBench-GSM8K, maintaining high performance at low compression ratios. Specifically, at a compression ratio of 40%, ShotKV achieves 47.33% accuracy, surpassing the full kv cache baseline (46.00%) and showing substantial improvements over other methods (32.66%-39.50%). And Table 2 shows that ShotKV also achieves the best performance on many-shot GSM8K, maintaining high performance at low compression ratios. Even at aggressive compression ratios (25%-30%), ShotKV maintains relatively stable performance (26.83%-38.33%), while other methods experience more severe degradation (6.33%-16.67%). This superior performance can be attributed to two key design choices: (1) the preservation of complete shot examples during compression maintains the semantic coherence necessary for mathematical reasoning, and (2) the separation of prefill and decoding phase compression allows for more flexible and task-appropriate token retention strategies. These results suggest that our shot-aware compression strategy is particularly effective for long-context generation tasks that require maintaining complex reasoning chains, such as mathematical problem-solving."}, {"title": "6. Further Discussion", "content": "Our comprehensive analysis of KV cache compression reveals several important implications and limitations that warrant further discussion:\nTrade-off between Memory Efficiency and Task Performance While KV cache compression methods effectively reduce memory usage, our findings highlight a non-uniform impact across different tasks. This suggests that deployment strategies should carefully consider task-specific requirements when selecting compression ratios, potentially implementing adaptive compression based on the task type."}, {"title": "7. Conclusion", "content": "This paper presents a systematic study of KV cache compression's impact on LLMs' core capabilities, revealing several key findings: (1) Performance degradation is highly task-dependent, with arithmetic reasoning tasks showing particular sensitivity to aggressive compression; (2) Instruct-tuned models demonstrate higher sensitivity to compression compared to their base counterparts; (3) Shorter prompts are more vulnerable to compression effects; (4) Chunk-level compression strategies show superior performance on complex long-context reasoning tasks; (5) Long-context generation tasks are more sensitive to compression than long-context reasoning tasks.\nBased on these insights, we proposed ShotKV, a novel compression method that separately handles prefill and decoding phases while preserving shot-level semantic coherence. Our method demonstrates superior performance on long-context arithmetic reasoning tasks and long-context generation tasks, maintaining high accuracy even at low compression ratios.\nThese findings have important implications for the deployment of LLMs in resource-constrained environments and suggest several promising directions for future research, including: (1) Development of task-adaptive compression strategies; (2) Investigation of compression-aware training methods; and (3) Extension of compression techniques to other model architectures and modalities."}, {"title": "Impact Statement", "content": "This work advances the field of efficient large language model deployment through systematic analysis and improvement of KV cache compression techniques. Our research has several potential societal impacts:\nFirst, by enabling more efficient memory usage in LLMs while maintaining performance, our work contributes to reducing the computational resources and energy consumption required for AI deployment. This has positive environmental implications and makes AI technology more accessible to researchers and organizations with limited computing resources.\nSecond, our proposed ShotKV method specifically improves performance on long-context arithmetic reasoning tasks, which could enhance the practical applications of LLMs in education, scientific computing, and other fields requiring complex mathematical reasoning. This could lead to more reliable AI-assisted learning and problem-solving tools.\nHowever, we acknowledge that making LLMs more efficient could accelerate their widespread adoption, potentially raising concerns about AI's impact on employment and privacy. While our work focuses on technical improvements, we encourage the research community to carefully consider these broader implications when deploying such technologies.\nWe believe the benefits of more efficient and capable AI systems outweigh potential risks, particularly as our work promotes more sustainable and accessible AI development. Nevertheless, we emphasize the importance of responsible deployment and continued ethical consideration in the application of these technologies."}, {"title": "A. Additional Related work", "content": "KV cache sharing Recent work has explored various strategies for sharing KV caches across transformer layers. Layer-Condensed KV Cache (LCKV) (Wu & Tu, 2024) computes KVs only for the top layer and pairs them with queries from all layers, while optionally retaining standard attention for a few top and bottom layers to mitigate performance degradation. Similarly, You Only Cache Once (YOCO) (Sun et al., 2024) computes KVs exclusively for the top layer but pairs them with queries from only the top half of layers, employing efficient attention in the bottom layers to maintain a constant cache size. In contrast, Cross-Layer Attention (CLA) (Brandon et al., 2024) divides layers into groups, pairing queries from all layers in each group with KVs from that group's bottom layer. MiniCache (Liu et al., 2024b) introduces a novel method that merges layer-wise KV caches while enabling recovery during compute-in-place operations, optimizing KV cache size. These methods illustrate various trade-offs between computation, memory usage, and model performance when sharing KV caches across transformer layers.\nPrompting Compression Recent advances in prompt compression have yielded innovative approaches to information density optimization in natural language processing. Research by Wingate et al. (2022) demonstrates how soft prompting techniques can achieve higher information density per token. Building upon this foundation, AutoCompressor (Chevalier et al., 2023) leverages soft prompts to both condense input sequences and expand model context windows. Parallel developments by Zhou et al. (2023) and Wang et al. (2023) showcase iterative summarization strategies using LLMs, establishing persistent memory mechanisms particularly beneficial for narrative construction and conversational systems. The progressive development of the LLMLingua framework (Jiang et al., 2023; 2024; Fei et al., 2024) has advanced prompt compression capabilities across extended context processing, logical reasoning, and retrieval-augmented generation. Notable contributions from Fei et al. (2024) demonstrate effective context management through automated segmentation and semantic condensation using pre-trained language models.\nGeneral Tasks General tasks refer to evaluating the overall performance of LLMs under mathematical inference, logic reasoning, and common knowledge GSM8K (Cobbe et al., 2021) and MMLU (Hendrycks et al., 2020) are the representative tasks. The former focuses on the step-by-step reasoning ability of mathematical problem solving while the latter covers assessment of common sense and expertise in multiple areas. Besides, MATH (Hendrycks et al., 2021) spans various mathematical fields, ranging from elementary algebra to calculus, aiming to improve the mathematical problem-solving capabilities of LLMs. Meanwhile, MathQA (Amini et al., 2019) is a large-scale dataset comprising approximately 37,000 multiple-choice questions with precise annotations, designed to enhance the interpretability and performance of LLMs. In addition, BBH (Suzgun et al., 2022), a subset of BIG-Bench (Srivastava et al., 2022), focuses on challenging tasks. BBH includes multi-step reasoning problems, highlighting the importance of Chain-of-Thought prompting in LLMs. Similarly, CSQA (Talmor et al., 2019) is a task that combines knowledge graph-based multi-step reasoning with conversational capabilities. CSQA emphasizes inference and context understanding grounded in knowledge graphs. Normally, the general tasks apply automatic evaluation metrics (e.g. multi-choice accuracy) to ensure comparability and standardization. However, optimization strategies like KV cache compression may introduce challenges in executing the mentioned tasks. Filtering and dropping of contexts are involved in the compression strategy which may lead to an intermediate inference steps missing. In addition, in tasks such as MMLU that are highly dependent on knowledge coverage, compression may weaken the model's ability to capture long context or rare domain knowledge (Yuan et al., 2024).\nSecurity Tasks Security tasks focus on assessing the robustness and protections of LLMs against harmful content, including truthfulness (Lin et al., 2021), toxicity (Hartvigsen et al., 2022), and bias (Liang et al., 2021). Recently, researchers noticed the weakness of LLMs in adversarial prompts (Zhu et al., 2023), especially in generating illegal or inappropriate content under jailbreak prompts. Shen et al. (2024) analyze the jailbreak prompts in real cases to reveal the failure of model security mechanism under complex malicious input. Meanwhile, Deng et al. (2023) demonstrates the multilingual jailbreak makes model security in low-resource languages easier to bypass, significantly increasing the probability that users of low-resource languages will generate insecure content. Similar to general tasks, KV optimization techniques can cause the model to ignore potential security threats when dealing with jailbreak prompts, thereby improving the success rate of adversarial prompts (Li et al., 2024a).\nCode Generation Tasks Code generation tasks test the capacities of LLMs to generate code, which not only requires that the model can generate syntactic code based on natural language description but also has certain logical reasoning abilities. HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) are the commonly used benchmarks. They measure the"}]}