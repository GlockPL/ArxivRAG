{"title": "UNPICKING DATA AT THE SEAMS: VAES, DISENTANGLEMENT AND INDEPENDENT COMPONENTS", "authors": ["Carl S. Allen"], "abstract": "Disentanglement, or identifying salient statistically independent factors of the data, is of interest in many areas of machine learning and statistics, with relevance to synthetic data generation with controlled properties, robust classification of features, parsimonious encoding, and a greater understanding of the generative process underlying the data. Disentanglement arises in several generative paradigms, including Variational Autoencoders (VAEs), Generative Adversarial Networks and diffusion models. Particular progress has recently been made in understanding disentanglement in VAEs, where the choice of diagonal posterior covariance matrices is shown to promote mutual orthogonality between columns of the decoder's Jacobian. We continue this thread to show how this linear independence translates to statistical independence, completing the chain in understanding how the VAE's objective identifies independent components of, or disentangles, the data.", "sections": [{"title": "INTRODUCTION", "content": "Variational Autoencoders (VAEs, Kingma (2013); Rezende et al. (2014)) and a range of variants, e.g. B-VAE (e.g. Higgins et al., 2017) and Factor-VAE (Kim & Mnih, 2018), have been shown empirically to disentangle latent factors of variation in the data. For example, a trained VAE may generate face images that vary in distinct semantically meaningful ways, such as hair colour or facial expression, as individual latent variables are adjusted. This is both of practical use, e.g. for controlled generation of synthetic data with chosen properties, and intriguing as it is not knowingly designed into the training algorithm. A related phenomenon is observed in samples from a Generative Adversarial Network (GAN), which, in common with a VAE, applies a deterministic neural network function to samples of independently distributed latent variables, producing a push-forward distribution.\nUnderstanding why disentanglement arises, seemingly \u201cfor free\u201d, is of interest since identifying and separating generative factors underlying the data goes to the heart of many aspects of machine learning, from classification to generation, interpretability to identifiability, right down to a fundamental understanding of the data itself. With a better appreciation of why disentanglement happens, we might be able to induce it more reliably, particularly in domains where we cannot easily perceive when features are disentangled, as we can for images and text.\nResearch into the cause of disentanglement has gradually led to a refined understanding of what is meant by \"disentanglement\", which typically refers to the separation of semantically meaningful generative factors (Bengio et al., 2013). Recent progress has been made towards understanding why disentanglement occurs in VAEs, tracing the root cause to the common use of diagonal posterior covariance matrices, a seemingly innocuous design choice made for computational efficiency (Rolinek et al., 2019; Kumar & Poole, 2020). Diagonal covariances are shown to promote orthogonality between columns of the Jacobian of the decoder, a property linked to disentangled features (Ramesh et al., 2018) and independent causal mechanisms (Gresele et al., 2021). We extend this line of work by providing theoretical analysis to formally show how orthogonality in the Jacobian translates to disentanglement in the push-forward distribution of a VAE, connecting linear independence of partial derivatives to statistical independence of components, or generative factors, of the data.\nInterest in understanding how disentanglement arises in VAEs has increased as their generative quality has improved (e.g. Hazami et al., 2022) and they often a key component in state of the art diffusion"}, {"title": "BACKGROUND", "content": "Disentanglement: Disentanglement is not consistently defined in the literature, but typically refers to identifying salient, semantically meaningful features of the data with distinct latent variables, such that by varying a single variable, data can be generated that differ in a single aspect (Bengio et al., 2013; Higgins et al., 2017; Ramesh et al., 2018; Rolinek et al., 2019). Disentanglement has also been decomposed into necessary and sufficient-type concepts of consistency and restrictiveness (Shu et al., 2019). We show that disentanglement in a VAE relates to identifying statistically independent components of the data, comparable to independent component analysis (ICA).\nVariational Autoencoder (VAE): A VAE is a latent generative model for data x \u2208 X = Rm, that models the data distribution by po(x)= \\int_zpe(x|z)p(z) with parameters @ and latent variables z\u2208 Z=Rd. A VAE is trained by maximising a lower bound to the log likelihood (the ELBO),\n$l = \\mathbb{E}_{x \\sim p(x)} \\log p_{\\theta}(x) < \\mathbb{E}_{x \\sim p(x)} \\left[ \\log \\frac{p_{\\theta}(x|z)}{q_{\\phi}(z|x)} \\right] - \\beta D_{KL}\\left(q_{\\phi}(z|x) || p(z) \\right),$ (1)\nwhere q\u03c6(z|x) learns to approximate the model posterior po(z|x) = \\frac{p_{\\theta}(x|z)p(z)}{p_{\\theta}(x)}; and \u03b2=1. A VAE pararmeterises distributions by neural networks: q\u2084(z|x) = N(z; e(x), \u03a3x) has mean e(x) and diagonal covariance Ex output by an encoder network; and po(x|z) is typically of exponential family form (e.g. Bernoulli or Gaussian) with natural parameter 0=d(z) defined by a decoder network.\u00b9 The prior is commonly a standard Gaussian p(z)=N(z; 0, I).\nWhile samples generated from a VAE (\u03b2 = 1) can exhibit disentanglement, setting \u03b2 > 1 is found empirically to enhance the effect, typically with a cost to generative quality (Higgins et al., 2017).\nProbabilistic Principal Component Analysis (PPCA): PPCA (Tipping & Bishop, 1999) considers a linear latent variable model with parameters W\u2208Rm\u00d7d, \u03c3\u2208R and noise \u20ac \u2208 Rm.\u00b2\nx = Wz+\u20ac z ~ p(z) = N(z; 0, I) \u20ac ~ po(\u20ac) = N(\u20ac; 0, \u03c3\u00b2\u0399), (2)\nAll distributions are Gaussian and known analytically, in particular the model posterior is given by\npo(z|x) = N(z; M\u00af\u00b9W\u00afx, \u03c3\u00b2M\u00af\u00b9) where M = WW + \u03c3\u00b2 I . (3)\nThe maximum likelihood solution is fully tractable: WPPCA=U(S \u2013 02I)\u00b9/2 R, where S\u2208 Rd\u00d7d and U \u2208 Rmxd contain the largest eigenvalues and corresponding eigenvectors of the data covariance XX, and R\u2208 Rd\u00d7d is orthonormal (R\u012aR=I). As \u03c3\u00b2\u21920, WML approaches the singular value decomposition (SVD) of the data X=USVT up to ambiguity in V, as in classical PCA. Due to the ambiguity in R/V, the model is considered unidentified. While the exact solution is known, it can also be numerically approximated by optimising the ELBO, e.g. (Eq. 1) by expectation maximisation (PPCAEM), where (E) sets q$(z|x) to its exact optimum po(z|x) in Eq. 3; and (M) optimises w.r.t. \u03b8.\nLinear VAE (LVAE): A VAE with Gaussian likelihood pe(x|z) = N(x;d(z), o\u00b2I) and linear decoder d(x) = Wx (termed a linear VAE assumes the same underlying model as PPCA (2). Indeed,"}, {"title": "ORTHOGONALITY TO DISENTANGLEMENT", "content": "training an LVAE differs to PPCAEM only in approximating the posterior by q\u00a2(z|x) = N(z; Ex, \u03a3) rather than direcftly computing its optimum, While that may seem a backward step, Lucas et al. (2019) showed that the diagonal \u2211 of an LVAE breaks the symmetry of PPCA. This follows from \u03a3 being both diagonal and optimal per Eq. 3,\n$\u03a3 = \u03c3^2 M_{PPCA}^{-1} = (I + \\frac{1}{\\sigma^2} W_{PPCA}^T W_{PPCA})^{-1} = \\sigma^2 U (\\sigma^{-2}I + S) U^T = \u03c3^2 RSS^T$ Vx,\n(4)\n(using the definition of WPPCA), which requires R = I and restricts the solution of an LVAE to WLVAE = U (S \u2013 02 \u0399)1/2 (cf WPPCA), up to trivial transformations (axis permutation and sign).\nOrthogonality in a VAE Decoder's Jacobian: Beyond symmetry breaking in linear VAEs, diagonal posterior covariances are shown to promote disentanglement in non-linear VAEs by inducing columns of the decoder's Jacobian to be mutually orthogonal (Rolinek et al., 2019; Kumar & Poole, 2020). The generalised argument of Kumar & Poole (2020) reparameterises around the encoder mean, z = e(x)+\u20ac, e~N(0, \u03a3\u00a3), and Taylor expands to approximate a deterministic ELBO (det-ELBO):\n$l(x) = \\mathbb{E}_{\\epsilon \\sim q(\\epsilon|x)} \\left[log p_{\\theta}(x|z=e(x) + \\epsilon) - \\beta log \\frac{q(\\epsilon)}{p(\\epsilon|x=e(x)+\\epsilon)} \\right]$ (Reparameterise)\n$= \\mathbb{E}_{\\epsilon \\sim q(\\epsilon|x)} \\left[log p_{\\theta}(x|z=e(x)) + \\epsilon^T j_{e(x)}(x) + \\frac{1}{2} \\epsilon^T H_{e(x)}(x) \\epsilon + O (\\epsilon^3) - \\beta D_{KL}\\right]$ (Taylor)\n$\\approx log p_{\\theta}(x|z=e(x)) + \\frac{1}{2} H_{e(x)}(x) \\odot \\Sigma_{\\phi} - \\beta (||e(x)||^2 + tr(\\Sigma_{\\phi}) - log |\\Sigma_{\\phi}| - d).$ (5)\nHere, jz*(x)=(\\frac{\\partial}{\\partial z}log p_{\\theta}(x|z))_{z^*}; and Hz*(x)=(\\frac{\\partial^2}{\\partial z_i \\partial z_j} log p_{\\theta}(x|z))_{i,j} are the Jacobian and Hessian of log pe evaluated at z* \u2208 Z; and \u2299 is the Frobenius (dot) product. The last step drops \\mathbb{E}_{\\epsilon|x}[O(\\epsilon^3)] terms, as Kumar & Poole (2020) justify (see also A.2). If pe (x|z) is an exponential family distribution with natural parameter defined by the encoder 0=d(z), the Hessian in Eq. 5 satisfies\n$H_{e(x)}(x) = -D_{e(x)}^T A_{d \\circ e(x)} D_{e(x)} + (\\hat{x} - \\mu_{d \\circ e(x)}) D''_{A_{d \\circ e(x)} \\circ e(x)},$\nwhere $D_{z^*}$ ($D''_{z^*}$) is the Jacobian (Hessian) of the decoder evaluated at z*; $A = -\\frac{\\partial^2}{\\partial \\theta^2} log p_{\\theta^*}(x|z) = Var[x|\\theta^*]$; and $x^{hat}_{\\theta^*} = \\mathbb{E}[x|\\theta^*]$. Note Ae is diagonal if individual dimensions xi are conditionally independent given 0, e.g. pixel-wise noise. The last term is argued to be zero almost everywhere in common implementations, leaving $H_{e(x)}(x) = -D_{e(x)}^T A_{d \\circ e(x)} D_{e(x)}$ (see A.3 for discussion and A.4 for Gaussian and Bernoulli examples). Differentiating det-ELBO (Eq. 5) with this Hessian form reveals a connection between the decoder Jacobian D\u2082 and encoder variance \u03a3x,\n$\\nabla_{\\Sigma_{\\phi}} l(x) = \\frac{1}{2} (H_{e(x)}(x) - \\beta(I-\\Sigma_{\\phi}^{-1})) $\n$\\Rightarrow \\Sigma_{\\phi}^{-1} = I - \\frac{1}{\\beta} D_{e(x)}^T A_{d \\circ e(x)} D_{e(x)}$\nIt is argued that a diagonal Ex should thus drive the likelihood's Hessian to also be diagonal, and in the Gaussian case (A = I) for columns of the decoder Jacobian De(x) to be orthogonal, \u2200x \u2208 X.\u00b3"}, {"title": "OR ORTHOGONALITY TO DISENTANGLEMENT", "content": "While diagonal covariances in a VAE are shown to cause orthogonality in the Jacobian (\u00a72), the link from orthogonality to disentanglement is less clear, which we now work towards.\nThe SVD of any m\u00d7d matrix defines a 1-to-1 correspondence between left and right singular vectors that define orthogonal bases for Rm and Rd, respectively. Thus, if J = USVT is the SVD of a decoder's Jacobian evaluated at z* \u2208 Z, a small perturbation to z* in the direction of Vi causes a small perturbation to e(z*) in U.,i. If columns of J are orthogonal (\u2200z*), then V = I and all local bases for Z are the standard basis, which is therefore a global basis. Column orthogonality aside, note that if the data happens to vary in an independent factor along a path that follows the basis vectors U.,i, that factor is identifiable from the SVD of J (e.g. Ramesh et al., 2018).\nNote, however, that this relates two very different notions of \u201cindependence\u201d: orthogonality, pertaining to geometric or linear independence; and independence of factors of a distribution, meaning statistical"}, {"title": "VAES PERFORM LOCAL PCA", "content": "We briefly highlight the close relationship between the deterministic ELBO for Gaussian VAES (A.4) and the analogue for PPCA given by the exact expectation of the PPCAEM objective w.r.t. q(z|x). To aid comparison, from Eq. 2 and 3, let d(z) = \\mathbb{E}[x|z] = Wz, \u00ea(x) = \\mathbb{E}[z|x] = M\u00af\u00b9W\u00afx, and\n= Var[z|x] = (I+WTW)\u00af\u00b9 (we drop constant factors and \u03b2 for clarity).\n\\mathcal{L}_{PPCA} = \\mathbb{E}_x [-||x - d_\\theta(\\hat z(x))||^2 - \\frac{1}{2\\sigma^2} (W^T W)  -\\frac{d}{2}log(|\\Sigma_\\phi|) - (||\\hat z(x)||^2 + tr(\\Sigma_\\phi))]\n\\mathcal{L}_{VAE} \\approx \\mathbb{E}_x [-\\frac{1}{2\\sigma^2}||x-d_\\theta(e(x))||^2 - \\frac{1}{2\\sigma^2} (D_{e(x)}^T D_{e(x)}) \\Sigma_{\\phi}  - (||e(x)||^2 + tr(\\Sigma_{\\phi}) -log |\\Sigma_\\phi| - d)]\nThe main difference lies in the second terms.\u2074 In PPCA, the Hessian of the log likelihood (-WTW) and posterior variance (2) are constant for all x, whereas in a VAE those terms are local to every x \u2208 X. This shows that VAEs approximately perform local PCA as suggested by Rolinek et al. (2019). We also see that the link between encoder covariance and the decoder Jacobian identified in Eq. 7, essentially generalises the optimal posterior covariance in linear PPCA (Tipping & Bishop, 1999):\n$ \\Sigma^{PPCA} = (I + \\frac{1}{\\sigma^2} W^T W)^{-1} \\qquad \\Sigma_{\\phi}^{VAE} = (I + \\frac{1}{\\beta} D_{e(x)}^T D_{e(x)})^{-1}. $\n(8)\nFor later reference (\u00a73.3), note that higher Var[x|z]=02 corresponds intuitively to higher Var[z|x] =\n\u2211 and vice versa, i.e. uncertainty in one domain goes hand in hand with uncertainty in the other."}, {"title": "FROM LINEAR INDEPENDENCE TO STATISTICAL INDEPENDENCE", "content": "We now develop our main result, extending prior work to explain how optimising the ELBO with diagonal posterior covariances leads to disentanglement of generative factors of the data. This elucidates how a seemingly innocuous design choice, which encourages linear independence between columns of the decoder's Jacobian, disentangles statistically independent components of the data.\nPush-forward distribution p(): The generative model of a VAE can be decomposed into stochastic and deterministic steps: sample the prior z ~ p(z); apply a deterministic function x = \u03bc\u03bfd(z) (composing the decoder d: z \u2192 0 and mean function \u03bc: \u03b8 \u2194 \u00ee\u00f7E[x[z]); and add element-wise noise x ~ p(x\u00ea). In many cases, e.g. for image data, element-wise noise serves only as \u201cblur\u201d and is omitted when generating samples, hence samples come from the \u201cpush-forward\u201d distribution of mean parameters p(x). We consider such distributions more generally. For clarity, we define:\nDefinition 1 (push-forward distribution). For a function f : Z \u2192 X and prior distribution pz(z) over Z, the push-forward distribution p #_{fz #p_z}(x) is that defined implicitly by {x= f(z) | z~p(z)} \u2286 X.\nThroughout, we assume:\nA#1. Latent variables are sampled from independent standard normals, pz(z) =\u03a0_{i=1}^d N(zi; 0, 1).\nA#2. f:Z\u2192X is injective, continuous and differentiable almost everywhere (e.g. piece-wise linear).\nUnder A#2, f: (i) defines Mf = {f(z)|z\u2208 Z} \u2286 X, a quasi-differentiable (see Appendix A.3), d-dimensional manifold embedded in Rm supporting p #_{f}; (ii) is bijective between Z and Mf; and (iii) by injectivity, has full-rank Jacobian J (evaluated at z* \u2208 Z), \u2200z* \u2208 Z.\nLetting J = USVT be the SVD of J (U\u00afU = I, VTV = VVT = I), columns of V, denoted vi \u2208 Z = Rd, define a (local) orthonormal basis for Z, while columns ui \u2208 X = Rm of U define a basis for the tangent space to Mf at f(z*). Let si = Si,i denote the ith singular value. Since Jvi = siui, for every z* \u2208 Z the Jacobian identifies a local basis for Z with a local basis for X."}, {"title": "LINEAR f", "content": "For intuition, we first consider the linear model f(z) = Wz (satisfying A#2) for which Mf is a d-dimensional subspace (hyperplane through the origin), Jacobian J = W is constant \u2200z* \u2208 Z, and\n$p_f^#(x=f(z)) = |W|^{-1}p(z) = \\prod_i s_i^{-1}p(z_i)$ (9)\nfactorises. We consider the factors s\u012b\u00b9p(zi). Let v = V\u012bz be z expressed in the V-basis, a transformation with Jacobian \\frac{\\partial z}{\\partial v} = V^T, so p(v) = |V|p(z) = p(z) (as expected since only the basis/perspective changes). By rotational symmetry, p(v) = \u03a0_{i=1}^d p(vi) and p(vi)=N(0,1). We can now consider x as a function of v, x = f(z)=USv=fv (v). Since \\frac{\\partial x_i}{\\partial v_j} = s_i s_j u_i u_j = {s_i^2 if i=j, else 0}, (\\frac{\\partial x}{\\partial v})=US has orthogonal columns siui and || \\frac{\\partial x}{\\partial v} || = s_i. Thus, Eq. 9 becomes\n$p_f^#(x=f_v(v)) = \\prod_i ||\\frac{\\partial x}{\\partial v}||^{-1}p(v_i)$,\nwith factors of the form of uni-variate probability distributions under a change of variables. As illustrated in Figure 1 (left), we let v* = V z* denote an evaluation point in the V-basis, and define\nv_f^{(i)} = {(v_1, ..., v_i, ..., v_d) | v_i \\in \\mathbb{R}}, that describe lines passing through v* as each co-ordinate i (only) varies, or 1-D linear sub-manifolds of Z expressed in the V-basis (blue dashed lines). Let\n$\\mathcal{M}_f^{(i)} = {f_v (v) | v \\in v_f^{(i)} } \\subseteq M_f$\nbe the linear sub-manifold (i.e. line) traced by the image of fv restricted to v_f^{(i)}, essentially a single-variable function of vi (red dashed lines). Note that for different\nv^*, \\mathcal{M}_f^{(i)}\nare always parallel to the U-basis. Each push-forward distribution p_f^#^{(i)} for v_i \\sim p(v_i)\nand fv restricted to v \\in v_f^{(i)}, is supported on \\mathcal{M}_f^{(i)}, with density p_f^#^{(i)} (x) = ||\\frac{\\partial x}{\\partial v}||^{-1}p(v_i), where v=f^{-1}(x), giving meaning to the factors in Eq. 10. Thus for data points on the manifold x \u2208 M f,\n$p_f^#(x) = \\prod_i p_{f,z^*}^#^{(i)}(x)$ (11)\nfactorises as a product of distributions. Since a perturbation to x = f(z*) = fv (v*) in basis vector ui corresponds (exclusively) to a perturbation to v* in vz, only p(vi) is affected, so only p_f^#^{(i)}(x) and no other component p_f^#^{(j \\neq i)} (x) can change. In simple terms, linearly independent basis vectors vi in Z are projected by f to linearly independent basis vectors ui in X, and statistical independence over sub-manifolds defined by v\u2081 is preserved over sub-manifolds defined by u\u017c. In summary, this proves:\nTheorem 1. Assuming A#1 (independent Gaussian latent variables) and a linear function f : Z\u2192X, f(z) = Wz, the push-forward distribution p #_{f} factorises as a product of statistically independent components in X, which align with the left singular vectors of W"}, {"title": "NON-LINEAR f, COLUMN-ORTHOGONAL JACOBIAN", "content": "Theorem 1 for linear f may be unsurprising, but notably its proof does not rely on linearity of f. We therefore follow a similar argument where f may be non-linear and we assume\nA#3. (\u2200z*\u2208 Z) columns of J are mutually orthogonal, \\frac{\\partial x^T}{\\partial z_i} \\frac{\\partial x}{\\partial z_j} =0,i\u2260 j; or, equivalently, V = I.\nTheorem 2. Assuming A#1-3, the push-forward distribution p #_{f} factorises as a product of statistically independent components in X and at each point z*, those components align with columns of U, the left singular vectors of the Jacobian J evaluated at z*.\nProof. The pushforward distribution satisfies\n$p_f^#(f(z)) = |W|^{-1}p(z) = \\prod_i s_i^{-1}p(z_i) = \\prod_i ||\\frac{\\partial x}{\\partial z}||^{-1}p(z_i),$\nequivalent to Eq. 10 but without the need for a change of basis. As illustrated in Figure 1 (right) and analogously to the linear case, we define\n$Z_f^{(i)} = {(z_1, ..., z_i, .., z^*) | z_i \\in \\mathbb{R} }, which describe\northogonal lines in Z passing through z* parallel to the standard basis (blue dashed lines); and uni-variate functions\n$f^{(i)} (z_i) = f(z_1,..., z_i,..., z^*)$, which trace the image of f restricted to\nz\u2208 Z (\u00b9). By construction, each ft) (zi): (i) has derivative\n$\\frac{\\partial}{\\partial z_i} f^{(i)} = \\frac{\\partial f}{\\partial z_i}$ matching a corresponding\npartial derivative of f; and (ii) defines a 1-D sub-manifold\n$\\mathcal{M}_f^{(i)} = {f(z_i) | z_i \\in \\mathbb{R}^d} \\subset M_f$\npassing through f(z*) with tangent vector (red dashed lines). Since \\frac{\\partial x}{\\partial z_i} are orthogonal, sub-manifolds \\mathcal{M}_f^{(i)}\nare orthogonal at z*. Lastly, each tuple {f(zi), \\frac{\\partial x}{\\partial z_i}, p(zi)} defines a push-forward distribution p(x) over a sub-manifold\nx\u2208 M_f^{(i)}, satisfying p(x) = ||\\frac{\\partial x}{\\partial z_i}||^{-1}p(z_i) for z=f^{-1}(x), giving meaning to the factors of Eq. 12. Thus, for x \u2208 Mf,\n$p_f^#(x) = \\prod_i p_{f,z^*}^#^{(i)}(x)$ (13)\nfactorises into uni-variate distributions supported over sub-manifolds \\mathcal{M}_f^{(i)} that are mutually orthogonal where they meet. By orthogonality, traversing sub-manifold \\mathcal{M}_f^{(i)}corresponds to varying a single latent factor zi, hence all other probability factors remain constant and {\\mathcal{M}_f^{(i)}}\nserve as a (non-linear) co-ordinate system in X corresponding to statistically independent components."}, {"title": "NON-LINEAR f", "content": "Having seen that column orthogonality (A#3) is sufficient for independent factors in X, we now consider if it is necessary. Relaxing A#3, we consider a general push-forward distribution under A#1 and A#2, i.e. independent latent variables and an injective, (quasi-)differentiable function.\nIn previous scenarios, sub-manifolds in Z (V), Z)) are linear, defined by right singular vectors of the Jacobian that are constant \u2200z \u2208 Z. Those sub-manifolds can also be defined parametrically, as continuous paths that follow right singular vectors at each point (cf integrating over a vector field). In our relaxed scenario, singular vectors can vary \u2200z \u2208 Z, but J is (quasi-)continuous w.r.t. z (by A#2) and the SVD of a matrix M is continuous w.r.t. M (Papadopoulo & Lourakis, 2000), hence right singular vectors v\u2081 trace continuous sub-manifolds V(i) \u2286 Z, that are orthogonal where they meet. As always for an SVD, mutually orthogonal V(i) \u2286 Z map to mutually orthogonal sub-manifolds M(i) \u2264 X and p(x) factorises as a product of component push-forward distributions over each M(i). However, sub-manifolds V(i) \u2208 Z need not be linear and are not necessarily statistically independent, i.e. the density at z* \u2208 Z may not equal the product of densities over V(i) \u2208 Z passing through z*.\nThis suggests that either: (1) sub-manifolds are not statistically independent and p(x) can not necessarily factorise as a product of independent components, i.e. f entangles zi; or (2) sub-manifolds are statistically independent and map under f to independent factors in X. In case 2, since an optimal Gaussian VAE maps independent components M(i) \u2264 X to the standard basis in Z, independent factors in X can be identified, but sub-manifolds V(i) \u2208 Z are unidentifiable, analogous to V in PCA."}, {"title": "INTERPRETING \u1e9e OF \u03b2-VAE", "content": "Note that \u1e9e in Eq. 1 has played no role, yet is seen to affect disentanglement (Higgins et al., 2017), we therefore consider its role in the (8-)ELBO. Previous works interpret \u1e9e as re-weighting the KL and reconstruction components of the ELBO, or serving as a Lagrange multiplier for a KL \u201cconstraint\". We provide an interpretation more in keeping with the original ELBO.\nTo model data from a given domain, a (3-)VAE requires a suitable likelihood po(x|z), e.g. a Gaussian likelihood for coloured images, and a Bernoulli for black and white images where pixel values xk \u2208 [0, 1] are bounded (Higgins et al., 2017) . In the Gaussian case, dividing Eq. 1 by \u1e9e shows that training a B-VAE with encoder variance Var[x|z]=o\u00b2 is equivalent to a VAE with Var[x|z] = \u03b2\u03c3\u00b2 and adjusted learning rate (Lucas et al., 2019). We now interpret 8 for other likelihoods.\nIn the Bernoulli example mentioned above, black and white image pixels are not strictly black or white (xk \u2208 {0, 1}) and may lie between (xk \u2208 [0, 1]), hence the Bernoulli distribution appears invalid as it does not sum to 1 over the domain of xk. That is, unless each sample is treated as the mean of multiple (true) Bernoulli samples. Multiplying the likelihood by a factor \u043a> 1 is then tantamount to scaling the number of observations as though each were made k times, lowering the variance of the"}, {"title": "REVISITING THE PROOF OF JACOBIAN ORTHOGONALITY", "content": "Having established that orthogonality in the Jacobian is key to disentangling independent components, we go back and (i) prove that a diagonal covariance matrix Ex leads to a diagonal Hessian, and (ii) consider the optimal singular values of the decoder Jacobian.\nTheorem 4. [Diagonal Hessian] det-ELBO (Eq. 5) is maximised w.r.t. eigenvectors VH Of H iff VH=I, \u2200x.\nSee Appendix B for a proof (by the Cauchy-Schwartz inequality with matrix trace norm) and as a sanity check, a proof that the optimum holds when considering \u2211 as a function of H, i.e. assuming\nA#4. The decoder is sufficiently optimised to maintain the relationship in Eq. 7: \u2211 = (I \u2212 +H)\u00af\u00b9.\nThis confirms the hypothesis of Kumar & Poole (2020) that the det-ELBO is maximised if H is diagonal (\u00a72). We note that H and its eigenvectors V are not parameters that are directly optimised. Neither are typically computed and both can vary for every x. Rather, Theorem 4 indicates implicit pressure during optimisation, as Rolinek et al. (2019) and Kumar & Poole (2020) verify empirically."}, {"title": "RELATED WORK", "content": "Many works study aspects or variants of VAEs, or disentanglement in other modelling paradigms. Here, we review those that offer insight into understanding the underlying cause of disentanglement in VAEs. Higgins et al. (2017) first showed that disentanglement is enhanced by increasing \u1e9e in Eq. 1, and Burgess et al. (2018) hypothesised that diagonal posterior covariances may be the cause, encouraging latent dimensions to align with generative factors of the data. Rolinek et al. (2019) showed theoretically and empirically that diagonal posterior covariances promote orthogonality in the decoder's Jacobian, which they deemed responsible for disentanglement. Kumar & Poole (2020) simplified and generalised that argument. These works demonstrated that diagonal posteriors provide an inductive bias that breaks the rotational symmetry of an isometric Gaussian prior, side-stepping impossibility results related to independent component analysis (e.g. Locatello et al., 2019). Several works investigate the more analytically tractable objective and learning dynamics of linear VAEs (Lucas et al., 2019; Bao et al., 2020; Koehler et al., 2022). Zietlow et al. (2021) show that disentanglement is sensitive to perturbations to the data distribution. Reizinger et al. (2022) relate the VAE objective to independent causal mechanisms (Gresele et al., 2021) which consider non-statistically independent sources that contribute to a mixing function by orthogonal columns of the Jacobian. This clearly relates to the orthogonal Jacobian bias of VAEs, but differs to our approach that identifies statistically independent components/sources. Ramesh et al. (2018) trace independent factors by following leading left singular vectors of the Jacobian of a GAN generator. In the opposite direction, Chadebec & Allassonni\u00e8re (2022) trace manifolds in latent space by following a locally averaged metric derived from VAE posterior co-variances. Pan et al. (2023) claim that the data manifold is identifiable from a geometric perspective assuming Jacboian-orthogonality, which"}, {"title": "CONCLUSION", "content": "Unsupervised disentanglement of independent factors of the data is a fundamental aim of machine learning and significant recent progress has been made in the case of VAEs. We extend that work to show that the choice of diagonal posterior covariances in a VAE causes statistically independent components of the data to align with distinct latent variables of the model, i.e. disentanglement. In the process, we provide a novel yet straightforward interpretation of \u03b2 in a B-VAE, which plausibly explains why increasing \u03b2 promotes disentanglement but degrades generation quality; while decreasing \u03b2 mitigates posterior collapse. We also supplement the proof of orthogonality by showing that the likelihood's Hessian is necessarily encouraged to be diagonal and giving a detailed analysis of the Jacobian's optimal singular values.\nNeural networks are often considered too complex to explain, yet recent advances make their deployment in everyday applications all but inevitable. Improved theoretical understanding is therefore essential to be able to confidently take full advantage of machine learning progress, possibly in critical systems, and we hope that the body of work that we add to here is a useful step. Interestingly, our approach rests on the fact that, regardless of the model's complexity, its Jacbian, which transforms the density of the prior, can be considered in relatively simple terms.\nNot only is a better understanding of VAEs of interest in itself, VAEs are often part of the pipeline in recent diffusion models that achieve state-of-the-art generative performance (e.g. Pandey et al., 2022; Yang et al., 2023; Zhang et al., 2022). Other recent works show that supervised learning (Dhuliawala et al., 2023) and self-supervised learning (Bizeul et al., 20"}]}