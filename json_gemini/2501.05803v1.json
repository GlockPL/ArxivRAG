{"title": "ALIGNMENT WITHOUT OVER-OPTIMIZATION: TRAINING-FREE SOLUTION FOR DIFFUSION MODELS", "authors": ["Sunwoo Kim", "Minkyu Kim", "Dongmin Park"], "abstract": "Diffusion models excel in generative tasks, but aligning them with specific objec-\ntives while maintaining their versatility remains challenging. Existing fine-tuning\nmethods often suffer from reward over-optimization, while approximate guidance\napproaches fail to optimize target rewards effectively. Addressing these limita-\ntions, we propose a training-free sampling method based on Sequential Monte\nCarlo (SMC) to sample from the reward-aligned target distribution. Our approach,\ntailored for diffusion sampling and incorporating tempering techniques, achieves\ncomparable or superior target rewards to fine-tuning methods while preserving\ndiversity and cross-reward generalization. We demonstrate its effectiveness in\nsingle-reward optimization, multi-objective scenarios, and online black-box opti-\nmization. This work offers a robust solution for aligning diffusion models with\ndiverse downstream objectives without compromising their general capabilities.\nCode is available at https://github.com/krafton-ai/DAS.", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021c; Park et al., 2024)\nhave revolutionized generative AI, excelling in tasks from Text-to-Image (T2I) generation (Rom-\nbach et al., 2022) to protein structure design (Watson et al., 2023). However, diffusion models are\ntypically pre-trained on large uncurated datasets that may not accurately represent the desired target\ndistribution. For instance, in the T2I generation, real users want to produce aesthetically pleasing\nimages while faithful to prompt instructions, rather than generating random internet images from the\npre-trained dataset. Also, one might want to produce only specific cartoon character images, rather\nthan general styles. These challenges underscore the importance of alignment, a process to adapt\ndiffusion models for specific customized rewards.\nExisting alignment approaches mainly fall into two categories: (1) fine-tuning and (2) guidance\nmethods. Fine-tuning approaches, including Reinforcement Learning (RL) (Fan et al., 2024; Black\net al., 2023) and direct backpropagation (Clark et al., 2024; Prabhudesai et al., 2024), have shown\npromising results in optimizing target rewards. However, it often suffers from the reward over-\noptimization problem, sacrificing general image quality and diversity (Clark et al., 2024; Gao et al.,\n2022). On the other hand, guidance methods (Bansal et al., 2023; Yu et al., 2023; Song et al., 2023;\nHe et al., 2024) offer a training-free alternative that stays closer to the pre-trained model distribution.\nMeanwhile, they suffer from the reward under-optimization problem, failing to effectively optimize\ntarget rewards due to relying on estimated inference-time corrections of the generation process.\nTo address these limitations, we propose Diffusion Alignment as Sampling (DAS), a training-free\napproach that both achieves effective reward alignment and preserves model generalization. To\nguide latents toward high-reward samples, DAS leverages multiple candidate latents through Se-\nquential Monte Carlo (SMC) sampling, averaging out errors in estimated corrections to enable sam-\npling from a reward-aligned target distribution. By carefully designing intermediate target distribu-\ntions with tempering techniques, DAS achieves high sample efficiency with multiple candidates, as\nwe demonstrate both theoretically and empirically."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 FINE-TUNING DIFFUSION MODELS FOR ALIGNMENT", "content": "Aligning pretrained models through fine-tuning has been extensively studied in language models\n(Ziegler et al., 2020; Ouyang et al., 2022; Rafailov et al., 2023). For diffusion models, several\napproaches have emerged. Lee et al. (2023) and Wu et al. (2023b) employ supervised fine-tuning\nwith preference-based reward models. Black et al. (2023) and Fan et al. (2024) formulate sampling\nas a Markov decision process and apply reinforcement learning (RL) to maximize rewards. Xu\net al. (2024); Clark et al. (2024) and Prabhudesai et al. (2024) fine-tune by direct backpropagation\nthrough differentiable reward models. These approaches, however, face challenges with reward\nover-optimization (Gao et al., 2022; Coste et al., 2024), which may distort alignment or reduce\nsample diversity. KL regularization has been proposed as a mitigation strategy (Fan et al., 2024;\nUehara et al., 2024a), inspired by its success in language models (Stiennon et al., 2020; Ouyang\net al., 2022; Korbak et al., 2022). Section 3.2 examines the limitations of this approach, focusing on\nthe mode-seeking behavior observed in the context of variational inference. While diffusion-based\nsamplers (Zhang & Chen, 2022; Vargas et al., 2023; Berner et al., 2024; Sanokowski et al., 2024) use\nsimilar training objective to sample from multimodal, unnormalized target density, the fine-tuning\nsetup makes training more susceptible to mode collapse (Appendix E). Alternatively, Zhang et al.\n(2024) approached over-optimization in RL fine-tuning through inductive and primacy biases."}, {"title": "2.2 GUIDANCE METHODS", "content": "Building on the score-based formulation of diffusion models (Song et al., 2021c), various guidance\nmethods have been developed. While classifier guidance (Dhariwal & Nichol, 2021) requires addi-\ntional training, recent works approximate guidance to use off-the-shelf classifiers or reward models\ndirectly (Ho et al., 2022; Song et al., 2022; Chung et al., 2023; Bansal et al., 2023; Yu et al., 2023;\nSong et al., 2023; Yoon et al., 2023; He et al., 2024). These methods rely on Tweedie's formula\n(Efron, 2011; Chung et al., 2023) for prediction of the original data given noisy data, but our exper-\niments indicate that such inaccurate prediction limits effectiveness in maximizing complex rewards.\nSequential Monte Carlo (SMC) methods have been applied to address inexactness (Trippe et al.,\n2023; Cardoso et al., 2024; Wu et al., 2023a; Dou & Song, 2024), but their application has been\nlimited to inverse problems and class-conditional sampling. While SMC methods offer asymptotic\nexactness, naive applications may fail to sample from complex targets within finite samples due\nto inefficiency. Our approach incorporates a tempered SMC sampler to enhance sample efficiency,\nachieving comparable or superior performance to fine-tuning methods without additional training."}, {"title": "3 DIFFUSION ALIGNMENT AS SAMPLING (DAS)", "content": "This section formulates the diffusion alignment problem as sampling from a reward-aligned distri-\nbution, examines limitations of existing methods, and introduces DAS, a Sequential Monte Carlo\n(SMC) based algorithm with theoretical guarantees for asymptotic exactness and sample efficiency."}, {"title": "3.1 PROBLEM SETUP: ALIGNING DIFFUSION MODELS WITH REWARDS", "content": "Aligning diffusion models with rewards can be seen as finding a new distribution that maximizes\nthe expectation given reward r. Formally, it can be written as solving:\n$p_{tar} = arg max_{p} E_{x~p}[r(x)].$\nHowever, this approach may lead to reward over-optimization (Gao et al., 2022), disregarding the\npre-trained distribution. To mitigate this, we employ KL regularization (Korbak et al., 2022; Uehara\net al., 2024a):\n$p_{tar} = arg max_{p} E_{x\u223cp}[r(x)] - \u03b1 D_{KL}(p || p_{pre})$\nwhere $p_{pre}$ is the sample distribution of the pre-trained diffusion model. Following Rafailov et al.\n(2023), it is straightforward to show that the target distribution can be written in an equivalent form:\n$p_{tar}(x) = \\frac{1}{Z} p_{pre}(x) exp(\\frac{r(x)}{\u03b1})$\nwhere Z is normalization constant. We frame the diffusion alignment problem as sampling from this\nreward-aligned target distribution $p_{tar}$. Note, however, that we only have access to an unnormalized\ndensity of $p_{tar}$ and its evaluation requires running a probability flow ODE (Song et al., 2021c), even\nfor a single sample, making the sampling problem highly non-trivial.\nBefore we continue, we introduce binary optimality variable $O \u2208 \\{0, 1\\}$ with $p(O = 1|x) \u221d$\n$exp(r(x)/\u03b1)$, where samples with high reward are interpreted as more likely \"optimal\u201d. Then the\nposterior $p(x|O = 1)$ characterizes the distribution of samples that achieve high rewards. Using\nBayes' rule with prior $p = p_{pre}$ give $p(x|O = 1) \u221d p(x)p(O = 1|x) = p_{pre}(x) exp (r(x)/\u03b1) \u221d$\n$p_{tar}(x)$, revealing the equivalence between two perspectives. We drop '=1' from now on following\ncommon convention."}, {"title": "3.2 LIMITATIONS OF EXISTING METHODS", "content": "Previous approaches to sampling from the target distribution (Equation 3) primarily fall into two\ncategories: fine-tuning and direct sampling using approximate guidance. In this subsection, we first\ndemonstrate how these approaches struggle to sample from multimodal target distributions, even for\nsimple Gaussian mixtures, and explain their limitations leading to potential failures.\nWe first investigate the source of the mode-seeking behavior of fine-tuning methods. Fine-tuning\nmethods can be interpreted as variational inference, with the following objective (Rafailov et al.\n(2023) Appendix A.1) :\n$minimize_{\u03b8} D_{KL}(p_{\u03b8} || p_{tar}).$\nThis can be optimized using reinforcement learning (RL) (Fan et al., 2024) or direct backpropaga-\ntion (Uehara et al., 2024a). However, the mode-seeking behavior of reverse KL divergence (Chan\net al., 2022; Wang et al., 2023) may cause the model to fit only the modes of the target distribution,\nespecially when $p_{tar}$ is multimodal.\nThis connects to low diversity of fine-tuning\nmethods, which we further demonstrate in Section 4 for real-world examples.\nNext, we turn to approximate guidance methods. If the exact score function of the posteriors\n$\u2207_{x_t} log p_t(x_t|O) = \u2207_{x_t} log p_t(x_t) + \u2207_{x_t} log p(O|x_t)$"}, {"title": "3.3 SAMPLING FROM REWARD-ALIGNED TARGET DISTRIBUTION VIA TEMPERED SMC", "content": "Fine-tuning often leads to over-optimization, while approximate guidance methods struggle with\nreward optimization. To improve approximate guidance, we can use multiple candidate latents (par-\nticles) during sampling, selecting those with high predicted rewards that stay close to the pre-trained\ndiffusion model's distribution. This approach leverages Sequential Monte Carlo (SMC) methods,\nwhich take incremental guided steps rather than sampling directly from the target distribution. At\neach diffusion step, the process evaluates and resamples candidates based on both reward scores and\nalignment with the pre-trained model, ultimately producing samples that satisfy both criteria.\nTraditional SMC typically requires thousands of particles, making it computationally expensive\nfor diffusion models. However, by employing techniques like tempering, we achieve high-quality,\nreward-aligned samples with fewer particles, making the method practical for real-world applica-\ntions. To formally describe our approach, we first outline the key design choices of SMC samplers\nthat enable this guided sampling process:\n* Sequence of intermediate target distributions $\u03c0_t(x_t) = \u03b3_t(x_t)/Z_t$ for t = 0 : T that bridge\nbetween the prior $\u03c0_T$ and target distribution $\u03c0_0$, where $\u03b3_t$ is unnormalized density of $\u03c0_t$"}, {"title": "3.3.1 BACKWARD KERNEL", "content": "To incorporate pre-trained diffusion models, we define the backward kernel using Bayes' rule\nwith general stochastic diffusion samplers. For any stochastic diffusion sampler $p_\u03b8(x_{t-1}|x_t) =$\n$\\mathcal{N}(\u03bc_\u03b8(x_t, t), \u03c3^2 I)$, we define the backward kernels as:\n$L_t(x_t|x_{t-1}) := \\frac{p_\u03b8(x_{t-1}|x_t)p_t(x_t)}{p_{t-1}(x_{t-1})}.$\nThis formulation also serves as an approximation for general non-Markovian forward processes\ngiven pre-trained generation processes"}, {"title": "3.3.2 INTERMEDIATE TARGETS: APPROXIMATE POSTERIOR WITH TEMPERING", "content": "As stated in section 3.2, sampling from the target $p(x_0|O)$ requires score functions of the true pos-\nteriors $p_t (x_t|O)$. Instead, approximate guidance gives a score function of an alternative distribution,\nwhich we refer to as the approximate posterior:\n$p_t(x_t|O) \u221d p_t(x_t)p(O|\\hat{x_0}(x_t)) \u221d p_t(x_t) exp (\\frac{r(\\hat{x_t})}{\u03b1})$\nHowever, we can't sample even from these approximate posteriors since they are not defined by\nany forward diffusion process anymore. Nevertheless, this approximate posterior becomes exact at\nt = 0 as $x_0 = \\hat{x_0}$, thus defining a sequence of distributions interpolating $p_T = \\mathcal{N}(0, I)$ and $p_{tar}$\nwhich can be incorporated as intermediate targets for SMC sampler. Since prediction $\\hat{x_0}$ gets more\naccurate as t goes to 0, the approximate posteriors get closer to the true posteriors while the error\nmay be large at the beginning of sampling. Hence, we add tempering for intermediate targets as:\n$\u03c0_t(x_t) \u221d p_t(x_t)p(O|\\hat{x_0}(x_t))^{\u03bb_t} \u221d p_t (x_t) exp (\\frac{r(\\hat{x_t})}{\u03b1})^{\u03bb_t} = \u03b3_t(x_t).$\nwhich can interpolate $\u03c0_T = p_T$ to $\u03c0_0 = p_{tar}$ more smoothly where $0 = \u03bb_T \u2264 \u03bb_{T\u22121} \u2264 \u2026 < \u03bb_0 = 1$\nis sequence of inverse temperature parameters.\nWhile modern SMC samplers often use adaptive tempering (Chopin & Papaspiliopoulos, 2020;\nMurphy, 2023), we find out simply setting $\u03bb_t = (1 + \u03b3)^{-t}$ works well in our setting where \u03b3\nis a hyperparameter. In Section 4.1, we compare different tempering schemes and explain how to\nselect \u03b3. To the best of our knowledge, this adaptation of density tempering is novel among works\napplying SMC methods to diffusion sampling."}, {"title": "3.3.3 PROPOSAL: APPROXIMATING LOCALLY OPTIMAL PROPOSAL", "content": "Given the backward kernels and intermediate targets, we derive the locally optimal proposal that\nminimizes the variance of the weights. Minimizing weight variance ensures more uniform impor-\ntance among particles, thereby enhancing sample efficiency."}, {"title": "3.3.4 ASYMPTOTIC BEHAVIOR", "content": "This section presents asymptotic analysis results for DAS. We first demonstrate asymptotic exact-\nness, a key property distinguishing SMC methods from other approximate guidance approaches.\nAlthough SMC samplers are asymptotically exact, their sample efficiency depends on design\nchoices. Using a Central Limit Theorem analysis, we bound the asymptotic variance of sample\nestimations. This approach allows us to prove the benefits of tempering for sample efficiency, pro-\nviding theoretical justification beyond intuitive advantages."}, {"title": "4 EXPERIMENTS", "content": "The main benefits of DAS are twofold: (1) it can avoid over-optimization by directly sampling\nfrom the target distribution, and (2) it is efficient since there is no need for additional training. We\ninvestigate these benefits through various experiments by addressing the following questions:\n* Can DAS effectively optimize a single reward while avoiding over-optimization? (\u00a74.1)\n* Can DAS optimize multiple rewards all at once without training for each combination? (\u00a74.2)\n* Can DAS effectively search diverse viable solutions in an online black-box optimization? (\u00a74.3)\n* Does tempering increase sample efficiency as predicted by the theory? (\u00a74.1)"}, {"title": "4.1 SINGLE REWARD", "content": ""}, {"title": "4.1.1 EXPERIMENT SETUP", "content": "Tasks. For single reward tasks, we use aesthetic scores (Schuhmann et al., 2022) and human prefer-\nence evaluated by PickScore (Kirstain et al., 2023) as objectives. For fine-tuning methods, we used\nanimals from Imagenet Deng et al. (2009) and prompts from Human Preference Dataset v2 (HPDv2)\n(Wu et al., 2023b) when training on aesthetic score and PickScore respectively, like previous settings\nEvaluation uses unseen prompts from the same dataset.\nEvaluation metrics. We assess three aspects: target rewards, cross-reward generalization, and\nsample diversity. For cross-reward generalization, we use HPSv2 (Wu et al., 2023b) and ImageRe-\nward (Xu et al., 2024), both alternative rewards that measure human preference. For sample diver-\nsity, we use Truncated CLIP Entropy (TCE) (Ibarrola & Grace, 2024) which measures entropy of\nCLIP embeddings, and mean pairwise distance (MPD) calculated with LPIPS which quantifies perceptual differences.\nBaselines. We employ Stable Diffusion v1.5 (Rombach et al., 2022) as the pre-trained model. Other\nbaselines include fine-tuning methods and training-free guidance methods"}, {"title": "4.1.2 RESULTS", "content": "Quantitative evaluation. Figure 2 shows quantitative results on both the target reward and eval-\nuation metrics. Fine-tuning methods generally cluster in the bottom right, indicating reward over-\noptimization with high target rewards but low diversity and poor generalization to similar rewards.\nAlignProp with KL exhibits a similar trend, failing to mitigate over-optimization due to mode-\nseeking behavior, as demonstrated in the mixture of Gaussian example (Section 3.2). TDPO, pro-\nposed as an alternative to early stopping and KL regularization, fails to effectively mitigate over-"}, {"title": "4.2 MULTI REWARDS", "content": "Experiment setup. Multi-objective optimization is crucial for real-world applications that balance\ncompeting goals (Deb et al., 2016) - for instance, generating visually appealing images while being\nfaithful to prompts. To evaluate DAS in this practical setting, we combine aesthetic score and\nCLIPScore (Hessel et al., 2021), which measures image-text alignment. We use a weighted sum:\n$w \\cdot Aesthetic \\text{ }Score + (1 \u2013 w) \\cdot 20 \\cdot CLIPScore$\nwith w \u2208 {0,0.1, 0.3, 0.5, 0.7, 0.9,1.0}. Baselines include interpolated LoRA weights fine-tuned\nseparately on each objective using DDPO and AlignProp, and a model directly fine-tuned on the weighted sum. We use\nHPDv2 prompts for training and evaluation.\nPareto-optimality without fine-tuning. Figure 6a shows DAS achieving Pareto-optimal solutions\nwithout any fine-tuning or model interpolation, outperforming methods that require extensive train-"}, {"title": "4.3 ONLINE BLACK-BOX OPTIMIZATION", "content": "Online black-box optimization with diffu-\nsion models. This approach optimizes an un-\nknown function by receiving iterative feed-\nback, especially useful when offline data is\ninsufficient or objectives (e.g., human prefer-\nences) change over time. Minimizing feed-\nback queries is key to reducing costs. SEIKO\n(Uehara et al., 2024b) is a feedback-efficient\nmethod using an uncertainty-aware optimistic\nsurrogate model built through linear model\n(UCB) or ensembling (Bootstrap). While SEI-\nKO guarantees theoretical regret bounds, this\nresult relies on sampling from an aligned dis-\ntribution using the surrogate model, similar to\n$p_{tar}$ in which they incorporate direct backprop-\nagation to solve it. Instead, we adapt DAS to\ndirectly sample from this distribution (A.3).\nExperiment setup. We adopt aesthetic score as a black-box reward model, and limit to use only\n1024 feedback queries for all methods. Experiment is conducted in a batch online setting through\nan iterative cycle: proposing samples, recieving feedbacks from the black-box\nreward, and updating the surrogate model.\nEfficient exploration of diverse viable solutions. Figure 7 highlights that DAS preserves pre-\ntrained characteristics and generates diverse, high-quality images, while SEIKO, using AlignProp\nwith KL, distorts animal features. Quantitatively, in Table 1, DAS matches SEIKO in optimizing\naesthetic scores but significantly outperforms in unseen rewards and diversity, proving its ability to\nexplore a broader solution space. This demonstrates the advantage of DAS over direct backpropaga-\ntion for the online setting: high sample diversity enhances exploration, leading to a robust surrogate\nmodel and avoiding over-optimization. Furthermore, DAS bypasses fine-tuning the diffusion model\nevery time the surrogate model is updated, enhancing adaptability through frequent updates.\nNon-differentiable rewards. Reward maximization often involves non-differentiable or computa-\ntionally expensive models. While DAS requires differentiable rewards for guidance, it can handle\ngeneral rewards by posing the reward as black-box reward and learning a differentiable surrogate\nmodel with online feedback."}, {"title": "5 CONCLUSIONS", "content": "We introduce DAS, a training-free method using Sequential Monte Carlo sampling to align diffusion\nmodels with rewards. DAS optimizes rewards while preserving generalization without fine-tuning.\nIn single and multi-reward experiments, DAS achieves comparable or superior target rewards to\nfine-tuning methods while excelling in diversity and cross-reward generalization. The online op-\ntimization results demonstrate DAS's ability to efficiently explore diverse, high-quality solutions.\nThese findings establish DAS as a versatile and efficient approach for aligning diffusion models\napplicable to a wide range of objectives and scenarios while significantly reducing the cost and\ncomplexity of the alignment process."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "We have made several efforts to ensure the reproducibility of our work. We provide complete proofs\nfor all theoretical results in Appendix C, including formal statements and proofs for Propositions 2\nand 3 in Appendix C.2.4 and C.2.5. Detailed pseudocode for our full DAS algorithm is included in\nAppendix A, with versions with adaptive resampling and adaptation to online setting. Appendix D contains comprehensive implementa-\ntion details for our method and baselines, including hyperparameter settings, training and sampling\nprocedures. We will release our full codebase upon publication to enable others to replicate our re-\nsults, including implementations of DAS. We use publicly available datasets and evaluation metrics,\nwith details of experiment setup provided in Section 4. Appendix F contains additional experimental\nresults to supplement those in the main paper. By providing these materials, we aim to enable other\nresearchers to reproduce our results and build upon our work. We are committed to addressing any\nquestions or requests for additional information to further support reproducibility efforts."}]}