{"title": "Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant", "authors": ["Marc Ballestero-Rib\u00f3", "Daniel Ortiz Mart\u00ednez"], "abstract": "The dream of achieving a student-teacher ratio of 1:1 is closer than ever thanks to the emergence of large language models (LLMs). One potential application of these models in the educational field would be to provide feedback to students in university introductory program-ming courses, so that a student struggling to solve a basic implementation problem could seek help from an LLM available 24/7. This article focuses on studying three aspects related to such an application. First, the performance of two well-known models, GPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The empirical results showed that GPT-4T per-forms much better than GPT-3.5T, however, it is not yet ready for use in a real-world scenario. This is due to the possibility of generating incorrect information that potential users may not always be able to detect. Second, the article proposes a carefully designed prompt using in-context learning techniques that allows automating important parts of the evaluation process, as well as providing a lower bound for the fraction of feedbacks containing incorrect information, saving time and effort. This was possible because the resulting feedback has a programmatically analyzable structure that incorporates diagnostic information about the LLM's performance in solving the requested task. Third, the article also suggests a possible strategy for implementing a practical learning tool based on LLMs, which is rooted on the proposed prompting techniques. This strategy opens up a whole range of interesting possibilities from a pedagogical perspective.", "sections": [{"title": "Introduction", "content": "Achieving a 1:1 student-to-teacher ratio holds the promise of deeply alter educational environments. With this personalized approach, teaching methods could be tailored to each student, accommodat-ing the diverse learning styles, abilities, and preferences that exist within a classroom. It would also facilitate timely feedback on assignments, enabling students to learn from their mistakes and make continuous improvements. Unfortunately, having a 1:1 student-to-teacher ratio has been an elusive goal due to the significant resource challenges that would be inherently involved, from the necessity of finding qualified teachers or additional classroom spaces to the tremendous financial impact that implementing this model would have on the limited budgets of educational institutions.\nOne good example of this problematic are the programming courses during the first year of Computer Engineering degrees. Such courses have a crucial role in the basic training of students as future professionals, since for many of them, it is within those courses where they are in contact with the task of programming for the first time. However, the learning process for students within these initial courses is not straightforward. In the first year of a regular Computer Engineering degree, the number of enrolled students can be expected to be very high (e.g., more than 100 students enrolled in the Computer Engineering degree of the Universitat de Barcelona during the 2023-2024 academic year). Furthermore, in this context characterized by a high concentration of students, it would be ideal for each one to be able to solve a large number of basic code implementation exercises"}, {"title": "Research Questions", "content": "In order to investigate the ability of ChatGPT to function as a teaching assistant in undergraduate computer programming courses, we will explore the following research questions:\nRQ1 Is ChatGPT able to determine whether a student submission is correct or not according to the problem definition? Being able to understand the student code is a prerequisite for the subsequent explanation of the potential code errors.\nRQ2 Is ChatGPT able to propose a new version of a faulty student submission that is correct according to the problem definition? This is strongly related to the previous point, since, in order to guide a student towards obtaining correct code, another prerequisite would be the ability to generate a correct solution starting from the defective code.\nRQ3 Is ChatGPT able to successfully identify the errors present in a student submission? This constitutes the main task that an application implementing an LLM-based teaching assistant should carry out.\nRQ4 Are the students able to discern if ChatGPT's feedback about a particular exercise is correct or not? A well-known problem with ChatGPT is the possibility of generating responses that appear to be correct but are not actually accurate. This issue is particularly relevant when considering the"}, {"title": "Methodology", "content": "This work focused on comparing the performance of two OpenAI's LLMs\u00b9 for their use as teaching assistants:\n\u2022 GPT-3.5 Turbo (GPT-3.5T): a family of LLMs that take textual information as input and generate text as output. According to the OpenAI documentation, they constitute improved versions of GPT-3.5.\n\u2022 GPT-4 Turbo (GPT-4T): they are large scale, multimodal models that accept image and text inputs, and produce text outputs [OpenAI et al.(2023)]. These models are also described as improved versions of GPT-3.5 in the OpenAI's documentation.\nIn all cases, the models were operated by means of the application programming interface (API) provided by OpenAI."}, {"title": "Data Acquisition", "content": "The experiments presented in this article are based on the solutions to five Python programming problems provided by first-year computer science students at the University of Barcelona. These problems were presented to the students in an evaluative exam during the academic years 2022/2023 and 2023/2024 within the subject of algorithms.\nMore specifically, the two problems to be solved were:\n\u2022 Rotated Palindromes (P1): A Python function should be implemented that receives a string as input. The function should return True if there is any rotation of the input that constitutes a palindrome and False otherwise. Given a string, a rotation is defined as a new string where the last character is removed and inserted again at the start. The function should return True if at least one of the rotations is a palindrome. For instance, if the input string is AAB, the function should return True, since one of the rotations (ABA) is a palindrome. On the other hand, for the input string AB, the function should return False, since there are no palindromic rotations."}, {"title": "User Evaluation", "content": "In this study, we specifically acquired data involving a small group of potential end users of the LLM as a teaching assistant, in connection with RQ4. For this purpose, two implementations for the above-described problems P1 and P2 (cases a and b) were analyzed with the GPT-4T model, providing feedback about the code issues. From the four implementations, GPT-4T pointed out really existent issues with the code in two cases (one for problem P1 and one for problem P2), and mentioned non-existent issues for the other two. Below there is a description of the main features of each case:\n\u2022 P1 case a: the student implementation was wrong and ChatGPT provided correct feedback about the errors.\n\u2022 P1 case b: the student presented a faulty implementation, but ChatGPT's feedback did not correctly identify the errors.\n\u2022 P2 case a: the student's solution was incorrect and ChatGPT successfully identified some (but not all) errors.\n\u2022 P2 case b: the student solution was correct but ChatGPT pointed out non-existing problems in its feedback."}, {"title": "Characterizing Good LLM Feedback", "content": "When using an LLM to provide feedback on programming problems, it is important to establish the qualities that would be desirable in such feedback under a pedagogic point of view. In this article, we identify the following:\n\u2022 The feedback should guide towards the solution instead of providing it directly, which is key to driving the learning process. Therefore, good feedback should not contain code that implements the solution to a problem, but rather provide clues to resolve possible errors. Previous evaluations of LLMs in contexts similar to ours have shown that the GPT model family has a strong tendency to incorporate code that solves a problem even if the prompt used does not request it. For example, in [Hellas et al.(2023)], it is reported that the evaluated feedback almost always contained code, often related to the solution.\n\u2022 When evaluating the usefulness of LLM feedback, it is not as important from our perspective if it points out all the existing problems in a particular implementation. It is sufficient if it indicates a subset of them. At least in the personal experience of the author of this work, the interaction between the student and the teacher typically consists in the student iterating on his/her implementation with the help of the teacher, gradually approaching a correct solution."}, {"title": "Prompting Strategy", "content": "Prompt design constitutes a central aspect of this work. In the previous section, two important features of high-quality feedback from a pedagogical point of view were mentioned, whose treatment could be addressed by means of an appropriate prompt. These two aspects would be, on the one hand, avoiding the presence of code within the feedback, and on the other hand, introducing mechanisms that allow diagnosing problems with the feedback content, particularly if it points out the presence of non-existent errors in the code under analysis.\nBoth problems could benefit from an automatic treatment of the feedback content. In this way, if a response contained code, it could be removed provided that it could be effectively detected and extracted without altering the integrity of the rest of the feedback. On the other hand, problems with the reliability of feedback content might be detected early if it was possible to extract relevant information about the decisions made by the model to generate the feedback. Elements such as a description of the sequence of operations carried out by the analyzed code and whether the model considers that this sequence of operations solves the problem correctly or not could be helpful if they could be analyzed programmatically.\nPrevious works similar to ours have not emphasized the potential advantages of a carefully designed prompt to address the previously mentioned problems. Instead, in this article, the prompt design is particularly focused on producing feedback with a structure that is susceptible to be analyzed in a completely automated manner. To achieve this, the use of in-context-learning (ICL) techniques [Dong et al.(2023)] is proposed, and more specifically, a variety of ICL called chain of thought (CoT) [Wei et al.(2023)].\nThe basic idea behind ICL is to include in the prompt itself a certain number of input-output pairs belonging to the task that one wants to solve through the LLM, aiming for the model to learn to perform its task by analogy (although ICL does not have the ability to directly influence the model's parameters). As a result, this strategy has been shown to improve the capability of LLMs in various tasks compared to the use of standard prompts. CoT further specializes the philosophy of ICL, so that the examples belonging to the task to be solved are no longer limited to providing input-output pairs but also the intermediate reasoning steps that lead to the originally sought answers. This approach has demonstrated significant improvements in tasks requiring complex reasoning, among which would be the analysis of code generated by students investigated in this work."}, {"title": "LLM Feedback Analysis Automation", "content": "Provided that the LLM feedback adheres to the fixed structure given by the prompt in Figure 1, its automated analysis should be straightforward. Since the feedback is provided as a Markdown document, it can be converted into other programmatically-analyzable formats using previously ex-isting libraries. For this work we used the markdown-to-json Python package2, which incorporates a function converting a Markdown string into a Python dictionary.\nOnce the Markdown document is converted into a dictionary, the content of each section can be extracted and analyzed separately. One fundamental item that will be analyzed is the prediction of the LLM regarding the correctness of the implementation. This binary value can be automatically extracted and used to gain knowledge about the LLM ability to understand code. For this purpose, it is important to note that the functions that should be implemented in introductory computer programming assignments can typically be checked by executing a small set of unit tests or asserts. Provided that the set of unit tests covers all relevant cases, the function correctness can be deter-mined automatically and then compared with the LLM correctness prediction. This results in four possible cases that are depicted in Table 1."}, {"title": "Research Question Data Analysis", "content": "In this section, the details of the data treatment related to the different research questions are described."}, {"title": "RQ1", "content": "This research question relates to ChatGPT's ability to understand code developed by a student, determining whether it is correct or not. The prompt adopted in this work allows retrieving the"}, {"title": "RQ2", "content": "For this research question, the corrected version of the student code generated by the LLM is analyzed. Such code can be extracted from the LLM feedback, since it contains a specific section to store it. The extracted code can be used to execute a set of unit tests, determining its correctness. In addition to this, we are also interested to measure how different the corrected version of the code is with respect to the original student version. Higher degrees of similarity means that the LLM generates the corrected version starting from the student's code instead of from scratch. We measure the difference using the Levenshtein distance [Levenshtein(1966)] at character level between the code versions. This measure is also called character error rate (CER)."}, {"title": "RQ3", "content": "The goal of RQ3 is to study whether the LLM feedback points out real issues in the student code. For this purpose, when evaluating an LLM feedback, we will consider two different situations. First, the feedback identifies at least one issue affecting code correctness. Second, the feedback identifies issues uninvolved with code correctness. As a result of this identification, a set of counts will be collected. Counts are only relevant when the system correctly identifies a student code as faulty, that is, for the TN cases shown in Table 1."}, {"title": "RQ4", "content": "This research question studies the ability of the students to detect helpful feedback and distinguish it from that pointing out non-existent or irrelevant code issues. For this part of the evaluation, specific data have been collected, as it is described in Section 3.3. Such data consisted in the answers to three questions for four problem implementations (two for P1 and another two for P2). The first question, Q1, asked the student whether he/she thinks that the LLM feedback pointed out to real issues in the code being analyzed. The second question, Q2, requested students to provide a score between 1 and 5 evaluating the usefulness of the feedback. The third question, Q3, requested the students to explain which information they used to decide about the feedback correctness. For Q1, counts about correct or incorrect identification of relevant LLM feedback will be collected. For Q2, the student score for the LLM feedbacks will be averaged. Finally, for Q3, a qualitative analysis of student's strategies to identify unhelpful feedback will be carried out."}, {"title": "RQ5", "content": "The goal of RQ5 is to determine whether ChatGPT's feedback shows a consistent structure that can be analyzed in a programmatic manner. For this purpose, the model responses will be compared to the intended feedback structure detailed by the used prompt (see Figure 1). Again, this verification will produce a set of counts that will be reported."}, {"title": "RQ6", "content": "The purpose of the last research question is to determine if there are relationships between manual and automatic evaluation measures, in case the latter exist. The existence of strong relationships between automatic and manual measures could reduce the need for costly manual evaluation pro-cesses.\nAs indicated in the previous section, it has been possible to automate several evaluation mea-sures, particularly those related to research questions RQ1 and RQ2. On the other hand, the evaluation measures obtained within research questions RQ3 and RQ4 correspond to manual mea-sures. We will not study the relationship between the measures of RQ1 and RQ2 with those of RQ4, as we consider that the interest in acquiring the latter is not influenced by the existence of automatic measures. This is because the measures of RQ4 collect the criteria of end-users of a hypothetical learning tool based on LLMs, and those criteria are very difficult to simulate or predict.\nHowever, we will study the relationship between the automatic evaluation measures of RQ1 and RQ2 and those obtained for RQ3. The measures of RQ3 evaluate the model's ability to identify problems in defective code. In this regard, it is interesting to consider the cases presented in Table 1, where four situations are shown based on whether the unit tests are passed or not and whether the model predicts that the implementation under analysis is correct or incorrect. Under these circumstances, it is important to highlight that the model will only produce feedback when it considers the implementation to be incorrect. If we focus on implementations not passing the asserts, we will know from the outset that the feedback is erroneous when the LLM classifies the code as correct (FPs), since it will not point out any code issues. It is only when the program is incorrect and the LLM identifies it as such (TNs) that the feedback will need to be evaluated manually.\nIf we consider the situation where ChatGPT generates feedback that does not indicate any problem in faulty code, in other words, the complement of the \"One or More\" column of Table 5, then we have a measure of the rate of erroneous feedbacks made by the model. Due to the fact that the feedback will always be wrong for the FP cases, it is possible to automatically provide a lower bound for such rate. Table 9 shows, for the two models and the five problems under study, the percentage of feedbacks requiring manual evaluation with respect to the total. Additionally, for the defective implementations, the percentage of erroneous feedbacks, and the lower bound of this percentage calculated automatically. For both models, less than half of the generated feedbacks required manual evaluation. Regarding the error measures, for GPT-3.5T, both the error rate and its lower bound were high for problems P1, P2 and P3, to the extent that only considering the lower bound could discard the use of the model without the need for manual evaluation, saving time and effort. In contrast, the lower bound was reduced for P4 and P5, making manual evaluation necessary. Regarding, GPT-4T, it showed lower values for both measures with respect to GPT-3.5T. In addition to this, the lower bound for the percentage of erroneous feedbacks was always quite low, introducing again the necessity of manually evaluating the output.\nIt is important to highlight the relationship between the automatic specificity measure for RQ1 presented in Table 3 and the measures presented in Table 9 just shown. In particular, the lower bound of the rate of erroneous feedbacks is the complement of the specificity, also known as the false positive rate (FPR). A low specificity results in a higher number of FP cases, and therefore the rate of erroneous feedbacks and its lower bound will be higher. This situation would be similar to that observed for the GPT-3.5T model and problems P1 to P3. On the contrary, a model with a high specificity generates a large number of TNs that need to be evaluated manually, reducing the lower bound of the rate of erroneous feedbacks. This would correspond to the situation observed for the GPT-4T model."}, {"title": "Discussion", "content": "The following sections are devoted to discuss the three article goals specified in the introductory section: evaluating the performance of ChatGPT as a teaching assistant (Section 5.1), program-matically analyzing ChatGPT's feedback (Section 5.2) and proposing a way to operate ChatGPT within a practical application (Section 5.3)."}, {"title": "Performance of ChatGPT as a Teaching Assistant", "content": "Regarding the use of ChatGPT as a teaching assistant, its capability was evaluated in various dimensions by providing feedback for five problems, from P1 to P5, using the models GPT-3.5T and GPT-4T. Substantial differences were found between them. Specifically, GPT-4T proved to be clearly superior to GPT-3.5T in determining whether the student's code was correct or not (with accuracies above 60% and as high as 86.4% using GPT-4T compared to those around 50 or 60% with GPT-3.5T), in generating corrected versions of the code that could pass the predefined unit tests (GPT-4T generated corrected versions that passed the asserts in more that a 90% of the cases for all problems, while GPT-3.5T obtained less uniform results, including rates of 73.3% and 74.5% for problems P1 and P2), and in identifying issues actually present in the code under analysis (GPT-4T identified at least one error affecting the correctness of the code in at least a 67.5% and as high as a 91.2% of the occasions depending on the problem being considered, with GPT-3.5T achieving at most a 76.5% performance).\nIn light of the results, the GPT-3.5T model appears to be far from possessing sufficient ability to function as a teaching assistant in the context being considered. Regarding the GPT-4T model, despite offering a more solid foundation, it also could not be used without exposing the students to the risk of receiving misleading or irrelevant information regarding key aspects affecting code correctness. This problem is particularly important in the context studied in the article, as the students' ability to detect erroneous information in LLM feedback is still developing. The article incorporates a study with potential end users of the system to investigate this issue. The results of this study demonstrate that these users are not always able to correctly identify situations in which certain feedback points out issues unrelated to code correctness."}, {"title": "Automating ChatGPT's Evaluation", "content": "Regarding the possibility of automating evaluation, the use of a prompt incorporating ICL tech-niques was key in obtaining responses from ChatGPT with a consistent structure that could be programmatically analyzed. This capability was combined with the inclusion of diagnostic infor-mation about the model's performance, specifically, its decision on whether the code was correct or not. Since the accuracy of that decision can be verified by executing unit tests on the code under analysis, it was possible to automatically obtain all results related to ChatGPT's ability to understand code (RQ1) and its ability to generate corrected versions of faulty code (RQ2).\nThe semi-automatic evaluation presented in this article provides interesting advantages com-pared to conventional manual evaluation. The adopted approach allows, given an implementation problem, to automatically establish a lower bound for the rate of erroneous feedback generated by an LLM. This lower bound would have been useful, for example, to immediately discard the use of GPT-3.5T in three of the five problems considered in this study without the need for subsequent manual evaluation.\nThe information provided by the automatic measures proposed in this work is interesting be-cause it can be used to easily analyze multiple factors involved in the implementation of LLM-based"}, {"title": "Effective Operation of ChatGPT as a Teaching Assistant", "content": "The automatic handling of LLM feedback has applications not only in model evaluation but could also be useful when operating the model as an ingredient of a real software application. Whenever a student needs help with a particular implementation, only the part of the feedback explaining the code errors could be extracted and shown to the student. In this way, the possibility of showing the student LLM feedback containing the code implementing the solution would be completely avoided. Other works focused on a context similar to ours have already pointed out the tendency of ChatGPT to incorporate code, without finding a solution. For example, in [Hellas et al.(2023)], it is mentioned that the generated feedback almost always contained code, and in [MacNeil et al.(2023)], a similar situation is detailed in which the feedback includes code even though the used prompt explicitly requested otherwise.\nAccessing the structured content of the LLM would offer additional opportunities. For instance, since the error description is structured as a list of items, it would be possible to show only some of them, so as to favor a gradual construction of the solution by the student (as was mentioned in Section 3.4). Another content with pedagogic interest that could be shown to the student would be the brief description of the code generated by the LLM, which could help the student to understand what his/her code is exactly doing.\nAdditionally, automatic feedback handling could also be useful for the early diagnosis of errors in the feedback itself. To do this, it would be necessary to extract from the feedback the prediction of whether the code being analyzed is correct (see Section 3.5), as well as to run a set of appropriate unit tests on the code being analyzed. From this information, if the student requested help for correct code (because no asserts have been provided, or these do not cover all the cases of interest), it would be possible to detect the FN cases (see Table 1). Similarly, if the student requested help for incorrect code, the FP cases could be detected as well. That is, for both the FN and FP cases, the situation where the system shows incorrect feedback to the student would be prevented.\nThe only feedback that could not be treated trivially in an automated manner would be the TN cases. In these situations, the system feedback would contain a description of the errors identified in the incorrect code. However, this description would not necessarily have to be accurate, requiring manual evaluation to verify such correctness.\nInterestingly, even in these situations, the automated treatment of LLM feedback could also be useful, providing information that could perhaps be used to build some sort of predictor that offers an estimation of feedback quality. For this purpose, one possible source of inspiration to address the problem could be a discipline in the area of machine translation (MT) called quality estima-tion [Specia et al.(2009)]. MT is the process of translating text from one language to another using computer software (the original language is called source language and the destination language is called the target language). The discipline of quality estimation deals with predicting the quality of translations generated by a translation system. For this purpose, a predictor whose parameters have been trained from features extracted from the source and target sentences is built. The trained predictor is then used to predict translation quality measures for the system translations.\nIn the case of using an LLM as a teaching assistant, features useful for quality estimation could be extracted from the different elements that make up the feedback. Examples of such features could be"}, {"title": "Related Work", "content": "The articles that explore the applications of LLMs in an educational context can be classified into two groups: those that work with LLMs specialized in coding and those that work with general-purpose LLMs. LLMs specialized in coding include, for example, CodeBERT [Feng et al.(2020)], CodeT5 [Wang et al.(2021)], WizardCoder [Luo et al. (2023)] or StarCoder[Li et al.(2023)]. On the other hand, general-purpose LLMs include, in addition to ChatGPT, a wide range of highly het-erogeneous models, such as the Llama [Touvron et al.(2023a)] and Llama 2 [Touvron et al.(2023b)] family of models developed by Meta, or the LaMDA model [Thoppilan et al.(2022)] developed by Alphabet.\nWorks studying LLMs specialized in coding include [Chen et al.(2021)], where Codex, a GPT language model fine-tuned on publicly available code from GitHub is compared to other code-focused LLMs when used in code generation tasks. Additionally, in [Yuan et al.(2023)], instruction-tuned and fine-tuned LLMs are evaluated in comprehension and code generation tasks. In this case, the study considers both general purpose (e.g. Llama) and code specific (e.g. StarCoder) models.\nOn the other hand, articles centered on general purpose LLMs encompass evaluations in different settings. For instance, in [Guo et al.(2023)], the performance of ChatGPT is evaluated in fields such as legal, medical or financial. Text summaries created with ChatGPT within the medical domain are analyzed in [Gao et al.(2023)]. In [Kabir et al.(2023)], the responses provided by ChatGPT are analyzed in comparison to those from Stack Overflow. Additionally, some studies focus on studying the current limitations of ChatGPT [Borji(2023), Koco\u0144 et al.(2023), Mitrovi\u0107 et al.(2023)], finding that the model frequently generate inaccurate information. ChatGPT alternatives such as Llama or LaMDA were evaluated in their foundational papers [Touvron et al.(2023a), Touvron et al.(2023b), Thoppilan et al.(2022)].\nWithin an educational context, several works can be found that deal with evaluating LLMs. For instance, in [Jacobsen and Weber(2023)], the quality of the feedback provided by ChatGPT in the design of learning objectives is evaluated, testing different prompts for this purpose. In [Laskar et al.(2023)], ChatGPT is evaluated alongside other LLMs in a series of academic bench-marks. Similarly, in [Savelka et al.(2023)], it was studied whether ChatGPT was capable of pass-ing university programming course exams. On the other hand, there are also studies focused on working with code. In [Jalil et al.(2023)], the effectiveness of ChatGPT in teaching software testing courses is evaluated. In [MacNeil et al.(2023)], GPT-3 and GPT-4 are applied to detect logical errors in three code examples, and their performance is compared with that of begin-ner students. In [Hellas et al.(2023)], the ability of the Codex and ChatGPT models to provide feedback to pre-university students on simple programming problems is studied. More recently, in [Azaiz et al. (2024)], authors conduct work similar to that presented in [Hellas et al.(2023)], but using GPT-4 to provide feedback for programming tasks in a university context.\nFrom the studies mentioned earlier, those presented in [Hellas et al.(2023)] and [Azaiz et al.(2024)] would be the most related to ours. Nevertheless, there are significant differences. The most impor-tant one is that our work is not only focused on carrying out an evaluation of ChatGPT to generate"}, {"title": "Conclusions", "content": "This article has focused on the use of ChatGPT, specifically the GPT-3.5T and GPT-4T mod-els, as teaching assistants in the context of university introductory programming courses. Within this context, three aspects were studied: the quality of the generated feedback, the possibilities for automating evaluation, and finally, how an LLM-based learning tool could be implemented in practice.\nBased on the empirical results, we conclude that GPT-3.5T is not ready to be used within the context studied in the article. On the other hand, although GPT-4T showed superior performance, it cannot be ruled out that the feedback it generates may contain misleading information or in-formation that is not directly related to the problems present in the code under analysis. This problem is particularly important provided that a small study we conducted with potential end users of a hypothetical LLM-based learning tool demonstrates that they cannot always detect these situations.\nFurthermore, the application of ICL techniques to construct the prompts used within the study resulted in feedback with a consistent structure that could be programmatically analyzed in almost all cases. This allowed the automation of a substantial part of the evaluation process. Although manual feedback evaluation continued to be necessary for a fraction of them, a way to provide a lower bound on the fraction of erroneous feedbacks that can be obtained automatically was proposed. All these advantages can be leveraged to evaluate different LLMs, different problems on which these models are intended to be applied, different prompts (as long as they provide the feedback with an adequate minimal structure) and even multiple runs for the same prompt (to investigate stochastic effects in the LLM output).\nFinally, the structure of the generated feedback is not only useful in evaluation tasks but also in the practical implementation of a hypothetical learning support system, allowing to overcome problems reported in other works, such as handling those situations where the model's feedback contains the code with the solution to the problem (which we do not want to show to the students for pedagogic reasons).\nAs future work, we find interesting to carry out further research on the development of more advanced ICL-based prompts with improved performance and additional pedagogic properties. In addition to this, we plan to explore the feasibility of implementing feedback quality estimation techniques. These techniques would allow deciding how such feedback or any of its elements should"}]}