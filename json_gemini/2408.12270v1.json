{"title": "Variance reduction of diffusion model's gradients with Taylor approximation-based control variate", "authors": ["Paul Jeha", "Will Grathwohl", "Michael Riis Andersen", "Carl Henrik Ek", "Jes Frellsen"], "abstract": "Score-based models, trained with denoising score matching, are remarkably effective in generating high dimensional data. However, the high variance of their training objective hinders optimisation. We attempt to reduce it with a control variate, derived via a k-th order Taylor expansion on the training objective and its gradient. We prove an equivalence between the two and demonstrate empirically the effectiveness of our approach on a low dimensional problem setting; and study its effect on larger problems.", "sections": [{"title": "1. Introduction", "content": "In the field of probabilistic generative models, we find several established methods to model unknown data distribution, such as Variational Auto-Encoders (VAEs; Kingma & Welling, 2019; Vincent, 2011), Energy-Based Models (EBMs; Lecun et al., 2006; Grathwohl, 2021; Xie et al., 2022; Du et al., 2023) and Normalising Flows (Papamakarios et al., 2021). Each of these methods has been designed to model and maximise the log-likelihood of the data. However, direct optimisation of the log-density incurs important constraints on the design of these models: VAEs maximise a lower bound (ELBO) of the log-likelihood, a bound that is often not tight (Rainforth et al., 2019). EBMs address the challenging task of estimating the partition function of the density, and Normalising Flow can only train specialised neural network architecture for which the inverse can be computed. Score-based models emerge as an attractive alternative that circumvents those challenges by modelling the Stein score of the log-density, that is, the gradient of the log-density (Hyv\u00e4rinen, 2005; Vincent, 2011). In addition, it has been shown that training score-based models is equivalent, under certain assumptions, to maximise the log-likelihood of the data (Huang et al., 2021; Song et al., 2021a).\nIn practice, score-based models require no specialised architecture and are trained via a score matching loss, such as sliced score matching and denoising score matching (Song et al., 2019; 2021b). Denoising score matching is a technique similar to denoising diffusion probabilistic models (Ho et al., 2020; Luo, 2022), where the data is corrupted with a varying amount of noise and a denoiser is trained to recover the signal from the corrupted data. While very effective, this solution suffers from high variance, making optimisation challenging (Song & Kingma, 2021). We propose to use a popular variance reduction method, control variate (Owen, 2013), to address this high variance. Control variate reduces the variance by leveraging an auxiliary Monte Carlo integration problem that correlates with the original one. Control variate for score-based model has been originally introduced by Wang et al. (2020) through a linearisation of the training objective for small noise level.\nWe propose to generalise their method to k-th order Taylor approximation, which is designed for any noise value \u03c3. Our contributions include: (1) deriving a control variate with an arbitrary order Taylor polynomial; (2) proving an equivalence between controlling the training objectives and its gradient; (3) empirically demonstrating the necessity of having a regression coefficient; (4) demonstrating the effectiveness of control variate in a low dimensional problem setting; (5) studying the impact of control variate in a high dimensional case; (6) showing the limitation of Taylor based control variate."}, {"title": "2. Related work", "content": "Score matching Hyv\u00e4rinen (2005) originally introduced score matching as a method to train EBMs (Lecun et al., 2006; Grathwohl, 2021) through their Stein score, that is, the gradient of their log-density. Modelling the Stein score elegantly circumvented the need to approximate the normalization constant, a notorious challenge in the EBM literature (Grathwohl, 2021). The central idea in score matching is that aligning the model's gradients with those of the data is sufficient to learn a model from which we can sample from. Different variants of that idea exist with the most notable ones being implicit score matching (Hyv\u00e4rinen, 2005; Kingma & Cun, 2010; Martens et al., 2012), sliced score matching (Song et al., 2019) and denoising score matching (Vincent, 2011; Song & Ermon, 2019; Song et al., 2021b). Denoising score matching was originally introduced by Vincent (2011). While there were initial attempts to scale it (Kingma & Cun, 2010; Martens et al., 2012), it was not until the work of Song & Ermon (2019; 2020); Song et al. (2021b) that it has successfully scaled. Their successful insight was to combine multiple denoising score-matching objectives, each with a different amount of corruption. Concurrently, diffusion models emerged (Ho et al., 2020; Yang et al., 2023) as an equivalent method to score-based models. Together, they have successfully been applied to various data modalities of very high dimensions (Rombach et al., 2022; Xu et al., 2022; Austin et al., 2023; Harvey et al., 2022). In addition to this empirical success, Song et al. (2021a); Huang et al. (2021); Albergo et al. (2023) have laid a theoretical foundation for this learning procedure exhibiting profound links to the variational framework and to stochastic and ordinary differential equations.\nControl variate Control variate (Owen, 2013) is a variance reduction technique for Monte Carlo integration problems that has been popular in various fields, such as in variational inference (Blei et al., 2017). Ranganath et al. (2014) use control variate to reduce the variance of variational objectives; in VI, Miller et al. (2017) mitigate the variance of reparameterization gradients estimator with control variate, hence providing more reliable gradient and getting faster and more stable convergence. In a similar vein, Grathwohl et al. (2018) propose to control the variance of gradients through a surrogate neural network, in which its own gradients act as a control variate. Building on this idea, Boustati et al. (2020) learn a linear control variate to control deep Gaussian processes' variance. Geffner & Domke (2020) offer a comprehensive review of control variate for VI. In addition to VI, control variate is a popular tool in reinforcement learning, such as controlling the gradient in the REINFORCE algorithm (Williams, 1992), in advantage actor-critic (Mnih et al., 2016) or in policy optimisation (Liu et al., 2018)."}, {"title": "3. Theory", "content": "Suppose an unknown data distribution \\(P_{data}(x)\\) and a dataset consisting of i.i.d. samples \\({x_i \\in \\mathbb{R}^D\\}_{i=1}^N\\), sampled from \\(P_{data}\\). The Stein score, \\(s : \\mathbb{R}^D \\rightarrow \\mathbb{R}^D\\), \\(s(x) = \\nabla_x \\log P_{data}(x)\\), maps a data point to the gradient field of its log-density; and it is sufficient to model it to asymptotically and approximately sample from \\(P_{data}\\) using, e.g., Langevin based methods (Song & Ermon, 2019; Hyv\u00e4rinen, 2005). We use a neural network \\(s_\\theta\\) to model the Stein score, parameterised by \\(\\theta\\), and train it with denoising score matching, where we learn the score of a corrupted version of the original data distribution (Vincent, 2011).\nWe follow the approach of Song & Ermon (2019) to learn our score network \\(s_\\theta\\), parameterised by \\(\\theta \\in \\mathbb{R}^P\\) using a weighted denoising score matching objective \\(L_\\theta (z, x, \\Sigma)\\), and refer the reader to their work for the derivation of the training objective:\n\n\n\nwhere\n\n\n\nand \\(\\Sigma = {\\sigma_i}_{i=1}^n\\) is an increasing geometric sequence, \\(\\sigma \\sim U(\\Sigma)\\) is uniformly sampled from the sequence, and \\(\\lambda\\) is a positive function such that \\(\\lambda(\\sigma)L_\\theta(z, x, \\sigma)\\) has approximately a constant magnitude across the different noise levels. This training objective, unfortunately, suffers from high variance (Song et al., 2021a; Song & Kingma, 2021; Wang et al., 2020), which hinders the optimisation process. We aim to reduce the variance of the Monte Carlo estimator by constructing a control variate of that estimator.\nControl variate (Owen, 2013) is a technique to reduce the variance of an estimator \\(\\hat{\\mu} = (1/N) \\sum_{i=1}^N L(z_i)\\) of a Monte Carlo integration problem, \\(\\mu = E_z[L(z)]\\), by using a similar known problem, \\(\\gamma = E_z[C(z)]\\), where C is the control variate. Using the control variate, we construct an equivalent integration problem in Equation (3) and its regression estimator \\(\\mu_{cv,\\beta}\\),\n\n\n\n\nwhere \\(\\beta\\) is the regression coefficient and controls the scale of the control variate. When \\(\\beta = 0\\), \\(\\mu_{cv,\\beta}\\) equals the original estimator \\(\\hat{\\mu}\\). For any \\(\\beta\\), \\(\\mu_{cv,\\beta}\\) is an unbiased estimator, that is \\(E_{z_1,\\ldots,z_N}[\\mu_{cv,\\beta}] = E_{z_1,\\ldots,z_N}[\\mu] = \\mu\\) for all N. There exists an optimal value \\(\\beta_{opt}\\) for which the reduction in variance is maximised. To obtain it, we derive first the variance of \\(\\mu_{cv,\\beta}\\):"}, {"title": "3.3. Taylor series", "content": "A Taylor series represents a function s as a power series, whose coefficients are successive derivatives of s, with an additional remainder (Levi, 1967). Practically, a Taylor series approximates any function (cf. Theorem 3.1) with a polynomial, allowing control over the approximation quality through the degree of the polynomial. Taylor series is widely used in the context of perturbation theory (Holmes, 1998), where we approximate a function at a perturbed point, s(x + \\( \\epsilon \\)), which is also the context of denoising score matching.\nTheorem 3.1. Let U be an open subset of \\(\\mathbb{R}^d\\) and \\(s \\in C^l(U, \\mathbb{R}^d)\\) be a l-differentiable mapping taking value in U to \\(\\mathbb{R}^d\\). For \\(k < l\\) and a point \\(a \\in U\\), we define the Taylor polynomial \\(T_a^k\\), using a multi-index notation, such that:\n\n\nThen the mapping \\((a,x) \\rightarrow R_a^k = s - T_a^k\\) is l - k differentiable on U \u00d7 U where \\(R_a^k\\) is called the remainder. In addition, for every compact K C U and every \\(\\delta > 0\\) there exists \\(h > 0\\) such that\n\n\nRemarks We can re-write the Taylor expansion such that for x, z \u2208 U we have\n\n\nNote that in multi-index notations \\(z^\\alpha = z_1^{\\alpha_1} \\times \\ldots \\times z_d^{\\alpha_d} \\in \\mathbb{R}\\) and \\(|\\alpha| = \\alpha_1 + \\ldots + \\alpha_d\\). We sample z from \\(\\mathcal{N}(0, I)\\) and derive the expectation of the Taylor expansion with respect to z. For that, we state in Lemma 3.2 a known result on the moments of a normal distribution (Winkelbauer, 2014), that is, all the odd moments of a normal distribution equal zeros and all the even moments are known in closed form.\nLemma 3.2. Let z be sampled from a standard Gaussian distribution \\(\\mathcal{N} (0, I)\\), then all moments equal:\n\n\n\nwhere,\n\n\n\nIn addition \\(E[z z^T] = (E[z_1 z_1], ..., E[z_d z_d])^T\\) and \\(E[z \\otimes z \\otimes z^T] = \\prod_{i=1}^l \\omega_{\\alpha_i + \\delta_ik}\\), where \\(\\delta_{ik}\\) the Kronecker delta.\nTheorem 3.3. Recalling notations from Theorem 3.1 and Equation (9), we have\n\n\nTheorem 3.3 provides a closed-form expectation for any Taylor expansion where the perturbation is sampled from a Gaussian distribution. As this is the case for denoising score matching, we leverage this result to derive a control variate of the training objective."}, {"title": "3.4. A control variate on the training objective", "content": "We recall the training objective \\(L_\\theta(z, x, \\sigma) = \\frac{1}{2} ||z||^2 + ||s_\\theta(x + \\sigma z)||^2\\) and approximate the score network with a Taylor expansion of order k around the data point x. We derive the approximation and the control variate in Appendix B.1.1, and provide the result here:\n\n\n\nNote that the control variate has zero expectation with respect to z, by applying Theorem 3.3 and Lemma 3.2. In addition, we introduce a regression coefficient \\(\\beta\\) and the"}, {"title": "3.5. A control variate on the gradients", "content": "(Wang et al., 2013) shows that excessive variance in the gradient estimator leads to longer convergence and thus argues for reducing the variance of the gradients. Following that line of thought, we derive a control variate on the gradient of the training objective, \\(\\nabla_\\theta L_\\theta(z, x, \\sigma)\\), using the same methodology as in Section 3.4. We begin by deriving the gradient and approximate the score \\(s_\\theta(x + \\sigma z)\\) and its gradient \\(\\nabla_\\theta s_\\theta(x + \\sigma z)\\) with Taylor expansions. We derive the approximation and the control variate in Appendix B.2.1 and obtain the control variate for the gradient\n\n\n\nNote that each parameter of the network is individually controlled. We introduce a regression coefficient \\(\\beta_g\\) to scale the control variate for each parameter. If we set k = 0, we recover the gradient of the objective's control variate derived by (Wang et al., 2020). This hints at an equivalence between the control variate on the training objective and on the gradients, which we prove in the following section"}, {"title": "3.6. Controlling the training objective is equivalent to controlling its gradient", "content": "The previous result suggests an equivalence between the control variate of the objective and of the gradients. Indeed, Theorem 3.4 proves this claim to any k-th order Taylor approximation. We prove (Appendix B.3) that controlling the training objective is equivalent to controlling its gradients. We derive the gradients of the objective's control variate, \\(\\nabla_\\theta C^k(z, x, \\sigma)\\) and prove that it equals the gradient's control variate \\(C_{\\theta,k}^g(z, x, \\sigma)\\):\nTheorem 3.4. Let \\(C^k(z, x, \\sigma)\\) be the control variate on the training objective and \\(C_{\\theta,k}^g(z, x, \\sigma)\\) the control variate on the training objective's gradient, we have the equality:\n\nThis equality explains the benefits observed by (Wang et al., 2020). However, the regression coefficient of the objective's control variate is unrelated to that of the gradients. Since this coefficient is decisive for the quality of the control variate, we cannot expect to control the variance of the gradient through the objective alone. That comes as an unfortunate cost. Indeed, computing the regression coefficient \\(\\beta\\) for the training objective is inexpensive since it only involves computing a batch of training loss values. Conversely, computing the regression coefficient \\(\\beta_g\\) for the gradients is expensive, as it requires the gradients in batches, which is memory-intensive. In addition, we require a reliable estimate of \\(\\beta_g\\), which necessitates a large batch size."}, {"title": "3.7. A control variate for large values of \u03c3", "content": "We derived the previous control variate around the data point x and considered \\(\\sigma z\\) to be the perturbation. While that approach is valid for small values of \\(\\sigma\\), in score-based modelling, \\(\\sigma\\) ranges up to 100 (Song et al., 2021b). In such cases, this assumption does not hold, and the Taylor expansion is of poor quality, negatively impacting the training (Song & Kingma, 2021). When \\(||\\sigma z|| > ||x||\\), it is more appropriate to derive the Taylor series around \\(\\sigma z\\) and consider x to be the perturbation. We present the control variate on the training objective\n\n\nand on the gradients of the training objective\n\n\nAdditionally, we note \\(\\mu_n\\) the n-th moment of the data, \\(\\mu_n = E[x^n]\\). In our experiments, we use k = 1 and normalise the data such that \\(\\mu_1 = 0\\). Lastly, a similar derivation as the one done in Section 3.6 shows that the gradients with respect to the parameter of C equals the control variate on the gradients, \\(\\nabla_\\theta C^k(z, x, \\sigma) = C_{\\theta,k}^g(z, x, \\sigma)\\). This shows that it is equivalent to controlling the training objective or its gradients."}, {"title": "4. Experiments", "content": "We are now equipped with two sets of control variate, (\\(C_\\theta^k\\), \\(C_{\\theta,k}^g\\)) and (\\(C_\\theta^0\\), \\(C_{\\theta,0}^g\\)), and we study their ability to reduce variance. We measure the reduction in variance of the training objective as the ratio \\(\\rho_\\beta = Var(L_\\theta - \\beta C)/Var(L_\\theta)\\), and similarly we measure the reduction in the variance of the gradients and denotes it \\(\\rho_{\\beta,g}\\). A ratio smaller than one indicates a reduction in variance, and lower is better.\nWe consider three sets of experiments: (1) we explore variance reduction on a toy dataset and show a setup where control variate enables convergence to the solution; (2) we explore variance reduction on MNIST and show the limitation of control variate in this setting; (3) we study the variance reduction on an MLP of varying width and depth and justify the limitation.\nIn the following set of experiments, we train an MLP on a two-dimensional, bi-modal, Gaussian distribution generated by \\(p(x) = 1/5\\mathcal{N}(x; 5, I) + 4/5\\mathcal{N}(x; -5, I)\\). We (1) will show the necessity to control the gradients with a regression coefficient \\(\\beta\\); and (2) reduce the variance of the gradients for \\(\\sigma \\in [0.1,90]\\) using \\(C_\\theta^k\\) and \\(C_{\\theta,k}^g\\); (3) present a setup where control variate enables the convergence to the solution; (4) compare \\(C_\\theta^0\\), \\(C_\\theta^k\\), and \\(C_{\\theta,k}^g\\).\nWe will now demonstrate the importance of \\(\\beta\\) and underscore the need to control the gradients, not the training objective. We control the variance of the training objective with \\(C^0\\) with and without the regression coefficient \\(\\beta\\). In addition, we also measure the reduction in the variance of the gradients, while controlling the objective only.  reports that using \\(\\beta\\) is always beneficial, and not using it drastically increases variance.  reports that even though there is an equivalence between controlling the objective and the gradients, the variance of the gradients increases, regardless of the use of \\(\\beta\\). This increase comes from the regression coefficient \\(\\beta\\) being designed specifically for the objective rather than its gradients. This evidence supports our argument of the necessity to control the gradients and not the objective.\nWe will now proceed to control the objective's gradients with the control variate \\(C_{\\theta,k}^g\\) and \\(C_{\\theta,0}^g\\) and report our results in . This experiment shows that either the former or the latter control variate reduces the variance of the gradients for any \\(\\sigma\\) value in the range [0.1,90], showing the effectiveness of our approach in that problem setting."}, {"title": "4.3. Studying the variance reduction with respect to the irregularity of the network", "content": "The previous experiment showed poor variance reduction, one possible explanation is that the Taylor expansion poorly approximates the gradients of complex maps, such as a U-Net. However, the variance reduction was significant when using a small MLP, suggesting a faithful Taylor approximation. Thus, we hypothesise that the approximation quality decreases with increased network capacity, which we attempt to confirm in the following experiment.\n(Telgarsky, 2015; 2016) proved that the irregularity of an MLP increases exponentially with its depth and linearly with its width. Thus, for a fixed number of parameters, a deep and narrow MLP should be harder to approximate with Taylor expansion than a shallow and wide one, and so the variance reduction should be worse for the deep network than the wide one. To test that, we report the variance reduction of various MLP with N parameters, \\(MLP_{W,D}(N)\\), allocated through different width and depth combination (W, D).  reports that \\(MLP_{W,D_1}(N + n)\\) suffers from worse variance reduction than \\(MLP_{W_1,D}(N + n)\\), where \\(W_1 > W\\) and \\(D_1 > D\\). This supports (Telgarsky, 2015; 2016)'s result, that \\(MLP_{W,D_1}(N + n)\\), is more irregular than \\(MLP_{W_1,D}(N + n)\\), hinting that increasing the irregularity of the network makes it harder to approximate with Taylor expansion; consequently reducing the variance through Taylor based control variate becomes increasingly hard when dealing with large networks with complicated transformation.\nTo confirm furthermore this hypothesis, we smooth the loss landscape, which should be easier to approximate, and measure the variance reduction. We apply to every linear layer of the MLP spectral normalisation (Miyato et al., 2018), which constrains its Lipschitz constant to one. When applied to every layer of an MLP with ReLU activation, the Lipschitz constant of the MLP is also constrained to one. In Appendix C, we prove that the remainder of the Taylor approximation can be bounded by the Lipschitz constant of the k-th derivative (assuming it exists), which motivates this constraint.   reports the variance reduction of the same MLP as in , and we observe improved variance reduction, which supports the hypothesis that a smoothed landscape is easier to control."}, {"title": "4.4. Optimiser", "content": "To study the effect of the optimiser, we train an MLP on the same simple setting as earlier using both Adam and SGD. As we can see in (Figure 4 we are able to reduce more the variance when training with SGD; because more variance is available to be reduced. The decrease in variance reduction indicates that Adam suffers less from variance in the objective."}, {"title": "4.5. Discussion", "content": "We have introduced a framework to derive arbitrarily precise control variate for the training objective of a score-based model and its gradient through Taylor expansions. We have shown experimentally that in a simple controlled setting, the benefit of using the control variate to reduce the variance. Surprisingly, this benefit does not translate to the more complicated datasets and models we have tried. This un-intuitive result raises the question if the variance present in diffusion models is actually harmful to the learning objective, or actually a benefit."}, {"title": "5. Conclusion", "content": "In this study, we introduced a framework to derive arbitrarily precise control variate for the training objective of a score-based model and its gradient through Taylor expansions. In addition, we proved an equivalence between controlling the training objective and its gradients, thereby laying the foundation for future work on the relationship between reducing the variance of a training objective and its gradients. We show, theoretically and empirically that despite this equivalence, it is necessary to control the gradients variance, because of the regression coefficient that scales the control variate and allows it to take effect only when the estimator and the control variate are correlated. In this initial investigation we have shown that the quality of the control variate decreases with the complexity of the network Section 4.3, and presented evidence that higher-order expansion yields better variance reduction Section 4.2. An avenue of research would be to study the relationship between k and the variance reduction ratio \\(\\rho\\), for any order.\nWe also proved an equivalence between controlling the objective function and its gradients, with the equality: \\(\\nabla_\\theta C^k(z, x, \\sigma) = C_{\\theta,k}^g(z, x, \\sigma)\\). However, their regression coefficient \\(\\beta\\) and \\(\\beta_g\\) differ, which is why we can not control the gradient\u2019s variance through the objective\u2019s function. We hypothesise that Wang et al. (2020) were able to achieve it because their case happened to have \\(\\beta = 1\\), which happens when the network and the dataset are simple enough, for which most of the signal is included in the zero-th order term of the Taylor expansion. Estimating \\(\\beta_g\\) poses a challenge, as it requires computing the variance of the gradients"}]}