{"title": "NITRO-D: Native Integer-only Training of Deep Convolutional Neural Networks", "authors": ["Alberto Pirillo", "Luca Colombo", "Manuel Roveri"], "abstract": "Quantization has become increasingly pivotal in addressing the steadily increasing computational and memory requirements of Deep Neural Networks (DNNs). By reducing the number of bits used to represent weights and activations (typically from 32-bit floating-point to 16-bit or 8-bit integers), quantization reduces the memory footprint, energy consumption, and execution time of DNN models. However, traditional quantization methods typically focus on the inference of DNNs, while the training process still relies on floating-point operations. To date, only one work in the literature has addressed integer-only training for Multi-Layer Perceptron (MLP) architectures. This work introduces NITRO-D, a new framework for training arbitrarily deep integer-only Convolutional Neural Networks (CNNs) that operate entirely in the integer-only domain for both training and inference. NITRO-D is the first framework in the literature enabling the training of integer-only CNNs without the need to introduce a quantization scheme. Specifically, NITRO-D introduces a novel architecture integrating multiple integer local-loss blocks, which include the proposed NITRO Scaling Layer and the NITRO-ReLU activation function. Additionally, it introduces a novel integer-only learning algorithm derived from Local Error Signals (LES), utilizing IntegerSGD, an optimizer specifically designed to operate in an integer-only context. NITRO-D is implemented in an open-source Python library. Extensive experimental evaluations demonstrate its effectiveness across several state-of-the-art image recognition datasets. Results show significant performance improvements from 2.47% to 5.96% for integer-only MLP architectures over the state-of-the-art solution, and the capability of training integer-only CNN architectures with minimal accuracy degradation from -0.15% to -4.22% compared to floating-point LES.", "sections": [{"title": "1 Introduction", "content": "In recent years, the quantization of Deep Neural Networks (DNNs) has emerged as a pivotal trend in both the scientific and technological landscapes [7]. This technique aims at lowering the computational demand and memory requirements of DNNs by reducing the number of bits to represent weights and activations (i.e., typically from 32-bit floating-point to 16-bit or 8-bit integers). However, quantization frameworks for DNNs retain dependence on floating-point (FP) operations during the training. In this perspective, the aim of this paper is to address the following research question: is it possible to train arbitrarily deep Convolutional Neural Networks (CNNs) by using integer-only values in both inference and training?\nThe literature in this field is quite wide but highly fragmented, and current frameworks for integer-only DNNs can be divided into three main categories. The first one consists of applying quantization solely during inference, while the training relies on FP operations (to support the computation of the"}, {"title": "2 Related literature", "content": "Research on frameworks for the training and inference of integer-only DNNs is divided into three main categories. The first one involves applying quantization solely during inference, specifically to weights and activations. This is the most straightforward strategy and it has been extensively explored in several approaches, such as Post-Training Quantization (PQT) [12], Quantization-Aware Training (QAT) [10], BinaryConnect [4], XNOR-Net [17], TTQ [28], Banner et al. [1], Quantune [15] and QDrop [22]. However, these techniques rely on FP operations for the entire training process.\nTo address these shortcomings, the second category, known as complete quantization, extends the quantization scheme to include the training process, encompassing errors, gradients, and weight updates. This category features several significant contributions including DoReFa-Net [27], FxpNet [3], WAGEUBN [25], IM-Unpack [26], NITI [21], and Ghaffari et al. [6]. This approach often leads to negligible performance degradation compared to traditional FP training, at the cost of increased complexity and overhead of the quantization scheme. However, while these techniques utilize integer arithmetic, they are not suitable for true integer-only scenarios as they either partially rely on FP arithmetic or employ custom numeric formats to emulate it.\nFinally, the third category, known as native integer-only training, consists of employing exclusively integer-only arithmetic throughout the training and inference. This approach fully harnesses the benefits of integer arithmetic in every single operation and enables the training of DNNs in scenarios that do not support FP arithmetic. Currently, this technique has been explored only by PocketNN [20], which successfully implemented the native integer-only training for MLPs."}, {"title": "3 The proposed solution", "content": "This section introduces the proposed NITRO-D framework. In particular, Section 3.1 provides an overall description of the proposed framework. Section 3.2 details the proposed NITRO-D architecture, while in Section 3.3 the proposed NITRO-D integer-only learning algorithm is presented."}, {"title": "3.1 Overview", "content": "The proposed NITRO-D framework is designed to support the integer-only training and inference of CNNS. NITRO-D builds on the Local Error Signals (LES) algorithm [16], a variant of Back-propagation (BP) [18] that utilizes multiple local layer-wise loss functions and organizes the neural network into segments, known as local-loss blocks. The goal of these local-loss blocks is to confine the gradient propagation, and this is exploited to prevent the integer overflow typically associated with the backward pass in integer-only DNNs trained with BP. The novelty brought by NITRO-D is the adoption of LES in an integer-only training scenario, where gradients, weights, activations, and updates are confined in the integer domain.\nThe NITRO-D architecture is exemplified in Figure 1, with a focus on the integer local-loss blocks, which represent the core components of NITRO-D. These blocks are composed of the forward layers, which handle the flow of activations during the forward pass and determine the block's output, and the learning layers, dedicated solely to train the forward layers through a specific loss computed inside the block. Hence, NITRO-D enables the construction of arbitrarily deep integer-only neural networks through the adoption of multiple integer local-loss blocks, which can be trained independently.\nIn more detail, as shown in the box of Figure 1, the integer local-loss blocks consist of several components designed to ensure the stability of the training process by preventing integer overflow and maintaining stable signal propagation. Specifically, the NITRO Scaling Layer and the NITRO-ReLU activation function prevent integer overflow during the forward pass by controlling the magnitude of the activations. Additionally, the NITRO Amplification Factor ensures consistent magnitude of weight updates across the forward layers and the learning layers. As a result, NITRO-D guarantees integer-only training and inference without any quantization scheme, entirely eliminating FP operations and values from the training process. Furthermore, NITRO-D introduces IntegerSGD, an optimizer specifically redesigned for integer weights and gradients."}, {"title": "3.2 The NITRO-D architecture", "content": "The base component of the NITRO-D architecture is the integer local-loss block. During a forward pass, each block $l = \\{1, ..., L\\}$, where $L$ is the total number of integer local-loss blocks in the considered CNN, receives as input the integer activations $a_{l-1}$ of the previous block $l - 1$. It then produces as output the new integer activations $a_l$, which are then forwarded to the following block. At the same time, the integer activations $a_l$ are processed by the learning layers, which reduce their dimensionality to the number of classes of the considered classification task, producing the intermediate output $\\hat{y_l}$ that will be used for the training of the $l$-th block. The activations flow among all the $L$ blocks until they reach the output layers, where the final prediction $\\hat{y}$ is produced.\nConversely, during a backward pass, the gradients flow only within each integer local-loss block $l$. Specifically, the intermediate output $\\hat{y_l}$ and the one-hot encoded true label $y$ are used by the integer local loss function to compute the error $L_l$ and its gradient $\\nabla L_l$, which is then passed to the forward layers to calculate the weight updates.\nTo effectively enable integer-only training with the proposed NITRO-D framework, the following two layers have been defined: the NITRO Scaling Layer and the NITRO-ReLU activation function.\nNITRO Scaling Layer The NITRO Scaling Layer has been introduced to adjust the magnitude of the pre-activations $z_l$ computed by the Integer Conv2D and Integer Linear layers, before passing through the NITRO-ReLU activation function. This step is necessary to bring the pre-activation values"}, {"title": "3.3 The NITRO-D learning algorithm", "content": "NITRO-D's capability to train integer-only deep CNNs stems from its novel learning algorithm, described in this section. In more detail, this section provides an overview of the training process and then details the most critical components: the selected loss function, the novel IntegerSGD optimizer, and the NITRO Amplification Factor.\nOnce the CNN has been initialized using the proposed weight initialization methodology (see Appendix B.1), the training process begins with the forward pass, where only the blocks' forward layers and the network's output layers are used. At each iteration, a batch of the input data (X, y) is fed to the first block, which processes it through its forward layers to generate the integer activation $a_1$. This activation is then passed to the following block, with a sequential process that feeds the integer activation $a_{l-1}$ to block $l$ until the network's output layers are reached. Finally, the output layers process the activations $a_L$ to generate the final prediction $\\hat{y}$.\nAt this point, the backward pass, which involves the learning layers of each block $l$, can start. First, the gradient of the output loss $\\nabla L_L$ is computed by using $\\hat{y}$ and the target $y$, following Equation 1. This gradient is then propagated to the output layers, where it is used to update their parameters by the IntegerSGD optimizer. Subsequently, within each block $l$, the integer activation $a_l$ is fed to its learning layers to generate a local prediction $\\hat{y_l}$, used alongside $y$ to compute the gradient of the local loss $\\nabla L_l$. This gradient is used to perform the backward pass of the learning layers, updating their parameters by the IntegerSGD optimizer and obtaining the gradient of $a_l$, which is then propagated to the forward layers of that specific block $l$ and used to update their parameters using the IntegerSGD"}, {"title": "4 Experimental results", "content": "This section evaluates the performance of the proposed NITRO-D framework by using state-of-the-art image recognition datasets. The goal of this experimental campaign is to show both that NITRO-D overcomes the performance of the state-of-the-art native integer-only training framework [20] for MLPs and that its performance is close to that of traditional FP training algorithms for CNNs.\nIn particular, three image classification datasets are considered: MNIST [5], FashionMNIST [23], and CIFAR-10 [13]. The first two datasets consist of 28 \u00d7 28 grey-scale images and contain 60000 training samples and 10000 testing samples, whereas the last one consists of 32 \u00d7 32 RGB images with 50000 training samples and 10000 testing samples. While the primary focus is on image recognition tasks, it should be noted that NITRO-D supports every task applicable to MLPs and CNNs. The code implementing NITRO-D is made available to the scientific community as a public repository.\nThe experiments have been carried out on an Ubuntu 20.04 LTS workstation equipped with 2 Intel Xeon Gold 5318S CPUs, 384 GB of RAM, and an Nvidia A40 GPU. Bayesian hyperparameter optimization was utilized for each configuration to determine the optimal settings that maximize the test accuracy. Specifically, the NITRO-D hyperparameter tuning involved: the learning rate $\\gamma_{inv}$, the weight decay rate of forward layers $\\eta_{inv}^{fw}$, the weight decay rate of learning layers $\\eta_{inv}^{lr}$, the dropout rate of Integer Conv2D Blocks $p_c$, the dropout rate of Integer Linear Blocks $p_l$, and the number of input features of the learning layers $d^{lr}$. The complete set of hyperparameters used for each configuration can be found in Appendix D.\nSection 4.1 and Section 4.2 introduce the comparisons between the proposed NITRO-D framework and state-of-the-art solutions for MLP and CNN architectures, respectively. An ablation study on hyperparameter selection is detailed in Section 4.3, while final considerations on the limitations of the proposed NITRO-D framework are discussed in Section 4.4."}, {"title": "4.1 Experiment #1: comparing NITRO-D on MLP architectures", "content": "The objective of the first set of experiments is to compare the performance of MLP architectures using the proposed NITRO-D framework with the state-of-the-art integer-only MLP training framework [20], floating-point LES [16] and the traditional floating-point Backpropagation (BP) algorithm. The considered MLP architectures are the same used in [20] and [16] (see Appendix C for more details).\nThe first two rows of Table 1 highlight that the proposed NITRO-D framework outperforms the state-of-the-art solution on MLPs with both two and three hidden layers evaluated on the MNIST and FashionMNIST datasets, respectively. Moreover, the results are comparable with traditional floating-point BP, exhibiting minimal accuracy degradation of 0.64% and 1.13% due to the superior suitability of the CrossEntropy loss over RSS for classification tasks, and the proven effectiveness of the Adam optimizer [11] over SGD."}, {"title": "4.2 Experiment #2: comparing NITRO-D on CNN architectures", "content": "The goal of the second set of experiments is to show that the proposed NITRO-D framework enables the native integer-only training of deep CNNs. Experiments are conducted on the same VGG-like [19] architectures used in [16] and the performance of NITRO-D is compared with floating-point LES [16] and BP. These architectures, called VGG8B and VGG11B, are characterized by eight and eleven trainable layers, respectively (see Appendix C for more details).\nTable 2 shows that the use of CNNs, as enabled by NITRO-D, significantly surpasses the state-of-the-art of native integer-only training of MLPs, showcasing considerable improvements of 2.47% and 5.96% in the MNIST e FashionMNIST datasets, respectively.\nFurthermore, these results highlight a small accuracy degradation when compared with traditional FP learning algorithms, ranging from 0.15% to 7.05% (due to the lack of the CrossEntropy loss and the Adam optimizer). These findings validate the capability of the proposed methodology to effectively handle integer-only CNNs, showing that the NITRO-D framework is capable of performing native integer-only training of deep CNNs."}, {"title": "4.3 Ablation study: selecting the hyperparameters", "content": "This section investigates the effects and the relative importance of NITRO-D hyperparameters. Primarily, the weight decay rates for the forward layers $\\eta_{inv}^{fw}$ and the learning layers $\\eta_{inv}^{lr}$ play a significant role in mitigating overfitting by constraining the magnitude of the weights. The decision to utilize two separate decay rates originated from the empirical observation, highlighted in Appendix E.3 by Figure 3, that weights in the forward layers are generally larger than those in the learning layers. Given this disparity and the fact that the proposed weight decay mechanism acts only on the weights that are larger than the selected $\\eta_{inv}$ (as described in Section 3.3), applying the same decay rate across all layers leads to sub-optimal regularization, i.e., being either unnecessarily strong for the forward layers or negligible for the learning layers."}, {"title": "4.4 Limitations", "content": "While the proposed NITRO-D framework allows us to train integer CNNs using integer-only arithmetic, it suffers from two main limitations. First, integer-only arithmetic might lead to a sub-optimal solution compared to the same CNN trained with the traditional floating-point BP algorithm. Second, integer values used in the training process are not always guaranteed to fit within a fixed bit-width (e.g., int8). However, as detailed in Appendix E.3, experiments showed that the weights of a VGG8B model trained on the FashionMNIST dataset are constrained to the int16 datatype."}, {"title": "5 Conclusions", "content": "In this paper we introduced NITRO-D, the first framework enabling the native integer-only training of deep CNNs without the need for a quantization scheme, thereby completely eliminating FP operations from both the training and inference of DNNs. The results demonstrate that NITRO-D significantly advances the state-of-the-art of the field by facilitating the use of CNNs. Additionally, extensive comparisons with floating-point training algorithms for DNNs indicate that, despite its ability to consider integer-only operations on training and inference, NITRO-D provides similar or slightly lower classification accuracies with floating-point solutions.\nFuture work includes further theoretical advancements in the NITRO-D framework to enhance its computational efficiency by reducing the bit-width used in computations, thereby optimizing performance. Additionally, the development of an improved optimizer tailored specifically for integer-only training could help narrow the performance gap with floating-point solutions. Finally, practical application testing of NITRO-D will be conducted within the fields of tiny machine learning and privacy-preserving machine learning, validating its effectiveness in real-world scenarios."}]}