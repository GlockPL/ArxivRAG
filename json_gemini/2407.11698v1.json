{"title": "NITRO-D: Native Integer-only Training of Deep Convolutional Neural Networks", "authors": ["Alberto Pirillo", "Luca Colombo", "Manuel Roveri"], "abstract": "Quantization has become increasingly pivotal in addressing the steadily increasing computational and memory requirements of Deep Neural Networks (DNNs). By reducing the number of bits used to represent weights and activations (typically from 32-bit floating-point to 16-bit or 8-bit integers), quantization reduces the memory footprint, energy consumption, and execution time of DNN models. However, traditional quantization methods typically focus on the inference of DNNs, while the training process still relies on floating-point operations. To date, only one work in the literature has addressed integer-only training for Multi-Layer Perceptron (MLP) architectures. This work introduces NITRO-D, a new framework for training arbitrarily deep integer-only Convolutional Neural Networks (CNNs) that operate entirely in the integer-only domain for both training and inference. NITRO-D is the first framework in the literature enabling the training of integer-only CNNs without the need to introduce a quantization scheme. Specifically, NITRO-D introduces a novel architecture integrating multiple integer local-loss blocks, which include the proposed NITRO Scaling Layer and the NITRO-ReLU activation function. Additionally, it introduces a novel integer-only learning algorithm derived from Local Error Signals (LES), utilizing IntegerSGD, an optimizer specifically designed to operate in an integer-only context. NITRO-D is implemented in an open-source Python library. Extensive experimental evaluations demonstrate its effectiveness across several state-of-the-art image recognition datasets. Results show significant performance improvements from 2.47% to 5.96% for integer-only MLP architectures over the state-of-the-art solution, and the capability of training integer-only CNN architectures with minimal accuracy degradation from -0.15% to -4.22% compared to floating-point LES.", "sections": [{"title": "Introduction", "content": "In recent years, the quantization of Deep Neural Networks (DNNs) has emerged as a pivotal trend in both the scientific and technological landscapes [7]. This technique aims at lowering the computational demand and memory requirements of DNNs by reducing the number of bits to represent weights and activations (i.e., typically from 32-bit floating-point to 16-bit or 8-bit integers). However, quantization frameworks for DNNs retain dependence on floating-point (FP) operations during the training. In this perspective, the aim of this paper is to address the following research question: is it possible to train arbitrarily deep Convolutional Neural Networks (CNNs) by using integer-only values in both inference and training?\nThe literature in this field is quite wide but highly fragmented, and current frameworks for integer-only DNNs can be divided into three main categories. The first one consists of applying quantization solely during inference, while the training relies on FP operations (to support the computation of the gradients) [12, 10, 4, 17, 28, 1, 15, 22]. The second category extends quantization to the training process, exploiting further the advantages of integer arithmetic at the expense of a more complex quantization scheme [27, 3, 25, 26, 21, 6]. Despite the advancements, these approaches still typically rely on FP arithmetic or employ custom numeric formats that emulate it. The last category is the native integer-only training, which consists of employing integer-only arithmetic throughout the training and inference. Currently, the only solution [20] available in this category focuses on Multi-Layer Perceptrons (MLPs).\nThis work introduces NITRO-D, a novel native integer-only training framework that enables the training of arbitrarily deep CNNs exclusively and natively employing integer arithmetic, hence eliminating the need for explicit quantization steps or FP operations during training. NITRO-D features a novel network architecture integrating multiple integer local-loss blocks, which comprise novel components such as the NITRO Scaling Layer and the NITRO-ReLU activation function to prevent integer overflow. Moreover, NITRO-D employs a novel learning algorithm that builds on the Local Error Signals (LES) algorithm [16] to enable the integer-only training of the proposed architecture, by relying on the novel IntegerSGD optimizer and the NITRO Amplification Factor to calibrate the learning rate. To the best of our knowledge, NITRO-D is the first framework in the literature to enable the integer-only training of deep CNNs.\nNITRO-D leverages the benefits of reduced hardware complexity and lower energy consumption due to the inherent simplicity of integer arithmetic. Integer operations can be performed using smaller bit-widths (such as 8-bit and 16-bit) without compromising stability, enabling faster execution times and reduced memory requirements compared to traditional 32-bit FP operations. The framework is implemented in a new open-source Python library with support for GPU acceleration. The source code is made available to the scientific community as a public repository.\nThe contributions of this work are summarized as follows:\n1.  The introduction of the NITRO-D architecture, enabling the construction of arbitrarily deep CNNs that operate entirely in the integer-only domain;\n2.  The development of the NITRO-D learning algorithm, which enables the native integer-only training of the NITRO-D architecture, without a quantization scheme or FP operations.\nThe rest of this paper is organized as follows. Section 2 describes the related literature. Section 3 introduces the NITRO-D framework developed in this study, while Section 4 presents experimental results to evaluate its effectiveness. Conclusions and future works are finally drawn in Section 5."}, {"title": "Related literature", "content": "Research on frameworks for the training and inference of integer-only DNNs is divided into three main categories. The first one involves applying quantization solely during inference, specifically to weights and activations. This is the most straightforward strategy and it has been extensively explored in several approaches, such as Post-Training Quantization (PQT) [12], Quantization-Aware Training (QAT) [10], BinaryConnect [4], XNOR-Net [17], TTQ [28], Banner et al. [1], Quantune [15] and QDrop [22]. However, these techniques rely on FP operations for the entire training process.\nTo address these shortcomings, the second category, known as complete quantization, extends the quantization scheme to include the training process, encompassing errors, gradients, and weight updates. This category features several significant contributions including DoReFa-Net [27], FxpNet [3], WAGEUBN [25], IM-Unpack [26], NITI [21], and Ghaffari et al. [6]. This approach often leads to negligible performance degradation compared to traditional FP training, at the cost of increased complexity and overhead of the quantization scheme. However, while these techniques utilize integer arithmetic, they are not suitable for true integer-only scenarios as they either partially rely on FP arithmetic or employ custom numeric formats to emulate it.\nFinally, the third category, known as native integer-only training, consists of employing exclusively integer-only arithmetic throughout the training and inference. This approach fully harnesses the benefits of integer arithmetic in every single operation and enables the training of DNNs in scenarios that do not support FP arithmetic. Currently, this technique has been explored only by PocketNN [20], which successfully implemented the native integer-only training for MLPs."}, {"title": "The proposed solution", "content": "This section introduces the proposed NITRO-D framework. In particular, Section 3.1 provides an overall description of the proposed framework. Section 3.2 details the proposed NITRO-D architecture, while in Section 3.3 the proposed NITRO-D integer-only learning algorithm is presented."}, {"title": "Overview", "content": "The proposed NITRO-D framework is designed to support the integer-only training and inference of CNNS. NITRO-D builds on the Local Error Signals (LES) algorithm [16], a variant of Back-propagation (BP) [18] that utilizes multiple local layer-wise loss functions and organizes the neural network into segments, known as local-loss blocks. The goal of these local-loss blocks is to confine the gradient propagation, and this is exploited to prevent the integer overflow typically associated with the backward pass in integer-only DNNs trained with BP. The novelty brought by NITRO-D is the adoption of LES in an integer-only training scenario, where gradients, weights, activations, and updates are confined in the integer domain.\nThe NITRO-D architecture is exemplified in Figure 1, with a focus on the integer local-loss blocks, which represent the core components of NITRO-D. These blocks are composed of the forward layers, which handle the flow of activations during the forward pass and determine the block's output, and the learning layers, dedicated solely to train the forward layers through a specific loss computed inside the block. Hence, NITRO-D enables the construction of arbitrarily deep integer-only neural networks through the adoption of multiple integer local-loss blocks, which can be trained independently.\nIn more detail, as shown in the box of Figure 1, the integer local-loss blocks consist of several components designed to ensure the stability of the training process by preventing integer overflow and maintaining stable signal propagation. Specifically, the NITRO Scaling Layer and the NITRO-ReLU activation function prevent integer overflow during the forward pass by controlling the magnitude of the activations. Additionally, the NITRO Amplification Factor ensures consistent magnitude of weight updates across the forward layers and the learning layers. As a result, NITRO-D guarantees integer-only training and inference without any quantization scheme, entirely eliminating FP operations and values from the training process. Furthermore, NITRO-D introduces IntegerSGD, an optimizer specifically redesigned for integer weights and gradients."}, {"title": "The NITRO-D architecture", "content": "The base component of the NITRO-D architecture is the integer local-loss block. During a forward pass, each block $l = {1, ..., L}$, where $L$ is the total number of integer local-loss blocks in the considered CNN, receives as input the integer activations $a^{l-1}$ of the previous block $l-1$. It then produces as output the new integer activations $a^l$, which are then forwarded to the following block. At the same time, the integer activations $a^l$ are processed by the learning layers, which reduce their dimensionality to the number of classes of the considered classification task, producing the intermediate output $\\hat{y}_l$ that will be used for the training of the $l$-th block. The activations flow among all the $L$ blocks until they reach the output layers, where the final prediction $\\hat{y}$ is produced.\nConversely, during a backward pass, the gradients flow only within each integer local-loss block $l$. Specifically, the intermediate output $\\hat{y}_l$ and the one-hot encoded true label $y$ are used by the integer local loss function to compute the error $L_l$ and its gradient $\\nabla L_l$, which is then passed to the forward layers to calculate the weight updates.\nTo effectively enable integer-only training with the proposed NITRO-D framework, the following two layers have been defined: the NITRO Scaling Layer and the NITRO-ReLU activation function.\nNITRO Scaling Layer The NITRO Scaling Layer has been introduced to adjust the magnitude of the pre-activations $z^l$ computed by the Integer Conv2D and Integer Linear layers, before passing through the NITRO-ReLU activation function. This step is necessary to bring the pre-activation values inside the operational range of the NITRO-ReLU, which is [-127, 127]. In particular, the NITRO Scaling Layer takes as input the integer pre-activations $z^l \\in \\mathbb{Z}$ and produces as output the rescaled integer pre-activations $\\hat{z}^l \\in [-127, +127]$, computed as follows:\n$\\hat{z}^l = \\frac{z^l}{SF_l}$\nwhere $SF_l \\in \\mathbb{N}$ represents the scaling factor. Identifying an appropriate $SF_l$ is critical to ensure that the magnitudes of the pre-activations $z^l$ are appropriately scaled to the intended range. To achieve this goal, we delve into how a generic input $a^{l-1}$ is affected by the weights $W_l$ of the considered layer $l$. Let $l$ be an Integer Linear layer, and let\n$z_i^l = a^{l-1} W_i = \\sum_{j=1}^{M_{l-1}} a_j^{l-1} W_{ij}$\nbe the i-th element of the layer's output $z^l$, where $W_i$ is the i-th column of the weight matrix $W_l$, and $M_{l-1}$ is the size of the input activations $a^{l-1}$. An upper bound $b_{z_i}$ for the bit-width required to represent $z_i$ can be defined as:\n$b_{z_i} = O(b_{a^{l-1}} + b_{W_i} - 1) = O(b_{a^{l-1}} + b_{W_i} - 1 + log_2(M_{l-1}))$\nwhere $b_{a^{l-1}}$ and $b_{W_i}$ are the upper bounds for the bit-width required to represent each element of the input $a^{l-1}$ and of the weights matrix $W_l$, respectively. More specifically, for each block $l = {2, ..., L}$, the input $a^{l-1}$ consists of the activations of the previous block $l - 1$, i.e., the output of a NITRO-ReLU activation function, always confined in the interval [-127, +127]. For this reason, $b_{a^{l-1}} = 8$. In the first layer (i.e., $l = 1$), $a^{l-1} = a^0 = x$ and thus, by employing the proposed pre-processing method (see Appendix B.2), $b_x = 8$. Assuming that the weights do not exceed the same range (i.e., $b_{W_i} = 8$), the computation of $b_{z_i}$ simplifies to $O(15 + log_2(M_{i-1}))$. Following this analytical approach, which can be extended to convolutional layers, the scaling factor $SF_l$, employed in the NITRO Scaling Layer to revert the magnitudes in $z^l$ to the range [-127, +127], is computed as:\n$SF_l = \\begin{cases} \\frac{128 \\times M_{l-1}}{2^8} & \\text{ for linear layers} \\\\ \\frac{128 \\times K_{l-1}^2 \\times C_{l-1}}{2^8} & \\text{ for convolutional layers} \\end{cases}$\nwhere $K_{l-1}$ and $C_{l-1}$ are respectively the kernel size and the number of input channels of the filters of the Integer Conv2D layer that precedes the NITRO Scaling Layer, and $2^8$ represents the range of values in the considered interval.\nConversely, the backward pass of the NITRO Scaling Layer is implemented using the straight-through estimator [2] due to the non-differentiability of the integer division. Indeed, scaling each feature by a uniform factor does not alter the direction of the activations vector. Hence, for each block $l$, $\\delta_i^l = \\nabla L_l$ and $\\delta_o^l = \\delta_i^l$, with $\\nabla L_l$ being the gradient of the loss function and $\\delta_o^l$ the gradient propagated by the NITRO-ReLU activation function.\nNITRO-ReLU Activation Function The proposed NITRO-ReLU is a specifically designed LeakyReLU [24] activation function that natively handles integer values. It is divided into four segments $i = {0, ..., 3}$, with two segments derived from the traditional LeakyReLU, and two additional ones created by limiting the output to the range [-127, +127]. Specifically, the NITRO-ReLU takes as input the integer rescaled pre-activations $\\hat{z}^l \\in [-127, +127]$ produced by the NITRO Scaling Layer, and produces as output the integer activations $a^l \\in [-127, +127]$ as follows:\nNITRO-ReLU(x) = \\begin{cases} \\text{max}{x,-127} & x 0 \\\\ \\frac{\\alpha_{inv}}{2 \\alpha_{inv}}  &-127 < x < 0 \\\\ 63 & 0 < x < 127 \\\\ 127 & x > 127 \\end{cases}\nThen, $\\mu_{int8}$ is computed as the mean of all $\\mu_{int8}^i$ for $i = {0, ..., 3}$. This normalization is motivated by the fact that maintaining zero-centered values during the training process is beneficial for its convergence [14]. Given that the NITRO-ReLU operates within an integer-bounded range, this behavior is obtained by pre-computing the mean value $\\mu_{int8}$ and subtracting it from the output of the function, thereby centering the activations. Moreover, in an integer-only framework, this approach is particularly crucial since implementing Batch Normalization [9] would be infeasible due to the non-differentiability of the integer division and the challenges associated with the need to accurately compute the standard deviation, which under integer arithmetic can easily lead to overflow."}, {"title": "The NITRO-D learning algorithm", "content": "NITRO-D's capability to train integer-only deep CNNs stems from its novel learning algorithm, described in this section. In more detail, this section provides an overview of the training process and then details the most critical components: the selected loss function, the novel IntegerSGD optimizer, and the NITRO Amplification Factor.\nOnce the CNN has been initialized using the proposed weight initialization methodology (see Appendix B.1), the training process begins with the forward pass, where only the blocks' forward layers and the network's output layers are used. At each iteration, a batch of the input data (X, y) is fed to the first block, which processes it through its forward layers to generate the integer activation $a^1$. This activation is then passed to the following block, with a sequential process that feeds the integer activation $a^{l-1}$ to block $l$ until the network's output layers are reached. Finally, the output layers process the activations $a^L$ to generate the final prediction $\\hat{y}$.\nAt this point, the backward pass, which involves the learning layers of each block $l$, can start. First, the gradient of the output loss $\\nabla L_o$ is computed by using $\\hat{y}$ and the target $y$, following Equation 1. This gradient is then propagated to the output layers, where it is used to update their parameters by the IntegerSGD optimizer. Subsequently, within each block $l$, the integer activation $a^l$ is fed to its learning layers to generate a local prediction $\\hat{y}_l$, used alongside $y$ to compute the gradient of the local loss $\\nabla L_l$. This gradient is used to perform the backward pass of the learning layers, updating their parameters by the IntegerSGD optimizer and obtaining the gradient of $\\delta^l$, which is then propagated to the forward layers of that specific block $l$ and used to update their parameters using the IntegerSGD optimizer and the NITRO Amplification Factor. It is noteworthy that, during the backward pass, the training of all the integer local-loss blocks operates independently of each other, allowing them to be executed in parallel and enhancing the efficiency of the training process. Once the backward pass of all the integer local-loss blocks is finished, the next iteration of the training process commences.\nLoss function The computational limitations associated with integer arithmetic also extend to the selection of the loss function. NITRO-D utilizes the Residual Sum of Squares (RSS), defined as:\n$\\begin{aligned} L_l &= \\frac{1}{2} (\\hat{y}_l - y)^2, \\\\ \\nabla L_l &= \\hat{y}_l - y \\end{aligned}$\nwhere $\\hat{y}_l$ represents the output of the learning layers of the l-th block or of the output layers of the network, and y is the supervised information processed through one-hot encoding. This choice is motivated by the straightforward nature of the function's derivative, which is computed as the subtraction between the model's prediction $\\hat{y}$ and the target output y.\nIntegerSGD optimizer The proposed IntegerSGD (described in Algorithm 1) extends the traditional SGD optimizer to operate in the native integer-only training scenario by also introducing an ad-hoc weight decay procedure. Firstly, the learning rate $\\gamma$ and the weight decay rate $\\lambda$ required in the traditional SGD are redefined as $\\gamma_{inv} = \\frac{1}{\\gamma} \\in \\mathbb{N}$ and $\\lambda_{inv} = \\frac{1}{\\lambda} \\in \\mathbb{N}$. Consequently, the multiplications involving $\\gamma$ and $\\lambda$ are replaced by integer divisions by their inverse $\\gamma_{inv}$ and $\\lambda_{inv}$.\nIn the integer-only domain, computing weight decay introduces specific challenges, primarily because the truncation effect of integer division makes the impact of this operation on the weight updates nearly negligible. To address this issue, we reformulated the update rule to separately account for the contributions of the gradient $\\delta_t$ (see Line 3) and the weight decay term $\\frac{W_{t-1}}{\\lambda_{inv}}$. Furthermore, the division between the weight decay rate $\\lambda_{inv}$ and the learning rate $\\gamma_{inv}$ has been replaced by a new composite decay rate $\\Omega_{inv} = \\frac{\\gamma_{inv}}{\\lambda_{inv}}$, as formalized in Line 5. Thanks to this approach, the effect of weight decay on the CNN's weights becomes surprisingly straightforward, since only the weights whose absolute value exceeds $\\Omega_{inv}$ are subject to penalization. In fact, for every element $w_i^l$ of the weight matrix $W_l$ of layer $l$ such that $|w_i^l| < \\Omega_{inv}$, the weight decay term will result in zero, indicating that no regularization will be applied to its corresponding update.\nNITRO Amplification Factor Within a block, the gradients flowing through the forward layers and the learning layers are characterized by different magnitudes. Specifically, the learning layers receive exactly the gradient of the loss $\\nabla L_l$, whereas the forward layers receive an amplified version of $\\delta_l$ due to the matrix multiplication with the weights $W_{il}$ of the Integer Linear layer in the learning layers. Consequently, the learning rate $\\gamma_{inv}^{ll}$ that is suitable for the learning layers is revealed to be inadequate for the forward layers, leading to disproportionately large weight updates and the divergence of the optimization process."}, {"title": "Experimental results", "content": "This section evaluates the performance of the proposed NITRO-D framework by using state-of-the-art image recognition datasets. The goal of this experimental campaign is to show both that NITRO-D overcomes the performance of the state-of-the-art native integer-only training framework [20] for MLPs and that its performance is close to that of traditional FP training algorithms for CNNs.\nIn particular, three image classification datasets are considered: MNIST [5], FashionMNIST [23], and CIFAR-10 [13]. The first two datasets consist of 28 \u00d7 28 grey-scale images and contain 60000 training samples and 10000 testing samples, whereas the last one consists of 32 \u00d7 32 RGB images with 50000 training samples and 10000 testing samples. While the primary focus is on image recognition tasks, it should be noted that NITRO-D supports every task applicable to MLPs and CNNs. The code implementing NITRO-D is made available to the scientific community as a public repository.\nThe experiments have been carried out on an Ubuntu 20.04 LTS workstation equipped with 2 Intel Xeon Gold 5318S CPUs, 384 GB of RAM, and an Nvidia A40 GPU. Bayesian hyperparameter optimization was utilized for each configuration to determine the optimal settings that maximize the test accuracy. Specifically, the NITRO-D hyperparameter tuning involved: the learning rate $\\gamma_{inv}$, the weight decay rate of forward layers $\\eta_{inv}^{fw}$, the weight decay rate of learning layers $\\eta_{inv}^{lr}$, the dropout rate of Integer Conv2D Blocks $p_c$, the dropout rate of Integer Linear Blocks $p_l$, and the number of input features of the learning layers $d^{lr}$.\nSection 4.1 and Section 4.2 introduce the comparisons between the proposed NITRO-D framework and state-of-the-art solutions for MLP and CNN architectures, respectively. An ablation study on hyperparameter selection is detailed in Section 4.3, while final considerations on the limitations of the proposed NITRO-D framework are discussed in Section 4.4."}, {"title": "Experiment #1: comparing NITRO-D on MLP architectures", "content": "The objective of the first set of experiments is to compare the performance of MLP architectures using the proposed NITRO-D framework with the state-of-the-art integer-only MLP training framework [20], floating-point LES [16] and the traditional floating-point Backpropagation (BP) algorithm. The considered MLP architectures are the same used in [20] and [16] (see Appendix C for more details).\nThe first two rows of Table 1 highlight that the proposed NITRO-D framework outperforms the state-of-the-art solution on MLPs with both two and three hidden layers evaluated on the MNIST and FashionMNIST datasets, respectively. Moreover, the results are comparable with traditional floating-point BP, exhibiting minimal accuracy degradation of 0.64% and 1.13% due to the superior suitability of the CrossEntropy loss over RSS for classification tasks, and the proven effectiveness of the Adam optimizer [11] over SGD."}, {"title": "Experiment #2: comparing NITRO-D on CNN architectures", "content": "The goal of the second set of experiments is to show that the proposed NITRO-D framework enables the native integer-only training of deep CNNs. Experiments are conducted on the same VGG-like [19] architectures used in [16] and the performance of NITRO-D is compared with floating-point LES [16] and BP. These architectures, called VGG8B and VGG11B, are characterized by eight and eleven trainable layers, respectively (see Appendix C for more details).\nTable 2 shows that the use of CNNs, as enabled by NITRO-D, significantly surpasses the state-of-the-art of native integer-only training of MLPs, showcasing considerable improvements of 2.47% and 5.96% in the MNIST e FashionMNIST datasets, respectively.\nFurthermore, these results highlight a small accuracy degradation when compared with traditional FP learning algorithms, ranging from 0.15% to 7.05% (due to the lack of the CrossEntropy loss and the Adam optimizer). These findings validate the capability of the proposed methodology to effectively handle integer-only CNNs, showing that the NITRO-D framework is capable of performing native integer-only training of deep CNNs."}, {"title": "Ablation study: selecting the hyperparameters", "content": "This section investigates the effects and the relative importance of NITRO-D hyperparameters. Primarily, the weight decay rates for the forward layers $\\eta_{inv}^{fw}$ and the learning layers $\\eta_{inv}^{lr}$ play a significant role in mitigating overfitting by constraining the magnitude of the weights. The decision to utilize two separate decay rates originated from the empirical observation, highlighted in Appendix E.3 by Figure 3, that weights in the forward layers are generally larger than those in the learning layers. Given this disparity and the fact that the proposed weight decay mechanism acts only on the weights that are larger than the selected $\\Omega_{inv}$ (as described in Section 3.3), applying the same decay rate across all layers leads to sub-optimal regularization, i.e., being either unnecessarily strong for the forward layers or negligible for the learning layers."}, {"title": "Limitations", "content": "While the proposed NITRO-D framework allows us to train integer CNNs using integer-only arithmetic, it suffers from two main limitations. First, integer-only arithmetic might lead to a sub-optimal solution compared to the same CNN trained with the traditional floating-point BP algorithm. Second, integer values used in the training process are not always guaranteed to fit within a fixed bit-width (e.g., int8). However, as detailed in Appendix E.3, experiments showed that the weights of a VGG8B model trained on the FashionMNIST dataset are constrained to the int16 datatype."}, {"title": "Conclusions", "content": "In this paper we introduced NITRO-D, the first framework enabling the native integer-only training of deep CNNs without the need for a quantization scheme, thereby completely eliminating FP operations from both the training and inference of DNNs. The results demonstrate that NITRO-D significantly advances the state-of-the-art of the field by facilitating the use of CNNs. Additionally, extensive comparisons with floating-point training algorithms for DNNs indicate that, despite its ability to consider integer-only operations on training and inference, NITRO-D provides similar or slightly lower classification accuracies with floating-point solutions.\nFuture work includes further theoretical advancements in the NITRO-D framework to enhance its computational efficiency by reducing the bit-width used in computations, thereby optimizing performance. Additionally, the development of an improved optimizer tailored specifically for integer-only training could help narrow the performance gap with floating-point solutions. Finally, practical application testing of NITRO-D will be conducted within the fields of tiny machine learning and privacy-preserving machine learning, validating its effectiveness in real-world scenarios."}]}