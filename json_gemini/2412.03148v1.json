{"title": "Fine-grained User Behavior Simulation on Social Media Based on Role-playing Large Language Models", "authors": ["Kun Li", "Chenwei Dai", "Wei Zhou", "Songlin Hu"], "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in role-playing tasks. However, there is limited research on whether LLMS can accurately simulate user behavior in real-world scenarios, such as social media. This requires models to effectively analyze a user's history and simulate their role. In this paper, we introduce FineRob, a novel fine-grained behavior simulation dataset. We collect the complete behavioral history of 1,866 distinct users across three social media platforms. Each behavior is decomposed into three fine-grained elements: object, type, and content, resulting in 78.6k QA records. Based on FineRob, we identify two dominant reasoning patterns in LLMs' behavior simulation processes and propose the OM-CoT fine-tuning method to enhance the capability. Through comprehensive experiments, we conduct an in-depth analysis of key factors of behavior simulation and also demonstrate the effectiveness of OM-CoT approach\u00b3.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have attracted significant attention for their ability to engage in role-playing. These models can be guided by predefined role profiles to generate conversations that align with a character's speaking style[32], knowledge[16], and personality traits [5]. Recently, numerous agent frameworks have been introduced to extend LLMs' capabilities beyond simple dialogue generation[24,1,23]. However, accurately simulating human-like behaviors poses a substantial challenge, particularly in complex real-world scenarios.\nIn this paper, we focus on simulating real social media users' behavior. Researches have shown that the primary motivations for social network users are self-presentation and self-disclosure[20]. These self-presentations are consciously or unconsciously re-flected in all aspects of user behavior[13].Considering that, we are going to break down each behavior into three fine-grained aspects: object (the target or recipient of the behavior), type (the nature of the behavior), and content (the specific details). An ideal role-playing agent should be able to accurately predict each aspect of user behavior."}, {"title": "Related Work", "content": ""}, {"title": "Role-Playing LLM", "content": "Recently, numerous RP-LLMs have been designed for conversational applications and have already found commercial uses like Character.ai4. Researchers collect a wide range of dialogue datasets to support the study, including the data from real-life individuals [9,6] or fictional characters from novels [5,2]. Beyond this, techniques such as in-context learning (ICL)[29] and retrieval-augmented generation (RAG)[15] have been employed. Additionally, supervised fine-tuning on targeted dialogue datasets[21] and methods like LORA[28] have further enhanced RP-LLMs' role-playing capabilities. Despite these advancements, research in RP-LLMs remains in its early stages, with a primary focus on mimicking conversations."}, {"title": "LLM for User Behavior simulation", "content": "Researchers have recognized that LLMs are not only adept at mimicking conversation but also capable of simulating complex behaviors[18,24,4,23]. For instance, [22] demonstrated that LLMs can mimic real users' preferences to movie recommendations. [8] explored the use of reinforcement learning algorithms to optimize recommendation systems based on user feedback simulated by LLMs. The work by [25] is particularly relevant, which introduced the \"LIFECHOICE\" dataset to assess LLMs' ability to make"}, {"title": "FineRob Dataset", "content": ""}, {"title": "Data Collecting", "content": "Our goal is to explore how LLMs simulate the behaviors of real internet users. To achieve this, we focused on social media platforms, including Twitter (now X), Reddit, and the Chinese question-and-answer website Zhihu. From these platforms, we collected extensive behavioral histories of real users, as shown in the first part of Figure 2. Unlike other role-playing tasks, we emphasize fine-grained behavior simulation in real-world scenarios. A detailed comparison is provided in Table1.\nPrinciples Our data collection strategy is guided by several key principles. (1) Pop-ularity: We focused on mainstream, widely discussed topics and scenarios to ensure the dataset reflects a representative sample of user behaviors. (2) Diversity: We include a broad range of user profiles and behavioral patterns to enhance the generalizability of our findings.(3)Activity: We select users who are active within the community and engage in various types of behavior, helping to minimize data contamination from social bots, fake accounts, or other non-human users.\nUser Selection To collect our dataset, we target active users by selecting them from trending topics or communities displayed on aggregation page of each platform. For Reddit, we choose popular posts from the top 20 communities and filter participants"}, {"title": "Fine-Grained Behavior Building", "content": "Next, we convert the raw user timelines into a fine-grained behavior simulation QA dataset with multiple-choice format. Specifically, each behavior record is broken down into three elements: object (the recipient of the behavior), type (the nature of the behavior), and content (the specific details of the behavior). This process is illustrated in the middle section of Figure 2. 2.\nA significant challenge lies in constructing valid alternative options for each behavior element in the multiple-choice format. The behavior type options are relatively straight-forward, as platforms typically have predefined actions like \"Post\" and \"Comment.\"\nThese behavior types can be found in Table 2. For the behavior's object and content options, we construct a candidate set based on the user's active times and communities."}, {"title": "Methodology", "content": ""}, {"title": "Preliminary Analysis", "content": "We conduct preliminary experiments using a zero-shot Chain of Thought (CoT) approach. Our goal is to understand the reasoning processes LLMs use in behavior simulation tasks. Our analysis reveal two primary reasoning patterns. The first, termed role stereotype-based reasoning, derives outcomes by analyzing character profiles. The second, observation and memory-based reasoning, involves analysing all observed options and linking them to similar past scenarios. Examples of these patterns are illustrated in Figure 3.\nIn our comparison, we find that more advanced models, such as GPT-40, tend to prefer the \"observation and memory-based reasoning\" pattern, leading to more accurate simulations. To investigate this further, we conduct a quantitative analysis of the similarity between CoT reasoning text and each parts of prompt: behavior history, observed options, and role info. The results, shown in Figure 4, reveal an interesting insight: A higher similarity to role profiles, which may involve more character analysis, does not always lead to better behavior simulations. Instead, focusing on historical data and observed options proves to be more effective in improving simulation accuracy."}, {"title": "OM-CoT Finetune", "content": "Based on these findings, we propose a straightforward yet effective method called OM-CoT Finetune (Observation and Memory-based Chain of Thought Finetune) to enhance behavior simulation accuracy in LLMs. This method promotes the \"observation and memory-based reasoning\" pattern by explicitly integrating observation and memory analysis into the CoT reasoning. The approach involves three steps, as illustrated in the right part of Figure 2.\nOracle CoT Generation First, we use a powerful large language model to generate CoT reasoning. To prevent error propagation, we adopt an oracle setting, where the correct answer is provided in the input prompt. This ensures the model references the correct behavior during reasoning. We carefully adjust the prompt to make sure the generated CoT does not inadvertently reveal the correct answer.\nReorganize CoT with special tokens Next, we introduce two special tokens:  and . A smaller LLM reorganizes the CoT results by wrapping observation-based analysis within  and historical behavior analysis within . At the end of each reasoning process, the model explicitly states the final behavior decision (e.g., \"Therefore, the behavior type is A.Comment\").\nSFT with Enhanced Dataset Finally, we perform Supervised Fine-Tuning (SFT) on the LLM using the reorganized dataset. The training optimizes for standard language model loss, guiding the model to effectively utilize the special tokens along with system prompts to control the CoT process. We create 60K instruction training data for OM-COT fine-tuning, which is also available in the codebase."}, {"title": "Experiment", "content": ""}, {"title": "Settings", "content": "Models We evaluate a total of nine large language models on FineRob. This in-cludes three commercial LLMs, ChatGPT-3.5-turbo-0125/GPT-40-mini/GPT-408,as well as six open-source LLMs:: Mistral-7b-Instruct\u2079[12],Llama3-8b-Instruct10[7], Solar-10.7b-Instruct[14], Yi-1.5-34B-Chat[27], Baichuan2-13B-Chat\u00b9\u00b9[26],and Qwen2-72B-instruct12. The last two models is specialize for Chinese context.\nBaselines We conduct extended experiments on the Mistral-7b-Instruct and Solar-10.7b-Instruct models using four baseline methods: zero-shot, few-shot, standard-CoT fine-tune, and OM-CoT fine-tune. In the few-shot setup, we include a reasoning example created by GPT-40, which follows the \"observation and memory-based reasoning\" pattern. The standard-CoT fine-tuning method uses un-reorganized CoT data without special tokens. By comparing these approaches, we aim to evaluate how different training and prompting methods affect LLM behavior simulation performance.\nPrompts The prompts were similarly structured across all baseline methods and consisted of four main parts: (1) a task description instructing the model to simulat three behavior elements while role-playing a specific character, (2) the role's profile, which included username, self-description, and areas of interest, (3) behavior history, detailing the target, type, content, and timing of past behaviors, and (4) method-specific instructions and output format requirements. For example, in OM-CoT, the model was instructed to use the  and  tokens for analysis based on observation and memory.\nImplementation Details We utilize LoRA [11] for efficient parameter fine-tuning, setting \u03b1 to 1.0 and \u03b7 to 0.025. All training are conducted with fp16 mixed-precision on 4 \u00d7 A100 GPUs over 10 epochs with LLama-factory13[30]. For inference, we use vLLM14 to accelerate the process, employing sampling decoding with a temperature of 0.1. The F1 score serves as the evaluation metric across all experiments. To mitigate the impact of randomness inherent in LLMs, we run three trials and compute the mean and standard deviation, ensuring more reliable results."}, {"title": "Main Result", "content": "We first compare the behavior simulation capabilities of the main LLMs under the same zero-shot setting, as shown in Table 3. Following this, we perform extended experiments to highlight the advantages of the OM-CoT fine-tuning method, with the results presented in Table 4. Next, we will discuss some conclusions drawn from the main results."}, {"title": "Discussion", "content": "In this subsection, we will conduct ablation studies to further analyze the key factors that influence behavior simulation performance. Specifically, we seek to address the following three research questions."}, {"title": "LLMs Struggle with Short-Behavior Tasks, Even When Fine-Tuned.", "content": "A notable result emerge from the Reddit dataset, particularly in the behavior content simulation task, where fine-tuning methods failed to yield performance improvements. Upon further investigation, we find that Reddit content is often brief and lacks clear indicators of user characteristics. This suggests that current language models still struggle to differentiate subtle variations in tone and punctuation within behaviors (e.g., \"Good work\" vs. \"Pretty Nice!!\")."}, {"title": "Which part of the prompt is the most important?", "content": "Behavior simulation prompt includes role's basic information, interests, and past behaviors. To assess the importance of each component, we conduct ablation experiments by removing individual parts from the input prompts. Table 5 shows the results on the Twitter dataset, demonstrating how these components affect model performance. As highlighted in our preliminary experiments, role history is the most influential, especially for OM-CoT fine-tuned models that are trained to analyze historical behaviors. Removing role history leads to a notable performance drop. On the other hand, the effect of basic information and interests varies across different behavior elements. For example, excluding basic info and interests has minimal impact on simulating behavior object and type, but they are useful for accurately simulating behavior content. Notably, while OM-CoT emphasizes observation and memory-based reasoning, it still integrates character profile analysis within the CoT process, which relies on role information in the input prompt."}, {"title": "Does adding more user history input improve the accuracy of behavior simulation?", "content": "In the main experiment, we consistently choose the 30 most recent behavior history as the input. However, a plausible hypothesis suggests that including more behavior history could enhance behavior simulation, provided it fits within the model's token limit. To explore this, we evaluate the performance across different history window sizes, ranging from 10 to all entries(average 74)16, as shown in Figure 5. Contrary to intuition, adding more user behavior history does NOT consistently improve behavior simulation. We find that performance peaks at around 30 behavior entries, with additional history leading to a decline in accuracy. We hypothesize that increasing historical data introduces more noise, making it harder for the model to focus on relevant information. This finding aligns with human decision-making, where recent actions tend to be more influential. Interestingly, OM-CoT-FT models display greater stability with increased input history, showing promise for handling longer behavior sequences."}, {"title": "Do both the  and  special token work effectively?", "content": "To investigate this, we conducted ablation experiments by selectively removing content enclosed by the special tokens  (analysis) and  (memory) during the reasoning process. This was done by either adjusting the system prompt or excluding these tokens during the decoding process. The results are shown in Table 6. The experimental findings reveal that removing either  or  leads to a decrease in behavior simulation performance, highlighting the importance of both tokens in the CoT reasoning process. However, the influence of these tokens varies across different sub-tasks. For behavior type simulation, the model relies more on analyzing available candidate options (), whereas behavior content simulation depends more on recalling and reproducing historical behaviors (). In the case of behavior object simulation, both observation and memory are equally important. This suggests that each sub-task requires a different balance between reasoning based on current observations and past behavior records to achieve optimal performance."}, {"title": "Ethics", "content": "Current LLMs have achieved highly realistic role-playing abilities, sometimes indistin-guishable from real human. This development unlocks potential applications in areas like companionship, entertainment, and education, where human-like agents could lead to significant progress. However, these capabilities also pose risks in social networks, such as the spread of propaganda, misinformation, and the proliferation of malicious bots. This study aims to enhance LLM' ability to accurately simulate human roles while also providing insights into identifying behavioral patterns and thinking styles of such LLM-driven social bots."}, {"title": "Conclusion", "content": "In this work, we introduce FineRob, a new fine-grained behavior simulation dataset designed to explore how LLMs understand and simulate real human behaviors. We collect user behavior history from three social media platforms and break down each behavior record into three key elements: object, type, and content. Preliminary experiment on FineRob reveals two dominant reasoning patterns: role stereotype-based, observation and memory-based. The latter proves to be more effective in behavior simulation, leading us to propose OM-CoT method that explicitly integrates observation and memory into the reasoning process. We conduct comprehensive experiments on nine mainstream LLMs"}]}