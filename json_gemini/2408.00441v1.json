{"title": "Focus, Distinguish, and Prompt: Unleashing CLIP for Efficient and Flexible Scene Text Retrieval", "authors": ["Gangyan Zeng", "Yuan Zhang", "Jin Wei", "Dongbao Yang", "Peng Zhang", "Yiwen Gao", "Xugong Qin", "Yu Zhou"], "abstract": "Scene text retrieval aims to find all images containing the query text from an image gallery. Current efforts tend to adopt an Optical Character Recognition (OCR) pipeline, which requires complicated text detection and/or recognition processes, resulting in inefficient and inflexible retrieval. Different from them, in this work we propose to explore the intrinsic potential of Contrastive Language-Image Pre-training (CLIP) for OCR-free scene text retrieval. Through empirical analysis, we observe that the main challenges of CLIP as a text retriever are: 1) limited text perceptual scale, and 2) entangled visual-semantic concepts. To this end, a novel model termed FDP (Focus, Distinguish, and Prompt) is developed. FDP first focuses on scene text via shifting the attention to the text area and probing the hidden text knowledge, and then divides the query text into content word and function word for processing, in which a semantic-aware prompting scheme and a distracted queries assistance module are utilized. Extensive experiments show that FDP significantly enhances the inference speed while achieving better or competitive retrieval accuracy compared to existing methods. Notably, on the IIIT-STR benchmark, FDP surpasses the state-of-the-art model by 4.37% with a 4 times faster speed. Furthermore, additional experiments under phrase-level and attribute-aware scene text retrieval settings validate FDP's particular advantages in handling diverse forms of query text. The source code will be publicly available at https://github.com/Gyann-z/FDP.", "sections": [{"title": "1 INTRODUCTION", "content": "Since text is ubiquitous in natural scenes and conveys rich semantic information, scene text understanding has received a lot of attention for decades [36, 46, 53]. Different from common scene text understanding tasks such as text detection [32, 33, 38, 44], text recognition [6, 30, 31, 56], and end-to-end text spotting [14, 19, 23, 43], Scene Text Retrieval (STR) is an emerging topic that only focuses on text of interest, i.e., searching images containing a given query text from an image gallery. As such, STR is beneficial for many applications like product image search, program recommendation, and electronic book archives management [9, 10, 48].\nWith the aid of Optical Character Recognition (OCR) techniques, STR has made remarkable progress in recent years [12, 15, 41]. Nevertheless, existing methods still suffer from two critical limitations. First, as illustrated in Fig.1, there is a dilemma of how to balance retrieval accuracy (mAP scores) and inference speed (FPS). Specifically, most STR models follow the two-stage pipeline that first detects text regions and then compares these regions with the query text for retrieval. In this pipeline, either an exact text detection or recognition process is required, which significantly slows down the inference speed. Comparatively, Gomez et al. [11] achieve fast text retrieval using a single-shot CNN architecture, but it is limited by relatively low retrieval accuracy. Second, in real life, the query text that people expect to retrieve is often in various forms. However, current efforts rely on the local retrieval mechanism that treats word instances as query units, leading to inherent inflexibility in phrase-level or attribute-aware scene text retrieval (see Fig.2).\nRecently, Contrastive Language-Image Pre-training (CLIP) [34] has become a powerful foundation model for learning cross-modal representations and enabling zero-shot transfer to downstream tasks [7, 18, 21]. More remarkably, several works [20, 37] have demonstrated CLIP also implies OCR capabilities via pre-training on massive image-text pairs. It gives us a new insight: can we explore the intrinsic potential of CLIP for efficient and flexible STR? To this end, we investigate the advantages and deficiencies of CLIP in the STR task through an empirical study. A surprising finding is that simply applying the frozen CLIP can even achieve better accuracy than some dedicated STR models. Moreover, thanks to CLIP's simple network design, the retrieval speed is also superior. Despite these impressive results, there are still two challenges that hinder CLIP from being an ideal retrieval engine: 1) Limited text perceptual scale. As the image resolution input into CLIP is very limited (e.g., 224x224), and scene text usually occupies only a small part of the scene image, a lot of text may be ignored or misrecognized by CLIP. 2) Entangled visual-semantic concepts. Due to the prevalence of text in natural images, there is confusion between visual text and semantic concepts in CLIP's cognition [27]. Its specific impact on STR is that the CLIP-based retrieval model performs much better on content words (e.g., \"coffee\", \"hotel\") than on function words (e.g., \"and\", \"with\") because only content words represent exact semantics. Besides, the model may have difficulty distinguishing similar words (e.g., \"advice\" and \"advise\") because their semantics are close in the embedding space.\nIn this paper, we propose a model named FDP (Focus, Distinguish, and Prompt) to tackle the above challenges. Concretely, for each image in the gallery, we firstly force CLIP to focus on scene text by 1) applying the rough text localization results to refine the model attention on images, and 2) leveraging CLIP's well-aligned vision-language representations to prob text knowledge. Then, given a query text, we distinguish whether it is a content word or a function word via unsupervised clustering and determine the retrieval solution accordingly. Finally, a semantic-aware prompting scheme is developed, which converts the query text into a learnable prompt and ranks images by computing their similarity scores with each image. In addition, a distracted query assistance strategy is involved during training to resist the negative effects of similar words. Extensive experiments on three benchmarks show that FDP can achieve better or competitive accuracy compared to existing models with a faster inference speed. To further evaluate the effectiveness of STR methods over arbitrary-length query text, we introduce a new benchmark of phrase-level scene text retrieval (PSTR). Meanwhile, qualitative experiments regarding attribute-aware scene text retrieval are conducted. These experimental results demonstrate the generalization and flexibility of FDP.\nOverall, the main contributions of this work are three-fold:\n1) To the best of our knowledge, it is the first work to directly extend CLIP for scene text retrieval. We summarize both the advantages and deficiencies of CLIP in dealing with the STR task and propose a novel FDP (Focus, Distinguish, and Prompt) method.\n2) In contrast to previous works, FDP steers the prior knowledge from CLIP and eliminates the complicated text detection/recognition process, thus achieving a better trade-off between retrieval accuracy and inference speed. Notably, FDP outperforms the state-of-the-art method [47] by 4.37% mAP score with a 4 times faster speed on the IIIT-STR benchmark.\n3) We evaluate existing STR methods in phrase-level and attribute-aware scene text retrieval settings, further verifying the superiority of FDP in handling diverse forms of query text."}, {"title": "2 RELATED WORK", "content": "Most of the early STR approaches tend to follow the OCR pipeline [1, 40, 50]. They first take two separate steps of text detection and recognition to extract words in each image, and then match these words with the query word for retrieval. For instance, Mishra et al. [28] first investigate the STR task, proposing to rank all images based on the ordering and positioning of localized characters. Jaderberg et al. [15] perform text spotting with a CNN network and evaluate the occurrences of the query word within the spotted words. However, those straightforward attempts could not obtain satisfactory performance and are also not efficient. To solve this problem, Gomez et al. [11] leverage a compact representation named Pyramidal Histogram of Character (PHOC) [2] and propose a single-shot CNN architecture that simultaneously predicts text proposals and corresponding PHOCs. In this way, the STR task can be completed by a simple nearest neighbor search. Considering current handcraft representations (including PHOC) still cannot well reflect the distance between text and image modalities, recent methods are dedicated to mining better similarity measures. TDSL [41] establishes an end-to-end network that jointly optimizes text detection and cross-modal similarity learning. To mitigate the gap across different modalities, Wen et al. [47] propose to cast STR as an image-to-image matching problem. Although better retrieval accuracy is achieved, it comes at the cost of inference speed."}, {"title": "2.2 Exploring CLIP's OCR Capabilities", "content": "Vision-language models pre-trained on web-scale data have been demonstrated to exhibit certain OCR capabilities [13, 24, 25, 49, 54]. As reported in [34], the CLIP model shows favorable OCR performance in rendered text images. To further mine the underlying rationales, [20] thoroughly inspects different versions of CLIP. This work uncovers that CLIP suffers from severe text spotting bias because many captions in CLIP's training dataset tend to parrot the visual text embedded within images. Through orthogonal projections of the learned representation space into \"learn to spell\" and \"forget to spell\" parts, [27] disentangles such bias to some extent. Besides, LoGoPrompt [37] finds that synthetic text images are good visual prompts for vision-language models, which can be used to improve image classification performance.\nInspired by these observations, several works aim to enhance OCR tasks by transferring knowledge from CLIP. In the field of scene text recognition, CLIP4STR [55] designs a two-branch framework in which the recognition results are predicted by the visual branch and then refined by the cross-modal branch. CLIP-OCR [45] resorts to the knowledge distillation technique and guides the recognition with both visual and linguistic knowledge from CLIP. In the field of scene text detection, TCM [51] integrates CLIP with existing text detectors, leading to obvious performance improvements in domain adaptation and few-shot capabilities. However, CLIP merely acts as an auxiliary module in these works. Whether it is possible to turn CLIP directly into a scene text reader (spotter or retriever) remains an unexplored problem."}, {"title": "3 FDP METHOD", "content": "The overview of the proposed FDP framework is illustrated in Fig.3. Given a query text (Q = \"house\"), FDP fulfills the STR task with a pipeline of \"Focus, Distinguish, and Prompt\"."}, {"title": "3.1 Focus", "content": "Considering CLIP is pre-trained on conventional image-text pairs and thus lacks fine-grained awareness of visual text information, the first step of FDP is directing CLIP to focus on scene text. To be specific, for each image from the gallery, we first square it to obtain the input image \\(I_{input} \\in \\mathbb{R}^{L\\times L}\\) (L is the image length), i.e., perform zero-padding to make the shorter side match the longer side. The goal is to avoid the loss of image content caused by the center cropping operation during CLIP's preprocessing. Then, the frozen ResNet-based vision encoder of CLIP is employed to extract the global feature \\(f_{global} \\in \\mathbb{R}^{C\\times H\\times W}\\) of \\(I_{input}\\), where C, H and W stand for the channel, height and width dimensions respectively. Based on this global feature, two modules including dynamic attention shift and text knowledge probing are proposed to highlight scene text information and address the limited text perceptual scale problem.\nDynamic Attention Shift. The limitation of input resolution is an intractable problem encountered by pre-trained vision-language models. It greatly impairs scene text understanding performance because text often occupies a very small part of the image. Existing efforts resolve this problem by subdividing into image patches [17], retraining a vision encoder [3], or processing in the frequency domain [8], which are not efficient. Instead, in this work we find that it may be enough to give the model a glimpse of the rough area where text is clustered. To this end, we employ text detection supervision to train a lightweight text localization network, and then utilize the normalized probability map to reweigh the features in the average pooling layer. Specifically, as the multi-head attention layer in CLIP's vision encoder loses the 2D image information, we first introduce a reformulated head following [57] to recover the 2D convolutional image feature \\(f_{conv} \\in \\mathbb{R}^{E\\times H\\times W}\\), where E is the embedding dimension in CLIP. Then, the localization probability map \\(I_{loc} \\in \\mathbb{R}^{H\\times W}\\) is obtained via a learnable convolutional layer. We train the text localization network via a class-balanced cross-entropy loss, given by:\n\\[L_{loc} = -\\beta Ylog(I_{loc}) \u2013 (1 \u2013 \\beta) (1 \u2013 Y)log(1 \u2013 I_{loc})\\]\nwhere Y is the ground-truth localization map generated by the text detection annotation, and \u03b2 is a balancing factor defined as:\n\\[\\beta = 1-\\frac{|Y|}{L*L}\\]\nAfter that, the predicted localization probability map is adopted as a new attention mask to dynamically refine the attention applied to the global feature. The details of the dynamic attention shift module are illustrated in Fig.4. CLIP uses Transformer-style multi-head attention to perform average pooling, where the 2D global feature is first flattened into a 1D sequence and then generates a key-value pair to interact with the globally average-pooled feature (query)."}, {"title": "3.2 Distinguish", "content": "Several works [4, 20] have revealed that the CLIP model exhibits inherent bias towards visual text, e.g., an image of \"dog\" may be recognized as \"cat\" by placing the text that says \"cat\". The reason is that the captions CLIP pre-trained with are often simple repetitions of text embedded in images. In this work, we further observe that this bias is essentially the entanglement between visual and semantic concepts. To be specific, we select 500 words with the highest frequency from the MLT [29] training set, and then group their CLIP language embeddings into two clusters via K-Means. The t-SNE visualization result is depicted in Fig.5 (a). As can be seen, these words naturally fall into two clusters, namely, the content words and the function words. Between them, the content words have explicit semantics, usually appearing next to the thing they represent, so they exhibit strong visual-semantic entanglement. In contrast, the function words generally appear in the captions as conjunctions, which do not correspond to specific concepts. To investigate this effect on STR, we evaluate the retrieval accuracy of content words and function words respectively by applying the frozen CLIP models on SVT [42] dataset, as shown in Fig.5 (b). It is obvious that for the three CLIP models with different capacities, the mAP scores on function words are significantly lower than those on content words. This inspires us that different retrieval solutions should be taken on these two clusters. Thus, before each retrieval process, we pre-distinguish whether the given query is a content word or a function word. Without the need for specialized tools, this can be easily achieved via unsupervised clustering, namely predicting which K-Means cluster the query text belongs to."}, {"title": "3.3 Prompt", "content": "Prompt tuning is a promising paradigm that aims to adapt the knowledge from a pre-trained model to a target domain [22, 35, 52]. Mirroring the success of prompt tuning in natural language processing and cross-modal learning, we leverage it to facilitate CLIP for efficient text retrieval.\nSemantic-aware Prompting. To improve retrieval performance on both content words and function words, we develop a semantic-aware prompting scheme. Inspired by CoOp [58], we introduce two sets of learnable context vectors to serve the retrieval of content words and function words respectively. Formally, the text prompts fed to the frozen CLIP language encoder are organized as:\n\\[P_c = [p_{c_0},..., p_{c_{M}}, Q]\\]\n\\[P_f = [p_{f_0},..., p_{f_{N}}, Q]\\]\nwhere \\(P_c\\) and \\(P_f\\) denote the prompts for content words and function words respectively. M and N are the length of learnable context vectors, and Q represents the query text.\nThen, the CLIP language encoder outputs the prompt feature \\(f_{prompt}\\). We calculate the cosine similarity between \\(f_{img}\\) and \\(f_{prompt}\\) to measure the pairwise similarity score between the input image and query text, i.e., S(I, Q). The symmetric cross-entropy loss \\(L_{align}\\) over a batch is applied to contrastively align the matched (I, Q) pairs.\nDistracted Queries Assistance. Due to the limited input resolution and the visual-semantic entanglement, CLIP can perceive text to some extent, but it indeed lacks the ability of fine-grained character discrimination. To address this problem, a distracted queries assistance module is proposed, which teaches the FDP model to better recognize text during training. In particular, for a query text Q, we utilize a dictionary to generate K (set to 5) distracted queries \\(Q^1, Q^2, ...Q^K\\) that have the smallest edit distances with Q. These distracted queries are taken as hard negative samples which are also converted to text prompts and fed into the CLIP language encoder, predicting similarities scores S(I, Qk), k = 1, ..., K. Meanwhile, the edit distance of each distracted query from the ground-truth query D(Q, Q*) is calculated. Subsequently, we convert these K sets of similarity scores and edit distances into probability distributions and compute the KL divergence between them as the training loss \\(L_{distract}\\). The objective is to maximize the similarity scores of the distracted queries that are close to the ground-truth while minimizing the similarity scores of the distracted queries that are far from the ground-truth."}, {"title": "3.4 Optimization", "content": "The proposed FDP is trained with the following loss functions:\n\\[L = \\lambda_1 L_{loc} + \\lambda_2 L_{align} + \\lambda_3 L_{distract}\\]\nwhere \u03bb1, \u03bb2, and \u03bb3 are used to balance these losses, which are all set to 1 in our implementation.\nDuring inference, the distracted queries assistance module is removed. Given a query text Q, images in the gallery are ranked according to the predicted similarity score S(I, Q). When extending our FDP model to phrase-level or attribute-aware scene text retrieval settings, Q is directly assigned the corresponding form of query, and the inference process remains unchanged."}, {"title": "4 EXPERIMENTS", "content": "Datasets\nIIIT Scene Text Retrieval (IIIT-STR) [28] is a popular benchmark that contains 10000 images and 50 predefined queries. The images are collected using Google image search, so this dataset has a large variability in text fonts, styles, and viewpoints.\nStreet View Text (SVT) dataset [42] is a collection of natural street scenes. It consists of 100 training images and 249 testing images. All annotated words (427 words) in the test set are employed as the query text.\nTotalText [5] is a scene text dataset consisting of 1255 training and 300 testing images. The 60 words that occur most frequently in the test set are selected as queries.\nMulti-lingual Scene Text (MLT)-Eng is the English subset of MLT [29], which includes about 5000 images of natural scenes.\nIn our experiments, MLT-Eng is only used for training the proposed model. IIIT-STR, SVT, and TotalText are the testing datasets. It should be noted that as CLIP's potential is fully explored, 900k synthetic training images used in [41, 47] can be saved in FDP.\nThe query terms in existing datasets are all single words. To validate whether the STR models can be generalized to arbitrary-length query text, we introduce a new Phrase-level Scene Text Retrieval (PSTR) dataset. To build it, we select 36 phrases that occur frequently in life as queries, each containing 2 to 4 words, e.g., \"school bus\", \"handle with care\". For each query, we collect 15 images from TextOCR dataset [39] and Google image search respectively. In total, PSTR includes 1080 images and 36 query text."}, {"title": "4.2 Implementation Details", "content": "Based on CLIP with different capacities, we build several versions of FDP models, as summarized in Tab.1. As the input image size supported by CLIP is very limited, we expand the image size in FDP. However, directly expanding the image size makes the position embedding inherited from CLIP incompatible. To tackle it, we propose a new learnable position embedding whose parameters are initialized with the nearest interpolation of original parameters.\nWe optimize FDP using Adam [16] optimizer with an initial learning rate of 2e-3. The batch size is 48, and the number of training epochs is about 8. For fair comparisons, our experiments are implemented with Pytorch. All FDP models are trained on an NVIDIA A6000 GPU and tested on an NVIDIA 1080 GPU."}, {"title": "4.3 Comparison with Existing Methods", "content": "In this section, we compare FDP with existing methods on three STR benchmarks, i.e., IIIT-STR, SVT, and TotalText. As a task to pursue practical applications, the inference speed of STR is undoubtedly very important, while previous methods are subject to the balance of retrieval accuracy and inference speed. In this paper, we first investigate employing the frozen CLIP model directly as the retrieval engine. As reported in Tab.2, it is surprising that CLIP already exhibits some retrieval capabilities even though it has not been specially trained on STR tasks. In particular, CLIP-RN50 obtains 52.93% and 65.07% mAP scores on the IIIT-STR and SVT datasets respectively, which even exceeds several dedicated STR models [12, 28] at a much faster speed (76.32 FPS).\nBased on this observation, FDP is proposed to better unleash CLIP's potential for the STR task. On the IIIT-STR benchmark, we can notice that FDP-S initialized with the CLIP-RN50 base model boosts the mAP score by 28.84% (52.93%->81.77%), achieving an appealing result of 81.77%. Meanwhile, the inference speed is also superior (45.11 FPS), even faster than PHOC-based methods [11, 26]. When upgrading the model to a large size, FDP-L significantly outperforms the competitive model [47] by 12.06% (77.40%->89.46%) at a comparable speed. Compared with IIIT-STR, the query terms of SVT and TotalText contain many function words and often occupy small areas in images, which are more challenging for STR models. Nevertheless, even without a complicated network design, FDP also reaches competitive performance on these datasets. To further boost the retrieval accuracy, we attempt to integrate the subdivision enhancement strategy here, i.e., subdividing the input image into 4 equal patches and combing the outputs of these patches. The mAP scores are improved by 2.03%, 1.55%, and 2.84% on IIIT-STR, SVT, and TotalText, outperforming existing STR methods.\nTo provide intuitive analyses of FDP in comparison with previous methods, a typical example is visualized in Fig.6 (a). Given the query word \"adobe\", TDSL relies entirely on character composition for retrieval. If the scene text is blurry or small, it can easily be misrecognized. Besides, text-like patterns tend to interfere with model decisions. Instead, our FDP model takes full advantage of visual context information, returning the desired images from an image gallery. From the rank@7 and rank@10 images retrieved by FDP-S, we notice that the proposed method can recall images where the query text is not so salient."}, {"title": "4.4 Ablation Study", "content": "Overall results. In Tab.3, a detailed ablation experiment is conducted to verify the effectiveness of each module. We start by training a model that only utilizes the new learnable position embedding, whose mAP scores on IIIT-STR and SVT are 75.74% and 79.97% respectively. It reveals that enlarging the image size (i.e., enhancing the text perceptual scale) is of critical importance for STR. Based on this, we gradually add the proposed modules and observe that each module brings noticeable improvements. In the \"Focus\" step, the dynamic attention shift and text knowledge probing modules can be considered to highlight scene text information from visual space and semantic space respectively. They bring 2.64% and 0.55% gains on the IIIT-STR dataset, which are proven to be effective. In particular, as IIIT-STR contains a large number of images without any text, the \"Focus\" step has a more significant effect on the IIIT-STR dataset than on the SVT dataset. Then, we study the effect of different prompt strategies. When simply adopting the learn-able prompt method in [58] (#4), the mAP scores reach 80.07% and 81.03% on these two datasets. In contrast, we claim that content words and function words should be distinguished and exploit different customized prompts. Following this idea, our semantic-aware prompting scheme improves the performance to 81.27% and 81.94%. Further, by adding the training strategy of distracted queries assistance, 81.77% and 82.56% mAP scores are finally obtained.\nAnalysis of the context length. In the semantic-aware prompting module, the hyperparameters M and N determine the context length for content words and function words respectively. As shown in Tab.4, we evaluate the model performance on the SVT dataset to analyze the effect of these hyperparameters. According to the results, FDP reaches the best performance when M = 4 and N = 2. It may suggest that function words contain less semantics than content words, so they do not require complicated descriptions of context. More ablation experiment results can be found in the supplementary material."}, {"title": "4.5 Extending to More Retrieval Settings", "content": "Phrase-level scene text retrieval. In reality, the scene text that people expect to retrieve is often of arbitrary length, such as \"ice cream\", \"do it yourself\". To validate the generality of our method over arbitrary-length query text, we evaluate FDP and several STR models on PSTR. For the comparison models [11, 41], since they can only accept word instances, we split each phrase into words, search them separately, and then average the corresponding similarity scores. As shown in Tab.5, FDP is more flexible than existing STR models in phrase-level retrieval. Specifically, the PHOC-based method [11] only achieves 68.01% mAP score on PSTR. We speculate this is because many split words are too short (e.g., \u201cdo\u201d in \u201cdo it yourself\") to be accurately retrieved by the PHOC-based search algorithm. Although TDSL [41] can get 89.40% with the simple splitting operation, it is inherently flawed due to the local retrieval mechanism. From Fig.6 (b), we can see that for the query text \"no smoking\", TDSL may return images containing \"no engine\" (rank@3) or \"no softener\" (rank@5), which do not meet retrieval expectations. In addition, due to the dense text distribution in the PSTR dataset, these OCR-based comparison models run slower than on IIIT-STR. In contrast, the FDP-S model reaches 92.28% mAP score on PSTR, outperforming existing methods by great margins. More importantly, as FDP does not rely on text detection or recognition processes, the retrieval speed will not be affected.\nAttribute-aware scene text retrieval. Considering that people often query scene text with fine-grained attributes for more accurate search results, we further explore extending FDP to the attribute-aware scene text retrieval setting. We design some attribute-related queries and search corresponding images from the IIIT-STR dataset. Several typical retrieval examples are illustrated in Fig.7. These results manifest that the CLIP-based FDP model is naturally suitable for attribute-aware scene text retrieval because it takes advantage of CLIP's prior knowledge. FDP can well deal with attribute-related information such as color, font, and even position of scene text, returning images that users want. Admittedly, this is not available for conventional OCR-based STR models."}, {"title": "5 CONCLUSION", "content": "In this paper, we explore CLIP's intrinsic potential for efficient and flexible scene text retrieval. An OCR-free retrieval model named FDP (Focus, Distinguish, and Prompt) is proposed, in which the \"Focus\" design highlights scene text information hidden in CLIP while \"Distinguish\" and \"Prompt\" designs further overcome the negative effects caused by visual-semantic entanglement. Experimental results on three datasets demonstrate the effectiveness of our proposed modules and show that FDP achieves a better trade-off between retrieval accuracy and inference speed. In addition, FDP can easily generalize to other settings like phrase-level and attribute-aware scene text retrieval, which are more practical for requirements in real scenarios."}, {"title": "1 PSTR DATASET", "content": "To validate whether the STR models can be generalized to arbitrary-length query text, we introduce a new Phrase-level Scene Text Retrieval (PSTR) dataset. Specifically, we select 36 phrases that occur frequently in life as queries, each containing 2 to 4 words. All queries are listed in Tab.1."}, {"title": "2 MORE ABLATION STUDIES", "content": "Analysis of the predefined probe. The goal of the predefined probe is to stimulate the text-related knowledge hidden in CLIP. In Tab.2, we conduct an ablation study of the predefined probe on the IIIT-STR benchmark. The results show that if the predefined probe is removed, the mAP score decreases from 81.77% to 79.58%. Furthermore, different strings are utilized to generate language embeddings that interact with the image attention feature. Compared to the \"without predefined probe\" baseline, these text-related probes can enhance the performance. Among them, \"scene text\" contributes to the best accuracy, implying that in CLIP's training data, the plain text \"scene text\" may appear frequently with the scene text content from images."}, {"title": "3 COMPARISON OF OCR-BASED AND\nOCR-FREE METHODS", "content": "To better demonstrate the superiority of our OCR-free STR framework, we compare it with a CLIP-based OCR pipeline. Specifically, this pipeline leverages TCM for text detection and CLIP-OCR for text recognition, which performs retrieval considering edit distances between the given query and spotted words. As shown in Tab.7, the OCR-based pipeline reaches 72.45% mAP score on IIIT-STR, lagging behind FDP-S by 9.32%. A case study is provided in Fig.1, from which we can observe that, compared to the OCR-based STR pipeline, FDP-S avoids the accumulation of errors from detection and recognition, thus ranking the images reasonably."}, {"title": "4 MORE QUALITATIVE RESULTS", "content": "To further support our claim in the paper, we provide more qualitative examples retrieved by FDP-S in Fig.2. The proposed FDP method could deal with scene text in various scenarios, and has ability to recalling complicated cases such as multi-oriented and curve text. In particular, with the aid of our semantic-aware prompting technique, the retrieval accuracy on the function words (such as \"the\") is significantly strengened compared to the original CLIP model. Nevertheless, for the challenging SVT and TotalText benchmarks, FDP still suffers from some limitations. On the one hand, when the query text to be retrieved is extremely tiny and meanwhile there are many disturbing words appearing in the image, the model has difficulty locating the target text. On the other hand, FDP still tends to return the images containing the similar words (e.g., \"port\" vs. \"sport\", \"since\u201d vs. \u201cvenice\u201d) from the image gallery, suggesting that the ability of fine-grained character discrimination needs to be further improved. We would like to go on with exploration for addressing these problems in the future."}]}