{"title": "LASSI: An LLM-based Automated Self-Correcting Pipeline for Translating Parallel Scientific Codes", "authors": ["Matthew T. Dearing", "Yiheng Tao", "Xingfu Wu", "Zhiling Lan", "Valerie Taylor"], "abstract": "This paper addresses the problem of providing a novel approach to sourcing significant training data for LLMs focused on science and engineering. In particular, a crucial challenge is sourcing parallel scientific codes in the ranges of millions to billions of codes. To tackle this problem, we propose an automated pipeline framework, called LASSI, designed to translate between parallel programming languages by bootstrapping existing closed- or open-source LLMs. LASSI incorporates autonomous enhancement through self-correcting loops where errors encountered during compilation and execution of generated code are fed back to the LLM through guided prompting for debugging and refactoring. We highlight the bi-directional translation of existing GPU benchmarks between OpenMP target offload and CUDA to validate LASSI.\n\nThe results of evaluating LASSI with different application codes across four LLMs demonstrate the effectiveness of LASSI for generating executable parallel codes, with 80% of OpenMP to CUDA translations and 85% of CUDA to OpenMP translations producing the expected output. We also observe approximately 78% of OpenMP to CUDA translations and 62% of CUDA to OpenMP translations execute within 10% of or at a faster runtime than the original benchmark code in the same language.", "sections": [{"title": "I. INTRODUCTION", "content": "The problem addressed in this paper is that of providing a novel approach to sourcing significant training data for Large Language Models (LLMs) focused on science and engineering, a key objective of the Trillion Parameter Consortium (TPC) [1]. TPC brings together international communities that encompass three areas: (1) those working to advance AI methods with a focus on LLMs, (2) those with existing or emerging exascale platforms necessary for training LLMs, and (3) those who will use the resulting LLMs to address problems in science and engineering. In particular, this paper is focused on the need to source parallel scientific codes.\n\nTo adequately source parallel scientific codes, it is important to have a framework that can easily generate millions to billions of codes in different programming languages widely used in the sciences, such as FORTRAN, C++, CUDA, HIP, OpenMP, Julia, and SYCL. Further, the generated code should be \"good\" in the sense that it should compile, execute, be performant, and correct. The framework should be automated to facilitate the large number of codes needed. To address this problem, we present LASSI, an LLM-based Automated Self-correcting pipeline for generating parallel Scientific codes. Currently, LASSI features a pipeline that automatically generates code, test for compilation, and checks execution. Future work will consider the correctness and performance.\n\nIn this paper, LASSI is used to translate codes between OpenMP and CUDA executed on an NVIDIA A100 GPU. For the case of OpenMP code, we use the offload to GPU feature. We provide the results of using four LLMs with ten codes from the HeCBench suite [2], which serve as a basis for comparison of the execution time of the code generated by LASSI. We observe the importance of incorporating the capability within the pipeline to provide feedback from compilation and execution errors for self-correcting. Further, we observe that approximately 78% of the translations from OpenMP to CUDA and 62% of the translations from CUDA to OpenMP are within 10% of or faster than the original codes in HeCBench in the same programming language.\n\nThe main contributions of this paper are the following:\n\n\u2022\tPresent a novel approach, called LASSI, to automate the generation of parallel scientific codes. LASSI includes feedback from compilation and execution errors for self-correction and can be easily modified to incorporate different LLMs.\n\u2022\tProvide results from the use of LASSI for code translation with HeCBench that demonstrate solid performance of the LASSI-generated codes.\n\nThe remainder of the paper is organized as follows. The subsequent section discusses related work, followed by a description of LASSI in \u00a73. Next, \u00a74 outlines the benchmark codes and the four LLMs used for experimenting with LASSI, followed by the actual results presented in \u00a75. The paper summary is given in \u00a76."}, {"title": "II. RELATED WORK", "content": "Many existing commercial and open-source LLMs, such as GPT-4 [3], Codestral [4], StarCoder [5], and Code Llama [6], are trained on massive collections of shared code primarily developed in widely-used programming languages like Python and JavaScript. Specialized open-source models are being released frequently that are further tuned to general purpose coding tasks, e.g., Wizard Coder [7] and DeepSeek Coder v2 [8]. However, these state-of-the-art code-centric models still lack sufficient training data for parallel scientific codes, especially those typically utilized for HPC scientific applications. This gap may stem from the limited volume of scientific developers who actively share code in the HPC domain, compared to the broader industries of web and mobile app development, which typically do not require the use of HPC for code execution.\n\nLLMs routinely demonstrate highly effective general capabilities when incorporating context, such as domain-specific knowledge [9] or even an entertaining personality type [10], which guides generated responses specific to the context. The learned representations of human language by an LLM are harnessed along with this context to reduce so-called hallucination. The addition of context involves enhancing the probabilities during inference towards responses that include the provided source content over other content learned in the weights of the model. The technique of retrieval augmented generation (RAG) [11] leverages this LLM behavior. We adopt a similar yet simplified approach, aiming to enhance an LLM's performance in parallel scientific code translation by employing carefully crafted prompts imbued with contextual knowledge and tailored programming expectations.\n\nA recent study evaluated the capabilities of state-of-the-art LLMs in generating parallel code [12] and developed a prompting benchmark and metrics to evaluate LLM performance in this domain. The study noted significantly poorer responses in parallel code generation, often resulting in inefficient resource utilization compared to serial code. Nichols et al. evaluated LLMs with direct prompting without providing additional context or domain knowledge. In contrast, LASSI incorporates expanded prompting strategies with a programming language-specific dictionary into the pipeline to enhance the core capabilities of LLMs for translating parallel scientific codes.\n\nThe concept of a self-improving LLM prompting framework for enhancing generative AI performance is shared by the DSPy programming model [13]. In DSPy, hard-coded prompt chains are replaced by a text transformation graph that enables the construction of optimized language model invocation strategies and prompts derived from a program. Following a similar inspiration, we develop an automated self-correcting pipeline to enhance model inference and improve overall generative performance.\n\nAn intended outcome of the LASSI automated pipeline is to support the generation of synthetic parallel codes for training new foundational LLMs. We highlight the recent release of NVIDIA's Nemotron-4 340B open-access suite of LLMs [14]. These very large models are competitive with recent large Llama-3 70B [15], Mixtral 8x22B [16], and Qwen-2 72B [17] models in common benchmarks, demonstrating their value in synthetic data generation for improving the quality of pre-training processes. These very large LLMs provide promising justification for future steps as the LASSI pipeline scales to generate massive parallel codes for training LLMs focused on science, such as AuroraGPT, part of TPC [18]."}, {"title": "III. LASSI: AUTOMATED SELF-CORRECTING PIPELINE", "content": "We propose LASSI, an automated pipeline framework designed to translate between parallel programming languages by bootstrapping existing closed- or open-source LLMs. LASSI incorporates domain knowledge as a core feature, offering the advantage of tailoring prompts that guide the LLM towards synthesizing desired programming languages and performance outcomes. This is particularly beneficial given that the model does not have a high-quality foundation training in parallel coding techniques. Furthermore, the impact prompting has on the quality of an LLM response is significant. The prompting strategies and techniques presented here suggest reasonable performance, which were developed through extensive trial-and-error.\n\nWith a pipeline that is LLM-agnostic, we acknowledge that continuous effort is required to optimize prompt content, especially as new LLMs are released in the future. An intriguing avenue towards this end is to leverage an LLM to help design its prompts [19], an approach we explored and incorporate into our solution to improve generated code results.\n\nFigure 1 summarizes the LASSI architecture with the LLM at the core of all operations, taking input from an extensive prompting strategy with domain knowledge and feedback from compilation and execution errors that autonomously guide the generation of working code. In the following subsections, we provide a detailed description of LASSI, including the specific prompting utilized to guide an LLM to generate the reported results through automated self-correction iterations."}, {"title": "A. Source Code Preparation", "content": "The initial step of the LASSI framework establishes an experimental baseline by compiling and executing the original target language code. This step ensures the viability of our approach by providing a basis for comparison with the code generated by LASSI. Upon successful execution, the standard output of the executed code is captured for later comparison with the output of the generated code. This initial step also serves to verify that LASSI's compilation command is appropriate for the local compute platform because the same command will be used for compiling the LASSI generated code. If an error occurs, LASSI halts and does not move forward with the translation until the code is corrected by the user.\n\nLASSI also checks that the original source language code compiles and executes in the local environment before processing through the translation pipeline. If an error occurs, again LASSI halts and does not move forward with the translation until the code is corrected by the user."}, {"title": "B. Programming Language-Specific Context Preparation", "content": "LASSI implements a series of prompt engineering strategies to prepare the LLM for a tuned prompt query. A predefined dictionary containing both system and user prompts is made available for on-demand use by the pipeline during the automated process. For the purpose of demonstration, our prompt dictionary includes tailored queries for CUDA and OpenMP. The LASSI implementation of the dictionary enables easy extensibility of the pipeline to additional programming languages and code generation goals without the need to adjust the core pipeline process. The same prompts are used for each LLM in this study to ensure consistent performance comparison. However, in practical implementations, these prompts may be tailored as needed for the specific coding language and selected LLM.\n\nA system prompt is a high-level guidance provided to an LLM as a precursor to the main prompt request that can help it generate more appropriate or expected responses. For this experiment, the LASSI system prompts suggest to the LLM that it is a \"professional\" in translating code. The prompts in Table I are examples that suggest good performance. These prompts, however, are customizable. Future work will continue to explore tuning these and other prompting strategies.\n\nA key feature of the LASSI prompting strategy is to incorporate specific knowledge tailored to the target programming language (CUDA or OpenMP). We integrated content sourced from the official CUDA manual for the translation in the OpenMP to CUDA pipeline, and content from OpenMP resources for the CUDA to OpenMP translations.\n\nAn important challenge when creating a prompting strategy is to respect the amount of input text an LLM can process. This is referred to as its context window, which is limited to the number of tokens used for training the model. Our selected LLMs feature a range of context limits from approximately 16k to 164k tokens. Our intention for this study is to ensure a consistent application of the pipeline configuration for all scenarios across LLMs and application codes. Therefore, we limit the scale of the provided programming language knowledge to fit reasonably within the lower bound LLM context window (Table V). Specifically, for the OpenMP context, we included all text from the OpenMP API 4.0 C/C++ Syntax Quick Reference Card [20] (7,290 tokens). We extracted Chapter 5 from the CUDA C++ Programming Guide, Release 12.5 [21] (4,053 tokens).\n\nBefore the translation stage of LASSI, we prompt the LLM to generate a summary of the provided programming language knowledge using a \"self-prompting\" approach [22] that supports tuning the context toward how the specific LLM would represent this knowledge. The generated response is then inserted as part of the constructed prompt to be used later in the pipeline when requesting the code translation. We continue this self-prompting by asking the LLM to summarize the source code in the original language so that the response may offer a tailored representation of the likely functionality in the code. Again, the LLM-generated code description is inserted as part of the full translation prompt."}, {"title": "C. Code Generation", "content": "With the background context prepared, LASSI begins the self-correcting code generation process. We build the full prompt with (1) programming language knowledge context, (2) LLM-generated summary of context, (3) LLM-generated description of source code, and (4) translation prompt with source code. Here, the translation prompt is specified as \"Think carefully before developing the following code that you describe as: [insert LLM-generated code description]. Now, [insert translation prompt tailored for target language, see Table II]: [insert source code].\"\n\nLASSI submits the constructed prompt content to the selected LLM, and the generated response is captured to filter out the code block, which is saved to a local file. Within the pipeline, the saved code is compiled locally with the standard and error outputs captured from the run command. This output is passed into the self-correction phase of LASSI, as described below."}, {"title": "D. Self-Correcting Loops for Autonomous Improvement", "content": "While current LLMs may not yet be fully trained on parallel codes used especially for science simulations, state-of-the-art models demonstrate strong capability in processing text across many languages, spanning those for human communication and programming computer logic. LLMs are also quite useful as code debugging partners when prompted with troublesome code and resulting error messages. Even if the LLM does not identify a fix, it might provide useful guidance to its human user toward a resolution.\n\nLASSI incorporates a novel self-correction routine. The LLM attempts to rectify errors by re-prompting with the context of specific compile or execute errors. This unique integration provides some autonomous control within the code generation pipeline. In the following subsection, we detail how errors encountered during compilation and execution are returned to the LLM through guided prompting for debugging and refactoring the generated code.\n\n1) Integrated Code Compilation with Self-Correction: After a generated code is compiled in the local environment through a command line call by LASSI, the standard error output is captured from this process. If an error is returned, then the pipeline iterates back to the LLM call with another prompt that includes the generated code, the compilation error message, and instructions to refactor the code with a fix. The specific prompt strategy for self-correcting compiler errors is shown in Table III.\n\nNew code is generated again by the LLM followed by another compilation attempt with a capture of any resulting error messages. This iteration continues until no error is returned when compiling the generated code.\n\n2) Integrated Code Execution with Self-Correction: Only after the compiler does not return an error does LASSI continue to the next step of executing the most recently generated code. This is also performed through a command line call by the pipeline in the local environment after assigning the necessary execute privileges to the saved code file. If an error is returned, then the pipeline iterates back to the LLM call with another prompt that includes the generated code, the execution error message, and instructions to refactor the code with a fix. The specific prompt strategy for self-correcting compiler errors is shown in Table III.\n\nNew code is generated once again by the LLM followed by a compilation attempt with a capture of resulting error messages. If a compile error occurs again, then the pipeline remains in the compilation self-correction loop. This iteration through the compiler and execution attempts continue until no error is returned from executing the generated code."}, {"title": "IV. BENCHMARK CODES AND LLMS", "content": "The goal of LASSI is to translate existing science code from one parallel programming language to another. To accomplish this task we leverage the HeCBench repository [2] for our code base. HeCBench offers an extensive curation of open-source heterogeneous computing applications available in OpenMP, CUDA, HPI, and SYCL. For this study, we focus on bi-directional translation between GPU benchmarking codes written in OpenMP with target offload and CUDA. We selected a suite of HeCBench codes to use the application version in one language as the source for our pipeline to translate to another language and the corresponding version of the same application in the target language to compare with the LASSI generated code. Moreover, we ensured diversity in computational categories to demonstrate the robustness in the translation capabilities. Following HeCBench's categorization, we selected ten applications across nine categories for our test cases, as listed in Table IV.\n\nWe compiled and executed each HeCBench test case written in either CUDA or OpenMP using the same compilers and flags. Further, identical input parameters were used with LASSI for execution on the same compute resources. The runtimes were measured for each benchmark code to compare the runtime for the LASSI generated code. These runtimes are also listed in Table IV and represent an average runtime of three executions on an NVIDIA A100 GPU. The average is used because the standard deviation is small due to the single-user access to this local server.\n\nRecall that a key requirement of LASSI is that it should be LLM-agnostic. This is especially important as new models are released frequently. To demonstrate this requirement, we selected four LLMs, listed in Table V. In particular, we use three recently released open-source models for code generation, along with one private model."}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "We experimented with LASSI on a Linux server equipped with two NVIDIA A100 GPUs, each with 40 GB of memory. The open-source models were hosted through a local deployment of Ollama [25], and GPT-4 was accessed through an API calling a private instance of the model. We experimented with several current open-source, code-centric LLMs available at the time of this work, and found that Codestral [4], Wizard Coder [7], and DeepSeek Coder v2 [8] performed sufficiently well to provide a viability demonstration of LASSI.\n\nWith the ten HeCBench applications, as outlined in Section IV, we sequentially ran the complete pipeline, covering 80 bi-directional translation scenarios between CUDA and OpenMP across ten applications and four LLMs. With each run, we captured compilation and execution results, code similarity metrics, and the runtime and standard output for those with successful execution."}, {"title": "A. Evaluation Metrics", "content": "For the initial demonstration of LASSI's viability, we focus on basic metrics for the generated code, aiming to assess usability without delving into theoretical correctness or performance enhancements. Future work will include exploring additional metrics and refining prompt strategies.\n\nTables VI and VII provide five metrics for each application code translation. The first metric, $Runtime$, provides the runtime of the LASSI-generated code. The second metric, $Ratio$, is defined as the runtime of the original source code in the target programming language divided by the runtime of the LASSI-generated code. If N/A is given, then the LASSI-generated code either failed to execute or its standard output did not match the expected result compared to the output of the source code.\n\nRecognizing that code may be developed more than one way to achieve the same solution, we do not expect the code generated by LASSI to match the source code line-for-line. Nevertheless, evaluating the similarity between source and LASSI-generated codes provides valuable insights for comparing performance across LLMs. We include two string comparisons for code similarity, corresponding to metrics three and four:\n\n\u2022\t$Sim-T$ is token-based, which tokenizes both codes and uses a Ratcliff-Obershelp sequence comparison algorithm [26] to find contiguous matching subsequences. It generates a similarity ratio within [0, 1], with values over 0.6 indicating high similarity.\n\u2022\t$Sim-L$ is line-based, comparing codes line-by-line by counting identical lines regardless of order. The ratio represents the number of identical lines over the total lines in the longer code, with a higher ratio indicating more similarity, even if lines are in different order.\n\nThe final metric reported is $Self-corr$, corresponding to the number of self-correcting iterations the pipeline performed to re-prompt the LLM to correct compilation and execution errors. If the $Self-corr$ value is 0, then LASSI generated code that successfully compiled and executed on the first try. If this value is > 0 and the scenario also includes a $Ratio$, then the final generated code successfully compiled and executed, but the LLM required multiple self-corrections to obtain its code translation."}, {"title": "B. OpenMP to CUDA Translations", "content": "We ran the automated pipeline configured to refactor codes developed in OpenMP to CUDA. The selected HeCBench codes were input into the pipeline for translation. Also, the corresponding HeCBench codes in the target CUDA were compiled and executed with their standard outputs captured for visual inspection with the output of the translated code. The process for each code was repeated with four LLMs.\n\nTable VI lists the results for our OpenMP to CUDA translations. If an LLM could not generate code that was compiled, executed, or if the output was significantly different from the expected result, the record is marked as N/A.\n\nWe observe that 80% of the translations from OpenMP to CUDA successfully generated executable code with results similar to the source HeCBench code in CUDA. This result strongly indicates the effectiveness of LASSI that incorporates a novel prompting strategy, provided domain knowledge, and automated self-correction to translate OpenMP to CUDA. Of these successful generations, we observe 78.1% execute with average runtimes within 10% or faster than the average runtime of the source CUDA code. Taking the ratio of 0.6 as a heuristic measure for reasonable similarity between codes, the pipeline generated 40.6% of the successful codes in the experimental set at this threshold or higher. Finally, the self-correction counts across all OpenMP to CUDA code translations remained quite low, with 65.6% of the trials generating executable code on the first attempt."}, {"title": "C. CUDA to OpenMP Translations", "content": "Table VII lists the results for our CUDA to OpenMP translations. The results strongly validate the feasibility of LASSI. Specifically, 85% of the translation samples successfully generated executable code with similar output as the source. Among these, 61.8% achieved average runtimes near or below those of the source OpenMP codes, 47.1% generated heuristically similar codes, and 55.9% generated executable code on the first attempt."}, {"title": "D. Discussion", "content": "We highlight two noteworthy findings from the experiments given in Tables VI and VII to shed light on the initial quality of the LASSI generated code. First, we observe a likely lower-quality generated code in Codestral's translation of bsearch from CUDA to OpenMP, which may necessitate additional self-correcting prompts. The code similarity measures are moderate, and the translation successfully executed on the first attempt without errors requiring correction. We compared standard outputs between the original HeCBench and translated codes, confirming identical results except for reported timings. However, the average runtime of the translated code over multiple runs is 20\u00d7 longer than that of the source code. Upon comparing the two codes, we noted the translated code only implements the default single thread, whereas the original source code explicitly sets 256 threads.\n\nSecond, we examine DeepSeek Coder's translation of atomicCost from CUDA to OpenMP and observe over a 66x speedup. Upon comparing the standard outputs of the HeCBench source and the translated code, we confirm identical results. The translated version appears to utilize several alternative approaches to parallelization, including thread limits, memory allocation, loop structures with fewer atomic operations, and timing methods. These examples emphasize the known sensitivity of existing LLMs [13] in generating content for which they are ill-trained and the opportunity for enhancing these models through strategic prompting with domain knowledge and self-correction. Also, we anticipate the development of enhanced pipelines configured with prompted goals, such as improving performance or reducing energy consumption, as feasible extensions to our current architecture."}, {"title": "VI. SUMMARY AND FUTURE WORK", "content": "In this work, we have prototyped an LLM-based automated self-correcting pipeline, LASSI, for translating between parallel programming languages. The initial results of evaluating LASSI with different application codes across four LLMs demonstrate the effectiveness of LASSI for generating executable parallel codes, with 80% of OpenMP to CUDA translations and 85% of CUDA to OpenMP translations producing the expected output. We also observe approximately 78% of OpenMP to CUDA translations and 62% of CUDA to OpenMP translations execute within 10% of or at a faster runtime than the original benchmark code in the same language.\n\nWe plan to explore several extensions to LASSI for generating verifiable and more performant codes. In particular, we will integrate code verification to automatically compare expected results, with feedback incorporated into another self-correcting cycle."}]}