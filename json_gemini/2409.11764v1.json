{"title": "One Map to Find Them All: Real-time Open-Vocabulary Mapping for Zero-shot Multi-Object Navigation", "authors": ["Finn Lukas Busch", "Timon Homberger", "Jes\u00fas Ortega-Peimbert", "Quantao Yang", "Olov Andersson"], "abstract": "The capability to efficiently search for objects in complex environments is fundamental for many real-world robot applications. Recent advances in open-vocabulary vision models have resulted in semantically-informed object navigation methods that allow a robot to search for an arbitrary object without prior training. However, these zero-shot methods have so far treated the environment as unknown for each consecutive query. In this paper we introduce a new benchmark for zero-shot multi-object navigation, allowing the robot to leverage information gathered from previous searches to more efficiently find new objects. To address this problem we build a reusable open-vocabulary feature map tailored for real-time object search. We further propose a probabilistic-semantic map update that mitigates common sources of errors in semantic feature extraction and leverage this semantic uncertainty for informed multi-object exploration. We evaluate our method on a set of object navigation tasks in both simulation as well as with a real robot, running in real-time on a Jetson Orin AGX. We demonstrate that it outperforms existing state-of-the-art approaches both on single and multi-object navigation tasks. Additional videos, code and the multi-object navigation benchmark will be available on https://finnbsch.github.io/OneMap.", "sections": [{"title": "I. INTRODUCTION", "content": "Object search in complex, cluttered environments presents significant challenges for robots, despite being relatively in-tuitive for humans. Humans can navigate novel environments by remembering semantic information along with the spatial layout of the area. As individuals explore new surroundings, they continuously update both of these while choosing search directions that semantically correlate with the object they are looking for, such as going to what looks like a kitchen when searching for the fridge. Additionally, by maintaining such an internal spatial-semantic representation over time we can also more efficiently guide our search for objects in the future. E.g., we may not have seen the oven, but we remember where the kitchen is. Here we want to similarly enable robots to remember such semantic-spatial information for multi-object search.\nPre-trained Vision-Language Models (VLMs) have demonstrated the ability to correlate visual appearance and natural language, facilitating perceptual reasoning at increasingly abstract levels. Recent research has highlighted the benefits of using VLMs to inform zero-shot object navigation tasks [1][2]. However, there remains a gap in understanding of how to effectively leverage VLMs to inform search while maintaining reusable semantic-spatial information over consecutive object navigation queries.\nWe propose a novel method to construct a real-time spatial-semantic map (OneMap) for efficient open-vocabulary object search. The contributions of our work include:\n1) We present an open-vocabulary, spatial-semantic map that is built and queried online, enabling multi-object navigation with retained semantic information across tasks. The method is developed for onboard deploy-ment and we demonstrate it in real-world experiments running on a Jetson Orgin AGX.\n2) We propose a probabilistic observation model to miti-gate common errors when mapping semantic features, and utilize the resulting probabilistic-semantic map to make uncertainty-informed decisions for semantic exploration.\n3) We introduce a new multi-object navigation benchmark to evaluate the semantic memory capabilities of au-tonomous agents, and release it as open source.\n4) We conduct extensive evaluations on benchmarks in the HM3D scene set [3]. The results validate that our method outperforms state-of-the-art approaches both on single and multi-object navigation tasks."}, {"title": "II. RELATED WORK", "content": "Efforts in object goal navigation [4] aim to equip mobile systems with the ability to use geometric and semantic prop-erties of a given environment to efficiently guide an agent towards a target object. A large body of work has studied semantic goal navigation with a predefined set of object classes (c.f. [5][6][7]). Advances in large pre-trained vision-language models have further allowed the object navigation problem to be solved for objects and environments without prior training. In several recent works, such VLM-based approaches have been used for zero-shot object navigation.\nClosest to our method is VLFM [1]. The method generates a 2D semantic similarity map, conditioned on an open-vocabulary text query, which is used to score frontiers when exploring the environment in search for a target object. The authors propose to use the BLIP-2 [8] VLM to produce open-set, image-level features. Instead, we extract patch-level CLIP-aligned features and formulate a probabilistic mapping scheme, which allows us to associate the features more accurately with locations in the map. Moreover, our method builds a map that contains language-queryable features, while VLFM only stores similarity scores for a specific query.\nThe approach of [9] projects semantic object and room la-bels into a 2D navigation map with the help of the GLIP [10] model, which yields language grounded bounding boxes. They additionally use a large language model, enabling commonsense reasoning about object locations. However, the integration of large language models typically bears the cost of high computational expense. The works of [2] and [11] present methods for constructing semantic navigation maps offline, which can be utilized during navigation. [2] uses 3D projection of pixel-level features, extracted using LSeg [12], which produces a highly detailed, queryable semantic rep-resentation. In [11] the visual semantic information is com-bined with a large language model, which allows planning from unstructured natural language input. They follow [13] by combining a class agnostic region proposal network with CLIP [14]. [15] introduces and evaluates variations of a straightforward, heuristic frontier navigation method, using CLIP or open-set object detectors for guidance.\nIn [16] and [17] the authors propose a learning based approach with implicit neural semantic representations. The method allows reinforcement learning agents to find differ-ently colored cylinders that were added to environments, but does not generalize to arbitrary objects. [18] proposes to combine an LLM, a VLM and a pre-trained visual navigation model [19] to generate actionable plans from natural language inputs and a set of RGB observations. The authors of [20] propose to train navigation agents on RGB observations and semantic embeddings of goal images from the ImageNav [21] dataset. The work of [22] tackles the object-goal navigation problem via behavior cloning on hu-man demonstrations with reinforcement learning based fine-tuning. Generally, learned policies tend to exhibit specificity to the tasks and environments they were trained with.\nAs compared to 2D navigation maps, 3D representations are usually more expensive in terms of memory and com-putation and typically not specifically designed for object goal navigation. Recently proposed 3D mapping approaches that store open-vocabulary semantic features include neu-ral [23][24][25] and non-neural metric-semantic represen-tations [26][27][28][29]. [23] and [25] use semantic fea-ture extraction in combination with gaussian splatting [30], while [24] uses a generalizable NERF approach [31] to ground semantics in implicit 3D representations. The works of [26][27][29] extract semantic features using a combination of a segmentation model (SAM [32]) and CLIP [14] and project the features to 3D pointclouds. In [28] the authors in-troduce a real-time volumetric representation using a region-level VLM [33] at a small cost of accuracy."}, {"title": "III. METHOD", "content": "This section describes our proposed method for real-time generation and exploration of open-set semantic belief maps via a probabilistic observation model of semantic features from a VLM. See Fig. 2 for a system overview.\nFrom a stream of RGB and depth images with associ-ated camera poses we construct a two-dimensional, open-vocabulary belief map $\\mathcal{M} = (\\mathcal{F}, \\sigma^2)$ over CLIP-aligned fea-tures $\\mathcal{F}(x, y) \\in \\mathbb{R}^{n_x \\times n_y \\times f}$ and a variance per map location $\\sigma^2(x, y) \\in \\mathbb{R}^{n_x \\times n_y}$, where $f$ is the CLIP feature dimension. We index the map $\\mathcal{M}$ with $x \\in [0, n_x], y \\in [0, n_y]$ and data points in image-space with $i \\in [0, H], j \\in [0, W]$, where $n_x$, $n_y$ are the map's dimensions in grid cells and $H, W$ are the image height and width respectively in pixels.\nGiven an RGB image $I \\in \\mathbb{R}^{H\\times W\\times 3}$ in Fig. 2a, we use SED [34] to obtain a per-patch feature field $\\mathcal{F}_I\\in \\mathbb{R}^{H_F\\times W_F\\times f}$, where $H_F$ and $W_F$ are the image dimensions in patches. The feature field is projected to 3D using the corresponding depth image $D\\in \\mathbb{R}^{H\\times W}$ and subsequently fused into the map in a probabilistic fashion, accounting for uncertainties in feature extraction and depth sensing.\nThe following sections describe the construction of the open-vocabulary belief map $\\mathcal{M}(x, y)$ and detail how it is leveraged for effective object-goal navigation.\nWe extract patch-level descriptive features in the CLIP [14] feature space by employing the image encoder of SED [34]. SED uses a hierarchical encoder based on the ConvNeXT [35] architecture. This is motivated by the fact that ConvNeXT has linear complexity with respect to the size of the input data and has been shown to perform well at capturing the locality of information when compared to plain cameras decreases with distance, resulting in less reli-able feature projections.\nWe incorporate the uncertainties stemming from Feature Leakage and Feature Extraction by assigning a variance value to pixels of $\\mathcal{I}(i, j)$ (Fig. 2a):\n$\\sigma^2_{\\mathcal{I}}(i, j) = \\sigma^2_L(i, j)\\sigma^2_E(i, j).$ (1)\nWe model the feature leakage variance as\n$\\sigma^2_L(i, j) = \\text{tanh}(\\nabla_x \\mathcal{D}(i, j)^2 + \\nabla_y \\mathcal{D}(i, j)^2),$ (2)\ni.e. the standard deviation is linear in the gradient of the depth image. As a result, the system considers features less reliable where the depth image has large gradients.\nWe assume the extracted features to be most reliable at an optimal detection distance $d_{opt}$, with their variance rapidly increasing for pixels closer or further to the camera. The resulting variance is then given by\n$\\sigma^2_E(i, j) = \\text{exp}\\Big(-\\frac{(d_{opt} - \\mathcal{D}(i, j))^2}{2}\\Big).$ (3)\nSubsequently, we project each point $(i, j)$ to a 3D point-cloud of uncertain features $\\mathcal{P}$ using $\\mathcal{D}(i, j)$ and the camera intrinsics (Fig. 2b). $\\mathcal{P}$ is then projected onto the 2D plane. Features $\\mathcal{F}_I(i, j)$ of points that come to lie in the same map cell are combined through a weighted summation approach, with weights $\\propto \\sigma^2_{\\mathcal{I}}(i, j)$, and the variances $\\sigma^2_{\\mathcal{I}}(i,j)$ are averaged. As a result, we obtain a feature vector $\\mathcal{F}_M(x, y)$ with corresponding variance $\\sigma^2_{M(x,y)}$ for each map cell affected by the current observation, as seen in Fig. 2c.\nLastly, we incorporate the uncertain locations of projected features, resulting from depth sensing noise, by blurring the features proportional to the location uncertainty. For this, we adopt the stereo camera accuracy model from [36] and model the variance of feature locations as quadratically dependent on the distance $d(x, y)$ between the map position and the camera:\n$\\sigma^2(x,y) = pd^2(x, y),$ (4)\nwith $p$ a proportionality factor.\nWe achieve the blurring by convolving each mapped feature $\\mathcal{F}_M(x,y)$ with a spatially varying Gaussian blur kernel. The Gaussian convolution\n$\\mathcal{F}_{M, \\partial M} = \\text{2DConv}_{\\sigma^2_{\\partial M}(x,y)} (\\mathcal{F}_M, \\sigma^2_M)$ (5)\nis parameterized by the spatially-varying $\\sigma^2_{\\partial M}(x,y)$ and af-fects both, features and variances. We obtain the convolved features $\\mathcal{F}_M(x, y)$ and variances $\\sigma^2_M(x,y)$ in map space. Intuitively, this results in stronger blur for distant geometries, accounting for the loss of accuracy in the spatial grounding of the semantics. Conversely, the scene is resolved in higher detail if the camera is close, as visualized in Fig. 2d. To ensure real-time feasibility and limit memory usage on-board the robot, we implement this as a custom sparse inverse Gaussian convolution operation, optimized for GPU.\nThe convolved features are fused into the persistent map $\\mathcal{M}$ per grid cell following the scheme of recursive Bayesian estimation (Fig. 2e), using the Kalman gain $K$:\n$K = \\frac{\\sigma^2_{I^2}(x,y)}{\\sigma^2_M(x, y) + \\sigma^2_{I^2}(x, y)},$ (6)\n$\\mathcal{F}^{t+1}(x, y) = \\mathcal{F}^t(x, y) + K(\\mathcal{F}_{M}(x,y) - \\mathcal{F}^t(x,y)),$ (7)\n$\\sigma^{t+1}(x, y) = (1 - K)\\sigma^t(x,y).$ (8)\nFor a given text prompt we use the language encoder of CLIP [14] to produce an embedded object query $\\mathcal{Q}\\in \\mathbb{R}^f$. We calculate the cosine similarity between $\\mathcal{Q}$ and each map cell to obtain a query-conditioned similarity map $\\mathcal{S}_Q(x, y) \\in \\mathbb{R}^{n_x \\times n_y}$. For the purpose of object exploration we keep track of an additional variance estimate $\\sigma^2_S(x, y)$, which is analogously updated according to Eq. 8. In contrast to $\\sigma^2_M(x, y)$, $\\sigma^2_S(x, y)$ can be reset during the exploration process, as laid out in the following. We introduce the notion of four binary sub-maps, shown in Fig. 3:\nWe score them based on the highest query-to-feature similarity $\\text{max}(\\mathcal{S}_Q(x,y))$ in map cells in $\\mathcal{O}-\\mathcal{E}$ reachable from the frontier, i.e. cells that are observed but not semantically explored.\nSecondly we cluster regions with high values of $\\mathcal{S}_Q(x, y)$ in the semantically explored, but not searched map, $\\mathcal{E} - \\mathcal{C}$. Similarly, we score them with respect to the maximum similarity value contained in the cells of a cluster.\nNote that the latter type of navigation goals are only obtained if the searched map is different from the seman-tically explored map, i.e. the searched map has been reset at least once. Hence, upon obtaining a new text prompt, e.g. after successfully navigating to a goal, $\\sigma^2_S(x, y)$ is reset, allowing previously seen high-similarity areas to be eligible as navigation targets again.\nWe then greedily select the highest-scoring navigation goal and use A* [37] to plan a collision free path. We further employ a visual object detector as a second measure to iden-tify a potential target object. We apply consensus-filtering between the map and object detector by only considering an object as detected when 1) the object detector triggers, and 2) the corresponding area in the map $\\mathcal{S}_Q(x,y)$ has a similarity score within the top 5-percentile. Upon detection, the agent navigates close to the object and terminates the exploration process. We follow [1] and employ YOLOv7 [38] if the object is part of the classes in MS-COCO [39] and open-set detector Yolo-World [40] otherwise."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "We evaluate our method on a standard single-object simu-lation navigation benchmark, on a novel benchmark that we construct for the multi-object navigation problem, as well as on navigation experiments with a real robot.\nNavigation Tasks. Single-Object Navigation [4] requires the agent to navigate from a given initial position to any instance of a target object category. We evaluate on the validation split of the Habitat ObjectNav Challenge [41] based on the Habitat Matterport 3D (HM3D) [3] dataset. It comprises 2000 single-object episodes in 20 scenes over six object categories.\nMulti-Object Navigation: We define multi-object naviga-tion as a sequence of object goals. The agent is asked to navigate to a given object category. Upon successfully finding the object, the agent is informed about the next object goal. The episode concludes if the agent does not succeed in finding a given object. Goal navigation is considered successful if the agent terminates the task and if it terminates in a position within 1.5 m of any instance of the target object category. To construct the task, we build on the Habitat ObjectNav challenge: We generate episodes by sampling from starting poses present in the single-object task and then randomly generate a sequence of reachable objects, that are reachable without traversing stairs. Our dataset consists of 236 episodes in 20 scenes over six object categories. Each episode consists of a sequence of three goal objects. The dataset generation is further detailed on our project page.\nMetrics. For single-object navigation, we report the Success Rate (SR) and Success weighed by Path Length (SPL) [42]. For successful episodes, the SPL is the optimal path length divided by the agent's path length, else zero.\nFor multi-object navigation, we report the overall success rate (SR), the fraction of episodes in which all objects were found, as well as the overall success weighted by path length (SPL). The SPL is the optimal path length divided by the agent's path length if all objects were found, and zero otherwise. The optimal path is hereby defined as the sum of sequence-wise optimal paths, i.e. the shortest path to the next object. This is done to account for the fact that the agent gets informed about the next target object only after succeeding in finding the previous one. Moreover, following [43], we report the progress (PR) and the progress weighed by path length (PPL). The progress is the fraction of found objects per episode. Note that an episode is terminated if the agent fails to find or misidentifies any target object of the episode.\nBaselines. For single-object navigation, we compare against two SOTA zero-shot methods: ESC [9], and VLFM [1]. Similar to our method, ESC and VLFM perform frontier-based exploration. ESC scores frontiers based on object detections in proximity of the frontier by querying an LLM and VLFM queries BLIP2 to determine promising frontiers. Moreover, we compare against two methods that ei-ther require task-specific training or supervision. ZSON [20] employs CLIP to transfer a model trained for ImageNav to the ObjectNav task. PIRLNav [22] is an end-to-end policy trained on human demonstrations.\nTo the best of our knowledge, ours is the first method that performs real-time, zero-shot multi-object navigation and without extensive task and environment-specific training. Hence, for multi-object navigation, we compare against VLFM as the strongest performing baseline for the single-object task."}, {"title": "V. RESULTS", "content": "Our experiments are designed to answer the following questions: (1) How does our method compare against state of the art methods for single-object navigation? (2) Can our method effectively use its semantic map to improve perfor-mance over a sequence of multiple object goals compared to existing approaches? (3) Can our method be successfully be deployed on real robot searching real environments?\nWe compare our approach against all baseline methods on the single object navigation dataset, detailed in Section IV. The corresponding results are presented in Table I. Our method outperforms the state-of-the-art zero-shot methods, VLFM and ESC, in both success weighed by path length (SPL) and success rate (SR). Specifically, it achieves an SPL of 37.4% and a success rate of 55.8%. When comparing with methods that require task- and environment-specific training, our method OneMap still outperforms the baselines PIRLNav and ZSON in terms of SPL, surpassing them by 10.3% and 24.8%, respectively. Although PIRLNav demon-strates a 8.3% higher success rate, it requires additional time-consuming training and human demonstrations (77k demonstrations, accounting for ~ 2378 human annotation hours) for the specific task and environment, whereas our method achieves competitive results zero-shot. Our superior performance in SPL indicates that our proposed semantic feature map provides more informative guidance, resulting in shorter paths to the target object. However, it is interesting that we also outperform VLFM on success rate alone, as this does not factor in path length and we use the same object detector. Moreover, VLFM uses a PointNav locomotion pol-icy trained specifically to better traverse the HM3D domain while we simply compute the shortest path to the frontier on a grid. We suspect that our higher SR compared to VLFM on the single-target task can be attributed to our ability to do consensus-filtering with our high-confidence semantic beliefs as outlined in Section III-D, whereas VLFM only uses heuristics to mitigate false detections. 3D reconstruction datasets such as HM3D additionally suffer from artifacts [3] which can exacerbate misdetections. To investigate our surprisingly good results on single-object SR further, we repeat the evaluation without any consensus-filtering or heuristics to mitigate false detections at all. Instead, we just accept all detections from the detector. The results of this ablation shown in Table II indicate that false positives are a considerable source of error on the ObjectNav task and our filtering can effectively reduce the false positive rate (FP) of the object detector by 7.1%. We note that we still achieve higher SPL than VLFM (with its misdetection heuristics that our semantic belief map can more effectively guide an agent for single-object navigation tasks, and moreover it also reduces false positives from the object detection stage.\nWe evaluate the performance of our method OneMap against VLFM on the multi-object navigation task in HM3D. Table III highlights our method's superiority over the base-line VLFM across all metrics. We achieve an average progress (PR) of 65.54%, meaning that we successfully find 2 objects per episode on average. Moreover, as expected the performance gap to VLFM grows larger for multi-object tasks than for single-object tasks, indicating that our method can effectively reuse the map for multiple objects.\nWe evaluate this further in Fig. 5, where we compute the average SPL for the first, second, and third object goals of each episode, given that the previous object was successfully found. As evident in the figure, the efficiency (SPL) for our method monotonically increases, whereas it remains relatively constant for VLFM. This can be attributed to our uncertainty-aware exploration resulting in high-quality open-vocabulary navigation maps that can effectively reuse the data from previous object goals, unlike VLFM, which builds a query-specific scoring map that cannot leverage prior searches. As a result, our method sees a relative increase in efficiency (SPL) by a factor of 1.5x compared to VLFM for the third object, benefiting from the high-quality open-vocabulary semantic map constructed during the search for the first and second objects. We note that 100% SPL here represents perfect (oracle) knowledge of where every object is in advance and is therefore an unattainable upper bound."}, {"title": "C. Real World Experiments", "content": "We deploy our method on a Boston Dynamics Spot quadruped using only one front-facing Realsense D455 cam-era, and a Livox Mid 360 lidar with Fast-LIO2 [44] for odometry. We use Yolo-World [40] instead of YOLOv7 to allow searching arbitrary objects, instead of MS-COCO [39] classes only. We run the complete system on-board a Jetson Orin AGX, and achieve an overall update frequency of 2 Hz, which we found to be sufficient for an indoor, legged robot.\nFor deploying our method in the real-world, we only adapt the feature localization parameter in Eq. 4 to account for larger depth noise, but conduct no additional tuning of method parameters used for the evaluation in simulation. We follow the planned paths with a path-tracking controller and send velocity commands to the robot. Real-world ex-periments of multi-object navigation tasks can be viewed at https://finnbsch.github.io/OneMap."}, {"title": "VI. CONCLUSION", "content": "In this paper we present a real-time capable, zero-shot, semantic object navigation method that builds a reusable semantic belief map from patch-level language-aligned se-mantic image features. Leveraging the rich visual-semantic space of CLIP [14] and performing a probabilistic map update is shown to yield a high-quality semantic represen-tation that provides state-of-the-art performance on single-object as well as multi-object open-vocabulary navigation tasks. Furthermore we demonstrate the method's real-world applicability via successful experiments running onboard a quadruped robot.\nAlthough our methods works well with projection 2D, future work could include examining suitable 3D map repre-sentations, though this will introduce additional complexity to the mapping and planning parts of our method."}]}