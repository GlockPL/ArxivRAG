{"title": "PIP: Prototypes-Injected Prompt for Federated Class Incremental Learning", "authors": ["Muhammad Anwar Ma'sum", "Mahardhika Pratama", "Savitha Ramasamy", "Lin Liu", "Habibullah Habibullah", "Ryszard Kowalczyk"], "abstract": "Federated Class Incremental Learning (FCIL) is a new direction in continual learning (CL) for addressing catastrophic forgetting and non-IID data distribution simultaneously. Existing FCIL methods call for high communication costs and exemplars from previous classes. We propose a novel rehearsal-free method for FCIL named prototypes-injected prompt (PIP) that involves 3 main ideas: a) prototype injection on prompt learning, b) prototype augmentation, and c) weighted Gaussian aggregation on the server side. Our experiment result shows that the proposed method outperforms the current state of the arts (SOTAs) with a significant improvement (up to 33%) in CIFAR100, MiniImageNet and TinyImageNet datasets. Our extensive analysis demonstrates the robustness of PIP in different task sizes, and the advantage of requiring smaller participating local clients, and smaller global rounds. For further study, source codes of PIP, baseline, and experimental logs are shared publicly in https://github.com/anwarmaxsum/PIP.", "sections": [{"title": "INTRODUCTION", "content": "Federated learning (FL) offers a collaborative approach for many clients to produce a shared global model while protecting their data privacy [26] [17] [32]. FL has recently sparked a great deal of academic interest and achieved outstanding success in various application areas, including medical diagnosis [15], autonomous vehicle [12], and wearable technology [4]. However, the majority of FL methods are designed for a static application scenario, assuming the target classes are fixed and known in advance. In real-world applications, the data are dynamic, allowing local clients to access unseen target classes online.\nExisting studies have addressed dynamic data challenges in FL through Federated Class Incremental Learning (FCIL) where each local client gathers training data continually according to their continuous observation of the environment. In contrast, new clients with unforeseen classes are always welcome to join the FL training [8] [9] [41]. The clients have to cooperatively train a global model that continually learns new classes while maintaining its capability to recognize the previous classes. Despite its capability to learn new classes, the global model tends to lose its knowledge of previous classes known as catastrophic forgetting. Therefore, FCIL calls for answers on how to handle catastrophic forgetting in a dynamic collaborative continual learning, while preserving data privacy. In addition, the clients may carry non-independently and identically distributed (non-i.i.d.) data with imbalance classes.\nThe current FCIL state-of-the-art (SOTA) i.e. GLFC[9] and LGA[8] train and share whole backbone parameters resulting in a large number of parameters (11.3M) to train that requires a longer train- ing time, and a high communication cost (61.7MB) as shown in Table 1. In addition, in the case of client-server limited communi- cation costs e.g. due to limited bandwidth, this approach is forced to utilize a smaller (more shallow) model to reduce the shared pa- rameter size. Second, in the current SOTAs, the clients privately share perturbed images with the central server for the aggregation process. Besides its inefficient size (16.5MB), sharing perturbed im- ages may violate data privacy principles since perturbation doesn't guarantee information leakage. Furthermore, this mechanism can't answer the data openness problem, where the data is only open for a client at a specific moment. These intriguing gaps motivate us to develop a more efficient yet more effective approach to the FCIL problem, where a client trains and shares as small parameters as possible but produces a highly accurate global model without sharing any (perturbed) samples.\nIn this study, we propose a new approach named federated prompt for the FCIL problem that is implemented in our proposed baselines and our proposed method named prototype-injected prompt (PIP). In our proposed approach, a client trains and shares only a"}, {"title": "RELATED WORKS", "content": "a). Class Incremental Learning (CIL): Prompt-based approach e.g. L2P [39], DualPrompt [38], CODA-Prompt [34] offers a break- through solution for CIL by training only small amounts of param- eters called prompt for down-streaming tasks sequence, while the backbone model which contains the biggest parameter numbers stays frozen. This approach reduces the training time and offers utilization of a more complex backbone model e.g. ViT instead of CNN. The rehearsal approach e.g. ICARL [30], EEIL, [7], GD [28], DER++ [6], trains exemplars (memory) from the previous task joined with current task samples to minimize model forgetting on previous tasks. This approach doesn't work well if the memory samples are not available. The bias correction approach e.g. BiC [40] and LUCIR [13] enhances the rehearsal approach by creating a task-wise bias layer to help the model achieve stability-plasticity balance in compensation to add extra parameters to be trained. The regularization approaches adaptively tune the base learner parame- ter to accommodate the previous task and current task e.g. EWC [18], MAS [2], LWF [23], DMC [44]. The regularization has the advantage that It works on memory-free CIL but on the contrary, it is outperformed by rehearsal and bias correction approaches if the memory is available. Prompt-based approach outperforms the other three approaches despite requiring no memory, but not yet been proven effective in a distributed (federated) setting.\nb). Federated Class Incremental Learning(FCIL): The recent study on FCIL problem e.g. FedWeIT[41], GLFC [9], and LGA [8] strives to achieve an optimal global model by aggregating locally trained models sent by the clients. The current SOTAs are proven to be more effective than combining FedAvg [26] and class incremental learning approaches such as ICARL and BiC. However, as empha- sized in Table 1, the current SOTAs train and share all backbone parameters, resulting in a less efficient training time and high com- munication costs. In Addition, the SOTAs assume that a client saves several exemplars as local memory and shares perturbated images with the server which is not always applicable in real applications. The other study i.e. TARGET [42] utilizes a synthetic dataset in- stead of saving exemplars from the previous tasks to train the whole backbone network. TARGET is proven to be more effective than Fed-LWF, Fed-EWC, and FedWeIT[41] but still outperformed by LGA and GLFC. Another approach i.e. FedCIL [29] uses a locally trained generative model i.e. ACGAN [27] to generate fake samples in the aggregation process. Despite its better performance than the combination of FedAvg or FedProx [20] with ACGAN, DGR[31] or LWF-2T [36] this method requires higher training time both in local and server-side and higher communication cost due to sending the generative model along with the backbone. To tackle the weak- nesses above, our approach uses memory-free prompt learning for local training with the frozen backbone to achieve an efficient yet powerful local model, then aggregates the prompt and prototypes to obtain a generalized global model with low communication cost."}, {"title": "PRELIMINARIES", "content": "3.1 Problem Formulation\nClass incremental learning (CIL) problem is defined as a learning problem of a sequence of fully supervised learning tasks {Tt}t=1T where T represents the number of consecutive tasks, the value of T is unknown to the model before CIL process and can be infinite. Each task carries Nt pairs of training samples Tt = {xi,yi}i=1Nt where xi \u2208 Xt denotes input image and yi \u2208 yt denotes its corresponding class label. Each task carries the same image size but possesses disjoint target classes to the other tasks. Suppose that"}, {"title": "PROPOSED METHOD: PROTOTYPE-INJECTED PROMPT (PIP)", "content": "Our proposed method, as visualized in Figure 1 performs prompt learning on each client to handle local catastrophic forgetting (sec- tion 4.1), handling non-i.i.d distribution by applying shared proto- type injection(section 4.2), handling class-imbalance by performing prototypes augmentation (section 4.3), and finally improving global model generalization by using weighted aggregation (section 4.4).\nThe uniqueness of our method is that we utilize prompt and prototype as the shared knowledge between clients and the central server. Second, we propose a prototype injection mechanism in local training to improve local model generalization. Third, we propose a weighted aggregation on the server side. Our proposed method is distinct from prompt-based methods e.g. Fed-CPrompt[3] and FCILPT[24] that shared and trained prompts only. Our proposed method is also distinguished from prototype-based FL methods e.g. FedProto [35] and CCVR [25] shared only prototypes to refine the local model or use the prototype as classifiers. Besides, FedProto and CCVR are developed for federated learning only, meaning the methods didn't address the catastrophic forgetting issue."}, {"title": "Handling Local Catastrophic Forgetting via Prompt Learning", "content": "On a round r \u2013 th of t th task, each client S\u0131 optimizes its local prompt pi and head layer parameter \u03d5\u03b9 using its available training samples Tit. Please note that pi can be any prompt structure as mentioned in our baselines e.g.in L2P, DualPrompt, or another prompt structure. Given a pre-trained ViT backbone f with M Multi head Self-Attention (MSA), where h(i), i = 1, 2..., M represents the input for i \u2013 th MSA layer, suppose that a local client wants to attach a latest updated prompt pit into the i \u2013 th MSA layer, the prompt instance prom transform feature h(i) via prompt function as defined in equation 1.\nhrit = fprompt(pri t ,h(i))\n(1)\nNote that hi is the extension of h, a sequence-like parameter pro- duced by the ViT embedding layer for the i - th MSA layer. Prompt function fprompt can be implemented by using prompt tuning (fprompt) [19] or prefix (fprompt) tuning [22] approaches as defined in equation 2 and 3.\nfprompt (pri t , h(i)) = MSA([pit h(i)], [pi k h(i)], [pi v h(i)]) (2)\nfprompt (pit, h(i)) = MSA(h(i), [pik h(i)], [pi v h(i)]) (3)\nwhere hi = Attention(h(i wq i ,h(i wk i ,h(i wv ),W0) (4)\nMSA function is defined as in equation 4 following [37] where hq i , hki , and hvi are input query, key, and value, for i - th MSA layer, PIK \u2208 RLP/2\u00d7D and PIV \u2208 RLP/2\u00d7D are splits of pit, W0, Wq, Wk , and Wv are the projection matrices, and m is number of head, \u2295 represents concatenation function, and in ViT h=h h(i q) = h(i k) = h(i v) = h(i) \u2208 RL\u00d7D. Given a pair of samples (xl , yl ) \u2208 Tit, each client S\u0131 trains its latest updated parameters pit and \u03d5\u03b9 using L\u2081\nppt \u2190 ppt \u2212 a\u2207L1\n\u03a6\u03b9 \u2190 \u03a6\u03b9 \u2212 a\u2207L1\nrt\nrt\nas defined in equation 5, where L represents cross-entropy loss, Lm represents matching loss between sample xl and key kl , \u03bb represents a constant factor, fprit represents prompt function, and fprort represents of l \u2013 th client.\nL1 = L(f\u03a6l (fpprt(xl ), yl )) + \u03bb Lm (x, krt), (xl , yl ) \u2208 Tit (5)\n(6)\nThe client's parameters are updated using equation 6 where a represents learning rate and L1 represents gradient with respect to L1."}, {"title": "Shared Prototype Injection: What, Why and How?", "content": "We define a prototype set on the t th task of a client S\u0131 as Zt = {ztc}, ztc\u2208 R1\u00d7D is the prototype for class c\u2208 Ct, Ct is the available classes in Tlt, and D is the embedding dimension. Assuming that the prototype follows a Gaussian distribution zc \u223c \u039d(\u00b5c , \u03c3c2) and the prototype is considered as D disjoint uni-variate distribution, then we have zc \u223c \u039d(\u00b5ic, \u03c3ic2) where \u03c3c2 = I\u03c1.\u03c3c2, i \u2208 {1, 2..D}, I\u03c1 is identity matrix. Suppose that Tltc = {(xl c , yl c ) \u2208 Tlt, yl c = c} is the samples of class-c in Tlt and |Tltc| is the number of samples in Tltc, we compute the prototype properties by Eq. 7 and 8.\n1 Tlt ztc = T c fpprt (xlci), xlci \u2208 Tltc (7) lci=1\n1 Tlt \u03c32c = 2 (\u03bcic \u2212fpprt(xlci))2, xlci \u2208 Tltc (8)\nDue to non-i.i.d data between clients, FCIL satisfies Tlt \u2286 (Tt = \u222al=1LTlt) and \u2200l, l\u2032 \u2208 [1..L], l \u2260 l\u2032 \u21d2 (Tlt \u2260 Tl\u2032t ) \u2227 (Clt \u2260 Cl\u2032t ,). It implies a client only optimize pit and \u03d5\u03b9 with regard to Tlt but not yet to Tt. The client S\u0131 only learns Clt but not Cl\u2032t = Ct \u2212 Clt, classes that exist in Tt but not in Tlt. Meanwhile, we can't afford to share any exemplar sample (x, y) due to data privacy. Aggregating the prompts and head doesn't guarantee optimal global model (p\u2032Gt , \u03a6Gt ) for Tt since in the next round, the participating clients and their available data may be different from the current rounds\u2019 \u2200r,r\u2032 \u2208 [1..R], \u2200l \u2208 [1..L], r \u2260 r\u2032 \u21d2 (S\u2032lrt \u2260 S\u2032lrt\u2032 ) \u2227 (T\u2032lrt \u2260 T\u2032lrt\u2032 ). Furthermore, training in too many rounds leads to catastrophic forgetting.\nTo handle this challenge, we propose prototype sharing between clients and the central server, where the server collects all clients' C prototype {Zlt}l=1L then distributes global prototype Z t = zt = \u222al=1LZlt. The shared prototype set Zt ensures each client learns C via Zt = zt \u2212 Zlt, therefore each client has chances to learn all classes Ct in Tt. Besides, the prototype set Zt has a small size in comparison to prompt pit or head layer \u03d5\u03b9 where z \u2208 Zt has the size of 1 \u00d7 D, where D is the embedding dimension, and It doesn't contain any raw information as contained in an exemplar sample.\nNow each client can enhance its local training by injecting the shareable prototype set Zlt into its training process by considering"}, {"title": "Handling Class Imbalance via Prototype Augmentation", "content": "We can simply assign z \u2208 Zt by \u00b5c to generate a single prototype for class-c. However, to handle classes imbalance between locally available classes and unavailable classes that are represented by the shared prototypes. we enrich the prototypes by using an augmen- tation. Suppose that xc1 is the sample for class c1 \u2208 Clt, z2 \u2208 Zlt is the prototype of class c2 \u2208 Cl\u2032t = Ct \u2212 Clt, c1 is locally available class, and c2 is locally the unavailable class in client-l, then we have |xc1 | > 1, while |z2| = 1. The prototype augmentations create artificial prototypes that satisfy |z2| \u2248 |xc1| as defined in equation 11 to generate m augmented prototypes for class-c based on Gauss- ian distribution z \u223c N(\u00b5c , \u03c3c2), and \u03b2 is a random value in (0, 1) range.\nlt zc\u2032 = {z2} \u222a {\u03bai }i=1m, where \u03bai = \u03bclci + \u03b2\u03c3ic , m \u2265 1 (11)"}, {"title": "Server Weighted Aggregation", "content": "We proposed weighted Gaussian aggregation on the server side to improve global model generalization. The aggregation treats clients' contribution to global aggregation proportionally based on their participation and their training sample size following best practice where a model that learns more produces more convergent weight. We consider clients to learn their local data and then pro- duce Gaussian distributed local parameters. We define \u03c9l = \u03c1t |Tit| as the weight of a client-l on the t-th task, where \u03c1t is the total of client-l participation until t \u2013 th task and |Tit| is the number of sam- ples in Tit. Given {(p\u2032lt, \u03a6lt, Zlt)}l=1L is a set of locally optimized parameters by selected local clients {S1}l=1L on the r \u2013 th of t \u2013 th task, then the global parameters (pGt , ZGt, and \u03a6Gt are computed by Gaussian-based weighted aggregation as We define in Eq. 12-15. Equation The derivation of the proposed weighted aggregation in presented in our supplemental document. The pseudo-code of PIP is presented in algorithm 1, while the pseudocode of our proposed baselines is presented in our supplemental document.\nPGt = \u03a3Ll=1(p\u2032lt\u03c9l) (12)\n\u03a3Ll=1\u03c9l\n\u03a6Gt = \u03a3Ll=1(\u03a6\u2032lt\u03c9l) (13)\n\u03a3Ll=1\u03c9l\n\u03bcGtC = \u03a3Llai \u03c9l\u03bclC (14)\n\u03a3Ll=1ai \u03c9l\n\u03a3Ll=1(\u03c3lC2 + \u03c3\u03c9l \u2212 \u03bc\u03b52C )\u03c9l (15)"}, {"title": "PRELIMINARIES", "content": "5.1 Problem Formulation\nLet 0 = (p, \u03d5) is the trainable parameters and F(0) = E[L1+(T;0)] = E[L1+(T; (p, \u03d5))] is the expected loss function, k, E, R, and Ls is local iteration, local epoch, global round, and number of selected local clients respectively. We follow L-smooth and \u00b5-strongly con- vex F, random uniformly distributed batches, G-bounded uniformly"}, {"title": "4.5.4 Server Weighted Aggregation", "content": "gradient assumptions, and decreasing learning rate as in [21],[5] as detailed below:\nAssumption 1: F1, ...F1, ..., FLs are all L-smooth: for all 0 and \u03b8\u2032, F1(0) \u2264 F1(\u03b8\u2032) + (0 \u2212 \u03b8\u2032)T \u2207F1(0) + L/2 ||0 \u2212 \u03b8\u2032 ||2.\nAssumption 2: F1, ...F1, ..., FLs are all \u00b5-strongly convex: for all 0 and \u03b8\u2032, F1(0) \u2264 F1(\u03b8\u2032) + (0 \u2212 \u03b8\u2032)T \u2207F1(0) + \u03bc/2 ||0 \u2212 \u03b8\u2032 ||2.\nAssumption 3: Let \ud835\udc58 be the random uniformly sampled from l-th local data at k \u2013 th iteration. The variance of stochastic gradients in each client is bounded by: E||\u2207F1(\u03b8, \u03be\ud835\udc59\ud835\udc58 ) \u2212 \u2207F1(0)|| \u2264 \u03c3\ud835\udc592 for l = 1, 2, ..., Ls.\nAssumption 4: The expected squared norm of stochastic gradients in each client is bounded by: E||\u2207F1(\u03b8, \u03be\ud835\udc59\ud835\udc58 )|| \u2264 G2 for all 1 = 1, 2, ..., Ls and k = 1, 2, ...., K where K \u2208 N.\n\u03a3\ud835\udc58=1\ud835\udefc\ud835\udc59\ud835\udc58 = \u221e and \u03a3\ud835\udc58=1(\ud835\udefc\ud835\udc59\ud835\udc58 )2 < \u221e where \ud835\udefc\ud835\udc59\ud835\udc58 is the learning rate of l \u2013 th client in k-th step training.\nAssumption 5. Assumption 6: The objective function F1 and \ud835\udc46\ud835\udc3a satisfy the following conditions."}, {"title": "EXPERIMENT RESULTS AND ANALYSIS", "content": "6.1 Experimental Setting\nDatasets: Our experiment is conducted using three main bench- marks in FCIL i.e. split CIFAR100, split MiniImageNet, and split TinyImageNet. The CIFAR100 and miniImageNet datasets each con- tain 100 classes while TinyImagenet is a dataset with 200 classes. We follow settings from [9] and [8] where the dataset is split equally into all tasks. In our main numerical result, The dataset is split into 10 tasks i.e. 10 classes per task for CIFAR100 and MiniImageNet, and 20 classes per task for the TinyImageNet dataset. In our further analysis, we investigate the performance of the proposed methods in different task sizes e.g. T=5 and T=20.\nBenchmark Algorithms: PIP is implemented in two version al- gorithms i.e. PIP-L2P and PIP-DualP with L2P and DualP prompt structure respectively, PIP-L2P and PIP-DualP are compared with 10 state-of-the-art algorithms: LGA [8], TARGET [43], GLFC[9], AFC[16]+FL, DyTox[11]+FL, SS-IL[1]+FL, GeoDL[33]+iCaRL[30]+FL,"}, {"title": "Numerical Results", "content": "The numerical result of the consolidated algorithms is shown in tables 2 and 3. Except to Fed-CPrompt, The baseline method (Fed- DualP) already achieves higher average accuracy (Avg) than the SOTA methods with 1 \u2013 13% improvement in accuracy. Fed-DualP also experiences a lower performance drop (PD) (19-27%) com- pared to the SOTA methods (\u2265 26%) except in the CIFAR100 dataset vs. LGA. Fed-L2P achieves higher performance than SOTAs in Mini- ImageNet and TinyImageNet datasets with \u2265 11% gap, but lower performance in the CIFAR100 dataset. The proposed method (PIP- DualP) achieves the highest accuracy with \u2265 10% gap compared to the baseline method, and \u2265 14% gap compared to the competitor methods (except Fed-CPrompt). The proposed method also achieves the lowest performance drop with (10-13%) gap compared to the"}, {"title": "Forgetting Analysis", "content": "Table 4 shows the average forgetting of the consolidated methods in each task T. The table shows that our proposed method (PIP- DualP) achieves smaller average forgetting than existing SOTAS i.e. LGA and Fed-CPrompt with a significant gap i.e. 2.7% and 9.3% margin respectively. Fed-DualP achieves a comparable average for- getting with TARGET, but looking at the trend, TARGET forgetting is increasing along with the number of tasks, while PIP-DualP for- getting is relatively stable. Please note that TARGET achieves lower average accuracy than PIP-DualP with a 64% gap. Our baselines i.e. Fed-DualP and Fed-L2P archives have lower average forgetting than PIP-DualP despite achieving lower accuracy than PIP-DualP with a huge gap i.e. (> 13%). This shows that the baselines achieve better stability (old task accuracy) but in exchange struggle to achieve plasticity (new task accuracy)."}, {"title": "Ablation Study", "content": "We conducted an ablation study to investigate the contribution of each component of the proposed method. The result is sum- marized in Table 5, while the detailed result is presented in our supplemental document. The result shows that the prototype and augmentation contribute the most to the improvement of the per- formance as shown by the performance difference of configurations E vs. F (12%), and G vs. H (16%). The weighted aggregation improves performance up to 1% as shown by the performance difference of PIP and configuration F. The head layer aggregation also plays an important role in the proposed method as shown that the absence of this component decreases performance with \u2265 6% margin. The absence of two components e.g. prototype and head aggregation"}, {"title": "Further Analayis", "content": "a) Different task size: We evaluate the performance of the pro- posed method compared to the competitor methods in different task sizes i.e. T=5 and T=20 to further investigate the robustness of the proposed method. Figure 2 summarizes the performance of the con- solidated methods in CIFAR100, MiniImageNet, and TinyImageNet with T=5 (upper figures) and T=20 (bottom figures). The detailed nu- merical result is presented in our supplemental document.. Both in T=5 and T=20 settings, PIP-DualP achieves the highest performance almost in every task in all configurations. It achieves a slightly lower performance than PiP-L2P in the last 3 tasks of CIFAR100 with T=5. Besides, all figures show that the proposed method has gentle slopes compared to the baseline and competitor methods. It shows that the proposed method experiences the lowest perfor- mance drop from tth to t + 1th task. It confirmed the robustness of the proposed method in different task-size settings. The baseline (Fed-DualP) method achieves better accuracy than the competitor methods in all 6 settings, except in CIFAR100 with the T=5 setting. In CIFAR100 with the T=5 setting, the baseline method achieves higher performance in task-1 and task-5, but lower performance in task-3, and comparable performance in task-2 and task-4. It shows the promising idea of the federated prompt-based approach for the FCIL problem."}, {"title": "Complete and Running Time Analysis", "content": "Following the pseudo-code in Algorithm 1, PIP generates prototypes when its prototype set is empty after a local epoch of a round-r (line 21), augment the prototypes in each local epoch (23), and updates the prototypes after local epochs (line 26-27). Knowing that generating prototypes from Tt costs O(), augmenting the"}, {"title": "CONCLUDING REMARKS", "content": "In this paper, we propose a new approach named prompt-based federated learning, new baselines named Fed-L2P and Fed-DualP, and a novel method named prototype-injected prompt (PIP) for the FCIL problem. PIP consists of three main ideas: a) prototype injection on prompt, b) prototype augmentation, and c) weighted Gaussian aggregation on the server side. Our experimental result shows that the proposed method outperforms the current SOTAS with a significant gap (up to 33%) in CIFAR100, MiniImageNet, and TinyImageNet datasets. Our extensive analysis demonstrates the robustness of our proposed method in different task sizes, smaller participating local clients, and smaller global rounds. Our proposed method has the same complexity as the baseline method and exper- imentally requires shorter training time than the current SOTAS.\nIn practice, our proposed method can be applied in both cross-silo and cross-domain federated class incremental learning. Our future work is directed to federated few-shot class incremental learning where each client only holds a few samples for each task."}, {"title": "ACKNOWLEDGEMENT", "content": "Muhammad Anwar Ma'sum acknowledges the support of Tokopedia- UI Centre of Excellence for GPU access to run the experiments. Savitha Ramasamy acknowledges the support of the National Re- search Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-RP-2021-027)"}]}