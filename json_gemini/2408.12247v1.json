{"title": "Enhanced Fine-Tuning of Lightweight Domain-Specific Q&A Model Based on Large Language Models", "authors": ["Shenglin Zhang", "Pengtian Zhu", "Minghua Ma", "Jiagang Wang", "Yongqian Sun", "Dongwen Li", "Jingyu Wang", "Qianying Guo", "Xiaolei Hua", "Lin Zhu", "Dan Pei"], "abstract": "Large language models (LLMs) excel at general question-answering (Q&A) but often fall short in specialized domains due to a lack of domain-specific knowledge. Commercial companies face the dual challenges of privacy protection and resource constraints when involving LLMs for fine-tuning. This paper propose a novel framework, Self-Evolution, designed to address these issues by leveraging lightweight open-source LLMs through multiple iterative fine-tuning rounds. To enhance the efficiency of iterative fine-tuning, Self-Evolution employ a strategy that filters and reinforces the knowledge with higher value during the iterative process. We employed Self-Evolution on Qwen1.5-7B-Chat using 4,000 documents containing rich domain knowledge from China Mobile, achieving a performance score 174% higher on domain-specific question-answering evaluations than Qwen1.5-7B-Chat and even 22% higher than Qwen1.5-72B-Chat. Self-Evolution has been deployed in China Mobile's daily operation and maintenance for 117 days, and it improves the efficiency of locating alarms, fixing problems, and finding related reports, with an average efficiency improvement of over 18.6%. In addition, we release Self-Evolution framework code in https://github.com/Zero-Pointer/Self-Evolution.", "sections": [{"title": "I. INTRODUCTION", "content": "With the emergence of large language models (LLMs) such as Qwen [1], LLaMA [2], and GPT [3], their exceptional generation, understanding of complex language structures and dialogue capabilities have garnered widespread attention [4], [5]. However, in specific domains, their performance often fails to meet practical requirements. For instance, GPT-4 may cite incorrect legal provisions when answering legal questions, leading to erroneous analytical conclusions. ChatLaw-MoE [6], fine-tuned on high-quality law data, has outperformed GPT-4 across multiple application scenarios. Therefore, enabling general models to acquire domain-specific knowledge allows for deploying a domain model with minimal computational resources, potentially outperforming general models with ten times the number of parameters.\nState-of-the-art approaches extensively utilize instruction fine-tuning (IFT) to align general-purpose models with specific application domains and maximize their effectiveness. InstructGPT [7] employed instruction fine-tuning to bridge the performance gap between models with a hundredfold difference in parameter count. In the absence of instruction data, certain approaches [8]-[11] use advanced LLMs to construct instruction datasets, achieving performance close to GPT-3.5 and GPT-4. However, these methods cannot guarantee the correctness and diversity of the generated instruction data. Fortunately, high-quality instruction data is scarce in most scenarios, while the volume of knowledge documents is enormous.\nIn summary, applying general-purpose models to specific domains presents the following challenges:\n1) Limitation of Computational Resources. Model performance is typically proportional to the scale of the model's parameters. However, fine-tuning and deploying powerful general-purpose language models requires substantial computational resources. For example, a LLM with 72B parameters using fp16 precision requires five Tesla V100-32GB GPUs for inference. Fine-tuning such a model incurs even greater costs. This is prohibitively expensive and impractical for tasks that must be continuously available.\n2) High-quality data scarcity. Domain-specific high-quality instruction data is often scarce. Manually correcting instruction data requires significant human effort, making it expensive. A solution is needed to automatically construct high-quality data without human assistance.\n3) Lack of diversity and correctness. Firstly, using a fixed model to construct instruction data tends to generate overly similar data. Additionally, relying solely on the model's internal capabilities for data generation may result in incorrect or irrelevant data for the domain. The"}, {"title": null, "content": "model might need more domain understanding or have learned incorrect knowledge, leading to hallucination issues. We hope the model can dynamically learn from unsupervised domain documents, continually improving its capabilities while ensuring the diversity and accuracy of data generation.\n4) Data privacy. Due to the inclusion of private information in domain-specific data, fine-tuning commercial LLMs poses major challenges when dealing with sensitive internal company data, including privacy leakage and high costs.\nIn this paper, we propose a novel framework Self-Evolution to address the aforementioned challenges. The contributions of this paper are summarized as follows:\n1) Considering the costs and privacy concerns during actual deployment, we select an open-source model with 7B parameters as the data generation, scoring model, and model for QA tasks in real scenarios. All phases in Self-Evolution can be completed with just one Tesla V100-32GB GPU, significantly reducing computational resource requirements. (Addressed challenges 1 and 4.)\n2) Self-Evolution uses LLM to generate instrution data based on a large number of unlabeled knowledge documents, ensuring domain relevance and correctness while avoiding the need for manual assistance. Additionally, the LLM undergoes iterative updates, generating a new batch of data each time. This process ensures diversity between different batches of data. (Addressed challenges 2 and 3.)\n3) We conducted extensive evaluation experiments using real-world data from China Mobile, a top-tier telecommunications provider providing services for one billion+ monthly active users (MAU). Self-Evolution achieves a performance score 174% higher on domain-specific question-answering evaluations than without using Self-Evolution and even 22% higher than Qwen1.5-72B-Chat. The Self-Evolution has been deployed in China Mobile's daily operation and maintenance for 117 days."}, {"title": "II. RELATED WORK", "content": null}, {"title": "A. Instruction Fine-tuning", "content": "The potential of LLMs in the specific domain is vast and promising. For example, Microsoft deployed GPT to summarize anomalous events in its services [12]. However, as task complexity and requirements increase, instruction fine-tuning (IFT) is widely adopted to enhance model performance. FLAN [13] achieved significant improvements in generalization by fine-tuning a high-quality instruction dataset. InstructGPT [7] successfully aligned GPT-3 [3] with human intent by fine-tuning a dataset rich in real-world instruction forms and task types. OWL [14] collected numerous operation domain instructions and achieved remarkable results in log parsing and anomaly detection. However, these methods require a large amount of manually annotated data, which becomes a bottleneck for widespread application due to the high cost."}, {"title": "B. Instruction Data Generation", "content": "Researchers have extensively explored methods to reduce human involvement in generating instruction data. Some methods [8]-[10], [15] use advanced commercial models to create instruction datasets. For instance, Alpaca [8] uses a small amount of manually constructed data to extract knowledge from DaVinci-003 [16], creating a 52k instruction dataset. It fine-tunes LLaMA to achieve performance close to GPT-3.5. Peng et al. [9] extract knowledge from GPT-4, resulting in higher quality and more diverse responses.\nAnother class of methods [11], [17], [18] employs a self-guided approach. These methods extract knowledge from the model and then use this newly constructed data to enhance domain or task capabilities. Self-Instruct [17], for instance, proposes using self-generated samples to enhance the instruction-following ability of pre-trained language models. Self-Align [18] mainly adopts topic-guided red-blue adversarial self-guidance and principle-driven self-calibration to construct data and fine-tune models, requiring less than 300 lines of manually constructed data (including 195 seed prompts, 16 principles, and five examples) to achieve high-quality fine-tuned model. The potential of these self-guided methods is certainly worth exploring further.\nHowever, these methods still require manually constructed supervision data and are limited by the model's inherent knowledge constraints, preventing them from generating instruction data beyond the model's capabilities."}, {"title": "C. Instruction Data Selection", "content": "In the early stages of IFT research, many works improved model capabilities by building large instruction datasets. However, LIMA [19] proposed that \u201cless alignment is more\u201d showing that fine-tuning the model with only 1,000 high-quality samples can achieve a performance comparable to GPT-4. Appropriate data filtering strategies can improve learning efficiency and help reduce hallucinations caused by overtraining [2].\nALPAGASUS [20] uses ChatGPT for scoring but might miss the target model's strengths and lacks clarity. The forgetting score [21] monitors shifts in sample classification during training. GraNd [22] trims data based on the sample's gradient magnitude. Both forgetting score and GraNd are costly, as they need constant model updates, prolonging training time.\nInstruction Following Difficulty (IFD) [23] stands out for its efficiency, using the representation features of the target model to identify high-quality instruction data. It provides a simpler, cheaper, and interpretable approach by computing the generation complexity of the answer using a single fixed scoring model."}, {"title": "III. METHOD", "content": "The overview of Self-Evolution is illustrated in Fig. 1. To start, Self-Evolution requires a LLM \\(\\theta_0\\) as the initial"}, {"title": null, "content": "QA\u00b9 generation model and scoring model and a collection of domain-related documents T. Self-Evolution consists of three phases. In the first stage, the QA generation model generates QA pairs based on the domain-related documents. In the second phase, the scoring model and a scoring metric are employed to identify valuable samples from all historical instruction QA pairs. In the third phase, these valuable instruction samples are used to conduct a new round of IFT, reinforcing the model's domain knowledge. These three phases iterate continuously until the desired performance is achieved. The following sections will provide a detailed description of these phase."}, {"title": "A. QA generation", "content": "More new QA data are generated in the QA generation phase. Self-Evolution constructs new questions and answers based on each domain-related document rather than deriving them from manually constructed questions.\nThis questions generation process is represented as \\(q_{ij} = LLM(\\theta_i, t_j)\\), where \\(t_j\\) is the j-th document in T. During this process, we design delicated prompt to prioritize two key aspects: 1) Question conciseness: Preventing the generation of content with multiple sub-questions, which could lead to model hallucinations (Note 2). 2) Question validity: Ensuring each generated question is answerable (Note 6). The detailed prompt used for question generation is as follows:"}, {"title": null, "content": "This answer generation process is represented as \\(a_{ij} = LLM(\\theta_i, t_j, q_{ij})\\). Incorporating \\(t_j\\), ensures that the questions are correctly answered. In this process, we emphasize response completeness, ensuring that the generated content is a complete answer rather than one containing pronouns referring back to the document."}, {"title": "B. Data Selection And Training", "content": "Prior to conducting the i-th round of IFT, we can filter and select a subset of instruction data from the previous i - 1 rounds to enhance the training process. Self-Evolution employs the IFD metric [23] to identify more valuable instruction data. Equation 3 represents the calculation method for the IFD score, while Equations 1 and 2 denote the Conditioned Answer Score and Direct Answer Score, respectively.\n\\(S_c(A | Q) = -\\frac{1}{N} \\sum_{i=1}^{N} log P(w_i^A | Q, w_1^A, ..., w_{i-1}^A; \\theta)\\) (1)\n\\(S_d(A) = -\\frac{1}{N} \\sum_{i=1}^{N} log P(w_i^A | w_1^A, ..., w_{i-1}^A; \\theta)\\) (2)\n\\(IFD(Q, A) = \\frac{S_c(A | Q)}{S_d(A)}\\) (3)\nThe Conditioned Answer Score quantifies a model's ability to produce responses that align with both the given instructions and the correct answers. It assesses the model's output congruence with the directive and the expected solution. The Direct Answer Score evaluates the LLM's capacity to independently generate correct answers, reflecting the answer's intrinsic complexity in the absence of contextual instructions. A high IFD score indicates the model's difficulty in aligning responses with instructions, thereby highlighting the instruction's complexity."}, {"title": "C. Next Iteration", "content": "Self-Evolution employs a model self-evolution scheme. To elucidate the principles underlying this scheme, we define a scoring function score = f(q, a) that evaluates the quality of an answer a with respect to a question q. As previously mentioned, \\(a = LLM(\\theta_i, q)\\) denote the response of model \\(\\theta_i\\) to q, and \\(a' = LLM(\\theta_i, t, q)\\) represent the response of model \\(\\theta_i\\) to q given a highly relevant knowledge document t. We define \\(\\theta_{i+1} = IFT(\\theta_i, q, a)\\) as the next-generation model \\(\\theta_{i+1}\\) resulting from fine-tuning \\(\\theta_i\\) on the instruction data pair (q, a). We leverage In-context Learning [24] to establish the first inequality:\n\\(f(q, LLM(\\theta_i, q)) \\le f(q, LLM(\\theta_i, t, q))\\) (4)\nThis inequality demonstrates that the instruction data \\(((q, LLM(\\theta_i, t, q))\\) provides valuable learning opportunities for model \\(\\theta_i\\). Consequently, we derive \\(\\theta_{i+1}\\) through \\(\\theta_{i+1} = IFT(\\theta_i, q, a)\\). Post-training, we obtain the second inequality:\n\\(f(q, LLM(\\theta_i, q)) \\le f(q, LLM(\\theta_{i+1}, q))\\) (5)\nThus, model \\(\\theta_i\\) completes one iteration of evolution. The iterative process can be terminated by setting an iteration threshold. Empirically, this threshold is proportionally related to the model's parameter count and inversely related to the data volume. Smaller parameter counts tend to be more susceptible to hallucinations, necessitating threshold adjustments based on both parameter count and data volume."}, {"title": "IV. EXPERIMENTAL SETUP", "content": null}, {"title": "A. Model and Dataset", "content": "The base model selected for our experiments is Qwen1.5-7B-Chat [1], denoted as \\(\\theta_0\\). We use the LoRA (Low-Rank Adaptation) [25] method to fine-tune models. The LORA hyperparameters are configured as follows: lora-rank is set to 4, and lora-alpha is set to 8. Notably, we set lora-target to \"all\" [11], which enables us to achieve superior training results. The model chosen for IFD scoring is Qwen1.5-7B-Chat, denoted as \\(\\theta_{IFD}\\). It is important to note that \\(\\theta_{IFD}\\) does not participate in the subsequent training process. Its parameters remain fixed throughout the iteration process, ensuring consistent scoring criteria in each round of evaluation.\nWe select 4,000 valuable internal knowledge documents from China Mobile, denoted as T, where \\(|T| = 4000\\). As shown in Table III, T contains crucial operational knowledge such as alert analysis, configuration analysis, and operational experience, enabling operation engineers to quickly familiarize themselves with and solve problems. These knowledge documents are incorporated into the training process. Specifically,"}, {"title": "B. Baseline", "content": "1) Qwen1.5-7B-Chat-Fine-Tuned by High Quality QA: The Qwen1.5 series of language models has demonstrated exceptional performance in the Chinese language domain [1], with Qwen1.5-72B-Chat achieving capabilities comparable to GPT-3.5 on certain tasks. Consequently, we utilized Qwen1.5-72B-Chat to generate 4,000 high-quality question-answer pairs"}, {"title": "V. EVALUATION METRICS", "content": "We use the BLEU [26] score of \\(\\theta_{HQ}\\), denoted as BLEU(\\(0_{HQ}\\)), as a benchmark score and calculate the relative scores of other models in comparison to it. The performance score for a model \\(\\theta\\) is calculated as:\n\nScore = \\frac{BLEU(\\theta)}{BLEU(\\theta_{HQ})}\\) (6)\nTo better illustrate the differences between methods, we let \\(\\theta_{HQ}\\) serves as a target model for comparison. We collected 100 valuable subjective questions internally from China Mobile, which are related to the knowledge documents T. These questions can reflect the model's learning of T through question-answering performance. This score represents how closely a given model's performance in the domain-specific task approaches that of the optimally fine-tuned model HQ."}, {"title": "VI. EXPERIMENTAL RESULTS", "content": "We compare Qwen1.5-7B-Chat, trained using Self-Evolution, with multiple baseline models. As shown in Figure 2, untrained models perform poorly in domain-specific"}, {"title": "VII. ABLATION EXPERIMENT", "content": null}, {"title": "A. Historical Data Retrieval Module", "content": "One of the core components of Self-Evolution is the historical data retrieval module. To investigate its specific role, we designed targeted experiments. After generating the instruction data \\(D_i\\) in the i-th iteration, instead of performing historical data retrieval, we directly used it as the complete training dataset. The results, as shown in Figure 3, indicate that the iterated model failed to surpass the performance of HQ, and the training effectiveness was compromised to some extent. This demonstrates that historical instruction data is valuable and needs to be retrieved and relearned."}, {"title": "B. Historical Data Retrieval Strategy", "content": "To validate the effectiveness of using IFD scores for efficient historical instruction data filtering in Self-Evolution, we designed two experiments.\nTo demonstrate our data filtering's logic, the corresponding experiment used all previously generated data for training. The results, as shown in Figure 3, indicate that the performance of the iterated model rapidly deteriorated. Training with too much data caused the model to hallucinate. In the eight-iteration experiment, the full retrieval strategy took about three times longer than Self-Evolution. This proves that discarding a portion of the data not only accelerates training speed but also enhances training effectiveness.\nTo demonstrate the superiority of our data filtering strategy, we designed an experiment using a random retrieval strategy"}, {"title": null, "content": "during the recall phase, where k instruction data were randomly recalled from historical instruction data and added to the training set. Figure 3 shows performance gains only in the first two generations, with further training harming results. This indicates the need for a proper data filtering strategy, as an unstable retrieval approach can degrade model performance.\nIn the aforementioned experiments, we tested three alternative approaches: removing the historical data retrieval module, employing a full retrieval strategy, and using a random retrieval strategy. All of these approaches resulted in some degree of performance degradation compared to Self-Evolution. These results demonstrate that the data retrieval module in Self-Evolution is essential, and the data filtering strategy centered on IFD plays a crucial role in the method's effectiveness."}, {"title": "VIII. CONCLUSION AND FUTURE WORK", "content": "In this paper, we address a key challenge in applying LLMs to the specific domain: the difficulty in utilizing vast amounts of unlabeled knowledge documents. To tackle this issue, we employ self-alignment in Self-Evolution to rapidly construct a large volume of instruction data. As the iteration progresses, both the model's capabilities and the quality of generated data improve. To maximize the utilization of instruction data generated in each iteration, we use IFD scores to filter out high-quality data to assist in training. In the China Mobile business question-answering evaluation, our approach, using only a 7B model throughout, outperforms solutions assisted by 72B models, conserves a significant amount of computational resources.\nIn current business scenarios, multi-turn dialogue capabilities are becoming increasingly important. Therefore, in future work, we plan to extend Self-Evolution to improve the model's domain-specific multi-turn dialogue capabilities using only unsupervised text data."}]}