{"title": "Enhanced Fine-Tuning of Lightweight Domain-Specific Q&A Model Based on Large Language Models", "authors": ["Shenglin Zhang", "Pengtian Zhu", "Minghua Ma", "Jiagang Wang", "Yongqian Sun", "Dongwen Li", "Jingyu Wang", "Qianying Guo", "Xiaolei Hua", "Lin Zhu", "Dan Pei"], "abstract": "Large language models (LLMs) excel at general question-answering (Q&A) but often fall short in specialized domains due to a lack of domain-specific knowledge. Commercial companies face the dual challenges of privacy protection and resource constraints when involving LLMs for fine-tuning. This paper propose a novel framework, Self-Evolution, designed to address these issues by leveraging lightweight open-source LLMs through multiple iterative fine-tuning rounds. To enhance the efficiency of iterative fine-tuning, Self-Evolution employ a strategy that filters and reinforces the knowledge with higher value during the iterative process. We employed Self-Evolution on Qwen1.5-7B-Chat using 4,000 documents containing rich domain knowledge from China Mobile, achieving a performance score 174% higher on domain-specific question-answering evaluations than Qwen1.5-7B-Chat and even 22% higher than Qwen1.5-72B-Chat. Self-Evolution has been deployed in China Mobile's daily operation and maintenance for 117 days, and it improves the efficiency of locating alarms, fixing problems, and finding related reports, with an average efficiency improvement of over 18.6%. In addition, we release Self-Evolution framework code in https://github.com/Zero-Pointer/Self-Evolution.", "sections": [{"title": "I. INTRODUCTION", "content": "With the emergence of large language models (LLMs) such as Qwen [1], LLaMA [2], and GPT [3], their exceptional gen- eration, understanding of complex language structures and di- alogue capabilities have garnered widespread attention [4], [5]. However, in specific domains, their performance often fails to meet practical requirements. For instance, GPT-4 may cite in- correct legal provisions when answering legal questions, lead- ing to erroneous analytical conclusions. ChatLaw-MoE [6], fine-tuned on high-quality law data, has outperformed GPT- 4 across multiple application scenarios. Therefore, enabling general models to acquire domain-specific knowledge allows for deploying a domain model with minimal computational resources, potentially outperforming general models with ten times the number of parameters.\nState-of-the-art approaches extensively utilize instruction fine-tuning (IFT) to align general-purpose models with spe- cific application domains and maximize their effectiveness. InstructGPT [7] employed instruction fine-tuning to bridge the performance gap between models with a hundredfold difference in parameter count. In the absence of instruction data, certain approaches [8]\u2013[11] use advanced LLMs to construct instruction datasets, achieving performance close to GPT-3.5 and GPT-4. However, these methods cannot guarantee the correctness and diversity of the generated instruction data. Fortunately, high-quality instruction data is scarce in most scenarios, while the volume of knowledge documents is enormous.\nIn summary, applying general-purpose models to specific domains presents the following challenges:\n1) Limitation of Computational Resources. Model per- formance is typically proportional to the scale of the model's parameters. However, fine-tuning and deploying powerful general-purpose language models requires sub- stantial computational resources. For example, a LLM with 72B parameters using fp16 precision requires five Tesla V100-32GB GPUs for inference. Fine-tuning such a model incurs even greater costs. This is prohibitively expensive and impractical for tasks that must be contin- uously available.\n2) High-quality data scarcity. Domain-specific high- quality instruction data is often scarce. Manually cor- recting instruction data requires significant human effort, making it expensive. A solution is needed to automat- ically construct high-quality data without human assis- tance.\n3) Lack of diversity and correctness. Firstly, using a fixed model to construct instruction data tends to generate overly similar data. Additionally, relying solely on the model's internal capabilities for data generation may result in incorrect or irrelevant data for the domain. The"}, {"title": "II. RELATED WORK", "content": "A. Instruction Fine-tuning\nThe potential of LLMs in the specific domain is vast and promising. For example, Microsoft deployed GPT to summarize anomalous events in its services [12]. However, as task complexity and requirements increase, instruction fine- tuning (IFT) is widely adopted to enhance model performance. FLAN [13] achieved significant improvements in generaliza- tion by fine-tuning a high-quality instruction dataset. Instruct- GPT [7] successfully aligned GPT-3 [3] with human intent by fine-tuning a dataset rich in real-world instruction forms and task types. OWL [14] collected numerous operation domain instructions and achieved remarkable results in log parsing and anomaly detection. However, these methods require a large amount of manually annotated data, which becomes a bottleneck for widespread application due to the high cost.\nB. Instruction Data Generation\nResearchers have extensively explored methods to reduce human involvement in generating instruction data. Some meth- ods [8]\u2013[10], [15] use advanced commercial models to create instruction datasets. For instance, Alpaca [8] uses a small amount of manually constructed data to extract knowledge from DaVinci-003 [16], creating a 52k instruction dataset. It fine-tunes LLaMA to achieve performance close to GPT-3.5. Peng et al. [9] extract knowledge from GPT-4, resulting in higher quality and more diverse responses.\nAnother class of methods [11], [17], [18] employs a self- guided approach. These methods extract knowledge from the model and then use this newly constructed data to enhance domain or task capabilities. Self-Instruct [17], for instance, proposes using self-generated samples to enhance the instruction-following ability of pre-trained language models. Self-Align [18] mainly adopts topic-guided red-blue adversar- ial self-guidance and principle-driven self-calibration to con- struct data and fine-tune models, requiring less than 300 lines of manually constructed data (including 195 seed prompts, 16 principles, and five examples) to achieve high-quality fine- tuned model. The potential of these self-guided methods is certainly worth exploring further.\nHowever, these methods still require manually constructed supervision data and are limited by the model's inherent knowledge constraints, preventing them from generating in- struction data beyond the model's capabilities.\nC. Instruction Data Selection\nIn the early stages of IFT research, many works improved model capabilities by building large instruction datasets. How- ever, LIMA [19] proposed that \u201cless alignment is more\u201d showing that fine-tuning the model with only 1,000 high- quality samples can achieve a performance comparable to GPT-4. Appropriate data filtering strategies can improve learn- ing efficiency and help reduce hallucinations caused by over- training [2].\nALPAGASUS [20] uses ChatGPT for scoring but might miss the target model's strengths and lacks clarity. The forget- ting score [21] monitors shifts in sample classification during training. GraNd [22] trims data based on the sample's gradient magnitude. Both forgetting score and GraNd are costly, as they need constant model updates, prolonging training time.\nInstruction Following Difficulty (IFD) [23] stands out for its efficiency, using the representation features of the target model to identify high-quality instruction data. It provides a simpler, cheaper, and interpretable approach by computing the generation complexity of the answer using a single fixed scoring model."}, {"title": "III. METHOD", "content": "The overview of Self-Evolution is illustrated in Fig. 1. To start, Self-Evolution requires a LLM \u03b8\u2080 as the initial\nQA\u00b9 generation model and scoring model and a collection of domain-related documents T. Self-Evolution consists of three phases. In the first stage, the QA generation model generates QA pairs based on the domain-related documents. In the second phase, the scoring model and a scoring metric are employed to identify valuable samples from all histori- cal instruction QA pairs. In the third phase, these valuable instruction samples are used to conduct a new round of IFT, reinforcing the model's domain knowledge. These three phases iterate continuously until the desired performance is achieved. The following sections will provide a detailed description of these phase.\nA. QA generation\nMore new QA data are generated in the QA generation phase. Self-Evolution constructs new questions and answers based on each domain-related document rather than deriving them from manually constructed questions.\nThis questions generation process is represented as q\u1d62\u2c7c = LLM(\u03b8\u1d62, t\u2c7c), where t\u2c7c is the j-th document in T. During this process, we design delicated prompt to prioritize two key aspects: 1) Question conciseness: Preventing the generation of content with multiple sub-questions, which could lead to model hallucinations (Note 2). 2) Question validity: Ensuring each generated question is answerable (Note 6).\nThis answer generation process is represented as a\u1d62\u2c7c = LLM(\u03b8\u1d62, t\u2c7c, q\u1d62\u2c7c). Incorporating t\u2c7c, ensures that the questions are correctly answered. In this process, we emphasize response completeness, ensuring that the generated content is a com- plete answer rather than one containing pronouns referring back to the document.\nAfter obtaining the newly generated questions and answers, Self-Evolution combines them into new instruction data D\u1d62 = {(q\u1d62\u2080, A\u1d62\u2080), (q\u1d62\u2081, A\u1d62\u2081), ..., (q\u1d62|T|, A\u1d62|T|)}.\nB. Data Selection And Training\nPrior to conducting the i-th round of IFT, we can filter and select a subset of instruction data from the previous i \u2212 1 rounds to enhance the training process. Self-Evolution em- ploys the IFD metric [23] to identify more valuable instruction data. Equation 3 represents the calculation method for the IFD score, while Equations 1 and 2 denote the Conditioned Answer Score and Direct Answer Score, respectively.\n$S_e(A|Q) = -\\frac{1}{N}\\sum_{i=1}^{N}log P(w_i^A|w_1^Q, w_2^Q, ..., w_{i-1}^Q;\\theta)$ (1)\n$S_e(A) = -\\frac{1}{N}\\sum_{i=1}^{N}log P(w_i^A|w_1^A, w_2^A, ..., w_{i-1}^A;\\theta)$ (2)\n$IFD(Q, A) = \\frac{S_e(A|Q)}{S_e(A)}$ (3)\nThe Conditioned Answer Score quantifies a model's ability to produce responses that align with both the given instructions and the correct answers. It assesses the model's output congru- ence with the directive and the expected solution. The Direct Answer Score evaluates the LLM's capacity to independently generate correct answers, reflecting the answer's intrinsic complexity in the absence of contextual instructions. A high IFD score indicates the model's difficulty in aligning responses with instructions, thereby highlighting the instruction's com- plexity."}, {"title": "IV. EXPERIMENTAL SETUP", "content": "A. Model and Dataset\nThe base model selected for our experiments is Qwen1.5- 7B-Chat [1], denoted as \u03b8\u2080. We use the LoRA (Low-Rank Adaptation) [25] method to fine-tune models. The LoRA hyperparameters are configured as follows: lora-rank is set to 4, and lora-alpha is set to 8. Notably, we set lora-target to \"all\" [11], which enables us to achieve superior training results. The model chosen for IFD scoring is Qwen1.5-7B- Chat, denoted as \u03b8\u1d62_{IFD}. It is important to note that \u03b8\u1d62_{IFD} does not participate in the subsequent training process. Its param- eters remain fixed throughout the iteration process, ensuring consistent scoring criteria in each round of evaluation.\nWe select 4,000 valuable internal knowledge documents from China Mobile, denoted as T, where |T| = 4000. As shown in Table III, T contains crucial operational knowledge such as alert analysis, configuration analysis, and operational experience, enabling operation engineers to quickly familiarize themselves with and solve problems. These knowledge docu- ments are incorporated into the training process. Specifically,"}, {"title": "V. EVALUATION METRICS", "content": "We use the BLEU [26] score of \u03b8_{HQ}, denoted as BLEU(\u03b8_{HQ}), as a benchmark score and calculate the relative scores of other models in comparison to it. The performance score for a model \u03b8 is calculated as:\n$Score = \\frac{BLEU(\\theta)}{BLEU(\\theta_{HQ})}$ (6)\nTo better illustrate the differences between methods, we let \u03b8_{HQ} serves as a target model for comparison. We col- lected 100 valuable subjective questions internally from China Mobile, which are related to the knowledge documents T. These questions can reflect the model's learning of T through question-answering performance. This score represents how closely a given model's performance in the domain-specific task approaches that of the optimally fine-tuned model HQ."}, {"title": "VI. EXPERIMENTAL RESULTS", "content": "We compare Qwen1.5-7B-Chat, trained using Self- Evolution, with multiple baseline models. As shown in Fig- ure 2, untrained models perform poorly in domain-specific knowledge question answering tasks. The \u03b8_{HQ} model, fine- tuned with high-quality data, demonstrates excellent perfor- mance. Notably, Self-Evolution surpasses the performance of both GPT-3.5 and Qwen1.5-72B-Chat in its first iteration. As the iterations progress, the model's performance gradually approaches that of \u03b8_{HQ}, ultimately surpassing it by the seventh round. Based on the above experiments, we can conclude that Self-Evolution enables Qwen1.5-7B-Chat to surpass the performance of Qwen1.5-72B-Chat-assisted alignment. This demonstrates the effectiveness of the proposed method."}, {"title": "VII. ABLATION EXPERIMENT", "content": "A. Historical Data Retrieval Module\nOne of the core components of Self-Evolution is the histor- ical data retrieval module. To investigate its specific role, we designed targeted experiments. After generating the instruction data D\u1d62 in the i-th iteration, instead of performing historical data retrieval, we directly used it as the complete training dataset. The results, as shown in Figure 3, indicate that the iterated model failed to surpass the performance of HQ, and the training effectiveness was compromised to some extent. This demonstrates that historical instruction data is valuable and needs to be retrieved and relearned.\nB. Historical Data Retrieval Strategy\nTo validate the effectiveness of using IFD scores for effi- cient historical instruction data filtering in Self-Evolution, we designed two experiments.\nTo demonstrate our data filtering's logic, the corresponding experiment used all previously generated data for training. The results, as shown in Figure 3, indicate that the performance of the iterated model rapidly deteriorated. Training with too much data caused the model to hallucinate. In the eight-iteration experiment, the full retrieval strategy took about three times longer than Self-Evolution. This proves that discarding a portion of the data not only accelerates training speed but also enhances training effectiveness.\nTo demonstrate the superiority of our data filtering strategy, we designed an experiment using a random retrieval strategy"}, {"title": "VIII. CONCLUSION AND FUTURE WORK", "content": "In this paper, we address a key challenge in applying LLMs to the specific domain: the difficulty in utilizing vast amounts of unlabeled knowledge documents. To tackle this issue, we employ self-alignment in Self-Evolution to rapidly construct a large volume of instruction data. As the iteration progresses, both the model's capabilities and the quality of generated data improve. To maximize the utilization of instruction data generated in each iteration, we use IFD scores to filter out high-quality data to assist in training. In the China Mobile business question-answering evaluation, our approach, using only a 7B model throughout, outperforms solutions assisted by 72B models, conserves a significant amount of computational resources.\nIn current business scenarios, multi-turn dialogue capabili- ties are becoming increasingly important. Therefore, in future work, we plan to extend Self-Evolution to improve the model's domain-specific multi-turn dialogue capabilities using only unsupervised text data."}]}