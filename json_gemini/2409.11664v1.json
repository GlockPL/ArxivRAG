{"title": "Agent Aggregator with Mask Denoise Mechanism for Histopathology Whole Slide Image Analysis", "authors": ["Xitong Ling", "Minxi Ouyang", "Yizhi Wang", "Xinrui Chen", "Renao Yan", "Hongbo Chu", "Junru Cheng", "Tian Guan", "Sufang Tian", "Xiaoping Liu", "Yonghong He"], "abstract": "Histopathology analysis is the gold standard for medical diagnosis. Accurate classification of whole slide images (WSIs) and region-of-interests (ROIs) localization can assist pathologists in diagnosis. The gigapixel resolution of WSI and the absence of fine-grained annotations make direct classification and analysis challenging. In weakly supervised learning, multiple instance learning (MIL) presents a promising approach for WSI classification. The prevailing strategy is to use attention mechanisms to measure instance importance for classification. However, attention mechanisms fail to capture inter-instance information, and self-attention causes quadratic computational complexity. To address these challenges, we propose AMD-MIL, an agent aggregator with a mask denoise mechanism. The agent token acts as an intermediate variable between the query and key for computing instance importance. Mask and denoising matrices, mapped from agents-aggregated value, dynamically mask low-contribution representations and eliminate noise. AMD-MIL achieves better attention allocation by adjusting feature representations, capturing micro-metastases in cancer, and improving interpretability. Extensive experiments on CAMELYON-16, CAMELYON-17, TCGA-KIDNEY, and TCGA-LUNG show AMD-MIL's superiority over state-of-the-art methods.", "sections": [{"title": "1 INTRODUCTION", "content": "The advancement of deep learning technologies and increased computational capacity have significantly enhanced the field of computational pathology [2, 12, 16, 23]. This progress assists physicians in diagnosis and standardizes pathological diagnostics [7, 20]. However, analyzing histopathology whole slide images (WSIs) significantly differs from typical computer vision tasks [19]. A single WSI, with its gigapixel resolution, makes obtaining pixel-level annotations impracticable, in contrast to natural images [6]. The Multiple Instance Learning (MIL) method is currently the mainstream framework for analyzing histopathology slides using only WSI-level annotations [18, 24, 31]. MIL methods consider the entire WSI as a bag, with each patch within it as an instance [4, 29]. If any instance within the WSI is classified as cancerous, then the entire WSI is labeled as such [3, 28]. The WSI is labeled as normal only if all instances within it are normal.\nCurrent MIL methods have two stages: segmenting the WSI into patches and using a pre-trained feature extractor to embed features. These features are then aggregated using methods such as mean-pooling, max-pooling, ABMIL [11], DSMIL [13], and TransMIL [21], and then mapped for classification.\nABMIL [11] and DSMIL [13] use lightweight attention mechanisms for information aggregation. However, they overlook relationships between instances, hindering global modeling and capturing long-distance dependencies. TransMIL introduces self-attention [27] within MIL's aggregator. Self-attention calculates relations between any two patches in a WSI, capturing long-distance dependencies. It also dynamically allocates weights based on input importance, enhancing the model's ability to process complex data. However, the quadratic complexity of self-attention challenges its application in MIL aggregators. TransMIL uses Nystr\u00f6m [33] Self-attention instead. Nystr\u00f6m Self-attention selects a subset of elements, known as landmarks, to approximate attention scores. Nystr\u00f6m Self-attention down-samples query and key vectors locally along the instance token dimension. This approach has two main issues: sampling based on adjacent instances can dilute significant instance contributions, and the variance in instance numbers across bags requires padding during down-sampling. This can lead to aggregation imbalances and unstable outcomes.\nTo address the quadratic complexity issue of self-attention, Trans-MIL [21] employs Nystr\u00f6m attention [33] as the substitute for the standard self-attention module. Nystr\u00f6m attention selects a subset of sequence elements, also known as landmarks, to approximate the attention scores for the entire sequence. Specifically, in the Nystr\u00f6m attention mechanism, the local downsampling of query and key matrices is implemented along the dimension of the instance tokens. This approach has two significant issues. Firstly, since the sampling process relies on adjacent instances, many insignificant ones might dilute the impact of significant instances. Secondly, equidistant division is not always the optimal sampling strategy, as the distribution of information in a sequence may be uneven. Fixed sampling intervals might fail to capture all crucial information points, leading to a decrease in approximation quality.\nTo address these challenges, We transform the pooling agent into trainable matrices for effective mapping. Furthermore, to indirectly achieve a more rational distribution of attention scores through adjustments in instance representations, we introduce the mask denoise mechanism for dynamic adaptation.\nAgent attention [9] introduces the agent tokens in addition to query, key, and value tokens. Agent tokens act as the agent for the query tokens, aggregating information from the key and value tokens, and then information is returned to the query tokens via a broadcasting mechanism. Given the lesser number of agent tokens compared to sequence tokens, the agent mechanism can reduce the computational load of standard self-attention. However, agent tokens are obtained through mean pooling of the query tokens in standard agent attention, making it challenging to adapt to the variable-length token inputs of pathological multiple instance tasks. Additionally, mean pooling, by aggregating features through local averaging, may result in missing important information. Consequently, we adjust the number of agent tokens as a hyperparameter and substitute the mean pooling agent tokens with trainable agent tokens.\nMoreover, we introduce the mask denoise mechanism to dynamically refine attention scores by adjusting instance representations. Mask and denoising matrices, matching the agent's aggregated value dimension, are generated by projecting this value through a linear layer. Mask matrices transform into binary matrices via threshold filtering, not directly from the value token but their high-level mapping, allowing dynamic adaptation to the input. Then, the mask directly multiplies with the value, filtering out non-significant representations. However, as the mask applies binary filtering to the value, it might suppress unimportant instances excessively, thereby introducing relative noise. Therefore, we introduce the denoising matrices from the agent-aggregated values to correct the relative noise. We conducted extensive comparative experiments and ablation studies on four datasets to verify the effectiveness of the trainable agent aggregator and mask denoise mechanism."}, {"title": "2 RELATED WORK", "content": "2.1 Multiple Instance Learning for WSI Analysis\nMIL methods demonstrate significant potential in classifying and analyzing histopathology images. In this framework, a WSI is treated as a bag, and its local regions are instances. MIL paradigms are categorized into three types: instance-based, embedding-based, and bag-based methods. The instance-based method scores each instance and then aggregates these scores to predict the bag's label. The embedding-based method employs a pre-trained feature encoder to generate instance representations, which are then aggregated for classification. This enhances fit with Deep Neural Networks (DNN) but reduces the interpretability. Bag-based approaches classify by comparing distances between bags, with the main challenge being identifying a universal distance metric. Current MIL advancements focus on developing specialized feature encoders, enhancing aggregators, data augmentations, and improving training strategies.\nFeature encoders pre-trained on natural images often struggle to extract high-level histopathology features, such as specific textures and morphological structures. Transpath [30] trains A hybrid architecture of CNN and a swin-transformer feature encoder on one hundred thousand WSIs using a semantically relevant contrastive learning approach. IBMIL [15] also utilizes a feature encoder pre-trained on nine pathological datasets through a self-supervised method with MOCOv3 [32]. Representations generated by these pathology-specific feature extractors significantly outperform those obtained from feature encoders pre-trained on ImageNet [5] in downstream tasks.\nThe most common aggregation strategies for instance-based and embedding-based methods include pooling and attention mechanisms. Mean-MIL and Max-MIL aggregate representations and categorize through the average and maximum values respectively, but fixed aggregation mechanisms cannot adapt to varying inputs. In contrast, ABMIL employs attention mechanisms to aggregate features through trainable weights. Similarly, CLAM uses gated attention and a top-k selection strategy for bag label prediction. TransMIL, on the other hand, applies a linear approximation of self-attention to explore relationships between instances. WiKG [14] introduces a knowledge-aware attention mechanism, enhancing the capture of relative positional information among instances. HAT-Net+ [1] advances cell graph classification by leveraging a unique, parameter-free strategy to dynamically merge multiple hierarchical representations, effectively capturing the complex relationships and dependencies within cell graphs.\nTo enhance performance and stability, various methods employ data augmentation. For example, DTFD [35] increases the number of bags using a partitioning pseudo-bag split strategy and uses the double-tier MIL framework to use the intrinsic features. In terms of training strategies, IBMIL [15] uses interventional training to reduce contextual priors. SSC-MIL [34] leverages semantic-similarity knowledge distillation to exploit latent bag information. MHIM-MIL [25] addresses key instances with hard example mining. LNPL-MIL [22] proposes the training strategy of learning from noise pseudo labels, which can address the problem of semantical unalignment between instances and the bag."}, {"title": "2.2 Approximate Self-Attention Mechanism", "content": "The self-attention [27] mechanism is capable of grasping dependencies over long distances to facilitate comprehensive modeling, but its quadratic complexity limits the increase in input sequence length. Consequently, research on approximate self-attention mechanisms aims to reduce the complexity to linear while maintaining global modeling capability.\nNystr\u00f6m attention [33] uses the Nystr\u00f6m method to estimate eigenvalues and eigenvectors, approximating self-attention [27] by selecting a few landmarks, reducing computational and storage needs. Focused Linear attention [8] uses nonlinear reweighting to focus on important features. Agent attention [9] introduces agents representing key information, significantly lowering computational complexity by computing attention only among these agents.\nThese advancements in approximate attention mechanisms provide a new perspective for enhancing aggregators in MIL methods."}, {"title": "3 METHODOLOGY", "content": "3.1 MIL and Feature extraction\nIn the MIL methodology, each WSI is conceptualized as a labeled bag, wherein its constituent patches are considered as instances possessing indeterminate labels. Taking binary classification of WSIs as an example, the input WSI X is divided into numerous patches {(x1, y1), \u00b7\u00b7\u00b7, (XN, YN)}, encompassing N instances of xi. Under the MIL paradigm, the correlation between the bag's label, Y, and the labels of instances yi is established as follows:\n$$Y = \\begin{cases} 1, & \\text{iff } \\sum y_i > 0 \\\\ 0, & \\text{else} \\end{cases}$$\nGiven the undisclosed nature of the labels for the instances yi, the objective is to develop a classifier, M(X), tasked with estimating \u0176. In alignment with methodologies prevalent in contemporary research, the classifier can be delineated into three steps: feature extraction, feature aggregation, and bag classification. These processes can be defined as follows:\n$$\\widehat{Y} \\leftarrow M(X) := h(g(f(X))),$$\nwhere f, g, and h represent the feature extractor, feature aggregator, and the MIL classifier.\nThe feature aggregator is considered to be the most important part of summarizing features, which can aggregate features of different patches. The attention mechanisms can discern the importance of patches in a WSI, and it is widely used in the feature aggregator. Attention-based and self-attention-based MIL are the main methods currently used.\nIn the attention-based MIL [35], the feature aggregator can be defined as:\n$$G = \\sum_{i=1}^{N} a_i h_i = \\sum_{i=1}^{N} a_i f(x_i) \\qquad (x_i) \\in \\mathbb{R}^D,$$\nwhere G is the bag representation, hi \u2208 RD is the extracted feature for the patch xi through the feature extractor f, ai is the trainable scalar weight for hi and D is the dimension of vector G and hi.\nIn the self-attention-based [27] MIL, the feature aggregator can be defined as:\n$$Q = HW_Q, K = HW_K, V = HW_V,$$\n$$O = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_q}})V = SV,$$\nwhere WQ, WK, and WV represent trainable matrices, H denotes the collection of patch features, O has integrated the attributes of the other features, and dq is the dimension of the query vector."}, {"title": "3.2 Attention Aggregator", "content": "During the computation of Sim(Q, K) as defined in Eq. 5, the algorithmic complexity scales quadratically with O(N\u00b2). Given that N frequently comprises several thousand elements, this substantially extends the expected computational time. Linear attention offers a reduction in computational time but at the expense of information. To mitigate this issue, transmil [21] employs the Nystr\u00f6m approximation for Eq. 5 [33]. The matrices Q and K are constructed, and the mean of each segment is computed as follows:\n$$\\overline{Q} = [\\overline{q_1};...;\\overline{q_m}], \\quad \\overline{q_j} = \\frac{1}{m} \\sum_{i=(j-1)xl+1}^{(j-1)xl+m} q_i, \\quad j = 1,..., m$$\n$$\\overline{K} = [\\overline{k_1};...;\\overline{k_m}], \\quad \\overline{k_j} = \\frac{1}{m} \\sum_{i=(j-1)xl+1}^{(j-1)xl+m} k_i, \\quad j = 1,..., m$$\nwhere \\(\\overline{Q} \\in \\mathbb{R}^{m \\times D}\\) and \\(\\overline{K} \\in \\mathbb{R}^{m \\times D}\\).\nThe approximation of the \u015c in Eq. 5 can then be expressed as:\n$$\\widehat{S} = \\text{softmax}(\\frac{\\overline{Q} \\overline{K}^T}{\\sqrt{d_q}})V = Z \\text{softmax}(\\frac{\\overline{K} \\overline{Q}^T}{\\sqrt{d_q}})$$\nwhere, Z* represents the approximate solution to z(\u00d5, \u0158, Z) = 0, necessitating a linear number of iterations for convergence.\nIn MIL tasks, Nystr\u00f6m attention filters out patches with important features because of the sampling mechanism. Moreover, the difference in N will lead to an overall imbalance during local down-sampling. So we consider agent attention methods with linear time complexity and the agent attention mechanism [9] can be written as:\n$$O = \\sigma(Q A^T) \\sigma(A K^T) V,$$\nwhere \u03c3(\u00b7) is the Softmax function, Q, K, V are defined in equation Eq. 4. Here A \u2208 Rn\u00d7D is the agent matrix pooling from Q. The term D stands for the feature dimension, while n refers to the agent dimension and acts as a hyperparameter.\nGiven that the agent is non-trainable and the distribution of attention scores may not be optimal, it becomes imperative to establish an adaptive agent capable of dynamically adjusting the attention score distribution to enhance model performance and flexibility."}, {"title": "3.3 Agent Mask Denoise Mechanism", "content": "As illustrated in Figure 1, our overall framework is based on Eq. 5 and Eq. 9. The proposed overall framework is shown in Figure 2. Before the input features are processed by the model, a class token is embedded into them, resulting in the feature matrix H \u2208 RD\u00d7(N+1), where D is the dimension of the features and (N + 1) represents the number of patches, including the embedded class token.\nTrainable Agent. In the previously outlined methodology, matrix A in Eq. 9 is initially from matrix Q through mean pooling, A = pooling(Q) \u2208 Rn\u00d7D, indicating a limitation in encapsulating the entirety of information present within Q. To overcome this limitation, A is defined as a trainable matrix. Through matrix A \u2208 Rn\u00d7D, the intermediate matrices QA = QAT \u2208 R(N+1)\u00d7n and KA = AKT \u2208 Rn\u00d7(N+1) can be obtained. Utilizing the general attention strategy, the intermediate variable is:\n$$V_A = \\sigma(K_A)V$$\n$$= \\sigma(A K^T)V \\in \\mathbb{R}^{n \\times D},$$\nMask Agent. In this MIL task, most regions of a WSI do not contribute much to the prediction, so a learnable mask is generated by using the trainable threshold to mask the information.\n$$\\tau = \\sigma(p(W_V)),$$\nwhere WV \u2208 R1\u00d7D, function p is an adjustable aggregate function such as mean-pooling, and t is the threshold used in Eq. 12.\nCalculate the importance of each feature to optimize the important features in the hidden space. The selection of features will have the risk of information loss. To balance important information selection and the original characteristics of the aggregation, we proposed a new module which can be defined as:\n$$VMD_{ij} = VA_{ij} M_{ij>t} + DN_{ij},$$\nwhere M = WMVA is the threshold matrix to obtain the importance of each feature, and DN = WDNVA is the denoise matrix to aggregate information. Here, WM and WDN are learnable parameters.\nAgent Visualization. The foundational agent attention architecture lacks the capability to produce a variable concentration score for sequences. To address this limitation, we outline a methodology that facilitates the visualization of attention scores:\n$$Att_i = \\sum_{j=1}^{n} \\frac{QA_{i,j}KA_{j,i+1}}{\\sum_{k=1}^{n}QA_{o,k}KA_{k,i+1}}$$\nwhere Atti is the attention score of the feature hi.\nAMD. Establishing the aforementioned modules, we introduce a novel framework titled mask denoise mechanism. This framework, as illustrated in Figure 2, encompasses a learning-based agent attention mechanism, representation refinement, and feature aggregation. The algorithm process is shown in Algorithms 1 and the module can be expressed as:\n$$O = \\sigma(Q A^T) V_{MD},$$\nwhere md represents the mask denoise mechanism, and VMD represents the matrix calculated from the mechanism Eq. 12.\nDue to the difference in the threshold selection method, the other two feature threshold selection strategies are considered as follows:\n\u2022 Mean-AMD. Mean selection: selected the average value in the features as the threshold selected by all features.\n\u2022 CNN-AMD. CNN selection: through the method of group convolution, the characteristics of different groups are reduced, and the average value between the groups is the threshold."}, {"title": "4 EXPERIMENTS", "content": "4.1 Datasets and Evaluation Metrics\nIn our study, We use four public datasets to assess our approach. CAMELYON-16 is a dataset for early-stage breast cancer lymph node metastasis detection. The dataset comprises 399 WSIs, which are officially split into 270 for training and 129 for testing. We use 6-fold cross-validation to ensure that all data are utilized for both training and testing, thereby preventing overfitting to the official test set. In addition, we employ the pre-trained weights from the CAMELYON-16 dataset to perform inference on the external dataset CAMELYON-17 only once. Subsequently, we report both the mean and variance of the evaluation metrics. TCGA-LUNG includes 1034 WSIs: 528 from LUAD and 507 from LUSC cases. We split the dataset into 65:10:25 for training, validation, and testing. 4-fold cross-validation is used, and the mean and standard deviation of metrics are reported. TCGA-KIDNEY includes 1075 WSIs: 117 from KICH, 539 from KIRC, and 419 from KIRP cases. We split the dataset into 65:10:25 for training, validation, and testing. We use 4-fold cross-validation and report the mean and standard deviation of metrics. We report the mean and standard deviation of the macro F1 score, the AUC for one-versus-rest, and the slide-level accuracy (ACC)."}, {"title": "4.2 Implementation Details", "content": "During the pre-processing phase, we generate non-overlapping patches of 256x256 pixels at 20x magnification for the datasets CAMELYON-16, CAMELYON-17, TCGA-KIDNEY, and TCGA-LUNG. This procedure yields an average count of approximately 9024, 7987, 13266, and 10141 patches per bag for the respective datasets.\nUniform hyperparameters are maintained across all experiments. Each experiment is conducted on a workstation equipped with NVIDIA RTX A100 GPUs, utilizing the ImageNet [5] pre-trained ResNet50 [10] as the feature encoding model. The Adam optimization algorithm is used, incorporating a weight decay of 1e-5. The initial learning rate is set at 2e-4, and cross-entropy loss is employed as the loss function."}, {"title": "4.4 Interpretability Analysis", "content": "We conduct an interpretability analysis of AMD-MIL. In Figure 3, the blue-annotated areas denote the official annotations of cancerous regions in the CAMELYON dataset, whereas the heatmap regions represent the distribution of agent attention scores across all patches constituting the WSIs, calculated according to Eq. 13. The attention scores indicate the contribution level of instances to the classification outcome, and it is distinctly observable that areas with high attention scores align closely with the annotated cancerous regions. This demonstrates that the AMD-MIL classification relies on cancerous ROIs, mirroring the diagnostic process of pathologists and thereby providing substantial interpretability for clinical applications. AMD-MIL possesses robust localization capabilities for both macro-metastases and micro-metastases. For example, in Figure 3 (f), which includes both macro and micro-metastases, AMD-MIL can also concurrently localize to different areas."}, {"title": "4.5 Ablation Study", "content": "Effectiveness of Agent Aggregator. The trainable agent aggregator uses agent tokens as intermediate variables for the query and key in the self-attention mechanism. This approach ensures global modeling and approximates linear attention. We compare the trainable agent aggregator with the original pooling agent aggregator and the Nystr\u00f6m attention aggregator from TransMIL. The original pooling agent aggregator reduces parameter count using an agent mechanism and achieves enhanced global modeling through broadcasting. As shown in Table 2, it significantly outperforms the Nystr\u00f6m attention aggregator from TransMIL across three datasets. However, the pooling agent aggregator struggles to adapt dynamically to inputs, and its pooling mechanism may average out important instances. Initializing the agent as a trainable parameter results in improved metrics compared to the pooling agent aggregator. This change allows the model to better adapt to varying inputs and maintain the significance of crucial instances, thereby enhancing overall performance."}, {"title": "CONCLUSION", "content": "In pathological image analysis, attention-based aggregators significantly advance MIL methods. However, traditional attention mechanisms, due to their quadratic complexity, struggle with processing high-resolution images. Additionally, approximate linear self-attention mechanisms have inherent limitations. To address these challenges, we introduce AMD-MIL, a novel approach for dynamic agent aggregation and representation refinement. Our validation on four distinct datasets demonstrates not only AMD-MI's effectiveness but also its ability for instance-level interpretability."}]}