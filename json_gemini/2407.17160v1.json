{"title": "A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for\nAutomatic Speech Recognition in Multilingual Oral History Archives", "authors": ["Jan Lehe\u010dka", "Josef V. Psutka", "Lubo\u0161 \u0160m\u00eddl", "Pavel Ircing", "Josef Psutka"], "abstract": "In this paper, we are comparing monolingual Wav2Vec 2.0\nmodels with various multilingual models to see whether we\ncould improve speech recognition performance on a unique\noral history archive containing a lot of mixed-language sen-\ntences. Our main goal is to push forward research on this unique\ndataset, which is an extremely valuable part of our cultural her-\ntitage. Our results suggest that monolingual speech recognition\nmodels are, in most cases, superior to multilingual models, even\nwhen processing the oral history archive full of mixed-language\nsentences from non-native speakers. We also performed the\nsame experiments on the public CommonVoice dataset to verify\nour results. We are contributing to the research community by\nreleasing our pre-trained models to the public.", "sections": [{"title": "1. Introduction", "content": "One of the most valuable possessions mankind has is the her-itage from previous generations in the form of historical docu-ments and recordings. This heritage preserves the memory ofhumanity which should not be forgotten. Since collections ofhistorical documents and recordings can grow huge in size, oneof the key challenges of our age is to preserve and curate thisheritage and make it as accessible and searchable to the publicand researchers as possible in order to enable advanced analyz-ing, studying, and learning of new lessons from our history.\nIn this paper, we focus mainly on one very important andunique oral history archive: MALACH. It is an audiovisualarchive initially collected in the 1990s to preserve the authenticmemories of Holocaust survivors. This archive stores vast andextremely valuable testimonies from our recent history recordedthrough audiovisual interviews. Today, these interviews arestored in the Visual History Archive (VHA) at the Shoah Foun-dation Institute (SFI) at the University of Southern California(USC)\u00b9, along with other interviews with witnesses to the his-tory of the entire 20th century (more than 56k interviews). TheHolocaust part of the archive contains testimonies in 32 lan-guages of the personal memories of people who survived theWorld War II Holocaust. In this paper, we denote this archiveand derived datasets as MALACH (Multilingual Access to LargeSpoken Archives) after the name of the first project that began in2001 and laid the foundations for work on this unique archive.2\nThe natural way how to process large oral history archives to make the content more accessible is to transcribe the speechusing an Automatic Speech Recognition (ASR) system. Al-though ASR systems have improved rapidly in the last years [1,2, 3], automatically transcribing interviews from the MALACH\nis still a challenge since the interviews contain spontaneousspeech full of disfluencies, emotional excitements, mixed-language sentences, and heavy accents, and are often influenced\nby the high age of speakers (the average age of all speakers at\nthe time of recording was about 75 years) [4, 5].\nThe large number of mixed-language sentences, mainly\nthe German phrases present in interviews across all other\nlanguages\u00b3, together with the natural multilingualism of this\narchive (32 languages with many non-native speakers) moti-vated us to study multilingual aspects of this archive and try to\nanswer some interesting questions: Would for example adding\nsome German speech data into the training process of the En-glish ASR model solve the problem of transcribing mixed-language sentences? More generally, would the bilingual pre-trained models be more suitable for this task than monolingual\nmodels? And how about trilingual models \u2013 could they be even\nmore suitable? Ultimately, would a large-scale massively mul-tilingual ASR model transcribe the interviews better than a set\nof per-language monolingual ASR models? How much will re-sults from these approaches differ?\nTo answer these questions, in this work, we present a com-parative analysis over a large set of experiments with different\nlanguage combinations in both pre-training and fine-tuning of\nASR systems based on Wav2Vec 2.0 models [1] while simpli-fying the problem only to 3 languages: English, German, and\nCzech. Since some of the MALACH datasets are not publically\navailable, we fine-tuned, evaluated, and compared our models\nalso on another well-known and publicly available dataset\nCommon Voice [6] to see if our findings are applicable also\nto other multilingual datasets than oral history archives."}, {"title": "2. Related work", "content": "The original MALACH project took place between 2001 and\n2006. The WER of the ASR systems developed within\nthe project reached 39.40% for English [7] and 38.57% for\nCzech [8] by the end of the project in 2006. German part was\nnot processed with ASR at that time. Even after the project fin-\nished, the efficiency of the ASR systems has continuously im-proved using new approaches, so in 2011 the WER of 27.11%\nwas achieved for Czech recordings [9]. New training methods\nbased on DNN brought further improvement of WER (21.70%\nfor English [10] and 19.11% for Czech [11]). The best WER\nwithout using end-to-end approaches reached 17.85% for En-glish and 14.65% for Czech [4]. After the introduction of end-to-end Transformer-based audio models, [12] reported a signif-icant improvement for the Czech dataset (WER=10.48%) and"}, {"title": "2.1. Wav2Vec 2.0 model", "content": "After the introduction of the Transformer architecture [14], a\nnew era of AI began. Not long after that, Transformer-based\nmodels also established a new paradigm in the task of auto-matic speech recognition by introducing Wav2Vec 2.0\u2074 model\n. It is a Transformer encoder pre-trained to reconstruct the\ncorrupted signals. The raw audio signal input is processed by\nthe model into a sequence of frame-level contextualized speech\nrepresentations encoding individual audio frames within their\ncontext. The training of Wav2Vec ASR models typically con-sists of two main phases: self-supervised pre-training and su-pervised fine-tuning. When fine-tuning, the model is supple-mented with the final Connectionist Temporal Classification\n(CTC) layer [15], in which the most probable sequence of text\ntokens (i.e., the predicted transcription) is decoded."}, {"title": "3. Selected languages", "content": "To simplify the experiments, we chose 3 languages well-represented in both fine-tuning datasets we were working with\n(see Sec. 5): English, German, and Czech. They all belong\nto the Indo-European language family, Czech belongs to the\nBalto-Slavic branch while English and German are Germanic\nlanguages with higher mutual lexical similarity. We chose\nthese languages for several reasons: (1) German is a language\nwhose phrases intertwine throughout the MALACH archive, so\nit should be included; (2) English is the most represented and\nstudied part of both datasets; and (3) Czech is well-represented\nin both datasets while it is an example of language from a com-pletely different language branch. This language selection al-lows us to test various language combinations and see whether\nlexical similarity will be somehow reflected in results from mul-tilingual models. We didn't include more languages to keep the\nnumber of possible language combinations reasonably small for\nexperimenting and the results interpretable."}, {"title": "4. Pre-trained models", "content": "We started from the Wav2Vec-base English model [1] pre-trained from 50k hours from Libri-light dataset [16]. We used\nthe model as a base for English monolingual ASR models.\nTo ensure comparable results of our experiments, we adopted\nthe exact same pre-training setup for all other models we pre-trained from scratch, and scaled pre-training data for the other\ntwo languages to the same amount. For Czech and German, we\ncollected 50k hours of speech from public sources, mainly from\nthe VoxPopuli dataset [17] and a mix of self-crawled publicly\navailable podcasts and audiobooks.\nThe scheme of pre-training our models is depicted in\nFig. 1. First, we pre-trained German and Czech monolingual\nWav2Vec-base models from 50k hours of speech data each.\nThen, we prepared a mixture of all pairs of languages to train"}, {"title": "4.1. Large-scale multilingual models", "content": "To compare our Wav2Vec models also with large-scale multi-lingual models, we selected Wav2Vec-XLS-R-300M [2] and\nWhisper [3]. Wav2Vec-XLS-R-300M is a popular model pre-trained by Meta AI on 128 languages and approximately 436\nthousand hours of unlabeled speech data. We experimented\nwith the 300M variant, which has more than 3\u00d7 more parame-ters than the Wav2Vec-base model. Whisper is another popular\nmodel trained by OpenAI on 99 languages from 680,000 hours\nof multilingual and multitask labeled data. It is an encoder-decoder model already fine-tuned on multilingual ASR tasks by\nthe authors, so it can also be used as a zero-shot speech recog-nizer without fine-tuning. We experimented with three sizes of\nthe Whisper model: base, small, and large."}, {"title": "5. Fine-tuning datasets", "content": "We were experimenting with two multilingual datasets: Com-monVoice and MALACH. For both datasets and all selected\nlanguages, we cleaned all transcripts by removing non-speech\nevents and punctuation and mapping texts into lowercase. The\ndata statistics are shown in Tab. 1."}, {"title": "5.1. Common Voice", "content": "The CommonVoice is a crowdsourced dataset collected by\nMozilla [6]. We used corpus version 16.0 containing 19,673\nvalidated hours in 120 languages. English portion contains\n1,718 hours, German 912, and Czech 26 hours of training data.\nSince a mixture of these datasets is highly unbalanced and every\nmixed-language fine-tuning we ran with the full datasets ended\nin favor of English ASR with poor performance for the other\ntwo languages, we decided to balance the dataset equally and\nuse only a randomly selected subset with 25 hours of training\ndata per language. With this significant reduction of English\nand German training data, we ensure equal importance and fair\nconditions for all languages during the training. The develop-ment and test splits were used completely without any change."}, {"title": "5.2. MALACH", "content": "The full MALACH archive is a monumental collection with\nover 100,000 hours of interviews in 32 languages. About half\nof the archive is in English, although most English interviews\nare given by non-native speakers. The annotated English part\ncontains 375 hours of speech, the German part almost 2,000\nhours, and the Czech part 130 hours. Similarly to the Common-Voice dataset, we decided to balance this dataset and use an equal amount of training data for each language. The smallest training split is in the Czech part with about 87 hours of training data, so we randomly selected a subset with 85 hours of training data per language. We used full development and test splits without any additional changes. The test part had no speaker overlaps with train or development parts in all languages.\nFor English and Czech, we used datasets released under the\nLinguistic Data Consortium (LDC) \u2013 English [18] and Czech\n. We adopted the same train-dev-test splits as in [4] and seg-mented train and development parts using time labels from the\nannotations into segments not exceeding 30 s, which is a reason-able limit of input examples during training due to GPU mem-ory limits. The test parts for these two languages were already\ncleaned and contained only selected shorter segments (usually\ncovering the maximum length of a single speaker's utterance\nwithout overlaps). As we found in [12], the Czech MALACH\ntranscripts contain a mix of formal and colloquial Czech, caus-ing a mismatch between train and test data, so we converted all Czech training transcripts into formal Czech to close the gap.\nFor German, we adopted the same data splitting and pre-processing as in [5] with additional removing of the non-speech\ntoken ah from transcripts as we observed inconsistent annota-tions of this token. We used the full unsegmented test split with-out any further segmentation of filtration, so the recordings in the German test dataset are much longer and less clean than test data from other languages."}, {"title": "6. Experiments", "content": "We fine-tuned all Wav2Vec models with the same setting as the\nbase model in [1], i.e., we trained for 80 thousand steps with a\nbatch size of about 26 minutes per step, and the learning rate\nwarmed up over the first 8000 steps to a maximum value of\n2 \u00d7 10-5, where it was held for the next 32000 steps, and fi-\nnally decayed exponentially to zero. The weights of the feature\nencoder were frozen for the first 10000 steps of the fine-tuning.\nWhisper models were fine-tuned differently because we\nobserved some overfitting tendencies. For each model size,\ndataset, and language, we run 4 different fine-tunings with\nlearning rates set to 1 \u00d7 10\u22125, 5\u00d710-5, 1\u00d710-4 and 5\u00d710-4.\nWe trained all models for 10 epochs with a batch size of 32\nand measured the error rate after each epoch on the develop-ment dataset. Finally, we chose a checkpoint with the low-est error rate for evaluation. We fine-tuned Wav2Vec mod-els with Fairseq and Whisper with the Transformers li-brary', both on a machine with eight NVIDIA A100 GPUs.\nThe training took approx. 12 hours (Wav2Vec-base), 30 hours (Wav2Vec-XLS-R), resp. less than 3 hours (Whisper).\nWe compared models in terms of word error rate (WER).\nSince all transcripts were in lowercase and cleaned from punc-tuation, our fine-tuned models cannot predict punctuation or\nupper-cased characters, so we did not consider casing and punc-tuation differences with the reference as errors.\nOur results are tabulated in Tab. 2. In the first part of the ta-ble (rows 1-12), we are comparing monolingual Wav2Vec mod-els with bilingual models in all possible language combinations, and with the trilingual model. We fine-tuned each pre-trained model on all combinations of languages that were possible. For monolingual fine-tuning (i.e. we used data from one language only during the fine-tuning), we aggregated results into a sin-gle row and separated individual fine-tuning languages by a slash in the language columns. For example, on the 8th row, we took a trilingual pre-trained model (pre-training languages were CS+EN+DE), fine-tuned 6 different ASR models, one per each language (CS/EN/DE) and each dataset (Common Voice, MALACH), and evaluated each model on corresponding test data. On the contrary, on the 12th row, we took the same pre-trained trilingual model and fine-tuned 2 different ASR mod-els (one per dataset) from a mixture of 3 languages in the fine-tuning data (denoted as CS+EN+DE). In other words, we denote the joining of datasets for multilingual training by \"+\" (the re-sulting model is one multilingual model per dataset), and a set of language-specific monolingual training runs by \"/\" resulting in one monolingual model per each language and dataset.\nIn the second part of Tab. 2 (rows 13-19), we present re-sults using large-scale multilingual models. It is important to consider individual models' sizes and fairly compare only mod-els of similar sizes, so we also included the column with the number of trainable parameters in the table. We fine-tuned all 4 models (Wav2Vec-XLS-R and three sizes of Whisper) on all languages and datasets. The zero-shot performance of the Whisper models is reported on rows 14, 16, and 18."}, {"title": "7. Discussion", "content": "Our results suggest that adding more languages into the pre-training phase while keeping the model at the same size did\nnot bring any improvement for either dataset. On the contrary, we observed a trend in WER increasing when adding more lan-guages. We plotted this interesting trend in Fig. 2, where for\neach dataset and each language, we compared the monolingual model (row 1 in Tab. 2), the average WER scored by bilingual models (rows 2, 4, and 6) and the trilingual model fine-tuned on a single language (row 8). This suggests that even when our dataset is multilingual and contains a lot of mixed-language sen-tences, the best we can do (when we want to keep the model rea-sonably small for production) is to train the monolingual model on each language separately from scratch.\nIt is worth noting that our best results in Tab. 2 are not state-of-the-art results. We are just comparing different models under the same conditions. Significantly better results could be scored with monolingual models when using more training data and a language model in the CTC decoder [13, 5].\nAs for adding more languages into fine-tuning (assuming"}, {"title": "8. Conclusions", "content": "In this paper, we have presented a comparative analysis over\na large set of experiments with different language combina-tions in both pre-training and fine-tuning of ASR systems based\non Wav2Vec 2.0 models. We evaluated our models on two multilingual datasets and three languages. Our results suggest that monolingual Wav2vec models are, in most cases, superior to multilingual models. Only large-scale multilingual models, many times larger, can outperform monolingual Wav2Vec mod-els, but only at the cost of much higher decoding complexity and carbon footprint for each transcribed word."}]}