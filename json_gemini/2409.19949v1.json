{"title": "TASK-AGNOSTIC PRE-TRAINING AND TASK-GUIDED\nFINE-TUNING FOR VERSATILE DIFFUSION PLANNER", "authors": ["Chenyou Fan", "Chenjia Bai", "Zhao Shan", "Haoran He", "Yang Zhang", "Zhen Wang"], "abstract": "Diffusion models have demonstrated their capabilities in modeling trajectories of\nmulti-tasks. However, existing multi-task planners or policies typically rely on\ntask-specific demonstrations via multi-task imitation, or require task-specific re-\nward labels to facilitate policy optimization via Reinforcement Learning (RL).\nTo address these challenges, we aim to develop a versatile diffusion planner that\ncan leverage large-scale inferior data that contains task-agnostic sub-optimal tra-\njectories, with the ability to fast adapt to specific tasks. In this paper, we pro-\npose SODP, a two-stage framework that leverages Sub-Optimal data to learn a\nDiffusion Planner, which is generalizable for various downstream tasks. Specifi-\ncally, in the pre-training stage, we train a foundation diffusion planner that extracts\ngeneral planning capabilities by modeling the versatile distribution of multi-task\ntrajectories, which can be sub-optimal and has wide data coverage. Then for\ndownstream tasks, we adopt RL-based fine-tuning with task-specific rewards to\nfast refine the diffusion planner, which aims to generate action sequences with\nhigher task-specific returns. Experimental results from multi-task domains includ-\ning Meta-World and Adroit demonstrate that SODP outperforms state-of-the-art\nmethods with only a small amount of data for reward-guided fine-tuning.", "sections": [{"title": "1 INTRODUCTION", "content": "There has been a long-standing pursuit to develop agents capable of performing multiple tasks (Reed\net al., 2022; Lee et al., 2022). While traditional RL methods have made significant strides in training\nagents to master individual tasks (Silver et al., 2016; OpenAI et al., 2019), extending this capability\nto handle diverse tasks remains a significant challenge due to the diversity of task variants and opti-\nmization directions with different rewards. Multi-task RL aims to address this by developing agents\nvia task-conditioned optimization (Yu et al., 2020; Lee et al., 2022) or parameter-compositional\nlearning (Sun et al., 2022; Lee et al., 2023). However, the inherent diversity in task trajectory\ndistributions makes it challenging to model and accommodate the modeling across different task\nstructures. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020), originally designed for\ngenerative tasks, provide a powerful framework for addressing these difficulties. Their capacities\nto capture complex, multi-modal distributions within high-dimensional data spaces (Podell et al.,\n2023; Ho et al., 2022; Jing et al., 2022) makes them well-suited for representing the broad variabil-\nity encountered in multi-task environments.\nMotivated by this, existing methods have employed diffusion models to imitate expert behaviors\nderived from human demonstrations across various tasks (Pearce et al., 2023; Xu et al., 2023; Chi\net al., 2023). However, acquiring task-specific demonstrations is often time-consuming and costly,\nespecially in environments requiring specialized domain expertise. Alternative approaches attempt\nto optimize diffusion models with return guidance (He et al., 2024; Liang et al., 2023) or conven-\ntional RL paradigm (Wang et al., 2022b), which demands a large volume of data with reward labels\nfor each task. To address the above limitations, we wonder whether a generalized diffusion plan-\nner can be learned from a large amount of low-quality trajectories without reward labels, with the\nability to adapt quickly to various downstream tasks. The inferior dataset comprises a mixture of"}, {"title": "2 PRELIMINARIES", "content": "Multi-task RL We consider the multi-task RL problem involving N tasks, where each task T ~\np(T) is represented by a task-specified Markov Decision Process (MDP). Each MDP is defined by a\ntuple $(S_T, A, P_T, R_T, \\mathcal{P}_T, \\gamma)$, where $S_T$ is the state space of task T, A is the global action space,\n$P_T (s_{t+1}|s_t, a_t): S_T \\times A \\rightarrow S_T$ is the transition function, $R_T (s_t, a_t) : S_T \\times A \\rightarrow \\mathbb{R}$ is\nthe reward function, $\\gamma \\in (0,1]$ is the discount factor, and $\\mathcal{P}_T$ is the initial state distribution. We\nassume that all tasks share a common action space, executed by the same agent, while differing\nin their respective reward functions, state spaces, and transition dynamics. At each timestep t, the\nagent perceives a state $s_t \\in S_T$, takes an action $a_t \\in A$ according to the policy $\\pi_T (a_t|s_t)$, and\nreceives a reward $r_t$. The agent's objective is to determine an optimal policy that maximizes the\nexpected return across all tasks: $\\pi^* = arg \\underset{\\pi}{max} E_{T~p(T)} E_{a_t~\\pi_T} [\\Sigma_{t=0}^{\\infty} \\gamma^t r_t]$.\nDiffusion Models Diffusion models (Sohl-Dickstein et al., 2015) are a type of generative model\nthat first add noise to the data $x_0$ from a unknown distribution $q(x_0)$ in K steps through a forward\nprocess defined as:\n$q(x_k|x_{k-1}) := N(x_k; \\sqrt{1 - \\beta_k}x_{k-1}, \\beta_k I)$,\nwhere $\\beta_k$ is a predefined variance schedule. Then, a trainable reverse process is constructed as:\n$p_\\theta(x_{k-1}|x_k) := N(x_{k-1}; \\mu_\\theta(x_k, k), \\Sigma_k)$,\nwhere $\\mu_\\theta(x_k, k)$ is the forward process posterior mean as a function of a noise prediction neural\nnetwork $\\epsilon_\\theta(x_k, k)$ with a learnable parameter $\\theta$ (Ho et al., 2020). $\\epsilon_\\theta(x_k, k)$ can be trained via a\nsurrogate loss as\n$\\mathcal{L}_{denoise}(\\theta) := E_{k~[1,K], x_0~q, \\epsilon~N(0,1)} [||\\epsilon - \\epsilon_\\theta(x_k, k)||^2]$.\nAfter training, samples can be generated by first drawing Gaussian noise $x_K$ and then iteratively\ndenoising $x_K$ into a noise-free output $x_0$ over K iterations using the trained model $\\epsilon_\\theta(x_k, k)$ by\n$x_{k-1} = \\frac{1}{\\sqrt{\\alpha_k}}(x_k - \\frac{1-\\alpha_k}{\\sqrt{1-\\bar{\\alpha_k}}}\\epsilon_\\theta(x_k, k)) + \\sigma_k N(0, I)$,\nwhere $\\alpha_k := 1 - \\beta_k$, $\\bar{\\alpha}_k := \\Pi_{s=1}^k \\alpha_s$ and $\\sigma_k = \\sqrt{\\beta_k}$."}, {"title": "3 METHOD", "content": "We propose SODP, a two-stage framework that leverages large amounts of sub-optimal data to train\na diffusion planner that can generalize to downstream tasks. The process is depicted in Figure 3. In\nthe pre-training stage, we train a guidance-free diffusion model to predict future actions based on\nhistorical states, using an mixture offline dataset cross tasks without reward labels. In the fine-tuning\nstage, we refine the pre-trained model using policy gradient to maximize the task-specific rewards,\nadditionally incorporating a regularization term to prevent the model from losing acquired skills."}, {"title": "3.1 PRE-TRAINING WITH LARGE-SCALE SUB-OPTIMAL DATA", "content": "Previous works (He et al., 2024) typically model multi-task RL as a conditional generative problem\nusing diffusion models trained on datasets composed of multiple task subsets $D = \\cup_{i=1}^{N}D_i$, as:\n$\\underset{\\theta}{max} E_{\\tau \\sim \\cup D_i} [log p_\\theta(x_0(T) | Y(T)]$,\nwhich requires additional condition $Y(T)$ to guide diffusion model to generate desirable trajecto-\nries. For instance, $Y(T)$ should contain the return of trajectory $R(T)$ and task description $Z$ as\nprompt. However, the reward label and trajectory description may be scarce or costly to obtain in\nthe real-world. To overcome this challenge, we train a diffusion planner that can learn from offline\ntrajectories transitions (i.e., ${(s_t, a_t, s_{t+1})}$) without reward label or task descriptions. Specifically,\nwe model the problem as a guidance-free generation process (Chi et al., 2023):\n$\\underset{\\theta}{max} E_{(s_t,a_t)~\\cup D_i} [log p_\\theta(a_t | s_t)]$"}, {"title": "3.2 REWARD FINE-TUNING FOR DOWNSTREAM TASKS", "content": "MDP notation. The fine-tuning stage involves two distinct MDPs: one for RL decision process\nand the other for the diffusion model denoising process. We use the superscript diff (e.g., $s^{diff}$,\n$a^{diff}$) to denote the MDP associated with diffusion model denoising process, while no superscript is\nused for the MDP related to the RL process (e.g., $s_t, a_t$). Additionally, we use $k \\in \\{K, ...,0\\}$\nto represent the diffusion timestep and $t \\in \\{1, . . ., T\\}$ to represent the trajectory timestep.\nWe model the denoising process of our pre-trained diffusion planner as a K-step MDP as follows:\n$s^{diff} = (s_t, a_{K-k}), \\quad \\mathcal{P}^{diff}_{K} (s^{diff}) = (d_{s_t}, N(0, 1)),$\n$a^{diff} = a_{K-k-1}, \\quad \\mathcal{P}^{diff} (s^{diff}_{k+1} | s^{diff}_k, a^{diff}) = (s_t, a_{k}),$\n$R^{diff} (s^{diff}, a^{diff}) = \\{\\begin{matrix} r(s_t) \\\\ 0 \\end{matrix} \\} = r(a_t) \\text{ if } k = K - 1,$\n$\\pi_\\theta^{diff} (a^{diff} | s^{diff}) = p_\\theta (a_{k-k-1} | a_{k-k}, s_t)$,\nwhere $s^{diff}_k$ and $a^{diff}_k$ are the state and action at timestep k, $\\mathcal{P}^{diff}$ and $\\mathcal{P}^{diff}$ are the initial distribution and\ntransition dynamics, $\\delta$ is the Dirac delta distribution, $R^{diff}$ is the reward function and $p_\\theta (a_{k-k-1} |$\n$a_{k-k}, s_t)$ is the pre-trained diffusion planner. This formulation allows the state transitions in the\nMDP to be mapped to the denoising process in the diffusion model. The MDP initiates by sampling\nan initial state $s^{diff}_K ~ \\mathcal{P}^{diff}_{K}$, which corresponds to sample Gaussian noise $a^K_k$ at the beginning of"}, {"title": "4 RELATED WORK", "content": "Diffusion Models in RL. Diffusion models are a leading class of generative models, achieving\nstate-of-the-art performance across a variety of tasks, such as image generation (Ramesh et al.,\n2021), audio synthesis (Kong et al., 2020; Huang et al., 2023), and drug design (Schneuing et al.,\n2022; Guan et al., 2024). Recent studies have applied them in imitation learning to model human\ndemonstrations and predict future actions (Li et al., 2024; Reuss et al., 2023). Other approaches\nhave trained conditional diffusion models either as planners (Ajay et al., 2022; Brehmer et al., 2024)\nor policies (Hansen-Estruch et al., 2023; Kang et al., 2024). However, most of these efforts focus\non single-task settings. While some recent works aim to extend diffusion models to multi-task\nscenarios, they often rely on additional conditions, such as prompts (He et al., 2024) or preference\nlabels (Yu et al., 2024). These methods are limited by their dependence on expert data or explicit\ntask knowledge. In contrast, our method learns broad action-sequence distributions from inferior\ndata to enhance action priors, enabling effective generalization across a range of downstream tasks.\nFine-tuning Diffusion Models. Despite the impressive success of diffusion models, they often face\nchallenges in aligning with specific downstream objectives, such as image aesthetics (Schuhmann\net al., 2022), fairness (Shen et al., 2023), or human preference (Xu et al., 2024), primarily due\nto their training on unsupervised data. Some methods have been proposed to address this issue\nby directly fine-tuning models using downstream objectives (Prabhudesai et al., 2023; Clark et al.,\n2023), but they rely on differentiable reward models, which are impractical in RL since accurately\nmodeling rewards with neural networks is quite costly (Kim et al., 2023). Other methods reformulate\nthe denoising process as an MDP and apply policy gradients for fine-tuning (Black et al., 2023; Fan\net al., 2024). However, they heavily depend on strong pre-trained models and have proven ineffective\nin our case. Our goal is to fine-tune a less powerful model that has been trained on inferior data.\nConcurrent with our work, DPPO (Ren et al., 2024) also explores reward fine-tuning for refining RL\ndiffusion planners. However, their approach focuses exclusively on single-task settings and allows\naccess to expert demonstrations. In contrast, we train our model on multi-task data without the need\nfor superior demonstrations. Additionally, we analyze the limitations of current regularization meth-\nods for versatile RL diffusion models and propose a new regularizer that improves the performance\nof sub-optimal pre-trained models."}, {"title": "5 EXPERIMENTS", "content": "In this section, we conduct experiments to evaluate our proposed method and address the following\nquestions: (1) How does SODP's performance compare to current methods? (2) Can SODP scale\nto high-dimensional observation inputs? (3) How does SODP achieve higher rewards during online\nfine-tuning?"}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "We evaluate SODP in both state-based and image-based environments. For state-based environ-\nments, we conduct experiments using the Meta-World benchmark (Yu et al., 2019). For image-based\nenvironments, we perform experiments using the Adroit benchmark (Rajeswaran et al., 2017).\nMeta-World. The Meta-World benchmark comprises 50 distinct manipulation tasks, each requiring\na Sawyer robot to interact with various objects. These tasks are designed to assess the robot's ability\nto handle different scenarios, such as grasping, pushing, pulling, and manipulating objects of varying\nshapes, sizes, and complexities. While the state space and reward functions differ across tasks, the\naction space remains consistent. Following recent studies (He et al., 2024; Hu et al., 2024), we\nextend all tasks to a random-goal setting, referred to as MT50-rand.\nAdroit. The Adroit benchmark includes three dexterous manipulation tasks, requiring a 24-degree-\nof-freedom dexterous hand to solve complex challenges such as in-hand manipulation and tool use.\nThe goals in this environment are also randomized. For Adroit, we use images as the observation to\nassess whether our method can scale to high-dimensional input.\nDatasets. Following previous work (He et al., 2024), for Meta-World, we use a sub-optimal dataset\ncomprising the first 50% of experiences (50M transitions) obtained from the replay buffer of a\nSAC (Haarnoja et al., 2018) agent during training. To verify the applicability of our method to\ntasks of varying difficulty levels, we divide the entire dataset into four subsets based on the task\ncategories presented in Seo et al. (2023). All baselines and our pre-training stage are trained on\nsub-optimal datasets. We then use 1M online transitions per task for fine-tuning. For Adroit, we\ntrain a VRL3 (Wang et al., 2022a) agent for each task and use the initial 30% experiences (90K\ntransitions) from the converged replay buffer. All baselines are trained on expert demonstrations\nand our pre-training stage is trained on sub-optimal transitions.\nBaselines. For Meta-World, we compare our proposed SODP with the following baselines:\n(1) MTBC. Extended BC to multi-task offline policy learning through network scaling and a task-\nID-conditioned actor. (2) MTIQL. Extended IQL (Kostrikov et al., 2021) with multi-head critic\nnetworks and a task-ID-conditioned actor for multi-task policy learning. (3) MTDT. Extended De-\ncision Transformer (DT) (Chen et al., 2021a) to multitask settings by incorporating task ID encoding\nand state inputs for task-specific learning. (4) Prompt-DT (Xu et al., 2022). An extension of DT,\nwhich is designed to leverage multi-task data to generalize policies for previously unseen tasks. It\ngenerates actions by utilizing trajectory prompts and reward-to-go signals. (5) MTDIFF (He et al.,\n2024). A diffusion-based approach that integrates Transformer architectures with prompt learning to\nfacilitate generative planning in multitask offline environments. (6) HarmoDT (Hu et al., 2024). A\nDT-based approach that leverages parameter sharing to exploit task similarities while mitigating the\nadverse effects of conflicting gradients simultaneously. The results for these baselines are directly\nreplicated from those reported in HarmoDT (Hu et al., 2024).\nLearning policies for Adroit is more challenging due to the significantly larger state and action\nspaces. The baselines used in Meta-World struggled to handle this high-dimensional data struc-\nture. Therefore, we compare SODP with following baselines designed for complex environments:\n(1) BCRNN (Mandlekar et al., 2021). A variant of BC that employs a Recurrent Neural Network\n(RNN) as the policy network, predicting the sequence of actions based on the sequence of states\nas input. (2) IBC (Florence et al., 2022). Extended BC with energy-based models (EBM) to train\nimplicit behavioral cloning policies. (3) Diffusion Policy (Chi et al., 2023). A diffusion-based ap-\nproach that predicts future action sequences based on historical states. (4) DP3 (Ze et al., 2024). A\nvisual imitation learning algorithm that incorporates 3D visual representations into diffusion poli-\ncies, using a point clouds encoder to process visual observations into visual features. The results for\nthese baselines are directly replicated from those reported in DP3 (Ze et al., 2024)."}, {"title": "5.2 RESULTS", "content": "We use the average success rate across all tasks as the evaluation metric and report the mean and\nstandard deviation of success rates across three seeds. All baselines are trained on sub-optimal\ndata. As shown in Table 1, our method achieves over a 60% success rate when learning from infe-\nrior data, outperforming all baseline methods. Compared to the existing state-of-the-art approach,"}, {"title": "Does SODP generalize to high-dimensional observations?", "content": "We scale our method to image-based\nobservations using the Adroit benchmark by em-\nploying a point-cloud encoder from DP3 (Ze\net al., 2024) to process the 3D scene represented\nby point clouds. Specifically, we capture depth\nimages directly from the environment and con-\nvert them into point clouds using Open3D (Zhou\net al., 2018). These point clouds are then pro-\ncessed by the DP3 Encoder, which maps them\ninto visual features. We then train our diffusion\nplanner following the same procedure in Algo-\nrithm 1 except the input states are visual fea-\ntures. Following DP3 (Ze et al., 2024), We com-\npute the average of the highest 5 evaluation suc-\ncess rates during training and report the mean\nand std across 3 seeds."}, {"title": "5.3 EFFECTIVENESS OF BC REGULARIZATION", "content": "To demonstrate the effectiveness of our BC regularization, we conduct an ablation study on fine-\ntuning same pre-trained model with our BC regularization and other variants. We consider following\nvariants:\n\u2022 SODP w/o regularization. This variant is similar to DDPO (Black et al., 2023) and DPPO (Ren\net al., 2024), which fine-tunes the model directly using Eq. (12) without any regularization.\n\u2022 SODP_kl. This variant is similar to DPOK (Fan et al., 2024), with the addition of a KL regular-\nization term to constrain the divergence between the fine-tuned model and the pre-trained model.\n\u2022 SODP_pl. This variant is similar to DLPO (Chen et al., 2024), incorporating the original diffusion\npre-training loss (PL) into the fine-tuning objective to prevent the model from deviation.\nThe details of these variants are presented in Appendix D. Figure 5 demonstrates the effectiveness\nof our regularization in achieving a higher success rate. We observe that directly fine-tuning the\nmodel without any regularization results in the worst performance, with a decline in success rate,\nas the model may degrade the capabilities acquired from pre-training due to the lack of constraints.\nHowever, adding KL and PL is insufficient, as they cause oscillations near the pre-trained model.\nThis aligns with the original intent of these regularizers, which is to prevent excessive deviation.\nThis is reasonable for methods like DPOK and DLPO, which utilize pre-trained models such as\nStable Diffusion (Rombach et al., 2022) and WaveGrad2 (Chen et al., 2021b). These models already\nexhibit strong generative capabilities without fine-tuning, and the goal is to make slight adjustments\nto align them with more fine-grained attributes, such as aesthetic scores and human preferences.\nIn contrast, our model is pre-trained on sub-optimal data and lacks the ability to solve complex tasks.\nWe expect it to develop new skills for completing these tasks through fine-tuning. However, directly\napplying KL regularization to the pre-trained model leads to conservative policies that heavily rely\non the existing capability, thereby confining the model to a sub-optimal region. While PL regular-ization allows some slight exploration, it is uncontrolled and random. Consequently, we observe\nthat the KL regularization almost remains unchanged and the PL regularization slightly increases\nthe performance in basketball but decreases in other tasks."}, {"title": "Visualization", "content": "We hypothesize that the effectiveness of our BC regularization lies in two aspects:\n(i) it ensures that our model can reuse the skills it has acquired, thereby preventing a decline in\nperformance; (ii) It guides our model to effectively explore optimal regions due to the utilization\nof optimal u as the target policy. To demonstrate this, we visualize trajectories of using the actions\ngenerated by our planner using t-SNE (Van der Maaten & Hinton, 2008). As shown in Figure 6, the\ntrajectory distribution after fine-tuning with KL regularization closely resembles the original pre-\ntraining distribution, indicating that the model is reusing learned actions and lacks exploration into\nnew regions. The exploration in PL is unstructured as it may lead to worse regions (e.g., the upper-\nleft region in basketball). In contrast, our method demonstrates superior exploration capabilities\nto discover new, high-reward regions based on acquired knowledge (e.g., the lower-left region in\nbasketball and the bottom region in plate-slide). Meanwhile, the model can derive valuable insights"}, {"title": "5.4 EFFECTIVENESS OF PRE-TRAINING", "content": "We investigate the impact of pre-training. We compare the performance of SODP with a version\ntrained from scratch (SODP_scratch). For SODP_scratch, we use the same rollouts generated by\nthe pre-trained model to approximate the target policy and initialize the replay buffer.\nFigure 7 shows that fine-tuning the planner from scratch results in worse performance. Without\npre-training, the planner lacks an action prior to guide its behavior, leading to stagnation as it strug-\ngles to move towards high reward regions. Additionally, it becomes unstable, as the limited useful\nknowledge is easily disrupted by a large number of ineffective trials."}, {"title": "6 CONCLUSION", "content": "We propose SODP, a novel framework for training a versatile diffusion planner using sub-optimal\ndata. By effectively combining pre-training and fine-tuning, we capture broad behavioral patterns\ndrawn from large-scale multi-task transitions and then rapidly adapt them to achieve higher perfor-\nmance in specific downstream tasks. During fine-tuning, we introduce a BC regularization method,\nwhich preserves the pre-trained model's capabilities while guiding effective exploration. Exper-\niments demonstrate that SODP achieves superior performance across a wide range of challenging\nmanipulation tasks. In future work, we aim to develop embodied versatile agents that can effectively\nlearn to solve real-world tasks using inferior data."}, {"title": "A DERIVATIONS", "content": ""}, {"title": "A.1 DERIVATION OF POLICY GRADIENT IN EQUATION (11)", "content": "Assume $p_\\theta (a_{1:K}|s_t)r^T(a)$ and $\\nabla_\\theta p_\\theta(a_{1:K}|s_t)r^T(a)$ are continuous (Fan et al., 2024), we have:\n$\\nabla_\\theta J^T(\\theta) = \\nabla_\\theta \\sum_{t} \\mathbb{E}_{p_\\theta(a_{1:K}|s_t)} [r^T(a)]$\\n$= \\sum_{t} \\nabla_\\theta \\int p_\\theta(a_{1:K}|s_t) r^T(a) da_{1:K}$\\n$= \\sum_{t} \\int \\nabla_\\theta r^T(a) \\cdot p_\\theta(a_{1:K}|s_t) da_{1:K}$\\n$= \\sum_{t} \\int r^T(a) \\cdot \\nabla_\\theta p_\\theta(a_{1:K}|s_t) da_{1:K}$\\n$= \\sum_{t} \\int r^T(a) \\cdot (\\int \\nabla_\\theta p_\\theta(a_{1:K}|s_t) da_{1:K}) da_{1:K}$\\n$= \\sum_{t} \\int r^T(a) \\cdot \\nabla_\\theta log (p_\\theta (a_{1:K} | s_t)) \\cdot p_\\theta (a_{1:K} | s_t) da_{1:K}$\\n$= \\sum_{t} \\mathbb{E}_{p_\\theta(a_{1:K}|s_t)} [r^T(a) \\cdot \\sum_{k=1}^{K} \\nabla_\\theta log p_\\theta (a_{k-1}|a_{k}, s_t)]$"}, {"title": "A.2 DERIVATION OF LOSS FUNCTION IN EQUATION (12)", "content": "By using importance sampling approach, we can rewrite Eq. (16) as follows:\n$\\sum_{t} \\mathbb{E}_{p_\\theta(a_{1:K}|s_t)} [r^T(a) \\cdot \\sum_{k=1}^{K} \\nabla_\\theta log p_\\theta (a_{k-1}|a_{k}, s_t)]\\frac{p_\\theta(a_{1:K}|s_t)}{p_{\\theta_{old}}(a_{1:K}|s_t)}$\nThen, we can get a new objective function corresponding to Eq. (17) as:\n$J^{clip}(\\theta) = max_{\\theta} \\sum_{t} \\mathbb{E}_{p_{\\theta_{old}}(a_{1:K}|s_t)} [r^T(a) min( \\frac{p_\\theta(a_{1:K}|s_t)}{p_{\\theta_{old}}(a_{1:K}|s_t)}, clip(\\rho_k(\\theta, \\theta_{old}), 1+\\epsilon, 1-\\epsilon)]$\nLet $\\rho_k(\\theta, \\theta_{old}) = \\frac{p_\\theta(a_{1:K}|s_t)}{p_{\\theta_{old}}(a_{1:K}|s_t)}$ denote the probability ratio. Based on PPO (Schulman et al.,\n2017), we clip $\\rho_k$ and use the minimum between the clipped and unclipped ratios to derive a lower\nbound of the original objective (18), which serves as our final objective function:\n$J^{clip}(\\theta) = max_{\\theta} \\sum_{t} \\mathbb{E}_{p_{\\theta_{old}}(a_{1:K}|s_t)} [r^T(a) min( \\rho_k(\\theta, \\theta_{old}), clip(\\rho_k(\\theta, \\theta_{old}), 1+\\epsilon, 1-\\epsilon)]$\nTo refine our pre-trained planner, we employ the negative of objective (19) as the loss function to\nfacilitate reward maximization during fine-tuning."}, {"title": "A.3 DERIVATION OF LOSS FUNCTION IN EQUATION (14)", "content": "Directly computing and minimizing the NLL is difficult. However, we can derive an upper bound of\nEq. (14) as follows:\n$\\mathbb{E}_{a\\sim\\mu} [-logp_\\theta(a_{1:K})] \\leq \\mathbb{E}_{a\\sim\\mu} \\mathbb{E}_{q(a_k|a_0)} \\begin{bmatrix} -logp_\\theta(a_{1:K})\\\\ \\frac{p_\\theta(a_{K-1})}{q(a_{k}|a_{K-1})} \\end{bmatrix}$\\n$\\mathbb{E}_{a\\sim\\mu} \\mathbb{E}_{q(a_k|a_0)} \\begin{bmatrix} -logp_\\theta(a_{1:K})\\\\ \\sum_{k=1}^{K} D_{KL} [q(a_{K-1}|a_{k}, a_{0})]\\\\ D_{KL}(q(a_K|a_{0})||p(a_K))\\\\ - \\mathbb{E}_{q(a_k|a_0)} [logp_\\theta(a_k|a_0)]\\end{bmatrix}$\\n$\\mathbb{E}_{a\\sim\\mu} [\\sum_{k=2}^{K} \\mathbb{E}_{q(a_k|a_0)} D_{KL} [q(a_{K-1}|a_{k}, a_{a\\mu})||p(a_{K-1})] + D_{KL}(q(a_k|a_{a\\mu})) - \\mathbb{E}_{q(a_k|a_0)} [logp_\\theta(a_k|a_0)]]$\nFollowing previous work (Ho et al., 2020), the optimization of the bound can be simplified as:\n$\\underset{\\theta}{argmin} \\frac{1}{20^2(k)} \\mathbb{E} [\\frac{1}{((1-a_{K})^2}| |\\epsilon_\\theta(a_k)-\\epsilon_{\\mu}(a_k) ]||^2]$\nwhere:\n$\\sigma^2(k) = (1-\\bar{a}_{k-1}^2)(1-a_{K})/(1-a_{K})$\nHere, $\\epsilon_\\theta(a, k)$ is a noise model that learns to predict the source noise $\\epsilon_\\theta(a, k)$ which determines\na, from a."}, {"title": "B THE DETAILS OF SODP", "content": ""}, {"title": "B.1 DIFFUSION POLICY", "content": "We use diffusion policy (Chi et al.", "https": "github.com/\nCleanDiffuserTeam/CleanDiffuser"}, {"https": "github.com/\nYanjieZe/3D"}]}