{"title": "Collaborative Stance Detection via Small-Large Language Model Consistency Verification", "authors": ["Yu Yan", "Sheng Sun", "Zixiang Tang", "Teli Liu", "Min Liu"], "abstract": "Stance detection on social media aims to identify attitudes expressed in tweets towards specific targets. Current studies prioritize Large Language Models (LLMs) over Small Language Models (SLMs) due to the overwhelming performance improving provided by LLMs. However, heavily relying on LLMs for stance detection, regardless of the cost, is impractical for real-world social media monitoring systems that require vast data analysis. To this end, we propose Collaborative Stance Detection via Small-Large Language Model Consistency Verification (Cover) framework, which enhances LLM utilization via context-shared batch reasoning and logical verification between LLM and SLM. Specifically, instead of processing each text individually, CoVer processes texts batch-by-batch, obtaining stance predictions and corresponding explanations via LLM reasoning in a shared context. Then, to exclude the bias caused by context noises, CoVer introduces the SLM for logical consistency verification. Finally, texts that repeatedly exhibit low logical consistency are classified using consistency-weighted aggregation of prior LLM stance predictions. Our experiments show that CoVer outperforms state-of-the-art methods across multiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per tweet while significantly enhancing performance. Our CoVer offers a more practical solution for LLM deploying for social media stance detection.", "sections": [{"title": "1 Introduction", "content": "Stance Detection (SD) is a powerful tool to reveal public viewpoints across a variety of social events. It has many important applications in social research"}, {"title": "2 Problem Statement", "content": "Zero-Shot Stance Detection (ZSSD) [1,9,12,14] is defined as the task of classifying the stance (Favor, Against, None) expressed in a given tweet towards a specific target without providing any specific training data or reference samples about the target. In our study, for a given raw tweet xraw toward target t, we pre-process it as the augmented tweet x, and develop a Large Language Model (LLM) and Small Language Model (SLM) collaboration framework to classifying the stance label y for x in zero-shot setting."}, {"title": "3 Methodology", "content": "In this section, we introduce our Collaborative Stance Detection via Small-Large Language Model Consistency Verification (Cover) framework, which combines the strengths of LLM and SLM to achieve balanced computational consumption and model performance in stance detection tasks. The overall structure of CoVer is shown in Fig.2 and the workflow of our CoVer is shown in Algorithm 1."}, {"title": "3.1 Context Reconstruction", "content": "To ensure effective reasoning, it's crucial to optimize the input context for the LLM, especially when processing multiple tweets within a limited context window. Therefore, we employ Context Reconstruction to attach external knowledge and then filter out non-relevant content to ensure LLMs' unbiased reasoning.\nKnowledge Augmentation To identify the stance of those tweets with implicit expressing, we introduce external knowledge to enrich the context.\nGiven the external knowledge base K = {(ko, do), (k1, d\u2081)...}, where ki is the knowledge entity, and di refers to the description associated with knowledge ki, we employ entity linking match the knowledge. For those matched knowledge entries and corresponding descriptions, we concatenate them with the raw tweet xraw to create an knowledge augmented tweet xk:\n\\(x_k = x_{raw} \\oplus D, D = \\{k_i \\oplus d_i | (k_i, d_i) \\in K \\text{ and } k_i \\in X_{raw}\\}\\),\nwhere \\( \\oplus \\) denotes the string concatenation, & is the set of matched knowledge via entity linking.\nSentence Filtering To determine a sentence or the knowledge weather contributes to externalizing the stance expression of the tweet, we measure its impact through Stance Entropy (SE). A lower entropy for tweet x indicates more discriminative stance labels, suggesting the processed text effectively externalizes the stance. Entropy for tweet x is calculated as:\n\\(SEx = - \\sum_{y \\in Y} P(y|x, t) \\log P(y|x, t),\\)\nwhere y is the stance label set {Favor, Against, None}. A lower SE implies the tweet better externalizes a stance expression, as the stance likelihood is more concentrated on a specific stance.\nTo refine xk, we split the knowledge augmented tweet xk into the sentence set Xk = {$1,$2,... } based on stance entropy. Our goal of sentence filtering is defined as:\n\\(X = \\arg \\max_{X \\subset X_k} SEx, x= \\bigoplus_{s_i \\in X} s_i,\\)\nwhere x is the refined tweet concatenated from the optimal subset of sentences X that maximizes SEx. We filter the irrelevant sentences according to the change in stance variance after the removal of each sentence. Specifically:\nRedundant Sentence: If removing a sentence si results in an obvious de- crease in stance entropy SEx\\si, i.e., SEx > SEx\\si, this suggests that si is redundant or has minimal impact on clarifying the stance. Thus, si is excluded from the tweet.\nRelevant Sentence: If removing a sentence si leads to a significant in- crease in stance entropy SEx\\si, i.e., SEx < SEx\\si, this indicates that si contributes meaningfully to the stance expression, and it is retained."}, {"title": "3.2 Batch Reasoning", "content": "To enhance the LLM's utilization of context in stance detection, we group a batch of tweets as LLM input and classify the stance by reasoning. By processing a batch of tweets together, LLM gains access to a broader context that helps it understand relations between tweets, especially when tweets share a common theme or topic. Furthermore, the shared context also enhances the robustness and consistency of LLM's predictions across similar stance expressions.\nTo conduct the batch reasoning, we guide LLM to predict the stance likelihood PLLM,i and output corresponding explanation er for each tweet xi in the text batch B = {(xo, to), (x1, t\u2081), ...(x,t)}:\n\\(\\{ (P_{LLM,i}, e_i)\\}_{i=1}^B = LLM(\\text{prompt} \\oplus B),\\)\n\\(P_{LLM,i} = P(y|x_i, t_i, B),\\)\nwhere P(y xi,ti, B) is the conditional probability of stance output by LLM for tweet xi with respect to target ti and the context of text batch B, the prompt is the task instruction for stance detection. Shared context improves the model's ability to maintain consistency across similar stance expressions."}, {"title": "3.3 Consistency Verification", "content": "However, due to the fact that cross-influence of contextual information can potentially lead LLM to mistakenly apply the context of one tweet to another, it is important to exclude such negative influence caused by the shared context.\nTo verify LLM's predictions, we use the SLM as a third-party model to observe only the explanation for stance classification and compare the stance entropy of the prediction distribution before and after LLM reasoning. Specifically, for the corresponding explanation ei generated by LLM for the tweet xi from formula (5), it serves as the input to SLM:\n\\(P_{SLM,i} = P(y|e_i, t_i) = SLM([CLS]e_i[SEP]t_i[SEP]),\\)\nwhere PSLM,i = P(y|ei,ti) is the stance likelihood produced by the SLM based solely on the explanation e\u00bf for tweet xi toward target ti.\nThen, we calculate the stance entropy of explanation e\u00bf and tweet xi, denoted as SEe and SEx\u2081, and stance likelihood similarity between SLM and LLM"}, {"title": "Invalid Prediction:", "content": "If LLM reasoning can not expose the stance, i.e., SEe > SEx\u2081, the prediction and corresponding explanation generated by LLM for x is invalid. xi will be re-classified."}, {"title": "Valid Prediction:", "content": "If LLM reasoning exposes the stance and the predic- tion distribution from SLM and LLM is consistent, i.e., SEe < SExi and sim(PLLM,i, PSLM,i) > d, the prediction and corresponding explanation generated by LLM for xi is valid. PLLM,i will be used as the predicted result for tweet xi."}, {"title": "Referable Prediction:", "content": "If LLM reasoning exposes the stance but the pre- diction distribution from SLM and LLM is inconsistent, i.e., SEer > SExi but sim(PLLM,i, PSLM,i) < d, the prediction and corresponding explana- tion generated by LLM for xi is referable. xi will be re-classified and PLLM,i will be used for weighted-aggregation as the final prediction if the stance of xi still can not be correctly predicted after M round classifying."}, {"title": "3.4 Reasoning-Augmented Training", "content": "To ensure the SLM classifier in consistency verification (\u00a73.3) is capable of ver- ifying the correctness of LLM reasoning, we fine-tune a BERT model [13] as the classifier. To ensure that the classifier learns the correct reasoning patterns, we trained it on data collected from LLMs' correct reasoning explanations. We introduce the multi-task learning framework combining the cross-entropy and the contrastive loss [10] as:\n\\(L = L_{CE} + \\lambda. L_c,\\)\n\\(L_{CE} = - \\frac{1}{|B|} \\sum_{i=1}^{|B|} \\sum_{j=1}^{|y|} Y_{i, j} \\log(P_{i,j}),\\)\n\\(L_c = \\frac{1}{|B|} \\sum_{x_i \\in B} l_c(h_{x_i}),\\)\n\\(l_c(h_{x_i}) = \\log \\frac{\\sum_{(x_i, x_j^+) \\in P_i} \\exp(\\text{sim}(h_{x_i}, h_{x_j^+}) / T_s)}{\\sum_{x_j \\in B} \\exp(\\text{sim}(h_{x_i}, h_{x_j}) / T_s)},\\)\nwhere A is weight hyperparameter, 7, is temperature hyperparameter, h(.) is the embedding of tweet output by SLM. B = {x0,x1,...} is mini-batch training data, and B\\xi is B excluding sample xi. Pi = {(xi, X0+), (xi, X\u2081+), ...} is the set of positive pairs for xi, which consists of samples with the same label in mini-batch."}, {"title": "3.5 Workflow of CoVer", "content": "To provide a clear understanding of the workflow of CoVer, we present a detailed pseudocode in Algorithm 1, which focuses on the stages of reasoning and stance classification of CoVer."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experiment Settings", "content": "Datasets To demonstrate the effectiveness of CoVer, we perform experiments of zero-shot stance detection on three benchmarks: Sem16 (SemEval-2016) [14], P-stance [9], and VAST [1]. For Sem16 and P-stance, we use the leave- one-target-out evaluation setup. For the VAST dataset, we use their original zero-shot dataset settings. We adhere to standard train, validation, and test splits in alignment with previous studies [6,7,8]."}, {"title": "Baseline Methods", "content": "We provide an overview of the baseline methods for com- parison in our experiments, including Small Language Model Based Methods: 1) BERT-GCN [12] leverages commonsense knowledge from ConceptNet to improve the model's generalization. 2) TOAD [2] uses adversarial learning to"}, {"title": "4.2 Experimental Results", "content": "We aim to answer the following research questions (RQs) by conducting a series of experiments:\nRQ1: Does CoVer demonstrate superior effectiveness and adaptability com- pared to existing state-of-the-art stance detection methods?\nRQ2: If CoVer outperforms existing methods, what mechanisms contribute to its success?\nRQ3: Given that CoVer employs multiple re-generations strategy for those samples with low consistency, does this imply lower efficiency compared to other LLM-based methods?\nBaseline Comparison (RQ1) To answer RQ1, the baseline comparison ex- periment is conducted. As evidenced in Table 1 and Table 2, CoVer attains a comparable performance to the baselines on cross-target datasets. Specifically, CoVer achieves the best performance on sem16 and P-stance by outperforming the top existing methods with 1.98% and 2.44% on average, and outperforming all large language model based methods on VAST, showcasing robust effective- ness and adaptability across datasets.\nWe observe a clear performance gap between small and large language model based methods. LLMs utilize their internal commonsense and background knowl- edge for effective stance inference. In contrast, SLMs depend on heuristic train- ing and explicit background knowledge modeling, limiting their generalization in scenarios with imbalanced targets, such as CC, where no SLM-based method exceeds 41%, and in low-resource settings, such as Sem16, where only KASD- BERT's average performance surpasses that of the weakest LLM-based method, KASD-LLAMA-2. Furthermore, we also observe that LLMs cannot fully utilize their capabilities without consistency verification, such as GPT-3.5-Turbo-CoT- Demo outperforms GPT-3.5-Turbo-Task-Des 12.06% on Sem16 and 19.93% on VAST. Therefore, CoVer enhances consistency verification utilizing SLM, which is more efficient and effective than relying solely on LLMs for verification."}, {"title": "Ablation Study (RQ2)", "content": "To answer RQ2, the contributory of every component in CoVer is investigated by ablation study as shown in Table 3. The ablation settings and analysis are as follows:\nEffectiveness of Consistency Verification (Ver.) Ver. plays a crucial role in enhancing CoVer's overall performance by ensuring reasoning consistency. To evaluate it, we remove Ver. from CoVer for testing, (denoted as w/o Ver.). Experimental results indicate that without Ver., CoVer's FAVG significantly de- creases by 5.00% on Sem16, 6.90% on VAST and 4.85% on P-stance, requiring less LLM queries QAVG. Without Ver., the LLM performs reasoning without ver- ification, potentially leading to more biased outputs and negatively impacting overall performance. This phenomenon further highlights the importance of en- suring the consistency of reasoning. Different from existing LLM self-verification approaches, Ver. ensure the LLM's reasoning consistency via SLM with fewer (0.54 on average) LLM queries per tweet.\nEffectiveness of Contextual Reconstruction (Ctx.) Ctx. Largely en- sures CoVer's performance. To evaluate it, we remove the Ctx. from Cover for testing (denoted as w/o Ctx.). Experimental results show that without Ctx., the CoVer's FAVG decreases by 3.62% on Sem16, 4.35% on VAST, and 1.43% on P-stance. Additionally, the higher QAVG indicates that the lack of context aug- mentation also causes the inefficiency of the overall methods. This phenomenon suggests that the clearer context allows CoVer to capture implicit reasons and key information in tweets, thereby ensuring LLM to generate the consistent rea-"}, {"title": "Efficiency Comparison (RQ3)", "content": "To answer RQ3, we selected several LLM- based methods for a comparison of model performance and LLM utilization on Sem16, as shown in Table 4. We can observe that CoVer achieves the highest FAVG 74.15% with the lowest average query count (QAVG) 0.53 with less compli- cated prompt tactics. The comparison results demonstrate that CoVer achieves high performance with less LLM utilization by combining LLM batch reasoning with SLM consistency verification. This efficiency is attributed to CoVer's SLM and LLM collaboration mechanism, which leverages the strengths of the LLM for reasoning and uses SLM to reduce redundant queries to the LLM."}, {"title": "Case Study", "content": "To illustrate CoVer's consistency verification of LLM reasoning to ensure correct predictions, we conduct the case study shown in Figure 3. In this case, the tweet implies a critique of alimony but does not explicitly connect this critique to the \"Feminist Movement\", making the stance challenging to classify with certainty. Both reasoner 1 and reasoner 3 predict a \"Neutral\" stance, with moderate consistency scores (0.8341 and 0.8119, respectively), interpreting the text as lacking an explicit critical stance towards \"Feminist Movement\". Their explanations highlight that, while the text discusses alimony reform, it does not directly oppose \"Feminist Movement\". In contrast, reasoner 2 predicts an \"Against\" stance with a higher consistency score (0.8972), suggesting it inter- prets the text as implicitly critical of alimony, aligning with an opposition stance towards the \"Feminist Movement\".\nThrough weighted aggregation, CoVer assigns higher weight to reasoner 2 due to its higher consistency score, resulting in an \"Against\" stance as the final prediction. This case demonstrates CoVer's ability to reconcile differing model"}, {"title": "4.3 Discussion of CoVer", "content": "A fundamental component of CoVer is using batch reasoning to improve model efficiency. Intuitively, such shared context could introduce negative cross-influence between tweets, potentially causing bias in stance predictions. However, our ab- lation study in \u00a74.2 has shown that increasing batch size does not necessarily degrade model performance. This phenomenon warrants further investigation into the correlation between batch size scaling and LLM performance.\nWe conduct experiments across different LLMs with varying batch sizes on the Sem16 dataset. We compare the Cover with the Task-Demo baseline, whose task instruction consistent with CoVer, across batch sizes (1, 8, 16, 32) on four advanced LLMs (LLAMA-3.1-8B6, Qwen2.5-7B7, GLM4-9B8, GPT-40-mini\u00ba).\nAs shown in Fig.4, experimental results indicate that: 1) Single-sample process- ing does not achieve the best performance across different LLMs. Compared to single-sample processing, batch processing allows LLMs to simultaneously pro- cess multiple samples, which ensures the establishment of more robust pattern recognition and decision criteria. 2) Different LLMs demonstrate model-specific optimal batch sizes, e.g., LlaMa3.1-8B is 8, while Qwen2.5-7B and GPT-40-mini is 16, GLM4-9B is 32. This can be attributed to their capability for long-sequence processing. 3) CoVer consistently improves LLM performance across batch sizes, e.g., CoVer achieves an improvement of over 5% on GPT-40-mini. This sug- gests that the consistency verification and context reconstruction of CoVer can effectively remove the biases in LLM batch reasoning.\nOur CoVer validates the feasibility of batch reasoning. Furthermore, through the collaboration between SLM and LLM, CoVer achieves a balanced trade-off"}, {"title": "5 Related work", "content": "Stance Detection via Knowledge-Augmentation To enhance the under- standing and classification of a stance in a given text [7,12], many studies leverage external knowledge sources, such as knowledge graphs [12], structured databases [3], and external textual information [22] for knowledge augmentation. By in- corporating external knowledge such as DBpedia [3] or ConceptNet [16], models can gain a deeper contextual understanding, particularly useful for identifying implicit stances or understanding domain-specific terminology. Additionally, re- cent studies [17] indicate that integrating factual and contextual knowledge can significantly enhance the model's ability to detect subtle or implicit stances, especially in scenarios with limited or biased training data.\nIn summary, knowledge augmentation has been proven by existing studies to be an effective strategy for enhancing stance classification. It addresses infor- mation insufficiency by providing context, resolving ambiguities, and identifying subtle relationships between the text and the target, which is especially effective in complex scenarios where direct textual information is limited.\nStance Detection via Reasoning Many studies [11,12,21] emphasize identi- fying stances in text through logical reasoning. These methods focus on analyz- ing arguments, causal relations, and implicit cues within the text to determine the stance, making them particularly effective in few-shot and zero-shot scenar- ios with complex arguments. Recently, some studies have combined LLMs with such strategies to generate reasoning chains for stance detection. Specifically, the Logically Consistent Chain-of-Thought (LC-CoT) [21] enhances zero-shot stance detection by evaluating external knowledge requirements, invoking APIs to re- trieve background knowledge, and employing if-then logic templates to generate reasoning chains. The Collaborative Role-Infused LLM-based Agents (COLA) [6] sets up multi-role LLM agents (e.g., linguistic experts, domain specialists, social media experts) for multi-view analysis.\nIn summary, stance detection via reasoning effectively handles implicit mean- ings and multi-step reasoning contexts by logical reasoning, demonstrating sig- nificant advantages in few-shot and zero-shot scenarios."}, {"title": "6 Conclusion", "content": "In this study, we propose Collaborative Stance Detection via Small-Large Lan- guage Model Consistency Verification (CoVer), which combines the strengths of LLM and SLM to balance the computational consumption and model perfor- mance. Specifically, to ensure unbiased stance reasoning, CoVer uses the context reconstruction module for knowledge augmentation and irrelevant context fil- tering. Then, to improve the utilization of LLM, CoVer introduces the batch"}]}