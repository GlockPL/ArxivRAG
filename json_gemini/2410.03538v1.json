{"title": "Dreamming User Multimodal Representation for Micro-Video Recommendation", "authors": ["Chengzhi Lin", "Hezheng Lin", "Shuchang Liu", "Cangguang Ruan", "LingJing Xu", "Dezhao Yang", "Chuyuan Wang", "Yongqi Liu"], "abstract": "The proliferation of online micro-video platforms has underscored the necessity for advanced recommender systems to mitigate information overload and deliver tailored content. Despite advancements, accurately and promptly capturing dynamic user interests remains a formidable challenge. Inspired by the Platonic Representation Hypothesis, which posits that different data modalities converge towards a shared statistical model of reality, we introduce DreamUMM (Dreaming User Multi-Modal Representation), a novel approach leveraging user historical behaviors to create real-time user representation in a multimodal space. DreamUMM employs a closed-form solution correlating user video preferences with multimodal similarity, hypothesizing that user interests can be effectively represented in a unified multimodal space. Additionally, we propose Candidate-DreamUMM for scenarios lacking recent user behavior data, inferring interests from candidate videos alone. Extensive online A/B tests demonstrate significant improvements in user engagement metrics, including active days and play count. The successful deployment of DreamUMM in two micro-video platforms with hundreds of millions of daily active users, illustrates its practical efficacy and scalability in personalized micro-video content delivery. Our work contributes to the ongoing exploration of representational convergence by providing empirical evidence supporting the potential for user interest representations to reside in a multimodal space.", "sections": [{"title": "1 Introduction", "content": "The exponential growth of micro-video platforms like TikTok, Instagram Reels, and Kuaishou has revolutionized content consumption patterns, presenting both opportunities and challenges for recommender systems. While these platforms offer unprecedented access to diverse, short-form content, they also demand sophisticated algorithms capable of capturing users' rapidly evolving interests in real-time. The ephemeral nature of micro-video consumption, characterized by users watching numerous videos in quick succession, poses a unique challenge: how to accurately model and predict user preferences in an environment where interests can shift dramatically within a single session.\nTraditional approaches to user interest modeling have primarily focused on developing complex neural network architectures or refining optimization objectives to better integrate user feedback and content features [1, 7, 12, 16, 18, 19]. However, these methods often fall short in explicitly representing user interests in a unified multimodal space, limiting their ability to capture the nuanced interplay between different content modalities that shape user preferences.\nInspired by the Platonic Representation Hypothesis [6], which posits that representations of different data modalities are converging towards a shared statistical model of reality, we propose a novel approach to user interest modeling in the micro-video domain. As shown in Figure 1, we hypothesize that an effective user interest"}, {"title": "2 Method", "content": "In the domain of micro-video recommendation, accurately capturing users' dynamic interests in real-time is crucial. Let U and V denote the sets of users and micro-videos, respectively. For each user $u \\in U$, we have access to their historical interaction sequence $I_u = \\{(v_j, r_j)\\}_{j=1}^N$, where $v_j \\in V$ represents the $j$-th micro-video watched by user u, and $r_j$ indicates the corresponding interaction strength (e.g., watch time, likes, comments).\nOur goal is to learn a function $f : U \\rightarrow R^d$ that maps each user to a d-dimensional representation space, capturing their real-time interests based on their historical interactions. This representation should effectively model the rapid shifts in user preferences characteristic of micro-video consumption. Existing methods for user interest modeling, such as recurrent neural networks and self-attention mechanisms [10], often lack an explicit mechanism to map user interests into a multimodal representation space. This limits their ability to capture users' preferences across modalities. Our approach aims to address this limitation by leveraging insights from the Platonic Representation Hypothesis[6]."}, {"title": "2.2 The Platonic Representation Hypothesis for User Interests", "content": "Recently, the Platonic Representation Hypothesis [6] proposed that different data modalities are converging towards a unified representation that reflects objective reality. Inspired by this concept, we hypothesize that users' interest representations may reside in a multimodal space that is shared with the space of video content. This hypothesis is based on two key assumptions:\n(1) User interests are grounded in their perception and understanding of the real world, which is shaped by their interactions with content across different modalities.\n(2) If representations of different data modalities are indeed converging towards a unified multimodal space that effectively captures the real world, it is plausible that user interests can also be represented in this space.\nFormally, we posit that there exists a latent multimodal space Z that encapsulates both video content and user interests. In this"}, {"title": "2.3 DreamUMM: Dreaming User Multi-Modal Representation", "content": "DreamUMM leverages users' historical interactions to generate multimodal representations that reflect their dynamic interests. The key idea is to construct a user representation that is close to the representations of videos they prefer in the multimodal space."}, {"title": "2.3.1 User Multimodal Representation", "content": "Given a user's historical interaction sequence $I_u = \\{(u, v_j, r_j)\\}_{j=1}^M$, we aim to produce a multimodal representation $\\phi_{hist}$ for the user in the shared multimodal space Z. Let $x_j \\in Z$ be the multimodal representation of video $v_j$, derived from pre-trained multimodal models. As shown in Figure 2, we propose the following optimization criterion:\n$\\phi_{hist} = \\underset{\\mu, ||\\mu|| = 1}{\\operatorname{argmax}} \\sum_{j=1}^M a_j <x_j, \\mu>$\nwhere $a_j$ represents the user's preference for video $v_j$, and $\\langle \\cdot, \\cdot \\rangle$ denotes the inner product. This formulation has a closed-form solution:\n$\\phi_{hist} = \\frac{\\sum_{j=1}^M a_j x_j}{||\\sum_{j=1}^M a_j x_j||}$"}, {"title": "2.3.2 User Preference Scoring", "content": "Inspired by D2Q [16], we define the user's preference score $a_j$ for video $v_j$ as:\n$a_j = \\frac{1}{1 + exp(-\\alpha (w_j - t_j))}$\nwhere $w_j$ is the user's watched time on video $v_j$, $t_j$ is a long-view threshold, and $\\alpha$ controls the sensitivity between preference and watched time. This soft thresholding function accounts for the noise inherent in online behaviors."}, {"title": "2.3.3 Theoretical Justification", "content": "The DreamUMM approach aligns with the Platonic Representation Hypothesis in several ways:\n* It explicitly represents user interests in the same multimodal space as video content, reflecting the hypothesis of a shared underlying reality.\n* The use of pre-trained multimodal models to obtain video representations $x_j$ leverages the convergence of different modalities towards a unified representation.\n* The optimization criterion encourages the user representation to be similar to preferred video representations, potentially capturing the user's understanding of the \"real world\" as reflected in their video preferences."}, {"title": "2.4 Candidate-DreamUMM: Addressing Cold-Start Scenarios", "content": "While DreamUMM effectively captures user interests based on historical interactions, it may face challenges in scenarios where recent user behavior data is unavailable, such as when a user reopens the app after an extended period. To address this issue, we propose Candidate-DreamUMM, a variant designed to infer user interests solely based on the current context, i.e., the candidate videos."}, {"title": "2.4.1 Motivation", "content": "The motivation behind Candidate-DreamUMM is twofold:\n* It tackles the cold-start problem when recent user behavior data is unavailable.\n* It captures users' current interests more accurately by focusing on the candidate videos, which reflect the present context and are more likely to align with users' immediate preferences."}, {"title": "2.4.2 Formulation", "content": "For a given set of candidate videos $\\{v_i\\}_{i=1}^N$, Candidate-DreamUMM constructs a user representation as follows:\n$\\phi_{candidate} = \\underset{\\mu, ||\\mu|| = 1}{\\operatorname{argmax}} \\sum_{i=1}^N a_i \\langle x_i, \\mu \\rangle$\nThe closed-form solution is:\n$\\phi_{candidate} = \\frac{\\sum_{i=1}^N a_i x_i}{||\\sum_{i=1}^N a_i x_i||}$\nwhere $x_i$ is the multimodal representation of candidate video $v_i$, and $a_i$ is the predicted preference score for the candidate video."}, {"title": "2.4.3 Preference Score Prediction", "content": "In Candidate-DreamUMM, the preference score $a_i$ is predicted by an online sequence model:\n$a_i = f_{seq}(I_u, v_i)$"}, {"title": "2.4.4 Theoretical Connection", "content": "Candidate-DreamUMM maintains the core idea of the Platonic Representation Hypothesis by:\n* Representing user interests in the same multimodal space as video content.\n* Leveraging the predicted preferences on candidate videos to infer the user's current position in the multimodal space.\n* Adapting to the user's evolving interests by focusing on the current context, aligning with the dynamic nature of the \"real world\" representation."}, {"title": "2.5 Multimodal Representation Learning", "content": "A critical component of our approach is the learning of high-quality multimodal representations for videos. These representations form the foundation of both DreamUMM and Candidate-DreamUMM. We propose a novel framework that leverages large language models and knowledge distillation to create rich, informative video representations."}, {"title": "2.5.1 Motivation", "content": "Videos are inherently multimodal, containing visual, auditory, and textual information. Capturing the nuances of these different modalities and their interactions is crucial for effective recommendation. While large multimodal models have shown impressive capabilities in understanding such complex data, their computational requirements make them impractical for real-time recommendation systems. Our goal is to distill the knowledge from these large models into a more efficient representation."}, {"title": "2.5.2 Framework Overview", "content": "Our multimodal representation learning framework consists of the following key components:\n(1) A Multimodal Large Language Model (MLLM) for generating comprehensive video descriptions.\n(2) An encoder-decoder architecture for learning compact video representations.\n(3) A knowledge distillation process to transfer information from the MLLM to our efficient model."}, {"title": "2.5.3 Detailed Methodology", "content": "MLLM-based Video Description. We utilize a pre-trained MLLM to generate detailed descriptions of videos, including themes, characters, scenes, and other relevant information. These descriptions serve as a rich supervisory signal for our representation learning model.\nEncoder-Decoder Architecture. Our model consists of:\n* An encoder that processes multimodal inputs (e.g., video frames, audio features, metadata).\n* A fully connected layer that condenses the multimodal tokens into a single video token representation.\n* A decoder that generates the comprehensive description produced by the MLLM, using the video token as key and value inputs.\nFormally, let $E(\\cdot)$ be the encoder, $D(\\cdot)$ the decoder, and $F(\\cdot)$ the fully connected layer. The video representation x is computed as:\n$x = F(E(v))$\nwhere v represents the multimodal inputs of the video.\nTraining Objective. We train our model using a cross-entropy loss between the generated description and the MLLM-produced description:\n$L = - \\sum_i y_i log(D(x)_j)$\nwhere y is the one-hot encoded MLLM description, and $D(x)_j$ is the model's predicted probability for token j."}, {"title": "2.5.4 Theoretical Justification", "content": "This approach aligns with the Platonic Representation Hypothesis in several ways:\n* It leverages the MLLM's ability to generate unified representations across modalities.\n* The distillation process transfers this unified understanding to our more efficient model."}, {"title": "2.6 Online Application", "content": "Algorithm 1 presents the core components of our online recommendation process, integrating DreamUMM and Candidate-DreamUMM into a flexible, real-time recommendation workflow.\nThe main function, ProcessUserRequest (lines 1-13), handles each user request for recommendations. It takes four inputs: the user u, their historical interactions Iu, a set of candidate videos V, and a boolean flag useCandidate. This flag allows the system to dynamically choose between DreamUMM and Candidate-DreamUMM based on various factors such as the recency and sufficiency of the user's historical interactions, or other contextual information.\nIn our online system, the process flows as follows:\n(1) When a user requests recommendations, ProcessUserRequest is called with the appropriate parameters, including the useCandidate flag.\n(2) Based on the useCandidate flag, either DreamUMM or Candidate-DreamUMM is used to generate the user's representation.\n(3) The function then computes similarity scores between the user representation and each candidate video using their multimodal representations.\n(4) Finally, it ranks the candidate videos based on these similarity scores, potentially combining them with other relevance signals, and returns the top-k recommendations.\nThis approach allows us to efficiently generate personalized recommendations in real-time, adapting to both the user's historical preferences and the current context of available videos. By providing the flexibility to choose between DreamUMM and Candidate-DreamUMM at runtime, our system can handle various scenarios of user data availability and recommendation contexts, ensuring robust and personalized recommendations for all users.\nThe integration of multimodal representations throughout this process, from user modeling to video similarity computation, enables our system to capture rich, cross-modal information about both users and content. This aligns with our hypothesis that user interests can be effectively represented in a unified multimodal space, potentially leading to more accurate and diverse recommendations."}, {"title": "3 Experiments and Results", "content": "In this section, we present a comprehensive evaluation of DreamUMM and Candidate-DreamUMM through both online and offline experiments. The experiments are designed to answer the following research questions:\n(1)RQ1: How do DreamUMM and Candidate-DreamUMM perform in terms of improving user engagement in real-world micro-video platforms?\n(2)RQ2: How effective are DreamUMM and Candidate-DreamUMM in enhancing recommendation diversity and expanding users' interest range?\n(3)RQ3: How well does our multimodal representation learning framework capture video semantics and support accurate retrieval?"}, {"title": "3.1 Experimental Setup", "content": "Online Experiments: We conducted online A/B tests on two popular micro-video platforms, denoted as Platform A and Platform B. Each platform has hundreds of millions of daily active users (DAU). For each platform, we randomly split users into control and treatment groups, with at least 10% of the total DAU in each group. We employed several metrics to evaluate the online performance:\n* Play Count: The average number of micro-videos played per user during the experiment period."}, {"title": "3.2 Results and Analysis", "content": "RQ1: User Engagement. Table 1 presents the relative improvements of DreamUMM and Candidate-DreamUMM over the control group in terms of Play Count and Active Days on both platforms. We observe significant gains in both metrics, indicating the effectiveness of our methods in enhancing user engagement. Candidate-DreamUMM consistently outperforms DreamUMM, suggesting its superior ability to capture users' real-time interests by focusing on the current context. The lifts in Play Count and Active Days demonstrate that our methods can effectively encourage users to consume more videos and visit the platform more frequently.\nRQ2: Recommendation Diversity. Figure 4 visualizes the improvements of DreamUMM and Candidate-DreamUMM in Exposed Cluster and Surprise Cluster metrics over the control group. Both methods achieve substantial gains in recommendation diversity, with Candidate-DreamUMM showing larger improvements. The Surprise Cluster metric sees the most impressive boost, where Candidate-DreamUMM increases the proportion of surprised recommendations by 2.429% and 1.782% on Platform A and Platform B, respectively. These results validate the effectiveness of our methods, especially Candidate-DreamUMM, in expanding users' interest range and enhancing recommendation diversity.\nRQ3: Representation Quality. Table 2 presents the HitRat@100 and HitRat@200 of our model with MLLM-based representation learning and a variant without MLLM pre-training. Our full model"}, {"title": "4 Related Work", "content": "The field of video recommendation has seen substantial advancements with the evolution of deep learning techniques and the increasing availability of user interaction data. Traditional video recommendation systems focus on collaborative filtering and content-based filtering [4, 5, 13, 14]. Collaborative filtering leverages user-item interaction matrices but often struggles with cold-start problems and sparse data scenarios. Conversely, content-based filtering [2, 3, 15] utilizes video metadata and content features to recommend similar items but may not fully capture the nuanced preferences of users.\nRecent approaches have integrated deep learning models to enhance the understanding of video content and user preferences. Attention mechanisms and graph neural networks (GNNs) have been employed to model the temporal dynamics of user interactions and the complex relationships between videos [7, 9]. For instance, MARNET [7] aggregates multimodal information using a visual-centered modality grouping approach and learns dynamic label correlations through an attentive GNN.\nIn the video recommendation domain, where explicit feedback is sparse, some methods specifically address how to define whether a user is interested in a video through implicit feedback, utilizing techniques such as causal inference [16, 18], fine-grained mining [12], and distribution alignment [11, 19].\nOur approach diverges from the traditional focus on network design or defining interest. Inspired by the Platonic Representation Hypothesis [6], we concentrate on the explicit representation of user interest in a multimodal space. This method facilitates a more precise depiction of user preferences by leveraging multimodal data to construct robust user representation."}, {"title": "4.2 Multimodal Recommendation", "content": "Multimodal recommendation extends beyond traditional recommendation paradigms by incorporating diverse data modalities such as text, images, audio, and video to build a comprehensive understanding of user preferences. This approach is particularly beneficial in micro-video platforms where content is rich in multimodal features. The integration of these modalities provides a deeper semantic understanding and can significantly enhance recommendation performance.\nMultimodal learning frameworks have been developed to fuse information from various sources, leveraging techniques such as graph convolution, multimodal autoencoders, attention-based fusion methods, transformer architectures and Flat Local Minima Exploration [8, 17, 21-25]. For example, DRAGON [21] utilizes user-user co-occurrence graphs in combination with item-item multimodal graphs to enhance the user-item heterogeneous graph. MG [20] introduces a mirror-gradient method to address the training instability issues caused by multimodal input. The challenge remains in effectively combining multimodal data to reflect real-time user preferences.\nBy generating real-time user representations in a multimodal space, DreamUMM presents a practical solution for dynamic micro-video recommendation. Furthermore, our Candidate-DreamUMM variant addresses the cold start problem by inferring preferences from candidate videos alone, showcasing the flexibility and robustness of our approach in real-world applications."}, {"title": "5 Conclusion", "content": "This paper introduced DreamUMM and Candidate-DreamUMM, novel approaches for micro-video recommendation that leverage unified multimodal representations. By modeling user interests in the same multimodal space as video content, our framework addresses both dynamic preference changes and cold-start scenarios. Through extensive online A/B tests, we demonstrated significant improvements in user engagement and recommendation novelty. The successful deployment underscores the practical efficacy and scalability of our methods. Our work contributes empirical evidence supporting the Platonic Representation Hypothesis - the potential for user interest representations to reside in a multimodal space.\nThis insight opens new avenues for research in multimodal user modeling and content understanding. Looking ahead, future work will focus on designing end-to-end methods to jointly learn the shared multimodal space for users and videos, potentially enhancing personalized recommendations across domains."}]}