{"title": "The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks", "authors": ["Isaac Galatzer-Levy", "David Munday", "Jed McGiffin", "Xin Liu", "Danny Karmon", "Ilia Labzovsky", "Rivka Moroshko", "Amir Zait", "Daniel McDuff"], "abstract": "There is increasing interest in tracking the capabilities of general intelligence foundation models. This study benchmarks leading large language models and vision language models against human performance on the Wechsler Adult Intelligence Scale (WAIS-IV), a comprehensive, population-normed assessment of underlying human cognition and intellectual abilities, with a focus on the domains of Verbal Comprehension (VCI), Working Memory (WMI), and Perceptual Reasoning (PRI). Most models demonstrated exceptional capabilities in the storage, retrieval, and manipulation of tokens such as arbitrary sequences of letters and numbers, with performance on the Working Memory Index (WMI) greater or equal to the 99.5th percentile when compared to human population normative ability. Performance on the Verbal Comprehension Index (VCI) which measures retrieval of acquired information, and linguistic understanding about the meaning of words and their relationships to each other, also demonstrated consistent performance at or above the 98th percentile. Despite these broad strengths, we observed consistently poor performance on the Perceptual Reasoning Index (PRI; range 0.1-10th percentile) from multimodal models indicating profound inability to interpret and reason on visual information. Smaller and older model versions consistently performed worse, indicating that training data, parameter count, and advances in tuning are resulting in significant advances in cognitive ability.", "sections": [{"title": "1. Introduction", "content": "Generative artificial intelligence (GenAI) refers to a class of models capable of creating new content, whether it is text, images, music, or even code (Gardner et al., 2023; Ouyang et al., 2023; Team et al., 2023; Wu et al., 2023). Unlike traditional AI systems designed for specific tasks, these models learn the underlying patterns and structures within vast datasets and then use this knowledge to generate novel outputs that often mimic human creativity (Nath et al., 2024; Zhao et al., 2023). The excitement surrounding GenAI stems from its potential to revolutionize numerous fields by performing human-like cognitive functions (Wang et al., 2024; Zhang et al., 2024; Zhuang et al., 2023). This ability to understand, learn, adapt, and create in a way that mirrors our own thought processes opens doors to groundbreaking applications across industries like art, design, research, and communication (Ge et al., 2024; Ko et al., 2023; Lu et al., 2024; Yang et al., 2024). However, human cognition encompasses a vast array of specialized abilities in the processing, storage, interpretation, and generation of information across auditory and visual channels to accomplish these unique human capabilities (Cao et al., 2024; Subramonyam et al., 2023).\nRecent advances in generative AI have been driven by large parameter models (e.g., the GPT (Achiam et al., 2023), Gemini (Team et al., 2023) families) trained with a broad range of textual content. Scaling experiments have shown that testing losses often follow power law relationships with data, model size (number of parameters) and compute (Kaplan et al., 2020). This progress has extended to multimodal models that learn complex cross-modal relationships between representations such as language and visual media. The capabilities of these models has sparked debate about the"}, {"title": "2. Methods", "content": "To facilitate the assessment of GenAI models using the Wechsler Adult Intelligence Scale, Fourth Edition (WAIS-IV), we implemented a series of methodological adaptations to accommodate the unique input and output modalities of these models. This involved converting traditional verbal and visual stimuli into text-based prompts (see Appendix B) and interpreting model-generated text outputs as responses to test items. Specific adaptations for each subtest, along with validation procedures, are detailed in 2.1. For this study, we selected a representative set of state-of-the-art (SOTA) large language models (LLMs) and vision language models (VLMs), encompassing a range of model sizes, architectures, and training datasets (See section 2.2)."}, {"title": "2.1. Testing materials, administration, and scoring", "content": "Individual items from WAIS-IV subtests (Wechsler, 2008) were converted to prompts to be individually administered to the selected LLM (See Table 1 for a full list of tests with descriptions). Specifically, tasks comprising the verbal comprehension and working memory indices (VCI and WMI) were converted to language-based prompts and administered to all models including those with language-only capabilities as well as those with multimodal language and visual capabilities. Tests from the perceptual reasoning index (PRI) were only administered to multimodal models because they require both image recognition and language capabilities. We were unable to administer the Block Design subtest from the PRI, because it requires manual manipulation of real-world objects (cubes). However, valid PRI composite scores were still calculable substituting the alternate PRI subtest Figure Weights (Wechsler, 2008). Subtests from the Processing Speed Index (PSI) were not administered as there was no clear way to maintain fidelity to the WAIS-IV testing procedures that are required for valid comparison to human performance norms. A full-scale IQ score (FSIQ) could not be calculated for any model as PSI is a required component of FSIQ.\nIn each instance, the prompt presented to the model consisted of the instructions as outlined in the manual (Wechsler, 2008), including the example provided as part of the administration process."}, {"title": "3. Results", "content": "Index level results (VCI, WMI, PRI) demonstrate both exceptional capabilities and significant deficits when compared to a normative population (See Table 2 for full results). All but one model demonstrated abilities on the VCI in the Very Superior range, scoring within the top percentile of human normative ability with the exception of Gemini Flash which fell in the Superior range (82nd%ile) and Gemini Nano (23rd %ile) which demonstrated ability in the Borderline range. The majority of models also performed in the Very Superior range on the WMI, with the exception of Gemini Nano (37th%ile; Low Average). However, among the models with multimodal capabilities, performance on the PRI demonstrated a significant and consistent deficit with all models demonstrating capabilities in the Extremely Low range (< 1st%ile) with the lone exception of Claude Sonnet which demonstrated ability in the Low Average range (10th%tile).\nNext, we analyzed individual differences in index-level results to understand relative strengths and weaknesses within domains of intellectual ability (See Table 3 for full Results). First, we observed a consistent relative strength in WMI compared to other indexes representing a statistically significant difference compared to the normative population ($p<.15$ or $p<.05$ level). Overall, this indicates that models' generally evidence stronger ability to process, store and manipulate information compared to any other ability, including language. Further, all models with visual/multimodal capability to perform tests comprising the PRI demonstrated significantly worse perceptual reasoning performance p<.05) when compared verbal comprehension and working memory indexes. This indicates a general relative weakness that is invariant across distinct developers and models in the ability to understand and reason on visual information.\nNext, we examined performance on individual tests within each index (See Table 3 and Table 4). Within the WMI, a consistent relative weakness in Arithmetic, was observed in comparison to Digit Span, indicating a relative weakness in mathematical reasoning compared to the ability to encode, manipulate, and reorganize numeric values. Gemini Nano demonstrated further relative weaknesses in the ability to sequence and reverse numbers compared to the ability to encode and retrieve simple numeric lists. Analysis of the VCI demonstrated consistent relative strengths across models from independent developers, on the Information subtest compared to Similarities and Vocabulary ($p<.05$ and $p<.15$). This indicates that models are particularly strong in the storage and retrieval of natural language-encoded knowledge \u2013 often referred to as \u201ccrystallized knowledge\u201d in human subjects \u2013 in comparison to the cognitive capabilities required for verbal analogic and verbal abstract reasoning and semantic understanding. This difference in relative performance persisted across generations of models indicating that models are consistently stronger in predicting the write answer than understanding and reasoning with language concepts. Finally, within the PRI, relative differences in opposite directions between best-in class models were observed with GPT-40 demonstrating stronger relative ability in decoding Visual Puzzles and Claude Sonnet demonstrating significantly stronger relative performance in Matrix Reasoning.\nWide variability in performance was observed on the VCI. While scores were highly skewed towards exceptional performance (99.5th%ile to >99.9th%ile, models that are known to be small pa-"}, {"title": "4. Discussion", "content": "Results demonstrate that Generative AI models are capable of exceptional performance compared to normative human ability in key domains of cognition. First, and perhaps not surprising, all models demonstrate exceptional capabilities in storage, retrieval, and manipulation of arbitrary tokenized information such as sequences of numbers and letters while being significantly weaker in mathematical reasoning. While poor performance was most pronounced in small parameter models, the discrepancy between reasoning and information management persisted across model generations and developers indicating a generally stable discrepancy in ability.\nGenerative models also demonstrated exceptional verbal abilities, with variability in performance according to the size of the model. Once again, models were strongest in retrieval of stored information with consistent relative weaknesses in tasks that require understanding of linguistic concepts or the relationships between words and concepts. However, with the exception of Gemini Nano, all models demonstrated understanding as well as crystalized knowledge well above normative ability indicating that models generally excel in language-based tasks, even those requiring reasoning and understanding beyond simple regurgitation of acquired knowledge. In the case of small parameter models, they may be best for storing and retrieving information naturalistically (e.g., in the context of hardware constraints) but may not be capable of understanding or manipulating that information to solve a problem.\nFinally, the dramatically poorer performance on visual processing tasks indicates that generative models, as they stand today, have profound deficits in the ability to understand the meaning or"}, {"title": "A. Broader Impact", "content": "The development of benchmarks help to assess the performance of foundation models. Capable models need to be developed responsibility and with attention to their strengths and flaws. Leveraging knowledge and tools from disciples such as clinical and neuro-psychology has allowed us to develop a set of grounded tasks that shed insight on the functioning of LLMs that are complementary to existing public benchmarks. However, we must acknowledge that these tasks were designed specifically for evaluating human cognitive functioning and therefore extrapolation of the results to performance on more mundane, real-world tasks and conclusions that compare language model abilities to human cognitive functioning need to be treated with care."}]}