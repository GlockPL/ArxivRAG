{"title": "CodeGemma: Open Code Models Based on Gemma", "authors": ["CodeGemma Team"], "abstract": "This paper introduces CodeGemma, a collection of specialized open code models built on top of Gemma, capable of a variety of code and natural language generation tasks. We release three model variants. CodeGemma 7B pretrained (PT) and instruction-tuned (IT) variants have remarkably resilient natural language understanding, excel in mathematical reasoning, and match code capabilities of other open models. CodeGemma 2B is a state-of-the-art code completion model designed for fast code infilling and open-ended generation in latency-sensitive settings.", "sections": [{"title": "Introduction", "content": "We present CodeGemma, a collection of open code models based on Google DeepMind's Gemma models (Gemma Team et al., 2024). Continuing from Gemma pretrained models, CodeGemma models are further trained on more than 500 to 1000 billion tokens of primarily code, using the same architectures as the Gemma model family. As a result, CodeGemma models achieve state-of-the-art code performance in both completion and generation tasks, while maintaining strong understanding and reasoning skills at scale. We release a 7B code pretrained model and a 7B instruction-tuned code model. Further, we release a specialized 2B model, trained specifically for code infilling and open-ended generation. The lineage of these models is depicted in Figure 1.\n\nWe published the first collection (v1.0) on April 9th, 2024 with all three models. A month later, on May 3rd, 2024, we published a follow up (v1.1) to the pretrained 2B and instruction-tuned 7B models. Unless speed is critically important, we suggest v1.1 as it offers a well balanced quality improvement.\n\nIn this report, we provide an overview of the additions to Gemma, such as pretraining and instruction-tuning details for CodeGemma, followed by evaluations of all models across a wide variety of academic and real world tasks against similar models. Finally, we outline the areas in which CodeGemma excels and its limitations, followed by recommendations for using this model. Where applicable, we note the differences between v1.0 and v1.1."}, {"title": "Pretraining", "content": "All CodeGemma v1.0 models are further trained on 500 billion tokens of primarily English language data from web documents, mathematics, and code. The 2B v1.1 model is trained on 1 trillion tokens. All 2B models are trained with 100% code while the 7B models are trained with a 80% code-20% natural language mixture. Our code corpus comes from publicly available code repositories. Datasets are deduplicated and filtered to remove contamination of evaluation code and certain personal and sensitive data. In addition to the processing done for Gemma, we perform additional pretraining steps for code data."}, {"title": "Preprocessing for Fill-in-the-Middle", "content": "The pretrained CodeGemma models are trained using a method based on the fill-in-the-middle (FIM) task (Bavarian et al., 2022) with improvements that address the shortcomings cited in the original work as well as empirically-found systemic issues with existing FIM-trained models. The FIM rate is at 80% in most models, except the pretrained 2B v1.1 where it is at 90%. The relevant formatting control tokens are presented in Table 1. The models are trained to work with both PSM (Prefix-Suffix-Middle) and SPM (Suffix-Prefix-Middle) modes. Figure 2 shows a sample snippet formatted in PSM. We make detailed FIM usage instructions in the Inference Recommendations section."}, {"title": "Multi-file Packing", "content": "Many downstream code-related tasks involve generating code based on a repository-level context as opposed to a single file. To improve model alignment with real-world applications, we create training examples by co-locating the most relevant source files within code repositories and best-effort grouping them into the same training examples. Specifically, we employ two heuristics: dependency graph-based packing and unit test-based lexical packing.\n\nTo construct the dependency graph, we first group files by repository. For each source file, we extract imports from the top N lines and perform suffix matching to determine the longest matching paths within the repository structure. We determine edge importance (a heuristic measure) between files, and remove unimportant edges to break cyclic dependencies (common in Python). We then calculate all-pairs shortest paths within the graph, where shorter distances signify stronger file relationships. Finally, we linearize the graph of files using a topological sort, selecting the next unparented node based on minimum distance to sorted nodes and using lexicographic order to break ties.\n\nFiles not covered by this dependency graph method are sorted alphabetically within their repository with unit tests packed next to their implementations (e.g. TestFoo.java beside Foo.java)."}, {"title": "Instruction Tuning", "content": "Our training data consists of a combination of open-source math datasets and synthetically generated code, in addition to the finetuning datasets used by Gemma. By exposing the model to mathematical problems, we aim to enhance its logical reasoning and problem-solving skills, which are essential for code generation.\n\nThe instruction-tuned 7B v1.1 model differs from its 1.0 cousin in the reinforcement learning algorithm used (based on Gemma 1.1) and specifics of synthetic data generation. They all follow the general direction below."}, {"title": "Mathematics Datasets", "content": "To enhance the mathematical reasoning capabilities of coding models, we employ supervised fine-tuning on a diverse set of mathematics datasets, including:\n\nMATH Dataset A collection of 12,500 challenging mathematical problems from competitions, providing step-by-step solutions for training models in answer derivation and explanation generation (Hendrycks et al., 2021).\nGSM8k Dataset A collection of 8,500 grade school math problems. This dataset tests the multi-step reasoning abilities of models, highlighting their limitations despite the simplicity of the problems (Cobbe et al., 2021a).\nMathQA Dataset A large-scale dataset of math word problems (Amini et al., 2019) with annotations built on top of the AQuA dataset (Ling et al., 2017).\nSynthetic Mathematical Data A programmatically-generated dataset of algebraic problems used to improve ability to solve long algebra problems.\nBy leveraging these diverse datasets, we expose the model to a wide range of mathematical problems, increasing their ability to perform complex mathematical reasoning. Our training experiments indicate that these datasets significantly boost code generation performance."}, {"title": "Coding Dataset", "content": "Effectively instruction-tuning large language models for code generation tasks requires a substantial amount of question-answer pairs. We leverage synthetic code instruction data generation to create datasets used in the supervised-finetuning (SFT) and reinforcement learning from human feedback (RLHF) phase. We apply the following steps:\n\nExample Generation Following the approach outlined in the OSS-Instruct paper (Wei et al., 2023), we generate a set of self-contained question-answer pairs.\nPost-Filtering We filter question-answer pairs using an LLM tasked with evaluating the helpfulness and correctness of the generated question-answer pairs."}, {"title": "Evaluation", "content": "We evaluate CodeGemma for code completion and generation performance, as well as natural language understanding, with automated benchmarks across a variety of domains."}, {"title": "Infilling Capability", "content": "The CodeGemma models are trained for code completion purposes. We use the single-line and multi-line metrics in the HumanEval Infilling benchmarks introduced in Fried et al. (2023) to evaluate. Performance against other FIM-aware code models is shown in Table 2.\nWe observe that our 2B pretrained model is an excellent well-rounded model for code completion use cases, where low latency is a critical factor. It performs on par with the other models while being, in many cases, nearly twice as fast during inference. We attribute this speedup to the base Gemma architectural decisions."}, {"title": "Real-world Evaluation", "content": "We validate our model's infilling abilities by masking out random snippets in code with cross-file dependencies, generating samples from the model, and retesting the code files with the generated snippets to show that it performs as expected, a similar approach to Liu et al. (2023) or Ding et al. (2023). Due to our inclusion of very recently committed open source code, we do not use the evaluations directly, but use an internal version with the same testing methodology.\n\nIn addition to evaluating on offline evaluations, the model was tested within live coding environments to benchmark its performance against current Google completion models."}, {"title": "Coding Capability", "content": "The canonical benchmarks used in coding evaluation are HumanEval (Chen et al., 2021) and Mostly Basic Python Problems (Austin et al., 2021). We present our results in Table 3."}, {"title": "Python Coding", "content": "Compared to the base Gemma models (Gemma Team et al., 2024), CodeGemma models perform significantly better on tasks from the coding domain."}, {"title": "Multi-lingual Benchmarks", "content": "BabelCode (Orlanski et al., 2023) is used to measure the performance of CodeGemma on code generation across a variety of popular programming languages. Results are presented in Table 4."}, {"title": "Language Capability", "content": "We evaluate performance on a variety of domains including question answering (Bisk et al., 2019; Clark et al., 2019, 2018; Joshi et al., 2017), natural language (Hendrycks et al., 2020; Sakaguchi et al., 2019; Zellers et al., 2019) and mathematical reasoning (Cobbe et al., 2021b; Hendrycks et al., 2021). We present the results of our two 7B models next to the instruction-tuned Gemma 7B model in Figure 3.\n\nCodeGemma retains most of the same natural language capabilities seen in the base Gemma models. CodeGemma PT and IT both outperform Mistral 7B (Jiang et al., 2023) by 7.2% and Llama-2 13B model (Touvron et al., 2023) by 19.1% (numbers reported in Gemma Team et al. 2024). Further, we compare scores for GSM8K and MATH in Table 5 from several code models in the 7B size class, and show that CodeGemma excels at mathematical reasoning compared to similarly sized models."}, {"title": "Practical Considerations", "content": "CodeGemma is tailored for practical use and deployment in latency-sensitive settings. The 2B model is considerably faster than all models in our comparison set, which is critical for latency-sensitive applications such as code completion. This speedup does not come with a significant, measured compromise in quality according to our evaluations the 2B model performs as well or better compared to other open models in its class at code infilling tasks. Consequently, CodeGemma 2B is exceptionally suitable for utilization within Integrated Development Environments (IDEs), local environments, and other applications with memory constraints.\n\nThe 7B models, characterized by their strong performance, are general coding models that surpass the baseline Gemma models in terms of coding tasks while maintaining a high level of natural language comprehension. The larger memory requirement during inference renders these models particularly suitable for deployment in hosted environments and applications where model quality is of utmost importance.\n\nThe Responsible Deployment section in Gemma Team et al. (2024) contains a thorough discussion about the limitations and benefits of using an open model."}, {"title": "Inference Recommendations", "content": "For pretrained models, prompts should be formatted for code completion tasks such as function completion, docstring generation, and import suggestion. Figure 4 shows an example of a prompt format, where the file path is optional but recommended. The stopping strategy for model outputs should be chosen carefully to align with the deployment setting. The most straightforward method is to truncate upon generating a FIM sentinel token, as shown in Table 1."}, {"title": "Conclusion", "content": "We present a collection of open models specialized for coding applications, built on top of Gemma, an openly available family of language models (Gemma Team et al., 2024). These models push the state of the art in code completion and generation, while retaining natural language capabilities from the base models.\n\nThe CodeGemma models presented in this report are highly capable language models designed for effective real-world deployment, optimized to be run in latency-constrained settings while delivering high-quality code completion on a variety of tasks and languages. We show that the lessons and technologies that built Gemini and Gemma are transferable to downstream applications, and we are excited to release these models to the broader community and to enable the applications which will be built on top of these models."}]}