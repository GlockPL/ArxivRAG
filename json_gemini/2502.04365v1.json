{"title": "AI-BASED THERMAL VIDEO ANALYSIS IN PRIVACY-PRESERVING HEALTHCARE: A CASE STUDY ON DETECTING TIME OF BIRTH", "authors": ["Jorge Garc\u00eda-Torres", "\u00d8yvind Meinich-Bache", "Siren Rettedal", "Kjersti Engan"], "abstract": "Approximately 10% of newborns need some assistance to start\nbreathing and 5% proper ventilation. It is crucial that interventions\nare initiated as soon as possible after birth. Accurate documentation\nof Time of Birth (ToB) is thereby essential for documenting and\nimproving newborn resuscitation performance. However, current\nclinical practices rely on manual recording of ToB, typically with\nminute precision. In this study, we present an AI-driven, video-\nbased system for automated ToB detection using thermal imaging,\ndesigned to preserve the privacy of healthcare providers and moth-\ners by avoiding the use of identifiable visual data. Our approach\nachieves 91.4% precision and 97.4% recall in detecting ToB within\nthermal video clips during performance evaluation. Additionally,\nour system successfully identifies ToB in 96% of test cases with an\nabsolute median deviation of 1 second compared to manual anno-\ntations. This method offers a reliable solution for improving ToB\ndocumentation and enhancing newborn resuscitation outcomes.", "sections": [{"title": "1. INTRODUCTION", "content": "Approximately 10% of newborns need some assistance to start\nbreathing, and 5% proper ventilation, with birth asphyxia as one\nof the leading causes of neonatal mortality, responsible for an esti-\nmated 900,000 deaths annually [1, 2]. The Time of Birth (ToB) is\ndefined as the moment when the newborn's head, torso, and nates\nare fully visible outside the mother's perineum, and mark the crit-\nical reference point for resuscitation interventions [3]. According\nto neonatal resuscitation guidelines [4, 5], Newborn Resuscitation\nAlgorithm Activities (NRAA) should be started within the \"golden\nminute\" - the first 60 seconds after birth, as immediate action can\nsignificantly reduce the risks of death or long-term complications\ndue to birth asphyxia [1]. The NRAA guidelines are based on best\npractices and evidence research needs to be sought [6]. NRAA time-\nlines are crucial for retrospective analysis and debriefing, research\non optimal birth asphyxia treatment, and potential real-time clinical\ndecision support. Accurate ToB documentation is an essential part\nof an effective NRAA timeline; however, in clinical practice, ToB\nis recorded manually, often with minute precision. Such lack of\nprecision reduces the reliability of NRAA timeline documentation.\nArtificial Intelligence (AI) has demonstrated promise in au-\ntomating NRAA timelines [7, 8], offering a way to streamline the\ndocumentation of newborn resuscitation activities. Yet, existing AI-\nbased approaches still depend on manually recorded ToB, limiting\nthe broader applicability of these systems in critical care settings.\nThe NewbornTime project [9] seeks to address these gaps by\ndeveloping a fully automated system that generates accurate NRAA\ntimelines, including ToB and resuscitation activities. One of the key\ninnovations is our privacy-preserving method, which utilizes thermal\nimaging to detect ToB by identifying the higher skin temperature of\nnewborns compared to other persons in the room. Thermal or in-\nfrared (IR) imaging captures the heat emissions, forming images that\nshow temperature differences. This technique ensures the privacy of\nboth healthcare providers and mothers by avoiding identifiable vi-\nsual data. In healthcare, such technologies can be pivotal in building\ntrust and maintaining compliance with data protection regulations,\nsuch as the General Data Protection Regulation (GDPR).\nIn [10] we demonstrated that Al-driven, image-based systems\nusing individual thermal frames had potential for estimating ToB.\nHowever, the lack of temporal context led to imprecision in certain\nscenarios. To the author's knowledge, no other work on automatic\nToB detection exists. The primary contribution of this work is a\nspatiotemporal AI-based method for ToB detection using privacy-\npreserving thermal video. By analyzing continuous changes in\nmovement and thermal characteristics, the model captures the dy-\nnamic birth process, resulting in a more precise ToB documentation."}, {"title": "2. DATA MATERIAL", "content": "The dataset contains a total of 321 birth videos captured by ther-\nmal cameras. Approximately 80% of the births occurred in supine\nposition, 15% in side-lying position, and 5% in hands-and-knees\nposition. Data were collected in a semiautomatic manner [11] at\nStavanger University Hospital (SUS), Norway, using a passive ther-\nmal module supplied by Mobotix [12] and installed in eight delivery\nrooms. The sensor was mounted to the ceiling, centered above the\nhead of the mother. The recording was triggered when any pixel's\ntemperature exceeded 30\u00b0C, detecting human presence and stream-\nlining data capture. ToB was manually registered with second preci-\nsion by midwives or nurse assistants using the Liveborn Observation\nApp [13], specifically designed to document post-birth events in re-\nsearch projects, providing more accurate ToB logging than standard\npractice [14]. Pressing the \"Baby Born\" button in this app auto-\nmatically saved a 30-minute thermal video generated at 252\u00d7336\nresolution and 8.33 fps, covering 15 minutes before and after birth.\nThe rest of the video was discarded.\nThe registration of ToB in Liveborn was sometimes delayed due\nto all the activities that happened around the birth. Therefore, in this\nstudy, manual annotation of the ToB with second precision was done\nby carefully inspecting the thermal videos."}, {"title": "3. METHODS", "content": "In this work, we introduce an AI-driven, video-based system that\nuses spatiotemporal information to estimate ToB. The task is formu-\nlated as a binary classification problem where we want to identify\nthe birth within video clips. We propose a pipeline where thermal\nvideos are first normalized with an adaptive method based on Gaus-\nsian Mixture Models (GMM) and then streamed into our model us-\ning a sliding window with a constant stride for generating predic-\ntions. An overview of the proposed method is illustrated in Figure 1.\nGoing forward, let $V \\in \\mathbb{R}^{N\\times H \\times W}$ represent a single-channel\nthermal video with N thermal frames and a spatial resolution of H \u00d7\nW. I(n) denote the thermal frame at index n = 0, 1, ..., N \u2013 1. We\ndefine a video clip V(n) as the preceding F frames:\n$V(n) = \\{I(n), I(n \u2212 1), ..., I(n \u2212 F + 1)\\}$"}, {"title": "3.1. Video preprocessing", "content": "In Garc\u00eda-Torres et al. [15], we explored the challenges of using\nthermal sensors for ToB detection, finding limitations in relying on\nabsolute temperature values due to several distorting factors such as\nautocalibration and room temperature. This potential reliability issue\nrequired the use of relative temperatures and introduced a normaliza-\ntion challenge for our AI system. To address this, we later proposed a\nGMM-based normalization method [10] to effectively identify a rel-\nevant temperature range based on human skin temperature in each\nthermal video, preserving the physical meaning of relative tempera-\ntures and ensuring more consistency across videos (see Figure 2).\nGMM normalization involves modeling the data distribution of\na thermal video using three Gaussian components, each representing\nthree different regions: low temperatures (background), mid tem-\nperatures (clothes, hair, bed sheets), and high temperatures (human\nskin). For each thermal video V, intensity values are extracted every\n30 seconds, converted to temperature values, and organized into a\none-dimensional vector v, which is used to fit the GMM. The highest\nmean value \u00fb among the Gaussian components is selected after ap-\nplying empirical constraints to define a temperature range of interest\nbased on observations from the delivery room. We define the GMM\nnormalization function as $\\varphi_{\\mu}$, where $\\varphi_{\\mu}(V)$ represents the\nprocess of clipping the temperature values within the defined range\nof interest and rescaling them to fall between 0 and 1.\nTo generate video clips, we sample from V(n) at specific time\nintervals, mapping the frame index n to the timestamp t according\nto the frame rate fr and a constant temporal stride 7. The resulting\nsampled clip set $x(t) \\in \\mathbb{R}^{F\\times H\\times W}$ is defined as:\n$x(t) = V([frt]) \\quad t = t_0, t_0 + \\tau, t_0 + 2\\tau, ..., [N/fr]$\nwhere $t_0 = [F/fr]$ is the first valid timestamp and [] denotes the\nfloor function. Values of t lower than $t_0$ are not considered due to\nboundary problem."}, {"title": "3.2. CNN backbone", "content": "We selected I3D [16], X3D [17], and MoViNet [18] as feature ex-\ntractors for activity recognition due to their effectiveness in spa-\ntiotemporal learning and video analysis. I3D's inflated 3D convolu-\ntions enable it to capture rich spatial and temporal features, which is\ncrucial for accurately recognizing complex activities. X3D provides\na scalable and efficient architecture, making it ideal for balancing"}, {"title": "3.3. Inference process", "content": "To perform inference, a full thermal video is streamed into the model\nas x(t) using a sliding window with a constant temporal stride. We\nrepresent the positive prediction score for birth detection as \u0177 for\nsimplicity. Given a model with parameters 0, the temporal proba-\nbility score is defined as $\\hat{y}(t) = \\varphi_{\\theta}(x(t))$.\nThis temporal score is utilized for birth detection. To mitigate\npossible noise and risk of false detections, we filter this score signal\nwith a smoothing FIR filter of size K with filter coefficients h(k) =\n$\\frac{1}{K}$. The filtered score signal is computed as follows:\n$\\hat{y}_n(t) = \\sum_{k=0}^{K-1}h(k) \\cdot \\hat{y}(t - k)$\nTo estimate the ToB, we identify the first timestamp where the fil-\ntered scores exceed a predetermined confidence threshold \u03b3.\n$T_{birth} = min\\{t|\\hat{y}_n(t) \\geq \\gamma\\}$"}, {"title": "4. EXPERIMENTS", "content": "In this study, we conduct two distinct experiments. First, we imple-\nment a modeling pipeline to train, validate, and test our model on\nthermal video clips for a binary classification task: ToB or No Birth\n(NB). In the second experiment, we select the model with the best\nperformance from the first experiment to run inference on the test\nset and estimate ToB. The test set consists of 25 manually selected\nthermal videos, keeping the same distribution of the maternal posi-\ntion during birth as the dataset. Videos involving twins have been\nexcluded from the test set. The remaining videos are allocated to the\ntraining and validation sets, following an 85%/15% split."}, {"title": "4.1. Exp. 1. Video clip classification", "content": "During the model development, thermal videos are trimmed into\nshort clips, with labels assigned based on the manual annotations.\nClips are sampled using a window size of size of 4.5 seconds (F =\n37 frames), wide enough to capture the birth dynamics but allowing\nfor a fine-grained estimation of the time of birth. We set the stride to\n$\\tau$ = 0.5 seconds to generate more ToB samples, as birth occurs over\na very short time span. To guarantee birth dynamics are captured\neffectively, we ensure that the birth event is not positioned near the\nwindow boundaries.\nTo address the inherent class imbalance in the dataset, we down-\nsample the NB class by extracting a video clip every 30 seconds from\ncontinuous sequences of NB. This strategy helps retain NB variabil-\nity while significantly reducing the number of instances, though full\ndata balance is not achieved, making the use of a weighted cross-\nentropy loss function necessary during training."}, {"title": "4.1.2. Implementation details", "content": "Binary cross-entropy [19] is employed as the loss function. To mit-\nigate the impact of class imbalance, we estimate the inverted class\nweight $w_c$ for each class c \u2208 {0 (NB), 1 (ToB)}. Representing the\nground truth of the data sample index q as $y_q$, the weighted binary\ncross-entropy loss function L is defined as:\n$L(\\Upsilon_q, \\hat{\\Upsilon}_q) = w_1y_q log(\\hat{y}_q) + w_0(1 \u2013 y_q) log(1 \u2013 \\hat{y}_q)$\nWe then deploy and evaluate several binary video-based models to\nclassify ToB and NB video clips using different CNN backbones\nas feature extractors. For each backbone, we train for a maximum\nof 100 epochs with early stopping. Adaptive Moment Estimation\n(Adam) with \u03b2\u2081=0.9 and \u03b22=0.999 is utilized. Weight decay of 0.97\nis applied every 1k steps with a learning rate of 1e-5. We also use a\nmoving average with a 0.9999 decay rate. Three Teslas V100 GPUs\nwith 32 GB RAM are employed, setting a batch size of 16 per GPU.\nData augmentation is performed at video clip level during train-\ning. Brightness and contrast augmentation are applied to modify the\noverall luminance and the intensity differences. Additionally, we\napply random left-right flipping. We also implement temporal crop-\nping by randomly picking 25 consecutive frames in the clip. During\nvalidation and test, we pick the 25 frames centered in the video clip.\nWe assess precision and recall as evaluation metrics for binary\nclassification [20]. We also employ Matthew's Correlation Coeffi-\ncient (MCC) [21] as a more comprehensive metric"}, {"title": "4.2. Exp. 2. Time of birth estimation", "content": "During inference, sampled clips x(t) are generated by applying a\nsliding window of the previous F = 25 frames (3 seconds) to meet\nthe model's input requirements. As second precision is seen as pre-\ncise enough for a ToB detector, we use a stride of T = 1 second to\nimprove temporal precision. We apply a FIR filter of size K = 3\nsamples to the prediction scores y(t), resulting in the filtered scores\n$\\hat{y}_n(t)$, followed by a confidence threshold analysis to estimate ToB.\nFor evaluation purposes, we define the error err as the time dif-\nference between $T_{birth}$ and the manual annotated ToB ($T_{birth}$):\n$err = T_{birth} - T_{birth}$\nA positive err indicates that the predicted ToB occurs after the actual\nbirth, while a negative err implies that the predicted ToB is before\nthe birth. Additionally, we use the absolute error |err| to compute\nstatistical measures, including the first quartile (Q1), median (Q2),\nthird quartile (Q3), and mean value."}, {"title": "5. RESULTS & DISCUSSION", "content": "Table 1 summarizes the evaluation results for video clip classifica-\ntion. In this experiment, all the models demonstrate strong recall\ndespite the class imbalance problem, effectively identifying birth\nevents. However, our primary focus is on minimizing the FP in-\nstances to obtain a more precise ToB estimate. In this respect,\nMoViNet-A2 outperforms other 3D CNN backbones, achieving\nhigher precision and superior overall performance, as reflected by\nthe MCC score. It is worth noticing the more efficient architec-\nture of MoViNet and X3D compared to I3D, achieving competitive\nperformance with significantly fewer parameters.\nIn Exp. 2, we evaluate the capability of our system to estimate\nthe ToB. As detailed in Section 3.3, we first filter the prediction\nscores, followed by an analysis of the confidence threshold. Exam-\nples of the predicted and filtered scores for this model are presented"}, {"title": "In this work, we present a spatiotemporal AI-based approach for\nestimating the ToB using thermal video. Our system achieves a\nprecision of 91.4% and a recall of 97.4% in identifying the birth\nwithin thermal video clips and estimates ToB with a median abso-\nlute deviation of 1 second from manual annotations. The use of ther-\nmal videos protects the privacy of healthcare providers and mothers\nwhile enabling accurate ToB detection. This data could also help\nassess whether breathing aid interventions like stimulation are initi-\nated at the bedside. In future work, we will combine the image-based\nand video-based approaches to mitigate the risk of missing the birth.\nWe will also culminate the creation of Newborn Timeline a sys-\ntem capable of automatically generating detailed NRAA timelines,\nincluding ToB detection and AI-based activity recognition from re-\nsuscitation videos.\nWe believe that the integration of thermal imaging and AI-\nbased activity recognition from video can be extended to other\nnon-diagnostic healthcare applications, such as documenting activ-\nities in emergency care or security surveillance, while maintaining\nprivacy for both healthcare providers and patients.", "content": "6. CONCLUSION"}, {"title": "7. COMPLIANCE WITH ETHICAL STANDARDS", "content": "This study was performed in line with the principles of the Declara-\ntion of Helsinki. The study has been approved by the Regional Eth-\nical Committee, Region West, Norway (REK-Vest), REK number:\n222455. The project has been recommended by Sikt - Norwegian\nAgency for Shared Services in Education and Research, formerly"}]}