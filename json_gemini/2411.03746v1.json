[{"title": "Optimal Defenses Against Gradient Reconstruction Attacks", "authors": ["Yuxiao Chen", "Gamze G\u00fcrsoy", "Qi Lei"], "abstract": "Federated Learning (FL) is designed to prevent data leakage through collaborative model training without centralized data storage. However, it remains vulnerable to gradient reconstruction attacks that recover original training data from shared gradients. To optimize the trade-off between data leakage and utility loss, we first derive a theoretical lower bound of reconstruction error (among all attackers) for the two standard methods: adding noise, and gradient pruning. We then customize these two defenses to be parameter- and model-specific and achieve the optimal trade-off between our obtained reconstruction lower bound and model utility. Experimental results validate that our methods outperform Gradient Noise and Gradient Pruning by protecting the training data better while also achieving better utility. The code for this project is available here.", "sections": [{"title": "Introduction", "content": "Recent advancements in machine learning have led to remarkable achievements across multiple domains. These successes are driven largely by the ability to gather vast, diverse datasets to train these powerful models. However, this can be challenging to obtain in certain sectors such as healthcare and finance due to concerns about privacy and institutional restrictions.\nFederated or Collaborative Learning (FL) (McMahan et al., 2017) has emerged as a solution to these concerns. Federated learning is a machine learning approach where multiple institutions or devices collaboratively train a model while keeping their data localized. Instead of sharing raw data, each participant shares model updates, aggregated centrally to create a global model that benefits from all participants' insights without compromising data privacy. The assumption is that summary-level information about the model (whether the model weights or the intermediate gradients) contains less information about the trained data. However, FL is not immune to privacy risks, one type of attack that may harm privacy is the Gradient Reconstruction Attack (GRA), where adversaries attempt to reconstruct original training data from the shared gradients. Methods such as DLG (Zhu et al., 2019), CAFE (Jin et al., 2021), and GradInversion (Yin et al., 2021) have shown the feasibility of these attacks. To mitigate these risks, several defense mechanisms have been proposed. The most common approach is perturbing the gradients, such as DP-SGD (Abadi et al., 2016) and Gradient Pruning (Zhu et al., 2019). Although these methods offer some level of data protection, they often encounter a trade-off between maintaining privacy and preserving model performance (Zhang et al., 2023).\nIn this work, we develop a new defense mechanism based on generalizing existing gradient perturbation methods to an optimal and parameter-specific defense. As also noted by Shi et al. (2022), a universal defense strategy provides undifferentiated protection and is not optimal for utility-privacy trade-offs. We customize defenses for each parameter and examine how these adjustments impact both"}, {"title": "Preliminaries", "content": "Notations. We denote $x \\in \\mathbb{R}^m$ as the training data generated from a distribution $\\mathcal{D}$. $\\mathcal{L}(\\cdot, \\Theta) : \\mathbb{R}^m \\rightarrow \\mathbb{R}$ is the loss function parameterized by $\\Theta \\in \\mathbb{R}^d$. The model gradient for $x$ is $g_{\\Theta}(x) := \\nabla \\mathcal{L}_\\Theta(x)$. When no ambiguity, we write $g(x)$ for brevity. $y$ is an (random) observation generated from $x$: $y = S(g(x))$, where $S$ is a random mechanism such as adding noise. Let $\\mathbb{P}(S)$ denote the family of distributions over a set $S$."}, {"title": "Federated Learning and Gradient Reconstruction Attacks", "content": "Different from traditional centralized optimization where we train a model on curated datasets, federated learning (FL) collaboratively trains a model while the data remains decentralized and stored locally on clients. This setup intends to protect users' sensitive data without directly sharing them.\nIn FL, each client $u_i \\in \\{ u_1,..., u_n \\}$ owns a private dataset $D_i$, and the global dataset is $D = \\bigcup_{i=1}^n D_i$. A central server aims to train a model by solving the optimization problem:\n$\\min_{\\Theta} \\sum_{i=1}^n \\sum_{x_j\\in D_i} \\mathcal{L}(x_j, \\Theta)$.\nDuring training, stochastic gradient descent (SGD) is conducted, where a subset of (well-connected and active) clients $U \\subset \\{ 1, ..., n \\}$ will interact with the global server: Each active client $i \\in U$ uses a subset $D' \\subset D_i$ to create a minibatch $B = \\bigcup_{i\\in U} D'_i$. The global minibatch gradient $\\nabla_{\\Theta} \\mathcal{L}(B, \\Theta)$ is computed as a weighted average of the individual client gradients:\n$\\nabla_{\\Theta} \\mathcal{L}(B, \\Theta) = \\frac{1}{|B|} \\sum_{i \\in U} |D'_i| \\nabla_{\\Theta} \\mathcal{L}(D'_i, \\Theta)$.\nEach client shares $\\langle |D'_i|, \\nabla_{\\Theta} \\mathcal{L}(D'_i, \\Theta) \\rangle$ with the server, which then updates the model parameters as:\n$\\Theta^{t+1} \\leftarrow \\Theta^t - \\eta \\nabla_{\\Theta} \\mathcal{L}(B, \\Theta)$."}, {"title": "The Bayesian Cram\u00e9r-Rao Lower Bound", "content": "The data reconstruction problem is essentially the problem of estimation from random observations. Let $x \\in \\mathbb{R}^d$ represent training data drawn from a distribution $\\mathcal{D}$, $y \\in \\mathbb{R}^K$ denote random observations generated from $x$, and $\\hat{x}(y)$ be an estimator of $x$. We will introduce the Bayesian Cramer-Rao lower bound that relates to the lowest possible estimation error $\\mathbb{E}[||\\hat{x} - x||^2]$. First, assume the following regularity conditions hold (Crafts et al., 2024; Van Trees, 1992):\nAssumption 1 (Support). The support of $\\mathcal{D}$ is either $\\mathbb{R}^d$ or an open bounded subset of $\\mathbb{R}^d$ with a piecewise smooth boundary.\nAssumption 2 (Existence of Derivatives). The derivatives $[\\nabla_x p(x, y)]_i$ for $i = 1, . . . , d$, exist and are absolutely integrable.\nAssumption 3 (Finite Bias). The bias, defined as\n$B(x) := \\int (\\hat{x}(y) - x) p(y \\vert x) dy$,\nis finite for all $x$.\nAssumption 4 (Exchanging Derivative and Integral). The probability function $p(x, y)$ and estimator $\\hat{x}(y)$ satisfy:\n$\\nabla_x \\int p(x, y) [\\hat{x}(y) - x]^T dy = \\int \\nabla_x (p(x, y) [\\hat{x}(y) - x]^T) dy$\nfor all $x$.\nAssumption 5 (Error Boundary Conditions). For any point $x$ on the boundary of supp($\\mathcal{D}$), and any sequence $\\{ x_i \\}$ such that $x_i \\in$ supp($\\mathcal{D}$) and $x_i \\rightarrow x$, we have $B(x_i) p(x_i) \\rightarrow 0$.\nThese assumptions are satisfied by a wide range of setups. For image classification, the dataset has bounded support and the defense a differentiable density function $p(x, y)$. When we add a small Gaussian noise to the training data, all Assumptions 1 to 5 hold. (Crafts et al., 2024)\nGiven these assumptions, the Bayesian Cram\u00e9r-Rao Lower Bound is as follows:\n$\\mathbb{E}_{x, y} [(\\hat{x}(y) - x)(\\hat{x}(y) - x)^T] \\geq V_B := J_B^{-1}$;\nwhere $J_B \\in \\mathbb{R}^{D \\times D}$ is the Bayesian information matrix:\n$J_B := \\mathbb{E}_{x, y} [\\nabla_x \\log p(x, y) \\nabla_x \\log p(x, y)^T]$.\nThe matrix $J_B$ can be decomposed into two components:\n$J_B = J_p + J_D$;\nwhere $J_p$ is the prior-informed term:\n$J_p := \\mathbb{E}_x [\\nabla_x \\log p(x) \\nabla_x \\log p(x)^T]$;\nand $J_D$ is the data-informed term:\n$J_D := \\mathbb{E}_x [J_F(x)]$.\nHere, $J_F(x)$ represents the Fisher information matrix:\n$J_F(x) := \\mathbb{E}_{y \\vert x} [\\nabla_x \\log p(y \\vert x) \\nabla_x \\log p(y \\vert x)^T]$."}, {"title": "Methodology", "content": "To optimize the trade-off between the reconstruction error lower bound and training utility, we treat each observed coordinate separately and design defending strategies customized to the current data batch and model parameters, instead of a universal strategy like a constant noise level in DP-SGD. We will first present our derivation of the reconstruction error lower bound and our definition of the training utility. Then, we introduce an optimization objective to find the optimal defense parameters (such as noise's covariance matrix) that balance reconstruction error and utility."}, {"title": "The Reconstruction Error Lower Bound", "content": "To prevent data leakage, our goal is to maximize the lower bound of the reconstruction error among all estimators (reconstruction algorithms).\nFor a randomized defense mechanism $S : \\mathbb{R}^d \\rightarrow \\mathbb{P}(\\mathbb{R}^d)$ (e.g., adding noise to the gradients), the defended gradients are $y \\sim S(g(x))$. For any reconstruction algorithm $R : \\mathbb{R}^d \\rightarrow \\mathbb{R}^m$, the expected reconstruction error against the defense is:\n$\\mathbb{E}_{x \\sim \\mathcal{D}} \\mathbb{E}_{y \\sim S(g(x))} ||R(y) - x||^2$.\nDefinition 1. For a data distribution $\\mathcal{D} \\in \\mathbb{P}(\\mathbb{R}^m)$, a gradient function $g : \\mathbb{R}^m \\rightarrow \\mathbb{R}^d$, and a defense mechanism $S : \\mathbb{R}^d \\rightarrow \\mathbb{P}(\\mathbb{R}^d)$, the reconstruction error lower bound $B_{\\mathcal{D}, S}$ is as follows:\n$B_{\\mathcal{D}, S} := \\min_{R : \\mathbb{R}^d \\rightarrow \\mathbb{R}^m} \\mathbb{E}_{x \\sim \\mathcal{D}} \\mathbb{E}_{y \\sim S(g(x))} ||R(y) - x||^2$.\nWe utilize the Bayesian C-R lower bound to lower bound the reconstruction error lower bound:\nTheorem 1. Let $B_{\\mathcal{D},s}$ be as defined in Definition 1. Under Assumptions 1 to 5, we lower bound $B_{\\mathcal{D}, S}$ by:\n$B_{\\mathcal{D}, S} \\geq \\frac{d^2}{\\mathbb{E}_{x \\sim \\mathcal{D}} [\\text{tr}(J_F(x))] + d \\cdot \\Lambda_1(J_p)}$  (1)"}, {"title": "Training Utility", "content": "To assess utility, we analyze the model loss after one step of gradient descent update. Due to the complexity of the loss landscape, we make an approximation by the first-order Taylor expansion. The second-order Taylor approximation might seem straightforward and accurate, but the result is only meaningful when the loss function is convex with respect to the model parameters.\u00b9 Fay et al. (2023) analyzed the utility of DP-SGD by using the lower bound of the expected loss, derived by assuming the loss function M-smooth. However, this oversimplifies the loss landscape by using the same isotropic convex function regardless of training data or model parameters. Optimizing this bound also requires choosing the optimal learning rate, while we aim to separate the defense method from the learning rate to make our defense more general.\nTo avoid these issues, we use the expectation and variance of the model loss after one gradient update, approximated by the first-order Taylor expansion, as our utility measure. These measures 1) are independent of the learning\nrate; and 2) contain information about the loss function's landscape. A good defense method should minimally impact training utility, therefore we maximize the expectation of the training loss and minimize its variance.\nDefinition 2. Given training data $x \\in \\mathbb{R}^m$ from distribution $\\mathcal{D}$, a model with $d$ parameters $\\Theta$, and a loss function $\\mathcal{L} : \\mathbb{R}^m \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$, the first-order utility of a defense method $S$ on $x$ is the expected decrease in loss after one gradient update:\n$\\mathcal{U}_1(S, \\Theta) = \\mathbb{E}_{x \\sim \\mathcal{D}} \\mathbb{E}_{y \\sim S(\\nabla_{\\Theta}\\mathcal{L}(x,\\Theta))} \\nabla_x\\mathcal{L}(x, \\Theta) \\cdot y$. (3)\nThe second-order utility is defined as the negative variance of the loss after the update:\n$\\mathcal{U}_2(S, \\Theta) = -\\mathbb{E}_{x \\sim \\mathcal{D}} \\text{Var}_{y \\sim S(\\nabla_{\\Theta}\\mathcal{L}(x,\\Theta))} \\nabla_x\\mathcal{L}(x, \\Theta) \\cdot y$. (4)\nWe optimize the first-order utility first, and when the first-order utility is constant, we compare the second-order utility.\n\u00b9Otherwise, the approximation will suggest that larger noise increases utility, leading to an unrealistic result of infinitely large optimal noise."}, {"title": "Optimal Gradient Noise", "content": "Gradient Noise. One of the simplest defense methods, also a step in DP-SGD (Abadi et al., 2016), is to add Gaussian noise to the model gradients before sharing. For a given covariance matrix $\\Sigma \\in \\mathbb{R}^{d \\times d}$, the gradient noise defense is as follows:\n$S_{\\text{noise}, \\Sigma}(x) = \\mathcal{N}(x, \\Sigma)$. (5)\nOptimal Gradient Noise. The first-order utility defined in Eq. 3 remains constant regardless of the choice of the covariance matrix $\\Sigma$:\n$\\mathcal{U}_1(S_{\\text{noise},\\Sigma}, \\Theta) = \\mathbb{E}_{x \\sim \\mathcal{D}} \\nabla_{\\Theta}\\mathcal{L}(x, \\Theta) \\cdot \\nabla_{\\Theta}\\mathcal{L}(x, \\Theta)^T$.\nThus, we focus on maximizing the second-order utility. Assuming independent noise across parameters (as in DP-SGD), we limit our analysis to diagonal matrices. In this case, the second-order utility equals:\n$\\mathcal{U}_2 (S_{\\text{noise}, \\Sigma}, \\Theta) = - \\sum_{i=1}^d \\mathbb{E}_{x \\sim \\mathcal{D}} (\\frac{\\partial \\mathcal{L}(x, \\Theta)}{\\partial x_i})^2 \\Sigma_{i,i}$. (6)\nFor a higher reconstruction error lower bound, we minimize $\\mathbb{E}_{x \\sim \\mathcal{D}} \\text{tr}(J_F(x))$, where:\n$J_F(x) = \\sum_{i=1}^d \\frac{\\| \\nabla_x g_i(x) \\|^2}{\\Sigma_{i,i}}$. (7)\nThis decomposition allows us to separate the influence the defense of each parameter has on utility and privacy, setting the stage for deriving the optimal noise."}, {"title": "Optimal DP-SGD", "content": "DP-SGD. We extend our analysis in Section 3.3 to optimize the noise in DP-SGD. DP-SGD differs from gradient noise by a gradient clipping step before adding noise. For a fixed clipping threshold $P$, let $g_P(x)$ represent the clipped gradients with elements:\n$g_P(x)_i = \\begin{cases} g(x)_i, & \\text{if } -P < g(x)_i < P, \\\\ P, & \\text{if } g(x)_i \\geq P, \\\\ -P, & \\text{if } g(x)_i \\leq -P. \\end{cases}$ (9)\nThe generalized DP-SGD defense, $S_{\\text{DPSGD}, \\Sigma, P} : \\mathbb{R}^d \\rightarrow \\mathbb{P}(\\mathbb{R}^d)$, is as follows:\n$S_{\\text{DPSGD}, \\Sigma, P}(x) = \\mathcal{N}(g_P(x), \\Sigma)$, (10)\nwhere $\\Sigma$ is the noise covariance. For $\\Sigma = \\epsilon I_d$, this reduces to standard DP-SGD. Since the first-order utility is constant, we find the $\\Sigma$ that optimizes the second-order utility:\n$\\mathcal{U}_2 (S_{\\text{DPSGD}, \\Sigma, P}, \\Theta) = - \\frac{d}{2} \\sum_{i=1} (\\mathbb{E}_{x \\sim \\mathcal{D}} (\\frac{\\partial \\mathcal{L}(x, \\Theta)}{\\partial x_i})^2) \\Sigma_{i,i}$. (11)\nTheorem 3 (Optimal DP-SGD). Under assumptions 1 to 5, and assuming $\\mathbb{E}_{x \\sim \\mathcal{D}} g_i(x)^2 > 0$ for all $i$, the optimal noise matrix $\\Sigma$ that maximizes the reconstruction error lower bound (Eq. 1) for a utility budget of\n$\\mathcal{U}_2(S_{\\text{DPSGD}, \\Sigma, P}, \\Theta) \\geq -C$\nhas diagonal elements:\n$\\Sigma_{i,i} = \\lambda \\frac{\\mathbb{E}_{x \\sim \\mathcal{D}} || \\nabla_x g_P(x)_i ||^2}{\\mathbb{E}_{x \\sim \\mathcal{D}} g_P(x)^2}$ (12)\nwhere $\\lambda$ is a constant, and $g(x) = \\nabla_x \\mathcal{L}_\\Theta(x)$ is the model gradient on the data $x$.\nIn the special case where $\\mathcal{D}$ is supported on a small neighborhood of $x$, the locally optimal DP-SGD noise becomes:\n$\\Sigma_{i,i} = \\begin{cases} 0, & \\text{if } |g_i(x)| < P, \\\\ \\frac{\\mathbb{E}_{x \\sim \\mathcal{D}} g_i(x)^2}{\\mathbb{E}_{x \\sim \\mathcal{D}} || \\nabla_x g_i(x) \\|^2}, & \\text{if } |g_i(x)| = P. \\end{cases}$ (13)\nTo summarize, we could optimize the privacy-utility tradeoff for DP-SGD by changing the noise to zero for clipped gradients, and to our optimal noise for other gradients."}, {"title": "Optimal Gradient Pruning", "content": "Gradient Pruning. Gradient pruning reduces the number of parameters in the shared gradient by zeroing out less significant gradients during training. Inspired by gradient compression (Lin et al., 2018; Tsuzuku et al., 2018), this approach prunes gradients with the smallest magnitude (Zhu et al., 2019). It is also the most effective defense against DLG (Zhu et al., 2019).\nFor a given set of parameters $A$, the gradient pruning defense method $S_{\\text{prune}, A} : \\mathbb{R}^d \\rightarrow \\mathbb{P}(\\mathbb{R}^d)$ is as follows:\n$S_{\\text{prune}, A}(x)_i = \\begin{cases} 0, & \\text{if } i \\in A, \\\\ x_i, & \\text{if } i \\notin A. \\end{cases}$ (14)\nOptimal Gradient Pruning Under assumptions 1 to 5, the first-order utility of gradient pruning equals the sum of the squared unpruned gradients:\n$\\mathcal{U}_1 (S, \\Theta) = \\sum_{i \\notin A} (\\mathbb{E}_{x \\sim \\mathcal{D}} \\frac{\\partial \\mathcal{L}(x, \\Theta)}{\\partial x_i})^2$. (15)\nSince gradient pruning introduces no randomness, an accurate reconstruction is theoretically possible when the number of unpruned parameters exceeds the input dimension. To address this problem, we add a small noise to the unpruned gradients and analyze the noisy version of gradient pruning:\n$S_{\\text{prune}, A, \\Sigma}(x) = \\begin{cases} 0, & \\text{if } i \\in A, \\\\ \\mathcal{N}(x_i, \\Sigma_{i,i}), & \\text{if } i \\notin A. \\end{cases}$ (16)\nFor $\\Sigma = \\epsilon^2 I_d$, this collapses to the original pruning method when $\\Sigma_{i,i}$ remains constant and $\\epsilon \\rightarrow 0$."}, {"title": "Algorithm design", "content": "Because of the high computational cost of the expectation terms in the globally optimal defense methods (Theorems 2 to 4), our implementations are based on the locally optimal versions (Eqs. 8, 13 and 17).\nWhen computing optimal defense parameters, calculating the Jacobian matrix of model gradients on input data is especially challenging. The full Jacobian matrix for an image with a resolution of 32 \u00d7 32 would require roughly 3000 times the memory of the model itself, which is prohibitively large. We resolve this problem by using the forward differentiation method to save computational cost and use approximation to save memory cost.\nThe forward method (Griewank and Walther, 2008) tracks gradients based on the input tensor size rather than the output tensor size, and therefore more efficient since we are dealing with low input and high output dimensions. We approximate the l2-norm of the gradients using Lemma 1:\nLemma 1. Given a differentiable function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ and a constant $\\epsilon > 0$. For any number of samples $k \\in \\mathbb{Z}$ and random vectors $X_1, ..., X_k$ sampled from $\\mathcal{N}(0, I_n)$, we have that\n$|| \\nabla_x f(x) ||^2 - \\frac{2}{k} \\sum_{i=1}^k (\\frac{\\partial f(x + \\alpha x_i)}{\\partial \\alpha})_{\\alpha=0}^2 < \\epsilon || \\nabla_x f(x) ||^2$\nwith probability at least $1 - ke^2$ for any $x \\in \\mathbb{R}^d$.\nThis allows us to approximate the l2-norms without the entire Jacobian matrix, significantly reducing computational and memory cost. Our resulting algorithm is outlined in Algorithm 1."}, {"title": "Experiments", "content": "We compare our proposed algorithms with existing defense methods on two datasets: MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky and Hinton, 2009). As our algorithm employs different defenses on different parameters, we use an attack that treats parameters equally. One attack with such property is the Inverting Gradients attack (Geiping et al., 2020), a powerful attack that does not require extra information or specific model architecture."}, {"title": "MNIST", "content": "The MNIST dataset consists of 28 \u00d7 28 grayscale images of handwritten digits, serving as a simple test for our algorithm."}, {"title": "Gradient Pruning", "content": "We apply different pruning thresholds to a randomly initialized Convolutional Neural Network (Fukushima, 1969), using 4 batches of 16 images to compute gradients. These gradients were defended using gradient pruning and our optimal gradient pruning, followed by an Inverting Gradients attack. Figure 2 shows that our method consistently achieves higher Mean Squared Error (MSE) and lower Peak Signal-to-Noise Ratio (PSNR) at commonly used high pruning ratios, indicating stronger defenses.\nTo assess training utility, we trained the models under a federated learning setting. Figure 3 shows that 80% optimal pruning outperforms 90% gradient pruning in training speed, while we showed that they have similar privacy in Figure 2. The scatter plot in Figure 4 shows the privacy-utility trade-off for a wider range of pruning ratios, indicating the superior privacy-utility trade-off of our method."}, {"title": "DP-SGD", "content": "We also evaluated our defense on DP-SGD. As shown in Figure 5, our optimal noise achieves comparable performance to DP-SGD in terms of defense at the same noise scale, while our algorithm has faster learning speed (Figure 6). The scatter plot in Figure 7 further demonstrates the improved privacy-utility trade-off of our approach. Visualization of the reconstruction in Figure 8 shows better protection against attacks using our optimal noise for the same level of training utility."}, {"title": "CIFAR-10", "content": "We extend our experiments to the CIFAR-10 dataset, consisting of colored images with size 32 \u00d7 32. We used a larger model with 2.9M parameters."}, {"title": "Optimal Pruning", "content": "As shown in Figure 9, our optimal pruning achieved higher reconstruction error than gradient pruning for the same level of training utility, with a pruning ratio of 70% outperforming 90% pruning in gradient pruning. Visual comparison (Figure 10) also indicates better protection using our method."}, {"title": "Optimal Noise", "content": "Reconstructing CIFAR-10 images is more challenging, with failures occurring at noise scales larger than 10-3. However, our noise method still offers a better privacy-utility trade-off, as shown in Figure 11. The advantage in training utility is more significant when the noise scale is larger. (Details in Appendix D.)"}, {"title": "Discussion", "content": "In this work, we derived a theoretical reconstruction lower bound and used it to formulate optimal defense methods as improvements of gradient noise and gradient pruning. Our experimental results on MNIST and CIFAR-10 demonstrate the effectiveness of our approach.\nAs we are only presenting the possibility of a better privacy-utility tradeoff, a key limitation of our methods is the high computational cost of our algorithm. This could be mitigated through simplifications (e.g. layer-wise defense) or lowering the frequency of updating defense parameters. Additionally, the reconstruction bound used in our analysis is not tight. The utilization of more precise bounds or indices that integrate current attack methods remains an open challenge for further research.\nFurthermore, our analysis could potentially be applied to other defense methods or designed against other types of attacks. This also remains an open challenge."}, {"title": "MISSING PROOFS", "content": ""}, {"title": "Proof of Theorem 1", "content": "Proof. Recall the definition\n$B_{\\mathcal{D},S} = \\min_{R : \\mathbb{R}^d \\rightarrow \\mathbb{R}^m} \\mathbb{E}_{x \\sim \\mathcal{D}} \\mathbb{E}_{y \\sim S(g(x))} ||R(y) - x||^2$.\nBy the Bayesian Cram\u00e9r-Rao lower bound, for any reconstruction algorithm R we have that:\n$\\mathbb{E}_{x \\sim \\mathcal{D}} \\mathbb{E}_{y \\sim S(g(x))} [(R(y) - x) (R(y) - x)^T] \\geq J_B^{-1}$,\nwhere $J_B = J_P + J_D$ and $J_D := \\mathbb{E}_{x \\sim \\mathcal{D}} [J_F(x)]$.\nTherefore:\n$\\mathbb{E}_{x \\sim \\mathcal{D}} \\mathbb{E}_{y \\sim S(g(x))} ||R(y) - x||^2 = \\text{tr} (\\mathbb{E}_{x \\sim \\mathcal{D}} \\mathbb{E}_{y \\sim S(g(x))} [(R(y) - x) (R(y) - x)^T])$ \n$\\geq \\text{tr} (J_B^{-1})$.\nSince both $J_D$ and $J_P$ are Fisher information matrices and hence symmetric, we could apply Weyl's inequality to bound the eigenvalues of $J_B$. Let $\\lambda_i$ denote sorted eigenvalues with $\\lambda_1$ being the smallest and $\\lambda_d$ the largest. For the eigenvalues $\\lambda_i$ of $J_B$, we have:\n$\\lambda_i(J_B) \\leq \\lambda_i(J_D) + \\Lambda_1(J_P)$.\nThis implies that\n$\\text{tr}(J_B^{-1}) = \\sum_{i=1}^d \\frac{1}{\\lambda_i(J_B)}$\n$\\geq \\sum_{i=1}^d \\frac{1}{\\lambda_i(J_D) + \\Lambda_1(J_P)}$\n$\\geq \\frac{d^2}{\\text{tr}(J_D) + d \\cdot \\Lambda_1(J_P)}$.\nThe last equation is from Cauchy's inequality since $\\lambda_i(J_D) + \\Lambda_1(J_P) > 0$.\nSubstituting $\\text{tr}(J_D) = \\mathbb{E}_{x \\sim \\mathcal{D}} [\\text{tr}(J_F(x))]$, we obtain:\n$\\text{tr}(J_B^{-1}) \\geq \\frac{d^2}{\\mathbb{E}_{x \\sim \\mathcal{D}} [\\text{tr}(J_F(x))] + d \\cdot \\Lambda_1(J_P)}$.\nThus, we have shown that\n$B_{\\mathcal{D},S} \\geq \\frac{d^2}{\\mathbb{E}_{x \\sim \\mathcal{D}} [\\text{tr}(J_F(x))] + d \\cdot \\Lambda_1(J_P)}$\n$\\Box$"}, {"title": "Proof of Theorem 2", "content": "To prove the theorem", "J_F(x)$": "nLemma. Let $J_F(x)$ be the Fisher information matrix defined in Theorem 1. Let $y = S(x)$ be gradients defended with gradient noise using covariance matrix $\\Sigma$. We have that:\n$J_F(x) = \\nabla_x g(x) \\Sigma^{-1"}, "nabla_x g(x)^T$.\nProof. $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}^n$ is the function from input data to model gradients.\nGiven $y = g(x) + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, \\Sigma)$, we have the log-likelihood function:\n$\\log p(y \\vert x) = - \\frac{d}{2} \\log(2 \\pi) - \\frac{1}{2} \\log |\\Sigma| - \\frac{1}{2} (y - g(x))^T \\Sigma^{-1} (y - g(x))$.\nThe gradient of the log-likelihood with respect to x is:\n$\\nabla_x \\log p(y \\vert x) = (y - g(x)) \\Sigma^{-1} \\nabla_x g(x)$.\nBy definition:\n$J_F(x) = \\mathbb{E}_{y \\vert x} [\\nabla_x \\log p(y \\vert x) \\nabla_x \\log p(y \\vert x)^T"], "2": "nProof. We want to minimize $\\mathbb{E"}, {"equals": "n$\\mathcal{U}_2(S, \\Theta) = -\\mathbb{E}_{x \\sim \\mathcal{D}} \\sum_{i=1}^d (\\frac{\\partial \\mathcal{L}(\\Theta, x)}{\\partial x_i})^2 \\Sigma_{i,i} = -\\sum_{i=1}^d \\mathbb{E}_{x \\sim \\mathcal{D}} g_i(x)^2 \\Sigma_{i,i}$,\nwhere the second equation is from the definition of $g_i(x)$. By Cauchy's inequality, we have that\n$\\mathbb{E}_{x \\sim \\mathcal{D}} \\text{tr} (J_F(x)) = \\sum_{i=1}^d \\frac{\\mathbb{E}_{x \\sim \\mathcal{D}} \\| \\nabla_x g_i(x) \\|^2}{\\Sigma_{i,i}}$\n$\\geq \\frac{(\\sum_{i=1}^d \\mathbb{E}_{x \\sim \\mathcal{D}} \\| \\nabla_x g_i(x) \\|^2 \\cdot \\mathbb{E}_{x \\sim \\mathcal{D}} g_i(x)^2)^2}{\\sum_{i=1}^d \\mathbb{E}_{x \\sim \\mathcal{D}} g_i(x)^2 \\Sigma_{i,i}}$\n$\\geq \\frac{1}{C} (\\sum_{i=1}^d \\sqrt{\\mathbb{E}_{x \\sim \\mathcal{D}} \\| \\nabla_x g_i(x) \\|^2} \\sqrt{\\mathbb{E}_{x \\sim \\mathcal{D}} g_i(x"}]