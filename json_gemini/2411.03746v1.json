{"title": "Optimal Defenses Against Gradient Reconstruction Attacks", "authors": ["Yuxiao Chen", "Gamze G\u00fcrsoy", "Qi Lei"], "abstract": "Federated Learning (FL) is designed to prevent data leakage through collaborative model training without centralized data storage. However, it remains vulnerable to gradient reconstruction attacks that recover original training data from shared gradients. To optimize the trade-off between data leakage and utility loss, we first derive a theoretical lower bound of reconstruction error (among all attackers) for the two standard methods: adding noise, and gradient pruning. We then customize these two defenses to be parameter- and model-specific and achieve the optimal trade-off between our obtained reconstruction lower bound and model utility. Experimental results validate that our methods outperform Gradient Noise and Gradient Pruning by protecting the training data better while also achieving better utility. The code for this project is available here.", "sections": [{"title": "Introduction", "content": "Recent advancements in machine learning have led to remarkable achievements across multiple domains. These successes are driven largely by the ability to gather vast, diverse datasets to train these powerful models. However, this can be challenging to obtain in certain sectors such as healthcare and finance due to concerns about privacy and institutional restrictions.\nFederated or Collaborative Learning (FL) (McMahan et al., 2017) has emerged as a solution to these concerns. Federated learning is a machine learning approach where multiple institutions or devices collaboratively train a model while keeping their data localized. Instead of sharing raw data, each participant shares model updates, aggregated centrally to create a global model that benefits from all participants' insights without compromising data privacy. The assumption is that summary-level information about the model (whether the model weights or the intermediate gradients) contains less information about the trained data. However, FL is not immune to privacy risks, one type of attack that may harm privacy is the Gradient Reconstruction Attack (GRA), where adversaries attempt to reconstruct original training data from the shared gradients. Methods such as DLG (Zhu et al., 2019), CAFE (Jin et al., 2021), and GradInversion (Yin et al., 2021) have shown the feasibility of these attacks. To mitigate these risks, several defense mechanisms have been proposed. The most common approach is perturbing the gradients, such as DP-SGD (Abadi et al., 2016) and Gradient Pruning (Zhu et al., 2019). Although these methods offer some level of data protection, they often encounter a trade-off between maintaining privacy and preserving model performance (Zhang et al., 2023).\nIn this work, we develop a new defense mechanism based on generalizing existing gradient perturbation methods to an optimal and parameter-specific defense. As also noted by Shi et al. (2022), a universal defense strategy provides undifferentiated protection and is not optimal for utility-privacy trade-offs. We customize defenses for each parameter and examine how these adjustments impact both"}, {"title": "Related Works", "content": "Federated learning (FL) was introduced by McMahan et al. (2017) as a framework for collaborative model training without centralized data storage. Differential privacy (DP) (Dwork et al., 2006b,a; Dwork and Roth, 2014) has been used to define privacy of the algorithm. (Abadi et al., 2016) introduced a differential private SGD algorithm to provide DP guarantees to the trained model. Stock et al. (2022) used R\u00e9nyi differential privacy to provide another type of guarantee.\nHowever, Zhu et al. (2019) revealed a significant vulnerability in FL by demonstrating how training data can be reconstructed from shared gradients using the DLG algorithm. This attack was refined through subsequent works, such as iDLG (Zhao et al., 2020) and Inverting Gradients (Geiping et al., 2020). More advanced techniques, including GradInversion (Yin et al., 2021) and CAFE (Jin et al., 2021), further enhance reconstruction quality but often rely on additional information or specific model architectures. More works featuring attacks include Wang et al. (2023); Jeon et al. (2021); Chen and Campbell (2021).\nIn response to these attacks, several defense strategies have been proposed. One line of methods perturb the gradients shared to the server(Sun et al., 2021; Andrew et al., 2021), while another line of work like InstaHide (Huang et al., 2020), mixup (Zhang et al., 2018), pixelization (Fan, 2018) focus on directly protecting the data instead of the gradients. More details about attacks and defenses could be found in Zhang et al. (2022); Bouacida and Mohapatra (2021); Jegorova et al. (2023). To consolidate research in this area, Liu et al. (2024) proposed a framework to systematically analyze the effectiveness of different attacks and defenses.\nIn a similar vein, Fay et al. (2023) explored hyperparameter selection to optimize the privacy-utility trade-off in DP-SGD, Xue et al. (2024) proposed DP-SGD with adaptive noise. However, these approaches do not account for the local parameter landscape, which we address in our work. The lower bound on the reconstruction error was first introduced in previous work (Liu et al., 2024) except that only a local approximation was used."}, {"title": "Preliminaries", "content": "Notations. We denote $x \\in \\mathbb{R}^m$ as the training data generated from a distribution $D$. $L(\\cdot, \\Theta) : \\mathbb{R}^m \\to \\mathbb{R}$ is the loss function parameterized by $\\Theta \\in \\mathbb{R}^d$. The model gradient for $x$ is $g_\\Theta(x) := \\nabla L_\\Theta(x)$. When no ambiguity, we write $g(x)$ for brevity. $y$ is an (random) observation generated from $x$: $y = S(g(x))$, where $S$ is a random mechanism such as adding noise. Let $P(S)$ denote the family of distributions over a set $S$."}, {"title": "Federated Learning and Gradient Reconstruction Attacks", "content": "Different from traditional centralized optimization where we train a model on curated datasets, federated learning (FL) collaboratively trains a model while the data remains decentralized and stored locally on clients. This setup intends to protect users' sensitive data without directly sharing them.\nIn FL, each client $u_i \\in \\{u_1, ..., u_n\\}$ owns a private dataset $D_i$, and the global dataset is $D = \\bigcup_{i=1}^n D_i$. A central server aims to train a model by solving the optimization problem:\n$\\min_{\\Theta} \\sum_{i=1}^n \\sum_{x_j \\in D_i} L(x_j, \\Theta)$.\nDuring training, stochastic gradient descent (SGD) is conducted, where a subset of (well-connected and active) clients $U \\subset \\{1, ..., n\\}$ will interact with the global server: Each active client $i \\in U$ uses a subset $D_i' \\subset D_i$ to create a minibatch $B = \\bigcup_{i\\in U} D_i'$. The global minibatch gradient $\\nabla_{\\Theta} L(B, \\Theta)$ is computed as a weighted average of the individual client gradients:\n$\\nabla_{\\Theta} L(B, \\Theta) = \\frac{1}{|B|} \\sum_{i \\in U} |D_i'| \\nabla_{\\Theta} L(D_i', \\Theta)$.\nEach client shares $\\langle |D_i'|, \\nabla_{\\Theta} L(D_i', \\Theta) \\rangle$ with the server, which then updates the model parameters as:\n$\\Theta^{t+1} \\leftarrow \\Theta^{t} - \\eta \\nabla_{\\Theta} L(B, \\Theta)$."}, {"title": "Methodology", "content": "To optimize the trade-off between the reconstruction error lower bound and training utility, we treat each observed coordinate separately and design defending strategies customized to the current data batch and model parameters, instead of a universal strategy like a constant noise level in DP-SGD. We will first present our derivation of the reconstruction error lower bound and our definition of the training utility. Then, we introduce an optimization objective to find the optimal defense parameters (such as noise's covariance matrix) that balance reconstruction error and utility."}, {"title": "The Reconstruction Error Lower Bound", "content": "To prevent data leakage, our goal is to maximize the lower bound of the reconstruction error among all estimators (reconstruction algorithms).\nFor a randomized defense mechanism $S : \\mathbb{R}^d \\to P(\\mathbb{R}^d)$ (e.g., adding noise to the gradients), the defended gradients are $y \\sim S(g(x))$. For any reconstruction algorithm $R : \\mathbb{R}^d \\to \\mathbb{R}^m$, the expected reconstruction error against the defense is:\n$E_{x \\sim D}E_{y \\sim S(g(x))} ||R(y) - x||^2$.\nDefinition 1. For a data distribution $D \\in P(\\mathbb{R}^m)$, a gradient function $g : \\mathbb{R}^m \\to \\mathbb{R}^d$, and a defense mechanism $S : \\mathbb{R}^d \\to P(\\mathbb{R}^d)$, the reconstruction error lower bound $B_{D,S}$ is as follows:\n$B_{D,S} := \\min_{R: \\mathbb{R}^d \\to \\mathbb{R}^m} E_{x \\sim D}E_{y \\sim S(g(x))} ||R(y) - x||^2$.\nWe utilize the Bayesian C-R lower bound to lower bound the reconstruction error lower bound:\nTheorem 1. Let $B_{D,s}$ be as defined in Definition 1. Under Assumptions 1 to 5, we lower bound $B_{D,S}$ by:\n$B_{D,S} \\geq \\frac{d^2}{E_{x \\sim D} [tr(J_F(x))] + d \\cdot \\Lambda_1(J_P)}$"}, {"title": "Training Utility", "content": "To assess utility, we analyze the model loss after one step of gradient descent update. Due to the complexity of the loss landscape, we make an approximation by the first-order Taylor expansion. The second-order Taylor approximation might seem straightforward and accurate, but the result is only meaningful when the loss function is convex with respect to the model parameters. Fay et al. (2023) analyzed the utility of DP-SGD by using the lower bound of the expected loss, derived by assuming the loss function M-smooth. However, this oversimplifies the loss landscape by using the same isotropic convex function regardless of training data or model parameters. Optimizing this bound also requires choosing the optimal learning rate, while we aim to separate the defense method from the learning rate to make our defense more general.\nTo avoid these issues, we use the expectation and variance of the model loss after one gradient update, approximated by the first-order Taylor expansion, as our utility measure. These measures 1) are independent of the learning"}, {"title": "Optimal Gradient Noise", "content": "Gradient Noise. One of the simplest defense methods, also a step in DP-SGD (Abadi et al., 2016), is to add Gaussian noise to the model gradients before sharing. For a given covariance matrix $\\Sigma \\in \\mathbb{R}^{d\\times d}$, the gradient noise defense is as follows:\n$S_{\\text{noise},\\Sigma}(x) = N(x, \\Sigma)$.\nOptimal Gradient Noise. The first-order utility defined in Eq. 3 remains constant regardless of the choice of the covariance matrix $\\Sigma$:\n$U_1(S_{\\text{noise},\\Sigma}, \\Theta) = E_{x \\sim D} \\nabla_x L(x, \\Theta) \\cdot \\nabla_x L(x, \\Theta)^T$.\nThus, we focus on maximizing the second-order utility. Assuming independent noise across parameters (as in DP-SGD), we limit our analysis to diagonal matrices. In this case, the second-order utility equals:\n$U_2(S_{\\text{noise},\\Sigma}, \\Theta) = - \\frac{\\eta}{2} \\sum_{i=1}^d E_{x \\sim D} (\\frac{\\partial L(x, \\Theta)}{\\partial x_i})^2 \\Sigma_{i,i}$.\nFor a higher reconstruction error lower bound, we minimize $E_{x \\sim D} tr(J_F(x))$, where:\n$J_F(x) = \\sum_{i=1}^d \\frac{||\\nabla_x g(x)||^2}{\\Sigma_{i,i}}$.\nThis decomposition allows us to separate the influence the defense of each parameter has on utility and privacy, setting the stage for deriving the optimal noise."}, {"title": "Optimal DP-SGD", "content": "DP-SGD. We extend our analysis in Section 3.3 to optimize the noise in DP-SGD. DP-SGD differs from gradient noise by a gradient clipping step before adding noise. For a fixed clipping threshold $P$, let $g_P(x)$ represent the clipped gradients with elements:\n$g_P(x)_i = \\begin{cases} g(x)_i, & \\text{if } -P < g(x)_i < P, \\\\ P, & \\text{if } g(x)_i \\geq P, \\\\ -P, & \\text{if } g(x)_i \\leq -P. \\end{cases}$\nThe generalized DP-SGD defense, $S_{\\text{DPSGD}, \\Sigma, P} : \\mathbb{R}^d \\to P(\\mathbb{R}^d)$, is as follows:\n$S_{\\text{DPSGD}, \\Sigma, P}(x) = N(g_P(x), \\Sigma)$,\nwhere $\\Sigma$ is the noise covariance. For $\\Sigma = \\epsilon I_d$, this reduces to standard DP-SGD. Since the first-order utility is constant, we find the $\\Sigma$ that optimizes the second-order utility:\n$U_2(S_{\\text{DPSGD},\\Sigma, P}, \\Theta) = - \\frac{\\eta}{2} \\sum_{i=1}^d E_{x \\sim D} (\\frac{\\partial^2 L(x, \\Theta)}{\\partial x_i^2})^2 \\Sigma_{i,i}$.\nTheorem 3 (Optimal DP-SGD). Under assumptions 1 to 5, and assuming $E_{x \\sim D} g_i(x)^2 > 0$ for all $i$, the optimal noise matrix $\\Sigma$ that maximizes the reconstruction error lower bound (Eq. 1) for a utility budget of\n$U_2(S_{\\text{DPSGD},\\Sigma, P}, \\Theta) \\geq -C$"}, {"title": "Optimal Gradient Pruning", "content": "Gradient Pruning. Gradient pruning reduces the number of parameters in the shared gradient by zeroing out less significant gradients during training. Inspired by gradient compression (Lin et al., 2018; Tsuzuku et al., 2018), this approach prunes gradients with the smallest magnitude (Zhu et al., 2019). It is also the most effective defense against DLG (Zhu et al., 2019).\nFor a given set of parameters $A$, the gradient pruning defense method $S_{\\text{prune}, A} : \\mathbb{R}^d \\to P(\\mathbb{R}^d)$ is as follows:\n$S_{\\text{prune}, A}(x)_i = \\begin{cases} 0, & \\text{if } i \\in A, \\\\ x_i, & \\text{if } i \\notin A. \\end{cases}$\nOptimal Gradient Pruning Under assumptions 1 to 5, the first-order utility of gradient pruning equals the sum of the squared unpruned gradients:\n$U_1(S, \\Theta) = \\sum_{i \\notin A} (\\frac{\\partial L(x, \\Theta)}{\\partial x_i})^2$.\nSince gradient pruning introduces no randomness, an accurate reconstruction is theoretically possible when the number of unpruned parameters exceeds the input dimension. To address this problem, we add a small noise to the unpruned gradients and analyze the noisy version of gradient pruning:\n$S_{\\text{prune}, A, \\Sigma}(x)_i = \\begin{cases} 0, & \\text{if } i \\in A, \\\\ N(x_i, \\Sigma_{i,i}), & \\text{if } i \\notin A. \\end{cases}$\nFor $\\Sigma = \\epsilon^2 I_d$, this collapses to the original pruning method when $\\Sigma_{0}$ remains constant and $\\epsilon \\to 0$."}, {"title": "Algorithm design", "content": "Because of the high computational cost of the expectation terms in the globally optimal defense methods (Theorems 2 to 4), our implementations are based on the locally optimal versions (Eqs. 8, 13 and 17).\nWhen computing optimal defense parameters, calculating the Jacobian matrix of model gradients on input data is especially challenging. The full Jacobian matrix for an image with a resolution of 32 \u00d7 32 would require roughly 3000 times the memory of the model itself, which is prohibitively large. We resolve this problem by using the forward differentiation method to save computational cost and use approximation to save memory cost.\nThe forward method (Griewank and Walther, 2008) tracks gradients based on the input tensor size rather than the output tensor size, and therefore more efficient since we are dealing with low input and high output dimensions. We approximate the $l_2$-norm of the gradients using Lemma 1:\nLemma 1. Given a differentiable function $f : \\mathbb{R}^d \\to \\mathbb{R}$ and a constant $\\epsilon > 0$. For any number of samples $k \\in \\mathbb{Z}$ and random vectors $X_1, ..., X_k$ sampled from $N(0, I_n)$, we have that\n$\\|\\nabla f(x)\\|^2 - \\frac{2}{k} \\sum_{j=1}^k (\\frac{\\partial f(x + \\alpha x_j)}{\\partial \\alpha})\\_{\\alpha = 0} < \\epsilon \\|\\nabla f(x)\\|^2$\nwith probability at least $1 - k e^2$ for any $x \\in \\mathbb{R}^d$.\nThis allows us to approximate the $l_2$-norms without the entire Jacobian matrix, significantly reducing computational and memory cost. Our resulting algorithm is outlined in Algorithm 1."}, {"title": "Experiments", "content": "We compare our proposed algorithms with existing defense methods on two datasets: MNIST (LeCun et al., 1998) and CIFAR-10 (Krizhevsky and Hinton, 2009). As our algorithm employs different defenses on different parameters, we use an attack that treats parameters equally. One attack with such property is the Inverting Gradients attack (Geiping et al., 2020), a powerful attack that does not require extra information or specific model architecture."}, {"title": "MNIST", "content": "The MNIST dataset consists of 28 \u00d7 28 grayscale images of handwritten digits, serving as a simple test for our algorithm."}, {"title": "Gradient Pruning", "content": "We apply different pruning thresholds to a randomly initialized Convolutional Neural Network (Fukushima, 1969), using 4 batches of 16 images to compute gradients. These gra-"}, {"title": "DP-SGD", "content": "We also evaluated our defense on DP-SGD. As shown in Figure 5, our optimal noise achieves comparable performance to DP-SGD in terms of defense at the same noise scale, while our algorithm has faster learning speed (Figure 6). The scatter plot in Figure 7 further demonstrates the improved privacy-utility trade-off of our approach. Visualization of the reconstruction in Figure 8 shows better protection against attacks using our optimal noise for the same level of training utility."}, {"title": "CIFAR-10", "content": "We extend our experiments to the CIFAR-10 dataset, consisting of colored images with size 32 \u00d7 32. We used a larger model with 2.9M parameters."}, {"title": "Optimal Pruning", "content": "As shown in Figure 9, our optimal pruning achieved higher reconstruction error than gradient pruning for the same level of training utility, with a pruning ratio of 70% outperforming 90% pruning in gradient pruning. Visual comparison (Figure 10) also indicates better protection using our method."}, {"title": "Optimal Noise", "content": "Reconstructing CIFAR-10 images is more challenging, with failures occurring at noise scales larger than $10^{-3}$. However, our noise method still offers a better privacy-utility trade-off, as shown in Figure 11. The advantage in training utility is more significant when the noise scale is larger. (Details in Appendix D.)"}, {"title": "Discussion", "content": "In this work, we derived a theoretical reconstruction lower bound and used it to formulate optimal defense methods as improvements of gradient noise and gradient pruning. Our experimental results on MNIST and CIFAR-10 demonstrate the effectiveness of our approach.\nAs we are only presenting the possibility of a better privacy-utility tradeoff, a key limitation of our methods is the high computational cost of our algorithm. This could be mitigated through simplifications (e.g. layer-wise defense) or lowering the frequency of updating defense parameters. Additionally, the reconstruction bound used in our analysis is not tight. The utilization of more precise bounds or indices that integrate current attack methods remains an open challenge for further research.\nFurthermore, our analysis could potentially be applied to other defense methods or designed against other types of attacks. This also remains an open challenge."}, {"title": "MISSING PROOFS", "content": ""}, {"title": "Proof of Theorem 1", "content": "Proof. Recall the definition\n$B_{D,S} = \\min_{R: \\mathbb{R}^d \\to \\mathbb{R}^m} E_{x \\sim D}E_{y \\sim S(g(x))} ||R(y) - x||^2$.\nBy the Bayesian Cram\u00e9r-Rao lower bound, for any reconstruction algorithm R we have that:\n$E_{x \\sim D}E_{y \\sim S(g(x))} [(R(y) - x) (R(y) - x)^T] \\geq J_B^{-1}$,\nwhere $J_B = J_P + J_D$ and $J_D := E_{x \\sim D} [J_F(x)]$.\nTherefore:\n$E_{x \\sim D}E_{y \\sim S(g(x))} ||R(y) - x||^2 = tr (E_{x \\sim D}E_{y \\sim S(g(x))} [(R(y) - x) (R(y) - x)^T]) \\geq tr (J_B^{-1})$.\nSince both $J_D$ and $J_P$ are Fisher information matrices and hence symmetric, we could apply Weyl's inequality to bound the eigenvalues of $J_B$. Let $\\lambda_i$ denote sorted eigenvalues with $\\lambda_1$ being the smallest and $\\lambda_d$ the largest. For the eigenvalues $i$ of $J_B$, we have:\n$\\lambda_i(J_B) \\leq \\lambda_i(J_D) + \\Lambda_1(J_P)$.\nThis implies that\n$tr(J_B^{-1}) = \\sum_{i=1}^d \\frac{1}{\\lambda_i(J_B)} \\geq \\sum_{i=1}^d \\frac{1}{\\lambda_i(J_D) + \\Lambda_1(J_P)} \\geq \\frac{d^2}{tr(J_D) + d \\cdot \\Lambda_1(J_P)}$.\nThe last equation is from Cauchy's inequality since $\\lambda_i(J_D) + \\Lambda_1(J_P) > 0$.\nSubstituting $tr(J_D) = E_{x \\sim D} [tr(J_F(x))]$, we obtain:\n$tr(J_B^{-1}) \\geq \\frac{d^2}{E_{x \\sim D} [tr(J_F(x))] + d \\cdot \\Lambda_1(J_P)}$.\nThus, we have shown that\n$B_{D,S} \\geq \\frac{d^2}{E_{x \\sim D} [tr(J_F(x))] + d \\cdot \\Lambda_1(J_P)}$"}, {"title": "Proof of Theorem 2", "content": "To prove the theorem, we first need to calculate $J_F(x)$:\nLemma. Let $J_F(x)$ be the Fisher information matrix defined in Theorem 1. Let $y = S(x)$ be gradients defended with gradient noise using covariance matrix $\\Sigma$. We have that:\n$J_F(x) = \\nabla_x g(x) \\Sigma^{-1} \\nabla_x g(x)^T$."}, {"title": "Proof of Theorem 3", "content": "We only need to replace $g(x)$ by $g_P(x)$ in Theorem 2. This is because the two defenses have the same utility measure when the noise covariance matrices are the same. Moreover, the lower bound only differs by changing $\\nabla_x g(x)$ to $\\nabla_x g_P(x)$. Therefore Theorem 3 directly follows by replacing $g(x)$ with $g_P(x)$ in the optimal defense in Theorem 2."}, {"title": "Proof of Theorem 4", "content": "For generalized gradient pruning, we need to use mixed defense. Similar as Theorem 2, we first calculate $tr(J_{F, S_0(\\cdot, r)}(x))$ in the mixed defense version of Theorem 1. For the noisy gradient pruning defense, the identifier $r$ is $A$, $S_0(\\cdot, A)$ is noisy gradient pruning that prunes parameters in the set $A$. $\\mathfrak{R}$ is the distribution generating the pruning set. We find the optimal distribution $\\mathfrak{R}$.\nLemma. Let $tr(J_{F, S_{\\text{prune},\\Sigma, A}}(x))$ be the Fisher information matrix for mixed defense. Let $y = S(x)$ be the model gradients defended by noisy gradient pruning with covariance matrix $\\Sigma = \\epsilon I_d$ and pruning set $A \\sim \\mathfrak{R}$. The Fisher information matrix $tr(J_{F, S_0(\\cdot, r)}(x))$ defined in Theorem 1 equals:\n$tr(J_{F, S_0(\\cdot, r)}(x)) = \\sum_{i \\notin A} \\|\\nabla_x g_i(x)\\|^2$.\nProof. By the lemma used in the proof of Theorem 2, the left-hand side equals\n$tr(J_{F, S_0(\\cdot, r)}(x)) = tr(\\nabla_x \\mathbb{1}\\_{\\{1:d\\}-A}g(x) \\epsilon I_d^{-1} \\nabla_x \\mathbb{1}\\_{\\{1:d\\}-A}g(x)) = \\frac{1}{\\epsilon} \\|\\nabla_x \\mathbb{1}\\_{\\{1:d\\}-A}g(x)\\|^2 = \\frac{1}{\\epsilon} \\sum_{i \\notin A} \\|\\nabla_x g_i(x)\\|^2$.\nThe last equation is true since for each $i$, $\\nabla_x g_i(x)$ corresponds to a column in $\\nabla_x \\mathbb{1}\\_{\\{1:d\\} - A}g(x)$.\nNow we can prove Theorem 4:\nProof. For the training utility, we have:\n$U_1(S, \\Theta) = E_{A \\sim \\mathfrak{R}} E_{x \\sim D} \\sum_{i \\notin A} (\\frac{\\partial L(\\Theta, x)}{\\partial x_i})^2 = E_{A \\sim \\mathfrak{R}} \\sum_{i \\notin A} E_{x \\sim D} (g_i(x))^2 = \\sum_{i=1}^d P_{\\mathfrak{A} \\sim \\mathfrak{R}}(i \\notin A) E_{x \\sim D} (g_i(x))^2$,\nwhere $P_{\\mathfrak{A} \\sim \\mathfrak{R}}(i \\notin A)$ is the probability of $i \\notin A$ when $A$ is sampled from $\\mathfrak{R}$. With the given utility constraint $U_1(S, \\Theta) \\geq C$ we want to minimize $E_{\\mathfrak{A} \\sim \\mathfrak{R}} \\sum_{i \\notin A} E_{x \\sim D} \\|\\nabla_x g_i(x)\\|^2$. By the previous lemma, we have:\n$E_{\\mathfrak{A} \\sim \\mathfrak{R}, x \\sim D} tr(J_{F, \\mathfrak{A}}(x)) = E_{\\mathfrak{A} \\sim \\mathfrak{R}} \\sum_{i \\notin A} E_{x \\sim D} \\|\\nabla_x g_i(x)\\|^2 = \\sum_{i=1}^d P_{\\mathfrak{A} \\sim \\mathfrak{R}}(i \\notin A) E_{x \\sim D} \\|\\nabla_x g_i(x)\\|^2$."}, {"title": "Proof of Lemma 1", "content": "Proof. Notice that for any given $y \\in \\mathbb{R}^d$,\n$\\frac{\\partial g(x + \\alpha y)}{\\partial \\alpha} = \\nabla_x g(x + \\alpha y) \\odot y$,\nwhere $\\odot$ represents element-wise multiplication. Therefore\n$\\frac{\\partial g(x + \\alpha y)}{\\partial \\alpha}|_{\\alpha = 0} = \\nabla_x g(x) \\odot y$.\nWhen $y \\sim N(0, I_d)$, $\\|\\nabla_x g(x) y\\|$ follows a normal distribution with mean 0 and variance $\\|\\nabla_x g(x)\\|^2$.\nTherefore we have that\n$\\frac{\\frac{\\partial g(x + \\alpha y)}{\\partial \\alpha}|_{\\alpha = 0}}{\\|\\nabla_x g(x)\\|} \\sim N(0, 1)$,\nfurthermore,\n$A := \\sum_{i=1}^k \\frac{\\frac{\\partial g(x + \\alpha x_i)}{\\partial \\alpha}|_{\\alpha = 0}}{\\|\\nabla_x g(x)\\|} \\sim \\chi^2(k)$.\nSince $E(A) = k$ and Var$(A) = 2k$, by Markov's inequality we have that\n$P(A - k > k \\epsilon) \\leq \\frac{2k}{k^2 \\epsilon^2} = \\frac{2}{k \\epsilon^2}$."}, {"title": "OPTIMAL DEFENSE WITH ReLU ACTIVATION", "content": "In this section, we briefly discuss how we modify our optimal defenses when $E_{x \\sim D} g_i(x)^2 = 0$. The defenses for the entries where the gradients are not 0 are the same, but we deal with entries with gradient 0 separately.\nFor Gradient Pruning, pruning gradients that are 0 do not affect the defended gradients. Therefore, we focus on analyzing how we apply Optimal Gradient Noise. By the definition of second-order utility, the second-order utility is unaffected by the noise scale added on parameters with gradient 0. If we follow the proof of Theorem 2, the reconstruction lower bound scales negatively with\n$\\sum_{i=1}^d \\frac{E_{x \\sim D} \\|\\nabla_x g_i(x)\\|^2}{\\Sigma_{i,i}}$.\nSince $E_{x \\sim D} g_i(x)^2 = 0$, we have that $g_i(x)$ is constant on the support of $D$ so $E_{x \\sim D} \\|\\nabla_x g_i(x)\\|^2 = 0$. Therefore, the noise scale does not affect the reconstruction lower bound and any noise scale for these parameters are optimal. This typically happens when the model has activation functions that are constant on an interval (e.g. ReLU). Nonetheless, since a larger noise scale typically decreases training utility, the optimal method would be not to add noise to the gradients.\nHowever, the above case does not completely cover problems in the locally optimal defenses. In the locally optimal versions, we used the values at $x$ as approximations of expectations. Therefore we might have $\\|\\nabla_x g_i(x)\\|^2 > 0$. In this case, theoretically we should set the noise to be as large as possible, which deviates from reality. To resolve this problem, a good solution would be to set an upper limit to the noise scale and clip the noise scales added to the gradients.\nTo summarize, for optimal gradient pruning, pruning gradients with a scale of 0 is trivial. For optimal gradient noise (and its application in DP-SGD), we set an upper limit to the noise scale."}, {"title": "EXPERIMENT DETAILS", "content": "For all datasets and algorithms, we used the implementation in Algorithm 1 with $k = 10$ as the number of samples and $c = 10^{-6}$ as the small constant. When comparing our optimal noise with DP-SGD, we applied clipping threshold 1 for DP-SGD and included the same clipping step before adding our optimal noise."}, {"title": "Experiments with MNIST", "content": "For experiments on the MNIST dataset, we used a Convolutional Neural Network with 120k parameters. To avoid the special case where the model utilizes the ReLU activation function, we use the LeakyReLU activation function instead. We trained the model on a subset of size 4096 from the whole dataset and used SGD algorithms with batch size 64. We simulated 4 clients each possessing one-fourth of the dataset (1024 samples). When training, each mini-batch contains data from all 4 clients, with each client providing 16 samples to form a 64-sample mini-batch. The gradients are computed and defended separately and then averaged to be provided to the central server. For the training process with DP-SGD (or our optimal noise) applied, we used the Adam optimizer with learning rate $10^{-3}$. For the training process with gradient pruning (or our optimal pruning) applied, we used Adam with learning rate $5 \\times 10^{-4}$. For the reconstruction process, we used the Inverting Gradients algorithm with a budget of 2000 updates."}]}