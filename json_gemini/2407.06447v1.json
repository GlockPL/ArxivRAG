{"title": "Geospatial Trajectory Generation via Efficient Abduction: Deployment for Independent Testing", "authors": ["Divyagna Bavikadi", "Dyuman Aditya", "Devendra Parkar", "Paulo Shakarian", "Graham Mueller", "Chad Parvis", "Gerardo I. Simari"], "abstract": "The ability to generate artificial human movement patterns while meeting location and time constraints is an important problem in the security community, particularly as it enables the study of the analog problem of detecting such patterns while maintaining privacy. We frame this problem as an instance of abduction guided by a novel parsimony function represented as an aggregate truth value over an annotated logic program. This approach has the added benefit of affording explainability to an analyst user. By showing that any subset of such a program can provide a lower bound on this parsimony requirement, we are able to abduce movement trajectories efficiently through an informed (i.e., A*) search. We describe how our implementation was enhanced with the application of multiple techniques in order to be scaled and integrated with a cloud-based software stack that included bottom-up rule learning, geolocated knowledge graph retrieval/management, and interfaces with government systems for independently conducted government-run tests for which we provide results. We also report on our own experiments showing that we not only provide exact results but also scale to very large scenarios and provide realistic agent trajectories that can go undetected by machine learning anomaly detectors.", "sections": [{"title": "Introduction", "content": "The ability to generate artificial human movement patterns while meeting location and time constraints is an important problem in the security community, particularly as it enables the study of the analog problem of detecting such patterns without the need for data from actual humans. This work is part of a larger effort to establish models of normal human movement at a fine-grain level\u00b9 and operationalize those models and techniques in a system deployed to a government environment for evaluation. This contrasts with current techniques used to model populations that operate at a more coarse-grain level (country, county-level than building-level) as seen in previous work in specific areas such as population migration [4] or disease spread [7].\nIn this work, we focus on the generation of human movement patterns based on historical data. We frame this problem as an instance of abduction [23] in a geographic setting [29] guided by a novel parsimony function represented as an aggregate truth value over an annotated logic program [16]. This approach has the added benefit of affording explainability to an analyst user. By showing that any subset"}, {"title": "Motivating Application", "content": "In the aftermath of an unexpected event, such as a natural disaster, war, or large-scale industrial accident, human movement patterns can change significantly. As a result, IARPA (Intelligence Advanced Research Projects Activity) has identified problems relating to the characterization and generation of normal human movement patterns as a key problem of study in the HAYSTAC program\u00b2. In this problem, a given"}, {"title": "Technical Preliminaries", "content": "Syntax of Annotated Logic with Temporal Extensions. We use annotated logic syntax [16] with a lower-lattice based semantics [27] and temporal syntactic and semantic extensions [3]. We assume a set of constant, variable, and predicate symbols (resp., $\\mathcal{C}$, $\\mathcal{V}$, $\\mathcal{P}$) where the set of constants is divided into domains (i.e., $\\mathcal{D}_{i} \\subset \\mathcal{C}$). Atoms are formed with terms (constants or variables) and predicates. Literals include atoms and their negation, with $\\mathcal{I}$ being the set of all (ground) literals formed with no variables. Atoms and literals can be annotated with elements (intervals in [0,1]) of a lower semi-lattice structure $\\mathcal{L}$ (not necessarily complete) with ordering $\\sqsubseteq$. Here, to generalize fuzzy logic in the spirit of [22], we define the bottom element $\\perp$ as [0,1] and a set of top elements {$\\left[x, y\\right]|x=y$}. If a is an atom and $\\mu$ is an annotation that denotes truth probability, then $a:\\mu$ is an annotated atom. We can create annotated negations (of atoms) in the same way, and together with annotated atoms this gives us the set of annotated literals. We also find it useful to extend the logic to make statements about time, and we do this in two different ways. First, following the convention of [28] we have temporally annotated facts (TAFs). Given time point t and annotated literal f, $f_t$ is a TAF that is true at t. The second extension is similar to that of"}, {"title": "Example", "content": "We define specific domains as follows: $\\mathcal{C}$ consists of disjoint sets $\\mathcal{D}_{agent}, \\mathcal{D}_{loc}, \\mathcal{D}_{movtype}$. $\\mathcal{D}_{agent}$ are constants associated with agents in the environment whose behavior we wish to model as mentioned in Section 2. We will have various predicates such as the binary predicate conn that takes elements of $\\mathcal{P}_{loc}$ as arguments representing the connection of two locations. The truth values associated with ground atoms created with this predicate specify a road network (as depicted in Figure 1). We will also have a binary predicate, at specifying that an agent is at a certain location, e.g., for agent agent and location $loc \\in \\mathcal{D}_{loc}$, $at(agent,loc)$ is a ground atom. We also have various unary predicates such as prim-Banks, occ-Education, occ-Residential,... (where prim encodes the primary use of the location like the building is primarily used as a Bank, and occ implies the oc- cupancy type of the building like for Residential purpose) that take elements of $\\mathcal{D}_{loc}$ as an argument. Finally, predicates of the form anomalyType(agent) are concluded from rules and are considered true with confidence indicated by its annotation if the agent is conducting abnormal behavior."}, {"title": "Semantics and Satisfaction", "content": "Following previously introduced temporal extensions to annotated logic (i.e., [3, 26]), we assume a finite series of time points that we wish to reason about in an associated semantic structure of an interpretation I that, given timepoints $\\mathcal{T} = t_1,...,t_{max}$, is any mapping $\\mathcal{G} \\times \\mathcal{T} \\rightarrow \\mathcal{L}$. The set $\\mathcal{I}$ of all interpretations can be partially ordered via the ordering: $I_{1} \\prec I_{2}$ iff for all ground literals $g \\in I$ and time t, $I_{1}(g,t) \\subseteq I_{2}(g,t)$. $\\mathcal{I}$ forms a complete lattice under the $\\prec$ ordering. From this, we define a satisfaction relationship \"$\\models$ \" for temporally annotated facts in the usual manner (i.e., [16, 27, 3])."}, {"title": "Rules and Programs", "content": "We adopt the temporally extended GAP rules from [3]. If $f_0$ is an annotated atom and, $f_{1},..., f_{m}$ are annotated formulas, then\n$r=f_{0} \\leftarrow f_{1} \\land ...\\land f_{m} \\qquad \\qquad At \\ge 0$\nis called a GAP rule. When a conjunction of annotated formulas in the body is satisfied at time t, the annotation of the atom $f_0$ in the head gets updated after a delay, At. A GAP rule is called a fact when the body is empty, and ground when it has no occurrences of variables from $\\mathcal{V}$.\nWe define a program, \u03a0, as a set of rules and TAFs. An interpretation I satisfies \u03a0 if for all $e \\in \\Pi, I \\models e$. We also leverage a fixpoint operator presented in [27, 3] designed for use on lower-lattice annotated logic (which provides analogous results shown earlier for the operator introduced in [16]). The key here is that the operator provides a minimal model, hence exact answers to entailment under the assumption of consistency. As we learn logical rules from data, we can control consistency, so this is a reasonable assumption. We also use a fixpoint operator as per [16, 27, 3] to perform deductive inference. As per the previous work, fixpoint operator $\\Gamma$ is a map from interpretations to interpretations and is applied multiple times until convergence, denoted by $\\Gamma^*.$"}, {"title": "Abductive Inference", "content": "In this paper, we formalize an abduction problem as: given observations, represented as a set of TAFs denoted $\\mathcal{O}$, a set of hypotheses, which is also a set of TAFs denoted $\\mathcal{H}$, program $\\Pi$, and parsimony function value that maps interpretations to positive reals, the goal is to identify an"}, {"title": "Example", "content": "Building on the notion from example 3.1: in our use case, a set of observations $\\mathcal{O}$ is a set of TAF s (essentially sets constraints on the agent's location at certain times), where each TAF indicates that an agent a is at $loc \\in \\mathcal{D}_{loc}$ at time t. $\\mathcal{H}$ is the set of all possible locations $\\mathcal{D}_{loc}$ of the agent. A set of learned temporally extended GAP rules \u03a0 indicate the normalcy of an agent in a graph G. Here, G is a series of TAF s that is formed with nodes from $\\mathcal{D}_{loc}$. If $at(a,loc):\\mu_{1}$ is a ground atom, then we impose graphical constraints like $\\neg at(a,loc'):\\mu_{t+1}$, where $conn(loc,loc'):\\mu_{1}$ is not true in G and $loc \\ne loc'$. As seen in use the lower bound of the annotation in the rule's head to represent the confidence of the body being historically abnormal. When the agent's movements relate to the body, its annotations are updated to [1,1] for time t. Explanation E is a movement sequence (which is a set of TAFs) from $loc_{start}$ to $loc'_{end}$ that fires most rules in \u03a0."}, {"title": "Framing the Abduction Problem as a Search Problem", "content": "While the general case of such an abduction problem is intractable, we have two key insights that apply to our domain problem. The first is a structural concern: if we are reasoning about an agent moving in a geospatial setting using a ground vehicle, we know that the agent cannot possibly teleport, so it is restricted to traveling along the graph at a certain speed. Similar restrictions have been applied to other problems such as social media diffusion [3], power grid failure modeling [18], and knowledge graph completion [21]. This allows for TAFs from set $\\mathcal{H}$ to be selected in a sequential manner while at the same time limiting the TAFs to those consistent with the graphical structure, and the application of search routines such as depth-first search (which we implement in our experiments). However, we note that this does not reduce the branching factor enough to afford tractability. In the next section, we present results that allow for a provable lower bound on value in the general case, which we employ in our use case to obtain tractability and further scalability."}, {"title": "Efficient and Scalable Geospatial Abduction for Trajectory Generation", "content": "The complexity of logic-based abduction [9, 17] can be reduced by using a logic-based parsimony function instead of a standard logic-based function that is intractable due to the number of explanations. We introduce a parsimony function based on the aggregate truth values assigned using a logic program learned from data. The value obtains the lower bound of aggregate over the annotations on an atom b at time t for the minimal model $\\mathcal{I}$ of $\\mathcal{H}\\cup\\mathcal{E}\\cup\\mathcal{O}$. Using a lower bound on such an aggregate, we obtain an admissible heuristic allowing us to use informed search (i.e., A*). By extension, this addresses both intractability and scalability issues."}, {"title": "Bounding the Parsimony Function to enable Informed Search", "content": "We now provide new results that allow us to create a lower bound on the parsimony function value by taking a subset of the logic program. These rigorous results imply that we obtain an admissible and consistent heuristic function for A* \u2013 hence, the resulting usage of the lower bound of value in A* can provide an exact solution. Note that these are general results, not specific to the use case we are studying. However, clearly, their applicability depends on the subset of \u03a0 being non-trivial (e.g., the use of \u00d8 would be unhelpful). Further, the idea is that the subset of the logic program also offers a computational advantage. By showing for a given ground atom b, time t, and $\\Pi' \\subseteq \\Pi$ and I, $\\Gamma^*_{\\Pi'\\cup\\mathcal{E}\\cup\\mathcal{O}}(\\mathcal{I})(b,t) \\subseteq \\Gamma^*_{\\Pi}(\\mathcal{I})(b,t)$, we state the following, which in turn gives us a lower bound on value:\nTheorem 4.1 For ground atom b, timepoint t, $\\Pi' \\subseteq \\Pi$, and $\\mathcal{I}' \\sqsubseteq \\mathcal{I}$, we have $\\Gamma^*_{\\Pi'\\cup\\mathcal{E}\\cup\\mathcal{O}}(\\mathcal{I}')(b,t) \\sqsubseteq \\Gamma^*_{\\Pi}(\\mathcal{I})(b,t)$."}, {"title": "Informed Search Strategy", "content": "For our use case, a logic program \u03a0 is learned where the head of the rules is anomalyType(agent) from Example 3.1. The body of the rule is determined by two major symbolic landmarks in G that are $n \\in \\mathbb{Z}^+$ hops away. Consider a single movement as moving and staying in a location (in our case, that is one hop away) from the current location during time t. For single hops, a subset of the logic program $\\Pi_{SH} \\subseteq \\Pi$ is learned. In the general case, Theorem 4.1 shows that for a subset of the logic program $\\Pi' \\subseteq \\Pi$, we get a lower bound on value. For a given set of movements I in G, we employ the value for a $\\Pi_{SH}$ as the heuristic function in an informed search strategy. We note that the increase in value resulting from single hop rules is inherently modular, meaning that for any node in the frontier set, such quantity is invariant. This allows us to precompute this increase for all nodes and store it in a graph-based data structure $G_w$. Considering the number of iterations of $\\Gamma$ as well as grounding for single movements versus a sequence of multiple movements, FISH (I) is easier to compute, towards obtaining the heuristic value."}, {"title": "Scalable Heuristic Computation", "content": "Computation of value can be expensive on a whole logic program \u03a0 as it involves computing $\\Gamma^*_{\\Pi}(\\mathcal{I})$ given a set of movements I as multiple anomalies can be inferred from multiple sequences of movements. The process of grounding can be expensive computationally, but we can gain efficiency by considering $\\Pi_{SH} \\subseteq \\Pi$, where anomalous behavior rises from single movements. Using Theorem 4.1 we can efficiently prune abnormal candidate movements with informed search. We precompute the heuristic function by weighting the graph G with the lower bound on value for all possi- ble single movements and obtain $G_w$. For further scalability, we compute value in a need-based fashion called ad-hoc weighting instead of precomputing it for all possible single movements in G. During heuristic computation we obtain the lower bound of value only when a certain movement is needed. For ad-hoc weighting, we compute value considering the agent's frontier up to a depth of 1 at each step."}, {"title": "Software Stack", "content": "Our logic program-guided abduction strategy is part of a software stack that was deployed in a cloud environment on Amazon Web Services (AWS) as per mandate by the government customer. Figure 2 depicts the overall system architecture (DAG structure).\nOverall Workflow. The pipeline interfaces with the government system to access the raw geospatial data with related knowledge for 4 geolocations as well as training agent trajectories (as seen in Figure 1 (left)) and required objectives for each agent (framed as a set of observations O). The required objectives include typical human activities of single (visiting a friend, restaurant, etc) and recurring (going to work, etc.) movement types. Initially, we host the raw geospatial data in a Neo4j server and form a consolidated knowledge graph G (symbolic landmarks extracted from G are seen by the symbols in Figure 1 (left)). This is stored in an S3 bucket also containing the training data (consisting of both trajectories and objec- tives for each agent). For each agent, we extract spatial and temporal constraints from the training data. Based on a single trajectory of an agent, we learn a set of anomalous rules \u03a0rules (as seen in Table 1). Both G and \u03a0rules are used to compute the heuristic values by creating a weighted graph Gw. We also use constraints to perform an informed search algorithm to generate a normal trajectory (as seen in Figure 1 (right)).\nData Ingest. Our initial ingest and staging containerized processes are held in the DAG as nodes. Our ingest mechanism first parses the objective files for the agent to determine the locations of corresponding training trajectories. Secondly, based on the geolocation, we retrieve the appropriate knowledge graph and link it to each agent. Finally, we process the data associated with each agent (objectives, training data, graph) to a predefined staging area in the S3 bucket.\nInstantiation. This step analyzes the staging folders and creates the necessary string commands specific to each agent. We launch pods for all agents with a movement-generation Docker image to process its particular objectives. As the container runs, generated movement instruction files are pushed to the appropriate output directory."}, {"title": "Rule Learning", "content": "To generate normal movement, we learn rules from the agent's historical data capturing realistic behavior. Sequences of movements deviating from this behavior are considered anomalous. Inferring from longer sequences can make the algorithm computationally expensive, but when we only compute a subset of rules involving shorter sequences, we can efficiently prune candidate movements that are highly abnormal using search algorithms. There are different kinds of rule types, which we call single-hop and multi-hop rules ($\\Pi_{sh},\\Pi_{MH}$). Here, $\\Pi_{sh}$ leverages single movement frequency (for instance Table 1) while $\\Pi_{MH}$ leverages a set of multiple movements (in a similar format of Table 1). The logic program \u03a0 includes both types of rules. From Theorem 4.1, we can get value $(\\Gamma^*_{\\Pi_{SH}\\cup\\mathcal{E}\\cup\\mathcal{O}}(\\mathcal{I})(b,t)) \\sqsubseteq value (\\Gamma^*_{\\Pi\\cup\\mathcal{E}\\cup\\mathcal{O}}(\\mathcal{I})(b,t))$ as $\\Pi_{SH} \\subseteq \\Pi$; this is demonstrated in Figure 3."}, {"title": "Search", "content": "We use PyReason [3] to compute the fixpoint operator \u0393 used for both the actual calculation of value and the creation of the heuristic function. We use the aggregate function as an intersection over all annotations of the rules fired by $\\Gamma^*$. This is computed in an ad-hoc fashion for possible movements, and we weight the graph based on those movements to form $G_w$. Given the set of constraints O, we form sub-abduction problems to perform an A* search, and generate normal trajectories satisfying all required objectives."}, {"title": "Internal Evaluation", "content": "Experimental Setup. The government provided us with simulated data we used for the internal devel- opment of our approach. We leverage three main kinds of data. Firstly, curated data for four geospatial locations: Knoxville, Singapore, Los Angeles, and San Francisco is collected from multiple source datasets, namely USAStructures [2], Planetsense [30] (which provides geospatial information), Open street map road network (which gives transportation information), Urbanpop [31] (containing population data), and other data collected as part of the program. Secondly, we have simulated trajectory data of 40,000 human agents across all 4 locations, which mimic realistic human activity. Four different teams each provide a different simulation environment for generating realistic training data. Note that this data comprises only location data and does not include information on actual people. Thirdly, we have a set of objectives for each agent that specifies spatio-temporal constraints. From the curated data, we use three types of input data: geospatial (building types, occupancy, etc), population (census data, population survey data, etc), and transportation (road network, statistics on traffic flow, etc) to build a consolidated knowledge graph G. Each node in G is either an intersection point from the road network or has attributes that convey its landmark category resembling building occupancy such as Commercial (stores, parking), Unclassified (does not require much security, non-residential), Non-Profit (general offices, Emergency Operation Centers), Residential (apartments, hotels), Assembly (convention centers, stadium), Education (libraries, schools), Utility (barns, water treatment), Industrial (hazardous factories- chemical factories, metal processing factories, construction), Agriculture (agricultural use land), Government (military, fire station). Each trajectory is a sequentially ordered tuple of length 2 weeks, consisting of latitude, longi- tude, and timestamp indicating the agent's location at each time point (cf. Figure 1 for an example of the trajectory on the graph). Moreover, every set of objectives describes spatial and temporal constraints on the trajectory to be generated. We generated 38 trajectories that satisfy all constraints for which the objectives were provided.\nWe conducted all experiments on a high-memory CPU machine with 128 cores, and 2000GB mem- ory, using PyReason software [3] for inference. Rules (similar to Table 1) were learned using a bottom-up technique comparable to related work [25] where you restrict the body of the rule to contain historically possible single movements."}, {"title": "Empirical Validation of Theoretical Claims", "content": "We present the results of two experiments as seen in Figure 3 to validate our claims concerning the use of the heuristic function and its employment as part of an informed search strategy. Figure 3 (left) shows that the heuristic value computed by the subset of the logic program is lower than the actual value in all our experiments since the data points lie below the dashed line \u2013 this aligns with Theorem 4.1. On the right panel, we see that the heuristic effectively addresses the tractability issues of the search, where the search conducted with the depth-first search algorithm without the heuristic could not be completed in under 48 hours even on small graphs with 50 nodes. Our approach using a heuristic maintains a lower running time as shown in Figure 3 (right)."}, {"title": "Trajectory Robustness", "content": "A key aspect we wish to achieve with the generation of realistic movement trajectories is their robustness to anomaly detection. In this experiment, we run an ensemble of machine learning methods to perform anomaly detection over the generated and training trajectories to get their anomaly scores. The anomaly score of the ensemble is then compared with the average of that score over the historical trajectories of the agent, resulting in the relative anomaly ratio. In Figure 4 (left), we can see a box plot of anomaly scores of generated trajectories relative to the training data. In 90% of the data, a ratio lower than 1 is observed, indicating that mostly, anomalous movements identified in generated trajectories are no more frequent than those occurring in the training trajectories. For the cases where the anomaly detector found more anomalies than in the training data, the proportions are 46%,31%, 14%, and 11%, which is likely to be lower than a practically employed high-precision threshold."}, {"title": "Scalability", "content": "We implemented both ad-hoc and non ad-hoc weighting for all 4 locations \u2013 see results in Figure 4 (right). Ad-hoc technique gave a maximum speedup of 245.42 and a minimum of 0.32 when compared to the non ad-hoc technique. Over all, ad-hoc weighting is beneficial due to reduced runtimes. Several outliers (not depicted in the plot) for 3 AOIs were observed with speedups of 128.05, 245.04, and 140.46. Furthermore, when the graph size (the number of edges) increased by a factor of 4.69, the speedup was boosted by a factor of 72.05. The median speedups as the graph gets denser are 0.44, 1.30, 2.86, and 17.98. These improvements show that the ad-hoc technique will scale well as the graph size grows significantly or gets denser when compared to non ad-hoc technique. In many cases, the non ad-hoc technique did not complete after running for more than 10 days, suggesting a speedup of over 1,000x for those cases. Though such results are favorable to our approach, we did not consider these samples in the determination of the numbers in this section."}, {"title": "Explainability", "content": "Movemments in a trajectory are generated using a fixpoint-based algorithm [16] on \u03a0."}, {"title": "Deployment for Independent Evaluation", "content": "Independent Evaluation Setup. The generated trajectories are evaluated against nine different anomaly detection algorithms, each created by a government contractor. Further, these evaluations take place in the 4 locations examined in Section 6 but with newly simulated historical data for a duration of 2 months not previously available to our team. The results were collected and assessed by the government test and evaluation team using government-created cloud infrastructure to receive our outputs. The evaluation teams included an external T&E team, four simulation teams that run simulation engines to provide training data replicating real-world dynamics innate in human movement, and other teams that designed bespoke anomaly detection algorithms to evaluate the generated trajectories. Additionally, with the same"}, {"title": "Related Work", "content": "Relation to Abduction Literature. Traditionally, in an abductive inference problem, an explanation is determined based on a model, which imposes some form of constraints, and a set of observations that though observed are phenomena that would be inferred from the model and explanation. The hardness of logic-based abduction (e.g., when the model is some form of logic program) is well established [9, 17], and a key source of this complexity is that the number of explanations is intractable in the general case. As a result, parsimony requirements are used to specify criteria over which explanation is preferable [23] \u2013 for example, previous work on geospatial abduction (but not abducing trajectories) minimized cardi- nality as a parsimony requirement [29]. In another example, recent applications of abduction to machine learning, such parsimony requirements are expressed in terms of training loss [12, 8]. Our work dif- fers from all of the aforementioned papers in that we introduce a parsimony requirement based on the aggregate truth values assigned by a logic program that can be learned from data."}, {"title": "Relation to Literature on Geospatial Trajectory Generation", "content": "The abundance of trajectory data per- mits the analysis of mobility patterns and trajectories such as Markov-based paradigms have been applied for momentary goals (location prediction) [10, 19] and coarse-grain human movement analysis [32, 7, 4] \u2013 both different problems than this paper. Recent studies used recurrent neural networks (RNN) [15], generative adversarial networks (GAN) [11, 33], graph neural networks (GNN) [20], and transformer- based approaches [32] on real datasets to generate trajectories. Though these approaches tend to capture spatial-temporal correlations, they lack explainability. Recurrent-based approaches generate trajectories with short duration and spatial coverage. Adversarial approaches need more training data to get real- istic outcomes while GNN-based models are time-consuming. A reinforcement-learning approach for sequence modeling [14, 6] can be extended for human trajectory generation but has challenges for large graphs. They heavily rely on the structure of the reward function. Interpretability has been induced using the latent space to see the distribution of uncertainty of semantic concepts [13]. Our approach can gener- ate long-range trajectories spanning over a city with a length of two months, in a data-efficient manner 3 and scales to denser graphs. The modularity of our approach allows for detailed explanations."}, {"title": "Conclusion", "content": "In this paper, we described a system that generates realistic but synthetic human movement trajectories using abduction guided by a logic program. This system was recently deployed for independent gov- ernment testing. We note that in our current iteration, there were several limitations. For example, we did not employ ad-hoc graph weighting (essentially a sequential operation) and parallelization together finding how to achieve the right balance between the two techniques can result in further scalability. We also look to extend our logical language to provide additional insights into anomalies; for example, time of day is not currently considered, so an anomaly detector explicitly considering that aspect will readily find our trajectories. Further, we also look to leverage the ability of our underlying logic to accept arbi- trary functional symbols to assign truth, allowing to leverage neurosymbolic techniques [24] to directly integrate ML anomaly detectors."}, {"title": "Ethics Statement", "content": "This work is part of the IARPA HAYSTAC program, which is designed to create simulated environments with generated human movement patterns not associated with actual persons and enable further study of human movement trajectories without relying on actual human data."}]}