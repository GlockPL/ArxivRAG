{"title": "Benchmarking Pretrained Attention-based Models for Real-Time Recognition in Robot-Assisted Esophagectomy", "authors": ["Ronald L.P.D. de Jong", "Yasmina al Khalil", "Tim J.M. Jaspers", "Romy C. van Jaarsveld", "Gino M. Kuiper", "Yiping Li", "Richard van Hillegersberg", "Jelle P. Ruurda", "Marcel Breeuwer", "Fons van der Sommen"], "abstract": "Esophageal cancer is among the most common types of cancer worldwide. It is traditionally treated using open esophagectomy, but in recent years, robot-assisted minimally invasive esophagectomy (RAMIE) has emerged as a promising alternative. However, robot-assisted surgery can be challenging for novice surgeons, as they often suffer from a loss of spatial orientation. Computer-aided anatomy recognition holds promise for improving surgical navigation, but research in this area remains limited. In this study, we developed a comprehensive dataset for semantic segmentation in RAMIE, featuring the largest collection of vital anatomical structures and surgical instruments to date. Handling this diverse set of classes presents challenges, including class imbalance and the recognition of complex structures such as nerves. This study aims to understand the challenges and limitations of current state-of-the-art algorithms on this novel dataset and problem. Therefore, we benchmarked eight real-time deep learning models using two pretraining datasets. We assessed both traditional and attention-based networks, hypothesizing that attention-based networks better capture global patterns and address challenges such as occlusion caused by blood or other tissues. The benchmark includes our RAMIE dataset and the publicly available CholecSeg8k dataset, enabling a thorough assessment of surgical segmentation tasks. Our findings indicate that pretraining on ADE20k, a dataset for semantic segmentation, is more effective than pretraining on ImageNet. Furthermore, attention-based models outperform traditional convolutional neural networks, with SegNeXt and Mask2Former achieving higher Dice scores, and Mask2Former additionally excelling in average symmetric surface distance.", "sections": [{"title": "1. INTRODUCTION", "content": "Esophageal cancer is the eleventh most common cancer worldwide, and the seventh most common cause of death from cancer.\u00b9 Treatment of esophageal cancer generally consists of neoadjuvant chemoradiotherapy followed by esophagectomy.\u00b2 Esophagectomy is traditionally performed through open surgery. However, in recent years, robot-assisted minimally invasive esophagectomy (RAMIE) has emerged as an alternative approach. RAMIE minimizes surgical trauma by enabling procedures through small incisions, while the robotic system provides stable movements and tremor suppression. Compared to open surgery, RAMIE leads to fewer complications, shorter hospital stays, and less blood loss during surgery. 3-5 A drawback of RAMIE is the complexity of the procedure, as is evident from its learning curve of 18-80 cases. Surgical orientation and identifying crucial anatomical landmarks during RAMIE are particularly challenging for novice surgeons. While the close-up view of the camera on the robot allows for greater visual detail and accurate surgical dissection, it can also lead to a loss of spatial orientation. This is particularly challenging during the thoracic phase of the surgery, where many vital organs are in close proximity to one another. Given the complexity of RAMIE, expert surgeons face a challenge in training novice surgeons. Computer-aided anatomy recognition holds the promise of improving surgical navigation and thereby lowering the learning curve for novice surgeons."}, {"title": "2. METHODS", "content": "In this section, we outline the methodologies employed in our research. First, Sec. 2.1 and 2.2 describe the datasets used in our experiments. Sec. 2.3 details the models and pretraining datasets used, while Sec. 2.4 discusses the implementation details. Lastly, Sec. 2.5 outlines the evaluation procedure."}, {"title": "2.1 RAMIE dataset", "content": "To create the RAMIE dataset, we acquired surgical recordings of thoracoscopic RAMIE procedures between January 2018 and July 2021 at the University Medical Center Utrecht in The Netherlands. These recordings included patients who underwent RAMIE for esophageal cancer, with or without neoadjuvant chemoradiotherapy. The procedures were carried out by two expert RAMIE surgeons, each having performed over 200 RAMIE cases. RAMIE typically involves both thoracic and abdominal phases. However, this research focuses exclusively on videos of the thoracic phase, as surgical navigation is more critical during this part of the procedure. The videos were recorded at a frame rate of 25 Hz with a resolution of 960x540 pixels and have an average duration of two hours. Black borders and the graphical user interface were removed from the recordings to eliminate irrelevant information, resulting in a final resolution of 668\u00d7502.\nWe randomly sampled 879 frames from the videos of 32 distinct patients. These frames were labeled by two students, one Ph.D. candidate, and one research fellow in the field of surgery and medical imaging under the supervision of an expert surgeon. In total, 12 distinct classes were annotated, including four classes for surgical instruments: forceps, hook, suction & irrigation, and vessel sealer. The other eight classes are vital anatomical structures appearing during RAMIE: airways, aorta, azygos vein & vena cava, esophagus, nerves, pericardium, right lung, and thoracic duct. Apart from the 12 distinct classes, a background class was added to create dense semantic labels. Outlines of the different classes are depicted in Fig. 1. The airways include the trachea, left main bronchus, and right main bronchus. These structures were depicted as one single class due to their similarity in appearance, as well as difficulties in defining the exact boundary between the trachea and bronchi. For the same reason, the vena cava and azygos vein were combined into a single class, including the subbranches of the azygos vein, technically known as intercostal veins. The nerves class comprises the four most vital nerves during RAMIE: the left and right vagus nerves, along with the left and right recurrent laryngeal nerves. Finally, the pericardium class also includes the pulmonary veins, as this structure is embedded under the same tissue layer."}, {"title": "2.2 CholecSeg8k dataset", "content": "To allow for a more comprehensive assessment, we additionally used the CholecSeg8k dataset, 10 as it has been employed frequently in similar studies.12,16,17 This dataset includes sequences of 80 consecutive frames from 101 video fragments, yielding 8,080 semantic segmentation masks. Consistent with previous studies, 16, 17 we excluded low-prevalence classes (blood, cystic duct, hepatic vein, and liver ligament) to ensure a robust analysis."}, {"title": "2.3 Models and pretraining datasets", "content": "Tab. 1 provides an overview of eight state-of-the-art models selected for comparison, including their hyperpa-rameters and pretraining type. DeepLabv3, DeepLabv3+, PSPNet, and FPN were chosen for their wide usage in medical imaging and surgical segmentation, while Mask2Former, Segformer, Segmenter, and SegNeXt were selected because they utilize attention mechanisms.18 Attention can help in extracting both local and global features, which is particularly important for segmenting objects of variable size and shape. Attention-based models can also possibly handle occlusion better than traditional CNNs, rendering them suitable for RAMIE."}, {"title": "2.4 Implementation details", "content": "The pretrained ImageNet and ADE20k model weights were obtained from the Segmentation-Models-Pytorch and MMSegmentation packages. 28, 29 Subsequently, the models were fine-tuned with fully unfrozen weights using these frameworks. The hyperparameters were kept mostly similar to those found in the original papers of the models, to ensure a consistent evaluation and to avoid biases that could arise from tuning hyperparameters differently among the evaluated models. Since not all weights were available in the used packages, only a subset of models was trained from scratch and pretrained with ImageNet. All models were fine-tuned on the RAMIE and CholecSeg8k datasets. For both datasets, 85% of the frames were used for training, and 15% of the frames were used for testing. Within the training set, five-fold cross-validation was applied, where each fold consists of approximately 80% for training and 20% for validation. Each set contains data from separate patients to exclude potential biases. Both frames and annotations were resized to 512\u00d7512 pixels using bicubic interpolation. The losses used are cross-entropy (CE) or a combination of CE and Dice score. The optimizers used are stochastic gradient descent (SGD) and AdamW.30 Tab. 1 shows initial learning rates for each model. The learning rate was halved after 10 epochs without validation loss improvement, and early stopping was applied after 25 epochs without improvement. All models were trained on an NVIDIA GeForce RTX 2080 Ti GPU with a batch size of 2. To improve model performance and robustness, augmentations were used, including horizontal flip, vertical flip, blur, brightness, contrast, saturation, scaling, translation, and rotation, all with a probability of 50%."}, {"title": "2.5 Evaluation", "content": "We evaluated all models using the Dice score and the average symmetric surface distance (ASSD).31 We selected ASSD over Hausdorff distance because it is less sensitive to outliers. The Dice score ranges from 0 to 1, whereas ASSD is measured in pixels on a 512\u00d7512 frame. A high Dice score and a low ASSD are preferable. Metrics were calculated on a per-image basis, and we assessed statistical significance using a Wilcoxon signed-rank test. For visual evaluation, the predictions were scaled back to the original image size using bicubic interpolation."}, {"title": "3. RESULTS", "content": "This section presents the benchmark results in three parts. Sec. 3.1 compares the performance of the ImageNet and ADE20k pretraining datasets using a subset of models. Sec. 3.2 offers a quantitative analysis of both attention-based and non-attention-based models utilizing the best pretraining dataset. Finally, Sec. 3.3 provides a qualitative evaluation on a selected subset of models."}, {"title": "3.1 Pretraining evaluation", "content": "Tab. 2 presents performance metrics for DeepLabv3, DeepLabv3+, and PSPNet pretrained on various pretraining datasets and fine-tuned on the RAMIE and CholecSeg8k datasets. On both datasets, the models pretrained on ADE20k significantly outperform those pretrained on ImageNet or without pretraining. This could be explained by the fact that ADE20k is specific to segmentation, which closely aligns with the fine-tuning task. Since ADE20k performs best, it is used for the remaining experiments."}, {"title": "3.2 Quantitative model evaluation", "content": "Tab. 3 displays the frames per second (FPS) and performance metrics for various models pretrained on ADE20k. Among the evaluated models, the traditional CNNs (DeepLabv3, DeepLabv3+, PSPNet, and FPN) achieve the highest FPS, partly because they do not rely on complex attention mechanisms. However, attention-based net-works (Segformer, Mask2Former, Segmenter, and SegNeXt) excel in terms of segmentation quality. In particular,"}, {"title": "3.3 Qualitative model evaluation", "content": "Fig. 4 presents model predictions on the RAMIE dataset using DeepLabv3+, SegNeXt, and Mask2Former. We have selected SegNeXt and Mask2Former for visual evaluation, as they demonstrated the highest performance"}, {"title": "4. DISCUSSION", "content": "In this study, we made an initial attempt at constructing a large semantic segmentation dataset for RAMIE procedures, with the eventual goal of making it freely accessible. To the best of our knowledge, it is the most comprehensive dataset for semantic segmentation in RAMIE to date, encompassing a wide range of anatom-ical structures and surgical instruments. We evaluated multiple pretraining datasets and models using both this unique dataset and the publicly available CholecSeg8k dataset to identify the most effective approach for addressing the challenges in surgical segmentation, as well as to uncover the limitations of current methods.\nFrom the pretraining evaluation on the RAMIE and CholecSeg8k datasets, it can be concluded that pre-training leads to better segmentation performance, most likely due to the small sizes of the surgical datasets. Additionally, models pretrained on ADE20k perform best, which could be explained by the fact that this is a semantic segmentation dataset, and therefore closer to the final application than ImageNet. Given these findings, it is recommended to investigate pretraining on more segmentation datasets in future research.\nBased on the comparison of models on the RAMIE and CholecSeg8k datasets, it can be concluded that the ones that do use attention outperform those that do not. An explanation for this could be that attention can capture long-range dependencies between pixels or regions in an image, which is more difficult to achieve with traditional CNNs. Furthermore, attention allows for focusing on relevant objects even in the presence of occlusions or clutter in the frames. Notably, attention-based models exhibit a lower FPS compared to convolutional models, highlighting a tradeoff between high segmentation quality and low inference time. Nevertheless, attention-based models remain capable of operating near or in real-time, rendering them suitable for surgical anatomy recognition tasks. Mask2Former and SegNeXt especially show superior performance and are therefore recommended for future research. Mask2Former additionally excels at precise boundary delineation, as indicated by its low ASSD. This may be attributed to its use of masked attention, which enables focused processing of specific areas of interest.\nThe performance gain of attention-based models over traditional CNNs is particularly notable in classes that are underrepresented, occluded by blood, or partially obstructed by instruments. On RAMIE, all models achieve strong segmentation results for surgical instruments, owing to their frequent occurrence and distinct appearance in the dataset. Furthermore, the models score well on the aorta, azygos vein & vena cava, and right lung, which usually have high contrast. The model achieves lower scores for the airways and pericardium, which are less prevalent in the dataset. Segmentation accuracy for the esophagus is limited, despite its frequent appearance in the procedure. One potential explanation is the variation in the visual appearance of the esophagus throughout the surgical procedure. The low segmentation performance for the nerves and thoracic duct may be attributed to their rare appearance in the dataset and their smaller size compared to other anatomical structures. Additionally, in our dataset, the thoracic duct was annotated along with the surrounding fat, making it difficult to distinguish from regular fat tissue. Nerves, on the other hand, can be difficult to detect since they are often embedded in connective tissue. However, the nerves and thoracic duct are among the most important anatomical structures for surgeons. Therefore, it is vital to include more data from these specific classes in future research.\nA limitation of this study is that we have primarily focused on the presence of attention mechanisms rather than the specific types of attention used. Additionally, other factors that affect performance, such as model size, were not considered. In future research, we will address these aspects with a more extensive analysis. An additional limitation is the exclusive use of general computer vision pretraining datasets. To explore the benefits of in-domain pretraining, we plan to extend the benchmark by incorporating pretraining on surgical data through self-supervised learning, as proposed in our prior study. 32"}, {"title": "5. CONCLUSION", "content": "In this study, we have developed a comprehensive dataset for semantic segmentation in RAMIE, including a wide range of anatomical structures and surgical instruments. Through the evaluation of various pretraining datasets and models on both our RAMIE dataset and the publicly available CholecSeg8k dataset, we have identified key pretraining and model features, as well as highlighted significant challenges in surgical segmentation. With this work, we hope to facilitate future studies aimed at enhancing surgical navigation, potentially reducing the learning curve for novice surgeons."}]}