{"title": "Kandinsky 3: Text-to-Image Synthesis for Multifunctional Generative Framework", "authors": ["Vladimir Arkhipkin", "Viacheslav Vasilev", "Andrei Filatov", "Igor Pavlov", "Julia Agafonova", "Nikolai Gerasimenko", "Anna Averchenkova", "Evelina Mironova", "Anton Bukashkin", "Konstantin Kulikov", "Andrey Kuznetsov", "Denis Dimitrov"], "abstract": "Text-to-image (T2I) diffusion models are popular for introducing image manipulation methods, such as editing, image fusion, inpainting, etc. At the same time, image-to-video (I2V) and text-to-video (T2V) models are also built on top of T2I models. We present Kandinsky 3, a novel T2I model based on latent diffusion, achieving a high level of quality and photorealism. The key feature of the new architecture is the simplicity and efficiency of its adaptation for many types of generation tasks. We extend the base T2I model for various applications and create a multifunctional generation system that includes text-guided inpainting/outpainting, image fusion, text-image fusion, image variations generation, I2V and T2V generation. We also present a distilled version of the T2I model, evaluating inference in 4 steps of the reverse process without reducing image quality and 3 times faster than the base model. We deployed a user-friendly demo system in which all the features can be tested in the public domain. Additionally, we released the source code and checkpoints for the Kandinsky 3 and extended models. Human evaluations show that Kandinsky 3 demonstrates one of the highest quality scores among open source generation systems.", "sections": [{"title": "1 Introduction", "content": "Text-to-image (T2I) models play a dominant role in generative computer vision technologies, providing high quality results and language understanding along with near real-time inference speed. This led to their popularity and accessibility for many applications through graphic AI editors and web-platforms, including chatbots. At the same time, T2I models are also used outside the image domain, e.g. as a backbone for text-to-video (T2V) generation models. Similar to trends in natural language processing (NLP) (et al, 2024), in generative computer vision there is increasing interest in systems that solve many types of generation tasks. The growing computational complexity of such methods is raising interest in distillation and inference speed up approaches.\nContributions of this work are as follows:\n\u2022 We present Kandinsky 3, a new T2I generation model and its distilled version, accelerated by 3 times. We also propose an approach using the distilled version as a refiner for the base model. Human evaluation results demonstrate the quality of refined model is comparable to the state-of-the-art (SotA) solutions.\n\u2022 We create one of the first feature-rich generative frameworks with open source code and public checkpoints12. We also extend Kandinsky 3 model with a number of generation options, such as inpainting/outpainting, editing, and image-to-video and text-to-video.\n\u2022 We deploy a user-friendly web editor that provides free access to both the main T21 model and all the extensions mentioned\u00b3. The video demonstration is available on YouTube\u2074."}, {"title": "2 Related Works", "content": "To date, diffusion models (Ho et al., 2020) are de facto standard in the text-to-image generation task (Saharia et al., 2022; Balaji et al., 2022; Arkhipkin et al., 2024). Some models, such as Stable Diffusion (Rombach et al., 2022; Podell et al., 2023), are publicly available and widespread in the research community (Deforum, 2022). From the user's point of view, the most popular models are those that"}, {"title": "3 Demo System", "content": "Kandinsky 3 model underlies a comprehensive user interaction system with free access. The system contains different modes for image and video generation, and for image editing. Here we describe the functionality and capabilities of our two key user interaction resources \u2013 Telegram bot and FusionBrain website.\nFusionBrain is a web-editor that supports loading images from the user, and saving generated images and videos (Figure 1). The system accepts text prompts in Russian, English and other languages. It is also allowed to use emoji in the text description. The maximum prompt size is 1000 characters. In terms of generation tasks, this web editor provides the following options:\n\u2022 Text-to-image generation with maximum resolution 1024 \u00d7 1024 and the ability to choose the aspect ratio. In the Negative prompt field, the user can specify which information (e.g., colors) the model should not use"}, {"title": "4 Text-to-Image Model Architecture", "content": "Overview. Kandinsky 3 is a latent diffusion model, which includes a text encoder for processing a prompt from the user, a U-Net-like network (Ronneberger et al., 2015) for predicting noise, and a decoder for image reconstruction from the generated latent (Figure 2). For the text encoder, we use the encoder of the Flan-UL2 20B model (Tay, 2023; Tay et al., 2022), which contains 8.6 billion parameters. As an image decoder, we use a decoder from Sber-MoVQGAN (Arkhipkin et al., 2024). The text encoder and image decoder were frozen during the U-Net training. The whole model contains 11.9 billion parameters (Table 1).\nDiffusion U-Net. To decide between large transformer-based models (Dosovitskiy et al., 2021; Liu et al., 2021; Ramesh et al., 2021) and convolutional architectures, both of which have demonstrated success in computer vision tasks, we conducted more than 500 experiments and noted the following key insights:\n\u2022 Increasing the network depth while reducing the total number of parameters gives better results in training. A similar idea of residual blocks with bottlenecks was exploited in the ResNet-50 (He et al., 2016) and BigGAN-deep architecture (Brock et al., 2019);\n\u2022 We decided to process the latents at the first network layers using convolutional blocks only. At later stages, we introduce transformer layers in addition to convolutional ones. This choice of architecture ensures the global interaction of image elements.\nThus, we settled on the ResNet-50 block as the main block for our U-Net. Using bottlenecks in residual blocks made it possible to double the number of convolutional layers, while maintaining approximately the same number of parameters as without bottlenecks. At the same time, the depth"}, {"title": "5 Extensions and Features", "content": "Many T2I diffusion models suffer from the dependence of the visual generation quality on the level of detail in the text prompt. In practice, users have to use long, redundant prompts to generate desirable images. To solve this problem, we have built a function to add details to the user's prompt using LLM. A prompt is sent to the input of the language model with a request to improve the prompt, and the model's response is sent as the input into Kandinsky 3 model. We used Neural-Chat-7b-v3-1 (Lv et al., 2023), based on Mistral 7B (Jiang et al., 2023)), with the following instruction: ### System: \\nYou are a prompt engineer. Your mission is to expand prompts written by user. You should provide the best prompt for text to image generation in English.\\n### User:\\n{prompt}\\n### Assistant:\\n.\nHere {prompt} is the user's text. Example of generation for the same prompt with and without beautification are presented in the Appendix D.1. In general, human preferences are more inclined towards"}, {"title": "5.1 Prompt Beautification", "content": "generations with prompt beautification (Section 7)."}, {"title": "5.2 Distilled Model", "content": "Inference speed is one of the key challenges for using diffusion models in online-applications. To speed up our T2I model we used the approach from (Sauer et al., 2023), but with a number of significant modifications (see Appendix A). We trained a distilled model on a dataset with 100k highly-aesthetic image-text pairs, which we manually selected from the pretraining dataset (Section 6). As a result, we speed up Kandinsky 3 by 3 times, making it possible to generate an image in only 4 passes through U-Net. However, like in (Sauer et al., 2023), we had to sacrifice the text comprehension quality, which can be seen by the human evaluation (Figure 5). Generation examples by distilled version can be found in Appendix D.2.\nRefiner. We observed that the distilled version generated more visually appealing examples than the base model. Therefore, we propose an approach that uses the distilled version as a refiner for the base model. We generate the image using the base T2I model, after which we noise it to the second step out of the four that the distilled version was trained on. Next, we generate the enhanced image by doing two steps of denoising using the distilled version."}, {"title": "5.3 Inpainting and Outpainting", "content": "We initialize the in/outpainting model by the Kandinsky 3 weights in GLIDE manner (Nichol et al., 2022). We modify the input convolution layer of U-Net so that it takes 9 channels as input: 4 for the original latent, 4 for the image latent,"}, {"title": "5.4 Image Editing", "content": "Kandinsky 2 (Razzhigaev et al., 2023) natively supported images fusion technique through a complex architecture with image prior. Kandinsky 3 has a simpler structure (Figure 2), allowing it to be easily adapted to existing image manipulation approaches.\nFusion and variations. Kandinsky 3 also provides generation using an image as a visual prompt. To do this, we extended an IP-Adapter-based approach (Ye et al., 2023). To implement it based on our T2I generation model, we used ViT-L-14, finetuned in the CLIP pipeline (Radford et al., 2021), as an encoder for visual prompt. For image-text fusion, we get CLIP-embeddings for input text and image, and sum up the cross-attention outputs for them. To create image variations, we get the visual prompt embeddings and feed them to the IP-Adapter. For image fusion, the embeddings for each image are summed with weights and fed into the model. Thus, we have three inference options (Figure 3). We trained our IP-Adapter on the COYO 700m dataset (Byeon et al., 2022).\nStyle transfer. We found that the IP Adapter-based approach does not preserve the shape of objects, so we decided to train ControlNet (Zhang et al., 2023a) in addition to our T2I model to consistently change the appearance of the image, preserving more information compared to the original one (Figure 3). We used the HED detector (Xie and Tu, 2015) to obtain the edges in the image fed to the ControlNet. We train model on the COYO 700m dataset (Byeon et al., 2022).."}, {"title": "5.5 Custom Face Swap", "content": "This service allows one to generate images with real people who are not present in the Kandinsky 3 training set without additional training. The pipeline consists of several steps, including: creating a description of a face on an uploaded photo using the OmniFusion VLM model (Goncharova et al., 2024), generating an image based on it using Kandinsky 3, and finally face detection and then transferring the face from the uploaded photo to generated one using GHOST models (Groshev et al., 2022). Also at the end, enhancement of the transferred face images is done using the GFPGAN model (Wang et al., 2021). Examples are presented in Appendix D.3."}, {"title": "5.6 Animation", "content": "Our I2V generation pipeline is based on the Deforum technique (Deforum, 2022) and consists of several stages as shown in Figure 4. First, we convert the image into a 2.5D representation using a depth map, and apply spatial transformations to the resulting scene to induce an animation effect. Then, we project a 2.5D scene back onto a 2D image, eliminate translation defects and update semantics using image-to-image (I2I) techniques. More details can be found in Appendix C."}, {"title": "5.7 Text-to-Video Generation", "content": "We created the T2V generation pipeline (Arkhipkin et al., 2023), consisting of two models \u2013 for keyframes generation and for interpolation. Both of them use the pretrained Kandinsky 3 as a backbone. Please refer to the main paper for additional details and results regarding the T2V model."}, {"title": "6 Data", "content": "Our dataset for the T2I model training consists of popular open-source datasets and our internal"}, {"title": "7 Human Evaluation", "content": "We found that when a high level of generation quality is achieved, FID values do not correlate well with visually noticeable improvements. For the previous version of Kandinsky model (Razzhigaev et al., 2023) we reported FID, but in this work we focused on human evaluation results for model comparison.\nWe conducted side-by-side (SBS) comparisons between the refined version of Kandinsky 3 with beautification and other competing models: Midjourney 5.2 (Midjourney, 2022), SDXL (Podell et al., 2023) and DALL-E 3 (Betker et al., 2023). For SBS we used generations by prompts from DrawBench dataset (Saharia et al., 2022). We also compared our base T2I model with a distilled and refined version, as well as a version with prompt"}, {"title": "8 Conclusion", "content": "We presented Kandinsky 3, a new open source text-to-image generative model. Based on this model, we presented our multifunctional generative framework that allows users to solve a variety of generative tasks, including inpainting, image editing, and video generation. We also presented and deployed an accelerated distilled version of our model, which, when used as a refiner for the base"}, {"title": "9 Ethical Considerations", "content": "We performed multiple efforts to ensure that the generated images do not contain harmful, offensive, or abusive content by (1) cleansing the training dataset from samples that were marked to be harmful/offensive/abusive, and (2) detecting abusive textual prompts.\nTo prevent NSFW generations we use filtration modules in our pipeline, which works both on the text and visual levels via OpenAI CLIP model (Radford et al., 2021).\nWhile obvious queries, according to our tests, almost never generate abusive content, technically it is not guaranteed that certain carefully engineered prompts may not yield undesirable content. We, therefore, recommend using an additional layer of classifiers, depending on the application, which would filter out the undesired content and/or use image/representation transformation methods tailored to a given application."}, {"title": "A Architecture details", "content": "\u2022 We found that the generator quickly becomes more powerful than the discriminator, which leads to learning instability. To solve this problem, we have significantly increased the learning rate of the discriminator. For the discriminator the learning rate is $1e-3$, and for the generator it is $1e \u2013 5$. To prevent divergence, we used gradient penalty, as in the (Sauer et al., 2023)."}, {"title": "B Training strategy", "content": "We divided the training process into several stages to use more data and train the T2I model to generate images in a wide range of resolutions:\n1. 256 \u00d7 256 resolution: 1.1 billions of text-image pairs, batch size = 20, 600k steps, 104 NVIDIA Tesla A100;\n2. 384 x 384 resolutions: 768 millions of text-image pairs, batch size = 10, 500k steps, 104 NVIDIA Tesla A100;\n3. 512 x 512 resolutions: 450 millions of text-image pairs, batch size = 10, 400k steps, 104 NVIDIA Tesla A100;\n4. 768 x 768 resolutions: 224 millions of text-image pairs, batch size = 4, 250k steps, 416 NVIDIA Tesla A100;\n5. Mixed resolution: $768^2 <W\u00d7H < 1024^2$, 280 millions of text-image pairs, batch size = 1, 350k steps, 416 NVIDIA Tesla A100."}, {"title": "C Animation pipeline details", "content": "The scene generation process involves depth estimation along the z-axis in the interval $[(z_{near}, z_{far})]$. Depth estimation utilizes AdaBins (Bhat et al., 2020). The camera is characterized by the coordinates $(x, y, z)$ in 3D space, and the direction of view, which is set by angles $(\\alpha, \\beta, \\gamma)$. Thus, we set the trajectory of the camera motion using the dependencies $x = x(t), y = y(t), z = z(t), a = a(t), \\beta = \\beta(t)$, and $\\gamma = \\gamma(t)$. The camera's first-person motion trajectory includes perspective projection operations with the camera initially fixed at the origin and the scene at a distance of $z_{near}$. Then, we apply transformations by rotating points around axes passing through the scene's center and translating to this center. Due to the limitations of a single-image-derived depth map, addressing distortions resulting from camera orientation deviations is crucial. We adjust scene position through infinitesimal transformations and employ the I2I approach after each transformation. The I2I technique facilitates the realization of seamless and semantically accurate transitions between frames."}, {"title": "D Additional generation examples", "content": ""}, {"title": "D.1 Prompt beautification", "content": ""}, {"title": "D.2 Distillation and prior works", "content": ""}, {"title": "D.3 Custom Face Swap", "content": ""}]}