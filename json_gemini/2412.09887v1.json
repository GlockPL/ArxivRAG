{"title": "CSL-L2M: Controllable Song-Level Lyric-to-Melody Generation Based on Conditional Transformer with Fine-Grained Lyric and Musical Controls", "authors": ["Li Chai", "Donglin Wang"], "abstract": "Lyric-to-melody generation is a highly challenging task in the field of AI music generation. Due to the difficulty of learning strict yet weak correlations between lyrics and melodies, previous methods have suffered from weak controllability, low-quality and poorly structured generation. To address these challenges, we propose CSL-L2M, a controllable song-level lyric-to-melody generation method based on an in-attention Transformer decoder with fine-grained lyric and musical controls, which is able to generate full-song melodies matched with the given lyrics and user-specified musical attributes. Specifically, we first introduce REMI-Aligned, a novel music representation that incorporates strict syllable- and sentence-level alignments between lyrics and melodies, facilitating precise alignment modeling. Subsequently, sentence-level semantic lyric embeddings independently extracted from a sentence-wise Transformer encoder are combined with word-level part-of-speech embeddings and syllable-level tone embeddings as fine-grained controls to enhance the controllability of lyrics over melody generation. Then we introduce human-labeled musical tags, sentence-level statistical musical attributes, and learned musical features extracted from a pre-trained VQ-VAE as coarse-grained, fine-grained and high-fidelity controls, respectively, to the generation process, thereby enabling user control over melody generation. Finally, an in-attention Transformer decoder technique is leveraged to exert fine-grained control over the full-song melody generation with the aforementioned lyric and musical conditions. Experimental results demonstrate that our proposed CSL-L2M outperforms the state-of-the-art models, generating melodies with higher quality, better controllability and enhanced structure.", "sections": [{"title": "Introduction", "content": "Deep learning techniques have been increasingly applied to various music generation tasks (Duan, Yu, and Oyama 2024; Hahn et al. 2023; Yu, Srivastava, and Canales 2021; Tian et al. 2023). Lyric-to-melody generation, one of the most essential and common tasks in songwriting, has attracted growing interest from both academia and industry. A high-quality lyric-to-melody generation is required to generate melodies not only following good musical patterns but also aligning with the given lyrics. Due to the scarcity of paired lyric-melody data with alignment information and the difficulty of learning the strict but weak correlations between lyrics and melodies, this task remains under-explored.\nMany deep learning methods have been explored for lyric-to-melody generation. A sequence-to-sequence based melody composition model is proposed in (Bao et al. 2019) which is the first work to use an end-to-end network model to generate melodies from lyrics. Subsequently, Yu (Yu, Srivastava, and Canales 2021) proposes a conditional LSTM-GAN generative model for melody generation from lyrics. In (Srivastava et al. 2022), a novel architecture, three branch conditional LSTM-GAN is proposed to further improve generation quality. However, the direct mapping from lyrics to melodies is difficult to learn because they are weakly correlated (e.g., a melody can correspond to many different lyrics and vise versa.). Accordingly, these end-to-end generation methods suffer from low generation quality due to the limited available parallel lyric-melody data. To this end, an unsupervised method is proposed in (Sheng et al. 2021), which performs self-supervised masked sequence to sequence pre-training on large amount of unpaired lyric and melody data. In addition, a two-stage generation method with music template is proposed in (Ju et al. 2021), which is data efficient and addresses the issues of limited paired data to some extent. With the tremendous success of large language models (LLMs) (Touvron et al. 2023), more recently, the work in (Ding et al. 2024) attempts to leverage the capability of LLMs to model the lyric-melody relationship.\nCurrently, the lyric-to-melody generation methods including those mentioned above focus on generating short melodies from lyrics typically consisting of one sentence or a few sentences, where a full-song melody is usually composed by simply concatenating these sentence-level melodies resulting in incoherent musical structure without both repetition patterns and distinguishable verse-chorus structure. In addition, controllability is a crucial aspect of the lyric-to-melody generation task, which allows users to interact with the generation process to create their expected melodies. Nevertheless, only a few lyric-to-melody works have explored the controllability. In (Ju et al. 2021), the generated melodies can be controlled by adjusting the musical elements in music templates including tonality and chord progression. A reference style embedding technique is proposed in (Zhang, Yu, and Takasu 2023) to achieve the control over the style of generated melodies. The research of (Duan et al. 2022) enables users to interact with the generation process and recreate music by selecting from recommended musical attributes. However, these works only provide a few coarse-grained musical attribute controls. One more thing, since one syllable may correspond to one or more notes, the alignment between the given lyrics and corresponding melodies could be \"one-to-one \"or \u201cone-to-many \". Most of previous methods only consider the \u201cone-to-one \"alignment, which introduces bias into the melody composition.\nTo address the aforementioned issues, we propose a controllable song-level lyric-to-melody generation method called CSL-L2M, which is capable of generating melodies aligning with lyrics and user-specified musical attributes at the full-song level. Specifically, we first introduce a novel music representation called REMI-Aligned. This representation incorporates strict syllable- and sentence-level lyric-melody alignments, which makes both exact and \u201cone-to-many \"alignment learning feasible. Inspired by (Wu and Yang 2023), which equips conditional Transformer with the capability to model long sequences under fine-grained time-varying conditions through in-attention, we integrate the in-attention technique into our CSL-L2M model. Multiple multi-granularity lyric controls (including sentence-level semantic embeddings, word-level part-of-speech (POS) embeddings, and syllable-level tone embeddings) and musical controls (including coarse-level human-labeled musical tags, sentence-level statistical musical attributes, and learned high-fidelity musical features (von R\u00fctte et al. 2023) from a pre-trained Vector Quantized-Variational AutoEncoder (VQ-VAE)) are extracted and fed into the conditional Transformer decoder through in-attention to realize tight fine-grained control of lyrics and musical attributes over melody generation process. This enables the generation of high-quality melodies from lyrics, precisely tailored to the user's desired musical style. Moreover, the musical controls not only enable user-controllable generation but also provide the model with additional musical information that is beneficial for melody modeling. Experiments conducted on our 10,170 Chinese pop songs demonstrate that CSL-L2M could generate melodies that are both well-matched with the lyrics and consistent with the user-specified musical attributes. Compared to the state-of-the-art lyric-to-melody generation methods, CSL-L2M generates melodies with higher quality, better controllability and enhanced structure."}, {"title": "Related Work", "content": "Lyric-to-Melody Generation The development of lyric-to-melody generation has evolved from traditional rule-based (Nichols 2009; Monteith, Martinez, and Ventura 2012) and statistical methods (Long, Wong, and Sze 2013) to deep learning methods. The traditional methods usually rely on specific hand-designed musical rules and suffer from low generation quality. Currently, the end-to-end deep generative models are the mainstream methods but they suffer from several challenges: 1) weak correlations between lyrics and melodies are difficult to capture by the network models, where much paired training data with alignment information is required; 2) strict alignment between each syllable in the given lyric and note in the corresponding melody is required, which needs additional alignment modeling. As for the first challenge, limited available paired lyric-melody data affects generation quality. The end-to-end models which directly learn the mapping from lyrics to melodies with the limited paired data often lead to poor generation quality. To this end, SongMASS (Sheng et al. 2021) improves the generation performance of end-to-end models by leveraging self-supervised pre-training on much unpaired lyric and melody data. Furthermore, TeleMelody (Ju et al. 2021), a two-stage generation pipeline based on musical templates, is proposed to enhance data efficiency and further improve generation performance. In addition, ReLyMe (Zhang et al. 2022) introduces several principles of lyric-melody relationships from music theory into the decoding process, enhancing the harmony between lyrics and melodies. However, these methods fail to exploit melody-related lyric information and additional musical information for tightly controlling over the melody generation. Consequently, they are unable to adequately capture the intricate relationships between lyrics and melodies, resulting in limited generation quality. Moreover, few of them generate melodies at the full-song level, causing poor musical structure. As for the second challenge, most existing works either focus solely on the \u201cone-to-one \"lyric-melody alignment or do not ensure precise alignment, which can easily degrade generation quality.\nControllable Music Generation Controllability in music generation aims to provide user control over the process in a desired direction (Briot and Pachet 2020). According to the levels of controllability, it can be divided into global/coarse-grained control and fine-grained control. The former refers to the fact that generation process is guided by time-invariant controls. Instead, the later refers to the fact that the generation process is guided by time-varying controls, which can provide more flexible and precise control, especially in the generation of long sequences. Controllable music generation has attracted increasing research interest. In (Dong et al. 2018; Yang, Chou, and Yang 2017; Neves, Fornari, and Florindo 2022), global conditions are injected into the training procedure of generative adversarial networks. Some works (Payne 2019; Sarmento et al. 2023) achieve global control through conditional Transformer models with prompt-based control tokens. Many methods based on VAE enable users to exert global control by manipulating latent conditioning vectors (Brunner et al. 2018; Roberts et al. 2018; Tan and Herremans 2020). Transformer autoencoders are used in (Choi et al. 2020) to realize improved control by learning global performance representations. However, these global controls often become less effective during long sequence generation, as the model may forget or weaken the global conditions over time. In contrast, MuseMorphose (Wu and Yang 2023) and FIGARO (von R\u00fctte et al. 2023) introduce fine-grained control, where the former is realized through one Transformer VAE based on an in-attention conditioning technique and the later is achieved through description-to-sequence learning. Existing research on lyric-to-melody generation rarely pays attention to controllability. Only a few works have explored this area and do not offer fine-grained and flexible control. In this paper, we delve into the controllability of lyric-to-melody generation."}, {"title": "Methodology", "content": "To overcome the difficulty of learning strict yet weak correlations between lyrics and melodies and enable user controls over full-song melody generation, we propose a controllable song-level lyric-to-melody generation method called CSL-L2M, as shown in Figure 2. This method is capable of generating full-song melodies that match the given lyrics and adhere to user-specified musical attributes. We achieve this by employing the in-attention technique, as proposed in (Wu and Yang 2023), to tightly control the conditional autoregressive Transformer decoder's generation process under multiple multi-granularity lyric and musical conditions."}, {"title": "Technical Background", "content": "The unconditional Transformer decoder's autoregressive generation process can be formulated as $p(x_t|x_{<t})$, where $x_t$ is the element of a sequence to predict at timestep t, and $x_{<t}$ represents all previously generated elements of the sequence. If a global condition vector c is offered to the model, the modeling could be formulated as $p(x_t|x_{<t}, c)$. However, the global control tends to lose its effectiveness during long sequence generation. It is needed to incorporate fine-grained control mechanisms. Assuming that the target sequence consists of N segments and each timestep index $t \\in [1, T]$ belongs to one of the N sets of indices $I_1, I_2, ..., I_N$, where $I_n \\cap I_{n'} = \\O$ for $n \\neq n'$ and $\\bigcup_{n=1}^{N} I_n = [1,T]$, the fine-grained control is achieved by providing the generation model with each segment-level condition vector $c_n$ during the corresponding time interval $I_n$, formulated as:\n\n$P(X_t|X_{<t}; C_n), \\quad for \\ t \\in I_n,$\\newline\n\nwhere the time-varying condition vectors $C_1, C_2, ..., c_n$ provide a high-level blueprint of the sequence to model. This could be helpful for long sequence generation, particularly for full-song music generation.\nThere are many ways to condition autoregressive Transformer decoders at fine-grained level, where the in-attention conditioning (Wu and Yang 2023) offers tight control. Specifically, the in-attention method projects the segment-level condition vector $c_n$ to the same space as the self-attention hidden stats via\n\n$\\hat{c_n} = c_n W_{in}, \\quad W_{in} \\in \\mathbb{R}^{d \\times d}$\\newline\n\nThen the hidden condition state $\\hat{c_n}$ is added to each hidden state of all the self-attention layers to obtain the input to the subsequent layer, formulated as:\n\n$h_l^\\prime = h_l + \\hat{c_n},\\quad \\forall l \\in \\{0, ..., L - 1\\};\\newline\n\n$h_{l+1} = SelfAttention(h_l^\\prime),$\\newline\n\nwhich serves as a frequent reminder of the segment-level conditions for the Transformer decoder, thereby achieving tight control over the generation process."}, {"title": "REMI-Aligned Representation", "content": "To apply neural sequence models to symbolic music generation tasks, it is necessary to first convert a musical piece into a time-ordered sequence of discrete tokens. There are several ways to implement the conversion, leading to different sequence representations of the same music piece. One prevalent music representation is based on revamped MIDI-derived events (REMI) (Huang and Yang 2020). In REMI, a musical piece is represented as a time-ordered sequence of event tokens including bar, position, pitch, duration, velocity, tempo and chord.\nTo precisely model the strict alignments between lyrics and melodies, we propose incorporating both sentence-level and syllable-level alignments into the music representation to enable explicit learning. Accordingly, we extend REMI to form a REMI-Aligned music representation by adding these alignments, while discarding tempo, chord and note velocity tokens given the fixed tempo in our dataset, irrelevance of note velocity to our task, and potential issues with chord accuracy. Furthermore, since 64th notes are the shortest notes in our dataset, we improve the temporal resolution of note position from 4 to 16 sub-beats per quarter note, enabling precise quantization of each note in our fixed 4/4 time signature dataset."}, {"title": "Model Architecture", "content": "Figure 2 illustrates the architecture of our proposed CSL-L2M, consisting of a sentence-wise bidirectional Transformer encoder and an autoregressive Transformer decoder equipped with the capability to model full-song melodies under both lyric and musical multi-granularity controls."}, {"title": "Lyric Controls", "content": "In CSL-L2M, we fully utilize the lyric information related to melodies in training, which improves the model capability to capture the correlations between lyrics and melodies. Specifically, we extract syllable-level tone embeddings, word-level POS embeddings, and sentence-level semantic embeddings serving as time-varying conditions at different granularities to exert fine-grained controls over the Transformer decoder's generation via the in-attention technique.\n1) Tone: Tone 1, in tonal languages, refers to the pitch variations that help distinguish words with the same spelling but different meanings, playing a crucial role in minimizing semantic ambiguities. Around 60% of languages have tone."}, {"title": "Musical Controls", "content": "To enable user control over the melody generation, we introduce human-labeled musical tags, sentence-level statistical musical attributes, and learned latent musical representations extracted from a pre-trained VQ-VAE serving as coarse-grained, fine-grained and high-fidelity controls, respectively, to the generation process.\n1) Human-Labeled Musical Tags: We offer a high-quality, precisely annotated parallel lyric-melody dataset with alignment information consisting of 10,170 Chinese pop songs with time signature 4/4. Moreover, tags of key 3, emotion, and structure 4 of each song are meticulously annotated, encompassing 24 5, 3 6, and 5 7 distinct categories respectively. The three types of musical tags serving as coarse-grained conditions are introduced into the decoder to realize human-interpretable control over the melody generation. Specifically, key is highly correlated with pitch distribution of the entire melody. The key of each song $c_{key}$ is transformed into an embedding vector by an embedding layer, i.e., $\\hat{c_{key}} = Emb_{key} (c_{key})$ and then fed into the decoder as a global condition. Similarly, the emotion of each song $c_{emot}$ is transformed into an embedding vector $\\hat{c_{emot}} = Emb_{emot} (c_{emot})$ and then fed into the decoder as a global condition. The verse-chorus form, serving as the cornerstone of pop songs, comprises two core sections a verse and a chorus-that typically contrast melodically, rhythmically, harmonically and dynamically. We convert the uth structure-level attribute of each song into an embedding vector $\\hat{c_{struc}} = Emb_{struc}(c_{struc})$, and then fed it into the decoder as a structure-varying condition."}, {"title": "Statistical Musical Attributes", "content": "To enable fine-grained user controls over melody generation and help the model better learn correlations between lyrics and melodies, we introduce sentence-level statistical musical attributes. Their granularity is set to a sentence instead of a bar to maintain consistency with the semantic lyric embedding control granularity. We utilize 12 types of statistical musical attributes: pitch mean (PM), pitch variance (PV), pitch range (PR), direction of melodic motion (DMM), amount of arpeggiation (AA), chromatic motion (CM), duration mean (DM), duration variance (DV), duration range (DR), prevalence of most common note duration (MCD), note density (ND), fraction of syllables in lyrics to notes in the corresponding melodies (Align) 8. They are calculated for each sentence-level melody sequence. We first quantize these attributes into K classes with approximately equal sample sizes, where K = 64 in our work. Then the nth sentence-level attributes of the 12 statistical musical attributes for each song are converted into embedding vectors $\\hat{c_{PM}}, \\hat{c_{PV}}, \\hat{c_{PR}}, \\hat{c_{DMM}}, \\hat{c_{AA}}, \\hat{c_{CM}}, \\hat{c_{DM}}, \\hat{c_{DV}}, \\hat{c_{DR}}, \\hat{c_{MCD}}, \\hat{c_{ND}}, \\hat{c_{Align}}$, respectively and fed into the decoder. These controls can be grouped into four categories, i.e., pitch-related controls (pitch $Ctls=Concat(\\hat{c_{PM}}; \\hat{c_{PV}} ; \\hat{c_{PR}} ; \\hat{c_{DMM}}; \\hat{c_{AA}}; \\hat{c_{CM}}))$, duration-related controls (Dur $Ctls=Concat(\\hat{c_{DM}}; \\hat{c_{DV}}; \\hat{c_{DR}}; \\hat{c_{MCD}}))$, rhythm-related controls ($\\hat{c_{ND}}$), and note-number-related controls ($\\hat{c_{Align}}$)."}, {"title": "Learned Musical Features", "content": "Inspired by (von R\u00fctte et al. 2023), we introduce learned musical features extracted from the latent space of a pre-trained VQ-VAE model to provide high-fidelity information to the decoder. This helps to alleviate non-injectivity problem in the lyric-to-melody generation task. The VQ-VAE model consists of a Transformer encoder, a Transformer decoder, and a vector quantization. For training, first, the full-song melody of each song is split into sentence-level melody sequences $X = \\{X_1, X_2, ..., X_N\\}$, where $X_n$ is the nth sentence-level melody sequence and tokenized by REMI-Aligned. Then the Transformer encoder maps these sequences to the latent space in parallel. The encoder's attention output at the first timestep (corresponding to the SEQ token of the sentence-level melody sequence tokens) is considered as contextualized representation of the sequence. Finally, the quantized latent representations are obtained via the vector quantization and then fed into the decoder through in-attention to reconstruct the original full-song melody. Note that the hyperparameters in the vector quantization, i.e., latent group and codebook sizes, are set to 64 and 2048 respectively in our work. Thus the sentence-level quantized latent representations $z_{learned}$ extracted from the pre-trained VQ-VAE serving as learned musical features are introduce into our CSL-L2M training to provide high-fidelity information."}, {"title": "Feeding Multi-Granularity Controls into the Transformer Decoder", "content": "Borrowing the in-attention technique from (Wu and Yang 2023), which conditions Transformer decoders with time-varying conditions during long sequence generation, we feed aforementioned multi-granularity controls into our Transformer decoder to achieve firm control over the full-song melody generation. Specifically, the word-level POS embeddings and sentence-level semantic embeddings are expanded to the syllable level by replication. Then they are added to the tone embeddings to get the syllable-level lyric-related controls $c_{lyric} = \\hat{c_{tone}} + \\hat{c_{POS}} + z_{sem}$. Next, aforementioned multi-granularity musical controls are expanded to the syllable level by replication according to the alignment information between lyrics and melodies. Finally, these syllable-level lyric and musical controls are fed into the decoder through in-attention after concatenation, i.e.,\n\n$C_s = concat([c_{lyric}; \\hat{c_{key}}; \\hat{c_{emot}}, \\hat{c_{struc}}; \\hat{c_{PM}}, \\hat{c_{PV}}; \\hat{c_{PR}}; \\hat{c_{DMM}},\\hat{c_{AA}}; \\hat{c_{CM}}, \\hat{c_{DM}}, \\hat{c_{DV}}; \\hat{c_{DR}}; \\hat{c_{MCD}}; \\hat{c_{END}}; \\hat{c_{Align}}; z_{learned}]),\\newline\n\n$y_t = Dec(x_{<t}; C_s).$"}, {"title": "Experiments", "content": "Experimental Settings\nDataset The collection of paired lyric-melody data is difficult due to the need for precise synchronization between lyrics and melodies as shown in the sheet music in Figure 1, which requires detailed annotation and specific expertise. Currently, available paired lyric-melody dataset with alignment information is limited and of insufficient quality. To this end, we offer a high-quality, precisely annotated parallel lyric-melody dataset, encompassing 10,170 Chinese pop songs with time signature 4/4. Moreover, musical tags for each song including key, lyric emotion, song structure, and beats per minute (BPM) are precisely annotated. We perform some statistics on this dataset shown in Figure 3. The following observations are made: 1) the most pitch/MIDI numbers fall within the range of 50 to 80; 2) in contrast to melodies in the English dataset (Yu, Srivastava, and Canales 2021), the melodies in our Chinese dataset feature a predominance of short musical notes, specifically 8th and 16th notes; 3) more than 80% of characters/syllables correspond to a single musical note (i.e. \u201cone-to-one \"alignment), and nearly 20% of characters correspond to two or more notes (i.e. \u201cone-to-many \"alignment); 4) the BPM of most songs falls within the range of 60 to 120. The 10,170 Chinese pop songs are split into the training, validation, and test sets in an 9:0.5:0.5 ratio for our experiments.\nImplementation Details Both the encoder and decoder of our CSL-L2M and VQ-VAE models consist of 12 self-attention layers with 8 self-attention heads, 512 hidden size and 2048 feed-forward dimension. The dimension of each lyric attribute embedding as well as learned musical feature is 128. And the dimension of each human-annotated and statistical musical attribute embedding is 32. The models are trained with Adam optimizer and teacher forcing. We use linear warm-up to increase the learning rate to $10^{-4}$ in the first 200 steps, followed by a 150k-step cosine decay down to $5 \\times 10^{-6}$. The batch size is set to 4. During inference, nucleus sampling (Holtzman et al. 2020) is used to sample from the decoder output distribution at each timestep with a softmax temperature $\u0442 = 1.2$ and truncating the distribution at cumulative probability $p = 0.9$.\nEvaluation Metrics Unlike previous works that evaluate generated melodies from lyrics at the sentence level, we conduct evaluations on full-song melodies.\n1) Objective Metrics: Objective evaluations are conducted on our test set comprising around 500 songs from our 10,170 Chinese pop songs. We focus on assessing the similarity between the generated and the ground-truth melodies. The following objective metrics proposed by (Sheng et al. 2021) are adopted: 1) Pitch Distribution Similarity (PD); 2) Duration Distribution Similarity (DD); 3) Melody Distance (MD).\n2) Subjective Metrics: Subjective evaluations are conducted on 10 songs randomly selected from our test set. We invite 70 participants (including 50 amateurs and 20 professionals) to score the melody properties using a scale from 1 (Poor) to 5 (Perfect). The following subjective metrics are considered: 1) Harmony: Is the melody itself harmonious as well as harmonized with the lyrics ? 2) Rhythm: Does the rhythm sound natural and match the rhythm of the lyrics? 3) Structure: How well does the melody structure match lyric structure? Specifically, whether lyrics with similar rhythm patterns have similar melodies? Does the melody feature a distinguishable verse-chorus structure? Are the transitions between contiguous phrases natural and coherent? 4) Emotion: Does the melody convey a consistent emotion with the lyrics? 5) Quality: What is the overall quality of the melody?"}, {"title": "Experimental Results", "content": "Main Results Since learned musical features need to be extracted from existing melodies, unless otherwise specified, our reference to CSL-L2M refers to the version without learned musical controls. The evaluation of the full version will be conducted later in the context of style transfer and controllable generation. We first compare our CSL-L2M with two state-of-the-art models, i.e. TeleMelody (Ju et al. 2021) and SongComposer (Ding et al. 2024). As shown in Table 1, CSL-L2M significantly outperforms advanced models, namely SongComposer and TeleMelody, in both objective and subjective evaluations, demonstrating the effectiveness of CSL-L2M in generating high-quality song-level melodies from lyrics. We further perform ablation study to verify the effectiveness of lyric and musical controls in CSL-L2M. As illustrated in Table 2 and Figure 4, by successively removing duration-related controls, pitch-related controls, note density and alignment controls, and human-annotated musical attribute controls, we observe a continuous performance degradation. This indicates that the musical controls, in addition to enabling user-controlled generation, can help conditional Transformer in modeling melodies because they offer the model more musical information for reference. Besides, the learned musical features include high-fidelity information of melodies, which aids in reducing non-injectivity of the generation model. As a result, we find that CSL-L2M equipped with learned musical controls achieves top performance that nearly reaches the ceiling. Moreover, CSL-L2M with only lyric controls exceeds the performance of the two state-of-the-art models. This confirms the effectiveness of our designed fine-grained lyric controls and the lyric-to-melody generation framework based on conditional Transformer with the in-attention conditioning mechanism.\nControllability Study Given that our statistical musical controls are ordinal by nature, following (Kawai, Esling, and Harada 2020) and (Wu and Yang 2023), we use the Spearman's rank correlation coefficient $p$ to quantitatively assess the strength of statistical musical attribute control. To simultaneously evaluate the impact on other unrelated attributes when transferring a specific attribute, we calculate Spearman's rank correlation coefficient matrix between the user-specified attribute classes and attribute raw scores derived from the generated melodies. Results in Figure 5 reveal the strong and independent controllability strengths of CSL-L2M in attribute control. Specifically, for example, $P_{PM} = 0.98$ denotes a strong and positive correlation between the user-specified attribute class $\\hat{c_{PM}}$ and the attribute raw class $\\tilde{c_{PM}}$ computed from the generated melodies, which demonstrates strong controllability of the pitch mean attribute. In contrast, $P_{PM|Align} = 0.09$ is the correlation coefficient between the user-specified alignment attribute class $\\hat{c_{Align}}$ and the unrelated attribute class $\\tilde{c_{PM}}$ computed from the generated melodies, revealing the independent controllability of attributes in the multi-attribute scenario.\nCase Study In Figure 6, we present some generated sheet music given lyrics to demonstrate the advantages of our CSL-L2M in terms of generation quality and controllability 9. Due to space constraint, only a portion of the full-song melody is provided here. Specifically, Figure 6a shows that the generated melodies not only harmonize with given lyrics but also exhibit a coherent and distinguishable verse-chorus structure, along with repetition patterns matching lyrics. Besides, it is observed that our model can well model the \u201cone-to-many\" alignment relationship between lyrics and melodies. In Figure 6b, the generate melodies well adhere to user-specified musical attributes. Figure 6c presents high-fidelity style transfer results of CSL-L2M equipped with the learned musical features, confirming that the learned features provide high-fidelity melody information to the Transformer decoder. In summary, our proposed CSL-L2M could generate melodies that not only match with the given lyrics but also adhere to user-specified musical attributes."}, {"title": "Conclusion", "content": "To address weak controllability, low-quality and poorly structured generation issues in the lyric-to-melody generation task, we propose CSL-L2M in this paper towards controllable song-level melody generation conditioning on lyrics and user-specified musical attributes. We first introduce a novel music representation named REMI-Aligned to facilitate precise lyric-melody alignment relationship modeling. Then multiple multi-granularity lyric and musical attribute controls are extracted and fed into the conditional Transformer decoder through in-attention to achieve firm control over the generation process. Experiments demonstrate that our proposed CSL-L2M outperforms the state-of-the-art models in terms of generation quality and controllability. We believe our contributions will further advance the under-explored field of lyric-to-melody generation."}]}