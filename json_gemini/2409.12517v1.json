{"title": "SCALING FP8 TRAINING TO TRILLION-TOKEN LLMS", "authors": ["Maxim Fishman", "Brian Chmiel", "Ron Banner", "Daniel Soudry"], "abstract": "We train, for the first time, large language models using FP8 precision on datasets up to 2 trillion tokens - a 20-fold increase over previous limits. Through these extended training runs, we uncover critical instabilities in FP8 training that were not observable in earlier works with shorter durations. We trace these instabilities to outlier amplification by the SwiGLU activation function. Interestingly, we show, both analytically and empirically, that this amplification happens only over prolonged training periods, and link it to a SwiGLU weight alignment process. To address this newly identified issue, we introduce Smooth-SwiGLU, a novel modification that ensures stable FP8 training without altering function behavior. We also demonstrate, for the first time, FP8 quantization of both Adam optimizer moments. Combining these innovations, we successfully train a 7B parameter model using FP8 precision on 256 Intel Gaudi2 accelerators, achieving on-par results with the BF16 baseline while delivering up to a ~ 34% throughput improvement.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have revolutionized natural language processing, demonstrating remarkable capabilities across a wide range of tasks. However, the computational demands of training these models have become increasingly challenging, driving the need for more efficient training methods. Low precision formats, particularly FP8, have emerged as a promising solution to reduce memory usage and accelerate training. Recent work by Peng et al. (2023) has demonstrated the potential of FP8 training for LLMs. However, these studies have been limited to datasets of up to 100 billion tokens, leaving open questions about the scalability and stability of FP8 in truly large-scale training scenarios.\nIn this paper, we present advancements in FP8 training for LLMs, successfully scaling to datasets of up to 2 trillion tokens - a 20-fold increase over previous limits. This leap in scale has revealed critical instabilities in FP8 training that were not observable in earlier, shorter-duration studies. Through rigorous analysis, we trace these instabilities to a previously unidentified phenomenon: the amplification of outliers by the SwiGLU activation function (Shazeer (2020a)), which becomes pronounced only after extended training periods. The severity of this issue is illustrated in Fig. 2(a), where we present the training loss of Llama2 7B using FP8 precision. The graph clearly shows a dramatic divergence caused by outliers after processing 220B tokens - twice the dataset size explored in previous work (Peng et al. (2023)).\nTo address this newly discovered challenge, we introduce Smooth-SwiGLU, a novel modification to the standard SwiGLU activation that effectively mitigates outlier amplification without altering the function's behavior. This innovation ensures stable FP8 training across extended durations, enabling the use of FP8 precision in large-scale LLM training without compromising model performance.\nFurthermore, we push the boundaries of low-precision optimization by demonstrating, for the first time, the successful quantization of both Adam optimizer moments to FP8. This advancement reduces memory usage during training, further enhancing the efficiency of large-scale LLM development."}, {"title": "2 BACKGROUND AND CHALLENGES OF FP8 TRAINING IN LLMS", "content": "The computational demands of Large Language Models (LLMs) have driven a shift from traditional FP32 to reduced-precision formats. While FP16 and BF16 have become standard in many training tasks (Micikevicius et al., 2017; Scao et al., 2022; Smith et al., 2022), FP8 represents the next step in this progression towards lower precision. Micikevicius et al. (2022) standardized two FP8 formats for deep learning: E4M3 (4 exponent bits, 3 mantissa bits) optimized for weights and activations, and E5M2 (5 exponent bits, 2 mantissa bits) suitable for gradients.\nFP8 shows promise for large-scale training, especially with support from modern hardware like NVIDIA's H100 and Intel's Gaudi2. However, its limited range necessitates careful scaling techniques to maintain numerical stability and model performance.\nThe primary challenge in FP8 training for LLMs stems from its limited dynamic range. To address this, researchers have developed various scaling techniques. Global loss scaling (Micikevicius et al., 2017) multiplies the entire loss by a constant factor to prevent gradient underflow during backpropagation. Per-tensor scaling (Sun et al., 2019) takes a more granular approach, scaling each tensor individually based on its specific range of values. These techniques allow for better utilization of the FP8 format's limited range. However, (Lee et al., 2024) demonstrates that, even with these techniques, FP8 training can lead to significant instabilities, highlighting the ongoing challenges in reduced-precision LLM training.\nImplementation of these scaling techniques typically follows one of two approaches: just-in-time scaling or delayed scaling. Just-in-time scaling dynamically adjusts scaling factors based on the current data distribution. However, it often proves impractical in FP8 training due to the need for multiple data passes, which can negate the performance benefits of using FP8. Delayed scaling, on the other hand, selects scaling factors based on data distributions from preceding iterations. While more practical, it assumes consistent statistical properties across iterations, making it vulnerable to outliers that can disrupt this consistency and potentially destabilize the training process.\nRecent work by Peng et al. (2023) demonstrated the first empirical results of training LLMs using FP8 format up to 100 billion tokens, validating the potential of FP8 for large-scale training. Our research extends this work, successfully scaling FP8 training to datasets of up to 2 trillion tokens. We introduce novel techniques to address the challenges of FP8's limited dynamic range and the instabilities that emerge in extended training scenarios. Our approach not only overcomes these limitations but also achieves substantial improvements in memory usage and training speed, demonstrating the viability of FP8 training for truly large-scale LLM development."}, {"title": "3 OUTLIER AMPLIFICATION IN LARGE-SCALE FP8 TRAINING", "content": "The presence of outliers has been observed in numerous studies (Yang et al., 2024), particularly in the activations during inference. These outliers can significantly impact the stability and performance of the model, as they introduce extreme values that are difficult to manage within the limited dynamic range of reduced-precision formats like FP8. Our work reveals that these outliers become particularly prominent in the later stages of training large language models (LLMs) with large-scale datasets.\nAs shown in Fig. 1, these outliers emerge only after processing approximately 200 billion tokens during training. This phenomenon poses significant challenges to maintaining numerical stability and model performance, especially when using delayed scaling methods that assume consistency across iterations. The sudden appearance of these outliers disrupts the statistical assumptions underlying FP8 training techniques, potentially leading to instability or divergence in the training process.\nThe emergence of these outliers in the later stages of training is particularly problematic for FP8 formats due to their limited dynamic range. Unlike higher precision formats such as FP32 or even BF16, FP8 has a much narrower range of representable values. When outliers exceed this range, they can cause overflow or underflow, leading to a loss of critical information and potentially destabilizing the entire training process.\nMoreover, the sporadic nature of these outliers, as evident in Fig. 1b, makes them challenging to predict and manage. Traditional scaling techniques, which rely on consistent statistical properties across iterations, struggle to adapt to these sudden, extreme values. This unpredictability further complicates the task of maintaining numerical stability in FP8 training, especially as the scale of the dataset and the duration of training increase."}, {"title": "4 SWIGLU AND OUTLIER AMPLIFICATION", "content": "While the previous section highlighted the general problem of outlier emergence in large-scale FP8 training, our investigation reveals that the SwiGLU (Swish Gated Linear Unit) activation function plays a crucial role in amplifying these outliers. This section explores the structure of SwiGLU and demonstrates how its unique properties contribute to the generation and amplification of outliers."}, {"title": "4.1 SWIGLU STRUCTURE", "content": "The Transformer architecture (Vaswani et al., 2017), which forms the foundation of modern LLMs, has undergone several modifications to enhance performance and efficiency. One notable example is the inclusion of the SwiGLU (Swish Gated Linear Unit) (Shazeer, 2020b) activation function in models like LLaMA (Touvron et al., 2023) and PaLM (Chowdhery et al., 2022)."}, {"title": "4.2 THEORETICAL ANALYSIS OF WEIGHT CORRELATION IN SWIGLU", "content": "Next, we analyze the behavior of the SwiGLU neuron during training and show its weight vectors tend to align perfectly if the magnitude of its input increases above some threshold. This causes the SwiGLU output magnitude to increase significantly during training, potentially resulting in outliers.\nTo show this, We assume the SwiGLU neuron is embedded in a neural network with k parameters. The rest of the parameters in the network are denoted by \u03b8 \u2208 Rk\u22122d. We train the neural network with some l2 regularization,\nminw1,w2,\u03b8\u2211Nn=1ln(SwiGLUw1,w2(xn(\u03b8)),\u03b8)+\u03bc2(||w1||2+||w2||2)   (1)\nwhere \u03bc > 0 is regularization strength, N is the number of training samples, and ln(un,\u03b8) is the per-sample loss as a function of the SwiGLU output and the rest of the neural network parameters. We find that\nTheorem 1. Suppose we converge to a stationary point (w1,w2,\u03b8) of the loss function, and for all samples n, \u03c3\u2032(xn(\u03b8)Tw2)\u21920. Then, at this stationary point, w1\u2192w2 or w1\u2192\u2212w2.\nProof. At a stationary point (w1,w2,\u03b8) we have \u2200i\u2208{1,2}:\n\u2211Nn=1\u2202wln(SwiGLUw1,w2(xn(\u03b8)),\u03b8)+\u03bcwi=0\nUsing the chain rule we have:\n0=\u2211nxnTw2\u03c3\u2032(wT2xn)\u03b4\u03b7+\u03bcw1\n0=\u2211nxnT1(\u03c3(wT2xn)+\u03c3\u2032(wT2xn)\u03c3\u2032(wT2))\u03b4\u03b7+\u03bcw2\nNote where we defined \u03b4n(w1,w2,\u03b8)\u225c\u2202ln(un,\u03b8)\u2202un|un=SwiGLUw1,w2(xn(\u03b8))\nand with a slight abuse of notation, we suppressed the dependence of (w1,w2,\u03b8) and xn(\u03b8) on \u03b8. Given the assumption\n\u2200n:\u03c3\u2032(wT2xn)\u21920\nat this limit we obtain\n0=\u2211nxnT2\u03c3(wT2xn)\u03b4\u03b7+\u03bcw1;0=\u2211nxnT1\u03c3(wT2xn)\u03b4\u03b7+\u03bcw2\nNow, defining \u03bbn=\u2212\u03bc\u22121\u03b4\u03b7\u03c3(wT2xn), the above equations become:\nw1=\u2211n\u03bbnxnT2=A w2; w2=\u2211n\u03bbnxnT1=A w1\nwhere A=\u2211n\u03bbnxnxnT is a symmetric matrix. This implies\nw1=A w2=A2w1;w2=A w1=A2w2   (2)"}, {"title": "4.3 OBSERVING WEIGHT CORRELATION GROWTH AND TRAINING INSTABILITY", "content": "In our experiments, we observed a clear relationship between the increasing correlation of the weight matrices w\u2081 and w2 and the eventual divergence in FP8 training loss.\nThe process of weight alignment and its impact on FP8 training develops precisely as our theory predicts. As training progresses, input magnitudes in certain channels grow, eventually exceeding a critical threshold. When this occurs, weight vectors w\u2081 and w2 in these channels begin to align rapidly. Figure 2b illustrates this evolution: initially, the correlation between w\u2081 and w2 is low, but it increases drastically between 125B and 210B tokens. Simultaneously, we observe a significant increase in the norm of these weights. This combination of high correlation and increased norm creates ideal conditions for generating extreme activation values, or \"spikes.\u201d\nThese activation spikes, in turn, lead to the divergence of FP8 training, as shown in Fig. 2a. Importantly, while we primarily observe strong positive correlations in this example, our theory also predicts the possibility of strong negative correlations. We also observe these, as can be seen in Fig. 7 in the Appendix."}, {"title": "4.4 SMOOTH-SWIGLU", "content": "As demonstrated earlier, the SwiGLU activation function can lead to outliers in the input of the last linear layer of the MLP component. These outliers pose a significant challenge when using FP8 precision with delayed scaling, which relies on the assumption that statistical properties remain consistent across layers. The sudden spike in value caused by SwiGLU disrupts this continuity, leading to instability in the training process. In Fig. 3 we demonstrate that disabling the quantization of the last linear layer in the MLP component (output of SwiGLU) allows Llama2 FP8 to successfully converge with large datasets, addressing the previously observed divergence issues.\nWhile disabling quantization of the SwiGLU output effectively prevents divergence, it reduces the potential acceleration benefits of FP8. To maintain full FP8 acceleration while addressing the outlier problem, we propose a novel modification called Smooth-SwiGLU. Figure 4 illustrates the key idea behind Smooth-SwiGLU: applying a scaling factor to the linear branch of the SwiGLU function and rescaling it back after the last linear layer. This approach prevents outliers in the quantization of the input to the last linear layer while preserving the overall function of the SwiGLU activation, enabling us to fully leverage FP8 precision throughout the network.\nMathematically, we express the quantized Smooth-SwiGLU function for each channel i as:\nSmooth-SwiGLU\u02c6w1,\u02c6w2,\u02c6w3,s(x)=si\u22121\u22c5Q(si\u22c5(\u02c6w1,iQ(x))Swish(\u02c6w2Q(x))))   (3)"}, {"title": "5 FP8 OPTIMIZER", "content": "The Adam optimizer and its variants are widely used in deep learning due to their effectiveness in handling various training challenges. A key characteristic of the Adam optimizer is its storage of two moments, traditionally in high precision (FP32). This significantly increases memory usage, particularly for large-scale models. While previous research Peng et al. (2023) has shown the feasibility of reducing the first moment to FP8 precision, they retained the second moment in FP16. Our work pushes the boundaries further by successfully quantizing both moments to FP8, significantly improving the optimizer efficiency for large language models."}, {"title": "CHALLENGES", "content": "The Adam optimizer uses two moments to adapt learning rates for each parameter:\n1. The first moment is an estimate of the mean of the gradients.\n2. The second moment is an estimate of the uncentered variance of the gradients.\nA critical aspect of the Adam optimizer is the use of the inverse square root of the second moment in the parameter update step. This operation has important implications for precision requirements.\nDue to this inverse square root operation, the smallest values in the second moment become the most significant in determining parameter updates. This characteristic creates a unique challenge when considering precision reduction for the second moment."}, {"title": "METHODOLOGY", "content": "We conducted extensive experiments to determine the optimal FP8 formats for both moments. Our investigation revealed that different precision requirements exist for each moment:\n1. First Moment: The E4M3 format (4 exponent bits, 3 mantissa bits) provides sufficient precision. This format offers a good balance between range and accuracy for representing the mean of the gradients.\n2. Second Moment: The E5M2 format (5 exponent bits, 2 mantissa bits) is necessary. This format provides a higher dynamic range, crucial for preserving information about the smallest values in the second moment. The additional exponent bit ensures that we can accurately represent both very small and moderately large values, which is critical given the inverse square root operation applied to this moment.\nIn our experiments, presented in Fig. 5 we show that while the first moment is able to converge with E4M3, the second moment requires a wider dynamic range and is able to converge only with E5M2 format. In Table 1 we compare the proposed quantization scheme for the optimizer moments with the one presented in Peng et al. (2023). Our scheme shows, for the first time, the ability to quantize both moments with standard FP8 formats."}, {"title": "6 EXPERIMENTS", "content": "We conducted extensive experiments to evaluate the effectiveness of our proposed FP8 training method for Large Language Models (LLMs) across various scales."}, {"title": "6.1 EXPERIMENTAL SETUP", "content": "Model and Dataset. We used the Llama2 model (Touvron et al., 2023) as our baseline. This model is a decoder-only Transformer (Brown et al., 2020) with pre-normalization RMSNorm (Zhang & Sennrich, 2019), SwiGLU activation function (Shazeer, 2020b), and rotary positional embeddings (Su et al., 2024). We trained the models on the open-source Red Pajama dataset (Computer, 2023) for 2 trillion tokens, maintaining hyperparameters consistent with Touvron et al. (2023).\nHardware. All training was conducted on 256 Intel Gaudi2 devices."}, {"title": "6.2 RESULTS", "content": "Training Stability. In Fig. 6 we show the training loss of Llama2 over 300B tokens with the proposed scheme, which include the use of Smooth SwiGLU (Section 4.4) + FP8 quantization of both Adam moments (Section 5). Notice we are able to overcome the divergence point of standard FP8 training. Additional training steps will be added in the next version of the manuscript. The FP8 model was trained using the standard format (Micikevicius et al., 2022) which include E4M3 for the forward phase and E5M2 for the backward phase.\nZero-shot Performance. Table 2 compares the zero-shot performance (accuracy and perplexity) on downstream tasks between the BF16 baseline and our FP8 model with Smooth-SwiGLU and FP8 optimizer. The results demonstrate that our FP8 approach achieves on-par performance with the BF16 baseline across all tested metrics.\nPerformance gains. Table 3 presents the performance of different configurations on Intel Gaudi2 hardware. While full FP8 quantization achieves the highest acceleration (~37%), it leads to training divergence (as shown in Figure 2a). Disabling quantization for the w3 layer enables convergence (Figure 3) with a ~27% speedup. Our proposed Smooth-SwiGLU scheme not only converges with results on par with the BF16 baseline (Figure 6) but also delivers a substantial ~34% acceleration."}, {"title": "7 CONCLUSIONS", "content": "In this paper, we successfully demonstrated FP8 training on datasets up to 2 trillion tokens, significantly exceeding the previous limit of 100 billion tokens Peng et al. (2023), with on-par results with the BF16 baseline. Importantly, we discovered that earlier FP8 training attempts were not long enough to reveal critical instabilities caused by outliers. Through both analytical methods and simulations, we showed that these outliers emerge over time, particularly in extended training runs. Our investigation revealed that the SwiGLU activation function amplifies these outliers, destabilizing FP8 training in large-scale scenarios.\nTo address this issue, we applied per-channel quantization to the SwiGLU activation function, a technique we refer to as Smooth-SwiGLU. Although identical to SwiGLU in function, this method effectively reduces outlier amplification, ensuring stable FP8 training with a moderate effect on model performance during training, and without any effect on the inference. Additionally, we introduced the first implementation of FP8 quantization for both Adam optimizer moments, further optimizing memory usage.\nOur proposed method, combining Smooth-SwiGLU and FP8 optimizer moments, achieved comparable performance to BF16 baselines on downstream tasks while providing significant throughput improvementsd. This approach successfully overcome the divergence challenges typically encountered in standard FP8 training on large datasets."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 TRAINING INSTABILITY - ADDITIONAL DATA", "content": null}]}