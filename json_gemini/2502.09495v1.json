{"title": "Cracking the Code: Enhancing Development finance understanding with artificial intelligence", "authors": ["Pierre Beaucoral"], "abstract": "Analyzing development projects is crucial for understanding donors' aid strategies, recipients' priorities, and to\nassess development finance capacity to adress development issues by on-the-ground actions. In this area, the\nOrganisation for Economic Co-operation and Development's (OECD) Creditor Reporting System (CRS) dataset is a\nreference data source. This dataset provides a vast collection of project narratives from various sectors\n(approximately 5 million projects). While the OECD CRS provides a rich source of information on development\nstrategies, it falls short in informing project purposes due to its reporting process based on donors' self-declared\nmain objectives and pre-defined industrial sectors. This research employs a novel approach that combines Machine\nLearning (ML) techniques, specifically Natural Language Processing (NLP), an innovative Python topic modeling\ntechnique called BERTopic, to categorise (cluster) and label development projects based on their narrative\ndescriptions. By revealing existing yet hidden topics of development finance, this application of artificial intelligence\nenables a better understanding of donor priorities and overall development funding and provides methods to\nanalyse public and private projects narratives.", "sections": [{"title": "Introduction", "content": "As development projects become increasingly complex and diverse, and public opinion\nregarding aid in both donor and recipient countries becomes more critical, there is a growing\nrecognition of the need for advanced methodologies to analyse and categorize them, ensuring\ntransparency and accountability of those projects (Honig and Weaver 2019). Traditional\nmethods of analysis frequently rely on predetermined variables, simplified categorisations, or\nfixed labelling systems. Thus, they may struggle to capture the intricacies embedded in project\nnarrative descriptions, paradigm shifts, or new schemes that emerge over time. Especially if they\nare not detected and implemented in advance. This work aims to address the aforementioned\nlimitation by introducing a new and easily replicable framework. Machine Learning (ML),\nspecifically Natural Language Processing (NLP), is used to extract significant patterns directly\nfrom project narratives in the OECD CRS dataset, a commonly used data source in development"}, {"title": "Methodology and motivations", "content": "This work builds upon the research conducted by Toetzke, Banholzer, and Feuerriegel (2022),\nwhich aimed to be replicated. However, due to significant advancements in the ML NLP field and\ntechnologies (Johri et al. 2021), it was deemed more effective to enhance the original concept.\nThe objective of this previous work was to use NLP techniques to create accurate clusters of aid\nactivities and label their topics based on the narratives declared in the dataset. The authors were\nable to classify projects into almost 180 categories, going beyond sector or objective-based\nclassifications, such as Rio markers. In the replication process, the classification appears to be\nunconvincing since relying on metric (silhouette score) to assess the optimal number of clusters.\nHowever, the highest silhouette score obtained for a fixed number of clusters seemed to be poor\n(around 0). Yet, in this work, the use of an alternative clustering algorithm, which will be\nelucidated in the subsequent sections, enabled the identification of over 400 distinct and\nanalysable thematic clusters, allowing a more refined analysis of development finance. The\nclustering process also seems to benefit from improvements thanks to theses changes, reflected\nby the different metrics used to assess for the clustering quality. This work entails several steps\nthat are common in natural language processing (NLP), including data collection, cleaning, and\npreprocessing, before employing machine learning (ML) techniques."}, {"title": "Data Collection", "content": "The study relies on the OECD CRS dataset, a comprehensive collection of development projects,\nincluding data about the donor country, implementing agency, channel of delivery, recipient\ncountry, amount committed and disbursed yearly on the project and project narratives. The\ndataset spans various sectors, providing a representative sample of public development finance\nfor analysis. The sample comprises projects declared from 1973 to 2022, totalling approximately\n5 million observations. The OECD data portal provides free access to the raw yearly datasets.\nThe OECD gathers, computes, and updates information on development projects each year based\non declarations from donor and recipient entities. This study will focus on three main variables:\nthe project title, short description, and long description, which will be concatenated in order to\nobtain one \"raw text\" variable."}, {"title": "Original method from Toetzke, Banholzer, and Feuerriegel (2022)", "content": "It is found to be necessary to explain the original methodology which inspired this study, in order\nto get a better understanding of the changes implemented and their effects on the outputs."}, {"title": "The idea", "content": "The primary objective is to group projects into homogeneous clusters using project descriptions.\nTo achieve this, it is necessary to code each project description, identify themes, and classify\nproject descriptions according to these themes. Thus, in terms of the first main output, our work\ndoes not necessarily differ from that of Toetzke, Banholzer, and Feuerriegel (2022)."}, {"title": "The original model from Toetzke, Banholzer, and Feuerriegel (2022)", "content": "In their work, they used Doc2Vec, a distributed Bag of Words version of Paragraph Vector model\n(Le and Mikolov 2014). The goal of the algorithm is to create a numeric representation of a\ndocument, regardless of its length. The vectors that represent similar documents will be closed\npoints in the created vector space. In the end, this algorithm allows to represent all the\ndocuments D in a high-dimensional vector space. The authors' original method trains the model\nthree times: once on a large corpus of Wikipedia articles redacted in English (approximately\nrepresenting 20 gigabytes of compressed data), once on project descriptions, and once on both\nsimultaneously."}, {"title": "Their clustering process", "content": "In their original article, they use K Means Clustering. K-Means Clustering is a method of vector\nquantization from signal processing. It aims to partition n observations into k clusters, with each\nobservation belonging to the cluster with the nearest mean (cluster centers or cluster centroids)\nserving as a prototype of the cluster. It is beneficial to note that this method necessitates the\nmanual specification of k, which can prove to be challenging in identifying the optimal number\nof clusters. Once done, the data space is partitioned into Voronoi cells. A Voronoi cell is a part of\na voronoi diagramm where each point from this cell is closer from a given point belonging to the\ncell (here those given points are our centro\u00efds) than another given point from other Voronoi\ncells. The within-cluster variances (squared Euclidean distances) are minimized by k-means\nclustering. However, it does not optimize regular Euclidean distances. The mean optimizes\nsquared errors, whereas only the geometric median minimizes Euclidean distances."}, {"title": "The output", "content": "Using this unsupervised method, the authors have identified 173 activity clusters, which is an\nimprovement compared to conventional aid activity classifications as it allows a freer\ndistribution of projects within categories that are not defined a priori. Therefore, it is necessary\nto determine if the quality of the clustering is sufficient enough for the output to be used. In this\nkind of set-up, it may represent a huge challenge as clusters are created ex nihilo, meaning we\ndo not possess any counter factual to compare the clustering with. To assess the quality of the\nclustering process, it is then needed to get a testable definition of a good clustering. Another\nchallenge with K-means clustering is that it forces each observation (here, project description)\nto be in a cluster. However, some generic projects with non-specific descriptions may be\ndescribed as \"noise projects\" and may not help to get an idea of donors' concerns when providing\naid."}, {"title": "What is a good clustering?", "content": "A definition of a good clustering could be a situation where (i) each object is assigned to only one\ncluster, (ii) each cluster is clearly defined and does not overlap with others, (iii) there are high\n(low) intra (inter)-class similarity within clustered objects. The first part of the definition is\ninsured by the use of the method of K-means. Indeed, K-mean clustering forces each processed\ndocument into only one existing cluster. However, in k-means, the desired number of cluster is\nfixed by human and may not be optimal. In this case, it might create overlap between clusters\nand create sub-optimal intra and inter similarity within clusters and documents (project\ndescriptions)."}, {"title": "Assessing the quality of clustering process", "content": "How can we assess the quality of clustering? As an unsupervised machine learning technique,\nwe cannot compare our results to the 'ground truth' as it does not exist. Therefore, there are two\nremaining solutions: compare machine learning results with human classification (as suggested\nby Toetzke, Banholzer, and Feuerriegel 2022) or conduct statistical tests to ensure the three\nmain characteristics of a good clustering process mentioned above. In the second case, three\nmetrics may be mentioned: Silhouette score, Davies-Bouldin and Calinski-Harabasz Index"}, {"title": "Silhouette score", "content": "The silhouette score (Rousseeuw 1987) is one of the most commonly used metric for assessing\nclustering quality. The Silhouette Coefficient is defined for each sample and is composed of two\nscores:\n\u2022\ta: The mean distance between a sample and all other points in the same class:\n$a(i) = \\frac{1}{|I_k|-1} \\sum_{j \\in I_k, j \\neq i} d(x^i, x^j)$\n\u2022\tb: The mean distance between a sample and all other points in the next nearest cluster:\n$b(i) = \\min_{k' \\neq k} \\frac{1}{|I_{k'}|} \\sum_{j \\in I_{k'}} d(x^i, x^{j})$\nWhere $d(x^i,x^j)$ represents the dissimilarity between our two individuals and $I_k = i \\in [ [1,N]]/C(i) = k$ represents all the points belonging to a group k issued from a cluster C(i). The\nSilhouette Coefficient s for a single sample is then given as:\n$s(i) = \\frac{b(i) \u2013 a(i)}{\\max(a(i), b(i))}$\nThe Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient\nfor each sample:\n$S = \\frac{1}{K} \\sum_{k=1}^{K} \\frac{1}{|I_k|} \\sum_{i \\in I_k} s(i)$\nThe value of this score is defined between \u22121 and 1. It is generally considered as good if the\nscore is above 0,5."}, {"title": "Davies-Bouldin Index", "content": "The Davies-Bouldin index (Davies and Bouldin 1979) calculates the average \"similarity\u201d\nbetween clusters. To do so, it measures the distance between clusters compared to their size.\nPractically, it is an average of the similarity Rij of each cluster $C_i$ and its most similar one $C_j$ :\nThe index is defined as:\n$DB = \\frac{1}{K} \\sum_{i=1}^{K} \\max_{i \\neq j} R_{ij}$,"}, {"title": null, "content": "with $R_{ij} = \\frac{S_i+S_j}{d_{ij}}$ , $s_i$ representing the cluster diameter, measured as the average distance between\neach point of the cluster i and the centroid of that same cluster, and $d_{ij}$ representing the distance\nbetween clusters' i and j centroids.\nThe closer to zero this index is, the better the partition."}, {"title": "Calinski-Harabasz Index", "content": "Also known as the Variance Ration Criterion (Calinski and Harabasz 1974) is the ratio of the sum\nof between-clusters dispersion and of within-cluster dispersion for all clusters (where\ndispersion is defined as the sum of distances squared) for a set of data E of size $n_E$ clustered in\nk clusters:\n$S = \\frac{\\frac{tr(B_k)}{k}}{\\frac{tr(W_k)}{n_E-k}}$\nWhere tr(Bk) and tr(Wk) are respectively the trace of the between and within group dispersion\nmatrix as:\n$W_k = \\sum_{q=1}^{k} \\sum_{x \\in C_q} (x - c_q) (x - c_q)^T$\n$B_k = \\sum_{q=1}^{k} n_q (c_q - c_E)(c_q-c_E) ^T$\nWith $C_q$ the observations within the cluster q, $c_q$ the center of cluster q and $c_E$ the center of E.\nThe higher the index is, the denser and well separated the clusters are."}, {"title": "About these metrics", "content": "The three scores presented above are initially designed for convex clusters resulting from\nclustering techniques such as K-means. For Density-Based clustering such as HDBSCAN, the\nvalues of these scores will automatically seem \u201clower\u201d (Moulavi et al. 2014). In the set-up that is\nabout to be presented, this downward bias may be seen as an advantage as it might be\ninterpreted as the minimum estimated quality of our clustering process."}, {"title": "A custom model to classify development finance activities", "content": "Now that we have investigated the motivation and origin of this work, we can delve into the new\nmethodology deployed in itself, including the data cleaning and preprocessing and the model\nused."}, {"title": "Data cleaning and preprocessing", "content": "One of the major challenges in this work is the treatment of \u2018raw data' (i.e. textual descriptions\nof projects) specifically the preprocessing of text data which involves translation, cleaning,\ntokenisation, and stemming. These steps are performed to reduce noise around the information\nencapsulated in the 'raw data' in order to enhance the quality and relevance of extracted\nfeatures. Despite attempts to harmonize project description declarations, projects are declared\nheterogeneously depending on the declaring agency, their reporting habits, and the types of\nimplemented projects. As the model used is from the family of transformers model (Vaswani et\nal. 2017), it is not necessary to realize any type of preprocessing to obtain a good understanding\nof the \"raw data\u201d variable. In fact, bi-directional transformer models are capable of\ncontextualizing each document, allowing them to gain quality in natural language processing.\nUnlike previous models, they are less sensitive to noise and stop-words (e.g. \u201cor\u201d, \u201cand\u201d, \u201cif\u201d, etc.),\nexploring the nuances embedded in these words. What sets transformers apart is their attention\nmechanism, which allows them to weigh the relevance of different parts of the input data when\nmaking predictions. This mechanism enables transformers to capture long-range dependencies\nin sequences, making them particularly effective for tasks involving sequential data like\nlanguage translation, text generation, sentiment analysis, and more."}, {"title": "Machine Learning Model used for Natural Language Processing", "content": "This section will be dedicated to the explanation of the global structure of the model\nimplemented in this work. The global structure of this work can be illustrated by Figure 1.\nAs said earlier, an unsupervised learning approach is adopted. The main part of the process\nrelies on the first step: text embedding, which is a multi-dimensional representation of words\nand their associations with other words. To do so, it is now common to use a BERT model (Devlin\net al. 2018). Especially, the model used in this study is BERTopic (Grootendorst 2022), a BERT-\nbased model specifically designed for quality clustering and topic modelling. BERTopic is\ndivided in several steps detailed in Figure 2."}, {"title": "Results", "content": "As depicted earlier, the main output of this work is a new and rearranged dataset with a thinner\nlevel of development project classification, determined by unsupervised machine learning\ntechniques. The proposed methodology, described in Figure 1, provides several practical\nimplications for policymakers, economists, and stakeholders involved in development planning.\nAutomated categorisation enables rapid analysis, leading to more informed decision-making\nand resource allocation. It also enables a more detailed analysis of development projects with\nfiner categories, leveraging the study of several finance for development topics such as climate\nfinance or poverty reduction. This new classification may reveal hidden patterns in project\nclassifications between and within donors and implementing agencies. It can be compared with\nalready existing sectors and thematic variables for a more diverse analysis. Another advantage\nof this work is that it breaks down the available data at a project level, allowing for\nreclassification and overcoming the micro/macroeconomics dichotomy in empirical analysis."}, {"title": "High level of detail clustering", "content": "The primary outcome of this work is a novel classification encompassing 406 distinct topics of\nthe projects listed in the OECD CRS dataset. To conserve the integrity of clusters, the model used\nfor clustering creates outlier projects. For transparency purposes, Figure 3 shows what\nrepresent the outliers in the final dataset. It can be observed that this clustering process is able\nto capture more than half of the development projects since their monitoring by the OECD.\nFurthermore, this algorithm is able to regroup projects with high semantic similarity and then\nextract and label the common theme of these newly created groups."}, {"title": "Discussion", "content": "This application of machine learning, especially natural language processing, may induce several\npossible applications in monitoring global development finance, including the following\nexamples. Yet, the proposed method includes limitations one should consider."}, {"title": "Possible applications", "content": "As explained earlier, the main goal of this work is to provide valuable insights concerning\ndevelopment finance and aid patterns and to leverage the possibilities of studies concerning\ndevelopment finance topic, going from very specific to larger ones. The HDBSCAN approach,\nwhich will be described in the methods section, guarantees this modularity thanks to its\nhierarchical clustering technique, allowing aggregation of clusters at different levels for more\nglobal subjects such as global climate or health financing. It also allows micro-level analysis of\naid such as studying the great apes' conservation projects presented in Figure 6. The HDBSCAN\nmodel, as it is suited for high-density data setup, also allows and even fosters to use project-level\ndata, enabling in the case of this work dyadic analysis as studying the Syrian emergency Aid\nflows as displayed in Figure 7. Last, the topic modelling process allows to study trends in Aid\nwithin differents topics. To illustrate, these differents scenario will be developed in this section."}, {"title": "A donor analysis application: climate finance-related activities", "content": "To illustrate this purpose, we can move on to testing its implications for one or several\ndevelopment themes. Let's assume we are researchers in climate economics and we want to\nhave an idea of global climate finance. As a policymaker or researcher assessing the overall level\nof development finance committed and disbursed for climate and environmental purposes, it\nmay be necessary to select only projects dealing with those themes. Figure 4 in extended data\nshows an example of the number of unique project descriptions from the climate and\nenvironment-related cluster. The importance of these descriptions can be interactively\ndetermined over time on the following website: https://pierrebeaucoral.github.io/project/crs-\nml/topics_over_time_visualization.html."}, {"title": "A recipient analysis application: Great Apes conservation development projects", "content": "In this second application, one could desire to study the effects of dollars spent in one or another\n\"niche\" or very specific field of development finance. Then it will be needed to identify a specific\ntopic and the areas where projects from this topic are located. Here, the application selected is\nthe \"Great Apes Conservation Parks\u201d Cluster. Then one can map the location of these projects\naccording to the number of projects per countries in order to get an idea of the area of study.\nThe results of such idea can be highlighted in Figure 6."}, {"title": "A dyadic analysis application: Studying Syrian humanitarian aid flows", "content": "Then, one can also want to study relationship between donor and recipient countries for specific\ntopics. For example, we may wonder what could be the relative importance of a topic for donors.\nFor example, would the donor countries closest to Syria be encouraged to provide more support?"}, {"title": "Is aid a fashion victim?", "content": "Aid represents a complex and multifaceted subject that cannot be adequately captured through\nsimplistic sectoral categorizations. Such oversimplification risks obscuring the nuanced\nobjectives of Aid and may lead to a generalized portrayal that fails to recognize instances where\nAid extends beyond its traditional mandates\u2014such as eradicating hunger, fostering peace, and\nalleviating poverty\u2014toward more contemporary and trend-driven goals."}, {"title": "Policy implications", "content": "The application of advanced machine learning techniques like BERTopic enables policymakers\nand researchers to gain a granular understanding of development finance. This can help identify\ndiscrepancies or inefficiencies in aid allocation and promote transparency by categorizing\nprojects based on detailed narrative descriptions rather than self-declared objectives. Such\ntransparency is critical for maintaining public trust in development finance initiatives. By\nrevealing hidden thematic clusters, this methodology aids in pinpointing underfunded or\nemerging areas within development finance. For instance, identifying niche topics such as\nLGBTQ rights or conservation projects can lead to more targeted and effective resource\nallocation, ensuring aid reaches sectors and populations with specific needs. The findings\nhighlight significant differences between donor-reported metrics (e.g., Rio markers) and the\nclassification derived from narrative descriptions. This can serve as a tool to verify donor"}, {"title": "Limitations", "content": "Even though this work deploys state of the art methodologies to get a better description of the\ntopics of interest in development finance strategies across time, it might show some limitations\nconcerning the \u201craw material\u201d of this analysis, the methodology use and the insights of this work\nin terms of volume of allocated aid.\nThe quality of textual description of projects has improved across the period of this study,\nhowever it remains heterogeneous in terms of length and details between implementing\nagencies and across time. Even by using a performing multilingual BERT model for embedding,\nthis heterogeneity may reduce the quality of embedding and then the ability of precise clustering\nby the HDBSCAN model. One way of addressing this difficulty would be to compute the same\nanalysis by getting the project descriptions available on the donors websites. It will improve the"}, {"title": "Concluding remarks", "content": "This work presents an innovative application of machine learning and natural language\nprocessing techniques to categorise development projects based on narrative descriptions. The\nuse of the OECD CRS dataset, which is a well-known reference in development finance data\nsources, demonstrates the feasibility and effectiveness of the proposed approach. The findings\ncontribute to the ongoing discourse on innovative techniques in economics, machine learning,\nand project analysis. This work also allows for a better understanding of all development\nassistance, from its geographical to its thematic distribution. Although this work provides\nvaluable insights, its methodology can also be applied in other fields of science. The use of text\nembeddings and classification methods can be of interest to several fields, including behavioural\nsciences for sentiment analysis, labour and international trades and geopolitics studies for\ncontract classification, as well as historical or human sciences research for creating quantitative"}]}