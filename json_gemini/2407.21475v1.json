{"title": "Fine-grained Zero-shot Video Sampling", "authors": ["Dengsheng Chen", "Jie Hu", "Xiaoming Wei", "Enhua Wu"], "abstract": "Incorporating a temporal dimension into pretrained image diffusion models for video generation is a prevalent approach. However, this method is computationally demanding and necessitates large-scale video datasets. More critically, the heterogeneity between image and video datasets often results in catastrophic forgetting of the image expertise. Recent attempts to directly extract video snippets from image diffusion models have somewhat mitigated these problems. Nevertheless, these methods can only generate brief video clips with simple movements and fail to capture fine-grained motion or non-grid deformation. In this paper, we propose a novel Zero-Shot video Sampling algorithm, denoted as ZS\u00b2, capable of directly sampling high-quality video clips from existing image synthesis methods, such as Stable Diffusion, without any training or optimization. Specifically, ZS\u00b2 utilizes the dependency noise model and temporal momentum attention to ensure content consistency and animation coherence, respectively. This ability enables it to excel in related tasks, such as conditional and context-specialized video generation and instruction-guided video editing. Experimental results demonstrate that ZS\u00b2 achieves state-of-the-art performance in zero-shot video generation, occasionally outperforming recent supervised methods.", "sections": [{"title": "1. Introduction", "content": "Generative Al has recently attracted substantial attention in the computer vision domain, especially with the advent of diffusion models [15, 40, 41, 43]. These models have demonstrated remarkable efficacy in generating high-quality images from textual prompts, a process referred to as text-to-image synthesis [10, 30, 35, 38, 52].\nAttempts have been made to extrapolate this success to video generation and editing tasks [9, 16, 17, 23, 39, 50]. This is achieved by integrating a temporal dimension into the existing image diffusion models. Despite the encouraging outcomes, these methods generally necessitate extensive training with a large corpus of image and/or video data. This requirement can be prohibitively costly and impractical for many users. Moreover, the disparity in training data between image and video datasets often leads to catastrophic forgetting of the image expert [20].\nTo mitigate the cost issue associated with video generation, Tune-A-Video [50] introduced a mechanism that adapts the Stable Diffusion model [35] to the video domain. This strategy significantly curtails the training effort to tuning a single video. However, the generative capabilities of Tune-A-Video are limited to text-guided video editing applications, rendering video synthesis from scratch unachievable.\nRecently, Text2Video Zero [19] and FateZero [25] have made progress in exploring the novel problem of zero-shot, \u201ctraining-free\u201d video synthesis. This task involves generating videos from textual prompts without the necessity for any optimization or fine-tuning. By utilizing pre-trained text-to-image models, it capitalizes on their superior image generation quality and extends their applicability to the video domain without additional training. However, these methods primarily generate brief video clips, typically consisting of a few frames, and lack effective control over content, particularly in terms of motion speed.\nThe fundamental premise of this work is the observation that continuous video sequences often exhibit substantial correlations within the noise (latent) space [11, 21]. In light of this, we propose an innovative noise initialization model, named the dependency noise model, which supersedes the traditional random noise initialization. To further enhance the continuity of sampled video content over longer segments, we incorporate a temporal momentum mechanism within the self-attention function. The amalgamation of these two techniques gives rise to a new sampling method, denoted as Zero-Shot video Sampling or ZS\u00b2 in brief. In comparison to image sampling algorithms such as DDIM [41], our proposed ZS\u00b2 video sampling algorithm incurs negligible additional computational overhead. It is straightforward to implement and can be effectively integrated with various sampling algorithms to produce satisfactory video segments. Furthermore, the applicability of our method extends beyond text-to-video synthesis, covering conditional and specialized video generation, as well as instruction-guided video editing.\nOur contributions can be encapsulated into the following three aspects:\n\u2022 We propose a novel zero-shot video sampling algorithm that enables the direct sampling of high-quality video segments from pretrained image diffusion models.\n\u2022 We present a dependency noise model and temporal momentum attention, which, for the first time, allow us to flexibly control the temporal variations in the generated videos.\n\u2022 We demonstrate the effectiveness of our method through a broad spectrum of applications, including conditional and specialized video generation, as well as video editing guided by textual instructions."}, {"title": "2. Background", "content": "Present text-to-video synthesis techniques either require costly training on large-scale text-video paired data [1], ranging from 1 million to 100 million data points [48] or necessitate fine-tuning on a reference video [50]. Our objective is to streamline and minimize the cost of video generation by approaching it from a zero-shot video synthesis perspective.\nFormally, given a text description 7 and a positive integer $m \\in \\mathbb{N}$, our aim is to construct a function F that generates video frames $V \\in \\mathbb{R}^{m \\times H \\times W \\times 3}$ (for a predetermined resolution $H \\times W$) that exhibit temporal consistency [19]. Crucially, the function F should be determined without the necessity for training or fine-tuning on a video dataset. A zero-shot text-to-video method inherently leverages the quality improvements of text-to-image models, thus avoiding the catastrophic forgetting of the image expert.\nIn this research, we address the zero-shot text-to-video task by utilizing the text-to-image synthesis capability of Stable Diffusion (SD) [34]. Given that our objective is to generate videos rather than images, SD should operate on sequences of latent codes. A direct approach is to independently sample m latent codes from a standard Gaussian distribution, apply DDIM [41] sampling to obtain the corresponding tensors $x_0^i$ for $i = 1,..., m$, and then decode to acquire the generated video sequence. However, this results in entirely random image generation that only shares the semantics described by T, but lacks object appearance and motion coherence.\nTo overcome these challenges, we propose to (i) incorporate a dependency noise model between adjacent latent codes to ensure consistency in object appearance, and (ii) devise a temporal momentum attention to maintain the motion coherence and identity of the foreground object. Consequently, we construct the Zero-Shot video Sampling (ZS\u00b2) algorithm by integrating these two techniques into the DDIM sampling methods, enabling the direct sampling of high-quality videos from SD and other diffusion models."}, {"title": "3. Dependency noise model", "content": "The image diffusion model is trained to eliminate independent noise from a perturbed image. The noise vector e in the denoising objective is sampled from an i.i.d. Gaussian distribution $\\epsilon \\sim \\mathcal{N}(0,I)$. However, after training the image diffusion model and applying it to reverse real frames from a video into the noise space on a per-frame basis, the noise maps corresponding to different frames exhibit high correlation [11, 21].\nIn this study, our goal is to explore the design space of noise priors and propose a model that is optimally suited for our video sampling task, which results in significant performance improvements. We represent the noise corresponding to individual video frames as $\\epsilon_1, \\epsilon_2, ...\\epsilon_m$, where $\\epsilon_i$ corresponds to the ith element of the noise tensor $\\epsilon$. PYoCo [11] has developed two intuitive noise models, namely, the mixed and progressive noise model, to introduce correlations among $\\epsilon_{1:m}$.\nThe Mixed noise model, also known as the residual noise model or individual noise model, has been utilized in [21] to expedite the convergence of the video diffusion model. In the mixed noise model, we generate two noise vectors: $\\epsilon_{shared}, \\epsilon_{ind} \\sim \\mathcal{N}(0,I)$. $\\epsilon_{shared}$ is a universal noise vector shared across all video frames, while $\\epsilon_{ind}^i$ is the individual noise per frame. The final noise is a linear combination of these two vectors: $\\epsilon^i = \\sqrt{\\alpha} \\epsilon_{shared} + \\sqrt{1 - \\alpha}\\epsilon_{ind}^i$.\nThe Progressive noise model, also known as the linear noise model, generates noise for each frame in an autoregressive manner, where $\\epsilon^i$ is produced by perturbing $\\epsilon^{i-1}$. Let $\\epsilon_0, \\epsilon_{ind}^i\\sim \\mathcal{N}(0, I)$ denote the independent noise generated for the first frame and ith frame. Then, progressive noising can be formulated as: $\\epsilon^i = \\sqrt{\\alpha}\\epsilon^{i-1} + \\sqrt{1 - \\alpha}\\epsilon_{ind}^i$.\nIn both models, the parameter \u03b1, ranging from 0 to 1, governs the extent of noise shared across different video frames. A larger \u03b1 signifies a stronger correlation among the noise maps corresponding to various frames. As \u03b1 approaches 1, all frames are imbued with identical noise, resulting in the creation of a static video. On the contrary, \u03b1 = 0 is indicative of independent and identically distributed (i.i.d.) noise.\nThe employment of mixed and progressive noise models for the training of a video diffusion model has demonstrated effectiveness, as evidenced in [11]. This approach enables the efficient learning of animation transitions between frames during the training process. However, as illustrated in the first section of supplementary material, despite the strong correlations induced among $\\epsilon_{1:m}$ by these two sampling techniques, their direct implementation in zero-shot video sampling is not feasible."}, {"title": "3.1. Dependency noise model", "content": "To generate more structured noise sequences, $\\epsilon_{1:m}$, that encapsulate animation more effectively, we propose a novel dependency noise model. This model employs KL divergence as a regulatory mechanism for the correlation between two successive frames.\nSpecifically, the model stipulates that for all $\\epsilon^i \\sim \\mathcal{N}(0, I)$, the KL divergence between $\\epsilon^i$ and $\\epsilon^{i-1}$ should approximate $\\lambda_i$. This requirement necessitates the minimization of the following objective function: $\\mathcal{L}(\\epsilon^i, \\epsilon^{i-1}, \\lambda_i) = ||KL(\\epsilon^i, \\epsilon^{i-1}) - \\lambda_i||_2^2$:\n$\\arg \\min_{\\epsilon_{1:m}} \\mathcal{L}(\\epsilon^i, \\epsilon^{i-1}, \\lambda_i), \\text{ s.t., }\\epsilon^i \\sim \\mathcal{N}(0, I) \\qquad (1)$\nfor $i \\in \\{2,...,m\\}$. Here, $\\lambda_i$ serves as a control parameter for the KL divergence between two consecutive frames. By adjusting $\\lambda_i$, we can more effectively regulate the rate of content changes between frames. When $\\lambda_i \\rightarrow 0$, all frames incorporate identical noise, resulting in a static video, a situation analogous to that of $\\alpha \\rightarrow 1$. Conversely, $\\lambda_i \\rightarrow \\infty$ corresponds to i.i.d. noise.\nRevisiting Eq. 1, given $\\epsilon^{i-1}$, $\\epsilon^i$ can be computed via $\\epsilon^i = \\epsilon^{i-1}e^{\\lambda_i}/\\exp(\\epsilon^{i-1})$, a derivation that originates from the definition of KL divergence. However, this analytical solution $\\epsilon^i$ does not necessarily adhere consistently to the constraint, i.e., $\\epsilon^i \\sim \\mathcal{N}(0, I)$. In fact, as the video sequence extends, this analytical solution tends to deviate significantly from the normal distribution, which results in the sampled noise being unable to generate valid content via diffusion models.\nAs illustrated in Algorithm 1, we propose a two-stage noise search algorithm, which is a departure from the conventional analytical solution.\nIn the first stage, referred to as the random search stage, we generate a set of independent noises by sampling from the normal distribution $\\mathcal{N}(0, I)$. The noise with the KL divergence closest to $\\lambda_i$ when juxtaposed with $\\epsilon^{i-1}$ is selected as the initial value of $\\epsilon^i$, represented as $\\tilde{\\epsilon}$.\nIn the subsequent stage, we aim to find a coefficient $\\alpha \\in [0, 1]$ that results in $\\epsilon^i = \\sqrt{\\alpha}\\epsilon^{i-1} + \\sqrt{1 - \\alpha}\\tilde{\\epsilon}$, thereby minimizing Eq. 1.\nAs demonstrated in the second section of supplementary material, our proposed two-stage algorithm effectively identifies the necessary noise sequence $\\epsilon_{1:m}$ within a limited number of iterations. Concurrently, the first section of supplementary material provides evidence that the dependency noise model, to a certain degree, exhibits superior regularity in comparison to the other two noise models."}, {"title": "4. Temporal momentum attention", "content": "To leverage the potential of cross-frame attention and employ a pretrained image diffusion model without necessitating retraining, FateZero [26] replaces each self-attention layer with cross-frame attention. In this setup, the attention for each frame is primarily directed towards the initial frame. A similar structure is also adopted in [19].\nIn more detail, within the original UNet architecture $\\epsilon_\\theta(x_t,\\tau)$, each self-attention layer receives a feature map $x \\in \\mathbb{R}^{h \\times w \\times c}$, which is then linearly projected into query, key, and value features $Q, K, V \\in \\mathbb{R}^{h \\times w \\times c}$. The output of the layer is computed using the following formula (for simplicity, this is described for only one attention head) [45]:\n$SA(Q, K, V) = \\text{Softmax}(\\frac{Q K^T}{\\sqrt{c}}) V$.\nIn the context of video sampling, each attention layer is provided with m inputs: $x_{1:m} = [x_1,...,x_m] \\in \\mathbb{R}^{m\\times h \\times w \\times c}$. As a result, the linear projection layers produce m queries, keys, and values $Q_{1:m}, K_{1:m}, V_{1:m}$, respectively. Therefore, we replace each self-attention layer with cross-frame attention, where each frame's attention is focused on the initial frame, as follows:\n$CFA(Q^i, K^{1:m}, V^{1:m}) = \\text{Softmax}(\\frac{Q^i (K^1)^T}{\\sqrt{c}}) V^1 \\text{ for } i = 1,..., m$.\nThe application of cross-frame attention aids in the transfer of appearance, structure, and identities of objects and backgrounds from the first frame to the subsequent frames. However, this method lacks the connection between adjacent frames, which could lead to significant variations in the generated video sequence, as depicted in Figure 3."}, {"title": "4.1. Temporal momentum attention", "content": "Our observations indicate that self-attention, due to its lack of inter-frame context, results in a more diverse set of sampled features. On the other hand, cross-frame attention relies solely on information from the initial frame. While this ensures the consistency of the sampled results, it also leads to a reduction in diversity.\nTo strike a balance between the distinct effects of self-attention and cross-frame attention, we introduce Temporal Momentum Attention (TMA). The mathematical representation of TMA is as follows:\n$\\text{TMA}(Q^i, K^{1:i}, V^{1:i}) = \\text{Softmax}(\\frac{Q^i(\\tilde{K}^{1:i})^T}{\\sqrt{c}}) V^{1:i} \\qquad (2)$\nThis applies for $i = 1,...,m$, where $\\tilde{K}^{1:i} = \\mu_i \\tilde{K}^{1:i-1} + (1 - \\mu_i)K^i$ and $\\tilde{K}^{1:1} = K^1$. The same definition applies to $V^{1:i}$.\nIt's clear that when all $\\mu_i$ values are set to 1, TMA becomes equivalent to cross-frame attention. Conversely, when all $\\mu_i$ values are set to 0, TMA becomes equivalent to self-attention. As illustrated in Figure 4, by suitably controlling the value of \u00b5, we can generate more optimal video sequences.\nEfficient Computation of $\\tilde{K}^{1:i}$. A straightforward approach to calculate the values of $\\tilde{K}^{1:i}$ would involve computing them individually using a for loop. However, to fully leverage the computational capabilities of the GPU, we propose the use of matrix operations to concurrently compute all $\\tilde{K}^{1:i}$ values. This method specifically requires the construction of an upper triangular coefficient matrix $U \\in \\mathbb{R}^{m \\times m}$. The vector of $\\tilde{K}^{1:i}$ is then obtained through a matrix multiplication operation as follows:\n$[\\tilde{K}^{1:1},..., \\tilde{K}^{1:i},..., \\tilde{K}^{1:m}] = [K^1, (1 - \\mu) K^2, ..., (1 - \\mu) K^m] U$"}, {"title": "5. Zero-Shot Video Sampling Algorithm", "content": "By incorporating the dependency noise model and temporal momentum attention, we have successfully sampled high-quality videos from the image diffusion model, leveraging the existing DDIM algorithm.\u00b9 This process is outlined in Algorithm 1.\nInterestingly, when the video sampling a single image, i.e., m = 1, the dependency noise model simplifies to a random noise model, and the temporal momentum attention simplifies to self-attention. This suggests that irrespective of the values assigned to \u03bb\u2081 and \u00b5i, the ZS\u00b2 sampling algorithm will consistently produce results identical to those of the original DDIM algorithm. This feature ensures the high compatibility of the ZS\u00b2 algorithm with various sampling algorithms and coding frameworks, eliminating the need for additional project maintenance.\nComparison with related works. Text2Video-Zero [19] and ZS\u00b2 are contemporaneous works, both aiming to develop innovative sampling methods for zero-shot video generation. However, Text2Video-Zero, to achieve satisfactory sampling results, incorporates motion dynamics in latent codes, necessitating additional DDIM backward and DDPM forward computations. To further ensure the continuity of the video background, it also employs a saliency detection method for background smoothing. This not only escalates the computational overhead but also complicates the algorithm implementation, thereby limiting its flexibility and applicability. In contrast, ZS\u00b2 offers a significant advantage in these aspects. Moreover, our experimental results demonstrate that the video clips sampled by ZS\u00b2 are noticeably superior to those generated by Text2Video-Zero."}, {"title": "6. Experiments", "content": "6.1. Implementation Details\nUnless otherwise stated, our primary diffusion model is Dreamlike Photoreal v1.0. In this model, all \u03bbi values are set to 0.01, and all \u00b5i values are configured to 0.98. Algorithm 1 includes a random search phase and a linear search phase, with the iteration count set to 10 and 15, respectively. In our experiments, we generate m = 8 frames, each with a resolution of 512 \u00d7 512, for every video clip. However, our framework is inherently flexible and can generate an arbitrary number of frames. This can be achieved either by increasing m or by using our method in an auto-regressive manner, where the last generated frame m is used as the initial frame for computing the subsequent m frames. All prompts used for generating video clips in each figure are provided in supplementary material."}, {"title": "6.2. Comprehensive comparison in text to video task", "content": "In this study, we provide an extensive comparison between our method and Text2Video-Zero, another zero-shot video synthesis method, from both quantitative and qualitative aspects. You can refer to supplementary material or our homepage for more sampled video clips with different diffusion models."}, {"title": "6.3. Extentions", "content": "The ZS\u00b2 algorithm exhibits excellent adaptability across various tasks. To illustrate this, we conducted conditional generation based on ControlNet [55], specialized generation based on DreamBooth [37], and implemented the Video Instruct-Pix2Pix task based on Instruct Pix2Pix [3].\nWe present the corresponding results in supplementary material and our homepage. It is evident from these figures that our algorithm can achieve satisfactory results in a variety of task contexts."}, {"title": "7. Conclusion", "content": "In conclusion, this paper presents ZS\u00b2, a pioneering zero-shot video sampling algorithm, specifically engineered for high-quality, temporally consistent video generation. Our method, which requires no optimization or fine-tuning, can be effortlessly incorporated with a variety of image sampling techniques, thereby democratizing text-to-video generation and its associated applications. The effectiveness of our approach has been substantiated across a multitude of applications, such as conditional and specialized video generation, and instruction-guided video editing. We posit that ZS\u00b2 can stimulate the development of superior methods for sampling high-quality video snippets from the image diffusion model. This enhancement can be realized by merely adjusting the existing sampling algorithm, thus eliminating the necessity for any supplementary training or computational overhead."}, {"title": "8. Related works", "content": "Text-to-Image Generation. The evolution of text-to-image synthesis began with early approaches that relied on techniques such as template-based generation [22] and feature matching [31], but these methods had limitations in generating realistic and diverse images. The advent of Generative Adversarial Networks (GANs) [12] led to the development of deep learning-based methods for text-to-image synthesis, such as StackGAN [54], AttnGAN [51], and MirrorGAN [27], which enhanced image quality and diversity through innovative architectures and attention mechanisms. The advancement of transformers [45] further revolutionized the field, with the introduction of Dall-E [29], a 12-billion-parameter transformer model that introduced a two-stage training process. This was followed by the development of Parti [53], which proposed a method to generate content-rich images with multiple objects, and Make-a-Scene, which introduced a control mechanism by segmentation masks for text-to-image generation. The current state-of-the-art approaches are built upon diffusion models like GLIDE, which improved Dall-E by adding classifier-free guidance, and Dall-E 2 [30], which utilized the contrastive model CLIP [28] to obtain a mapping from CLIP text encodings to image encodings. Other notable models include LDM / SD [35], which applied a diffusion model on lower-resolution encoded signals of VQ-GAN [8], and Imagen [38], which utilized large language models for text processing. Versatile Diffusion [52] further unified text-to-image, image-to-text, and variations in a single multi-flow diffusion model. While these models have significantly improved image quality, their application in the video domain is challenging due to their probabilistic generation procedure, which makes it difficult to ensure temporal consistency.\nText-to-Video Generation. Text-to-video synthesis, an emerging field of research, utilizes various methodologies for generation, often employing autoregressive transformers and diffusion processes. NUWA presents a 3D transformer encoder-decoder framework that supports both text-to-image and text-to-video generation [49]. Phenaki utilizes a bidirectional masked transformer with a causal attention mechanism, enabling the creation of arbitrarily long videos from text prompts [47]. CogVideo enhances the text-to-image model, CogView 2, by employing a multi-frame-rate hierarchical training strategy to better synchronize text and video clips [5, 18]. Video Diffusion Models extend text-to-image diffusion models, training concurrently on image and video data [17]. Imagen Video establishes a cascade of video diffusion models, leveraging spatial and temporal super-resolution models to generate high-resolution, time-consistent videos [16]. Make-A-Video builds on a text-to-image synthesis model, utilizing video data in an unsupervised fashion [39]. Gen-1 extends SD, proposing a structure and content-guided video editing method based on visual or textual descriptions of desired outputs [9]. However, these approaches are computationally intensive and require extensive video datasets. More detrimentally, the heterogeneity of the training data between image and video datasets often leads to catastrophic forgetting of the image expert.\nZero-shot Text-to-Video Sampling. Recently, to mitigate the substantial computational requirements of video generation models, the concept of zero-shot text-to-video generation has been introduced, where videos are sampled directly from image generation models without any additional training. This innovative task was first introduced in the work of Tune-A-Video [50], which proposed a one-shot video generation task by extending and tuning SD on a single reference video, albeit with training on a limited number of video sequences. Subsequent studies, such as Text2Video Zero [19] and FateZero [25], have made significant strides in this field, exploring the novel problem of zero-shot, \"training-free\" text-to-video synthesis. These methods build upon pre-trained text-to-image models, leveraging their superior image generation quality and extending their applicability to the video domain without additional training. However, they primarily generate brief video clips, usually consisting of a few frames, and lack effective control over content, particularly in terms of motion speed. To address these limitations, we propose ZS2, which uses a dependency noise sequence and temporal momentum trick to generate high-quality, more controllable long video sequences."}, {"title": "9. The advantage of dependency noise model", "content": "Refer to Fig. 7, Fig. 8 and Fig. 9."}, {"title": "10. Convergence", "content": "Refer to Fig. 10 for more details."}, {"title": "11. Guidance generation", "content": "Refer to Fig. 11 and Fig. 12."}, {"title": "12. Prompts in text to video synthesis.", "content": "Prompts in Fig. 6 of paper :\n\u2022 An astronaut is riding a horse.\n\u2022 A blue unicorn flying over a mystical land.\n\u2022 Anime girl looking through a window of stars and space, sci-fi.\n\u2022 Clownfish swimming through the coral reef.\nPrompts in Fig. 2 of paper :\n\u2022 A man is riding a bicycle in the sunshine.\n\u2022 A cat is wearing sunglasses and working as a lifeguard at a pool.\n\u2022 A cute cat running in a beautiful meadow.\n\u2022 A group of squirrels rowing crew.\n\u2022 A beautiful girl.\nPrompts in Fig. 1 of paper :\n\u2022 A bunch of colorful marbles spilling out of a red velvet bag. (Sampled from Dreamlike Photoreal v1.0)\n\u2022 A steaming basket full of dumplings. (Sampled from Dreamlike Photoreal v1.0)\n\u2022 Balloon full of water exploding in extreme slow motion. (Sampled from Stable Diffusion v1.4)\n\u2022 A beautiful girl. (Sampled from Dreamlike Photoreal v2.0)\nPrompts in Fig. 3 of paper :\n\u2022 A cat is running on the grass.\n\u2022 An astronaut is skiing down the hill.\n\u2022 A horse galloping on a street.\nPrompts in Figure 13 :\n\u2022 A beagle in a detective's outfit.\n\u2022 A bumblebee sitting on a pink flower.\n\u2022 A completely destroyed car.\n\u2022 A red pickup truck driving across a stream.\n\u2022 A steaming hot plate piled high with spaghetti and meatballs.\n\u2022 An adorable piglet in a field.\n\u2022 An extravagant mansion, aerial view.\n\u2022 A horse galloping on a street.\n\u2022 A video showcases the beauty of nature from mountains and waterfalls to forests and oceans.\n\u2022 An aerial view shows a white sandy beach on the shore of a beautiful sea.\n\u2022 There is a flying through an intense battle between pirate ships in a stormy ocean.\n\u2022 Balloon full of water exploding in extreme slow motion.\n\u2022 An astronaut is skiing down the hill.\n\u2022 An astronaut is waving his hands on the moon."}]}