{"title": "LM-PROTAC: a language model-driven PROTAC\ngeneration pipeline with dual constraints of structure and\nproperty", "authors": ["Jinsong Shao", "Qineng Gong", "Zeyu Yin", "Yu Chen", "Yajie Hao", "Lei Zhang", "Linlin Jiang", "Min Yao", "Jinlong Li", "Fubo Wang", "Li Wang"], "abstract": "The imperfect modeling of ternary complexes has limited the application of\ncomputer-aided drug discovery tools in PROTAC research and development. In this\nstudy, a language model for PROTAC molecule design pipeline named LM-PROTAC\nwas developed, which stands for language model-driven Proteolysis Targeting Chimera,\nby embedding a transformer-based generative model with dual constraints on structure\nand properties. This study started with the idea of segmentation and representation of\nmolecules and protein. Firstly, a language model-driven affinity model for protein\ncompounds to screen molecular fragments with high affinity for the target protein.\nSecondly, structural and physicochemical properties of these fragments were\nconstrained during the generation process to meet specific scenario requirements.\nFinally, a two-round screening was performed on the preliminary generated molecules\nusing a multidimensional property prediction model. This process identified a batch of\nPROTAC molecules capable of degrading disease-relevant target proteins. These\nmolecules were subsequently validated through in vitro experiments, thus providing a\ncomplete solution for language model-driven PROTAC drug generation. Taking Wnt3a,\na key tumor-related target, as a POI of degradation, the LM-PROTAC pipeline\nsuccessfully generated effective PROTAC molecules. The molecular distribution\nexperiments demonstrated the high similarity of the generated molecules to the original\ndataset, validating the generative model's effectiveness in accurately defining chemical\nspace. Molecular dynamics simulations confirmed the stable interactions between the\nPROTAC molecules and target proteins, while protein degradation experiments\nverified the efficacy of the generated PROTAC molecules in degrading target proteins.\nThe entire LM-PROTAC pipeline is reusable and can generate degraders for other\ntarget proteins within 50 days, significantly improving the efficiency of drug discovery\nfor undruggable targets.", "sections": [{"title": "Key words", "content": "Generative models, Artificial intelligence, Molecular generation, Targeted protein\ndegradation, Drug discovery pipeline"}, {"title": "Introduction", "content": "PROTAC hijacks the activity of E3 ubiquitin ligase to ubiquitinate the POI, leading\nto its degradation by the 26S proteasome and mediating the degradation of the POI.\nThis hijacking mechanism has been employed to degrade various types of disease-\nrelated POIs\u00b9. Over the past two decades, there has been continuous effort to target the\nUbiquitin-proteasome System (UPS) for therapeutic purposes. The approval of\nproteasome inhibitors by the U.S. Food and Drug Administration has demonstrated the\npharmacological potential of UPS, prompting further research and expansion into\nmanipulating this pathway for disease treatment. Unlike traditional small molecule\ninhibition principles, UPS can be artificially intervened through a Target based\ndegradation strategy to selectively target and degrade specific proteins. Currently, many\napproaches are being developed by researchers, including protein-targeting chimeras,\nAdPROMs\u00b2, biological PROTAC\u00b3, molecular glues, and selective estrogen receptor\ndownregulators 4, 5.\nIn the early development of PROTAC, the first successful case of protein\ndegradation mediated by PROTAC was achieved using pPROTAC6. However, due to\nissues such as the instability and poor cell permeability of pPROTAC itself as a peptide\nmolecule, the research focus gradually shifted towards small molecule PROTAC7-9. As\na result, the study of small molecule PROTAC in the field has gained more recognition\namong industry researchers. Al is experiencing robust growth. Among various Al\nmethods, generation models have gained considerable attention in recent years. In\naddition, NLP techniques offer new possibilities for the design of PROTAC molecules,\nparticularly when targeting proteins that lack well-defined binding pockets. Traditional\nsmall-molecule drug design relies on the binding pockets of target proteins, while NLP\ntechniques enable the identification of various binding sites by fragmenting molecules.\nThis technique has the potential to facilitate the design of efficient PROTAC molecules,\npotentially expanding the application of PROTAC. Inspired by these successful\ndevelopments, researchers are now applying generation model technology to de novo\ndrug design, considered to be the origin of drug discovery. From this perspective,\nvarious models such as Recurrent Neural Networks, Autoencoders, Generative\nAdversarial Networks, Transformers, and Hybrid Models with Reinforcement Learning\nhave demonstrated exceptional capabilities in various molecular generation tasks.\nConsequently, applying these de novo drug design approaches to the scenario of\nPROTAC drug generation has become an important application in the specialized field."}, {"title": "Molecular and protein encoding", "content": "Molecular representation is a crucial task in the molecular generation workflow.\nResearchers have constructed models for accurate molecular representation from\nmultiple dimensions and perspectives, including 1D10-15, 2D16-19, 3D20-23, and images24-\n26, and validated them through experiments. However, most attention has been given to\nmolecular representation methods based on atomic and bond structures, overlooking\nthe impact of interactions between molecular structural fragments on molecular\nproperties. Therefore, from the perspective of a language model, molecules are divided\ninto combinations of multiple fragments, focusing on the interaction relationships\namong these token fragments to identify those fragments that have a significant impact\non molecular properties.\nMolecular and protein sequences are analogous to full sentences in natural\nlanguage. Initially, molecules and proteins are segmented into fragments called S-mol\nand S-pro, representing Segment molecular and Segment protein, respectively. This\nsegmentation approach is inspired by the concept of \u201cSegment\u201d in NLP, simplifying\ncomplex molecular and protein sequences into relatively complete functional fragments.\nSubsequently, S-mol and S-pro are further divided into smaller units called T-mol and\nT-pro, derived from Token molecular and Token protein."}, {"title": "Structure constraint", "content": "Molecules are composed of S-mol, and S-mol represent the fundamental chemical\nstructures that exhibit various properties of the molecule. Research on molecular S-mols\nallows for the understanding of interactions between internal or intermolecular local S-\nmols. Interactions between molecules arise from the collective interactions among\nmolecular S-mols, making high-affinity S-mol a crucial source of molecular affinity.\nConstructing a molecule based on the generation of high-affinity S-mol has the potential\nto yield molecules with high affinity. Similar to natural language, molecular encoding\nas a chemical language relies on segmentation that adheres to specific chemical logic,\nwhich is a crucial prerequisite for a deeper understanding of molecular encoding.\nTherefore, successful segmentation methods applied to natural language can also be\napplied to molecular encoding segmentation.\nReinforcement learning is a target-oriented machine learning approach that takes\nenvironmental feedback as input and adapts to the environment. Its main idea is to find\nthe optimal behavioral strategy by interacting with the environment through trial and\nerror, mimicking the fundamental way humans or animals learn. The core principle of\nreinforcement learning is to learn a series of actions that guide the model to achieve its\ngoal or maximize its objective function. If an action by the agent leads to a positive\nreward from the environment, i.e., a reinforced signal, the tendency of the agent's\nsubsequent actions will be strengthened. Otherwise, the inclination of the agent to\nproduce such actions will weaken. This is consistent with the principles of classical\nconditioning in physiology. Structural constraints in molecular generation are achieved\nby reinforcing molecules that are generated with structures closer to the desired\nconstrained structure through repeated rewards in reinforcement learning.\nMolecular structures are constrained through the application of reinforcement\nlearning. To preserve the structural features of highly affine molecular S-mol in the\ngenerated molecules, reinforcement learning is employed by imposing constraints on\nthe similarity between the generated molecules and target S-pro. The model rewards\nthe agent with positive reinforcement each time a newly generated molecule is\nstructurally closer to the target S-pro and imposes penalties when the generated\nmolecule deviates from the target S-pro. The use of reinforcement learning avoids the\nlow degrees of freedom approach of directly connecting target S-pros, thus preventing\nlimitations on the model's ability to generate entirely new molecules. A structure-\nconstrained molecular generation strategy involves restricting the output molecules to\ncontain a specific skeleton or S-mol. Langevin et al. and Li et al. established generative\nmodels that output drug molecules with specific skeletons27, 28. These skeletons are\noften extracted from existing drugs with favorable biological properties. Several\nresearchers also developed skeleton-based generative models, learning to generate\nmolecules with specific S-mols29-32. However, structure-constrained molecular\ngeneration models often produce a large number of repetitive structures and molecules,"}, {"title": "Physicochemical Property Constraint", "content": "limiting the model's freedom by constraining the primary structure of the molecular\nskeleton. This results in the generation of numerous structurally similar molecules for\nthe same drug, thereby reducing the model's learning and generative capabilities for\nnew drugs. Therefore, considering multiple possible high-affinity S-pros in generative\nmodels and using them as a basis for molecular generation, along with the application\nof reinforcement learning, enables the generated molecules to retain the structural\nfeatures of the target S-pros while introducing variability based on fixed structural\ncharacteristics. This enhances the model's ability to generate diverse molecules.\nCurrently, a large number of drug generation models have been developed by\nresearchers, such as VAE33-38, GF39, 40, AAE41, and others. However, during the\ndevelopment of these models, a significant issue has gradually emerged. Most models\nonly focus on the biological activity of drugs to targets, with a few models considering\none or a few other drug properties, such as drug concentration, solubility, etc. When a\ncompound is gradually classified as a drug, a large number of drug properties need to\nbe considered. These drug properties contribute differently to the compound's\ndrugability. In many generative models, important drug properties such as water\nsolubility and lipid solubility are not taken into account. These crucial drug properties\nare often incorporated only in the subsequent screening of potential drugs, resulting in\nadditional costs during both the generation process and the later dry and wet\nexperiments for property screening. Therefore, it is necessary to consider some\nimportant properties as constraints during the drug generation process.\nThere are multiple reasons why people might be interested in discovering new\nmolecules. To integrate generative models into molecular design, it is essential to define\nthese various applications as specific problem statements. For example, molecules that\npossess a particular property X, may be discovered while certain constraints Y, are met.\nThe generated molecules are enhanced by adding property constraints, ensuring that the"}, {"title": "Molecular generation by language model", "content": "compounds are chemically valid and exhibit specific desirable properties, such as good\nsolubility, low toxicity, or high potency. Since it is impractical to experimentally\nvalidate each generated compound, it becomes necessary to train a property predictor\nto assess compound properties, also known as a QSAR model. The property predictor\nis trained on a separate molecular dataset labeled with their properties (e.g., IC50/EC50\nfor potency). When the training is completed, the property predictor is used to estimate\nwhether the generated molecules satisfy the given constraint conditions. In this way,\nthe generative model learns to generate compounds predicted by the property predictor\nto meet the constraints. This task is often considered a discrete optimization problem\nand can be addressed using reinforcement learning, Bayesian optimization, or genetic\nalgorithms. In reinforcement learning, a model is trained to maximize the expected\nreward based on the property predictor's output. Additionally, Bayesian optimization\nmethods can transform discrete optimization problems into continuous optimization\nproblems by learning continuous embeddings of molecules through methods such as\ntraining a VAE to map discrete molecules into a continuous embedding space. Another\nneural network is then trained to predict the chemical properties of the original\nmolecules from their continuous embedding vectors. Bayesian optimization is then\napplied in the continuous embedding space to find an embedding with the best\ncorrelation property scores. This optimal embedding is decoded by a decoder network\ninto a discrete molecule. Genetic algorithms solve discrete optimization problems by\nsearching for favorable compounds through molecular mutations. Genetic algorithms\nconsist of a set of mutation rules and a fitness function. The fitness function is a\nweighted interpolation of predicted property scores and penalty scores for long-lived\nmolecules. Additional penalty terms encourage the model to explore a diverse set of\nmolecules. New compounds are derived by applying mutation rules to existing\nmolecules. In each iteration, molecules with lower fitness scores are removed. Nigam\net al. applied genetic algorithms to design molecules with high logP scores, where the\nfitness function was parameterized as a neural network42. It is worth noting that while\nmany studies use logP as a convenient metric for method development, it is an artificial\ntask that cannot be directly tied to any practical application42.\nMolecular generation can be viewed from the perspective of natural language\nprocessing as the generation of language sequences. The Transformer model has\ndemonstrated state-of-the-art performance in natural language processing43, 44, and it\nhas recently found applications in the field of drug generation. The original version of\nthe Transformer consists of an encoder and a decoder, with a key feature being the\nattention mechanism that can capture long-range dependencies in sequences. Hybrid\nmodels, which combine deep generative models with reinforcement learning, have been\napplied to generate molecules from scratch biased towards desired properties38. The\nTransformer, being a sequence-based model, exhibits characteristics that are well-\nsuited for molecular representation, particularly in 1D sequence encoding similar to\nSMILES. The multi-head attention mechanism of Transformer, which focuses on long-\nrange dependencies, aligns with the nature of molecular sequences where distant\nsegments produce remote correlations. Introducing the Transformer in the\nrepresentation of compounds allows it to capture these distant correlations, considering\nthe interactions between segments that are far apart in the sequence. The unified\ninteractions among these local segments constitute the interactions between molecules.\nThese segments, representing local information in the molecule, serve as crucial carriers\nof information for achieving precise molecular representation. A rational splitting logic\nenhances the accuracy of functional information division and improves the reliability\nand stability for molecular generation based on S-mol structure constraints. The C-\nTransformer model is a conditional language generation model that originated from\nSalesforce's work on targeted writing45. Hou et al. introduced the Conditional\nTransformer into the field of molecular generation. By incorporating a conditional"}, {"title": "Methods and materials", "content": "Transformer model, they generated molecules that meet specified property\nrequirements, imposing constraints on molecular properties46.\nCompared to generated molecules by C-Transformer, traditional approaches, such\nas the Rosetta method, use physical energy functions and high-resolution models for\nprotein structure prediction and molecular design, providing high-accuracy results. This\nis particularly important for designing and optimizing protein-ligand interactions.\nRosetta is also versatile, as it can be used not only for protein-ligand docking but also\nfor protein design, protein structure prediction, free energy calculations, and small\nmolecular design, among other applications. Additionally, by simulating folding and\nenergy minimization, Rosetta can identify the most stable molecular conformations,\nthereby enhancing the affinity and stability of molecules. However, the computational\nprocess of Rosetta is complex and time-consuming, requiring significant computational\nresources and time, especially when dealing with large molecular systems. The\naccuracy of the predictions also depends on the quality of the input data; any\ninaccuracies in the input can affect the results. For large molecular systems or complex\nsetups, the computational difficulty and time costs of Rosetta increase significantly47-\n49\nAdditionally, molecular docking methods can rapidly screen a large number of\nmolecules to identify potential high-affinity ligands, making them suitable for initial\nscreening50. Compared to full physical simulations, docking methods are more\nstraightforward and faster, allowing for the processing of numerous molecules in a short\ntime. They have been widely applied in drug discovery, demonstrating their\neffectiveness. However, docking methods often simplify the physical and chemical\nprocesses of the system, which may lead to insufficient predictive accuracy.\nFurthermore, due to the simplified models, docking results may contain a significant\nnumber of false positives that require further experimental validation. The results of\ndocking are highly dependent on the initial conformations, which may cause some\nimportant binding modes to be overlooked51, 52.\nIn the field of drug design, traditional molecular generation and screening methods,\nsuch as virtual enumeration and scoring methods, have become foundational tools in\ndrug discovery. These methods help researchers identify potential drug molecules by\nexploring a vast number of possibilities in chemical space. However, as the complexity\nof drug design and the demands for precision increase, traditional methods have\nrevealed significant limitations in terms of their ability to accurately predict molecular\nproperties, assess synthetic feasibility, and handle the vast diversity of chemical\nstructures. To overcome these challenges, researchers have begun to explore more\nintelligent and efficient molecular generation methods that leverage advanced\nalgorithms and machine learning techniques to better address the evolving needs of\nmodern drug discovery.\nVirtual enumeration is a method that generates a large number of candidate\nmolecules by combining predefined chemical S-mol. The main advantage of this\napproach is its ability to systematically cover a wide chemical space, resulting in\nmillions of candidate molecules. However, a significant drawback of virtual\nenumeration is that the sheer number of generated molecules is overwhelming, with\nmost lacking ideal bioactivity. This method relies on subsequent high-throughput\nscreening techniques to identify a few potential candidate molecules, but this process\nconsumes substantial computational resources and may lead to inefficient screening.\nAdditionally, virtual enumeration typically does not take into account the specific\nphysicochemical properties of the molecules and the requirements of the target,\nresulting in a variable quality of generated molecules and further complicating the\nscreening process53.\nScoring is a commonly used subsequent step in virtual screening to evaluate the\nbinding affinity and pharmacological properties of candidate molecules with target\nproteins. The basis of scoring methods typically lies in molecular docking models or\nmachine learning-based predictive models, which score molecules by calculating the\ninteraction energy between them and the target protein. The advantage of scoring"}, {"title": "Dataset", "content": "methods is that they provide researchers with a quantitative standard to help them filter\nout molecules that may possess high bioactivity. However, the effectiveness of scoring\nmethods is highly dependent on the initial quality of the generated molecules. If the\nquality of the candidate molecules is low, it is difficult to identify truly effective drug\nmolecules, even if the scoring model is very accurate. Since the generation and scoring\nprocesses are separate steps, scoring methods often fail to fully utilize the information\nobtained during molecular generation, leading to the possibility that the final selected\nmolecules are not optimal54, 55.\nLanguage model-driven molecular generation methods may rapidly produce a large\nnumber of potentially bioactive molecules, greatly improving design efficiency. These\nmodels are efficient and flexible, capable of quickly adapting to different design tasks\nthrough training data, generating diverse molecular structures and exploring new\nchemical spaces that traditional methods find difficult to discover. Additionally,\nlanguage models can generate molecules with novel structures and potential bioactivity,\nproviding more options and possibilities for drug discovery. However, the molecules\ngenerated by these models may lack rigorous validation of their physical and chemical\nfoundations, necessitating further experimental verification to ensure their actual\neffectiveness. The performance of the models largely depends on the quality and\ndiversity of the training data; if the training data is insufficient or of low quality, the\neffectiveness of the generated molecules will also be impacted. The generated\nmolecules need further optimization and screening to yield actual high-affinity\ncandidates, which may require combining with other methods for subsequent\noptimization.\nIn this work, important functional proteins were targeted involved in disease\nprogression. The problem was approached through the lens of language generation\nmodels, with reinforcement learning applied to constrain the structure of generated\nmolecules. Additionally, the C-Transformer model was integrated to control the\nphysicochemical properties of generated molecules. By combining the constraints on\nS-mol structure and physicochemical properties, a comprehensive solution was\ndeveloped for the targeted generation of PROTAC small molecule drugs aimed at\nspecific protease targets. The feasibility of this approach was validated by using Wnt3a,\nan important early-stage target in liver cancer, as a case study.\nThe ZINC dataset was developed to bridge the gap between cheminformatics and\nbiology. The ZINC database team developed a suite of ligand annotation, purchasability,\ntarget, and biology association tools, incorporated into ZINC and meant for\ninvestigators who are not computer specialists. The new version incorporates over 120\nmillion purchasable drug-like compounds. ZINC links purchasable compounds with\nhigh-value compounds such as metabolites, drugs, natural products, and annotated\ncompounds from the literature. Compounds can be accessed based on the genes\nannotated to them and the primary and secondary target classes to which these genes\nbelong. It provides new analysis tools that are user-friendly for non-experts but have\nalmost no limitations for experts56. ZINC is freely available at the following website:\nhttp://zinc15.docking.org.\nBindingDB is a publicly accessible database released by the laboratory of Michael\nK. Gilson at the University of California, San Diego. It primarily collects non-covalent\nbinding affinity data between drug target proteins and drug-like small molecules57.\nResearchers can access non-covalent binding data for relevant molecules, thereby\nfacilitating drug development and the construction of binding prediction models. As of\nDecember 31, 2023, BindingDB's patent dataset includes, 6,765 patents, 1,059,214\nbinding measurements, 505,009 compounds, 2,578 target proteins, 9,728 assays.\nThe Davis dataset is a publicly available dataset primarily used for drug discovery\nand chemical biology research58. It includes binding affinity data between drugs and"}, {"title": "Solutions for PROTAC small molecule drug generation models", "content": "proteins. Researchers selected 68 drug compounds and 379 protein targets, using drug-\nprotein pairs with binding affinities less than 30 units as positive samples.\nThe Biosnap dataset is another publicly available dataset focused on bioinformatics\nand chemical biology research, specifically targeting the prediction of interactions\nbetween compounds and proteins. This dataset contains 13,741 compound-protein\ninteraction pairs, involving 4,510 different drugs and 2,181 protein targets.\nThe DUD-E database is a commonly used benchmark in structure-based virtual\nscreening for evaluating the performance of various methods59. It includes 22,886\nactive ligands and their affinities against 102 targets.\nAll datasets are subjected to experiments using five-fold cross-validation. The\ndatasets are divided into 80% training sets and 20% testing sets to ensure the stability\nand generalization ability of the model's performance. This partitioning method allows\nfor the effective utilization of the information in the datasets during model training,\nensuring consistent and reliable performance across different datasets.\nFigure 1 depicts the main workflow of the LM-PROTAC pipeline. Initially,\nmolecular and protein representations are obtained by segmenting molecules and\nproteins from the dataset, which are used to train the FOTF-CPI model60, as shown in\nFigure 1A. FOTF-CPI is a model designed for calculating the affinity between S-mols\nand S-pros. For the target protein Wnt3a, the model is employed to screen S-pro-S-mol\nInteraction Pairs, referred to as SSI pairs, with high affinity from the ZINC250 dataset,\nby calculating the global and local interaction relationships between T-mol and T-pro,\nas shown in Figure 1B. Figure 1C illustrates the construction of a PROTAC molecule\ngeneration model called DCT based on S-mol structure and physicochemical property\nconstraints. This model, relying on C-Transformer and Reinforcement Learning,\ngenerates PROTAC molecules with high target affinity and specific attributes. The\ngenerated molecules meeting the requirements undergo molecular property filtering\nusing a MDAM for further specific drug property requirements, resulting in potential\ncandidate PROTAC compounds. Wet experiments are conducted to validate the\ninhibitory effects of the compounds on the target as shown in Figure 1D."}, {"title": "Data preprocessing for compounds and proteins", "content": "Data preprocessing, as shown in Figure 1A, involves the preprocessing of protein\nand small molecule compound data, including data filtering and molecular sequence\nsegmentation.\nThe preprocessing of small molecule compounds begins with the selection of\ncompounds according to the following rules: 1) Compilation of the source dataset based\non the ZINC Clean Lead database; 2) Removal of molecules containing other\nelectronegative atoms besides carbon, nitrogen, sulfur, oxygen, fluorine, bromine, and\nhydrogen; 3) Selection of drug-like compounds with a molecular weight between 200\nand 600; 4) LogP (calculated using RDKit) ranging from -2 to 6. RDKit is used to\nconvert small molecules into a unique representation in Canonical SMILES format. The\nobtained SMILES are parsed and split, and the positions containing side chains in the\nmain chain are filled with the character 'R' to retain information about the side chains\nin the main chain. The positions where side chains are connected to the main chain are\nmarked with \"(\", preserving the topological information of the side chains. The split\nSMILES are concatenated in the order of the main chain and side chains, forming a\nstring-form SMILES. Subsequent steps are similar to protein splitting: the SMILES\nstrings are segmented into S-mol using the VOLT algorithm, and a dictionary of S-mols\nis constructed. S-mol with a frequency count below 5 are identified, and low-frequency\nmasking is applied to them. During the encoding process, each SMILES is treated as a\nsentence, and each small S-mol is considered a word in this sentence. All small molecule\nSMILES are sequentially encoded, and after pretraining with Transformer, embeddings\nfor each S-mol are obtained.\nThe preprocessing of protein involves concatenating the amino acid residue\nsequence according to the subunit order, thereby forming a complete amino acid residue\nsequence in that order. The residue sequence is segmented into S-pro using the VOLT\nalgorithm, and a dictionary of S-pros is constructed. S-pros with a frequency count\nbelow 5 are identified as low-frequency S-mols. During encoding, these S-mols are all\nrepresented by the same S-mol, a method called low-frequency masking. During the\nencoding process, the encoding is performed according to the appearance order of S-\npro in the dictionary. Here, each protein is treated as a sentence, and each S-pro is\nconsidered a word in this sentence. All proteins are sequentially encoded, and then\nprocessed through Transformer to obtain embeddings for each S-pro."}, {"title": "Segmentation of molecular and proteins", "content": "The VOLT algorithm is employed in this work to seg proteins and molecules.\nVOLT is a vocabulary learning method based on Optimal Transport theory. The\npurpose of VOLT is to determine suitable vocabulary segmentation for specific tasks\nby leveraging optimal transport techniques, thereby enhancing the model's ability to\nrepresent and understand vocabulary in NLP. The algorithmic process is outlined as\nfollows:\nStep 1, VOLT ranks all candidate S-mols based on the pre-generated S-mol\nfrequencies. For simplicity, VOLT typically employs S-mols generated by BPE, such\nas BPE-100k, as candidates.\nStep 2, All S-mols with probabilities are used to initialize the optimal transport\nalgorithm. At each time step, the vocabulary with maximum entropy can be obtained\nbased on the transport matrix. Due to the relaxation strategy included in the optimal\ntransport algorithm, situations of non-compliant transport may arise. Therefore, VOLT\nremoves S-mols with distribution frequencies less than 0.001.\nStep 3, Exhaustively explore all time steps, selecting the vocabulary that satisfies\nthe specified exponentiated search space as the final vocabulary.\nStep 4, Use a greedy strategy similar to BPE to encode the text. Segment the\nsentence into character-level S-mols. If the merged S-mol is present in the vocabulary,\ncombine two consecutive S-mols into one S-mol until no further merging is possible. S-\nmols outside the vocabulary will be segmented into smaller S-mols. If the S-mols\n\u201cCC(=O)NC\u201d and \u201cCC1=CN\u201d are two adjacent S-mols, and both are in the vocabulary,\nthe sequence formed by concatenating them will be combined into a new S-mol, namely,\n\u201cCC(=O)NC CC1=CN\u201d.\nIn order to ensure the acquisition of reasonable S-mol and to avoid including\nextremely small or excessively long S-mol in the S-mol library, further screening of\nSMILES S-mol was conducted based on the Chembridge approach. Supplementary\nTable S2 outlines the criteria used for filtering the S-mols from the Chembridge\nFragment Database during the construction of the S-mol library; S-mols meeting these\ncriteria were included in the library. Following the standards of the Chembridge\nFragment Database, this experiment eliminated meaningless S-mols and optimized the\nS-mol library to prevent the generation of invalid molecules caused by ineffective S-\nmols."}, {"title": "Screening for high SSI pairs", "content": "Extracting information from the protein-compound affinity library, the data is\npaired based on the embeddings obtained for S-pros and S-mol in the previous step,\ngenerating pairs composed of S-pro and S-mol. In the design of PROTAC molecules,\nNLP technology aids in identifying key binding sites through the segmentation analysis\nof target proteins and S-mols. Similar to word analysis in natural language, NLP\nprocesses these S-mol and generates efficient PROTAC molecules based on their\ninteraction relationships. This approach reduces dependency on clearly defined binding\npockets and expands the range of potential targets. The overall interaction between\nproteins and small molecule compounds is used to obtain local interactions, specifically\nthe affinity between S-pro and S-mol. The affinity values from the protein-compound\naffinity library are used to train the FOTF-CPI prediction model.\nStep 1 fragment-based sequence encoding and optimization, the residual sequences\nof proteins are segmented into S-pros using the VOLT algorithm, as shown in Figure\n2A. The SMILES sequences of small molecule compounds are parsed and cut, and the\ncut SMILES sequences are concatenated in the order of main chain and side chain to\nform a string representation of SMILES. The protein sequence string is segmented into\nS-mol using the VOLT algorithm. The preprocessed sequences of small molecule\ncompounds and protein sequences are randomly initialized as representations of small\nmolecule compounds and proteins, respectively, based on S-mol. Similar to NLP"}, {"title": "Molecular generation based on structural and molecular property\nconstraints", "content": "methods, both the sequences of small molecule compounds and protein sequences are\ntreated as complete sentences, with each S-mol considered as a word in the sentence.\nThe representations of small molecule compounds and proteins, encoded in order based\non fragments, are obtained separately after passing through an encoder. The entire\nnetwork is continuously optimized based on the prediction results and real labels, using\na combination of binary cross-entropy loss functions. The specific loss function is\nshown in the formula below:\nloss = $\\frac{1}{N} \\sum_{i=1}^{N} (l_{i} \\times log(y_{i}) + (1 - l_{i}) \\times log(1 \u2013 y_{i})$\ny\u1d62 is the predicted result value, and l\u2081 is the true label value.\nStep 2 global-local affinity feature fusion, the affinity relationship between\nfragments is obtained by multiplying the representations of small molecule compounds\nand proteins to generate a local affinity matrix as shown in Figure 2B. In order to avoid\nexcessively high affinity scores in the local affinity matrix, normalization is applied,\nresulting in a new normalized local affinity matrix. The affinity matrix is then\ntransformed through Softmax to obtain the affinity relationship matrix between each S-\nmol and different S-pros. Simultaneously, by transposing the affinity matrix and\napplying Softmax, the affinity relationship matrix between each S-pro and different S-\nmols is obtained. The product of the affinity relationship matrix and the representation\nof small molecule compounds yields the representation of small molecule compounds\nafter local fragment correction. Similarly, the product of the affinity relationship matrix\nand the representation of proteins yields the representation of proteins. The\nrepresentation of small molecule compounds after local fragment affinity attention\ncorrection is concatenated with the representation of small molecule compounds\nextracted under global attention correction in vector dimension, resulting in a mixed\nrepresentation of small molecule compounds. Similarly, a mixed representation of\nproteins is obtained. The concatenated mixed representation of small molecule\ncompounds and mixed representation of proteins, after passing through a fully\nconnected layer and global adaptive pooling, respectively, yield the representations of\nsmall molecule compounds and proteins after fusion of global and local features.\nStep 3 affinity prediction via fused feature optimization, concatenate the fused\nrepresentations of small molecule compounds and proteins in the fragment dimension\nto obtain affinity features as shown in Figure 2C. The obtained affinity features are then\nsequentially passed through a global adaptive pooling layer and an activation function\nlayer to obtain a pair of predictions for small molecule compounds and proteins. Based\non the predicted results and real labels, continuously optimize the entire network using\na binary cross-entropy loss function.\nIn this research, a Conditional Transformer is employed the physicochemical\nproperties of the generative model. Within the foundational model of the Conditional\nTransformer, constraints based on two attributes are introduced to guide the molecular\ngeneration process. This approach serves to constrain crucial properties during the\nmolecular generation process, enhancing efficiency. Simultaneously, reducing the\nnumber of introduced attributes helps avoid an excessive increase in model size due to\nan overabundance of parameters, thereby reducing both training and operational costs\nof the model.\nThe molecular generation model is based on the C-Transformer model. Similar to\nthe S-pro pretraining model, it involves molecular segmentation for encoding and\ntraining. The training data is derived from Chembl and ZINC, consisting of 250k\nselected drug-like small molecules. In the encoding and generation embedding process,\nthe Transformer is used to extract long-range interactions between compound atoms.\nThis C-Transformer generation model is trained using small molecule data with labeled\nproperties (LogP and LogSW). Throughout the model training iterations, the labeled\nproperties serve as one of the conditional encodings for the C-Transformer, while the"}, {"title": "Performance evaluation of generative models", "content": "molecular SMILES are used for structural encoding training. Simultaneously", "following\nparameters": "nloss = $\\frac{1}{n} \\sum_{"}]}