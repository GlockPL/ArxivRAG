{"title": "Multiple-Input Variational Auto-Encoder for Anomaly Detection in Heterogeneous Data", "authors": ["Phai Vu Dinh", "Diep N. Nguyen", "Dinh Thai Hoang", "Quang Uy Nguyen", "Eryk Dutkiewicz"], "abstract": "Outlier/anomaly detection plays a pivotal role in Al applications, e.g., in classification, and intrusion/threat detection in cybersecurity. However, most existing methods face challenges of heterogeneity amongst feature subsets posed by non-independent and identically distributed (non-IID) data. To address this, we propose a novel neural network model called Multiple-Input Auto-Encoder for Anomaly Detection (MIAEAD). MIAEAD assigns an anomaly score to each feature subset of a data sample to indicate its likelihood of being an anomaly. This is done by using the reconstruction error of its sub-encoder as the anomaly score. All sub-encoders are then simultaneously trained using unsupervised learning to determine the anomaly scores of feature subsets. The final Area Under the ROC Curve (AUC) of MIAEAD is calculated for each sub-dataset, and the maximum AUC obtained among the sub-datasets is selected. To leverage the modelling of the distribution of normal data to identify anomalies of the generative models, we then develop a novel neural network architecture/model called Multiple-Input Variational Auto-Encoder (MIVAE). MIVAE can process feature subsets through its sub-encoders before learning the distribution of normal data in the latent space. This allows MIVAE to identify anomalies that significantly deviate from the learned distribution. We theoretically prove that the difference in the average anomaly score between normal samples and anomalies obtained by the proposed MIVAE is greater than that of the Variational Auto-Encoder (VAEAD), resulting in a higher AUC for MIVAE. Extensive experiments on eight real-world anomaly datasets from different domains, e.g., health, finance, cybersecurity, and satellite imaging, demonstrate the superior performance of MIAEAD and MIVAE over conventional methods and the state-of-the-art unsupervised models, by up to 6% in terms of AUC score. Furthermore, experimental results show that the AUC obtained by MIAEAD and MIVAE is mostly not impacted as the ratio of anomalies to normal samples in the dataset increases. Alternatively, MIAEAD and MIVAE have a high AUC when applied to feature subsets with low heterogeneity based on the coefficient of variation (CV) score.", "sections": [{"title": "INTRODUCTION", "content": "UTLIER/ANOMALY detection plays an essential role in many applications, i.e., network security, medical diagnosis, and fraud detection [2], [3], [4], [5], [6], [7]. However, for non-independent and identically distributed (non-IID) data, detection methods face critical challenges by the coupling between data samples and the heterogeneity amongst feature subsets, deteriorating the accuracy of the detection engine [8], [9]. For non-IID data, the coupling problem likely exists between data samples or amongst features of a data sample [10], [11]. For example, when fraudsters steal a credit card, they may attempt multiple purchases before the card is locked. Consequently, these transactions are not independent because they are linked to fraudulent activity that started after the card was stolen. Additionally, the heterogeneous data presents differences in their distributions, heterogeneity of feature subsets, and non-identical distributions of data subsets [8]. For instance, IoT intrusion detection systems collect data from various resources, e.g., network traffic, log systems, and webpage content, resulting in the final feature space comprised of heterogeneous feature subsets. Therefore, anomaly features are more likely to be present within a feature subset rather than across all features of a sample. As illustrated in"}, {"title": "PROBLEM STATEMENT", "content": "Given a dataset of $N = n_a + n_b$ samples $X = \\{x^{(1)}, x^{(2)},...,x^{(N)}\\}$ with $x^{(i)} \\in R^d$, $i = \\{1,2,..., N\\}$, d is dimensionality of $x^{(i)}$. $n_a$ and $n_b$ present the numbers of abnormal and benign samples, respectively. The goal is to find $n_a$ abnormal samples from the dataset X. This can be achieved by learning a scoring functions: $s: X \\rightarrow R$ that assigns scores to data samples, such that $s(x^{(i)}) < s(x^{(k)})$ where $x^{(i)}$ represents a normal sample and $x^{(k)}$ represents an abnormal sample. Here, $i \\in \\{1,2,..., n_b\\}$ and $k \\in \\{n_b + 1, n_b + 2, ..., n_a + n_b\\}$."}, {"title": "AUTO-ENCODER FOR ANOMALY DETECTION (AEAD)", "content": "The AE architecture consists of two components, i.e., an Encoder and a Decoder. The Encoder uses the function"}, {"title": "VARIATIONAL AUTO-ENCODER FOR ANOMALY DETECTION (VAEAD)", "content": "To leverage a probabilistic latent space for capturing the data distribution of normal data, the authors in [23] used a Variational Autoencoder for Anomaly Detection (VAEAD). The Evidence Lower Bound (ELBO) for a single data sample $x^{(i)} \\in X = \\{x^{(i)}\\}^N_{i=1}$ as follows:\n$\\mathcal{L}_{VAE} = E_{q_{\\phi}(z|x^{(i)})} [\\log p_{\\theta}(x^{(i)}| z)] - D_{KL} (q_{\\phi}(z|x^{(i)})||p(z))$,\nwhere the first term in Eq. (2) is the expected log-likelihood of reconstructing the input $x^{(i)}$ from the latent space z, whilst the second term measures the KL-divergence between the posterior distribution $q(z|x^{(i)})$ and the prior distribution $p(z)$.\nIn practice, the encoder of VAEAD maps input data into the latent space as $e^{(i)} = f(x^{(i)}, \\phi)$, where $\\phi = (W_e,b_e)$ are weights and biases of the neural network. The mean $\\mu^{(i)} = f_{\\mu}(e^{(i)})$ and standard deviation $\\sigma^{(i)} = f_{\\sigma}(e^{(i)})$ represent the distribution of the input $x^{(i)}$ in the latent space. Unlike AEAD, the latent space in VAEAD aims to force the data samples to follow a distribution of the normal data. To achieve this, a latent vector $z^{(i,l)}$ is sampled from the learned distribution, defined as $z^{(i,l)} = \\mu^{(i)} + \\sigma^{(i)} \\odot \\epsilon^{(i,l)}$, where $\\epsilon^{(i,l)} \\sim \\mathcal{N}(0, I)$ and $l \\in \\{1, 2, . . ., L\\}$ represents the $l^{th}$ sampling iteration. The VAEAD aims to reconstruct the input $x^{(i)}$ at the output of the decoder, i.e., $\\hat{x}^{(i,l)} = f(z^{(i,l)}, \\theta)$, where $\\theta = (W_d, b_d)$ are weights and biases of the neural network of the decoder. To ensure that the reconstruction $\\hat{x}^{(i,l)}$ is as close to the original input $x^{(i)}$ as possible, VAEAD uses the KL-divergence to regularize the learned posterior"}, {"title": "THE PROPOSED APPROACH", "content": "Given a general problem described in 2.1, we aim to identify anomalies in a non-IID dataset by using feature subsets instead of all features. Let $X = \\{X^{(1)}, X^{(2)},... X^{(M)}\\}$ be a non-IID dataset, where $X^{(i)} = \\{x^{(i,j)},...,x^{(N,j)}\\}$ is a sub-dataset with dimensionality $d_j$, and $j = \\{1,..., M\\}$. Each $x^{(i,j)} \\in R^{d_j}$ represents the $i^{th}$ data sample of the $j^{th}$ sub-dataset $X^{(i)}$, and N is the number of data samples in $X^{(i)}$, where $1 < M < d$. $F^{(i)} = \\{f_{(j,1)},..., f_{(j,d)}\\}$ denotes the set of features of sub-dataset $X^{(i)}$. Note that, all sub-datasets $X^{(i)}$ have the same size, which is N. In cases where the number of samples in the sub-datasets differs, it is possible to use undersampling techniques to achieve a balance among these sub-datasets [30]. Let's have $X = \\{x^{(1)}, x^{(2)},...,x^{(N)}\\}$, where $x^{(i)} \\in R^d$ is the $i^{th}$ data sample of X. Here $d = \\sum_{j=1}^{M}(d_j)$ as the dimensionality of $x^{(i)}$, and $F = \\{f_1,..., f_d\\} = F^{(1)} \\cup F^{(2)}\\cup...\\cup F^{(M)}$ as a feature set of X. $x^{(i)} = x^{(i,1)}\\oplus ...\\oplus x^{(i,M)}$, where $\\oplus$ denotes the combination operator. The combination operator is implemented by concatenating M vectors, i.e., $x^{(i,j)}$, $j = \\{1, . . ., M\\}$. For example, if $x^{(i,1)} = \\{1,2\\},x^{(i,2)} = \\{3,4\\}$, and M = 2 then $x^{(i)} = \\{1,2,3,4\\}$. The goal of finding $n_a$ abnormal samples from the N samples of dataset X in subsection 2.1 is equivalent to simultaneously finding $n_a$ abnormal samples from all sub-datasets $\\{X^{(1)}, X^{(2)}, ..., X^{(M)}\\}$. One can apply anomaly detection methods/models on each sub-dataset $X^{(j)}$ and then use ensemble techniques to find the final anomaly detection model [15]."}, {"title": "The Proposed Multiple-Input Auto-Encoder For Anomaly Detection (MIAEAD)", "content": "To learn an anomaly scoring function $s: X \\rightarrow R$ based on AE in subsection 2.2 with sub-datasets $\\{X^{(1)}, X^{(2)},... X^{(M)}\\}$,\nwe need to create M separate AE models. This can lead to the problem of learning many models with inconsistent goals. To tackle this, the MIAE model was first presented in [22] to transfer the heterogeneous input with different dimensionality into a lower-dimensional space, which fa- cilitates classifiers. The representation data of MIAE are extracted from the bottleneck layer before being fed to classifiers. Note that, MIAE is a single model with only a loss function. Based on the MIAE architecture, we propose a novel anomaly detection model referred to as MIAEAD, as illustrated in Fig. 2. First, sub-data samples $x^{(i, 1)}, . . ., x^{(i,M)}$ are simultaneously put into sub-encoders of MIAEAD be- fore being transferred into a lower-dimensional space at the output of the sub-encoders, i.e., $e^{(i,1)}, e^{(i,M)}$. Sub- sequently, $z^{(i)}$ is calculated as: $z^{(i)} = e^{(i,1)}\\oplus ...\\oplus e^{(i,M)}$. Next, $z^{(i)}$ is put into the Decoder to obtain the $\\hat{x}^{(i)}$ at its out- put. Because the numbers of neurons of the Encoder and De- coder are equal, we can separate $\\hat{x}^{(i)}$ into $(\\hat{x}^{(i,1)},..., \\hat{x}^{(i,M)})$ where: $x^{(i)} = \\hat{x}^{(i,1)}\\oplus ...\\oplus\\hat{x}^{(i,M)}$. Here, dimensionality of $x^{(i,j)}$ and $\\hat{x}^{(i,j)}$ is equal to $d_j$. In this way, we can assign an anomaly score of a sub-data sample $x^{(i,j)}$ as the reconstruction error of the $j^{th}$ sub-encoder as:\n$s_{MIAEAD}(x^{(i,j)}) = \\frac{1}{d_j} \\sum_{t=1}^{d_j} |x_t^{(i,j)} - \\hat{x}_t^{(i,j)}|^2$.\nThe anomaly score of the sub-dataset $X^{(i)}$ is measures as:\n$s(X^{(i)}) = \\frac{1}{N d_j} \\sum_{i=1}^{N} \\sum_{t=1}^{d_j} |x_t^{(i,j)} - \\hat{x}_t^{(i,j)}|^2$.\nThe anomaly score of all sub-datasets, i.e., $X^{(1)}, X^{(2)}, ..., X^{(M)}$, is measures as:\ns(X) = \\frac{1}{N} \\sum_{j=1}^{M} (\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{t=1}^{d_j} |x_t^{(i,j)} - \\hat{x}_t^{(i,j)}|^2)$.\nAfter training the MIAEAD with a loss function $s(X)$, $s(i,j)$ is measured to determine $x^{(i,j)}$ as anomaly or benign sam- ple in the sub-dataset $X^{(i)}$. If $x^{(i,j)}$ is assigned as anomaly, data sample $x^{(i)}$ is assigned as anomaly in the final dataset X. Note that, unlike MIAE which reconstructs the input $x^{(i)}$ at its output $\\hat{x}^{(i)}$, MIAEAD tries to separately reconstruct sub-data sample $x^{(i,j)}$ at its output $\\hat{x}^{(i,j)}$. Therefore, MI- AEAD can detect anomalies by the sub-data sample $x^{(i,j)}$ instead of using the data sample $x^{(i)}$."}, {"title": "The Proposed Multiple-Input Variational Auto- Encoder For Anomaly Detection (MIVAE)", "content": "To leverage the effectiveness of MIAEAD in assigning anomaly scores to each feature subset of a data sample and distinguishing anomalies from the learned distribution of normal data in the latent space of VAEAD, we propose the Multiple-Input Variational Autoencoder for Anomaly De- tection (MIVAE) architecture/model, as illustrated in Fig. 3. Unlike the original VAE model, the Evidence Lower Bound (ELBO) for a single data sample $x^{(i)} = x^{(i, 1)} \\oplus...\\oplus x^{(i,M)}$ of MIVAE is formulated as follows:\n$\\mathcal{L}_{MIVAE} = \\frac{1}{LM} \\sum_{l=1}^{L} \\sum_{j=1}^{M}  E_{q_{\\phi}(z^{(i,l)}|x^{(i,j)})} [\\log p_{\\theta}(x_t^{(i,j)}| \\hat{x}_t^{(i,j,l)})]  -D_{KL} (q_{\\phi} (z^{(i,l)}|x^{(i,j)})||p(z^{(i,l)}))$.\nThe first term in Eq. (7) is the sum of the reconstruction loss over all $d_j$ dimensions of sub-data $x^{(i,j)}$, whilst the second term represents the KL-divergence between the pos- terior distribution $q(z^{(i,j)}|x^{(i,j)})$ and the prior distribution $p(z^{(i,j)})$.\nIn practice, MIVAE maps the sub-data $x^{(i,j)}$ into the latent representation $e^{(i,j)}$ by using the $j^{th}$ sub-encoder, i.e., $e^{(i,j)} = f(x^{(i,j)}, \\phi_j)$, where $\\phi_j = \\{W_e^{(e,j)}, b_e^{(e,j)}\\}$ are weights and biases of the $j^{th}$ sub-encoder. Next, $\\bar{e}^{(i)} = e ^{(i,1)}\\oplus ...\\oplus e^{(i,M)}$ is a combination of sub-data before $\\mu^{(i)} = f(\\bar{e}^{(i)}, \\phi_{\\mu})$ and $\\sigma^{(i)} = f(\\bar{e}^{(i)}, \\phi_{\\sigma})$ are calculated. $\\phi_{\\mu}$ and $\\phi_{\\sigma}$ are weights and biases of the neural networks, respectively. Like VAEAD, $z^{(i,l)} = \\mu^{(i)} + \\sigma^{(i)} \\odot \\epsilon^{(i,l)}$, where $\\epsilon^{(i,l)} \\sim \\mathcal{N}(0, I)$ and $l \\in \\{1,2, . . ., L\\}$. Finally, the decoder of MIVAE aims to reconstruct the input $x^{(i)}$ at its output,"}, {"title": "PERFORMANCE EVALUATION", "content": "This subsection discusses the performance of anomaly de- tection by using two anomaly score functions of VAEAD in Eq. (3) and MIVAE in Eq. (10) when they both use all features of $x^{(i)}$. This is done by accepting the hypothesis as follows:\nHypothesis 1. In anomaly detection using reconstruction error from Auto-Encoder (AE) variants, anomalies exhibit higher recon- struction errors compared to normal samples, i.e., $s_{AEAD}(x^{(i)}) < s_{AEAD}(x^{(k)})$, where $i = \\{1, ..., n_b\\}$ and $k = \\{n_b+1, ..., n_b + n_a\\}$ are the indices of the normal samples and anomalies in the dataset X, which can be separated by sub-datasets, i.e., $X = \\{X^{(1)}, X^{(2)}, ..., X^{(M)}\\}$."}, {"title": "Model Complexity", "content": "This subsection discusses the parameters used by the MIVAE model compared to the fundamental model, i.e., VAEAD. Let $\\{a_1d, a_2d, ..., a_Td\\}$ be the number of neurons of the hidden layers of the encoder of AE, T is the number of hidden layers of the encoder, and $0 < a_1, ...,a_T < 1$.\nLemma 1. Assuming that MIVAE and VAEAD have the same numbers of neurons at the tth and (t + 1)th layers, the number"}, {"title": "Training Evaluation", "content": "To discuss the convergence of MIVAE, we first show that MIVAE is an advanced version of VAEAD [23]. The key difference is that MIVAE processes feature subsets using sub-encoders and aims to reconstruct these separated fea- ture subsets simultaneously, rather than reconstructing all features together of the VAEAD. As observed in Table 3, $h_i^{(t)}$ is the activation of node i in the layer t of the encoder of VAEAD, whilst that of MIVAE for node i in the sub- encoder j is $h_{ij}^{(t)}$. $\\delta_k^{(t)}$ is error term for node k in layer t, and $g'(h_i^{(t)})$ is the derivative of the activation function applied to $h_i^{(t)}$. $\\odot$ is an element-wise operation. $d^{(t)}$ is"}, {"title": "EXPERIMENTAL SETUPS", "content": "To evaluate the performance of the anomaly detection mod- els, we define TP, TN, FP, and FN as True Positive, True Negative, False Positive, and False Negative, respectively. The Receiver Operating Characteristic (ROC) curve is a plot of the TP against the FP for different threshold values. Similar to [18], we use the area under the ROC curve (AUC) to evaluate the performance of anomaly detection models. To calculate the AUC, we use anomaly scores, i.e., $s(i)$, and the ground truth labels. This is because the anomaly scores can be used to create a binary classification that s(i) greater than a threshold is predicted as an anomaly, while others are labelled as normal. Note that, to calculate the AUC, we do not need to know the number of anomalies in the dataset. To further evaluate the performance of the anomaly detection models, we use three scores, i.e., F-score, Miss Detection Rate (MDR) and False Alarm Rate (FAR), where $MDR = \\frac{FN}{FN+TP}$ and $FAR = \\frac{FP}{FP+TN}$. To calculate F-score, FAR and MDR, we assign $n_a$ samples that have the largest anomaly scores as anomalies while others are predicted as normal labels. In addition, we measure the heterogeneity of each dataset by using the coefficient of variation (CV) [28], where $CV = 100 \\frac{\\sum_{t=1}^{d} \\sigma_t}{\\sum_{t=1}^{d} \\mu_t}$, $\\mu_t > 0$. Here, $\\sigma_t$, $\\mu_t$, and d represent the standard deviation, the mean of the $t^{th}$ feature, the dimensionality of dataset, respectively. A high CV indicates large variability compared to the mean, whilst a low CV indicates small variability around the mean. In other words, a high CV suggests that the dataset is highly heterogeneous."}, {"title": "Datasets", "content": "We evaluate the performance of anomaly detection models on eight datasets, i.e., cardio disease (M1), credit card fraud (M2) from Kaggle, Arrhythmia (M3), medical image Mam- mography (M4), network security NSLKDD (M5), satellite image (M6), Shuttle (M7), and Spambase (M8) and from the UCI repository [18]. The numbers of samples N, dimen- sionality d, numbers of anomalies $n_a$, numbers of benign samples $n_b$, and the ratio of the number of anomalies over"}, {"title": "Experimental Setting", "content": "We use grid search to tune the hyper-parameters of anomaly detection models, as observed in Table 6. For LOF, KDE [12], and OSVM [13], the hyper-parameters are n_neighbor, bandwidth, and $\\gamma$, respectively. n_neighbor is the number of nearest neighbours used to compute the local reachability density. We use the Gaussian kernel to set the KDE method, and the bandwidth is the bandwidth of the kernel. $\\gamma$ is an important hyper-parameter used for OSVM to determine the spread of the Gaussian kernel. Next, RDA uses $\\lambda$ to tune the level of sparsity of noise and outliers in the original dataset [16]. A small value of $\\lambda$ encourages much of the data to be isolated in the set of noise/outliers to minimize the reconstruction error while the large $\\lambda$ increases the reconstruction error of the AE. For RandNet, the number of models randomly generated to form an ensemble of autoen- coders is denoted as $N_{model}$ [15]. For DAGMM, we refer to the results from [18]. For AEAD in [14], the dimensionality of the latent space, $d_z$, is tuned. Similarly, for VAEAD in [23], we tune both the number of samples drawn L and the latent dimensionality $d_z$. In the case of AnoGAN [24], SkipGAN [26], and AAE [27], the hyperparameter $\\alpha$ is used to balance the generator's reconstruction error and the reconstruction error in the discriminator's latent space.\nTo conduct experiments on neural networks, i.e., AEAD, VAEAD, RDAE, RandNet, AnoGAN, SkipGAN, AAE, MI- AEAD, and MIVAE, we use Tensorflow and the Adam optimization algorithm to train the model [22]. The num- bers of batch size, epoch, and learning rate are 100, 5000, and $10^{-4}$, respectively. The number of sub-encoders used is $n_{sub\\_encoder}$, as observed in Table 6. Similar to au- thors of [22], the numbers of neurons of layers are the list $\\{d, 0.8d, 0.6d, 0.4d, d_z, 0.4d, 0.6d, 0.8d, d\\}$, where d is the dimensionality of the input data, $d_z = [\\sqrt{d}]+1$ is the number of neurons of bottleneck layer of the MIAEAD."}, {"title": "EXPERIMENT RESULTS", "content": "We present the main results of MIAEAD and MIVAE com- pared to other methods. First, we compare the AUC of MIAEAD and MIVAE with anomaly detection methods, i.e., LOF, KDE [12], SVM [13], AEAD [14], RandNet [15], RDA [16], and DAGMM [17]. As observed in Table 7, the AUC obtained by MIAEAD and MIVAE is significantly greater than other methods over eight datasets. In addition, the av- erage AUC obtained by MIAEAD and MIVAE is greater than those of the state-of-the-art anomaly detection methods, i.e., DAGMM and RandNet, by up to 4.3% and 6%, respectively. For example, MIAEAD and MIVAE achieve average of 0.866 and 0.883 in terms of AUC on the M3 dataset compared to 0.0.626, 0.464, 0.576, 0.732, 0.738, 0.614, and 0.823, for LOF, KDE, SVM, AE, RandNet, RDA, and DAFGMM, respectively. Three methods, e.g., LOF, KDE, and OSVM, report low AUC over eight datasets since these conventional methods are likely to suffer from high-dimensional prob- lems of non-IID data and a high value of heterogeneity of datasets (CV). Similarly, the RDA significantly depends on the hyper-parameter selected, i.e., $\\lambda$, causing low values of AUC. AEAD and RandNet present low values of AUC on the M5 dataset because their ratio of $\\frac{n_a}{n_b}$ is large.\nSecond, we compare the results of MIVAE to those of the generative models, including VAEAD, AAE, AnoGAN, and SkipGAN, as shown in Table 8. MIVAE uses feature subsets to identify anomalies based on Eq. (9). MIVAE achieves the AUC by selecting the maximum value from feature subsets corresponding to sub-encoders. In addition, AAE, AnoGAN, and SkipGAN obtain their AUC values by select- ing the maximum value from three anomaly score functions, i.e., the generator's reconstruction error, the discriminator's reconstruction error in the latent space, and the sum of both. In general, MIVAE achieves a greater AUC than the generative models. For example, the average AUC obtained by VAEAD, AAE, AnoGAN, SkipGAN, and MIVAE is 0.716, 0.808, 0.776, 0.883, and 0.898, respectively. These results highlight the superior performance of MIVAE over the other"}, {"title": "Model Analysis of MIAED and MIVAE", "content": "First, we compare the AUC obtained by MIVAE with that of generative models, such as VAEAD, AAE, AnoGAN, and SkipGAN when MIVAE uses all features to identify anomaly, as shown in Table 11. While MIVAE uses the anomaly function defined in Eq. (10), AAE, AnoGAN, and SkipGAN use the sum of the anomaly scores based on the generator's reconstruction error and the discriminator's reconstruction error in the latent space [24], [26], [27]. This means that MIVAE focuses on using all feature space instead of feature subsets to identify anomalies. The AUC obtained by MIVAE is significantly greater than that of the generative models. For example, MIVAE, VAEAD, AAE, AnoGAN, and SkipGAN achieve 0.716, 0.776, 0.745, 0.729, and 0.797, respectively, in terms of average AUC across eight datasets. This demonstrates that MIVAE effectively identifies anoma- lies by leveraging all feature space instead of only using feature subsets, as proven in subsection 4.1.\nSecond, we evaluate the AUC obtained by MIAEAD and MIVAE in two methods. In the first method, called MIAEAD-max and MIVAE-max, we calculate the AUC of each sub-encoder of the $j^{th}$ sub-dataset $X^{(i)}$ by using the list scores, i.e., $\\{s(1,1), . . ., s(N,j)\\}$ which bases on the Eqs. (4) and (9). The final AUC is obtained by taking the maximum value of the AUC of all sub-encoders. The second way is called MIAEAD-sum and MIVAE-sum which is measured by using the list scores based on Eqs. (6) and (10). As shown in Table 12, the AUC obtained by MIAEAD- max and MIVAE-max is significantly greater than that of MIAEAD-sum and MIVAE-sum. For example, MIAEAD- sum, MIAEAD-max, MIVAE-sum, and MIVAE-max achieve 0.715, 0.886, 0.797, and 0.883, respectively, in terms of av-"}, {"title": "Influence of Ratio of Anomalies by Benign Samples", "content": "We report the AUC obtained by anomaly detection methods as the ratio of anomalies to benign samples increases. We select the ratio of anomalies by benign samples $\\frac{n_a}{n_b}$ by the list $\\{0.005, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9\\}$. The experiments are tested on the M5 dataset with two methods, i.e., AE, MIAEAD, VAEAD, and MIVAE. The number of sub-encoders is 25. There are two scores, $\\delta$ and E are used. $\\delta$ is an average anomaly score of benign samples, whilst E is an average score of abnormal samples. Note that, for MIAEAD and MIVAE, $\\delta$ and E are calculated by sub-datasets. First, as observed in Fig. 5 (a), AUC obtained by AEAD decreases when the ratio $\\frac{n_a}{n_b}$ increases. In addition, the values of $\\delta$ are nearly unchanged, whilst the values of E decline the same that of the AUC. Interestingly, when the $\\frac{n_a}{n_b} < 0.2$, the values of $\\delta$ are greater than that of E. In contrast, $\\delta < E$ if $\\frac{n_a}{n_b} \\geq 0.3$."}, {"title": "Influence of Heterogeneity Score on Anomaly De- tection Performance.", "content": "We evaluate the influence of the heterogeneity score of the dataset, i.e., CV, on the performance of the anomaly detection model, as observed in Fig. 6. The CV and AUC are obtained from 15 feature subsets of the M5 dataset. We can draw four interesting results. First, the CV obtained by 15 feature subsets are different. This indicates that different feature subsets are heterogeneous, and features in a feature subset are also heterogeneous. Second, AUC obtained by MIAEAD and MIVAE has the same trend. Third, the CV and AUC are likely inverse. For example, for sub-encoder num- bers, i.e., 6, 7, and 8, the values of CV are high compared to"}, {"title": "Tuning Hyper-parameters of MIVAE", "content": "We present the performance of MIVAE on the hyper- parameter L, which is the number of samples drawn in the latent space, as observed in Fig. 7. The AUC obtained by MIVAE is low for L = 1, but stabilizes when L > 10. For example, the AUC remains unchanged at about 0.94 when L \u2265 5 on the M1 dataset. This result indicates the effectiveness of modelling the distribution of normal data in the latent space, where anomalies may deviate from normal samples."}, {"title": "Data Simulation of MIVAE", "content": "Fig. 8 uncovers the data simulation of MIVAE through the M8 dataset. The dataset M8 includes three sub-datasets: X(1), X(2), and X(3), which correspond to three sub- encoders of MIVAE. As observed in Fig. 8 (a).1, anomalies tend to distinguish from the normal samples, while both anomalies and normal samples overlap in Figs. 8 (a).2 and (a).3. The input data are transferred to the latent space e(1), e(2), and e(3) to uncover latent features, and anomalies likely distinguish from normal samples in three branches in Fig. 8 (b). Next, the data samples $z^{(i,l)}$ are shown in Fig. 8 (c), where anomalies may deviate from the Gaussian distribution of the normal data. Subsequently, Figs. 8 (d) and (e) present reconstructed data $\\hat{x}^{(i,j,l)}$ at the output of the decoder of MIVAE. We observe that anomalies tend to sit at the border of the Gaussian distribution. The data simulation helps explain why MIVAE can differentiate anomalies from normal samples by using sub-encoders to identify anoma-"}, {"title": "Convergence Analysis and Anomaly Detection Per- formance of MIVAE", "content": "To verify the convergence of MIVAE, we show the KL- divergence and loss functions (reconstruction errors) for each sub-encoder when MIVAE runs for 300 epochs on the M7 dataset. As observed in Fig. 9 (a), the MIVAE model converges after approximately 20 epochs. However, the AUC obtained by Branch 2 is 0.83, which is significantly lower than 0.98 of Branches 1 and 3, even though the loss function line graphs for Branch 2 and Branch 3 are nearly the same. This discrepancy arises because the loss function is measured by the average anomaly score of both normal samples and anomalies. To further explain, we plot the average anomaly score of normal samples $\\delta$ and anomalies E, as observed in Fig. 9 (b). The line graphs of $\\delta$ and E for Branch 2 are nearly overlapped, implying little difference between them. In contrast, the values of E for Branches 1 and 3 are greater than those of $\\delta$, resulting in a higher AUC. These results demonstrate the effectiveness of using feature subsets to identify anomalies. We discuss histogram of anomaly scores obtained by three branches, as illustrated in Fig. 9 (c). The mean of the anomaly score distribution for normal samples obtained from Branch 1 and Branch 3 is lower than that for anomalies, while the mean of the anomaly score distribution for normal samples and anomalies may be the same on Branch 2. This results in a higher AUC obtained by Branch 1 and Branch 3 compared to Branch 2."}, {"title": "CONCLUSIONS", "content": "This paper proposed a novel deep learning model called MI- AEAD for anomaly detection with non"}]}