{"title": "Tactic: Adaptive Sparse Attention with Clustering and Distribution Fitting for Long-Context LLMs", "authors": ["Kan Zhu", "Tian Tang", "Qinyu Xu", "Yile Gu", "Zhichen Zeng", "Rohan Kadekodi", "Liangyu Zhao", "Ang Li", "Arvind Krishnamurthy", "Baris Kasikci"], "abstract": "Long-context models are essential for many applications but face inefficiencies in loading large KV caches during decoding. Prior methods enforce fixed token budgets for sparse attention, assuming a set number of tokens can approximate full attention. However, these methods overlook variations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and calibration-free sparse attention mechanism that dynamically selects tokens based on their cumulative attention scores rather than a fixed token budget. By setting a target fraction of total attention scores, Tactic ensures that token selection naturally adapts to variations in attention sparsity. To efficiently approximate this selection, Tactic leverages clustering-based sorting and distribution fitting, allowing it to accurately estimate token importance with minimal computational overhead.\nWe show that Tactic outperforms existing sparse attention algorithms, achieving superior accuracy and up to 7.29\u00d7 decode attention speedup. This improvement translates to an overall 1.58\u00d7 end-to-end inference speedup, making Tactic a practical and effective solution for long-context LLM inference in accuracy-sensitive applications.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) power a wide range of applications, from conversational assistants to document analysis systems and search engines. The demand for multi-turn interactions and long-document processing has driven an expansion of context length, growing from thousands to as many as one million tokens (Liu et al., 2024b).\nHowever, supporting long contexts in LLM inference presents significant challenges, primarily due to the growing memory footprint of the Key-Value (KV) cache (Tang et al., 2024). The memory requirements of the KV cache scale proportionally with the context length, therefore, it can quickly become a bottleneck despite optimizations such as Grouped-Query Attention (GQA) (Ainslie et al., 2023). Furthermore, the need to repeatedly load the KV cache for every generated token becomes a bottleneck. For instance, loading the large KV cache can account for over 50% of the total latency during auto-regressive decoding, significantly impeding the efficiency of large-scale serving systems. (Tang et al., 2024)\nTo mitigate the high cost of KV-cache loading, recent methods approximate full attention by selecting a subset of stored Key and Value vectors, corresponding to a subset of tokens, within a fixed token budget (Liu et al., 2024a; Tang et al., 2024; Zhang et al., 2023; Xiao et al., 2023). These approaches exploit the natural sparsity of attention, where only a small fraction of tokens significantly influence the output due to the softmax operation. By leveraging this sparsity, they aim to reduce the overhead of loading the KV-cache without sacrificing model accuracy.\nAlas, existing fixed budget-based methods have several shortcomings. Some methods employ a global fixed token budget (Tang et al., 2024; Xiao et al., 2023; Zhang et al., 2023), not accounting for variations in attention sparsity across attention heads, and layers. In practice, some attention heads focus on significantly more tokens than others, and the level of sparsity fluctuates across layers. More adaptive methods (Cai et al., 2024; Feng et al., 2024; Ge et al., 2024) attempt to distribute token budgets more effectively using calibration data or predefined rules, but they remain constrained by static allocation and cannot adapt to query tokens and contexts, often leading to suboptimal approximations in different cases.\nTo address the limitations of fixed-budget-based methods, we propose Tactic, a sparsity-adaptive and calibration-free post-training sparse attention mechanism that improves both the accuracy and efficiency of long-context LLM inference. Fig. 1 shows a comparison between existing fixed budget-based methods and Tactic. Instead of enforcing a fixed budget, Tactic dynamically selects tokens starting from ones with the highest attention score to ensure that their cumulative attention scores (where attention score represents the softmax output of the Query-Key product) reach a target fraction of the full attention score.\nDynamic and selective accumulation of attention scores offers two key advantages. First, it provides inherent flexibility-Tactic selects fewer tokens in high-sparsity cases and more in low-sparsity cases without requiring calibration. Second, since attention scores are multiplied by V vectors with similar norms, and the selected attention scores cumulatively reach at least a fraction of the total attention score, a cumulative attention score target guarantees, unlike token budgets in prior works, a bounded difference between sparse and full attention (see Sec. 3.2 and App. A).\nHowever, efficiently selecting tokens to reach a certain fraction $P$ of cumulative attention score is challenging. To minimize the number of tokens selected (i.e., loads from memory), the optimal way is to select tokens following a descending order of attention score until the cumulative attention score surpasses $P$. Thus, similar to prior works, efficiently sorting tokens by their contribution to the cumulative attention score is crucial for Tactic. However, unlike fixed budget-based methods that simply stop at a fixed token count, Tactic must track cumulative attention score in real time, requiring the exact attention score values for each token, making the selection process more complex.\nTo approximate optimal token selection, Tactic introduces two key techniques: clustering and distribution fitting. First, to efficiently sort tokens, Tactic clusters similar tokens to reduce computational overhead. However, we observe that positional proximity, which is used for clustering tokens by prior work (Tang et al., 2024), does not necessarily guarantee similarity in Key vectors, which are fundamental to attention computation. Since attention operates on Query-Key interactions rather than token positions, Tactic groups tokens using K-means clustering based on Key-vector similarity (i.e., vector distance). During decoding, Tactic approximates the sorted list of tokens by sorting clusters based on the similarity between Query vectors and cluster centroids. After approximating token sorting, Tactic estimates the attention score for each token by leveraging the observation that attention scores follow a smooth distribution. Using distribution fitting, Tactic effectively keep track of attained cumulative attention score to determine the end of selection.\nBy loading only the cluster centroids along with a small sampled subset of tokens ($\\sim 2.5\\%$ of the KV cache size in practice), Tactic efficiently selects the most critical tokens that reach the target cumulative attention score. To balance efficiency and accuracy, Tactic performs full attention on newly generated tokens and updates the clustering every fixed number of decoding steps (e.g., 2048).\nOur experiments show that Tactic achieves superior and consistent accuracy compared to existing algorithms including Quest (Tang et al., 2024), PyramidKV (Cai et al., 2024) and Ada-KV (Feng et al., 2024), offering a more effective solution for long-context LLM inference in accuracy-sensitive applications. Tactic achieves up to 7.29\u00d7 decode attention speedup, which leads to 1.58\u00d7 end-to-end speedup.\nIn summary, we contribute the following:\n\u2022 A detailed analysis of the dynamic nature of attention sparsity across heads, layers, queries, and contexts.\n\u2022 Tactic, a sparsity-adaptive attention algorithm that uses clustering and distribution fitting to dynamically determine the token budget for achieving cumulative attention score targets.\n\u2022 A comprehensive evaluation of Tactic, demonstrating Tactic consistently achieves high accuracy and significant speedup."}, {"title": "2. Background", "content": ""}, {"title": "2.1. Large Language Models", "content": "LLMs consist of transformer blocks, each with an attention and a feed-forward module. In the attention module, input embeddings are projected into Query (Q), Key (K), and Value (V) vectors. For a given Q vector, attention weights $w_i$ for the i-th token are computed as $q^T k_i / \\sqrt{d}$ , then normalized via softmax to obtain attention scores. These attention scores are multiplies by the V vectors to produce the output (Eq. (1)). This result is then processed by the feed-forward module, with residual connections and layer normalization refining the final token representation."}, {"title": "2.2. Long-Context Models", "content": "The recent increasing demand for long-context models has driven advancements in extending the context window of LLMs. Techniques like Rotary Position Embeddings (Su et al., 2023) have enabled a significant expansion in context length, such as increasing LLaMA-2's context window to 32K in LongChat (Li et al., 2023) and 128K in Yarn-Llama-2 (Peng et al., 2023), and even 1 million recently (Liu et al., 2024b). However, as context length grows dramatically, loading the KV cache becomes an increasingly significant bottleneck during token generation, which can account for over half of the total decode time (Tang et al., 2024)."}, {"title": "3. Analysis", "content": "In this section, we first analyze the intrinsic sparsity in attention and how this sparsity affects downstream task performance (Sec. 3.1 & Sec. 3.2). We then present the drawbacks of existing fixed token budget approaches and propose cumulative attention score as the new target (Sec. 3.4 & Sec. 3.5). Finally, we illustrate the challenges of applying cumulative attention score and propose clustering and distribution-fitting techniques as the solution (Sec. 3.6 & Sec. 3.7)."}, {"title": "3.1. Intrinsic Sparsity in Self-Attention Mechanisms", "content": "In the decode phase, for one request, assuming there are $n$ previous tokens, the attention formula in Eq. (1) can be rewritten as\n$\\begin{aligned}\n    o &= \\sum_{i=1}^{n} s_i V_i,\n\\end{aligned}$\nwhere $s_i = \\frac{\\exp(\\frac{q^T k_i}{\\sqrt{d}})}{\\sum_{j=1}^{n} \\exp(\\frac{q^T k_j}{\\sqrt{d}})} $ \nFig. 2 shows that the distribution of $||v_i||$ for each token has a small variance. Thus, the contribution of the token is mostly determined by $s_i$. Due to the exponential term $exp(q^T k_i / \\sqrt{d})$, only a small subset of tokens has a significant impact on the model output (Zhang et al., 2023; Xiao et al., 2023), indicating that the attention operation in LLMs is inherently sparse, which motivates the possibility of only loading a subset of tokens to approximate the attention output and incur lower computational overhead."}, {"title": "3.2. Rethinking Attention Approximation", "content": "The sparse attention methods can be formulated as\n$\\tilde{o}(I) = \\sum_{i \\in I} s_i V_i, \\ \\ s_i = \\frac{\\exp(\\frac{q^T k_i}{\\sqrt{d}})}{\\sum_{j\\in T} \\exp(\\frac{q^T k_j}{\\sqrt{d}})}, $\nwhere $I$ is the index set of tokens selected by the sparse attention method, $I \\subset [n]$. The distance between $o$ and $\\tilde{o}(I)$ can be formulated as\n$\\epsilon(I) = ||o - \\tilde{o}(I)||$.\nIntuitively, a smaller distance between $o$ and $\\tilde{o}(I)$ reduces the difference between the output token distributions of sparse attention and full attention. We demonstrate this by measuring the Kullback-Leibler (KL) divergence, a statistical metric that quantifies the difference between probability distributions, on the output logits as a function of attention distance. Fig. 3a shows that decreasing attention distance consistently lowers KL-divergence, meaning the sparse attention output more closely resembles the full attention output. This alignment in token distributions also improves downstream task accuracy, as demonstrated in Fig. 3b for tasks in the RULER benchmark, which is widely used to assess the long-context abilities of a model.\nTherefore, the goal of sparse attention is to find an index set $I$ that minimizes $\\epsilon(I)$ under some constraint of $|I|$."}, {"title": "3.3. Fixed Token Budget Approaches Lead to Accuracy Variations", "content": "Several methods have been proposed to choose a small set of tokens $I$ minimizing the distance $\\epsilon(I)$ between full and approximate attention. Some of the work, including Quest (Tang et al., 2024), uniformly chooses tokens across attention heads and layers. These results in a large variance of $\\epsilon(I)$, as shown in Fig. 5. This variance stems from the intrinsic sparsity difference across heads and layers. As illustrated in Fig. 4a, attention heads exhibit distinct sparsity patterns. Some heads display a more uniform distribution of $s_i$ (retrieval heads), whereas others are dominated by a few high-magnitude $s_i$ values (streaming heads). When a fixed number of tokens $|I|$ is selected per head, it leads to inefficiencies\u2014allocating excessive tokens to streaming heads while introducing significant estimation errors in retrieval heads. Similarly, Fig. 4b highlights variation in sparsity across layers, where earlier layers exhibit lower sparsity compared to later ones, similarly making it inefficient to select a fixed number of tokens from different layers.\nMotivated by the diversity of sparsity patterns across heads and layers, some works, including AdaKV(Feng et al., 2024) and PyramidKV(Cai et al., 2024), fix the total budget $|I|$ but use calibration data or assumptions to statically assign different budgets to different layers and heads. However, as we show in Fig. 4c, the sparsity of particular heads varies significantly depending on the query token. For example, in the model output \"The Answer is ...\", the token \"Answer\" attends to far fewer tokens compared to \"is\". This is because \"Answer\" relies primarily on local information to generate \u201cis\u201d, whereas \u201cis\u201d requires broader context to produce the subsequent answer. Thus, relying on static partitioning of a fixed token budget also falls short of maintaining a consistent low attention distance $\\epsilon(I)$."}, {"title": "3.4. Cumulative Attention Score: A More Robust Target for Sparse Attention", "content": "The key drawback of existing work is the reliance on a fixed total token budget, making it hard to adapt to sparsity variations. Instead, we propose directly using the cumulative attention score of tokens in $I$ to guide token selection.\nSpecifically, we define $p(I)$ as the cumulative attention score of tokens in $I$, which is\n$p(I) = \\sum_{i \\in I} s_i = \\sum_{i\\in I} \\frac{\\exp(\\frac{q^T k_i}{\\sqrt{d}})}{\\sum_{j=1}^{n} \\exp(\\frac{q^T k_j}{\\sqrt{d}})}.$\nThese cumulative attention score targets offer two key advantages over fixed token budgets. First, they inherently adapt to sparsity variations without requiring assumptions or calibration data. Less sparse heads, layers, query tokens, and contexts naturally require more tokens to reach a given cumulative attention score than sparser ones. Second, targeting cumulative attention score provides a theoretical guarantee on attention distance. Specifically, the attention distance is bounded by"}, {"title": "3.5. Challenges of Attaining Cumulative Attention Scores", "content": "Identifying the minimal subset of tokens that achieve a target cumulative attention score is a challenging task. The optimal way is to select tokens following a descending order of attention score until the cumulative attention score surpasses the target value. Therefore, like prior approaches, Tactic must rank tokens by attention score to minimize the number of tokens needed to reach the desired cumulative attention score. However, unlike previous methods, Tactic also requires the attention score values for each token to track the cumulative sum of selected tokens in real-time. This process involves two key components: (1) computing the sum of attention intermediate values, $exp(q^T k_i /\\sqrt{d})$, for the selected token set $I$, and (2) computing the total sum of $exp(q^T k_i /\\sqrt{d})$ used for normalization. Additionally, this estimation must be computationally efficient, as it lies on the critical path during decoding."}, {"title": "3.6. Sorting Tokens via Clustering", "content": "Similar to prior works, Tactic groups tokens to reduce computational overhead. However, existing methods rely on positional order, assuming consecutive tokens share similar attention patterns (Tang et al., 2024). As shown in Fig. 6, this is suboptimal since Key vectors of consecutive tokens are often scattered in the embedding space, meaning positional proximity does not imply similarity in attention behavior. Moreover, modern attention kernels efficiently handle non-contiguous KV-cache access, making positional grouping unnecessary. Instead, Tactic applies K-means clustering to group tokens based on Key-vector similarity, then ranks them using the dot product between Query vectors and cluster centroids, ensuring selection aligns with actual attention behavior.\nThe runtime performance overhead of cluster-based sorting is $\\frac{2 \\times Average Cluster Size}{SeqLen} \\times Cost_{Attention} $ compared to full attention\u00b9, which in practice is below 2%.\nThe term 2 comes from clustering being only performed on K-cache, while the KV cache is twice as large as K-cache."}, {"title": "3.7. Estimating Attention Score via Distribution Fitting", "content": "While clustering effectively sorts tokens by attention score, it introduces large errors when estimating absolute attention score values. This occurs because the cluster centroid represents the center of tokens, but due to non-linearity, its attention score does not accurately reflect the average attention score of individual tokens. Thus, Tactic requires a more precise approach to estimating attention score. We observe that after partial sorting, the attention score distribution follows a consistent pattern across heads, layers, and contexts. For example, as shown in Fig. 7, the attention score is high for a few tokens and then smoothly decreases, forming a long-tail distribution. This structure suggests that function fitting can be used to estimate attention score. Despite outliers at the beginning of the curve, sampling tokens along the distribution allows accurate parameter estimation, enabling precise attention score predictions."}, {"title": "4. Methodology", "content": ""}, {"title": "4.1. Algorithm Overview", "content": "Fig. 8 provides an overview of Tactic's workflow. During prefill, Tactic performs K-means clustering on key vectors to group similar tokens. During decode, Tactic ranks tokens based on the dot product between cluster centroids and the current query vector. Tactic then models the distribution of attention score with a fitted curve and determines the tokens to meet the desired cumulative attention score threshold. After token selection, Tactic handles the Group Query Attention (GQA) and then performs the attention using FlashInfer (Ye et al., 2025)."}, {"title": "4.2. Clustering", "content": "To organize tokens for efficient sorting, Tactic performs K-means clustering on the key vectors for each head in every layer during the prefill phase. We empirically choose the average cluster size to be 32 to balance accuracy and efficiency. Clustering begins by randomly sampling SeqLen/Average cluster size data points as the initial cluster centroids. 2In each iteration, the distance between K-vectors and centroids is computed and the token will be assigned to the nearest cluster. After the assignment step, the centroids are updated as the mean of the key vectors assigned to each cluster. This process repeats until convergence or until a maximum of 10 iterations is reached\u00b3.\n2Note that neither multiple initializations nor K-Means ++ initialization drastically improves the clustering quality, and in fact leads to high-performance overhead.\n\u00b3More iterations do not improve the quality of clustering."}, {"title": "4.3. Querying", "content": "Once the tokens are organized into clusters, Tactic identifies critical clusters for a given query vector Q in the decode phase. The criticality of each cluster is determined by the dot product between Q and each cluster centroid\u2074. This process produces a sequence of clusters sorted by the criticality, from which we can derive a partially sorted token list.\n\u2074Compared to distance, dot product directly relates to the attention score, which is more accurate."}, {"title": "4.4. Fitting Attention Score Distribution", "content": "The next step of Tactic is to determine the token budget required to meet the cumulative attention score. Tactic models the distribution of the exponential values of the dot products (exp($\\frac{q^T k_i}{\\sqrt{d}}$)) for each token using a lightweight function $y = \\frac{a}{x} + b$, where $a$ and $b$ are parameters to be determined and $x$ is the position in the sorted list of tokens. To estimate these parameters, we select two segments of the tokens in the middle of the curve (e.g., 10% and 60% of all the tokens), and calculate the average of tokens within each segment (as labeled in Fig. 7). Using these two data points, we can solve for $a$ and $b$, which provides an estimation of attention score for all tokens.\nHowever, initial tokens are often outliers and cannot be accurately described by the curve. Moreover, these tokens feature high attention score, and thus a bad estimation would cause high deviations of estimated cumulative attention score which affects the accuracy of Tactic. Luckily, we observed that this only happens within 1-2% of total tokens. Therefore, Tactic directly calculates the exponential values of the dot products for these tokens. A detailed description of the Distribution Fitting stage is provided in Alg. 1."}, {"title": "4.5. Taking Union for Group Query Attention models", "content": "Modern models use Grouped Query Attention (GQA) to reduce the KV cache size (Dubey et al., 2024), where multiple query heads share a single KV head. However, loading KV heads separately for each query head is inefficient. To optimize this, query heads within the same group are batched. A challenge arises when using sparse attention, as different query heads may select to attend to different KV tokens. Finding the minimal set of KV tokens that satisfies the cumulative attention scores (attention score) across all query heads is NP-hard. To address this, Tactic simplifies the problem by taking the union of selected tokens across all query heads and loading them at once, ensuring that each head retains the KV tokens it requires to perform attention while reducing repetitive loading."}, {"title": "4.6. Attention on Selected Tokens", "content": "Finally, Tactic performs actual attention for selected tokens using FlashInfer (Ye et al., 2025). Notably, variations in sparsity across different heads cause an imbalanced attention workload. Traditional implementations primarily address imbalances across varying request lengths but struggle to handle head-level imbalance efficiently. To address this, Tactic divides each request into subrequests. Each subrequest processes a KV head and its corresponding Query head, with sequence length determined by the tokens selected for each KV head. This transforms head-level imbalance back into sequence-level imbalance, which FlashInfer handles efficiently."}, {"title": "5. Experiments", "content": ""}, {"title": "5.1. Setting", "content": "We evaluate Tactic for both accuracy and efficiency. We use two models: Llama-3.1-8B-Instruct (Grattafiori et al., 2024), a widely used model with Grouped-Query Attention; and MegaBeam-Mistral-7B-512k (Chen Wu and Yin Song and Eden Duthie, 2024), an extended version of Mistral-7B-Instruct-v0.2 with a 512k token context window.\nFor accuracy evaluations, we use the PG19 language modeling dataset (Rae et al., 2019), six tasks from the LongBench dataset(Bai et al., 2024), including HotpotQA(Yang et al., 2018), TriviaQA(Joshi et al., 2017), MultifieldQA(Bai et al., 2024), NarrativeQA(Ko\u010disk\u00fd et al., 2018), Qasper(Dasigi et al., 2021), and Musique(Bai et al., 2024). Additionally, we conduct experiments on the RULER benchmark(Hsieh et al., 2024), using 50 examples for each dataset. We compare Tactic with the most popular fixed token budget KV cache eviction algorithms, Quest (Tang et al., 2024), PyramidKV (Cai et al., 2024) and Ada-SnapKV (Feng et al., 2024). To ensure consistency, we set the page size in Quest and the cluster size in our method to 16. Both Ada-SnapKV and PyramidKV follow the configuration settings outlined in (Feng et al., 2024), including an observation window size of 32 and a max pooling kernel size of 7. For the clustering process, we limit the maximum number of iterations to 10.\nFor efficiency evaluations, we perform the evaluation on Nvidia Ada 6000 GPUs with CUDA 12.4 compared with full attention using Flashinfer (Ye et al., 2025)."}, {"title": "5.2. Accuracy Evaluation", "content": ""}, {"title": "5.2.1. ACCURACY OF CLUSTERING & DISTRIBUTION FITTING", "content": "To identify the minimal number of tokens to reach the threshold, Tactic employs clustering and distribution fitting (explained in Sec. 4). We evaluate our method on the PG19 dataset, focusing on how well it aligns with the target cumulative attention score and how many tokens it selects. We set specific attention score thresholds and compare the actual cumulative score achieved by our method against two oracles: the global optimal, which sums tokens in the descending order of attention score, and the clustering optimal, which sums attention score from sorted clusters. Tab. 1 presents the relative error between target and obtained cumulative attention score, as well as the comparison of the number of tokens selected by these methods. Tactic achieves the target threshold of cumulative attention score on average with high success rates. Also, the values of Cluster Optimal and Tactic are close, indicating that the distribution fitting presents an accurate estimation of number of tokens."}, {"title": "5.2.2. OUTPUT ACCURACY", "content": "We assess the KL-divergence of model output probability distribution of Tactic relative to the full attention under Top-K sampling using the PG19 test set(Rae et al., 2019). We include all texts in PG19 with the number of tokens larger than 32k. In the prefill stage, we truncate the input to 32k tokens and feed it into the model. In the decode stage, we feed tokens one by one and collect the output logits of each decode step. We collect 32 decode steps in total. As shown in Fig. 9, Tactic achieves the most accurate output compared to all baselines."}, {"title": "5.2.3. ACCURACY FOR LONG-CONTEXTS TASKS", "content": "LongBench. We evaluate Tactic on six LongBench tasks, namely, HotpotQA, TriviaQA, MultiFieldQA, Qasper, NarrativeQA, and Musique, spanning a wide range of scenarios such as single-document QA, multi-document QA, few-shot learning, and synthesis tasks. For each dataset, we first evaluate Tactic by setting the cumulative attention score threshold as 70% and 90%. The average number of tokens selected at each threshold serves as the token budget for evaluating Quest, PyramidKV, and Ada-SnapKV.\nAs shown in Tab. 2, Tactic consistently outperforms all other baselines. At a threshold of 90%, Tactic achieves performance close to full attention. We provide a detailed table of the average number of tokens selected by Tactic across various thresholds, datasets and models in Tab. 3, which is set as token budgets for baselines.\nRULER. We evaluate Tactic and baselines on all tasks in RULER (Hsieh et al., 2024) with context length ranging from 16K to 96K. As shown in the Tab. 4, Tactic consistently outperforms all baselines in each configuration in terms of average accuracy. Furthermore, at higher thresholds, Tactic achieves similar accuracy to full attention, significantly higher than other methods. Similar as Tab. 3, we provide a detailed table of token budgets used by baselines in Tab. 5."}, {"title": "5.3. Efficiency Evaluation", "content": "We begin by analyzing the latency breakdown of Tactic, focusing on token clustering during the prefill phase and attention computation for critical tokens during decoding (Sec. 5.3.1). Next, we evaluate Tactic's end-to-end performance and its speed-up relative to full attention (Sec. 5.3.3)."}, {"title": "5.3.1. LATENCY BREAKDOWN", "content": "Latency of clustering during prefill. We measure the time taken clustering for different sequence lengths in Tab. 6. We divide the clustering time into two steps. The first step is called distance calculation, where each K-vector computes its distance from the cluster centroids and assigns itself to the nearest cluster. The second step is called cluster update, where the centroids are updated based on the distances of K-vectors in the cluster.\nWe observe that, as the sequence length increases, the clustering time increases quadratically and is dominated by the distance calculation. However, large sequences also significantly increase the prefill time. Overall, across different sequence lengths, the clustering time always stays below 6% of the prefill time.\nLatency of attention during decode. In the decode stage, Tactic identifies and performs attention on critical tokens. We break down this process into four parts: 1) Cluster sorting, where the clusters are ranked based on the dot product of centroids and queries, 2) Distribution fitting, where Tactic samples a small portion of tokens and derives the attention score to identify the token budget for each attention head, 3) performing attention for the selected tokens. Fig. 10 shows the latency of this breakdown for different sequence lengths.\nThe latency of sparse attention during decode is reduced significantly, while the overheads of sorting and distribution fitting remain low across various sequence lengths. Overall, Tactic achieves up to 7.29\u00d7 speedup compared to the full attention."}, {"title": "5.3.2. ABLATION STUDY FOR QUERY HEAD UNION", "content": "We evaluate the benefits of taking the union of grouped query heads versus computing attention for each query head individually. As shown in Fig. 11 Across different context lengths and ratio $P$, taking unions can achieve up to 1.65\u00d7 attention speedup, due to the reduced memory loading."}, {"title": "5.3.3. END-TO-END PERFORMANCE", "content": "We compute the end-to-end performance of Tactic with different output tokens, sequence lengths, and ratios in Fig. 12 considering the prefill stages and the clustering overhead. Overall, Tactic achieves a speedup up to 1.58\u00d7 compared to full attention."}, {"title": "6. Conclusion", "content": "We presented Tactic, a sparsity-adaptive attention mechanism for efficient long-context LLM inference. Unlike fixed token budget methods, Tactic dynamically selects tokens based on cumulative attention scores, adapting to variations in attention sparsity. By leveraging clustering-based sorting and distribution fitting, Tactic accurately estimates token importance with low overhead. Our results showed that Tactic outperforms existing sparse attention methods, achieving higher accuracy and significant inference speedups, making it a practical solution for long-context LLMs."}, {"title": "A. Bound of approximation error", "content": "In Sec. 3.2, the upper bound of $\\epsilon(I)$ can be derived as\n$\\epsilon(I) =||o - \\tilde{o}(I)||$   \n$=|| o - \\frac{1}{p(I)} \\sum_{i\\in I} s_i v_i||$ \n$=||\\sum_{i=1}^n s_i v_i - \\frac{1}{p(I)} \\sum_{i\\in I} s_i v_i||$  \n$=||(\\frac{1}{1-p(I)} \\sum_{i \\notin I} s_i v_i +  \\sum_{i\\in I} s_i v_i ||$\n$\\le || \\frac{1}{p(I)}  \\sum_{i \\notin I} s_i v_i || + \\sum_{i\\in I} s_i v_i||$ \n$\\le (\\frac{1}{1-p(I)} ) \\sum_{i \\notin I} s_i || v_i || + \\sum_{i\\in I} s_i|| v_i||$   \n$ < (\\frac{1}{p(I)}-1) max || v_i|| + (1-p(I)) max || v_i||.$\nHence we can get\n$\\epsilon(I) \\le 2(1 - p(I)) max || v_i||.$\nBoth (10) to (11) and (11) to (12) are based on triangle inequality."}, {"title": "B. Algorithm of Distribution Fitting", "content": "Algorithm 1 Estimating Token Budget via Distribution Fitting\n1: Input: Token sequence unpacking from sorted clusters {\ud835\udc4b1,\ud835\udc4b2,\u2026, \ud835\udc4b\ud835\udc5b}, query Q, weight percentage threshold P, initial token count N, head dimension d\n2: Output: Token budget K\n4: Compute \u03bc\u2081 and \u03bc\u2082 as the means of exp(\ud835\udc65\ud835\udc56 \u00b7 \ud835\udc44/\u221a\ud835\udc51) within fixed windows around p\u2081 and p2. Solve for parameters a and b in y = a/x + b the two data points.\n6: Initialize array w\u2081 to store the simulated attention scores for all tokens\n7: for i = 1 to n do\n8: If i < N, w\u2081 = exp(\ud835\udc65\ud835\udc56. \ud835\udc44/\u221a\ud835\udc51)\n9: Else, w\u2081 = a/i + b\n10: end for\n11: Compute the minimal k such that the cumulative sum $\\sum w_i \\ge P. \\sum \\hat w_i .$\n13: return k"}]}