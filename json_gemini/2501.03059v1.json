{"title": "Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation", "authors": ["Guy Yariv", "Yuval Kirstain", "Amit Zohar", "Shelly Sheynin", "Yaniv Taigman", "Yossi Adi", "Sagie Benaim", "Adam Polyak"], "abstract": "We consider the task of Image-to-Video (I2V) generation, which involves transforming static images into realistic video sequences based on a textual description. While recent advancements produce photorealistic outputs, they frequently struggle to create videos with accurate and consistent object motion, especially in multi-object scenarios. To address these limitations, we propose a two-stage compositional framework that decomposes I2V generation into: (i) An explicit intermediate representation generation stage, followed by (ii) A video generation stage that is conditioned on this representation. Our key innovation is the introduction of a mask-based motion trajectory as an intermediate representation, that captures both semantic object information and motion, enabling an expressive but compact representation of motion and semantics. To incorporate the learned representation in the second stage, we utilize object-level attention objectives. Specifically, we consider a spatial, per-object, masked-cross attention objective, integrating object-specific prompts into corresponding latent space regions and a masked spatio-temporal self-attention objective, ensuring frame-to-frame consistency for each object. We evaluate our method on challenging benchmarks with multi-object and high-motion scenarios and empirically demonstrate that the proposed method achieves state-of-the-art results in temporal coherence, motion realism, and text-prompt faithfulness. Additionally, we introduce SA-V-128, a new challenging benchmark for single-object and multi-object I2V generation, and demonstrate our method's superiority on this benchmark. Project page is available at https://guyyariv.github.io/TTM/.", "sections": [{"title": "1. Introduction", "content": "Image-to-Video (I2V) generation transforms static images into realistic video sequences guided by textual descriptions. Recently, significant progress has been made in this task, with models such as [6, 12, 30, 41, 43, 55], which enable the generation of photorealistic and consistent output. However, current works still struggle to generate videos with consistent and faithful object motion. These limitations are especially evident in scenarios with multiple objects, as shown in our experiments, where capturing the correct motion and interactions is challenging.\nSeveral works [6, 12, 30, 41] directly map an input image (and possibly text) to an output video in a single, end-to-end pipeline. By doing so, the underlying model must implicitly reason about object semantics and motion while simultaneously generating a plausible appearance of all objects. As the range of possible motions and interactions scales significantly with the number of objects, this makes it difficult for current models to generate plausible outputs. An alternative approach is to decompose the training process into a two-stage compositional process: (i) Given the input image, generate an explicit intermediate representation; (ii) Utilize the generated representation and the input image to generate the full video. Recent work [43], proposed using Optical Flow (OF) for this representation. However, this has several drawbacks. First, only motion, without semantics, is represented in OF. Second, such motion representation is redundant in the context of I2V generation. Predicting per-point pixel motion may not be required to depict plausible object motion. Doing so may result in unnecessary errors (e.g., wrong prediction in pixels from non-moving objects). Such errors can then significantly influence the second stage, as the model tries to predict the correct appearance while adhering to incorrect motion.\nIn this work, we argue that the choice of representation is critical and should capture several properties: (i) it should express both motion and semantics; (ii) it should represent the motion and interaction of individual objects; and (iii) it should be robust to signal variations and operate at the object level rather than at the pixel level. We argue that a suitable choice satisfying these properties is a mask-based motion trajectory, a time-consistent per-frame semantic mask capturing semantic objects and their motion (see Fig. 1).\nOur method follows a two-stage process: first, a network is trained to generate a mask-based motion trajectory conditioned on the input image, segmentation mask, and text prompt. In the second stage, the motion-to-video network generates the final video conditioned on the input image, text prompt, and the generated motion trajectory from the first stage.\nFor the second stage of our framework (i.e., motion-to-video), we propose to integrate the generated mask-based motion trajectory's structure softly, using learned attention layers, ensuring the network adheres to the generated semantics and motion. Specifically, we propose using (i) a masked cross-attention objective, which integrates object-specific prompts directly into corresponding regions of the latent space, using masked cross-attention, and (ii) a masked self-attention objective, which ensures that each object maintains consistency across frames, by using the generated mask in the self-attention mechanism to restrict attention to positions corresponding to the same object.\nWe compare our approach to a diverse set of recent image-to-video generation approaches on challenging benchmarks that include several objects and significant motion. We demonstrate state-of-the-art performance across diverse metrics, including temporal coherence, motion realism, visual consistency across frames, and text faithfulness to the input prompt. To further advance research on I2V generation, we introduce a new benchmark that includes distinct sets for single-object and multi-object videos, demonstrating superior performance. Finally, we ablate our method, demonstrating the contribution of each component."}, {"title": "2. Related work", "content": "Text-to-Video Generation. Recent advances in diffusion models [20, 45, 46] and flow matching techniques [1, 27, 28] have enhanced the capability to generate high-quality images conditioned on textual descriptions [4, 11, 14]. In the context of text-conditioned video generation, several approaches perform diffusion in a low-dimensional latent space, adopting the Latent Diffusion Models (LDM) architecture [37, 42]. Many text-to-video (T2V) models adapt T2I architectures to generate temporally coherent videos, extending beyond the spatial knowledge of T2I training [2, 7, 10, 15, 17, 18, 21, 22, 44, 49-51, 56]. A common approach extends pre-trained T2I models with temporal modules, such as convolutions or attention layers, followed by additional training for video generation [15, 21, 22, 44].\nEmuVideo [16] and VideoGen [24], for instance, factorized the text-to-video to two stages: text-to-image and image-to-video. Recent studies have adopted the transformer-based Diffusion Transformer (DiT) [36] due to its performance over U-Net [31, 32, 34, 38]. Notably, our approach is architecture-agnostic and works with both U-Net and DiT.\nImage-To-Video Generation. In I2V, the video generation model is conditioned on the input text, as well as an additional visual input that represents the initial frame of the output video [5, 35]. Several works leverage this additional visual input by fine-tuning of a pre-trained T2V model [6, 9]. Despite encouraging progress in generated video aesthetic [16], current I2V models struggle to generate complex actions or interactions between objects [43].\nRecent work has attempted to tackle this challenge. VideoCrafter [9] incorporate an additional image input to preserve the content and style of the reference image. DynamiCrafter [54] use a query transformer to project the image into a text-aligned context, leveraging motion priors from T2V models to animate images. I2VGen-XL [55] employ a two-stage cascade, with the second stage refining resolution and temporal coherence. ConsistI2V [41] use first-frame conditioning by combining the initial latent frame with input noise and enhancing self-attention with intermediate states. AnimateAnything [12] include an additional mask to constrain motion areas. Cinemo [30] introduce three enhancements: (i) prediction of motion residuals, (ii) fine-grained control over motion intensity, and (iii) noise refinement at inference to reduce motion shifts. Our work considers a different two-step compositional approach and is orthogonal to these advances.\nPerhaps most relevant to our work is Motion-I2V [43], which follows a two-step generation process: (i) prediction of optical flow displacement, and (ii) generation of the video based on generated optical flow. Our method similarly follows a two-stage approach. However, we offer two key differences. First, we use a different intermediate representation of mask-based motion trajectories. We argue that this choice is significant in multiple aspects: (i) we represent not only motion, but also semantics, enhancing expressivity. (ii) Simultaneously, our representation captures only object-level motion as opposed to pixel-level motion. Doing so makes our generation less susceptible to errors in the first stage, as also observed in [3]. Second, our representation also enables additional flexibility in the second generation stage, which generates a video conditioned on this representation. Specifically, instead of wrapping the generated video using predicted flow, we softly condition the video generation model on the intermediate representation using object-level and temporal attention. We note that recent T2I models have introduced conditioning on specific areas with targeted information to improve fine-grained controllability [25, 33]. To address the I2V setting, we extend the masked cross-attention introduced in [33] to the video setting and introduce a novel masked self-attention objective."}, {"title": "3. Method", "content": "Our method THROUGH-THE-MASK, illustrated in Fig. 2, factorizes I2V into two compositional stages:\n1. Image-to-Motion Generation: In the first stage, outlined in Sec. 3.2, we generate motion trajectory conditioned on the reference image and motion prompt. This motion trajectory encapsulates the dynamic behavior of individual objects.\n2. Motion-to-Video Generation: In the second stage, outlined in Sec. 3.3, we use the generated motion trajectory, along with the object-specific prompts and the reference image, to produce a photorealistic video.\nOur two-stage process is based on the choice of an explicit intermediate representation of objects' motion. Ideally, such representation should (i) express both motion and semantics, (ii) represent the interaction of objects, and (iii) be robust to signal variations. We claim that a motion trajectory, i.e., a consistent per-frame video segmentation, satisfies these properties by definition. First, it captures motion, interactions, and type (i.e., semantics), hence satisfying both (i) and (ii). Second, as image segmentation operates at a coarse level (i.e., object level rather than pixel level), we achieve the following desired separation of tasks: The first stage handles coarse object-level semantics, motion, and inter-object interactions. The second stage then handles the intra-object level semantics and appearance. On average, this results in fewer overall errors produced than the pixel-level counterpart in the first stage, satisfying (iii). It also enables greater flexibility in the second stage.\nTo allow the modeling of our framework, we pre-process our training data, as outlined in Sec. 3.1. The Image-to-Motion and Motion-to-Video stages are trained independently but are combined during inference to produce the final video (see supplementary Sec. 9). Additional implementation details are provided in supplementary Sec. 10."}, {"title": "3.1. Data Pre-processing", "content": "We assume a training dataset of text-video pairs, where the input contains a reference image $x^{(0)}$ and a text prompt $c$. Our pre-processing pipeline comprises the following components applied on each text-video pair: (i) extraction of prompts for motion-capable objects from the input text, (ii) video segmentation, and (iii) extraction of motion-specific and object-specific prompts from the input text.\nMotion-capable Object Prompt Extraction. Using a pre-trained Large Language Model (LLM), we extract $L_{\\text{objects}}$ prompts, $\\{o^{(1)}, ..., o^{(L)}\\}$, relevant to generating specific motion pathways (motion-capable objects) from the input text, $c$. Notice, $L$ is variable and video-specific. We refer the readers to supplementary Sec. 8 for more details."}, {"title": "3.2. Image-to-Motion", "content": "In the first stage of our framework, we train a model to generate a sequence of fine-grained, mask-based motion trajectories, $\\hat{s}$, conditioned on an input frame $x^{(0)}$, a segmentation of the input frame $s^{(0)}$, and a motion-specific prompt $c_{\\text{motion}}$. The Image-to-Motion model is denoted as $\\hat{s}_{\\theta}(s_t, t, \\mathcal{E}(x^{(0)}), \\mathcal{E}(s^{(0)}), c_{\\text{motion}})$, where $s_t$ is a noisy masked-based motion trajectory at the denoising timestep $t$, $\\theta$ are the learned parameters of the network, and $\\mathcal{E}$ is a VAE [23] encoder. For brevity, we omit the activation of the encoder, $\\mathcal{E}$, in the rest of the section. We apply a denoising process in the latent space of a VAE as in LDM [42]. We initialize $\\hat{s}_{\\theta}$ with a pre-trained text-to-video model by concatenating the encodings of the first frame $x^{(0)}$ and its mask $s^{(0)}$ along the input channel dimension. Text conditioning is applied as in LDM, using cross-attention layers. See the supplementary for full details."}, {"title": "3.3. Motion-to-Video", "content": "In the second stage, we train a model to generate a sequence of video frames $\\hat{x}$, conditioned on the reference image $x^{(0)}$, the generated motion trajectory $\\hat{s}$, the text prompt $c$, and the object-specific prompts $c_{\\text{local}}$. Using the same denoising approach as in the first stage, we train the Motion-to-Video model, $x_{\\psi}(x_t, t, x^{(0)}, \\hat{s}, c, c_{\\text{local}})$ with parameters $\\psi$, to iteratively refine a noisy latent representation towards a clean video output, following LDM [42]'s formulation. As in the first stage, we finetune a pre-trained text-to-video model, concatenating the encodings of the first frame $x^{(0)}$ and the predicted mask-based motion trajectory $\\hat{s}$ to the noisy latent representation $x_t$ along the channel dimension. Text is integrated using cross-attention as in the first stage."}, {"title": "3.3.1. Masked Attention Blocks", "content": "We introduce two masked attention-based objectives to condition I2V models in specific areas with targeted information, as shown in Fig. 3. We apply these objectives in the first $K$ blocks to extend the model's attention capabilities. In the following section, we build upon the notation of [33].\nMasked Cross-Attention. We employ masked cross-attention to integrate object-specific prompts directly into the corresponding regions of the latent space, ensuring each object's latent representation attends only to its own prompt. Our approach extends that of [33], which considered an object-level cross-attention for text-to-image generation.\nFormally, let $z \\in \\mathbb{R}^{N' \\times H' \\times W' \\times d}$ be the latent features serving as queries, where $N'$, $H'$, $W'$ are the temporal and spatial dimension of the latent features and $d$ is the model dimension size. For $L$ object-specific prompts $\\{c_{\\text{local}}^{(i)}\\}_{i=1}^L$, we encode the prompts to obtain a sequence of embeddings $\\{e^{(i)}\\}_{i=1}^L \\in \\mathbb{R}^{N_{\\text{text}} \\times d}$, where $N_{\\text{text}}$ is the sequence length of the encoded prompts. We denote the query, key, and value of the masked-cross attention layers as follows $q = zW_q \\in \\mathbb{R}^{N_{\\text{tokens}} \\times d}$, $k^{(i)} = e^{(i)} W_k \\in \\mathbb{R}^{N_{\\text{text}} \\times d}$, and $v^{(i)} = e^{(i)} W_v \\in \\mathbb{R}^{N_{\\text{text}} \\times d}$, where $N_{\\text{tokens}} = N' \\cdot H' \\cdot W'$. All object-specific keys and values are concatenated along the sequence dimension, $k = [k^{(1)}; ...; k^{(L)}]$ and $v = [v^{(1)}; ... ; v^{(L)}]$. The masked cross-attention then becomes,\n$M_{\\text{cross}} = [M^{(1)}; ...; M^{(L)}]$\n$\\tilde{h}_{\\text{cross}} = \\sigma\\left( \\frac{qk^T}{\\sqrt{d}} + \\text{log } M_{\\text{cross}} \\right)v,$\nwhere $[;]$ is a concatenation along the sequence dimension, $\\sigma(\\cdot)$ is the softmax function, and $\\tilde{h}$ is the intermediary hidden features passed to the next layer. We construct binary masks $M^{(l)} \\in \\{0, 1\\}^{N_{\\text{tokens}}}$ indicating the spatial locations associated with each object $l$ along the frames, derived from bounding boxes obtained during training (from ground truth segmentation $s$) or inference (from generated segmentation $\\hat{s}$). The masked cross-attention is computed by restricting each query position to attend only to the keys corresponding to objects present at that location.\nMasked Self-Attention. Unlike cross-attention, where the queries come from one sequence and the keys and values come from another, self-attention derives the query $q$, key $k$, and value $v$ from the same input sequence, which is the latent features $z$. We introduce a novel objective that ensures that each position attends only to positions of the same object, enhancing temporal consistency and preventing interference between different objects. To this end, we introduce a mask into the self-attention mechanism that restricts attention to positions corresponding to the same object. We construct an attention mask $M_{\\text{self}} \\in \\{0, 1\\}^{N_{\\text{tokens}} \\times N_{\\text{tokens}}}$, where $M_{\\text{self}}^{(i, j)} = 1$ if positions $i$ and $j$ belong to the same object (based on the segmentation masks $s$), and $M_{\\text{self}}^{(i, j)} = 0$ otherwise. The masked self-attention then becomes,\n$\\tilde{h}_{\\text{self}} = \\sigma\\left( \\frac{qk^T}{\\sqrt{d}} + \\text{log } M_{\\text{self}} \\right)v.$\nApplying this attention mask we get masked self-attention."}, {"title": "4. Experiments", "content": "To evaluate our method, we assess temporal coherence, motion realism, visual consistency across frames, and text faithfulness. First, we compare our approach with current state-of-the-art image-to-video methods on the Image-Animation-Bench, featuring 2,500 high-quality videos. We use two different neural network architectures for the denoising network: a U-Net, adapted from AnimateDiff [17], and a DiT, adapted from Movie Gen [38]. Following this, we ablate our method's design. Image-to-video examples are presented in Fig. 4 with additional samples and qualitative comparisons in the supplementary. We additionally introduce a new benchmark, SA-V-128, for image-to-video generation, which includes distinct sets for single-object and multi-object videos. This separation enables focused testing on both scenarios."}, {"title": "4.1. Experimental Setup", "content": "Evaluation Benchmarks. To evaluate the effectiveness of our method, we introduce the SA-V-128 benchmark, designed to test performance across both single- and multi-object animations in diverse scenarios. Current image-to-video benchmarks lack explicit distinctions between single- and multi-object cases, particularly when assessing \"motion-capable objects\u201d such as humans and animals. This limitation hinders accurate evaluation of models in complex multi-object animations. To address this, we constructed a balanced test set of 128 videos from the SA-V dataset [40], with equal representation of single-object and multi-object cases (64 videos each), averaging 14 seconds per video. The filtering process consisted of generating text captions and categorizing each video from a set of predefined categories using Llama v3.2-11B [13], based on selected frames. Each video was then assigned aesthetic and motion scores, with motion quantified by optical flow magnitude via RAFT [47]. From this, the 500 videos with the highest combined scores were automatically selected, and 64 single-object and 64 multi-object videos were randomly chosen from this set. We provide further details in the supplementary Sec. 11. Additionally, we use the Image-Animation-Bench, a curated collection of 2,500 videos, all meeting strict resolution requirements and filtered based on aesthetic standards. Further details are provided in the supplementary Sec. 12. We then evaluate our method's effectiveness across diverse scenarios using both the Image-Animation-Bench and the SA-V-128 benchmark.\nEvaluation Metrics. The objective of image-to-video generation is to produce videos that are high-quality, temporally consistent, faithful to the input text, and maintain key elements of the initial input image across frames. We assess these aspects using both objective and subjective metrics. For video realism, we employ Fr\u00e9chet Video Distance (FVD) [48], which measures the visual disparity between feature embeddings of generated and reference videos. To evaluate temporal consistency, we use CLIPFrame (Frame Consistency) [53], which computes the average cosine similarity between CLIP [39] embeddings of individual frames to measure frame-to-frame coherence. To verify text faithfulness, we use ViCLIP-T, a metric based on ViCLIP [52], a video CLIP model that incorporates temporal information when processing videos. ViCLIP-T calculates the cosine similarity between text and video embeddings, measuring how well the generated video aligns with the input text prompt. For image faithfulness, we use ViCLIP-V, which, similar to ViCLIP-T, measures cosine similarity between the ViCLIP embeddings of the generated video and a reference video derived from the input image to ensure that the generated video maintains essential visual elements of the initial input image across frames. Given the generation setting of a maximum of 128 frames, this metric ensures relative alignment with the reference video, supporting fidelity to the original input. Furthermore, we report the Average Displacement by taking the average magnitude of the OF vector between consecutive frames to estimate the degree of dynamics. In this metric, we ensure that the videos exhibit realistic motion by maintaining displacement levels that are neither excessively high nor unnaturally low. For subjective evaluation, we rely on human raters to compare our approach against the baselines. We present the raters with the input frame, a caption describing the motion, and two generated videos: one from our method and one from the baseline. The raters are tasked with answering three questions: (i) Text faithfulness: Which video better matches the caption? (ii) Motion: Which video has the best overall motion consistency? and (iii) Quality: Aesthetically, which video is better? To ensure a fair comparison, each pair of videos, along with the input image and text prompt, was rated by 5 different raters. We rate 128 randomly sampled samples from the Image-Animation-Bench and all 128 samples from SA-V-128 benchmark. We then used the majority vote to determine which video was preferred."}, {"title": "4.2. Baseline Comparisons", "content": "For comparison with U-Net-based models, we evaluate our method against several open-sourced state-of-the-art image-to-video models: VideoCrafter [9], DynamiCrafter [54], Motion-I2V [43], and ConsistI2V [41].\nFor both U-Net and DiT models, we also report results for a single-step image-to-video baseline, denoted as TI2V, which is a variant of the proposed architecture with two main differences: (i) TI2V accepts a concatenation of the first frame and input noise as input, without any motion trajectory representation, and (ii) instead of our proposed masked attention blocks, TI2V includes additional standard attention layers to match the parameter count. Specifically, TI2V includes additional self-attention and cross-attention layers that attend to the full video patches without masking and with text prompt, respectively."}, {"title": "4.3. Ablation Study", "content": "Effect of The Masked Attention Mechanism. We evaluate the impact of spatial and temporal masked attention layers in THROUGH-THE-MASK. We consider the following configurations: (i) without masked attention (no mask attn), (ii) with only masked cross-attention (w. cross-attn), (iii) with only masked self-attention (w. self-attn), and (iv) with both masked cross and self-attention (THROUGH-THE-MASK). We also compare against the TI2V baseline model that includes additional attention layers positioned as our masked layers, but performs self-attention and cross-attention without masking, ensuring it has the same number of parameters as THROUGH-THE-MASK. Results indicate that the addition of masked attention significantly improves performance across all metrics, particularly when compared to the baseline with the same architecture but without masking.\nMotion Representation Ablation. Lastly, we compare a mask-based trajectory versus optical flow based motion trajectory. We train separate models for Stage 1 and Stage 2 of THROUGH-THE-MASK for each of the trajectories, keeping all other aspects the same."}, {"title": "5. Conclusion", "content": "We presented THROUGH-THE-MASK, a novel two-stage framework for image-to-video generation leveraging mask-based motion trajectories as an intermediate representation, enabling coherent and realistic multi-object motion in generated videos. Motion trajectories are injected via two attention-based objectives that effectively use this representation to enforce the predicted motion and semantics in the generated video. Our approach empirically achieves SOTA performance in challenging single- and multi-object settings."}, {"title": "6. Additional Results", "content": null}, {"title": "6.1. Qualitative Comparison of Masked Attention Mechanism", "content": "Fig. 6 shows qualitative comparison of generated videos for each configuration of THROUGH-THE-MASK, demonstrating the differences when applying masked cross-attention, self-attention, both, or no masked attention layers."}, {"title": "6.2. Qualitative Comparison of Motion Representation Ablation", "content": "Fig. 7 shows a qualitative comparison of the generated videos for different intermediate representation configurations of THROUGH-THE-MASK. Specifically, it compares our chosen representation, which is mask-based motion trajectories, to optical flow."}, {"title": "6.3. Additional Qualitative Comparisons of DiT Architecture", "content": "Building upon the comparisons presented in Sec. 4.2, we provide further qualitative results comparing our approach to existing baselines, based on DiT architecture, in Fig. 8"}, {"title": "6.4. Additional Qualitative Comparisons of U-Net Architecture", "content": "Building upon the comparisons presented in Sec. 4.2, we provide further qualitative results comparing our approach to existing baselines, based on U-Net architecture, in Fig. 9"}, {"title": "7. Motion and Object-Specific Prompts Details", "content": "As described in Sec. 3.1, our pre-processing pipeline extracts a motion-specific prompt, $c_{\\text{motion}}$, from the input text $c$, using a pre-trained LLM. This prompt provides a consolidated description of all motion in the scene, excluding any spatial, color, or object-specific details, and serves as a high-level guide for motion generation.\nTo generate the motion-specific prompt, we use Llama v3.1-8B [13] in a frozen configuration. The input prompt instructs the LLM to focus solely on motion, as shown in Fig. 10, ensuring that descriptions remain centered on movement dynamics, ignoring background information and visual characteristics of objects."}, {"title": "8. Motion-capable Objects' Prompt Extraction Details", "content": "As described in Sec. 3.1, the pre-processing process begins with extracting motion-capable object prompts from the global prompt $c$. We utilize Llama v3.1-8B [13] as a frozen LLM and provide the prompt shown in Fig. 11, which outlines the process for generating local prompts for motion-capable objects."}, {"title": "9. Inference", "content": "Given the reference image $x^{(0)}$ and text prompt $c$, inference is carried out in two stages. First, the initial segmentation $s^{(0)}$ is extracted from $x^{(0)}$ using SAM2 [40]. Concurrently, the text prompt $c$ is processed by a pre-trained LLM to obtain the motion-specific prompt $c_{\\text{motion}}$ and object-specific prompts $c_{\\text{local}} = \\{c_{\\text{local}}^{(1)}, ..., c_{\\text{local}}^{(L)}\\}$ as detailed in Section 3.1. At stage 1, the image-to-motion generates motion trajectories $\\hat{s}$ conditioned on $(s^{(0)}, x^{(0)}, c_{\\text{motion}})$.\nNext, in stage 2, the motion-to-video produces the final video $\\hat{x}$ by conditioning on $(x^{(0)}, \\hat{s}, c, c_{\\text{local}})$ and incorporating masked attention mechanisms to ensure consistency and controllability, as described in Section 3.3. For both stages, we adapt the Classifier-Free Guidance [19] approach suggested by Brooks et al. [8], where, to align exactly with their method, we treat the concatenated visual conditions as a single visual condition, and do the same for the text."}, {"title": "10. Implementation Details", "content": "As detailed above, we demonstrate the applicability of our approach to two architectures.\nThe first is the U-Net architecture. We follow the AnimateDiff V3 [17] design, consisting of approximately 1.4B parameters. In the second stage of motion-to-video, detailed in Sec. 3.3, we set $K = 6$, where $K$ represents the number of attention blocks expanded into masked attention blocks - specifically, by adding masked self-attention and masked cross-attention into the spatial attention blocks within the U-Net's encoder layers. The U-Net-based model was optimized using the solver suggested by [26], incorporating the DDIM diffusion solver with v-prediction and zero signal-to-noise ratio (SNR). The latter was found to be critically important to enable image-to-mask-based motion trajectory generation.\nThe second architecture is DiT-based. We train a DiT model following the MovieGen [38] design, containing four billion parameters. For the DiT-based model in stage two, we used $K = 10$, corresponding to the first 10 attention blocks out of a total of 40. The DiT-based model was optimized as described in the MovieGen paper, with Flow Matching [27], using a first-order Euler ODE solver. During inference, we adopted MovieGen's efficient inference method by combining a linear-quadratic t-schedule, as detailed in the MovieGen paper.\nFor both architectures, text-to-video pre-training followed the methodology outlined in MovieGen. Across both training stages (Sec. 3.2 and Sec. 3.3), we utilized the fine-grained mask-based motion trajectories dataset described in Sec. 3.1. The U-Net model was trained at a resolution of 512 \u00d7 512, predicting 16 frames, while the DiT model was trained at a resolution of 256 \u00d7 256, predicting 128 frames. Both models were trained with a batch size of 32, using a constant learning rate of 2 \u00d7 10-5, a warm-up period of 2000 steps, and a total of 50,000 steps."}, {"title": "11. SA-V-128 Benchmark", "content": "We introduce a balanced test set of 128 videos from the SA-V dataset [40], comprising 64 single-object and 64 multi-object cases, with an average duration of 14 seconds per video. The filtering of 128 videos, out of the full SA-V dataset, involved several steps. First, for each video, we generated a text caption using Llama v3.2-11B [13] by providing the first, middle, and last frames and asking the model to generate a caption describing the video. Next, from a closed set of categories (Animal, Architecture, Digital Art, Food, Landscape, Lifestyle, Plant, Vehicles, Visual Art, and Other), we used Llama v3.2-11B [13] to categorize each video based on these frames. We then iterated over the categories, selecting a unique category at each step and adding a related video to ensure a balanced test set. We assigned an aesthetic score and a motion score by calculating the magnitude of the optical flow extracted with RAFT [47]. After assigning captions and scores, we filtered 500 videos by iterating through each category and selecting those with the highest combined aesthetic and motion scores. From these 500 automatically filtered videos, we randomly selected 64 single-object and 64 multi-object videos. To ensure a fair comparison for shorter video settings, we also provided short captions, generated using the same methodology, extracted from frames 0 to 127 of each video."}, {"title": "12. Image-Animation-Bench", "content": "The Image-Animation-Bench comprises 2,500 videos, meticulously curated to meet high-resolution requirements and aesthetic quality thresholds. To ensure comprehensive coverage of diverse visual scenarios, the dataset is divided into 16 categories: Portraits, Scenery-Nature, Pets, Food, Animation, Science, Sports, Scenery-City, Animation-Static, Music, Game, Animals, Industry, Painting, Vehicles, and others."}]}