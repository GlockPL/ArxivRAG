{"title": "A Large Encoder-Decoder Family of Foundation Models For Chemical Language", "authors": ["Eduardo Soares", "Victor Shirasuna", "Emilio Vital Brazil", "Renato Cerqueira", "Dmitry Zubarev", "Kristin Schmidt"], "abstract": "Large-scale pre-training methodologies for chemical language models represent a breakthrough in cheminformatics. These methods excel in tasks such as property prediction and molecule generation by learning contextualized representations of input tokens through self-supervised learning on large unlabeled corpora. Typically, this involves pre-training on unlabeled data followed by fine-tuning on specific tasks, reducing dependence on annotated datasets and broadening chemical lan- guage representation understanding. This paper introduces a large encoder-decoder chemical foundation models pre-trained on a curated dataset of 91 million SMILES samples sourced from PubChem, which is equivalent to 4 billion of molecular tokens. The proposed foundation model supports different complex tasks, including quantum property prediction, and offer flexibility with two main variants (289M and 8 \u00d7 289M). Our experiments across multiple benchmark datasets validate the capacity of the proposed model in providing state-of-the-art results for different tasks. We also provide a preliminary assessment of the compositionality of the embedding space as a prerequisite for the reasoning tasks. We demonstrate that the produced latent space is separable compared to the state-of-the-art with few-shot learning capabilities.", "sections": [{"title": "Introduction", "content": "Understanding molecular properties is crucial for accelerating discoveries in different fields, including drug development and materials science [1]. Traditional methods rely on labor-intensive trial-and- error experiments, which are both costly and time-consuming [2]. However, recent advances in deep learning have enabled the use of foundation models to predict molecular properties and generate molecule candidates [3, 4, 5], marking significant progress in scientific exploration.\nThe introduction of large-scale pre-training methodologies for chemical language models (LMs) represents a significant advancement in cheminformatics [6]. These methodologies have demonstrated impressive results in challenging molecular tasks such as predicting properties and generating molecules [7]. The success of these models can be attributed to their ability to learn contextualized representations of input tokens through self-supervised learning on large unlabeled corpora [8]. This methodological approach typically involves two phases: pre-training on unlabeled data followed by fine-tuning on specific downstream task [9]. By reducing the reliance on annotated datasets, this approach has broadened our understanding of chemical language representations [10]."}, {"title": "Overview of the proposed approach", "content": "This section presents an overview of the proposed SMI-TED289M foundation model for small molecules. Here, we outline the process of collecting, curating, and pre-processing the pre-train data. Additionally, we describe the token encoder process and the SMILES encoder-decoder process. Finally, we explain the Mixture-of-SMI-TED-Experts approach used to scale the base model. Fig. 1 illustrates the general architecture of the base model."}, {"title": "Pre-training Data", "content": "The pretraining data originated from the PubChem data repository, a public database containing information on chemical substances and their biological activities [18]. Initially, 113 million SMILES strings were collected from PubChem. These molecular strings underwent deduplication and canon- icalization processes to ensure uniqueness [20]. Subsequently, a molecular transformation was conducted to verify the validity of the molecules derived from the unique SMILES strings, resulting in a set of 91 million unique and valid molecules."}, {"title": "Model Architecture", "content": "We conduct training for SMI-TED289M model employing a deep-bidirectional-transformers-based encoder [22] for tokens and an encoder-decoder architecture to compose SMILES. The hyper- parameters of SMI-TED289M base model are detailed in Table 1\nTo optimize the relative encoding through position-dependent rotations $R_m$ of the query and keys at position m, the SMI-TED289M uses a modified version of the RoFormer [23] attention mechanism. These rotations can be implemented as pointwise multiplications and do not significantly increase computational complexity as shown in Eq. (1).\n$Attention_m (Q, K, V) = \\frac{\\sum_{n=1}^{N} (\\varphi(R_m q_m), \\varphi(R_n k_n)) v_n}{\\sum_{n=1}^{N} (\\varphi(R_m q_m), \\varphi(R_n k_n))}$ (1)\nwhere Q,K,V are the query, key, and value respectively, and $\\varphi$ is a random feature map.\nWe start with a sequence of tokens extracted from SMILES, each embedded in a 768-dimensional space. The encoder-decoder layer is designed to process molecular token embeddings, represented as $x \\in \\mathbb{R}^{D\\times L}$, where D denotes the maximum number of tokens and L represents the embedding space dimension. We limited D at 202 tokens, as 99.4% of molecules in the PubChem dataset contain fewer tokens than this threshold.\nIn encoder-only models, a mean pooling layer is typically employed to represent tokens as SMILES in the latent space. However, this approach is limited by the lack of a natural inversion process for the mean pooling operation. To overcome this limitation, we aim to construct a latent space representation for SMILES by submersing the x in a latent space, denoted as z, as described in Eq. 2.\n$z = \\varphi (LayerNorm (GELU (xW_1 + b_1))) W_2,$(2)"}, {"title": "Pre-training strategies", "content": "Pre-training of SMI-TED289M was performed for 40 epochs through the entire curated PubChem dataset with a fixed learning rate of 1.6e-4 and a batch size of 288 molecules on a total of 24 NVIDIA V100 (16G) GPUs parallelized into 4 nodes using DDP and torch run. It involves two distinct phases: i) Learning of token embeddings through a masking process; ii) Subsequently, the token embeddings are mapped into a common latent space that encapsulates the entire SMILES string. This latent space not only facilitates the representation of the SMILES but also enables the reconstruction of both individual tokens and complete SMILES strings. Consequently, the pre-training process involves two separate loss functions: one for the token embeddings, which is based on the masking process, and another for the encoder-decoder layer, which focuses on the reconstruction of tokens. Two pre-training strategies are employed:\n\u2022 In phase 1, the token encoder is initially pre-trained using 95% of the available samples, while the remaining 5% is reserved for training the encoder-decoder layer. This partitioning is necessary as the token embeddings may encounter convergence difficulties in the initial epochs, which could adversely affect the training of the encoder-decoder layer.\n\u2022 In phase 2, once the token embeddings layer has achieved convergence, the pre-training process is expanded to utilize 100% of the available samples for both phases. This approach leads to an enhancement in the performance of the encoder-decoder layer, particularly in terms of token reconstruction.\nFor encoder pre-training we use the masked language model method defined in [22]. Initially 15% of the tokens are selected for possible learning. From that selection, 80% of the tokens are randomly selected and replaced with the [MASK] token, 10% of the tokens are randomly selected to be replaced with a random token, while the remaining 10% of the tokens will be unchanged.\nThe adoption of different pre-training strategies has proven instrumental in enhancing the efficiency of our model, as evidenced by improvements observed in the loss functions. For detailed insights into the loss functions and pre-training methodologies, refer to the Supplementary Materials."}, {"title": "Mixture-of-SMI-TED-Experts", "content": "The Mixture-of-SMI-TED-Experts, SMI-TED8x289M comprises a set of n \u201cexpert networks\u201d labeled as $E_1, E_2,..., E_n$, augmented through a gating network denoted as G, tasked with generating a sparse n-dimensional embedding space optimized for a downstream task as illustrated by Fig. 2.\nHere, we map each SMILES into tokens and then convert the input tokens to the latent space. A mean pooling method is applied to all token embeddings in order to produce a meaningful embedding of the molecule. The architecture is equipped with a router module responsible for determining the n"}, {"title": "Experiments", "content": "To evaluate the effectiveness of our proposed methodology, we conducted experiments using a set of 11 datasets sourced from MoleculeNet [27] as demonstrated in Table 2. Specifically, we evaluated 6 datasets for classification task and 5 datasets for regression tasks. To ensure an unbiased assessment, we maintained consistency with the original benchmark by adopting identical train/validation/test splits for all tasks [27]. We also conducted the experiments considered 10 different seeds for all the tests in other to guarantee the robustness of the approach. Details are provided in the Supplementary Materials."}, {"title": "Results and Discussion", "content": "In this section, we present the analysis of results obtained using SMI-TED289M for different experiments conducted with various versions of the base model. We include: i) A study comparing"}, {"title": "Comparison with SOTA on benchmarking tasks", "content": "Results for classification tasks: The analysis investigates the comparative efficacy of SMI- TED289M in its fine-tuned and frozen states versus state-of-the-art algorithms for molecular properties classification, as demonstrated in Table 3.\nTable 3 displays the performance of different advanced methods on different benchmarking datasets used for molecule classification tasks. SMI-TED289M consistently shows superior performance in four out of six datasets. Interestingly, using SMI-TED289M with its initial settings provided comparable results to SOTA methods available. However, fine-tuning SMI-TED289M further enhances its performance across all datasets. This indicates SMI-TED289M potential for accurate molecule classification, with potential for further optimization through fine-tuning. Detailed results for all the experiments are presented in the Supplementary Materials due to limit of pages.\nResults for regression tasks: Next, we applied SMI-TED289M for prediction of chemical proper- ties. The performance results across five challenging regression benchmarks, namely QM9, QM8, ESOL, FreeSolv, and Lipophilicity, are summarized in Table 4.\nResults presented in Table 4 indicates that SMI-TED289M presents superior results when compared to the state-of-the-art, outperforming its competitors in all the 5 datasets considered. To fine-tune SMI-TED289M is important to achieve state-of-the-art results in regression datasets, due to the complexity of such tasks. Table 4 elucidates the superiority of SMI-TED289M over the QM9 dataset. The QM9 dataset is composed by 12 tasks regarding to the quantum properties of molecules. A detailed overview over the results for QM9 are depicted in the next subsection. Detailed results for all experiments are in the Supplementary Materials of this paper.\nA deeper analysis over the QM9 benchmark: In this subsection, we provide a deeper analysis over the results for the QM9 dataset. Table 5 details the results of the SOTA approaches each property"}, {"title": "Mixture-of-SMI-TED-Experts perform studies", "content": "This study compare the results of MoE-SMI-TED against a single SMI-TED289M models (frozen and fine-tuned). SMI-TED8x289M is composed by 8 \u00d7 289M fine-tuned models for each specific task, we set k = 2, which means that 2 models are activated every step. The results for this study are shown in Table 6, which considers classification and regression tasks for molecular properties. Results refers to the best run of each version."}, {"title": "Decoder evaluation over MOSES benchmarking dataset", "content": "Next, we compared SMI-TED289M with different baseline models, such as the character-level recurrent neural network (CharRNN) [19], SMILES variational autoencoder (VAE) [19], junction tree VAE (JT-VAE) [44], latent inceptionism on molecules (LIMO) [45], MolGen-7b [46], and GP- MoLFormer [47]. All baseline performances are reported on their corresponding test set consisting of 176k molecules. Standard metrics for evaluating model-generated molecules are reported in Table 7. All metrics are computed using MOSES.\nWhen compared to baselines, SMI-TED289M is equally performant in generating unique, valid, and novel molecules that share high cosine similarity with the corresponding reference molecules at the fragment (Frag) level, consistent with low Fr\u00e9chet ChemNet Distance (FCD). At the same time, SMI-TED289M generates molecules with high internal diversity (IntDiv), i.e., average pairwise dissimilarity. The scaffold cosine similarity (Scaf) and similarity to the nearest neighbor in the test set (SNN) of SMI-TED289M is superior to the baselines demonstrating that SMI-TED289M is effective in generating molecules of varying structures and quality compared to baseline methods."}, {"title": "Latent space study", "content": "We conducted an experiment to investigate the structure of the latent space created by Large Language Models in the context of Chemistry. Molecular structures are composable from fragments, motifs, and functional groups. The composability of structure often translates into compositionality of structure-property relations, which is exemplified by powerful group contribution methods in chemical sciences. Compositionality of the learnt representation, however, does not follow automatically from the structure of the data and requires some combination of the learning architecture and learning constraints to emerge. Our approach was to utilize simple chemical structures that can be easily understood by humans, allowing us to anticipate relationships between elements, and examine the latent space for similar patterns. We constructed a dataset consisting of six families of carbon chains: $F = \\{CC,CO,CN,CS, CF, CP\\}$. For each family, we generated a sequence of molecules by incrementally adding carbon atoms to the end of the SMILES string, up to a maximum of ten carbon atoms. For example, the family CO consists of $\\{CO, CCO,\u2026\u2026\u2026 \u201eCCCCCCCCCCO\\}$. According to the domain expert's intuition consistent with the theory of chemical structure, in a metric space, such sequences should exhibit a hierarchical distance structure, where the distance between consecutive elements is smaller than the distance between elements with a larger difference in carbon count, i.e., $|C_nF_i \u2013 C_{n+1}F_i| < |C_nF_i \u2013 C_{n+2}F_i|$. Here, n represents the number of carbon atoms, and SMILE denotes the projection of the SMILE string onto the embedding space.\nFirst, we generated the embeddings for two different encoders, the MoLFormer and SMI-TED289M, and used the t-SNE [48] projection technique to generate pictures (Fig. 3) for visually inspecting the spaces. It is worth noting that the SMI-TED289M generated an embedding space that creates a nice separation of each family and respects the hierarchical distance structure, almost creating a linear relationship between each family. To quantify this relationship, we created a dataset of triples of SMILES, $T = \\{(C_nF_c c,C_kF_i, C_{n+k}F_i) | 0 < n \\leq 4,0 < k \\leq 5\\}$, for the six families Fi, resulting in six sub-datasets with 20 elements each, e.g., (CC,\u0421\u0421\u041e,\u0421\u0421\u0421\u0421O) is one element of the subset of type CO where n = 1, k = 2. Then, we randomly selected one triple from each subset to feed a linear regression calculating \u03b1, \u03b2, and Bo such that $a \u00b7 C_nF_{cc}+\u03b2\u00b7C_kF_i+ B_0 = C_{n+k}F_i$. We validated the linearity using the remaining 114 elements. The linear regression on the MoLFormer embeddings resulted in R2 = 0.55 and MSE = 0.237, while on our model embeddings, it resulted in R2 = 0.99 and MSE = 0.002."}, {"title": "Conclusion", "content": "This paper introduces the SMI-TED289M family of chemical foundation models, which are pre- trained on a curated dataset of 91 million SMILES samples from PubChem, amounting to 4 billion molecular tokens. The SMI-TED289M family includes two configurations: the base model with 289 million parameters and the MoE SMI-TED8x289M model, which consists of 8 \u00d7 289M parameters.\nThe performance of these models was evaluated through an extensive experimentation on different tasks, including molecular properties classification and prediction. Our approach achieved state- of-the-art results in most tasks, particularly in predicting molecular quantum mechanics, where it achieved the best or second-best results in 11 out of 12 tasks of the QM9 dataset.\nWe also investigated the structure of the latent space created by these language-based foundation models, using simple chemical structures for clarity. SMI-TED289M generated an embedding space that creates a nice separation of each family and respects the hierarchical distance structure, almost creating a linear relationship between each family. The encoder-decoder model's capabilities in few-shot learning were assessed by generating embeddings from a few example triples and using them to recreate SMILES strings, achieving a Tanimoto score of 0.92 in the best case.\nThe family of chemical foundation models presented in this paper offers flexibility and scalability for different scientific applications. The source code is available at: https://github.com/IBM/materials."}, {"title": "Supplementary Materials", "content": "A.1 Detailed results - frozen weights\nHere, we provide the detailed results for every experiment conducted in this paper. First, we present the detailed results for the experiments considering frozen weights of SMI-TED289M for both, classification and regression tasks, considering the MoleculeNet benchmarking dataset. For SMI-TED289D frozen weights, we considered XGBoost [50] as learner, and Optuna [51] for hyper-parameters optimization. Table 8 illustrates the results for the classification tasks using for 10 different seeds, and considering frozen weights.\n A.2 Detailed results - Fine-tuning\nTo fine-tune SMI-TED289M, we used a fully connected network with 2 layers. Table 10 provides a detailed overview of the hyper-parameters considered for the fine-tuning of SMI-TED289M. We used a single V100 NVIDIA (16G) GPU for the task. Detailed results considering SMI-TED289M for both, classification and regression tasks using the MoleculeNet benchmarking dataset are illustrated in Table 11 and Table 12. We run each task for 10 different seeds to guarantee the robustness of the results."}]}