{"title": "A Large Encoder-Decoder Family of Foundation Models For Chemical Language", "authors": ["Eduardo Soares", "Victor Shirasuna", "Emilio Vital Brazil", "Renato Cerqueira", "Dmitry Zubarev", "Kristin Schmidt"], "abstract": "Large-scale pre-training methodologies for chemical language models represent a breakthrough in cheminformatics. These methods excel in tasks such as property prediction and molecule generation by learning contextualized representations of input tokens through self-supervised learning on large unlabeled corpora. Typically, this involves pre-training on unlabeled data followed by fine-tuning on specific tasks, reducing dependence on annotated datasets and broadening chemical language representation understanding. This paper introduces a large encoder-decoder chemical foundation models pre-trained on a curated dataset of 91 million SMILES samples sourced from PubChem, which is equivalent to 4 billion of molecular tokens. The proposed foundation model supports different complex tasks, including quantum property prediction, and offer flexibility with two main variants (289M and 8 \u00d7 289M). Our experiments across multiple benchmark datasets validate the capacity of the proposed model in providing state-of-the-art results for different tasks. We also provide a preliminary assessment of the compositionality of the embedding space as a prerequisite for the reasoning tasks. We demonstrate that the produced latent space is separable compared to the state-of-the-art with few-shot learning capabilities.", "sections": [{"title": "Introduction", "content": "Understanding molecular properties is crucial for accelerating discoveries in different fields, including drug development and materials science [1]. Traditional methods rely on labor-intensive trial-and-error experiments, which are both costly and time-consuming [2]. However, recent advances in deep learning have enabled the use of foundation models to predict molecular properties and generate molecule candidates [3, 4, 5], marking significant progress in scientific exploration.\nThe introduction of large-scale pre-training methodologies for chemical language models (LMs) represents a significant advancement in cheminformatics [6]. These methodologies have demonstrated impressive results in challenging molecular tasks such as predicting properties and generating molecules [7]. The success of these models can be attributed to their ability to learn contextualized representations of input tokens through self-supervised learning on large unlabeled corpora [8]. This methodological approach typically involves two phases: pre-training on unlabeled data followed by fine-tuning on specific downstream task [9]. By reducing the reliance on annotated datasets, this approach has broadened our understanding of chemical language representations [10]."}, {"title": null, "content": "Simplified Molecular-Input Line Entry System, SMILES, provide natural graphs that encode the connectivity information from the line annotations of molecular structures [11]. SMILES defines a character string representation of a molecule by performing a depth-first pre-order spanning tree traversal of the molecular graph, generating symbols for each atom, bond, tree-traversal decision, and broken cycles [12]. Therefore, the resulting character string corresponds to a flattening of a spanning tree of the molecular graph. SMILES is widely adopted for molecular property prediction as SMILES is generally more compact than other methods of representing structure, including graphs [13]. There are billions of SMILES available on different open-sources repositories [14]. However, most SMILES sequences do not belong to well-defined molecules [15]. Alternative string-based representations exist, such as SELFIES. However, focusing on molecular optimization tasks on the learned representation space, suggested no obvious shortcoming of SMILES with respect to SELFIES in terms of optimization ability and sample efficiency [16]. The quality of the pre-training data plays a more important role on the outcome of the foundation model [4, 17].\nTowards this direction, we present a novel family of molecular encoder-decoder foundation models, denoted as SMI-TED289M. Our SMI-TED289M encoder-decoder foundation model was obtained using a transformer-based molecular tokens encoder model aligned with an encoder-decoder mechanism trained on a large corpus of 91 million carefully curated molecules from PubChem [18], resulting in 4 billion molecular tokens. Our main contributions are:\n\u2022 We pre-train a large-scale family of encoder-decoder molecular open-source foundation models, denoted as SMI-TED289M, on over 91 million molecules carefully curated from PubChem [18], which is equivalent to 4 billion of molecular tokens.\n\u2022 A molecular dataset for pre-training of chemical foundation models, 91 million molecules carefully curated from PubChem [18].\n\u2022 Our SMI-TED289M family of foundation models encompasses two distinct configurations: base, which has 289 million parameters; and the Mixture-of-SMI-TED-Experts, SMI-TED8x289M, characterized by a composition of 8 \u00d7 289M parameters. The source code is available at: https://github.com/IBM/materials.\n\u2022 We perform extensive experimentation on several classification and regression tasks from 11 benchmark datasets, covering quantum mechanical, physical, biophysical, and physiological property prediction of small molecules. We also evaluate the reconstruction capacity of our SMI-TED289M considering the MOSES benchmarking dataset [19]. Furthermore, a study investigating the embedding created by SMI-TED289M and few-shot learning is also provided, indicating compositionality of the learned molecular representations.\nOur results section demonstrates state-of-the-art performance of SMI-TED289M on different tasks, molecular properties prediction, molecule reconstruction, and an efficient metric for molecular latent space. Compositionality of the latent space suggests strong potential for chemical reasoning tasks. The SMI-TED289M family consists of two main variants (289M, and 8 \u00d7 289M), offering flexibility and scalability for different scientific applications."}, {"title": "Overview of the proposed approach", "content": "This section presents an overview of the proposed SMI-TED289M foundation model for small molecules. Here, we outline the process of collecting, curating, and pre-processing the pre-train data. Additionally, we describe the token encoder process and the SMILES encoder-decoder process. Finally, we explain the Mixture-of-SMI-TED-Experts approach used to scale the base model. Fig. 1 illustrates the general architecture of the base model."}, {"title": "Pre-training Data", "content": "The pretraining data originated from the PubChem data repository, a public database containing information on chemical substances and their biological activities [18]. Initially, 113 million SMILES strings were collected from PubChem. These molecular strings underwent deduplication and canon-icalization processes to ensure uniqueness [20]. Subsequently, a molecular transformation was conducted to verify the validity of the molecules derived from the unique SMILES strings, resulting in a set of 91 million unique and valid molecules."}, {"title": "Model Architecture", "content": "We conduct training for SMI-TED289M model employing a deep-bidirectional-transformers-based encoder [22] for tokens and an encoder-decoder architecture to compose SMILES. The hyper-parameters of SMI-TED289M base model are detailed in Table 1\nTo optimize the relative encoding through position-dependent rotations \\(R_m\\) of the query and keys at position m, the SMI-TED289M uses a modified version of the RoFormer [23] attention mechanism. These rotations can be implemented as pointwise multiplications and do not significantly increase computational complexity as shown in Eq. (1).\n\\(\\text{Attention}_m (Q, K, V) = \\frac{\\sum_{n=1}^{N} \\langle \\varphi(R_m q_m), \\varphi(R_n k_n) \\rangle v_n}{\\sum_{n=1}^{N} \\langle \\varphi(R_m q_m), \\varphi(R_n k_n) \\rangle}\\) (1)\nwhere Q,K,V are the query, key, and value respectively, and \\(\\varphi\\) is a random feature map.\nWe start with a sequence of tokens extracted from SMILES, each embedded in a 768-dimensional space. The encoder-decoder layer is designed to process molecular token embeddings, represented as \\(x \\in \\mathbb{R}^{D \\times L}\\), where D denotes the maximum number of tokens and L represents the embedding space dimension. We limited D at 202 tokens, as 99.4% of molecules in the PubChem dataset contain fewer tokens than this threshold.\nIn encoder-only models, a mean pooling layer is typically employed to represent tokens as SMILES in the latent space. However, this approach is limited by the lack of a natural inversion process for the mean pooling operation. To overcome this limitation, we aim to construct a latent space representation for SMILES by submersing the x in a latent space, denoted as z, as described in Eq. 2.\n\\(z = \\varphi(\\text{LayerNorm}(\\text{GELU}(xW_1 + b_1))) W_2,\\) (2)"}, {"title": null, "content": "where \\(z \\in \\mathbb{R}^{L}\\), \\(W_1 \\in \\mathbb{R}^{D \\times L}\\), \\(b_1 \\in \\mathbb{R}^{L}\\), \\(W_2 \\in \\mathbb{R}^{L \\times L}\\), with L denoting the latent space size (specifically, L = 768) and D representing the original feature space size (namely, D = 202). Subsequently, we can immerse z back by calculating Eq. 3.\n\\(x = \\varphi(\\text{LayerNorm}(\\text{GELU}(zW_3 + b_3))) W_4\\) (3)\nwhere \\(x \\in \\mathbb{R}^{D \\times L}\\), \\(W_3 \\in \\mathbb{R}^{L \\times L}\\), \\(b_3 \\in \\mathbb{R}^{L}\\), \\(W_4 \\in \\mathbb{R}^{L \\times D}\\).\nA language layer (decoder) is used to process x, where it applies non-linearity and normalization, and projects the resulting vector into a set of logits over the vocabulary, which can then be used to predict the next token in the molecular [24]."}, {"title": "Pre-training strategies", "content": "Pre-training of SMI-TED289M was performed for 40 epochs through the entire curated PubChem dataset with a fixed learning rate of 1.6e-4 and a batch size of 288 molecules on a total of 24 NVIDIA V100 (16G) GPUs parallelized into 4 nodes using DDP and torch run. It involves two distinct phases: i) Learning of token embeddings through a masking process; ii) Subsequently, the token embeddings are mapped into a common latent space that encapsulates the entire SMILES string. This latent space not only facilitates the representation of the SMILES but also enables the reconstruction of both individual tokens and complete SMILES strings. Consequently, the pre-training process involves two separate loss functions: one for the token embeddings, which is based on the masking process, and another for the encoder-decoder layer, which focuses on the reconstruction of tokens. Two pre-training strategies are employed:\n\u2022 In phase 1, the token encoder is initially pre-trained using 95% of the available samples, while the remaining 5% is reserved for training the encoder-decoder layer. This partitioning is necessary as the token embeddings may encounter convergence difficulties in the initial epochs, which could adversely affect the training of the encoder-decoder layer.\n\u2022 In phase 2, once the token embeddings layer has achieved convergence, the pre-training process is expanded to utilize 100% of the available samples for both phases. This approach leads to an enhancement in the performance of the encoder-decoder layer, particularly in terms of token reconstruction.\nFor encoder pre-training we use the masked language model method defined in [22]. Initially 15% of the tokens are selected for possible learning. From that selection, 80% of the tokens are randomly selected and replaced with the [MASK] token, 10% of the tokens are randomly selected to be replaced with a random token, while the remaining 10% of the tokens will be unchanged.\nThe adoption of different pre-training strategies has proven instrumental in enhancing the efficiency of our model, as evidenced by improvements observed in the loss functions. For detailed insights into the loss functions and pre-training methodologies, refer to the Supplementary Materials."}, {"title": "Mixture-of-SMI-TED-Experts", "content": "The Mixture-of-SMI-TED-Experts, SMI-TED8x289M comprises a set of n \u201cexpert networks\u201d labeled as \\(E_1, E_2, ..., E_n\\), augmented through a gating network denoted as G, tasked with generating a sparse n-dimensional embedding space optimized for a downstream task as illustrated by Fig. 2.\nHere, we map each SMILES into tokens and then convert the input tokens to the latent space. A mean pooling method is applied to all token embeddings in order to produce a meaningful embedding of the molecule. The architecture is equipped with a router module responsible for determining the n experts that will be activated, refining the adaptability and specialization of the system. Let G(x) and \\(E_i(x)\\) denote the output of the gating network and the output of the i-th expert network, respectively, for a given input \\(\\hat{x}\\) of SMILES and x, which is the embeddings space, following a similar notation as proposed in [25]. The resulting output y is defined as follows:\n\\(y = \\sum_{i=1}^{n} G(x)_i E_i(x)\\)\nThe resulting embedding space y is used to train a task-specific feed-forward network, where the loss function is chosen according to the studied downstream task. The optimization process refines the parameters of G(x). If the gating vector is sparse, we can use softmax over the Top-K logits of a linear layer [25].\n\\(G(x) := \\text{Softmax}(\\text{TopK}(x \\cdot W_g))\\)\nwhere \\((\\text{TopK}(l))_i := l_i\\) if \\(l_i\\) is among the TopK coordinates of logits \\(l \\in \\mathbb{R}^{n}\\) and \\((\\text{TopK}(l))_i := - \\infty\\) otherwise. The router layer retains only the top k values, setting the remaining values to -\u221e (which effectively assigns corresponding gate values as 0). This sparsity-inducing step serves to optimize computational efficiency [26]. Here, we define SMI-TED8x289M as n = 8 and k = 2, which means that SMI-TED8x289M is composed by 8\u00d7 SMI-TED289M models, which 2 models are activated through the router each round."}, {"title": "Experiments", "content": "To evaluate the effectiveness of our proposed methodology, we conducted experiments using a set of 11 datasets sourced from MoleculeNet [27] as demonstrated in Table 2. Specifically, we evaluated 6 datasets for classification task and 5 datasets for regression tasks. To ensure an unbiased assessment, we maintained consistency with the original benchmark by adopting identical train/validation/test splits for all tasks [27]. We also conducted the experiments considered 10 different seeds for all the tests in other to guarantee the robustness of the approach. Details are provided in the Supplementary Materials."}, {"title": "Results and Discussion", "content": "In this section, we present the analysis of results obtained using SMI-TED289M for different experiments conducted with various versions of the base model. We include: i) A study comparing frozen and fine-tuned versions of SMI-TED289M; and a comparison with the State-of-the-Art (SOTA) on different benchmarking datasets for classification and regression molecular prediction tasks; ii) An evaluation of SMI-TED8x289M for molecular properties prediction; iii) An evaluation of the Decoder module considering the MOSES benchmarking dataset; iv) A study comparing the latent space of SMI-TED289M based on compositional molecules metrics."}, {"title": "Comparison with SOTA on benchmarking tasks", "content": "Results for classification tasks: The analysis investigates the comparative efficacy of SMI-TED289M in its fine-tuned and frozen states versus state-of-the-art algorithms for molecular properties classification"}, {"title": "Decoder evaluation over MOSES benchmarking dataset", "content": "Next, we compared SMI-TED289M with different baseline models, such as the character-level recurrent neural network (CharRNN) [19], SMILES variational autoencoder (VAE) [19], junction tree VAE (JT-VAE) [44], latent inceptionism on molecules (LIMO) [45], MolGen-7b [46], and GP-MoLFormer [47]. All baseline performances are reported on their corresponding test set consisting of 176k molecules. Standard metrics for evaluating model-generated molecules are reported in Table 7. All metrics are computed using MOSES.\nWhen compared to baselines, SMI-TED289M is equally performant in generating unique, valid, and novel molecules that share high cosine similarity with the corresponding reference molecules at the fragment (Frag) level, consistent with low Fr\u00e9chet ChemNet Distance (FCD). At the same time, SMI-TED289M generates molecules with high internal diversity (IntDiv), i.e., average pairwise dissimilarity. The scaffold cosine similarity (Scaf) and similarity to the nearest neighbor in the test set (SNN) of SMI-TED289M is superior to the baselines demonstrating that SMI-TED289M is effective in generating molecules of varying structures and quality compared to baseline methods."}, {"title": "Latent space study", "content": "We conducted an experiment to investigate the structure of the latent space created by Large Language Models in the context of Chemistry. Molecular structures are composable from fragments, motifs, and functional groups. The composability of structure often translates into compositionality of structure-property relations, which is exemplified by powerful group contribution methods in chemical sciences. Compositionality of the learnt representation, however, does not follow automatically from the structure of the data and requires some combination of the learning architecture and learning constraints to emerge. Our approach was to utilize simple chemical structures that can be easily understood by humans, allowing us to anticipate relationships between elements, and examine the latent space for similar patterns. We constructed a dataset consisting of six families of carbon chains: \\(F = \\{CC,CO,CN,CS, CF, CP\\}\\). For each family, we generated a sequence of molecules by incrementally adding carbon atoms to the end of the SMILES string, up to a maximum of ten carbon atoms. For example, the family CO consists of \\(\\{CO, CCO,.........,CCCCCCCCCCO\\}\\).\nAccording to the domain expert's intuition consistent with the theory of chemical structure, in a metric space, such sequences should exhibit a hierarchical distance structure, where the distance between consecutive elements is smaller than the distance between elements with a larger difference in carbon count, i.e., \\(|C_nF_i \u2013 C_{n+1}F_i| < |C_nF_i \u2013 C_{n+2}F_i|\\). Here, n represents the number of carbon atoms, and SMILE denotes the projection of the SMILE string onto the embedding space.\nFirst, we generated the embeddings for two different encoders, the MoLFormer and SMI-TED289M, and used the t-SNE [48] projection technique to generate pictures (Fig. 3) for visually inspecting the spaces. It is worth noting that the SMI-TED289M generated an embedding space that creates a nice separation of each family and respects the hierarchical distance structure, almost creating a linear relationship between each family. To quantify this relationship, we created a dataset of triples of SMILES, \\(T = \\{(C_nF_{cc},C_kF_i, C_{n+k}F_i) | 0 < n \\leq 4,0 < k \\leq 5\\}\\), for the six families Fi, resulting in six sub-datasets with 20 elements each, e.g., (CC,\u0421\u0421\u041e,\u0421\u0421\u0421\u0421O) is one element of the subset of type CO where n = 1, k = 2. Then, we randomly selected one triple from each subset to feed a linear regression calculating \u03b1, \u03b2, and Bo such that \\(\\alpha \\cdot C_nF_{cc}+\\beta\\cdot C_kF_i+ B_0 = C_{n+k}F_i\\). We validated the linearity using the remaining 114 elements. The linear regression on the MoLFormer embeddings resulted in R2 = 0.55 and MSE = 0.237, while on our model embeddings, it resulted in R2 = 0.99 and MSE = 0.002."}, {"title": null, "content": "We evaluated our encoder-decoder model using a few-shot learning process, where we input a few examples of triples, such as those mentioned earlier, to calculate \u03b1, \u03b2, and Bo. We then use these parameters to generate embeddings for subsequent SMILES pairs and recreate the SMILES strings. To validate our approach, we tested the process on the same dataset of triples. We calculated the molecule similarity between the expected and generated results using the Tanimoto score (TS) [49]. We repeated this test with different combinations of input triples, yielding similar results. For example, when using the input triples [CC+CCCS = CCCCCS, CCCCC+CCCS = CCCCCCCCS] and querying all pairs in our subsets, we obtained a mean TS of 0.52. The top two similar results were CC + CCCCCS = CCCCCS with TS = 0.92 and CC + CCCCCO = CCCCCO with TS = 0.92, while the bottom two results were CCCCC + CF = F[PH3+]F with TS = 0.06 and CCCC + CF = F[PH3+]F with TS = 0.07.\nHistorically, group contribution was introduced in supervised learning context of structure-property relations. Our simple tests indicate that SMI-TED289M derived an equivalent of group contribution method purely from self-supervised learning of molecular structure. Signs of the emergence of compositionality of the learned molecular representations suggest strong potential of SMI-TED289M for reasoning applications. Further studies consistent with methodologies of compositionality analysis in natural languages are required to make stronger statements."}, {"title": "Conclusion", "content": "This paper introduces the SMI-TED289M family of chemical foundation models, which are pre-trained on a curated dataset of 91 million SMILES samples from PubChem, amounting to 4 billion molecular tokens. The SMI-TED289M family includes two configurations: the base model with 289 million parameters and the MoE SMI-TED8x289M model, which consists of 8 \u00d7 289M parameters.\nThe performance of these models was evaluated through an extensive experimentation on different tasks, including molecular properties classification and prediction. Our approach achieved state-of-the-art results in most tasks, particularly in predicting molecular quantum mechanics, where it achieved the best or second-best results in 11 out of 12 tasks of the QM9 dataset.\nWe also investigated the structure of the latent space created by these language-based foundation models, using simple chemical structures for clarity. SMI-TED289M generated an embedding space that creates a nice separation of each family and respects the hierarchical distance structure, almost creating a linear relationship between each family. The encoder-decoder model's capabilities in few-shot learning were assessed by generating embeddings from a few example triples and using them to recreate SMILES strings, achieving a Tanimoto score of 0.92 in the best case.\nThe family of chemical foundation models presented in this paper offers flexibility and scalability for different scientific applications. The source code is available at: https://github.com/IBM/materials."}, {"title": "A Supplementary Materials", "content": null}, {"title": "Detailed results - frozen weights", "content": "Here, we provide the detailed results for every experiment conducted in this paper. First, we present the detailed results for the experiments considering frozen weights of SMI-TED289M for both, classification and regression tasks, considering the MoleculeNet benchmarking dataset. For SMI-TED289D frozen weights, we considered XGBoost [50] as learner, and Optuna [51] for hyper-parameters optimization. Table 8 illustrates the results for the classification tasks using for 10 different seeds, and considering frozen weights."}, {"title": "Detailed results - Fine-tuning", "content": "To fine-tune SMI-TED289M, we used a fully connected network with 2 layers. Table 10 provides a detailed overview of the hyper-parameters considered for the fine-tuning of SMI-TED289M. We used a single V100 NVIDIA (16G) GPU for the task. Detailed results considering SMI-TED289M for both, classification and regression tasks using the MoleculeNet benchmarking dataset are illustrated in Table 11 and Table 12. We run each task for 10 different seeds to guarantee the robustness of the results."}]}