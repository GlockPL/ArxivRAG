{"title": "See Further for Parameter Efficient Fine-tuning by Standing on the Shoulders of Decomposition", "authors": ["Chongjie Si", "Xiaokang Yang", "Wei Shen"], "abstract": "The rapid expansion of large foundation models within the pre-training and fine-tuning framework has underscored that larger models often yield better results. However, the scaling up of large foundation models has led to soaring costs in fine-tuning and parameter storage, rendering extensive adaptations impractical. This challenge has sparked the development of parameter-efficient fine-tuning (PEFT), which focuses on optimizing a select subset of parameters while keeping the rest fixed, significantly lowering computational and storage overheads. While recent years have witnessed a significant success in PEFT, a deep understanding of the fundamental principles behind these methods remains unexplored. To this end, here we take the first step to unify all approaches by dissecting them from a decomposition perspective. We initiate a comprehensive mathematical analysis of these methods, allowing us to delve deeply into their underlying mechanisms, and we explore the reasons behind the variations in performance among different techniques. Furthermore, inspired by our theoretical analysis, we introduce two novel PEFT methods alongside a simple yet effective framework designed to enhance the performance of PEFT techniques across various applications. Our empirical validations, conducted across multiple datasets, demonstrate the efficacy of these methods, showcasing both theoretical validity and practical performance improvements under the guidance of our analytical findings. Moreover, to support research in subspace tuning, we are developing an open-source toolkit called Subspace-Tuning\u00b9. This toolkit allows practitioners to efficiently and flexibly implement subspace tuning on different pretrained models for different tasks. We believe our work will deepen researchers' understanding of PEFT and other techniques, prompting further contemplation and advancing the research across the whole community.", "sections": [{"title": "1 Introduction", "content": "The emergence of foundation models, as referenced in multiple studies (Brown et al. [2020], Radford et al. [2019, 2021], Devlin et al. [2018], Liu et al. [2019]), has fundamentally altered the landscape of artificial intelligence, demonstrating substantial effectiveness across a variety of domains. For example, Segment Anything Model (SAM) (Kirillov et al. [2023]) has been widely implemented across a variety of visual tasks (Zhang and Liu [2023], Si et al. [2024b], Zhang et al. [2023]), and Generative Pre-trained Transformer (GPT) (Brown et al. [2020], Radford et al. [2019]) has even seamlessly integrated into our daily lives, evolving into an exceedingly practical tool (Achiam et al. [2023], Waisberg et al. [2023], Mao et al. [2023]). Traditionally, the adaptation of pre-trained models to specific downstream tasks required fully fine-tuning of all parameters (Ma et al. [2024], Raffel et al. [2020], Qiu et al. [2020]). However, as the complexity and size of these models have increased, this traditional approach to fine-tuning has become less feasible, both from a computational and resource standpoint.\nIn response to these challenges, there has been a pivot towards developing more parameter-efficient fine-tuning techniques (Chen et al. [2024], Guo et al. [2020], He et al. [2021a], Hu et al. [2021]), collectively known as parameter-efficient fine-tuning (PEFT). The goal of PEFT is to achieve comparable performance on downstream tasks by tuning a minimal number of parameters. Presently, PEFT strategies can be categorized into three predominant groups (Liu et al. [2024], Ding et al. [2023]), each with its distinctive mechanisms and intended use cases.\nFirstly, adapter-based methods, as discussed in several works (Houlsby et al. [2019], Chen et al. [2022], Luo et al. [2023], He et al. [2021a], Mahabadi et al. [2021], Karimi Mahabadi et al. [2021]), involve the insertion of small, trainable linear modules within the pre-existing network architectures. These modules are designed to adapt the model's outputs without changing the original network weights. Secondly, the prompt-based approaches (Lester et al. [2021], Razdaibiedina et al. [2023], Wang et al. [2023], Shi and Lipani [2023], Fischer et al. [2024]) make use of mutable soft tokens placed at the beginning of inputs. This strategy focuses on fine-tuning these prompts to steer the model's behavior during specific tasks. Thirdly, low-rank decomposition approaches like LoRA (Hu et al. [2021], Liu et al. [2024], Hyeon-Woo et al. [2021], Qiu et al. [2023], Renduchintala et al. [2023], Kopiczko et al. [2023], YEH et al. [2023], Zhang et al. [2022], Si et al. [2024c]) are applied to network weights during fine-tuning, enhancing their adaptability while maintaining overall compatibility with the pre-trained settings. Additionally, the landscape of PEFT is enriched by other innovative methods such as BitFit (Zaken et al. [2021], Lawton et al. [2023]), which focus solely on fine-tuning bias terms. Collectively, these diverse strategies significantly augment the adaptability and efficiency of models, enabling them to meet specific task requirements without the need for extensive retraining. Through these developments, the whole community continues to evolve towards more sustainable and manageable model training methodologies.\nHowever, despite that recent years have witnessed significant advancements in PEFT (Han et al. [2024], He et al. [2021a], Fu et al. [2023]), the mathematical foundations underpinning different PEFT methods have scarcely been studied. Moreover, the performance differences between various PEFT methods and the reasons behind these differences have not been systematically explored. This lack of theoretical depth limits our understanding of the potential advantages and limitations of these methods, hindering their optimization and innovation in practical applications. Therefore, conducting theoretical research in this field will be crucial for advancing PEFT technologies, providing a fundamental basis for selecting and designing more efficient fine-tuning strategies.\nTherefore, in this paper, we undertake a pioneering theoretical examination of PEFT techniques, leveraging insights from decomposition theory including matrix (decomposition) and subspace (decomposition) theory. We introduce a novel framework termed subspace tuning, which encapsulates all known PEFT methods under a unified theory. The subspace tuning method primarily focuses on adjusting the subspace of the original parameter, involving both the reconstruction and the extension of subspaces. We delve into how different methods manipulate subspaces and elucidate the mathematical principles underlying each approach from the perspective of decomposition theory. Additionally, we analyze why these methods result in performance differences, providing a comprehensive theoretical foundation to understand the dynamics within different PEFT strategies.\nFurthermore, inspired by our theoretical analysis, we propose two novel PEFT methods. Compared to existing techniques, these new approaches achieve performance close to fully fine-tuning with only"}, {"title": "2 Subspace Tuning", "content": "Consider $W \\in \\mathbb{R}^{n\\times m}$ as the frozen weight matrix for a layer in any given backbone network and $n\\leq m$, without loss of generality. We quantify the performance of the model using the weight matrix $W$ as $P(W)$, where a higher value indicates better performance. For a specific task, assuming the existence of an optimal weight matrix $W^*$ (Ding et al. [2023]), we assert that $P(W^*) > P(W)$ for $\\forall W\\in \\mathbb{R}^{n\\times m}$. The objective of PEFT is therefore formulated as:\n$\\min_\\Phi l (W^*, \\Phi(W))$,\n(1)\nwhere $l$ measures the difference between the two matrices. In previous works, the function $\\Phi$ has been conceptualized as delta-tuning, representing a modification to each element of the matrix $W$. While this characterization is accurate, it is overly general and does not adequately capture the underlying logic of each approach. Indeed, from the perspective of decomposition theory, adjusting matrices essentially involves modifying their corresponding subspaces. Therefore, all the PEFT methods can be viewed as Subspace Tuning (Fig. 1a), and we propose viewing $\\Phi(\\cdot)$ as a transformation function that modifies the subspace associated with a matrix. Consequently, what equation (1) aims to do is to find the maximal projection of $W^*$ within the subspace spanned by the bases of $\\Phi(W)$ and then align $W$ to it. It is evident that there are two ways to achieve this objective:\n*   Approximation of the projection of $W^*$ by adjusting $W$;\n*   Manipulation of the subspace of $\\Phi(W)$ to approach or encompass $W^*$.\nTherefore, we attribute two primary roles to function $\\Phi$:\n*   Direct reconstruction of the subspace corresponding to $W$ to better align $W^*$;\n*   Introduction of a new subspace and span it with the original subspace.\nThese two processes can be mathematically represented as follows:\n$\\Phi(W) = g(f(W)).$\n(2)\nHere, $f(W)$ encapsulates the subspace reconstruction process for $W$, and $g(f(W))$ describes the union of the subspaces. We denote these operations as \u201cSubspace Reconstruction\u201d and \u201cSubspace Extension\", respectively (Fig. 1b). Therefore, we categorize the existing methods into the following three categories: subspace reconstruction-based, extension-based and combination-based (Fig. 1c).\n*   Reconstruction-based methods decompose the complex space associated with the original weight matrix $W$ into more intuitive and comprehensible subspaces and adjusting the bases of these derived subspaces.\n*   Extension-based methods introduce a subspace associated with the matrix $\\Delta W$. They seek to identify the maximal projection of the optimal weight $W^*$ within the space, which is spanned by the bases of the new subspace and that of the subspace corresponding to the original weight matrix $W$.\n*   Combination-based methods simultaneously adopt the aforementioned subspace adjustments."}, {"title": "3 Subspace Reconstruction", "content": "Building upon the framework outlined previously, methods leveraging subspace reconstruction initiate by segmenting the space of $W$ into interpretable subspaces. These subspaces are then refined to improve model efficiency. For these methods, the transformation function $\\Phi(\\cdot)$ is succinctly expressed as $\\Phi(W) = f(W)$. Numerous PEFT strategies concentrate on directly reconstructing the subspaces related to the original weight matrix. Prominent examples include SAM-PARSER (Peng et al. [2024]), Diff Pruning (Guo et al. [2020]), (IA)\u00b3 (Liu et al. [2022]), BitFit (Zaken et al. [2021]), Prefix-tuning (Li and Liang [2021]), and Prompt-tuning (Lester et al. [2021]), etc.\nWe commence by exploring the Singular Value Decomposition (SVD), a pivotal technique in subspace decomposition. The original weight matrix $W$ is decomposed into orthogonal subspaces that together encompass the entirety of the original matrix space. This decomposition is formally represented as"}, {"title": "3.1 Mode 1 Reconstruction: Singular Value Adjustment", "content": "Mode 1 methods operate under the premise that the optimal weight matrix $W^*$ and the original weight matrix $W$ share identical bases for their row and column spaces. Consequently, their SVD decomposition is characterized by the same singular vectors $U$ and $V$. Given this assumption, the SVD composition of $W^*$ can be expressed as\n$W^* = U\\Sigma^*V^T,$\n(4)\nwhere $\\Sigma^*$ represents the singular values that are optimized for the desired configuration. This formulation suggests that pinpointing the appropriate singular values for the optimal subspace is a direct and feasible approach, and this is what SAM-PARSER (Peng et al. [2024]) actually does."}, {"title": "3.2 Mode 2 Reconstruction: Simple Singular Vector Adjustment", "content": "Shifting our focus from adjusting singular values, we now turn our attention to manipulating the subspaces defined by the singular vectors, i.e., Mode 2. Initially, we will concentrate on scaling the subspaces, which can be applied to the column or row spaces of the singular vectors. This is formally represented as:\n$T(U) = UD \\text{ or } DU,$\n(5)\nwhere $T(\\cdot)$ denotes the subspace scaling function and $D$ is a diagonal matrix."}, {"title": "3.2.1 Scale the Column Space", "content": "If we scale the column space of singular vectors by assigning distinct weights to each vector, we define the transformations as $T_1(U) = UD_1$ and $T_2(V) = VD_2$, where $D_1 \\in \\mathbb{R}^{n\\times n}$ and $D_2 \\in \\mathbb{R}^{m\\times m}$ are diagonal matrices. The reconstructed weight matrix $\\tilde{W}$ can then be obtained as:\n$\\tilde{W} = T_1(U)\\Sigma T_2(V^T)$\n$= UD_1\\Sigma D_2V^T$\n$= U\\tilde{\\Sigma}V^T,$\n(6)\nwhere $\\tilde{\\Sigma}= D_1\\Sigma D_2$. Therefore, scaling the column space of singular vectors is essentially an adjustment of the singular values."}, {"title": "3.2.2 Scale the Row Space", "content": "We can also apply distinct weights to each row of the singular vectors, defined as $T_1(U) = D_1U$ and $T_2(V) = D_2V$. Consequently, the reconstructed weight matrix $W$ is articulated as follows:\n$\\tilde{W} = T_1(U)\\Sigma T_2(V^T)$\n$= D_1U\\Sigma V^T D_2$\n$= D_1WD_2.$\n(7)\nThus, scaling the row space spanned by the left and right singular vectors essentially corresponds to scaling both the row and column spaces of the original weight matrix.\nFrom this perspective, some methods can yield more in-depth explanations, such as (IA)\u00b3 (Liu et al. [2022]). In the original paper (Liu et al. [2022]), this method seeks to directly modify the activations within the model by introducing a learnable vector $1 \\in \\mathbb{R}^m$ to rescale the original weight matrix. The transformation is implemented via the Hadamard product, represented as $1 \\odot W$. However, it can equivalently be expressed as $WD_2$, where $D_2 \\in \\mathbb{R}^{m\\times m}$ is a diagonal matrix. Consequently, this approach actually scales the subspace of the right singular vectors, thereby reconstructing the original weight matrix $W$.\nThe results in Fig. 2c demonstrate that merely scaling the subspace of the right singular vectors, i.e., (IA)\u00b3, can achieve performance comparable to fully fine-tuning. This insight naturally gives rise to an additional adjustment method: Scaling the Subspace of the Left singular vectors (SSL). If the dimensions of the subspaces spanned by both left and right singular vectors are comparable, the performance of SSL and (IA)\u00b3 are expected to be similar, since both methods enhance model adaptation by scaling a singular subspace. This is corroborated by the results shown in Fig. 2c (Supplementary Tables 2-4).\nFurther expanding on this concept, we introduce the method of Scaling the Subspace of Both left and right singular vectors (SSB). Theoretically, SSB should outperform both SSL and (IA)\u00b3 as it simultaneously scales both subspaces, potentially enhancing the reconstruction quality beyond the capabilities of single-subspace scaling. Results from Fig. 2c and detailed in Supplementary Tables 2-4, indicate that SSB is significantly superior to SSL and (IA)\u00b3. Additionally, while training fewer than one-thousandth of the parameters, SSB closely approximates the outcomes of fully fine-tuning.\nOverall, these findings underscore the effectiveness of the adjustments specified in equation (7), confirming the potential of subspace scaling."}, {"title": "3.3 Mode 3 Reconstruction: Complex Singular Vector Adjustment", "content": "Mode 3 methods allow for more complicated transformations of the subspace spanned by singular vectors, with the form as $T_1(U) = UT_1$ and $T_2(V) = VT_2$, where $T_1 \\in \\mathbb{R}^{n\\times n}$ and $T_2 \\in \\mathbb{R}^{m\\times m}$ are two arbitrary matrices. Furthermore, $T_1$ and $T_2$ for nonlinear transformations are also allowed in Mode 3. These transformations can effectively convert the linear subspaces spanned by singular vectors into new nonlinear subspaces. In practice, we can achieve this by directly altering specific elements of $W$. Representative methods include BitFit derivatives (Zaken et al. [2021], Lawton et al. [2023]), soft prompt derivatives (Li and Liang [2021], Lester et al. [2021]) and others (Guo et al. [2020], Sung et al. [2021], Das et al. [2023], Gheini et al. [2021], He et al. [2023], Liao et al. [2023])."}, {"title": "3.3.1 BitFit Derivatives", "content": "BitFit (Zaken et al. [2021]) is designed to optimize solely the bias terms within a model while keeping all other parameters frozen, achieving performance comparable to full fine-tuning. Extending this concept, S-BitFit (Lawton et al. [2023]) integrates Network Architecture Search (NAS) with the BitFit strategy, maintaining the structural integrity of BitFit by imposing constraints on the NAS algorithm to determine whether the gradient of the bias term should be zero.\nWe consider the scenario of fine-tuning the bias term of a layer. For an input $x \\in \\mathbb{R}^{l\\times n}$ with frozen weights $W$ and bias term $b \\in \\mathbb{R}^{m}$, the output of the layer is computed as follows:\n$\\text{output} = xW + 1_l b^T,$\n(8)\nwhere $1_l \\in \\mathbb{R}^l$ is an all-one vector. To facilitate the integration of the bias term into the weight matrix, we can augment $W$ by appending $b^T$ as an additional row. This alteration leads to the following representation:\n$\\text{output} = [x \\quad 1] \\begin{bmatrix} W \\\\ b^T \\end{bmatrix} = \\tilde{x}\\tilde{W},$\n(9)\nwhere $1 \\in \\mathbb{R}^l$ and $\\tilde{W} \\in \\mathbb{R}^{(n+1)\\times m}$ is the augmented matrix. Therefore, BitFit fundamentally involves fine-tuning each element of the final row of $W$, corresponding directly to reconstructing the row space of the augmented weight matrix."}, {"title": "3.3.2 Soft Prompt Derivatives", "content": "Soft prompt derivatives, such as Prefix-tuning (Li and Liang [2021]), and prompt-tuning (Lester et al. [2021]), are prevailing in natural language processing (Gao et al. [2020], Tan et al. [2021]). Prefix-tuning introduces trainable continuous tokens, or prefixes, appended to either the input or output of a layer. These prefixes, sourced from a specific parameter matrix, remain trainable while other parameters of the pre-trained model are fixed during training. Conversely, Prompt-tuning simplifies this approach by incorporating soft prompts solely at the input layer. These prompts also originate from an independent parameter matrix and are updated exclusively through gradient descent. Both methods preserve the original model parameters, providing benefits in low-data scenarios and demonstrating potential for generalization across various tasks.\nFocusing on the design rather than specific layers to place prefixes, we consider a general case where for an input $x \\in \\mathbb{R}^{l\\times n}$ and the output $xW$. I learnable vectors $P \\in \\mathbb{R}^{l\\times m}$, known as soft prompts, are concatenated in the following formulation:\n$\\text{concat}(P, xW) = \\begin{bmatrix} P \\\\ xW \\end{bmatrix} = \\begin{bmatrix} P \\\\ x \\end{bmatrix} W.$\n(10)\nSimilar to the approach used for BitFit, we can augment the weight matrix to restate equation (10) as\n$\\begin{bmatrix} P \\\\ xW \\end{bmatrix} = \\begin{bmatrix} I \\\\ O_{l\\times n} \\end{bmatrix} \\begin{bmatrix} O_{l\\times l} \\\\ x \\end{bmatrix} W = \\tilde{W} \\tilde{x}.$\n(11)\nHere, $I \\in \\mathbb{R}^{l\\times l}$ is the identity matrix, $O_{l\\times n} \\in \\mathbb{R}^{l\\times n}$ and $O_{l\\times l} \\in \\mathbb{R}^{l\\times l}$ are zero matrices, and $\\tilde{W} \\in \\mathbb{R}^{(n+l)\\times m}$ is the augmented matrix. Thus, soft prompt derivatives essentially involve adjusting the elements of the initial several rows of the augmented weight matrix $\\tilde{W}$, thereby reconstructing the original subspace."}, {"title": "3.3.3 Others", "content": "There are also methods that adjust the singular vectors by directly modifying elements within the original weight matrix, such as Diff pruning (Guo et al. [2020]), FishMask (Sung et al. [2021]), Fish-Dip (Das et al. [2023]), Xattn Tuning (Gheini et al. [2021]), SPT (He et al. [2023]), and PaFi (Liao et al. [2023]), etc."}, {"title": "4 Subspace Extension", "content": "Extension-based methods introduce a new subspace, incorporating the bases of this new subspace alongside those of the original weight matrix $W$ to span an expanded space. These methods aim to"}, {"title": "4.1 LORA Derivatives", "content": "Building on the hypothesis that the change in weight matrices exhibits low-rank characteristics (Aghajanyan et al. [2020], Li et al. [2018]), LoRA introduces an addition term represented as $\\Delta W = AB$, where $A \\in \\mathbb{R}^{n\\times r}$ and $B \\in \\mathbb{R}^{r\\times m}$, and $r < \\{n, m\\}$. This approach aims to enhance parameter efficiency by exploiting the low-rank structure. Without loss of generality, assuming both $A$ and $B$ fully utilize the carrying capacities of their low ranks, i.e., $\\text{rank}(A) = \\text{rank}(B) = r$.\nThe addition term in LoRA aligns with the principles of full rank decomposition (Piziak and Odell [1999])\u00b2. Subsequent research has introduced variants employing different decomposition strategies, such as TriLoRA (Feng et al. [2024]) and AdaLoRA (Zhang et al. [2022]), which utilize SVD decomposition (Eckart and Young [1936]) to formulate $\\Delta W = ADB$ with $D \\in \\mathbb{R}^{r\\times r}$ being diagonal. Additionally, FLORA (Si et al. [2024c]) implements a variation where $\\Delta W = AGB$, with $G \\in \\mathbb{R}^r$ as an arbitrary matrix."}, {"title": "4.2 The Inspiration Gained: MPC Framework", "content": "The analysis of different decompositions in the LoRA derivatives provides valuable insights into optimizing model performance through matrix interactions. Specifically, structuring the learning process to emphasize specific interactions between these matrices can greatly enhance the model's efficiency and accuracy.\nWhen the matrix $G$ is diagonal, such as AdaLoRA and TriLoRA, to facilitate the model's ability to learn specific patterns in $A$ and $B$, a semi-orthogonality regularizer term can be introduced as:\n$\\min ||A^TA - I|| + ||BB^T - I||.\n(20)\nHowever, based on the previous analysis, the situation changes if $G$ is an identity matrix (such as LoRA). In such cases, $D_1$ and $D_2$ are highly correlated, and we can constrain one of $A$ and $B$ to"}, {"title": "4.3 Adapter Derivatives", "content": "Adapter derivatives (Houlsby et al. [2019], Pfeiffer et al. [2020], He et al. [2021a]) represent another class of extension-based methods. These adapters, which mainly consist of a down-projection followed by an up-projection, modify only specific parts of the model during training. For an input $x \\in \\mathbb{R}^{n\\times m}$, the adapters are integrated with a residual connection, resulting in the final transformation:\n$x \\rightarrow x + h(xA)B,$\n(22)\nwhere $h(\\cdot)$ is a nonlinear activation function, $A \\in \\mathbb{R}^{m\\times r}$ and $B \\in \\mathbb{R}^{r\\times m}$. This configuration can be further expressed by considering the weight matrix as the input $W = x \\in \\mathbb{R}^{n\\times m}$ and a hypothetical input $x = I \\in \\mathbb{R}^{m\\times m}$ as\n$Wx \\rightarrow Wx + h(WA)Bx,$\n(23)\nTherefore, the addition term introduced by the Adapter can be formulated as $\\Delta W = h(WA)B$. Following a similar analysis to LoRA, we can derive\n$h(WA) = \\Delta W^* B^*.$\n(24)\nCompared with LoRA, where $A = \\Delta W^* B^\\dagger$, the Adapter's inclusion of a nonlinear activation layer further reduces the constraints on learning the relationships between $A$ and $B$. According to Proposition 2, this reduction in constraints should lead to better model performance. However, the input to the Adapter is fixed as $I$, which implies that Adapters can only be placed at specific modules within a backbone architecture, potentially limiting their overall effectiveness.\nSubsequent developments, such as the MAM Adapter (He et al. [2021a]), which includes the PA or Scaled PA, adopt a parallel architecture similar to LoRA to configure adapter layers. This configuration allows Adapters to be applied to any weight matrix within the model. We focus particularly on the design of the PA and its application across various model positions. Specifically, for an input $x \\in \\mathbb{R}^{d\\times n}$, we have\n$xW \\rightarrow xW + h(xA)B,$\n(25)\nwhere $A \\in \\mathbb{R}^{n\\times r}$ and $B \\in \\mathbb{R}^{r\\times m}$. Following equation (23), we have\n$\\tilde{W}x \\rightarrow \\tilde{W}x + h(WA)B,$\n(26)\nresulting in $\\Delta W = h(WA)B\\alpha^\\dagger$. According to Proposition 2, this nonlinear flexibility typically results in enhanced model performance, as evidenced by our experiments in Fig. 4."}, {"title": "5 Subspace Combination", "content": "Combination-based methods perform both subspace reconstruction and extension simultaneously, blending the principles of both approaches. Moreover, for some methods which can be categorized as both a reconstruction-based and an extension-based method, we also classify them as the combination-based methods. We here analyse several representative combination-based methods as follows.\nDoRA (Liu et al. [2024]) first decomposes the model weights $W$ into two parts: magnitude and direction. The process of adjusting these components is defined as follows:\n$\\Phi(W) = m\\cdot \\frac{W+AB}{||W+AB||_c},$\n(27)\nwhere $m \\in \\mathbb{R}^{1\\times m}$ represents the magnitude, and $|| \\cdot ||_c$ is the vector-wise norm of a matrix applied across each column. Given that $m$ is learnable during the training process, this formula can be simplified as:\n$\\Phi(W) = WD + ABD,$\n(28)\nwhere $D \\in \\mathbb{R}^{m\\times m}$ is a diagonal matrix."}, {"title": "6 Implementation Details", "content": "In our experiments, we employed three distinct scales of models: RoBERTa-base (Liu et al. [2019]), DeBERTaV3-base (He et al. [2021b]), and RoBERTa-Large (Liu et al. [2019]). We used the General Language Understanding Evaluation (GLUE) (Wang et al. [2018]) benchmark as our dataset, which comprises two single-sentence classification tasks, three similarity and paraphrase tasks, and four natural language inference tasks. Details of the GLUE dataset are provided in Supplementary Table 5.\nThe configurations and hyper-parameters for all methods were adopted according to their respective original publications. Specifically, (IA)\u00b3 was applied to the keys and values in both self-attention and"}, {"title": "7 Discussion", "content": "The adaptation of pre-trained foundation models for a diverse array of downstream tasks has become a ubiquitous practice in artificial intelligence. Given the extensive range of tasks and the prohibitive costs associated, it is impractical to adjust all parameters comprehensively. In response, the development of parameter-efficient fine-tuning techniques (PEFT) has emerged, facilitating updates to the pre-trained model weights in a manner that is significantly more resource-efficient. Although methods of PEFT continue to proliferate, a comprehensive understanding of their underlying mathematical principles and the variance in their performance remains elusive. Therefore, in this work, we take the first step by conceptualizing all PEFT methods from a decomposition perspective, unifying them under the subspace tuning methodology. The mathematical foundations underlying each PEFT method are dissected, identifying that each represents a distinct manipulation of the subspace. Inspired by theoretical insights, we propose two novel PEFT methods. Extensive experiments show that by training less than one thousandth of the parameters, can approximate the effects of full fine-tuning.\nFurthermore, we elucidate the reasons behind the performance disparities among different methods. Our analysis yields significant conclusions. The comparative analysis of various PEFT strategies such as LoRA, AdaLoRA, and FLORA, reveals distinct patterns in their efficacy during training. The more stringent the matrix pattern learning, the more the model performance is constrained. We tested the performance of nearly ten algorithms on three different large pretrained models under four levels of parameter budgets, validating our conclusions with more than 3800 experimental runs. Based on this analysis, we propose a framework that enhances the learning of matrix patterns during model training. The effectiveness of this framework has been confirmed through more than 2000 experimental runs across three methods, four parameter budgets, and three large pretrained models.\nThe significance of our findings extends beyond the immediate scope of parameter-efficient fine-tuning. The principles underlying PEFT methods can be extrapolated to other domains of artificial intelligence, such as transfer learning (Mudrakarta et al. [2018], Houlsby et al. [2019]), multi-task learning (Liu et al. [2023], Mahabadi et al. [2021]), fast training (Mahabadi et al. [2021], R\u00fcckl\u00e9 et al. [2020]), and also areas where computational resources are a limiting factor, such as real-time systems (Jovanovic and Voss [2024]) and embedded devices (Feng and Narayanan [2023]). By analyzing the theoretical aspects of PEFT methods in different scenarios, we can comprehend the underlying logic and, based on these theoretical insights, refine these methods to further enhance their impact across related fields. Additionally, the theoretical underpinnings of subspace tuning present intriguing possibilities for further exploration in this domain as well as others, potentially catalyzing advancements and influencing developments across the broader artificial intelligence landscape."}, {"title": "8 Conclusion", "content": "In this paper, we unify all PEFT methods from the perspective of decomposition for the first time and explain the operations of different methods at the subspace level. We provide a detailed analysis of the mathematical principles behind each method and explain why their performance varies. Additionally, we propose two new methods that can achieve 99% of the performance with less than one-thousandth of the training parameters. We also introduce a framework that can help existing algorithms effectively"}]}