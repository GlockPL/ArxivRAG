{"title": "Closed-loop multi-step planning with innate physics knowledge", "authors": ["Giulia Lafratta", "Bernd Porr", "Christopher Chandler", "Alice Miller"], "abstract": "We present a hierarchical framework to solve robot planning as an input control problem. At the lowest level are temporary closed control loops, (\"tasks\"), each representing a behaviour, contingent on a specific sensory input and therefore temporary. At the highest level, a supervising \"Configurator\" directs task creation and termination. Here resides \"core\" knowledge as a physics engine, where sequences of tasks can be simulated. The Configurator encodes and interprets simulation results, based on which it can choose a sequence of tasks as a plan. We implement this framework on a real robot and test it in an overtaking scenario as proof-of-concept.", "sections": [{"title": "1 Introduction", "content": "Living organisms interact with their surroundings through sensory inputs in a closed-loop fashion [1]. To achieve basic closed-loop navigation in a robot it is sufficient to directly connect a robot's sensors to its motor effectors, and the specific excitatory or inhibitory connections determine the control strategy [2]. These sensor-effector connections represent a single closed-loop controller, which produces stereotyped behaviour in response to an immediate stimulus.\nThe state-of-the-art for autonomous navigation falls into the categories of closed-loop output control (e.g. Reinforcement Learning, RL), or trajectory planning (see [3]). In closed-loop output control, an agent uses environmental feedback to learn to associate input states with actions, aiming to maximise the cumulative reward. Feedback is provided at the end of each training episode, which makes a RL model inherently slow to train. Trajectory planners represent, on the other hand, open-loop controllers. Environment feedback may be provided on trajectory following. As an embodied agent is oblivious to its trajectory, this requires an external observed (e.g. a camera on the ceiling).\nClosed-loop behaviours (CLB) have some advantages over the state-of-the-art. First, the behaviour displayed by a robot depends on hard-wired connections; thus, responses to inputs do not need to be learned from scratch through"}, {"title": "2 Methods", "content": "2.1 Preliminaries\n2.1.1 Tasks as closed-loop controllers\nFig. 1 provides a visual representation of a task. As shown in Fig. 1B, a task is a closed-loop behaviour contingent on a disturbance $D$ which enters the environment $P$ and generates an error signal $E$ via the robot's sensors. As long as signal $E$ is nonzero, the agent $H$ produces a continuous motor output $M$ aimed at counteracting the disturbance (see Fig. 1A). Otherwise, the control behaviour is no longer necessary and it terminates.\n2.1.2 The Configurator\nFig. 2 illustrates the role of the Configurator in the planning process. Fig. 2A presents a scenario requiring multi-step planning: a robot is driving straight (task Ts) aimed at reaching the target $D_G$ in an environment where obstacles are present in front of it and to its right. The Configurator (Fig. 2B) thus initiates planning. First, the environment has to be explored in order to know what tasks are feasible to perform to reach the goal. As denoted by the thought balloon, this exploration is not actual but simulated using core knowledge. Fig. 2D presents the first stage of the exploration process. From the present task, the robot has the option to proceed straight (task $T_s$), turn left (task $T_L$) and then drive straight, or turn right (task $T_R$) and then go straight. The first and second options result in collisions (marked as stars) with the obstacle in front, while the"}, {"title": "2.1.3 Formalism", "content": "Definition 1. A state is a tuple $q = (T, D)$, where $T$ is a task and $D$ is a finite, possibly empty, set of disturbances.\nIn set $D$ we use $D_i$ to indicate the disturbance to which the task is contingent, and $D_N$ to define a disturbance which interrupts the task execution.\nDefinition 2. Set $T$ is the task space such that $T \\in T$. Set $D$ is the set of all disturbances such that $D \\in D$.\nDefinition 3. A cognitive map is a tuple $(Q, \\rightarrow)$, where $Q : T \\times 2^D$ is a set of states and $\\rightarrow : Q \\times Q$ is a set of transition relations.\nDefinition 4. The Configurator is a tuple $(G, D_G, \\Psi, R)$ where $G = (Q, \\rightarrow)$ is a cognitive map representing the searchable state-space, $D_G$ is the system's overarching goal (a disturbance which the plan aims to counteract), function $\\Psi : \\rightarrow [1,0]$ is a guard which assigns a supervisory control pattern [5] to each transition, $R : \\rightarrow D$ is a reset through which the Configurator injects disturbances from disturbance set $D$ into states in a top-down fashion."}, {"title": "2.2 Implementation", "content": "2.2.1 Core knowledge\nThe physics engine Box2D\u00b9 represents the core knowledge using which sequences of tasks can be simulated. Objects in the Box2D simulation are constructed from ego-centric coordinates captured by the robot's LiDAR sensor. The raw point cloud is resampled so that only the points in the way of the present task are represented. A model of the robot is also present in the environment. At the beginning of the simulation, the robot is located at the origin of the plane\n2.2.2 Reset\nIn this work, we define reset\n$R(q_1, q_2): D'_i = D_G$ if $D_N = \\varnothing$\nwhere $D'_i$ is the disturbance to be injected in the task in state $q_2$, and $D_N$ is new disturbance which interrupted the task in state $q_1$. In other words, unless the task in $q_1$ is interrupted, the next task in $q_2$ will be contingent on the goal $D_G$. As we seek to plan to avoid collisions, transitions from tasks where set $D_N \\neq \\varnothing$ are not permitted. The reset for this case is not defined."}, {"title": "2.2.3 Construction of cognitive map G", "content": "As mentioned in Section 2.1.2, we use a best-first approach to assign priority of expansion to states. As in Hart et al. [6], we define a cost function $\\phi : Q \\rightarrow R$ such that\n$\\phi(q) = \\gamma(q) + \\chi(q)$\nwhere $q$ is a state in cognitive map G, $\\gamma(q)$ is a past cost function and $\\chi(q)$ is a heuristic future cost function. We calculate $\\gamma(q)$ as the normalised distance from the start of the task to interrupting disturbance $D_N$, if present, and $\\chi(q)$ as the normalised negative distance from the end of the task to goal $D_G$. Algorithm 1 provides pseudocode for the construction of cognitive map G."}, {"title": "2.2.4 Guard \u03a8", "content": "Guard $\\Psi$ is used to extract a plan $p$ from cognitive map G. Formally, the plan is\n$p = \\{q_0, q_1...q_n | (q_i, q_{i+1}) \\in \\rightarrow \\forall i \\in N, q_n = \\min_{q \\in Q} \\phi(q)\\}$\nor a set of adjacent states where the last state has the least cost $\\phi$ in the set of states Q. This results in the assignment of $\\psi(q) = 1$ if $q \\in p$ and zero otherwise."}, {"title": "3 Results", "content": "We test the decision-making process in an overtaking scenario, where a target location DG located 1 meter in front of the robot must be reached in presence of an obstacle. We contrast our new approach against a reactive control strategy which uses a single closed-loop controller. In the planning condition, the robot was able to formulate a plan to reach the target in all 10 runs (example run depicted in Fig. 3A). The mean time for state-space exploration and planning was 0.064 \u00b1 0.009 seconds for a state-space comprised of 32.9 \u00b1 1.758 states (see Fig. 3C for an example simulation trace with state labels). During execution, the robot brushed the obstacle twice in the planning condition, both times in one particular scenario due to execution error accumulation. The robot never reached the target in the reactive condition (see Fig. 3B); moreover, it collided 8 times with obstacles over the ten runs. Differences in number of collisions approached, but did not reach, statistical significance (paired T=-1.747, p=0.111), possibly due to the small sample size."}, {"title": "4 Conclusion", "content": "We have developed a framework to achieve multi-step ahead, real-time planning over CLB thanks to core knowledge as a physics simulation. This work showcases an innovative use of a physical simulation as a tool for an agent to actively reason over its environment. This allows for constructing a cognitive map without need for physical exploration, where the environment is discretised in terms of CLB afforded by objects in the environment."}]}