{"title": "Transformer Dynamics: A neuroscientific approach to interpretability of large language models", "authors": ["Jesseba Fernando", "Grigori Guitchounts"], "abstract": "As artificial intelligence models have exploded in scale and capability, understanding of their internal mechanisms remains a critical challenge. Inspired by the success of dynamical systems approaches in neuroscience, here we propose a novel framework for studying computations in deep learning systems. We focus on the residual stream (RS) in transformer models, conceptualizing it as a dynamical system evolving across layers. We find that activations of individual RS units exhibit strong continuity across layers, despite the RS being a non-privileged basis. Activations in the RS accelerate and grow denser over layers, while individual units trace unstable periodic orbits. In reduced-dimensional spaces, the RS follows a curved trajectory with attractor-like dynamics in the lower layers. These insights bridge dynamical systems theory and mechanistic interpretability, establishing a foundation for a \"neuroscience of AI\" that combines theoretical rigor with large-scale data analysis to advance our understanding of modern neural networks.", "sections": [{"title": "1. Introduction", "content": "Artificial intelligence models-particularly modern deep learning systems-have scaled in both size and capability at an astonishing rate (Bahri et al., 2024). Today's large language models (LLMs), vision models, and other predictive models (e.g. recommender systems, weather prediction, navigation, etc) are operating in the real world. Yet, despite their ubiquity, we lack a comprehensive understanding of how these models work, where understanding is meant roughly as to be able to predict how a particular change to the input or to the model would affect its output in a wide range of cases. Our lack of understanding around such systems raises myriad concerns about their safety, fairness, and whether they might pose an existential risk to humanity (Amodei et al., 2016; Bengio et al., 2024).\nLike the brain, deep neural networks are complex systems composed of billions of parameters interacting in highly non-linear ways. Systems with even a small number of such interacting elements can give rise to emergent behaviors that are unpredictable from a strictly bottom-up perspective (Lorenz, 1963). Consequently, it is not surprising that existing methods for investigating the workings of these models have yielded only fragmentary insights.\nCurrent approaches in mechanistic interpretability often focus on identifying discrete circuits within neural networks-sub-networks or groups of neurons that implement particular functions (Heimersheim & Nanda, 2024; Singh et al., 2024). While these circuit-based approaches have provided some explanatory power, they tend to mirror pitfalls of early approaches to understanding the brain. One historical example in neuroscience was the concept of \u201cgrandmother cells,\u201d which posited that the unit of representation in the brain may be individual neurons that encode highly specific concepts (like a single neuron firing selectively for one's grandmother) (Plaut & McClelland, 2010). This idea, together with sparse coding-the notion that only a small fraction of neurons are active at any one time, and that representations are distributed among populations of cells (Olshausen & Field, 2004)-led to a wave of work around sparse distributed representations as a way to explain how the brain encodes information. Yet, while artificially sparsifying model activations with sparse autoencoders (SAEs) has yielded model-specific information about which sets of activations represent monosemantic concepts, many questions about how the models compute remain unanswered (Wattenberg & Vi\u00e9gas, 2024).\nIn neuroscience, a recent promising approach to interpret neural encoding of information comes from dynamical systems (Shenoy et al., 2013; Barack & Krakauer, 2021; Vyas et al., 2020). Treating the activity of populations of neu-"}, {"title": "2. Methods", "content": "rons as a time-evolving dynamical system has shed light on how such populations collectively implement sensory perception, compute cognitive variables, and produce behavior. Instead of aiming to explain the representations of single neurons\u2014or even snapshots in time of the activities of populations\u2014these dynamical approaches aim to understand how network-wide activity evolves over time to generate complex outputs. For example, in the motor system, preparatory activity appears to guide the population to an appropriate pre-movement state; and in some cases these states are attractors that are robust to noise (Inagaki et al., 2019). Dynamical approaches have yielded insights into the computations in recurrent artificial networks as well (Maheswaranathan et al., 2019).\nWhile transformers do not have inherent time-evolving dynamics like recurrent networks, some have examined their activations and treated them as dynamically-evolving systems (Geshkovski et al., 2024; Lu et al., 2019; Hosseini & Fedorenko, 2023; Lawson et al., 2024). Specifically, the residual stream, which is updated linearly after each layer's attention and MLP operations, can be considered as a dynamic system that evolves over the layers. Lu et al. proposed that the transformer residual stream be considered as an ordinary differential equation (ODE) of multiple particles moving through space (i.e. across layers) and influenced by convection (external forces) and diffusion (internal forces"}, {"title": "2.1. Data", "content": "Given a corpus of text sequences from WikiText-2, we first filter the dataset D to include only sequences s with length constraints:\n$D_{filtered} = \\{s \\in D \\mid l_{min} < |s| <l_{max}\\}$\nwhere $l_{min} = 100$ and $l_{max} = 500$ characters. For each sequence s, we obtain its tokenized representation:\n$x = [t_0, t_1, ..., t_n] = tokenize(s)$\nwhere $t_0$ is the beginning-of-sequence (BOS) token. For the shuffled condition, we create a permuted sequence $x'$ while preserving the BOS token:\n$x' = [t_0, t_{\\pi(1)}, t_{\\pi(2)}, ..., t_{\\pi(n)}]$\nwhere $\\pi$ is a random permutation of indices $\\{1, ..., n\\}$.\nFor each sequence (original and shuffled), we collect activations at two key points in each transformer layer $l \\in \\{1, ..., L\\}$: Pre-attention normalization ($h^{Attn}$) and Pre-MLP normalization ($h^{MLP}$).\nThese constitute the activations of the residual stream, RS, with $h^{Attn}$ and $h^{MLP}$ interleaved to make up 2L effective 'layers'.\nWe focused on the representation at the last token only, for each activation extracting:\n$h_i \\in R^{B \\times D}$\nwhere B is the batch size and D is the model dimension.\nAltogether the extracted activations corresponded to:\n$RS \\in R^{B \\times 2L \\times D}$\nFor experiments in this paper, we used Llama 3.1 8B, where L = 32 and D = 4096 (Fig1A).\nSince activations in the RS do not correspond to individual artificial neurons, we term each dimension in $D^a$ 'unit,' borrowing terminology from neuroscience to indicate the recording of the activation of a single element in the stream. We will thus refer to the dynamics of units in the RS as they unfold over the layers."}, {"title": "2.2. Transformer Residual Stream Activations", "content": "Mean activations across batches for each unit and layer were calculated in the following manner:\n$\\hat{h}_i^u = \\frac{1}{B} \\sum_{b=1}^B h_i^u$\nMean activations across N = 1000 data batches were sorted by the mean activation at the last layer:\n$\\pi = argsort(\\hat{h}_{2L})$"}, {"title": "2.3. Correlations and Cosine Similarity in the Residual Stream", "content": "For each unit u, we computed the Pearson correlation coefficient between its activations at different layers across all samples:\n$r_{l,l+1}^u = \\frac{cov(h_l^u, h_{l+1}^u)}{\\sigma_{h_l^u} \\sigma_{h_{l+1}^u}}$\nwhere $h_l^u \\in R^B$ represents the activations of unit i at layer l across all samples.\nFor each unit, we analyzed the distribution of correlations across all layer pairs:\n$C^2 = \\{r_{l,m}^u : l,m \\in \\{1, ..., 2L\\}, l < m\\}$\nThe distribution was binned into intervals [0, 1] to create a density plot.\nCosine similarity between consecutive layers was computed as:\n$CS = \\frac{h_i \\cdot h_{i+1}}{||h_i|| ||h_{i+1} ||}$\nFor this and other analyses, we treated the interleaved pre-attention and pre-MLP activations as layers, highlighting the two transition types: pre-attention to pre-MLP within the same layer ($h^{Attn} \\rightarrow h^{MLP}$) and pre-MLP to pre-attention of the subsequent layer ($h^{MLP} \\rightarrow h^{Attn}$)."}, {"title": "2.4. Velocity", "content": "To understand how the dynamics of the residual stream change over the layers, for each layer, we calculated the magnitude of the velocity of the residual stream representation:\n$||V_i|| = ||h_{i+1} - h_i||_2$"}, {"title": "2.5. Mutual Information", "content": "To understand how information is processed through the layers, we analyzed the mutual information (MI) of units between layers in the residual stream. For each pair of"}, {"title": "2.6. Dynamics of Individual RS Units", "content": "consecutive layers l and l + 1, we computed the mutual information using kernel density estimation:\n$MI(l, l+1) = \\sum p(x, y) log(\\frac{p(x, y)}{p(x)p(y)})$\nwhere p(x, y) is the joint probability density of activations at layers l and l + 1, and p(x) and p(y) are their respective marginal densities.\nWe implemented this calculation using Gaussian kernel density estimation to handle the continuous nature of the activation space. For numerical stability, we added a small constant (1e-10) to avoid division by zero and taking logs of zero. The mutual information was computed in nats using the natural logarithm.\nThe computation was performed independently for each unit in the residual stream, using the distribution of activations derived from N = 1000 batches, to track how different components of the representation evolved through the layers.\nThe resulting portraits revealed rotational dynamics. To quantify these, the number of rotations in this 2D space was calculated by tracking the cumulative change in angle of the tangent vector along this trajectory. Specifically, for each unit we centered the trajectory at the origin by subtracting initial values:\n$x_i = a_i - \\overline{a_i}$\n$y_i = \\nabla_i a_i - \\overline{\\nabla_i a_i}$\nWe then computed tangent vectors between consecutive points:\n$\\Delta x_i = x_{i+1} - x_i$\n$\\Delta y_i = y_{i+1} - y_i$\nSubsequently we calculated the angles of these tangent vectors:\n$\\theta_i = arctan2(\\Delta y_i, \\Delta x_i)$\nFollowing this, we computed angle changes between consecutive points, adjusting for discontinuities at $\\pm \\pi$:\n$\\Delta \\theta_i = \\theta_{i+1} - \\theta_i$\nTo examine the dynamics of individual RS units' activations across layers, we created phase portraits in 2D phase space defined by each unit's activation value and the gradient of that activation across layers. For each unit i, we constructed a phase portrait where the x-axis represents the unit's activation $a_i$ at layer l, and the y-axis represents its gradient $\\nabla_i a_i = \\frac{\\partial a_i}{\\partial l}$."}, {"content": "Finally, the total number of rotations was calculated as:\n$R = \\frac{1}{2\\pi} \\sum \\Delta \\theta_i$\nTo establish statistical significance, we compared the observed number of rotations with a null distribution generated"}, {"title": "2.7. Dimensionality Reduction with a Compressing Autoencoder", "content": "To analyze the high-dimensional activation patterns in the RS, we trained an autoencoder on the RS activations and visualized the trajectories across layers in reduced dimensional space. The compressing autoencoder\u00b9 was trained to minimize reconstruction error while learning a low-dimensional representation of activation patterns. The architecture consists of an encoder and decoder, each with k layers where k is determined by the input dimension $d_{in} = 4096$ and target bottleneck dimension $d_{bottle} = 2$. The dimensions of intermediate layers follow a geometric progression, with each layer i having dimension:\n$d_i = d_{in} \\cdot r^i$\nwhere r = $(d_{bottle}/d_{in})^{1/(k-1)}$ is the reduction ratio between consecutive layers.\nThe Wikitext dataset was used for training and evaluating the CAE, with a train set of 85k batches and test set of 15k batches, each of which contained 64 samples (i.e. an RS vector for each layer).\nEach layer consists of a linear transformation followed by layer normalization and ReLU activation (except for the final encoder and decoder layers which omit these nonlinearities). The model was trained using the Adam optimizer with learning rate $\\alpha = 10^{-3}$ to minimize the mean squared error loss:\n$L(\\theta) = \\frac{1}{n} \\sum_{i=1}^n ||x_i - f_{\\theta}(x_i)||^2$\nwhere x are the input activation patterns and $f_{\\theta}$ is the autoencoder with parameters $\\theta$. Training proceeded for a maximum of 100 epochs with early stopping based on validation loss with a patience of 10 epochs. The model achieving the lowest validation loss was retained."}, {"title": "2.8. PCA and Perturbation of Activation Trajectories", "content": "To better understand the trajectories of the RS in reduced dimensional space and perform interpretable perturbations in these trajectories in reduced space, we performed Principal Component Analysis (PCA) using Singular Value Decomposition (SVD). For a dataset of N samples with"}, {"content": "activations from L layers, each of dimension D, we first reshape the activation tensor $RS \\in R^{N \\times 2L \\times D}$ into a matrix $X \\in R^{2NL \\times D}$ by combining the sample and layer dimensions. Then we center the data by subtracting the mean: $X = X - \\mu$, where\n$\\mu = \\frac{1}{2NL} \\sum_{i=1}^{2NL} X_i$\nThen we compute the SVD of the centered data:\n$X = U \\Sigma V^T$\nwhere $U \\in R^{2NL \\times D}$, $\\Sigma \\in R^D$, and $V \\in R^{D \\times D}$\nFor subsequent analysis, we project the data onto the first two principal components:\n$Z = XV[:, :2]$\nwhere $V[:, :2]$ contains the first two right singular vectors. The explained variance ratio for component k is computed as:\n$r_k = \\frac{\\sigma_k^2}{\\sum_i \\sigma_i^2}$\nwhere $\\sigma_k$ is the k-th singular value.\nThe resulting low-dimensional representation Z $\\in R^{(2NL) \\times 2}$ is then reshaped to $R^{N \\times 2L \\times 2}$ for visualization and analysis of activation trajectories.\nTo investigate how perturbations in the learned low-dimensional space affect model behavior, we systematically explored the 2D PCA space by creating a uniform grid of points to which we could teleport the activations at various stages (layers) in the RS trajectory. Specifically, we generated n x n evenly spaced points across a range [$r_{min}, r_{max}$] in each principal component dimension, where n is the number of points per dimension (typically 10) and r represents the range of perturbation magnitudes. For each point $z_i$ in this 2D grid, we projected it back to the original activation space using the inverse PCA transformation:\n$\\hat{x_i} = z_i V[:, :2]^T + \\mu$\nwhere $V[:, :2]$ contains the first two principal components and $\\mu$ is the mean of the original activation distribution. We then injected these reconstructed activations into specific layers of the language model by replacing the original activations at the input to the attention layer. This process was repeated across multiple network layers to analyze how perturbations at different depths affect the model's internal representations. As perturbations above the first layer we used a standard input prompt (\"I'm sorry, Dave. I'm afraid I can't do that.\") to record the initial trajectories. This input also served as a control to establish a baseline for comparison of perturbed trajectories."}, {"title": "3. Results", "content": "We hypothesize that RS progression through transformer layers might reveal attractor-like dynamics, such that moving the activations to various portions of this phase space would eventually bring them back to the mean trajectory.\nThe response to these perturbations varied systematically with layer depth and location of the teleported points. For instance,\nStill, the effect of the perturbations varied systematically. Perturbations at layer 0 (i.e. starting the trajectories at new points) resulted in highly variable dynamics, with trajectories often ending up far from the unperturbed mean. This effect was even more drastic when interfering in the dynamics at the penultimate LLM layer, 31. Mid-layer perturbations (layers 7, 15, 23) exhibited a more robust recovery, with lower variance on the perturbed trajectories and lower mean squared error between the control sequence (Fig. 4D).\nThis data suggests that the transformer develops stable computational channels that actively maintain desired trajectories, possibly self-correcting errors through its dynamics."}, {"title": "3.1. Transformer residual stream (RS) activations grow dense and are highly correlated over the layers", "content": "Our initial investigation into the activations of the RS showed that activations at the last token position for input sequences increase in magnitude over the layers (Fig. 1B). Most units showed low-magnitude activations at the lowest layers, with the majority increasing in magnitude progressively over the layers. Sorting the mean activations across N = 1000 data batches by the mean activation at the last layer, $\\pi = argsort(\\hat{h}_{2L})$, revealed that activations not only grow dense as the layers progress, but that units tend to preserve their sign over the layers.\nTo quantify the continuity of representations between layers, we analyzed pairwise correlations between layer activations. We analyzed two types of transitions (Fig. 1C): within-layer transitions ($h^{Attn} \\rightarrow h^{MLP}$) and cross-layer transitions ($h^{MLP} \\rightarrow h^{Attn}$). The within-layer correlations were consistently higher than the cross-layer correlations, suggesting different information processing regimes of attention and MLP operations, with the former producing smaller changes to the RS vectors. The correlation strength increased over layers for both transition types, with correlations starting high (r > 0.8) even in the earliest layers. For each unit, the distribution of correlations over the 63 layer transitions was binned into intervals [0, 1] to create a density plot (Fig. 1D), revealing that despite the RS being a nonprivileged basis, most units maintain strong correlations throughout the network.\nWhile correlations of activations revealed that individual units exhibited ever stronger linear relationships for successive layers, we were keen to examine the changes of the RS vector as a whole. Cosine similarity of layerwise RS vector pairs increased as a function of layers, with within-layer ($h^{Attn} \\rightarrow h^{MLP}$) transitions more similar than cross-layer (Fig. 1E).\nTo characterize the dynamics of representation change"}, {"title": "3.2. RS Unit Phase Space Exhibits Rotational Dynamics", "content": "through layers, we computed the velocity of the RS representation. The velocity profile showed a distinct pattern of acceleration through the model (Fig. 1F). Early layers maintain relatively constant velocities, while later layers exhibited a slight increase in velocity, with the steepest acceleration occurring in the last third of the model. This acceleration pattern holds for both within-layer and cross-layer transitions, though cross-layer transitions consistently showed higher velocities. This progressive acceleration of representational change, combined with our observations of increasing activation magnitudes and correlations, suggests that the transformer RS systematically amplifies certain representational directions in later layers.\nAnalysis of mutual information (MI) for given RS units at successive layers revealed three distinct phenomena in information flow (Fig. 1G,H). First, MI showed a sharp decline in early layers, with the steepest drops occurring at cross-layer transitions ($h^{MLP} \\rightarrow h^{Attn}$). Second, the reduction in MI occured simultaneously with increasing linear correlations between layers (Fig. 1C,D). Third, the MI decrease coincided with growing activation magnitudes through the layers (Fig. 1B).\nThe apparent paradox between decreasing MI and increasing correlations suggests a systematic transformation of the representation space. While correlation captures only linear relationships, MI measures both linear and nonlinear dependencies. This pattern, combined with our observation of decreasing dimensionality through the layers, suggests that the model may be redistributing information across more dimensions while favoring simpler, linearly-aligned features in later layers over complex nonlinear relationships.\nFor each unit i, we constructed phase portraits by plotting the unit's activation value $a_i$ against its gradient $\\nabla_i a_i = \\frac{\\partial a_i}{\\partial l}$ across layers. Analysis of individual RS units in activation-gradient space revealed rotational dynamics characteristic of unstable periodic orbits (Fig. 2A). While most trajectories"}, {"title": "3.3. Reduced Dimensional Trajectories", "content": "To analyze the high-dimensional activation patterns in the RS, we used complementary dimensionality reduction approaches: a compressing autoencoder (CAE) (Fig 3) and PCA (Fig 4). The CAE was trained to pass RS vectors through successively lower-dimensional layers, until passing through a 2-dimensional bottleneck and being reconstructed again. Here RS vectors at each sublayer were"}, {"content": "treated as individual samples to be learned. The bottleneck representation revealed structured, curving trajectories (Fig. 3B). The trajectories in this space straightened over the layers, with increasing distance between successive layers (Fig. 3C). The model's reconstruction of RS vectors showed increasing explained variance, with a large jump after the early layers and a slower increase thereafter (Fig. 3D).\nPrincipal Component Analysis revealed that early layers distribute variance across more dimensions than later layers, with later layers requiring fewer principal components to explain the same amount of variance (Fig. 4A,B,C)."}, {"title": "3.4. RS Trajectories Exhibit Attractor-like Dynamics in Lower Layers", "content": "Visualization of RS trajectories in PCA space revealed systematic patterns in how representations evolve through the network (Fig. 4A). Individual trajectories and their layer-wise means demonstrated a consistent path through this reduced space, suggesting a structured computation process, with slightly offset trajectories for the within-layer $h^{Attn} \\rightarrow h^{MLP}$ and cross-layer $h^{MLP} \\rightarrow h^{Attn}$ transitions.\nTo understand the stability of these trajectories, we performed perturbation analysis by \"teleporting\" the RS state to various points in the PCA space at different layers (Fig."}, {"title": "4. Discussion", "content": "Finally, while perturbing activations by 'teleporting' them to various portions of reduced-dimensional space was revealing, it is possible this approach does not realistically capture how a model might respond to more subtle changes to its activations. Future efforts may attempt noise injection or swapping of activations from one input data sample to another.\nThe discovery of rotational dynamics in activation-gradient space, combined with the self-correcting properties observed in our perturbation analysis, points to an emerging organizational principle. The network appears to construct stable computational channels that actively maintain desired trajectories. This self-correction is most robust in lower layers, where cosine similarity and velocity among succeeding RS vectors are lowest, and mutual information the highest. Finally, the strong correlations and low-dimensional flows imply that the network may perform highly distributed computations rather than localizable \"grandmother cell\" style encoding. Insights such as presented here could inform both a theoretical understanding of transformer dynamics and practical approaches to architecture design and training optimization."}, {"title": "Impact Statement", "content": "This work explores the mechanistic interpretability of transformers from a dynamical systems perspective inspired by neuroscience. By bridging dynamical systems theory with transformer interpretability, we introduced a novel way to understand the behavior of LLMs. This approach follows"}]}