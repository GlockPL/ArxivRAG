{"title": "VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents", "authors": ["Xiao Liu", "Tianjie Zhang", "Yu Gu", "Iat Long Iong", "Yifan Xu", "Xixuan Song", "Shudan Zhang", "Hanyu Lai", "Xinyi Liu", "Hanlin Zhao", "Jiadai Sun", "Xinyue Yang", "Yu Yang", "Zehan Qi", "Shuntian Yao", "Xueqiao Sun", "Siyi Cheng", "Qinkai Zheng", "Hao Yu", "Hanchen Zhang", "Wenyi Hong", "Ming Ding", "Lihang Pan", "Xiaotao Gu", "Aohan Zeng", "Zhengxiao Du", "Chan Hee Song", "Yu Su", "Yuxiao Dong", "Jie Tang"], "abstract": "Large Multimodal Models (LMMs) have ushered in a new era in artificial intelligence, merging capabilities in both language and vision to form highly capable Visual Foundation Agents. These agents are postulated to excel across a myriad of tasks, potentially approaching general artificial intelligence. However, existing benchmarks fail to sufficiently challenge or showcase the full potential of LMMs in complex, real-world environments. To address this gap, we introduce VisualAgentBench (VAB), a comprehensive and pioneering benchmark specifically designed to train and evaluate LMMs as visual foundation agents across diverse scenarios, including Embodied, Graphical User Interface, and Visual Design, with tasks formulated to probe the depth of LMMs' understanding and interaction capabilities. Through rigorous testing across nine proprietary LMM APIs and eight open models, we demonstrate the considerable yet still developing agent capabilities of these models. Additionally, VAB constructs a trajectory training set constructed through hybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and Human Demonstrations, promoting substantial performance improvements in LMMs through behavior cloning. Our work not only aims to benchmark existing models but also provides a solid foundation for future development into visual foundation agents. Code, train & test data, and part of fine-tuned open LMMs are available at https://github.com/THUDM/VisualAgentBench.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Foundation Models, particularly Large Language Models (LLMs)  and Large Multimodal Models (LMMs) , have showcased their profound capabilities in understanding and processing vast amounts of world knowledge, factual information, and common sense reasoning. Notably, these models have demonstrated potential as intelligent agents , addressing a broad spectrum of real-world challenges . LMMs, in particular, enhance the capabilities of these agents by integrating visual inputs, thereby expanding the scope of intelligent agent applications.\nThis progress has given rise to the concept of Foundation Agents-generalist agents adept at mastering a plethora of skills across various virtual and embodied environments, mirroring human versatility. These agents, especially those powered by LMMs, are envisioned to excel in multitask environments without the need for task-specific fine-tuning, a paradigm already set by LLM-based language agents. The burgeoning field of visual foundation agents offers promising pathways toward achieving AGI, with the potential to significantly elevate human productivity and creativity.\nHowever, the setup for LMM-as-Visual-Foundation-Agent remains underdeveloped. Most existing evaluations on LMMs focus on traditional tasks like Visual Question Answering (VQA) , Optical Character Recognition (OCR) , and Referring Expression Generation (REG) , or on performance in standardized human exams . These assessments rarely measure the models' higher-level reasoning and planning capabilities or their specific strengths as visual agents. In contrast, the role of LLMs as agents in text environments has been extensively explored and validated as a reliable measure of their capabilities .\nRecent benchmarks for multimodal agents, while valuable, do not adequately address the comprehensive evaluation required for LMM-as-Visual-Foundation-Agent. These benchmarks often limit their focus to single environments such as Household , Gaming , Web , or Desktop scenarios . This narrow scope prevents a holistic assessment of LMMs' multitask agent capabilities. Furthermore, the prevalent prompting-only evaluation in existing benchmarks does not suffice for open LMMs , which typically show limited instruction-following capabilities so far, thus hindering a comprehensive evaluation.\nTo bridge this gap, we introduce VISUALAGENTBENCH (VAB)\u2014the first systematic benchmark designed to multitask train and evaluate visual foundation agents across a diverse array of realistic vision-centric tasks. We present three representative scenarios and develop five distinct datasets for this study: Embodied (VAB-OmniGibson, VAB-Minecraft), Graphical User Interface (GUI) (VAB-Mobile [1], VAB-WebArena-lite ), and Visual Design (VAB-CSS), enabling comprehensive testing and development of agents that can navigate complex spaces, interact with digital interfaces, and understand aesthetic and functional aspects of visual design. This diversity not only challenges the agents' capabilities across different settings but also enhances their adaptability and utility in practical applications, paving the way for more robust and versatile visual foundation agents.\nWe have standardized the prompting and data formats to facilitate a consistent evaluation of visual foundation agents across these environments. Each VAB task is assessed through interactive evaluation , where LMMs engage directly with the environment, and their performance"}, {"title": "2 Problem Formulation and VAB Design Features", "content": "In this section, we introduce the problem definition of LMM-as-Visual-Foundation-Agent. Upon the definition, we explain a series of practical principles we follow during the design of VAB.\nLMM-as-Visual-Foundation-Agent. An agentic problem could be generally formulated as a Partially Observable Markov Decision Process (POMDP) problem, where S denotes the state space, A denotes the action space, T denotes the transition function, R refers to the reward function, I refers to the instruction space, and O refers to the observation space. Compared to LLM-as-Agent , the observation space O must incorporate visual inputs (e.g., images or videos) in LMM-as-Visual-Foundation-Agent, significantly extending the application scope but also casting a substantial challenge for LMMs to reconcile their multimodal understanding and high-level reasoning.\nDesign Features of VAB. Given that LMMs are still evolving rapidly, we adhere to several principles in our design of VAB to accommodate the current capabilities and limitations of LMMs.\n\u2022 Vision-Centric: VAB agent tasks are designed to primarily rely on visual inputs to solve problems. While additional text inputs could be beneficial, VAB aims to evaluate how LMMS perform when perceiving the environment as humans do in agent tasks. For example, while HTML is shown useful for Web GUI Agent , humans typically browse the internet from screens without reading HTMLs.\n\u2022 High-Level Decision Making: VAB focuses on evaluating LMMs' high-level decision-making abilities. Compared to prior smaller visual-language models that specifically target low-level policies , LMMs excel at high-level planning and interacting [11] in text response thanks to their commonsense, knowledge, and flexible instruction following with mere prompting. Therefore, in VAB, we simplify the low-level control by providing convenient action interfaces, and ask tested LMMs to concentrate on delivering high-level decision sequences in text.\n\u2022 Interactive Evaluation: Evaluating LLMs or LMMs on real-world agent tasks is challenging, as task goals can be achieved by various means. As a result, it becomes a mainstream practice to evaluate in an interactive manner . VAB also adheres to this principle."}, {"title": "3 VISUALAGENTBENCH: Tasks and Environments", "content": "In VAB, we carefully select the most representative and promising tasks that could be enabled by LMM-based agents. These tasks generally fall into three categories: embodied agents, including household and game environments; GUI agents, covering mobile and web apps; and visual design agents, focusing on frontend CSS debugging (Figure 2). They span diverse domains and feature unique challenges, providing an ideal testbed for a comprehensive evaluation of LMM-based agents. When constructing VAB, we strictly follow the principles outlined in Section 2. Our efforts focus on addressing gaps in evaluating LMM-based agents while leveraging existing resources to avoid redundancy, ensuring all our work is meaningful and avoids reinventing the wheel. For 4 out of 5 tasks, we collect new data from scratch. For web agents, we adapt and clean WebArena as our test set, as it is already suitable for LMM-based evaluation. For household agents, we use the OmniGibson environment from Behavior-1k [29] and create new tasks based on high-level actions we defined, which are crucial for evaluating LMM-based agents and absent in existing datasets. We similarly construct our tasks in Minecraft using the MineRL environment\u00b9 with our self-defined high-level actions. Finally, for our mobile app and CSS debugging tasks, we create new interactive environments due to the lack of suitable resources in the literature and collect data based on these environments. An overview of VAB is shown in Table 2."}, {"title": "3.1 Embodied Agent", "content": "Embodied agents have been a central topic in AI, naturally involving multimodal sensory data, including language and vision signals. The multimodal capabilities of LMMs could enable new possibilities for embodied agents.\nVAB-OmniGibson. One of the most actively researched environments in embodied AI is the household environment due to its complexity and range of everyday tasks [19, 55, 51]. We build the household environment for embodied agents using OmniGibson, a high-fidelity simulator based on Nvidia Omniverse\u00b2 that features diverse scenes and realistic physical effects. An example activity in VAB-OmniGibson would be \u201cPut all 8 plates from the countertops into the cabinet in the kitchen\", where agents should accomplish the tasks using provided high-level actions (e.g., \"grasp\", \"put_inside\"). We adopt the task success rate as the evaluation metric. (Cf. Appendix A).\nVAB-Minecraft. Minecraft has become a popular open-world environment for developing generalist embodied agents due to its diverse tasks (e.g., survival, harvesting, crafting, combat, and creative tasks), varied environments, and interactive mobs, necessitating generalized agent abilities [13, 30]. In VAB-Minecraft, the agent is expected to accomplish a wide range of tasks, including item collection and killing hostile mobs. An example task in VAB-Minecraft would be \"Get a fishing rod in your inventory\", and the LMM agent need to interact with the game environment using provided scripts (e.g., \"craft\", \u201csmelt\u201d) or calling a low-level controller Steve-1 [30] with prompt. We adopt the task success rate as the evaluation metric. (Cf. Appendix B)"}, {"title": "3.2 GUI Agent", "content": "GUI is another typical scenario where LMM agents may excel. Compared to embodied environments, GUI environments are more information-intensive and require a good understanding of UI elements and layouts. We provide two interactive and reproducible GUI environments, Mobile (i.e., Android) and WebArena, to evaluate LMM GUI agents in a practical manner.\nVAB-Mobile [1]. Automated agents on Android GUI can significantly advance personal digital assistants. Although pioneer works like MOTIF [7] and AITW [47] have explored training and evaluating these agents, they typically use Step Success Rate evaluated offline. Recent works [70, 62] leverage LMMs as Android GUI agents but lack reproducible executive evaluation frameworks. We address this by creating tasks for LMM agents to perform human-like actions (e.g., Tap, Swipe) on smartphones using Android Virtual Device (AVD). For example, \u201cFind a hotpot restaurant nearby and make a reservation for me tonight.\" Agents must understand the Android GUI and make decisions based on screen observations. (Cf. Appendix C)\nVAB-WebArena-Lite [77]. Web browsing is an ideal testbed for evaluating LMMs as GUI agents. Previous works [50, 31, 10, 71] mainly focus on offline evaluation. We adopt WebArena [77], a benchmark for text-based web GUI agents with 812 tasks across 5 websites. LMMs perform tasks based on user instructions, such as finding and summarizing customer reviews on OneStopShop. We use HTML SOM [26] to annotate operable HTML elements, enabling LMMs to generate actions via playwright. WebArena-Lite is a subset of 165 tasks, refined and adapted for multimodal evaluation, removing cross-website tasks and fixing implausible conditions. (Cf. Appendix D)"}, {"title": "3.3 Visual Design Agent", "content": "Visual design tasks demand a nuanced understanding of visual signals, which text-only LLMs cannot handle with any easy augmentation, unlike embodied or GUI agent tasks that can rely on external object detectors [55] or textual representations like accessibility trees [68].\nVAB-CSS. We create a new task to evaluate LMMs on web frontend design, focusing on CSS style adjustments. Fixing CSS styles is a labor-intensive task that often requires engineers to iteratively adjust an element through trial and error. Such a task inherently entails fine-grained visual grounding and reasoning across a series of rendering outcomes resulting from iterative CSS edits. In VAB-CSS, the agent iteratively edits the CSS style using provided tools until it thinks the rendering matches a given target design. We adopt success rate (SR) as the metric, which evaluates whether the final rendering matches the target design. (Cf. Appendix E)"}, {"title": "4 Methodology for VAB Data Collection", "content": "For agent tasks, it is known to be very challenging to design practical and verifiable task instances; let alone creating high-quality training trajectories on top of them later. In constructing VAB, we not only aim to deliver a high-quality agent benchmark but also endeavor to develop a systematic methodology for the problem of LMM-as-Visual-Foundation-Agent data curation. For task instance collection, we follow a two-stage paradigm (prototyping and instantiation) for each new task instance to ensure data quality and executability. Additionally, we harness a suite of hybrid strategies to collect training trajectories that can be used to tune open LMMs into better visual foundation agents. Our rigorous data collection process in VAB is crucial for presenting a high-quality resource for LMM-based agents (Figure 3). The statistics of different tasks in VAB are shown in Table 3."}, {"title": "4.1 Task Instance Collection: Prototyping and Instantiation", "content": "Curating meaningful and testable task instances for LMM agent tasks can be difficult. On one hand, they should be diverse and useful to cover real-world applications. On the other hand, they should be grounded to environments carefully to ensure feasibility and practicality. As a result, we collect all our task instances in a two-stage paradigm:\n\u2022 Prototyping: We gather many task prototypes representing high-level goals based on the functionality provided by the environment. Related items are temporarily set to placeholders."}, {"title": "4.2 Training Trajectory Collection: 3-Leveled Strategies", "content": "Recently, there has been a rise in benchmarks for interactively evaluating LLM or LMM agents [35, 77, 26, 68]. Despite showcasing the substantial potential of LLM and LMM as agents, they usually only provide the test set and thus fail to facilitate the improving of open LLMs and LMMs on agent tasks. In light of the challenge, in VAB we are devoted to offering a preliminary behavior cloning setup for training open LMM agents.\nImitation learning, especially the behavior cloning (BC) [42, 74] method, has been demonstrated as effective for building LLM agents from scratch. However, a significant challenge lies in curating high-quality BC trajectories in large quantities, where the best strategy could be likely environment-depended. In VAB, we systematically summarize our trajectory collecting into 3-leveled strategies:\n1. Program-based Solvers: Trajectories are collected by prototype-specific programs written by human experts (e.g., Playwright scripts for automating web browsing tasks).\n2. LMM Agent Bootstrapping: Trajectories are collected by prompted LMM agents (e.g., gpt-4o), with optional memory augmentations [65] to enhance performance. For instance, in Minecraft we allow agent to access memories for solving easier sub-goals (e.g., how to collect a stick) when constructing trajectories for more complex goals (e.g., how to collect a hammer).\n3. Human Demonstrations: Trajectories are annotated by human experts. It is necessary for scenarios where humans are indispensable (e.g., mobile apps require logged-in human accounts).\nThese strategies are quite different from each other and present their own unique advantages in certain environments. We summarize their recommendation levels on 4 dimensions (Cf. Table 1):\n\u2022 Average Cost: The most important dimension. Program-based solvers are the cheapest for massive production. Human demonstrations are of medium average cost, especially in large quantities as annotators become more proficient over time. Bootstrapping is currently the most expensive, since it is so far necessary to harness proprietary LMM APIs for building visual agents, which are expensive but still suffer from high failure rates in many tasks. However, as open LMMs become stronger for visual agent tasks, the cost will be substantially reduced.\n\u2022 Adaptability: It indicates how easy we can implement a strategy to an environment. Bootstrapping can be easily adapted to environments given good system prompts. Program-based solvers take some days for researchers to implement. Humans need detailed annotation manuals and sufficient time for training, and could be helpless due to poor environmental accessibility (e.g., hardware).\n\u2022 Versatility: It refers to how versatile tasks a strategy could deal with. Well-trained annotators could accomplish almost all tasks, while LMM agents usually fail to handle difficult ones. Program-based solvers can only tackle given prototypes. Therefore, for situations where diverse instructions are needed (e.g., 18 apps involved in VAB-Mobile), versatility is a first concern.\n\u2022 Flexibility: It denotes the trial and error process in the solution trajectories, which is crucial for agent applications where there could exist ideal but impractical single-step solutions. While LMM bootstrapping naturally presents the process, it is unlikely for program-based solvers to act so. For humans, trial and error in annotation is usually discouraged due to quality control.\nConsidering all mentioned dimensions and their trade-offs, we adopt a hybrid set of strategies for each of the 5 environments in VAB as shown in Table 1:\nFor VAB-OmniGibson, we adopt the program-based solvers focusing on the cost and adaptability. OmniGibson has no friendly interface for humans to operate on, and requires high-end laptops with GPUs supporting ray tracing and large main memory (> 10 GB) to run. Thus it is unlikely for us to find a large number of qualified annotators to label for OmniGibson. LMM agent bootstrapping is fine but uneconomical, as the task usually takes more steps than others (i.e., up to 100). Program-based solvers, instead, are suitable for collecting massive high-quality trajectories in OmniGibson.\nFor VAB-Minecraft, we adopt LMM agent bootstrapping considering adaptability. Minecraft requires some flexible explorations (as environments are generated randomly), which is beyond the scope of program-based solvers. Humans need to be well-trained for some time on playing Minecraft before becoming qualified annotators. Since previous work has explored the usage of memory augmentation [65] for improving LMM agents in Minecraft, it becomes practical to leverage the bootstrapping strategy by LMM APIs such as gpt-4o for creating training trajectories."}, {"title": "5 Baseline Experiment", "content": "Using VAB, we evaluate a comprehensive array of proprietary LMMs with prompting and also some selected open LMMs with fine-tuning to serve as LMM-as-Visual-Agent baselines. We also dive into several insights we encounter during the testing of existing LMMs, which unveil the critical challenges and future research directions for the development of LMM agents."}, {"title": "5.1 Setup", "content": "Baselines. We evaluate on both proprietary LMM APIs and selected open LMMs. For proprietary LMMs, we include models from OpenAI GPT [45, 43, 44], Anthropic Claude [3], Google Gemini [48, 59], and Qwen-VL-Max [4]. For open LMMs, we select eight state-of-the-art models as representative fine-tuning baselines in VAB: InternVL-2 [8], GLM-4V [16], CogVLM2 [63], CogAgent [18], CogVLM [63], LLaVA-NeXT [33], LLaVA-1.5 [32], Qwen-VL [4].\nPrompting. We format LMM-as-Visual-Foundation-Agent as two roles (i.e., user and assistant) interacting in multiple rounds. The task description, action spaces, few-shot demonstrations, and important notices for each environment are formatted as the system prompt at the beginning of the conversation. Task instruction is given in the first user round. Environmental observations and feedback are passed via user in later rounds. Considering current LMM APIs' poorer support of multi-image and outrageous cost when interaction rounds soar up, in Embodied and GUI agents we only offer the vision input of the latest user round (following [26]) while reserving history text contents. An exception is the CSS agent in Visual Design. In this case, comparing differences in visual inputs is essential, and the interaction rounds are typically fewer than 10. Therefore, we retain all image inputs in the conversation history for this task.\nTraining for Open LMMs. We generally follow the prompting format of proprietary LMM APIs in each environment to organize our training trajectories, and make several minor modifications. In the system prompt we remove the few-shot demonstrations as we would fine-tune models. In addition, during fine-tuning, since open LMMs perform poorly on multi-image input (especially for CogVLM and CogAgent, whose expert architecture disallows simple implementation of multi-image input), we only use the vision input of the latest user turn, and concatenate histories together using role tokens (i.e., \u201c<|user|>\") and linebreaks. For CSS agent where multi-image input is necessary, we concatenate history images vertically into one as the input. To benchmark the potential of LMMs to serve as visual foundation agents, we conduct multitask fine-tuning over the dataset aggregation of all environments. To optimize performance, all LMMs undergo full-parameter fine-tuning, with a batch size of 64 and 5k training steps. Other hyperparameters are configured using the default ones provided by the model's original repository or the third-party's integrated training framework. For data composition, we uniformly combine all training samples except for VAB-CSS, which we duplicate an additional 2 times as the preliminary experiments show that the task requires more extensive training for open LMMs to adapt to the screenshot concatenation format.\""}, {"title": "5.2 Main Results", "content": "Table 4 show the main results on VAB, including both prompting proprietary LMMs and fine-tuned open LMMs. We have several important observations on the status quo of LMM-as-Visual-Foundation-Agent.\nVAB is challenging for existing LMMs. We observe that existing LMMs face significant challenges when evaluated on VAB. The majority of proprietary LMMs, with mere prompting, achieve an overall success rate above 20%, demonstrating their multimodal understanding and reasoning abilities. The most capable LMM, gpt-4o, achieves an overall success rate of 36.2%. However, these performances are still far from satisfactory and not yet qualified for direct deployment. Notably, despite its superiority on existing benchmarks, claude-3.5-sonnet still falls significantly behind gpt-4o. Additionally, we present the first systematic evaluation of gpt-4o-mini on agent tasks, which reveals that its performance is considerably inferior to gpt-4o but comparable to claude-3-opus and gemini-1.5-pro.\nTrajectory SFT can improve LMM agents. For open LMMs, we find they can rarely follow the system prompt's instruction without fine-tuning in preliminary trials, resulting in 0% success rates. After training on VAB, open LMMs present significant improvements. The strongest one, InternVL-2, even outperforms gemini-1.0-pro on all evaluated environments and claude-3-opus on CSS agent task. These results suggest that learning from trajectories would be a promising direction for us to build visual foundation agents.\nGaps between top proprietary and open LMMs are huge but likely to be narrowed. Despite the improvement from trajectory SFT, the gap between proprietary and tested open LMMs is much wider than expected. While many of them have claimed competitive performance to gpt-4-vision-preview on traditional vision benchmarks such as image captioning, VQA, and so on, their fundamental ability to serve as practical visual foundation agents is far from comparable even after fine-tuning on VAB datasets. It also demonstrates that VAB could serve as an ideal testbed for benchmarking the practical performance of LMMs. With larger backbone LLMs (which are insufficiently tested in this work due to limitations of our computing resources) and more high-quality trajectory data, it is likely that open LMMs will be comparable or even outperform more proprietary LMMs."}, {"title": "6 Analysis", "content": "Multimodal agent tasks encompass two critical challenges: visual grounding and planning. We conduct fine-grained analyses to gain deeper insights into performance in these two aspects and offer valuable perspectives for the future development of visual foundation agents based on LMMs."}, {"title": "6.1 Visual Grounding Analysis", "content": "Visual grounding refers to the ability to associate language concepts with content in visual perception [14, 76], which is crucial for LMM-as-Visual-Foundation-Agent. We look into 3 typical design choices in VAB related to visual grounding to show its current status and challenges.\nThe use of object labels in embodied environment. Despite the strong image caption and object recognizing ability of LMMs, they do not seem to play well in the context of an embodied agent task. In VAB-OmniGibson, we compare the LMM-as-Visual-Foundation-Agent performance with and without object labels annotated in the vision input. The result shows that LMM agents significantly underperform without object labels. It indicates that notwithstanding LMMs' strong performance on downstream benchmarks, they can still struggle in the same task in the context of LMM-as-Visual-Foundation-Agent.\nThe use of Set-of-Marks (SoM) in GUI environment. For GUI tasks, we also augment the image input with SoM by default because it is difficult to elicit accurate bounding box coordinates from the LMM, which is essentially a referring expression comprehension (REC) task [46]. With our training trajectories, we can evaluate whether LMMs can effectively perform visual grounding by directly outputting a bounding box without relying on external SoM signals. Specifically, we fine-tune CogVLM2 with and without SoM. The results in Figure 5 show that CogVLM2 struggles to learn to directly output a bounding box, and SoM plays an instrumental role in visual grounding."}, {"title": "6.2 Performance on Planning", "content": "The role of thought in ReAct. ReAct [72] is one of the most commonly used frameworks for language agents. The central concept emphasizes the importance of integrating the agent's reasoning and actions by intertwining the output with both thought and action components. However, in our study, we find that the thought step may not be essential. When using gpt-4o and claude-3.5-sonnet as the backbone of the agents, directly outputting an action field can yield comparable or even superior performance compared to using the ReAct framework (see Table 6).\nRecovering from errors during planning. In real-world applications, agents must dynamically adjust their actions and plans based on environmental feedback. A crucial capability required for this is error recovery. To understand error recovery capabilities in LMMs, we analyze two representative models: gpt-4o, the most powerful model currently available, and glm-4v, a prominent open LMM. Their performance, illustrated in Figure 6, reveals that gpt-4o exhibits robust error recovery across most tasks, with GUI tasks being an exception due to their often irreversible nature. Importantly, we find that incorporating error recovery scenarios in training data significantly enhances the performance of fine-tuned open LMMs, as evidenced by results from VAB-OmniGibson and VAB-CSS (Cf. Appendix A.1 and Appendix E.2 for details about error recovery of training trajectories)."}, {"title": "7 Related Work", "content": "LMM-as-Visual-Agent. In pre-LMM era, most visual agents are built with task specific training [51] and reinforcement learning [24]. With the rapid development of LMMs , the study of LMM-based visual agents begins to thrive. Leveraging the general capabilities of LMMs, these visual agents have the potential to perform complex tasks in various scenarios, including embodied and game tasks , GUI interaction [76, 77, 26, 68, 21, 70], and visual design tasks [53, 28]. However, these complex scenarios pose several challenges for LMM-based visual agents: basic visual understanding and grounding [76, 73], vision-text information comprehension [25], instruction following, and long-term planning ability [67, 35]. Most general-purpose LMMs still lack strong zero-shot capabilities, leading to different application paradigms when deploying LMMs as visual agents. While prompting methods offer great convenience, they may not achieve satisfactory performance in many areas [77, 12]. Consequently, task-specific training and alignment remain common practices in these applications [27]. In response, VAB aims to establish a comprehensive benchmark for LMM-based visual agents, covering a wide range of typical applications. In the meantime, VAB seeks to provide an in-depth evaluation of both prompting and training approaches, ultimately fostering the development of LMM visual agents.\nBenchmarking LMM-based visual agents. With the rapid development of LMM agents and their impressive performance in various scenarios [68, 21, 70, 69, 53, 41], it has made the evaluation of LMM agent an urgent problem. In the GUI interaction domain, recent works have proposed static datasets [10, 47, 57] and interactive environments [77, 26, 68] to evaluate LMM agents in different applications, including web [77, 26, 10], mobile phone [47, 57], and desktop [68]. In the embodied domain, previous works have proposed various game environments [17, 13] and household environments [29], but few works have explored benchmarking LMM agents on these environments. Most existing benchmarks are designed for relatively narrow domains and lack a comprehensive evaluation across different applications of LMM agents. Additionally, many benchmarks focus solely on the prompting evaluation of LMM agents. VAB aims to provide a training set for open-source foundation LMMs, offering a new perspective on benchmarking these models and advancing their applicability in diverse tasks."}, {"title": "8 Conclusion", "content": "We introduce a new benchmark for evaluating LMMs as visual agents. In addition, we also provide training trajectories essential for fine-tuning LMMs. However, open LMMs fine-tuned with our dataset still perform below the level of proprietary LMMs like gpt-4o. Behavior cloning with the offline collected trajectories is a vital first step, but future advancements will come from integrating reinforcement learning in diverse interactive environments."}, {"title": "A VAB-OmniGibson", "content": "In this section, we provide additional details about VAB-OmniGibson that are not covered in the main paper due to space limitations.\nA.1 Detailed Description\nCurrent household datasets or benchmarks are not originally designed for LMMs, making them less suitable for evaluating today's LMMs. Behavior-1K [29] offers an action space focused on low-level physical control over the robot (e.g., joint angles), while Alfred [51] requires actions to output masks on images, which may not be practical for most LMMs. The ThreeDWorld Transport Challenge [15] provides high-level action APIs, but the simulator environment is less realistic and the tasks may not fully challenge LMMs. The recent work Octopus [69] sets up household tasks for LMMs in the OmniGibson simulator. However, in this setting, vision input is less critical as the observed objects are also listed in text input for LMMs.\nIn order to set up a realistic and challenging benchmark for testing LMMs' embodied planning ability, we select the recent household simulator OmniGibson [29] as the interactive environment, and build a pipeline for LMM to serve as a high-level planner on everyday household activities. An example of the task is shown in Fig. 7: The ego-centric image with annotated bounding boxes, high-level activity instruction and environment feedback are fed into the LMM, and it is tasked with reasoning over the current progress to decide on the next low-level action. It must interact with objects using the corresponding tags attached to the bounding boxes.\nTest Set. We select 45 activity instances from Behavior-1K [29], and manually adapt some of them to ensure these activities are solvable within our provided action space and suitable for evaluating current LMMs' embodied planning ability. We instantiate each activity in several scenes, resulting in a total of 181 test task instances. All the activity instructions are manually annotated by us.\nTraining Set. We provide a set of successful trajectories using both rule-based solving and LMM bootstrapping. We newly design 47 activities, each instantiated in several different scenes with various initializations of object positions, resulting in a total of 901 task instances. To solve these tasks, we develop a rule-based solver that decomposes the long-horizon activities into subtasks and solves them sequentially. Running the rule-based solver on the 901 training task instances yields 785 successful trajectories. Then we manually add a type of error recovery process (agent fails to place an object into a closed container, and then opens the container) into these trajectories, aiming to enhance LMMs' capability to rectify errors. Additionally, we select 464 training instances and utilize gpt-4-vision-preview to bootstrap 87 successful trajectories, resulting in a total of 872 training trajectories.\nMetrics. We adopt task success rate as the metric of VAB-OmniGibson. In Behavior-1K [29], each activity is defined in the form of BEHAVIOR Domain Definition Language (BDDL) [56], which describes the concrete initial and goal conditions of a specific activity. Only when all the goal conditions are met within the limit of 100 turns, the task is judged as successfully completed."}, {"title": "A.2 Actions", "content": "In VAB-OmniGibson, we provide the LMM agent with 20 low-level actions to interact with objects and navigate the household environment. The actions marked with an asterisk (*) are adapted from OmniGibson [29], while the others are newly defined and implemented by us. With these provided actions, the LMM agent is possible to solve all the testing instances.\n\u2022\ngrasp: Grasp a specific object into the robot's hand.\n\u2022\nmove: Move towards a specific object.\n\u2022\nmove_to_room: Move to a specific room in the house.\n\u2022\nturn_left: Turn the robot left 90 degrees.\n\u2022\nturn_right: Turn the robot right 90 degrees.\n\u2022\nraise_camera: Raise the camera of the robot to see higher objects.\n\u2022\nlower_camera: Lower the camera of the robot to see lower objects.\n\u2022\nput_inside: Place the object from the robot's hand inside another object."}, {"title": "A.3 Rule-based Solver for Training Trajectory Collection", "content": "BDDL task goals. Among activities of VAB-OmniGibson", "types": "identifying the state of a specific object, or the positional relationship between two objects.\nMethod of rule-based solver. To achieve the BDDL task goal of a VAB-OmniGibson activity, the rule-based solver need to sequentially fulfill all the subgoals. For the first type of subgoal, the rule-based solver can navigate (move_to_room, move"}]}