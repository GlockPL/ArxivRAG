{"title": "VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents", "authors": ["Xiao Liu", "Tianjie Zhang", "Yu Gu", "Iat Long Iong", "Yifan Xu", "Xixuan Song", "Shudan Zhang", "Hanyu Lai", "Xinyi Liu", "Hanlin Zhao", "Jiadai Sun", "Xinyue Yang", "Yu Yang", "Zehan Qi", "Shuntian Yao", "Xueqiao Sun", "Siyi Cheng", "Qinkai Zheng", "Hao Yu", "Hanchen Zhang", "Wenyi Hong", "Ming Ding", "Lihang Pan", "Xiaotao Gu", "Aohan Zeng", "Zhengxiao Du", "Chan Hee Song", "Yu Su", "Yuxiao Dong", "Jie Tang"], "abstract": "Large Multimodal Models (LMMs) have ushered in a new era in artificial intelli-gence, merging capabilities in both language and vision to form highly capableVisual Foundation Agents. These agents are postulated to excel across a myriadof tasks, potentially approaching general artificial intelligence. However, existingbenchmarks fail to sufficiently challenge or showcase the full potential of LMMs incomplex, real-world environments. To address this gap, we introduce VisualAgent-Bench (VAB), a comprehensive and pioneering benchmark specifically designedto train and evaluate LMMs as visual foundation agents across diverse scenarios,including Embodied, Graphical User Interface, and Visual Design, with tasks for-mulated to probe the depth of LMMs' understanding and interaction capabilities.Through rigorous testing across nine proprietary LMM APIs and eight open models,we demonstrate the considerable yet still developing agent capabilities of thesemodels. Additionally, VAB constructs a trajectory training set constructed throughhybrid methods including Program-based Solvers, LMM Agent Bootstrapping,and Human Demonstrations, promoting substantial performance improvements inLMMs through behavior cloning. Our work not only aims to benchmark existingmodels but also provides a solid foundation for future development into visualfoundation agents. Code, train & test data, and part of fine-tuned open LMMs areavailable at https://github.com/THUDM/VisualAgentBench.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Foundation Models, particularly Large Language Models (LLMs) [6, 9,60, 75] and Large Multimodal Models (LMMs) [34, 43, 45, 3], have showcased their profoundcapabilities in understanding and processing vast amounts of world knowledge, factual information,and common sense reasoning. Notably, these models have demonstrated potential as intelligentagents [49, 40, 66], addressing a broad spectrum of real-world challenges [35]. LMMs, in particular,enhance the capabilities of these agents by integrating visual inputs, thereby expanding the scope ofintelligent agent applications.\nThis progress has given rise to the concept of Foundation Agents\u2014generalist agents adept atmastering a plethora of skills across various virtual and embodied environments, mirroring humanversatility. These agents, especially those powered by LMMs, are envisioned to excel in multitaskenvironments without the need for task-specific fine-tuning, a paradigm already set by LLM-basedlanguage agents. The burgeoning field of visual foundation agents offers promising pathways towardachieving AGI, with the potential to significantly elevate human productivity and creativity.\nHowever, the setup for LMM-as-Visual-Foundation-Agent remains underdeveloped. Most existingevaluations on LMMs focus on traditional tasks like Visual Question Answering (VQA) [23, 54, 38],Optical Character Recognition (OCR) [36], and Referring Expression Generation (REG) [22], or onperformance in standardized human exams [73, 37]. These assessments rarely measure the models'higher-level reasoning and planning capabilities or their specific strengths as visual agents. In contrast,the role of LLMs as agents in text environments has been extensively explored and validated as areliable measure of their capabilities [72, 35].\nRecent benchmarks for multimodal agents, while valuable, do not adequately address the compre-hensive evaluation required for LMM-as-Visual-Foundation-Agent. These benchmarks often limittheir focus to single environments such as Household [51, 52], Gaming [13, 67], Web [10, 77, 26], orDesktop scenarios [68, 21]. This narrow scope prevents a holistic assessment of LMMs' multitaskagent capabilities. Furthermore, the prevalent prompting-only evaluation in existing benchmarksdoes not suffice for open LMMs [34, 4, 63], which typically show limited instruction-followingcapabilities so far, thus hindering a comprehensive evaluation.\nTo bridge this gap, we introduce VISUALAGENTBENCH (VAB)\u2014the first systematic benchmarkdesigned to multitask train and evaluate visual foundation agents across a diverse array of realisticvision-centric tasks. We present three representative scenarios and develop five distinct datasetsfor this study: Embodied (VAB-OmniGibson, VAB-Minecraft), Graphical User Interface (GUI)(VAB-Mobile [1], VAB-WebArena-lite [77]), and Visual Design (VAB-CSS), enabling compre-hensive testing and development of agents that can navigate complex spaces, interact with digitalinterfaces, and understand aesthetic and functional aspects of visual design. This diversity not onlychallenges the agents' capabilities across different settings but also enhances their adaptability andutility in practical applications, paving the way for more robust and versatile visual foundation agents.\nWe have standardized the prompting and data formats to facilitate a consistent evaluation of visualfoundation agents across these environments. Each VAB task is assessed through interactive evalu-ation [35, 77, 20, 71], where LMMs engage directly with the environment, and their performance"}, {"title": "2 Problem Formulation and VAB Design Features", "content": "In this section, we introduce the problem definition of LMM-as-Visual-Foundation-Agent. Upon thedefinition, we explain a series of practical principles we follow during the design of VAB.\nLMM-as-Visual-Foundation-Agent. An agentic problem could be generally formulated as aPartially Observable Markov Decision Process (POMDP) problem, where $S$ denotes the state space,$A$ denotes the action space, $T$ denotes the transition function, $R$ refers to the reward function,$I$ refers to the instruction space, and $O$ refers to the observation space. Compared to LLM-as-Agent [35], the observation space $O$ must incorporate visual inputs (e.g., images or videos) inLMM-as-Visual-Foundation-Agent, significantly extending the application scope but also casting asubstantial challenge for LMMs to reconcile their multimodal understanding and high-level reasoning.\nDesign Features of VAB. Given that LMMs are still evolving rapidly, we adhere to several principlesin our design of VAB to accommodate the current capabilities and limitations of LMMs.\n\u2022 Vision-Centric: VAB agent tasks are designed to primarily rely on visual inputs to solveproblems. While additional text inputs could be beneficial, VAB aims to evaluate how LMMSperform when perceiving the environment as humans do in agent tasks. For example, whileHTML is shown useful for Web GUI Agent [77, 10], humans typically browse the internet fromscreens without reading HTMLs.\n\u2022 High-Level Decision Making: VAB focuses on evaluating LMMs' high-level decision-makingabilities. Compared to prior smaller visual-language models that specifically target low-levelpolicies [39, 5, 30], LMMs excel at high-level planning and interacting [11] in text responsethanks to their commonsense, knowledge, and flexible instruction following with mere prompting.Therefore, in VAB, we simplify the low-level control by providing convenient action interfaces,and ask tested LMMs to concentrate on delivering high-level decision sequences in text.\n\u2022 Interactive Evaluation: Evaluating LLMs or LMMs on real-world agent tasks is challenging, astask goals can be achieved by various means. As a result, it becomes a mainstream practice toevaluate in an interactive manner [35, 77, 20, 68]. VAB also adheres to this principle."}, {"title": "3 VISUALAGENTBENCH: Tasks and Environments", "content": "In VAB, we carefully select the most representative and promising tasks that could be enabled byLMM-based agents. These tasks generally fall into three categories: embodied agents, includinghousehold and game environments; GUI agents, covering mobile and web apps; and visual designagents, focusing on frontend CSS debugging (Figure 2). They span diverse domains and featureunique challenges, providing an ideal testbed for a comprehensive evaluation of LMM-based agents.When constructing VAB, we strictly follow the principles outlined in Section 2. Our efforts focuson addressing gaps in evaluating LMM-based agents while leveraging existing resources to avoidredundancy, ensuring all our work is meaningful and avoids reinventing the wheel. For 4 out of5 tasks, we collect new data from scratch. For web agents, we adapt and clean WebArena [77] asour test set, as it is already suitable for LMM-based evaluation. For household agents, we use theOmniGibson environment from Behavior-1k [29] and create new tasks based on high-level actionswe defined, which are crucial for evaluating LMM-based agents and absent in existing datasets. Wesimilarly construct our tasks in Minecraft using the MineRL environment\u00b9 with our self-definedhigh-level actions. Finally, for our mobile app and CSS debugging tasks, we create new interactiveenvironments due to the lack of suitable resources in the literature and collect data based on theseenvironments. An overview of VAB is shown in Table 2."}, {"title": "3.1 Embodied Agent", "content": "Embodied agents have been a central topic in AI, naturally involving multimodal sensory data,including language and vision signals. The multimodal capabilities of LMMs could enable newpossibilities for embodied agents.\nVAB-OmniGibson. One of the most actively researched environments in embodied AI is thehousehold environment due to its complexity and range of everyday tasks [19, 55, 51]. We buildthe household environment for embodied agents using OmniGibson, a high-fidelity simulator basedon Nvidia Omniverse that features diverse scenes and realistic physical effects. An exampleactivity in VAB-OmniGibson would be \u201cPut all 8 plates from the countertops into the cabinet in thekitchen\", where agents should accomplish the tasks using provided high-level actions (e.g., \"grasp\",\u201cput_inside\u201d). We adopt the task success rate as the evaluation metric. (Cf. Appendix A).\nVAB-Minecraft. Minecraft has become a popular open-world environment for developing generalistembedded agents due to its diverse tasks (e.g., survival, harvesting, crafting, combat, and creativetasks), varied environments, and interactive mobs, necessitating generalized agent abilities [13, 30]. InVAB-Minecraft, the agent is expected to accomplish a wide range of tasks, including item collectionand killing hostile mobs. An example task in VAB-Minecraft would be \"Get a fishing rod in yourinventory\", and the LMM agent need to interact with the game environment using provided scripts(e.g., \"craft\", \u201csmelt\u201d) or calling a low-level controller Steve-1 [30] with prompt. We adopt thetask success rate as the evaluation metric. (Cf. Appendix B)"}, {"title": "3.2 GUI Agent", "content": "GUI is another typical scenario where LMM agents may excel. Compared to embodied environments,GUI environments are more information-intensive and require a good understanding of UI elementsand layouts. We provide two interactive and reproducible GUI environments, Mobile (i.e., Android)and WebArena, to evaluate LMM GUI agents in a practical manner.\nVAB-Mobile [1]. Automated agents on Android GUI can significantly advance personal digitalassistants. Although pioneer works like MOTIF [7] and AITW [47] have explored training andevaluating these agents, they typically use Step Success Rate evaluated offline. Recent works [70, 62]leverage LMMs as Android GUI agents but lack reproducible executive evaluation frameworks. Weaddress this by creating tasks for LMM agents to perform human-like actions (e.g., Tap, Swipe) onsmartphones using Android Virtual Device (AVD). For example, \u201cFind a hotpot restaurant nearbyand make a reservation for me tonight.\u201d Agents must understand the Android GUI and make decisionsbased on screen observations. (Cf. Appendix C)\nVAB-WebArena-Lite [77]. Web browsing is an ideal testbed for evaluating LMMs as GUI agents.Previous works [50, 31, 10, 71] mainly focus on offline evaluation. We adopt WebArena [77], abenchmark for text-based web GUI agents with 812 tasks across 5 websites. LMMs perform tasksbased on user instructions, such as finding and summarizing customer reviews on OneStopShop. Weuse HTML SOM [26] to annotate operable HTML elements, enabling LMMs to generate actions viaplaywright. WebArena-Lite is a subset of 165 tasks, refined and adapted for multimodal evaluation,removing cross-website tasks and fixing implausible conditions. (Cf. Appendix D)"}, {"title": "3.3 Visual Design Agent", "content": "Visual design tasks demand a nuanced understanding of visual signals, which text-only LLMs cannothandle with any easy augmentation, unlike embodied or GUI agent tasks that can rely on externalobject detectors [55] or textual representations like accessibility trees [68].\nVAB-CSS. We create a new task to evaluate LMMs on web frontend design, focusing on CSS styleadjustments. Fixing CSS styles is a labor-intensive task that often requires engineers to iterativelyadjust an element through trial and error. Such a task inherently entails fine-grained visual groundingand reasoning across a series of rendering outcomes resulting from iterative CSS edits. In VAB-CSS,the agent iteratively edits the CSS style using provided tools until it thinks the rendering matchesa given target design. We adopt success rate (SR) as the metric, which evaluates whether the finalrendering matches the target design. (Cf. Appendix E)"}, {"title": "4 Methodology for VAB Data Collection", "content": "For agent tasks, it is known to be very challenging to design practical and verifiable task instances;let alone creating high-quality training trajectories on top of them later. In constructing VAB, weonly aim to deliver a high-quality agent benchmark but also endeavor to develop a systematicmethodology for the problem of LMM-as-Visual-Foundation-Agent data curation. For task instancecollection, we follow a two-stage paradigm (prototyping and instantiation) for each new task instanceto ensure data quality and executability. Additionally, we harness a suite of hybrid strategies tocollect training trajectories that can be used to tune open LMMs into better visual foundation agents.Our rigorous data collection process in VAB is crucial for presenting a high-quality resource forLMM-based agents. The statistics of different tasks in VAB are shown in Table 3."}, {"title": "4.1 Task Instance Collection: Prototyping and Instantiation", "content": "Curating meaningful and testable task instances for LMM agent tasks can be difficult. On one hand,they should be diverse and useful to cover real-world applications. On the other hand, they should begrounded to environments carefully to ensure feasibility and practicality. As a result, we collect allour task instances in a two-stage paradigm:\n\u2022 Prototyping: We gather many task prototypes representing high-level goals based on the func-tionality provided by the environment. Related items are temporarily set to placeholders."}, {"title": "4.2 Training Trajectory Collection: 3-Leveled Strategies", "content": "Recently, there has been a rise in benchmarks for interactively evaluating LLM or LMM agents [35,77, 26, 68]. Despite showcasing the substantial potential of LLM and LMM as agents, they usuallyonly provide the test set and thus fail to facilitate the improving of open LLMs and LMMs on agenttasks. In light of the challenge, in VAB we are devoted to offering a preliminary behavior cloningsetup for training open LMM agents.\nImitation learning, especially the behavior cloning (BC) [42, 74] method, has been demonstrated aseffective for building LLM agents from scratch. However, a significant challenge lies in curatinghigh-quality BC trajectories in large quantities, where the best strategy could be likely environment-depended. In VAB, we systematically summarize our trajectory collecting into 3-leveled strategies:\n1. Program-based Solvers: Trajectories are collected by prototype-specific programs written byhuman experts (e.g., Playwright scripts for automating web browsing tasks).\n2. LMM Agent Bootstrapping: Trajectories are collected by prompted LMM agents (e.g., gpt-40),with optional memory augmentations [65] to enhance performance. For instance, in Minecraft weallow agent to access memories for solving easier sub-goals (e.g., how to collect a stick) whenconstructing trajectories for more complex goals (e.g., how to collect a hammer).\n3. Human Demonstrations: Trajectories are annotated by human experts. It is necessary forscenarios where humans are indispensable (e.g., mobile apps require logged-in human accounts).\nThese strategies are quite different from each other and present their own unique advantages in certainenvironments. We summarize their recommendation levels on 4 dimensions (Cf. Table 1):\n\u2022 Average Cost: The most important dimension. Program-based solvers are the cheapest formassive production. Human demonstrations are of medium average cost, especially in largequantities as annotators become more proficient over time. Bootstrapping is currently the mostexpensive, since it is so far necessary to harness proprietary LMM APIs for building visual agents,which are expensive but still suffer from high failure rates in many tasks. However, as openLMMs become stronger for visual agent tasks, the cost will be substantially reduced.\n\u2022 Adaptability: It indicates how easy we can implement a strategy to an environment. Bootstrap-ping can be easily adapted to environments given good system prompts. Program-based solverstake some days for researchers to implement. Humans need detailed annotation manuals andsufficient time for training, and could be helpless due to poor environmental accessibility (e.g.,hardware).\n\u2022 Versatility: It refers to how versatile tasks a strategy could deal with. Well-trained annotatorscould accomplish almost all tasks, while LMM agents usually fail to handle difficult ones.Program-based solvers can only tackle given prototypes. Therefore, for situations where diverseinstructions are needed (e.g., 18 apps involved in VAB-Mobile), versatility is a first concern.\n\u2022 Flexibility: It denotes the trial and error process in the solution trajectories, which is crucialfor agent applications where there could exist ideal but impractical single-step solutions. WhileLMM bootstrapping naturally presents the process, it is unlikely for program-based solvers to actso. For humans, trial and error in annotation is usually discouraged due to quality control.\nConsidering all mentioned dimensions and their trade-offs, we adopt a hybrid set of strategies foreach of the 5 environments in VAB as shown in Table 1:\nFor VAB-OmniGibson, we adopt the program-based solvers focusing on the cost and adaptability.OmniGibson has no friendly interface for humans to operate on, and requires high-end laptops withGPUs supporting ray tracing and large main memory (> 10 GB) to run. Thus it is unlikely for usto find a large number of qualified annotators to label for OmniGibson. LMM agent bootstrapping is finebut uneconomical, as the task usually takes more steps than others (i.e., up to 100). Program-basedsolvers, instead, are suitable for collecting massive high-quality trajectories in OmniGibson.\nFor VAB-Minecraft, we adopt LMM agent bootstrapping considering adaptability. Minecraft requiressome flexible explorations (as environments are generated randomly), which is beyond the scopeof program-based solvers. Humans need to be well-trained for some time on playing Minecraftbefore becoming qualified annotators. Since previous work has explored the usage of memoryaugmentation [65] for improving LMM agents in Minecraft, it becomes practical to leverage thebootstrapping strategy by LMM APIs such as gpt-40 for creating training trajectories."}, {"title": "5 Baseline Experiment", "content": "Using VAB, we evaluate a comprehensive array of proprietary LMMs with prompting and also someselected open LMMs with fine-tuning to serve as LMM-as-Visual-Agent baselines. We also diveinto several insights we encounter during the testing of existing LMMs, which unveil the criticalchallenges and future research directions for the development of LMM agents."}, {"title": "5.1 Setup", "content": "Baselines. We evaluate on both proprietary LMM APIs and selected open LMMs. For proprietaryLMMs, we include models from OpenAI GPT [45, 43, 44], Anthropic Claude [3], Google Gemini [48,59], and Qwen-VL-Max [4]. For open LMMs, we select eight state-of-the-art models as representativefine-tuning baselines in VAB: InternVL-2 [8], GLM-4V [16], CogVLM2 [63], CogAgent [18],CogVLM [63], LLaVA-NeXT [33], LLaVA-1.5 [32], Qwen-VL [4].\nPrompting. We format LMM-as-Visual-Foundation-Agent as two roles (i.e., user and assistant)interacting in multiple rounds. The task description, action spaces, few-shot demonstrations, andimportant notices for each environment are formatted as the system prompt at the beginning ofthe conversation. Task instruction is given in the first user round. Environmental observations andfeedback are passed via user in later rounds. Considering current LMM APIs' poorer support ofmulti-image and outrageous cost when interaction rounds soar up, in Embodied and GUI agentswe only offer the vision input of the latest user round (following [26]) while reserving history textcontents. An exception is the CSS agent in Visual Design. In this case, comparing differences invisual inputs is essential, and the interaction rounds are typically fewer than 10. Therefore, we retainall image inputs in the conversation history for this task.\nTraining for Open LMMs. We generally follow the prompting format of proprietary LMM APIs ineach environment to organize our training trajectories, and make several minor modifications. In thesystem prompt we remove the few-shot demonstrations as we would fine-tune models. In addition,during fine-tuning, since open LMMs perform poorly on multi-image input (especially for CogVLMand CogAgent, whose expert architecture disallows simple implementation of multi-image input),we only use the vision input of the latest user turn, and concatenate histories together using role\ntokens (i.e., \u201c<|user|>\u201d) and linebreaks. For CSS agent where multi-image input is necessary, weconcatenate history images vertically into one as the input. To benchmark the potential of LMMsto serve as visual foundation agents, we conduct multitask fine-tuning over the dataset aggregationof all environments. To optimize performance, all LMMs undergo full-parameter fine-tuning, witha batch size of 64 and 5k training steps. Other hyperparameters are configured using the defaultones provided by the model's original repository or the third-party's integrated training framework.For data composition, we uniformly combine all training samples except for VAB-CSS, which weduplicate an additional 2 times as the preliminary experiments show that the task requires moreextensive training for open LMMs to adapt to the screenshot concatenation format."}, {"title": "5.2 Main Results", "content": "Table 4 show the main results on VAB, including both prompting proprietary LMMs and fine-tuned open LMMs. We have several important observations on the status quo of LMM-as-Visual-Foundation-Agent.\nVAB is challenging for existing LMMs. We observe that existing LMMs face significant challengeswhen evaluated on VAB. The majority of proprietary LMMs, with mere prompting, achieve an overallsuccess rate above 20%, demonstrating their multimodal understanding and reasoning abilities. Themost capable LMM, gpt-40, achieves an overall success rate of 36.2%. However, these performancesare still far from satisfactory and not yet qualified for direct deployment. Notably, despite itssuperiority on existing benchmarks, claude-3.5-sonnet still falls significantly behind gpt-40.Additionally, we present the first systematic evaluation of gpt-40-mini on agent tasks, whichreveals that its performance is considerably inferior to gpt-40 but comparable to claude-3-opusand gemini-1.5-pro.\nTrajectory SFT can improve LMM agents. For open LMMs, we find they can rarely follow the sys-tem prompt's instruction without fine-tuning in preliminary trials, resulting in 0% success rates. Aftertraining on VAB, open LMMs present significant improvements. The strongest one, InternVL-2,even outperforms gemini-1.0-pro on all evaluated environments and claude-3-opus on CSSagent task. These results suggest that learning from trajectories would be a promising direction for usto build visual foundation agents.\nGaps between top proprietary and open LMMs are huge but likely to be narrowed. De-spite the improvement from trajectory SFT, the gap between proprietary and tested open LMMsis much wider than expected. While many of them have claimed competitive performance togpt-4-vision-preview on traditional vision benchmarks such as image captioning, VQA, and soon, their fundamental ability to serve as practical visual foundation agents is far from comparableeven after fine-tuning on VAB datasets. It also demonstrates that VAB could serve as an ideal testbedfor benchmarking the practical performance of LMMs. With larger backbone LLMs (which areinsufficiently tested in this work due to limitations of our computing resources) and more high-qualitytrajectory data, it is likely that open LMMs will be comparable or even outperform more proprietaryLMMs."}, {"title": "6 Analysis", "content": "Multimodal agent tasks encompass two critical challenges: visual grounding and planning. Weconduct fine-grained analyses to gain deeper insights into performance in these two aspects and offervaluable perspectives for the future development of visual foundation agents based on LMMs."}, {"title": "6.1 Visual Grounding Analysis", "content": "Visual grounding refers to the ability to associate language concepts with content in visual percep-tion [14, 76], which is crucial for LMM-as-Visual-Foundation-Agent. We look into 3 typical designchoices in VAB related to visual grounding to show its current status and challenges.\nThe use of object labels in embodied environment. Despite the strong image caption and objectrecognizing ability of LMMs, they do not seem to play well in the context of an embodied agent task.In VAB-OmniGibson, we compare the LMM-as-Visual-Foundation-Agent performance with andwithout object labels annotated in the vision input. The result shows that LMM agents significantlyunderperform without object labels. It indicates that notwithstanding LMMs' strong performance ondownstream benchmarks, they can still struggle in the same task in the context of LMM-as-Visual-Foundation-Agent.\nThe use of Set-of-Marks (SoM) in GUI environment. For GUI tasks, we also augment the imageinput with SoM by default because it is difficult to elicit accurate bounding box coordinates fromthe LMM, which is essentially a referring expression comprehension (REC) task [46]. With ourtraining trajectories, we can evaluate whether LMMs can effectively perform visual grounding bydirectly outputting a bounding box without relying on external SoM signals. Specifically, we fine-tuneCogVLM2 with and without SoM. The results in Figure 5 show that CogVLM2 struggles to learn todirectly output a bounding box, and SoM plays an instrumental role in visual grounding."}, {"title": "6.2 Performance on Planning", "content": "The role of thought in ReAct. ReAct [72] is one of the most commonly used frameworks for lan-guage agents. The central concept emphasizes the importance of integrating the agent's reasoning andactions by intertwining the output with both thought and action components. However, in our study,we find that the thought step may not be essential. When using gpt-40 and claude-3.5-sonnet asthe backbone of the agents, directly outputting an action field can yield comparable or even superiorperformance compared to using the ReAct framework (see Table 6).\nRecovering from errors during planning. In real-world applications, agents must dynamicallyadjust their actions and plans based on environmental feedback. A crucial capability required for thisis error recovery. To understand error recovery capabilities in LMMs, we analyze two representativemodels: gpt-40, the most powerful model currently available, and glm-4v, a prominent open LMM.Their performance, illustrated in Figure 6, reveals that gpt-40 exhibits robust error recovery acrossmost tasks, with GUI tasks being an exception due to their often irreversible nature. Importantly, wefind that incorporating error recovery scenarios in training data significantly enhances the performanceof fine-tuned open LMMs, as evidenced by results from VAB-OmniGibson and VAB-CSS (Cf.Appendix A.1 and Appendix E.2 for details about error recovery of training trajectories)."}, {"title": "7 Related Work", "content": "LMM-as-Visual-Agent. In pre-LMM era, most visual agents are built with task specific training [51]and reinforcement learning [24]. With the rapid development of LMMs [45, 48, 43, 4, 3, 59,16], the study of LMM-based visual agents begins to thrive. Leveraging the general capabilitiesof LMMs, these visual agents have the potential to perform complex tasks in various scenarios,including embodied and game tasks [5, 69, 11, 58], GUI interaction [76, 77, 26, 68, 21, 70], andvisual design tasks [53, 28]. However, these complex scenarios pose several challenges for LMM-based visual agents: basic visual understanding and grounding [76, 73], vision-text informationcomprehension [25], instruction following, and long-term planning ability [67, 35]. Most general-purpose LMMs still lack strong zero-shot capabilities, leading to different application paradigmswhen deploying LMMs as visual agents. While prompting methods offer great convenience, they maynot achieve satisfactory performance in many areas [77, 12]. Consequently, task-specific trainingand alignment remain common practices in these applications [27]. In response, VAB aims toestablish a comprehensive benchmark for LMM-based visual agents, covering a wide range of typicalapplications. In the meantime, VAB seeks to provide an in-depth evaluation of both prompting andtraining approaches, ultimately fostering the development of LMM visual agents.\nBenchmarking LMM-based visual agents. With the rapid development of LMM agents and theirimpressive performance in various scenarios [68, 21, 70, 69, 53, 41], it has made the evaluationof LMM agent an urgent problem. In the GUI interaction domain, recent works have proposedstatic datasets [10, 47, 57] and interactive environments [77, 26, 68] to evaluate LMM agents indifferent applications, including web [77, 26, 10], mobile phone [47, 57], and desktop [68]. In theembodied domain, previous works have proposed various game environments [17, 13] and householdenvironments [29], but few works have explored benchmarking LMM agents on these environments.Most existing benchmarks are designed for relatively narrow domains and lack a comprehensiveevaluation across different applications of LMM agents. Additionally, many benchmarks focus solelyon the prompting evaluation of LMM agents. VAB aims to provide a training set for open-sourcefoundation LMMs, offering a new perspective on benchmarking these models and advancing theirapplicability in diverse tasks."}, {"title": "8 Conclusion", "content": "We introduce a new benchmark for evaluating LMMs as visual agents. In addition, we also providetraining trajectories essential for fine-tuning LMMs. However, open LMMs fine-tuned with ourdataset still perform below the level of proprietary LMMs like gpt-40. Behavior cloning with theoffline collected trajectories is a vital first step, but future advancements will come from integratingreinforcement learning in diverse interactive environments."}]}