{"title": "Curriculum Negative Mining For Temporal Networks", "authors": ["Ziyue Chen", "Tongya Zheng", "Mingli Song"], "abstract": "Temporal networks are effective in capturing the evolving interactions of networks over time, such as social networks and e-commerce networks. In recent years, researchers have primarily concentrated on developing specific model architectures for Temporal Graph Neural Networks (TGNNs) in order to improve the representation quality of temporal nodes and edges. However, limited attention has been given to the quality of negative samples during the training of TGNNs. When compared with static networks, temporal networks present two specific challenges for negative sampling: positive sparsity and positive shift. Positive sparsity refers to the presence of a single positive sample amidst numerous negative samples at each timestamp, while positive shift relates to the variations in positive samples across different timestamps. To robustly address these challenges in training TGNNs, we introduce Curriculum Negative Mining (CurNM), a model-aware curriculum learning framework that adaptively adjusts the difficulty of negative samples. Within this framework, we first establish a dynamically updated negative pool that balances random, historical, and hard negatives to address the challenges posed by positive sparsity. Secondly, we implement a temporal-aware negative selection module that focuses on learning from", "sections": [{"title": "1. Introduction", "content": "Temporal networks have gained popularity in recent years due to their capacity to capture the evolving nature inherent in diverse domains, such as social networks [18, 21, 28], recommendation systems [1, 16], financial transactions [22], transport [26], and political networks [6, 11, 19]. Unlike their static counterparts, these networks support additions, deletions, and changes in the features of nodes and edges over time by modeling a sequence of time-stamped events. To best leverage this temporal information, Temporal Graph Neural Networks (TGNNs) [8, 16, 24, 25, 27, 30, 33] have been developed.\nSimilar to static graph models, TGNNs [24, 33] employ negative sampling techniques to prevent the models from being overwhelmed by a disproportion-ally high number of negative over positive samples and enhance the models' efficiency. The most straightforward approaches often adopted are random sampling and recent sampling [24, 33]. However, temporal networks intro-duce two unique challenges that particularly undermine these approaches: positive sparsity and positive shift. Positive sparsity emerges as the inclusion of a temporal dimension significantly increases the negative-to-positive ratio by the number of timestamps, compared to static graphs. Consequently, ran-dom sampling in this context tends to produce a multitude of irrelevant and easily distinguishable negative samples, offering little to no value. Positive shift, on the other hand, occurs as the relationships between nodes change over time; for instance, in recommendation systems, users' preferences are likely to evolve [16]. Ignoring this factor may result in training models on outdated information, particularly in datasets spanning long periods.\nNonetheless, the negative sampling problem in temporal network learning has been seldom explored. While such efforts are limited for TGNNs, there"}, {"title": "2. Related Work", "content": "is a wealth of strategies developed for static graphs. Whether by adopting alternative distributions [4, 32, 36] or employing model-aware methods to select negatives that closely resemble positives [5, 12, 17, 38, 39], these existing strategies seek to enhance models' performance by focusing on more informative negative samples, specifically those that closely resemble positive ones, during training. However, applying these strategies directly to tempo-ral networks is problematic due to positive sparsity. In particular, given the scarcity of positive samples, using negative samples that are too similar to positives can skew the classification boundary excessively towards the nega-tives. This can lead to confusion within the model, particularly during early epochs, and result in poor performance. Thus, a more gradual transition from easy to hard negatives is needed. Moreover, positive shift induces a negative shift, rendering existing methods for static graphs ineffective for temporal networks. This highlights the need for components that can incorporate re-cent information and update dynamically over time. Till now, ENS [7] is the only negative sampling method specifically designed for temporal networks. While targeting temporal networks, ENS adopts a heuristic, global nega-tive mining strategy instead of an adaptive, personalized approach, leading to unstable performance. Additionally, it suffers from low efficiency due to sampling from all historical edges.\nTo address these challenges, we introduce the CurNM method. Overall, CurNM is a curriculum learning strategy that dynamically adjusts the learn-ing difficulty of negative samples based on the model's performance, thereby addressing the prevalent issue of steep difficulty curves in existing methods. Subsequently, we incorporate two components to address the unique char-acteristics of temporal networks. Firstly, to address positive sparsity, we construct a negative pool for selection, which includes a mix of random sam-ples and historical neighbors, with hard negatives added if the model shows strong learning progress. Starting with a relatively small negative pool also helps maintain computational efficiency. Secondly, we design a negative se-lection function that leverages temporal-aware disentanglement to capture positive shifts and individual node preferences, which further enhances the model's granularity.\nOur key contributions are summarized as follows:\n\u2022 We identify two crucial challenges of negative sampling in temporal networks, i.e., positive sparsity and positive shift.\n\u2022 An efficient curriculum learning framework on negative sampling is"}, {"title": "2.1. Temporal Graph Learning", "content": "Motivated by the significant progress of deep learning in Computer Vi-sion [15] and Natural Language Processing [20], learning dense embeddings for nodes has become a de facto standard paradigm in graph learning [9, 23], unifying different graph tasks into a graph representation learning task. Sub-sequently, Graph Neural Networks [10, 14, 31], which take both graph struc-ture and node features into account, replaced skip-gram models [9, 23] for graph representations. Influenced by the advances of static graph represen-tation learning, TGNNs have been proposed to handle the evolving dynamics of temporal networks [29], updating node embeddings as time elapses.\nExisting TGNNs can be classified into discrete-time and continuous-time TGNNs based on their temporal natures [13]. On the one hand, discrete-time TGNNs [8, 22, 25, 27] focus on the sequential dynamics of a series of graph snapshots, where the timespan of a snapshot depends largely on the graph domain. The consequent discrete-time TGNNs include a combination of Recurrent Neural Networks and Graph Neural Networks. DySAT [25], a typical discrete-time TGNN, first learns the structural representations of nodes within each graph snapshot and then aggregates across multiple snap-shots using a temporal self-attention layer.\nOn the other hand, continuous-time TGNNs focus on the sequential in-teractions of nodes, thereby offering more fine-grained temporal information. Various design paradigms exist for continuous-time TGNNs since temporal networks can be seen as sequential dynamics of either nodes or a graph. JODIE [16] utilizes two mutually updated RNNs to update user and item embeddings, alongside a temporal attention layer that projects user embed-dings after a certain time since their last interaction. Conversely, TGN [24] presents a generic framework on continuous temporal networks, simulating"}, {"title": "2.2. Hard Negative Sampling", "content": "both RNNs [16, 30] and TGNNs [33] with its flexible memory mechanism and GNNs. Recent state-of-the-art continuous-time TGNNs [34, 37] have further explored the self-attention mechanism on temporal networks, inspired by the advancement of Transformer networks [2]. However, TGNNs typically em-ploy random and recent negative sampling [24, 33], with little attention given to the specific study of negative sampling strategies within this domain.\nNegative sampling strategies [4, 5, 12, 17, 32, 36, 38, 39] have been abun-dant in graph representation learning such as collaborative filtering and knowledge graphs. These strategies are traditionally categorized into two types: static and model-aware negative sampling."}, {"title": "3. Problem Statement", "content": "Static negative sampling strategies [20] select negative samples based on a pre-defined probability distribution. The simplest and most common strategy is random sampling, which uniformly selects negative samples from uninter-acted nodes. However, random sampling falls short in ensuring the quality of the samples. In response, alternative distributions have been proposed. For instance, NNCF [4] and NCEPLRec [32] adopt item-popularity-based sampling distributions to preferentially select popular items as negative sam-ples, thereby mitigating the widespread popularity bias in recommender sys-tems [3]. Despite these advancements, all these methods fail to adequately account for distribution variations across different networks, thus leading to unsatisfactory performance.\nTo tackle this challenge, model-aware negative sampling strategies lever-age models' specific information to oversample negatives that closely resem-ble positive samples, thereby tightening models' decision boundaries and en-hancing accuracy. DNS [36] pioneers this approach by dynamically choosing negative samples with the highest prediction scores. NSCaching [38] builds on this by keeping a cache of hard negatives, whereas SRNS [5] introduces a variance-based criterion to reduce the risk of false negatives. Moreover, MixGCF [12] synthesizes hard negatives by infusing positive information into negative samples. DENS [17] splits embeddings into relevant and irrelevant factors and selects negatives that only differ in relevant factors from the pos-itives. ANS [39] combines factor-aware and augmentation techniques from these strategies; it separates the hard and easy factors of negative items and synthesizes new negatives by augmenting the easy factors. While these afore-mentioned strategies have shown encouraging outcomes, their direct applica-tion to TGNNs is disappointing due to their oversight of temporal networks' unique characteristics, particularly positive shift and positive sparsity.\nTo the best of our knowledge, ENS [7] is the only negative sampling method specifically designed for temporal networks. ENS selects negative samples by aiming to meet a difficulty target composed of the proportion of historical neighbors and average temporal proximity. While ENS tries to tackle positive sparsity by increasing the difficulty target by a predefined amount each epoch, its heuristic approach fails to adapt to the varying needs of different models and datasets. Moreover, although it addresses positive shift by considering temporal proximity, this method requires evaluating all historical edges to ensure accuracy, thereby consuming substantial resources. Finally, ENS does not account for individual node preferences; it indiscrim-inately prioritizes all nodes that have historically appeared, without distin-"}, {"title": "4. Methodology", "content": "Temporal Network. This section formulates the problem of negative sam-pling in temporal networks. Let G(V, E,T, X) be a temporal network where V and E denote the set of nodes and edges, T denotes the timestamps of interactions, and X denotes the node features. We denote the set of inter-actions by O+ = {(u, v+, t)|u, v+ \u2208 V and t \u2208 T} where each tuple (u, v\u207a, t) records an interaction between two nodes u,v\u207a that occurs at time t. By convention, we call u source node and v+ destination node. These pairs form positive training samples. Conversely, we define the complement of O+ as O\u00af = {(u,v\u00af,t)|u,v\u00af \u2208 V, t \u2208 T and (u, v\u00af, t) \u00a2 O+}. This set contains all node pairs u, v\u00af that are not connected by an edge at time t and is used to generate negative samples. Our goal is to define a negative sampling strategy f that selects negative samples (u, v\u00af, t) from O\u00af to enhance the performance of existing TGNNs.\nHistorical Neighbor. u, v' are called neighbors at t if (u, v', t) \u2208 O+. If there exists (u, v\", t') \u2208 O+ such that t' < t, v\" is called a historical neighbor of u at t. In our context, we further restrict historical neighbors to nodes that are not neighbors of u at the current time, i.e., (u, v\",t) \u00a2 O+. For each (u,t) \u2208 (V,T), we maintain a set Hut = {(v1, t1), (V2, t2), ..., (Un, tn)} that records all unique historical neighbors of u at t and their respective interaction timestamps. In cases where vi interacts with a multiple times before t, we use the latest timestamp for ti. Historical neighbors in our method are then drawn from this set Hu,t\nOverall, CurNM is a curriculum learning strategy that progressively in-creases the difficulty of selected negatives based on performance on the val-idation set, effectively addressing the steep difficulty curves found in ex-isting methods. Within this framework, we further develop two compo-nents. Firstly, to tackle positive sparsity, we establish a negative pool con-sisting of historical neighbors and random negatives, with hard negatives included when the model exhibits sustained improvements. Secondly, we use"}, {"title": "4.1. Curriculum Learning", "content": "a temporal-aware disentangling technique to score the information of can-didate negatives in the negative pool, emphasizing those that differ from positives only in the most critical dimensions, thus capturing positive shifts and individual preferences. These informative negative samples are then fetched for training TGNNs to ensure their performance and robustness.\nThis framework is illustrated in Figure 1, and the detailed workflow is presented in Algorithm 1.\nTo address the learning difficulty caused by positive sparsity, the cur-riculum learning strategy adaptively increases the difficulty level of selected negatives based on the learning performance of TGNNs. Additionally, these"}, {"title": "4.2. Negative Pool", "content": "selected negatives are accompanied by annealing random negatives for robust training of TGNNs.\nAdaptive Selected Negatives. Firstly, we introduce a sampling proportion \u03c0\u03b5 that self-adjusts based on the model's performance at e-th epoch \u2013 shrinking when performance improves and expanding otherwise - thus dynamically controlling the difficulty level of selected negatives. By starting with a large \u03c0\u03b5 to include both easy and hard negatives and tightening it only if the model consistently demonstrates improvements, this approach prevents the model from being overwhelmed by overly challenging negatives. Furthermore, we utilize a shrinkage rate \u03b3shrink to control the adjustment steps of \u03c0e. A high \u03b3shrink establishes a steep difficulty curve for model learning, while a low \u03b3shrink sets a flatter curve. This strategy is written as\n$\\pi_{e+1} = \\begin{cases} \\pi_e - \\gamma_{shrink}, & \\text{If performance improves}, \\\\ \\pi_e + \\gamma_{shrink}, & \\text{Otherwise} \\end{cases}$\n(1)\nIn addition, instead of the traditional method of selecting a set of nega-tive samples for each positive sample, we merge and evaluate all candidate negatives at the minibatch level. As the learning difficulties vary across source nodes, this approach allows us to focus on more informative nodes that present greater difficulty while automatically eliminating nodes that are easier to learn as we shrinks. Technically, we select the top-scored nb - \u03c0\u03b5 \u00d7 6| \u00d7 M samples for each minibatch bat e-th epoch, where |b| denotes the number of source nodes in minibatch b, and M represents the size of each source's negative pool V.\n$O_{sel} = \\arg \\text{top}_{n_b} {\\hat{s}_{uv^-} | \\forall u \\in b, v^- \\in V_u},$ (2)\nwhere Osel denotes the set of selected negatives, and \u015duv\u2013 is a scoring function between node pairs for negative selection, as defined by Eq. 13.\nAnnealing Random Negatives. To provide the model with additional space for exploration and adaptation, especially in early epochs, each minibatch also includes \u0463 random negatives Orand in addition to Osel. A weight de is applied to balance these two types. Initially, we assign a higher weight to ran-dom negatives to enable the model to understand the basic patterns. As the model gains experience, the emphasis gradually shifts toward the selected, presumably more challenging negatives, enhancing the learning depth. Fol-lowing this intuition, we define \u03b4\u03b5 = max{\u03c0\u03b5, \u03b4min}. dmin is a hyper-parameter"}, {"title": "4.3. Negative Selection", "content": "The negative pool, which is an essential component of the curriculum learning strategy, is also constructed dynamically.\nTo ensure efficiency, we start the entire process by sampling a negative pool V of fixed size M for each positive sample (u, v\u207a,t). During the ini-tial epochs, we sample V as a mixture of historical neighbors and random negatives, a simple approach to prevent overcrowding with easy negatives. Random samples here serve a different purpose than before and are included to capture newly emerging preferences not disclosed by users' historical in-teractions. This process can be written as\n$V_u = V^{hist}_u \\cup V^{rand}_u$ (4)\nwhere V = M, and a proportion hyper-parameter \u03b3hist is used to balance between | Vhist | = \\hist * M and |Vrand| = (1 - hist) * \u041c.\nOnce the model learns the fundamentals, we raise the difficulty level. We introduce this advanced stage when sampling proportion \u03c0e falls below a preset threshold \u03c4 for the first time. Since \u03c0e decreases only if the model is learning well, this condition ensures that the model only encounters more challenging negatives after it has adequately learned the basics. The indicator C\u03c0\u03b5 is determined as follows:\n$C_{\\pi_e} = \\mathbb{1}(\\pi_e \\leq \\tau)$. (5)\nIn this advanced stage, we implement a hard negative cache Vhard of size to store a subset of V from the current epoch, focusing on likely false positives and those showing less learning in previous epochs. The cached negatives\n$\\mathcal{L} = \\mathcal{L}_{pos} + \\delta_e \\mathcal{L}_{O^-_{rand}} + (1 - \\delta_e) \\mathcal{L}_{O^-_{sel}} + \\lambda \\mathcal{L}_{con} + \\mu ||\\Theta||_2$, (3)\nwhere Lpos, Lrand, and Lsel denote the mean binary cross-entropy losses for positive samples, random negatives, and selected negatives, respectively. Lcon is the average contrastive loss as defined in Eq. 11, and ||||2 is a regu-larization term.\nThe second critical component of our framework is the selection function used to choose negative samples from the negative pool V. This process consists of two steps: disentangle irrelevant factors of negative nodes and score negative samples with time embeddings."}, {"title": "5. Experiment", "content": "Irrelevant Factor Disentanglement. To facilitate TGNNs to differentiate pos-itive from negative samples, we disentangle nodes' relevant and irrelevant factors, which is in line with DENS [17].\nFor each positive sample (u, v\u207a, t), we construct a positive gate. This gate leverages the embeddings of the source node hu and the destination node hv+ to isolate the relevant factor hr h+ of the destination node v\u207a, illustrated as\n$h_r^+ = h_{v^+} \\oplus \\sigma(W_{src}h_u + W_{dst}h_{v^+} + b_p), (8)\nwhere Wsrc, Wdst and bp are trainable parameters shared by all positive sam-ples, o is the sigmoid function, and is the element-wise product. This factor"}, {"title": "5.1. Datasets and Evaluation", "content": "are sampled proportionally according to the negatives' performance metrics, as outlined in Eq. 6.\n$V^{hard}_u \\sim \\{P_{uv} - \\alpha_e \\times \\text{std}(P_{uv})| \\forall v \\in V_u \\}, (6)\nwhere | Vhard| = 11, puv denotes the probability score of node pair (u, v) at the current epoch, Puv records the historical scores within the last five epochs, and std(\u00b7) is set to 0 when results are not available. ae is a hyper-parameter that controls the importance of prediction standard deviation at the e-th epoch. To manage potential instability in early learning stages, we initially assign a modest weight to the standard deviation and gradually increase it. Specifically, we use ae = @max \u00d7 min(, 1), where E' is the epoch after which the weight remains constant, and amax is the maximum weight that standard deviations could contribute to the performance metrics.\nThis hard negative cache then accounts for half of the negative pool in the next epoch, while the remainder continues to be a mix of historical neighbors and random samples, maintaining the previous proportion. Therefore, the new updating strategy for each negative pool V is written as\n$V_u = V^{hard}_u \\cup V^{hist}_u \\cup V^{rand}_u$,\nwhere Thist Thist and Vhist | + |Vrand | = M. (7)\nDataset. We evaluate the performance of our CurNM method on 12 stan-dardized datasets obtained from DyGLib [35] and TGL [40]. These datasets encompass a broad spectrum of domains, sizes, durations, and network fea-tures, and are intended to represent a diverse array of datasets. According to Table 1, which summarizes the statistics, social and interaction networks are typically sparse, while social and economic networks often exhibit high recency and repetition. These characteristics are expected due to the limited number of relationships individuals can realistically maintain and the sig-nificant impact of recent events and cyclic patterns on public attention and market dynamics. Other features, such as average time proximity, present a more diverse result across different domains. We divide these datasets into training, validation, and test sets using a chronological split with ratios of 70: 15 : 15 to prevent temporal data leakage during model training."}, {"title": "5.2. Experiment Setting", "content": "and the embedding h\u2082- of the corresponding candidate negative v\u00af \u2208 Vare then used as inputs to isolate the relevant factor h of the negative.\n$h_r^- = h_{v^-} \\oplus \\sigma(W_p h_r^+ + W_n h_{v^-} + b_n), (9)\nwhere Wp, Wn, and bn are trainable parameters shared by all candidate neg-atives in the negative pool. Factors other than relevant factors are denoted as irrelevant factors:\n$h_{ir}^+ = h_{v^+} \\ominus h_r^+, h_{ir}^- = h_{v^-} \\ominus h_r^-, (10)\nwhere hir and hir denote irrelevant factors of the destination node v\u207a and the candidate negative v\u00af, respectively, and \u2295 is element-wise subtraction. To supervise the training of positive and negative gates, we introduce a four-component contrastive loss Leon. These components dictate that rele-vant factors mainly drive interactions in positive samples and the absence of interactions in negative samples, while irrelevant factors do not and may even favor negative samples. This approach is consistent with the procedure used in DENS [17], written as\n$\\mathcal{L}_{con} = -(\\hat{s}_{r,r}^{+,+} + \\hat{s}_{ir,ir}^{-,-} - \\hat{s}_{r,ir}^{+,-} - \\hat{s}_{ir,r}^{-,+}), (11)\n$where \\hat{s}_{g_1, g_2}^{sgn1, sgn2} = \\text{MLP}(h_u, h_{g_1}^{sgn1}) - \\text{MLP}(h_u, h_{g_2}^{sgn2})is the scoring function between different factors, with MLP(\u00b7,\u00b7) computing the probability score from the given embeddings, and a minus sign is added to perform maximiza-tion.\nTemporal Scoring Function. As our goal is for the model to predict neighbors based on relevant factors, the most useful negatives for training are those that differ from the positives only in their relevant factors. In practice, this involves selecting negative samples from the negative pool V by maximizing the differences in relevant factors and minimizing the differences in irrelevant factors. This approach is written as\n$\\hat{s}_{uv^-} = \\beta_e \\times \\hat{S}_T^{+,-} - (1 - \\beta_e) \\times \\hat{S}_{ir,ir}^{+,-}, (12)\nwhere suv- denotes the scoring function and Be is a hyper-parameter that balances the contributions of relevant and irrelevant factors at epoch e. In-tuitively, we want the model to first learn which factors are important and then understand how these factors drive interactions. This entails initially"}, {"title": "5.4. Complexity Analysis", "content": "focusing on aligning irrelevant factors and later shifting to differentiating relevant factors. Specifically, we set Be = min(, 1), where E\" marks the epoch from which the score is solely determined by relevant factors.\nAdditionally, to capture the unique aspects of temporal networks, we incorporate three Time2Vec models to represent complex temporal patterns, including recurrent and periodic behaviors. Therefore, the final temporal scoring function used for selecting negative samples is written as\n$\\hat{s}_{uv^-} = t_{u,v^+} * (t_{u,v^-} - t_{v^-}) \\oplus \\hat{S}_{uv^-}, (13)\nwhere tu,v+, tu,v-, and tv- are Time2Vec embeddings for the timestamps of the positive sample, the interaction between u and the candidate negative v\u00af \u2208 V, and the negative's most recent occurrence, respectively. Default inputs of 0 are provided into Time2Vec for cases where the negative is not a historical neighbor or if it never occurs. By incorporating the product of these three embeddings, we seek to prioritize historical neighbors and recently popular nodes. The rationale is that these negatives possess attributes that were favored by the source nodes not long ago, and they might trick the model. So, by assigning more weight to these negatives \u2013 particularly those that differ from positives only in relevant factors we can enhance the precision of the model's decision boundary and reduce the occurrence of false negatives."}, {"title": "5.5. Ablation Study", "content": "Evaluation Metrics. We use average precision (AP) as our primary evaluation metric. Specifically, we compare AP for each dataset across all combinations of TGNNs and sampling methods, since typically only the optimal setup is used in practice. Additionally, given that we conduct experiments across 12 datasets, we evaluate the overall performance of each sampling method using average rank.\nThese two metrics are assessed across two scenarios: transductive and inductive negative evaluation. In the transductive setting, our test includes a mix of half-random and half-historical (TRANS) negative samples. Given that our method is specifically tailored to enhance performance against hard negatives, we anticipate improved predictive accuracy for historical neigh-bors [35]. However, it is crucial to preserve the model's capability to accu-rately predict random negatives. In this context, TRANS provides a robust benchmark for assessing our method's ability to balance these two objectives. Conversely, the inductive test involves only nodes not seen during training and thus provides a valuable indicator of our method's generalizability to new data. This evaluation draws on code from DyGLib [35].\nBaselines. We compare our method against two commonly adopted negative sampling strategies \u2013 random sampling and recent sampling [24, 33] \u2013 along with ENS [7], the only existing method known to us tailored for temporal networks. Random sampling selects negative samples based on a uniform dis-tribution. Recent sampling selects the latest historical neighbors as negatives, offering a straightforward approach to addressing issues of positive sparsity\nTheoretically, CurNM adds a negative mining process with a time com-plexity of O(|E| \u00d7 n \u00d7 d) to TGNNs, which originally have a complexity of O(L \u00d7 |E|\u00d7 n \u00d7 d). Here, L denotes the number of layers, |E| is the number of edges, and n represents the number of positive and negative samples per node, with nonlinear operations simplified to O(d).\nEmpirically, CurNM maintains high efficiency by limiting n with a rela-tively small negative pool, sized at M = 8. Table 3 presents a runtime com-parison between CurNM and baseline methods on the TGN model, which demonstrates the best overall performance among the three TGNNs eval-uated. According to Table 3, CurNM adds minimal computational load compared to basic random and recent sampling, consistently requiring about twice the time regardless of dataset size. Moreover, it is significantly more efficient than the state-of-the-art ENS on large datasets, requiring approxi-mately one-seventh of the time on Flights, the largest dataset tested.\nThe ablation study detailed in Figure 2 examines the effects of adaptive \u03c0\u03b5, annealing random negatives, hard negative cache, factor disentanglement, and temporal-aware embeddings in our method on the TGN model. Due to space constraints, we present our study using six datasets here. In both transductive and inductive settings, the complete CurNM model delivers the"}, {"title": "5.6. Hyper-Parameter Sensitivity", "content": "performance in predicting random samples.\nAt the dataset level, our method excels in relatively dense networks with frequent recurrence of recent historical interactions (low Average Time Prox-imity), such as USLegis, CanParl, and Flights, where it surpasses the best existing methods by around 5%. This demonstrates that our method, by uti-lizing temporal-aware disentangled factors for selection, can more accurately distinguish between enduring and transient relationships over time. In net-works with high repetition and recency effects \u2013 typically favorable for recent sampling - such as enron, UNtrade, and UNvote, our method still outper-forms recent sampling. Furthermore, it achieves an average improvement of nearly 5% over the best random sampling and nearly 3% over ENS, show-casing its effectiveness in capturing periodic and recurrent patterns. These results confirm our method's capability in addressing positive shift.\nIn very sparse networks, our method also outperforms random sampling across all datasets and, on average, surpasses ENS and recent sampling. Even where it does not lead, it remains competitively close to the top performer. The only exception is myket, whose sparsity, recency, and repetition make it akin to a static network, rendering ENS and CurNM \u2013 methods designed for temporal networks \u2013 less effective. Despite this, the overall performance indicates that our method successfully circumvents the steep gradient curve prevalent in hard negative sampling, effectively addressing positive sparsity even in less favorable scenarios.\nInductive Setting. The inductive setting allows us to compare our method's generalizability against existing methods by evaluating their performances on unseen nodes. When comparing inductive to transductive settings, the average ranks of ENS and recent sampling worsen, suggesting that their heuristic strategies might not be generalizable and could lead to overfitting on seen nodes. On the contrary, the average rank of random sampling improves, indicating its robustness despite relatively weaker performance. Finally, our CurNM method consistently achieves the best across 9 datasets and delivers a higher average rank in the inductive setting compared to the transductive setting. This confirms that our method not only performs well but is also robust and highly generalizable.\nAt the dataset level, our method consistently outperforms or matches the existing methods in almost all datasets, with insignificant declines in only two cases (again, with myket being an outlier).\nImpact of Strategies for ae, \u00dfe, and de. We investigate our assumptions about the structures of ae, \u00dfe, and de by testing an increasing strategy (e.g., ae =\nmin(, 1)), a decreasing strategy (e.g., ae = max(0,1 -)), and a constant strategy (e.g., ae = 0.5). Our findings in Figure 3 confirm that, as expected, our current approach yields the most robust results. This underscores the ef-fectiveness of our dynamic strategies, which progressively guide the model to focus on relevant factors of informative nodes, thereby achieving a smoother and more advantageous learning curve.\nImpact of Strategies for \u03c0e and shrink. Under the framework where \u03c0e should shrink as learning progresses, we explore two approaches: a linear strategy"}, {"title": "6. Conclusion", "content": "and an adaptive strategy. The linear strategy consistently reduces the we by a set amount each epoch, whereas the adaptive strategy only does so when there's an improvement in the selected performance metric. For our baseline, both methods start with \u03c0e = 100%, with a shrinkage rate \u03b3shrink = 3%, and the we can decrease to a minimum of 10%. The choice of \u03b3shrink = 3% is based on that models generally stabilize after approximately 30 epochs, and having a larger \u03c0\u03b5 before this phase to capture more information would be beneficial. The adaptive strategy outperforms the linear one. We then experiment with a slower (\u03b3shrink = 2%) and a faster shrinkage rate (\u03b3shrink = 5%). Figure 3 shows that \u03b3shrink = 3% yields the best results.\nIn this paper, we identify two unique challenges of negative sampling in temporal networks positive sparsity and positive shift. Our proposed CurNM method employs a dynamically updated negative pool and a cur-riculum learning strategy to adaptively adjust the difficulty level of negative samples during training, thus addressing the common challenge of positive sparsity. Furthermore, we implement a temporal-aware negative selection function to effectively capture positive shift. Our method's efficacy is demon-strated through extensive experiments against 3 algorithms and 12 datasets. Although CurNM demonstrates superior performance, it requires tuning sev-eral hyper-parameters to optimize its performance. In the future, we aim to"}]}