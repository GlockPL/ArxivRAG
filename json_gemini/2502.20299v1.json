{"title": "An Exploration of Features to Improve the Generalisability of Fake News Detection Models", "authors": ["Mr. Nathaniel Hoy", "Dr. Theodora Koulouri"], "abstract": "Fake news poses significant global risks by influencing elections and spreading misinformation, making its detection a critical area of research. Existing approaches, primarily using Natural Language Processing (NLP) and supervised Machine Learning, achieve strong results under cross-validation and hold-out testing but struggle to generalise to other datasets, even those within the same domain. This limitation stems from reliance on coarsely labelled training datasets, where articles are often labelled based on their publisher, introducing biases that token-based representations such as TF-IDF and BERT are sensitive to. While Large Language Models (LLMs) represent a promising development in NLP, their application to fake news detection remains limited. This study demonstrates that meaningful features can still be extracted from coarsely labelled datasets to improve model robustness for real-world scenarios. Stylistic features, including lexical, syntactic, and semantic attributes, are explored as an alternative due to their reduced sensitivity to dataset biases. In addition, novel \u2018social-monetisation' features are introduced, capturing economic incentives behind fake news, such as the presence of advertisements, external links, and social media sharing elements. The study employs the coarsely labelled NELA 2020-21 dataset for training and the manually labelled Facebook URLs dataset for external validation, representing a gold standard for evaluating model generalisability. The results highlight the limitations of token-based models, when trained on coarsely labelled data. Additionally, this study contributes to the limited evidence on the performance of LLMs such as LLaMa in this domain. The findings indicate that stylistic features, complemented by social-monetisation attributes, provide more generalisable predictions for real-world scenarios in comparison to token-based methods and LLMs. Statistical and permutation feature importance analyses further reveal the potential of these features to enhance performance and address dataset biases, offering a path forward for improving fake news detection models.", "sections": [{"title": "1. Introduction", "content": "Fake news has been a topic of interest since the term was popularised at the time of Trump's 2016 Presidential Election bid. It is typically characterised as content that appears to be news but is intentionally misleading for the purposes of generating profit through advertising or exerting political influence (Allcott and Gentzkow, 2017). In recent times it has maintained its relevance in public discourse, particularly with the rise of generative text models that are capable of generating large amounts of misinformation quickly and easily (Xu et al., 2023). Detecting fake news before dissemination is crucial to uphold information integrity and maintaining public trust in media and institutions. In democratic societies, fake news can manipulate public opinion, sway elections, and undermine governance (Morgan, 2018). Moreover, false information about health crises, emergencies, or scientific discoveries can endanger public safety (Nelson et al., 2020). Socially, it deepens divisions, fuels polarisation, and exacerbates societal tensions (Olan et al., 2024). Therefore, effective detection and mitigation of fake news not only protects individuals from harm but also upholds the essential principles of truth, transparency, and responsible communication necessary for a well-functioning society.\nResearchers in the field of Computer Science have aimed to address this problem, often using Natural Language Processing (NLP) and Machine Learning (ML) classification techniques. While such approaches report good results under cross-validation and holdout test conditions, evidence suggests that current approaches struggle to generalise, particularly when relying on token-representation methods such as Bag-of-Words (BoW), Term Frequency - Inverse Document Frequency (TF-IDF) and Bidirectional Encoder Repre-"}, {"title": "2. Related Work", "content": "The challenge of detecting fake news has prompted extensive research across various domains, leading to the development of numerous models and approaches. This section reviews key studies that have explored token-level representations, stylistic features, and multimodal approaches to enhance the accuracy and generalisability of fake news detection models\n2.1. Overview\nCurrent approaches to fake news detection largely focus on using a variety of supervised ML algorithms and features. Typically, such approaches are tested using either holdout testing or cross-validation, using an unseen portion of the datasets on which they were trained. Overall, current approaches achieve accuracies of 80% on average, with many achieving significantly higher results (Hoy and Koulouri, 2021). The features that are used in such approaches can broadly be categorised as either: (i) content-based, which are features derived from things such as article text, title, publisher and images; (ii) socially-based, which are features derived from social networks data (typically from X/Twitter), such as the relationships between users who share\nOf the sub-categories of content-based features, textual features are the most prominently used. These can be broadly divided into the following sub-categories:\nToken-Level Representations: These convert words into numerical vectors using methods like Bag of Words (BoW), TF-IDF, and word embeddings such as Word2Vec and BERT (Thota et al., 2018). BoW and TF-IDF represent text data as vectors based on word frequency and importance, respectively, while word embeddings like Word2Vec and BERT capture semantic relationships and contextual information between words. Studies such as Kaur et al. (2020) and (Poddar et al., 2019) have shown high accuracies using these methods with various machine learning models, demonstrating their effectiveness in the field of fake news detection under hold-out testing conditions\nStylistic Features: These include statistical features (e.g., average word length, sentence complexity, part-of-speech tags) and psycholinguistic features (e.g., sentiment analysis scores) generated by analysing the text corpus. Tools like Linguistic Inquiry Word Count (LIWC) produce such features, encompassing linguistic aspects and psycholinguistic processes. These features provide insights into the author's writing style and emotional tone, enhancing the analysis of text data (Gravanis et al., 2019; Spezzano et al., 2021). Fernandez and Devaraj (2019) demonstrated strong performance using stylistic features, achieving up to 94.2% accuracy in their experiments, highlighting the importance of stylistic analysis in improving model performance\nStudies may also explore the combination of token-level representations and stylistic features to leverage the strengths of both approaches. Research combining these features have demonstrated strong performance in fake news detection. For instance, Ngada and Haskins (2020) demonstrated over 95% accuracy on the Kaggle \u2018fake and real news' dataset across six different classification algorithms by integrating token-level and stylistic features. Similarly, Verma et al. (2021) showed that such combined features can generalise well across four datasets. However, these generalisation results stand in significant contrast to the wider literature (Hoy and Koulouri, 2022; Gautam and Jerripothula, 2020; Blackledge and Atapour-Abarghouei, 2021; Janicka et al., 2019; Alnabhan and Branco, 2024; Liu et al., 2024), suggesting a likely overlap between the datasets used, particularly in experiments leveraging a Kaggle dataset, where data collection methodologies are not often described and combining datasets for training and testing is a common practice. Such overlap may artificially inflate performance metrics and does not reflect the challenges of generalising to entirely unseen data. These findings underscore the need for careful dataset selection and validation to ensure meaningful evaluations of generalisability.\nBeyond traditional token-based and stylistic methods, Large Language Models (LLMs) such as GPT and LLaMA represent a distinct advancement in natural language processing. Unlike embedding-based approaches like BERT, which focus on generating contextual embeddings for token-level representations, LLMs leverage extensive pretraining on vast datasets to perform text classification tasks holistically. These models excel in zero-shot and few-shot learning scenarios, demonstrating strong performance across tasks such as sentiment analysis and summarisation without requiring extensive fine-tuning (Kojima et al., 2022).\nPreliminary evidence suggests, however, that LLMs struggle to achieve similar success in the fake news detection domain. Unlike their strong performance on tasks such as summarisation or sentiment analysis, their results in fake news detection have been consistently poor. Recent studies, including Pavlyshenko (2023), highlight significant differences in performance between LLMs and specialised NLP classifiers that leverage token-based features, with the latter consistently outperforming LLMs in this domain.\nMultimodal Appraoaches using Textual Features\nMultimodal approaches leverage various types of data, including textual features, to enhance the performance of fake news detection models. By"}, {"title": "2.4. Current Issues in Fake News Detection", "content": "Despite positive outcomes in many studies, issues remain with datasets used for training fake news detection models, particularly those relying on token representations. Dataset size is a critical issue, as collecting and accurately labeling a large number of news articles is challenging (D'Ulizia et al., 2021). To manage this, articles are often labeled based on their publisher as a proxy for accuracy, which can introduce topical biases (Torabi Asr and Taboada, 2019). This can lead to models that perform well in hold-out test conditions but struggle to generalise outside of the training dataset (Suprem et al., 2022)\nLimited studies have explored the generalisability of these models. Gautam and Jerripothula (2020) observed a 39% accuracy drop when models trained on political news were tested on celebrity news. Similarly, Castelo et al. (2019) found comparable accuracy drops across different news domains using small datasets of fewer than 500 articles. Multimodal approaches, like those examined by Liu et al. (2024), combining text and image features, showed some cross-domain generalisation but significant accuracy drops to around 55% under cross-dataset conditions. Notably, embeddings from more recent large language models such as GPT-4 also suffer from an inability to generalise when trained on these coarsely labelled datasets, as demonstrated by Alnabhan and Branco (2024)"}, {"title": "2.5. Proposed Features", "content": "Given this evidence, this study seeks to propose a set of four novel features with the goal of producing more generalisable fake news detection models. These features are outlined as follows:\nFrequency of Ads: One of the primary motivations behind the creation and dissemination of fake news is financial gain through advertising. According to Allcott and Gentzkow (2017), fake news websites often rely on sensationalist and misleading content to attract high volumes of traffic, which in turn increases their advertising revenue. These sites typically feature a large number of advertisements, as their business model is heavily reliant on generating ad impressions and clicks. Therefore, the number of adverts associated with a given article could be a significant indicator of fake news. Articles that contain an unusually high number of ads may be designed to maximise revenue rather than to provide factual information, making this a critical feature to include in fake news detection models\nExternal Links: Similar to advertising, the prevalence of external links in an article can also be an indicator of fake news, especially when these links are intended for affiliate marketing purposes. Fake news articles often include numerous external links that direct readers to other sites, which can generate affiliate income for the publisher each\nSocial Media Share Links: The role of social media in the spread of fake news is well-established, with platforms like Facebook and X/Twitter being primary channels for misinformation dissemination. One of the mechanisms that facilitate this spread is the use of visual cues, such as share buttons, which prompt habitual behaviour in social media users (Ceylan et al., 2023). When users encounter these visual cues, they are more likely to share the content without critically evaluating its veracity. Including 'call to action' links that lead to social media platforms in the analysis is essential, as these links can significantly amplify the reach of fake news articles. By encouraging readers to share content on social media, these articles can quickly go viral, spreading misinformation at an unprecedented rate. Therefore, factoring in Facebook and X/Twitter links is expected to be important in identifying articles that are designed to exploit social media behaviour for rapid dissemination. It is important to note these social media features are distinct from others seen in the literature, which typically focus on user profiles and relationships between tweets and users\nThis novel group of features shall be characterised as \u2018social-monetisation' features"}, {"title": "3. Research Questions", "content": "The analysis in Section 2 provides the motivation for a focused investigation into the use of generalisable stylistic features as well as the novel social-monetisation features that may lend themselves to improved generalisability (the ability of a model to perform well when tested on a different dataset than the one on which it was trained) of fake news detection models. As such, the objectives of this study can be formalised into the following three research questions:\nHow well do fake news detection methods using token-representations/LLMs generalise?\nDo fake news detection methods using stylistic features generalise better than fake news detection models using token-representations/LLMs?\nDo fake news detection methods using stylistic features and the proposed social-monetisation features generalise better than models using stylistic features only?"}, {"title": "4. Methodology", "content": "This section details the methodology used to address the research questions outlined in Section 3. The study conducted two experiments (summarised in Figure 1) to achieve this.\nThe first experiment aimed to evaluate the generalisability of commonly used token-representations, including Bag of Words (BoW), TF-IDF, Word2Vec and BERT, as well as the Large Language Model \u2018LLaMA', addressing RQ1. Models using token-representations were trained on the NELA 2020-21 dataset and their performance evaluated using K-fold cross-validation. For LLAMA, the zero-shot configuration relied on its pre-trained knowledge and structured prompts, while the few-shot configuration incorporated labelled examples from the NELA dataset to guide classification. The fine-tuned LLaMA model followed a methodology similar to the token-based approaches, enabling a direct comparison of generalisability across techniques. Models trained/fine-tuned on the NELA dataset were then evaluated using external validation on"}, {"title": "4.1. Data Collection & Processing", "content": "This section outlines the datasets and data extraction methods used. Owing to the nature of the proposed social-monetisation features, the dataset required the source URL of the articles to facilitate the extraction of these features. Capuano et al. (2023)'s systematic review lists several datasets used in content-based fake news detection. However, out of the 19 datasets reviewed, only three FakeNewsNet, Buzzfeed, and Celebrity fake news\u2014include the article's source URL. These datasets are relatively small, which limits the likelihood of producing a generalisable model. To develop a more comprehensive and reliable model, a larger dataset is necessary. Therefore, the NELA series of datasets was chosen for its large size and inclusion of article URLs, providing a more extensive and diverse data source for training. Using a dataset of this size also ensures that a significant number of articles can be extracted to compensate for pages that are no longer available. While not as frequently used in the literature, a number of studies make use of this\ndataset including Horne et al. (2020); Raj et al. (2023) and Raza and Ding (2022)\nThe latest iterations of this dataset released in March 2023, NELA 2020 and 2021, were chosen for this study. Each dataset contains over a million articles from various sources and are coarsely labelled, with each article's legitimacy derived from its source's aggregated label from seven assessment sites: Media Bias Fact Check, Pew Research Center, Wikipedia, OpenSources, All-Sides, Buzzfeed News, and Politifact. The labels are categorised as unreliable, mixed, and reliable. For this study, only\u2018unreliable\u2019and\u2018reliable' labels were used, excluding the \u2018mixed' label to align with the binary labels\nin the external validation dataset\nThe combined NELA 2020-21 dataset includes 3,635,636 records from\n525 unique sources. After joining the labels file and excluding the\u2018mixed\u2019 category, the dataset consists of 1,013,808 'true' and 551,051 \u2018fake\u2019articles from 224 sources. To prevent any single source from dominating the training set 2, the number of URLs extracted from each source was reduced using the 1st quartile as a threshold (285 articles per source), resulting in a final set of 22,230 'true' and 25,650 \u2018fake' articles from 168 sources (Figure 3).\nThe Facebook URLs Dataset was chosen as an external validation dataset owing to its unique position as a dataset collected in a \u2018real-world' context and granular labelling by a third-party fact-checking organisation. Its individual article labels provide a robust standard for assessing model accuracy and practical applicability in fake news detection. This stands in contrast to commonly used datasets in the field, which often employ coarse labels based on article publishers, potentially misrepresenting the true nature of fake news. By using a coarsely labelled dataset for training and a manually labelled dataset for testing, the aim is to demonstrate that despite the limitations of coarsely labelled datasets, meaningful features can still be extracted to develop robust models applicable in real-world scenarios\nThe Facebook URLs dataset contains over 38 million URLs shared on Facebook since January 1, 2017, with 35,924 records identified as fake news. The dataset is protected with differential privacy, ensuring no information can be gathered regarding individuals (Messing et al., 2020). Given its restricted accessibility and limited usage in prior studies, this research represents one of the few to utilise the Facebook URLs Dataset for fake news classification, following a study by Barnab\u00f2 et al. (2022). The dataset initially comprised 28,271 fake and 7,653 true records, with non-English articles filtered out based on \u2018US' and 'UK' values in the \u2018Public Shares Top Country' field, resulting in 14,354 fake and 1,468 true records. To enhance dataset quality, URLs referring to Tweets and videos were excluded. Class balancing was implemented during experimentation. Due to its size, the Facebook URLs Dataset served as a test set for cross-dataset testing, complementing the larger training datasets to bolster the model's generalisability and validate its performance in diverse real-world scenarios\nIn order to extract the raw textual data from the URLs in these datasets, the BeautifulSoup library was used. As many webpages in these datasets may no longer be available, particularly in relation to 'fake' news pages, initial extraction was attempted through the use of the Wayback Machine API (Internet Archive). This was done to increase the likelihood of extracting a webpage with a complete article and not a splash page indicating the article"}, {"title": "4.2. Experiment 1 Features: Token-Representations & LLMs", "content": "This section outlines the features to be used in the first experiment, addressing RQ1. This experiment aims to address RQ1, by exploring how well models using token-representations generalise between two different datasets of the same topic, using the NELA and Facebook datasets. An overview of the procedure followed in this experiment is provided in Figure 4. In this section, each of the token-representations used in this experiment and the libraries used in extracting these features from the datasets are outlined. The results of this experiment are presented in Section 5.1.\nThe token-representations chosen for this study are motivated by the systematic review by Capuano et al. (2023), which identified the following token-representations among the most popular in the literature for content-based fake news detection:\nBag-of-Words (BoW) converts text into fixed-length vectors based on word frequencies. It does not take into account word-order. In this study, BoW was implemented with SKLearn's CountVectorizer with a max_features parameter of 10,000 words.\nTF-IDF enhances BoW by considering word frequency relative to the dataset, capturing token significance. This technique, therefore, better captures the significance of tokens compared to BoW. However, like BoW of words it fails to account for word-order. TF-IDF is among the most popular feature extraction approaches, as identified by Capuano et al. (2023), owing to its simplicity and focus on informative tokens. In this study, TF-IDF was implemented with SKLearn's TFIDFVectorizer and max_features set to 10,000 words\nWord2Vec generates word embeddings by training a neural network to predict target words from their context, capturing semantic meaning but ignoring word order (similar to the previous two techniques). It is one of the most popular feature extraction methods, identified in approximately 25% of studies in an ongoing systematic review by the"}, {"title": "4.2.2. LLMs", "content": "Building on the foundation of traditional token-based features, this study also incorporates the large language model LLaMa 3.2-1B, a transformer-based model with 1 billion parameters. Chosen for its robust natural language understanding and computational efficiency, the model provides a strong baseline for evaluating advanced detection techniques.\nLLaMa was utilised in three configurations: zero-shot, few-shot, and fine-tuning. In the zero-shot configuration, the model leveraged its pre-trained knowledge without task-specific training, providing an initial benchmark for its capabilities. Few-shot learning introduced a small number of labeled examples to guide predictions, while fine-tuning adapted the model comprehensively by training it on labeled datasets.\nTo operationalise LLaMa in zero-shot and few-shot learning, structured prompts were designed to align with the task requirements. The system"}, {"title": "4.3. Experiment 2 Features: Stylistic & Proposed Social-Monetisation Features", "content": "The following section outlines the stylistic features that were used in addressing RQ2 Experiment 2 follows a similar structure to Experiment 1 and evaluates the generalisability of five groups of stylistic features proposed by\nprevious research and compares it against the results of Experiment 1, which explored the generalisability of token-representations. Then, it addresses RQ3 by exploring whether the four social-monetisation features proposed by this study improve generalisability. An overview of the procedure followed in Experiment 2 is shown in Figure 5. The stylistic features and social monetisation features used in the experiment are presented in Sections 4.3.1 and 4.3.2, respectively. The results of this experiment are described in Section 5.2.\nFive groups of stylistic features, proposed in the literature, are evaluated in this study. Each feature group varies in complexity, with the first group focusing solely on linguistics, while subsequent feature-sets progressively incorporate additional groups such as psycholinguistics and document complexity. Due to the diverse scales of many features within these groups, we applied StandardScaler to ensure uniform treatment by machine learning algorithms. The selection of these five groups was motivated by their inclusion in the limited number of studies exploring generalisability of fake news detection models (with the exception of the NELA feature-set which was chosen owing to the use of the NELA dataset in this study). However, these studies only used coarsely labelled datasets for external validation. As such, we aimed to observe their performance using real-world data from the Facebook URLs dataset, underscoring the relevance of these features in practical applications of fake news detection. The complete table detailing these features is provided in the Appendix"}, {"title": "4.3.2. Proposed \u2018Social-Monetisation' Features", "content": "As motivated in Section 2.5, a number of additional novel features, categorised as 'social-monetisation' features, were investigated to complement the groups of stylistic features selected for this study. These were: (i) the number of advertisements; (ii) the number of external links; (iii) the number of links to Facebook; and (iv) number of links to Twitter/X. The number of ads was extracted through the use of EasyList, an open-source project that compiles a list of the most popular adblocking filters. Using this list enables searching the webpage's LXML tree and counting the frequency of"}, {"title": "4.4. Machine Learning Algorithms", "content": "As this study prioritises the exploration of stylistic features for generalisable fake news detection, less emphasis has been put on exploring the effect of different machine learning algorithms and their respective hyperparameters. However, for completeness and to offer an opportunity for comparison to the literature, a number of machine learning algorithms including Logistic Regression, SVM, Gradient Boosting, Decision Trees, Random Forest, and a feed-forward neural network (FFNN) are employed. Each of these algorithms was implemented using default hyperparameters in SKLearn. The exception to this was the neural network where default hyperparameters are not available and, as such, a shallow Sequential model was used with a single hidden layer of 10 neurons, a sigmoid activation layer compiled with binary cross-entropy loss and the Adam optimiser. To protect against overfitting, the EarlyStopping hyperparameter was set to stop training if the loss function did not improve by 0.01.\nAdditionally, a Long Short-Term Memory (LSTM) network was employed to process word embeddings generated by BERT and Word2Vec. LSTMs are well-suited for capturing sequential dependencies in embeddings, providing a deeper understanding of contextual relationships within the text. This approach was not applied to Bag-of-Words or TF-IDF features, as these methods represent text as sparse matrices, lacking sequential and contextual information, rendering them unsuitable for use with LSTMs. This distinction highlights the effort to align algorithms with feature sets that best leverage their strengths for fake news detection. The LSTM architecture used in this study consisted of an LSTM layer with 128 hidden units, designed to model complex temporal dependencies and contextual patterns in the data. A dropout rate of 40% was applied to the LSTM's output to reduce the risk of overfitting. Finally, a fully connected layer was used to map the final hidden state to two output classes: fake and real news."}, {"title": "4.5. Training/Testing Methodology", "content": "The training and testing methodology in this study integrates K-fold cross-validation, external validation, and tailored evaluation techniques for large language models (LLMs) to assess performance and generalisability in fake news detection. For traditional machine learning algorithms and feature sets, K-fold cross-validation is employed on the NELA dataset, partitioning it into 10 equal folds. Models are trained on 9 folds and validated on the remaining fold across iterations, ensuring robust performance estimates with a fixed random state (set to 42). External validation evaluates the generalisability of models by testing those trained on each NELA fold against 500 randomly sampled articles per class from the Facebook dataset, addressing class imbalance. Key evaluation metrics such as Accuracy, Precision, Recall, Specificity and F1-Score are used to assess model performance in distinguishing between true news' and fake news'. By evaluating the models using external validation after each fold.\nFor LLaMa zero-shot and few-shot models, a distinct methodology was adopted due to its pre-trained nature. In the zero-shot configuration, the model leveraged its existing knowledge without any task-specific training, relying entirely on structured prompts to classify text from both the entirety of the NELA dataset and 5 random balanced samples of 500 Facebook URLs dataset. In the few-shot configuration, the model was guided by labelled examples from the NELA dataset only to reflect the goals of this study to train on a coarsely labelled dataset but test on a manually labelled dataset. The fine-tuned LLaMa model followed a similar methodology to the above traditional methods.\nIn addition to the Mann-Whitney U-test, Permutation Feature Importance (PFI) was used to pinpoint the stylistic and proposed social monetisation features that positively contributed to a more generalisable model. PFI works by shuffling a random feature, thereby disrupting the relationship between that feature and the target variable. By repeating this process for all features in the dataset and observing the effects on model performance, the method reveals how much the model depends on each feature"}, {"title": "5. Results", "content": "This section outlines the results of the two experiments outlined in Section 4. The overarching objective of the experiments is to demonstrate whether models using different sets of stylistic and the proposed social-monetisation"}, {"title": "5.1. Experiment 1: Generalisability of Token-Representations & LLMs", "content": "Experiment 1 aimed to address RQ1 by examining the generalisation capabilities of token representations (BoW, TF-IDF, Word2Vec, and BERT, as detailed in Section 4.2.1) combined with different machine learning models, as well as evaluating the performance of the large language model LLaMa under zero-shot, few-shot, and fine-tuning configurations.\nAs seen in Table 2, under K-fold test conditions, token-representations trained and tested on the NELA dataset exhibit high performance across\nseveral models, achieving a mean accuracy of 0.93, with a range between 0.79 and 1.0. These results align with those commonly reported in the literature and validate the effectiveness of the feature extraction and modeling methods when applied in controlled conditions.\nAmong the tested approaches, Fine-Tuned BERT and LSTMs demonstrated the highest performance, achieving near-perfect metrics. Fine-Tuned BERT reached 1.0 accuracy, precision, recall, specificity, and F1 score, while LSTMs trained on BERT embeddings achieved 0.99 across all metrics. However, such results may reflect the model's ability to memorise patterns specific to the training data rather than a genuine ability to generalise beyond the dataset.\nTraditional feature extraction techniques such as BoW and TF-IDF also achieved strong results, with accuracies up to 0.99. Their reliance on high-frequency or significant terms may have contributed to their robust performance within the dataset. This aligns with our observations in Section 2.4 regarding biases within commonly used fake news datasets, where certain terms may strongly correlate with specific classes. Specifically, Logistic Regression outperformed SVM when using the BoW representation, likely due to its ability to leverage the sparse, linearly separable nature of BoW features. In contrast, the SVM with an RBF kernel may not have been optimally suited for this representation, as the kernel is designed for capturing non-linear relationships, which may not align with BoW's characteristics (Colas et al., 2007). This mismatch could partly explain the relatively poorer performance of SVM in this context.\nIn contrast to BoW and TF-IDF, Word2Vec and standard BERT-base embeddings performed slightly worse, with accuracies ranging from 0.81 to 0.95. These models may have struggled with the added noise in the textual data or the more complex representations introduced by their embeddings. In the case of Word2Vec, despite using a relevant model pre-trained on Google News, the performance may have been hindered by out-of-vocabulary OOV) words. Unlike BoW and TF-IDF, which construct their vocabularies directly from the dataset and therefore capture all words within that domain, Word2Vec relies on a fixed vocabulary from its pre-training corpus. Fake news datasets often contain domain-specific terms, slang, or creative language use that may not be present in the pre-trained Word2Vec vocabulary. As a result, OOV words are either ignored or mapped to suboptimal representations, leading to a potential loss of crucial information. This limitation reduces the model's ability to capture dataset-specific keywords"}, {"title": "5.2. LLaMa Results", "content": "This section evaluates the performance of LLaMa 3.2-1B in detecting fake news, focusing on its zero-shot, few-shot, and fine-tuned configurations. Unlike traditional algorithms relying on static token representations, LLaMa leverages its pre-trained, context-aware transformer architecture to adapt dynamically to the task.\nThe results presented in Table 4 reveal the performance of LLaMa 3.2-1B under zero-shot, few-shot, and fine-tuned configurations on the NELA dataset.\nIn the zero-shot configuration, LLaMa achieved an accuracy of only 0.53, with precision at 0.55 and a low recall of 0.39. The specificity was 0.68, and the F1-score stood at 0.46. These metrics are barely above random chance, indicating that the model struggled to effectively detect fake news without any task-specific training. Despite large language models often performing well in zero-shot settings for other tasks, LLaMa's performance here suggests that its pre-trained knowledge does not generalise well to the nuances of fake news detection.\nThe few-shot configuration showed a modest improvement, with accuracy increasing to 0.6, precision to 0.65, and recall to 0.42. Specificity improved to 0.77, and the F1-score to 0.51. While these results are slightly better than the zero-shot configuration, they remain unsatisfactory for practical applications. The limited enhancement implies that providing a small number of labeled examples was insufficient for the model to grasp the complex patterns associated with fake news, which often involve subtle linguistic cues and context-dependent nuances.\nIn stark contrast, the fine-tuned configuration achieved perfect scores across all metrics, with accuracy, precision, recall, specificity, and F1-score all at 1.00. While this suggests that the model can perform exceptionally well when extensively trained on the task-specific data, such flawless performance is unusual and may indicate overfitting to the NELA dataset. Overfitting\nreduces the model's ability to generalise to new, unseen data, limiting its practical utility in real-world scenarios where fake news can vary widely in form and content.\nThese findings underscore a critical limitation of large language models like LLaMa in the context of fake news detection. Despite their strong performance in zero-shot and few-shot settings on more general language tasks, these models do not perform well on the task of fake news detection without substantial task-specific training.\nThis issue is demonstrated further in regards to the Facebook URLS dataset, where Table 5 highlights the cross-dataset performance of LLaMa in these three configurations. These findings provide insights into the model's ability to generalise when tested on unseen, manually labelled data.\nIn the zero-shot configuration, LLaMa achieves an accuracy of 0.5, with precision and recall at 0.5 and 0.4, respectively. The specificity is moderately better at 0.61, and the F1-score stands at 0.44. These results indicate that the model performs at near-random levels when applied to the external dataset without any task-specific training. While large language models are often effective in zero-shot scenarios for general tasks, LLaMa's performance here underscores the difficulty of adapting pre-trained knowledge to the domain-specific challenges of fake news detection, particularly when confronted with nuanced and diverse real-world data.\nThe few-shot configuration using examples from the NELA dataset shows a marginal improvement over zero-shot performance. Accuracy increases to 0.55, precision rises to 0.57, and specificity improves to 0.72. However, recall remains low at 0.37, and the F1-score only improves slightly to 0.45. These results suggest that while a small number of labeled examples from the training dataset provided some task-specific guidance, they were insufficient for LLaMa to effectively generalise to the Facebook dataset. This limited improvement highlights the challenges of adapting models trained on coarsely labelled datasets, such as NELA, to manually curated datasets with more nuanced distinctions."}, {"title": "5.3. Experiment 2: Generalisability of Stylistic & Social Monetisation Features", "content": "The second experiment targeted RQ2 and RQ3 and aimed to determine whether the stylistic features suggested in the literature and the social-monetisation features introduced in this study are more generalisable than the token-level representations tested in Section 5.1. As detailed in Section 4.3.1", "evaluated": "Fernandez; Abonizio; LIWC; NELA; and modified NELA. Each of these groups was tested with and without the proposed social monetisation features identified in Section 4.3.2. A K-fold test was first performed with the same splits as in the first experiment, using the NELA dataset to provide a baseline for comparison and the Facebook dataset to perform a cross-dataset test for each model trained in each fold.\nAs can be seen from Table 6, in K-fold cross-validation test conditions the selected stylistic features performed comparably to the token-representations (see Table 2). Across the different groups of stylistic features and machine learning algorithms, the mean accuracy was 90% with a range between 78% and 98%. From this test, it can be seen that that Logistic Regression models using the Fernandez and Abonizio feature-sets excluding the proposed social monet"}]}