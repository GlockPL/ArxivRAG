{"title": "An Exploration of Features to Improve the Generalisability of Fake News Detection Models", "authors": ["Nathaniel Hoy", "Theodora Koulouri"], "abstract": "Fake news poses significant global risks by influencing elections and spreading misinformation, making its detection a critical area of research. Existing approaches, primarily using Natural Language Processing (NLP) and supervised Machine Learning, achieve strong results under cross-validation and hold-out testing but struggle to generalise to other datasets, even those within the same domain. This limitation stems from reliance on coarsely labelled training datasets, where articles are often labelled based on their publisher, introducing biases that token-based representations such as TF-IDF and BERT are sensitive to. While Large Language Models (LLMs) represent a promising development in NLP, their application to fake news detection remains limited. This study demonstrates that meaningful features can still be extracted from coarsely labelled datasets to improve model robustness for real-world scenarios. Stylistic features, including lexical, syntactic, and semantic attributes, are explored as an alternative due to their reduced sensitivity to dataset biases. In addition, novel \u2018social-monetisation' features are introduced, capturing economic incentives behind fake news, such as the presence of advertisements, external links, and social media sharing elements. The study employs the coarsely labelled NELA 2020-21 dataset for training and the manually labelled Facebook URLs dataset for external validation, representing a gold standard for evaluating model generalisability. The results highlight the limitations of token-based models, when trained on coarsely labelled data. Additionally, this study contributes to the limited evidence on the performance of LLMs such as LLaMa in this domain. The findings indicate that stylistic features, complemented by social-monetisation attributes, provide more generalisable predictions for real-world scenarios in comparison to token-based methods and LLMs. Statistical and permutation feature importance analyses further reveal the potential of these features to enhance performance and address dataset biases, offering a path forward for improving fake news detection models.", "sections": [{"title": "1. Introduction", "content": "Fake news has been a topic of interest since the term was popularised at the time of Trump's 2016 Presidential Election bid. It is typically characterised as content that appears to be news but is intentionally misleading for the purposes of generating profit through advertising or exerting political influence (Allcott and Gentzkow, 2017). In recent times it has maintained its relevance in public discourse, particularly with the rise of generative text models that are capable of generating large amounts of misinformation quickly and easily (Xu et al., 2023). Detecting fake news before dissemination is crucial to uphold information integrity and maintaining public trust in media and institutions. In democratic societies, fake news can manipulate public opinion, sway elections, and undermine governance (Morgan, 2018). Moreover, false information about health crises, emergencies, or scientific discoveries can endanger public safety (Nelson et al., 2020). Socially, it deepens divisions, fuels polarisation, and exacerbates societal tensions (Olan et al., 2024). Therefore, effective detection and mitigation of fake news not only protects individuals from harm but also upholds the essential principles of truth, transparency, and responsible communication necessary for a well-functioning society.\nResearchers in the field of Computer Science have aimed to address this problem, often using Natural Language Processing (NLP) and Machine Learning (ML) classification techniques. While such approaches report good results under cross-validation and holdout test conditions, evidence suggests that current approaches struggle to generalise, particularly when relying on token-representation methods such as Bag-of-Words (BoW), Term Frequency Inverse Document Frequency (TF-IDF) and Bidirectional Encoder Repre-"}, {"title": "2. Related Work", "content": "The challenge of detecting fake news has prompted extensive research across various domains, leading to the development of numerous models and approaches. This section reviews key studies that have explored token-level representations, stylistic features, and multimodal approaches to enhance the accuracy and generalisability of fake news detection models\n2.1. Overview\nCurrent approaches to fake news detection largely focus on using a variety of supervised ML algorithms and features. Typically, such approaches are tested using either holdout testing or cross-validation, using an unseen portion of the datasets on which they were trained. Overall, current approaches achieve accuracies of 80% on average, with many achieving significantly higher results (Hoy and Koulouri, 2021). The features that are used in such approaches can broadly be categorised as either: (i) content-based, which are features derived from things such as article text, title, publisher and images; (ii) socially-based, which are features derived from social networks data (typically from X/Twitter), such as the relationships between users who share\n2.2. Content-Based Textual Features\nOf the sub-categories of content-based features, textual features are the most prominently used. These can be broadly divided into the following sub-categories:\n\u2022 Token-Level Representations: These convert words into numerical vectors using methods like Bag of Words (BoW), TF-IDF, and word embeddings such as Word2Vec and BERT (Thota et al., 2018). BoW and TF-IDF represent text data as vectors based on word frequency and importance, respectively, while word embeddings like Word2Vec and BERT capture semantic relationships and contextual information between words. Studies such as Kaur et al. (2020) and (Poddar et al., 2019) have shown high accuracies using these methods with various machine learning models, demonstrating their effectiveness in the field of fake news detection under hold-out testing conditions\n\u2022 Stylistic Features: These include statistical features (e.g., average word length, sentence complexity, part-of-speech tags) and psycholinguistic features (e.g., sentiment analysis scores) generated by analysing the text corpus. Tools like Linguistic Inquiry Word Count (LIWC) produce such features, encompassing linguistic aspects and psycholinguistic processes. These features provide insights into the author's writing style and emotional tone, enhancing the analysis of text data (Gravanis et al., 2019; Spezzano et al., 2021). Fernandez and Devaraj (2019) demonstrated strong performance using stylistic features, achieving up to 94.2% accuracy in their experiments, highlighting the importance of stylistic analysis in improving model performance\n2.3. Multimodal Appraoaches using Textual Features\nMultimodal approaches leverage various types of data, including textual features, to enhance the performance of fake news detection models. By\n2.4. Current Issues in Fake News Detection\nDespite positive outcomes in many studies, issues remain with datasets used for training fake news detection models, particularly those relying on token representations. Dataset size is a critical issue, as collecting and accurately labeling a large number of news articles is challenging (D'Ulizia et al., 2021). To manage this, articles are often labeled based on their publisher as a proxy for accuracy, which can introduce topical biases (Torabi Asr and Taboada, 2019). This can lead to models that perform well in hold-out test conditions but struggle to generalise outside of the training dataset (Suprem et al., 2022)\nLimited studies have explored the generalisability of these models. Gautam and Jerripothula (2020) observed a 39% accuracy drop when models trained on political news were tested on celebrity news. Similarly, Castelo et al. (2019) found comparable accuracy drops across different news domains using small datasets of fewer than 500 articles. Multimodal approaches, like those examined by Liu et al. (2024), combining text and image features, showed some cross-domain generalisation but significant accuracy drops to around 55% under cross-dataset conditions. Notably, embeddings from more recent large language models such as GPT-4 also suffer from an inability to generalise when trained on these coarsely labelled datasets, as demonstrated by Alnabhan and Branco (2024)\n2.5. Proposed Features\nGiven this evidence, this study seeks to propose a set of four novel features with the goal of producing more generalisable fake news detection models. These features are outlined as follows:\n\u2022 Frequency of Ads: One of the primary motivations behind the creation and dissemination of fake news is financial gain through advertising. According to Allcott and Gentzkow (2017), fake news websites often rely on sensationalist and misleading content to attract high volumes of traffic, which in turn increases their advertising revenue. These sites typically feature a large number of advertisements, as their business model is heavily reliant on generating ad impressions and clicks. Therefore, the number of adverts associated with a given article could be a significant indicator of fake news. Articles that contain an unusually high number of ads may be designed to maximise revenue rather than to provide factual information, making this a critical feature to include in fake news detection models\n\u2022 External Links: Similar to advertising, the prevalence of external links in an article can also be an indicator of fake news, especially when these links are intended for affiliate marketing purposes. Fake news articles often include numerous external links that direct readers to other sites, which can generate affiliate income for the publisher each\n\u2022 Social Media Share Links: The role of social media in the spread of fake news is well-established, with platforms like Facebook and X/Twitter being primary channels for misinformation dissemination. One of the mechanisms that facilitate this spread is the use of visual cues, such as share buttons, which prompt habitual behaviour in social media users (Ceylan et al., 2023). When users encounter these visual cues, they are more likely to share the content without critically evaluating its veracity. Including 'call to action' links that lead to social media platforms in the analysis is essential, as these links can significantly amplify the reach of fake news articles. By encouraging readers to share content on social media, these articles can quickly go viral, spreading misinformation at an unprecedented rate. Therefore, factoring in Facebook and X/Twitter links is expected to be important in identifying articles that are designed to exploit social media behaviour for rapid dissemination. It is important to note these social media features are distinct from others seen in the literature, which typically focus on user profiles and relationships between tweets and users\nThis novel group of features shall be characterised as \u2018social-monetisation' features"}, {"title": "3. Research Questions", "content": "The analysis in Section 2 provides the motivation for a focused investigation into the use of generalisable stylistic features as well as the novel social-monetisation features that may lend themselves to improved generalisability (the ability of a model to perform well when tested on a different dataset than the one on which it was trained) of fake news detection models. As such, the objectives of this study can be formalised into the following three research questions:\n\u2022 RQ1. How well do fake news detection methods using token-representations/LLMs generalise?\n\u2022 RQ2. Do fake news detection methods using stylistic features generalise better than fake news detection models using token-representations/LLMs?\n\u2022 RQ3. Do fake news detection methods using stylistic features and the proposed social-monetisation features generalise better than models using stylistic features only?"}, {"title": "4. Methodology", "content": "This section details the methodology used to address the research questions outlined in Section 3. The study conducted two experiments (summarised in Figure 1) to achieve this.\nThe first experiment aimed to evaluate the generalisability of commonly used token-representations, including Bag of Words (BoW), TF-IDF, Word2Vec and BERT, as well as the Large Language Model \u2018LLaMA', addressing RQ1. Models using token-representations were trained on the NELA 2020-21 dataset and their performance evaluated using K-fold cross-validation. For LLAMA, the zero-shot configuration relied on its pre-trained knowledge and structured prompts, while the few-shot configuration incorporated labelled examples from the NELA dataset to guide classification. The fine-tuned LLaMA model followed a methodology similar to the token-based approaches, enabling a direct comparison of generalisability across techniques. Models trained/fine-tuned on the NELA dataset were then evaluated using external validation on\n4.1. Data Collection & Processing\nThis section outlines the datasets and data extraction methods used. Owing to the nature of the proposed social-monetisation features, the dataset required the source URL of the articles to facilitate the extraction of these features. Capuano et al. (2023)'s systematic review lists several datasets used in content-based fake news detection. However, out of the 19 datasets reviewed, only three FakeNewsNet, Buzzfeed, and Celebrity fake news\u2014include the article's source URL. These datasets are relatively small, which limits the likelihood of producing a generalisable model. To develop a more comprehensive and reliable model, a larger dataset is necessary. Therefore, the NELA series of datasets was chosen for its large size and inclusion of article URLs, providing a more extensive and diverse data source for training. Using a dataset of this size also ensures that a significant number of articles can be extracted to compensate for pages that are no longer available. While not as frequently used in the literature, a number of studies make use of this\nThe combined NELA 2020-21 dataset includes 3,635,636 records from\n525 unique sources. After joining the labels file and excluding the\u2018mixed\u2019 category, the dataset consists of 1,013,808 'true' and 551,051 \u2018fake\u2019articles from 224 sources. To prevent any single source from dominating the training set 2, the number of URLs extracted from each source was reduced using the 1st quartile as a threshold (285 articles per source), resulting in a final set of 22,230 'true' and 25,650 \u2018fake' articles from 168 sources (Figure 3).\nThe Facebook URLs Dataset was chosen as an external validation dataset owing to its unique position as a dataset collected in a \u2018real-world' context and granular labelling by a third-party fact-checking organisation. Its individual article labels provide a robust standard for assessing model accuracy and practical applicability in fake news detection. This stands in contrast to commonly used datasets in the field, which often employ coarse labels based on article publishers, potentially misrepresenting the true nature of fake news. By using a coarsely labelled dataset for training and a manually labelled dataset for testing, the aim is to demonstrate that despite the limitations of coarsely labelled datasets, meaningful features can still be extracted to develop robust models applicable in real-world scenarios\nThe Facebook URLs dataset contains over 38 million URLs shared on Facebook since January 1, 2017, with 35,924 records identified as fake news. The dataset is protected with differential privacy, ensuring no information can be gathered regarding individuals (Messing et al., 2020). Given its restricted accessibility and limited usage in prior studies, this research represents one of the few to utilise the Facebook URLs Dataset for fake news classification, following a study by Barnab\u00f2 et al. (2022). The dataset initially comprised 28,271 fake and 7,653 true records, with non-English articles filtered out based on \u2018US' and 'UK' values in the \u2018Public Shares Top Country' field, resulting in 14,354 fake and 1,468 true records. To enhance dataset quality, URLs referring to Tweets and videos were excluded. Class balancing was implemented during experimentation. Due to its size, the Facebook URLs Dataset served as a test set for cross-dataset testing, complementing the larger training datasets to bolster the model's generalisability and validate its performance in diverse real-world scenarios\nIn order to extract the raw textual data from the URLs in these datasets, the BeautifulSoup library was used. As many webpages in these datasets may no longer be available, particularly in relation to 'fake' news pages, initial extraction was attempted through the use of the Wayback Machine API (Internet Archive). This was done to increase the likelihood of extracting a webpage with a complete article and not a splash page indicating the article\nhad since been deleted. In instances where webpages were not available in this archive, a final extraction attempt was made directly from the webpage using the URL provided in the dataset to account for cases where webpages may not yet have been added to the Internet Archive. If through these methods a complete article was not extracted, the URL would be excluded from the resulting dataset.\nIn cases where full articles were available, rather than attempt to accurately extract only the text pertaining to the news articles from these URLs, all textual elements are extracted from the body of the webpage. While this may introduce additional noise to the feature-sets, it was a deliberate choice. Websites have different layouts, styles and coding structures, making it challenging to consistently and accurately extract only the article text. It is argued that models that extract all textual elements from the webpage body are more adaptable to the varying structures and formats of webpages and, as such, have the potential to be more robust and scalable across a wider range of online content. Following this data extraction phase, pages returning <3KB of data were excluded, as it was observed that pages with less than this amount of data had typically had their articles removed. The resulting datasets are summarised in Table 1:\n4.2. Experiment 1 Features: Token-Representations & LLMs\nThis section outlines the features to be used in the first experiment, addressing RQ1. This experiment aims to address RQ1, by exploring how well models using token-representations generalise between two different datasets of the same topic, using the NELA and Facebook datasets. An overview of the procedure followed in this experiment is provided in Figure 4. In this section, each of the token-representations used in this experiment and the libraries used in extracting these features from the datasets are outlined. The results of this experiment are presented in Section 5.1.\n4.2.1. Token-Representations\nThe token-representations chosen for this study are motivated by the systematic review by Capuano et al. (2023), which identified the following token-representations among the most popular in the literature for content-based fake news detection:\n\u2022 Bag-of-Words (BoW) converts text into fixed-length vectors based on word frequencies. It does not take into account word-order. In this study, BoW was implemented with SKLearn's CountVectorizer with a max_features parameter of 10,000 words.\n\u2022 TF-IDF enhances BoW by considering word frequency relative to the dataset, capturing token significance. This technique, therefore, better captures the significance of tokens compared to BoW. However, like BoW of words it fails to account for word-order. TF-IDF is among the most popular feature extraction approaches, as identified by Capuano et al. (2023), owing to its simplicity and focus on informative tokens. In this study, TF-IDF was implemented with SKLearn's TFIDFVectorizer and max_features set to 10,000 words\n\u2022 Word2Vec generates word embeddings by training a neural network to predict target words from their context, capturing semantic meaning but ignoring word order (similar to the previous two techniques). It is one of the most popular feature extraction methods, identified in approximately 25% of studies in an ongoing systematic review by the\n\u2022 BERT, similar to Word2Vec, also generates word-embeddings but with the distinct advantage of being context-dependent thus allowing for unique representations of words morphologically similar words. BERT achieves this through a novel approach of training at the sub-word level, encoding word-positions and training on tasks such as Masked-Language-Modelling (MLM) and Next-Sentence Prediction (NSP). Although other, more advanced, transformer-based models like GPT-4 exist, they are still much less well-established in the literature for this specific task. Therefore, BERT has been chosen for this study In this study, BERT was implemented using the SentenceTransformers library using the pre-trained\u2018bert-base-uncased\u2019model. Additionally, a fine-tuned version of this model trained on the NELA dataset was employed to further optimise performance for the fake news detection task.\nIn the case of the BoW and TF-IDF approaches, the following steps were taken to remove any unwanted noise from the text: (i) converting the text to lowercase to ensure all words were treated uniformly; (ii) lemmatising the text; and (iii) removing punctuation, URLs, Twitter handles, extra-whitespace and stop words. Word2Vec and BERT did not undergo the above three steps because these techniques require contextual information in order to generate their embeddings.\n4.2.2. LLMs\nBuilding on the foundation of traditional token-based features, this study also incorporates the large language model LLaMa 3.2-1B, a transformer-based model with 1 billion parameters. Chosen for its robust natural language understanding and computational efficiency, the model provides a strong baseline for evaluating advanced detection techniques.\nLLaMa was utilised in three configurations: zero-shot, few-shot, and fine-tuning. In the zero-shot configuration, the model leveraged its pre-trained knowledge without task-specific training, providing an initial benchmark for its capabilities. Few-shot learning introduced a small number of labeled examples to guide predictions, while fine-tuning adapted the model comprehensively by training it on labeled datasets.\nTo operationalise LLaMa in zero-shot and few-shot learning, structured prompts were designed to align with the task requirements. The system\n4.3. Experiment 2 Features: Stylistic & Proposed Social-Monetisation Features\nThe following section outlines the stylistic features that were used in addressing RQ2 Experiment 2 follows a similar structure to Experiment 1 and evaluates the generalisability of five groups of stylistic features proposed by\n4.3.1. Selection of Stylistic Features\nFive groups of stylistic features, proposed in the literature, are evaluated in this study. Each feature group varies in complexity, with the first group focusing solely on linguistics, while subsequent feature-sets progressively incorporate additional groups such as psycholinguistics and document complexity. Due to the diverse scales of many features within these groups, we applied StandardScaler to ensure uniform treatment by machine learning algorithms. The selection of these five groups was motivated by their inclusion in the limited number of studies exploring generalisability of fake news detection models (with the exception of the NELA feature-set which was chosen owing to the use of the NELA dataset in this study). However, these studies only used coarsely labelled datasets for external validation. As such, we aimed to observe their performance using real-world data from the Facebook URLs dataset, underscoring the relevance of these features in practical applications of fake news detection.\nGroup 1: Fernandez and Devaraj Stylistic Features. This study employed the collection of 34 linguistic attributes (referred to as 'Linguistic Dimension' and 'Punctuation Cues') that demonstrated the highest efficacy in classifying fake news, as determined through a sequence of tests outlined by Fernandez and Devaraj (2019). The inclusion of these features was motivated by Hoy and Koulouri (2022) which used these features in providing preliminary evidence that stylistic features have the potential to be more generalisable than token-representations. But both studies relied on coarsely labelled datasets The two groups of features can be summarised as follows:\n\u2022 Linguistic Dimensions: Based on the Linguistic Dimensions of LIWC, this category aims to capture the complexity of news through inclusion of features such as word-per-sentence, average word size and type-token ratio (a measure of lexical variety) as well as the different types of words used such as the ratio of adjectives, nouns, verbs and named-entities.\n\u2022 Punctuation Cues: Focuses solely on the different types of punctuation used relative to all punctuation in a given article.\nAs such linguistic features are a staple in NLP, the other groups of features described below also include similar groups of features.\nGroup 2: Abonizio Features. This study leverages 21 features organised into three groups: complexity, stylometric and psychological. The inclusion of these features, similar to the previous group, is motivated by their use in another generalisability test on coarsely labelled datasets in Abonizio et al. (2020) Similar groups of features can also be found in (Paschalides et al., 2019; Garg and Kumar Sharma, 2022; Reis et al., 2019) thus motivating their inclusion in this study.\nThe 'complexity' and \u2018stylometric' features overlap with some of the features used in the previous \u2018Fernandez' feature-set however it should be noted that the Abonizio feature-set is not as granular. However, unlike the Fernandez feature-set, the Abonizio feature-set does extend to include a psychological category, capturing the sentiment analysis score of a given article.\nGroup 3: Linguistic Inquiry Word Count (LIWC). As previously mentioned in Section 2, LIWC is a dictionary-based approach comprising of linguistic\nelements, punctuation characteristics as well as psycholinguistic features organised in a number of categories. These categories can be summarised as follows:\n\u2022 Summary Variables: Aim to summarise the features from the below three categories and attempt to capture document complexity and psychological features.\n\u2022 Linguistic Dimensions: Aims to capture different types of words such as pronouns, verbs and adjectives as well as words denoting grammatical person and numbers.\n\u2022 Psychological Processes: captures a number of psychological words relating to sentiment (such as 'good' and 'bad') as well as words relating to cognition (e.g. \u2018know' and 'think') and social processes (such as \u2018love\u2019 and 'fight').\n\u2022 Expanded Dictionary: captures a range of different words relating to a number of different topics such as culture and lifestyle as well as temporal words such as \u2018when', 'now' and 'then'.\nThe total number of features in this feature-set amount to 118. Similar to the previous feature groups, LIWC is used in a generalisability study by P\u00e9rez-Rosas et al. (2017) which observed the performance of LIWC trained on the FakeNewsAMT dataset and tested Celebrity news datasets and vice versa. A number of other studies also leverage these features, thus further justifying their inclusion (Ahmad et al., 2020; Spezzano et al., 2021; Shu et al., 2019a)\nGroup 4: NELA Feature Extractor. The NELA feature extractor is a tool hosted on GitHub designed by the authors of the NELA dataset which has been used throughout this study. This therefore motivated the inclusion of these features in this study. It includes a rich, hand-crafted feature-set of 91 features which can be summarised into the following categories:\n\u2022 Style: Largely similar to those from the previous three studies, focusing on POS tags\n\u2022 Complexity: Similar to the \u2018Linguistic Dimensions' and \u2018Complexity' categories of the Abonizio feature-set, this category of features aims to\ncapture how complex an article is through analysing lexical diversity, reading-difficulty metrics and the average length of words and sentences\n\u2022 Bias: Based on Recasens et al. (2013) work, this category of features aims to capture the subjectivity of the text by identifying the number of hedges, factives, assertives, implicatives, and opinion words.\n\u2022 Affect: Relying on VADER sentiment analysis, this category aims to capture the emotion and sentiment of the text\n\u2022 Moral: The objective of this feature category is to encompass the ethical content present in a text, and it is built upon the principles of Moral Foundation Theory (MFT) introduced by Graham et al. (2013).Lin et al. (2018) subsequently expanded upon this theory and developed a lexicon specifically designed for assessing the moral aspects of text. This feature group employs the lexicon established by Lin et al. (2018) to gauge the morality of the text under consideration.\n\u2022 Event: Aims to capture words relating to dates, times and locations.\nGroup 5: Modified NELA Features. Through the use of the NELA Feature Extractor, it was noted that a number of the features were either duplicated or returning zero values, in particular, when attempting to extract punctuation. As such, the NELA Feature Extractor was modified to remediate this and to include additional punctuation such as \u2018#', \u2018@', \u2018\u00a3', '$', '&' and '%'. The normalisation was also adjusted depending on the features. For example, rather than scaling the punctuation based on the word-count of an article, punctuation was scaled based on the total number of punctuation items.\n4.3.2. Proposed \u2018Social-Monetisation' Features\nAs motivated in Section 2.5, a number of additional novel features, categorised as 'social-monetisation' features, were investigated to complement the groups of stylistic features selected for this study. These were: (i) the number of advertisements; (ii) the number of external links; (iii) the number of links to Facebook; and (iv) number of links to Twitter/X. The number of ads was extracted through the use of EasyList, an open-source project that compiles a list of the most popular adblocking filters. Using this list enables searching the webpage's LXML tree and counting the frequency of\n4.4. Machine Learning Algorithms\nAs this study prioritises the exploration of stylistic features for generalisable fake news detection, less emphasis has been put on exploring the effect of different machine learning algorithms and their respective hyperparameters. However, for completeness and to offer an opportunity for comparison to the literature, a number of machine learning algorithms including Logistic Regression, SVM, Gradient Boosting, Decision Trees, Random Forest, and a feed-forward neural network (FFNN) are employed. Each of these algorithms was implemented using default hyperparameters in SKLearn. The exception to this was the neural network where default hyperparameters are not available and, as such, a shallow Sequential model was used with a single hidden layer of 10 neurons, a sigmoid activation layer compiled with binary cross-entropy loss and the Adam optimiser. To protect against overfitting, the EarlyStopping hyperparameter was set to stop training if the loss function did not improve by 0.01.\nAdditionally, a Long Short-Term Memory (LSTM) network was employed to process word embeddings generated by BERT and Word2Vec. LSTMs are well-suited for capturing sequential dependencies in embeddings, providing a deeper understanding of contextual relationships within the text. This approach was not applied to Bag-of-Words or TF-IDF features, as these methods represent text as sparse matrices, lacking sequential and contextual information, rendering them unsuitable for use with LSTMs. This distinction highlights the effort to align algorithms with feature sets that best leverage their strengths for fake news detection. The LSTM architecture used in this study consisted of an LSTM layer with 128 hidden units, designed to model complex temporal dependencies and contextual patterns in the data. A dropout rate of 40% was applied to the LSTM's output to reduce the risk of overfitting. Finally, a fully connected layer was used to map the final hidden state to two output classes: fake and real news.\n4.5. Training/Testing Methodology\nThe training and testing methodology in this study integrates K-fold cross-validation, external validation, and tailored evaluation techniques for large language models (LLMs) to assess performance and generalisability in fake news detection. For traditional machine learning algorithms and feature sets, K-fold cross-validation is employed on the NELA dataset, partitioning it into 10 equal folds. Models are trained on 9 folds and validated on the remaining fold across iterations, ensuring robust performance estimates with a fixed random state (set to 42). External validation evaluates the generalisability of models by testing those trained on each NELA fold against 500 randomly sampled articles per class from the Facebook dataset, addressing class imbalance. Key evaluation metrics such as Accuracy, Precision, Recall, Specificity and F1-Score are used to assess model performance in distinguishing between true news' and fake news'. By evaluating the models using external validation after each fold.\nFor LLaMa zero-shot and few-shot models, a distinct methodology was adopted due to its pre-trained nature. In the zero-shot configuration, the model leveraged its existing knowledge without any task-specific training, relying entirely on structured prompts to classify text from both the entirety of the NELA dataset and 5 random balanced samples of 500 Facebook URLs dataset. In the few-shot configuration, the model was guided by labelled examples from the NELA dataset only to reflect the goals of this study to train on a coarsely labelled dataset but test on a manually labelled dataset. The fine-tuned LLaMa model followed a similar methodology to the above traditional methods.\nIn addition to the Mann-Whitney U-test, Permutation Feature Importance (PFI) was used to pinpoint the stylistic and proposed social monetisation features that positively contributed to a more generalisable model. PFI works by shuffling a random feature, thereby disrupting the relationship between that feature and the target variable. By repeating this process for all features in the dataset and observing the effects on model performance, the method reveals how much the model depends on each feature"}, {"title": "5. Results", "content": "This section outlines the results of the two experiments outlined in Section 4. The overarching objective of the experiments is to demonstrate whether models using different sets of stylistic and the proposed social-monetisation\n5.1. Experiment 1: Generalisability of Token-Representations & LLMs\nExperiment 1 aimed to address RQ1 by examining the generalisation capabilities of token representations (BoW, TF-IDF, Word2Vec, and BERT, as detailed in Section 4.2.1) combined with different machine learning models, as well as evaluating the performance of the large language model LLaMa under zero-shot, few-shot, and fine-tuning configurations.\n5.1.1. Token-Representation Results\nAs seen in Table 2, under K-fold test conditions, token-representations trained and tested on the NELA dataset exhibit high performance across\nseveral models, achieving a mean accuracy of 0.93, with a range between 0.79 and 1.0. These results align with those commonly reported in the literature and validate the effectiveness of the feature extraction and modeling methods when applied in controlled conditions.\nAmong the tested approaches, Fine-Tuned BERT and LSTMs demonstrated the highest performance, achieving near-perfect metrics. Fine-Tuned BERT reached 1.0 accuracy, precision, recall, specificity, and F1 score, while LSTMs trained on BERT embeddings achieved 0.99 across all metrics. However, such results may reflect the model's ability to memorise patterns specific to the training data rather than a genuine ability to generalise beyond the dataset.\nTraditional feature extraction techniques such as BoW and TF-IDF also achieved strong results, with accuracies up to 0.99. Their reliance on high-frequency or significant terms may have contributed to their robust performance within the dataset. This aligns with our observations in Section 2.4 regarding biases within commonly used fake news datasets, where certain terms may strongly correlate with specific classes. Specifically, Logistic Regression outperformed SVM when using the BoW representation, likely due to its ability to leverage the sparse, linearly separable nature of BoW features. In contrast, the SVM with an RBF kernel may not have been optimally suited for this representation, as the kernel is designed for capturing non-linear relationships, which may not align with BoW's characteristics (Colas et al., 2007). This mismatch could partly explain the relatively poorer performance of SVM in this context.\nIn contrast to BoW and TF-IDF, Word2Vec and standard BERT-base embeddings performed slightly worse, with accuracies ranging from 0.81 to 0.95. These models may have struggled with the added noise in the textual data or the more complex representations introduced by their embeddings. In the case of Word2Vec, despite using a relevant model pre-trained on Google News, the performance may have been hindered by out-of-vocabulary OOV) words. Unlike BoW and TF-IDF, which construct their vocabularies directly from the dataset and therefore capture all words within that domain, Word2Vec relies on a fixed vocabulary from its pre-training corpus. Fake news datasets often contain domain-specific terms, slang, or creative language use that may not be present in the pre-trained Word2Vec vocabulary. As a result, OOV words are either ignored or mapped to suboptimal representations, leading to a potential loss of crucial information. This limitation reduces the model's ability to capture dataset-specific keywords\nand patterns, whereas simpler methods like BoW and TF-IDF, by leveraging dataset-dependent vocabularies, retain the ability to represent all terms present in the text.\nWhile these results highlight the apparent effectiveness of both traditional and deep learning approaches in controlled settings, they must be interpreted cautiously. The high performance observed here may not translate to real-world applications or unseen datasets, as will be discussed in the cross-dataset results.\nThe results from the cross-dataset testing (Table 3) reveal a significant reduction in performance across all models and feature sets compared to the K-fold test conditions discussed in the previous section. While token-representation models achieved high accuracy within the NELA dataset (Section 2), their ability to generalise across datasets is markedly lower. On average, models experienced a 0.28 drop in accuracy, underscoring the challenge\nof applying these approaches to unseen data.\nAmong the tested feature sets, Fine-Tuned BERT and LSTM models, which previously excelled in the controlled K-fold setting, demonstrated similar vulnerabilities to generalisation issues. Fine-Tuned BERT achieved an accuracy of 0.68, precision of 0.75, recall of 0.53, specificity of 0.82, and an F1 score of 0.62. While these metrics are comparable to other models in cross-dataset conditions, they represent a stark decline from"}]}