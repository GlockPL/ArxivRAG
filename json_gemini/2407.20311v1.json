{"title": "Physics of Language Models: Part 2.1,\nGrade-School Math and the Hidden Reasoning Process", "authors": ["Tian Ye", "Zicheng Xu", "Yuanzhi Li", "Zeyuan Allen-Zhu"], "abstract": "Recent advances in language models have demonstrated their capability to solve mathematical reasoning problems, achieving near-perfect accuracy on grade-school level math benchmarks like GSM8K. In this paper, we formally study how language models solve these problems. We design a series of controlled experiments to address several fundamental questions: (1) Can language models truly develop reasoning skills, or do they simply memorize templates? (2) What is the model's hidden (mental) reasoning process? (3) Do models solve math questions using skills similar to or different from humans? (4) Do models trained on GSM8K-like datasets develop reasoning skills beyond those necessary for solving GSM8K problems? (5) What mental process causes models to make reasoning mistakes? (6) How large or deep must a model be to effectively solve GSM8K-level math questions?\nOur study uncovers many hidden mechanisms by which language models solve mathematical questions, providing insights that extend beyond current understandings of LLMs.", "sections": [{"title": "Introduction", "content": "The field of language models has made significant progress in recent years. Large models like GPT-4 [17] have shown initial signs of general intelligence [8], while smaller models have demonstrated good reasoning abilities by solving challenging coding and math problems [11, 15, 16].\nIn this paper, we focus on the ability of small language models to solve grade-school math problems. Unlike previous works that empirically push the accuracy of models on grade-school math benchmarks like GSM8K [9] and its augmentations (e.g., [16, 22]), we take a more principled approach. We aim to understand the following fundamental questions:\n1. How do language models learn to solve grade-school level math problems? Do they just\nmemorize templates, or do they learn reasoning skills similar to humans? Or do they discover\nnew skills to solve the problems?\n2. Do models trained solely on grade-school math problems only learn to solve these problems,\nor do they develop some more general intelligence?\n3. How small can a language model be while still solving grade-school math problems? Is depth\n(number of layers) more important than width (number of neurons per layer), or does only\nsize matter as suggested by practitioners [14]?\nThese questions are fundamental to understanding the intelligence of language models. To study them, it might seem tempting to start with a pre-trained model and fine-tune it on existing datasets like GSM8K or GPT-4 augmented ones (e.g., [16, 22]). However, this approach has significant limitations:\n\u2022 DATA CONTAMINATION. The pretrain data of existing models mostly come from publicly\navailable internet [10], which is a pile of mess. We do not know how many math problems\nare included or their structures. There is significant concern regarding whether the GSM8K\nbenchmark has been leaked to language models' training datasets [22]. Even if the exact data is\nnot, the pre-trained model might have seen almost identical questions (e.g., the same problem\nwith different numbers). Thus, this approach cannot answer questions 1-3. We do not know\nwhether a model truly learns the reasoning skills or it simply memorizes problem templates\nduring training. Therefore, we need full control over the model's pretrain data and\nmust train a language model from scratch. This point has been reiterated recently in [2, 3].\n\u2022 SOLUTION DIVERSITY. The existing fine-tuning data, such as the GSM8K training set, contains\nonly 7.5K grade-school math problems, which is insufficient to train a model from scratch.\nAlthough recent works use GPT-4 to augment GSM8K, this is not enough for our purpose.\nGPT-4 augmented problems might be biased towards a small number of solution templates,\nsince the original GSM8K data has very few (obviously, at most 8K) solution templates. We\nneed a much larger, more diverse set of grade-school math problems.\nWith these points in mind, we introduce our framework to generate a large set of diverse grade- school math (GSM) problems and use the dataset to train (from scratch) and test a GPT2-like language model. In the framework, we focus on the \"logical reasoning\" aspect of grade-school math problems, which involves the dependency of parameters in the problem statement, such as \"Alice's apple is three times the sum of Bob's orange and Charles's banana.\" We use synthetic sentences to reduce the difficulty arising from Common Sense, like \"a candle burned for 12 hours at 1 inch per hour\" (implying the candle is reducing in length). We also remove the difficulty from pure"}, {"title": "Result 2.  We demonstrate that the GPT2 model, pretrained on our synthetic dataset, not\nonly achieves 99% accuracy in solving math problems from the same distribution but also out- of-distribution generalizes, such as to those of longer reasoning lengths than any seen during training.", "content": "This is similar to length generalization in arithmetics [6, 13], however, in our case, the\nmodel has never seen any training example of the same length as in test time.  This\nsignifies that the model can truly learn some reasoning skill instead of memorizing solution\ntemplates.\nRESULT 3.  Crucially, the model can learn to generate shortest solutions, almost always avoid- ing unnecessary computations.  This suggests that the model formulates a plan before it generates, in order to avoid computing any quantities that are not needed towards solving the\nunderlying math problem.\nRESULT 4.  We examine the model's internal states through probing, introducing six probing tasks to elucidate how the model solves math problems.  For instance, we discover the model\n(mentally!) preprocesses the full set of necessary parameters before it starts any generation.  Likewise, humans also do this preprocess although we write this down on scratch pads.\nRESULT 5.  Surprisingly, the model also learns unnecessary, yet important skills after pretrain- ing, such as all-pair dependency.  Before any question is asked, it already (mentally!) computes with good accuracy which parameters depend on which, even though some are not needed for\nsolving the math problem.  Note that computing all-pair dependency is a skill not needed to fit all the solutions in the training data.  To the best of our knowledge, this is the first evidence that a language model can learn useful skills beyond those necessary to fit its pretraining data.2 This may be a preliminary signal of where the G in AGI can come from.3\nRESULT 6.  We explain why mistakes occur.  For instance, the model makes systematic er- rors that can be explained by probing its internal states.  Sometimes, these mistakes can be\npredicted before the model generates answers, making them independent of the random gen- eration process.  We connect this to practice, noting that GPT-4/4o also makes similar errors\n(though we cannot probe their internal states).\nRESULT 7+8.  The depth of the language model is crucial for its reasoning ability.  For example, a 16-layer, 576-dim transformer solves harder problems (in reasoning length) than a 4-layer, 1920-dim one, despite the latter being twice as large.  This holds even when Chain-of-Thought\n(CoT) is used.  We explain this necessity in depth by the complexity of the mental processes"}, {"title": "Result 1: Data Generation", "content": "Motivation. Recall a standard grade-school math problem in the GSM8K dataset [9] looks like:\nBetty is saving money for a new wallet which costs 100. Betty has only half of the money she needs. Her parents decided to\ngive her 15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to\nbuy the wallet?\nThis problem involves multiple parameters whose values are connected through various equalities, such as \"Betty's current money = 0.5 \u00d7 cost of the wallet\" and \"money given by grandparents =\n2 \u00d7 money given by parents.\" Motivated by this, we build a GSM8K-like math dataset through a synthetic generation pipeline that captures the dependencies of parameters. We wish to capture at least the following three types of dependencies.\n1. Direct dependency (\u2661): such as $A = 5 \u00d7 (X + Y)$, so A can be computed after X and Y.\n2. Instance dependency (\u2660): such as \u201cevery classroom has X chairs, and there are Y classrooms.\u201d\nHere, the model must infer the total number of chairs by multiplying X by Y.\n3. Implicit dependency (\u2663): such as \u201cBob has 3 times more fruits than Alice. Alice has 3 apples,\n4 eggs and 2 bananas.\" Here, the model must learn that apples and bananas are fruits and egg is not, and \"Alice's fruits\" is an abstract parameter derived from the problem statement."}, {"title": "Step 1: Graph Construction and Problem Generation", "content": "Hierarchical categorization. We use a layered structure of categories, each contains possible items. For instance, categories = (School, Classroom, Backpack) has three layers; category School =\n{Central High, Riverview High, . . . }; category Classroom = {Dance Studio, Film Studio, . . . }; category\nBackpack = {School Daypack, Messenger Backpack, ...}. We prepare 4 predefined hierarchical categorizations, each with 4 layers and 100 items in each layer; this represents the world knowledge."}, {"title": "Structure graph", "content": "In each math problem, only specific items exist, leading to a structure graph that outlines what sub-items can appear under what item, see Figure 1 (left). For instance,\n\u2022 Connecting Dance Studio and School Daypack with an edge signifies an instance parameter, \"the number of school daypacks in each dance studio,\" which is a quantifiable variable that can be assigned. This captures the instance dependency as mentioned above.\n\u2022 Abstract parameters, like \"the total number of classrooms in Central High,\" cannot be assigned\nand are excluded from the structure graph. They reflect implicity dependency (+)\nRemark 2.1. Rather than using simple objects like Alice's apple or fake items like Items A/B/C/D, this structure allows us to describe abstract parameters and adds 2 levels of complexity to the data:\n\u2022 The model must implicitly learn English concepts, such as a classroom category includes 100 different classroom types. These concepts cannot be derived from individual math problems, as only a limited selection of classrooms will be mentioned in each problem.\n\u2022 The model is required to hierarchically access multiple items to calculate abstract parameters,\nas opposed to a straightforward retrieval of \u201cAlice's apple\" in the context."}, {"title": "Dependency graph", "content": "The dependency graph is a directed acyclic graph that outlines the de- pendency among parameters. For each instance parameter, we choose a random set of (up to 4) parameters it can depend on including possibly a special vertex RNG representing a random number generator. For instance, if \"[param A ] is X more than the difference of [param B ] and\n[param C]\" for X being randomly generated, then we draw edges from B, C and RNG to parameter\nA. The dependency of abstract parameters is implied by the dependency of instance parameters. This captures direct dependency (\u2661) as mentioned above. We give an examples on the right side of Figure 1, and details for how we randomly generate such dependency graph are in Appendix D.2."}, {"title": "Problem generation", "content": "The problem is articulated by describing the dependency graphs in En- glish, one sentence for each instance parameter.6 (Abstract parameters are not described because they are inherited by the structure graph.) We randomly permute the sentence ordering to\nfurther increase difficulty. A parameter is selected and asked with a question in the end (or at the beginning). Below is an easy example corresponding to Figure 1; a harder example is in Figure 11.\n(Problem - Easy) The number of each Riverview High's Film Studio equals 5 times as much as the sum of each Film Studio's Backpack and each Dance Studio's School Daypack. The number of each Film Studio's School Daypack equals\n12 more than the sum of each Film Studio's Messenger Backpack and each Central High's Film Studio. The number of each Central High's Film Studio equals the sum of each Dance Studio's School Daypack and each Film Studio's Messenger\nBackpack. The number of each Riverview High's Dance Studio equals the sum of each Film Studio's Backpack, each Film\nStudio's Messenger Backpack, each Film Studio's School Daypack and each Central High's Backpack. The number of each Dance Studio's School Daypack equals 17. The number of each Film Studio's Messenger Backpack equals 13. How many\nBackpack does Central High have?\n(2.1)"}, {"title": "Step 2: Solution Construction (CoT)", "content": "Let solution be a sequence of sentences describing the necessary steps towards solving the given problem, where the sentences follow any topological order also known as Chain-of-Thought, CoT. For each parameter necessary towards answering the final question, we assign to it a random letter among the 52 choices (a..z or A..Z), and use a sentence to describe its computation:7\nDefine [param] as X; [intermediate steps]; so X =\nThroughout this paper, we consider arithmetics mod 23 to avoid errors from computation in- volving large numbers. It is perhaps the easiest to directly see a solution example (corresponding to (2.1)), and a more involved example is in Figure 11:\n(Solution - Easy) Define Dance Studio's School Daypack as p; so p = 17. Define Film Studio's Messenger Backpack as W;\nso W = 13. Define Central High's Film Studio as B; so B = p + W = 17 + 13 = 7. Define Film Studio's School Daypack as g; R = W + B = 13 + 7 = 20; so g = 12 + R = 12 + 20 = 9. Define Film Studio's Backpack as w; so w=g+W= 9 + 13 = 22. Define Central High's Backpack as c; so c = B * w = 7 * 22 = 16. Answer: 16.\nWe emphasize that:\n(2.2)\n\u2022 The solution only contain parameters necessary towards calculating the final query parameter.\n\u2022 The solution follows the correct logical order: i.e. all the parameters used in the calculation must have appeared and been computed beforehand.\n\u2022 We break computations to binary ops: $g = 12+13+7$ is broken into $g = 12+R$ and $R = 13+7$ in the above solution. The number of semicolons \";\" equals the number of operations. This reduces the arithmetic complexity of the solution, which is not the focus of this paper.8"}, {"title": "Difficulty Control", "content": "Although deferring all the pseudocode to Appendix D, we summarize below the main randomness used in the data generation process. This includes the random choice of a hierarchical catego- rization (i.e., the English part); a structure graph (i.e., the instance parameters); a dependency graph; arithmetic computations on the dependency graph; integer numbers (i.e., the RNG); problem sentence permutation; and the query parameter.\nWe use two parameters to control data's difficulty: ip is the number of instance parameters, and op is the number of solution operations; the data's difficulty is an increasing function over them. We call our dataset iGSM, to reflect the nature that such synthetic dataset can be of infinite size. We use iGSM op\u2264op,ip\u2264ip to denote the data generated with constraint $op \\le \\overline{op}$ and $ip \u2264 \\overline{ip}$, and use iGSM op=op,ip\u2264ip to denote those restricting to $op = \\overline{op}$."}, {"title": "Train and Test Datasets", "content": "We consider two families of datasets.\n\u2022 In the iGSM-med data family we use $ip \u2264 20$.\nThe training data is $iGSM-med^{op\\le15} \\overset{\\text{def}}{=} iGSM^{op \\le 15, ip \\le 20}$. We evaluate the pretrained model both in-distribution, on $iGSM-med^{op \\le 15}$ and $iGSM-med^{op = 15}$, and out-of-distribution (OOD),"}, {"title": "Summarize Model's Behavior Process", "content": "We use the GPT2 architecture [18] but replacing its absolute positional embedding with rotary embedding [7, 20], yet still referring to it as GPT2 for short. 12 We mostly stick to the 12-layer, 12-head, 768-dim GPT2 (a.k.a. GPT2-small) for experiments, but we explore larger models in Section 6. We use a context length of 768 / 1024 for pretraining on iGSM-med/iGSM-hard and 2048 for evaluation. More details are in Appendix F."}, {"title": "Result 2: accuracy", "content": "After sufficient pre-training, we give the model a problem from the test set (without solution) and let it continue to generate (allegedly a solution followed by an answer). Because we have restricted ourselves to a fixed solution format, language models can learn the format easily, allowing us to write a solution parser to check if the solution is fully correct. 13\nResult 2. Figure 3 shows that GPT2 performs well when pretrained using iGSM-med or iGSM-hard data, even when evaluated out-of-distribution on harder (i.e., larger op) math problems. Thus, the model can truly learn some reasoning skill instead of memorizing solution templates.14\nThis could be reminiscent of language models' length generalization capability on arithmetic com- putations [13, 23]; however, in our case, op captures the \u201creasoning length\" in grade-school math, and our model has never seen any training example of the same length as in test time.15\nSuch accuracies also indicate that our iGSM data families are indeed good for pretraining pur- pose, allowing us to investigate how LLMs can solve grade-school math problems."}, {"title": "Result 3: solution redundancy", "content": "We examine whether GPT2 achieves high accuracy by\n\u2022 brute-forcedly computing all the parameters during generation (a \u201clevel-0\" reasoning skill), or\n\u2022 computing only necessary parameters to give shortest solutions (a \u201clevel-1\" reasoning skill).\nRecall our iGSM (pretrain) data only contains necessary solution steps (i.e., CoT) to simulate what we see in textbook solutions for math problems. For instance, if a problem describes $X =3+2$, \u0415 =3+X, Y =X+2 and asks for the value of Y, then a shortest solution would be \u201cX =3+2=5 and Y =X+2 =7\" without ever computing E.\nResult 3. Figure 4 shows that GPT2 predominantly solves the iGSM problems with a \u201clevel-1\" reasoning skill, avoiding unnecessary computations, even when evaluated out-of-distribution.\nThis finding is significant as it suggests that, unlike humans who usually rely on \"backward rea- soning\" and a scratch pad to write down necessary parameters by backtracking the dependencies"}, {"title": "Result 4-5: Discover Model's Mental Process", "content": "To understand how the model learns to solve math problems, we propose studying the following probing tasks, which align closely with human problem-solving strategies:\n\u2022 nece(A): if parameter A is necessary for computing the answer.\n\u2022 dep(A, B): if parameter A (recursively) depends on parameter B given the problem statement.\n\u2022 known(A): if parameter A has already been computed.\n\u2022 value(A): the value of parameter A (a number between 0-22, or 23 if known(A) = false).\n\u2022 can_next(A): if A can be computed in the next solution sentence (namely, its predecessors\nhave all been calculated). Note that A might not be necessary to answer the question.\n\u2022 nece_next(A): if parameter A satisfies both can_next(A) and nece(A).\nFor a model to generate the shortest solutions, it must identify nece(A) for all A's in its mental process. This is because whether nece(A) is true directly corresponds to whether there is a solution sentence to compute A. However, how early does the model recognize this, and how is it stored?\nSimilarly, does it recognize dependencies between parameters (dep)? If so, how early is this mental process completed? Moreover, in the middle of solution generation, does the model keep track of\neach parameter A's value at all times (value, known)? Does the model mentally know all possible parameters A that are ready to compute in the next sentence (can_next)? Or does it only focus on A that is both ready and necessary (nece_next)?\nThis section proposes probing technique to answer all of these questions."}, {"title": "V-Probing: A Nearly-Linear Probing Method", "content": "As illustrated in Figure 5, we conduct probing at the end of the problem description for the dep task, and end of the question description nece task.16 For other tasks, we probe them at the end of every solution sentence (including the start of the first solution sentence).\nRecall that standard linear probing involves freezing a pretrained language model and checking if a property is linearly encoded at a hidden layer (usually the last layer) for a given token position. This is done by introducing a trainable linear classifier on the hidden states and performing a lightweight finetuning task for this property (see [12] and references therein).\nOur setting is more complex because the properties have one or two conditional variables, A and B, described in plain English. To handle this, we truncate the math problems to the probing position and append tokens [START] and [END] around the descriptions of A (or A, B). We then probe from the token position of [END] to see if the property is linearly encoded at the last layer.\nUnlike standard linear probing, to account for the input change, we introduce a small trainable rank-8 (linear) update on the input embedding layer. We freeze the pretrained language model and finetune both the linear classifier and the rank-8 update for the desired property. We refer to this as V(ariable)-probing and provide details in Appendix B. An illustration of the nece(A) probing task is shown in Figure 6.\nWe compute the V-probing accuracies on a language model pretrained from iGSM and compare them with the V-probing accuracies on a randomly-initialized transformer model. If the former accuracies are significantly higher, we conclude that the probing signals must have (or be very close to having) come from the pretrained weights, rather than the (lightweight) finetuning stage."}, {"title": "Probing Results and Findings", "content": "We present our probing results in Figure 7. The probing accuracies are high for all the tasks, compared to majority guess and random-model probing except for the very hard OOD cases"}, {"title": "Result 4: model solves math problems like humans", "content": "We make the following observations:\n\u2022 When generating solutions, the model not only remembers which parameters have been com- puted and which have not (value, known) but also knows which parameters can be computed next (can_next, nece_next). These abilities ensure that the model can solve the given math problem step by step, similar to human problem-solving skills.\n\u2022 By the end of the problem description, the model already knows the full list of necessary parameters (nece). This indicates that the model has learned to plan ahead, identifying nec- essary parameters before starting to generate the solution. This aligns with human behavior, except that the model plans mentally while humans typically write this down. This further\nconfirms that the model reaches the \u201clevel-1\" reasoning skill discussed in Section 3.\nRemark 4.1. The mental process described can be compared to (out-of-context) knowledge manip- ulation [2], which involves retrieving factual knowledge and performing single-step computations (e.g., retrieving two people's birth dates to determine who was born earlier). Allen-Zhu and Li [2] found that even single-step computations cannot be performed mentally without a substantial number of pretrain samples. In contrast, this paper studies in-context reasoning and demonstrates that the model can execute very complex mental calculations."}, {"title": "Result 5: model learns beyond human reasoning skills", "content": "Remarkably, the model learns dep(A, B) and can_next(A), even for parameters A not necessary for answering the question, as shown in Figure 7(b). This differs from human problem-solving, where we typically use back- ward reasoning from the question to identify necessary parameters, often overlooking unnecessary ones [19]. In contrast, language models can pre-compute the all-pair dependency graph dep(A, B) mentally even before a question is asked. We consider this a \"level-2\" reasoning skill that is very different from human behavior or mental processes.\nThus, although this skill is not needed for solving the math problems and although no pre- train data teaches the model to compute \u201call-pair dependency\u201d fitting the data only requires computing necessary parameters the model still discovers it after training. This enables the model to sort relationships among the things it hears, a skill that can be useful for future tasks (via instruction fine-tuning). To our knowledge, this may be the first evidence of a language model acquiring skills beyond those needed for learning its pretrain data; and this may be a preliminary signal of where the G in AGI can come from (generalizing to skills not taught in the pretrain data).\nCorollary: the backward thinking process. A key question for AGI success is whether the \"backward thinking process\u201d (e.g., \u201cbecause I want to compute X, but X depends on Y and Y depends on Z, so let me compute Z first\") needs to be explicitly included in the training data. This differs from CoT, where CoT breaks down complex computations into simpler steps, but planning is still required to decide which step to compute first.\nOur findings suggest that, at least for grade-school math problems, with abundant data, this backward thinking process can be autonomously learned through language modeling, without need- ing to be directly included in the training data.\""}, {"title": "Result 6: Explain Model's Mistakes", "content": "We further examine the relationship between our probing results and the model's generated solu- tions, focusing on two questions: (1) When does the model answer correctly but include unnecessary"}, {"title": "Result 6 (Figure 8)", "content": "Combining these, we conclude:\n\u2022 Many reasoning mistakes made by the language model are systematic, stemming from errors in its mental process, not merely random from the generation process.\n\u2022 Some of the model's mistakes can be discovered by probing its inner states even before the model opens its mouth (i.e., before it says the first solution step).\nWe also observe that GPT-4/4o makes similar mistakes by outputting unnecessary parameters"}, {"title": "Result 7-8: Depth vs. Reasoning Length", "content": "Our controlled dataset enables a systematic exploration of the relationship between a language model's depth and its reasoning length.\nRecent studies have demonstrated that for knowledge storage and extraction, only model size matters (even for 2-layer transformers) [4]. Furthermore, both the seminal scaling-law paper by OpenAI [14] and theoretical studies in deep learning [5] suggest that model depth/width might have a minimal impact universally. Contrary to these findings, we present evidence that 19\nResult 7 (Figure 9). Language model depth is crucial for mathematical reasoning.\nSpecifically, we experimented with models of depths 4/8/12/16/20 and two sizes (a smaller size\n1 and a larger size 2).20 From Figure 9, we observe that a 4-layer transformer, even with 1920 hidden dimensions, underperforms on our math datasets. Conversely, deeper but smaller models, such as a 20-layer 576-dim, perform very well. Comparing accuracies vertically reveals a clear correlation between model depth and performance. Thus, we infer that depth is likely essential for reasoning tasks, such as solving grade-school math problems.\nNext, we try to reveal \"why\" this happens. We delved into how depth influences math problem- solving skills through the nece probing task, focusing on necessary parameters at distance t from\nthe query parameter, for $t \\in \\{1,2,...,8\\}$. These parameters all have nece(A) = true, but we can probe the model to see how correct they are at predicting nece(A) at different hidden layers.\nFigure 10 shows our result. It reveals a correlation between the model's layer hierarchy, reason- ing accuracy, and mental reasoning depth. Shallower layers excel at predicting nece(A) for param- eters A closer to the query, whereas deeper layers are more accurate and can predict nece(A) for parameters further from the query. This suggests that the model employs layer-by-layer reasoning during the planning phase to recursively identify all parameters the query depends on, and:\nResult 8 (Figure 10+14). The depth of a language model is crucial, likely due to the complexity of its hidden (mental) reasoning processes. A t-step mental reasoning, such as mentally computing nece(A) for parameters A that are a distance t from the query, may require deeper models for larger t, assuming all other hyperparameters remain constant."}, {"title": "Conclusion", "content": "We use a synthetic setting to demonstrate that language models can learn to solve grade-school math problems through true generalization, rather than relying on data contamination or template memorization. We develop probing techniques to examine the models' hidden reasoning processes. Our findings reveal that these models can learn math skills aligned with human cognitive processes, as well as \"new thinking processes\" not present in the training data. Additionally, we propose a method to predict a model's errors before it begins to solve a problem and to explain why models make mistakes when they occur. Based on this discovery, we write a separate paper to improve language models' math reasoning accuracy [21]. We also provide a principled approach to connect the model's depth to its capable reasoning length. We believe this research opens doors to study the mathematical reasoning skills of language models from a different angle compared to pushing math benchmarks.\nOne may argue that iGSM may be very different from the pretrain data that modern LLMs use. While this may be true, we are looking into the future. Recall, even GPT-4/40 of today cannot few-shot learn to solve $iGSM-med^{op=11}$ (see Figure 2). From this perspective, it is reasonable to believe that future versions of LLMs will rely on synthetic math pretrain data to improve their reasoning skills. While one may not directly use iGSM, it is tempting to use existing LLMs (such as Llama-3) to turn iGSM into more natural formats while keeping the logical chains. On the other hand, we have discovered that models trained purely on the iGSM data make similar mistakes compared to GPT-4/40 (see Section 5 and Appendix G). This further confirms that our findings do connect to practice, regarding the model's hidden reasoning process.\nFinally, Part 2 of this work series focuses on how language models solve grade-school math problems (including Part 2.2 [21]). We also cover how language models learn language structures in Part 1 [1] (in particular, how they mentally perform dynamical programming), and learn world knowledge in Part 3 [2-4]."}, {"title": "Result 1 \u2014 An Example in iGSM-hard with op = 21", "content": "(Problem- A Hard Example) The number of each Jungle Jim's International Market's Cheese equals the sum of each Parmesan Cheese's Pear\nand each The Fresh Market's Ice Cream. The number of each Ice Cream's Pineapple equals 2 more than each Goat Cheese's Grape. The number\nof each New Seasons Market's Goat Cheese equals the sum of each Residential College District's Jungle Jim's International Market, each Jungle\nJim's International Market's Parmesan Cheese and each Residential College District's Supermarket. The number of each Arts Campus's New\nSeasons Market equals each Cheese's Pineapple. The number of each Goat Cheese's Banana equals each Vocational School District's Product.\nThe number of each Residential College District's Jungle Jim's International Market equals 5 more than each Ice Cream's Grape. The number\nof each Parmesan Cheese's Pineapple equals each Parmesan Cheese's Pear. The number of each Residential College District's The Fresh Market\nequals each Arts Campus's Trader Joe's. The number of each Arts Campus's Trader Joe's equals each Parmesan Cheese's Ingredient. The\nnumber of each Goat Cheese's Grape equals 0. The number of each The Fresh Market's Ice Cream equals 13 more than the difference of each\nResidential College District's The Fresh Market and each Parmesan Cheese's Grape. The number of each Goat Cheese's Pineapple equals each\nNew Seasons Market's Product. The number of each Vocational School District's The Fresh Market equals the sum of each Trader Joe's's Cheese\nand each The Fresh Market's Cheese. The number of each Trader Joe's's Cheese equals 6. The number of each The Fresh Market's Cheese\nequals 3. The number of each Jungle Jim's International Market's Ice Cream equals the difference of each Ice Cream's Banana and each Goat\nCheese's Grape. The number of each Jungle Jim's International Market's Parmesan Cheese equals each Ice Cream's Pineapple. The number of\neach Parmesan Cheese's Pear equals the difference of each Goat Cheese's Grape and each Ice Cream's Grape. The number of each Parmesan\nCheese's Grape equals 12 times as much as each Residential College District's Jungle Jim's International Market. The number of each The Fresh\nMarket's Parmesan Cheese equals each The Fresh Market's Cheese. The number of each Ice Cream's Banana equals the sum of each Parmesan\nCheese's Pineapple and each Ice Cream's Pineapple. The number of each School District's Jungle Jim's International Market equals each The\nFresh Market's Ice Cream. The number of each Cheese's Pineapple equals 20 more than the sum of each Trader Joe's's Cheese and each The\nFresh Market's Cheese. The number of each Trader Joe's's Parmesan Cheese equals 16. The number of each Ice Cream's Pear equals 8. The\nnumber of each Ice Cream's Grape equals each Goat Cheese's Grape. How many Product does School District have?\n(Solution- A Hard Example) Define Goat Cheese's Grape as u; so u = 0. Define Ice Cream's Grape as x; so x = u = 0. Define Residential\nCollege District's Jungle Jim's International Market as N; so N = 5 + x = 5+0=5. Define Parmesan Cheese's Pear as G; so G = u- x = 0 -\n0=0. Define Parmesan Cheese's Grape as f; so f = 12 * N = 12 * 5 = 14. Define Parmesan Cheese's Pineapple as C; so C= G = 0. Define\nParmesan Cheese's Ingredient as Z; Z f + C"}]}