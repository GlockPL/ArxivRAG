{"title": "Physics of Language Models: Part 2.1,\nGrade-School Math and the Hidden Reasoning Process", "authors": ["Tian Ye", "Zicheng Xu", "Yuanzhi Li", "Zeyuan Allen-Zhu"], "abstract": "Recent advances in language models have demonstrated their capability to solve mathematical reasoning problems, achieving near-perfect accuracy on grade-school level math benchmarks like GSM8K. In this paper, we formally study how language models solve these problems. We design a series of controlled experiments to address several fundamental questions: (1) Can language models truly develop reasoning skills, or do they simply memorize templates? (2) What is the model's hidden (mental) reasoning process? (3) Do models solve math questions using skills similar to or different from humans? (4) Do models trained on GSM8K-like datasets develop reasoning skills beyond those necessary for solving GSM8K problems? (5) What mental process causes models to make reasoning mistakes? (6) How large or deep must a model be to effectively solve GSM8K-level math questions?\nOur study uncovers many hidden mechanisms by which language models solve mathematical questions, providing insights that extend beyond current understandings of LLMs.", "sections": [{"title": "Introduction", "content": "The field of language models has made significant progress in recent years. Large models like GPT-4 [17] have shown initial signs of general intelligence [8], while smaller models have demonstrated good reasoning abilities by solving challenging coding and math problems [11, 15, 16].\nIn this paper, we focus on the ability of small language models to solve grade-school math problems. Unlike previous works that empirically push the accuracy of models on grade-school math benchmarks like GSM8K [9] and its augmentations (e.g., [16, 22]), we take a more principled approach. We aim to understand the following fundamental questions:\n1. How do language models learn to solve grade-school level math problems? Do they just memorize templates, or do they learn reasoning skills similar to humans? Or do they discover new skills to solve the problems?\n2. Do models trained solely on grade-school math problems only learn to solve these problems, or do they develop some more general intelligence?\n3. How small can a language model be while still solving grade-school math problems? Is depth (number of layers) more important than width (number of neurons per layer), or does only size matter as suggested by practitioners [14]?\nThese questions are fundamental to understanding the intelligence of language models. To study them, it might seem tempting to start with a pre-trained model and fine-tune it on existing datasets like GSM8K or GPT-4 augmented ones (e.g., [16, 22]). However, this approach has significant limitations:\n\u2022 DATA CONTAMINATION. The pretrain data of existing models mostly come from publicly available internet [10], which is a pile of mess. We do not know how many math problems are included or their structures. There is significant concern regarding whether the GSM8K benchmark has been leaked to language models' training datasets [22]. Even if the exact data is not, the pre-trained model might have seen almost identical questions (e.g., the same problem with different numbers). Thus, this approach cannot answer questions 1-3. We do not know whether a model truly learns the reasoning skills or it simply memorizes problem templates during training. Therefore, we need full control over the model's pretrain data and must train a language model from scratch. This point has been reiterated recently in [2, 3].\n\u2022 SOLUTION DIVERSITY. The existing fine-tuning data, such as the GSM8K training set, contains only 7.5K grade-school math problems, which is insufficient to train a model from scratch. Although recent works use GPT-4 to augment GSM8K, this is not enough for our purpose. GPT-4 augmented problems might be biased towards a small number of solution templates, since the original GSM8K data has very few (obviously, at most 8K) solution templates. We need a much larger, more diverse set of grade-school math problems.\nWith these points in mind, we introduce our framework to generate a large set of diverse grade-school math (GSM) problems and use the dataset to train (from scratch) and test a GPT2-like language model. In the framework, we focus on the \"logical reasoning\" aspect of grade-school math problems, which involves the dependency of parameters in the problem statement, such as \"Alice's apple is three times the sum of Bob's orange and Charles's banana.\" We use synthetic sentences to reduce the difficulty arising from Common Sense, like \"a candle burned for 12 hours at 1 inch per hour\" (implying the candle is reducing in length). We also remove the difficulty from pure"}, {"title": "Result 1: Data Generation", "content": "Motivation. Recall a standard grade-school math problem in the GSM8K dataset [9] looks like:\nBetty is saving money for a new wallet which costs 100. Betty has only half of the money she needs. Her parents decided to give her 15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\nThis problem involves multiple parameters whose values are connected through various equalities, such as \"Betty's current money = 0.5 \u00d7 cost of the wallet\" and \"money given by grandparents =\n2 \u00d7 money given by parents.\" Motivated by this, we build a GSM8K-like math dataset through a synthetic generation pipeline that captures the dependencies of parameters. We wish to capture at least the following three types of dependencies.\n1. Direct dependency (\u2661): such as A = 5 \u00d7 (X + Y), so A can be computed after X and Y.\n2. Instance dependency (\u2660): such as \u201cevery classroom has X chairs, and there are Y classrooms.\u201d\nHere, the model must infer the total number of chairs by multiplying X by Y.\n3. Implicit dependency (\u2663): such as \u201cBob has 3 times more fruits than Alice. Alice has 3 apples, 4 eggs and 2 bananas.\" Here, the model must learn that apples and bananas are fruits and egg is not, and \"Alice's fruits\" is an abstract parameter derived from the problem statement."}, {"title": "Step 1: Graph Construction and Problem Generation", "content": "Hierarchical categorization. We use a layered structure of categories, each contains possible items. For instance, categories = (School, Classroom, Backpack) has three layers; category School =\n{Central High, Riverview High, . . . }; category Classroom = {Dance Studio, Film Studio, . . . }; category Backpack = {School Daypack, Messenger Backpack, ...}. We prepare 4 predefined hierarchical categorizations, each with 4 layers and 100 items in each layer; this represents the world knowledge."}, {"title": "Step 2: Solution Construction (CoT)", "content": "Let solution be a sequence of sentences describing the necessary steps towards solving the given problem, where the sentences follow any topological order also known as Chain-of-Thought, CoT. For each parameter necessary towards answering the final question, we assign to it a random letter among the 52 choices (a..z or A..Z), and use a sentence to describe its computation:\nDefine [param] as X; [intermediate steps]; so X =\nThroughout this paper, we consider arithmetics mod 23 to avoid errors from computation involving large numbers. It is perhaps the easiest to directly see a solution example (corresponding to (2.1)), and a more involved example is in Figure 11:\n(Solution - Easy) Define Dance Studio's School Daypack as p; so p = 17. Define Film Studio's Messenger Backpack as W; so W = 13. Define Central High's Film Studio as B; so B = p + W = 17 + 13 = 7. Define Film Studio's School Daypack as g; R = W + B = 13 + 7 = 20; so g = 12 + R = 12 + 20 = 9. Define Film Studio's Backpack as w; so w=g+W= 9 + 13 = 22. Define Central High's Backpack as c; so c = B * w = 7 * 22 = 16. Answer: 16.\nWe emphasize that:\n\u2022 The solution only contain parameters necessary towards calculating the final query parameter.\n\u2022 The solution follows the correct logical order: i.e. all the parameters used in the calculation must have appeared and been computed beforehand.\n\u2022 We break computations to binary ops: g = 12+13+7 is broken into g = 12+R and R = 13+7 in the above solution. The number of semicolons \";\" equals the number of operations. This reduces the arithmetic complexity of the solution, which is not the focus of this paper."}, {"title": "Difficulty Control", "content": "Although deferring all the pseudocode to Appendix D, we summarize below the main randomness used in the data generation process. This includes the random choice of a hierarchical categorization (i.e., the English part); a structure graph (i.e., the instance parameters); a dependency graph; arithmetic computations on the dependency graph; integer numbers (i.e., the RNG); problem sentence permutation; and the query parameter.\nWe use two parameters to control data's difficulty: ip is the number of instance parameters, and op is the number of solution operations; the data's difficulty is an increasing function over them. We call our dataset iGSM, to reflect the nature that such synthetic dataset can be of infinite size. We use $\\text{iGSM}^{\\text{op}\\leq op,\\text{ip}\\leq ip}$ to denote the data generated with constraint op < op and ip \u2264 ip, and use $\\text{iGSM}^{\\text{op}=op,\\text{ip}\\leq ip}$ to denote those restricting to op = op."}, {"title": "Train and Test Datasets", "content": "We consider two families of datasets.\n\u2022 In the iGSM-med data family we use ip < 20.\nThe training data is $\\text{iGSM-med}^{\\text{op}\\leq 15} \\stackrel{\\text{def}}{=} \\text{iGSM}^{\\text{op}\\leq 15,\\text{ip}\\leq 20}$. We evaluate the pretrained model both in-distribution, on $\\text{iGSM-med}^{\\text{op}\\leq 15}$ and $\\text{iGSM-med}^{\\text{op}=15}$, and out-of-distribution (OOD),"}, {"title": "Result 2-3: Summarize Model's Behavior Process", "content": "We use the GPT2 architecture [18] but replacing its absolute positional embedding with rotary embedding [7, 20], yet still referring to it as GPT2 for short. We mostly stick to the 12-layer, 12-head, 768-dim GPT2 (a.k.a. GPT2-small) for experiments, but we explore larger models in Section 6. We use a context length of 768 / 1024 for pretraining on iGSM-med/iGSM-hard and 2048 for evaluation. More details are in Appendix F."}, {"title": "Result 2: accuracy", "content": "After sufficient pre-training, we give the model a problem from the test set (without solution) and let it continue to generate (allegedly a solution followed by an answer). Because we have restricted ourselves to a fixed solution format, language models can learn the format easily, allowing us to write a solution parser to check if the solution is fully correct."}, {"title": "Result 3: solution redundancy", "content": "We examine whether GPT2 achieves high accuracy by\n\u2022 brute-forcedly computing all the parameters during generation (a \u201clevel-0\" reasoning skill), or\n\u2022 computing only necessary parameters to give shortest solutions (a \u201clevel-1\" reasoning skill).\nRecall our iGSM (pretrain) data only contains necessary solution steps (i.e., CoT) to simulate what we see in textbook solutions for math problems. For instance, if a problem describes X =3+2, \u0415 =3+X, Y =X+2 and asks for the value of Y, then a shortest solution would be \u201cX =3+2=5 and Y =X+2 =7\" without ever computing E.\nResult 3. Figure 4 shows that GPT2 predominantly solves the iGSM problems with a \u201clevel-1\u201d reasoning skill, avoiding unnecessary computations, even when evaluated out-of-distribution.\nThis finding is significant as it suggests that, unlike humans who usually rely on \"backward rea- soning\" and a scratch pad to write down necessary parameters by backtracking the dependencies"}, {"title": "Result 4-5: Discover Model's Mental Process", "content": "To understand how the model learns to solve math problems, we propose studying the following probing tasks, which align closely with human problem-solving strategies:\n\u2022 nece(A): if parameter A is necessary for computing the answer.\n\u2022 dep(A, B): if parameter A (recursively) depends on parameter B given the problem statement.\n\u2022 known(A): if parameter A has already been computed.\n\u2022 value(A): the value of parameter A (a number between 0-22, or 23 if known(A) = false).\n\u2022 can_next(A): if A can be computed in the next solution sentence (namely, its predecessors have all been calculated). Note that A might not be necessary to answer the question.\n\u2022 nece_next(A): if parameter A satisfies both can_next(A) and nece(A).\nFor a model to generate the shortest solutions, it must identify nece(A) for all A's in its mental process. This is because whether nece(A) is true directly corresponds to whether there is a solution sentence to compute A. However, how early does the model recognize this, and how is it stored?\nSimilarly, does it recognize dependencies between parameters (dep)? If so, how early is this mental process completed? Moreover, in the middle of solution generation, does the model keep track of each parameter A's value at all times (value, known)? Does the model mentally know all possible parameters A that are ready to compute in the next sentence (can_next)? Or does it only focus on A that is both ready and necessary (nece_next)?\nThis section proposes probing technique to answer all of these questions."}, {"title": "V-Probing: A Nearly-Linear Probing Method", "content": "As illustrated in Figure 5, we conduct probing at the end of the problem description for the dep task, and end of the question description nece task. For other tasks, we probe them at the end of every solution sentence (including the start of the first solution sentence).\nRecall that standard linear probing involves freezing a pretrained language model and checking if a property is linearly encoded at a hidden layer (usually the last layer) for a given token position. This is done by introducing a trainable linear classifier on the hidden states and performing a lightweight finetuning task for this property (see [12] and references therein).\nOur setting is more complex because the properties have one or two conditional variables, A and B, described in plain English. To handle this, we truncate the math problems to the probing position and append tokens [START] and [END] around the descriptions of A (or A, B). We then probe from the token position of [END] to see if the property is linearly encoded at the last layer.\nUnlike standard linear probing, to account for the input change, we introduce a small trainable rank-8 (linear) update on the input embedding layer. We freeze the pretrained language model and finetune both the linear classifier and the rank-8 update for the desired property. We refer to this as V(ariable)-probing and provide details in Appendix B. An illustration of the nece(A) probing task is shown in Figure 6.\nWe compute the V-probing accuracies on a language model pretrained from iGSM and compare them with the V-probing accuracies on a randomly-initialized transformer model. If the former accuracies are significantly higher, we conclude that the probing signals must have (or be very close to having) come from the pretrained weights, rather than the (lightweight) finetuning stage."}, {"title": "Probing Results and Findings", "content": "We present our probing results in Figure 7. The probing accuracies are high for all the tasks, compared to majority guess and random-model probing except for the very hard OOD cases"}, {"title": "Result 4: model solves math problems like humans", "content": "We make the following observations:\n\u2022 When generating solutions, the model not only remembers which parameters have been computed and which have not (value, known) but also knows which parameters can be computed next (can_next, nece_next). These abilities ensure that the model can solve the given math problem step by step, similar to human problem-solving skills.\n\u2022 By the end of the problem description, the model already knows the full list of necessary parameters (nece). This indicates that the model has learned to plan ahead, identifying nec- essary parameters before starting to generate the solution. This aligns with human behavior, except that the model plans mentally while humans typically write this down. This further confirms that the model reaches the \u201clevel-1\" reasoning skill discussed in Section 3."}, {"title": "Result 5: model learns beyond human reasoning skills", "content": "Remarkably, the model learns dep(A, B) and can_next(A), even for parameters A not necessary for answering the question, as shown in Figure 7(b). This differs from human problem-solving, where we typically use back- ward reasoning from the question to identify necessary parameters, often overlooking unnecessary ones [19]. In contrast, language models can pre-compute the all-pair dependency graph dep(A, B) mentally even before a question is asked. We consider this a \"level-2\" reasoning skill that is very different from human behavior or mental processes.\nThus, although this skill is not needed for solving the math problems and although no pre- train data teaches the model to compute \u201call-pair dependency\u201d fitting the data only requires computing necessary parameters the model still discovers it after training. This enables the model to sort relationships among the things it hears, a skill that can be useful for future tasks (via instruction fine-tuning). To our knowledge, this may be the first evidence of a language model acquiring skills beyond those needed for learning its pretrain data; and this may be a preliminary signal of where the G in AGI can come from (generalizing to skills not taught in the pretrain data).\nCorollary: the backward thinking process. A key question for AGI success is whether the \"backward thinking process\u201d (e.g., \u201cbecause I want to compute X, but X depends on Y and Y depends on Z, so let me compute Z first\") needs to be explicitly included in the training data. This differs from CoT, where CoT breaks down complex computations into simpler steps, but planning is still required to decide which step to compute first.\nOur findings suggest that, at least for grade-school math problems, with abundant data, this backward thinking process can be autonomously learned through language modeling, without need- ing to be directly included in the training data."}, {"title": "Result 6: Explain Model's Mistakes", "content": "We further examine the relationship between our probing results and the model's generated solu- tions, focusing on two questions: (1) When does the model answer correctly but include unnecessary"}, {"title": "Result 6 (Figure 8)", "content": "Combining these, we conclude:\n\u2022 Many reasoning mistakes made by the language model are systematic, stemming from errors in its mental process, not merely random from the generation process.\n\u2022 Some of the model's mistakes can be discovered by probing its inner states even before the model opens its mouth (i.e., before it says the first solution step).\nWe also observe that GPT-4/4o makes similar mistakes by outputting unnecessary parameters"}, {"title": "Result 7-8: Depth vs. Reasoning Length", "content": "Our controlled dataset enables a systematic exploration of the relationship between a language model's depth and its reasoning length.\nRecent studies have demonstrated that for knowledge storage and extraction, only model size matters (even for 2-layer transformers) [4]. Furthermore, both the seminal scaling-law paper by OpenAI [14] and theoretical studies in deep learning [5] suggest that model depth/width might have a minimal impact universally. Contrary to these findings, we present evidence that Result 7 (Figure 9). Language model depth is crucial for mathematical reasoning.\nSpecifically, we experimented with models of depths 4/8/12/16/20 and two sizes (a smaller size 1 and a larger size 2). From Figure 9, we observe that a 4-layer transformer, even with 1920 hidden dimensions, underperforms on our math datasets. Conversely, deeper but smaller models, such as a 20-layer 576-dim, perform very well. Comparing accuracies vertically reveals a clear correlation between model depth and performance. Thus, we infer that depth is likely essential for reasoning tasks, such as solving grade-school math problems.\nNext, we try to reveal \"why\" this happens. We delved into how depth influences math problem- solving skills through the nece probing task, focusing on necessary parameters at distance t from the query parameter, for t \u2208 {1,2,...,8}. These parameters all have nece(A) = true, but we can probe the model to see how correct they are at predicting nece(A) at different hidden layers.\nFigure 10 shows our result. It reveals a correlation between the model's layer hierarchy, reason- ing accuracy, and mental reasoning depth. Shallower layers excel at predicting nece(A) for param- eters A closer to the query, whereas deeper layers are more accurate and can predict nece(A) for parameters further from the query. This suggests that the model employs layer-by-layer reasoning during the planning phase to recursively identify all parameters the query depends on, and:\nResult 8 (Figure 10+14). The depth of a language model is crucial, likely due to the complexity of its hidden (mental) reasoning processes. A t-step mental reasoning, such as mentally computing nece(A) for parameters A that are a distance t from the query, may require deeper models for larger t, assuming all other hyperparameters remain constant."}, {"title": "Conclusion", "content": "We use a synthetic setting to demonstrate that language models can learn to solve grade-school math problems through true generalization, rather than relying on data contamination or template memorization. We develop probing techniques to examine the models' hidden reasoning processes. Our findings reveal that these models can learn math skills aligned with human cognitive processes, as well as \"new thinking processes\" not present in the training data. Additionally, we propose a method to predict a model's errors before it begins to solve a problem and to explain why models make mistakes when they occur. Based on this discovery, we write a separate paper to improve language models' math reasoning accuracy [21]. We also provide a principled approach to connect the model's depth to its capable reasoning length. We believe this research opens doors to study the mathematical reasoning skills of language models from a different angle compared to pushing math benchmarks.\nOne may argue that iGSM may be very different from the pretrain data that modern LLMs use. While this may be true, we are looking into the future. Recall, even GPT-4/40 of today cannot few-shot learn to solve iGSM-medop=11 (see Figure 2). From this perspective, it is reasonable to believe that future versions of LLMs will rely on synthetic math pretrain data to improve their reasoning skills. While one may not directly use iGSM, it is tempting to use existing LLMs (such as Llama-3) to turn iGSM into more natural formats while keeping the logical chains. On the other hand, we have discovered that models trained purely on the iGSM data make similar mistakes compared to GPT-4/40 (see Section 5 and Appendix G). This further confirms that our findings do connect to practice, regarding the model's hidden reasoning process.\nFinally, Part 2 of this work series focuses on how language models solve grade-school math problems (including Part 2.2 [21]). We also cover how language models learn language structures in Part 1 [1] (in particular, how they mentally perform dynamical programming), and learn world knowledge in Part 3 [2-4]."}, {"title": "Result 1", "content": "An Example in iGSM-hard with op = 21\n(Problem- A Hard Example) The number of each Jungle Jim's International Market's Cheese equals the sum of each Parmesan Cheese's Pear and each The Fresh Market's Ice Cream. The number of each Ice Cream's Pineapple equals 2 more than each Goat Cheese's Grape. The number of each New Seasons Market's Goat Cheese equals the sum of each Residential College District's Jungle Jim's International Market, each Jungle Jim's International Market's Parmesan Cheese and each Residential College District's Supermarket. The number of each Arts Campus's New Seasons Market equals each Cheese's Pineapple. The number of each Goat Cheese's Banana equals each Vocational School District's Product. The number of each Residential College District's Jungle Jim's International Market equals 5 more than each Ice Cream's Grape. The number of each Parmesan Cheese's Pineapple equals each Parmesan Cheese's Pear. The number of each Residential College District's The Fresh Market equals each Arts Campus's Trader Joe's. The number of each Arts Campus's Trader Joe's equals each Parmesan Cheese's Ingredient. The number of each Goat Cheese's Grape equals 0. The number of each The Fresh Market's Ice Cream equals 13 more than the difference of each Residential College District's The Fresh Market and each Parmesan Cheese's Grape. The number of each Goat Cheese's Pineapple equals each New Seasons Market's Product. The number of each Vocational School District's The Fresh Market equals the sum of each Trader Joe's's Cheese and each The Fresh Market's Cheese. The number of each Trader Joe's's Cheese equals 6. The number of each The Fresh Market's Cheese equals 3. The number of each Jungle Jim's International Market's Ice Cream equals the difference of each Ice Cream's Banana and each Goat Cheese's Grape. The number of each Jungle Jim's International Market's Parmesan Cheese equals each Ice Cream's Pineapple. The number of each Parmesan Cheese's Pear equals the difference of each Goat Cheese's Grape and each Ice Cream's Grape. The number of each Parmesan Cheese's Grape equals 12 times as much as each Residential College District's Jungle Jim's International Market. The number of each The Fresh Market's Parmesan Cheese equals each The Fresh Market's Cheese. The number of each Ice Cream's Banana equals the sum of each Parmesan Cheese's Pineapple and each Ice Cream's Pineapple. The number of each School District's Jungle Jim's International Market equals each The Fresh Market's Ice Cream. The number of each Cheese's Pineapple equals 20 more than the sum of each Trader Joe's's Cheese and each The Fresh Market's Cheese. The number of each Trader Joe's's Parmesan Cheese equals 16. The number of each Ice Cream's Pear equals 8. The number of each Ice Cream's Grape equals each Goat Cheese's Grape. How many Product does School District have?\n(Solution- A Hard Example) Define Goat Cheese's Grape as u; so u = 0. Define Ice Cream's Grape as x; so x = u = 0. Define Residential College District's Jungle Jim's International Market as N; so N = 5 + x = 5+0 = 5. Define Parmesan Cheese's Pear as G; so G = u - x = 0 - 0 = 0. Define Parmesan Cheese's Grape as f; so f = 12 * N = 12 * 5 = 14. Define Parmesan Cheese's Pineapple as C; so C = G = 0. Define Parmesan Cheese's Ingredient as Z; Z = f + C = 14+0 = 14; so Z = e + G = 14+0 = 14. Define Arts Campus's Trader Joe's as q; so q = Z = 14. Define Residential College District's The Fresh Market as j; so j = q = 14. Define Ice Cream's Pineapple as X; so X = 2 + u = 2+0 = 2. Define Ice Cream's Banana as K; so K = C+X=0+2= 2. Define The Fresh Market's Ice Cream as P; i = j - f = 14-14 = 0; so P = 13+i = 13+0 = 13. Define Jungle Jim's International Market's Ice Cream as R; so R = K-u = 2 - 0 = 2. Define School District's Jungle Jim's International Market as V; so V = P = 13. Define Jungle Jim's International Market's Cheese as v; so v =G+P=0+13 = 13. Define Jungle Jim's International Market's Parmesan Cheese as S; so S = X = 2. Define Jungle Jim's International Market's Product as y; U = S + R = 2 + 2 = 4; so y = U + v = 4 + 13 = 17. Define School District's Product as J; so J = V * y = 13 * 17 = 14. Answer: 14."}, {"title": "Results 4-5", "content": "Details on V-probing\nRecall that we wish to conduct probing at the end of the problem description for the nece and dep tasks (before the solution for nece; before the solution or even the question for dep). For other tasks, we probe at the end of every solution sentence (including the start of the first solution sentence). The goal is to freeze a pretrained language model, then introduce a very small number of additional trainable parameters on top of it, and finetune them for each probing task.\nSpecifically, we take a pretrained language model, e.g., pretrained from the iGSM-hard training data. We freeze its parameters completely except for adding a trainable rank-r update on the em- bedding layer to account for the task change (from next-token prediction to probing). Throughout this paper we use a small value r = 8. We feed this network with training data that are the same as iGSM-hard, but truncated at exactly the position we wish to probe. Importantly, we append such inputs with a special starting token [START] along with a parameter name (or two names, if it is the dep(A, B) task). We then extract the hidden states of the last token position at the last transformer layer, and add a trainable linear layer (a.k.a. linear head) to perform classification for one of the six probing tasks.\nThis probing method is illustrated in Figure 13. We call it V(ariable)-Probing, because it can take an arbitrary number of variables (i.e., parameters in this paper) to allow us to perform functional probing inside the transformer.\nNote, if it were only a trainable linear head such probing would be called linear probing [12]. Unlike traditional linear probing, we are adding a small low-rank update on the model's embedding layer. This is arguably the minimum change needed (to account for the task change, for special tokens like [START] [MID] [END], etc.) in order to perform any non-trivial probing. This is related but different from the nearly-linear probing methods introduced in Allen-Zhu and Li [1, 3], because they do not support taking variables as probing inputs.\nUnbalanced probing tasks. Our probing accuracies for the six tasks were presented in Figure 7. However, we notice that the dep and nece next tasks have unbalanced labels even guessing \"all false\" would give 83% accuracy for dep(A, B) and 92% for nece_next(A). For such reason, we also present their probing accuracies restricted to positives/negatives labels separately in Figure 12."}, {"title": "Result 8", "content": "Additional Figure"}, {"title": "Result 1 Details", "content": "Math Data Generation\nOur math data generation process consists of first generating the structure graph (see Figure 1 and 11 left), which defines the set of parameters we shall use; then generating the dependency graph (see Figure 1 and 11 right), which defines the arithmetic relationship between the parameters; and finally generating the English problem and solution descriptions.\nNotations. In this section, to make the description concise, when we say \"randomly sampling\" in the pseudocode, we mean uniform random unless otherwise noted. Whenever we consider a (directed) graph G, slightly abusing notation, we write a \u2208 G to indicate that a is a vertex in G and (a \u2192 b) \u2208 G to indicate that there is an edge from a to b in G."}, {"title": "Generate Structure Graph", "content": "Recall the structure graph (see Figure 1 and 11 left) describes the set of possible items (nodes) and instance parameter (edges) that we shall rely on to construct our math problem.\nWe use Gs to denote such structure graph, and it is generated Gs = DrawStructure(e, d, wo, w1) from a random distribution defined with hyperparameters e,d, wo, w1 \u2208 N. At a high level, we construct Gs so that it has d layers, e edges, and each layer has between wo and w\u2081 items. Specifically, suppose li \u2208 {wo, wo + 1, ..., w\u2081} represents the number of items for each layer i. In this configuration, one must have at least $\\overline{e} = l_1 + \\cdots + l_d$ edges to ensure the graph is \"connected\", and at most $e^{+} = l_1l_2 + \\cdots + l_{d-1}l_d$ edges. Using this formula, we first randomly choose a configuration (11, ..., la) so that $\\overline{e} \\leq e < e^{+}$ for the given parameter e. Then, after the configuration is chosen, we randomly generate edges accordingly. Details are given in Algorithm 1."}, {"title": "Draw Structure", "content": "Gs = DrawStructure(e", "W1)\nInput": "e", "1)w\n1": "l \u2190 (wo", "i\n2": "p \u2190 uniform random from (0", "1)\n3": "while l \u2260 (W1", "do\n4": "e\u00af", "give\n5": "if e\u207a < e then\n6: | randomly select i \u2208 [d", "1.\n7": "else if e = e then\n8: | break\n9: else if randomly choose a number in (0", "then\n10": "randomly select i \u2208 [d", "1.\n11": "else\n12: | break\n13: end\n\u25c7 after while loop", "d": "wo \u2264 li \u2264 W1\n14: Construct Gs with exactly li items on layer i \u2208 [d", "n15": "for each item a in each layer i \u2265"}]}