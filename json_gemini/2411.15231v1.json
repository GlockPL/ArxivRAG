{"title": "IterIS: Iterative Inference-Solving Alignment for LoRA Merging", "authors": ["Hongxu Chen", "Runshi Li", "Bowei Zhu", "Zhen Wang", "Long Chen"], "abstract": "Low-rank adaptations (LoRA) are widely used to fine-tune large models across various domains for specific downstream tasks. While task-specific LoRAs are often available, concerns about data privacy and intellectual property can restrict access to training data, limiting the acquisition of a multi-task model through gradient-based training. In response, LoRA merging presents an effective solution by combining multiple LoRAs into a unified adapter while maintaining data privacy. Prior works on LORA merging primarily frame it as an optimization problem, yet these approaches face several limitations, including the rough assumption about input features utilized in optimization, massive sample requirements, and the unbalanced optimization objective. These limitations can significantly degrade performance. To address these, we propose a novel optimization-based method, named IterIS: 1) We formulate LORA merging as an advanced optimization problem to mitigate the rough assumption. Additionally, we employ an iterative inference-solving framework in our algorithm. It can progressively refine the optimization objective for improved performance. 2) We introduce an efficient regularization term to reduce the need for massive sample requirements (requiring only 1-5% of the unlabeled samples compared to prior methods). 3) We utilize adaptive weights in the optimization objective to mitigate potential unbalances in LoRA merging process. Our method demonstrates significant improvements over multiple baselines and state-of-the-art methods in composing tasks for text-to-image diffusion, vision-language models, and large language models. Furthermore, our layer-wise algorithm can achieve convergence with minimal steps, ensuring efficiency in both memory and computation.", "sections": [{"title": "1. Introduction", "content": "Recent advancements have produced increasingly larger pre-trained models with notable performance improvements [2, 19, 21, 31, 32, 35, 41]. However, their size makes full fine-tuning resource-intensive. To mitigate this, various parameter-efficient fine-tuning methods [9] have emerged. Especially, low-rank adaptation tuning (LoRA-tuning) [12] stands out for its effectiveness. By learning low-rank adaptations within linear layers, LoRA-tuning efficiently captures task-specific information, making it a popular choice for fine-tuning large models. As shown in Figure 1, LoRA-tuning can be effectively applied to various models, including text-to-image diffusion models, vision-language models, and large language models [44, 50].\nFor multi-task applications, a common approach is learning a set of task-specific LoRAs, and each for a specific task. Take the multi-concept customization task as an example (cf., Figure 1(a)), we learn a single LoRA for each visual concept. Then, these specialized LoRAs are applied separately to manage each task. However, when the specific type of testing task is unknown, the model cannot automatically identify and utilize the corresponding LoRA. Therefore, a straightforward improvement is to develop a single unified adapter, which is capable of managing multiple tasks. As illustrated in Figure 2(b), we can acquire a unified adapter by learning from the mixed dataset across multiple tasks. However, gradient-based training on large models requires substantial computational resources and time. Moreover, data privacy and intellectual property concerns often limit access to training datasets, further complicating the creation of the unified adapter.\nTo bypass gradient-based training and preserve data privacy, some LoRA merging methods [8, 13, 15, 18, 30, 39, 48, 51] have been proposed. These methods usually formulate LoRA merging as an optimization problem. As shown in Figure 2(c), they assume all LoRAs share the same pre-trained model, enabling a training-free and layer-wise merging process to acquire each unified adapter. The most intuitive way is linear merging, i.e., linearly combining the parameters of multiple LoRAs [48, 51]. However, the feasibility of it is under the isotropic assumption: the inputs features to LoRAs follow isotropic distributions. Since this simplified assumption often does not hold in practice, linear merging generally results in serious performance degradation. To move beyond the isotropic assumption, some real-distribution-based merging methods are proposed [8, 15, 18, 39]. These methods formulate the optimization objective utilizing these features acquired from the real distribution. Specifically, they generally involve three steps: 1) Extract input features for LoRAs by performing inference on samples. 2) Utilize these features to formulate each optimization objective, aiming to align the output features of the unified adapter with those of each LORA (cf., Figure 2(c)). 3) Acquire each unified adapter using the closed-form solution.\nIn this paper, we argue that almost all existing real-distribution-based merging methods still have three overlooked limitations:\n\u2022 Rough Assumption\u00b9: They assume the input features for each individual LoRA to be those of the unified adapter. As Figure 3(a) shows, the discrepancy (measured by Frobenius distance) between approximated and true features increases with encoder layer depth. This rough approximation significantly limits overall performance.\n\u2022 Massive Sample Requirement: They usually require massive samples (cf., Figure 3(b)) to enhance algorithm robustness. However, obtaining such a large sample size is often impractical in real-world applications and leads to increased computational time and resource consumption.\n\u2022 Unbalanced Optimization: Significant variations in the magnitudes of output features across task-specific LoRAS can introduce bias into the optimization process as illustrated in Figure 3(c). It potentially leads to unbalanced performance within tasks.\nTo address these limitations, we propose a novel real-distribution-based merging method, IterIS. To relax the rough assumption\u00b9, besides the features for LoRAs, it directly extracts the input features for each unified adapter. These features are then used to formulate our optimization objective, providing a more accurate representation compared to previous methods. Additionally, we employ an iterative inference-solving framework to progressively refine the objective. Specifically, in each iteration: (i) We extract the input features for each unified adapter through inference. (ii) We then update each objective using these features. (iii) Finally, we obtain each unified adapter from the closed-form solution for the next iteration. Furthermore, to address the massive sample requirement, we introduce an efficient regularization term in IterIS, which significantly reduces the number of required samples, only needing 1-5% of the unlabeled samples compared to prior methods. Additionally, to account for potential unbalance in the optimization, adaptive weights are introduced to balance the magnitudes of the terms in the objective.\nBenefiting from the directed acyclic structure of deep learning models, IterIS converges with minimal iterations. Besides, its layer-wise updating can ensure both computational and memory efficiency. We evaluate our methods on various domains, demonstrating its capability for LORA merging. In summary, our contributions are threefold: 1) We propose IterIS, a novel and versatile LoRA merging algorithm for various domains, supporting applications on text-to-image diffusion, vision-language models, and large language models; 2) We introduce an advanced representation of LoRA merging by employing a progressively refined optimization objective and an iterative inference framework. 3) We empirically demonstrate the effectiveness of IterIS, highlighting its significant improvements in LoRA merging over previous methods."}, {"title": "2. Related Work", "content": "Parameter-Efficient Fine-Tuning (PEFT) [9] enables pre-trained models to adapt to downstream tasks with minimal additional parameters, reducing both computational overhead and storage needs. Recent PEFT methods include prompt tuning [4, 14, 23], adapter tuning [11, 49], and low-rank adaptation tuning (LoRA-tuning) [12]. Prompt tuning introduces a set of task-specific learnable tokens, while adapter tuning adds lightweight modules between layers to capture task-specific knowledge. Among various PEFT methods, LoRA-tuning has become one of the most widely adopted techniques. It has been widely applied for diffusion models and large language models for various downstream tasks [8, 12, 50, 51]. By incorporating low-rank decomposition matrices into select layers, LoRA captures task-specific nuances efficiently. Its memory and computational efficiency make it a powerful solution for fine-tuning large models.\nLORA Merging [8, 13, 15, 18, 30, 39, 48, 51] is a technique designed to combine multiple LoRAs into a single unified adapter. The simplest approach referred to as linear merging [48], involves taking a linear combination of individual LoRAs to create a unified adapter. This method assumes the inputs to LoRA follow isotropic distributions. However, this assumption is overly simplistic and may lead to performance degradation in real-world applications.\nTo move beyond the assumption of isotropic distributions, recent studies have proposed LoRA merging as an optimization problem using features from real distribution. The optimization objective aims to align output features between the unified adapter and each individual LoRA [8, 15, 18, 39]. In the context of text-to-image diffusion, custom diffusion [22] uses constrained optimization to integrate multiple concepts by adjusting key and value projection matrices for text feature alignment. Mix-of-Show [8] adopts a similar optimization strategy during gradient fusion to merge multiple LoRAs, using LBFGS [20] to find optimal solutions. In integrating multiple NLP tasks, Reg-Mean [15] pursues the same alignment objective as Mix-of-Show but achieves it through a closed-form solution for greater computational efficiency. These approaches assume that the input features for each individual LoRA to be those for the unified adapter, presenting a rough approximation that can cause serious performance degradation. In contrast, we directly extract the input features for the unified adapter and refine the optimization objective iteratively, which successfully mitigates the performance degradation."}, {"title": "3. Method", "content": "In this section, we begin with a brief overview of the LoRA merging's background, followed by an analysis of the limitations of existing methods, which naturally motivates our proposed approach, IterIS.\n3.1. Preliminaries\nDefinition of LoRA Merging. Given a pre-trained model and N distinct tasks, we derive N sets of task-specific Lo-RAs, each fine-tuned on a corresponding dataset and consistently applied at the same positions within the pre-trained model. For each layer, we state that the N task-specific LoRAs operate at the same position, with \\(W_i\\) denoting the weights of the i-th task-specific LoRA. The unified adapter results from merging these N task-specific LoRAs and is denoted by W. After integrating all unified adapters into the pre-trained model, the resulting model is referred to as the \"unified model\". It is worth noting that LoRA merging doesn't need any labeled data or gradient-based training.\nLinear Merging. Linear merging is the most straightforward method. As shown in Figure 4(a), It merges individual LoRAs by linearly combining their parameters to create a unified adapter. Let \\(X_i\\) denote the random variable representing the input features for the i-th task, extracted from its corresponding task-specific LoRA. Linear merging can be formulated as the following optimization, aiming to ensure the output of the unified adapter closely matches that of each individual LoRA:\n\\[W^* = \\arg \\min_W \\sum_{i=1}^N \\lambda_i E_{X} [||W X - W_i X_i||]_F \\] \nwhere \\(E_{X}[ ]\\) denotes the expectation with respect to X ( \\(X=(X_1, ..., X_N)\\)), and \\(\\lambda_i\\) is a constant. and \\(|| . ||_F\\) represents the Frobenius norm. It is important to note that Eq. (9) does not have a closed-form solution.\nAssuming each \\(X_i\\) follows an isotropic distribution and \\(X_1...X_N\\) are mutually independent, Eq. (9) yields a closed-form solution for the unified adapter2:\n\\[W^* = \\sum_{i=1}^N \\lambda_i \\bar{W_i}, \\ \\lambda_i = \\frac{\\lambda_i E_{X_i} [X_i]}{\\sum_{j=1}^N \\lambda_j E_{X_j} [||X_j||_F^2]} \\]\nwhere \\(E_{X_i}[]\\) denotes the expectation with respect to the distribution of \\(X_i\\). This solution represents a linear merging of the parameters of the N task-specific LoRAs. However, assuming an isotropic distribution for each \\(X_i\\) is overly simplistic and often does not hold in practice, it significantly limits the performance of the unified model. Additionally, treating the weighting coefficients \\(\\lambda_i\\) as hyperparameters, as in PEMs [48], greatly expands the hyperparameter space, making manual tuning impractical as the number of tasks (N) increases. Therefore, a common and practical approach is to average the N task-specific LoRAs, i.e., \\(\\lambda_i = \\frac{1}{N}\\).\nReal-distribution-based Merging. To relax the assumption of isotropic distribution, some LoRA merging methods utilize features from real distribution [8, 15, 18, 39]. Specifically, these methods perform inference on unlabeled samples to obtain input features for the LoRAs, and then approximate the mathematical expectation in Eq. (9) using these features. By setting all \\(\\lambda_i\\) to be equal, we can obtain the following optimization\u00b2 from Eq. (9):\n\\[W^* = \\arg \\min_W \\sum_{i=1}^N ||W^T X_i - W_i^T \\bar{X_i}||_F \\] \nwhere \\(\\bar{X_i}\\) represents the input features of the i-th task samples for the corresponding task-specific LoRA. This optimization can be viewed as a linear regression in the matrix space, enabling a closed-form solution2 for the unified adapter:\n\\[W^* = (\\sum_{i=1}^N \\bar{X_i} \\bar{X_i}^T)^{-1} (\\sum_{i=1}^N \\bar{X_i} \\bar{X_i}^T W_i)\\]\nBuilding on the derivations above, we highlight three key limitations of exiting real-distribution-based merging methods: 1) Eq. (3) relies on rough assumption\u00b9, which assumes the input features for the unified adapter W is identical to \\(\\bar{X_i}\\) (i.e., in Eq. (3), \\(\\bar{X_i}\\) in \\(W^T \\bar{X_i}\\) represents the approximated input features). Specifically, the input features for the first layer of the unified model are identical to those of the LoRA-tuned models when provided with the same input. However, as model depth increases, the discrepancies grow significantly, making this approximation rough and severely limiting performance. 2) To ensure the invertibility of \\((\\sum_{i=1}^N \\bar{X_i} \\bar{X_i}^T)^{-1}\\) in Eq. (4) and enhance robustness, they are usually accompanied by massive sample requirements. However, this is often impractical in real-world applications. 3) Since the weight of \\(W_i\\) in Eq. (4) is based on the inner product matrix \\(\\bar{X_i} \\bar{X_i}^T\\), variations in the magnitudes of \\(\\bar{X_i}\\) can cause the solution to be overly influenced by the most prominent term. It potentially renders Eq. (3) an unbalanced optimization.\n3.2. IterIS Algorithm\nAs illustrated in Figure 4(c), our algorithm employs an iterative inference-solving framework. In each iteration, IterIS performs inference using unlabeled samples, extracting input features for all unified adapters. These features are then used to formulate IterIS's optimization objectives. By solving each optimization, IterIS obtains the unified adapters for the next iteration. This iterative process refines the optimization objective gradually to acquire the unified adapter with increasing performance. Unlike previous methods that usually rely on massive unlabeled samples, IterIS only needs to utilize a small set of unlabeled samples. Additionally, by incorporating an optimization objective with adaptive weights, our method ensures a balanced solution for each unified adapter. We now present a detailed discussion of the three primary improvements introduced in our work.\nIterative Inference-Solving. We further build upon the core principles of real-distribution-based merging methods. Let \\(X\\) denote the input features for the unified adapter (W), replacing approximation \\(\\bar{X_i}\\). We derive a more accurate representation for LoRA merging than prior methods:\n\\[W^* = \\arg \\min_W \\sum_{i=1}^N ||W X - W_i^T \\bar{X_i}||_F \\] \nwhere \\(X\\) is a constant defined later. This optimization objective has the following closed-form solution:\n\\[W^* = (\\sum_{i=1}^N \\bar{X_i} \\bar{X_i}^T)^{-1} (\\sum_{i=1}^N \\bar{X_i} \\bar{X_i}^T W_i)\\]\nAll \\(\\bar{X_i}\\) are updated iteratively, starting with \\(\\bar{X_i} = X_i\\). At each iteration, IterIS has two steps: 1) Inference Step: By performing inference on the i-th task samples within the current unified model, we can extract \\(\\bar{X_i}\\). Here, \\(\\bar{X_i}\\) represents the input features for the unified adapter, 2) Solving Step: After updating the \\(\\bar{X_i}\\) in Eq. (6), we compute the solution \\(W^*\\) for each unified adapter. These unified adapters are then incorporated into the pre-trained model, resulting in a new unified model for the next iteration. By directly leveraging the input features for the unified adapter, our method mitigates the rough assumption\u00b9. Furthermore, through iterative refinement of the optimization objective, IterIS can achieve a more accurate representation of LORA merging.\nFew-Sample Requirement. Unlike previous methods that require a large number of unlabeled samples, our approach relies on a limited set (requiring only 1-5% of the unlabeled samples compared to prior methods). This is achieved by introducing regularization to the inner product matrices in Eq. (6), replacing them with the following:\n\\[\\bar{X_i} \\bar{X_i}^T + \\alpha || \\bar{X_i} \\bar{X_i}^T ||_F I, \\ \\bar{X_i} \\bar{X_i}^T + \\alpha || \\bar{X_i} \\bar{X_i}^T ||_F I\\]\nwhere I denotes the identity matrix. Notably, this regularization alleviates overfitting on these samples, thereby enhancing the algorithm's robustness. In our experiments, we typically set \\(\\alpha\\) to \\(1 \\times 10^{-4}\\) or lower.\nAdaptive Weight Balancing. Setting uniform weights for \\(\\lambda_i\\) in Eq. (5) leads to the same unbalanced issues encountered in previous works. To mitigate this, we introduce adaptive weights for balancing, defined as follows:\n\\[i = ||W_i||_F ||W_i^T \\bar{X_i}||_F^2.\\]\nThis choice of \\(\\lambda_i\\) mitigates unbalances in the optimization process, preventing any terms from being overly prioritized.\nAnalysis of IterIS. By leveraging the directed acyclic graph structure in deep learning models, we derive an upper bound on the iteration count for our algorithm's convergence2. Given an encoder with N-layer self-attention modules, where the Q, K, and V matrices are fine-tuned using LoRA, our algorithm can converge after N-1 iterations. To prevent overfitting, we cap the iterations at 20 in our experiments. Despite the increased number of iterations, IterIS converges with significantly fewer samples compared to previous methods, maintaining a low overall inference count and ensuring computational efficiency. Additionally, our method bypasses the computational demands of gradient-based training through closed-form solutions and layer-wise updates. Full process of IterIS is illustrated in the appendix, with the complete procedure detailed in Algorithm 1."}, {"title": "4. Experiments", "content": "In this section, we evaluate the effectiveness of IterIS across various tasks with different pre-trained generative models, including text-to-image diffusion, vision-language models, and large language models. Due to the limited space, more details of the experimental settings and the ablation studies are left in the Appendix.\n4.1. IterIS for Text-to-Image Diffusion Model\nExperimental Setup. We applied IterIS to text-to-image diffusion model for multi-concept customization, utilizing Stable Diffusion v1.5 [35]. Multiple LoRAs were obtained through the widely used Dreambooth-LoRA [36] tuning method, with each concept fine-tuned using only 5-10 images without any regularized data. The target images are sourced from the CustomConcept101 [18] and Dream-Booth [36] datasets. In all experiments, images were processed at a resolution of \\(512 \\times 512\\), employing a DDPM sampler that runs for 100 steps per composition. Each composition features 2-3 concepts, and we set the guidance scale to 12. We utilized 50 input samples for inference to derive input features for our algorithm. Adhering to similar settings as custom diffusion [18], we present the results of generating two new concepts within the same scene for the following seven pairs: 1) Wooden Pot+ Teddybear; 2) Flower + Dog; 3) Teddybear + Tortoise Plushy; 4) Cat + Barn; 5) Wooden Pot + Cat; 6) Flower + Wooden Pot; 7) Dog + Sunglasses.\nMetrics and Baselines. Following standard evaluation practices, we assess our method using two key metrics, measured by CLIP-large-patch14 [33]: (1) image alignment, which evaluates the coherence of two new concepts within the same scene, and (2) text alignment, which measures the consistency between textual descriptions and their corresponding visual representations. For each composition pair, we generate 400 images across 8 challenging prompts. Each metric is calculated as the average across 400 generated images per composition. We benchmark our approach against linear merging, textual inversion [4], and the state-of-the-art method for multi-concept customization, custom diffusion [18], which employs data regularization."}, {"title": "4.2. IterIS for Vision-Language Model", "content": "Experimental Setup. We employed IterIS to vision-language models for multi-style caption generation, utilizing BLIP-image-captioning-base [19]. Style-specific Lo-RAs were fine-tuned on BLIP. We used 50 prompt-image pairs per style-specific captioning task to derive input features. We set \\(\\alpha = 8 \\times 10^{-7}\\) and 6 iterations. Our method was evaluated on the combination of two caption text styles. Specifically, we utilized the SentiCap dataset [24] to train style-specific LoRAs (\"positive\" and \"negative\"). We also conducted experiments with other caption style combinations, presented in the appendix.\nMetrics and Baselines. Following common practices [17], we evaluate the performance of the style caption generation using style accuracy [38], CIDEr [43], and BLEU 1-4 [28]. Apart from style accuracy, we report the average of all other metrics across test sets for both styles. Our method is benchmarked against linear merging and RegMean.[15].\nMain Results. In Table 2, we present experimental results for the \"positive\u201d + \u201cnegative\" style combination. We fine-tuned two specialized LoRA models: POS LORA, focused on generating positive (POS) captions, and NEG LORA, specialized in negative (NEG) captions. In single-style generation tests, each LoRA model performs well within its designated style but struggles to produce the other style, with POS LORA achieving only 1.8% accuracy in NEG caption generation. Linear merging enables dual-style generation, though it results in limited style controllability, yielding only 52.2% accuracy for POS and 55.7% for NEG. Reg-Mean improves style control, achieving 62.4% accuracy for POS and 69.2% for NEG captions. Our proposed method maintains the highest caption generation quality, surpassing RegMean by 20.7% in POS accuracy and 8.9% in NEG accuracy."}, {"title": "4.3. IterIS for Large Language Model", "content": "Experimental Setup. We applied IterIS to large language models for integrating multiple NLP tasks including in-domain task integration and multi-task integration 3. For in-domain task integration, we utilized FLAN-t5-large [2] on Emotion datasets [26, 27, 27, 37] which cover a wide range of sentiments. We applied our method with 50 inputs per task for inference, \\(\\alpha\\) set to \\(1 \\times 10^{-7}\\), and used 10 iterations. We explored all the possible task combinations, specifically two-task, three-task, and four-task integration for in-domain scenarios. For multi-task integration, we used FLAN-t5-base [2] as the backbone model across seven datasets in the GLUE benchmark [45]. Input features were derived by performing inference on 50 inputs per task,\nMetrics and Baselines. For in-domain task integration, we evaluate the performance of the unified model across all task combinations using the F1 score. For multi-task integration, Accuracy is used for all the tasks except COLA [46], which is assessed using the Matthews correlation coefficient (MCC) [25]. To align the MCC metric range with accuracy, we apply linear normalization to scale MCC values between 0 and 1. Our algorithm is benchmarked against linear merging and the state-of-the-art method, RegMean [15].\nMain results. 1) For in-domain task integration, there are a total of 4 in-domain tasks, resulting in \\(C_4^2 + C_4^3 + C_4^4 = 11\\) combinations. We present the average F1 scores across all 11 combinations for each task in the In-Domain section of Table 3. It can be observed that our IterIS significantly outperforms both the Linear and RegMean approaches, with improvements over RegMean of 3.6%, 2.1%, 0.3%, and 2.6% on all the in-domain tasks, respectively. 2) For Multi-Task integration, we tested all pairwise combinations, totaling \\(C_7^2 = 21\\) combinations. We calculated the average metric for each GLUE sub-task across these 21 combinations, as shown in the Multi-Task section of Table 3. Our method consistently outperforms both Linear and RegMean, with the only exception being a slightly lower average score than linear merging on COLA. Notably, our method shows significant improvement over RegMean on MNLI, COLA, and MRPC, with increases of 1.7%, 1.4%, and 0.9%, respectively."}, {"title": "5. Conclusion", "content": "In this paper, we propose a novel, versatile algorithm for efficiently merging multiple LoRAs. Building on an iterative inference-solving framework, our method gradually refines the optimization objective, yielding a more effective LORA merging representation than previous approaches. Additionally, our layer-wise algorithm converges in minimal steps, ensuring efficiency in both memory and computation. Notably, our method outperforms multiple baselines across various domains. Moving forward, we aim to: 1) Extend and refine our algorithm to accommodate other PEFT methods, and 2) Explore a new perspective on the formulation of the optimization for LoRA merging."}, {"title": "IterIS: Iterative Inference-Solving Alignment for LoRA Merging", "content": "In the supplementary materials, we provide:\n\u2022 Mathematical Proofs (Section A): We present the mathematical proofs for the equations and properties of our proposed algorithm, as discussed in the main text.\n\u2022 Datasets and Experimental Setup (Section B): This section details the datasets employed in our experiments, the LORA training configuration, and the key settings of the baseline implementations.\n\u2022 Ablation Studies (Section C): We report the results of ablation studies, demonstrating the necessity of the three mechanisms integrated into our algorithm.\n\u2022 Experimental Results (Section D): A detailed presentation of the experimental results is provided in this section.\n\u2022 Workflow and Limitations (Section E): Finally, we illustrate the complete workflow of IterIS and conduct an analysis of the limitations of our approach."}, {"title": "A. Detailed Mathematical Derivation", "content": "A.1. Linear Merging\nConsider the following optimization problem:\n\\[W^* = \\arg \\min_W \\sum_{i=1}^N \\lambda_i E_{X} [||W X - W_i X_i||]_F\\]\nwhere \\(E_{X}[ ]\\) denotes the expectation with respect to X ( \\(X=(X_1, ..., X_N)\\)), and \\(\\lambda_i\\) is a constant. and \\(|| . ||_F\\) represents the Frobenius norm. Assuming each \\(X_i\\) follows an isotropic distribution and \\(X_1... X_N\\) are mutually independent, we have:\n\\[E_{X_i} [[[W X - W_i X_i||]] = E_{X_i} [||X_i||_F^2]||W-W_i||_F^2\\]\nwhere D is the dimension of each \\(X_i\\). we have the following based on Eq. 10:\n\\[W^* = \\arg \\min_W \\sum_{i=1}^N \\lambda_i E_{X_i} [||W X - W_i X_i||]_F\\]\n\\[= \\arg \\min_W \\sum_{i=1}^N \\lambda_i E_{X_i [||X_i||_F^2] ||W - W_i||_F^2\\]\n\\[= \\arg \\min_W \\sum_{i=1}^N \\lambda_i D E_{X_i} [X_i] ||W - W_i||_F^2\\]\nThis expression admits a closed-form solution.\n\\[W^* = \\sum_{i=1}^N \\lambda_i \\bar{W_i}, \\ \\lambda_i = \\frac{\\lambda_i E_{X_i} [X_i]}{\\sum_{j=1}^N \\lambda_j E_{X_j} [||X_j||_F^2]} \\]"}, {"title": "A.2. Real-distribution-based Merging", "content": "We can derive the following from the linear merging derivation in Subsection A.1, and set each \\(\\lambda_i\\) to 1:\n\\[W^* = \\arg \\min_W \\sum_{i=1}^N  E_{X_i} [||W X_i - W_i \\bar{X_i}||_F ]\\]\nTo compute each expectation in Eq. 15, we can sample from the distribution of \\(X_i\\), and using the law of large numbers, approximate the expectation as:\n\\[E_{X_i} [||W X_i - W_i \\bar{X_i}||_F ] \\approx \\frac{1}{S} \\sum_{s=1}^S ||W X_{is} - W_i \\bar{X_{is}}||_F^2\\]\n\\[=  W X_i - W_i \\bar{X_i}\\]\nwhere \\(x_{is} \\in Sample(X_i)\\) and \\(X_i = (x_{i1}, x_{i2},...,x_{is})\\). Therefore, we can reformulate the optimization objective based on the real distribution:\n\\[W^* = \\arg \\min_W \\sum_{i=1}^N ||W X_i - W_i \\bar{X_i}||_F^2\\]\nIn fact, this optimization problem can be viewed as a linear regression problem, where X = \\([X_1, X_2,\\ldots, X_N]^T\\) is mapped to Y = \\([W X_1,W X_2,\\ldots,WX_N]^T\\). Thus, the problem has a closed-form solution:\n\\[W^* = (X^TX)^{-1}X^TY\\]\n\\[= (\\sum_{i=1}^N \\bar{X_i} \\bar{X_i}^T)^{-1} (\\sum_{i=1}^N \\bar{X_i} \\bar{X_i}^T W_i)\\]\nSimilarly, for the optimization problem corresponding to our algorithm in Eq. 5 of Section 3, we can also obtain its solution."}, {"title": "A.3. Maximum Iteration Limit for IterIS", "content": "Starting with the topology of the generative model", "G_M\\)": "if there is a directed path from node A to node B in \\(G_V\\)", "convergence.\nProof": "Let s denote the length of the longest path in \\(G_M\\) originating from the input node \\(O_{in"}, ".", "We define a function g : v(\\(G_M\\))\\{\\(O_{in}\\)} \u2192 {1, 2, ..., s}, where g(A) indicates the length of the longest path from \\(O_{in}\\) to node A. We will use mathematical induction to demonstrate that at the k-th iteration, all nodes in \\(g^{-1}({1,2,..., k + 1})\\) and their corresponding updated matrices remain unchanged in subsequent iterations. This observation relies on the fact that if the unified adapter's input \\(\\bar{X_i}\\) remains constant during iterations, the computed \\(W_i\\) will also remain constant.\n1. Base Case (k = 0): For the initial iteration, it is clear that \\(g^{-1}(1)\\) is non-empty and contains only nodes with \\(O_{in}\\) as their sole parent. Therefore, \\(g^{-1}(1)\\) is entirely dependent on the input, indicating that these nodes and their corresponding matrices will remain unchanged in subsequent iterations.\n2. Inductive Step (Assume true for k = \\(k_0\\)): Assume that at iteration k = \\(k_0\\), all nodes in \\(g^{-1}({1, 2, ..., k_0 + 1})\\) and their associated matrices remain constant. Since each node in \\(g^{-1} (k_0+\\2)\\) has its parent nodes contained within \\(g^{-1}({1, 2, . . ., k_0+1})\\), it follows that the nodes in \\(g^{-1} (k_0 +2)\\) are reliant on the unchanged nodes in \\(g^{-1}({1, 2, . . ., k_0 + 1})\\). Consequently, these nodes and their corresponding updated matrices will also remain constant in subsequent iterations.\nBy this reasoning, when k = s \u2212 1, it holds that \\(g^{-1}({1,2,...,s}) = v(G_M) \\ {O_{in}}\\) will remain unchanged in subsequent iterations. Thus, we prove and compute the upper bound on the maximum number of iterations required for the convergence of the IterIS algorithm.\nFor a transformer model consisting of L layers of encoders and L layers of decoders, if we"]}