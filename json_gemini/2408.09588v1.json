{"title": "SynTraC: A Synthetic Dataset for Traffic Signal Control from Traffic Monitoring Cameras", "authors": ["Tiejin Chen", "Prithvi Shirke", "Bharatesh Chakravarthi", "Arpitsinh Vaghela", "Longchao Da", "Duo Lu", "Yezhou Yang", "Hua Wei"], "abstract": "This paper introduces SynTraC, the first public image-based traffic signal control dataset, aimed at bridging the gap between simulated environments and real-world traffic management challenges. Unlike traditional datasets for traffic signal control which aim to provide simplified feature vectors like vehicle counts from traffic simulators, SynTraC provides real-style images from the CARLA simulator with annotated features, along with traffic signal states. This image-based dataset comes with diverse real-world scenarios, including varying weather and times of day. Additionally, SynTraC also provides different reward values for advanced traffic signal control algorithms like reinforcement learning. Experiments with SynTraC demonstrate that it is still an open challenge to image-based traffic signal control methods compared with feature-based control methods, indicating our dataset can further guide the development of future algorithms. The code for this paper can be found in https://github.com/DaRL-LibSignal/SynTraC.", "sections": [{"title": "I. INTRODUCTION", "content": "Inefficient traffic signal plans contribute significantly to roadway congestion, wasting commuters' time. Traditional traffic control systems, such as the widely adopted SCATS, often depend on static, manually designed signal plans that fail to adapt to changing traffic conditions. In contrast, the advent of Al technologies and enhanced traffic monitoring capabilities, like surveillance cameras, has paved the way for innovative approaches. Recent studies have explored the application of deep reinforcement learning (RL) to traffic signal control, offering a dynamic alternative. Unlike traditional systems, RL-based methods continuously learn and adjust signal timing in response to real-time traffic data, demonstrating superior efficacy in improving traffic flow.\nDespite the advances in methods, one of the critical gaps in traffic signal control (TSC) research is the absence of an image-based dataset. Existing datasets are insufficient for developing methods that interpret real-world visual cues [15], [9]. They either focus on vehicle counting without traffic signal data or use simulators that provide detailed feature data but lack real images. This paper addresses this gap by introducing a novel dataset that merges these two critical aspects: detailed image data and traffic signal state information. Integrating these elements into a single dataset is a significant step toward developing more sophisticated traffic management solutions that can directly leverage the visual information available from surveillance cameras. Feature-based datasets inherently lack the depth of context that visual data offers. For instance, camera data can reveal not just the number of vehicles at an intersection but also their behavior, formation, and even the presence of pedestrians or cyclists-factors that significantly influence optimal signal timing. Furthermore, with the majority of modern traffic management systems incorporating camera data for real-time monitoring and decision-making, the relevance of an image-based approach is more pronounced than ever.\nThis paper introduces SynTraC, a comprehensive dataset designed to facilitate the development of image-based TSC systems. Unlike existing datasets derived from 2D traffic simulators, SynTraC is generated using CARLA [2], a so-phisticated 3D traffic simulation platform. This approach enables collecting real-style images from roadside cameras at intersections, closely mimicking real-world conditions. Building such a dataset can also help reduce the delay of using CARLA directly. Besides Each image in SynTraC is accompanied by ground truth data, including bounding box locations for vehicles, making it highly compatible with computer vision technologies, such as image detection [14].\n\u2022 Content and Features: SynTraC contains over 86,000 RGB images, each tagged with corresponding traffic signal states and reward values, across six hours of simulated traffic under various weather conditions and times of day. Weather conditions were diversified to encompass scenarios such as Sunny, Fog, Rainy, and Cloudy, each offering distinct challenges for traffic monitoring systems. Similarly, time conditions ranged from Day to Night. This rich dataset is further augmented with more than 250,000 reward values and approximately 225,000 vehicle bounding boxes. Additionally, we provide three rewards, i.e., queue length, waiting time, and throughput. Such detailed annotation makes SynTraC ideally suited for training image-based TSC policies using RL with multi-objective optimization techniques.\n\u2022 Utility and Flexibility: Beyond its comprehensive content, SynTraC is designed to support advanced traffic management research and application development. To this end, we also provide the source code for dataset generation, enabling researchers and practitioners to produce customized datasets. Our automated generation pipeline offers flexibility in scenario creation, allowing for the selection of different weather conditions, times of day, and traffic flows. This adaptability ensures that users can tailor the dataset to meet specific research needs or operational challenges.\nOverall, we provide the overview of SynTraC in Fig. 1"}, {"title": "II. SYNTRAC DATASET GENERATION PIPELINE", "content": "Our data generation pipeline is based on CARLA, a three-dimensional simulator supporting traffic signal control. However, the simulator does not have built-in support for reward calculation and lane detection which are important for creating the offline RL dataset for TSC. Hence, we developed an extension module with CARLA Python API to implement our data generation pipeline. The pipeline contains three stages, (a) traffic scene configuration, (b) simulation, and (c) reward calculation, which are illustrated in Fig. 2 and detailed in the following three subsections."}, {"title": "A. Traffic Scene Configuration", "content": "We manually set a series of traffic scenarios to ensure a diverse dataset. The environmental conditions for scenarios were set by randomizing the weather and time of the day. We chose different weather conditions such as sunny, rainy, foggy and cloudy with different time slots. The setup also contains 12 distinct traffic flows at an intersection with four paths, with each scenario identifying different paths that have vehicles. Such diversity of traffic flows ensures that SynTraC even contains various situations for RL training.\nFor camera setup, we set up 4 RGB cameras in the intersection in CARLA. All cameras had an image resolution of 1920\u00d71080 pixels and a field-of-view of 90\u00b0. Frames were stored from the RGB camera after every second. For vehicle spawning with auto-pilot, spawn points were chosen on the map, where vehicles were periodically spawned. Exactly one exit point was set for the map where vehicles were destroyed. Different spawn points were chosen for each traffic scenario.\nFinally, we adjusted the traffic manager settings to change traffic signals with different periods of green time. Various scenarios were configured with green light times set at either 10 seconds or 15 seconds, adding variability to the traffic flow simulations."}, {"title": "B. Simulation and Data Generation", "content": "We developed a Python script to gather the images and annotations during the simulation in CARLA and the gather-ing procedure is shown in Fig. 2 (b). Firstly, we captured the image from the camera every one second. We set the CARLA server-client communication in fixed-time synchronous mode to avoid potentially skipping frames during the processing of images.\nThe simulation has a fixed timestep of 0.08 seconds and CARLA takes thirteen steps (1/0.08) to recreate one second of the simulated world. We take the images of CARLA every 13 timesteps to get one image every second.\nNow we could obtain images in 2D format from the cameras, along with information such as the traffic signal state and the speed of vehicles. However, information about coordinates in the frame, including bounding boxes and lane locations, is still in 3D format. Thus, we calculated the camera's projection matrix K, which can project 3D coordinates into a 2D format consistent with the images. The projection matrix was derived using the focal length, which compressed the scene's depth onto the sensor, and the principal point, which anchored the image center to the camera's optical axis.\nIn detail, we have:\n$K = \\begin{bmatrix}\nS_x\\over 2\\cdot tan({FOV \\over 360})\n & 0 & 1/2 \\\\\n0 & S_x\\over 2\\cdot tan({FOV \\over 360}) & 1/2 \\\\\n0 & 0 & 1\n\\end{bmatrix}$\nHere, $S_x$ and $S_y$ are the width and the height of 2D images captured by cameras, respectively, and f is the field of view of the cameras. $\\frac{1}{2}$ and $\\frac{1}{2}$ determine the principal point and $\\frac{S_x}{2\\cdot tan(\\frac{FOV}{360})}$ is the focal length. After obtaining the projection matrix K, we calculated the 2D coordinates for a given 3D pixel with coordinates [x,y,z] using the following equation:\n$P = K. [x,y,z] = [\\hat{x}, \\hat{y}, \\hat{z}]^T$.\nSubsequently, it was necessary to apply perspective projec-tion to $\\hat{x}$ and $\\hat{y}$ for accounting foreshortening:\n$(x',y') = (\\frac{\\hat{x}}{\\hat{z}}, \\frac{\\hat{y}}{\\hat{z}})$.\nWhere x' and y' are the final 2D coordinates.\nTo fulfill the TSC's requirement for counting vehicles within each lane, we conducted lane detection on the 2D"}, {"title": "C. Reward Calculation", "content": "We calculated three different rewards related to TSC for SynTraC during generation. The reward is an RL term that measures the return by taking action under the state. Considering a diversity of rewards, our dataset contains:\n\u2022 Waiting Time (WT) measures the amount of time that vehicles spend waiting in the network. For a control policy, the smaller WT indicates a better quality of TSC.\n\u2022 Queue length is the number of vehicles waiting to pass through the intersection in the road network. A good TSC policy should not make too many vehicles wait. Therefore, a smaller queue length indicates a better quality of TSC.\n\u2022 Throughput (TP) is the number of vehicles that reach their destinations within an amount of time. A larger TP, which means more vehicles are arriving at their destinations, indicates a better traffic flow and thus a better TSC.\nQueue length was calculated for each camera individually when we took the image every time. Waiting Time was also calculated for each camera and we summed up the waiting time from every stopped vehicle which has a speed of less than 0.1m/s. In contrast, throughput was calculated as a sum of all the cameras together, and the value of throughput is accumulated through the simulation and never decreases unless we start a new simulation for a new traffic scene."}, {"title": "III. EXPERIMENTAL EVALUATION", "content": "Our experiments focus on evaluating the image-based traffic signal control methods trained with our image dataset on both synthetic and real-world data. Every image-based traffic signal control method contains a detection model and count-based control policy.\nFor the online testing, we adopt the CARLA and several types of traffic flow in CARLA to make a variant test environment. The whole simulation duration is 240 seconds with 3 different traffic flows. Consistent with the previous count-based TSC methods, we first obtain the control policies with ground truth counts for each lane and then acquire the counts for each lane through image detection models. We do not directly use an end-to-end training method using images as input because the resolution of images is so high that training time will become unacceptable. The random seed is set to the same for every experiment."}, {"title": "A. Evaluation Setting", "content": "We adopt the commonly used metrics, detection models, and RL models for our evaluation.\nRL Model. We adopt four different popular RL train-ing methods that fit offline RL training: 1) Deep Q-Network (DQN) [13]; 2) Double DQN (DDQN) [17]; 3) Soft Actor-Critic (SAC) [5]; and 4) Conservative Q-Learning (CQL) [8]. We also compared RL-based con-trol policies with traditional methods such as MaxPressure, which is a rule-based method using count information as well and we use ground truth counts for MaxPressure.\nDetection Models. Our final TSC methods contain the image detection models to get the vehicle numbers for each lane. To test the influence of different detection models, we consider 3 detection models: 1) Masked R-CNN [6]; 2) Faster R-CNN [4] and 3) RetinaNet [10]. We mainly focus on the pre-trained detection models while we also provide the results for fine-tuned detection models on SynTraC.\nMetrics. For evaluating the performance of image detec-tion models, we mainly use Mean Square Error (MSE) and Mean Absolute Error (MAE) to evaluate the detected vehicle numbers with ground truth vehicle numbers. For evaluating control policies, we use total WT (in seconds), queue length (in counts), and throughput which we introduced in the previous section. We also consider travel time (TT) which calculates how much time a vehicle requires to arrive at the destination."}, {"title": "B. Evaluation Results", "content": "We use three different pre-trained detection models and four offline RL models. We evaluate the performance of dif-ferent detection models and the image-based TSC methods.\nEvaluation of Detection Models. In Table I, we present the results of evaluating image detection models. We have the following main observations:\n\u2022 Compared to other methods, faster R-CNN has a better result across different weather and times. This is possible because other more advanced methods are overfitting to the pre-trained dataset. Besides, faster R-CNN has a better inference time which is another advantage in the real world.\n\u2022 Different weather and times influence the performance of all detection models. Compared with performance under sunny and daytime, the performance of all models drops in other weather and times. We find that nighttime influences the performance the most because most images in the pre-trained dataset are taken in well-lit conditions to depict the objects. The variety of images in SynTraC plays an important role in fine-tuning detection models that be used at night.\nEvaluation of Control Policies. In the Table II, we present the results of evaluating control models (with detection models together) and we have the following observations:\n\u2022 Compared with the rule-based policies, all RL-based models using ground-truth counts in each lane are better in all metrics except for DDQN, showing the superiority of RL-based TSC. This observation is consistent with previous works [19], [22].\n\u2022 Compared with using ground truth count values, after combining the detection models, the performance drops a lot even with sunny and daytime when detection models have the best performance. This indicates that improving the detection models or proposing new algorithms handling the non-accurate count information is required in the feature.\n\u2022 Among all RL models, CQL works the best in all metrics, showing that CQL has the advantage of utilizing the limited data setting in offline reinforcement learning. However, CQL also drops the performance the most when using detection models. This is reasonable since a better policy is more sensitive to the wrong information.\n\u2022 When the weather becomes rainy, the drop in performance is insignificant compared with sunny. When time becomes nighttime, the performance drop is more significant, which is consistent with the detection model's performance."}, {"title": "C. Influence of Different Camera Angles", "content": "We evaluate our methods with three different camera angles in CARLA. We provide the evaluation results for raising and lowering the cameras' angles and raising the location of cameras to provide a bird-view image in Table III.\n\u2022 Changing the angles of cameras will influence the RL poli-cies significantly. After raising the cameras, the performance of DQN becomes better even compared with using ground truth count values. This is because DQN performs better if the number of vehicles decreases. Raising the cameras' angles will cause the detection numbers to become less. Therefore, the performance of DQN is even better than using ground truth count information. The same thing happens when we lower the cameras. The performance increases especially for SAC because lowering the cameras can output the detection results that fit better for SAC.\n\u2022 Raising the locations of cameras captures images having the most comprehensive information to the detection model and thus the performance of CQL is the best in this case. Besides, the performances of SAC and DQN become similar to using the ground truth count information."}, {"title": "D. Generalization on Real-world Data", "content": "We evaluate the image detection models and TSC model learned with SynTraC dataset using a real-world dataset from an intersection at Tempe, Arziona. By doing so, we justify our dataset uniformly enhances a better generalizability on both image detection methods and prevalent end-to-end image-based TSC methods.\nEvaluation on Detection Model. First, we evaluate the detection models fine-tuned with SynTraC by comparing them with the per-trained detection model. We use RetinaNet as the detection method and annotate 100 images from a 6-minute video captured by the cameras from the intersection with the count of vehicles. The comparison results are provided in Table IV. From the results, we can see that fine-tuning with SynTraC increases the performance of the detection model even in the real world, indicating that a generalization ability of SynTraC in the real world and SynTraC can help to develop better image-based TSC methods in the real world.\nEvaluation on TSC Methods. Since we cannot directly change the traffic signal states on the real-world data we collected, we do not provide a detailed evaluation of normal metrics such as waiting time. To evaluate image-based TSC methods using real-world data, we utilize scenarios where the real-world TSC method exhibited suboptimal performance. In this scenario, we applied our end-to-end TSC method, trained with SynTraC (utilizing detection models and the control policy trained with SynTraC), to obtain the decision. This approach allows us to demonstrate the effectiveness of our method in addressing real-world challenges. In detail, we can get one real-world scenario shown in Fig. 4. From the images, we can see that the vehicles are waiting in the direction of north and south for the traffic signal state to become green while no cars coming from west or east. In fact, in this scenario, the red light continues for around 30 seconds and more than 10 vehicles are waiting. In contrast, only 3 cars from west or east go through this intersection in this period. Therefore, from a human perspective, it is a bad decision made by real-world control. However, when we feed the same scenario into the end-to-end traffic signal control method trained with SynTraC, our method outputs it should change the traffic signal states, which is a correct decision."}, {"title": "IV. DISCUSSION", "content": "Recent datasets [3], [16] on traffic signal control offer traffic signal timings with traffic states. However, none of these datasets are suitable for RL approaches, which have gained much attention in recent traffic signal control area [19], [18] since they only contain information from traffic signals, which is less informative for RL approaches. Most current RL approaches are based on traffic simulators like Cityflow [20], [1] where RL policy takes the counts of vehicles in each lane as the inputs for online training. There also exists some offline RL datasets. For example, [21] collected data from Cityflow for offline RL while [15], [9] collected data from SUMO simulator [11], [12]. However, all of these simulators or datasets do not contain traffic monitoring images [7] considering the difficulty of directly getting the count information from real-world sensors. We need to utilize more realistic data such as traffic monitoring images. Our dataset SynTraC fills the blank in this area."}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In this paper, we introduce SynTraC, the first image-based dataset with reward data for traffic signal control (TSC) using offline reinforcement learning, along with a generation pipeline that aligns TSC with real-world conditions. Besides, SynTraC also contains information for other downstream tasks such as vehicle detection. The evaluation on SynTraC shows that the RL policies that control the traffic signal can perform very well using ground truth information while integrating the detection models into the TSC system hurts the performance of RL policies.\nIn the future, We aim to propose new control algorithms designed to handle uncertain information, thereby prevent-ing performance degradation of the detection model under various weather conditions. Besides, a thorough evaluation of the end-to-end TSC system trained with SynTraC in the real world is also in our next plan."}]}