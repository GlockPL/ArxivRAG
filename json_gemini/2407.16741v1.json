{"title": "OpenDevin: An Open Platform for AI Software Developers as Generalist Agents", "authors": ["Xingyao Wang", "Boxuan Li", "Yufan Song", "Frank F. Xu", "Xiangru Tang", "Mingchen Zhuge", "Jiayi Pan", "Yueqi Song", "Bowen Li", "Jaskirat Singh", "Hoang H. Tran", "Fuqiang Li", "Ren Ma", "Mingzhang Zheng", "Bill Qian", "Yanjun Shao", "Niklas Muennighoff", "Yizhe Zhang", "Binyuan Hui", "Junyang Lin", "Robert Brennan", "Hao Peng", "Heng Ji", "Graham Neubig"], "abstract": "Software is one of the most powerful tools that we humans have at our disposal; it allows a skilled programmer to interact with the world in complex and profound ways. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. In this paper, we introduce OpenDevin, a platform for the development of powerful and flexible AI agents that interact with the world in similar ways to those of a human developer: by writing code, interacting with a command line, and browsing the web. We describe how the platform allows for the implementation of new agents, safe interaction with sandboxed environments for code execution, coordination between multiple agents, and incorporation of evaluation benchmarks. Based on our currently incorporated benchmarks, we perform an evaluation of agents over 15 challenging tasks, including software engineering (e.g., SWE-BENCH) and web browsing (e.g., WEBARENA), among others. Released under the permissive MIT license, OpenDevin is a community project spanning academia and industry with more than 1.3K contributions from over 160 contributors and will improve going forward.", "sections": [{"title": "1 Introduction", "content": "Powered by large language models (LLMs; [5, 20, 40, 60]), user-facing AI systems (such as ChatGPT) have become increasingly capable of performing complex tasks such as accurately responding to user queries, solving math problems, and generating code. In particular, AI agents, systems that can perceive and act upon the external environment, have recently received ever-increasing research focus. They are moving towards performing complex tasks such as developing software [21], navigating real-world websites [79], doing household chores [1], or even performing scientific research [4, 56].\nAs AI agents become capable of tackling complex problems, their development and evaluation have also become challenging. There are numerous recent efforts in creating open-source frameworks that facilitate the development of agents [7, 16, 67]. These agent frameworks generally include: 1) interfaces through which agents interact with the world (such as JSON-based function calls or code execution), 2) environments in which agents operate, and 3) interaction mechanisms for"}, {"title": "2 OpenDevin Architecture", "content": "We describe, using OpenDevin, (1) how to define and implement an agent (\u00a72.1), (2) how each action execution leads to an observation (\u00a72.2), (3) how to reliably manage and extend commonly used skills for agents (\u00a72.3), and (4) how to compose multiple agents together for task solving (\u00a72.4). Fig. 3 provides an overview."}, {"title": "2.1 Agent Definition and Implementation", "content": "An agent can perceive the state of the environment (e.g., prior actions and observations) and produce an action for execution while solving a user-specified task.\nThe State and Event Stream. In OpenDevin, the state is a data structure that encapsulates all relevant information for the agent's execution. A key component of this state is the event stream, which is a chronological collection of past actions and observations, including the agent's own actions and user interactions (e.g., instructions, feedback). However, the state extends beyond just the event stream. It"}, {"title": "2.2 Agent Runtime: How Execution of Actions Results in Observations", "content": "Agent Runtime provides a general environment that equips the agent with an action space comparable to that of human software developers, enabling OpenDevin agents to tackle a wide range of software development and web-based tasks, including complex software development workflows, data analysis projects, web browsing tasks, and more. It allows the agent to access a bash terminal to run code and command line tools, utilize a Jupyter notebook for writing and executing code on-the-fly, and interact with a web browser for web-based tasks (e.g., information seeking).\nLinux SSH Sandbox. For each task session, OpenDevin spins up a securely isolated docker container sandbox, where all the bash commands from the agent are executed. OpenDevin connects to the sandbox through SSH protocol, executes arbitrary commands from the agent, and returns the execution results as observations to the agent. A configurable workspace directory containing files the user wants the agent to work on is mounted into that secure sandbox for OpenDevin agents to access.\nJupyter IPython. The Linux sandbox also supports running an interactive Jupyter server, which can be used by the agent for interactive python code execution [19] and debugging.\nWeb Browser. OpenDevin implements a Chromium browser based on Playwright [47]. It interfaces with agents using a set of browser action primitives defined by BrowserGym [12, 52], such as navigation, clicking, typing, scrolling. The full set of actions is detailed in \u00a7I. After executing these actions, the browser runtime provides a rich set of observations about the current state of the browser, including HTML, DOM, accessibility tree [35], screenshot, opened tabs, etc. These observations can be also augmented with configurable attributes that could allow agents to better understand web page observations, such as using a set-of-marks on screenshot [15, 71], visible element marking, focused element, interactable element marking, in-viewport element filtering [79], etc."}, {"title": "2.3 Agent Skills: The Extensible Agent-Computer Interface", "content": "SWE-Agent [72] highlights the importance of a carefully crafted Agent-Computer Interface (ACI, i.e., specialized tools for particular tasks) in successfully solving complex tasks. However, creating, maintaining, and distributing a wide array of tools can be a daunting engineering challenge, especially when we want to make these tools available to different implementations of agents (\u00a73). To tackle these, we build an AgentSkills library, a toolbox designed to enhance the capabilities of agents, offering utilities not readily available through basic bash commands or python code.\nEasy to create and extend tools. AgentSkills is designed as a Python package consisting of different utility functions (i.e., tools) that are automatically imported into the Jupyter IPython environment (\u00a72.2). The ease of defining a Python function as a tool lowers the barrier for community members to contribute new tools to the skill library. The generality of Python packages also allows different agent implementations to easily leverage these tools through one of our core action IPythonRunCellAction (\u00a72.1).\nRigorously tested and maintained. We follow best practices in software engineering and write extensive unit tests for tools in AgentSkills to ensure their reliability and usability.\nInclusion criteria and philosophy. In the AgentSkills library, we do not aim to wrap every possible Python package and re-teach agents their usage (e.g., LLM already knows pandas library that can read CSV file, so we don't need to re-create a tool that teaches the agent to read the same file format). We only add a new skill when: (1) it is not readily achievable for LLM to write code directly (e.g., edit code and replace certain lines), and/or (2) it involves calling an external model (e.g., calling a speech-to-text model, or model for code editing [51]).\nCurrently supported skills. AgentSkills library includes file editing utilities adapted from SWE-Agent [72] like edit_file, which allows modifying an existing file from a specified line; scrolling functions scroll_up and scroll_down for viewing a different part of files. It also contains tools that support reading multi-modal documents, like parse_image and parse_pdf for extracting information from images using vision-language models (e.g., GPT-4V) and reading text from PDFs, respectively. A complete list of supported skills can be found in \u00a7H."}, {"title": "2.4 Agent Delegation: Cooperative Multi-agent Interaction", "content": "OpenDevin allows interactions between multiple agents as well. To this end, we use a special action type AgentDelegateAction, which enables an agent to delegate a specific subtask to another agent. For example, the generalist CodeActAgent, with limited support for web-browsing, can use AgentDelegateAction to delegate web browsing tasks to the specialized BrowsingAgent to perform more complex browsing activity (e.g., navigate the web, click buttons, submit forms, etc.)."}, {"title": "3 AgentHub: A Hub of Community-Contributed Agents", "content": "Based on our agent abstraction (\u00a72.1), OpenDevin supports a wide range of community-contributed agent implementations for end users to choose from and act as baselines for different agent tasks.\nCodeAct Agent. CodeActAgent is the default generalist agent based on the CodeAct framework [63]. At each step, the agent can (1) converse to communicate with humans in natural language to ask for clarification, confirmation, etc., or (2) to perform the task by executing code (a.k.a., CodeAct), including executing bash commands, Python code, or browser-specific programming language (\u00a72.2). This general action space allows the agent (v1.5 and above) to perform various tasks, including editing files, browsing the web, running programs, etc.\nBrowsing Agent. We implemented a generalist web agent called BrowsingAgent, to serve as a simple yet effective baseline for web agent tasks. The agent is similar to that in WebArena [79], but with improved observations and actions, with only zero-shot prompting. At each step, the agent prompts the LLM with the task description, browsing action space description, current observation of the browser using accessibility tree, previous actions, and an action prediction example with chain-of-thought reasoning. The expected response from the LLM will contain chain-of-thought reasoning plus the predicted next actions, including the option to finish the task and convey the result to the user. Full prompts are in \u00a7J. It can be extended to create more capable web agents, or called by other agents through delegation (\u00a72.4) to enable browsing capability.\nGPTSwarm Agent. GPTSwarm [83] pioneers the use of optimizable graphs to construct agent systems, unifying language agent frameworks through modularity. Each node represents a distinct operation, while edges define collaboration and communication pathways. This design allows automatic optimization of nodes and edges, driving advancements in creating multi-agent systems.\nMicro Agent(s). In addition, OpenDevin enables the creation of micro agent, an agent specialized towards a particular task. A micro agent re-uses most implementations from an existing generalist agent (e.g., CodeAct Agent). It is designed to lower the barrier to agent development, where community members can share specialized prompts that work well for their particular use cases. Without programming, a user can create a micro agent by providing the agent's name, description, the schema for its inputs and outputs, and optionally a specialized prompt (e.g., example demonstrations showing how to perform a particular task) that gear the generalist agent toward specific tasks, for example, the CommitWriterAgent for generating git commit messages, and the TypoFixerAgent for correcting typos across the entire code repository."}, {"title": "4 Evaluation", "content": "To systematically track progress in building generalist digital agents, as listed in Tab. 2, we integrate 15 established benchmarks into OpenDevin. These benchmarks cover software engineering, web browsing, and miscellaneous assistance. In this section, we compare OpenDevin to open-source reproducible baselines that do not perform manual prompt engineering specifically based on the benchmark content. Please note that we use 'OD' as shorthand for OpenDevin for the rest of this section for brevity reasons."}, {"title": "4.1 Result Overview", "content": "In OpenDevin, our goal is to develop general digital agents capable of interacting with the world through software interfaces (as exemplified by the code actions described in \u00a72.1). We recognize that a software agent should excel not only in code editing but also in web browsing and various auxiliary tasks, such as answering questions about code repositories or conducting online research."}, {"title": "4.2 Software Engineering", "content": "Next, we report results specifically for software engineering benchmarks in Tab. 4."}, {"title": "4.2.1 SWE-Bench", "content": "SWE-bench [21] is designed to assess agents' abilities in solving real-world GitHub issues, such as bug reports or feature requests. The agent interacts with the repository and attempts to fix the issue provided through file editing and code execution. The agent-modified code repository is tested against a test suite incorporating new tests added from human developers' fixes for the same issue. Each test instance accompanies a piece of \"hint text\" that consists of natural language suggestions for how to solve the problem. Throughout this paper, we report all results without using hint text. A canonical subset, SWE-bench Lite, is created to facilitate accessible and efficient testing. We default to use this subset for testing for cost-saving consideration.2\nResult. As shown in Tab. 4, our most recent version of CodeActAgent v1.8 generalist, using claude-3.5-sonnet, achieves a competitive resolve rate of 26% compared to other open-source agents specialized for software development. We also evaluated CodeActAgent v1.8 using gpt-40-mini. While it only solves 6.3% of the problems, it does so at less than 1% of the cost compared to other models."}, {"title": "4.2.2 HumanEvalFix", "content": "HumanEvalFix [37] tasks agents to fix a bug in a provided function with the help of provided test cases. The bugs are created to ensure one or more test cases fail. We focus on the Python subset of"}, {"title": "4.2.3 ML-Bench", "content": "ML-Bench [57] evaluates agents' ability to solve machine learning tasks across 18 GitHub repositories. The benchmark comprises 9,641 tasks spanning 169 diverse ML problems, requiring agents to generate bash scripts or Python code in response to user instructions. In the sandbox environment, agents can iteratively execute commands and receive feedback, allowing them to understand the repository context and fulfill user requirements progressively. Following the setup from the original paper, we perform agent evaluation on the quarter subset of ML-Bench.\nResults. As shown in Table 4, OpenDevin agents with GPT-40 achieve the highest success rate of 76.47% on ML-Bench, outperforming SWE-Agent (42.64%). Performance drops with less capable models. These results demonstrate the effectiveness of OpenDevin agent in complex ML tasks. We notice that agents show potential in reducing hallucination and syntax errors compared to non-agent approaches in the ML-LLM-Bench settings [57]."}, {"title": "4.2.4 Gorilla APIBench", "content": "Gorilla APIBench [46] evaluates agents' abilities to use APIs. it incorporates tasks on TorchHub, TensorHub, and HuggingFace. During the evaluation, models are given a question related to API usage, such as \"identify an API capable of converting spoken language in a recording to text.\" Correctness is evaluated based on whether the model's API call is in the correct domain.\nResults. As shown in Table 4, OpenDevin using GPT-40, with a success rate of 36.4%, outperforms baselines not specifically finetuned for API calling. While Gorilla shows higher performance on APIBench, Patil et al. [46] finetune this model for API calling in particular."}, {"title": "4.2.5 ToolQA", "content": "ToolQA [81] evaluates agents' abilities to use external tools. This benchmark includes tasks on various topics like flight status, coffee price, Yelp data, and Airbnb data, requiring the use of various tools such as text tools, database tools, math tools, graph tools, code tools, and system tools. It features two levels: easy and hard. Easy questions focus more on single tool usage, while hard questions emphasize reasoning. For evaluation, the easy subset is used to assess tool use capabilities.\nResults. Compared to all baselines, OpenDevin with GPT-40 shows the highest performance. We notice that agents perform better on tasks related to CSV and database tool usage but requires improvements on math and calculator tool usage."}, {"title": "4.2.6 BioCoder", "content": "BioCoder [58] is a repository-level code generation benchmark that evaluates agents' performance on bioinformatics-related tasks, specifically the ability to retrieve and accurately utilize context. The original prompts contain the relevant context of the code; however, in this study, we have removed them to demonstrate the capability of OpenDevin to perform context retrieval, self-debugging, and reasoning in multi-turn interactions. BioCoder consists of 157 Python and 50 Java functions, each targeting a specific area in bioinformatics, such as proteomics, genomics, and other specialized domains. The benchmark targets real-world code by generating code in existing repositories where the relevant code has been masked out.\nResults. Table 4 shows that OpenDevin, using GPT-40, achieves a success rate of 44.0%. This outperforms all prompting-based non-agent baselines, with GPT-4 alone only achieving 6.4%. BioCoder proves to be a particularly challenging benchmark for non-agent methods, as they did not incorporate any repository-level retrieval methods, making these models lack access to crucial repo-level information such as global variables and function declarations."}, {"title": "4.2.7 BIRD", "content": "BIRD [27] is a benchmark for text-to-SQL tasks (i.e., translate natural language into executable SQL) aimed at realistic and large-scale database environments. We select 300 samples from the dev set to integrate into OpenDevin and evaluate on execution accuracy. Additionally, we extend the setting by allowing the agent to engage in multi-turn interactions to arrive at the final SQL query, enabling it to correct historical results by observing the results of SQL execution.\nResults. As shown in Table 4, OpenDevin with GPT-4o achieves an execution accuracy of 47.3% on a subset of BIRD, showcasing the potential of OpenDevin as a SQL agent. The result outperforms approaches utilizing prompting with code LLMs, such as CodeLlama-7B-Instruct (18.3%) and CodeQwen-7B-Chat [3] (31.3%)."}, {"title": "4.3 Web Browsing", "content": "We report evaluation results for web browsing benchmarks in Tab. 5."}, {"title": "4.3.1 WebArena", "content": "WebArena [79] is a self-hostable, execution-based web agent benchmark that allows agents to freely choose which path to take in completing their given tasks. WebArena comprises 812 human-curated task instructions across various domains, including shopping, forums, developer platforms, and content management systems. Each task is paired with a handwritten test case that verifies agent success, e.g., by checking the status of a web page element against a reference or the textual answer returned by the agent.\nResults. From Tab. 5, we can see that our BrowsingAgent achieves competitive performance among agents that use LLMs with domain-general prompting techniques. Some agents (e.g., AutoWebGLM) require manual effort tailored to the WebArena task domain. This showcases the performance trade-off between a generalist vs. a domain-tailored specialist web agent, and we opt for a more general browsing agent as a building block in OpenDevin."}, {"title": "4.3.2 Mini WoB", "content": "MiniWoB++ [30] is an interactive web benchmark, with built-in reward functions. The tasks are synthetically initialized on 125 different minimalist web interfaces. Unlike WebArena, tasks are easier without page changes, require fewer steps, and provide low-level step-by-step task directions. Note that it contains a portion of environments that require vision capability to tackle successfully,"}, {"title": "4.4 Miscellaneous Assistance", "content": "Results for miscellaneous assistance benchmarks are reported in Tab. 6. In particular, we report results for:"}, {"title": "4.4.1 GAIA", "content": "GAIA [34] evaluates agents' general task-solving skills, covering different real-world scenarios. It requires various agent capabilities, including reasoning, multi-modal understanding, web browsing, and coding. GAIA consists of 466 curated tasks across three levels. Setting up GAIA is traditionally challenging due to the complexity of integrating various tools with the agent, but OpenDevin's infrastructure (e.g., runtime \u00a72.2, tool library \u00a72.3) simplifies the integration significantly.\nResults. In our experiments, we achieved a score of 32.1 on the GAIA (level-1 val), significantly improving over the original AutoGPT [14]. GAIA is sensitive to the support of multimodal input and web navigation skills, suggesting further score improvements as OpenDevin's infrastructure improves."}, {"title": "4.4.2 GPQA", "content": "GPQA [50] evaluates agents' ability for coordinated tool use when solving challenging graduate-level problems. It consists of 448 curated and difficult multiple-choice questions in biology, physics, and chemistry. Tool use (e.g., python) and web search are often useful to assist agents in answering these questions since they provide accurate calculations that LLMs are often incapable of and access to information outside of the LLM's parametric knowledge base.\nResults. Results are shown in Tab. 6 and 7. We observe that OpenDevin's integrated for supporting diverse tool use (e.g., python for calculations) as well as web-search (for searching relevant facts) allows the resulting agent to better solve complex multi-step problems, surpassing the prior state-of-the-art by 9.6% and 12.3% on the main and diamond subsets respectively on GPQA [50]."}, {"title": "4.4.3 AgentBench", "content": "AgentBench [31] evaluates agents' reasoning and decision-making abilities in a multi-turn, open-ended generation setting. We selected the code-grounded operating system (OS) subset with 144 tasks. Agents from OpenDevin interact directly with the task-specific OS using bash commands in a multi-turn manner, combining interaction and reasoning to automate task completion.\nResults. In our experiments (Tab. 6), OpenDevin CodeActAgent v1.5 achieves a score of 57.6% on the AgentBench using gpt-4o, outperforming the 42.4% baseline using gpt-4 from the original paper. Interestingly, when employing weaker models such as gpt-3.5-turbo, OpenDevin agents generally underperform compared to the original baseline agents. This finding suggests that generalist agents, like those implemented in OpenDevin, require a certain threshold of foundation model capability - particularly instruction following - to function effectively."}, {"title": "4.4.4 MINT", "content": "MINT [64] is a benchmark designed to evaluate agents' ability to solve challenging tasks through multi-turn interactions using tools and natural language feedback simulated by GPT-4. We use coding and math subsets used in Eurus [76] for evaluation. We follow the same setting as the original paper and allows the agent to interact up-to five iterations with two chances to propose solutions.\nResults. As shown in Tab. 6), OpenDevin agents achieve comparable performance to the default agent in the original benchmark, with a performance improvement in the math subset."}, {"title": "4.4.5 ProofWriter", "content": "ProofWriter [55] is a synthetic dataset created to assess deductive reasoning abilities of LLMs. Same as Logic-LM [43], we focus on the most challenging subset, which contains 600 instances requiring 5-hop reasoning. To minimize the impact of potential errors in semantic parsing, we use the logical forms provided by Logic-LM.\nResults. In Tab. 6, OpenDevin agent employs a symbolic solver to solve the task, achieving performance comparable to the state-of-the-art neuro-symbolic model (i.e., Logic-LM) [43]."}, {"title": "4.4.6 Entity Deduction Arena", "content": "Entity Deduction Arena (EDA) [77] evaluates agents' ability to deduce unknown entities through strategic questioning, akin to the 20 Questions game. This benchmark tests the agent's state tracking, strategic planning, and inductive reasoning capabilities over multi-turn conversations. We evaluate two datasets \"Things\u201d and \u201cCelebrities\u201d, each comprising 100 instances, and report the average success rate over these two datasets.\nResults. Tab. 6 shows that CodeActAgent yields comparable performance comparing with the results reported in the original paper [77]."}, {"title": "5 Conclusion", "content": "We introduce OpenDevin, a community-driven platform that enables the development of agents that interact with the world through software interfaces. By providing a powerful interaction mechanism, a safe sandboxed environment, essential agent skills, multi-agent collaboration capabilities, and a comprehensive evaluation framework, OpenDevin accelerates research innovations and real-world applications of agentic AI systems. Despite challenges in developing safe and reliable agents (\u00a7A), we are excited about our vibrant community and look forward to OpenDevin's continued evolution."}, {"title": "B Ethics Statement", "content": "Most AI agents today are still research artifacts and lack the ability to perform complex, long-horizon tasks in the real world reliably. However, as their performance continues to improve and they are increasingly deployed in real world, they have the potential to boost productivity while also posing security risks to society significantly. OpenDevin helps mitigate risks by:\n(1) Enabling systematic evaluation of these agents, which can identify and address risks before they are widely deployed.\n(2) Facilitating human-agent interaction rather than allowing agents to operate autonomously without oversight.\n(3) More importantly, we hope OpenDevin allows researchers worldwide to access the best suites of agents to conduct frontier safety research towards building safe and helpful agents."}, {"title": "C Related Work", "content": "The breakthroughs in large language models (LLMs) like ChatGPT [39] and GPT-4 [41] have significantly enhanced the capabilities of autonomous agents across various domains [10, 44, 59, 74]. These advances have spurred a multitude of generalist agent proposals [14, 38, 67] aimed at performing diverse user tasks and have gained attention from both developers and broader audiences. Notable works such as Auto-GPT [14] harness LLMs for task completion by decomposing user goals into executable steps. Multi-agent collaboration systems leverage LLMs for elements like role-playing and task-solving capabilities [26, 61, 80, 82], with MetaGPT [16] emphasizing standardized operating procedures, and AutoGen [67] providing a conversation framework for interactive systems. AGENTS [80] and AutoAgents [7] offer new paradigms for customizable agent architecture, while XAgent [61] and GPTSwarm [83] introduce complex management systems and optimizable graphs, respectively, for enhanced agent operations.\nSoftware development, a front-runner in applying LLM-based agents, has seen advancements in frameworks for facilitating the development processes [16, 48]. Innovations such as ChatDev [48] automate the software development lifecycle akin to the waterfall model, and AutoCodeRover [78] addresses GitHub issues via code search and abstract syntax tree manipulation. AgentCoder [17] iteratively refines code generation with integrated testing and feedback, while SWE-Agent [72] integrates LLMs for automated Github issue fixing, streamlining software engineering."}, {"title": "D Graphical User Interface", "content": "Besides running from the command line, OpenDevin features a rich graphical user interface that visualizes the agent's current actions (e.g., browsing the web, executing base commands or Python code, etc.) and allows for real-time feedback from the user. Screenshots of the UI are shown in Fig. 1. The user may interrupt the agent at any moment to provide additional feedback, comments, or instruction while the agent is working. This user interface directly connects with the event streams (\u00a72.1) to control and visualize the agents and runtime, making it agent and runtime agnostic."}, {"title": "E Quality Control: Integration Tests for Agents", "content": "Integration tests [25] have long been used by software developers to ensure software quality. Unlike large language models with simple input-output schema, agents are typically complex pieces of software where minor errors can be easily introduced during the development process and hurt final task performance. While running a full suite evaluation (\u00a74) is the ultimate measure of performance degradation, running them for every code changes can be prohibitively slow and expensive. 7. In OpenDevin, we pioneer an end-to-end agent test framework that tests prompt regression, actions, and sandbox environments. It combines integration testing from software engineering and foundation model mocking for deterministic behavior to prevent the accidental introduction of bugs during agent development.\nDefining an integration test. The integration test framework for OpenDevin is structured to validate end-to-end functionality by automating task execution and result verification. Developers define tasks and expected results; for instance, a task might involve correcting typos in a document named \"bad.txt\". Upon task execution through OpenDevin, outputs are compared against a predefined \"gold file\" to ensure accuracy.\nMocking LLM for deterministic behavior. Addressing the challenge of non-determinism in large language models (LLMs) and the associated high costs, the framework intercepts all LLM calls and supplies predefined responses based on exact prompt matches. This method not only ensures consistency in test outcomes but also reduces operational costs by minimizing the reliance on real LLMs.\nRegenerate LLM responses on breaking changes. Prompt-response pairs are managed through a script that generates and stores these pairs when new tests are introduced or existing prompts are modified. For routine tests, the framework attempts to reuse existing LLM responses by slightly adjusting the prompts. Substantial changes that affect task handling require regeneration of these pairs using real LLMs.\nBenefits of integration tests. The framework offers several advantages, including 1) Prompt regression testing: Stored prompt-response pairs facilitate change tracking and provide a reference for new team members to understand LLM interactions, 2) Multi-platform support: Tests are automatically scheduled for every pull request and commit on the main branch, running across multiple platforms, environments, and agents, including Linux and Mac, and in local, SSH, and exec sandboxes, and 3) Comprehensive error detection: It captures errors in prompt generation, message passing, and sandbox execution, thereby maintaining a high test coverage."}, {"title": "F Additional Results For GPQA Benchmark", "content": "We showcase more detailed results, including performance on other subsets for GPQA benchmark in Tab. 7."}, {"title": "GIn-context Demonstration for CodeActSWEAgent", "content": "The prompt is re-adopted from the SWE-agent's released trajectory (https://github.com/princeton-nlp/SWE-agent/tree/main/trajectories/demonstrations). The prompt can be found at https://github.com/OpenDevin/OpenDevin/blob/main/agenthub/codeact_swe_agent/prompt.py."}, {"title": "H Supported AgentSkills", "content": "As of OpenDevin v0.6, we support the following list of skills. Please refer to the source code for the most up-to-date list of skills: https://github.com/OpenDevin/OpenDevin/blob/main/opendevin/runtime/plugins/agent_skills/agentskills.py\ndef open_file(path: str, line_number: Optional [int] = None) -> None:\n\"\"\"\nOpens the file at the given path in the editor. If line_number is\nprovided, the window will be moved to include that line.\n\nArgs:\npath: str: The path to the file to open.\nline_number: Optional[int]: The line number to move to.\n\"\"\"\npass\ndef goto_line(line_number: int) -> None:\n\"\"\"\nMoves the window to show the specified line number.\nArgs:\nline_number: int: The line number to move to.\n\"\"\"\npass\ndef scroll_down() -> None:\n\"\"\"Moves the window down by 100 lines.\nArgs:\nNone\n\"\"\"\npass\ndef scroll_up() -> None:\n\"\"\"Moves the window up by 100 lines.\nArgs:\nNone\n\"\"\"\npass\ndef create_file(filename: str) -> None:"}, {"title": "I BrowserGym Actions", "content": "The following are all the supported actions defined in BrowserGym as of v0.3.4. The actions can be categorized into several types and can be configured to use only a subset of the functionality. There are agent control actions, navigation actions, page element-based actions, coordinate-based actions, as well as tab-related actions. We use these actions from the BrowserGym library as our main browsing action primitives.\ndef send_msg_to_user(text: str):\n\"\"\"\nSends a message to the user.\nExamples:\nsend_msg_to_user(\"Based on the results of my search, the city was\nbuilt in 1751.\")\n\"\"\"\npass\ndef report_infeasible(reason: str):\n\"\"\"\nNotifies the user that their instructions are infeasible.\nExamples:\nreport_infeasible(\"I cannot follow these instructions because there\nis no email field in this form.\")\n\"\"\"\npass\ndef noop(wait_ms: float = 1000):\n\"\"\"\nDo nothing, and optionally wait for the given time (in milliseconds).\nExamples:\nnoop()\nnoop(500)\n\"\"\"\npass\n# https://playwright.dev/docs/input#text-input\ndef fill(bid: str, value: str):"}, {"title": "J Browsing Agent Details", "content": "The following shows an example prompt containing all the information required for the current step to make a prediction about the next browsing actions. Note that we also instruct the agent to predict multiple actions in one turn if the agent thinks they are meant to be executed sequentially without any feedback from the page. This could save turns for common workflows that consist of a sequence of actions on the same page without any observation change, such as filling the username and password and submit in a login page.\n# Instructions\nReview the current state of the page and all other information to find the best possible next action to accomplish your goal. Your answer will be interpreted and executed by a program, make sure to follow the formatting instructions.\n# Goal:\nBrowse localhost:8000, and tell me the ultimate answer to life. Do not ask me for confirmation at any point.\n# Action Space\n16 different types of actions are available.\nnoop(wait_ms: float = 1000)\nExamples:\nnoop()"}]}