{"title": "Temporal Grounding of Activities using Multimodal Large Language Models", "authors": ["Young Chol Song"], "abstract": "Temporal grounding of activities, the identification of specific time intervals of actions within a larger event context, is a critical task in video understanding. Recent advancements in multimodal large language models (LLMs) offer new opportunities for enhancing temporal reasoning capabilities. In this paper, we evaluate the effectiveness of combining image-based and text-based large language models (LLMs) in a two-stage approach for temporal activity localization. We demonstrate that our method outperforms existing video-based LLMs. Furthermore, we explore the impact of instruction-tuning on a smaller multimodal LLM, showing that refining its ability to process action queries leads to more expressive and informative outputs, thereby enhancing its performance in identifying specific time intervals of activities. Our experimental results on the Charades-STA dataset highlight the potential of this approach in advancing the field of temporal activity localization and video understanding.", "sections": [{"title": "1 Introduction", "content": "We address the problem of temporal grounding of activities, also known as temporal activity localization (Gao et al., 2017) or video moment retrieval (Cui et al., 2022). This task involves determining the interval of a specific target activity within a longer context of activities, where the context is provided as a video and the target activity is described in text. However, accurately grounding activities in time poses significant challenges due to the complex and dynamic nature of real-world events, variability in human behavior, and the need for fine-grained understanding of temporal relationships.\nRecent advancements in multimodal large language models (LLMs) (Liu et al., 2023b; Zhang et al., 2024) have opened new avenues for addressing this task. Many methods for temporal grounding primarily rely on specialized architectures and task-specific training (Zhang et al., 2020; Cui et al., 2022), which can limit their generalizability and scalability. In contrast, multimodal LLMs, with their broad knowledge base and reasoning capabilities (Huang et al., 2023; Li et al., 2024), have the potential to overcome some of these limitations.\nIn this paper, we explore the use of pretrained and tuned multimodal LLMs for temporal grounding of fine-grained activities, such as 'putting on shoes' or 'pouring a drink.' We demonstrate that a combination of image-based and text-based LLMs achieves higher accuracy in temporal activity localization than a purely video-based LLM of a similar parameter size. Additionally, we show that instruction-tuning a smaller multimodal LLM on a dataset generated by GPT-4 Vision (OpenAI, 2023) not only qualitatively enhances the model's visual description of activities but also improves its performance in accurately identifying the time intervals of these activities."}, {"title": "2 Related Work", "content": "Multimodal Large Language Models Historically, multimodal vision and language systems for activity or action recognition focused primarily on solving challenges in vision. While some models emphasized aspects of language modeling (Li et al., 2017; Wang et al., 2020; Radford et al., 2021), the role of language was often reduced to providing simple captions or labels in many cases. With the"}, {"title": "3 Approach", "content": "We introduce a two-stage approach for temporal activity grounding. The initial stage utilizes the image-based multimodal LLM to generate detailed descriptions of actions occurring in the individual video frames. In the second stage, we feed the responses from the multimodal LLM to a text-based LLM along with the activity query to pinpoint when the activity occurs within the sequence."}, {"title": "3.1 Multimodal LLM (Image-based)", "content": "The first stage consists of a multimodal LLM capable of image input, such as LLaMA-Adapter (Zhang et al., 2024), LLaVA (Liu et al., 2023b), or GPT-4 Vision (OpenAI, 2023). The role of this stage is to describe the activities occurring in each individual video frame. In our current approach, we use single frames from a video as input as it provides the best quality response. We have experimented with providing different types of inputs to the multimodal LLM and discuss this further in Section 5. We use two different strategies when providing prompts to the multimodal LLM, which we will refer to as the activity prompt and the general prompt.\nActivity prompt This type of prompt provides the context of the action during the prompting of the multimodal LLM. This enables the model to search for the particular action in the frame(s) and provide information with regards to the particular action in question. However, the activity prompt comes with a complication where, if there are multiple activities to identify in a particular video sequence, separate prompts will be required for each frame image/activity combination."}, {"title": "3.2 Text-based LLM", "content": "The second stage uses a text-based LLM, which utilizes descriptions aggregated with each frame output from the first stage, and executes the task of predicting the most likely interval where the particular action would happen. The target action in question is necessarily included in this prompt."}, {"title": "3.3 Video-based LLM", "content": "For comparison purposes, we also use Gemini 1.5 Pro, a multimodal LLM that can take videos directly as input. We use a prompt similar to the one used for the text-only LLM but without providing the image description context."}, {"title": "3.4 Constraints", "content": "A major limiting factor in our approach was the amount of compute resources and cost needed to process ~100k images via the multimodal LLM for each experiment. To mitigate this, while we use the Charades-STA full dataset for our instruction-tuning experiments on our 7B models, we employ the general prompting strategy as described above. For our model comparison experiments, we use the activity prompting strategy to focus on performance but select a smaller test set (STA-Subset, described in Section 4.1) during evaluations."}, {"title": "4 Experiments", "content": "We conduct experiments exploring the different aspects of temporal activity localization using multimodal LLMs. To demonstrate the benefits of instruction-tuning in multimodal LLMs, we create an instruction-tuning dataset from a larger multimodal LLM (e.g., GPT-4 Vision) and instruction-tune a smaller multimodal LLM (e.g., LLaVA 7B) based on this dataset to enhance its descriptive power in responding to activity-related prompts.\nWe also formally evaluate our two-stage approach to temporal activity localization using pretrained and instruction-tuned LLMs, where the first stage involves a multimodal LLM tasked with describing activities within frames from a video, and the second stage employing a language-only LLM for reasoning about the timing of the activity within the video clip.\nWe compare the performance metrics of our tuned model against similar parameter-sized video-based LLMs and evaluate our results by comparing the base multimodal LLM to our tuned model, showing our model performs better in the task of grounding activities temporally via our two-stage LLM approach. We also use LLMs of various types and sizes to compare our two-stage approach, showing how the multimodal and text-based stages perform differently when the underlying LLMs change."}, {"title": "4.1 Data", "content": "Instruction-tuning Dataset A set of ~19,000 activity-related queries generated by GPT-4 Vision using frames randomly extracted from the training set portion of the Charades-STA dataset (Gao et al., 2017). These queries are designed to help the model become more descriptive in responding to activity-related prompts. Each sample is split into two parts, the first prompt always asks for a description of what is happening in the frame, followed by an optional Q&A sequence asking for specific activity-related questions about the frame. The samples were created by using GPT-4 Vision to analyze the frame and provide a verbose description, and generate questions and their answers regarding activities occurring in the frame.\nCharades-STA Activity Dataset We use an activity dataset with longer duration activities for our evaluations, which were also similarly used in Cui et al. (2022). The Charades (Sigurdsson et al., 2016) dataset contains 9848 videos of 157 action classes in total, with 27847 textual descriptions. The Charades-STA (Gao et al., 2017) dataset (abbreviated as STA in subsequent sections) is a selected portion of Charades that contains sentence temporal annotations, with a total of 6672 videos of 16128 sentence temporal annotations, where 3720 annotations from 1334 videos are used for testing. For image inputs, we extracted images from each video at a rate of 1 frame per second (fps), resulting in an average of 31 frames per video. We use the Charades-STA test set for our instruction-tuning evaluations for the purpose of comparing our results with existing video-based LLM evaluations.\nCharades-STA Subset (STA-Subset) We defined a smaller subset of the Charades-STA activity dataset, denoted as STA-Subset in the subsequent sections, consisting of 128 videos each with one sentence temporal annotation. This results in a total of 4000 frames when extracted into images at 1 fps. The smaller subset is used due to the prohibitive cost in both compute and time in running multimodal LLMs over each frame/sentence temporal annotation combination in the original Charades-STA dataset. We use this subset for comparing between different LLMs in our two-stage approach comparison evaluations."}, {"title": "4.2 Evaluation method", "content": "We use the metric recall of threshold-bounded temporal intersection over union, denoted as \"R@n, IoU=m\" (Gao et al., 2017). In all of our quantitative evaluations, we use n = 1, and abbreviate the metric as R@IoU=m. The variable m in R@IoU can have fractional values, indicating the threshold of overlap of the temporal prediction of the action with the ground truth. As used in Gao et al. (2017), evaluations are conducted using R@IoU thresholds of {0.3, 0.5, 0.7}, and a single value averaging the different R@IoU values is noted as mean averaged IoU (mIoU)."}, {"title": "4.3 Experimental details", "content": "Instruction-tuning For the baseline, we ran inference on a non-modified LLaVA 7B (LLAVA-1.5-7B) model using the general prompt strategy introduced in Section 3.1. We then instruction-tuned the model using the dataset described in Section 4.1 using the finetune_task_lora.sh script\u00b9 provided by LLaVA over 1 epoch, with a learning rate of 2e-4, for a total of 1.4k steps. Total time for instruction-tuning took around 2 hours on an NVIDIA H100 GPU. Due to the longer context generated by describing each frame in the video, we chose Qwen 7B (Bai et al., 2023) for the text-based LLM as it has a context length of 8192 tokens. Using LLaMA 2 (Touvron et al., 2023), we ran into issues with context length, as the model only has half the context length of that of Qwen models."}, {"title": "4.4 Results", "content": "Instruction-tuning metrics In Table 1, we report the R@IoU metrics for the base model and the instruction-tuned model for both the full Charades-STA dataset (STA) and the STA-Subset. For reference, we also include R@IoU metrics for video-based LLMs as described in Huang et al. (2023); Li et al. (2024), for VideoLLaMA (Zhang et al., 2023), VideoChat (Li et al., 2023b) and VideoChatGPT (Maaz et al., 2023). We observe that the instruction-tuned 7B model performs significantly better than the original LLaVA model and outperforms other video-based 7B LLM models. The instruction-tuned model is more action-focused in its descriptions and is more verbose, enabling the text-based LLM to work with more context compared with the base model. We cover qualitative analysis of instruction-tuning results with examples in Section 5.\nTwo-stage LLM comparisons Metrics for each combination of multimodal and text-based LLMs are shown in Table 2. Overall, the two-stage GPT-4 Vision with GPT-4 model performed best, mostly outperforming the current state-of-the-art video-based LLM, Gemini 1.5 Pro, except when R@IoU=0.7. As seen in Table 2, both the multimodal LLM and text-based LLM contribute in different ways to the temporal activity localization accuracy. There are two interesting observations: in the multimodal LLM, LLaVA 7B seems to perform better than 16B. Also, there appear to be pairs of multimodal LLM and text-based LLMs that work well together and others that don't. For example, given GPT-4 Vision's output, Gemma 7B has a hard time providing proper time interval outputs, many of the outputs in this case are undefined, which significantly affects the resulting metric."}, {"title": "5 Analysis", "content": "Instruction-tuning and multimodal LLM outputs We look at the qualitative effects of instruction-tuning a multimodal LLM to provide a more elaborate description of the scene. This will provide the text-based LLM with more information to better discern the interval in which the activity in question occurred. The following two text boxes show the description extracted from the scene in Figure 1 at timestep 00:19, where the individual is in the process of putting on their shoes.\n The example shows that the tuned model provides a more descriptive output regarding the scene. This includes detailed descriptions of the movements being conducted and the environment, all of which help in gathering context about when a particular activity starts or ends.\nText-based LLM outputs In the text-based LLM setting, each model has its own quirks with regards to outputs. GPT-4 LLM models, while most accurate, simply generate a strict JSON file, making for easy parsing. Gemma 7B models are similar, providing a simple start and end timestamp without explanation. The Qwen 14B and 7B models are more verbose, and the JSON strings that are generated sometimes cannot be parsed using a standard json parser. If this is the case, heuristics are used to extract out time signatures embedded within the text, however, some time signatures may have been missed, negatively impacting performance.\nAnalyzing different input modes In Figure 2, we present prompting outputs for different types of visual input modes: a) using a single image, b) concatenating multiple consecutive images as a single image, and c) using a short 1-second video clip as input. We find that the multiple image and video modalities tend to give inconsistent descriptions, with the single image input mode providing the most detailed and accurate description. This may be due to the fact that multiple split images or 1-second clips are generally not as abundantly provided in the pretraining stage for these multimodal LLMs. Additionally, due to the fact that these models are relatively small (7B), it may even be a struggle to generate accurate descriptions for a single image, let alone multiple images or a video."}, {"title": "6 Conclusion", "content": "We demonstrate that instruction-tuning can be an effective way to generate elaborate and rich descriptions of scenes in smaller-sized multimodal LLMs. Our experiments show that a tuned multimodal LLM model can provide significant performance improvements in the task of temporal activity localization.\nOur proposed two-stage approach, using only pretrained models, performs better than the state-of-the-art video-based LLMs, although it does not outperform state-of-the-art vision models specifically designed for temporal activity localization. The advantage over video-based LLMs is likely due to the higher accuracy and detail provided by image-based multimodal LLMs, as they are currently better trained and developed. Notably, the LLaVA 7B multimodal LLM performs well in the task of temporal grounding of activities, even exceeding the performance of Gemini 1.5 Pro when coupled with a strong text-based LLM.\nWhile our current work focused on a single description of a scene, an obvious next step is to tap into the reasoning capabilities of the LLM. Our instruction-tuning dataset includes additional Q&A regarding actions within the scene, which, through further instruction-tuning, is expected to help the model gain additional insight into the actions occurring in the scene. Furthermore, exploring methods such as chain of thought (Lu et al., 2022) or tree of thought (Yao et al., 2024) prompting could be a valuable direction for tapping into the reasoning capabilities of the LLM and improving performance."}]}