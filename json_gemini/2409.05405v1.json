{"title": "A Survey of Multimodal Composite Editing and Retrieval", "authors": ["Suyan Li", "Fuxiang Huang", "Lei Zhang"], "abstract": "In the real world, where information is abundant and diverse across different modalities, understanding and utilizing various data types to improve retrieval systems is a key focus of research. Multimodal composite retrieval integrates diverse modalities such as text, image and audio, etc. to provide more accurate, personalized, and contextually relevant results. To facilitate a deeper understanding of this promising direction, this survey explores multimodal composite editing and retrieval in depth, covering image-text composite editing, image-text composite retrieval, and other multimodal composite retrieval. In this survey, we systematically organize the application scenarios, methods, benchmarks, experiments, and future directions. Multimodal learning is a hot topic in large model era, and have also witnessed some surveys in multimodal learning and vision-language models with transformers published in the PAMI journal. To the best of our knowledge, this survey is the first comprehensive review of the literature on multimodal composite retrieval, which is a timely complement of multimodal fusion to existing reviews. To help readers' quickly track this field, we build the project page for this survey, which can be found at https://github.com/fuxianghuang1/Multimodal-Composite-Editing-and-Retrieval.", "sections": [{"title": "INTRODUCTION", "content": "In today's digital landscape, information is conveyed through various channels such as text, images, audio, and radar, etc. resulting in a significant increase in data volume and complexity. As data expands exponentially, the challenge of processing and integrating diverse information becomes critical. Efficient retrieval of personalized and relevant information is increasingly challenging.\nTraditional unimodal retrieval methods [37], [49], [55], [83], [86], [87], [226]\u2013[228], [237], [239] depend on a single modality, such as images or text, as queries. However, these approaches often struggle to fully capture the complexities and subtleties of real-world information-seeking scenarios. This limitation has led to the emergence of multimodal composite image retrieval [11], [21], [28], [88], [106], [172], [190], a promising framework that transcends the boundaries of individual modalities. By utilizing the complementary strengths of various data types, multimodal composite retrieval systems enhance the comprehension of user queries and contexts, resulting in improved retrieval performance and user satisfaction.\nAs illustrated in Fig. 1, multimodal composite retrieval involves the intricate process of merging and analyzing diverse data forms (e.g., text, images, audio) to retrieve information. This methodology is invaluable across multiple real-world contexts, including multimedia content [80], social media platforms, and e-commerce [59], [70], [150], [194], [203]. Furthermore, its applications refer to specialized fields such as medical image retrieval [19], [65], [144], document retrieval [72], [80], and news retrieval [178]. By employing diverse multimodal queries, these techniques yield flexible and accurate results, thereby enhancing user experience and facilitating informed decision-making. Consequently, multimodal composite retrieval possesses significant potential and research value in information science, artificial intelligence, and interdisciplinary applications.\nMost existing multimodal composite retrieval methods [4], [11], [27], [28], [77], [85], [88], [106], [115], [132], [190] primarily focus on integrating images and text to achieve desired outcomes. Early methods employed Convolutional Neural Networks (CNNs) for image encoding and Long Short-Term Memory (LSTM) networks [108] for text encoding. With the rise of powerful transformers, such as Vision Transformer (ViT) [186], Swin Transformer (Swin) [128], and BERT [102], numerous transformer-based multimodal composite retrieval methods [184], [208] have been proposed to enhance image retrieval performance. Additionally, Vision-Language Pre-training (VLP) [94], [120], [121], [158] has transformed tasks related to image understanding and retrieval by bridging the semantic gap between textual descriptions and visual content. Various VLP-based multimodal composite image retrieval methods [11], [85], [132] have shown promising results. Furthermore, image-text composite editing methods [31], [39], [46], [71], [118], [119], [126], [152], [232] allow users to modify images or generate new content directly through natural language instructions, achieving precise retrieval that aligns with user intentions. The exploration of additional modalities, such as audio [2] and motion [215], is also gaining momentum.\nMotivation. Despite extensive research on multimodal composite retrieval models, new challenges continue to emerge and remain unresolved. There is a pressing need for comprehensive and systematic analysis in this rapidly evolving field. This survey aims to facilitate a deeper understanding of multimodal composite editing and retrieval by systematically organizing application scenarios, methods, benchmarks, experiments, and future directions. We review and categorize over 130 advanced methods in multimodal composite retrieval, providing a solid foundation for further research.\nLiterature Collection Strategy. To ensure a thorough overview of multimodal composite retrieval, we adopted a systematic search strategy that covers a wide range of relevant literature. Our focus includes studies on innovative methodologies, applications, and advancements in multimodal retrieval systems. We selected keywords such as \u201cmultimodal composite retrieval,\u201d \u201cmultimodal learning,\u201d \u201cimage retrieval,\u201d \u201cimage editing,\u201d and \u201cfeature fusion\u201d to encompass various facets of this field. These terms reflect foundational concepts, specific techniques, and emerging trends commonly found in multimodal research. We conducted searches across prominent academic databases, including Google Scholar, DBLP, ArXiv, ACM and IEEE Xplore. This exploration yielded diverse sources, including journal articles, conference proceedings, and preprints. To refine our selection, we excluded studies primarily focused on unimodal approaches or unrelated modalities and manually reviewed the remaining literature for relevance and quality. The final selection process involved evaluating each paper based on its contributions and impact, enabling us to curate key studies for in-depth analysis. By applying these criteria, we aim to provide a comprehensive perspective on the current landscape and future directions of multimodal composite retrieval.\nTaxonomy. To clarify our discussion on related work in multimodal composite editing and retrieval, we taxonomize them into three categories through application scenarios in this survey, i.e., 1) image-text composite editing, 2) image-text composite retrieval and 3) other multimodal composite retrieval, as illustrated in Fig. 2. Specifically, image-text composite editing involves modifying images or creating entirely new content using natural language instructions, which allows users to clearly and intuitively convey their intentions. Image-text composite retrieval involves searching for personalized results by inputting both text and image information, which enhances the search experience by enabling users to locate relevant images based on textual descriptions or generate descriptive text from images. Other multimodal composite retrieval tasks feed various combinations of different modalities, such as audio, motion, and other modalities as inputs, which provides richer and more flexible context-aware retrieval experiences.\nContribution. In summary, our contributions are as follows:\n\u2022\tTo the best of our knowledge, this paper is the first comprehensive review of multimodal composite retrieval, aiming to offer a timely overview and valuable insights for future research in this rapidly evolving field.\n\u2022\tWe systematically organize research achievements, technical approaches, benchmarks, and experiments to enhance understanding of the topic and provide extensive coverage of existing studies with a multilevel taxonomy to cater to the diverse needs of readers.\n\u2022\tWe address the challenges and open questions in multimodal composite retrieval, identifying emerging trends and proposing actionable future research directions that can stimulate innovation in this area."}, {"title": "2 PRELIMINARY CONCEPTS AND APPLICATION", "content": "Preliminary Concepts\nAs illustrated in Fig. 1, multimodal composite retrieval aims to improve information retrieval flexibility and the overall user experience by integrating text, image, and other data forms. The core strength of this technology lies in leveraging the complementary advantages of different data modalities to meet diverse user demands and cognitive preferences.\nImage-Text Composite Retrieval. Fig. 1 (a) depicts the image-text composite retrieval process, which integrates image and text modalities as input to retrieve target images. Specifically, the input consists of a reference image paired with descriptive text, which provides guidance for retrieving the target image. The reference image encapsulates complex abstract information, effectively capturing details such as color, texture, and spatial relationships. Conversely, language enables detailed and explicit descriptions, allowing for the articulation of specific attributes, relationships, and context that may not be immediately apparent in an image. By synergistically combining these complementary modalities, the system could construct a more comprehensive representation of the target object.\nOther Multimodal Composite Retrieval. Fig. 1 (b) illustrates a broader spectrum of multimodal composite retrieval, extending beyond image and text to include additional modalities such as audio, mouse traces, segmentation maps, key poses, color maps, and depth maps. This integration provides a more nuanced understanding of the user's search intention, significantly enhancing the precision and relevance of the retrieved results. By leveraging the comprehensive data offered from different modalities, the system can be well-equipped to accurately identify and retrieve target information.\n2.2 Application Scenarios\nThe applications of multimodal composite retrieval are extensive and encompass multiple industries and domains. Several potential applications for multimodal composite retrieval technology are as follows.\nFashion and E-commerce. The integration of text and image modalities shows considerable potential in the fashion industry [70], [218]. This approach accommodates various cognitive preferences and individual requirements, allowing users to search for items such as clothing based on specific characteristics like color, pattern, and style.\nMedical Diagnostics. In the healthcare sector, multimodal retrieval systems [19] can aid clinicians in locating pertinent images or case studies by merging specific textual descriptions with patient scans, thus facilitating more accurate diagnoses and informed treatment planning.\nSmart Cities and Traffic Management. City management systems can integrate video surveillance, captured images, and remote sensing data to swiftly retrieve relevant pictures or videos through text queries (e.g., a person wearing a red shirt or the most recent traffic accident). This system can also amalgamate sensor data to provide a comprehensive situational analysis, applicable to traffic management, target searches, and emergency response.\nSmart Homes and Personalized Services. In a smart home setting, users can articulate their desired atmosphere"}, {"title": "3 METHODS", "content": "Image-Text Composite Editing\nImage-Text Composite Editing (ITCE) manipulates specific elements within an image based on a given text prompt, which is closely-related with image-text composite retrieval. This is known as text-conditioned image generation, selectively modifying parts of the image related to textual input while leaving unrelated areas intact. Due to its versatility and potential for iterative enhancement, ITCE has wide-ranging applications across various fields. Two major categories for ITCE include generative adversarial networks (GANs) and diffusion models, as shown in Table 1.\n3.1.1 GAN-based Methods\n3.1.1.1 Conditional GAN-based Methods: In the category of GAN-based methods, conditional GANs (cGANs) [141] utilize additional information (e.g., text guidance) as conditioned inputs to generate specific images. We categorize CGAN-based methods into two categories: single-turn generation approaches [46], [119], [126], [137], [145], [192], [232], [235] and multi-turn generation approaches [22], [31], [39], [129].\nSingle-Turn Generation. Most existing image-text composite editing tasks are static, single-turn generation [46], [119], [126], [137], [145], [192], [232], [235]. Among them, [46], [137], [192] focus on enhancing the generator G component. To be specific, SISGAN [46] utilizes an encoder-decoder architecture and a residual transformation unit within the generator, where the encoder and the transformation unit encode combined features of the image and text, based on which the decoder synthesizes images. GEI [192] investigates three distinct generator architectures, including a bucket-based model with individual encoder-decoder structure, grouping similar image transformations, an end-to-end model featuring a single encoder-decoder for images and a recurrent neural network (RNN) for text, and a filter-bank model that specifies transformations using trained convolutional filters. BRL [137] employs a Bilinear Residual Layer as a conditional layer within the generator to improve representation learning. This network consists of an encoding module, a fusion module that integrates the semantics of multiple modalities, a decoding module, and a discriminator that acts as a classifier to determine whether the generated image aligns with the text description. [119], [145], [235] focus on discriminator D enhancement. TAGAN [145] employs a text-adaptive discriminator that evaluates the alignment of text descriptions with images at the word level, which enables fine-grained modifications that precisely target text-related areas while preserving unrelated regions. LightweightGAN [119] adopts a lightweight structure with fewer parameters, including a novel word-level discriminator. It utilizes two distinct image encoders to capture both coarse and detailed information. FocusGAN [235] incorporates a Subject-Focusing Attention (SFA) module to prioritize text-related subjects, a word-level discriminator to discern fine-grained semantic changes and employs a Background-Keeping Cyclic Loss to maintain background consistency. [126] focus on improvement on both the generator G and the disctriminator D, operating under the premise that each image can be decomposed into a domain-invariant content space and a domain-specific attribute space [60], [90], [127]. It models high-dimensional content features to improve generation performance. Specifically, TIM-GAN [232] models the text as neural operators to modify the input image in the feature space. It synthesize the edited from the image feature modified by the text operator on a predicted spatial attention mask.\nMulti-Turn Generation. Multi-turn generation approaches [22], [31], [39], [129] feature iterative modifications through a series of instructions, carried out in multiple steps. SeqAttnGAN [31] employs a neural state tracker to encode the previous image and corresponding text at each step in the sequence, utilizing a sequential attention mechanism. RAM [22] utilizes recurrent attentive models to integrate image and language features. It introduces a termination gate for each image region, which dynamically determines whether to continue extracting information from the textual description after each inference step. Long and Short-term Consistency Reasoning Generative Adversarial Network (LS-GAN) [39] features a Context-aware Phrase Encoder (CPE) and a Long-Short term Consistency Reasoning (LSCR) module, capturing long-term visual changes and aligning newly added visual elements with linguistic instructions. IR-GAN [129] includes a reasoning discriminator to evaluate the consistency between existing visual elements, visual increments, and corresponding instructions.\n3.1.1.2 StyleGAN-based methods: StyleGANs [97], [98] generate high-quality images by operating within welldisentangled latent spaces, which is renowned for its capability to produce high-fidelity images. Many approaches [78], [109], [134], [152], [198], [206], [243] leverage StyleGAN's latent space [98] to effectively disentangle and manipulate both coarse and fine visual features. For example, [206] embeds textual information into the latent space and enhances editing performance by modifying latent codes and searching for manipulation directions, and interpolates latent vectors within pre-trained GAN models [152], [204]. Traditional methods often require large amounts of labeled data to identify meaningful directions in GAN latent space, which ecessitates considerable human effort. Leveraging CLIP's powerful image-text representation capabilities can help relieve this problem. [3], [109], [134], [152], [197], [198], [206], [243] combine StyleGAN's image generation ability and CLIP's universal image-text representation ability to identify editing directions. These StyleGAN-based methods can be classified into two categories: \u201cwithout mask", "with mask": 3, "e4e": 185, "methods": "ControlGAN [117] enables the synthesis of high-quality images while allowing control over specific aspects of the generation process based on natural language descriptions. ManiGAN [118] builds on the multi-stage architecture of ControlGAN by introducing a multi-tiered framework that includes a text-image affine combination module (ACM) and a detail correction module (DCM). Segmentation-aware GAN [71] incorporates an image segmentation network into the generative adversarial framework, similar to ManiGAN [118]. The segmentation encoder is based on the pre-trained Deeplabv3 [26] and detects the foreground and background of the input image, improving the model's ability to generate contextually accurate and visually coherent images.\n3.1.1.4 Other GAN-based methods: Creating and editing images from open-domain text prompts has been challenging, often requiring expensive and specially designed models. VQGAN-CLIP [42] adopts an innovative approach by using CLIP to guide VQGAN, adjusting the similarity between candidate generations and the guiding text. OpenEdit [125] is the first to explore open-domain image editing with open-vocabulary instructions. DE-Net [181] dynamically assembles various editing modules to accommodate different editing needs. CAFE-GAN [113] focuses on editing facial regions relevant to target attributes by identifying specific areas with target and complementary attributes. IIM [173] constructs a neural network that operates on image vectors within the latent space, transforming the source vector into the target vector using an instruction vector.\nDiffusion-based Methods\nWe categorize diffusion-based methods according to the guidance mechanisms, i.e. mask-based methods [5], [18], [33], [41], [147], [151], [160], [195], [234], classifier-free methods [15], [47], [75], [100], [187], [188], [196], [233], and CLIP-based methods [104], [114].\n3.1.2.1 Mask-based methods: Mask-based methods utilize masks to localize specific areas for modification. For instance, Blended Diffusion [5] employs masks for image-text composite editing in either pixel or latent space. Subsequent work [41], [124], [151], [195] has automatic mask generation using cross-attention maps, replacing manual masks with automatic ones. These methods can be further divided into manual mask [5], [147], automatic mask [18], [33], [41], [195], [234], and optional mask [151], [160] approaches.\nManual mask. Glide [147] compares CLIP guidance and classifier-free guidance, finding that the latter is preferred for its ability to leverage internal knowledge for guidance, thereby simplifying conditioning processes that classifiers often struggle with. Blended Diffusion [5] combines CLIP guidance with a denoising diffusion probabilistic model to seamlessly blend edited and untouched image regions by introducing noise at various levels.\nAutomatic mask. InstructEdit [195] employs automatic masking for precise edits by using ChatGPT and BLIP2 [120] to convert text instructions into a segmentation prompt, input caption, and edited caption, using Grounded Segment Anything, which combines Segment Anything [107] and Grounded DINO [124] to generate masks, and using Stable Diffusion [162] to finalize the edited image. DiffEdit [41] automatically infers a mask to guide the denoising process in a text-conditioned diffusion model, minimizing unintended edits. Shape-Guided Diffusion [151] generates object masks from prompts and employs Inside-Outside Attention to constrain attention maps. Custom-Edit [33] customizes diffusion models by optimizing language-relevant parameters and applies P2P [75] and Null-text inversion techniques [142] for precise edits. IIR [234] introduces an Image Information Removal module to preserve non-text-related content while enhancing text-relevant details.\nOptional mask. PRedItOR [160] employs a Hybrid Diffusion Model (HDM), which uses CLIP embeddings for more accurate inversions and enables structure-preserving edits without needing additional inputs or optimization. SDEdit [138] edits images by starting the sampling process from a noisy version of the base image. However, this approach is less effective for fine detail restoration, especially when significant pixel-level changes are required.\n3.1.2.2 Classifier-free methods: Classifer-free methods [15], [47], [75], [100], [187], [188], [196], [233] refer to guiding the generation process by directly adjusting the results from both conditional and unconditional model outputs, instead of using a pre-trained classifier to steer the diffusion process. To mitigate overfitting issues when fine-tuning pre-trained diffusion models on a single image, SINE [233] introduces a novel model-based guidance approach built on classifier-free guidance, which distills the knowledge acquired from a model trained on a single image into the pre-trained diffusion model. Prompt-to-Prompt [75] enhances editing quality by leveraging the visual-semantic data encoded in the intermediate attention matrices of a text-to-image model. However, this technique relies on attention weights, limiting its application to images generated by the diffusion model. MasaCtrl [18] enhances text-image consistency by transforming traditional self-attention in diffusion models into mutual self-attention. Imagic [100], a pretrained text-to-image diffusion model, begins by optimizing a text embedding to produce images that resemble the input image. InstructPix2Pix [15] combines the strengths of GPT-3 [16] and Stable Diffusion [162] to create an image-editing dataset that captures complementary knowledge from both language and images. The success of this training process is highly dependent on the quality of the dataset and the performance of the diffusion model. Unitune [188] builds on the idea that image-generation models can be adapted for image editing through fine-tuning on a single image. PTI [47] designs Prompt Tuning Inversion, an efficient and accurate technique for text-driven image editing. Plug-and-Play [187] is a modern model that utilizes attention maps from intermediate layers to transfer features from one image to another. MDP [196] introduces a framework that delineates the design space for appropriate manipulations, identifying five distinct types: intermediate latent, conditional embedding, cross-attention maps, guidance, and predicted noise.\n3.1.2.3 CLIP-based methods: DiffuseIT [114] presents a diffusion-based unsupervised image translation method leveraging disentangled style and content representations. Inspired by the Splicing ViT [186], DiffuseIT incorporates a loss function that utilizes intermediate keys from the multihead self-attention layers of a pre-trained ViT model to guide the DDPM model's generation process, thereby ensuring content preservation and enabling semantic alterations. DiffusionCLIP [104] employs a deterministic DDIM noising process to accurately identify the specific noise required to generate the target image.\nSummary\nGANs are renowned for their ability to generate high-fidelity images, making them a popular choice in image-text composite editing. Key techniques within this approach include disentangling the latent space and optimizing generator parameters to improve cross-modal feature alignment. By leveraging the representational power of CLIP, GANs can more effectively identify latent directions and measure similarity with text prompts. These capabilities allow for precise and controlled image manipulation based on textual descriptions. Diffusion models have recently emerged as a powerful alternative, excelling in synthesizing high-quality images from noise through iterative denoising. By integrating with various image-text methodologies, diffusion models have significantly pushed the boundaries of image editing, particularly in terms of quality and flexibility. While GANs are adept at generating high-resolution images with controlled edits and diffusion models offer a more systematic and iterative approach to image generation, especially in complex scenarios, the key distinction lies in the underlying process and training strategy, i.e., adversarial training of GAN and progressive denoising of diffusion.\nSome challenges and perspectives are summarized as follows.\n1) Consistency Maintenance: Future techniques should focus on maintaining consistency by ensuring that text-irrelevant areas of an image remain unchanged, while selectively modifying attributes described in the text. This is crucial for preserving overall image coherence during targeted edits.\n2) Precision Enhancement: Improving precision by enabling the manipulation of specific attributes across multiple objects within an image is essential. This includes refining the granularity of edits, particularly in complex scenes with multiple objects.\n3) Robustness in Complex Scenarios: Enhancing the robustness of models to execute realistic modifications in open-domain scenarios and complex scenes is another key challenge. As the complexity of scenes increases, the ability to maintain realism after editing becomes increasingly important.\n3.2 Image-text Composite Retrieval\nIn the field of image-text composite retrieval, the objective is to identify a target image by utilizing a reference image alongside textual descriptors that specify differences between reference and target images. The text is used to modify the reference image. Since this task involves aligning reference image with target image by introducing the modification instructions in the text, the task can also be referred as", "retrieval": "nAs shown in Fig. 3, a standard framework for composite image retrieval comprises three main components: feature extraction, image-text composition module, and alignment. Traditionally, image representations are derived from the final layer of Convolutional Neural Networks (CNNs) or Vision Transformers (ViTs), while text encoders typically rely on Recurrent Neural Networks (RNNs), LSTMs [61], GloVe [154], BERT [102], and GRUs [32]. Recently, with the advent of large models, encoders from CLIP [158] have become increasingly popular due to their well-aligned text and image representations.\nBased on a comprehensive review, we provide a taxonomy of the image-text composite retrieval methods in terms of the image encoder backbone, including CNN-based (\u00a73.2.1), Transformer-based (\u00a73.2.2), large model-based (\u00a73.2.3), and hybrid methods (\u00a73.2.4). From a framework perspective, some methods [11], [28], [172], [190] focus on designing the composition module to enhance performance, while others [21], [30], [88], [106], [172] emphasize additional modules to improve performance, and [27], [44], [92] aim to enhance the overall framework. The development of image-text composite retrieval (ITCR) has seen a significant evolution, transitioning from CNN-based to Transformer-based backbones, and more recently to large models. This progression has been driven by advances in deep learning within both computer vision and natural language processing. Large-scale pre-trained cross-modal models like CLIP [158] and BLIP [121] have further enhanced ITCR, leveraging their robust capabilities in multimodal representation. For clarity, we provide a detaied summary of ITCR methods in Table 2.\n3.2.1 CNN-based Methods\nConvolutional Neural Networks (CNNs) [112] have been pivotal in extracting hierarchical features from images. [6] has demonstrated that activations in the upper layers of a CNN serve as sophisticated visual content descriptors of an image. Specifically, a CNN (e.g., AlexNet [111], VGG [175], ResNet [112], DenseNet [89], GoogleNet [177] and MobileNet [79]) pre-trained on ImageNet [193] can be used to obtain image embedding by employing global pooling in the last CNN layer, and show remarkable success in various computer vision tasks [51], [52], [73], [81], [199], [213]. Consequently, many CNN-based methods [4], [21], [27], [30], [44], [77], [88], [92], [106], [115], [190], [201], [214], [217], [221], [240], as shown in Table 2, adopt CNN backbones as the image encoder for conducting ITCR task. To achieve more granular feature extraction, SAC [92] employs multiple levels to capture both coarse and fine-grained features. LBF [77] utilizes Faster R-CNN [161] to improve the composition of text and image features. The fusion of these features is commonly categorized into coarse and fine-grained approaches. Coarse fusion, as proposed in [4], [27], [30], [44], [77], [88], [106], [115], [190], [214], [217], involve integrating high-level features from each modality into a single, unified representation, which enhances retrieval performance by maintaining the overall context. In contrast, fine-grained fusion, as proposed in [21], [92], [201], [221], [240], divide features into separate modules (e.g., style and content) and then combine the outputs to form a final representation.\n3.2.1.1 Coarse fusion methods: Coarse fusion is a technique commonly used in multimodal composite retrieval systems to integrate information. It involves synthesizing high-level features extracted from each modality into a single, unified representation. The goal is to capture the critical information from each modality while preserving the overall context, thus enhancing retrieval performance.\nGating Mechanism. In Text Image Residual Gating (TIRG) [190], the task of text-guided image semantic alignment is first proposed by employing a learned gated residual connection and a residual connection, in order to selectively modifies image features based on the text description while preserving the aspects of the image unrelated to the text. Many subsequent methods [27], [214], [217] adopt the gating mechanism of TIRG as their composition module. JVSM [27] jointly learn unified joint visual-semantic matching within a visual semantic embedding framework. It seeks to encode the semantic similarity between visual data (i.e., input images) and textual data (i.e., attribute-like descriptions). CurlingNet [217] designs two networks named Delivery filters and Sweeping filter, the former transits the reference image in an embedding space and the latter emphasizes query-related components of target images in the embedding space, which aims to find better ranking of a group of target images. DCNet [106] introduces the Dual Composition Network, by taking into account both forward (the composition network) and inverse (the correction network, which models the difference between the reference and target images in the embedding space and aligns it with the text query embedding) pathways. EER [224] addresses the composite image retrieval task by methodically modeling two key sub-processes: image semantics erasure and text semantics replenishment. To explore the intrinsic relationships between different modalities, Yang et al. introduce the Joint Prediction Module (JPM) [214]. To alleviate semantic inconsistencies caused by different pre-trained models and distinct latent spaces, AET [229] views the reference image and the target image as a pair of transformed images and regards the modification text as an implicit transformation. To slove the problem of data scarcity and low generalization, RTIC [172] utilizes a graph convolutional neural network (GCN) as a regularizer by facilitating information propagation among adjacent neighbors. Observing that the characteristics of training data significantly influence the training outcomes, and considering that traditional data often results in overfitting and exhibits a low diversity of training distributions, data augmentation becomes crucial. Therefore, Huang et al. [88] propose a gradient augmentation (GA) model for ITCR, an implicit data augmentation inspired by adversarial training for resisting perturbations and a rationale that gradient changes can also reflect data changes to some extent.\nAttention Mechanism. LBF [77] represents the reference image by a set of local entities and establishes relationships between each word of the modification text and these local areas. This approach achieves bidirectional correlation between text and image. It then operates the fusion process by incorporating a cross-modal attention module. JGAN [219] introduces a unified model that simultaneously manipulates image attributes based on modification text using a jumping graph attention network and an adversarial network to learn text-adaptive representations for queries. ARTEMIS [44] treats the modification text as a distributor of weights across the visual representations of both the reference image and the target image and designs an Explicit Matching module and an Implicit Similarity module. CRR [222] introduces a memory-augmented cross-modal attention module for integrating image and text features and two graph reasoning modules to establish intra-modal relationships within the query and the target separately. CIRPLANT [130] is a transformer-based model that utilizes a pre-trained vision-and-language model i.e. Oscar [122] and constructs a multi-layer transformer as the composition module to modify visual features. MAAF [45] extract vector", "tokens": "hat represent elements from each input modality, and then compile these tokens into a unified sequence via an attention model.\nOthers. ComposeAE [4] suggests a model based on auto-encoders and incorporates an explicit rotational symmetry constraint into the optimization process. AMC [240] is an Adaptive Multi-expert Collaborative network, whose routers can dynamically adjust the activation and achieve adaptive fusion of reference image and text embeddings. Scene Trilogy [35] is a unified framework that jointly model sketch, text, and photo to seamlessly support several downstream tasks like fine-grained sketch and text based image retrieval. RBIRR [76] is capable of performing instance retrieval related to multiple objects, providing the category or attribute of the objects and the position constraints among them, including spatial location, size, and relationship. SSIS [135] adopts a", "retrieve": "aradigm, training a convolutional neural network to synthesize visual features that capture the spatial-semantic constraints from the user"}]}