{"title": "Finetuning Generative Large Language Models with Discrimination Instructions for Knowledge Graph Completion", "authors": ["Yang Liu", "Xiaobin Tian", "Zequn Sun", "Wei Hu"], "abstract": "Traditional knowledge graph (KG) completion models learn embeddings to predict missing facts. Recent works attempt to complete KGs in a text-generation manner with large language models (LLMs). However, they need to ground the output of LLMs to KG entities, which inevitably brings errors. In this paper, we present a finetuning framework, DIFT, aiming to unleash the KG completion ability of LLMs and avoid grounding errors. Given an incomplete fact, DIFT employs a lightweight model to obtain candidate entities and finetunes an LLM with discrimination instructions to select the correct one from the given candidates. To improve performance while reducing instruction data, DIFT uses a truncated sampling method to select useful facts for finetuning and injects KG embeddings into the LLM. Extensive experiments on benchmark datasets demonstrate the effectiveness of our proposed framework.", "sections": [{"title": "1 Introduction", "content": "Knowledge graphs (KGs) store real-world facts in multi-relational structures, where nodes represent entities and edges are labeled with relations to describe facts in the form of triplets like (head entity, relation, tail entity). KGs often face the incompleteness problem [12], which adversely affects the performance of downstream knowledge-intensive applications such as question answering [11,24] and recommender systems [13]. KG completion models are designed to resolve the incompleteness issue by inferring the missing facts based on the facts already in KGs. Conventional KG completion models are based on KG embeddings. Given an incomplete fact where either the head or tail entity is missing and requires prediction, embedding-based models first compute the plausibility for candidate entities using an embedding function of entities and relations and then rank them to obtain predictions. Entity and relation embeddings can be learned based on either graph structures [1,3,35,39] or text attributes [17,20,29,30,36]."}, {"title": "2 Related Work", "content": "Related studies can be divided into embedding- and generation-based models."}, {"title": "2.1 Embedding-based KG Completion", "content": "Embedding-based KG completion methods compute prediction probability with entity and relation embeddings learned from either structural or textual features. We divide existing embedding-based models into two categories: structure-based models and PLM-based models."}, {"title": "Structure-based Models", "content": "These models learn embeddings using structural features such as edges (i.e., triplets), paths or neighborhood subgraphs. Therefore, they can be categorized into three groups. The first group comprises triplet-based embedding models to preserve the local relational structures of KGs. They interpret relations as geometric transformations [3,25] or utilize semantic matching methods for scoring triplets [1,19]. The second group contains path-based models [6,34], which predominantly learn probabilistic logical rules from relation paths to facilitate reasoning and infer missing entities. The models in the third group use various deep neural networks to encode the subgraph structures of KGs. CompGCN [28] captures the semantics of multi-relational graphs of KGs based on the graph convolutional networks (GCN) framework. Instead, HittER [5] uses Transformer to aggregate relational neighbor information. Recently, NBFNet [39] employs a flexible and general framework to learn the representation of entity pairs, demonstrating strong performance among structure-based models."}, {"title": "PLM-based Models", "content": "PLM-based models employ PLMs (e.g., BERT [10]) to encode the text attributes of entities and relations in facts, and compute prediction probabilities using the output embeddings. KG-BERT [36] is the first PLM-based KG completion model, which verifies that PLMs are capable of capturing the factual knowledge in KGs. It turns a fact into a natural language sentence by concatenating entity and relation names, and then predicts whether this sentence is correct or not. Following KG-BERT, some subsequent works make improvements in different aspects. StAR [29] divides each fact into two asymmetric parts and encodes them separately with a Siamese-style encoder. SimKGC [30] introduces three types of negatives for efficient contrastive learning. COLE [17] promotes structure-based models and PLM-based models mutually through a co-distillation learning framework. These works are all embedding-based models. They obtain query embeddings and entity embeddings with encoder-only PLMs like BERT."}, {"title": "2.2 Generation-based KG Completion", "content": "Different from embedding-based models that need to learn entity, relation or fact embeddings, generation-based models convert KG completion as a text generation task. These models first translate a KG completion query into a natural language question and then ask a generative language model (e.g., T5 [22] and BART [16]) to give an answer. Finally, they ground answers to entities in KGs using some matching methods. To compare with traditional KG completion models that rank entities based on their scores, generation-based models generate multiple entities with beam search or sampling and rank them by the generation probabilities. GenKGC [33] converts KG completion to sequence-to-sequence generation task to achieve fast inference speed. KGT5 [23] designs a unified framework for KG completion and question answering, but discards the pre-trained weights and trains T5 from scratch. KG-S2S [4] proposes to employ a generative language model to solve different forms of KG completion tasks including static KG completion, temporal KG completion, and few-shot KG completion [15]. Although these works provide some insight into how to conduct KG completion with LLMs, simply replacing PLMs with current LLMs is infeasible as finetuning LLMs on KGs is time-consuming and takes many computational resources.\nWith the emergence of LLMs, several works attempt to adapt LLMs for KG completion. KG-LLM [37] performs instruction tuning on KG completion tasks with relatively smaller LLMs (e.g., LLaMA-7B, ChatGLM-6B) and surpasses ChatGPT and GPT-4, but it still lags behind state-of-the-art KG completion models. KICGPT [31] employs an embedding-based model as the retriever to generate an ordered candidate entity list and designs an in-context learning strat- egy to prompt ChatGPT to re-rank the entities with a multi-round interaction. KICGPT is the most similar work to our proposed method DIFT, because we also employ an embedding-based model to obtain candidate entities and provide them to LLMs. However, accessing closed-source LLMs like ChatGPT is costly as the inference cost grows linearly with the number of missing facts. In contrast, we propose an effective and efficient method to finetune open-source LLMs."}, {"title": "3 The DIFT Framework", "content": "In this section, we describe the proposed DIFT framework for KG completion."}, {"title": "3.1 Notations", "content": "We start by introducing the definitions and notations used in this paper."}, {"title": "3.2 Framework Overview", "content": "Fig. 1 shows the overall framework of the proposed DIFT. In general, DIFT finetunes an LLM M on a given KG G with the help of an embedding-based model ME which has been trained on G in advance. To elaborate, take a tail prediction query q = (h, r, ?) as an example, we feed q into ME to get the top-m predicted entities C = [e\u2081, e\u2082, ..., e\u2098] where m is a predefined hyperparameter. Subsequently, we construct a discrimination instruction P(q) based on the query q and the candidate entities C. Finally, P(q) is fed into M to select the most plausible entity. In this way, we ensure that M always predicts an entity in E as the answer, avoiding grounding the unconstrained output texts from M to entities. For efficient finetuning, we employ ME to score the instruction samples and only keep samples with high confidence. Additionally, to enhance the graph reasoning ability of M, we design a knowledge adaption module to inject the embeddings of q and candidate entities in C obtained from ME into M."}, {"title": "3.3 Instruction Construction", "content": "For a query q = (h, r, ?), we construct the prompt P by integrating four pieces of information: Query Q, Description D, Neighbor facts N and Candidate entities C, which can be represented as:\nP(q) = [Q; D; N; C], (1)\nwhere \";\" is the concatenation operation between texts. We give an example of querying (Titanic, film language, ?), as illustrated in Fig. 1.\nQuery refers to a natural language sentence containing the incomplete fact (h, r, ?). Instead of designing a complex natural language question to prompt off-the-shelf LLMs, we simply concatenate the entity and relation names in the form of a triplet and indicate which entity is missing. During the finetuning process, the LLM M will be trained to fit our prompt format.\nDescription is the descriptive text of h, which contains abundant information about the entity. This additional information helps the LLM M get a better understand of the entity h. For instance, we depict Titanic in Fig. 1 as a 1997 American epic romantic disaster film directed by James Cameron.\nNeighbor facts are obtained by sampling facts related to the entity h. As there may be numerous facts associated with h, we devise a straightforward yet effective sampling mechanism, namely relation co-occurrence (RC) sampling. It is rooted in relation co-occurrence, and streamlines the number of facts while ensuring the inclusion of relevant information. The intuition behind RC sampling lies in the observation that the relations frequently co-occurring with r are considered to be crucial to complete (h, r, ?). For example, the relations film language and film country in Fig. 1 often co-occur, because the language of a film is closely related to the country where it is released. Therefore, we can infer that the language of Titanic is highly likely to be English from the fact that it is an American film. Drawing on the above observation, we sort the neighboring relations of h based on their frequency of co-occurrences with rand subsequently select facts containing these relations until a preset threshold y is reached.\nCandidate entities are the names of top-m entities ranked by the KG embedding model ME. We retain the order of candidate entities since the order reflects the confidence of each entity from ME. We instruct the LLM M to select an entity from the given candidates, thereby avoiding the grounding errors."}, {"title": "3.4 Truncated Sampling", "content": "We design a sampling method to select representative samples to reduce instruction data. The main idea is to opt for high-confidence samples indicated by ME, thereby empowering M to acquire intrinsic semantic knowledge of ME efficiently."}, {"title": "3.5 Instruction Tuning with Knowledge Adaption", "content": "Given the prompt P(q), we finetune the LLM M to generate the entity name of t. The loss of instruction tuning is a re-construction loss:\n\u0421\u1d39 = - \u2211\u1d62<super>i=1</super><super>N</super>log p(y\u1d62 | y<\u1d62, P(q)), (3)\nwhere N denotes the number of tokens in the entity name of t, y\u1d62 (i = 1, 2, . . ., N) represents the i-th token, and p(y\u1d62 | y<\u1d62, P(q)) signifies the probability of generating y\u1d62 with M given the prompt P(q) and tokens that have been generated.\nThe facts provided in P(q) are presented in the text format, losing the global structure information of KGs. Therefore, we propose to inject the embeddings learned from KG structure into M to further improve its graph reasoning ability. We align the embeddings from ME with the semantic space of M, to get the knowledge representations:\n\u00ea = W\u2082 (SwiGLU(W\u2081 \u00b7 e + b\u2081)) + b\u2082, (4)\nwhere \u00ea denotes the knowledge representation obtained based on the embeddings e. W\u2081 \u2208 \u211d<super>d\u2081\u00d7d\u2080</super>, b\u2081 \u2208 \u211d<super>d\u2081</super>, W\u2082 \u2208 \u211d<super>d\u2082\u00d7d\u2081</super>, and b\u2082 \u2208 \u211d<super>d\u2082</super> are trainable weights. d\u2080 is the embedding dimension of ME, d\u2082 is the hidden size of M, and d\u2081 is a hyperparameter. SwiGLU is a common activation function used in LLaMA [27].\nConsidering that Me scores a fact based on the embeddings of the query q and the candidate entity t, we inject the knowledge representations of q and all"}, {"title": "4 Experiments", "content": "4.1 Experiment Setup\nDatasets. In the experiments, we use two benchmark datasets, FB15K-237 [26] and WN18RR [8], to evaluate our proposed framework. FB15K-237 consists of real-world named entities and their relations, constructed based on Freebase [2]. On the other hand, WN18RR contains English phrases and the semantic relations between them, constructed based on WordNet [18]. Notably, these two datasets are updated from their previous versions (i.e., FB15K and WN18 [3]) respectively, they both removed some inverse edges to prevent data leakage. For a detailed overview, the statistics of these two datasets are shown in Table 1.\nEvaluation protocol. For each test fact, we conduct both head entity prediction and tail entity prediction by masking the corresponding entities, respectively. The conventional metrics are ranking evaluation metrics, i.e., Hits@k (k = 1,3, 10) and mean reciprocal rank (MRR). Hits@k is the percentage of queries whose correct entities are ranked within the top-k, and MRR measures the average reciprocal ranks of correct entities. In our framework, the finetuned LLM selects an entity as the answer from the ranking list of candidates. To assess its performance and make the results comparable to existing work, we move the selected entity to the top of the ranking list, and other candidates remain unchanged. We then use Hits@k and MRR to assess the reranked candidate list. We report the averaged results of head and tail entity prediction under the filtered ranking setting [3].\nImplementation details. We run our experiments on two Intel Xeon Gold CPUs, an NVIDIA RTX A6000 GPU, and Ubuntu 18.04 LTS. Text attributes are taken from KG-BERT [36]. We select three representative embedding-based models to experiment with DIFT, namely, TransE, SimKGC, and COLE. Each embedding-based model is pre-trained on the training set. We obtain the top 20 predicted entities for each query in the validation set and test set. We also obtain the embeddings of all queries and entities for knowledge adaption.\nAs for the instruction tuning, we select LLaMA-2-7B\u00b3 as the LLM. We employ LORA [14] for parameter-efficient finetuning. The hyperparameters of LORA are"}, {"title": "4.2 Baselines", "content": "Embedding-based models. We choose eight structure-based models as baselines. Three triplet-based models are selected, including TransE [3], RotatE [25], and TuckER [1]. We also choose two path-based models. Neural-LP [34] is the first model that learns logic rules from relation paths and NCLR [6] is the state-of- the-art path-based model. The remaining models are all graph-based. CompGCN [28] employs GCNs to encode the multi-relational graph structure of the KG, while HittER [5] leverages the Transformer architecture. NBFNet [39] currently performs best among the structure-based models. We also select five PLM-based models as the competitors, namely KG-BERT [36], StAR [29], MEM-KGC [7], SimKGC [30], and COLE [17]. Note that, SimKGC stands the state-of-the-art link prediction model on WN18RR, which benefits from efficient contrastive learning. CoLE promotes PLMs and structure-based models mutually to achieve the best performance on FB15K-237 among PLM-based models. To ensure a fair comparison, we present results derived solely from N-BERT, the PLM-based KG embedding module within CoLE, rather than the entire CoLE framework.\nGeneration-based models. We select three generation-based KG completion models, all of which are either based on BART or T5, namely, GenKGC [33], KGT5 [23], and KG-S2S [4]. Further, we select two recent models based on LLMs as baselines. ChatGPTone-shot is a baseline proposed by AutoKG [38], and KICGPT evaluates it on the whole test sets of FB15K-237 and WN18RR for comparison. KICGPT is the most competitive KG completion model, which employs Rotate to provide the top-m predicted entities for each query and re-ranks these candidates with ChatGPT through multi-round interactions. We also report the performance of DIFT without finetuning, denoted by LLaMA+TransE, LLaMA+SimKGC, and LLaMA+COLE, respectively."}, {"title": "4.3 Main Results", "content": "We report the link prediction results on FB15K-237 and WN18RR in Table 2. Generally speaking, our proposed framework DIFT achieves the best performance in most metrics on two datasets. Compared with the selected embedding-based models TransE, SimKGC, and COLE, DIFT improves the performance of these models on both datasets, significantly in terms of Hits@1. Without finetuning, the performance of DIFT drops dramatically, which demonstrates that it is necessary to finetune the LLM for KG completion task.\nCompared with the LLM-based model ChatGPTone-shot, DIFT consistently outperforms it in terms of Hits@1, regardless of the integration with any of the embedding-based models. This indicates that prompting ChatGPT with in-context learning is less effective than finetuning a smaller LLM with the help of existing embedding-based models for link prediction. Compared with the most competitive baseline model KICGPT which also provides the LLM with candidate entities, the relative improvement brought by DIFT is less. However, KICGPT needs multi-round interactions with ChatGPT, which has 175B parameters. In contrast, DIFT finetunes a small LLaMA with only 7B parameters.\nComparing different metrics, we find that the performance improvement is more significant on Hits@1 while less significant on Hits@10. In DIFT, we ask the LLM to select the plausible entity from the given candidate list. Given that the correct entity is more likely to be ranked in the top 10 entities rather than outside the top 10, the LLM is more likely to select an entity in the top 10 as the answer. Thus, the improvement is more obvious on Hits@1 rather than Hits@10.\nWe also find that the performance improvement on FB15K-237 is more significant than that on WN18RR. This discrepancy can be attributed to the stark disparity in density between the two datasets: FB15K-237 is considerably denser than WN18RR, implying a richer reservoir of knowledge. More knowledge leads to better improvement since the knowledge is provided for the LLM to comprehend in the form of prompts and embeddings."}, {"title": "4.4 Ablation Study", "content": "For the ablation study, we select CoLE as the embedding-based model to provide candidate entities since DIFT with COLE performs best overall on both datasets. We evaluate the effectiveness of two kinds of sampling mechanisms, i.e., truncated sampling and RC sampling, as well as three kinds of support information, i.e., description, neighbors, and embeddings used in knowledge adaption.\nFrom the results presented in Table 3, it is evident that all components contribute a lot to DIFT. Among all these components, truncated sampling has the most substantial impact on performance. The Hits@1 score experiences a degradation of at least 5.6% in the absence of truncated sampling. This shows that this mechanism can effectively select useful instruction data for the LLM to learn intrinsic semantic knowledge of the embedding-based model.\nWe can also observe that the impact of description, neighbors, and RC sampling differs significantly between the two datasets. Without description, the Hits@1 will drop more on WN18RR. This is attributed to WN18RR being a sparse KG with less structural information compared with FB15K-237. Therefore, it needs additional description to enrich entity information, aiding in the differentiation between similar entities. In addition, neighbor information is also"}, {"title": "4.5 Further Analyses", "content": "Effect of the number of candidates. In Section 4.3, we set the number of candidate entities m provided by the embedding-based model to 20. Here we investigate the effect of m on the performance and the training time of DIFT. The results are shown in Fig. 2. First, for the training time, we find that it grows linearly when we increase m. It is intuitive since increasing m leads to longer prompts. Second, as for the performance of DIFT, we find that the performance is best when m is set to 30 on FB15K-237, and there is a slight drop when m is set to 40. The same observation can be found on WN18RR if we continue to increase m after 20. This indicates that blindly increasing the number of candidate entities cannot improve performance. Third, we find that the performance is best when m is set to 30 on FB15K-237 and 20 on WN18RR. That is to say, to achieve the best performance, DIFT needs more candidate entities on FB15K-237 than WN18RR. We think that this discrepancy arises from the generally inferior performance of models on FB15K-237 compared to WN18RR. Consequently, to ensure the presence of answer entities within the prompts, a larger m is advisable on FB15K-237 than on WN18RR.\nEffect of truncated sampling thresholds. In Section 3.4, we use a threshold B to control the quantity of instruction data. To investigate the impact of \u00df on the performance and the training time of DIFT, we conduct an experiment by setting"}, {"title": "4.6 The Finetuning Learns What?", "content": "In this section, we investigate what the LLM learns during our finetuning process. DIFT employs a lightweight embedding-based model to provide candidate entities for both finetuning and inference. A natural question arises: Does the LLM learn the preference of the embedding-based model predictions or the knowledge in the KG? To answer this question, we design the following experiment to evaluate the effect of the candidate order in both the finetuning and inference stages.\nEffect of the order of candidates. DIFT takes the top-m predicted entities from the embedding-based model as the candidates for the LLM. We retain the order of candidates because we assume that the order reflects the knowledge learned by the embedding-based model. Here, to investigate the influence of the order of candidates, we shuffle the candidates in the finetuning or inference stages to ask the LLM to select an entity from the shuffled candidate list. Remember that the shuffled candidate list is only used for entity selection, we move the selected entity to the top of the ranking list from the embedding-based model for evaluation. Results are shown in Table 5, and we have the following observations.\nOn FB15K-237, we employ CoLE as the embedding-based model. We can find that the performance drops dramatically if we finetune the LLM with ordered candidates but shuffle the candidates during inference. We think the reason is that ordered candidates instruct the LLM to select within the top few entities as they are more plausible. Therefore, the LLM still focus on the top few candidates during inference, even though the candidates are shuffled. When we finetune the LLM with shuffled candidates, we find that the performance changes slightly whether the candidates are shuffled or not during inference. The reason is that the LLM has no idea about the preference that the top few candidates are more plausible, so it can not benefit from the order of candidates.\nOn WN18RR, we use SimKGC as the embedding-based model and similar observations can be found. However, we find that the performance of DIFT is even worse than SimKGC when we finetune the LLM with shuffled candidates. This demonstrates that the LLM can not outperform SimKGC solely based on its inherent knowledge without prediction preferences.\nBased on the above analyses, it appears that our DIFT not only captures prediction preferences but also primarily acquires knowledge from the KG."}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we propose a novel KG completion framework DIFT. It finetunes generative LLMs with discrimination instructions using LoRA, which does not involve grounding the output of LLMs to entities in KGs. To further reduce the computation cost and make DIFT more efficient, we propose a truncated sampling method to select facts with high confidence for finetuning. KG embeddings are also added into the LLMs to improve the finetuning effectiveness. Experiments show that DIFT achieves state-of-the-art results on KG completion. In future work, we plan to support other KG tasks such as KGQA and entity alignment."}]}