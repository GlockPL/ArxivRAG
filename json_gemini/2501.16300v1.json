{"title": "Large Models in Dialogue for Active Perception and Anomaly Detection", "authors": ["Tzoulio Chamiti", "Nikolaos Passalis", "Anastasios Tefas"], "abstract": "Autonomous aerial monitoring is an important task aimed\nat gathering information from areas that may not be easily accessible by\nhumans. At the same time, this task often requires recognizing anomalies\nfrom a significant distance and/or not previously encountered in the past.\nIn this paper, we propose a novel framework that leverages the advanced\ncapabilities provided by Large Language Models (LLMs) to actively col-\nlect information and perform anomaly detection in novel scenes. To this\nend, we propose an LLM-based model dialogue approach, in which two\ndeep learning models engage in a dialogue to actively control a drone\nto increase perception and anomaly detection accuracy. We conduct our\nexperiments in a high fidelity simulation environment where an LLM is\nprovided with a predetermined set of natural language movement com-\nmands mapped into executable code functions. Additionally, we deploy\na multimodal Visual Question Answering (VQA) model charged with\nthe task of visual question answering and captioning. By engaging the\ntwo models in conversation, the LLM asks exploratory questions while\nsimultaneously flying a drone into different parts of the scene, providing\na novel way to implement active perception. By leveraging LLM's rea-\nsoning ability, we output an improved detailed description of the scene\ngoing beyond existing static perception approaches. In addition to in-\nformation gathering, our approach is utilized for anomaly detection and\nour results demonstrate the proposed method's effectiveness in informing\nand alerting about potential hazards.", "sections": [{"title": "1 Introduction", "content": "In the last few years, drones have witnessed numerous technological advance-ments, as well as great commercial exposure for their ability to perform difficulttasks, such as surveillance, anomaly detection, and aerial monitoring in chal-lenging environments. To effectively support these tasks and ensure the efficientand autonomous operation of robots, large informative datasets, e.g., containingdrone images, action states, and/or anomalies, were necessary in order to coverevery possible scenario that could occur [1,2,3]. These approaches primarily fo-cused on collecting a large quantity of data and employing different learningtechniques to detect possible anomalies in autonomous drone flying scenarios.\nWith the major advancements in deep learning across numerous domains,there have been multiple attempts to incorporate these modern, more effectivetechnologies for the sake of enhancing autonomous systems' efficiency and ca-pability. By deploying larger, more advanced deep learning models a substantialimprovement in performance was witnessed [4,5]. Nevertheless, these methodslack the ability to actively perceive the scene in order to issue the appropriatecontrol commands and further improve the perception accuracy based on thecurrent conditions. Such active perception approaches have shown promising re-sults in other relevant domains in recent years [6,7,8]. However, it is not trivialto implement such methods in open-world setups."}, {"title": "2 Related Work", "content": "The task of Visual Question Answering [9] has increased in popularity in re-cent years, with the ability to combine computer vision with Natural LanguageProcessing (NLP) resulting in a system that can process two types of differentmodalities at the same time. Such an ability is crucial in robotics applicationsconsidering they are often applied to scenarios and environments that requirehandling such multimodal data. By giving a robot the ability to process multipledata together at once, they increase the quality and quantity of information theyacquire, which in turn expands their overall knowledge of the world. As a result,there have been multiple attempts at applying VQA in robotics. Some worksfocus on having the robot interact with the environment and come up with ananswer to a specific question, mimicking the VQA task. Deng et al. [10] usesVQA in a robotic manipulation scenario. They train a Deep Q Network (DQN)and through reinforcement learning teach the robot to continuously manipulateobjects until they come up with the right answer. In [11] a Hierarchical Inter-active Memory Network (HIMN) was deployed as a controller that allows thesystem to store and retrieve information hierarchically in the form of memoryand enables the robot to provide an answer by interacting with its environmentin real-time. EmbodiedQA [12] is another approach that deploys a robot in anunknown environment in which the robot learns to navigate through using imi-"}, {"title": "3 Proposed Method", "content": "In this work, we aim to equip a drone with active perception and anomaly de-tection capabilities in order to provide a robust scene description, as depicted inFig. 1. First, the drone leverages a VQA model which provides descriptions of theenvironment through captions. In this way, the VQA model provides a way forthe LLM to \"sense\" the environment through text. Additionally, the VQA modeloutputs an image-caption matching score in order to help the LLM distinguishbetween good and bad captions. Then, the LLM validates the gathered textualinformation through the VQAs question-answering module combined with ac-tive perception and ultimately provides a generalized scene description togetherwith explainable attention maps. The outline of the proposed approach is shownthrough an example in Fig. 2. This example should be used as a reference pointthrough the description provided in this Section, since it further clarifies howthe proposed method works.\nFor the VQA model, we incorporate the Plug-and-Play VQA (PnP-VQA)[30] framework, as shown in Fig. 3. To perform the task of image captioning,image-question pairs are processed by a pre-trained vision-language model calledBLIP [31] which is also able to output a similarity score between the image andthe question. The image is split into K patches and through GradCAM [32], afeature-attribution interpretability technique, they are able to provide the mostrelevant image patches. Finally, the image captioning module of BLIP is com-bined with top-k sampling to generate captions only for the relevant patches.Subsequently, the produced caption and question are fed into the question an-swering module to produce the answer. For the LLM, we employed the GPT3.5as our model [33].\nLet the LLM model denoted by $f(A, C)$, which takes two distinct text se-quences as input $A = [A_1, A_2, ..., A_n]$, $C = [C_1,C_2,...,C_m]$ and outputs aresponse sequence $Q = [Q_1, Q_2,..., Q_k]$, in the form of a question i.e., $Q =f(A, C)$, where A denotes the answer to a previous question by the VQA model(if exists) and C denotes a textual description (caption) of the current scene. Inthis work, we employed the GPT3.5 model to implement $f(.)$, while we feed theconcatenated A and C to the model. We assume $A_i$, $C_i$ and $Q_i$ denote the indicesof words, while n, m and k denote the corresponding sequence lengths. Similarly,the VQA network $g(Q, I)$ takes as input the output sequence of the LLM Q, aswell as an image I, producing two different textual sequences A, C = $g(Q, I)$,where A is the answer to the question and C denotes the caption for the image.Then, these outputs are fed to the LLM and this process repeats in an iterativefashion.\nTo grant the LLM control of the drone we first define a set of diverse func-tions, each one in charge of a specific navigational output. Afterwards, we provide"}, {"title": "5 Conclusion", "content": "In this paper, we presented a novel framework that employs LLMs to activelycollect information and detect anomalies, even in unprecedented situations. Wepropose a method where two deep learning models engage in dialogue to controla drone and improve anomaly detection accuracy. We test our approach in arealistic simulation environment, where the LLM follows natural language com-mands to move the drone, while a VQA model answers questions about images.By combining these models, the LLM asks questions while guiding the dronethrough the scene, providing a unique way to improve perception accuracy, aswell as detect potential anomalies. At the same time, by leveraging the explain-ability capabilities of the employed VQA model, the proposed method can alsofurther improve the explainability of the perception process. By providing fourdifferent types of scenes, with different hazardous situations in them and withoutrequiring any fine-tuning or retraining of the models, we demonstrate the po-tential of the proposed method for handling open-ended adaptation in-the-wild.Additionally, to the best of our knowledge, there is currently no other establishedway to implement and evaluate active perception in unstructured open-world se-tups. Therefore, this work opens several research directions, including effectiveevaluation of approaches that extend beyond static perception and pave the wayfor applications in other areas as well."}]}