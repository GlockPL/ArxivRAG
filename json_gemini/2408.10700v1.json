{"title": "AnyGraph: Graph Foundation Model in the Wild", "authors": ["Lianghao Xia", "Chao Huang"], "abstract": "The growing ubiquity of relational data structured as graphs has underscored the need for graph learning models with exceptional generalization capabilities. However, current approaches often struggle to effectively extract generalizable insights, frequently requiring extensive fine-tuning and limiting their versatility. Graph foundation models offer a transformative solution, with the potential to learn robust, generalizable representations from graph data. This enables more effective and adaptable applications across a wide spectrum of tasks and domains. In this work, we investigate a unified graph model, AnyGraph, designed to handle key challenges: i) Structure Heterogenity. Addressing distribution shift in graph structural information; ii) Feature Heterogenity. Handling diverse feature representation spaces across graph datasets; iii) Fast Adaptation. Efficiently adapting the model to new graph domains; iv) Scaling Law Emergence. Enabling the model to exhibit scaling law behavior, where its performance scales favorably with the amount of data and parameter sizes. To tackle these critical challenges, we build the AnyGraph upon a Graph Mixture-of-Experts (MoE) architecture. This approach empowers the model to effectively manage both the in-domain and cross-domain distribution shift concerning structure-level and feature-level heterogeneity. Furthermore, a lightweight graph expert routing mechanism is proposed to facilitate AnyGraph's fast adaptability to new data and domains. Our extensive experiments on diverse 38 graph datasets have demonstrated the strong zero-shot learning performance of AnyGraph across diverse graph domains with significant distribution shift. Furthermore, we have validated the model's fast adaptation ability and scaling law emergence, showcasing its versatility.", "sections": [{"title": "1 Introduction", "content": "The growing ubiquity of relational data in the form of graphs has underscored the pressing need for advanced graph learning models that excel at generalization [7, 13]. As real-world applications of graph-structured data continue to proliferate across diverse domains, including social networks, academic networks, transportation systems, and biological networks, the ability of graph learning models to effectively handle distribution shifts and adapt to new graph domains has become increasingly crucial [20, 29, 37, 38]. Developing models with robust zero-shot learning performance and fast adaptation capabilities can unlock transformative opportunities for leveraging the rich insights encoded within graph data.\nThe field of graph learning has seen significant advancements in recent years, largely driven by the power of Graph Neural Networks (GNNs) [15, 18, 31]. However, the current state-of-the-art models often fall short when it comes to truly generalizable performance. Existing approaches tend to be heavily reliant on arduous fine-tuning processes, making them ill-equipped to handle the diverse array of graph structures and distributions encountered in real-world applications. This inability to adapt swiftly and seamlessly to novel graph domains poses a critical barrier to the widespread adoption of graph learning technologies. Therefore, addressing this challenge is of paramount importance if we are to fully harness the transformative potential of graph-based insights.\nInspired by the principles that have driven the development of successful foundation models in understanding vision and language data [26, 27], the concept of a versatile graph foundation model holds immense potential to unlock new frontiers in graph learning. By learning rich, transferable representations from diverse graph-structured data, such a model can be efficiently adapted to a wide array of graph domains and tasks. However, building an effective and adaptive graph foundation model is not a trivial endeavor. Several key challenges must be overcome, including:\n(i) Structure Heterogeneity. The development of versatile graph models faces the challenge of accommodating diverse structural properties and data distributions in various graph datasets. For instance, graphs can exhibit substantial heterogeneity in node degree distributions, ranging from homogeneous to highly skewed patterns. Similarly, graph structures can vary greatly in complexity, from simple topologies to intricate, hierarchical arrangements. These structural variations can significantly impact the performance and generalization of graph learning algorithms. Effectively addressing this diversity is critical for developing unified models that can thrive across a wide range of graph-structured data.\n(ii) Feature Heterogeneity. Graphs exhibit substantial heterogeneity in their node and edge features, which can span categorical attributes, continuous numerical data, and multi-modal content. Furthermore, the dimensionality and semantics of these features often vary dramatically across different graph domains. For instance, a social interaction graph may include textual content and demographic information associated with its nodes, while a molecular graph may feature atomic compositions and bond types. Effectively handling this feature heterogeneity is crucial for building a versatile graph model capable of generalizing across diverse graph domains.\n(iii) Fast Adaptation for Broad Applicability. A key capability for effective graph foundation models is the ability to efficiently adapt to new graph dataset and domains. Rather than requiring extensive retraining or fine-tuning, the ideal model should be able to quickly adjust its parameters and learning strategies to handle the structural and distributional characteristics of previously unseen graph datasets. By seamlessly generalizing and performing well across a diverse range of real-world scenarios - from user behavior graphs to transportation networks and biological systems \u2013 these adaptable models can unlock transformative insights across an ever-expanding universe of graph-structured data.\n(iv) Scaling Laws for Transformative Graph Capabilities. A key characteristic of successful foundation models in domains like CV [5] and NLP [21] is their ability to exhibit scaling laws - where performance systematically improves as the model size or training dataset increases. By harnessing this emergent scaling phenomenon, graph foundation models can unlock unprecedented levels of capability and generalization, far surpassing the limitations of fixed-capacity architectures. As the size of graph datasets and model complexity grow, these scaling-aware designs can continue delivering transformative performance gains.\nThe Presented Work. To tackle the above challenges, our AnyGraph model is built upon a Mixture-of-Experts (MoE) architecture, which allows for effective handling of both the in-domain and cross-domain distribution shift in structure-level and feature-level heterogeneity. The proposed graph MoE paradigm empowers AnyGraph to learn a diverse ensemble of graph experts, each tailored to specific structural characteristics. This enables the model to effectively manage the distribution shift in graph topologies, ranging from homogeneous to highly skewed degree distributions, as well as handle graphs with varying levels of complexity. Furthermore, the MoE architecture of AnyGraph facilitates fast adaptation of the graph model. Rather than relying on a single, fixed-capacity model, the Graph MoE learns an ensemble of specialized expert networks, each tailored to capture distinct structural and feature-level characteristics of graph data. The lightweight graph expert routing mechanism allows AnyGraph to quickly identify and activate the most relevant experts for a given input graph, without requiring extensive retraining or fine-tuning across the entire model. The key findings of this work can be summarized as:\n\u2022 Methodology Design Motivations of AnyGraph. Current large graph models [4, 16, 17] often struggle when faced with the substantial heterogeneity found in real-world graph data. This is especially challenging when it comes to feature-level heterogeneity. These fixed-capacity models may encounter interference between different types of graph datasets, and can sometimes overfit to new data, leading to catastrophic forgetting. To address these challenges, the proposed Mixture-of-Experts (MoE) architecture for graph models was designed with a focus on adaptability. This new paradigm empowers the model to flexibly adjust to the nuances of diverse graph datasets, dynamically selecting the most appropriate experts to learn distinct patterns.\n\u2022 Stronger Gernealiation Capacities of AnyGraph. Through our extensive experiments, the proposed AnyGraph model with the graph MoE framework has demonstrated strong generalization capacities across a wide range of graph tasks and domains. The experimental results showcase the AnyGraph's ability to outperform existing graph models in terms of both predictive performance and robustness to distribution shift.\n\u2022 Fast Adapability of AnyGraph. Our innovative dynamic expert selection mechanism enhances AnyGraph's ability to swiftly adapt to new graph domains. By dynamically routing inputs through relevant experts, AnyGraph can quickly activate the specialized networks best suited for the task. This strong adaptation sets AnyGraph apart from baselines. Evaluation shows its superiority through rapid convergence and exceptional performance, further justifying its cross-domain versatility.\n\u2022 The Scaling Law of AnyGraph. Our experiments reveal that AnyGraph's performance follows the scaling law, where the model continues to improve as model size and training data increase. Additionally, AnyGraph exhibits emergent abilities, where its generalization capabilities see sudden significant improvements with further scaling. This critical scaling law property has been largely overlooked in prior investigations, but it underscores the immense value that AnyGraph derives from its scaling-driven enhancements to generalization performance."}, {"title": "2 Preliminaries", "content": "Graph-Structured Data. A graph G consists of a set of nodes V = {vi} and a set of edges & = {(vi, vj)}. In many cases, each node vi is associated with a feature vector fi \u2208 Rdo. To efficiently utilize such graph-structured data, the link information is typically recorded using an adjacency matrix A \u2208 R|V|\u00d7|V|. Each element ai,j of A is either 1 or 0, inddicating whether there is an edge from node vi to vj. Additionally, the feature vectors of the nodes are usually represented by a feature matrix F\u2208 R|V|\u00d7do, where each row corresponds to a node's feature vector. The primary goal of learning from such graph-structured data is to generate embeddings for the graph elements, typically nodes, that effectively capture both the structural and feature-based information of the graph.\nGraph Foundation Models (GFMs). The essence of GFMs lies in their strong generalization capabilities. Specifically, a graph foundation model should be able to handle unseen graph data that exhibits significant discrepancies from its training graph datasets. These discrepancies may include differences in feature spaces, as well as variations in node and edge semantics across datasets. Formally, let's denote the training graphs as S = Gs, where each graph Gs is associated with a label set Ys. Similarly, the set of test graphs is denoted as T = Gt, with labels Yt. With a differentiable training"}, {"title": "3 Methodology", "content": "The proposed AnyGraph framework aims to address both cross-domain and in-domain heterogeneity in graph structures and node features, while enabling fast adaptation to new data. The overall framework of AnyGraph is depicted in Fig. 2."}, {"title": "3.1 MoE Architecture of AnyGraph", "content": "3.1.1 Addressing Cross-domain Graph Heterogeneity. To model heterogeneous graph patterns across different domains, AnyGraph employs a MoE architecture that consists of multiple graph expert models, each responsible for handling graphs with specific characteristics. An automated routing algorithm is designed to assign input graph data to the most competent expert model for training and prediction. Specifically, the AnyGraph framework can be denoted as M = (fo\u2081, fo2,\u00b7\u00b7\u00b7, f\u04e9\u043a, \u03c8), where K denotes the number of experts. For an input graph G, the routing algorithm \u03c8 firstly identifies the most competent expert model, and the corresponding model is then used for predicting the graph data, as:\n\u0177i,j = \u00ea\u0109j, \u00ca = f\u00f8r(G), k = \u03c8(G)\nwhere each expert model f\u00f8r can be viewed as a projection from the graph space to a node embedding space with uniquely trained parameters Ok. And \u0177i, j represents the dot-product-based prediction of whether the entity vi should be related to the entity vj. Here, vi and vj could be vanilla graph nodes, class labels, or graph labels.\n3.1.2 Graph Expert Routing Mechanism. Inspired by the effectiveness of graph self-supervised learning tasks [12], we propose measuring the competence of expert models on specific graph datasets using the models' self-supervised learning loss values. Specifically, for an input graph G = (V, &), the routing mechanism \u03c8 calculates the dot-product-based relatedness scores for some positive edges (vc\u2081, vp\u2081),\u00b7\u00b7\u00b7, (vcs, vps) \u2208 & and analogously calculates the relatedness scores for some sampled negative node pairs (vc\u2081, vn\u2081),\u00b7\u00b7\u00b7, (vcs, vns) \u2208 &. The following score difference is then calculated as the competence indicator \u03c6k for the k-th expert model regarding the input graph G:\n\u03c6k = 1/S \u03a3 \u03c3 (\u00eacs\u00eaps - \u00eacs\u00eans)\ns=1\nwhere \u03c3(\u00b7) represents the sigmoid activation function, which constrains the competence score to the range of (0, 1). This prevents the few outlier cases where the non-activated score difference is excessively large or small, which could otherwise distort the results.\nTraining Frequency Regularization. Although being empirically accurate in measuring models' competence using the aforementioned self-supervised task loss, this routing mechanism tends to result in a winner-takes-all sub-optimal situation. In this scenario, a single model, or very few models, is predominantly selected as the most competent expert and is used to handle almost all input graphs. These expert models generally receive more or better training samples in the early training stages, giving them an advantage over other experts. Consequently, subsequent training samples are also mostly assigned to them due to their performance advantages, ultimately causing other experts to remain largely untrained.\nThis situation contradicts our motivation of using different expert models to learn different subsets of graph modeling knowledge. To address this, we propose a training frequency regularization approach that recalibrates the competence score as follows:\n\u03c6k = \u03c6\u03ba \u00b7 (1- (mk/\u03a3k' \u039c\u03ba').p) + (1/K (\u03c1 + 1.0 - (mk/\u03a3\u03ba' \u039c\u03ba').p) )\nwhere \u03c6'k represents the recalibrated routing score for the k-th expert model f\u00f8r, based on the number of previously assigned training steps mk for k = 1,..., K. The notation \u03c1 refers to a hyperparameter for the recalibration scale. A larger \u03c1 results in a greater adjustment to the competence score \u03c6\u03ba. With this additional step, the expert routing mechanism will assign more training instances to the less trained expert models, thereby preventing the aforementioned winner-takes-all situation.\n3.1.3 Fast Adaptation Capabilities of AnyGraph. With the aforementioned MoE architecture and routing mechanism, the training and inference process of AnyGraph is conducted by only one expert model. This approach consumes only 1/K of the computational and memory resources required for predictions and optimization, compared to other non-MoE graph foundation models based on complex networks like transformers. This endows AnyGraph with the advantage of fast adaptation when dealing with new datasets."}, {"title": "3.2 Adaptive and Efficient Graph Experts", "content": "3.2.1 Addressing In-domain Graph Heterogeneity. To handle graph data with different adjacency and feature dimensionalities, the expert models of our AnyGraph employ a structure and feature unification process. Adjacency matrices and node features of varying sizes are both mapped into initial node embeddings of fixed dimensionality using a unified mapping function. Inspired by the effectiveness of singular value decomposition (SVD) in extracting important latent features, we utilize SVD for this unified mapping process as follows:\nUA, AA, VA = SVD(\u00c3) UF, AF, VF = SVD(F)\nEo =LayerNorm (UA\u221aA + VAAA+ Flip (UFAF))\nHere, UA, UA \u2208 R|V|\u00d7d and UF \u2208 R|V|\u00d7d, VF \u2208 Rdxd refer to the d-dimensional features obtained through SVD of the Laplacian-normalized adjacency matrix \u00c3 [14] and the node feature matrix F, respectively. If the dimensionality of A or F is less than d, SVD uses a smaller rank d' equal to the smallest dimensionality of A/F, and the remaining dimensions are padded with zeros up to d.\nDue to the nature of SVD, the dimensions of these features (U*, V*) are ranked from the most important to the least important, corresponding to the descending eigenvalues in the diagonal matrices AA and AF. In light of this characteristic, we propose to better preserve the most important feature dimensions for both A and F. In particular, the function Flip() reverses the d dimensions of each row for the SVD features of F, such that the important features of A are aligned with the less important features of F, and vice versa.\nHigh-order Connectivity Injection. A non-trainable layer normalization LayerNorm(\u00b7) is applied for numerical stability. The initialized embeddings, denoted as Eo \u2208 R|V|\u00d7d, have consistent representation dimensionality and relatively stable semantics across datasets. To better preserve the multi-hop connection information into the initial embeddings, AnyGraph adopts a simplified GCN without parameters [28] for Eo as follows:\nE\u2081 =  \u2211 E(l), E(l) = \u00c3 \u00b7 E(l-1), E(0) = Eo\nl=1\n3.2.2 Efficient and Strong Feature Encoder. To achieve efficiency while retaining the capacity to encode graph features, our graph experts are configured by deep multi-layer perceptron (MLP) networks. Specifically, the final node embeddings given by an expert model is calculated iteratively as follows:\n\u0112(1+1) = LayerNorm (Dropout (ReLU(E(l) W+b)) + E(l))\nThe final embeddings are denoted as \u00ca = \u0112(L') \u2208 R|V|\u00d7d, where L' represents the number of fully-connected layers. And \u0112(0) is initialized by the aforementioned embeddings E1. Each layer of our MLP module comprises a linear transformation W \u2208 Rdxd and bias b \u2208 Rd, followed by a ReLU non-linear activation, a dropout layer, a residual connection, and layer normalization.\nMultiple Simple Experts as Strong Encoder. It is worth noting that each graph expert in AnyGraph adopts a very simple learnable network, foregoing the capacity to mine complex hidden relations like those in heavy graph neural networks such as GATs [25] and GraphTransformers [10]. This is because AnyGraph employs a MoE architecture, where each expert is expected to handle only a sub-domain of all graph data through simple feature transformations. Therefore, no complex models are needed to accommodate different types of graphs within a single network. Compared to other graph foundation models that rely on a single heavy network, this approach further accelerates the training and inference processes."}, {"title": "3.3 Efficient Cross-domain Model Training", "content": "To maximize the cross-graph generalizability of AnyGraph, the training samples from different datasets are mixed together and randomly shuffled for model training. Each batch of training samples contains the following information:\nS = {(Ucb, Upb) b \u2208 B} C &Gs,\nE\u2081 = InitialEmbed(Gs),\nfor where k = \u03c8(Gs)\nInspired by the effectiveness of link-wise graph pre-training tasks [12], we utilize link prediction as the training task. Here, (Ucb, Upb) denotes the positive edges for link prediction, and B denotes the batch size. To facilitate batch training, each training batch involves only one training graph Gs. The initial node embeddings E1 and the most competent expert model f\u00f8r are preprocessed in advance to accelerate the training. Specifically, the loss function used by our AnyGraph training is as follows:\nL =  log(exp(\u0443cb,pb - \u0177max)/ \u2211 (exp(\u0443cb,nb - \u0177max) ))\ns\u2208S\nThis training objective maximizes the prediction scores for positive samples (Ucb, Upb) and minimizes the predictions for all possible node pairs between Uch and all nodes un. To avoid numerical instability, we employ a technique where the maximum prediction score of the batch, \u0177max, is subtracted from all prediction scores."}, {"title": "3.3.1 Feature and Structure Augmentation", "content": "To further enrich the training data, the training of AnyGraph undergoes periodic reprocessing of, firstly, the initial graph embeddings E1, and secondly, the graph routing results. We demonstrate that such reprocessing augments the features and structures of the original graph data, thereby training AnyGraph using more diversified input data.\nFor the initial graph embeddings, we periodically reconduct the SVD and simplified GCN processes after a certain number of training steps. This helps generate different embedding spaces for the same data, thereby greatly improving the generalizability of AnyGraph regarding representation heterogeneity [16]. To prevent this process from consuming excessive computational time, we propose adopting different augmentation frequencies adaptive to the size of different datasets. Specifically, each dataset undergoes this representation augmentation after |E|/(10B) training steps.\nFor the graph routing results, we also periodically recalculate the recalibrated competence scores. Specifically, the positive sample pairs (Ucs, Ups) for s = 1,..., S, as well as the negative samples uns, are randomly sampled. This essentially performs structure augmentation by using a random subset to evaluate the performance of graph experts on the input graph, thereby enhancing the model's robustness against structural noise."}, {"title": "3.3.2 Complexity Analysis", "content": "The training and inference process of AnyGraph is conducted by only one expert model, which has a complexity of O(B\u00d7d\u00b2\u00d7L') for each batch. Since we preprocess the initial embeddings and the expert routing, these two processes do not increase the batch-wise computational complexity. As a result, the complexity of the forward and backward steps for AnyGraph is much lower than that of other graph foundation models that involve complex GNNs and graph transformers. Additionally, the expert routing performs O (\u03a3Gs |&s| \u00d7 d \u00d7 K + \u2211gs |Vs| \u00d7 d\u00b2 \u00d7 L' \u00d7 K) computations, where the latter term empirically has a larger scale compared to the former term. This dominant term is similar to a simple GCN network of a comparable model size. Overall, AnyGraph is more efficient than existing methods in both training and inference, and the additional computations for routing have a complexity comparable to simple GCNs."}, {"title": "4 Evaluation", "content": "Our experiments aim to answer the following Research Questions:\n\u2022 RQ1: How does the zero-shot prediction performance of AnyGraph compare to different baseline methods?\n\u2022 RQ2: How do AnyGraph's various modules impact its overall performance with the contribution of each component?\n\u2022 RQ3: How does the model size and the amount of training data influence the performance of AnyGraph?\n\u2022 RQ4: How interpretable is the expert routing mechanism within AnyGraph's graph Mixture-of-Experts (MoE) architecture?\n\u2022 RQ5: How is the scalability and efficiency of AnyGraph compare to fine-tuning methods when adapting to new datasets?"}, {"title": "4.1 Experimental Settings", "content": "4.1.1 Experimental Datasets. To conduct a comprehensive evaluation of the cross-domain generalizability of graph models, we employ a total of 38 graph datasets. These datasets span a wide range of domains, including e-commerce (e.g. user interactions and product-wise relations), academic graphs (e.g. citation and collaboration networks), biological information networks (e.g. relations among drugs and proteins), and other domains like email networks, website networks, trust networks, and road networks.\nWe set up different dataset groups and conduct cross-dataset evaluations on these groups. Specifically, all datasets are divided into two cross-domain groups, Link1 and Link2, which have a similar number of total edges and a similar number of domain-specific edges. Specifically, the Link1 and Link2 groups contain 15 and 18 datasets, respectively. For the node classification task, we use 5 datasets gathered from e-commerce and academic information scenarios. Additionally, we have three domain-specific groups: Ecommerce, Academic, and Others. The Others group is primarily composed of biological networks, combined with other small domains that have fewer datasets. See Appendix A.1 for more information of our experimental datasets.\n4.1.2 Experimental Settings. We follow previous works [9, 14] for dataset splitting and evaluation metrics. Our AnyGraph model and the graph foundation models are evaluated on a cross-graph zero-shot prediction task. For baselines that cannot handle cross-dataset transfer, we evaluate their few-shot performance. Details of the evaluation protocols are provided in Appendix A.2. The Hyperparameter Settings of AnyGraph are provided in Appendix A.3. The compared Baseline Methods are introduced in Appendix A.4."}, {"title": "4.2 AnyGraph's Zero-Shot Prediction (RQ1)", "content": "To assess the zero-shot prediction capabilities of the AnyGraph model, we conducted an extensive evaluation across 38 graph datasets from various domains. We independently trained two versions of the AnyGraph model - one on the Link1 dataset and the other on the Link2 dataset. Each trained model was then used to make zero-shot predictions on datasets it was not originally trained with. It is important to note that the Link1 and Link2 datasets do not share the same feature spaces or sources of data collection, which adds to the complexity and challenges of the zero-shot evaluation. We also compare our AnyGraph with existing graph foundation models. And in this comparison we add another AnyGraph-F version, which removes the utilization of node features. The outcomes of this evaluation are detailed in Table 1 and Table 2, and our key observations are listed as follows:\ni) Superior Generalizability across Diverse Datasets. Superior Prediction Accuracy. Compared to the few-shot capabilities of existing GNN models, pre-training techniques, and foundation models, AnyGraph demonstrates exceptional zero-shot prediction accuracy across various domains. This superior performance spans both link prediction and node classification tasks. Effectively Handling Heterogeneity. The enhanced generalizability can be attributed to the effective handling of structure-level and feature-level data heterogeneity through unified structure and feature representations in the expert models. This approach enables AnyGraph to develop comprehensive modeling functions that are universally applicable across different graph data scenarios. Comprehensive Training. Additionally, the extensive training regimen, which incorporates a variety of large-scale datasets, equips AnyGraph with a deep and broad expertise in graph modeling and prediction.\nii) Limitation of existing pre-training GNNs. \u2022 Challenges of Cross-Domain Transfer. Existing pre-training and tuning methods, like GPF, GraphPrompt, and GraphCL, employ self-supervised learning and are pre-trained on half the datasets, then fine-tuned on the remaining datasets using few-shot data. However, this pre-training often fails to yield significant improvements due to substantial distribution disparities across data domains. For instance, datasets may exhibit vastly different link densities or utilize distinct node features, which significantly challenges the transfer of useful knowledge from divergent pre-training datasets during fine-tuning and prediction. AnyGraph's Robust Adaptability To address this challenge, the AnyGraph model incorporates multiple graph expert models tailored to various sub-domains of graph data. This MoE architecture effectively manages datasets from distinctly different domains, such as e-commerce user behaviors, academic networks, and road networks, demonstrating its robust adaptability."}, {"title": "4.3 Scaling Law of AnyGraph Framework (RQ2)", "content": "In this section, we explore the applicability of the scaling law to AnyGraph. We conduct experiments using 18 different versions of AnyGraph, each differing in model size and quantity of training data. Specific configurations of these variants are discussed in Appendix A.5. The evaluation results are depicted in Figure 3, which includes overall and domain-specific performance, as well as zero-shot and full-shot outcomes. Our key findings are as follows:\ni) Generalizability of AnyGraph Follows the Scaling Law. As the model size and the volume of training data increase, we notice a saturation point in AnyGraph's full-shot performance. In contrast, the zero-shot prediction accuracy continues to improve. This pattern supports the scaling law of graph foundation models, illustrating that scaling up can significantly enhance the capabilities of graph models. Two key factors contribute to this phenomenon:\n\u2022 Task Difficulty. The saturation in full-shot performance is partly because the evaluation tasks might not be challenging enough. In-domain generalization can be more straightforward, leading to a plateau in performance improvements. This insight into the scaling law for graph data encourages further exploration of larger models on more complex graph learning tasks.\n\u2022 MoE Architecture. The integration of the Mixture of Experts (MoE) architecture allows AnyGraph to effectively manage and utilize a broader spectrum of knowledge, particularly in this zero-shot scenario characterized by significant distribution disparities.\nii) Emergent Abilities of AnyGraph. The overall zero-shot performance curve illustrates that as the model size increases, the performance sometimes experiences periodic stagnation. With further increments in parameters, AnyGraph's performance undergoes a sudden significant improvement. This phenomenon indicates the emergent abilities of AnyGraph, demonstrating the effectiveness of scaling up in enhancing its generalization capabilities.\niii) Insufficient training data may bring bias. In the initial stages of increasing the training data, the introduction of new datasets might negatively impact performance due to their differences from the test graphs. However, this issue can be mitigated by further expanding the training data. By providing the model with a more comprehensive set of training samples, it helps prevent overfitting and reduces bias stemming from dataset disparities."}, {"title": "4.4 Ablation Study (RQ3)", "content": "This section evaluates the effectiveness of AnyGraph's sub-modules by comparing ablated variants in terms of their zero-shot and full-shot performance across both cross-domain datasets and domain-specific datasets (specifically Academic data). The results are in Figure 4. We make the following observations:\n\u2022 MoE Significantly Enhances Zero-Shot Performance. The -MoE variant, which employs a single expert model without the MoE architecture, demonstrates decent performance on datasets on which it was trained, as shown in parts (b) and (c). However, this variant exhibits a substantial decline in zero-shot prediction capabilities. This underscores the critical role of the MoE architecture in enhancing AnyGraph's generalization abilities. The use of multiple expert models significantly expands AnyGraph's modeling capacity, effectively managing the large disparities between various domains using multiple seperated models.\n\u2022 Feature Modeling is Crucial in AnyGraph. In the -Feat variant, node features are omitted, leading to the most significant degradation in both zero-shot and full-shot performance. This underscores the effectiveness of AnyGraph's unified structure and feature representation method in successfully learning features. This component is crucial for tackling in-domain graph data heterogeneity. Additionally, this outcome highlights the feasibility of unifying different feature spaces created by various methods into a single model for general use.\n\u2022 Effectiveness of Frequency Regularization and Graph Augmentation. In the -FreqReg and -Aug variants of AnyGraph, the routing adjustment based on the training frequency of experts and the feature and structure augmentation are individually removed. Frequency regularization specifically prevents suboptimal training outcomes by ensuring that all experts are utilized, avoiding scenarios where the majority are overlooked. Meanwhile, the graph augmentation module plays a crucial role in enriching the training graph data, thus enhancing the model's robustness. The outcomes from these modifications affirm the beneficial impact of these two components within AnyGraph. Omitting them can lead to biased model training, which undermines the robustness of AnyGraph in handling diverse datasets."}, {"title": "4.5 Investigation on Expert Routing (RQ4)", "content": "This section delves into the expert routing mechanism of AnyGraph. Figure 5 displays the competence scores of various expert models for the input datasets, as determined by AnyGraph's routing algorithm based on the self-supervised loss. The figure illustrates that datasets sharing common characteristics-such as source of collection or feature construction method are often routed to the same expert models by AnyGraph. For instance, datasets like arxiv-ta, Photo, Goodreads, and Fitness, which utilize a common text-embedding-based feature space, are assigned to highly similar experts (expert 0, 2, 4, 5). Additionally, ML1M and ML-10M, both sourced from the movie-rating platform Movielens, are predominantly associated with expert 1. It is also notable that this routing pattern extends to zero-shot datasets, as shown on the right part of Figure 5. Here, YelpT, SteamT, and AmazonT, which share the same feature space, are assigned to very similar expert models. This outcome underscores the efficacy of AnyGraph's routing mechanism in identifying the appropriate expert models for various datasets, and also showcases its explainability in revealing graph-wise relatedness."}, {"title": "4.6 Efficiency Study (RQ5)", "content": "Tuning Curve Comparison. To evaluate the efficiency of AnyGraph, we compare its fine-tuning process with that of GraphCL and the training from scratch process of a GCN model. As depicted in Figure 6, when fine-tuned on a new dataset, the pre-trained AnyGraph rapidly achieves a high performance saturation point. In some instances, such as with the PPA dataset, GraphCL and the end-to-end trained GCN struggle to attain comparable performance levels. This advantage is based on i) the strong cross-domain generalization capabilities of AnyGraph, which bring a high starting point for the new dataset, and ii) the efficiency of AnyGraph's MoE architecture, which requires only one MLP network for efficient but effective modeling and parameter tuning.\nIn addition, it is observed that pre-training GraphCL does not consistently benefit its fine-tuning on new datasets, as evidenced by GraphCL's underperformance relative to GCN in Figure 6(b). This should be ascribed to the large distribution gap between the pre-training data Link2 and the test data PPA.\nTraining Time Comparison. To evaluate the efficiency of the models under consideration, we compared the training times of the three models. As indicated in Table 3, AnyGraph, despite having significantly more parameters, has training times that are comparable to, or even less than, the other two models. This underscores the efficiency of our model design, and demonstrates the efficiency of AnyGraph to adapt to new data through model tuning.\nSpecifically, AnyGraph avoids the cumbersome process of full-graph propagation at each training step. Instead, it utilizes structure-aware embeddings derived through a non-trainable pre-processing method. This approach significantly reduces both the time and memory requirements for AnyGraph. Furthermore, the MoE architecture equips AnyGraph with the capability to use only 1/K of the computational resources for most prediction and optimization processes, thereby greatly reducing overall computational costs."}, {"title": "5 Related Works", "content": "Graph Neural Models. Graph learning has garnered significant interest for its broad applicability across various fields such as user behavior modeling, social analysis, and studies in biology and chemistry [2, 8]. Graph neural networks (GNNs) learn node representation vectors for downstream tasks like node classification and link prediction. The core mechanism involves iterative message passing, refining node embeddings to capture both node-specific information and higher-order topological structures. This process ensures that the final node embeddings effectively encapsulate both node-specific information and higher-order topological structures. Notable techniques include Graph Convolutional Networks (GCNs) [11], Graph Attention Networks (GATs) [1], Graph Isomorphism Network (GIN) [33], and Graph Transformer [10], which improves the encoding function for better graph modeling. Despite these advancements, these methods still require high-quality training data and often struggle with generalization capabilities.\nSelf-Supervised Graph Learning. Given the challenges with the generalizability of GNNs, considerable research efforts [32] have focused on enhancing GNNs through self-supervised learning objectives, aiming to capture invariant graph features. Specifically, GraphCL [36] introduced a contrastive pre-training approach for graph data, designed to learn authentic graph characteristics that are robust to structural and feature perturbations. Building on this, JOAO [35] and GCA [39] have developed adaptive augmentation strategies for self-supervised tasks, effectively mitigating the adverse effects of random augmentations. Subsequent works have sought to quickly adapt these pre-trained models to downstream tasks and evolving graph data, as demonstrated by GPF [6] and GraphPrompt [19]. Despite these advancements, the generalizability of these methods remains confined to graph data with similar structural patterns and feature spaces, thus not addressing the cross-domain generalization challenges highlighted in this paper.\nLarge-scale Graph Pre-training. Recent advances in graph modeling have seen efforts to pre-train large-scale graph models across multiple datasets to improve their generalization abilities, drawing inspiration from the strong generalization capabilities of large language models (LLMs) [30]. For instance, OFA [17] and ZeroG [16] utilize text embeddings to standardize the feature spaces across various graph datasets and tasks, facilitating cross-dataset training of graph models. Models like InstructGLM [34] GraphGPT [23] and LLaGA [4] synchronize graph representation spaces with the hidden spaces of LLMs, thus enabling the application of general LLMs for graph prediction tasks. Furthermore, HiGPT [24] expands the capabilities of LLMs to accommodate heterogeneous graph data. Despite these advancements, most generalized graph models require substantial access to and integration of text features, which confines their use primarily to text-abundant environments such as academic networks. Additionally, these methods are typically trained within specific application realms, failing to address the significant variances between datasets from diverse domains."}, {"title": "6 Conclusion", "content": "In this work, the presented AnyGraph framework, an effective and efficient graph foundation model designed to address the multifaceted challenges of structure and feature heterogeneity across diverse graph datasets. AnyGraph's innovative Mixture-of-Experts (MoE) architecture, coupled with its dynamic expert routing mechanism, positions it at the state-of-the-art of cross-domain generalization capabilities. Extensive experiments on 38 varied graph datasets have not only underscored AnyGraph's superior zero-shot learning performance but also its robustness to distribution shifts and its adherence to scaling laws, thereby enhancing its predictive accuracy with increased model size and data volume. The model's efficiency in training and inference, validated through comparison with existing methods, further cements its practical applicability."}, {"title": "A Appendix", "content": "A.1 Experimental Datasets\nWe utilize a total of 38 graph datasets across various domains. The entire dataset contains 14,437"}]}