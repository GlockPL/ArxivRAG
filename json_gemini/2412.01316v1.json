{"title": "Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation", "authors": ["Xin Yan", "Yuxuan Cai", "Qiuyue Wang", "Yuan Zhou", "Wenhao Huang", "Huan Yang"], "abstract": "We introduce Presto, a novel video diffusion model designed to generate 15-second videos with long-range coherence and rich content. Extending video generation methods to maintain scenario diversity over long durations presents significant challenges. To address this, we propose a Segmented Cross-Attention (SCA) strategy, which splits hidden states into segments along the temporal dimension, allowing each segment to cross-attend to a corresponding sub-caption. SCA requires no additional parameters, enabling seamless incorporation into current DiT-based architectures. To facilitate high-quality long video generation, we build the LongTake-HD dataset, consisting of 261k content-rich videos with scenario coherence, annotated with an overall video caption and five progressive sub-captions. Experiments show that our Presto achieves 78.5% on the VBench Semantic Score and 100% on the Dynamic Degree, outperforming existing state-of-the-art video generation methods. This demonstrates that our proposed Presto significantly enhances content richness, maintains long-range coherence, and captures intricate textual details.", "sections": [{"title": "1. Introduction", "content": "Video diffusion models [9, 25, 29, 33, 36] have shown an impressive ability to generate high-quality videos based on a single text prompt [17, 27]. However, most current approaches primarily focus on generating short video clips ranging from 3 to 8 seconds, limiting the expressiveness and richness of the resulting content.\nTo generate longer videos, early approaches incorporate an additional interpolation or extrapolation phase to extend short clips, using techniques like noise scheduling [12, 20, 34] or attention manipulation [15, 32]. While these methods work well for generating minute-long videos, they struggle to extend beyond the scene content, as they are constrained by the limited capacity of the original short clips. An alternative approach adopts a more direct strategy, typically by adding new modules to extend the video length in an auto-regressive manner [8, 37, 44]. However, this introduces the challenge of error propagation.\nUnlike short clips, long videos require a balance between content diversity and long-range coherence, posing significant challenges for current video generation methods. To generate long videos with rich content, we recognize the importance of expanding the text input by incorporating multiple texts, as demonstrated in previous approaches [4, 18]. While combining videos generated from each individual text can enhance content diversity, it often leads to abrupt transitions between different scenarios. One alternative is to simultaneously incorporate multiple texts into the video generation model. This method provides the model with a broader range of textual inputs, enabling the generation of more content-rich videos, in contrast to the traditional approach of using a single text input, which limits the available information. By modeling the texts concurrently, this approach helps ensure long-term coherence in the generated videos, providing a seamless viewing experience.\nExisting long video generation methods overlook the importance of high-quality data [4, 8], leading to low consistency and content diversity in generated videos. To implement a model capable of generating content-rich and coherent long videos, a large-scale, high-quality video dataset is crucial. This dataset must include content-rich, long videos with long-range coherence, along with multiple distinct yet coherent textual descriptions for each video. To this end, we develop a systematic data curation pipeline to collect content-rich video-prompt pairs from public datasets, contributing to the creation of our LongTake-HD dataset. We filter 261k single-scene video clips from 8.9M publicly accessible videos as the pre-training set. Then, we apply an additional set of meticulous filtering steps to select the finest instances, resulting in a fine-tuning dataset of 47k clips. This refined dataset ensures that our method can generate high-quality, extended-duration, and content-rich videos.\nTo tackle the challenge of long videos, we propose Presto, a novel method capable of generating 15-second videos with rich content and long-range coherence, as shown in Fig. 1. Instead of using a single long caption for each video, we divide the visual content into segments and generate progressive sub-captions that align with the unfolding storyline. Next, we modify the cross-attention mechanism in the Diffusion Transformer (DiT). To adapt the DiT model for long video generation, we refine the text embedding process and the cross-attention mechanism to effectively handle multiple progressive text conditions alongside temporal information. In particular, we introduce Segmented Cross-Attention (SCA), which divides the hidden states into segments along the temporal dimension and cross-attends each segment with its corresponding sub-caption. SCA introduces no additional parameters or modules and can be seamlessly integrated into existing DiT-based methods with minimal fine-tuning. We explore three distinct SCA strategies to manage the interaction between text embeddings and segmented latent features: Isolate Segmented Cross-Attention (ISCA), Sequential Segmented Cross-Attention (SSCA), and Overlap Segmented Cross-Attention (OSCA). Our experiments demonstrate that OSCA enhances the content richness and long-range coherence in the generated long videos.\nExperimental results demonstrate the effectiveness of our methods. Presto achieves a 78.5% score on the VBench Semantic Score, outperforming both the leading open-source model, Allegro, and the commercial system, Gen-3. Notably, Presto achieves a perfect 100% score on the Dynamic Degree metrics, showcasing its outstanding ability to capture dynamics and transitions. The quantitative results highlight our approach's strength in capturing intricate textual details and generating videos with rich content. Furthermore, our user study indicates that Presto excels in scenario diversity, scenario coherence, and text-video alignment when compared to various open-source and commercial-level works.\nThe key contributions of our work are outlined as follows:\n\u2022 We propose a large-scale video dataset LongTake-HD, with 261k high-quality cases curated from publicly sourced videos, characterized by long-duration, content-rich, and long-range coherent videos, and each paired with progressive sub-captions.\n\u2022 We proposed Presto for long video generation with a simple yet effective Segmented Cross-Attention strategy which extends the DiT architecture to accommodate multiple text prompts, enabling the generation of videos with rich content and long-range coherence."}, {"title": "2. Related Work", "content": "Long-Video Generation is a challenging task in video generation, requiring videos to be both rich and coherent. One paradigm [12, 15, 20, 32, 34] is to modify noise scheduling or attention mechanisms during the inference stage, which are highly constrained by the original video clips, often resulting in limited content diversity. Another approach [8, 37, 44] directly generates long videos, by introducing additional modules with auto-regressive generation, requiring substantial training resources.\nMultiple-Text-to-Video Generation (MT2V) [4, 18, 22] aligns with our work, as we also incorporate multiple text prompts. While MT2V aims to create videos from multiple inputs, our model uniquely adheres to the traditional T2V framework by requiring only a single user prompt. Additionally, MT2V methods such as TALC [4], typically combine multi-scene captions rigidly (e.g., \"A panda is running in the park, sunny.\" and \"A golden retriever is running in the park, autumn.\u201d), while our progressive caption method eliminates redundant descriptions across sub-captions and focuses on scenario transitions. Our multiple sub-captions can coalesce into a continuous narrative, improving the coherence of the video content."}, {"title": "3. LongTake-HD Dataset", "content": "Data curation plays a crucial role in training our proposed model. To generate content-rich videos with long-range coherence, it is essential to have both high-quality video content and detailed, descriptive captions. Beginning with a dataset of 8.9 million publicly available raw videos, we filtered it down to single-scene video clips that exhibit diverse content, with a resolution of 720x1280, a minimum duration of 15 seconds, and high aesthetic quality. This process yielded 261k instances, each comprising a content-rich video paired with five coherent and progressively structured sub-captions, as well as an overall video caption. To further enable dataset stratification for different training stages, we utilized the full set of 261k instances during the pre-training phase and applied rigorous filtering criteria to extract the finest quality 47k instances for the fine-tuning set. We refer to this curated dataset as LongTake-HD. Further details and specific thresholds are shown in Sec. A and Tab. 4 of Appendix."}, {"title": "3.1. Collecting Content-Diverse Video Clips", "content": "Existing publicly available datasets, such as HD-VILA-100M [42], HD-VG-130M [35], Panda 70M [5], and WebVid-10M [3], offer a vast array of diverse and comprehensive video data that reflect the natural distribution of real-world content. However, these raw datasets often contain substantial amounts of noisy and low-quality material, lacking in careful curation for content quality and caption coherence. Inadequate data curation processes can lead to the inclusion of noisy, disjointed, or irrelevant video data, which negatively impacts the training of models, particularly for long-form videos.\nStarting from a dataset of 8.9 million publicly accessible raw videos, we apply a video data filtering pipeline including: (1) duration, speed, and resolution filtering; (2) scene segmentation; (3) low-level metrics filtering; and (4) aesthetic and motion contents filtering.\nDuration, speed, and resolution. We exclude videos shorter than 15 seconds, to ensure an adequate video length. To maintain smoothness, we filter out videos with a frame rate lower than 23 FPS. We also remove videos with resolutions below 720p to preserve the visual quality of the dataset. Additionally, videos with aspect ratios less than 1 (i.e., vertical videos) are excluded to ensure consistency throughout the dataset.\nScene segmentation. We detect the scene cuts in videos and filter out those with abrupt transitions using PySceneDetect [19]. To further strengthen the process and remove any lingering cuts or transitions, we manually discard the first and last 10 frames of each clip. After completing the scene segmentation, the remaining data filtering is applied to the individual single-scene video clips.\nLow-Level metrics. We use brightness and artifacts as key metrics for low-level filtering. We compute the average grayscale value of video frames and remove those that are overly dark or bright. Detection tools [2, 40] are applied to identify artifacts, such as watermarks and text, that are unrelated to the actual video content.\nAesthetics and motion contents. We employ the LAION Aesthetics Predictor [28] to evaluate the aesthetic quality of video frames and remove those with low aesthetics scores. Optical flow is calculated using Unimotion [41], and clips with higher flow scores are retained to ensure a substantial level of motion amplitude. Furthermore, we compute the coefficient of variation for all optical flow values within each clip, defined as the ratio of the standard deviation to the mean [7]. This standardized measure of dispersion allows us to assess the smoothness and consistency of motion dynamics, helping to avoid abrupt shifts in motion intensity."}, {"title": "3.2. Obtaining Coherent Video Captions", "content": "We apply captioning techniques to both images and videos. First, we conduct image-level diversity filtering based on the sampled keyframes. Next, we generate captions for each keyframe and perform semantic filtering on these captions. Finally, we leverage Large Language Models (LLMs) to create multiple progressive sub-captions that include camera motions.\nDiversity filtering for keyframes. We employ a comprehensive approach to evaluate the diversity and coherence of images, using a combination of low-level and semantic metrics. Specifically, we apply the Peak Signal-to-Noise Ratio (PSNR) [11] for pixel-wise filtering, the Structural Similarity Index Measure (SSIM) [38] for structure-wise filtering, and the Perceptual Similarity (LPIPS) [46] for semantic-wise filtering. Additionally, some sampled frames may contain minimal information, such as black screens or blurry images. To address this, we use the image file size as a filtering criterion, since frames with low information content typically result in smaller file sizes when compressed in the PNG format [6, 10, 49, 50].\nSemantic filtering from captions. We utilize Large Vision-Language Models (LVLMs) as caption generators to create detailed descriptions for both the entire video and its sampled frames. The captions for individual frames offer in-depth descriptions of the visual elements in each keyframe and represent the corresponding short video segments, while the video caption emphasizes the dynamics and transitions across the video, incorporating both spatial and temporal details. To generate embeddings for all keyframe captions, we employ MPNet [31] from SentenceTransformers [23, 24] and compute the cosine similarity [30] between each pair of captions. Additionally, we filter out negative captions, which occur when LVLMs fail to generate responses for frames that contain sensitive or abstract content.\nProgressive sub-captions generation strategy. We propose a progressive sub-captions generation approach to create coherent and non-redundant sub-captions that align with the video storyline. We show a simple example of three generated progressive sub-captions below:\nsub-caption 1: A close-up shot of the ground, focused on a small, slightly elevated, textured mound of soil. A single ant emerges from a tiny opening at the top of the mound, its tiny antennae gently probing the air as it moves cautiously forward.\nsub-caption 2: The camera gradually pulls back, maintaining focus on the ant as it traverses the surface of the mound.\nsub-caption 3: Continuing to pull back, the ant diminishes in prominence and focus transitions to reveal a larger view of the immediate ground area.\nAs in the example above, the first sub-caption describes the main subject, ant, and the environment, soil. When it comes to the second and third sub-captions, it continues the previous story and elaborates further on the transitions, e.g. traverses the surface of the mound, and a larger view of the immediate ground area. This narrative-style sub-caption annotation strategy helps enhance the long-range coherence of the generated videos, and distinguishes our approach from existing video datasets and MT2V methods.\nFor a given long video, we first divide the video clip into N segments and generate independent captions for each segment using our captioning model. We also create an overall description of the entire video, capturing both spatial and temporal details. Next, we refine each sub-caption using a causal approach. We employ a Large Language Model (LLM) to adjust each sub-caption, taking into account all previous sub-captions as well as the overall video description, ensuring that each sub-caption represents a distinct episode within the broader storyline. Additionally, we explicitly incorporate camera motion into each sub-caption to enable fine-grained control over the camera. This strategy results in a set of coherent and progressively linked sub-captions for diffusion model training.\nDuring inference, when given a short prompt from the user, we again use the LLM as a \"director\" to generate N scripts with consistent and detailed descriptions. For captioning both the full video and selected frames, we use Aria [13] as our captioning model, and GPT-40 [1] as the LLM for refining the sub-captions. The detailed prompt templates for these two models are provided in Listing 1 and Listing 2 of Appendix."}, {"title": "4. Method", "content": "We provide a brief overview of latent diffusion models (LDMs) for text-to-video generation. LDMs extend traditional transformer models to handle the generative task and conduct the diffusion process in the latent space. First, a pre-trained autoencoder is utilized to compress the raw image or videos from pixel to latent space and a text encoder takes the text input and creates text embeddings. A diffusion transformer (DiT) takes the visual input with noise and performs a denoising process during training. Specifically, as shown in Fig. 2(a), the diffusion transformer consists of a stack of self-attention and cross-attention blocks, which capture the spatial and temporal dependencies within the video as long as text embeddings condition. During inference, the diffusion transformer starts from an instance sampled from random Gaussian noise and applies the diffusion process iteratively across multiple timesteps, refining the output at each step.\nTo adapt the DiT model for long video generation, we modify the text embedding process and the cross-attention mechanism to effectively incorporate multiple progressive text conditions with temporal information. Specifically, we split the latent features into segments along temporal dimensions in the cross-attention mechanism and study three different strategies to implement the interaction between text embedding and segmented latent features. The proposed Segmented Cross-Attention (SCA), especially the overlap variant, improves the content richness and long-range coherence in generated long videos by a large margin. We study this proposed strategy and different variants in the next subsection."}, {"title": "4.1. Overview", "content": null}, {"title": "4.2. Segmented Cross-Attention", "content": "A standard paradigm of text-to-video generation relies on a single text prompt input, which is typically encoded into a fixed-sized embedding $c \\in \\mathbb{R}^{L\\times D}$ via text encoder. Text embeddings that exceed this size are truncated. This limitation can lead to severe information loss in long video generation, considering the length of our progressive sub-captions. Moreover, a single long text embedding presents challenges in capturing intricate details, as latent representations within hidden states struggle to effectively capture the intricate details of a lengthy text embedding through cross-attention mechanisms.\nInspired by window attention[14], which limits the scope of attention to local regions, we propose the Segmented Cross-Attention (SCA) method. This method splits the hidden states into temporal-local segments to better interact with the progressive sub-captions via cross-attention. For each group of progressive sub-captions, we separately encode N sub-captions with text encoder, and thus obtain a group of text embeddings $\\{c_i\\}_{i=1}^{N} \\in \\mathbb{N} \\times \\mathbb{R}^{L\\times D}$. Given the hidden state z with T frames in temporal dimension, we also evenly split z into N non-overlapped segments $\\{z_i\\}_{i=1}^{N}$ along the time dimension, aligning in quantity with the group of sub-captions. Thus, segment $z_i$ encapsulates the frame information ranging from $i \\times T/N$ to $(i + 1) \\times T/N$. The core idea for our SCA is to constrain each segment $z_i$ to only access its corresponding text condition.\nWe study three strategies for segmented attention computing including: 1) Isolate Segmented Cross-Attention (ISCA), 2) Sequential Segmented Cross-Attention (SSCA), and 3) Overlap Segmented Cross-Attention (OSCA), as shown in Fig. 2(b).\nFor Isolate Segmented Cross-Attention (ISCA) shown in the first row of Fig. 2(b), we treat it as the basic setting of our SCA, in which each segment $z_i$ only cross-attends to its corresponding text condition $c_i$ in cross-attention layers. Due to the lack of internal interactions among segments, ISCA tends to generate video with rich content but lacks long-range coherence.\nAnother strategy to improve temporal coherence is to enable long-range interactions between latent features and text embeddings, as shown in the second row of Fig. 2(b). We refer this variant to Sequential Segmented Cross-Attention (SSCA), which concatenates the latent segment $z_i$ sequentially with its latter segments $\\{z_j\\}_{j>i}$, and takes the average of all attention outputs in the final. Such a strategy greatly improves the long-range coherence. However, the content richness drops because the sequential interactive introduces more information to each segment and blends its content diversity.\nFinally, we adopt a simple yet effective strategy that performs feature fusion on adjacent segments, which is Overlap Segmented Cross-Attention (OSCA), as shown in the third row of Fig. 2(b). We relax the non-overlapping segment method above into the overlapping one by introducing $\\delta < [T/N]$ overlapping frames for each segment $z_i$. Through overlapping, frames at the boundary of two adjacent segments will attend to multiple text conditions. The cross-attention outputs in the overlapped regions are then averaged, promoting smoother transitions between segments. OSCA allows each segment to cross-attend to its relevant text embeddings, while self-attention facilitates global information exchange across segments, ensuring overall consistency. This interplay between local and global interactions helps Presto effectively capture the storyline and ensures content diversity and scenario coherence in long-form video generation."}, {"title": "4.3. Implementation", "content": "Our work is built upon Allegro[48], an open-source video diffusion model with 2.8B parameters. Allegro generates high-quality videos up to 88 frames and 720p resolution from simple text input. Text inputs are handled by T5 [21] text encoder. A video caption is decoupled into five progressive sub-captions and a hidden state is separated into five segments, which correspond to the notations N above. For post-processing, we adopt EMA-VFI[45] as the frame interpolation model, to further normalize the video speed and extend video length. During inference, for a single prompt from user input, we leverage GPT-40 as the refiner to generate five progressive sub-captions.\nThe training of Presto can be separated into two stages: Text-to-Video Pre-training, and Text-to-Video Fine-tuning. The pre-training stage is built upon the Allegro model with 88 frames and 720 \u00d7 1280 resolution. The pre-training dataset contains 261k instances and we sample frames from videos at 6 FPS during this stage. Presto is trained for 1500 steps on 64 Nvidia H100 GPUs with a batch size of 256 and constant learning rate of 1e-4, processing a total of 384k videos. For fine-tuning, we pick the most content-diverse 47k instances from the pre-training dataset and fine-tune for another 500 steps with a batch size of 256 learning rate of 1e-4, processing a total of 128k videos."}, {"title": "5. Experiment", "content": "In this section, We demonstrate the long video generation capability of Presto via both quantitative and qualitative evaluation in Sec. 5.2 and Sec. 5.3 respectively. Moreover, we provide an ablation study in Sec. 5.4 on key components of our model, including training data, progressive sub-caption strategy, and Segmented Cross-Attention. We exhibit more results in Sec. C and Sec. B of Appendix, and discuss the limitations and failure cases in Sec. D."}, {"title": "5.1. Baseline Models", "content": "To evaluate the effectiveness of our Presto on content diversity and long-range coherence, we compare it with the state-of-the-art text-to-video models. We select the best open-source model, Allegro [48], and commercial system, Runaway Gen-3 [26], as our baseline models. We also compare with the recent MT2V method, TALC [4]. To highlight the importance of scenario coherence, we add a naive approach of \"Merge Videos\u201d in qualitative evaluation, by utilizing multiple texts to generate multiple short clips."}, {"title": "5.2. Quantitative Evaluation", "content": "Setup We use VBench for our automatic quantitative evaluation. VBench offers 946 official prompts to validate different aspects of generated videos and is a widely adopted benchmark in video generation methods. We directly report the results for models that exist in the VBench Leaderboard. We report several specific dimensions as well as the holistic dimensions in VBench, as shown in Tab. 1, which demonstrate the exceptional capability of generating long videos with rich content and long-range coherence while adhering to the input text.\nPresto outperforms all state-of-the-art video generation models on Semantic Score. Specifically, Presto notably surpasses Allegro with +5.5%, the commercial Gen-3 with +3.3%, and the previous MT2V method TALC with +34.1%. We attribute the performance improvement to the leverage of the progressive sub-captions generation strategy, which decouples the text input and improves the text information. Besides, we achieve a full mark in Dynamic Degree metrics, reflecting the superior ability to capture dynamics and preserve camera control. Compared with TALC which achieves a relatively high score of 98.6% in Dynamic Degree, we achieve significantly better performance on all metrics, highlighting the long-range coherence aided by the meticulous data curation and Segmented Cross-Attention. For the degradation in quality scores, we hypothesize that it arises from the increased difficulty in maintaining consistency when the dynamics are complex and varied."}, {"title": "5.3. Qualitative Evaluation", "content": "Setup Evaluating the quality of generated videos is a highly subjective task, as automatic benchmarks often dis-align with human judgment. User study is a prevalent paradigm for qualitative assessment in previous work [43, 47, 48]. In this study, we collect 62 diverse text prompts encompassing a wide range of aspects, including humans, animals, landscapes, and so on. Human annotators are tasked with blindly comparing pairs of videos and making a preference judgment between two cases. To assess the enhanced content diversity achieved by our model, we evaluate three key dimensions: Scenario Diversity, Scenario Coherence, and Text-Video Adherence. We allow the tie situation when the differences are indistinguishable. We recruited 12 annotators, with each instance reviewed by three individuals, resulting in a total of 2,232 ratings. We report the win rate (%) with each method, and calculate an overall score by considering all three dimensions. as shown in Tab. 2.\nOur model surpasses all baselines on the Overall Score, indicating that our Presto can generate videos with improved scenario diversity and coherence with text-following capabilities, even better than the commercial model Gen-3. Specifically, Presto excels Gen-3 in scenario diversity, slightly outperforms in text-video adherence, and closely matches in scenario coherence. For the SOTA open-source model, Allegro, Presto wins in all dimensions, especially on the scenario diversity. For TALC, Presto significantly outperforms in all metrics, further demonstrating the importance of curated data. We also consider the naive approach of utilizing multiple texts by \"Merging Videos\", to serve as a reference for our metrics. Our method reaches similar results as Merging Videos on scenario diversity but significantly outperforms it in scenario coherence.\nWe further exhibit the real cases in our user study, as shown in Fig. 3. For the first case, our Presto is the only one that captures intricate text details of \"People hurry along the sidewalk.\" while all other methods fail to generate walking people. For the second case, the generated video from our Presto achieves the largest scenario motion while with the best long-range coherence. Considering Merge Video, which may generate highly diverse content, while failing to maintain consistency as five shots are totally different and unrelated."}, {"title": "5.4. Ablation Study", "content": "We now ablate on the key proposed components, including both Presto model design and LongTake-HD dataset curation. We use VBench to evaluate generated videos on two dimensions: 1) Overall Score; and 2) Dynamic Degree. As video generation methods consume huge computational resources in model training, we standardized our video generation model training to 360p resolution with 40 frames in this ablation study. Tab. 3 presents the ablation results on the automatic evaluation benchmark.\nSegmented Cross-Attention (SCA) Strategy. We ablate the three strategies of SCA including Overlap Segmented Cross-Attention (OSCA), Sequential Segmented Cross-Attention (SSCA), and Isolated Segmented Cross-Attention (ISCA), as we mentioned in Sec. 4 and Fig. 2(b). All three strategies achieve 100% in Dynamic Degree score, indicating the general concept of SCA plays a significant role in capturing dynamics. OSCA achieves an excellent result of 74.7% Overall Score, which is the best version among all strategies. This design not only maintains content richness but also facilitates transitions between segments by introducing proximal overlapping. SSCA achieves a 73.7% Overall Score, slightly behind the OSCA. We attribute this degradation to the information from previous frames disrupting the interaction of later segmented latent features. ISCA achieves a 73.1% Overall Score, performing poorly in the experiments, for it prohibits the information exchange in adjacent segments. This ablation further underscores the significance of the overlapping technique in OSCA, facilitating smoother transitions between segments.\nw/o Meticulous Filtering. We study the effect of our meticulously curated dataset LongTake-HD, with the same strategy of OSCA. We randomly select the same amount of pre-training data from the unfiltered video dataset, without the filtering of content diversity in Sec. 3.1 and captions in Sec. 3.2. Note that we still adopt the basic filtering for videos, including duration, speed, resolution, and low-level metrics, to ensure the basic visual quality, and ablate the effect of metrics contributing to rich content. The model with worse data achieves 72.0% on the overall score, largely lagging behind 2.7% in the same strategy with well-curated data. Moreover, the performance on Dynamic Degree will also drop by 2.8%, highlighting the effectiveness of our data curation pipeline.\nSingle Long Condition. To diminish the effect of the increased text length in our sub-captions, and demonstrate the effectiveness of separate modeling for each sub-caption in our Segmented Cross-Attention, we test the method of naive concatenation on text condition. Specifically, this approach directly concatenates N sub-captions along the sequence length dimension, forming a single long text condition. This long condition will attend to the whole hidden states, the same as standard text-to-video generation approaches. The results show that this naive concatenation method yields the lowest Overall Score with a 2.9% drop compared to our proposed separate modeling strategy, indicating that our Segmented Cross-Attention effectively enhances the overall video quality."}, {"title": "6. Conclusion", "content": "We introduced Presto, a simple yet effective method for generating long-range coherent, content-rich, long videos. Our Presto achieves 78.5% and 100% on the VBench Semantic Score and Dynamic Degree, surpassing the existing SOTA video generation approaches. Presto utilizes the Segmented Cross Attention mechanism to integrate multiple texts concurrently, which can be seamlessly adopted in the existing diffusion model with DiT architecture. We also curate a high-quality video-texts dataset LongTake-HD from public datasets and sources. We leave more exploration about the attention mechanism and model structure (e.g., auto-regressive generation) for long video generation as our future work."}]}