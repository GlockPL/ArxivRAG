{"title": "LANGUAGE MODELS ARE HIDDEN REASONERS: UNLOCKING LATENT REASONING CAPABILITIES VIA SELF-REWARDING", "authors": ["Haolin Chen", "Yihao Feng", "Zuxin Liu", "Weiran Yao", "Akshara Prabhakar", "Shelby Heinecke", "Ricky Ho", "Phil Mui", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "abstract": "Large language models (LLMs) have shown impressive capabilities, but still struggle with complex reasoning tasks requiring multiple steps. While prompt-based methods like Chain-of-Thought (CoT) can improve LLM reasoning at inference time, optimizing reasoning capabilities during training remains challenging. We introduce LaTent Reasoning Optimization (LaTRO), a principled framework that formulates reasoning as sampling from a latent distribution and optimizes it via variational approaches. LaTRO enables LLMs to concurrently improve both their reasoning process and ability to evaluate reasoning quality, without requiring external feedback or reward models. We validate LaTRO through experiments on GSM8K and ARC-Challenge datasets using multiple model architectures. On GSM8K, LaTRO improves zero-shot accuracy by an average of 12.5% over base models and 9.6% over supervised fine-tuning across Phi-3.5-mini, Mistral-7B, and Llama-3.1-8B. Our findings suggest that pre-trained LLMs possess latent reasoning capabilities that can be unlocked and enhanced through our proposed optimization approach in a self-improvement manner. The code of LaTRO is available at https://github.com/SalesforceAIResearch/LaTRO.", "sections": [{"title": "1 INTRODUCTION", "content": "The development of large language models (LLMs) with enhanced reasoning capabilities has emerged as a crucial area of research. Despite their impressive advances, the inherent next-token prediction mechanism of LLMs makes it challenging for these models to solve complex problems requiring multiple reasoning steps (Wang et al., 2022; Huang et al., 2023). For instance, LLMs often struggle to directly provide accurate solutions to mathematical problems or even simple puzzles like counting specific letters in a word. Consequently, researchers have explored various prompting strategies that guide LLMs to generate reasoning trajectories or rationales\u2014sequences of tokens that build a step-by-step progression toward an answer. Techniques such as Chain-of-Thought (CoT) (Wei et al., 2022), Tree-of-Thought (ToT) (Yao et al., 2024), and Program-of-Thought (PoT) (Chen et al., 2023) prompting methods exemplify this approach.\nRecent progress has also focused on inference-time techniques to enhance the reasoning abilities of LLMs (Wu et al., 2024; Brown et al., 2024), as observed in the OpenAI 01 model (OpenAI, 2024). These methods have demonstrated remarkable performance in diverse reasoning tasks, including mathematics (Cobbe et al., 2021b; Trinh et al., 2024; Luo et al., 2024), coding (Jimenez et al., 2023; Guo et al., 2024; Zhang et al., 2024), and scientific problem-solving (Rein et al., 2023). Notable inference-time methods, such as CoT with Self-Consistency (CoT-SC) (Wang et al., 2023) and CoT-Decoding (Wang & Zhou, 2024), extend the CoT approach by generating multiple reasoning paths and selecting the most consistent one. Additionally, techniques like ReAct (Yao et al., 2023a) and Reflexion (Shinn et al., 2023) integrate reasoning into LLM agent loops, further enhancing their problem-solving capabilities.\nDespite the promising results at inference time, improving the reasoning abilities of LLMs during their training phase remains a challenging problem. Several obstacles impede progress in this area. Firstly,"}, {"title": "2 RELATED WORK", "content": "Prompt-based LLM Reasoning Prompt-based reasoning methods prove to be effective across various domains, such as math problem-solving (Polu & Sutskever, 2020; Hendrycks et al., 2021; Cobbe et al., 2021a), logical reasoning (Sprague et al., 2024) and agentic tasks (Yao et al., 2023a; Shinn et al., 2023; Yao et al., 2023b). Chain-of-Thoughts or CoT (Wei et al., 2022) is the pioneering work that prompts LLMs to decompose challenging tasks into smaller reasoning steps. After that, two primary research directions further improved reasoning capabilities during inference. One direction searched over the reasoning trajectories against a process-based verifier, or reward model (Yao et al., 2024; Besta et al., 2024; Lightman et al., 2023). For example, tree-of-thoughts (Yao et al., 2024) explored over thoughts by depth-first search (DFS), breadth-first search (BFS) or beam search. The other approach used a critic model to provide verbal feedback, iteratively refining the responses with that feedback (Saunders et al., 2022; Shinn et al., 2023; Yao et al., 2023b; Madaan et al., 2023).\nSelf-Rewarding for LLM Reasoning Reasoning capabilities in LLMs can be enhanced in post-training through self-rewarding and reinforcement learning. The Self-Taught Reasoner, or STaR (Zelikman et al., 2022) introduced a bootstrapping technique that allows LLMs to generate rationales and fine-tune itself with self-generated reasoning paths. Quiet-STaR (Zelikman et al., 2024) extended this by training LLMs to infer implicit rationales across arbitrary text, enhancing both reasoning and predictive abilities without task-specific fine-tuning. Reinforced Fine-Tuning, or ReFT (Trung et al., 2024) took this further by leveraging reinforcement learning to improve generalization in reasoning tasks like math problem-solving, enabling LLMs to learn from multiple reasoning paths. Self-correction capabilities in LLMs can also be reinforced through self-generated data (Kumar et al., 2024). Lastly, Hoffman et al. (2024); Hu et al. (2023) formulated the reasoning process as latent variable models, aligning LLMs towards more accurate reasoning with fewer annotated data."}, {"title": "3 BACKGROUND ANMOTIVULATION", "content": "We start by briefly introducing reasoning techniques (e.g., chain-of-thought (Wei et al., 2022), ReAct (Yao et al., 2023a), etc). Given a user query x, the standard procedure to sample the response y is to leverage an autoregressive pretrained LLMS $\u03c0_\u03b8$ (parameterized by \u03b8): $y \\sim \u03c0_\u03b8(\\cdot|x)$. As for prompt-based reasoning techniques such as chain-of-thought (Wei et al., 2022), the LLM $\u03c0_\u03b8$ is firstly asked to generate thoughts (a.k.a reasoning rationale) before generating the answers to the response:\n$x' := \\text{Reason}(x) = x \\oplus \\text{Prompt Template of Thought}$,\n$z \\sim \u03c0_\u03b8(\\cdot|x'), y \\sim \u03c0_\u03b8(\\cdot | x' \\oplus z)$,\nwhere z is the thought or the reasoning rationale path, indicates the concatenate operator, and the prompt template of the thought can be some hint prompt such as \"Let's think step by step\" 1.Empirically, people observe that there is a higher chance for the LLM $\u03c0_\u03b8$ to generate the desired answer y following the above procedure than directly sampling the response $y \\sim \u03c0_\u03b8(\\cdot | x)$. From a statistical perspective, we hypothesize that good reasoning rationales can significantly improve the probability of generating good answers y: $\u2203z, \\text{S.t. }\u03c0_\u03b8(y|x \\oplus z) > \u03c0_\u03b8(y|x)$.\nWe omit the difference between x' and x for convenience in the latter notation."}, {"title": "4 OPTIMIZING THE REASONING PROCESS", "content": "In this section, we describe how to optimize the reasoning rationale without external feedback. Specifically, we introduce the objective for optimizing the reasoning rationale in Section 4.1 from a variational perspective of LLM training; we derive the gradient estimation for the new objective in Section 4.2, and discuss the sampling procedure together with reward shaping in Section 4.3. We summarize the proposed algorithm, LaTent Reasoning Optimization (LaTRO) in Algorithm 1, and illustrate the overall procedure in Figure 1."}, {"title": "4.1 LATENT REASONING OPTIMIZATION: A VARIATIONAL APPROACH", "content": "Suppose we are given a golden dataset $D_{Gold} := \\{(x_i, y_i)\\}_{i=1}^N$ consisting of N query and answer pairs, where (x, y) denotes the query and the answer respectively. A standard finetuning procedure to fit the LLM $\u03c0_\u03b8$ to the dataset $D_{Gold}$ is by likelihood maximization:\n$\\max_\u03b8 E_{(x,y)\\sim D_{Gold}} [\\log \u03c0_\u03b8(y | x)]$,\nwhere \u03b8 are the parameters of the LLM $\u03c0_\u03b8$ to optimize. Based on the discussion in Section 3, it is more feasible to optimize $\u03c0_\u03b8$ with additional reasoning rationale path z, compared with standard finetuning objective in Equation (1). Hence, we can introduce another \u201creasoner\u201d q(z|x) to sample the latent reasoning rationales that can help the optimization procedure of $\u03c0_\u03b8$. This is achievable by"}, {"title": "4.2 GRADIENT ESTIMATION FOR LATRO", "content": "From previous RL literature, we know that estimating $\\nabla_\u03b8 J(\u03b8)$ in Equation (3) involves the use of policy gradient methods, which usually suffers from high variances with the naive REINFORCE estimators (Williams, 1992). Inspired by the recent work on policy gradient for LLMs (Ahmadian et al., 2024), we also leverage the REINFORCE Leave-One-Out (RLOO) (Kool et al., 2019) to optimize the \u201creasoner\u201d $\u03c0_\u03b8(z|x)$, where we can achieve lower variances of gradient estimation by sampling multiple rationales. We summarize the empirical gradient estimation for solving LaTRO in Proposition 2.\nProposition 2. (LaTRO Gradient Estimation) Suppose we are given a set of training data $D_{Gold} := \\{x_i, y_i\\}_{i=1}^N$, we sample K i.i.d reasoning rationales $z_1^{(i)}, z_2^{(i)} ... z_K^{(i)} \\sim \u03c0_\u03b8(\\cdot|x_i)$ for each query and answer pair (xi, yi). The empirical gradient estimator for $\\nabla_\u03b8 J(\u03b8)$ is expressed as\n$\\nabla_\u03b8\\widehat{J}(\u03b8) := \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{k=1}^{K} (\\nabla_\u03b8 \\log \u03c0_\u03b8(z_k^{(i)}|x_i) \\cdot \\widehat{A}_k^{(i)} + \\nabla_\u03b8 \\log \u03c0_\u03b8(y_i | x_i \\oplus z_k^{(i)}))$,\nwith $\\widehat{A}_k^{(i)} = r(z_k^{(i)}) - \\frac{1}{K-1}\\sum_{j \\ne k}^{K} r(z_j^{(i)})$, $r(z_j^{(i)}) := \\log \u03c0_\u03b8(y_i | x_i \\oplus z_j^{(i)}) - \u03b2\\log \\frac{\u03c0_\u03b8(z_j^{(i)}|x_i)}{\u03c0_0(z_j^{(i)}|x_i)}$,"}, {"title": "4.3 PRACTICAL CONSIDERATIONS", "content": "To reduce computation overhead and better control the sampling of reasoning rationales during training, we limit their maximum token length to L. The rationale ends either at the EOS token or at the start of a predefined answer template (e.g., \"The answer is\"). We then use the truncated rationale z, along with the query and the answer z, for further computation.\nWe also encourage the LLM to finish its reasoning process with L tokens. Inspired by the implementation of the RLOO trainer in the TRL library (von Werra et al., 2020), we introduce a constant penalty for rationales truncated by the maximum token length L. This penalty encourages the generation of rationales that fit within the specified token limit."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 SETUP", "content": "We evaluate the performance of the proposed method across two datasets: a mathematical reasoning dataset (GSM8K, Cobbe et al. (2021b)) and a logical reasoning dataset (ARC-Challenge, Talmor et al. (2019)). The sizes of the datasets are listed in Table 1.\nTraining. For each dataset, we fine-tune three base models: Phi-3.5-mini-instruct (Abdin et al., 2024), Mistral-7B-Instruct-v0.3 (Jiang et al., 2023), and Meta-Llama-3.1-8B-Instruct (Dubey et al., 2024), abbreviated as Phi-3.5, Mistral-7B, and Llama-3.1-8B, respectively. We provide two baseline comparisons: the base model and the supervised fine-tuned (SFT) model. For GSM8K, LaTRO fine-tuning excludes golden rationales from the solutions in the training set, while the SFT model is trained using golden rationales. For ARC-Challenge, as suggested in (Zheng et al., 2024), the model is trained to generate answers to the text of"}, {"title": "5.2 RESULTS", "content": "In this subsection, we present evaluation results that demonstrate how effectively LaTRO enhances the reasoning abilities of LLMs on downstream datasets. The detailed results are provided inTable 2.\nFor the GSM8K dataset, LaTRO fine-tuned models outperform all base models by up to 19.5% (Mistral-7B, 47.8% \u2192 67.3%) and show an average improvement of 12.5% across the three models examined with greedy decoding. The greatest improvement margin is observed for Mistral-7B, while the smallest is seen for Llama-3.1-8B, consistent with our initial findings in Figure 2, where Mistral-7B exhibited the lowest log probability for directly answering questions and Llama-3.1-8B exhibited the highest. With self-consistency, the improvements are by up to 16.5% (Phi-3.5, 74.0% \u2192 90.5%) and the average improvement is 13.1%. Furthermore, LaTRO models demonstrate superior performance relative to SFT baselines, with an average improvement of 9.6% for greedy decoding and 13.2% for self-consistency. It is worth noting that for the SFT baseline of Llama-3.1-8B, overfitting on the test set is still observed after tuning the learning rate.\nFor ARC-Challenge, LaTRO fine-tuned models still outperform the baselines, though with a smaller margin. When using greedy decoding, the improvements over the base models are up to 1.6% with an average increase of 1%. We see more increment with self-consistency, where the improvement margins are on average 2.4%. Comparing to SFT baslines, we find that all three models are very sensitive when fine-tuning to directly generate the answer of ARC-Challenge questions. They perform"}, {"title": "5.3 ABLATION STUDY", "content": "In this subsection, we present our ablation study on the effect of different parameters in LaTRO. For consistency, we fix the base model to Phi-3.5 and the dataset to GSM8K throughout the ablation experiments.\nHow many tokens are enough? Liu et al. (2024) demonstrated that when the input length is n, a transformer model with a hidden size of O(log n) can solve problems equivalent to Boolean circuits of size m, using m CoT steps. However, the empirical determination of sufficient CoT tokens for optimal performance remains underexplored. In this section, we report zero-shot accuracy with generation length L ranging from 200 to 1000 tokens at inference time. Additionally, a Phi-3.5 model is fine-tuned with L = 200 for comparison. We distinguish two LaTRO fine-tuned models, referred to as LaTRO and LaTRO200. As shown in Figure 3(a) accuracy gains plateau when L > 500, suggesting 500 tokens might suffice for grade school math problems. In contrast, limiting L to 200 reduces accuracy, unless the model is trained accordingly. Interestingly, LaTRO significantly improves performance under this constraint by training the model to generate more concise rationales."}, {"title": "5.4 CASE STUDY", "content": "We take a closer look at the responses generated by the LaTRO fine-tuned models. We select a question from GSM8K and compare the responses from the base, the SFT model, and the LaTRO finetuned model. We choose the set of responses from the Mistral-7B models that we evaluated. As can be seen in Figure 4, the base model not only generates a lengthy response, it also makes a logical mistake at the first step, where the correct equation to establish here is \u201cF = 2B + 15\". The SFT model simplifies the answer and makes the first step correct. However, in the second step it first makes a wrong equation, then makes an arithmetic error when evaluating this equation. Further, LaTRO can give a concise and correct answer. We include more sample responses in Appendix C."}, {"title": "6 CONCLUSION", "content": "In conclusion, this work introduces LaTRO, a principled framework for optimizing language models' reasoning capabilities without external feedback or reward models. By formulating reasoning as sampling from a latent distribution and leveraging self-rewarding, LaTRO enables models to concurrently improve both their reasoning process and ability to evaluate reasoning quality. Our extensive experiments across multiple model architectures and tasks demonstrate significant performance gains, with LaTRO outperforming baseline models and supervised fine-tuning approaches. These findings suggest that pre-trained LLMs possess latent reasoning capabilities that can be unlocked through our proposed optimization approach, representing a significant step towards creating more intelligent systems that can self-evolve their problem-solving capabilities.\nWhile LaTRO shows promising results, there are some limitations to consider. The computational cost of sampling multiple rationales during training could be prohibitive for very large models. Future work could explore ways to reduce this computational overhead, such as using more efficient sampling techniques or adaptive rationale generation. Other promising directions include investigating the applicability of LaTRO to a wider range of reasoning tasks beyond math and science problems, and exploring how to conduct multi-step reasoning learning to to enhance reasoning capabilities further. Despite these limitations, our contributions advance both the state-of-the-art in LLM reasoning capabilities and provide valuable insights into the nature of LLM alignment and its potential for self-improvement."}, {"title": "A ADDITIONAL DETAILS ON OUR THEORETICAL FRAMEWORK", "content": ""}, {"title": "A.1 PROOF OF PROPOSITION 2", "content": "Proof. We restate the objective as follows:\n$J(\u03b8) := E_{(x;y)\\sim D_{Gold}} E_{z\\sim\u03c0_\u03b8(\\cdot|x)} [log \u03c0_\u03b8(y| x \\oplus z)] \u2013 \u00b7\u03b2D_{KL} [\u03c0_\u03b8 (z|x)||\u03c0_0(z|x)]$,\n$= E_{(x,y)\\sim D_{Gold}} [E_{\u03c0_\u03b8(z|x)} [log \u03c0_\u03b8(y|x \\oplus z) \u2013 \u03b2log \u03c0_\u03b8(z|x) + \u03b2 log \u03c0_0(z|x)]]$,\nwhere \u03b2 > 0 is a positive coefficient to control the regularization strength. We take the gradient w.r.t \u03b8 at each sample pair (x, y), and we get\n$\\nabla_\u03b8J(\u03b8;x,y) := \\nabla_\u03b8 (E_{\u03c0_\u03b8(z|x)}[log \u03c0_\u03b8(y|x \\oplus z)] \u2013 \u03b2log \u03c0_\u03b8(z|x) + \u03b2 log \u03c0_0(z|x))dz\n= E_{\u03c0_\u03b8(z|x)}[\\nabla_\u03b8 log \u03c0_\u03b8(y|x \\oplus z) \u2013 \u03b2\\nabla_\u03b8 log \u03c0_\u03b8(z|x)]\n+ E_{\u03c0_\u03b8(z|x)}[\\nabla_\u03b8 log \u03c0_\u03b8(y|x \\oplus z) \u2013 \u03b2\\nabla_\u03b8 log \u03c0_\u03b8(z|x)]$.\nWe further define $r(z) := \\frac{\u03c0_\u03b8(z|x)}{log \u03c0_\u03b8(z|x)} log \u03c0_\u03b8(y|x+z) - \u03b2 \\frac{\u03c0_\u03b8(x \\oplus z)}{\u03c0_0(x|z)}$, and use the fact that $E_{\u03c0_\u03b8(z/x)}[\\nabla_\u03b8 log \u03c0_\u03b8(z|x)] = \\nabla_\u03b8 \u222b\u03c0_\u03b8(z|x) \\frac{\u03c0_\u03b8(x \\oplus z)}{\u03c0_0(x|z)} dz = \\nabla_\u03b8 \u222b \u03c0_\u03b8(z|x)dz = 0$, we obtain the final gradient as\n$\\nabla_\u03b8J(\u03b8;x,y) = E_{\u03c0(z|x)} [\\nabla_\u03b8 log \u03c0_\u03b8(z|x) \u00b7 r(z) + \\nabla_\u03b8 log \u03c0_\u03b8 (y|z, x)]$.\nAnd when we use RLOO estimator with empirical samples, we can replace above gradient estimation with empirical samples, which gives us the following result:\n$\\nabla_\u03b8\\widehat{J}(\u03b8) := \\frac{1}{NK} \\sum_{i=1}^{N} \\sum_{k=1}^{K} (\\nabla_\u03b8 log \u03c0_\u03b8 (z_k^{(i)} | x_i) \u00b7 \\widehat{A}_k^{(i)} + \\nabla_\u03b8 log \u03c0_\u03b8 (y_i | x_i \\oplus z_k^{(i)}))$,\nwith $\\widehat{A}_k^{(i)} = r(z_k^{(i)}) - \\frac{1}{K-1} \\sum_{j \\ne k}^{K} r(z_j^{(i)})$, $r(z_j^{(i)}) := log \u03c0_\u03b8(y_i | x_i \\oplus z_j^{(i)}) - \u03b2 log \\frac{\u03c0_\u03b8(z_j^{(i)}|x_i)}{\u03c0_0(z_j^{(i)}|x_i)}$,"}, {"title": "B PROMPT TEMPLATES", "content": "In this section, we list all prompt templates used in our experiments. For the GSM8K dataset, we used a minimum prompt template. For the ARC-Challenge dataset, we use a CoT template for the base model and LaTRO fine-tuning, and another template direct answer generation in SFT."}]}