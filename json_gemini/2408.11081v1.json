{"title": "What can Large Language Models Capture about Code Functional Equivalence?", "authors": ["Nickil Maveli", "Antonio Vergari", "Shay B. Cohen"], "abstract": "Code-LLMs, LLMs pre-trained on large code corpora, have shown great progress in learning rich representations of the structure and syntax of code, successfully using it to generate or classify code fragments. At the same time, understanding if they are able to do so because they capture code semantics, and how well, is still an open question. In this paper, we tackle this problem by introducing SeqCoBench, a benchmark for systematically assessing how Code-LLMs can capture code functional equivalence. SeqCoBench contains over 20 code transformations that either preserve or alter the semantics of Python programs. We conduct extensive evaluations in different settings, including zero-shot and parameter-efficient finetuning methods on state-of-the-art (Code-)LLMs to see if they can discern semantically equivalent or different pairs of programs in SeqCoBench. We find that the performance gap between these LLMs and classical match-based retrieval scores is minimal, with both approaches showing a concerning lack of depth in understanding code semantics.", "sections": [{"title": "1 Introduction", "content": "Comprehending the semantics of code is crucial to generate new code accurately as well as to understand and verify existing code. Capturing code semantics would entail the ability to predict code functional equivalence, i.e., the property of two functions to produce the same outputs when given the same inputs, yielding the same observable behaviour, even if their implementations differ syntactically. In other words, functionally equivalent functions are interchangeable from the perspective of a program's functionality.\nIdentifying such functional equivalences is important for software development and formal verification, as it enhances software quality by detecting redundant code, encouraging reusability, preventing bug spread (Mondal et al., 2018), and boosting developer productivity. For example, when code is refactored (Shirafuji et al., 2023) or optimized (Shypula et al., 2024), it is desirable to automatically confirm that the new and old implementations behave the same. In static analysis, it is useful by reducing the risk of unexpected behaviour of a piece of code (Ding et al., 2023), reducing the effort required to verify a system's correctness and finding errors or inconsistencies in the code.\nWhile determining the equivalence between two code segments is an undecidable problem in general (Poonen, 2014), in practice, this can be partially achieved by focusing on a narrower input and code domain and by running unit tests on it. These execution-based code evaluation strategies have become increasingly widespread for evaluating code generation tasks, such as program synthesis, code translation and code summarization (Huang et al., 2022; Wang et al., 2023c) with Code-LLMs, LLMs pre-trained on large code corpora (Rozi\u00e8re et al., 2024; Li et al., 2023).\nHowever, execution-based evaluation comes with drawbacks. Firstly, it cannot scale to complex codebases that resemble real-world software domains. Currently, the test cases apply mostly to closed-domain problems having limited coverage due to the presence of either built-in functions (Li et al., 2022) or handpicked libraries from a specific field (Lai et al., 2023). Secondly, it is infeasible to cover all possible inputs, edge cases, and execution traces, and while passing tests is a good proxy for functional correctness, it does not necessarily imply the model truly understands the semantics behind the code. This leaves us with the open question: how much are Code-LLMs that are remarkable at code generation able to identify aspects of semantics such as code functional equivalence?\nIn this work, we try to answer this question by introducing the Semantic Equivalence Code"}, {"title": "2 Related Work", "content": "Code generation benchmarks. The common benchmarks, HumanEval (Chen et al., 2021) and Mostly Basic Python Problems (MBPP; Austin et al. 2021), help evaluate Python code synthesis based on functional correctness on relatively simple functions. HumanEval comprises 164 human-curated Python programming challenges proposed by OpenAI. Each task contains a docstring, function signature, function body, and a set of unit tests. Whereas MBPP consists of 974 crowd-sourced and hand-crafted Python functions. Each task contains a natural language description, code solution, and three test cases. To increase test coverage, EvalPlus (Liu et al., 2023) augments HumanEval test cases to automatically generate and diversify additional test inputs. To tackle dataset contamination, LiveCodeBench (Jain et al., 2024) gathers new coding problems not seen during model training, and EvoEval (Xia et al., 2024) which uses LLMs to transform existing benchmarks into novel coding tasks. Our approach focuses on the equivalence of the code itself rather than evaluating the correctness of a generated solution against a predefined problem. Some benchmarks test LLMs' capability to automate real-world software development processes. SWE-Bench (Jimenez et al., 2024) automatically"}, {"title": "3 Functional Equivalence of Programs", "content": "Determining the semantic equivalence of two code snippets is a challenging task that lies on a spectrum, as different codes can compute the same function, but in different ways. Consider a space of programs P, each explicitly accepting an input x \u2208 X and outputting an output y \u2208 Y. The semantics we attach to such programs is based on their input-output mapping, i.e., the function f : X \u2192 Y they implement. However, while two different programs p,p' \u2208 P can be functionally equivalent as they implement the same input-output mapping, they can concretely implement such a mapping in a very different way.\nFor example, two pieces of code can have different syntactic variations \u2013 from minor changes as inserting whitespaces to renaming variables as well as implement two different algorithms that however encode the same function. This can be done simply by using different library APIs or dependencies, different abstraction levels, or algorithms with different time and space complexity, e.g., sorting a list of integers can be equally done with mergesort or insertion sort and both will pass the same unit tests that check for input-output consistency.\nWe operationalise the question of whether LLMS can capture different functional equivalence relationships in this spectrum through the notion of code clones (Saini et al., 2018). Detecting code clones is a proxy task for checking functional equivalence that is highly relevant in software engineering, e.g., to retrieve similar code snippets for code search (Sun et al., 2023) or detect duplicates within a codebase (Yang et al., 2023). Classifying code clones into types can help us systematize possible functional equivalence classes. Following previous work (Roy and Cordy, 2007; Bellon et al., 2007), we categorise code clones into four types based on their complexity and degree of similarity.\ntype-1 clones comprise two almost syntactically identical pieces of code that differ only for minor variations in the layout, e.g., by the presence of whitespaces and comments.\ntype-2 clones resemble two syntactically identical code fragments except for variations in variable and function names, identifiers, literal values, types, etc. They also also comprise type-1 differences.\ntype-3 clones include two syntactically identical code fragments except for additions, deletions, modifications of several statements, and the differences already specified for to type-2 clones.\ntype-4 clones, also known as semantic clones. They exhibit identical functional behaviour despite having different syntax, control flow, data flow, or programming languages.\nThis typology of clones helps us devise diverse groups of transformations that can preserve or alter the semantics of a program while also modifying its syntax. Such transformations enable us to create a challenging benchmark to systematically evaluate whether LLMs can capture functional equivalence and inspect at which level they can disentangle syntax from semantics, as discussed next."}, {"title": "4 The SeqCoBench Dataset", "content": "Traditionally, code benchmarks for evaluating LLMs have focused on assessing their ability to generate single-function programs based on natural language descriptions. This is done for example in popular benchmarks such as HumanEval (Chen et al., 2021) and Mostly Basic Python Problems (MBPP; Austin et al. 2021), which comprise human-curated Python code snippets with a docstring, function signature, function body, and a set of unit tests to check if generated code satisfies specifications.\nInstead, as we want to evaluate the ability of LLMs to capture functional equivalence between already existing pieces of code, we construct our SeqCoBench by creating pairs of code snippets that are labelled to be functionally equivalent or not. More formally, given a program pi, we generate the tuple (pi, t(pi), li) where t is a semantic-preserving (SP; Section 4.1.1) or semantic-altering (SA; Section 4.1.2) transformation that generates a new code snippet whose semantics changes accordingly and li \u2208 {0,1} is a label indicating if the pair has been generating through a SP (1) or SA (0) transformation.\nWe build SeqCoBench by applying a set of SP or SA transformations to programs appearing in MBPP, they provide unit tests that help us check if our transformations correctly operate on the program semantics. We prepare the train/valid/test splits following a 60/16/24 ratio and ensure that there is no overlapping of the original code across different data splits."}, {"title": "4.1.1 Semantic-Preserving Transformations", "content": "Given a program p, we aim to generate a new code snippet p' that is functionally equivalent to p (i.e., they encode the same input-output mapping f) while maximizing the token-level differences between the original code where appropriate. To this end, we consider the four SP transformations. We group them by the corresponding clone type (Section 3) while trying to cover all types. Note that we omit type-1 clones as they typically include comment- and docstring-level perturbations written in natural language and are not technically part of code semantics. They are used for documentation purposes only and do not affect the execution or behaviour of the code itself.\nWhile adversarial perturbations that alter the function's intended meaning can cause the LLM to ignore the function body completely and instead give more emphasis to the perturbed entity's instructions, reformulating the perturbations using text augmentation strategies such as back-translation (Wang et al., 2023a), synonym substitutions, etc., can make it less challenging for LLMs. We create these transformations using the NatGen package (Chakraborty et al., 2022). We leave out transformations on the original code where the necessary requirements to perform the transformation are not met (e.g., lack of for/while loop or boolean operators in the code snippets).\nRename Variables (RV) type-2 We primarily use three different adaptations. Naive: It renames the most common variable name to VAR_i. CB: It identifies the variable name that appears most frequently in the partial code snippet and then substitutes all occurrences of that variable name throughout the prompt with a new name suggested by Code-BERT. RN: It identifies the variable name that occurs most frequently within the given partial code snippet and then generates a random string composed of an equal mix of alphabetic and numeric characters. Finally, it substitutes all instances of the most commonly used variable name with this newly generated random string.\nDead Code Insertion (DCI) type-3 It creates unreachable code blocks at a random location. These could be unused variables or redundant assignments. We place these statements in a block around either a looping (e.g., for, while) or a branching structure (e.g., if-else, switch), if any.\nOperand Swap (OS) type-3 It swaps the first occurrence of boolean operators and, if needed, changes the operator to preserve semantic equivalence. For example, if the original code had the condition a > b, the transformation could change it to b < a, swapping the operands \"a\" and \"b\" while also changing the operator from \">\" to \"<\" to preserve the same logical meaning.\nLoop Transformation (LT) type-4 It converts the first occurrence of for-loop into its equivalent while-loop and vice-versa.\nIn the for while case, we initialize the counter"}, {"title": "4.1.2 Semantic-Altering Transformations", "content": "In this case, our goal is to generate a program t(p) that is functionally not equivalent to the original code p while maximizing token-level similarity to the original code. In other words, generate pairs of programs that are not clones but might fool a superficial comparison. Accordingly, we consider six families of SA transformations. As before, we leave out transformations on the original code where the necessary requirements to perform the transformation are not met (e.g., unavailability of identity or boolean operators).\nArithmetic Operators Misuse (AOM). We search for the first occurrence of an arithmetic operator and modify it to its semantic opposite counterpart. For example, we replace a + b with a - b, a - b with a / b, and the other way around. Similarly, we replace augmented assignment operators a += b to a -= b and a *= b to a /= b.\nDissimilar Code Selection (DCS). We randomly select five distinct code snippets from the base dataset, excluding the original code p, and create five additional code pairs using p as a reference.\nIdentity Operators Misuse (IOM). We look for"}, {"title": "5 Experiments", "content": "Through the use of our dataset, we aim to answer the following research questions: RQ1: How good are state-of-the-art LLMs at zero-shot classification on SeqCoBench? RQ2: Which are the most/least challenging semantic transformations for Code-LLMs? RQ3: Can the performance improve with fine-tuning on some transformations?\nWe demonstrate that Code-LLMs are far from \"solving\" our dataset, leaving it for future work to further use our dataset as a benchmark for analysing the level of code understanding in such LLMs."}, {"title": "5.1 Models", "content": "To evaluate the benchmark, we choose general and state-of-the-art Code-LLMs that have performed well on code-related benchmarks (such as HumanEval) and are open-sourced for commercial use, as our test models. We exclude closed-source LLMs because we want to see the impact of finetuning and we are worried about possible data leaks, as HumanEval, MBPP, and relative benchmarks might have already been used to train the largest closed-source LLMs.\nLlama 2 (Touvron et al., 2023) is a family of LLMs developed by Meta AI, ranging from 7B to 70B parameters. It is an open-source successor to the original Llama model, offering improved performance through a larger training corpus, longer context length, and the use of grouped-query attention.\nStarCoder (Li et al., 2023) is a generative model with 15.5B parameters trained on source code and natural language. Its training data contains more than 80 different programming languages, and text crawled from GitHub issues/commits/notebooks.\nCode Llama (Rozi\u00e8re et al., 2024) is initialized using pre-trained weights of Llama 2 and trained on code-specific datasets. It then undergoes longcontext finetuning and can handle repository-level inputs of 100K tokens."}, {"title": "5.2 RQ1: Match-based vs. LLM-based Metrics Performance", "content": "To evaluate the model performance, we measure the area under the precision-recall curve to calculate the average precision (AP) score:\n$AP = \\sum_{n}(R_n - R_{n-1}) \\cdot P_n,$\nwhere Pn and Rn correspond to the precision and recall at the nth threshold. AP accounts for ranking by rewarding models that rank correct predictions higher and are calculated per class, allowing performance evaluation on individual classes. We note that AP is the area under the precision-recall curve (AUC-PR) curve and is a more accurate metric for slightly imbalanced datasets than the usual AUC-ROC. This imbalance mainly happens when the positive class (SP) is lesser in magnitude than the negative class (SA). This is because AP focuses more on the performance of the positive class and is more sensitive to improvements in the positive class predictions compared to AUC-ROC. At the"}, {"title": "5.3 RQ2: Impact of Semantic Transformations", "content": "To assess the performance of various CEMs on each transformation, we consider the true positive label for SP transformations as 1 and 0 for SA transformations. As it becomes a single-class classification per transformation, the precision will always be 1 due to zero false positives. In this case, the recall score measures the fraction of all actual positive instances correctly identified. So, we measure the area under the recall curve (AURC) corresponding to all the chosen thresholds, which varies between 0 and 1 and use the obtained result for our analysis as shown:\nR, T = recall_curve(y,\u0177,pos Label)\nAURC = auc(T,R)\nwhere recall_curve refers to the plot of recall scores against different thresholds. y is the true"}, {"title": "5.4 RQ3: Impact of Finetuning", "content": "While finetuning on a diverse set of transformations can improve performance on seen examples, it does not necessarily guarantee effective generalization to novel, unseen transformations. We propose a leave-one-out evaluation strategy for assessing the performance of PEFT methods on unseen semantic transformations. The approach involves finetuning the model on N-1 transformation for a given category, then evaluating on the held-out Nth transformation. We repeat the leave-one-out approach N times to assess performance on each held-out transformation. We note that Llama2-7B outperforms CodeLlama7B on most transformations."}, {"title": "6 Conclusion", "content": "We propose SeqCoBench, a new challenging benchmark to evaluate how well Code-LLMs capture functional equivalence between code snippets from the code semantics standpoint. We compare the performance of LLM- and match-based metrics on the SeqCoBench and find the performance gap to be minimal. We identify semantic transformations for the Code-LLMs from least to most challenging on a spectrum. We conduct extensive evaluations in different settings, including zero-shot (w/ prompting) and using PEFT methods. In the future, we would like to study code semantics in both static and dynamic settings at different granularities by incorporating approximate, operational, and abstract semantics (Ding et al., 2024)."}, {"title": "Limitations", "content": "We investigate open-source LLMs to evaluate for code functional equivalence, so we do not consider ICE-Score (Zhuo, 2024) that requires using closedsource LLMs like GPT-3.5 or GPT-4 in this analysis. Closed-source models are opaque, as their inner workings, data sources, and training methodologies are not disclosed, making it hard to draw meaningful comparisons with open-source models. In addition, they often come with significant usage costs, and finetuning is not fully supported in experimental access. Currently, the code functions in SeqCoBench are exclusively in Python. However, we aim to broaden the scope to encompass a broader range of programming languages and domains. By doing so, we strive to enhance the diversity and applicability of the dataset, making it more comprehensive and versatile for various software engineering tasks and scenarios. In addition, we do not check for compilation of semantic code transformations as we perform a static code evaluation to analyse the code without needing it to be executed or compiled. Further, we do not account for variations due to prompt and temperature as we do not optimise the prompting format, although we ensure it is kept consistent across different LLMs. Finally, we refrain from chaining multiple transformations of the same type (either preserving or altering) to make the analysis straightforward."}, {"title": "I Other Works", "content": "Robustness against adversarial attacks. There is a strong connection between adversarial attacks and semantic transformations for code. Both involve minor changes to a code input that preserve the original meaning or functionality but cause a machine learning model to make incorrect predictions. We derive inspiration from previous works in software engineering to leverage semantic-preserving code transformations. For instance, Rabin et al. (2021) highlights the importance of considering semantics-preserving transformations when building reliable neural program analysers, and Compton et al. (2020) rely on data augmentation methods based on variable renaming to design efficient models.\nA few works focused on addressing the robustness problem for the code on attacks and defences. Miceli Barone et al. (2023) demonstrates how LLMs understand code semantics by approximating a-equivalence of Python code and suggest improved defences against identifier substitution attacks. Yang et al. (2022) argue that adversarial examples should preserve natural semantics in addition to operational semantics. To identify vulnerabilities of Code-LLMs to adversarial attacks, Jha and Reddy (2023) introduces CodeAttack that generates adversarial samples for a code fragment, and Wang et al. (2023b) designs a robustness evaluation benchmark and evaluates Code-LLMs on their ability to generate equivalent code across perturbed prompts while we aim to comprehend and analyze existing code."}, {"title": "J Code Clone Detection and Code Obfuscation", "content": "While there is a degree of overlap in spirit between the transformations appearing in code clone detection datasets and our benchmark, our primary goal is different. As stated above, capturing functional equivalence is a harder task and lets us evaluate how LLMs disentangle syntax from semantics. We note that our transformations are designed to be the simplest as possible (e.g., flipping an operator) to detect how much LLMs get confused by transformations that are trivial for us humans. This is a design choice that lets us carry on a finer grain analysis that is not possible with current benchmarks such as BigCloneBench (Svajlenko et al., 2014) and, as such, should not be rated as not substantial. Current code clone detection benchmarks, e.g., BigCloneBench, propose a different task than ours: they are designed to identify code fragments that share the same high-level functionality, but that can be very different from semantically equivalent problems. For example, two functions that can have different side-effects, or even having different input and output types are still considered \u201cclones\u201d. Our semantic equivalence definition is more stringent and better captures what LLMs understand about code execution. Moreover, Krinke and Ragkhitwetsagul (2022) highlight how many clone pairs are falsely tagged as actual clones in BigCloneBench.\nWe remark that our objective is not to create a dataset to improve LLMs capabilities to detect clones in the real world. Instead, we want a controlled environment where we can exactly understand which simple transformations are able to confuse LLMs by discriminating syntax from semantics. We also argue that simple transformations can be common in real-world scenarios, e.g., out of simple typos and wrong copy-pasting actions that are common in programming. While having real-world large code repositories would be nice, it is a non-trivial task to collect the samples (for e.g., from GitHub) that can be validated using unit tests and at the same time allow us to have a controlled experimental setting. Hence, we rely on existing benchmarks which have samples tested by human programmers for functional correctness to build our benchmark.\nWe further note that our approach differs from code obfuscation approaches as the main difference is that the former intentionally reduces readability (up to the point that the code can be hard for humans to follow). Our semantic transformations, on the other hand, want to preserve human readability and intelligibility. Our transformations are simple by design and hard enough to show certain failure modes of LLMs that are commonly used and deployed. As we state in Section 4 we design them to make the simplest change in syntax or semantics that allows us to measure that code LLMS struggle to disentangle these two aspects. This is a feature, not a bug. Having more complex transformations would simply imply that LLMs failing on simple transformations are also failing on more complex ones (but without the simplest ones, we would not know what is the smallest perturbation that confuses the LLMs)."}, {"title": "K Examples", "content": "When evaluating LLMs for their ability to classify code snippets as functionally equivalent or not, it is essential to consider a variety of examples that highlight both the strengths and limitations of these models. Below, we present a selection of examples that demonstrate scenarios where LLMs may find it easy or hard to determine code equivalence for the different transformations in SeqCoBench."}]}