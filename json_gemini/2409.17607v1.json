{"title": "Dirichlet-Based Coarse-to-Fine Example Selection\nFor Open-Set Annotation", "authors": ["Ye-Wen Wang", "Chen-Chen Zong", "Ming-Kun Xie", "Sheng-Jun Huang"], "abstract": "Active learning (AL) has achieved great success\nby selecting the most valuable examples from unlabeled data.\nHowever, they usually deteriorate in real scenarios where open-\nset noise gets involved, which is studied as open-set annotation\n(OSA). In this paper, we owe the deterioration to the unreliable\npredictions arising from softmax-based translation invariance\nand propose a Dirichlet-based Coarse-to-Fine Example Selection\n(DCFS) strategy accordingly. Our method introduces simplex-\nbased evidential deep learning (EDL) to break translation invari-\nance and distinguish known and unknown classes by considering\nevidence-based data and distribution uncertainty simultaneously.\nFurthermore, hard known-class examples are identified by model\ndiscrepancy generated from two classifier heads, where we\namplify and alleviate the model discrepancy respectively for\nunknown and known classes. Finally, we combine the discrepancy\nwith uncertainties to form a two-stage strategy, selecting the most\ninformative examples from known classes. Extensive experiments\non various openness ratio datasets demonstrate that DCFS\nachieves state-of-art performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Active learning (AL) [1]\u2013[9] is a fundamental approach\naimed at alleviating considerable expenses associated with data\nannotation [10]. It selects the most informative examples from\nunlabeled data and subsequently queries labels from an oracle,\nthereby significantly enhancing model performance at the low-\nest possible labeling cost [11]-[13]. Traditional AL methods\ntypically choose examples based on either the model's predic-\ntion uncertainty or the diversity of sample features. For exam-\nple, diversity-based methods tend to choose examples based on\nthe diversity of example features. Uncertainty-based methods\naim to select examples with the highest prediction uncertainty.\nHybrid methods consider both diversity and uncertainty, such\nas BADGE [14]. However, these AL methods are predomi-\nnantly implemented in closed-set scenarios, where labeled and\nunlabeled data share the same class distribution. Unfortunately,\nin many real-world scenarios, since unlabeled examples are\ncollected through cost-effective methods, e.g., web scraping\n[15], unlabeled data inevitably contains examples not from a\nclosed set. Traditional AL methods often fall short in such\nopen-set scenarios. For example, diversity-based methods will\ninevitably select unknown-class examples for the diversity\nof their features, and the uncertainty-based methods tend to\nchoose the unknown-class examples for the inductive biases,\nthat the model will provide unreliable predictions for the\nunknown-class examples that mismatch with the distribution\nof training data. Manually identifying unknown classes in the\npool of unlabeled data often requires a significant amount of\ntime and financial resources. Therefore, effectively performing\nAL algorithms based on unlabeled data pools that contain\nunknown categories has become a pressing issue.\nRecently, the learning scenario has been formulated as open-\nset annotation (OSA), and several methods have been proposed\nto solve this problem. These methods often query examples\nthat have the largest similarity to labeled data [16], [17] or\nthe max activation value (MAV) [18]. Although these methods\nhave made great progress in OSA, there still exist two main\nchallenges:\nWe found that all previous methods encounter the same\nchallenge in OSA settings: the probabilities calculated by the\nsoftmax function is a point estimate that can only capture\nrelative relationships between logits, leading to inaccuracies\nwhen evaluating unknown-class examples. As shown in Figure\n1, given an example x1 belongs to \"Cat\" with a logit vector\nof [1.3, 2.3, 1.3] and another example x2 belongs to \u201cLynx\u201d\nwith a logit vector of [0.7,1.6,0.53]. Despite x2 being an\nunknown class \"Lynx\u201d, it will present the same probability\nto the category \"Cat\" by using traditional softmax-based\nprobability. This is attributed to the translation invariance of\nthe softmax function. Such inaccurate probabilities will mis-\nclassify unknown-class examples to known classes, severely\ncompromising the effectiveness of example selection when\nconducting AL in open-set scenarios.\nMoreover, despite previous OSA methods gathering more\nknown-class examples, the lack of an informativeness measure\noften results in models favoring simple examples, thereby\nlimiting their practicality on complex open-set scenarios. For\nexample, although [16] utilizes contrastive learning to select\nmany known-class examples that share similar features with\nthe labeled data, such examples promote little in model im-\nprovement for their low informativeness because the model\nhas learned from the similar examples queried before.\nWe attribute the first challenge to the translation invariance\ncaused by the softmax function. Inspired by evidential deep\nlearning (EDL), which treats the model's predictions as a\nrandom variable rather than a simple point estimate, we further\nrefine the logit outputs of the model by imposing a Dirichlet\nprior on the predicted probabilities. For example, as illustrated\nin Figure 2, although \"Cat\" and \"Lynx\" share the same\nprediction on \"Cat\", the prediction with a higher evidence\nvalue of 10 generates a more concentrated distribution closer\nto the corner, while the unknown \"Lynx\u201d with lower evidence\nvalue of 5 generates a more dispersed distribution. This implies\nthat, compared to softmax-based predictions, EDL can break\nits translation invariance and present more reliable predictions.\nFurthermore, to balance the informativeness and purity of\nexamples in querying, we use the mean and variance of the\ndistribution entropy of the simplex as measures of data and\ndistribution uncertainty. Normal known-class examples, like\n\u201cCat\u201d, located close to the corner of simplex, exhibit lower\nuncertainty, while unknown-class examples, like \u201cLynx\u201d, lo-\ncated farther from the corner, show higher uncertainty. Rare\nexamples between two classes, like the black \"Cat\", demon-\nstrate higher distribution uncertainty. This suggests that we\ncan differentiate informative known-class examples based on\nthese two metrics.\nBased on this, we propose a Dirichlet-based Coarse-to-\nFine Example Selection (DCFS) strategy to select informative\nknown-class examples. To break softmax's translation invari-\nance, we train the target model using simplex-based evidential\ndeep learning (EDL) which treats the model's predictions as\na random variable with the Dirichlet prior. To query examples\nthat are both informative and known-class ones, we decou-\nple the probabilistic simplex into two different uncertainty\nmetrics, i.e., distribution uncertainty for purity measurement\nand data uncertainty for informative measurement. Then, to\nfurther distinguish between hard known-class examples and\nunknown-class ones, inspired by [19], we introduce a model\ndiscrepancy score by implementing two classifier heads atop\nthe target model and fine-tuning the model on unlabeled data.\nAn example with a large discrepancy score will more likely\nbelong to an unknown class. Extensive experiments demon-\nstrate that DCFS not only outperforms previous methods on\nthe prediction accuracy across all openness ratios, but also\nselects more informative examples instead of simple known-\nclass examples, substantially improving model performance\nand achieving state-of-the-art performance."}, {"title": "II. METHODOLOGY", "content": "A. Preliminary\nNotations. In open-set annotation (OSA) problems, a limited\nlabeled data pool $D_l = \\{(x_i, y_i)\\}_{i=1}^{N_l}$ each $x \\in X$ belongs to\none of C known classes $y = \\{c\\}_{c=1}^{C}$ is provided, along with an\nunlabeled data pool $D_u = \\{x_i\\}_{i=1}^{N_u}$ which is a mixture of both\nknown and unknown class examples. r denotes the openness\nratio and represents the proportion of unknown-class examples\noccupying. In each iteration, active learning (AL) algorithms\nquery b examples from $D_u$ to form the query set $X_{query}$,\nwhich is then sent to the oracle for annotations and updated\nto the corresponding data pool.\nOverview. The framework of our proposed approach is illus-\ntrated in Figure 3, which primarily involves three stages: model\ntraining, example selection, and oracle labeling. In the Model\nTraining phase, two classifier heads are implemented atop the\ninitial model. We first train the target model with simplex-\nbased evidential deep learning (EDL) to collect evidence and\nproduce simplex-based distribution for examples. Then, we\ntrain the model to amplify the model discrepancy of the two\nclassifier heads for unlabeled data. These two training stages\nare processed alternately, leading that known-class examples\nown lower model discrepancy and a more close-to-corner\nsimplex-based distribution. In the Example Selection phase,\nwe extract the data uncertainty and distribution uncertainty\nfrom the distributed entropy of the simplex. These two uncer-\ntainties are combined with the model discrepancy to estimate\nwhether examples are from known classes and informative\naccordingly. In the Oracle Labeling phase, we send the queried\nexamples to annotators for class labels. Queried known-class\nexamples will be updated to the labeled data.\nB. Simplex-Based Evidential Training Methodology\nAs discussed in the Introduction, one distinct challenge in\nOSA is misclassifying unknown-class examples as known-\nclass ones due to the softmax's translation invariance. To\ncounter this, we propose to introduce Simplex-based evidential\ndeep learning (EDL), which is based on Subjective Logic [20]\nand Dempster-Shafer Theory [21] to collect the evidence of\nexamples being every category. Compared to the softmax-\nbased prediction, EDL serves the exponential of logits as\nevidence and represents the evidence for all categories as\nthe parameter vector a of Dirichlet prior. Through this, a\ndistribution of probability p for a given example can be\ndescribed on a class simplex $\u2206^C = \\{p|\\sum_{c=1}^C P_c = 1\\}$.\nFormally, EDL treats the probability p as a random variable\nand calculates it for each input $x_i$ based on the Dirichlet prior.\nWith denoting the model parameters, we compute the evi-\ndence vector $\u03b1_i$ for a given example $x_i$ as $\u03b1_i = g(f(x_i, \u03b8))$,\nwhere g() represents an exponential function to guarantee that\nevidence collected by EDL is positive. Then, the probability\ndensity of $p_i$ can be calculated by:\n$p(p_i|x_i, \u03b8) = Dir(p_i|\u03b1_i) = \\frac{\u0393(\\sum_{c=1}^C \u03b1_{ic})}{\u220f_{c=1}^C \u0393(\u03b1_{ic})}\u220f_{c=1}^C p_i^{\u03b1_{ic} -1}.$\\nBy marginalizing over $p_i$, the predicted probability on a given\nclass c can be obtained by:\n$P(y = c|x_i, \u03b8) = \u222b p(y = c/p_i)p(p_i, \u03b8)dp_i $\n$= \\frac{g(f_c(x_i, \u03b8))}{\\sum_{k=1}^C g(f_k(x_i, \u03b8))}.$\\nTo produce a sharp Dirichlet distribution situated at the corner\nfor known class examples, we introduce the loss $L_{nll}$ to\nminimize the negative logarithm of the marginal likelihood\nfor all labeled data by:\n$L_{nll} = \\frac{1}{N} \\sum_{i=1}^N log[P(y = c/P_i)]P(P_i|x_i, \u03b8)$\n$= \\frac{1}{N} \\sum_{i=1}^{N_C} \\sum_{c=1}^C Y_{ic}[log(\u03b1_{ic}) - log(\u0393(\u03b1_{ic}))]$,\nwhere $Y_{ic}$ is the c-th element of the one-hot label vector\n$Y_i$ for $x_i$ with the scalar label $y_i$. To reduce the evidence\ncollected on complementary labels (labels other than the given\nlabel), we reduce the evidence on other categories to zero\nby implementing the Kullback-Leibler Divergence loss $L_{kl}$\ndefined as:\n$L_{kl} = \\frac{1}{N_C} \\sum_{i=1}^N \\sum_{c=1}^C D_{KL}(Dir(p|\u03b1_i)||Dir(p|1))$\n$= \\frac{1}{N} \\sum_{i=1}^N [log[\\frac{\u0393(\\sum_{c=1}^C \u03b1_{ic})}{\u0393(C) \u220f_{c=1}^C \u0393(\u03b1_{ic})}]$\n$+ \\sum_{c=1}^C (\u03b1_{ic} - 1)[\u03a6(\u03b1_{ic}) - \u03a6(\\sum_{j=1}^C \u03b1_{ij})]]$,\nwhere $D_{KL}$ denotes the Kullback-Leibler Divergence (KLD),\n\u0393 represents the Gamma function, \u03a6 indicates the digamma\nfunction, is the element-wise multiplication, and $\\tilde{\u03b1_i} = Y_i+$\n$(1 \u2013 Y_i)\\frac{\u03b1_i}{\u03b1_i}$ is a temp form by removing the given label\nevidence from the original $\u03b1_i$. Finally, the overall EDL loss\n$L_{edl}$ is formulated as:\n$L_{edl} = L_{nll} + L_{kl}.$\nTraining with this will lead the \"Cat\" in Figure 2 to collect\nmore evidence of being \"Cat\" and less evidence of other\ncategories, generating a more concentrated distribution near\nthe corner than the unknown \"Lynx\u201d, which is superior to\nsoftmax-based metric on distinguishing between them by the\nsimplex-based distribution and thus can resolve the translation\ninvariance challenge.\nC. Evidence-Based Data and Distribution Uncertainty\nThe known-class examples usually own a sharp distribu-\ntion near the corner of the simplex, while the unknown-\nclass ones own those in the center. Inspired by [22], we\nmodel such correlations between examples and distributions\nby splitting expected prediction entropy into two parts: data\nand distribution uncertainty. Here, data uncertainty indicates\nthe probability of examples being known or unknown class.\nDistribution uncertainty represents the degree of evidence\ncollected during the training stage."}, {"title": "D. Discrepancy-Based Training Methodology", "content": "Due to data scarcity limiting the collection of evidence,\ndepending solely on $U_{data}$ may still result in the misclas-\nsification of hard known-class examples as unknown-class\nones. Previous work shows [19] that \u201cNoisy\u201d examples tend\nto exhibit high model discrepancy when models are trained\nin different manners. We extend this concept by considering\nunknown-class examples as noisy ones and retaining the hard-\nknown class with lower model discrepancy. In practice, we\nimplement two classifier heads atop the model to produce\nprediction discrepancy, which forms the discrepancy score\naiding in unknown-class filtering.\nFormally, let $\u03b8_g$ represents the parameters of feature rep-\nresentation, $\u03b8_{f_1}$ and $\u03b8_{f_2}$ denotes the parameters from two\nclassifier heads respectively. $\u03b1^1$ and $\u03b1^2$ are the evidence\noutputs for the given example $x_i$. Specifically, after the model\nhas been trained on the labeled data pool, we freeze the\nparameters $\u03b8_{f_1}$ and $\u03b8_{f_2}$, and only fine-tune the parameters $\u03b8_g$\non unlabeled data pool. In practice, we minimize the two-head\nprediction discrepancy with the Jensen-Shannon Divergence\n(JSD) $D_{JSD}$ by loss $L_{close}$ defined as:\n$arg \\underset{\u03b8_g}{min} L_{close} (\u03b1^1, \u03b1^2) = \\sum_{i=1}^N W_iD_{JSD}(\u03b1_i^1 || \u03b1_i^2)$\n$W_i = \\frac{1}{N} (1 - \\frac{(U^{data} - \u03c4_1)}{\u03c1}).$\nHere, data with low $U_{data}$ will be assigned a larger update\nweight. Then, we fix the parameters $\u03b8_g$, and only fine-tune\nthe parameters $\u03b8_{f_1}$ and $\u03b8_{f_2}$ same on unlabeled data pool. This\ntime, we amplify the two-head prediction discrepancy for high\n$U_{dist}$ examples by:\n$arg \\underset{\u03b8_{f_1}, \u03b8_{f_2}}{min} L_{dis} (\u03b1^1, \u03b1^2) = \\sum_{i=1}^N w_i(1 \u2013 D_{JSD}(\u03b1_i^1 || \u03b1_i^2))$\n$W_i = \\frac{1}{N} ( \\frac{(U^{dist} - \u03c4_2)}{\u03c1}).$\nThrough this, data in $D_u$ will exhibit two distinct simplex-\nbased distributions based on the outputs from the two classifier\nheads. Here, we define the L2-norm minus between outputs $\u03b1^1$\nand $\u03b1^2$ as the discrepancy score $S^{dis}$ and have it participate\ninto the known-class example selection:\n$S^{dis}(x_i; \u03b1_i^1), \u03b1_i^2)) = \\sum_{c=1}^C (\u03b1_{ic}^1 - \u03b1_{ic}^2)^2.$\nE. Dirichlet-Based Coarse-to-Fine Selection Strategy\nFinally, by integrating three uncertainty measurements, we\npropose a coarse-to-fine strategy to gradually identify informa-\ntive known-class examples. In the coarse stage, considering\nthat known-class examples usually present low data uncer-\ntainty and model discrepancy, we combine these two scores\nas $S^{dis} + \u03b1U^{data}$. Then, we fit a two-mode Gaussian Mixture\nModel (GMM) based on the score to filter out unknown class\nexamples. By denoting $G(S^{dis} + U^{data})$ as the probability\nbelonging to known-class mode, we adopt a threshold \u03bb to\nselect known class examples as\n$X_{sub} = \\bigcup_{x\u2208D_U}[G(S^{dis}(x) + \u03b1U^{data}(x)) > \u03bb]$\nwhere $X_{sub}$ is the subset of unlabeled data selected in the\ncoarse stage. Moreover, as distribution uncertainty indicates\nthe informativeness of examples and data uncertainty indicates\nthe dissimilarity of the given example to labeled ones, we\nselect the informative and representative examples based on\nthe measurement $\u03b2U^{data} +U^{dist}$. The selection procedure can\nbe formulated as:\n$X_{query} = arg \\underset{x\u2208X_{sub}}{max} (\u03b2U^{data}(x) + U^{dist}(x))$\nIII. EXPERIMENTS"}, {"title": "IV. CONCLUSION", "content": "In this paper, we propose a Dirichelet-based Coarse-to-\nFine Example Selection (DCFS) strategy that selects known-\nclass informative examples in open-set data. On the one hand,\nDCFS collects evidence for each class using evidential deep\nlearning (EDL), with split evidence-based data and distribution\nuncertainty depicting the correctness and evidence level of\nthe examples. On the other hand, two classifier heads are\nimplemented atop the model to retain hard known-class ex-\namples by evaluating the discrepancy score originating from\namplified model discrepancy. Finally, informative known-class\nexamples are selected with a strategy combined with the model\ndiscrepancy, data, and distribution uncertainty. Experimental\nresults demonstrate that our approach achieves state-of-the-art\nperformance in intricate open-set scenarios across all settings."}]}