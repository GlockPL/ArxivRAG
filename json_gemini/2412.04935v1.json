{"title": "Uncertainty-aware retinal layer segmentation in OCT through probabilistic signed distance functions", "authors": ["Mohammad Mohaiminul Islam", "Coen de Vente", "Bart Liefers", "Caroline Klaver", "Erik J Bekkers", "Clara I. S\u00e1nchez"], "abstract": "In this paper, we present a new approach for uncertainty-aware retinal layer segmentation in Optical Coherence Tomography (OCT) scans using probabilistic signed distance functions (SDF). Traditional pixel-wise and regression-based methods primarily encounter difficulties in precise segmentation and lack of geometrical grounding respectively. To address these shortcomings, our methodology refines the segmentation by predicting a signed distance function (SDF) that effectively parameterizes the retinal layer shape via level set. We further enhance the framework by integrating probabilistic modeling, applying Gaussian distributions to encapsulate the uncertainty in the shape parameterization. This ensures a robust representation of the retinal layer morphology even in the presence of ambiguous input, imaging noise, and unreliable segmentations. Both quantitative and qualitative evaluations demonstrate superior performance when compared to other methods. Additionally, we conducted experiments on artificially distorted datasets with various noise types shadowing, blinking, speckle, and motion-common in OCT scans to showcase the effectiveness of our uncertainty estimation. Our findings demonstrate the possibility to obtain reliable segmentation of retinal layers, as well as an initial step towards the characterization of layer integrity, a key biomarker for disease progression. Our code is available at https://github.com/niazoys/RLS_PSDF.", "sections": [{"title": "1. Introduction", "content": "In recent years, deep learning methods have demonstrated remarkable success in automating the delineation and segmentation of retinal layers in Optical Coherence Tomography (OCT) scans. The task of retinal layer segmentation is primarily tackled via pixel-wise classification, an approach extensively explored in recent works."}, {"title": "2. Method", "content": "Consider an OCT B-scan, or image, $I : X \\times Y \\rightarrow R$ that assigns to each coordinate x, y an intensity value $I(x, y)$, with $x \\in X$ the horizontal coordinate and $y \\in Y$ the vertical coordinate. Let $a_x \\in I$ denote a single A-scan at horizontal location x as $a_x: Y \\rightarrow R$, and it is defined $a_x(y) := I(x,y)$. The ground truth of a retinal layer can be parametrized as a function $y^{gt} : X \\rightarrow Y$ that assigns to every a-scan a corresponding vertical y coordinate. The y location in single A-scan $a_x$ may then be denoted as $y_x^{gt} := y^{gt}(x)$. With this notation in place, we present three methods for retinal layer segmentation: pixel-wise, regression, and signed distance function (SDF) approach.\nPixel-wise approach: A neural network f predicts a segmentation functions:$X \\times \u0423 \\rightarrow [0,1]$ that returns a probability for the layer to each position. Specifically, a network with parameters $\\theta$ generates $s = f_{segm}[I;\\theta]$, such that ideally $s(x, y) = 1$ at the location of the layer, and zero otherwise. Typically, the pixel-wise segmentation predictions are parametrized with Bernoulli distributions, giving a probability for object presence and uncertainty that is directly reflected by the entropy (i.e., $-s log(s) \u2013 (1 \u2013 s) log(1 \u2013 s)$) of the distribution. Notably, this approach can only express uncertainty in the presence of the structure, but not its shape.\nRegression approach (REGR & pREGR): A neural network $f_{regr}$ predicts a regression function $y^{regr} : X \\rightarrow Y$ that parametrizes the layer similarly as done in the ground truth, generating $y^{regr} = f_{regr} [I;\\theta]$ such that $ \\forall x : y_x^{regr} \\approx y_x^{gt}$. Such a model will be referred to with label REGR. To incorporate uncertainty in the predicted location we let the neural network parametrize a predictive distribution\n$p(y(x) | I, \\theta) = \\Nu(\\mu(x), \\sigma(x)^2),$    (1)"}, {"title": "", "content": "which we model as a Gaussian distribution with mean $\\mu(x) = f_{regr}[I;\\theta](x)$ and variance $\\sigma(x) = f_{regr}[I;\\theta](x)$. Such a probabilistic approach will be referred to with pREGR.\nSigned distance function approach (SDF): A neural network $f_{sdf}$ predicts a signed distance function $d : X \\times Y \\rightarrow R$ with the property that it satisfies the Eikonal constraint, that is $\\forall x,y : ||\\nabla d(x, y) || = 1$, and $d(x, y_x^{gt}) = 0$, such that d can be interpreted as a distance to the layer. The Eikonal constraint ensures that d linearly increases when moving away from the boundary. Note that in our method the Eikonal constraint is enforced by explicit construction of the ground truth to train the model. Then from a predicted SDF d, the actual location of the layer is obtained by solving\n$d(x, y) = 0,$    (2)\ni.e., by finding the zero-level set of d, where the level set of a function d is defined as the set of coordinates $S^l$ for which it equals the given level l. In our setting, this means that $S^l := {(x, y) \\in X \\times Y | d(x, y) = l }$. Since we can define the SDF per A-scan as $d_x$ we can also define the per-A-scan level set as $S_x^l := {y \\in Y | d_x(y) = l }$.\nDue to the laminar structure of the retina, and the Eikonal constraint, we can assume that the per-A-scan zero level set consists of only a single element, which we denote with $y^{sdf} (x)$ and which is the unique solution of (2). We thus have $S^0 = {y^{sdf} (x)}$. In contrast to the regression method which explicitly (directly) parametrizes a layer through a contour function $y^{regr} : X \\rightarrow Y$, the signed distance function approach implicitly parametrizes $y^{sdf} : X \\rightarrow Y$ as the zero level set of d. We label the latter method with SDF.\nProbabilistic SDF-based regression (pSDF): We model uncertainty on the location of the layer by letting the neural network predict the expected distance $\\mu(x, y) = f_{sdf} [I;\\theta](x, y)$ as well as an uncertainty for that prediction $\\sigma(x,y) = f_{sdf} [I; \\theta](x, y)$, to parametrize a predictive distribution for the SDF d via\n$p(d(x, y) | I, \\theta) = \\Nu(\\mu(x, y), \\sigma(x)^2).$   (3)\nThe uncertainty in the predicted SDF value is directly related to uncertainty in the actual contour location. Namely, if the SDF is normally distributed as above, then the uncertainty $\\sigma(x, y)$ on the SDF value translates to uncertainty on the vertical location via\n$p(y(x) | I, \\theta) = \\Nu(y^{sdf} (x), \\sigma(x, y^{sdf}(x))^2),$    (4)\nwhere $y^{sdf} (x)$ is the solution of (2) at x. This result follows from the fact that by the Eikonal constraint, we have $d(x,y+\\sigma) = d(x, y) + \\sigma$. We label this probabilistic SDF-based regression approach with pSDF.\nOptimization objective: With these probabilistic models in place, we take as optimization objective the likelihood of the ground truth under the probabilistic models. For the SDF approach, we construct the ground-truth $d^{gt}(x, y)$ following the process detailed"}, {"title": "", "content": "in A.1. The negative log-likelihood (NLL) for it at a given pixel (x, y) is obtained by\n$ln p(d^{gt} (x, y) | I, \\theta) =  \\frac{1}{2} \\frac{(\\dit(x, y) \u2013 f_{sdf} [I; \\theta](x, y))^2}{f_{sdf} [I; \\theta]^2(x, y)} + In fodf [1;\\theta]2 (x, y) + ln(2\\pi)$   (5)\n,which under i.i.d. assumption leads to the overall loss (summed over all pixels):\n$L(\\theta) = \u2212 \\sum_{x,y} lnp(d^{gt} (x, y) | 1, \\theta).$   (6)\nThe NLL becomes a least squares loss if $\\sigma = 1$, which we use for REGR and SDF."}, {"title": "3. Experimental Setup", "content": "Datasets. We utilized the public dataset (internal) from , comprising 384 OCT volumes from AMD and control participants, with three manually annotated retinal layers: the inner limiting membrane (ILM), the retinal pigment epithelium (RPE), and Bruch's membrane (BM). OCT volumes were acquired using Bioptigen SD-OCT scanners, covering a 6.7 mm \u00d7 6.7 mm area centered on the fovea, producing volumetric scans of 1,000 \u00d7 512 \u00d7 100 dimensions. Additionally, we also utilize an external validation dataset (external), a data set of 458 B-scans from 159 unique participants from the Rotterdam Study. This set was acquired using a Topcon system, obtaining 512 \u00d7 885 pixels or 512 \u00d7 650 pixels and covering a 6x6mm area centered on the macula.\nTraining and Inference. The internal dataset was divided into distinct sets for training (179 AMD, 71 normal), validation (10 AMD, 10 normal), and testing (80 AMD, 34"}, {"title": "4. Result and Discussion", "content": "The segmentation performance of different methods are detailed in Table 1. Our method, both in its probabilistic (pSDF) and non-probabilistic (SDF) forms, significantly boosted the performance over REGR/pREGR method, as evidenced by the improvement of the averaged MAE approximately 2.38x for the internal and 2.4x for the external test set in the probabilistic version, and similar improvements in the non-probabilistic one. Moreover, the lower standard deviation suggests improved segmentation precision and consistency across B-scans. We also note that the introduction of uncertainty estimation leads to slightly better segmentation performance. This improvement can be attributed to how uncertainty prediction modulates the residual loss in Eq. 5. Effectively functioning as a data-adaptive regression modifier, it allows the network to adjust the impact of residuals (Kendall and Gal, 2017a). This is particularly beneficial for reducing the influence of erroneous labels, a common challenge in segmentation tasks, and, consequently, making the model more robust to noisy data."}, {"title": "4.1. Layer Segmentation", "content": "Fig. 10 of the appendix shows a comparison of our method with previous studies, surpassing their reported performance. It is important to note, however, that these studies employed different training and test data partitions, and direct comparisons are not possible. Additionally, we present a comparative analysis featuring a pixel-wise segmentation method, as detailed in appendix B.1, alongside our approach."}, {"title": "4.2. Uncertainty Estimation", "content": "The quantitative results of our experiment with artificially distorted B-scans are shown in Fig. 4(a). These findings highlight that our approach consistently registers high un-"}, {"title": "5. Conclusion", "content": "In this paper, we concentrate on advancing retinal layer segmentation in OCT B-scans by focusing on two main aspects: improved segmentation of thin layers and enhanced uncertainty quantification. We introduce an SDF-based approach to achieve better layer shape representation, addressing the shortcomings of previous approaches and leading to improved segmentation. This representation also inherently leads to a more accurate estimation of the uncertainty concerning the layer shape, offering a deeper understanding of layer integrity and visibility in pathological and noisy conditions."}, {"title": "Appendix A. Additional Materials", "content": ""}, {"title": "A.1. Constructing Target Space", "content": "The ground truth distance function, which serves as the target for the neural network (NN) training, is generated from the segmented layers within 2D B-scans provided with the internal dataset. These layers, delineated by lines that denote boundaries, are transformed into distance functions. Since calculating the signed distance function (SDF) given a segmentation mask is non-trivial , we use Danielsson's algorithm  to accomplish it. Note that by using Danielsson's algorithm for converting binary masks into signed distance maps, we ensure adherence to the Eikonal constraint,$\u2200x,y : ||\u2207d(x, y)|| = 1$, where d represents the distance function. This algorithm computes the Euclidean distance from each pixel to the nearest object boundary, satisfying the Eikonal equation by maintaining a gradient magnitude of one in proximity to boundaries. Hence, our model implicitly learns to uphold the constraint, as the training data generation process inherently respects this fundamental geometric condition. The process is defined as follows:\nIn the domain $\u03a9\u2282 R2$, the SDF $d^* : R2 \u2192 R$ assigns each pixel $p\u2208 R2$ the shortest distance to the boundary $\u2202\u03a9$. For a binary mask M indicating segmented layers by line pixels $\u2202\u03a9 \u2282 M$, the function $d^*$ assigns each $p\u2208 M$ to its nearest point in $\u2202\u03a9$. Formally, this is given by $d^*(p) = min_{q\u2208\u2202\u03a9} ||p \u2013 q||$, where || . || denotes the Euclidean norm. As the structures delineated by $\u2202\u03a9$ do not enclose any area, i.e., there are no inherent 'interior' or 'exterior' regions, leading to an absence of signed distances in the initial computation; all distances are non-negative. To incorporate the notion of sign, we introduce an orientation to the line $\u2202\u03a9$ by defining a directional vector $v$. This vector establishes a 'positive' direction above the line and a 'negative' direction below the line. The signed distance function $dit$ is then constructed by assigning a sign to the distances in $d^*$ based on the position of each pixel relative to $\u2202\u03a9$. This is expressed as:\n$d^{gt} (p) = \\begin{cases}d^*(p) & \\text{if} (p - q) \u00b7 v \u2265 0 \\text{ for the closest } q\u2208 \u03a9,\\\\\\-d^*(p) & \\text{otherwise.} \\end{cases}$   (7)\nIn this way, $dgt (p)$ represents the SDF, with the zero level set corresponding precisely to the line $\u2202\u03a9$, pixels above the line having positive values, and those below have negative values. The NN is thus trained to approximate this SDF, learning the spatial relations encoded within the signed distance function."}, {"title": "A.2. Soft Boundary Extraction", "content": "Now having the SDF d predicted by the network, we retrieve the retinal layer boundary coordinate by first applying a Gaussian mask to generate a weight mask W for the whole OCT B-scan. The mask is calculated using the formula:"}, {"title": "A.3. Deep Learning Model", "content": "For the REGR approach, we modify the ResUnet++ architecture to regress the retinal layer boundary coordinates, producing an output of 3 \u00d7 512 for 512 \u00d7 512 input B-scans as shown in fig. 5(a). This is primarily done by a non-expanding decoder for one of the spatial dimensions as explained by . We use a significantly smaller model of 70 million parameters, in contrast to the 187 million parameters in  yet experiments with their proposed architecture yield comparable results. In the SDF approach we adhere closely to the original ResUnet++ configuration except, for outputting three signed distance functions that mirror the input's spatial dimensions. For the uncertainty-aware variants, pREGR and pSDF, a new prediction head is integrated to predict the mean and variance for each column and pixel location, as shown in 5(b) & 5(d) respectively. Despite the architectural variations in REGR/pREGR and SDF/pSDF, we try to keep the models as close as possible to facilitate a fair comparison between methods."}, {"title": "A.4. Loss Function", "content": "In alignment with our methodology, we optimize the Negative Likelihood (NLL) across all models, both probabilistic and deterministic. For the REGR approach, the NLL manifests simply as mean squared error (MSE) under the assumption of a Gaussian distribution with fixed variance. For SDF however, we slightly deviate since empirical observations indicate that an L1 loss results in superior segmentation fidelity compared to an L2 loss. Additionally, to increase the concentration capacity of the network on details near the surface , we introduce a 'clamp' function, restricting the error within a specified range:\n$L_{SDF}(\u03b8) = \\sum_{n,x,y} |clamp(d^{gt}(x, y), \u03b4) \u2013 clamp(f_{sdf} [I_n; \u03b8](x, y), \u03b4)|,$   (10)"}, {"title": "A.5. Noisy Sample Generation", "content": "Here we explain in detail how we generate the noisy sample with different types of noise and what some analogous naturally occurring phenomena in OCT to these noises.\nShadow artifact. Shadowing can naturally occur due to blockages or opacities in ocular media, often caused by cataracts or hemorrhages, leading to darkened or obscured regions in the scans .\nTo add the shadow artifact within a specific B-scan, we adhere to the procedure outlined by . For any given B-scan, each constituent A-scan ax undergoes an individualized modification process. Here, x spans the totality of A-scans, collectively numbered as X. The transformation of each A-scan arx is governed by the shadow function S, defined as $S(a_x) = a_x \u00b7 (1 \u2212 s(x))$, where the term s(x) is derived from a Gaussian probability density function: $s(x) = \\frac{1}{\u03c3\\sqrt{2\u03c0}}e^{\\frac{-(x-\u03bc)^2}{2\u03c3^2}}$. Parameters \u03bc and \u03c3 are randomly determined within the bounds [0, X] and $\\frac{X}{3}$,$\\frac{2X}{3}$ respectively.\nBlinking artifact: Blinking artifacts are analogous to motion artifacts caused by patient movement or blinking during the scan, resulting in discontinuities or stripes in the B-scan.\nTo introduce artificial blinking artifacts into B-scans, we commence with a B-scan where all pixel intensities are set to zero. Subsequently, we superimpose additive Gaussian noise onto this B-scan. The mean of the Gaussian distribution was set to the median pixel value"}, {"title": "Appendix B. Additional Results", "content": ""}, {"title": "B.1. Pixel-wise Segmentation", "content": "A pixel-wise model has also been trained to evaluate performance against our proposed approach. We use the same architecture described for SDF approach and cross-entropy loss to train the model. However, this immediately leads to the problem shown in Fig. 6, as some of the pixels that belong to the layer boundaries are classified as background pixels. The problem highlights the fact that due to the thin layer structure, using a pixel-wise cross-entropy loss fails to provide a robust supervisory signal to train the model properly as discussed in the section 1. The problem of disconnected lines can partially be alleviated"}, {"title": "B.2. Uncertainty Comparison", "content": "We compare our uncertainty estimation with more widely used approaches such as Monte Carlo Dropout (MCDO) and Deep Ensemble . We again use the same experimental setup as our proposed approach. Addi-"}]}