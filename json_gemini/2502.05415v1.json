{"title": "Show-o Turbo: Towards Accelerated Unified Multimodal Understanding and Generation", "authors": ["Chenkai Xu", "Xu Wang", "Zhenyi Liao", "Yishun Li", "Tianqi Hou", "Zhijie Deng"], "abstract": "There has been increasing research interest in building unified multimodal understanding and generation models, among which Show-o stands as a notable representative, demonstrating great promise for both text-to-image and image-to-text generation. The inference of Show-o involves progressively denoising image tokens and autoregressively decoding text tokens, and hence, unfortunately, suffers from inefficiency issues from both sides. This paper introduces Show-o Turbo to bridge the gap. We first identify a unified denoising perspective for the generation of images and text in Show-o based on the parallel decoding of text tokens. We then propose to extend consistency distillation (CD), a qualified approach for shortening the denoising process of diffusion models, to the multimodal denoising trajectories of Show-o. We introduce a trajectory segmentation strategy and a curriculum learning procedure to improve the training convergence. Empirically, in text-to-image generation, Show-o Turbo displays a GenEval score of 0.625 at 4 sampling steps without using classifier-free guidance (CFG), outperforming that of the original Show-o with 8 steps and CFG; in image-to-text generation, Show-o Turbo exhibits a 1.5x speedup without significantly sacrificing performance.", "sections": [{"title": "1. Introduction", "content": "Multimodal large models like LLaVA [35] and Stable Diffusion [13] have shown promise across a variety of multimodal understanding (e.g., image/video to text) and generation (e.g., text to image/video) tasks. Recently, the attention has been shifted from dedicated multimodal models to a unified, versatile one, with Chameleon [55], Transfusion [72], Emu3 [57] and Show-o [61] as popular examples. Among them, Show-o is distinguished by its outstanding capabilities given the small model size (of only 1.3B parameters) as well as its open-source nature.\nIn short, Show-o integrates the discrete diffusion modeling of image tokens (yielded by an image tokenizer like MAGVIT-v2 [68]) and discrete autoregressive (AR) modeling of text tokens into one single transformer. The discrete diffusion can boil down to a masked autoregressive formula in practice. As a result, the inference of Show-o involves progressively denoising image tokens and autoregressively decoding the text tokens. This, undesirably, causes tens to hundreds of sampling steps for both image and text generation and hence a high serving cost. Although prior studies on accelerating diffusion models [46, 52] or large language models (LLMs) [25, 28, 43] can be separately applied to remediate such issues, the question remains whether a more unified approach exists to enhance the efficiency of Show-o.\nThis work introduces Show-o Turbo to reply to the question. We first advocate exploring parallel decoding algorithms of text tokens [23, 47, 51] to establish a unified view for the generation of both images and text. Basically, such algorithms invoke the language model to refine n (n > 1) text tokens in parallel and iterate until the fixed point. The sampling trajectory exhibits a gradual noise removal pattern (see Figure 2). This way, we can unify the generation of both text and images from a denoising view, and the learning of Show-o Turbo reduces to the shortening of the multimodal denoising trajectories. Drawing inspiration from diffusion acceleration literature [37, 46, 48, 52], we resort to the promising consistency distillation (CD) [52] technique to realize this. Concretely, we train Show-o Turbo to consistently map an arbitrary point on the sampling trajectory of Show-o to the same endpoint. Such an objective aids in pushing Show-o Turbo to generate meaningful content as fast as possible [39, 49, 52]. Conceptually, our approach forms an empirical generalization of CD, which was originally defined on ODE trajectories, to general (deterministic) discrete sampling trajectories, and stands as a cross-modal extension of consistency LLMS (CLLMs) [23].\nWe can simply initialize Show-o Turbo as the pre-trained Show-o and perform consistency distillation based on distri-"}, {"title": "2. Related Work", "content": "Multimodal Large Models. There has been much effort in exploring multimodal large models for image generation [42, 45, 54, 64] and understanding [3, 35, 65, 73]. For image generation tasks, text-conditioned Diffusion-based models [7, 8, 30, 42, 45, 50] gradually remove Gaussian noise in latent space [45] to generate images aligning with prompts. For multimodal understanding tasks, LLaVA family [26, 31, 34\u201336, 74] employs a vision encoder to encode the image, integrating it with a large language model (LLM) architecture to facilitate image-to-text understanding. Recent unified multimodal models [9, 12, 58, 59, 70] have emerged that aim to handle both image and text tasks simultaneously. For example, Chameleon [55] and Emu3 [57] autoregressively predict the next token on both tasks, while Transfusion [72] combines the autoregressive and continuous diffusion generation methods to handle different tasks. Similar to the Transfusion, Show-o [61] applies the autoregressive text generation but uses the discrete diffusion methods in image generation process.\nAcceleration of Diffusion Model. Diffusion models (DMs), such as Stable Diffusion [42, 45], inherently suffer from slow generation speeds due to iterative sampling. In this context, numerous acceleration techniques [5, 37, 44, 46, 48, 66] have emerged in recent years, with the most influential being the Consistency Model (CM) family [39, 52]. This family introduces the concept of consistency, mapping any two points on a trajectory to the same endpoint and supporting fast one-step generation. Subsequent works built on them introduce multi-step consistency [18, 56, 62, 71]. Their idea of segmentation trajectories reduces the learning difficulty and enhances the effect of consistency distillation. However, these methods focus on continuous diffusion models, which makes them inconsistent with the discrete diffusion process. More importantly, they do not integrate acceleration for both text and image generation.\nAcceleration of LLMs. The acceleration of LLMs [4, 10, 11, 24, 53] has been a popular research area, including de-"}, {"title": "3. Preliminary: Show-o", "content": "This section provides a brief overview of Show-o, a unified generative model for both images and text.\nImage Tokenization. Show-o opts to model the distribution of discrete image tokens via discrete diffusion [2]. To this end, it exploits MAGVIT-v2 [68] to convert high-dimensional continuous images into discrete token sequences and employs a unified large vocabulary to represent both text and image tokens. This enables using a single transformer to jointly characterize these two modalities.\nTraining Objectives. Show-o adopts an autoregressive (AR) modeling for text tokens following the principle of Next Token Prediction (NTP). For image modeling, the discrete diffusion paradigm can be equivalently simplified to a Mask Token Prediction (MTP) objective [6]. Formally, let $u := {u_1, u_2, ..., u_m}$ and $v := {v_1, v_2, ..., v_n}$ de-\nnote a sequence of m image tokens and a sequence of n text tokens, Show-o maximizes these objectives for training:\n$L_{NTP} := \\sum log p_\\theta (v_i|v_1, ..., v_{i-1}, u),$                                   (1)\n$L_{MTP} := \\sum log p_\\theta (u_j|u^*, u_2, ..., u^*, u_m, v),$                              (2)\nwhere $p_\\theta$ is the prediction distribution represented by Show-o, $u^*$ refers to the special mask token [MASK], and i and j traverse all text tokens and mask image tokens respectively.\nInference. The text generation of Show-o is based on the naive AR strategy. For image generation, Show-o adheres to the methodology outlined in MaskGIT [6]. This involves predefining a progressively decreased mask ratio schedule over K steps, initializing a sequence of full mask tokens $u^0$, and iteratively reducing the number of mask tokens according to the schedule. Specifically, letting $u^k$ denote the sequence containing partial mask tokens at k-th iteration, the model yields the prediction distribution for each mask token in $u^k$ and use a sample from the distribution to replace each mask token. After that, the model follows the mask schedule to replace the low-confidence predictions back as mask tokens, yielding $u^{k+1}$. The sampling trajectory ${u^0, u^1, ..., u^K}$ are illustrated in Figure 2. Besides, it is shown that Classifier-Free Guidance (CFG) [20] can be incorporated into Show-o to improve the sample quality [61]. However, CFG introduces an additional evaluation of the model, thereby increasing the sampling cost."}, {"title": "4. Method", "content": "This section introduces Show-o Turbo to reduce the sampling steps of Show-o for inference acceleration."}, {"title": "4.1. View Text Generation as Denoising", "content": "We first establish a unified perspective for the generation of both images and text in Show-o so that a concise and general acceleration strategy can apply. We notice the gap between the generation of images and text is mainly that the (mask) image tokens are decoded in parallel but text tokens emerge in an autoregressive manner. This motivates us to resort to fixed-point iteration algorithms that decode multiple text tokens in parallel [23, 47, 51] to bridge the gap.\nJacobi Decoding [47] is a representative fixed-point iteration algorithm for parallel text decoding. Given that, for text generation, Show-o equals a regular language model, we can directly apply Jacobi decoding to Show-o. Starting from a sequence of n randomly initialized text tokens, denoted as $v^0 := {v_1^0,...,v_n^0}$, Jacobi decoding iteratively refines the token sequence until a stable output (i.e. a fixed point). At k-th iteration, the refinement corresponds to simultaneously solving the following n problems:\n$v_1^{k+1} = arg \\max p_\\theta (v|v_1^k, u)$,\n$v_2^{k+1} = arg \\max p_\\theta (v|v_1^k, v_2^k, u)$,\n...\n$v_n^{k+1} = arg \\max p_\\theta (v|v_1^k, ..., v_{n-1}^k, u)$.                     (3)\nThey can be solved simultaneously with only one forward pass of Show-o using a casual attention mask, which takes roughly identical time as decoding one new token. Note that the greedy sampling strategy is used here. Abusing K to denote the number of iterations to reach the fixed point $v^K$, it is easy to see $K < n+1$ because there is at least one token being correctly predicted in each iteration.\nRefer to Figure 2 for a visualization of the sampling trajectory ${v^0,...,v^K}$, which displays a gradual noise removal pattern. Although empirical studies in Table 2 show that applying Jacobi decoding to Show-o cannot witness a considerable inference speedup (because Show-o is originally trained to predict the next token instead of decoding multiple tokens concurrently), Jacobi decoding offers us a unified denoising view of the generation of images and text."}, {"title": "4.2. Show-o Turbo", "content": "Given the above discussion, the problem of accelerating Show-o amounts to shortening the multimodal denoising trajectories. Drawing inspiration from the diffusion acceleration community [37, 46, 48, 52], we propose to adapt the qualified consistency distillation (CD) strategy [52] to Show-o. In particular, we aim at learning a Show-o Turbo model, denoted as $p_\\phi$, to consistently map any point on the trajectory of $p_\\theta$ to the same endpoint. Such an objective can drive Show-o Turbo to generate meaningful content as fast as possible [39, 49, 52]. In practice, we initialize $\\phi$ with the parameter $\\theta$ of the teacher Show-o. We elaborate on the algorithmic details in the following.\nConsistency Loss. We use the Jacobi iteration algorithm and the image sampling algorithm detailed in Section 3 to collect the trajectories of the original Show-o $p_\\theta$ on both text-to-image and image-to-text tasks. Then, the consistency loss on image trajectories takes the form of:\n$L_u = E_{k \\sim u(0,K)}d\\left(p_\\theta^*(u_k|u, v), p_\\phi (u, v)\\right),$             (4)\nwhere $p_\\theta^*$ denotes the frozen version of $\\theta$ and d indicates a divergence measure. d aggregates the disparity between categorical prediction distributions (e.g., measured by KL divergence) over the mask image tokens as in Equation 2. The consistency loss on text trajectories, denoted as $L_v$, can be similarly defined.\nRecall that $u^K$ refers to the endpoint of the trajectory, i.e., the final generation, so $L_u$ corresponds to a global consistency loss, which is empirically proven superior over the local one (operating on the adjacent points of the trajectory) for the acceleration of text generation [23]. Conceptually, our objective forms an empirical generalization of the original CD defined on ODE trajectories and a cross-modal extension of [23]. Besides, when collecting the trajectories, we disable the randomness in the sampling process of both modalities by applying a greedy strategy, which makes the discrete sampling trajectories deterministic and is likely to remediate training instability. Despite being trained with deterministic trajectories, Show-o Turbo is empirically evidenced to be compatible with the random sampling method for inference (see Table 4). We also clarify that the trajectories corresponding to the image tokens are collected with the involvement of CFG to guarantee the quality of $u^K$.\nRegularization. Training with only the consistency loss can drive $p_\\phi$ to a trivial convergence (e.g., always yielding the same outputs for arbitrary inputs). To avoid this, we introduce regularizations for both modalities. On the text side, we demand $p_\\phi$ to fit the endpoint text tokens $v^K$ with an objective similar to $L_{NTP}$, which ensures Show-o Turbo excels in image-to-text modeling. On the image side, we record the prediction distributions of the recovered image tokens at each sampling step during the trajectory collection procedure. The concentration of these distributions contains rich information about the generation process of the teacher Show-o, such as the easy-to-difficult hierarchy, so we advocate using them as another guidance for $p_\\phi(\\cdot|u^k, v)$.\nWe use $L_{REG}^u$ and $L_{REG}^v$ to represent these two regularizations respectively. We also include an AR loss $L_{AR}$ on pure text to maintain the language modeling capacity following the original Show-o. That said, the total loss is\n$L = L_u + L_v + \\beta L_{REG}^u + \\gamma L_{REG}^v + \\delta L_{AR},$           (5)\nwhere $\\alpha, \\beta, \\gamma$, and $\\delta$ are the trade-off coefficnets."}, {"title": "5. Experiments", "content": "In this section, we evaluate on multiple text-to-image (T2I) generation and multimodal understanding (MMU) tasks to inspect the efficacy of Show-o Turbo."}, {"title": "5.1. Implementation Details", "content": "Datasets. We leverage three types of data for the training of Show-o Turbo: the captions in the train split of COCO 2017 [32] and the LLaVA instruction tuning dataset [36] are used to generate the image and text trajectories respectively; the RefinedWeb text dataset [40] is used to maintain the language modeling ability.\nTraining Details. We separate the training process into two stages. For 256 resolution, in the first stage, we get image trajectories from the original Show-o with a CFG scale of 10 and K = 16. We split each trajectory into 4 segments to train the student model, denoted as Show-o Turbo*. In the second stage, we initialize the teacher and student model using Show-o Turbo*. We sample image trajectories with a CFG scale of 1.5, K = 8, and the number of segments as 2. The text trajectories are collected similarly. We employ Jacobi decoding to iteratively produce 16 tokens in each round to finally form lengthy text, which proves to yield good acceleration performance while preserving the generative modeling capabilities [23]. In terms of loss coefficients, we set $\\alpha$ = 10 according to the relative values of the losses, set $\\beta$ = 20 and $\\gamma$ = 100 according to the ablation study in Table 3, and set $\\delta$ = 2 following [61]. For 512 resolution,"}, {"title": "5.2. Main Results", "content": "Benchmarks. For T2I generation, we evaluate the generated images with Human Preference Score v2 (HPS) [60], ImageReward (IR) [63], and CLIP Score (CS) [19] metrics, based on test prompts in Human Preference Dataset v2 (HPD) [60]. Additionally, following Show-o, we evaluate also Show-o Turbo on GenEval [17]. For MMU, we evaluate Show-o Turbo on the description benchmarks Flickr30K [41, 67] and NoCaps [1] measured by the bleu4 score, and calculate the accuracy on question answering benchmarks POPE [27], MMEMME [14], and MMMU [69].\nBaselines. For T2I generation, we compare Show-o Turbo without CFG to Show-o with CFG across various sampling steps to fully demonstrate the effectiveness of our method. We consider a CFG scale of 5 and 10 for Show-o following [61]. For MMU, we compare Show-o Turbo with the original Show-o in terms of both inference speed and accuracy, where the speed is measured on a single RTX 4090 GPU.\nQuantitative Results. Table 1 displays the results for T2I generation. We observe that in 2-8 step sampling, Show-o Turbo comprehensively outperforms Show-o, even without"}, {"title": "5.3. Ablation Studies", "content": "To analyze the influence of each part in our method, we conduct a comprehensive ablation study on 256 resolution in this subsection. Unless otherwise specified, we report the results of the model after the first training stage (i.e., Show-o Turbo*) and the T2I generation is done with 4 sampling steps.\nNumber of Segments. As shown in Table 3, models trained in two segments and without trajectory segmentation (i.e.,"}, {"title": "6. Conclusion", "content": "This work introduces Show-o Turbo, a unified approach for accelerated multimodal understanding and generation. To achieve this, with a unified denoising perspective for the image and text generation process, we extend the consistency distillation to the multimodal denoising trajectories. Additionally, the trajectory segmentation strategy and curriculum learning are introduced to improve the model convergence and performance. Extensive experiments demonstrate a significant improvement in generation speed for both tasks, with a minimal decrease in performance."}, {"title": "C. Regularization Loss Details", "content": "The regularization loss for text trajectories is straightforward to compute because we only need $p_\\theta$ to fit the endpoint text tokens $v^K$, which is similar to the objective $L_{NTP}$. However, for the regularization loss of image trajectories, we need to construct regularized logits labels by recording the predicted distribution of teacher model at each sampling step during the trajectory collection process. As shown in Fig. 7, the regularization label at time step k is denoted as $P^k$, with the initialization $P^0 = 0$. During the iteration process of the trajectory $u^k$, we simultaneously iterate $P^k$. For the known regions in $u^k$, we directly assign the corresponding positions in $P^{k-1}$ to $P^k$. For the masked regions in $u^k$, we record the logits output of the teacher model and assign them to the corresponding positions. Through this iterative process, we can obtain the regularization labels $P^k_r$ at the segmentation points, which are used to compute $L^u_{REG}$."}, {"title": "D. Results of 256 resolution", "content": "Table 6 and Table 7 show the performance of Show-o Turbo on T2I and MMU tasks at 256-resolution respectively. It can be observed that Show-o Turbo can also achieve the effect of 8 steps of the original model in 4-step sampling without CFG in 256-resolution image generation, and also achieves about 1.5 times acceleration in 256-resolution image understanding."}, {"title": "E. Additional Image Results", "content": "Figure 8 and Figure 9 show the image generation results for 512 and 256 resolutions respectively. Show-o Turbo can generate high-quality images with rich details with only 2 to 4 sampling steps and without CFG."}]}