{"title": "Cross-Modal Bidirectional Interaction Model for Referring Remote Sensing Image Segmentation", "authors": ["Zhe Dong", "Yuzhe Sun", "Yanfeng Gu", "Tianzhu Liu"], "abstract": "Given a natural language expression and a remote sensing image, the goal of referring remote sensing image segmentation (RRSIS) is to generate a pixel-level mask of the target object identified by the referring expression. In contrast to natural scenarios, expressions in RRSIS often involve complex geospatial relationships, with target objects of interest that vary significantly in scale and lack visual saliency, thereby increasing the difficulty of achieving precise segmentation. To address the aforementioned challenges, a novel RRSIS framework is proposed, termed the cross-modal bidirectional interaction model (CroBIM). Specifically, a context-aware prompt modulation (CAPM) module is designed to integrate spatial positional relationships and task-specific knowledge into the linguistic features, thereby enhancing the ability to capture the target object. Additionally, a language-guided feature aggregation (LGFA) module is introduced to integrate linguistic information into multi-scale visual features, incorporating an attention deficit compensation mechanism to enhance feature aggregation. Finally, a mutual-interaction decoder (MID) is designed to enhance cross-modal feature alignment through cascaded bidirectional cross-attention, thereby enabling precise segmentation mask prediction. To further forster the research of RRSIS, we also construct RISBench, a new large-scale benchmark dataset comprising 52,472 image-language-label triplets. Extensive benchmarking on RISBench and two other prevalent datasets demonstrates the superior performance of the proposed CroBIM over existing state-of-the-art (SOTA) methods.", "sections": [{"title": "I. INTRODUCTION", "content": "Over the past few years, deep learning has emerged as the cornerstone for a diverse range of remote sensing applications. Early efforts in intelligent interpretation within the remote sensing domain largely centered on the extraction of visual features from imagery to perform various tasks such as semantic segmentation [1], object detection [2], and change detection [3]. Despite the significant progress made, these studies have predominantly concentrated on visual comprehension, frequently neglecting the critical aspect of modeling object relationships and achieving a more profound semantic understanding.\nRecently, large language models (LLMs) have achieved unprecedented advancements in language comprehension, driven by their extensive expert knowledge and sophisticated reasoning abilities. This progress has, in turn, catalyzed significant research into vision-language models (VLMs). The integration of natural language with remote sensing imagery has emerged as a prominent research focus, encompassing tasks such as image captioning [4], [5], image-text retrieval [6], [7], text-based remote sensing image generation [8], [9], and visual question answering [10], [11]. Despite these advancements, the task of referring remote sensing image segmentation (RRSIS) remains relatively unexplored.\nAs illustrated in Fig. 1, given remote sensing images and language expressions, RRSIS aims to provide pixel-level masks for specific regions or objects based on the content of the images and expressions. Its core principle is to achieve precise object localization and segmentation by matching textual descriptions with image content. RRSIS breaks the boundaries of traditional semantic understanding of remote sensing data, enabling non-expert users to retrieve objects in remote sensing images through human-computer interaction. It has broad application prospects in land use analysis [12], search and rescue operations [13], environmental monitoring [14], military intelligence generation [15], agricultural production [16], and urban planning [17].\nAlthough referring image segmentation in natural scenarios has made some progress, research on RRSIS is still in its infancy. Yuan et al. [18] first introduced the concept of the RRSIS task and proposed a language-guided cross-scale enhancement (LGCE) module based on the language-aware vision Transformer (LAVT) [19] to improve segmentation performance for small and sparsely distributed objects. Furthermore, rotated multi-scale interaction network (RMSIN) [20] is designed to address the prevalent challenges of complex scales and orientations in RRSIS. To manage cross-scale fine-grained information, the intra-scale interaction module (IIM) and cross-scale interaction module (CIM) are developed. Additionally, adaptive rotated convolution (ARC) is introduced to enhance the model's robustness to rotational variations. The aforementioned methods rely solely on jointly embedding linguistic features during visual encoding to perceive relevant linguistic context at each spatial location. Although these approaches have achieved satisfactory performance, the interrelation and alignment of visual and linguistic features across multiple levels of the encoding process have not yet been thoroughly explored.\nSpecifically, as shown in Fig. 2(a) and Fig. 2(b), failing to consider the underlying correlations between linguistic and visual information and merely fusing cross-modal heterogeneous features at different stages can lead to attention drift, resulting in a mismatch between visual features and the regions described in the query expression. Moreover, compared to natural scene images, the diversity of remote sensing data and the complex geospatial relationships embedded in the corresponding expressions present significant challenges for accurately locating and segmenting the target regions.\nIn this paper, we introduce a novel cross-modal bidirectional interaction model (CroBIM) for the RRSIS task, addressing the previously identified challenges. As depicted in Fig. 2(c), the essence of CroBIM lies in its capability to facilitate bidirectional interaction and correlation between visual and linguistic features throughout both the encoding and decoding phases. This enables precise visual-linguistic alignment during the prediction stage. Specifically, the context-aware prompt modulation (CAPM) module is introduced to enhance the text feature encoding process by incorporating multi-scale visual contextual information via learnable prompts. This integration enables the model to effectively perceive the spatial structure and relative positioning of target objects described in the referring expressions. Additionally, we propose a language-guided feature aggregation (LGFA) module that fosters interaction between multi-scale visual representations and linguistic features, thereby capturing cross-scale dependencies and addressing complex scale variations. To further enhance feature aggregation, the LGFA incorporates an attention deficit compensation mechanism. Finally, we design a mutual-interaction decoder (MID) to achieve precise vision-language alignment via cascaded bidirectional cross-attention, ultimately generating highly accurate segmentation masks.\nTo further advance research in RRSIS, we construct a new dataset called RISBench, with images sourced from the DOTA-v2 [21] and DIOR [22] remote sensing object detection datasets. RISBench consists of 52,472 image-language-label triplets. The language expressions provide not only basic category information but also details on color, shape, location, size, relative position, and relative size, with an average length of 14.31 words. Additionally, we employed a semi- automated approach to generate pixel-level mask annotations using bounding box prompts from the VRSBench dataset [23] and a customized segment anything model (SAM) [24]. Compared to the RefSegRS and RISBench datasets, our RISBench offers a greater number of triplets, a wider range of spatial resolutions, and a richer diversity of objects within each category.\nIn summary, the contributions of this work can be summarized in the following three aspects:\n(1) We present a novel framework for the RRSIS task, named CroBIM, designed to address the significant challenges posed by the diversity of remote sensing data and the complex geospatial relationships inherent in the corresponding expressions.\n(2) We design the CAPM module to integrate multi-scale visual contextual with linguistic features by introducing learnable prompts, enabling precise recognition of the spatial structure and positioning of target objects. Meanwhile, the LGFA module is proposed to facilitate interaction between visual and linguistic features across multiple scales, capturing cross-scale dependencies and improving feature aggregation through attention mechanisms. Besides, We introduce the MID to achieve precise alignment between vision and language modalities via cascaded bidirectional cross-attention, leading to accurate segmentation mask predictions.\n(3) To foster the research of RRSIS, we meticulously construct the largest benchmark dataset to date, named RISBench. RISBench consists of 52,472 high-quality image-language-label triplets, featuring diverse referring expressions and corresponding masks generated semi- automatically.\n(4) Existing referring image segmentation methods are extensively evaluated on three benchmark datasets. The experimental results robustly validate the effectiveness and generalization capabilities of our proposed approach, demonstrating its superior performance in comparison to state-of-the-art (SOTA) methods.\nThe remainder of this paper is structured as follows: Section II reviews related works on RRSIS. Section III details the construction process of our proposed RISBench dataset and provides an analysis of its key characteristics. In Section IV, we describe the proposed methodology in detail. Section V presents a comprehensive set of experiments and in-depth analyses. Finally, Section VII concludes the paper and offers insights into potential future research directions."}, {"title": "II. RELATED WORK", "content": "Compared to other multimodal tasks, referring image segmentation is more challenging as it requires effective coordination and reasoning between language and vision to accurately segment the target regions in an image. Multimodal fusion, diversity of expression, and robustness are three critical challenges that need to be addressed in the current state of referring image segmentation tasks [25].\nHu et al. [26] proposed an innovative approach for referring image segmentation by integrating the convolutional neural network (CNN) and long short-term memory network (LSTM) framework. This approach effectively extracts visual features from images and linguistic features from natural language expressions, enabling precise and accurate image segmentation. A recurrent refinement network (RRN) [27] was proposed to capture multi-scale semantics in image representations. The RRN iteratively optimized the initial mask using a recursive optimization module to achieve a high- quality pixel segmentation mask.\nHowever, the aforementioned methods only focus on a single modality of vector representation, neglecting the modality gap and not fully considering the complex interaction between language expressions and images. To address the aforementioned limitations, attention mechanisms have been introduced in recent works. A cross-modal self- attention (CMSA) module by Ye et al. [28] was proposed to effectively captures long-range dependencies between language and visual features. A cascade-grouped attention network (CGAN) [29] is designed, consisting of cascade- grouped attention (CGA) and instance-level attention loss (ILA). By performing hierarchical reasoning on images and effectively distinguishing different instances, CGAN enhances the correlation between text and images. Besides, Hu et al. [30] introduced a bidirectional relationship inferring network (BRINet) to model cross-modal information dependencies. BRINet utilized a visual-guided language attention module to filter out irrelevant regions and enhance semantic matching between target objects and expressions."}, {"title": "A. Visual Grounding for Remote Sensing Data", "content": "Similar to RRSIS, visual grounding for remote sensing data (RSVG) specifically entails using a remote sensing image alongside an associated query expression to determine the bounding box for a target object of interest. By localizing objects in remote sensing scenes through natural language guidance, RSVG provides object-level understanding and enhances accessibility. Compared to query expressions in natural images, expressions in RSVG frequently encompass complex geospatial relationships, and the objects of interest are often not visually prominent.\nGeoVG [31] is the first RVSA framework, which utilizes a language encoder to learn spatial relationships in geographic space, an image encoder to adaptively attend to remote sensing scenes, and a fusion module to integrate textual and visual features for visual localization. Zhan et al. [32] proposed a large-scale benchmark dataset DIOR- RSVG, and designed a Transformer-based multigranularity visual language fusion (MGVLF) module is proposed, which addresses the challenges of large-scale variations and cluttered backgrounds in remotely sensed images. By leveraging multiscale visual features and multigranularity textual embeddings, more discriminative representations are learned. Besides, language-guided progressive visual attention framework (LPVA) [33] utilized a progressive attention module to adjust visual features at multiple scales and levels, enabling the visual backbone to focus on expression-related features. Additionally, a multi-level feature enhancement decoder aggregated visual contextual information, enhancing feature distinctiveness and suppressing irrelevant regions."}, {"title": "C. Referring Remote Sensing Image Segmentation", "content": "Referring image segmentation in the context of remote sensing data has emerged as a novel area of investigation in recent times. Studies pertaining to this specific task are currently in an ascent stage and remain relatively scarce. Yuan et al. [18] first introduced the RRSIS task in the remote sensing domain. To facilitate research on RRSIS, they constructed a benchmark dataset RefSegRS by designing various referring expressions and automatically generating corresponding masks. Specifically, the RefSegRS dataset consists of 4,420 image-language-label triplets. Furthermore, to address the challenge of segmenting small and scattered objects in remote sensing images, they devised a language- guided cross-scale enhancement (LGCE) module based on the language-aware vision Transformer (LAVT) [19]. The LGCE module leveraged linguistic features as guidance to improve the segmentation of small objects by integrating deep and shallow features, thereby enhancing the complexity and diversity of the approach. In addition, to address the spatial variations and rotational diversity of targets in aerial images, the rotated multi-scale interaction network (RMSIN) [20] was proposed. RMSIN introduced the intra-scale interaction module (IIM) and cross-scale interaction module (CIM) within the LAVT framework, enabling the extraction of detailed features and facilitating comprehensive feature fusion. Moreover, to effectively handle the intricate rotational variations of objects, the decoder of RMSIN integrated the adaptive rotated convolution (ARC). This integration enhances the network's capability to capture and represent complex object rotations, thereby improving the overall performance on the RRIS task."}, {"title": "III. DATASET CONSTRUCTION AND ANALYSIS", "content": "In this section, we will introduce the construction procedure and statistical analysis of our proposed RISBench dataset in Section III-A and Section III-B."}, {"title": "IV. METHODOLOGY", "content": "The overall architecture of our proposed framework is shown in Fig. 5. Our CroBIM framework processes an input image $I \\in R^{H \\times W \\times 3}$ and a language expression $E = \\{e_i\\}, i \\in \\{0,..., N\\}$, where H and W denote the height and width of the input image, respectively, and N represents the length of the referring expression. For the image encoder, we employ one of Swin Transformer [35] and ConvNeXt [36], which extracts multi-scale visual features $\\{V_i\\}_{i=1}^{4} \\in R^{H_i \\times W_i \\times C_i}$ from the input image. Here, $(H_i, W_i) = (H/2^{i+1}, W/2^{i+1})$ and $C_i$ denote the spatial resolution and channel dimension of the i-th visual feature, respectively. Additionally, We employ BERT [37] as the text encoder to process the referring description by tokenizing and padding it, which generates text tokens $T \\in R^{l_m \\times D_l}$. These tokens are then input into the BERT encoder to derive linguistic features $L \\in R^{l_m \\times D_l}$. In this context, $l_m$ denotes the maximum token length, while $D_l$ represents the dimension of the linguistic features.\nThe details of each part will be introduced in the following sections."}, {"title": "A. Context-Aware Prompt Modulation", "content": "Prompt learning enhances model adaptability to specific tasks by introducing learnable parameters. However, free-form text prompts often lack sufficient contextual information, leading to suboptimal quality of learned representations. To address this issue, we propose a context-aware prompt modulation (CAPM) module, as illustrated in Fig. 6. The CAPM module integrates multi-scale visual contextual information during the text encoding process, aiding the model in better perceiving the spatial structure and relative positioning of target objects, thereby improving its ability to capture and identify these objects effectively.\nGiven the multi-scale visual features $\\{V_i\\}_{i=1}^{4}$ produced by the image encoder, we first apply adaptive average pooling to extract cross-scale contextual information. Subsequently, the pooled features from each scale are concatenated and flattened to form a multi-scale context embedding $V_e$:\n$V_e = Flatten \\bigg(Concat (\\{Pool_{s \\times s} (V_i)\\}_{i=1}^{4} \\bigg) \\bigg) \\in R^{4s^2 \\times C_{total}}$,\nwhere $Concat(.)$ denotes the channel concatenation operation, $Flatten(.)$ converts a multidimensional tensor into a one- dimensional vector, and $Pool_{s \\times s}$ represents adaptive average pooling with an output size of $s \\times s$. Here, $C_{total} = \\sum_{i=1}^{4} C_i$, and s is set to 1 in this work.\nFurthermore, learnable textual prompts $P \\in R^{N_p \\times D_l}$ are introduced as supplementary inputs to guide the model in incorporating domain-specific knowledge pertinent to RRSIS task into the learning process, where $N_p$ is set to 4. This enhancement aims to improve the model's capability to comprehend and generate responses relevant to the current task. To achieve image-to-text cross-modal interaction, we introduce cross attention mechanism to integrate multi-scale context into the learnable textual prompts P with $V_e$:\n$P_\\Omega = CrossAttn(V_e, P) = Softmax \\bigg(PW_q \\Omega (V_eW_k)^\\top \\bigg) V_eW_v,$\nwhere $W_q, W_k, W_v$ are the projection matrices, and $P_\\Omega$ represents the context-aware textual prompts.\nThe context-aware textual prompts $P_\\Omega$ are then concatenated with the text tokens T and jointly input into the BERT encoder to obtain linguistic features $L_v \\in R^{(l_m+N_p) \\times D_l}$ that incorporate visual context."}, {"title": "B. Language-Guided Feature Aggregation", "content": "To forge a robust visual-linguistic synergy and seamlessly integrate dependable linguistic features of referred objects into multi-scale visual representations, we introduce a sophisticated language-guided feature aggregation (LGFA) module. As shown in Fig. 7, the LGFA module adeptly captures and models the intricate interdependencies between visual and linguistic modalities.\nInitially, the visual feature $V_i \\in R^{H_i \\times W_i \\times C_i}$ undergoes processing through a projection function $W_{iq}$. Subsequently, it is spatially expanded, as delineated by:\n$V_{iq} = Flatten(w_{iq}(V_i)),$\nwhere $W_{iq}$ denotes the projection function for the visual feature and $V_{iq} \\in R^{l_m \\times (H_i \\times W_i)}$ represents the projected and spatially expanded visual feature.\nSubsequently, the linguistic feature $L \\in R^{l_m \\times D_l}$ undergoes transformation through projection functions $W_{ik}$ and $W_{iv}$:\n$L_{ik}, L_{iv} = W_{ik}(L), W_{iv}(L),$\nwhere $W_{ik}$ and $W_{iv}$ function as projection mechanisms for the linguistic features, with $L_{ik}$ and $L_{iv}$ representing the linguistic features utilized for computing the attention scores and generating the final attention output, respectively.\nFollowing this, we compute the attention score matrix $S_i$ between the visual and linguistic features:\n$S_i = V_{iq}^\\top L_{ik} \\in R^{(H_i \\times W_i) \\times D_l},$\nAfterward, the attention score matrix $S_i$ is normalized using the Softmax function, multiplied by $L_{iv}$, and finally, gated cross-modal activation is obtained by applying a Gate operation subsequent to the unflatten operation:\n$Att_i = Gate \\bigg(Unflatten \\bigg(Softmax \\bigg(\\frac{S_i}{\\sqrt{D_l}}\\bigg) L_{iv} \\bigg)\\bigg),$\nwhere $Gate(.)$ represents the application of a 1\u00d71 convolution followed by a GELU activation function, while $Unflatten(.)$ indicates the inverse operation of $Flatten(.).$\nFinally, the input visual feature $V_i$ is reweighted by integrating the attention weights $Att_i$, resulting in the integrated cross-modal feature map $F_i \\in R^{H_i \\times W_i \\times C_i}$:\n$V_{li} = Conv_{1x1}(Att_i) \\odot V_i,$\nwhere $\\odot$ denotes element-wise matrix multiplication, and $Conv_{1x1}$ represents the 1 \u00d7 1 convolution function.\nTo align the cross-modal correlations between adjacent stages and refine the aggregated multi-scale features, we resample the attention maps $\\{S_i\\}_{i=1}^{4}$ of different scales to a uniform size $(H_4, W_4)$:\n$S_i^* = I(S_i, (H_4, W_4)), i \\in \\{1, 2, 3, 4\\},$\nwhere $I$ denotes the resampling operation.\nSubsequently, we calculate the attention deficit map $M \\in R^{H_4 \\times W_4}$ between cross-scale attentions and select the top K regions with the highest correlation differences:\n$M = \\sum_{i=1}^{3} |S_i - S_{i+1}|,$\n$\\{r_k\\}_{k=1}^{K} \\leftarrow TopK (M, K),$\nFor each attention deficit region $r_k$, the multi-scale features $\\{V_{li}\\}_{i=1}^{4}$ corresponding to that region are projected to a uniform channel dimension $C_0$ and concatenated to yield the cross-scale feature representation $F_{r_k}$:\n$V_{r_k}^{'} = Concat \\bigg(Proj \\bigg(\\big\\{I[V_{l1}^*, V_{l2}^*, V_{l3}^*, V_{l4}^*] \\big\\} \\bigg)\\bigg),$\nwhere $Concat(.)$ denotes the concatenation operaion, and $Proj(.)$ indicates the channel projection layer.\nWe then proceed to characterize cross-scale dependencies with the following steps:\n$V_{r_k}^{''} = MSA (LN (V_{r_k}^{'}) + V_{r_k}^{'}),$\nwhere $LN(.)$ denotes the layer normalization operator, and $MSA(.)$ signifies the multi-head self-attention mechanism. Following this, the enhanced sequence is meticulously restructured back into its original patch configuration, precisely adhering to the initial order of concatenation, ensuring a seamless and coherent transformation:\n$[V_{11}^{'''}, V_{12}^{'''}, V_{13}^{'''}, V_{14}^{'''}] = I' \\bigg(Split \\bigg(Proj \\bigg(V_{r_k}^{''}\\bigg)\\bigg)\\bigg),$\nwhere $Proj(.)$ and $I'$ represents the inverse operation of $Proj(.)$ and $I$, and $Split(.)$ represents the channel separation operation."}, {"title": "C. Mutual-Interaction Decoder", "content": "Integrating the complementary information between cross-modal features is a fundamental challenge in RRSIS task. Cross-modal features often encompass inconsistent information, and without considering the intermediate interactions and thorough alignment between different modalities, it is impossible to ensure the discriminative power of the learned representations. To address the aforementioned challenges, we have meticulously designed the mutual- interaction decoder (MID), as illustrated in Fig. 8, aiming to achieve more comprehensive cross-modal alignment and precise pixel prediction.\nThe MID utilizes visual features $\\{V_{li}\\}_{i=1}^{4}$ and linguistic features $L_v$ as inputs to predict the mask of the referred object. Prior to executing cross-modal alignment, a series of operations are performed to harmonize the dimensions of the visual and linguistic features.\n$V_{ms} = Flatten \\bigg(Concat (Proj_v (\\{V_{li}\\}_{i=1}^{4} ))\\bigg) \\in R^{N \\times D},$\nwhere $Proj_v$ projects the multi-scale visual features into a hidden dimension $D = 256$, and $N = \\sum_{i=1}^{4} H_iW_i$.\nSpecifically, given the linguistic features $L_v$ and visual features $V_{ms}$, the language-to-vision interaction is facilitated through cross-attention, self-attention, and feed-forward networks (FFN). These mechanisms are employed to update the linguistic features $L_v$ using $V_{ms}$. Each layer of cross- attention, self-attention, and FFN is followed by a residual connection and layer normalization, ensuring a coherent and stable transformation of the features.\n$\\hat{L_v} = FFN(SelfAttn(CrossAttn(L_v, V_{ms}))),$\nSubsequently, the refined linguistic features $\\hat{L_v}$ are meticulously aligned with the visual features $V_{ms}$ on a pixel- by-pixel basis through a sophisticated vision-to-language interaction mechanism:\n$\\hat{V_{ms}} = FFN(MSDeformAttn(CrossAttn(V_{ms}, \\hat{L_v}))),$\nwhere $MSDeformAttn(.)$ denotes the multiscale deformable attention [38].\nUpon executing mutual interactions through bidirectional cross-modal feature alignment, we derive the harmonized linguistic and visual features, denoted as $L_v$ and $V_{ms}$. Finally, the enhanced visual features $\\hat{V_{ms}}$ are combined with the original visual features $V_{ms}$ and subsequently mapped through through a 1\u00d71 convolutional layer followed by spatial resampling to obtain the mask embedding $V_{out} \\in R^{H_1 \\times W_1 \\times D}$. $V_{out}$ is then element-wise multiplied with the [CLS] token $L_{out} \\in R^D$ of the linguistic features $\\hat{L_v}$, resulting in the final prediction mask $\\hat{Y} \\in R^{H_1 \\times W_1}$.\n$\\hat{Y}^{(i,j)} = V_{out}^{(i,j)} \\cdot L_{out},$\nwhere $(i, j)$ represents the pixel position in a two-dimensional space. $\\hat{Y}$ is then upsampled to the same spatial resolution $(H, W)$ as the input image via bilinear interpolation."}, {"title": "D. Training Objective", "content": "In the RRSIS task, object mask prediction is typically framed as a pixel-wise binary classification problem. Due to the significant class imbalance in remote sensing images, where target pixels are relatively scarce compared to background pixels, a conventional cross-entropy loss function may lead to a model that prioritizes learning from background pixels, adversely affecting the performance in detecting target regions. To address this issue, we employ a combined loss function consisting of cross-entropy loss and dice loss [39] as our training objective:\n$L = \\lambda \\cdot L_{cross-entropy} (Y_{up}, Y) + (1 - \\lambda) \\cdot L_{dice} (Y_{up}, Y)$\nwhere $\\lambda$ is a hyperparameter to balance two losses functions and is set to 0.9 in the paper, $Y_{up}, Y \\in R^{H \\times W}$ represent the upsampled prediction $\\hat{Y}$ and the ground truth, respectively."}, {"title": "V. EXPERIMENTS", "content": "In this section, we perform comprehensive experiments to assess the efficiency and effectiveness of our proposed RRSIS framework."}, {"title": "A. Dataset and Evaluation Metrics", "content": "We conduct experiments on three datasets, including two publicly available datasets and our constructed RISBench dataset. The detailed information for these three datasets is provided as follows:\nRefSegRS [18] comprises 4,420 image-language-label triplets across 285 scenes. The dataset is divided into a training set with 151 scenes and 2,172 referring expressions, a validation set with 31 scenes and 431 expressions, and a test set with 103 scenes and 1,817 expressions. The images are sized at 512\u00d7512 pixels, with a spatial resolution of 0.13 meters.\nRRSIS-D [20] contains a diverse dataset of 17,402 images, each paired with corresponding masks and referring expressions. The dataset is organized into three subsets: a training set with 12,181 image-language-label triplets, a validation set containing 1,740 triplets, and a test set with 3,481 triplets. All images are standardized to a resolution of 800\u00d7800 pixels. Additionally, the semantic annotations cover 20 categories and include 7 attributes, thereby enriching the semantic context of the referring expressions.\nRISBench includes a total of 52,472 image-language-label triplets. It is divided into two subsets: a training set consisting of 26,300 triplets, a validation set consisting of 10,013 triplets and a test set comprising 16,159 triplets. All images are uniformly formatted to a resolution of 512\u00d7512 pixels, with spatial resolutions ranging from 0.1 meters to 30 meters. The dataset's semantic labels are divided into 26 unique classes, with each class further annotated by 8 attributes.\nFollowing the prior study [40], overall Intersection- over-Union (oIoU), mean Intersection-over-Union (mIoU), and precision at different threshold values $X\\in \\{0.5,0.6,0.7,0.8,0.9\\}$ (Pr@X) are selected as evaluation metrics.\noIoU is computed as the ratio of the cumulative intersection area to the cumulative union area across all test samples, thereby giving greater emphasis to larger objects:\n$oIoU = \\frac{\\sum_t I_t}{\\sum_t U_t},$\nIn contrast, mIoU is calculated by averaging the IoU values between predicted masks and ground truth annotations for each test sample, treating small and large objects equally:\n$mIoU = \\frac{1}{M} \\sum \\frac{I_t}{U_t},$"}, {"title": "B. Experimental Setup", "content": "We employ Swin Transformer [35] and ConvNeXt [36] as the visual backbones in our approach. The Swin Transformer backbone is initialized with classification weights from the Swin-Base model pre-trained on ImageNet22K [52], while the ConvNeXt backbone utilizes pre-trained weights from ConvNeXt-Base, obtained through self-supervised learning using the SMLFR algorithm [53]. For the language backbone, we use the base BERT model with 12 layers and a hidden size of 768, as available in the HuggingFace library [54]. The maximum sequence length for text descriptions is set to 20 tokens.\nOur method is implemented using PyTorch framework, and we employ the AdamW optimizer [55] with a weight decay of 0.01 and an initial learning rate of 0.00005. The learning rate is decayed polynomially throughout training. We use a batch size of 32, and each model is trained for 40 epochs on eight NVIDIA A800 GPUs. During both training and testing phases, images are resized to 480\u00d7480 pixels. No data augmentation or post-processing techniques are applied."}, {"title": "C. Comparison with State-of-the-art Methods", "content": "1) RRSIS-D: To evaluate the effectiveness of our proposed method", "27": "CSMA [28", "41": "CMPC [42", "30": "CMPC+ [43", "44": "ETRIS [45", "46": "LGCE [18", "19": "RMSIN [20", "47": "RIS- DMMI [48", "49": "SLViT [56", "50": ".", "RefSegRS": "We further conduct experiments on the RefSegRS dataset to validate the superiority of our proposed framework", "metrics": "oIoU and mIoU. For oIoU, CroBIM achieves test scores of 72.33% (ConvNeXt-B) and 72.30% (Swin- B), ranking first and second respectively, significantly outperforming competing methods such as LAVT and RIS-DMMI. Similarly, in terms of mIoU, CroBIM achieves 59.77% (ConvNeXt-B) and 52.69% (Swin-B) on the test set, once again outperforming all other methods. These results highlight that CroBIM not only excels at lower overlap thresholds but also delivers significant improvements in overall segmentation"}]}