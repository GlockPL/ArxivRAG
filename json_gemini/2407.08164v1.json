{"title": "Hierarchical Consensus-Based Multi-Agent Reinforcement Learning for Multi-Robot Cooperation Tasks", "authors": ["Pu Feng", "Junkang Liang", "Size Wang", "Xin Yu", "Rongye Shi", "Wenjun Wu"], "abstract": "In multi-agent reinforcement learning (MARL), the Centralized Training with Decentralized Execution (CTDE) framework is pivotal but struggles due to a gap: global state guidance in training versus reliance on local observations in execution, lacking global signals. Inspired by human societal consensus mechanisms, we introduce the Hierarchical Consensus-based Multi-Agent Reinforcement Learning (HC-MARL) framework to address this limitation. HC-MARL employs contrastive learning to foster a global consensus among agents, enabling cooperative behavior without direct communication. This approach enables agents to form a global consensus from local observations, using it as an additional piece of information to guide collaborative actions during execution. To cater to the dynamic requirements of various tasks, consensus is divided into multiple layers, encompassing both short-term and long-term considerations. Short-term observations prompt the creation of an immediate, low-layer consensus, while long-term observations contribute to the formation of a strategic, high-layer consensus. This process is further refined through an adaptive attention mechanism that dynamically adjusts the influence of each consensus layer. This mechanism optimizes the balance between immediate reactions and strategic planning, tailoring it to the specific demands of the task at hand. Extensive experiments and real-world applications in multi-robot systems showcase our framework's superior performance, marking significant advancements over baselines.", "sections": [{"title": "I. INTRODUCTION", "content": "Multi-Agent Reinforcement Learning (MARL) is garner-ing increasing attention for its capability to tackle complex tasks [1]. Tasks involving distributed multi-robots [2] often require several agents to collaborate based on their local ob-servations to accomplish a given objective. This requirement aligns with the commonly adopted MARL framework of Centralized Training with Decentralized Execution (CTDE), as exemplified by methods such as MADDPG (Multi-Agent Deep Deterministic Policy Gradient) [3] and MAPPO (Multi-Agent Proximal Policy Optimization) [4]. These approaches utilize global information during the training phase through the critic, while the actor relies solely on individual obser-vations during execution. This setup leads to a significant challenge: agents lack a consensus during task execution, hindering their collective performance for cooperation [5].\nIn addressing this issue, three primary methodolo-gies have been advanced. The first involves strategies for communication-based multi-agent reinforcement learn-ing [6], which are faced with challenges in selective infor-"}, {"title": "II. RELATED WORK", "content": "Consensus means the interaction between groups of agents in a team to reach an agreement on a common value or state [13], [14]. Consensus in Multi-Agent Systems has garnered extensive research interest, primarily focusing on achieving shared agreement among agents. Research in this area spans three main categories: first, studies inspired by biological mechanisms and wildlife collective behaviors [15], [16]; second, theoretical explorations using models like the graph theory for foundational insights into consensus [17], [18]; and third, practical applications, including the devel-opment of consensus models and protocols [19], with a keen focus on convergence, equilibrium, and implementation challenges. Notably, Multi-Agent Reinforcement Learning"}, {"title": "A. Consensus in Multi-Agent System", "content": null}, {"title": "B. Contrastive Learning", "content": "Recent studies have increasingly leveraged self-supervised learning [22] to enhance model capabilities for downstream tasks. Among these, contrastive learning [23] stands out as a particularly tractable approach. It operates on the principle of minimizing the distance between augmented versions of the same sample while maximizing the distance between distinct ones, as noted by Wang [24]. This method effectively boosts the model's understanding of equivalent data representations. However, the collection of positive and negative samples in contrastive learning presents challenges that significantly impact its effectiveness. In reinforcement learning (RL) [25], samples are collected through ongoing interactions with the environment, highlighting the potential benefits of integrating contrastive learning with RL. Recent research efforts [26], have successfully employed contrastive learning for represen-tation learning prior to reinforcement learning, achieving un-paralleled data efficiency in pixel-based RL tasks. Moreover, contrastive learning has been utilized to formulate reward functions within RL systems [27]."}, {"title": "C. Contrastive Learning for MARL", "content": "Despite the progress in single-agent RL contexts, the exploration of contrastive learning in MARL remains com-paratively underdeveloped. Lin [28] proposed incorporating contrastive learning outcomes as a loss function in the MARL training process, applying it to multi-agent path planning tasks. Xu [29] introduced using contrastive learning to articulate the observational differences among agents. Further, Liu [30] promoted the development of a common language by maximizing the mutual information between the messages of given trajectories in a contrastive manner. Our HC-MARL method employs contrastive learning to establish a multi-layer global consensus. To the best of our knowledge, this is the first study utilizing contrastive learning to create a"}, {"title": "III. PRELIMINARIES", "content": "The multi-robot cooperation task can be formulated as a decentralized partially observable Markov decision process (Dec-POMDP) [31], defined as (I, S, A, T, R, O, Z, \u03b3). The index set (I = {1, .., N}) represents the set of agents. S is the global state space. Note that each agent is only capable of partial observation of the environment s \u2208 S, and the individual observation or \u2208 O comes from the local observation function oi = Zi(s) : S \u2192 O. Each agent i chooses its action according to its policy ai ~ \u03c0i(\u00b7 Oi). The joint action space A consists of the union of all agent's action space U1 Ai. We define the state transition function T : S \u00d7 A \u2192 S and the discount factor \u03b3\u2208 (0,1). All agents share the same joint reward function R(s, a). The agents aim to maximize the expected joint return, defined as \u0395\u03c0 [\u03a3\u03c4\u03bf \u03b3R(st, at)]."}, {"title": "A. Problem Formulation", "content": null}, {"title": "B. Centralized Training with Decentralized Execution (CTDE)", "content": "To address the problems under decentralized partially observable Markov decision processes (Dec-POMDPs), the Centralized Training with Decentralized Execution (CTDE) framework emerges as a critical approach. CTDE delineates a methodology where the training phase is centralized, allow-ing agents to access global information and learn coordinated strategies. In contrast, during execution, each agent operates independently based on its local observations, aligning with the decentralized nature of many real-world applications.\nAlthough value decomposition methods and policy-based multi-agent methods differ significantly in structure, they both adhere to the CTDE principles and encounter challenges in providing unified guidance during decentralized execution in fully cooperative tasks. To illustrate the optimization process within the CTDE paradigm, we focus on the most commonly used method, MAPPO. MAPPO is an actor-critic method based on the CTDE paradigm. Each agent learns a policy in an on-policy manner. MAPPO consists of a cen-tralized critic and several independent actors corresponding to each agent. During the centralized training phase, the critic utilizes global state information to estimate the joint action-value Q. The critic is trained by minimizing the Temporal Difference (TD) error as follows:\n$\\\\qquad \\\\mathbb{L}_{Critic}(\\phi) = \\mathbb{E}_{(s,a,r,s')}[ (Q_{\\phi}(s, a) - (r + \\gamma Q_{\\phi'}(s', a')))^2]$\nwhere Q\u03d5(s, a) represents the critic's current estimate of the joint action-value for the global state s and actions a, parameterized by \u03d5; r is the immediate reward received after taking action a in state s; s' is the next state, and a' is the action taken in the next state as per the current policy; \u03b3 is the discount factor; and \u03d5' refers to the parameters of the target critic used for bootstrapping."}, {"title": "C. Contrastive Learning", "content": "The Knowledge Distillation with No Labels (DINO) [32] method offers a solution as a form of self-supervised con-trastive learning that leverages a teacher-student network architecture. In this framework, for a given sample u, a new sample u' is generated through data augmentation. Both u and u' are then fed into the student and teacher networks, respectively, producing classification distributions Ps(u) and PT(u'). In the absence of true labels, the teacher network's output serves as pseudo-labels, and the student network aims to optimize by minimizing the cross-entropy loss between its output and these pseudo-labels.The cross-entropy loss used for optimization can be formalized as:\n$\\\\qquad L_{CL} = - \\sum_{c} P_T(u')_c \\log P_S(u)_c$\nwhere c indexes over the classes, PT(u') is the pseudo-label probability for class c produced by the teacher network for augmented sample u', and Ps(u) is the probability produced by the student network for the original sample u.\nThe teacher and student networks share the same archi-tecture, with the teacher's parameters being an exponential moving average (EMA) of the student's parameters. This arrangement facilitates a continuous refinement of the student network's learning through guidance from a slowly evolving version of itself, represented by the teacher network.\nIn multi-agent reinforcement learning scenarios, local ob-servations made by different agents can be considered as diverse augmented samples of the same global state. The global consensus, therefore, corresponds to the classification output from the teacher-student network framework. By con-structing this consensus metric, our aim is to guide agents, operating under local observations, towards forming global cooperation."}, {"title": "IV. METHODS", "content": "In this section, we introduce Hierarchical Consensus-based Multi-Agent Reinforcement Learning (HC-MARL), a novel framework that dynamically guides agents towards cooperative execution under partial observations through a hierarchical consensus mechanism."}, {"title": "A. Consensus Builder", "content": "In multi-agent reinforcement learning, agents execute ac-tions based on local observations, leading to a lack of global information guidance during execution. This section focuses on building an effective consensus within the Centralized Training with Decentralized Execution (CTDE) framework."}, {"title": "B. Hierarchical Consensus Mechanism", "content": "Through the consensus builder, we have obtained a global consensus among agents based on their local observations. However, as shown in Fig. 2, obtaining a complete and effective global consensus from a single moment's local ob-servations is challenging in practical applications. To address"}, {"title": "C. HC-MARL Framework", "content": "As illustrated in Fig. 4, the hierarchical consensus mecha-nism enables us to derive the attention-weighted consensus, catt. This consensus serves as the agent's inferred under-standing of the global state, derived from partial observa-"}, {"title": "V. EXPERIMENT AND RESULTS", "content": "We conducted both simulations and hardware experiments to validate the efficacy of our proposed HC-MARL."}, {"title": "A. Environment Settings", "content": "We constructed three cooperative multi-agent tasks within the Webots simulation [33], including Predator-Prey, Ren-dezvous [34], and Navigation, as shown in Fig. 8. We implemented our HC-MARL in these three tasks and com-pared it against two main-stream MARL baselines Multi-Agent Proximal Policy Optimization (MAPPO) [4] and its variant Heterogeneous-Agent Proximal Policy Optimization (HAPPO) [35]. Note that our HC-MARL framework can seamlessly integrate with various MARL algorithms. To ensure fair comparisons with these variants of MAPPO, we constructed our HC-MARL framework based on the MAPPO architecture in this experiment."}, {"title": "B. Main Results", "content": "This section describes and analyzes the experimental re-sults in three tasks. The performance of each algorithm was evaluated with five different random seeds. The learning curves, in terms of episode reward under varying numbers of agents, are presented in Fig. 5, 6, and 7. In addition to episode rewards, Table I compares the differences in algorithm performance through the number of steps required to complete the tasks after training. These results demon-strate that our work, HC-MARL, achieved varying degrees of advantage over all baseline algorithms."}, {"title": "Predator-Prey task.", "content": "In this scenario, predators must pursue and catch the prey through movement. The number"}, {"title": "Rendezvous task.", "content": "In the Rendezvous task, where no"}, {"title": "Navigation task.", "content": "In the navigation task, agents are asked to navigate through two obstacles and reach the target point while avoiding collisions with other agents and obstacles. Fig. 7 demonstrates that our HC-MARL method significantly improves episode rewards across tasks with varying numbers of agents. Specifically, HC-MARL's episode rewards are approximately 20% higher than those of HAPO and MAPPO in tasks with three agents, and about 35% higher in tasks with ten agents. Furthermore, Table I indicates that at ten agents, the improvement in the number of steps required to complete the obstacle navigation task is even more sub-stantial. HC-MARL requires only 700 steps to complete the task, representing a reduction of 30% and 40% compared to HAPO and MAPPO, respectively."}, {"title": "C. Ablation Study", "content": "In the HC-MARL framework, we employ a hierarchical consensus mechanism to derive short-term and long-term consensus. To assess the efficacy of both the consensus mechanism and the hierarchical approach, we conducted ablation studies varying the number of consensus categories and consensus layers.\nInitially, we investigated the effect of global consensus categories k on the Rendezvous task, testing k values of 1, 4, 8, and 16 across agent counts of 3, 5, and 10. Fig. 9a shows scenarios with k > 1 yield higher convergence rewards than those with k = 1, highlighting the consensus mechanism's benefit. Optimal rewards for 3 and 5 agents occurred at k = 4; for 10 agents, k = 8 was most effective. This indicates that simpler scenarios benefit from fewer categories, while more agents necessitate more categories for optimal training.\nAdditionally, the impact of consensus layers m on training rewards was examined for m = 1 (no hierarchy), 3, 5, and 10, depicted in Fig. 9b. Optimal rewards were achieved at m = 5, suggesting that increasing consensus layers up to a point enhances task performance. However, beyond this optimal level, training efficiency declined, due to increased training complexity and instability with additional layers."}, {"title": "D. Real World Experiments", "content": "We validated the real-world applicability of HC-MARL by conducting experiments on E-puck swarm. We utilized the NOKOV motion capture system for indoor positioning,"}, {"title": "VI. CONCLUSION", "content": "We introduced the Hierarchical Consensus-Based Multi-Agent Reinforcement Learning (HC-MARL) framework, a novel approach that employs hierarchical consensus to facil-itate cooperative execution among agents based on local ob-servations. Recognizing that each agent's local observations are subsets of a consistent global state, our framework lever-ages contrastive learning from these observations to achieve a global consensus, which then serves as additional local observations for the agents. By implementing a hierarchical mechanism, we construct short-term and long-term consen-sus to cater to the dynamic requirements of various tasks. This process is further refined through an adaptive attention"}, {"title": "VII. ACKNOWLEDGMENT", "content": "We gratefully acknowledge the support of the National Key R&D Program of China (2022ZD0116401) and the National Natural Science Foundation of China (Grant No. 62306023)."}]}