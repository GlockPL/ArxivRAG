{"title": "Hierarchical Consensus-Based Multi-Agent Reinforcement Learning for Multi-Robot Cooperation Tasks", "authors": ["Pu Feng", "Junkang Liang", "Size Wang", "Xin Yu", "Rongye Shi", "Wenjun Wu"], "abstract": "In multi-agent reinforcement learning (MARL), the Centralized Training with Decentralized Execution (CTDE) framework is pivotal but struggles due to a gap: global state guidance in training versus reliance on local observations in execution, lacking global signals. Inspired by human societal consensus mechanisms, we introduce the Hierarchical Consensus-based Multi-Agent Reinforcement Learning (HC-MARL) framework to address this limitation. HC-MARL employs contrastive learning to foster a global consensus among agents, enabling cooperative behavior without direct communication. This approach enables agents to form a global consensus from local observations, using it as an additional piece of information to guide collaborative actions during execution. To cater to the dynamic requirements of various tasks, consensus is divided into multiple layers, encompassing both short-term and long-term considerations. Short-term observations prompt the creation of an immediate, low-layer consensus, while long-term observations contribute to the formation of a strategic, high-layer consensus. This process is further refined through an adaptive attention mechanism that dynamically adjusts the influence of each consensus layer. This mechanism optimizes the balance between immediate reactions and strategic planning, tailoring it to the specific demands of the task at hand. Extensive experiments and real-world applications in multi-robot systems showcase our framework's superior performance, marking significant advancements over baselines.", "sections": [{"title": "I. INTRODUCTION", "content": "Multi-Agent Reinforcement Learning (MARL) is garner- ing increasing attention for its capability to tackle complex tasks [1]. Tasks involving distributed multi-robots [2] often require several agents to collaborate based on their local ob- servations to accomplish a given objective. This requirement aligns with the commonly adopted MARL framework of Centralized Training with Decentralized Execution (CTDE), as exemplified by methods such as MADDPG (Multi-Agent Deep Deterministic Policy Gradient) [3] and MAPPO (Multi-Agent Proximal Policy Optimization) [4]. These approaches utilize global information during the training phase through the critic, while the actor relies solely on individual obser- vations during execution. This setup leads to a significant challenge: agents lack a consensus during task execution, hindering their collective performance for cooperation [5]. In addressing this issue, three primary methodolo- gies have been advanced. The first involves strategies for communication-based multi-agent reinforcement learn- ing [6], which are faced with challenges in selective infor- mation sharing and increased bandwidth requirements. The second method leverages intrinsic rewards to create leader- follower dynamics [7], yet this approach is constrained by its task-specific effectiveness and encounters difficulties with broad applicability and generalization. The third approach, employing mean field theory [8], offers a promising direction but often struggles to effectively handle complex tasks. Given existing methods' limitations, sociology's research [9], [10] on consensus team agents interacting to align on shared values offers potential solutions for CTDE's challenges with partial observations. Inspired by consensus mechanisms in multi-agent sys- tems [11], we introduce the Hierarchical Consensus-based Multi-Agent Reinforcement Learning (HC-MARL) frame- work, designed to facilitate substantive multi-agent collabo- ration in settings characterized by local observations and the absence of direct communication. As shown in Fig. 1, despite differing local observations, agents all correspond to the same environmental state at each timestep, merely offering diverse perspectives of a unified global state. Adapting ideas from contrastive learning [12], we first map local obser- vations into discrete latent spaces as forms of invariances using the consensus builder. We define these invariances as global consensus. This global consensus is then treated as an additional piece of local observation information fed into the actor network. Notably, utilizing the consensus only requires an agent's local observations, aligning with the CTDE framework's prerequisite for partial observability. The initial findings, however, highlighted a limitation: relying exclusively on observations from a single timestep to establish a single-layer consensus falls short in fully captur-"}, {"title": "II. RELATED WORK", "content": "Consensus means the interaction between groups of agents in a team to reach an agreement on a common value or state [13], [14]. Consensus in Multi-Agent Systems has garnered extensive research interest, primarily focusing on achieving shared agreement among agents. Research in this area spans three main categories: first, studies inspired by biological mechanisms and wildlife collective behaviors [15], [16]; second, theoretical explorations using models like the graph theory for foundational insights into consensus [17], [18]; and third, practical applications, including the devel- opment of consensus models and protocols [19], with a keen focus on convergence, equilibrium, and implementation challenges. Notably, Multi-Agent Reinforcement Learning (MARL) [4], [20], [21] has become a focal point within the third category, emphasizing the utilization of consensus for enhancing agent cooperation in complex environments.\nRecent studies have increasingly leveraged self-supervised learning [22] to enhance model capabilities for downstream tasks. Among these, contrastive learning [23] stands out as a particularly tractable approach. It operates on the principle of minimizing the distance between augmented versions of the same sample while maximizing the distance between distinct ones, as noted by Wang [24]. This method effectively boosts the model's understanding of equivalent data representations. However, the collection of positive and negative samples in contrastive learning presents challenges that significantly impact its effectiveness. In reinforcement learning (RL) [25], samples are collected through ongoing interactions with the environment, highlighting the potential benefits of integrating contrastive learning with RL. Recent research efforts [26], have successfully employed contrastive learning for represen- tation learning prior to reinforcement learning, achieving un- paralleled data efficiency in pixel-based RL tasks. Moreover, contrastive learning has been utilized to formulate reward functions within RL systems [27].\nDespite the progress in single-agent RL contexts, the exploration of contrastive learning in MARL remains com- paratively underdeveloped. Lin [28] proposed incorporating contrastive learning outcomes as a loss function in the MARL training process, applying it to multi-agent path planning tasks. Xu [29] introduced using contrastive learning to articulate the observational differences among agents. Further, Liu [30] promoted the development of a common language by maximizing the mutual information between the messages of given trajectories in a contrastive manner. Our HC-MARL method employs contrastive learning to establish a multi-layer global consensus. To the best of our knowledge, this is the first study utilizing contrastive learning to create a"}, {"title": "III. PRELIMINARIES", "content": "The multi-robot cooperation task can be formulated as a decentralized partially observable Markov decision process (Dec-POMDP) [31], defined as (I, S, A, T, R, O, Z, \u03b3). The index set (I = {1, .., N}) represents the set of agents. S is the global state space. Note that each agent is only capable of partial observation of the environment s \u2208 S, and the individual observation o\u1d62 \u2208 O comes from the local observation function o\u1d62 = Z\u1d62(s) : S \u2192 O. Each agent i chooses its action according to its policy a\u1d62 ~ \u03c0\u1d62(\u00b7|O\u1d62). The joint action space A consists of the union of all agent's action space \u222a\u1d62A\u1d62. We define the state transition function T:S\u00d7A\u2192 S and the discount factor \u03b3\u2208 (0,1). All agents share the same joint reward function R(s, a). The agents aim to maximize the expected joint return, defined as E\u03c0 [\u2211\u03c4=0 \u03b3\u03c4R (s\u209c, a\u209c)]\nTo address the problems under decentralized partially observable Markov decision processes (Dec-POMDPs), the Centralized Training with Decentralized Execution (CTDE) framework emerges as a critical approach. CTDE delineates a methodology where the training phase is centralized, allow- ing agents to access global information and learn coordinated strategies. In contrast, during execution, each agent operates independently based on its local observations, aligning with the decentralized nature of many real-world applications. Although value decomposition methods and policy-based multi-agent methods differ significantly in structure, they both adhere to the CTDE principles and encounter challenges in providing unified guidance during decentralized execution in fully cooperative tasks. To illustrate the optimization process within the CTDE paradigm, we focus on the most commonly used method, MAPPO. MAPPO is an actor-critic method based on the CTDE paradigm. Each agent learns a policy in an on-policy manner. MAPPO consists of a cen- tralized critic and several independent actors corresponding to each agent. During the centralized training phase, the critic utilizes global state information to estimate the joint action-value Q. The critic is trained by minimizing the Temporal Difference (TD) error as follows:\n$L_{Critic}(\\phi) = E_{(s,a,r,s')}[(Q_{\\phi}(s, a) \u2013 (r + \\gamma Q_{\\phi'}(s', a')))^2]$ (1)\nwhere Q\u03c6(s, a) represents the critic's current estimate of the joint action-value for the global state s and actions a, parameterized by \u03c6; r is the immediate reward received after taking action a in state s; s' is the next state, and a' is the action taken in the next state as per the current policy; \u03b3 is the discount factor; and \u03c6' refers to the parameters of the target critic used for bootstrapping.\nFor the actors, which operate based on their local obser- vations during decentralized execution, the policy gradient is adjusted to reflect their dependence on local observations o. The policy gradient for optimizing each agent's policy \u03c0, considering local observations, is derived as follows:\n$\\nabla_{\\theta}J(\\pi) = E_{o,a~p_{\\pi}}[\\nabla_{\\theta} log \\pi(o, a|\\theta)Q_{\\phi}(s, a)]$ (2)\nThe Knowledge Distillation with No Labels (DINO) [32] method offers a solution as a form of self-supervised con- trastive learning that leverages a teacher-student network architecture. In this framework, for a given sample u, a new sample u' is generated through data augmentation. Both u and u' are then fed into the student and teacher networks, respectively, producing classification distributions Ps(u) and Pr(u'). In the absence of true labels, the teacher network's output serves as pseudo-labels, and the student network aims to optimize by minimizing the cross-entropy loss between its output and these pseudo-labels.The cross-entropy loss used for optimization can be formalized as:\n$L_{CL} = -\\sum_c P_T(u')_c log P_S(u)_c$ (3)\nwhere c indexes over the classes, PT(u')c is the pseudo- label probability for class c produced by the teacher network for augmented sample u', and PS(u)c is the probability produced by the student network for the original sample u. The teacher and student networks share the same archi- tecture, with the teacher's parameters being an exponential moving average (EMA) of the student's parameters. This arrangement facilitates a continuous refinement of the student network's learning through guidance from a slowly evolving version of itself, represented by the teacher network.\nIn multi-agent reinforcement learning scenarios, local ob- servations made by different agents can be considered as diverse augmented samples of the same global state. The global consensus, therefore, corresponds to the classification output from the teacher-student network framework. By con- structing this consensus metric, our aim is to guide agents, operating under local observations, towards forming global cooperation."}, {"title": "IV. METHODS", "content": "In this section, we introduce Hierarchical Consensus- based Multi-Agent Reinforcement Learning (HC-MARL), a novel framework that dynamically guides agents towards cooperative execution under partial observations through a hierarchical consensus mechanism.\nIn multi-agent reinforcement learning, agents execute ac- tions based on local observations, leading to a lack of global information guidance during execution. This section focuses on building an effective consensus within the Centralized Training with Decentralized Execution (CTDE) framework. As discussed in a previous section, the DINO framework pro- cesses a sample and its data-augmented equivalent through a teacher-student network. We treat an agent's observation o as the sample, and observations from other agents as the equivalent samples. In other words, the observation function Z is considered an augmentation operation, where the global observation s is augmented into each agent's observation o. These observations o correspond to the same global state s. Drawing inspiration from human patterns of situational awareness-where individuals often derive a broad under- standing of their environment from local cues, such as dis- cerning general cardinal directions in a city without knowing precise coordinates-we propose a model that leverages discrete categories for consensus in multi-agent systems. This approach simulates how agents might infer a macro classification of the current state from limited, local infor- mation. Specifically, we define consensus in terms of K distinct classes, enabling the consensus module to categorize an agent's local observations into a unified class k, which subsequently acts as the global consensus for guiding the agents' actions.\nFor each agent i in a set of n agents, the classification distribution resulting from local observations is denoted by Ps(oi) for the student network and Pr(oi) for the teacher network. The consensus among agents is evaluated by pair- wise comparison, optimizing the Consensus Builder through minimizing the sum of cross-entropy loss:\n$L_S(\\theta) = - \\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k} P_T(o_j)_k log P_S(o_i)_k$ (4)\nwhere i and j are the indices of the agents. PT(oj)k and PS(oi)k denote the probability of category k in the distribution output by the teacher and student networks, respectively. The consensus c for each agent is obtained as follows:\n$c_i = arg \\max_k P_S(o_i)_{ck}$ (5)\nThis procedure identifies the consensus class ci for agent i by selecting the class k with the maximum probability in the classification distribution Ps(oi), as generated by the student network for observation Oi.\nSuch a method underscores the agents' collaborative push towards a harmonized environmental perception, thereby enabling a collective consensus derived from individual observations. By employing a cross-entropy loss function, the framework promotes similarity in probability distributions for observations by different agents, even in disparate local contexts, thereby fostering a unified understanding of the global state.\nThrough the consensus builder, we have obtained a global consensus among agents based on their local observations. However, as shown in Fig. 2, obtaining a complete and effective global consensus from a single moment's local ob- servations is challenging in practical applications. To address this issue, this section introduces a hierarchical consensus mechanism, divided into short-term consensus and long- term consensus. Short-term consensus considers only the current timestep's state, while long-term consensus takes into account information across multiple timesteps, incorporating a longer-term utilization of historical state information. This is dynamically leveraged through an attention mechanism that weighs the importance of short-term and long-term consensus. For instance, in scenarios requiring immediate collision avoidance, agents prioritize short-term consensus. Conversely, in collaborative search tasks, agents rely more on long-term consensus to allocate search areas efficiently.\nWe expand the foundational consensus builder into a Hierarchical Consensus Mechanism, distinguishing between short-term and long-term consensus. Short-term consen- sus leverages observations or consensus from a single timestep. For long-term consensus, we introduce x\u1d62\u1d50 = {o\u1d62\u1d57\u00b9, o\u1d62\u1d57\u00b2, o\u1d62\u1d57\u00b3, ..., o\u1d62\u1d57\u1d50}, a set representing the agent i's ob- servations at various timesteps within the trajectory history, where m indicates the inclusion of m historical observations. It's crucial to note that t1, t2, t3, and so forth, denote distinct timesteps, which are not required to be consecutive. This approach proves especially beneficial in scenarios with brief training intervals, where successive states may exhibit minimal differences. By considering observations at spaced intervals, we capture more pronounced changes over time, thereby gaining insights into significant state transitions.\nAs we transition from utilizing single-timestep observa- tions to aggregating multi-timestep observations for con- trastive learning, the optimization criterion for the m-th layer student network is correspondingly revised. The updated loss function is defined as follows:\n$L^m_S(\\theta) = - \\sum_{i=1}^n\\sum_{j=1}^n\\sum_{k} P_T(x^m_j)_k log P_S(x^m_i)_k$ (6)\nTherefore, the consensus for the m-th layer, cm, is redefined"}, {"title": "C. HC-MARL Framework", "content": "As illustrated in Fig. 4, the hierarchical consensus mecha- nism enables us to derive the attention-weighted consensus, catt. This consensus serves as the agent's inferred under- standing of the global state, derived from partial observa- tions. We integrate this consensus as an augmented obser- vation input within the multi-agent reinforcement learning framework. It's pivotal to emphasize that while information from other agents is leveraged during the training phase of the student network, the action execution phase exclusively relies on an agent's local observations. This design principle ensures that our HC-MARL approach is compatible with a wide range of MARL algorithms, adhering to the Centralized Training with Decentralized Execution (CTDE) paradigm.\nTaking MAPPO as an example, by incorporating consen- sus information into the observations, the update for the critic network becomes:\n$L_{Critic}(\\phi) = E_{(s,a,r,s')}[(Q_{\\phi}(s, c^{att}, a) \u2013 (r + \\gamma Q_{\\phi'}(s', c^{att'}, a')))^2]$ (9)\nwhere catt and catt' represent the attention-weighted consensus information at the current and next time steps, respectively. The actor network update is formulated as follows:\n$\\nabla_{\\psi}J(\\pi) = E_{o,c^{att},a~p_{\\pi}}[\\nabla_\\theta log \\pi(o, c^{att}, a|\\psi)Q_{\\phi}(s, c^{att}, a)]$ (10)\nThese formulations illustrate how consensus information, specifically the attention-weighted consensus catt, is seam- lessly integrated into the MARL process, enhancing the learning mechanism by providing a more informed perspec- tive on the environment."}, {"title": "V. EXPERIMENT AND RESULTS", "content": "We conducted both simulations and hardware experiments to validate the efficacy of our proposed HC-MARL."}, {"title": "VI. CONCLUSION", "content": "We introduced the Hierarchical Consensus-Based Multi- Agent Reinforcement Learning (HC-MARL) framework, a novel approach that employs hierarchical consensus to facil- itate cooperative execution among agents based on local ob- servations. Recognizing that each agent's local observations are subsets of a consistent global state, our framework lever- ages contrastive learning from these observations to achieve a global consensus, which then serves as additional local observations for the agents. By implementing a hierarchical mechanism, we construct short-term and long-term consen- sus to cater to the dynamic requirements of various tasks. This process is further refined through an adaptive attention"}]}