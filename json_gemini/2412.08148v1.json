{"title": "A Review of Intelligent Device Fault Diagnosis\nTechnologies Based on Machine Vision", "authors": ["Guiran Liu", "Binrong Zhu"], "abstract": "This paper provides a comprehensive review of\nmechanical equipment fault diagnosis methods, focusing on the\nadvancements brought by Transformer-based models. It details\nthe structure, working principles, and benefits of Transformers,\nparticularly their self-attention mechanism and parallel\ncomputation capabilities, which have propelled their widespread\napplication in natural language processing and computer vision.\nThe discussion highlights key Transformer model variants, such\nas Vision Transformers (ViT) and their extensions, which\nleverage self-attention to improve accuracy and efficiency in\nvisual tasks. Furthermore, the paper examines the application of\nTransformer-based approaches in intelligent fault diagnosis for\nmechanical systems, showcasing their superior ability to extract\nand recognize patterns from complex sensor data for precise fault\nidentification. Despite these advancements, challenges remain,\nincluding the reliance on extensive labeled datasets, significant\ncomputational demands, and difficulties in deploying models on\nresource-limited devices. To address these limitations, the paper\nproposes future research directions, such as developing\nlightweight Transformer architectures, integrating multimodal\ndata sources, and enhancing adaptability to diverse operational\nconditions. These efforts aim to further expand the application of\nTransformer-based methods in mechanical fault diagnosis,\nmaking them more robust, efficient, and suitable for real-world\nindustrial environments.", "sections": [{"title": "I. Introduction", "content": "With the continuous advancement of modern technology,\nmechanical equipment used in industrial applications has\nbecome increasingly systematized, automated, and intelligent,\nwith more diverse and complex functional structures[1]. These\nmachines are widely used across industries such as aerospace,\ntransportation, power generation, automotive manufacturing,\nand mechanical processing, including applications like aircraft\nengines, wind turbines, industrial gearboxes, high-speed trains,\nand construction machinery[2]. As industrial production\ndemands higher speed, load capacity, and automation,\nequipment failures can lead to significant downtime, causing\nsubstantial economic losses and even potential casualties[3]. It\nis estimated that mechanical equipment failures contribute to\napproximately 38%[4] of major accidents and economic losses\nin industrial production. Therefore, real-time monitoring and\nfault diagnosis have become critical for ensuring smooth\noperation and preventing serious accidents[5]."}, {"title": "II. The Progress and Limitations of Existing Fault Diagnosis\nMethods for Mechanical Equipment", "content": "Mechanical equipment fault diagnosis involves analyzing\nperformance data to identify specific fault types, traditionally\nusing either physical model-based methods or AI-based\napproaches. Physical model methods focus on the evolution of\nfailure mechanisms like wear, cracks, and fatigue, but building\naccurate models requires expert knowledge and assumptions,\nmaking it challenging for complex systems. AI-based methods,\non the other hand, have gained popularity due to advances in\ncomputer technology and data science, offering simplicity,\nbroad applicability, and independence from detailed physical\nmodels. These Al models use intelligent algorithms to process\nsensor data, extracting features that accurately represent\nequipment conditions and identifying fault patterns. While\nearlier Al methods relied on traditional machine learning,\nmodern deep learning techniques have demonstrated superior\nabilities in feature extraction and intelligent decision-making,\nopening new possibilities for effective fault diagnosis. Signal\nacquisition and feature extraction are critical stages where\nunderstanding physical principles, phenomena, and models is\nessential. Physical principles like wave motion and resonance\nprovide the theoretical basis for signal analysis, while physical\nphenomena, such as changes in amplitude or frequency,\nindicate different fault types, offering strong diagnostic\nevidence. Moreover, physical models act as a bridge,\nabstracting real systems into simplified representations that\nsimulate operational states and fault processes, generating\nvaluable training data for Al models. By integrating both\napproaches, this review outlines the current progress and\nchallenges in mechanical fault diagnosis, pointing to future\nopportunities for more efficient and intelligent diagnostic\nsystems."}, {"title": "A. Signal acquisition", "content": "Currently, the common types of signals collected during\nthe monitoring of mechanical equipment include vibration\nsignals[11], sound signals, temperature signals, as well as data\nfrom oil analysis instruments and infrared imaging. Each of\nthese methods exhibits varying sensitivity and maintenance\ncosts in the context of mechanical fault diagnosis."}, {"title": "B. Feature extraction", "content": "Feature extraction is a crucial step in the fault diagnosis\nprocess, as it aids in identifying patterns and structures within\ndata, thereby providing better inputs for determining fault types.\nThe quality of feature extraction directly influences the\neffectiveness of fault diagnosis, leading to extensive research\nand practical exploration in this domain. Common feature\nextraction methods include time-domain features, which\ncapture the characteristics and statistics of signals over time,\nsuch as mean, peak value, root mean square (RMS)[12], and\nentropy. Frequency-domain features, on the other hand,\ncharacterize the properties of signals in the frequency domain,\noffering insights into frequency content and distribution\nthrough parameters like center frequency and frequency\nvariance. Additionally, time-frequency analysis methods, such\nas Short-Time Fourier Transform and Wavelet Transform, have\nbeen integrated into mechanical fault diagnosis to address the\nnon-smooth and nonlinear nature of vibration and acoustic\nsignals. These methods decompose signals into different\ncomponents, creating two-dimensional representations that\nfacilitate pattern recognition. Furthermore, advancements in\ncomputer vision have led to image-based feature extraction\ntechniques, including color, texture, shape, and deep\nlearning-based methods, which leverage neural networks to\nlearn high-level feature representations from images. Lastly,\ntext features extracted from textual data, such as text length,\nsyntactic structures, and sentiment analysis, further contribute\nto the comprehensive understanding of mechanical systems for\neffective fault diagnosis."}, {"title": "III. The application of transformers in mechanical equipment\nfault diagnosis", "content": null}, {"title": "A. The network architecture and principles of the\nTransformer", "content": "The Transformer model, designed for sequence-to-sequence\ntasks, leverages the self-attention mechanism as its foundation.\nBefore the advent of the Transformer, RNNs dominated as the\nmost commonly applied models in the field of NLP[13], with\ntheir structure shown in Figure 2."}, {"title": "Figure 2. The standard architecture of RNN.", "content": "The Transformer model was initially utilized in natural\nlanguage processing for machine translation tasks, achieving\nsignificant results. In recent years, it has been creatively\napplied in the computer vision domain, contributing to image\nenhancement, generation, classification, object detection, and\nsegmentation, thus creating new milestones in the field. The\nTransformer consists of three main components: the encoder,\ndecoder, and positional encoding. The encoder generates input\nencodings, while the decoder receives these encodings to\nmerge contextual information and produce the output sequence.\nEach module of the Transformer is described in detail[14]."}, {"title": "Figure 2: Original Structure of the Transformer", "content": "The Transformer employs an encoder-decoder model\narchitecture that eliminates recurrence, as illustrated in Figure 6.\nThe first component is the encoder, which consists of six\nidentical stacked encoder layers. Each encoder layer comprises\ntwo sub-layers: Multi-Head Self-Attention (MSA) and\nFeed-Forward Neural Network (FFN)[15]. The MSA\nmechanism enables the model to focus on different positions\nwithin the input sequence, capturing global contextual\ninformation. The FFN is used for applying non-linear\ntransformations to the features at each position. By stacking\nmultiple encoder layers, the encoder progressively extracts an\nabstract representation of the input sequence.\nFFN is a fully connected feedforward neural network added\nafter the self-attention layers in both the encoder and decoder.\nIt takes the output from the self-attention layer as input and\nproduces a new representation vector that encapsulates more\nadvanced semantic information. The computation process of\nFFN is as follows:\n$FFN(X) = W_2\\sigma(W_1 X)$                                                                                                 (1)\nIn the FFN, $W_1$ and $W_2$ represent the linear\ntransformation matrices of the first and second fully connected\nlayers, respectively, while $p$ denotes the nonlinear activation\nfunction. The dimension of the hidden layer is dh=2,048d_h =\n2,048dh =2,048.\nThe FFN utilizes a two-layer fully connected structure, with\na ReLU activation function applied between the layers.\nSpecifically, in each FFN, the input representation vector first\nundergoes a linear transformation through a fully connected\nlayer, then a nonlinear transformation via the ReLU activation\nfunction, and finally, another linear transformation through the\nsecond fully connected layer to produce the output. The\nadvantage of FFN lies in its ability to extract higher-level\nsemantic features from the input through multiple layers of\nnonlinear transformations, enhancing the model's expressive\npower. Additionally, since the computation in the FFN is\nindependent, it can be parallelized, thereby accelerating the\nmodel's training process.\nSince the Transformer does not include any recurrent or\nconvolutional structures to capture the positional information\nof words in a text, it is necessary to incorporate some relative\nor absolute position information of tokens in the sequence,\nallowing the model to capture the relationships between\nsequential data. To achieve this, position encodings are added\nat the bottom of both the encoder and decoder stacks[16]. Each\nword in the text is assigned a position number, which\ncorresponds to a word vector, and the position vector is\ncombined with the word vector, embedding the positional\ninformation into each word.\nCompared to the sequential input method of RNNs, the\nTransformer allows for parallel data input while maintaining\nthe positional relationships between data, which enhances\ncomputational speed and reduces storage requirements.\nAdditionally, the dimensions of the position encoding and the\ninput sequence embedding vectors are the same, allowing them\nto be added together. Currently, there are various methods for\nposition encoding, and the Transformer uses sinusoidal\nfunctions with different frequencies to encode position\ninformation, preserving the relative relationships between\npositions. The specific computation can be expressed as:\n$PE_{(pos,2i)} = sin(pos/10,000^{2i/d_m})$                                                                                                            (1)\n$PE_{(pos,2i+1)} = cos(pos / 10,000^{2i/d_m})$                                                                                                         (1)\nHere, pos represents the position of each word in the text,\niii denotes the dimension, and dm refers to the dimension of\nthe position encoding. The term 2i represents the even\ndimensions of the position encoding, while_2i+12i+12i+1\ncorresponds to the odd dimensions of the position encoding\n(where 2i\u2264d2, 2i+1\u2264d).\nAs can be seen, each dimension of the position encoding\ncorresponds to a sine wave with wavelengths ranging from 2\u03c0\nto 10,000-2 in a geometric progression.\nThe attention mechanism, inspired by neuroscience, allows\nmodels to dynamically allocate varying attention weights to\ndifferent parts of the input data, enabling selective processing\nof critical information while disregarding irrelevant details.\nThis capability enhances the model's ability to understand input\ndata and extract key features, making the mechanism"}, {"title": "B. Transformer-based image classification model", "content": "In the field of computer vision, the original Transformer\nmodel is not widely used, as it was primarily designed for\nsequential data processing tasks. However, ongoing research\nhas led to improvements that allow its application in image\nprocessing, yielding significant success. While Transformers\nexcel in handling sequential data like text, image data can also\nbe viewed as a two-dimensional sequence. This insight has\ninspired researchers to adapt Transformer models for image\ntasks, resulting in several high-performing visual Transformer\nmodels. Among these, image classification is a prominent\napplication, where the goal is to differentiate images based on\ntheir embedded meanings and contextual information, serving\nas a foundation for other image processing activities such as\nobject detection and image segmentation. To enhance the\nefficiency of visual Transformer models, researchers have\nintroduced various modifications to the original architecture.\nThis study focuses\nseveral high-performing visual\nTransformer models, particularly ViT and its variants,\nsummarizing their research advancements.\non"}, {"title": "C. Methods for Intelligent Fault Diagnosis of Mechanical\nEquipment Based on Transformers", "content": "The original Transformer model is primarily designed for\nnatural language processing and is not directly applicable to\nimage data. However, recent modifications have enabled its use\nin visual tasks like image recognition. Two main approaches\nfor applying Transformer-based methods in mechanical\nequipment fault diagnosis emerge from the literature. The first\ninvolves preprocessing one-dimensional fault signals, such as\nvibrations and sounds, to convert them into a format suitable\nfor Transformer input, allowing for effective feature extraction.\nThese signals can be analyzed through their characteristics like\nfrequency and amplitude. The second approach transforms\nthese one-dimensional signals into two-dimensional images\nusing time-frequency methods, which are then input into visual\nTransformer models like ViT or Swin Transformer for training\nand fault pattern recognition. Datasets used for validating these\nmethods include the Case Western Reserve University\n(CWRU)[20] bearing dataset and others, which contain various\nfault modes and lifecycle vibration data, with data collection\narrangements illustrated in relevant figures 6."}, {"title": "D. Overview of Current Research Status", "content": "Current research indicates that significant advancements\nhave been made in applying Transformer-based methods for\nintelligent fault diagnosis of mechanical equipment. The\narchitecture's self-attention mechanism, parallel processing\ncapabilities, and flexible structure have attracted considerable\ninterest. Since its introduction, the Transformer has gained\nrapid traction, achieving impressive results in natural language\nprocessing, speech recognition, and image processing.\nHowever, its application in mechanical fault diagnosis remains\nlimited, facing challenges such as complex architecture\nrequiring large datasets, which increases the risk of overfitting\nand reduces effectiveness in small datasets or specialized tasks.\nMost studies focus on stable operating conditions and analyze\nonly single fault modes, which is unrealistic given that\nmachinery often experiences complex faults from multiple\nsources. While Transformers are commonly used for rotating\nmachinery like bearings and rotor systems, research on\nreciprocating machinery, such as diesel engines, is sparse.\nDespite these challenges, Transformer-based methods have\ndemonstrated remarkable effectiveness and accuracy in fault\ndiagnosis, revealing strong correlations between specific\nvibration signals and mechanical faults. These findings not\nonly offer new diagnostic indicators but also enhance\nunderstanding of the operational mechanisms and failure\nprocesses in mechanical systems."}, {"title": "IV. Research prospect", "content": "With advancements in fault diagnosis methods and\ntechnologies for machinery, Transformer-based intelligent fault\ndetection has become a rising and significant area of research.\nIn recent years, Transformers have gained widespread\napplication across various artificial intelligence domains, such\nas computer vision (CV), natural language processing (NLP),\nand multimodal analysis, establishing themselves as a leading\nmethodology. As deep learning (DL) technologies continue to\nevolve, the application of Transformer-based diagnostic\napproaches is anticipated to expand further. This discussion\nhighlights current challenges and outlines the future\ndevelopment directions for Transformer-based methods in\nmachinery fault diagnosis."}, {"title": "Enhanced Adaptability to Diverse Data", "content": "Currently, the application of Transformer-based diagnostic\ntechniques predominantly focuses on rotating machinery,\nincluding rolling bearings and rotor systems, using data such as\nvibration and sound signals. However, with the emergence of\nthe Industrial Internet, operational data types like temperature,\npressure, and flow rates are expected to be integrated into\nmachinery fault diagnosis. This integration will expand the\napplicability of these techniques, enabling their use in\ndiagnosing reciprocating machinery such as engines. The\ninclusion of diverse data sources and types will also enhance\nthe reliability and accuracy of diagnostic outcomes."}, {"title": "Addressing Data Scarcity with Few-Shot Learning", "content": "Diagnostic models built on Transformers often rely on\nextensive datasets for training, followed by fine-tuning to"}, {"title": "achieve task-specific accuracy and generalizability. However,\nin many fault diagnosis scenarios, gathering large volumes of\nlabeled data is challenging due to the rarity and randomness of\nmachinery faults. Few-shot learning presents a potential\nsolution by utilizing algorithms and model enhancements to\nextrapolate knowledge from a limited number of samples,\nenabling predictions on unseen data. This approach effectively\naddresses the Transformer's limitations in scenarios where\nsmall, specialized datasets are available.", "content": null}, {"title": "Development of Lightweight Transformers", "content": "Although Transformers deliver exceptional performance in\nfault diagnosis tasks, their high computational complexity and\nsubstantial parameter requirements can hinder deployment in\nreal-world applications. Specifically, in resource-constrained\nenvironments such as embedded systems or mobile devices, the\nneed for extensive computational resources and storage can\npose significant challenges. Research efforts focusing on\nlightweight Transformers aim to simplify models through\npruning, compression, and optimization techniques. These\nimprovements reduce memory requirements, accelerate\ninference speeds, and enhance performance in scenarios\ndemanding real-time diagnostics, such as speech processing or\nimage recognition in mechanical systems."}, {"title": "Synergy Between Transformers and CNNs", "content": "Transformers are particularly effective in processing\nlong-sequence data and capturing global dependencies but may\nunderperform with short-sequence data compared to CNNs.\nConversely, CNNs excel at extracting local features,\nparticularly for image and text data, but lack the ability to\neffectively capture global patterns. Combining the strengths of\nTransformers and CNNs allows for models that can\nsimultaneously learn global and local features. This integration\nmaximizes representational and generalization capabilities,\ncreating robust diagnostic systems adaptable to a variety of\ntasks and datasets.\nBy addressing these future directions, Transformer-based\napproaches for machinery fault diagnosis can become more\nversatile, efficient, and robust, enabling broader and more\nimpactful applications in industrial settings."}, {"title": "V. Conclusion", "content": "This study underscores the significant potential of\nTransformer-based models for intelligent fault diagnosis in\nmechanical systems. These models have demonstrated\nexceptional performance in capturing complex fault patterns\nand achieving high diagnostic accuracy, particularly in\ncontrolled scenarios. However, several challenges persist,\nincluding the high computational complexity of Transformer\narchitectures and their reliance on extensive labeled datasets for\ntraining. Additionally, current research predominantly\nemphasizes stable operational conditions and isolated fault\nmodes, which may not adequately reflect the diverse and\nmultifaceted nature of real-world machinery faults.\nMoreover,\nthere remains a gap in applying\nTransformer-based methods to diverse industrial contexts,\nparticularly in reciprocating machinery like diesel engines and\nother equipment beyond rotating machinery. The findings of\nthis study highlight the ability of Transformers to leverage\nfrequency-domain features from vibration signals, indicating\ntheir strong capability to extract and learn fault-relevant\npatterns. However, their practical implementation in\nresource-constrained or highly dynamic industrial settings\nremains limited.\nTo address these gaps, future research should focus on\ndeveloping lightweight and adaptable Transformer\narchitectures, integrating diverse operational data types, and\nenhancing the models' robustness under varying and\nunpredictable conditions. Furthermore, exploring hybrid\napproaches, such as combining Transformers with other deep\nlearning techniques like CNNs, can balance computational\nefficiency and diagnostic precision. By overcoming these\nchallenges, Transformer-based methods can evolve into\nversatile and scalable solutions, expanding their applicability in\nindustrial fault diagnosis and contributing significantly to\npredictive maintenance and operational efficiency."}]}