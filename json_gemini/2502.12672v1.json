{"title": "Speech-FT: A Fine-tuning Strategy for Enhancing Speech Representation\nModels Without Compromising Generalization Ability", "authors": ["Tzu-Quan Lin", "Wei-Ping Huang", "Hao Tang", "Hung-yi Lee"], "abstract": "Speech representation models are highly effec-\ntive at extracting general features for various\ntasks. While fine-tuning can enhance these rep-\nresentations for specific applications, it often\ncompromises their generalization ability. To\naddress this challenge, we propose Speech-FT, \na fine-tuning strategy for speech representation\nmodels that leverages model merging to pre-\nserve generalization ability while still benefit-\ning from fine-tuning. Speech-FT is effective\nacross different fine-tuning scenarios and is\ncompatible with various types of speech repre-\nsentation models, providing a versatile solution.\nSpeech-FT offers an efficient and practical ap-\nproach to further improving general speech rep-\nresentations after pre-training.", "sections": [{"title": "1 Introduction", "content": "In recent years, speech representation models\n(Chung et al., 2019; Baevski et al., 2020; Ling and\nLiu, 2020; Hsu et al., 2021; Chen et al., 2022) have\nemerged as powerful tools for extracting general\nspeech representations, benefiting a wide range of\ndownstream tasks (Yang et al., 2021; Tsai et al.,\n2022; Feng et al., 2023).\nTo further improve these representations, prior\nresearch has explored fine-tuning speech repre-\nsentation models in various settings. Some stud-\nies have focused on supervised fine-tuning, where\nmodels learn enhanced representations with the aid\nof labeled data (Chen et al., 2021). Others have\nadopted unsupervised fine-tuning by continuing the\npre-training process on new languages (Getman\net al., 2024).\nHowever, as shown in previous work (Chen et al.,\n2021), supervised fine-tuning can make representa-\ntions overly specialized for the target task, reducing\ntheir generalization ability. Meanwhile, as we will\ndemonstrate, continuously pre-training on new lan-\nguages may cause the model to forget previously\nlearned languages.\nOne possible reason fine-tuning degrades the\ngeneralization ability of representations is the sig-\nnificant distortion of pre-trained features (Kumar\net al., 2022). Minimizing weight modifications\nduring fine-tuning may help mitigate this issue. Al-\nthough adding a regularization term can effectively\nconstrain weight modifications, it also limits the\nmodel's ability to adapt to the fine-tuning task, lead-\ning to suboptimal performance.\nTo address this issue, we propose Speech-FT, a\nfine-tuning strategy based on model merging that\nretains the benefits of fine-tuning while preserv-\ning generalization ability. Specifically, we apply\nlinear interpolation between the pre-trained and\nfine-tuned models to reduce weight modifications.\nBeyond minimizing weight changes, model merg-\ning has also been shown to effectively integrate\nknowledge from different models (Ilharco et al.,\n2022), making it a practical strategy for preserving\nthe generalization ability of the pre-trained model\nwhile incorporating task-specific knowledge from\nfine-tuning.\nOur experiments show that Speech-FT is effec-\ntive across different fine-tuning scenarios, includ-\ning supervised, unsupervised, and multitask fine-\ntuning. Not only does it preserve generalization\nability, but it also enhances speech representations\nbased on the fine-tuning tasks, making it a valuable\napproach for refining speech representations after\npre-training. Moreover, Speech-FT introduces no\nadditional computational cost compared to stan-\ndard fine-tuning and is compatible with a broad\nrange of speech representation models.\nThe contributions of this work are summarized\nas follows:\n\u2022 We propose Speech-FT, a novel fine-tuning\napproach for enhancing speech representa-\ntion model while preserving the generalization\nability.\n\u2022 Speech-FT offers an efficient and effective"}, {"title": "2 Methodology", "content": "In this work, we investigate how fine-tuning can\nenhance pre-trained speech representation mod-\nels, denoted as \\theta_0, whose representations are used\nfor a set of evaluation downstream tasks T =\n{t_1, t_2,...,t_n}. Given a fine-tuning task t, our\nobjective is to design an algorithm C such that\nthe fine-tuned model, defined as \\theta = C(\\theta_0, t), im-\nproves performance across the evaluation task set\nT.\nStable Fine-tuning. Fine-tuning speech represen-\ntation models is challenging, as pre-trained fea-\ntures can undergo significant distortion due to mis-\nmatches between pre-training and fine-tuning tasks.\nTo address this issue, we propose a stable fine-\ntuning process that helps minimize such distortions.\nWhen fine-tuning with a randomly initialized\ntask prediction model D, the pre-trained features\nmay be severely altered as the model adapts to the\nfine-tuning task (Kumar et al., 2022). This issue\ncan be mitigated by first training a well-initialized\ntask prediction model D. Therefore, we begin fine-\ntuning by updating only the task prediction model\nD during the initial 8% of fine-tuning steps.\nWe further stabilize fine-tuning by freezing the\ndownsampling module. Modern speech representa-\ntion models often include a downsampling module\nat the beginning, typically a multi-layer convolu-\ntional encoder that processes raw audio and gen-\nerates downsampled features. Prior studies have\nshown that this module captures universal low-level\nfeatures, such as frequency patterns (Lin et al.,\n2023), which are generally crucial for various appli-\ncations. To preserve these robust low-level features,\nwe freeze the downsampling module throughout\nthe entire fine-tuning process.\nWeight Space Interpolation. While fine-tuning\nimproves task-specific representations, it often\nleads to substantial weight modifications, which\nmay compromise the generalization ability of the\npre-trained model.\nA straightforward approach to mitigating this\nissue is to add a regularization term during fine-\ntuning to keep the fine-tuned model closer to the\npre-trained model in weight space. However, as we\nwill show in Section 5, this approach restricts the\neffectiveness of fine-tuning, ultimately leading to\nsuboptimal performance.\nInstead, we adopt model merging as an alterna-\ntive, combining the pre-trained and fine-tuned mod-\nels using linear interpolation. Model merging has\nbeen demonstrated to effectively integrate knowl-\nedge from different models across various settings\n(Ilharco et al., 2022; Wortsman et al., 2022b; Lin\net al., 2024c,b; Kulshreshtha et al., 2024).\nSince the pre-trained model inherently possesses\nstrong generalization ability, interpolating between\nthe pre-trained and fine-tuned models offers a"}, {"title": "2.1 Problem Formulation", "content": "In this work, we investigate how fine-tuning can\nenhance pre-trained speech representation mod-\nels, denoted as \\theta_0, whose representations are used\nfor a set of evaluation downstream tasks T =\n{t_1, t_2,...,t_n}. Given a fine-tuning task t, our\nobjective is to design an algorithm C such that\nthe fine-tuned model, defined as \\theta = C(\\theta_0, t), im-\nproves performance across the evaluation task set\nT."}, {"title": "2.2 Speech-FT", "content": "Speech-FT consists of two main components: sta-\nble fine-tuning and weight space interpolation. See"}, {"title": "2.3 Extending to Multiple Fine-tuning Tasks", "content": "Given multiple fine-tuning tasks {t_1, t_2,..., t_k},\nour goal is to design an algorithm C such that \\theta = C(\\theta_0, t_1, t_2,..., t_k) leads to improved representa-\ntions. By leveraging multiple tasks t_1, t_2, ..., t_k,\nwe aim to incorporate diverse knowledge during\nfine-tuning, further enhancing speech representa-\ntions.\nWe explore four different strategies for integrat-\ning multiple fine-tuning tasks:\nMultitask Fine-tuning (MTF). We stably fine-\ntune the pre-trained model using multiple tasks si-\nmultaneously, where separate task prediction mod-\nels are appended to the pre-trained model. The\nresulting multitask model \\theta' is used in Speech-FT\nas follows:\n\\theta = (1 - \\alpha) \\cdot \\theta_0 + \\alpha \\cdot \\theta'\nLinear Merge. Denote the stably fine-tuned mod-\nels for multiple tasks as \\theta_1, \\theta_2, ..., \\theta_k. The merged\nmodel in Speech-FT is computed as:\n\\theta = (1 - a) \\cdot \\theta_0 + \\alpha\\cdot (\\frac{1}{k} \\sum_{i=1}^{k}\\theta_i)\nTIES Merge. We apply TIES merging (Yadav\net al., 2024), a more sophisticated merging tech-\nnique compared to linear merging. The resulting\nmodel in Speech-FT is given by:\n\\theta = (1 - a) \\cdot \\theta_0 + \\alpha \\cdot TIES(\\theta_1, \\theta_2, . . .,\\theta_k)\nSequential Fine-tuning. We apply Speech-FT se-\nquentially to each fine-tuning task t_1, t_2, ..., t_k."}, {"title": "3 Experiments", "content": "All experiments are conducted on HuBERT (Hsu\net al., 2021), unless otherwise specified. We eval-\nuate the generality of our method on wav2vec 2.0"}, {"title": "3.1 Implementation", "content": "All experiments are conducted on HuBERT (Hsu\net al., 2021), unless otherwise specified. We eval-\nuate the generality of our method on wav2vec 2.0\n(Baevski et al., 2020) and DeCoAR 2.0 (Ling and\nLiu, 2020) in Section 5. Instead of pre-training\nthese models from scratch, we use publicly avail-\nable pre-trained models123 as the starting point for\nfine-tuning. HuBERT, wav2vec 2.0, and DeCoAR\n2.0 contain approximately 94M, 95M, and 90M\nparameters, respectively.\nFor each fine-tuning task, we select the check-\npoint with the best validation score. For weight\nspace interpolation, we fix the scaling factor at\n\\alpha = 0.25 throughout the paper and discuss its im-\npact in Section 5. For stable fine-tuning, we set\n\\beta% = 10% across all experiments. We conduct\nall experiments using a full fine-tuning setup rather\nthan parameter-efficient fine-tuning. Additional\ndetails on the fine-tuning setup can be found in\nAppendix D."}, {"title": "3.2 Evaluation", "content": "Datasets and tasks. To evaluate the generaliza-\ntion ability of speech representations, we use the\nSUPERB (Yang et al., 2021) benchmark, which\nassesses models across a diverse set of downstream\ntasks, covering content, speaker, paralinguistic, and\nsemantic aspects. Evaluating the entire SUPERB\nbenchmark is computationally expensive due to its\nwide range of tasks. MiniSUPERB (Wang et al.,\n2023b) addresses this by selecting representative\ntasks, demonstrating that evaluation on a well-\nchosen subset sufficiently reflects a model's perfor-\nmance. Following this approach, we evaluate four\ntasks: phoneme recognition (PR, content), speaker\nidentification (SID, speaker), emotion recognition\n(ER, paralinguistic), and slot filling (SF, semantic).\nWe report phone error rate (PER%) for PR, accu-\nracy (ACC%) for SID and ER, and slot-type F1\nscore (F1) along with slot value CER (CER%) for\nSF. The full evaluation on SUPERB is conducted\nfor a specific set of experiments and is reported in\nAppendix C.\nWe adhere to the default SUPERB evaluation\nsetup, where the speech representation model re-\nmains frozen, and the weighted sum of its layer-\nwise features is used as input for the down-\nstream model. During training, only the learnable\nweighted sum and the downstream model are up-"}, {"title": "4 Main Results", "content": "In this section, we show the effectiveness of\nSpeech-FT in three different scenarios: supervised,\nunsupervised, and multitask fine-tuning."}, {"title": "4.1 Supervised Fine-tuning", "content": "We begin by fine-tuning HuBERT on various su-\npervised tasks, including automatic speech recog-\nnition (ASR) on TED-LIUM (Rousseau et al.,"}, {"title": "4.2 Unsupervised Fine-tuning", "content": "We further evaluate the effectiveness of Speech-\nFT through unsupervised fine-tuning. Specifically,\nwe continue HuBERT pre-training on AISHELL-\n3 (Shi et al., 2020), a Mandarin speech corpus.\nAdditional details on the continuous pre-training\nprocess for HuBERT can be found in Appendix E.\nTo assess the quality of speech representations\non Chinese data, we introduce Chinese ASR from\nthe Common Voice Corpus (Ardila et al., 2020) as\nan additional evaluation task.\nThe results are presented in Table 3. With\nSpeech-FT, we successfully preserve generaliza-"}, {"title": "4.3 Multiple Fine-tuning Tasks", "content": "We further evaluate Speech-FT in the context of\nmultiple fine-tuning tasks. In this study, we present\npreliminary results using two fine-tuning tasks: PR\non Librispeech and SID on VoxCeleb1. Recall that\nthere are four strategies for incorporating multiple\nfine-tuning tasks: multitask fine-tuning (MTF), lin-\near merge, TIES merge, and sequential fine-tuning.\nFor sequential fine-tuning, we follow a fixed order"}, {"title": "5 Discussion", "content": "Speech-FT is effective across different speech\nrepresentation models. We further evaluate\nSpeech-FT on different speech representation mod-\nels, specifically wav2vec 2.0 and DeCoAR 2.0, as\nshown in Table 6. Consistent with previous find-\nings, Stable-FT proves ineffective for both wav2vec\n2.0 and DeCoAR 2.0, leading to a significant drop\nin the SUPERB score. In contrast, Speech-FT pre-\nserves generalization ability and results in an over-\nall improvement in the SUPERB score.\nThese three speech representation models differ\nin their pretext tasks: HuBERT employs a predic-\ntive task, wav2vec 2.0 uses a contrastive task, and\nDeCoAR 2.0 adopts a generative task (Mohamed\net al., 2022). This experiment highlights the gener-\nality of Speech-FT, demonstrating its effectiveness\nacross different types of speech representation mod-\nels.\nSpeech-FT outperforms simple regularization.\nRegularization is a straightforward alternative to\nweight space interpolation, aiming to keep the fine-\ntuned model closer to the pre-trained model in\nweight space. To compare Speech-FT with reg-\nularization, we introduce a regularization term\n\\lambda||\\theta' \u2013 \\theta_0|| during fine-tuning instead of apply-"}, {"title": "6 Related Work", "content": "Model merging is a technique that combines multi-\nple models into a single model while preserving the\nkey attributes of each original model. Weight space\ninterpolation could be regarded as a kind of model\nmerging between pre-trained model and fine-tuned\nmodel. Several studies have investigated its applica-\ntions in the speech domain. These include merging\nASR models trained on different dataset distribu-\ntions (Ramesh et al., 2024; Plantinga et al., 2024),\nmerging TTS models to interpolate speaker at-\ntributes (Murata et al., 2024), merging speech trans-\nlation models to expand language pairs (Cheng\net al., 2024), and merging ASR models to facilitate\nlifelong learning (Kulshreshtha et al., 2024).\nIn contrast to these works, we propose a novel\napplication of model merging for speech represen-\ntation learning. While previous works focus on\ntask-specific models, our method aims to improve\ngeneral speech representations."}, {"title": "6.1 Model Merging", "content": "Model merging is a technique that combines multi-\nple models into a single model while preserving the\nkey attributes of each original model. Weight space\ninterpolation could be regarded as a kind of model\nmerging between pre-trained model and fine-tuned\nmodel. Several studies have investigated its applica-\ntions in the speech domain. These include merging\nASR models trained on different dataset distribu-\ntions (Ramesh et al., 2024; Plantinga et al., 2024),\nmerging TTS models to interpolate speaker at-\ntributes (Murata et al., 2024), merging speech trans-\nlation models to expand language pairs (Cheng\net al., 2024), and merging ASR models to facilitate\nlifelong learning (Kulshreshtha et al., 2024).\nIn contrast to these works, we propose a novel\napplication of model merging for speech represen-\ntation learning. While previous works focus on\ntask-specific models, our method aims to improve\ngeneral speech representations."}, {"title": "6.2 Out-of-distribution Generalization", "content": "Out-of-distribution (OOD) generalization aims to\npreserve a model's performance on OOD datasets\nafter fine-tuning.\nSeveral studies in computer vision have explored\nOOD generalization for vision pre-trained models"}, {"title": "6.3 Continuous Learning", "content": "Continuous learning, or lifelong learning, enables\nmodels to adapt to new domain data while retaining\npreviously acquired knowledge. It addresses the\nissue of catastrophic forgetting (Kirkpatrick et al.,\n2017) and is essential for handling dynamic and\nevolving environments. Continuous learning typ-\nically involves multiple rounds of fine-tuning to\nsimulate real-world scenarios where models con-\ntinuously learn from new data.\nSeveral prior studies have explored continuous\nlearning for automatic speech recognition (Kul-\nshreshtha et al., 2024; Houston and Kirchhoff"}]}