{"title": "VERITAS: A Unified Approach to Reliability Evaluation", "authors": ["Rajkumar Ramamurthy", "Meghana Arakkal Rajeev", "Oliver Molenschot", "James Zou", "Nazneen Rajani"], "abstract": "Large language models (LLMs) often fail to synthesize information from their context to generate an accurate response. This renders them unreliable in knowledge intensive settings where reliability of the output is key. A critical component for reliable LLMs is the integration of a robust fact-checking system that can detect hallucinations across various formats. While several open-access fact-checking models are available, their functionality is often limited to specific tasks, such as grounded question-answering or entailment verification, and they perform less effectively in conversational settings. On the other hand, closed-access models like GPT-4 and Claude offer greater flexibility across different contexts, including grounded dialogue verification, but are hindered by high costs and latency. In this work, we introduce VERITAS, a family of hallucination detection models designed to operate flexibly across diverse contexts while minimizing latency and costs. VERITAS achieves state-of-the-art results considering average performance on all major hallucination detection benchmarks, with 10% increase in average performance when compared to similar-sized models and get close to the performance of GPT4 turbo with LLM-as-a-judge setting.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) (Brown et al., 2020) have made remarkable strides in knowledge intensive tasks such as search, question answering, and natural language understanding. These models, trained on vast amounts of data, possess the ability to generate coherent and contextually relevant text. However, they also exhibit a concerning issue: their generated content often includes plausible but factually incorrect information. These incorrect outputs, known as hallucinations, have raised increasing concerns about the safety and reliability of LLM applications (Xu et al., 2024). We note that, although, hallucinations are undesired for knowledge intensive tasks, the same quirk is actually desirable in creative tasks such as story-telling, image-generation, prose or poetry generation, and brainstorming.\nHallucinations are particularly common in closed-book settings, where the knowledge encoded in the model's weights and the LLM has to recall it during generation. In this setting, there is no relevant source material or context provided to the LLM and it has to solely rely on the knowledge embedded in its weight during the pre-training and post-training stages. Kadavath et al. (2022) show that for such closed-book settings, self-evaluation, wherein a LLM evaluates the validity of its own claims and predicting whether it can correctly answer a user question, works well in the multiple-choice and true/false task settings.\nOn the other hand, in open-book settings, the LLM has access to relevant source materials either provided directly in context or as part of a compound system that uses Retrieval-Augmented Generation (RAG). RAG (Guu et al., 2020; Lewis et al., 2020; Izacard et al., 2024) is a widely used approach for building user facing NLP applications, such as systems for grounded question answering (QA), fact-checking, summarization and customer support. LLMs are often unreliable in such open-book settings, and generate responses that contradicts information provided in its context, especially when the knowledge domain of the information in context is out of distribution (Shuster et al., 2021; Magesh et al., 2024). This severely hinders the application of these powerful models on private knowledge stores on complex data involving multi-step reasoning.\nMany different reasons have been proposed for why LLMs exhibit such quirky behavior including defective training data and benchmarks (Dziri et al., 2022a), bias in training data (McKenna et al., 2023), post-training on new knowledge (Gekhman et al., 2024), and knowledge cutoff (Vu et al., 2023). To address the problem of reliability in LLMs, a number of benchmarks to evaluate factuality have been created in diverse task formats such as NLI, QA, and dialog. Humans or LLMs in the LLM-as-a-Judge settings are primarily used for evaluation on these benchmarks. It leverages the power of LLMs to perform the role of Judges, which can provide judgements on content quality, coherence, and alignment (Vu et al., 2024; Kim et al., 2024). Lynx (Ravi et al., 2024) and BeSpoke-MiniCheck (Tang et al., 2024a) are examples of LLM-as-a-judge trained to judge hallucinations for QA and NLI tasks respectively. These models do no generalize to tasks beyond what they are trained on. Consequently, there is no single model that works across different task formats and on diverse domain datasets.\nA unified approach to evaluating LLM reliability would not only enable better understanding of gaps in capabilities of LLMs, but also to develop more robust post-training techniques including RLAIF (reinforcement learning with AI feedback) (Lee et al., 2024) wherein factual responses are chosen over non-factual ones. As a result, the ability of the AI judge to distinguish between factual and non-factual responses by cross-checking against retrieved documents is critical for this alignment process (Tian et al., 2023; Lin et al., 2024). Our contributions are three-fold:\n\u2022 We unify the hallucination detection problem and propose a multi-task setup that includes NLI, QA, and dialog. Our VERITAS judge is state-of-the-art on LLM-AggreFact, Halubench, HalluDial, and two proprietary enterprise datasets, reaching the performance of GPT4 Turbo.\n\u2022 We curate VERITAS Collection, high quality, diverse training dataset covering different formats for training hallucination detection models.\n\u2022 We present VERITAS Bench, a unified benchmark for evaluating LLMs on reliability in open-book settings across 18 different datasets covering three different tasks."}, {"title": "2 Related Work", "content": "Reliability Evaluation There has been a slew of work on LLM evaluation and there is an entire line of research that focuses on reliability or hallucination evaluation. The hallucination evaluation task is to predict whether or not the LLMs response is consistent with a given context. The context is usually in the form of a long-form source document or k retrieved documents from a RAG retriever. The task setup for this kind of evaluation is like the open-book exam wherein the LLM gains new knowledge through the context. In contrast, the closed-book exam evaluates the ability of a LLM to retrieve knowledge accurately from its weights baked-in during pre-training or post-training.\nThe hallucination evaluators belong to two broad classes of models: 1. classifier models (encoder transformers) and 2. generative models (decoder transformers).\nARES (Saad-Falcon et al., 2024) is a suite of classifier models based on DeBERTa-v3- Large finetuned with synthetic data constructed by bootstrapping few-shot demonstrations. Belyi et al. (2024) proposed a similar DeBERTa-based proprietary classifier model, Luna, finetuned on proprietary datasets across different industry segments. Minicheck-DBTA is also a DeBERTA-v3-Large finetuned for fact-checking by training on a combination of synthetic and a subset of adversarial NLI dataset (Tang et al., 2024b).\nGenerative hallucination evaluators include the LLM-as-a-Judge models, that are not specialized for reliability, such as JudgeLM (Zhu et al., 2023), Prometheus (Kim et al., 2024), and others (Zheng et al., 2023; Gao et al., 2023). Lynx (Ravi et al., 2024) is a hallucination detector LLM based on the Llama-3-Instruct models in 8B and 70B, fine-tuned on 2400 training examples sampled from FinanceBench (Islam et al., 2023), DROP (Dua et al., 2019), CovidQA (M\u00f6ller et al., 2020) and PubMedQA (Jin et al., 2019) wherein half of those examples are perturbed to construct hallucinated answers.\nApart from the above two categories of evaluators, yet another line of work focuses on heuristic-based consistency checks (Agrawal et al., 2024; Manakul et al., 2023; Guerreiro et al., 2023).\nBenchmarks and Metrics HaluEval (Li et al., 2023) is a collection of 35k samples consisting of QA and dialog tasks created using a two-step process of filtering and annotations for hallucinations."}, {"title": "3 VERITAS Data Collection", "content": "Addressing the limitations of current open source models, our primary goal is to design a model that is capable of flexibly handling various input formats. To achieve this, we aim to curate a diverse training data that comprises of tasks like textual entailment, summarization, question answering and grounded dialogue verification."}, {"title": "3.1 Textual Entailment", "content": "For textual entailment, we primarily source data from the ANLI dataset (Nie et al., 2020), which consists of adversarially constructed entailment data points. Each data point includes a premise, a hypothesis, and a label indicating one of three relationships: entailment, contradiction, or neutral. To focus on more informative signals, we use data from the second iteration of ANLI, excluding neutral instances to emphasize clearer learning patterns. Additionally, we incorporate existing datasets for hallucination detection, particularly Minicheck (Tang et al., 2024a), which provides document-claim pairs with a focus on sentence-level claims. Both datasets are reformatted into a unified structure of (document, claim, label) to ensure consistency."}, {"title": "3.2 Grounded Question Answering", "content": "We repurpose existing reading comprehension datasets consisting of passage, question, and answer tuples, each emphasizing different aspects of evaluation.\n\u2022 DROP (Dua et al., 2019): This dataset contains passages, questions, and answers, where verifying the correctness of an answer often requires complex numerical reasoning. The task involves extracting numerical facts from various parts of the passage and performing operations like addition, counting, and sorting.\n\u2022 NewsQA (Trischler et al., 2016): Built on CNN news articles, this machine comprehension dataset provides question-answer pairs. It poses challenges as it requires reasoning beyond simple word matching or entailment.\n\u2022 TriviaQA (Joshi et al., 2017): This dataset consists of question-answer pairs grounded in Wikipedia and web articles written by trivia enthusiasts. We specifically consider samples grounded in web articles, as their unstructured nature helps in detecting hallucinations in real-world settings.\n\u2022 SearchQA (Dunn et al., 2017): Designed to simulate an end-to-end QA system, this dataset consists of question-answer pairs grounded in noisy documents, which may include irrelevant information. Similar to TriviaQA, this setup closely resembles a typical retrieval-augmented generation (RAG) system.\nAdditionally, we curate a validation split consisting of samples from DROP, TextBookQA (Kembhavi et al., 2017), and RACE (Lai et al., 2017), ensuring evaluation on out of domain datasets."}, {"title": "3.3 Summarization", "content": "For the summarization component, we curate a diverse collection of datasets that span multiple domains and styles. The datasets are selected to represent different types of source documents. The different datasets used are: BillSum (Kornilova and Eidelman, 2019) containing US congressional and California state bills and their summaries; SAMSum (Gliwa et al., 2019) comprising messenger-like conversations with abstractive summaries; BigPatent (Sharma et al., 2019) with 1.3 million U.S. patent documents and their human-written summaries; and Multi-News (Fabbri et al., 2019), which features human written summaries of news articles from newser.com.\nWe sample equally from these datasets. forming 12k datapoints to form our summarization task. Additionally, we augment the summarization dataset with negative examples by generating factually unsupported summaries, as all original summaries in these datasets are inherently factually supported.\nAdditionally, to bring the same domain diversity to the NLI task and to maximize the utility of this data, we also reformulate the summarization instances as NLI tasks by treating summaries as claims, thereby enriching our NLI task."}, {"title": "3.3.1 Generation of hallucination samples", "content": "Since the aforementioned datasets are primarily question-answer datasets, they do not naturally include answers that are unfaithful or incorrect relative to the context. To address this, we generate incorrect answers using Llama 70B Instruct, prompting it to produce unfaithful answers based on the given context.\nWe generate diverse hallucination types following the taxonomy proposed by Mishra et al. (2024), which includes: Entity errors, where an incorrect entity alters the factuality of a statement; Relation errors, involving incorrect semantic relationships like verbs or prepositions; Sentence errors, where the entire statement contradicts the evidence; Invented errors, containing fabricated information not found in the context; Subjective errors, based on personal opinions rather than facts; and Unverifiable errors, where the answer cannot be validated by the given evidence. This variety ensures we generate diverse factual errors for effective hallucination detection."}, {"title": "3.4 Grounded Dialogue Verification", "content": "To train models that can detect factual errors in conversational settings, we aim to include grounded dialogue verification task. In this task, there is a conversation between user and assistant, where the responses by assistant are evaluated against a reference document (e.g., RAG context). Since no publicly available datasets contain document-conversation-label triples for this purpose, we generate this dataset synthetically. To achieve this, we convert the curated QA samples into dialogue by prompting Llama 70B Instruct to transform question-answer pairs into multi-turn conversations between a user and an assistant, ensuring that the factual information in the answers is preserved."}, {"title": "3.5 Spanish data", "content": "We translated data from English to Spanish to improve the judge's proficiency in Spanish. This translation process included datapoints in NLI, QA, and Dialogue formats, with 37k for training and 1.5k for validation. The LLama3.1-8B Instruct model was used for translating these datapoints."}, {"title": "3.6 Rationale Generation and Data Cleansing", "content": "In order to train models that can learn from reasoning paths, we generate rationales for the collected dataset. These rationales provide a clear explanation of the logic behind each response, allowing the model to understand not only the correct label but also the reasoning that supports it. Beyond simple explanations, rationales act as critical learning signals for the models. To generate these rationales, we prompt GPT-40 to output both the rationale and the label for each data point multiple times. If the output labels consistently contradict the ground truth label, we discard the data point due to this inconsistency. When the labels align with the ground truth, we retain the rationale which corresponding to the correct prediction. This approach helps to ensure high-quality rationales and accurate labels."}, {"title": "4 VERITAS", "content": "We fine-tune two classes of transformer models by leveraging VERITAS Data: encoder-based classifier and decoder-based generative models."}, {"title": "4.1 Classifier models", "content": "For the classifier model, we fine-tune DeBERT-v3-large (He et al., 2023) as this backbone has demonstrated strong performance in claim verification tasks in previous works (Tang et al., 2024a; Belyi et al., 2024). To unify the input text format, we convert each data point into (document, conversation) structure applicable for all formats including NLI, QA and dialogue. For QA, the claim is formatted as a single-turn dialogue between the user and the assistant. In the case of NLI, the claim is presented as the assistant response. Please refer to Appendix A for exact input formatting. We use the standard cross-entropy loss for training and fine-tune the model for 2 epochs with a constant learning rate of 1e-6, a warm up ratio 0.1, batch size of 4, weight decay of 0.001. Hereafter, we refer to this resulting classifier as VERITAS DeBERTa."}, {"title": "4.2 Generative models", "content": "Next, we considered generative transformer models, as they are increasingly applied to tasks such as LLM-as-judges, including hallucination detection. For this, we fine-tune variants of LLaMA 3.2 3B and LLAMA 3.1 8B Instruct models. Given the strong instruction-following capabilities of these models, we adopt a multi-task setup without the need for strict unified formatting. Instead, we use task-specific instruction templates, tailored for each task while maintaining consistency across the templates by varying only the entity being assessed. This flexibility ensures that the models can handle different types of inputs effectively without compromising on performance. For detailed input formatting and instruction templates, please refer to Appendix B.\nWe fine-tune these models using teacher forcing, where the models generate JSON-formatted outputs that include both the rationale and the label for the entire training split of VERITAS Data, whose train and dev compositions are shown in Table 1. Each example is structured in a chat-based format, with the input appearing as a user query and the corresponding ground truth label and rationale in the assistant's response. To reduce computational costs, we adopt QLoRA (Dettmers et al., 2024), fine-tuning the models in a 4-bit format. We set the low-rank adapter rank to 64, specifically targeting key transformer components such as projection layers, embeddings, and the final language model layers. Training is performed with an 8192-token sequence length and a learning rate of 5e-6, along with a warm-up ratio of 0.1, for a single epoch. We apply gradient accumulation over two steps with a batch size of 2. The resulting models are referred to as VERITAS 3B and VERITAS 8B."}, {"title": "5 VERITAS Bench", "content": "Existing hallucination benchmarks have primarily focused on specific types of evaluation. For example, LLMAggreFact (Tang et al., 2024a) centers exclusively on claim verification, while benchmarks like HaluBench (Ravi et al., 2024) target question-answering (QA) tasks. Although some benchmarks for grounded dialogue verification exist (Luo et al., 2024), they are rarely utilized in the development of fact-checking models. This gap motivated us to design a comprehensive benchmark that incorporates three evaluation formats: claim verification, question answering, and dialogue verification, providing a more holistic assessment of model performance. The exact composition of different tasks in VERITAS Bench is depicted in Table 2."}, {"title": "Claim Verification", "content": "For the claim verification task, we leverage LLMAggreFact (Tang et al., 2024a), as it contains a wide array of subtasks within. It provides a diverse set of claims that cover multiple scenarios, including claims derived from model-generated summaries, LLM responses to search queries and claims sourced from Wikipedia. This diversity ensures a comprehensive evaluation of models for claim verification across various contexts and content types."}, {"title": "Question Answering", "content": "For the question-answering format of evaluation, we include two key benchmarks: HaluEval (Li et al., 2023) and HaluBench (Ravi et al., 2024). In HaluEval, we focus solely on the QA-type samples. Similarly for HaluBench, we incorporate datasets such as PubMedQA (Jin et al., 2019), FinanceBench (Islam et al., 2023), which provide domain-specific challenges for QA. Notably, we exclude any samples from the DROP dataset, as some of VERITAS training data is derived from it. Additionally, we exclude splits of RAGTruth (Wu et al., 2023) and HaluEval are excluded as they are already included in under other splits."}, {"title": "Grounded Dialogue Verification", "content": "We consider the dialogue split from HaluEval and include HalluDial (Luo et al., 2024), one of the most extensive benchmarks for hallucination detection in dialogue contexts. It is to be noted that test split of HalluDial is not publicly available, so we use random subset containing 10k samples from train split."}, {"title": "Enterprise", "content": "Additionally, we include two proprietary enterprise datasets from domains of persona chatbots and real estate QA system covering dialogue and QA formats respectively."}, {"title": "6 Results", "content": "We benchmarked the VERITAS models on the VERITAS Bench by comparing them against several baselines, including current state-of-the-art classifier and generative models. In the generative models category, we selected GPT-4 Turbo as the closed-source baseline due to its competitive performance across various LLM-as-judge benchmarks. Additionally, we included Lynx 8B (Ravi et al., 2024) and Bespoke MiniCheck 7B (Tang et al., 2024a), both of which have shown impressive results on QA and NLI evaluations respectively. For classifier models, we primarily considered Minicheck DeBERTa, known for its strong performance in claim verification tasks. As evaluation metrics, we use balanced accuracy for LLMAggreFact and for all others, we report accuracy as the main evaluation metric."}, {"title": "6.1 Main Results", "content": "VERITAS models generalize across all formats Table 3 presents the key results from VERITAS Bench. Minicheck DeBERTa performs well in NLI but falls behind in QA and dialogue tasks, primarily because it is exclusively trained on claim verification, making it less effective for handling QA and conversation-based inputs. In contrast, VERITAS DeBERTa, trained using a multi-task approach, surpasses Minicheck in both QA and dialogue formats. Among generative models, Lynx 8B and Bespoke MiniCheck 7B excel in QA but struggle with dialogue tasks. Lynx's strong QA performance can be attributed to its training data, which is partially sourced from PubMedQA. Similarly, training data of Bespoke MiniCheck 7B largely involves NLI samples, alongside proprietary data whose format is not publicly known. Our VERITAS 8B, trained on a more diverse dataset, performs competitively, nearly closing the gap with GPT-4.\nEncoder models are natural fact checkers DeBERTa-based models, despite having relatively fewer parameters (440M), perform exceptionally well on benchmarks such as LLMAggreFact (see Table 4). Interestingly, generative models such as LLama 3B, even when trained with reasoning traces, could not match the performance of VERITAS DeBERTa. There could be several reasons for this. First, encoder models excel in entailment based tasks, making them naturally well-suited for claim verification. Second, training generative models to predict a final label is inherently challenging. While rationales are helpful, the actual label appears at the end, which might lead to focus more on mimicking the style of rationales instead of learning the correct output. A potential solution to this issue could be augmenting the data with examples that omit rationales (Wang et al., 2024), allowing the model to better learn how to produce accurate labels.\nOn enterprise data As show in Table 5, the VERITAS models, particularly VERITAS 8B, demonstrate exceptional performance across both real estate QA and persona chatbot dialogue tasks. Similarly VERITAS 3B also performs strongly surpassing much larger models such as Lynx 8B and Minicheck 7B. Both models surpass all baselines and notably VERITAS 8B outperforms even GPT-4 Turbo highlighting its effectiveness in handling multi-lingual and diverse formats, showcasing its adaptability for entreprise-level applications."}, {"title": "7 Conclusion", "content": "VERITAS is a unified approach for judging reliability of LLMs that employs a multi-task training setup across NLI, QA, and dialog. Our approach outperforms existing open-access models while maintaining competitive performance to GPT4 turbo with blazing fast inference (~ 100 milliseconds latency) and low costs. The generative judges can learn and self-improve from rationale without any additional training data.\nVERITAS bench is a unified benchmark for evaluating hallucination across 18 different datasets from different domains. Our results on the VERITAS benchmark confirm that encoder models such as DeBERTa are natural fact checkers and perform competitively with models that are 16x their size. The VERITAS collection and benchmark lay the groundwork for robust post-training techniques of LLMs that rely on AI feedback for aligning LLMs to make them more truthful."}, {"title": "Limitations", "content": "Document Length One key limitation of our approach is the document length constraint in generative models. Despite training with a sequence length of 8192 tokens, handling much longer documents remain a challenge. This may limit performance on tasks requiring full-length articles or extensive reports, a few of them in LLMAggreFact benchmark. Currently, we mitigate this by splitting documents into smaller chunks and aggregating the results, following prior approaches (Tang et al., 2024a). However, this solution is suboptimal.\nBackbones The backbone architectures used in VERITAS, such as LLama and DeBERTa, may not be fully optimized for the specialized task of reliability assessment. Exploring alternative architectures, like Flan-T5, could lead to performance gains (Tang et al., 2024a), especially in our multi-task setups. The multi-task instruction fine-tuning of Flan-T5 (Chung et al., 2024) makes it a promising candidate for future work."}]}