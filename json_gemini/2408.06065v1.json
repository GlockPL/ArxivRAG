{"title": "An Investigation Into Explainable Audio Hate Speech Detection", "authors": ["Jinmyeong An", "Wonjun Lee", "Yejin Jeon", "Jungseul Ok", "Yunsu Kim", "Gary Geunbae Lee"], "abstract": "Research on hate speech has predominantly revolved around detection and interpretation from textual inputs, leaving verbal content largely unexplored. While there has been limited exploration into hate speech detection within verbal acoustic speech inputs, the aspect of interpretability has been overlooked. Therefore, we introduce a new task of explainable audio hate speech detection. Specifically, we aim to identify the precise time intervals, referred to as audio frame-level rationales, which serve as evidence for hate speech classification. Towards this end, we propose two different approaches: cascading and End-to-End (E2E). The cascading approach initially converts audio to transcripts, identifies hate speech within these transcripts, and subsequently locates the corresponding audio time frames. Conversely, the E2E approach processes audio utterances directly, which allows it to pinpoint hate speech within specific time frames. Additionally, due to the lack of explainable audio hate speech datasets that include audio frame-level rationales, we curated a synthetic audio dataset to train our models. We further validated these models on actual human speech utterances and found that the E2E approach outperforms the cascading method in terms of the audio frame Intersection over Union (IoU) metric. Furthermore, we observed that including frame-level rationales significantly enhances hate speech detection accuracy for the E2E approach.", "sections": [{"title": "1 Introduction", "content": "Online platforms such as YouTube, Dailymotion, and TikTok have undoubtedly experienced a notable surge in popularity over the years. While this has led to an increased dependence on audio as a primary mode of communication, this phenomenon has also brought the issue of hate speech in audio content to the forefront. YouTube, for instance, has consistently been proactive in removing hateful content since its inception, aligning with its hate speech policy. Nevertheless, it is worth noting that out of a total of 10,501,072 channels removed from the YouTube platform within the period of July to September 2023, 26,130 channels were specifically taken down due to their association with hate speech. These statistics underscore the unequivocal importance and the imperative need for the development of effective methodologies to precisely identify hate speech within verbal expressions.\nAn important point to note, however, is that most hate speech datasets are exclusively text-based. Consequently, research endeavors pertaining to hate speech detection (Qian et al., 2018; Park and Fung, 2017) as well as investigations into hate speech explainability (Mathew et al., 2021) are confined to textual inputs. In other words, despite the explosive increase of hate speech on audio-based online social platforms, there is a notable absence of research that addresses hate speech in verbal data. A few studies related to auditory hate speech detection have been proposed. For example, Iba\u00f1ez et al. (2021); Rana and Jha (2022) curated respective audio-visual multi-modal datasets. Yet, to the best of our knowledge, no research addresses explainable hate speech detection in the audio domain, that is, the understanding of the rationale behind the model's decisions.\nTherefore, we first introduce the new task of explainable audio hate speech detection, which encompasses two sub-tasks: audio hate speech classification (AHS-CLS) and audio hate speech frame"}, {"title": "2 Related Work", "content": "Over the years, there have been considerable efforts towards text-based hate speech research, a domain that has gone through various and separate nomenclatures such as cyber hate, offensive, and online abusive Nobata et al. (2016); Davidson et al. (2017) language detection. In this paper, we define these terms collectively as hate speech. In the initial stages of hate speech detection research, Spertuse (1997) predominantly employed feature-based rules. Similarly, Mahmud et al. (2008) incorporated a set of rules to extract semantic information. More recently, in response to the escalating prevalence of online hate speech, there has been a concerted effort to curate private or publicly accessible hate speech datasets Kwok and Wang (2013); Zampieri et al. (2019). However, training hate speech detection models on such datasets, which feature binary-level hate speech annotation, lacks interpretability.\nAs a result, it becomes difficult to comprehend the logic behind model decisions. In light of this, Mathew et al. (2021) curated a dataset with word-level annotations (rationales), which deviates from conventional datasets focused solely on increasing sentence-level model classification performance.\nIt is even more important to note that the predominant focus has been on text-based classification. In other words, more datasets and research for verbal hate speech detection and explanations must be needed. To address this, Iba\u00f1ez et al. (2021); Rana and Jha (2022) curated audio-visual multi-modal datasets, while Iba\u00f1ez et al. (2021) amassed short-form Filipino videos and compared different classification methods, including Support Vector Machine, logistic regression, and Random Forest. Similarly, Rana and Jha (2022) collected videos from Twitter and YouTube, then implemented a multi-task learning model to better identify hate speech by combining text, visual, and acoustic information.\nYet, to the best of our knowledge, no research addresses explainable hate speech detection in the audio domain, that is, understanding the rationale behind the model's decisions. Our research endeavors extend beyond conventional text-based approaches by expanding into the audio domain. Moreover, we address model explainability in addition to audio hate speech detection."}, {"title": "2.2 Audio Classification & Frame Detection", "content": "Classifying audio clips into specific categories, such as speech commands (Warden, 2018), urban sound events (Piczak, 2015), and the emotional content of speakers (Busso et al., 2008), has been extensively researched. In addition to classifying entire audio clips, there's been exploration into classifying audio frames at specific time intervals (e.g., every 10 milliseconds), as seen in speaker diarization studies (Canavan et al., 1997; Fujita et al., 2019). In this study, we combine both approaches to classify entire audio clips as containing hate speech or normal speech, while also pinpointing the exact segments within the audio where hate speech occurs, using a 10-millisecond time grid."}, {"title": "3 Dataset Generation", "content": "Given the absence of existing explainable audio hate speech datasets, we created a synthetic dataset using a text-to-speech (TTS) model. This section details the methods used to convert text transcripts into spoken utterances and generate audio rationales for explaining audio hate speech.\nTo delineate the process of audio rationale generation, it is imperative first to comprehend the foundational structure of the original text-based HateXplain (Mathew et al., 2021) dataset. This text-based HateXplain dataset, represented as $D = \\{(x^{(1)}, W^{(1)}, y^{(1)}), ..., (x^{(L)}, W^{(L)}, y^{(L)})\\}$, comprises L samples. Each sample consists of a textual sentence x paired with its corresponding binary class label $y \\in \\{0,1\\}$, denoting whether the sentence qualifies as hate speech (1) or as normal discourse (0). Moreover, each textual sentence x is supplemented by a set of word-level annotations W, which is defined as $W = \\{(w^{(1)}, \\delta_w^{(1)}), ..., (w^{(N)}, \\delta_w^{(N)})\\}$. Here, N signifies the position of a word w within the sentence, with each word linked to its word-level rationale $\\delta_w \\in \\{0,1\\}$. Specifically, a word w is assigned a rationale of 1 if it contributes to the classification of the sentence as hate speech, and 0 otherwise. These word-level rationales serve as discernible evidence aiding in identifying and classifying hate speech within textual content.\nText-to-Speech From the above-mentioned text dataset, we convert each text-based transcript x into audio samples of sample rate 22050 Hz. This conversion is achieved by leveraging the non-autoregressive FastSpeech (Ren et al., 2021) TTS model in conjunction with the HiFi-GAN (Kong et al., 2020) vocoder. To ensure the coherence of audio samples, we expand abbreviations, remove emojis, and exclude sentences in languages other than English, as well as those that contain semantically vacuous words like the placeholder \"<user>\".\nRationale Labeling Each TTS-generated audio sample a is then paired with its binary classification label $y \\in \\{0,1\\}$ to identify whether it is hate speech (1) or normal audio (0). Moreover, it is imperative to provide acoustic rationales to facilitate the explainability of hate speech detection in the audio domain. To accomplish this, we employ a pretrained Montreal Forced Aligner (McAuliffe et al., 2017) to identify the timestamps, i.e., the starting and ending times in milliseconds of each spoken word within a given sentence. Subsequently, we divide audio samples into M 10ms-long audio frames f, and each audio frame f is annotated with its audio frame-level rationale $\\delta_f \\in \\{0,1\\}$. It is possible to annotate each audio frame with rationales as text-based word-level rationale $\\delta_w$ are previously provided. Our AudioHateXplain dataset can thus be represented as $\\{(a^{(1)}, F^{(1)}, y^{(1)}), ..., (a^{(L)}, F^{(L)}, y^{(L)})\\}$ of L TTS-generated audio samples. In addition, the set of frame-level audio annotations F can be reorganized as\n$F = \\{(f^{(1)}, \\delta_f^{(1)}), ..., (f^{(M)}, \\delta_f^{(M)})\\}$                                                 (1)\nHuman Recordings In addition to the synthetic audio dataset, we curated a separate collection of authentic human recordings for evaluation purpose. The transcripts used for these recordings were derived from the original text-based HateXplain dataset, but underwent a two-step post-processing procedure. Specifically, among the 1,779 original test samples, we initially use ChatGPT (Appendix A) to select texts that were suitable for spoken format. This process enabled us to sample 695 spoken-form texts from the HateXplain test set. These texts were then manually filtered to ensure that the final utterances adhered to syntactic and lexical choices appropriate for spoken language (Ong, 2002; Biber, 1986). Ultimately, 300 utterances were selected for the test set. We synthesized these samples using TTS models and also recorded them with human participants.\nThe participant group consisted of 10 individuals, 6 males and 4 female speakers. Each participant read an average of 30 utterances, including hate speech and normal texts. Recordings were conducted in silent environments. For ethical considerations, all participants were fully informed about the nature of the transcripts, which included hateful language. Moreover, all recordings were conducted with the explicit consent of the volunteers for research purposes only."}, {"title": "4 Methodology", "content": "In order to classify entire audio clips as hate speech or normal, as well as precisely pinpoint the audio frames associated with hate speech, we introduce two models. The first model uses a cascading framework (Figure 1), which transcribes audio into text, predicts hate speech in the text, and then maps the word-level rationale onto a time grid. The second model (Figure 2) employs an E2E design, directly classifying and predicting hate speech frames from the audio."}, {"title": "4.1 Cascading Method", "content": "Two essential components comprise the cascading model: an Automatic Speech Recognition (ASR), and a BERT-based hate speech detection model (Figure 1).\nGiven an audio input, the WhisperX ASR model (Radford et al., 2023; Bain et al., 2023) converts the spoken words into text, while simultaneously generating timestamps for each input word. Following this ASR phase, the transcribed text is passed as input to a finetuned BERT-based hate speech detection model (Mathew et al., 2021). This detection model comprises 12 transformer encoder layers, each containing 768 hidden units and utilizing 12 attention heads. Additionally, a composite loss (Equation 2) is employed during the fine-tuning of the BERT-based hate speech detection model, which consists of two distinct losses:\n$L_{total} = L_{pred} + \\lambda L_{att}$  (2)\nClassification loss ($L_{pred}$) is derived from the classification of hate speech within the text. Simultaneously, $L_{att}$ denotes the loss associated with predicting attention values corresponding to the [CLS] token in the model's final attention layer. Both losses are computed using cross-entropy. The coefficient $\\lambda$ serves as the weighting factor for $L_{att}$, thereby adjusting its influence on the total loss, $L_{total}$.\nThe fine-tuned BERT-based hate speech model outputs the token-level rationales for each word in the input transcribed text. These rationales are produced by leveraging token-level attention scores associated with the [CLS] token in BERT (Devlin et al., 2018), and are transformed into binary format (0 or 1) based on whether they surpass a predefined threshold $\\theta$. Afterward, a majority voting mechanism consolidates these binary token values into word-level rationales. Specifically, if the majority of token-level rationales for a given word is 1, the word-level rationale is assigned a value of 1; otherwise, it is assigned 0. Finally, these word-level rationales are aligned with audio"}, {"title": "4.2 End-to-End (E2E) Model", "content": "In contrast to the cascaded method, the E2E model presents a direct approach to detect and locate instances of hate speech in audio content, as it eliminates the need to transcribe the audio into text as an intermediary step. By using the wav2vec 2.0 model (Baevski et al., 2020), input audio signals are converted into 1024-dimensional speech representation z every 25 milliseconds, with a stride of 20 milliseconds. This encoded speech representation z is then directed to two distinct audio- and frame-level classification heads (Figure 2).\nThe audio-level classification head is tasked with discerning whether the entire audio sample constitutes hate speech or normal speech. It achieves this through a series of transformations, including a projection layer [1024, 256], mean pooling for temporal feature aggregation, and a linear layer [256, 2] to convert z into classification logits.\nOn the other hand, the frame-level detection head is dedicated to identifying individual frames corresponding to hate speech. Comprised of a single linear layer [1024, 2], this head operates directly on individual frame-level features without any feature aggregation, which preserves the granularity necessary for precise frame-level detection. To effectively optimize both audio-level classification (AHS-CLS) and frame-level detection (AHS-FD) tasks simultaneously, we also employ a multi-task learning approach with the following loss function:\n$L_{total} = \\alpha L_{CLS} + (1 - \\alpha) L_{FD}$ (3)\nThe $L_{CLS}$ and $L_{FD}$ cross-entropy losses are associated with the AHS-CLS and AHS-FD tasks, respectively. During a hyperparameter search, the value of $\\alpha$ is varied from 0.1 to 0.9 in increments of 0.1 to determine the optimal balance between these two tasks within the multi-task learning framework. It is found that the most effective balance occurs when $\\alpha$ is set to 0.5."}, {"title": "5 Experimental Setup", "content": "Cascading Models We adapted WhisperX (Bain et al., 2023) for ASR and for word-level time stamping and utilized the BERT model from (Mathew et al., 2021) for hate speech detection within transcribed text. The BERT model\u00b3 was trained using the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 2e-5, a batch size of 64, a threshold $\\theta$ set to the mean value of attention scores, and a coefficient $\\lambda$ of 0.1. We selected the best checkpoint with the highest AHS-CLS F1 after 10 training epochs. The ASR model was not fine-tuned, using Whisper-large-v24 as the checkpoint. To enhance the robustness of the cascading model against ASR transcription errors, we trained the BERT-based model with both ASR transcriptions and golden texts. The model trained with ASR transcriptions is called Cas. (ASR text), while the model trained with golden texts is referred to as Cas. (gold text). The BERT model is fine-tuned using either golden transcriptions paired with corresponding word-level rationales or ASR transcriptions with word-level rationales that account for potential inaccuracies.\nE2E Models For the end-to-end (E2E) model, we utilized wav2vec 2.05 as the shared speech encoder. The audio-level classification head and the frame-level detection head were fine-tuned simultaneously or separately (E2E CLS-only and E2E FD-only). Unless specified otherwise, the E2E model was trained with both tasks. We employed the Adam optimizer with a learning rate 4e-3 and a batch size of 64, training the model for 50 epochs. The best model was selected based on the highest AHS-CLS F1 score."}, {"title": "5.1 Evaluation Metrics", "content": "To assess the performance of audio hate speech classification (AHS-CLS) and audio hate speech frame detection (AHS-FD), a diverse set of metrics is employed. For AHS-CLS, we employ conventional metrics such as accuracy, precision, recall, and F1 score. In the case of AHS-FD, our evaluation encompasses standard F1 score, as well as frame-level accuracy and Intersection over Union (IoU) metrics, which are used to measure rationales (DeYoung et al., 2019). Given the necessity to assess audio frame-level rationale, we include the 1D IoU metric, commonly used in speaker diarization (Huang et al., 2020), as it allows for the quantitative measure of a model's accuracy in determining the durations of hate speech within audio frames, and is computed as\n$IoU = \\frac{area(F_p \\cap F_{gt})}{area(F_p \\cup F_{gt})}$                               (4)\nHere, $F_p$ and $F_{gt}$ represent the sets of predicted and ground truth frame-level audio annotations, respectively. $area(F_p \\cap F_{gt})$ denotes the intersection, i.e., the number of overlapping audio frames with a hate speech rationale ($\\delta_f = 1$) between $F_p$ and $F_{gt}$, and $area(F_p \\cup F_{gt})$ denotes their union, applied over a 10ms time grid."}, {"title": "6 Result and Analysis", "content": "The AHS-CLS aims to accurately classify entire audio clips as either hate or normal speech. In Table 2, we report AHS-CLS performance for the cascaded and the E2E model using accuracy, F1 scores, recall, and precision metrics.\nUpon assessment using the AudioHateXplain test sets (human recording and synthetic), we observe cascaded models show robust classification results over the E2E model in terms of accuracy (77% vs. 71.43%). Also, we found that a cascaded model trained with golden text rather than ASR transcription shows better classification performance. The performance degrades in Cas. ASR text is likely attributed to overfitting on ASR noise, which is the ASR transcription of the AudioHateXplain dataset.\nNotably, all models exhibit slightly higher classification accuracy on human recordings than the synthetic test set. This indicates that models trained on TTS-generated audio can also be used for real human voices."}, {"title": "6.2 Audio Hate Speech Frame Detection", "content": "The objective of Audio Hate Speech Frame Detection (AHS-FD) is to accurately identify individual audio frames associated with hate speech. Table 3 summarizes the frame detection performance for both cascaded and end-to-end (E2E) models.\nAcross both datasets and all metrics, the E2E model consistently demonstrates superior performance compared to the cascaded models, except in frame recall for human recordings. In AHS-FD, each frame within a 10ms time grid is labeled as either hate speech or normal. Since there are more normal frames than hate frames, we must consider frame F1 scores to understand the proportion of false negatives and true positives. The E2E model shows more reliable frame F1 scores in all evaluations. Our primary interest lies in detecting hate speech frames rather than normal frames. Therefore, the IoU score is a more reliable metric for this task, as it accounts for both detection and precise localization of hate speech frames. As demonstrated in Table 3, the E2E model consistently outperforms the cascaded models in IoU scores, with differences of up to 5.39% compared to the cascaded models.\nIt is important to note that the E2E model shows reliable IoU scores on both the human recording test set and the synthetic test set. In contrast, the cascaded models exhibit a significant degradation"}, {"title": "6.3 Comparative Analysis for AHS-FD", "content": "In this section, we attempt to understand the reasons for the differences in AHS-FD performance, which is observed between the cascading and E2E models. We hypothesize that the audio-to-text conversions and text-to-audio alignment within the cascading model framework are what causes severe bottlenecks for audio frame-level detection performance, as indicated by IoU performance. To test this hypothesis, we conducted three different sets of analyses.\nFirst, we analyzed the difference in ASR error between hate and non-hate words. As depicted in Table 4, when using the Whisper-large-v2 model, the word error rate (WER) for the entire test set is 17.5%, while the WER for words annotated as hate speech is 30%. In other words, the ASR model shows instability in recognizing audio hate words.\nSecond, we examined how ASR error affects IoU performance. We initially segmented the audio data into three distinct groups based on varying WER intervals and then evaluated the IoU of each WER interval for both the cascading and E2E models. As depicted in Figure 3, an inverse correlation typically emerges within the cascading model; an increase in ASR errors correlates with a reduction in IoU performance. Conversely, the E2E model consistently exhibits robust performance across different ASR error intervals and outperforms the cascading model across all audio data sets. Since ASR model types can influence WER, we conducted further experiments utilizing various versions of the WhisperX ASR models (i.e., tiny to large-v2). As shown in Figure 4, an increase in ASR errors results in a proportional decline in IoU performance.\nLastly, we examined the effect of audio word-level timestamp errors in text-to-audio conversion."}, {"title": "6.4 Frame Detection Error Analysis", "content": "Using actual example data, we examine the timeframe detection capabilities of the E2E and cascading models. Figure 5 visually presents the alignment of ground truth (GT) and predicted audio frame-level rationales in blue and red, respectively. The substantial IoU overlap, depicted in green, between GT and predicted hate speech frames highlights the superior performance of the E2E model in identifying segments of audio containing hate speech. In contrast, the cascaded model exhibits a significant decrease in IoU (8.2%) compared to the E2E model. This decline can be attributed to ASR errors in the transcription process, where the original ethnic slur is inaccurately transcribed as \"cake.\"\nMoreover, in the case of the cascading model, only the timestamp corresponding to each word is known. This means that there is a potential risk where the entire frame-level rationale corresponding to one word is either completely correct or completely incorrect. For example, the cascading model's prediction for \"K***\" was entirely incorrect, with no partially correct segments. Conversely, in the case of the E2E model, since the audio frame itself is predicted, even if a perfect prediction is not made, the frame-level rationale corresponding to a specific part of the word can still be identified. For example, although the E2E model did not make a perfect prediction for \"N*****,\" it provided a partial correct prediction."}, {"title": "6.5 Effect of Multi-task Learning", "content": "In order to validate the effectiveness of employing multi-task learning for E2E model (referred Section 4.2), we conduct experiment in Table 6. We found that integration of both classification and frame detection learning (CLS+FD) yields better performance compared to models that only employ either classification (CLS-only) or frame detection (FD-only). This enhancement can be attributed to the contextual information that the E2E model gains as it traverses individual hate speech frames within an audio clip to identify frames corresponding to hate speech. Such context augments the model's proficiency in classifying the entire clip accurately as either hate speech or not, and vice versa."}, {"title": "7 Conclusion", "content": "In this paper, we introduced the new task of explainable audio hate speech detection, which encompasses two sub-tasks: audio hate speech classification (AHS-CLS) and audio hate speech frame detection (AHS-FD). Furthermore, we introduced E2E and cascading models. These models are capable of not only classifying hate speech directly from verbal speech, but also identifying hate rationales within audio frames. In particular, the proposed E2E model consistently outperforms the cascading model on the AHS-FD task. This superiority is attributed to the bottlenecks arising from conversion between audio and text within the cascading model. This suggests that, for the task of explainable audio hate speech detection, is it more effective to directly process audio inputs. Upon acceptance, we plan to make our dataset and code publicly available to encourage further research for the important topic of explainable audio hate speech detection."}, {"title": "Limitations", "content": "Our AudioHateXplain train split comprises synthetic audio generated through a TTS model, rather than authentic human verbal data. This choice stems from the scarcity of datasets containing real human-recorded audio featuring instances of hate speech, alongside the inherent challenges in curating such recordings. Despite this, our models trained using the synthetic train set demonstrate impressive performance when tested on the human recording test set. We plan to curate a more expansive dataset comprising genuine human recordings as future work. Moreover, this study focuses on English due to the limited resources in other languages. Consequently, our approach does not accommodate the detection of multi-lingual audio hate speech."}, {"title": "Ethical Considerations", "content": "This study on explainable audio hate speech detection involves several ethical considerations. Human recordings were obtained with informed consent, ensuring participants understood the research and potential exposure to offensive content. Sensitive content was handled carefully, with participants fully aware of its nature. The deployment of these models must prevent misuse, such as unjustified censorship, and be rigorously tested for biases to avoid unfair treatment of specific groups."}]}