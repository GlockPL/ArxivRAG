{"title": "TurtleBench: A Visual Programming Benchmark in Turtle Geometry", "authors": ["Sina Rismanchian", "Yasaman Razeghi", "Sameer Singh", "Shayan Doroudi"], "abstract": "Humans have the ability to reason about geometric patterns in images and scenes\nfrom a young age. However, developing large multimodal models (LMMs) capable\nof similar reasoning remains a challenge, highlighting the need for robust evalua-\ntion methods to assess these capabilities. We introduce TurtleBench, a benchmark\ndesigned to evaluate LMMs' capacity to interpret geometric patterns-given vi-\nsual examples, textual instructions, or both and generate precise code outputs.\nInspired by turtle geometry, a notion used to teach children foundational coding\nand geometric concepts, TurtleBench features tasks with patterned shapes that have\nunderlying algorithmic logic. Our evaluation reveals that leading LMMs struggle\nsignificantly with these tasks, with GPT-4o achieving only 19% accuracy on the\nsimplest tasks and few-shot prompting only marginally improves their performance\n(<2%). TurtleBench highlights the gap between human and AI performance in\nintuitive and visual geometrical understanding, setting the stage for future research\nin this area. TurtleBench stands as one of the few benchmarks to evaluate the\nintegration of visual understanding and code generation capabilities in LMMs,\nsetting the stage for future research. Code and Dataset for this paper is provided\nhere: https://github.com/sinaris76/TurtleBench", "sections": [{"title": "1 Introduction", "content": "Large Multimodal Models (LMMs) have the potential to handle tasks that combine visual, linguistic,\nand reasoning abilities, previously achievable only by humans. Indeed, LMMs such as GPT4-V [52]\nand Gemini 1.5 flash [46, 17] have been shown to be state of the art models in solving multi-modal\ntasks such as visual question answering [18, 28], visual mathematical questions [30], chart question\nanswering [33], etc. Despite these successes, there remains the question of how LMMs perform in\ntasks that intertwine visual reasoning and programming knowledge. That is, given an image of a\ngeometric pattern (and/or a verbal description of the pattern) can LMMs generate code that would be\nable to procedurally generate that pattern? Indeed, Bubeck et al. [6] showed that the large language\nmodel (LLM) with no visual training data was able to create a unicorn in TikZ-the LaTeX-based\ngraphics drawing library. This feat amazed many and provoked many discussions on the intelligence\nof large language models-but how general is this ability?\nIn this work, we introduce TurtleBench, a set of manually crafted image/text to code tasks in turtle\ngeometry [37, 1] to evaluate the abilities of these models to combine visual pattern recognition,\nmathematical reasoning, Python programming, and abstract geometrical reasoning. To ensure the\nvisual inputs and the programming language remain straightforward, TurtleBench harnesses turtle\ngeometry, a concept widely recognized for its effectiveness in introducing programming concepts\nto children within the K-12 education system. In turtle geometry, a turtle acts as a programmable\nobject that navigates the screen, drawing as it goes and turning at specified angles, to create simple"}, {"title": "2 Overview of TurtleBench", "content": "In turtle geometry [1], a turtle is a programmable object on the screen that leaves a trace while moving.\nAs illustrated in Figure 1 left side, we see an example of how creating a simple geometric shape, like\na square, involves the turtle moving forward and executing turns four times. It is a powerful, intuitive\ntool that enables novice learners to start learning programming by creating aesthetically beautiful\nartifacts. Although Turtle programming nowadays is used more as a tool to foster computational\nthinking, it can be used to teach geometry and mathematical reasoning [31, 9] as it enables learners\nto explore and learn geometrical relationships between shapes. The intuitive nature and learnability\nof turtle geometry, along with its ability to generate patterns of diverse complexities, make it a\ncompelling concept upon which to base a benchmark for LMMs. In the following, we describe our\nbenchmark in detail."}, {"title": "2.1 TurtleBench Task Types", "content": "TurtleBench is a set of 260 tasks that are designed to evaluate LMMs' performance on vision\nand language algorithmic reasoning tasks. To ensure the novelty of the tasks and their quality in\nincorporating authentic geometric shapes and concepts, we craft TurtleBench manually. All the tasks\nin TurtleBench are accurately solvable based on the provided information for each, which means that\nthere are no ambiguities or arbitrary parameters leading to inaccuracies in the tasks for humans as\nwell as the models. To remove possible ambiguities in the tasks, two independent annotators worked\nwith us to identify and resolve any unclear instructions. Each task consists of a black-and-white image\nillustrating a set of abstract geometric shapes as an input. An example of this task is presented in\nFigure 1. TurtleBench is made up of two different types of tasks, these types reflect the methodologies\nused in turtle geometry to introduce programming to children: Scratch tasks that are intended to\nshow how well a model understands a pattern and translates its understanding to an executable code.\nIn the general case of this type of task, an image is provided, and the requested output is code in\nPython Turtle that creates the shapes in the image. In all scratch tasks, the model is asked to generate\nthe code in Python Turtle for the desired input shape. TurtleBench includes a total of 130 scratch\ntasks and 130 tweak tasks resulting in 260 tasks overall. An example of these tasks is provided\nin Figure 1 top rows. To distinguish between the models' visual comprehension and their textual\nunderstanding, a subset of these tasks includes a text description of the image input in addition to\nthe visual representation. This setup facilitates the evaluation of how models respond differently to\nvisual and textual inputs, providing a clearer understanding of their capabilities. Tweak tasks that\nare intended to measure how well a model uses their understanding of a visual pattern, combined\nwith an instruction to make minimal alterations. Each tweak task presents a model with an image and\nan instruction; the expected output is Python Turtle code that modifies the shape in the input image\naccording to the given instruction. These tasks are particularly insightful for determining whether\na model is merely recalling memorized code for an image, or if it has developed a deeper, more\nhuman-like comprehension of the patterns depicted in the images. For instance, a model might be\ncapable of generating code for a certain shape based on training data, but the real challenge lies in its\nability to adapt that shape in response to various instructed changes. An example of these tasks is\nprovided in Figure 1 bottom row. Here, The model is given an input image of a rectangle, with an\ninstruction to connect the midpoint of each side to the midpoint of adjacent sides. As illustrated in\nFigure 1, we also introduce a code editing version of the tweak task. In this version, we supply the\ncode corresponding to the input image and then instruct the models to make specific modifications\nto this code, aiming to achieve a change in the image as per the provided instructions. Detailed\ninformation about types of tweaks and their examples is provided in Appendix A.4."}, {"title": "2.2 Automatic Evaluation of Code Output", "content": "Evaluation of the output code by an AI model is performed automatically. First, the output of the AI\nmodel is processed to extract the code piece of output. Then, this piece of code is run in a sandbox,\nand the shape produced by the code is stored. An illustration of this pipeline is provided in Figure\n9. Finally, using the OpenCV module in Python, the binary versions of the correct shape and the\nproduced shape are compared using an adjusted measure of bitwise similarity where we first use\nthe bounding box technique with OpenCV to find the exact location of the shape and then calculate\nsimilarity with the formula:\n$\\frac{B_a \\cap B_m}{B_a \\cup B_m}$\nwhere $B_a$ and $B_m$ represent black pixels in the input and LMM output, respectively. This metric\nmeasures the ratio of co-occurring black pixels to the total black pixels Here, we utilize a heuristic\napproach in labeling the correctness of the model's output. If the bitwise similarity between output\nand ground truth is higher than 95% the models' output is labeled as correct and incorrect otherwise.\nTo make sure that our heuristic in labeling the correctness of generated shapes is reliable, we manually\nannotated 2000 pairs of input and output images and we found that only three instances of pairs were\nlabeled incorrectly (two of them false negative and the other false positive.), leading to an error rate\nof 0.15% which shows the high level of reliability in the heuristic we used."}, {"title": "3 Evaluation Setup", "content": ""}, {"title": "3.1 Models", "content": "In the following section, we evaluate TurtleBench using two SOTA LMMs, GPT-40 and Gemini 1.5\nFlash and also an open sourced model, namely Llava-1.5-13B [27] employing greedy decoding in\nour evaluations. We evaluated two other open models, namely Qwen-VL-Max [3] and CogVLM [47]\non a subset of tasks in TurtleBench. However, CogVLM and Qwen are not successful in producing a\nsyntactically correct Python Turtle piece of code even for the simplest tasks, therefore we limited our\nbenchmark evaluation to models mentioned above."}, {"title": "3.2 Prompting", "content": "We use two types of prompting in our experiments, 1) basic, where we simply prompt the the model\n(c.f. Appendix A.2) to do our tasks., and 2) Chain-of-Thought (CoT) prompting [49], which has\nshown to be an effective prompting technique in eliciting reasoning in these models. Specifically,\nwe use a more detailed version of CoT prompting that is tailored to LMMs, namely v-CoT, recently"}, {"title": "4 Results", "content": "Results on the performance of the models are reported in percentage, where in each experiment we\nevaluate the performance of select models on all instances of the TurtleBench with test@1 method\n[10] where the model generates only one piece of code for each instance. We assign a binary value to\nthe success/failure of each task. We then run each experiment on a model five different times and we\nreport the average percentage of accumulative success."}, {"title": "4.1 Models perform poorly on TurtleBench", "content": "We initially examine the performance of the GPT-40, Gemini 1.5 Flash and Llava-1.5-13B models\non the comprehensive TurtleBench dataset. The findings, detailed in Table 1, reveal a notably poor\nperformance across the tasks in TurtleBench, with a peak accuracy of 20% achieved by GPT-40\nin the code editing tasks, facilitated by Chain of Thought (CoT) prompting. In the scratch tasks,\nwhich represent the simplest problem type within the dataset, GPT-40's success rate was just 19%,\nunderscoring the substantial challenges and complexities these tasks pose to the current models.\nA comparison between CoT and basic prompting within Table 1 illustrates that CoT prompting\noutperforms basic prompting on the same models, aligning with previous work that indicates CoT\nenhances models' reasoning abilities [54]. However, despite employing CoT, the task remains far\nfrom being solved."}, {"title": "4.2 Models fail to generalize", "content": "Given that these models have been extensively trained on vast datasets sourced from the internet,\nthere's an underlying uncertainty regarding the source of their performance-albeit poor-on the\nTurtleBench tasks. Specifically, it remains unclear whether this performance is the result of the\nmodels' ability to memorize aspects of our tasks, rather than genuinely understanding and solving\nthem based on their programming and reasoning capabilities. To address this issue, our next step"}, {"title": "4.3 Assessing Model Proficiency Across Programming Languages", "content": "The initial suspicion might be that the models struggle with tasks in turtle geometry due to a lack\nof exposure to specific programming syntax during pretraining: Is the poor performance because of\nunfamiliarity with Python Turtle?\nTo answer the question, we run an ablation study on GPT-40 as our best-performing model in the\nmain tasks. We allow it to generate code using any library, language, or similar tools it deems\nappropriate, such as Matplotlib, TikZ, etc., without restricting it to the Python Turtle library. The\nprompt for this subset of tasks is presented in Appendix A.2.4. We manually evaluate the GPT-40\noutput for this task. Despite this freedom, we observe no significant improvement in performance.\nThe model chooses Matplotlib for 50% of the tasks and offers pseudocode for 2%, with the remainder\nreverting to Python Turtle, even though we do not specify Python Turtle in the prompts. Notably, it\navoids using TikZ, despite its mention in the prompt and proven capabilities in prior work to produce\nTikZ code [6, 4]. We further isolate this observation by obligating the model to produce code using\nMatplotlib (refraining from generating pseudocode and Python Turtle) and in this experiment as well,\nwe do not see a major improvement in the model's results. This outcome underscores a deeper issue\nthan syntax familiarity: the models' fundamental challenge is accurately interpreting visual input and\napplying this understanding to generate corresponding programming code."}, {"title": "4.4 Limited Visual Understanding in LMMs: Insights from Textual vs. Visual Tweak Tasks", "content": "To distinctly assess the models' proficiency in interpreting textual versus visual information, we\nconducted an evaluation focusing on their ability to reason about the relationship between provided\ncode and corresponding images in Tweak code edit tasks. In this setup, models are given a base\nshape along with the Python Turtle code that generated it. Subsequently, they are prompted to\nadjust the given code to modify the shape according to specified instructions. These instructions are\ndelivered in two forms: 1) (I + T) as natural language descriptions, e.g., 'connect the midpoints of\neach side to the midpoints of its adjacent sides,' and 2) (I + I) as images explicitly showcasing the"}, {"title": "4.5 Vision component contributes poorly", "content": "One of the questions regarding LMMs' abilities in visual abstraction and understanding tasks is the\nextent the incorporation of the visual component has enhanced their abilities in reasoning [34].\nIn resonance with what Mitchell et al. [34] found, here we also found that the vision component\ncontributes poorly to fostering the models' visual reasoning abilities, at least in the domain of\nTurtleBench. Specifically, we annotated 27 (21%) Scratch code generation tasks and provided clear\ndescriptions for each in plain text (Textual descriptions were validated by two human annotators, one\nin the research team and the other recruited as a volunteer). The remaining shapes were too complex\nto describe without ambiguity in plain text. Then, we compared the three modes of presenting\nthe task, image only, text only, and the blend of an image and its textual description (I, T, and\nI + T, respectively in Table 1). Interestingly, for both GPT-4o and Gemini 1.5 Flash, the model\nperformed worse when the task was presented only in the image, compared to the other modes. This\nphenomenon is counterintuitive as for humans, perceiving the images should be easier than first\nreading a description, imagining it, and then writing a code for it. Additionally, as presented in\nTable 1 the blend of image and text only slightly improved GPT-40's performance (from 38% to\n40%). These two findings show that there is still much room for improvement especially in the visual\ncomponents of LMMs."}, {"title": "5 Related Work", "content": ""}, {"title": "5.1 Large Multi-modal Models", "content": "Recent advancements in foundational multimodal models have marked a significant stride towards\ndeveloping generalist AI systems capable of understanding and integrating information across\ndifferent modalities to solve tasks without the need for task-specific fine-tuning. Among these models\nare closed source models such as Gemini 1.5 Flash [45], GPT-40 [36], and open source models\nas LLaVA-1.5 [27], Mini-GPT4 [55], InstructBLIP [13] and CogVLM [48]. The versatility and\nmultimodal understanding exhibited by these foundational multimodal models have positioned them\nas prime candidates for applications such as AI software engineers or programming tutors for children.\nOur work evaluates the efficacy of these popular models on image/text-to-code tasks, measuring their\npotential in vision/programming context."}, {"title": "5.2 Probabilistic Program Induction", "content": "Recent work in Bayesian cognitive science has modeled various aspects of cognition and learning\nas probabilistic program induction [24, 23, 40, 15, 50, 19]. This has involved both modeling human"}, {"title": "5.3 Multimodal Reasoning", "content": "The existing literature features a range of studies that evaluate these models using naturalistic\nimages [20, 21, 2], yet humans naturally are able to reason over abstract shapes [8, 53, 42] and also\nmany use cases of LMMs involve understanding abstract shapes and sketches [16, 35]. Moreover,\nunlike naturalistic images [32, 43], the relationship between language and abstract shapes is highly\nintertwined as minimal alterations in language can lead to different visual perceptions in humans\n[14, 26]. Overall, recent surveys on deep learning for mathematical reasoning [29, 44] have pointed\nout that most of the available datasets and benchmarks on multimodal reasoning often rely on visual\nquestion-answering frameworks. However, these methods fall short because they are usually trained\non datasets composed of natural images, rather than on datasets tailored to the integration of vision\nand language for mathematical tasks.\nThe Multimodal Algorithmic Reasoning (MAR) task tests multi-modal models on fundamental skills\nunderstandable by children, focusing on interpreting visual and linguistic information to answer\nquestions. Perhaps the most relevant work to ours is the paper by [7] in which they introduced a\ndataset with 101 multiple-choice questions inspired by the Math Kangaroo contest for 6 to 8-year-olds,\ninvolving images and texts that the model must analyze together. The task has been shown to be\nchallenging for multimodal deep neural networks, and the following trials to solve the problem have\ngained less than 25% accuracy on the test set [51]. TurtleBench includes abstract geometric shapes,\nand the task only relies on knowledge and reasoning over a set of simple functions in the Python\nTurtle library. Our open-ended benchmark and its flexibility over different modalities make evaluating\ndifferent aspects of multimodal reasoning in LMMs more reliable."}, {"title": "6 Discussion on Educational Implications", "content": "While LLMs and LMMs have sparked interest among education researchers in AI tools such as\ntutors [22] for students, our work cautions against using these models without thorough evaluation.\nAlthough students can learn turtle programming on various platforms without AI help (e.g., Code.org\n[11, 12]), the effectiveness of these models as tutors or copilots is uncertain due to current limitations.\nOur work suggests that educational researchers can engage in systematically benchmarking LMMs to\nensure their reliability before integrating them into student learning processes."}, {"title": "7 Conclusions", "content": "This study introduces TurtleBench, the first of its kind in benchmarks that focus on converting\nvisual inputs to code outputs. The evaluation results from TurtleBench reveal a significant disparity\nbetween how humans tackle turtle programming and how SOTA AI models perform in understanding\nsimple geometric shapes, reasoning about these shapes, and converting such understandings into\nexecutable code [38]. This gap underscores the challenges that lie ahead in the quest to enhance AI's\ncomprehension and problem-solving abilities to match human levels. We believe that TurtleBench\nserves as a crucial tool in the evaluation of models, offering a clear benchmark testing the limits of\nLMMs."}, {"title": "8 Limitations", "content": "One of the limitations of our work is that we did not experiment with fine-tuning techniques to better\nunderstand multimodal reasoning abilities in these models and how they could be improved. However,\nwe argue that our experiments demonstrate that poor performance in the models is perhaps not due\nto their unfamiliarity with the syntax of Turtle, but rather is more related to vision components and\ntheir reasoning abilities. We plan to experiment with fine-tuning techniques in future work with\nsmaller models such as Llava-1.5-13B with newer architectures for vision towers [25] to examine the\neffectiveness of these techniques.\nFinally, as TurtleBench has a limited number of instances, future work can augment existing instances\nto produce a dataset plausible for training purposes."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Reasons of Failure", "content": "We manually investigated GPT-40's failures in solving Scratch tasks in a single run to find the major\ncauses of failure. We find four major causes: 1) Shape identification error: where the model fails to\ncompletely capture existent shapes in the input image, for instance, if it confuses a semicircle with a\ncircle or assigns non-existent shape attributes to the input image. 2) Counting error: where the model\nfails to count adequately, (e.g., three triangles counted as four), 3) Orientation error: where the model\nfails to correctly find the relationships between different components of a shape (e.g., semicircle on\ntop of a square vs. at its bottom), and 4) Implementation error: where the model's generated code\ndoes not follow the pre-planned pseudocode."}, {"title": "A.2 Prompting", "content": ""}, {"title": "A.2.1 Basic Prompt", "content": "In each task, the user provides an image of an abstract geometric shape or pattern\nand an instruction, you need to generate a code in Python Turtle that follows the\nuser's request."}, {"title": "A.2.2 v-CoT Prompt", "content": ""}, {"title": "A.2.3 A Complete Example", "content": "Figure 5 we provide an instance of a complete prompt we used for a tweak code generation task\nwith CoT prompting."}, {"title": "A.2.4 Arbitrary Output", "content": "Figure 6 provides the CoT prompt we used for the model to provide a code in any arbitrary language\nor library that creates the desired shape."}, {"title": "A.3 Rabbit", "content": ""}, {"title": "A.3.1 Prompt used", "content": "The prompt we used for this experiment is provided in Figure 7."}, {"title": "A.3.2 Definition of the class", "content": "The rabbit class is an arbitrary class that we defined based on Turtle class in the Python Turtle Module.\nThis minimal set of functions includes all functions that a programmer or a model needs to create all\nof the tasks in TurtleBench. We defined this new set of functions to measure how GPT-40 is able to\ngeneralize its abilities in generating code in Python Turtle to a similar but minimally different set of\nfunctions."}, {"title": "A.4 Types of Tweak Tasks", "content": "TurtleBench includes a total of 130 tweak tasks. We provide a categorization for the tweaks as\nfollows: There are five major types of tweaks in TurtleBench;\n\u2022 Deletion: Removing a specified part of a shape\n\u2022 Insertion: Adding a specific shape to the pattern as directed\n\u2022 Rotation: Rotating the entire shape\n\u2022 Reflection: Reflecting the entire shape or parts of it across specified lines\n\u2022 Generalization: maintaining a pattern in the image constant while varying its parameters.\nAn illustration of instances of each type is provided in Figure 8. These types are not mutually\nexclusive as 10% of the tasks involve a combination of two types (e.g., removing one side of a square\nand inserting a semicircle instead). To successfully complete deletion and insertion tweaks, a model\nneeds to demonstrate a nuanced understanding of the details in the image and program the resulting\nshape accordingly. In contrast, rotation tasks can be relatively easy as most of them can be solved\nonly using a simple function in Turtle that can rotate the starting heading of the turtle which results in\ncomplete rotation in the entire shape (i.e., turtle.right(angle))."}, {"title": "A.5 Evaluating Image Complexity Using Contour Counts", "content": "As our result suggests that the vision component is contributing poorly to the models' performance,\nto gain a better understanding of the visual obstacles for the models to solve the tasks, we defined a\nmeasure as a proxy for the complexity of shapes. For each provided image, we calculated the number\nof contours in each shape. In OpenCV, a contour is a curve joining all the continuous points (along\nthe boundary), having the same color or intensity. Contours are a useful tool for shape analysis and"}]}