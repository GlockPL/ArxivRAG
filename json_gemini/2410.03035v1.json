{"title": "SPINE: Online Semantic Planning for Missions with Incomplete Natural Language Specifications in Unstructured Environments", "authors": ["Zachary Ravichandran", "Varun Murali", "Mariliza Tzes", "George J. Pappas", "Vijay Kumar"], "abstract": "As robots become increasingly capable, users will want to describe high-level missions and have robots fill in the gaps. In many realistic settings, pre-built maps are difficult to obtain, so execution requires exploration and mapping that are necessary and specific to the mission. Consider an emergency response scenario where a user commands a robot, \"triage impacted regions.\" The robot must infer relevant semantics (victims, etc.) and exploration targets (damaged regions) based on priors or other context, then explore and refine its plan online. These missions are incompletely specified, meaning they imply subtasks and semantics. While many semantic planning methods operate online, they are typically designed for well specified tasks such as object search or exploration. Recently, Large Language Models (LLMs) have demonstrated powerful contextual reasoning over a range of robotic tasks described in natural language. However, existing LLM planners typically do not consider online planning or complex missions; rather, relevant subtasks are provided by a pre-built map or a user. We address these limitations via SPINE (online Semantic Planner for missions with Incomplete Natural language specifications in unstructed Environments). SPINE uses an LLM to reason about subtasks implied by the mission then realizes these subtasks in a receding horizon framework. Tasks are automatically validated for safety and refined online with new observations. We evaluate SPINE in simulation and real-world settings. Evaluation missions require multiple steps of semantic reasoning and exploration in cluttered outdoor envi-ronments of over 20,000m\u00b2 area. We evaluate SPINE against competitive baselines in single-agent and air-ground teaming applications. Please find videos and software on our project page: https://zacravichandran.github.io/SPINE", "sections": [{"title": "I. INTRODUCTION", "content": "Consider an inspection robot operating after a heavy storm. A user may provide the following mission: \u201cCommunications are down. Why?\" The robot will have to explore missing or changed regions of the map, locate relevant semantic entities (i.e., communication infrastructure), and collect precise mission-relevant information to assess infrastructure damage. We refer to these mission specifications as incomplete. They imply sugboals and semantic targets that are not directly given to the robot; rather, they must be inferred from context. Furthermore, in many real-world scenarios, environments are dynamic and data is hard to collect, so the robot must actively map its environment and plan online.\nSemantic planning methods have made progress on tasks such as object search, inspection, exploration, and mobile manipulation [1]\u2013[9]. These methods typically maintain a"}, {"title": "II. RELATED WORK", "content": "Representations for Semantic Planning. Effective se-mantic representations capture traversability, semantics, and spatial relationships needed for reasoning over contextual goals. Advances in semantic mapping have enabled online planning tasks such as active exploration or air-ground teaming for object search [1], [4], [16], [19], [38].\nScene graphs are a popular representation for semantic planning, as they concisely represent objects, topology, and traversable regions. Hydra provides a real-time scene graph engine [11] designed for indoor environments. Strader et al. [14] relax this assumption. Topological maps are similar, but do not include a hierarchy [39], [40]. Recent work incor-porates foundation models into mapping pipelines in order to create open-vocabulary representations. Mappers including"}, {"title": "Online Semantic Planning", "content": "Online Semantic Planning. Semantic planners reason over objects, regions, or other contextual information to address problems such as object search and inspection, and semantic exploration [1], [3], [5], [6], [44]. Many works address online planning in partially-known environments. In these settings, prior scene information may be obtained from previous mission data [3], and the planner augments priors with information acquired online [3], [7]. Beyond object-level reasoning, semantic information also accelerates exploration of partially-known or unknown environments [1], [45], [46]. Fusing semantic knowledge from foundation models with classical search methods such as frontier exploration has been shown to an expecially effective exploration strat-egy [8], [45]. Structured or formal planning languages, such as Linear Temporal Logic (LTL), may be used to compose more complex missions [10], [21], [22], [47], [48]. Notably, these methods require detailed mission specifications from a user, whereas our method infers specifications."}, {"title": "LLMs for Planning", "content": "LLMs for Planning. Language has emerged as a power-ful representation for specifying tasks, and LLM-enabled planners have been applied to domains including mobile"}, {"title": "III. ONLINE SEMANTIC PLANNING APPROACH", "content": "We outline SPINE's architecture in Fig. 2. The planner consists of an LLM, a behavior library, and a validation module. The planner receives a mission from the user and a prior from the Semantic Mapper (Sec. III-C). The planner composes a subtask sequence in a receding horizon manner (Sec. III-D), which is validated for syntactic and semantic correctness (Sec III-E). Verified subtasks are sent to their relevant module (mobility, active sensing, or user interaction,"}, {"title": "A. Problem Statement", "content": "We consider a scenario where a robot operates in a partially-known, unstructured environment and is assigned a mission with incomplete natural language specifications. The semantic planner has access to a set of behaviors for navigation, active sensing, and user interaction. At runtime, the planner receives an incomplete prior map and utilizes a semantic mapper to obtain real-time semantic updates of the environment. Given the incomplete mission specification, a fixed planning horizon, and online semantic feedback, the planner must infer and generate appropriate subtasks within a receding horizon framework to fulfill the mission."}, {"title": "B. System Overview", "content": "We outline SPINE's architecture in Fig. 2. The planner consists of an LLM, a behavior library, and a validation module. The planner receives a mission from the user and a prior from the Semantic Mapper (Sec. III-C). The planner composes a subtask sequence in a receding horizon manner (Sec. III-D), which is validated for syntactic and semantic correctness (Sec III-E). Verified subtasks are sent to their relevant module (mobility, active sensing, or user interaction, Sec. III-D). At each iteration, the Semantic Mapper provides updates which are stored in-context, and the planner refines its subtask sequence online (Sec. III-D)."}, {"title": "C. Semantic Mapper", "content": "Our architecture assumes a topological graph-based se-mantic mapper. Each node is either a region or object. As is standard in the semantic mapping literature, regions are traversable points in freespace [11], [42]. An edge between two regions indicate that there is an obstacle-free path. Object nodes represent localized objects in freespace. An edge between an object and region node indicate that the object can be observed from that region. Regions and ob-jects may be enriched with additional semantic information (e.g., this region is in a busy parking lot), which provides additional cues for planning. The mapper also maintains a local occupancy map, which the planner uses for Spatial Validation (Sec III-E). The mapper is initialized with priors from satellite imagery, UAV maps, or previous mission data. At each planning update it will provide updates to the planner via the API described in Sec. III-D)."}, {"title": "D. LLM Planner with Behavior Library", "content": "We configure the LLM via a system prompt with three pri-mary components: role description, mapping interface, and behavior library. Role description is described in the Problem Statement (Sec III-A), and the following componenets are described below.\nMapping interface: At each planning iteration, all map updates are provided to the LLM in-context via the following API which captures high-level graph manip-ulations: add_nodes, remove_nodes, add_edges,\nremove_edges, update_nodes. The nodes are defined as a dictionary of attributes, which allows for providing nodes with rich semantic descriptions (example in Fig. 6).\nBehavior Library: The planner has access to atomic behav-iors for mobility, active sensing, and user interaction (see Table I). At each planning iteration, the LLM generates a"}, {"title": "E. Semantic and Spatial Validation", "content": "To create subtasks, the planner must correctly invoke its behavior library while reasoning over constraints such as traversability (see Table I). LLMs are prone to hallucinate this information, thus we filter LLM-generated plans through a validation module, which is outlined in Algorithm 1.\nThe validation module first ensures that the task sequence is composed of behaviors which are invoked with semanti-cally correct arguments. Validation also checks reachability; meaning, for a given goal there must exists a path to that goal in the current map. If a given task is invalid, the validator forms state-specific feedback to the LLM. For example, because the goto behavior requires reachability, if the LLM tries to call goto on an unreachable node, node x, the verifier will provide the feedback \"node x is unreachable from your current location. Consider exploring, and update your plan accordingly.\"\nGiven a semantically valid plan, tasks with exploration constraints are spatially validated to prevent hazardous or unreachable goals from being sent to the controller. Spatial validation uses frontier-style exploration to iteratively search for a traversable path towards a given goal. The algorithm terminates after reaching the goal or encountering an ob-stacle. For each breaking condition, semantic feedback is provided to the planner such as \"exploration terminated after encountering an obstacle.\""}, {"title": "IV. EXPERIMENTS", "content": "We design experiments to assess our three contributions (Sec I):\n\u2022 Q1: How much time and distance savings does SPINE provide compared to mapping then planning approaches?"}, {"title": "A. Implementation Details", "content": "Both simulated and real robot experiments assume a mobile robot, a Lidar and RGB-D camera. Our planner uses GPT-4 [61]. Our graph-based semantic mapper enriches the object-oriented map from SlideSLAM [12] with a traversabil-ity graph as described in Sec. III-C. The mapper uses GroundGrid [62] for traversability estimation; these points are used to establish region nodes. The mapper performs open-vocabulary object detection with GroundingDino [63], and the detected objects are grounded using a multiple-hypothesis tracker. The mapper uses the LLaVA vision-language model for object inspection and region descrip-tion [64]. Faster-LIO provides odometry [65]. We use ROS MoveBase for control. Simulation experiments employ a photorealistic Unity testbed, which provides realistic sensor and control feeds on a ClearPath Husky. We then perform real robot experiments on Clearpath Jackal equipped with a Ouster Lidar, Realsense RGB-D Camera, Nvidia RTX 4000 GPU, and Ryzen 5 3600 CPU."}, {"title": "B. Baselines and Metrics", "content": "We compare against two baselines. First, we compare against Explicit Tasking, where the planner is given a step-by-step instruction by the user. The planner does not have to infer subtasks or exploration objectives. While this method does not use a formal planning language (eg Linar Temporal Logic), we note the similarity to existing formal planning methods where the user provides explicit mission instructions [35], [57]. We then consider the Mapping then LLM-as-Planner (henceforth referred to as Two Stage) approach, which is a common LLM planning paradigm [25], [27], [28], [36]. In this baseline, a map is built, then the planner is given the mission. Following previous work, all spatial regions are provided in the map given to the planner, but the planner can still discover new objects in the scene. [27], [32].\nFor SPINE and Mapping then LLM-as-Planner approach, the operator provides an initial mission. If the planner stops prematurely, the operator may intervene to provide subsequent instructions, which we report as interactions. However, a successful trial requires the planner to complete the mission without user intervention. We also report distance traveled, time elapsed, and LLM API calls (queries) required to complete a mission. When explicitly tasking the robot, more complex tasks will have more user iterations. Ideally, our method will infer these tasks with minimal interactions. We normalize all results against Explicit Tasking, as that provides nominal mission performance, and we refer to our method as Online."}, {"title": "C. Missions with Incomplete Specifications", "content": "We consider the missions 1) there was a storm last night. I am worried that impacted logistics, because I need to drop"}, {"title": "D. Simulation Results", "content": "We run simulation experiments in outdoor environments of over 40,000m\u00b2, where missions require the robot to travel up to 400m. Results are provided in Table II. Averaged over all scenarios, Explicit Tasking takes 532s, travels 292m, 8.6 API calls, and 4 user interactions to complete a mission. Despite only receiving partial knowledge of the mission and environment, our method performs competitively to the Explicit Tasking approach. Compared to Explicit Tasking, our method requires less user interaction but makes a similar amount of LLM queries, which implies that it reasons about the subgoals required for a mission online (for Q2). Imperfect success rate comes from the third mission, where the planner must inspect multiple communication towers for damage. After finding that the first tower is damaged, instead of inspecting the next tower, it declares the task complete. Because SPINE performs online mapping, it provides rich mission-specific detail. For example, when assessing a com-munication outage, the planner finds that the radio towers are rusted and in a state of disrepair (Fig. 6). While the Two Stage approach is also competitive in success, this method requires over twice the distance and time required"}, {"title": "E. Real Robot Results", "content": "We evaluate SPINE on a ClearPath Jackal in a semi-urban office park. The results are shown in Table III. Averaged over all scenarios, Explicit Tasking takes 1035s, travels 202m, 8.6 API calls, and requires 5 user interactions to complete a mission. In the real world, the planner must adapt to more complex perception input, greater actuation noise, and avoid more obstacles as compared to simulation, thus it travels slower on average. Results show that SPINE still compares favorably to Explicit Tasking (for Q2). Interestingly, the real robot success rate was higher than in simulation, which is likely due to the increased scale of the simulated environ-ment. Due to the environment scale and real constraints of the robot, there is a comparatively larger gap between the Mapping then LLM-as-Planner approach and our method (for Q1). See Fig. 1 for an example mission."}, {"title": "F. Importance of Validation for Online Planning", "content": "In order to measure the importance of validation for online mapping, we compare SPINE to an ablated version without the validation module (for Q3). We provide identical missions to each method, and we measure mission success rate as we randomly remove portions of the prior map. Results, shown in Fig. 7, indicate that verification is increas-ingly important as the environment becomes less certain."}, {"title": "V. CONCLUSION", "content": "We present SPINE, a method for online semantic planning in partially-known, unstructured environments. We consider missions with incomplete specifications given in natural language. Our planner uses an LLM to decompose these specifications into a sequence of navigation, active sensing, and user interaction subtasks in a receding horizon frame-work. These subtasks are automatically validated and refined online.\nSimulation and real-world experiments demonstrate that the performance of our method is comparable to explicit tasking, where an expert user goes through a tedious process of providing detailed subgoals to the planner. Our method is also significantly more efficient in terms of distance and time required to complete a mission as compared to the two step process of first mapping, and then using a LLM-as-Planner approach, without requiring full a priori knowledge of the environment.\nFuture work may take several directions. While the planner accomplishes missions with incomplete specifications, we find that the method may pick regions that are suboptimal given the complete environment knowledge. For example, if a task requires visiting multiple regions, the LLM may choose to visit the farther regions first, leading to inefficient behavior. This limitation of the LLM was one of driving factors in the validation module design, and we would like to design better techniques. Going forward, we would also like to explore the use of open-sourced LLMs such as Llama [66] or Gemma [67]. Finally, we are interested in extending our method to distributed multi-robot planning missions."}, {"title": "APPENDIX I", "content": "In this appendix we provide further detail on our proposed method. Subsection A2-A provides details on the LLM system prompt, including the perception api and planning interface. Subsection A2-B describes the behavior library im-plementation including the controller used. Subsection A2-C provides more details and visualizations on teh semantic mapping components traversability estimation, object de-tection, and VLM results. Section A3 provides details on the experimental setup. We provide more details on the experimental missions, including subtasks required and prior maps given to the planner and provide further discussion on results (Subsection A3-F), including why the performance of SPINE was 6% lower than baselines in the simulation experiments (see Tab. II)"}, {"title": "APPENDIX II", "content": "We provide details on the implementation of the LLM configuration, semantic mapper, and behavior library."}, {"title": "A. LLM configuration", "content": "The LLM configuration consists of four main parts: main system configuration, perception API, planning API, and planning advice. The system configuration provides an overview of the LLM's role in the planning framework and defines interfaces (see Listing A2-C). The perception API defines how the LLM will receive updates from the semantic mapper (see Listing A2-C). The planning API defines how the LLM will compose subtasks sequences (see Listing A2-C). Finally, the advice portion of the configuration preempts common mistakes we observed the LLM making during development (see Listing A2-C). We also provide five in-context examples of canonical planning behavior, and exam-ple of which is detailed in Listing A2-C, and we refer the reader to our software for a complete list. At runtime, the user-provided mission and current scene graph is appended to the context."}, {"title": "B. Behavior library and Constraint Feedback", "content": "We provide further details on the behaviors listed in Tab. I. goto takes a string, which is interpreted as a region node. The planner with find the shortest path to that node over the current graph, and it will then navigate to that node. The following behaviors call goto for navigation to a particular node, where applicable. map_region takes a string, which is interpreted as a region node. The robot will navigate to that node and report any objects detected along the way. explore_region takes a string and float, which is inter-preted as a region node and exploration radius, r, in meters. The robot will navigate to that node, then explore the circle of radius r around that region node. extend_map takes two floats, which is interpreted a 2D coordinate. The robot will attempt to navigate to that coordinate. inspect takes two strings, which is interpreted as an object node and inspection query. The robot will navigate to that object, which is obtain"}, {"title": "C. Semantic Mapper", "content": "An edge between an object and region node indicate that the object can be observed from that region. Regions and ob-jects may be enriched with additional semantic information (e.g., this region is in a busy parking lot), which provides additional cues for planning. The mapper also maintains a local occupancy map, which the planner uses for Spatial Validation (Sec III-E). The mapper is initialized with priors from satellite imagery, UAV maps, or previous mission data. At each planning update it will provide updates to the planner via the API described in Sec. III-D)."}, {"title": "APPENDIX III", "content": "This section provides details on the experimental tasks reported in Section IV. We describe the mission, instruction given the SPINE, and the subtasks required. We then provide further discussion on experiments."}, {"title": "A. Semantic Route inspection", "content": "Mission provided to SPINE: \"There was a storm last night. I am worried that impacted logistics, because I need to drop off supplies today. Can I still do that'\"\nImplied subtasks: The planner must recognize that the delivery depot is the most likely place for supply delivery. The user wants to make sure the path between the current location and delivery depot is free. These subtasks are\n1) Recognize semantics. Primarily current location and delivery depot. Bonus: recognize that debris, puddles, fallen trees, etc, will give information about the extend of the storm.\n2) Navigate along path path to delivery depot. At each step, if the robot cannot traverse an edge, it is likely blocked."}, {"title": "B. Search and inspection with implicit goals", "content": "Mission provided to SPINE: \"I sent a robot out to collect supplies from an incoming boat. I have not heard back. What happened?\"\nImplied subtasks: The planner must recognize that it is looking for a robot, and use the contextual information provided to infer the robot is likely near one of the three docks in the scene. The map does not provide a direct path to these docks, so the planner must explore in order to reach its goal locations. The planner must then find the mission robot, which is near the third dock.\nThe implied subtasks are:\n1) Infer correct semantic labels (robot) and best search locations (three docks)\n2) Understand gaps in map (three major gaps)\n3) Navigate to the map boundary\n4) Extend map to the first dock\n5) Extend map to the second dock\n6) Extend map to the third dock\n7) Find and inspect robot\n8) Report findings to user"}, {"title": "C. Multi-object inspection with implied semantics", "content": "Instruction provided to SPINE: \"Communications are down. Can you figure out why?\"\nImplied subtasks: There are two radio towers provided in the prior map. The planner must infer that radio towers are relevant for communication, so it should inspect those. There is no direct path between the planners start locations and the radio towers, so the planner must explore."}, {"title": "D. Semantic route inspection on real robot", "content": "Instruction given the SPINE I am worried that recent construction on roads and fences impacted maritime supply logistics. Can you check?\nImplied subtasks: The planner must recognize that the user is concerned about a path to the dock, which is provided"}, {"title": "E. Air-ground teaming on real robot", "content": "Mission provided to SPINE You are assisting a high-altitude UAV in responding to an emergency chemical spill. Triage regions that are not visible from the air.\nImplied subtasks: The planner must recognize that inside buildings and under trees cannot be observed from high-altitude UAVs, thus the planner should explore those regions. There are regions of the map that are not provided in the prior, so the planner must explore. The planner must also"}, {"title": "F. Discussion of results", "content": "We observed comparative performance drop in SPINE (see Table II) during multi-object inspection missions (Subsec-tion A3-C). This mission required the planner to inspect two radio towers in the scene. During some runs, the planner would inspect the first tower, learn that the tower was damaged, and terminate the mission. While this behavior is correct, it is not complete.\nFor both the explicit tasking baseline and SPINE, there was one manual takeover for each experiment. These takeovers were both due to the minimum range of the obstacle detector, which was around 1 meters. If the robot came closer to one meter to an obstacle, that obstacle would not be registered in the perception costmap, thus the robot would try to drive into the obstacle. See Fig. A10 for an illustration."}]}