{"title": "Economic span selection of bridge based on deep reinforcement learning", "authors": ["Leye Zhang", "Xiangxiang Tian", "Chengli Zhang", "Hongjun Zhang"], "abstract": "Deep Q-network algorithm is used to select economic span of bridge. Selection of bridge span has a significant impact on the total cost of bridge, and a reasonable selection of span can reduce engineering cost. Economic span of bridge is theoretically analyzed, and the theoretical solution formula of economic span is deduced. Construction process of bridge simulation environment is described in detail, including observation space, action space and reward function of the environment. Agent is constructed, convolutional neural network is used to approximate Q function, \u03b5 greedy policy is used for action selection, and experience replay is used for training. The test verifies that the agent can successfully learn optimal policy and realize economic span selection of bridge. This study provides a potential decision-making tool for bridge design.", "sections": [{"title": "0 Introduction", "content": "For a long bridge, laying aperture of bridge is an important task for bridge designers. The selection of bridge span has a huge impact on the total cost. When there are no rigid conditions such as navigable clearance, the bridge span should select the economic span, that is, the total cost of superstructure and substructure is the lowest. In terms of cost, the larger the span, the fewer the number of aperture of bridge, which can reduce the cost of substructure, but increase the cost of superstructure; On the contrary, the cost of superstructure will be reduced, but the cost of substructure will be increased. The terrain, geology, hydrology and other factors at the bridge site affect the value of economic span. For example, the value of economic span of sea-crossing bridges is usually greater than that of shoal of plain rivers. Laying aperture of bridge is a complex engineering problem, which needs to be analyzed and compared in terms of technology and economy, so as to obtain a more ideal design scheme.\nReinforcement learning (RL) originated from animal learning in psychology and optimization theory of optimal control in the 1950s and 1960s. In 2016, AlphaGo defeated the human world champion of Go and officially brought reinforcement learning that has been developing silently for half a century into the public's vision. Based on deep reinforcement learning (DRL), Guozhong Cheng proposed an intelligent design method for high-rise shear wall structures; Based on reinforcement learning, Jiachen He explored the active control of wind-induced vibration of long-span bridges; Based on deep reinforcement learning, Quan Yuan proposed the intelligent railway location design approach; Based on reinforcement learning, Ruifeng Luo proposed a generative design algorithm for truss structures; Cheng M used deep reinforcement learning to put forward the decision-making framework for load rating planning of aging bridges; Yang DY proposed a bridge management method considering asset and network risks based on deep reinforcement learning. However, the application of reinforcement learning in economic span of bridge has not been reported.\nIn this paper, we build bridge simulation environment and use deep Q-network (DQN) algorithm of deep reinforcement learning to explore economic span selection of bridge (open source address of this article's https://github.com/zhangleye/BridgeSpan-DQN)."}, {"title": "1 Theoretical analysis of economic span of bridge and introduction to deep reinforcement learning", "content": "1.  1 Theoretical analysis of economic span of bridge\nThe idea of reference [2] is used for reference here, and appropriate simplification is made for the convenience of theoretical analysis (for example, the cost of two abutments is combined into the cost of one pier, and assume that the cost is a differentiable function of the span without considering the mutation caused by the change of structural form).\nThe total cost of bridge is the sum of the costs of superstructure and substructure.\n$W_{total}=W_{upper}+W_{under}$                                                                                                                                                           (1)\nIn the formula: $W_{total}$ is the total cost of bridge; $W_{upper}$ is the cost of superstructure; $W_{under}$ is the cost of substructure.\nSuperstructure consists of bridge deck system (pavement, anti-collision wall, etc.) and load-bearing structure. The cost of bridge deck system has nothing to do with span. It is assumed that the cost of load-bearing structure is a power function of span.\n$W_{upper}=(gS_1+C_1x^mS_2)L$                                                                                                                                                           (2)\nIn the formula: $gS_1$ is the cost of bridge deck system per unit length in the longitudinal direction, $g$ is the consumption of bridge deck system materials per unit length in the longitudinal direction, $S_1$ is the unit price of bridge deck system materials; $C_1x^mS_2$ is the cost of load-bearing structure per unit length in the longitudinal direction, $C_1x^m$ is the material consumption of load-bearing structure per unit length in the longitudinal direction, $C_1$ is the coefficient of power function, $x$ is the span (independent variable of power function), $m$ is the exponent of power function (not less than 1), $S_2$ is the unit price of load-bearing structure material; $L$ is the total length of all spans.\nIt is assumed that the cost of substructure is also a power function of span.\n$W_{under}=C_2x^{\\frac{1}{n}}S_3\\frac{L}{X}$                                                                                                                                                            (3)\nIn the formula: $\\frac{L}{x}$ is the number of piers; $C_2x^{\\frac{1}{n}}S_3$ is the cost of a single pier, $C_2x^{\\frac{1}{n}}$ is the material consumption of a single pier, $C_2$ is the coefficient of the power function, $\\frac{1}{n}$ is the exponent of the power function (n>1), $S_3$ is the unit price of pier material.\nDerivative of total bridge cost with respect to span:\n$\\frac{d(total)}{dx} =C_1mx^{m-1}S_2L +(-\\frac{L}{x^2})C_2x^{\\frac{1}{n}}S_3\\frac{L}{X} +(\\frac{1}{n}x^{\\frac{1}{n}-1})C_2x^{\\frac{1}{n}}S_3L$                                                                                                                                                           (4)\nThe function graph of total bridge cost is concave. When the derivative is 0, the total bridge cost is the lowest, that is:\n$C_1mx^{m-1}S_2L = (\\frac{L}{x^2})C_2x^{\\frac{1}{n}}S_3\\frac{L}{X} -(\\frac{1}{n}x^{\\frac{1}{n}-1})C_2x^{\\frac{1}{n}}S_3L$                                                                                                                                                           (5)\n$C_1mx^{m-1}S_2L = (1-\\frac{1}{n})C_2x^{\\frac{1}{n}-2}S_3L$                                                                                                                                                           (6)\n$\\frac{C_1X^{m+1}S_2}{n-1} = \\frac{C_2x^{\\frac{1}{n}} S_3}{mn}$                                                                                                                                                           (6)\nIn the formula:: $C_1XS_2$ is the cost of single span load-bearing structure.\nThat is, when the cost of single span load-bearing structure $=\\frac{n-1}{mn}$ the cost of single pier, it is the economic span.\n1.  2 Introduction to deep reinforcement learning\nReinforcement learning is a branch of machine learning. It addresses what kind of action an agent should take in an environment in order to maximize the reward. The environment has built-in rules for status update and reward, but these rules are a black box for agents. Agent can only interact with the environment through Trail-and-error method, and find the optimal action policy through the feedback of the environment. For example, in a bridge simulation environment, agent first randomly"}, {"title": "2 Economic span selection of bridge based on deep reinforcement learning", "content": "To solve practical engineering problems by reinforcement learning, we should first establish the environment, then establish the agent, and finally train and test.\nThis task is based on Python 3.10 programming language, OpenAI Gym 0.26.2 reinforcement learning toolkit, TensorFlow 2.10 and Keras2. 10 deep learning platform framework.\n2.  1 Self-built bridge simulation environment\nSince almost no self-built environment is involved in reinforcement learning papers and books on the market, a detailed process is given here.\n1.  the environment is a two-dimensional grid with 3 rows and 80 columns, and the coordinate origin is located in the upper left corner. The row represents the material category of load-bearing structure, and the rows 0, 1 and 2 from top to bottom are concrete structure, steel-concrete composite structure and steel structure in turn. Columns represent spans, and columns 0, 1, 2,..., 77, 78 and 79 from left to right are 10, 20, 30,..., 780, 790 and 800 meters in sequence. Each cell is distinguished by black and gray alternately. The cell where the agent is located is represented by red, and the number of observation spaces is 3 \u00d7 80 = 240.\n2.  the step() method is the core of the environment. It receives the action of the agent as a parameter and returns the state, reward and other information.\nIt first calculates the row and column coordinate and state after the action, and then calculates the reward of the action.\nSolving the optimal policy is the only task of reinforcement learning. The agent maximizes the reward by improving the policy, so the design of the reward function is the key. This task is to find the economic span of bridge, and the lowest total cost is the goal. Therefore, actions that can reduce the cost produce high reward, and actions that can increase the cost produce low reward, that is, the cost is negatively correlated with the reward. So the negative value of the total bridge cost is used as the reward.\nBased on formulas (1), (2), (3) and engineering experience, it is assumed that the calculation formula for the cost of superstructure and substructure per unit area of bridge is(Unit: Yuan per square meter): \u2460 concrete structure: the unit price of superstructure is $250+40x^2$, and the unit price of substructure is $50000x^{0.5}$. \u2461 Steel-concrete composite structure: the unit price of superstructure is $500+90x^{1.07}$, and the unit price of substructure is $45000x^{0.5}$. \u2462 Steel structure: the unit price of superstructure is $2000+140x^{1.0}$, and the unit price of substructure is $40000x^{0.5}$.\n(Note: Calculated manually according to formula (6), the economic span of concrete structure, steel-concrete composite structure and steel structure and the total cost of per unit area of the corresponding bridge are 39.6 m/11501 yuan per square meter, 32.3 m/12125 yuan per square meter and 27.3 m/13478 yuan per square meter respectively. Taking the lowest cost of the three, the economic span of the environment is 40m/concrete structure, corresponding to the cell in row 0 and column 3 of the environment, which is the destination of the agent. If the agent wanders in the environment and can always spend less steps to reach and stay at the destination, it means learning success.)\n3.  The reset() method is used to reset the environment and randomly assign an initial position (cell) to the agent. The state_to_grid() and grid_to_state() methods are used to convert the state index to the row and column coordinate.\n4.  the render() and gridarray_to_image() methods are used to render state images.\n5.  2 Construction and training of agent\nThe establishment and training of DQN agent is a routine operation. Reinforcement learning books on the market have detailed descriptions, so here is only a brief introduction. See the source code for relevant details.\n6.  Establish convolutional neural network\nThe input is the state image, and the output is the Q value of various actions executed in this state.\nThe convolutional base is stacked by two Conv2d layers, and the number of filters is 16 and 32 in turn. The sampling window size is 4 \u00d7 4, the stride size is 4, and the activation function is relu.\nThe classifier is stacked by two Dense layers, the number of neurons is 128 and 5, and the activation function is relu and linear.\n7.  Use greedy policy to select action: select random action with probability \u025b, and select action with maximum Q value with probability (1- \u025b).\n8.  Establish experience replay cache in queue mode to save the data of current state, action, return and next state.\n9.  Replay the data in the experience replay cache and train the convolutional neural network. Label is the total reward of environmental feedback, which is composed of immediate reward and next state value (bootstrap). The purpose of training is to make the predicted value of the model match the label. The loss function uses mean square error (MSE), and the optimizer uses \"Adam\". Finally, the convolutional neural network can output the correct Q value for each state.\n10. Testing\nQ value is the total reward of the action. A successfully trained agent can successfully achieve the goal by executing the action with the maximum Q value in each state. For this environment, no matter which cell the agent is initially assigned to, it should be able to quickly move to the cell in row 0 and column 3 (economic span)."}, {"title": "3 Conclusion", "content": "In this paper, derivative method for finding the extreme value of a function is used to deduce the theoretical solution formula of economic span.\nBy constructing bridge simulation environment and applying deep Q-network algorithm, the application of deep reinforcement learning in economic span selection of bridge is successfully realized. The result shows that the agent can quickly and effectively identify bridge span with the lowest cost through learning, which provides a new perspective for the field of bridge design, and also provides a useful exploration for the application of reinforcement learning in engineering problems.\nThere are some limitations in this attempt, such as the cost function, the change of bridge structure is not considered in bridge simulation environment, and training time of agent is too long, which can be further optimized in the future."}]}