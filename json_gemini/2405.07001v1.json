{"title": "Evaluating Task-based Effectiveness of MLLMs on Charts", "authors": ["Yifan Wu", "Lutao Yan", "Yuyu Luo", "Yunhai Wang", "Nan Tang"], "abstract": "The ability to automatically interpret and extract insights from charts can support various applications, including assisting individuals with low vision in capturing insights from charts. With advancements in multimodal large language models (MLLMs), particularly GPT-4V, there is potential to meet this demand. However, existing evaluations mainly focus on high-level chart understanding tasks, such as chart captioning, which overlook the low-level data analysis tasks (e.g., characterize distribution) that humans encounter in daily life. In this paper, we explore a forward-thinking question: Is GPT-4V effective at low-level data analysis tasks on charts? To this end, we first curate a large-scale dataset, named ChartInsights, consisting of 89,388 quartets (chart, task, question, answer) and covering 10 widely-used low-level data analysis tasks on 7 chart types. Firstly, we conduct systematic evaluations to understand the capabilities and limitations of 18 advanced MLLMs, which include 12 open-source models and 6 closed-source models. Starting with a standard textual prompt approach, the average accuracy rate across the 18 MLLMs is 36.17%. Among all the models, GPT-4V achieves the highest accuracy, reaching 56.13%. To understand the limitations of multimodal large models in low-level data analysis tasks, we have designed various experiments to conduct an in-depth test of GPT-4V's capabilities. We further investigate how visual modifications to charts, such as altering visual elements (e.g. changing color schemes) and introducing perturbations (e.g. adding image noise), affect GPT-4V's performance. Secondly, we present 12 experimental findings. These findings suggest GPT-4V's potential to revolutionize interaction with charts and uncover the gap between human analytic needs and GPT-4V's capabilities. Thirdly, we propose a novel textual prompt strategy, named Chain-of-Charts, tailored for low-level analysis tasks, which boosts model performance by 24.36%, resulting in an accuracy of 80.49%. Furthermore, by incorporating a visual prompt strategy that directs GPT-4V's attention to question-relevant visual elements, we further improve accuracy to 83.83%. Our study not only sheds light on the capabilities and limitations of GPT-4V in low-level data analysis tasks but also offers valuable insights for future research.", "sections": [{"title": "1 Introduction", "content": "Visualization can effectively convey data insights. However, due to the abundance of information a visualization may convey, it is not easy for users to accurately extract the desired information [1, 2]. Therefore, it becomes crucial to automatically help users pinpoint this information [3, 4, 5] based on their needs [6], a process known as chart question answering, or ChartQA for short.\nHigh-Level and Low-Level ChartQA Tasks. ChartQA can generally be categorized into two types: high-level tasks and low-level tasks. High-level tasks typically refer to broader questions such as chart captioning, chart-to-text conversion, etc., while low-level tasks involve more specific inquiries, such as identifying correlations, detecting anomalies, and so forth [3, 6, 7].\nTraditionally, ChartQA has been a challenging problem due to the limited capabilities in natural language understanding and the high complexity of chart reasoning. However, recent advancements in multi-modal large language models (MLLMs) have made it possible for users to interact with systems using natural language to extract specific information from data across various modalities. This progress has illuminated new possibilities for ChartQA on different levels of tasks.\nPrior Art: High-Level Tasks using MLLMs. Recently, several studies [8, 9, 10, 11, 12, 13, 14] have explored the capabilities of MLLMs in performing high-level ChartQA tasks. The findings reveal that state-of-the-art MLLMs, such as GPT-4V, have demonstrated promising results in addressing high-level tasks. These studies have also outlined a range of intriguing future research directions in this field.\nOur Focus: Low-Level Tasks using MLLMs. While the above efforts have concentrated on high-level ChartQA tasks, the effectiveness of MLLMs for low-level data analysis tasks remains underexplored. In this paper, we aim to systematically investigate the capabilities of GPT-4V in addressing 10 low-level data analysis tasks [3, 6]. Our study seeks to answer the following critical questions, shedding light on the potential of MLLMs in performing detailed, granular analyses.\n\u2022 Q1: Impact of Textual Prompt Variations. What is the impact of different textual prompts on GPT-4V 's output accuracy? This question aims to assess the baseline performance and capabilities of GPT-4V in different low-level tasks.\n\u2022 Q2: Impact of Visual Variations and Visual Prompts: How do different visual prompts, such as alterations in color schemes, layout configurations (e.g., aspect ratio), and image quality, affect the performance of GPT-4V in low-level tasks?"}, {"title": "Contributions", "content": "We make the following notable contribution.\n(1) ChartInsights Dataset. We curate a large-scale dataset, ChartInsights for evaluating the low-level data analysis tasks on charts. This dataset features a diverse array of visual variants, visual and textual prompts, and comprehensive metadata, which can be used to investigate the performance of MLLMs in the low-level ChartQA task from different scenarios.\n(2) Setting Benchmarks. Our study establishes benchmarks by evaluating GPT-4V, a state-of-the-art MLLM, on 10 low-level ChartQA tasks. This benchmarking effort provides valuable insights into the current capabilities of MLLMs in processing and analyzing chart information.\n(3) New Experimental Findings. We conduct a thorough analysis of the experimental results and identify 12 key findings. These insights emphasize the critical role of visual prompts, chart elements, and image quality in successfully performing low-level ChartQA tasks.\n(4) Chain-of-Charts. We introduce the Chain-of-Charts strategy, a new textual prompt designed to enhance the reasoning capabilities of GPT-4V in the context of ChartQA. The Chain-of-Charts leverages a series of interconnected question-answer pairs to guide the model.\nWe also share all our data and code to facilitate further research: https://anonymous.4open.science/r/ChartInsights-D43E"}, {"title": "2 Related Work", "content": "2.1 Low-Level Analysis Tasks on Charts\nVisualization charts offer numerous insights that aid users in performing data analysis tasks. Low-level data analysis tasks typically involve activities requiring direct interpretation and processing of specific visual elements within a chart, such as data retrieval, outlier identification, and correlation determination [3, 7, 15]. Amar et al. [3] identified ten low-level tasks, highlighting the real-world activities users undertake with visualization tools to understand their data. Subsequently, Saket et al. [6] evaluated the effectiveness of five basic charts across ten low-level analysis tasks using two datasets through a crowdsourced experiment. In this paper, we aim to evaluate how effectively GPT-4V can interpret charts by using these ten low-level data analysis tasks as a framework.\n2.2 Multimodal Large Language Models\nThe field of Multimodal Large Language Models (MLLMs) is experiencing rapid advancements, with efforts concentrated on developing artificial intelligence systems capable of processing and producing multi-modal content, including text, images, videos, and more. Early research such as CLIP [16] demonstrated the effective combination of visual and linguistic information through contrastive learning, while subsequent work like DALL-E [17] further showcased the potential of Transformer [18] architecture in generating images that match text descriptions. Building on these foundational successes, the research community has ventured into refining these models for diverse multi-modal applications, employing strategies like fine-tuning and prompt-based learning. For example, VisualGPT [19] and BLIP [20] have been adapted for Visual Question Answering (VQA) tasks, significantly enhancing their multi-modal task performance. Concurrently, the development of various benchmarks [21, 22, 23, 24, 25], including MME [26], has been crucial. These benchmarks provide a wide array of tasks and datasets, facilitating a comprehensive evaluation of MLLMs\u2019 abilities across different contexts. In this paper, we try to harness the off-the-shelfMLLMs for low-level data analysis tasks on charts."}, {"title": "2.3 MLLMs for Chart Question Answering", "content": "With the advancements in MLLMs, such as GPT-4V, it becomes increasingly promising to automatically comprehend charts and extract insights according to user queries [1, 2, 4, 27]. This process is known as chart question answering, i.e. ChartQA for short. Recent research efforts have focused on understanding the capabilities of MLLMs in performing ChartQA tasks. These studies can be categorized into two groups: evaluation studies and the construction of datasets for ChartQA.\nEvaluating MLLMs on ChartQA Tasks. Several recent studies [8, 9, 10, 11, 12, 13, 14] have attempted to leverage the capabilities of MLLMs to perform high-level ChartQA tasks such as chart captioning and chart-to-text. For example, Huang et al. evaluated the capabilities of representative MLLMs, such as GPT-4V and Bard (i.e. Gemini) [28], on chart captioning tasks. Their findings indicated that GPT-4V faces challenges in generating captions that accurately reflect the factual information presented in charts. Moreover, these studies have highlighted various promising directions for future research in this field. Diverging from the emphasis on high-level tasks in previous works, our research uniquely targets low-level ChartQA tasks [3, 6].\nChartQA Datasets. In the last decade, several ChartQA datasets have been presented [2, 8, 11, 12, 13, 29, 30, 31, 32], as shown in Section 3 Table 1. For example, ChartBench [11] includes 2.1K charts for four types of ChartQA tasks. However, a gap remains evident in the landscape of existing ChartQA datasets: none are tailored to comprehensively evaluate the 10 low-level tasks identified as critical to the ChartQA task. Moreover, to conduct more customized evaluations, such as modifying the visual elements or adding a visual prompt, we need access to the metadata (e.g. the underlying data) of the charts, not just the chart images. Therefore, we curate a large-scale dataset ChartInsights, which consists of a total of 89,388 quartets, each including a chart, a specified task, a corresponding query, and its answer."}, {"title": "3 ChartInsights for Low-level Analysis on Charts", "content": "In this section, we will first discuss the design goals for curating datasets for low-level tasks (Section 3.1). We will then provide details of constructing ChartInsights (Section 3.2). We will close this section by elaborating on the characteristics of ChartInsights (Section 3.3)."}, {"title": "3.1 Design Goals", "content": "G1: Supporting Low-level Data Analysis Tasks. Our first goal is to facilitate the support of 10 low-level data analysis tasks [3, 6]. This focus addresses a critical gap in existing ChartQA datasets, which often overlook the granularity required to fully understand and interact with the data presented in charts.\nG2: Evaluating Visual and Textual Variants on Charts. We highlight the critical role of visual variants (e.g. color, size, shape) in data visualizations, which are key to conveying and interpreting information effectively. Despite their importance, these variants are often neglected in existing ChartQA datasets and evaluations. Our goal is to address this issue by incorporating a diverse array of visual variants, including varying chart elements, image quality, and visual prompts. In addition, we also want to investigate the impact of different textual prompts on the low-level analysis task.\nG3: Making Metadata Available. The third goal tackles the prevalent issue of inaccessible data and metadata in current ChartQA datasets. By offering comprehensive access to each chart's metadata, such as the source data, chart type, and visual element specifics (like color schemes and labels), our dataset enhances analytical depth into chart design's impact on ChartQA performance."}, {"title": "3.2 ChartInsights Construction", "content": "To fulfill our three design goals, our construction process begins with the collection of charts with metadata from existing datasets. Next, we meticulously assign specific low-level data analysis tasks to appropriate chart types. Lastly, we develop diverse textual prompt strategies, along with visual"}, {"title": "Step 1: Candidate Charts Selection", "content": "In order to more comprehensively evaluate the ability of MLLMs on low-level data analysis tasks, and to conduct more detailed and extended experiments, the datasets (tabular data) and visualization charts we collected need to meet the following three requirements: First, these datasets should contain the original metadata of the chart such as the underlying data for rendering, allowing us to create customized reasoning tasks based on the metadata data. Second, the charts in these datasets should contain data labels, because the lack of data labels will greatly limit the types of low-level tasks. Third, these datasets should contain both simple and complex charts so that the difficulty of the charts is reasonable.\nAfter considering the above three aspects and the characteristics of existing datasets, we chose to extract charts and corresponding metadata from the two existing datasets, namely ChartQA [2] and nvBench [33]. Then, we get a total of 2K high-quality charts as well as their metadata as our initial dataset. The initial dataset contains a total of 7 types of charts, namely stacked bar charts, grouped bar charts, basic bar charts, line charts, grouped line charts, scatterplots, and pie charts."}, {"title": "Step 2: Low-level Tasks Generation", "content": "Next, we design a set of low-level tasks for the collected charts. We follow the approach of previous works on designing low-level tasks for charts [3, 6, 7], resulting in 10 low-level tasks in this paper, as shown in top of Figure 1. We group the 10 low-level tasks into three categories, namely Analysis, Search, and Query, based on their purpose and required reasoning abilities [7].\nNext, we should decide which tasks are applicable to which types of charts. We will follow the recommendations on the task-based effectiveness of humans to assign the tasks to each chart type [6]. Finally, we have 22,347 (chart, task, question, answer)."}, {"title": "Step 3: Textual Prompts Design", "content": "In order to better explore the impact of different prompting methods on GPT-4V. We have designed 4 textual prompt methods, namely Fill-in-the-Blank, Multiple-choice, Yes-or-No, and Error Correction prompts. 1) For Fill-in-the-Blank prompt, we maintain the asking method of the initial question and set the answer format for Fill-in-the-Blank prompt; 2) For Multiple-choice prompt, we still maintain the asking method of the initial question, but at this time we will provide a list of choices for GPT-4V, which usually contains one correct answer and two wrong answers, and tells GPT-4V to choose the answer from the options; 3) For Yes-or-No prompt, we first change the initial question to a true or false question and tell GPT-4V whether it needs to be answered correctly or Wrong; and 4) For Error Correction prompt, we put the wrong answer into the original question with a certain probability and change it into a statement.\nWe expand the above 4 textual prompt variants on the 22,347 (chart, task, question, answer), and thus produce 89,388 (chart, task, question, answer) at the end."}, {"title": "Step 4: Visual Variants Generation", "content": "Visual variants (e.g. color, size, shape) of a chart play a key role in delivering insights, but these variants are often overlooked in existing ChartQA datasets and evaluations, and thus we aim to bridge this gap. To this end, we vary the chart elements and add image noise to vary the chart quality."}, {"title": "Step 4.1: Varying the Chart Elements", "content": "As shown in Figure 3(a), we change the visual elements of these charts from four aspects, namely labels, chart scale, element color, and legend. To achieve this, we sample 5 charts from each category of charts as seeds, resulting in 35 charts. For varying labels, we enlarge, reduce, and remove the x-axis, y-axis, and data labels, respectively. For varying view sizes, we enlarge and reduce the chart, respectively. For varying element color, we change the elements in the chart to the same color or a higher contrast color; For varying legend, we first add marks to different types of categories, and then delete the colors. Finally, we generate 356 visual variants for 35 charts. These 356 visual variants (charts) are associated with 17,972 textual prompts and cover 10 low-level tasks."}, {"title": "Step 4.2: Varying the Image Quality", "content": "We add image noise, apply image blur, and adjust the brightness to vary the chart image quality, as shown in Figure 3(b). To achieve this, we sample 5 charts from each category of charts as seeds, resulting in 35 charts. For adding image noise, we choose Gaussian noise and salt and pepper noise; For applying image blue, we use median blur and Gaussian blur; For adjusting image brightness, we choose to make the brightness of the chart higher and lower. Finally, we generate 245 visual variants for 35 charts. These 245 visual variants (charts) are associated with 8,456 textual prompts and cover 10 low-level tasks."}, {"title": "Step 5: Visual Prompts Design", "content": "Kong et al. [34] presented five types of graphical overlays to enhance users' capabilities in performing data analysis tasks such as extraction and comparison of numerical values. Intuitively, we want to verify whether overlays would have a positive impact on GPT-4V's performance. Therefore, we design three types of visual prompts (i.e. graphical overlays) for the charts."}, {"title": "3.3 ChartInsights Characteristics", "content": "The ChartInsights dataset contains 89,388 (chart, task, query, answer) ChartQA samples across 7 chart types for 10 low-level data analysis tasks on charts. Figure 2(b) shows the proportion of 10 low-level tasks (3 task groups) in ChartInsights. Among them, the analysis task group is the largest, accounting for 42.46%. This task group examines the reasoning power of multi-modal large models on charts. Figure 2(c) shows the distribution of 10 low-level tasks and 7 chart types. Note that because we consider the effectiveness of different chart types for different tasks, for some tasks, we did not allocate chart types in Step 2."}, {"title": "Comparison with Existing Datasets", "content": "As shown in Table 1, our dataset differs from existing ChartQA datasets [2, 29, 30, 31] in three aspects. First, our dataset covers 10 low-level data analysis tasks across 7 chart types. Second, we have designed a variety of textual prompts, visual variants, and visual prompts for each chart type and task, enabling us to perform numerous ChartQA tasks from different perspectives to evaluate GPT-4V comprehensively. Third, we make all metadata relevant to the charts available, facilitating future research."}, {"title": "4 Experiments", "content": "In this section, we randomly selected 20% of the data from ChartInsights as the test set. The test set contains 17,552 (charts, tasks, questions, answers) samples, involving 400 charts, covering 7 chart types and 10 low-level tasks. The basic textual prompts in the test set is shown in 1 Q1. We first"}, {"title": "4.1 Textual Prompt Evaluation Models on ChartInsights", "content": "Experimental Settings As discussed in Section 3.3, we use 17,552 testing samples to evaluate MLLMs. Then, we analyze the answers of MLLMs, compare them with Ground Truth, and calculate the accuracy."}, {"title": "4.1.1 Overall Evaluation on Various MLLMS", "content": "In Table 2 and 3, we found that among the 18 models, the performances of closed-source models are far superior to those of open-source models, and the average accuracy rate of these 18 models is calculated to be 38.25%. Among them, VisCPM [35]has the worst performance at 26.19%, while GPT-4V has the best performance at 56.13%.In all models, GPT-4V achieves its best performance in seven out of 10 tasks. We believe this is because although some open-source models may outperform current cutting-edge proprietary models such as GPT-4V or Gemini-Pro [28] on certain specific tasks after being fine-tuned on particular datasets, on more general multimodal datasets, closed-source models like GPT-4V still possess strong generalization capabilities and show clear advantages in aspects such as logical reasoning."}, {"title": "4.1.2 In-depth Evaluation of Textual Prompt Variations on GPT-4V", "content": "In this set of experiments, we aim to achieve two main goals: First, to benchmark GPT-4V's performance across 10 low-level ChartQA tasks. Second, to investigate the impact of 4 types of Textual Prompt strategies on GPT-4V.\nOverall Results. Figure 6 displays GPT-4V's performance across a range of chart types and task categories, highlighting a notably low overall accuracy within the Analysis task category. Specifically, GPT-4V shows its worst performance on stacked bar charts, with a mere average accuracy of 19.8%. Conversely, its strongest performance is observed in the Query task category, particularly with scatter plots, where it achieves an impressive accuracy of 89.8%.\nThe disparity in GPT-4V's experimental results can be attributed primarily to the nature of the tasks within each category. The Analysis task category includes a range of data analysis tasks that require complex reasoning, calculations, determination of correlations, understanding of data distributions, and identification of anomalies. In contrast, the Query task category involves simpler tasks, such as acquiring specific data values, which are inherently less complex."}, {"title": "4.2 The Impact of Visual Variations and Visual Prompts", "content": "Most ChartQA evaluations [8, 9, 10, 11, 12] focus on the impact of Textual Prompt, failing to consider the chart's quality and the Visual Prompts. Therefore, our research aims to explore how visual variations and visual prompts influence GPT-4V's performance."}, {"title": "4.2.1 Varying Chart Elements", "content": "A visualization chart can exhibit visual differences by varying its elements, as shown in Figure 3(a). Intuitively, these visual differences are likely to influence GPT-4V's performance on low-level"}, {"title": "4.2.2 Varying Image Quality", "content": "In addition to how visual modifications to chart elements alter the order and manner in which we interpret charts, the quality of chart images also plays a critical role in humans' comprehension of these visual representations. Thus, it raises a compelling question: do these factors, known to have varying degrees of negative impact on human understanding, similarly impede GPT-4V's ability to decipher charts?\nExperimental Settings. The settings for varying chart elements is illustrated in Figure 3(b). We introduce six types of noise to evaluate the robustness and reliability of GPT-4V in low-level ChartQA tasks. As shown in Figure 2(a)-Step 4, we use the 245 visual variants for 35 charts as the testing samples. These 245 visual variants (charts) are associated with 8,456 textual prompts and cover 10 low-level tasks.\nOverall Results. We calculate the overall accuracy of GPT-4V and compare it with the results in Tables 4 and 5 to record the change in performance. Figure 8 reports the experimental results. Generally, six methods of degrading image quality tend to negatively impact GPT-4V across a broad range of tasks and chart types. Among these, Median Blur stands out as the most detrimental, causing an average performance decline of 14.8%. We consider that median blurring makes numerical labels unreadable, resulting in a significant decrease in the performance of tasks directly related to numerical values.\nInterestingly, adjustments to brightness\u2014both increasing and decreasing-show a positive effect on the majority of tasks, with an average improvement of 0.6% and 1.4%, respectively.\nAs shown in Figure 8(b), the distribution task presents a unique case; it is adversely affected solely by Median Blur, whereas other forms of image quality manipulation tend to improve its performance."}, {"title": "4.2.3 The Impact of Visual Prompts", "content": "In the field of computer vision, many studies aim to enhance models' semantic extraction and entity recognition in images through the use of visual prompts [51]. Unlike general VQA (visual question answering) tasks, the ChartQA task, especially for our low-level data analysis tasks, demands greater sensitivity to detail and higher accuracy from the model. Therefore, we design different visual prompts for different low-level tasks to help GPT-4V better adapt to the requirements of different low-level tasks."}, {"title": "4.3 The Impact of Chain-of-Charts", "content": "The Chain-of-Thought (CoT) prompt strategy has proven effective across various scenarios [52]. The key idea of CoT is to guide the model towards producing outputs that are more coherent and logical, by mimicking the step-by-step reasoning process humans employ to solve problems. Recently, Xu et al. [11] implemented the CoT strategy for ChartQA tasks, namely ChartCoT. The key idea is to pose a series of questions to progressively guide the model in comprehending the chart's details before it formulates an answer. However, ChartCoT struggles to ensure the accuracy of GPT-4V's responses to guiding questions, particularly with complex charts.\nChain-of-Charts Prompts. Therefore, we introduce a novel prompting strategy, termed Chain-of-Charts, which builds on the chain-of-thought approach, as shown in Figure 1-Q3. The core of Chain-of-Charts lies in orchestrating a sequence of questions and their corresponding answers ((91, a1), (q2, a2),...(qm, am)) to progressively guide the model towards a deeper understanding of the chart's details, thereby enhancing its ability to formulate accurate responses.\nExperimental Settings. The testing samples for this set of experiments are the same as those used for evaluating visual prompts. In addition to Chain-of-Charts, we also evaluate two useful textual prompt strategies: Role-Play [53] and Tutorial."}, {"title": "4.4 The Synergistic Effect of Visual and Textual Prompts", "content": "Yet, their effectiveness wanes in complex reasoning tasks such as reasoning, anomaly detection, and sorting, as shown in Figure 5(b). Conversely, the Chain-of-Charts Prompt excels in these reasoning tasks but is less effective in the Correlation task, as indicated in Figure 5(c). This observation leads us to wonder: Could combining visual prompts with Chain-of-Charts enhance GPT-4V's performance across a spectrum of low-level ChartQA tasks?\nExperimental Settings. The test samples are the same as those used for evaluating visual prompts and Chain-of-Charts."}, {"title": "5 Lessons Learned", "content": "Effectiveness of Textual Prompts, Visual Prompts and Their Combination. Incorporating various prompt strategies, including textual and visual prompts, significantly impacts GPT-4V's accuracy. Textual prompts with structured candidate answers enhance reasoning capabilities, while visual prompts enhance the chart understanding through visual attention, particularly in anomaly detection and filtering tasks.\nImportance of Chart Elements and Image Quality. Alterations in chart elements and the quality of chart images influence GPT-4V's performance. Specifically, certain modifications like larger labels or the absence of data labels can improve the model's efficiency in specific tasks by focusing its attention on visual comparisons. However, image quality degradation, especially median blurring, negatively affects the model's ability to process numerical values accurately.\nGPT-4V's Strengths and Weaknesses in Low-level ChartQA Tasks. GPT-4V performs well in tasks requiring direct data retrieval and basic comparisons, showing high accuracy in Query and Search task categories. However, it faces challenges in more complex reasoning, anomaly detection, and correlation tasks, indicating a need for further optimization of prompting strategies and model training to overcome these limitations."}, {"title": "Potential for Future Application and Development", "content": "The experiments demonstrate a promising direction for enhancing MLLMs' performance in visual data analysis through the development of specialized prompting strategies and the careful manipulation of visual elements."}, {"title": "6 Limitations and Future Work", "content": "Limited Chart Types. In our experiments, we set benchmarks for the performance of GPT-4V across seven widely used chart types, providing valuable insights into the model's capabilities in chart interpretation. However, this focus inherently excludes a range of more complex chart types, such as heatmaps, radar charts, and others, which present unique analytical challenges and opportunities for data representation.\nTherefore, including a more diverse chart type, especially those with complex structure and interpretation such as heat maps and radar charts, will provide a more comprehensive perspective on ChartQA for MLLM. This extension is critical for assessing the adaptability and effectiveness of MLLM in a wider range of graph interpretation tasks.\nLimited Visual Prompts Design Space. Our exploration into the effectiveness of visual prompts in facilitating ChartQA tasks with GPT-4V has shown their potential to enhance model performance. Nevertheless, our investigation into the design space of visual prompts has been preliminary, lacking a comprehensive and systematic exploration of the full spectrum of visual prompt possibilities. This limitation narrows the scope of our findings and potentially overlooks more effective visual prompt strategies that could further improve the accuracy and efficiency of MLLMs in interpreting and analyzing charts.\nFuture research can systematically explore the design space of visual prompts tailored to various ChartQA tasks and chart types. An interesting direction is developing algorithms that can automatically generate visual prompts from given textual prompts, specific to ChartQA tasks and chart types. This would ensure that the prompts are accurately customized to improve both model interpretability and task performance.\nLacking of Considering the Data Prompts. Our approach primarily relied on chart images, neglecting the underlying data that generated these charts. This omission could hinder the model's ability to perform more complex analysis and reasoning based on the actual data points. Future work could explore integrating the underlying data as part of the prompt, potentially through multimodal inputs, to provide a richer context for the model's analyses.\nWithout Fine-tuning MLLMs. We only use the \"off-the-shelf\" GPT-4V to conduct evaluation, without considering other MLLMs because GPT-4V is known as one of the best models in the visual question-answering task. In addition, we don't perform task-specific fine-tuning because we want to benchmark GPT-4V in low-level tasks and investigate the impact of textual and visual prompts, which is orthogonal to fine-tuning the MLLMs. Future work can fine-tune MLLMs using our dataset to investigate their effectiveness.\nTherefore, a promising direction is to develop a framework that includes self-correction [54], debugging, or a multi-agent approach [55]-with specialized agents for data analysis, chart understanding, and textual reasoning\u2014that could enhance the model's accuracy and reliability in ChartQA tasks."}, {"title": "7 Conclusion", "content": "In this paper, we conduct a thorough evaluation to assess the performance of GPT-4V across 10 low-level ChartQA tasks. Our approach begins with the development of ChartInsights, a large-scale dataset designed to test the capabilities of MLLMs in handling various low-level ChartQA tasks across seven commonly used chart types. We benchmark GPT-4V's performance against a range of textual prompts, chart types, and specific ChartQA tasks. We further explore the impact of visual variants and visual prompts on performance, demonstrating the importance of chart quality and visual attention. To further improve the reasoning abilities of GPT-4V, we present a novel Chain-of-Charts prompt to guide the model with a series of interconnected question-answer pairs. We further enhance the effectiveness of Chain-of-Charts by integrating the visual prompts, which achieves the"}]}