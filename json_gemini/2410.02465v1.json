{"title": "RESPONSE TUNING: ALIGNING LARGE LANGUAGE MODELS WITHOUT INSTRUCTION", "authors": ["Seokhyun An", "Hyounghun Kim"], "abstract": "Instruction tuning\u2014supervised fine-tuning using instruction-response pairs is a foundational step in transitioning pre-trained Large Language Models (LLMs) into helpful and safe chat assistants. Our hypothesis is that establishing an adequate output space can enable such a transition given the capabilities inherent in pre-trained LLMs. To verify this, we propose Response Tuning (RT), which eliminates the instruction-conditioning step in instruction tuning and solely focuses on response space supervision. Our experiments demonstrate that RT models, trained only using responses, can effectively respond to a wide range of instructions and exhibit helpfulness comparable to that of their instruction-tuned counterparts. Furthermore, we observe that controlling the training response distribution can significantly improve their user preference or elicit target behaviors such as refusing assistance for unsafe queries. Our findings illuminate the role of establishing an adequate output space in alignment, highlighting the potential of the extensive inherent capabilities of pre-trained LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) are pre-trained to predict the next token using massive amounts of web-crawled text, implicitly learning a wide range of tasks (Radford et al., 2019; OpenAI, 2023; Dubey et al., 2024). To align them with human needs, LLMs typically undergo Instruction Tuning (IT) (Mishra et al., 2022; Wei et al., 2022a; Sanh et al., 2022)\u2014a supervised fine-tuning process using instruction-response paired data. This conditional supervision enables them to follow natural language instructions and responsibly handle unsafe queries, facilitating their real-world applications (Wang et al., 2023a;b; Ivison et al., 2023; OpenAI, 2023; Xu et al., 2024; Zhou et al., 2024; Bianchi et al., 2024; Dubey et al., 2024).\nWe note that LLMs might acquire abilities such as instruction-following and safety assessment during pre-training, as well as the extensive knowledge required to perform specific tasks (Radford et al., 2019; Brown et al., 2020; Zhou et al., 2024). Moreover, previous observations suggest that output space information plays a crucial role in canonical NLP tasks (Min et al., 2022; Kung & Peng, 2023). Our hypothesis is that establishing an adequate output space can surface these capabilities in the form of responses, enabling pre-trained LLMs to function as desired chat assistants.\nIn this study, we explore the impact of establishing response space in transforming pre-trained LLMs into helpful and safe assistants. To this end, we propose Response Tuning (RT), which focuses solely on response space supervision. Specifically, RT omits the instruction-conditioning process and the model is not supervised from instruction-response mappings (see Figure 1). Instead, it trains the model to construct responses and learn their distribution. This omission allows us to examine the isolated effect of response supervision.\nWe first investigate whether establishing an output space alone can enable LLMs to generate aligned responses without additional supervision from instruction-response mappings. To this end, we evaluate the instructability of RT models trained on four recent LLMs using three different datasets:\nLlama-3.1-8B (Dubey et al., 2024), Gemma-2-2B and Gemma-2-9B (Riviere et al., 2024), and\nMistral-7B-v0.3 (Jiang et al., 2023), utilizing only the responses from Alpaca (Taori et al., 2023),"}, {"title": "2 RELATED WORK", "content": "Instruction tuning. Instruction Tuning (IT) is a process of supervised fine-tuning LLMs using instruction-response pairs, where the model is trained to produce responses (assistant outputs) conditioned on paired instructions (user inputs). Recent studies, expanding from earlier research that focused on cross-task generalization across canonical NLP tasks (Mishra et al., 2022; Wei et al., 2022a; Sanh et al., 2022), have shifted their focus to generalization for unseen user instructions, demonstrating remarkable success in improving the usability of LLMs. Notable contributions include synthetic data generation frameworks (Wang et al., 2023b; Honovich et al., 2023; Ding et al., 2023; Xu et al., 2024), human-involved conversation collection schemes (Conover et al., 2023; K\u00f6pf et al., 2023; Chiang et al., 2023; Zhou et al., 2024; Zhao et al., 2024; Zheng et al., 2024), and efficient training techniques (Zhou et al., 2024; Lin et al., 2024; Chen et al., 2024; Liu et al., 2024). However, few controlled studies have investigated the impact of specific learning signals on producing instruction-following assistants.\nLarge Language Model safety. Instruction-following LLMs are susceptible to malicious use without proper safeguards, and their risks become more nuanced and pronounced as their capabilities grow (Bommasani et al., 2021; Hendrycks et al., 2023; Kang et al., 2023). This has motivated studies on safety measures to mitigate their potential risks in real-world applications (Perez et al., 2022; Bai et al., 2022b; Dubey et al., 2024; Mu et al., 2024). These measures typically aim to make the model refuse to answer unsafe queries such as requests for assistance in committing illegal activities\u2014while retaining their helpfulness in appropriate contexts. A common practice to achieve this is incorporating unsafe instruction-refusal response pairs into the IT stage (Touvron et al., 2023; Bianchi et al., 2024; Dubey et al., 2024). However, as with instruction-following alignment, it remains unclear which cues mainly elicit the desired behaviors.\nSupervision from input-output pairs. Min et al. (2022) show that the input-output mapping matters significantly less than the label space information in the in-context demonstrations of canonical NLP tasks (e.g., classification). Taking a step further, later works examine the role of task definitions given in the prompts of the IT dataset and demonstrate that models trained with minimal (only label space) or misleading task definitions have little effect on their task generalization performance (Kung & Peng, 2023). However, those findings are limited to conventional label spaces, which correspond to predefined categories or outputs expected in given NLP tasks (e.g., yes/no, positive/negative), rather than the open-ended output space in open-domain IT.\nOur approach. To address these gaps, we aim to examine the impact of supervision from the response distribution in alignment. Specifically, we focus on open-domain, open-ended IT\u2014a widely adopted practice in aligning both open-source and proprietary LLMs (Wang et al., 2023a; K\u00f6pf et al., 2023; Jiang et al., 2023; OpenAI, 2023; Dubey et al., 2024; Riviere et al., 2024). Our focus lies on two foundational objectives of LLM alignment: transforming pre-trained LLMs into instruction-following and safe assistants."}, {"title": "3 RESPONSE TUNING (RT)", "content": "To verify our hypothesis, we propose Response Tuning (RT) to examine the impact of establishing a response space in alignment.\nTraining data. We adopt the chatbot-style schema proposed by Wang et al. (2023a), which separates user instructions and assistant responses using special tokens: <|user|> and <|assistant|>. However, in RT, we omit both the <|user|> token and the user instructions during training. Therefore, the training data consists only of the <assistant |> token followed by the training response.\nTraining. RT employs standard teacher forcing and computes the loss only on the response tokens that appear after the <assistant |> token. The loss function for the autoregressive language model is defined as:\n$L = \\sum \\delta_i \\log p_\\theta(t_i | t_{<i})$\n$\\delta_i = \\begin{cases} 1 & \\text{if } t \\in R \\\\ 0 & \\text{otherwise} \\end{cases}$"}, {"title": "4 INSTRUCTABILITY OF RT MODELS", "content": "In this section, we evaluate the instructability of RT models. We first assess their open-ended instruction-following capabilities to determine whether they can handle typical user queries in real-world applications (e.g., creative writing, trip planning, and general question-answering). Additionally, we test their core capabilities needed to perform instructed tasks using multiple benchmarks. For reliable verification, we conduct both human and automatic evaluations."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Pre-trained LLMs. We use four recent open-source LLMs: Llama-3.1-8B (Dubey et al., 2024),\nGemma-2-2B and Gemma-2-9B (Riviere et al., 2024), and Mistral-7B-v0.3 (Jiang et al., 2023).\nThese models are widely adopted due to their high performance, steerability, and efficiency. In this section, we mainly report results for Llama-3.1-8B and Gemma-2-9B; results for the other models are provided in Appendix C.\nTraining dataset. To account for potential dataset dependency of RT, we use three different IT datasets, from which we only use the response subsets:\n\u2022 Alpaca (Taori et al., 2023): A dataset of 52,000 instruction-response pairs generated through the\nSelf-Instruct(Wang et al., 2023b) framework. We use its cleaned version, which fixes issues in the original dataset with the updated GPT-4 (OpenAI, 2023) generation backbone.\n\u2022 Dolly (Conover et al., 2023): A dataset of 15,000 instruction-response pairs manually crafted by human annotators.\n\u2022 LIMA (Zhou et al., 2024): A dataset of 1,000 instruction-response pairs curated from various sources, including web data from Stack Exchange, wikiHow, and Reddit, as well as examples manually written by the authors and sourced from Super-NaturalInstructions (Wang et al., 2022).\nTraining setup. We use a parameter-efficient fine-tuning method, QLORA (Dettmers et al., 2024), which has been shown to match the performance of full 16-bit fine-tuning while significantly reducing memory footprint. LoRA adapters (Hu et al., 2022) are applied to all linear layers and are double-quantized in 4-bit NormalFloat during training. We set the alpha, rank, and dropout rate of the adapters to 16, 64, and 0.1, respectively. A 32-bit paged AdamW optimizer (Dettmers et al., 2024) is used with a batch size of 64 and a constant learning rate of 1e-4 (Wang et al., 2022; Wei et al., 2022a). Models are trained for 10 epochs with a maximum token length of 2,048 using NVIDIA A6000 (48GB VRAM) or A100 (80GB VRAM) GPUs. We use vLLM with greedy decoding for generation (Kwon et al., 2023). This setup is applied to all experiments unless otherwise specified.\nInstructabiltiy evaluation. To determine whether RT models can handle user instructions, we independently assess their responses' acceptability and quality. Additionally, we conduct pairwise assessments to measure their preference compared to their IT counterparts."}, {"title": "4.2 RESULTS", "content": "Establishing a response space alone enables LLMs to behave as chat assistants. Figures 2, 3\nand 4 depict the human acceptability, response quality, and pairwise helpfulness assessment results,\nrespectively. The independent assessment results indicate that RT models, trained only using re-\nsponses without paired instructions, are capable of generating appropriate responses to a wide range\nof instructions. A significant majority of the responses generated by RT models are rated as Accept-\nable, with many achieving the Excellent rating. They perform comparably to their IT counterparts\nin response quality evaluations, achieving similar scores across all metrics of the JustEval bench-\nmark. The pairwise evaluation results also show that RT models exhibit competent, though slightly\ninferior, preference compared to their IT counterparts. This trend remains consistent across four dif-\nferent base LLMs and three different datasets. These findings suggest that the instruction-response\ndynamics are largely acquired during pre-training, and just establishing a response space can elicit\nthose dynamics to generate aligned responses. The examples of the model outputs are available in Appendix E.\nRT models exhibit core capabilities on par with IT models. Table 1 displays the core capabilities\nevaluation results. While there exist some fluctuations due to the nature of fine-tuning (Fu et al.,\n2024), we find no significant differences between the performance of RT and IT models across all\ntasks tested. These results indicate that RT models can comprehend inputs, adhere to constraints\nto yield answers in a zero-shot setting, and possess core capabilities comparable to IT models.\nThese core capabilities that RT models retain from pre-trained models can serve as the foundation\nin assisting users with diverse requests. Additionally, it supports previous findings that input-output\nmappings do not significantly impact end performance and that most knowledge required for specific\ntasks is acquired during pre-training (Kung & Peng, 2023; Zhou et al., 2024).\nInstruction-following capabilities are better internalized in stronger LLMs. Table 2 shows\nthe evaluation results for RT Gemma-2-2B and Gemma-2-9B models. While smaller models like\nGemma-2-2B generate highly acceptable responses, larger models such as Gemma-2-9B, which are\npre-trained on significantly more tokens (2T vs. 8T tokens), exhibit higher human acceptance rates\nand preferences against IT counterparts. This suggests that stronger models are better at learning\ninstruction-following capabilities during pre-training."}, {"title": "5 REFINING RESPONSE DISTRIBUTION FOR PREFERENCE ALIGNMENT", "content": "5.1 REFINING RESPONSE ATTRIBUTES\nMotivation. Our preceding experiments demonstrate that establishing a response space alone can enable LLMs to behave as instruction-following assistants. In this section, we investigate whether refining the training response distribution can further enhance user preference, which is one of the essential objectives of LLM alignment. While previous works have shown that techniques like feedback learning or completely regenerating responses in the IT dataset using other aligned LLMs can improve user preference (Bai et al., 2022a; Peng et al., 2023; Ivison et al., 2024), we are interested in verifying whether adjusting the inherent attributes of existing response distributions in IT or RT data can yield similar benefits.\n5.2 EXPERIMENTAL SETUP\nResponse refinement. We focus on refining the inherent attributes of the response space, specifically considering three core elements that correlate with user preference: clarity, structure, and tone. For the refinement, we utilize a strong instruction-following LLM, Llama-3.1-70B-Instruct (Dubey et al., 2024), with a manually designed refinement prompt. The responses from the Alpaca, Dolly, and LIMA datasets are refined according to the following guidelines:\n\u2022 Clarity: Make the response easy to understand. It should be direct and to the point, avoiding complex language that might confuse the user\n\u2022 Structure: Organize the content in a logical and coherent manner. The response should flow naturally, making it easy for the user to follow along and grasp the key points.\n\u2022 Tone: Adjust the tone to be friendly, conversational, and engaging. The response should feel approachable and enjoyable, as if having a pleasant conversation with the user.\nThe prompt and the example of refined data are available in Appendix A and D.\nEvaluation. We conduct simulated preference evaluations using the GPT-4 judge of AlpacaEval (Li et al., 2023) and report length-controlled win rates (Dubois et al., 2024). To directly measure the gains from response space refinement, we compare both the IT and RT models trained using the refined datasets with those trained on the original datasets.\n5.3 RESULTS\nRefined response distribution yields models with better user preference. As shown in Table 3, the pairwise evaluation results indicate that both IT and RT models trained with the refined responses significantly outperform their counterparts trained on the original responses. The improvements are consistent across different base models and datasets, except for Alpaca. This exception may be due to Alpaca's responses having limited room for improvement, as they are generated using GPT-4, a highly-aligned model using human feedback. These findings suggest that the training response"}, {"title": "6 EMBEDDING BEHAVIORAL GUIDANCE IN RESPONSE SPACE FOR SAFETY\nALIGNMENT", "content": "Motivation. Similar to instruction-following alignment, recent work demonstrates that incorporating only a small set of safety-focused examples unsafe queries paired with refusal responses into IT data can enable models to reject unsafe inputs (Bianchi et al., 2024; Zhou et al., 2024). On the other hand, our previous experiments have shown the abilities like instruction-following are also acquired during pre-training. Based on these observations, we hypothesize that the risk assessment capabilities required to evaluate the risk of queries are also acquired during pre-training. Therefore, we investigate whether properly establishing the response space\u2014by including refusal responses that demonstrate how to handle unsafe queries\u2014can induce responsible behaviors even without using paired data.\n6.1 EXPERIMENTAL SETUP\nResponse tuning with refusals. We incorporate contextual refusals\u2014which express an inability to assist along with a reason explaining how the model should handle unsafe queries\u2014into the response subsets of Alpaca, Dolly, and LIMA datasets. These refusals are obtained from existing safety-focused pairs using pattern matching (Bianchi et al., 2024). To control their proportion, we add 100, 200, and 500 examples into randomly sampled sets of 1,0003 responses from the base RT dataset."}, {"title": "6.2 RESULTS", "content": "Response supervision alone can elicit responsible behaviors. Figure 5 shows the evaluation results for the Gemma-2-9B model trained based on LIMA. The results for the other models and datasets are available in Appendix C. Notably, RT models trained with refusals exhibit substantially higher RR compared to those trained without refusals, indicating that they are able to handle unsafe queries as suggested in training responses. We also find that their FRR falls within a reasonable range. Although they require more data to achieve a safety level close to IT counterparts, their refusal rates are largely on par with IT counterparts that are additionally supervised from mappings between unsafe queries and refusals. These results indicate that complex capabilities, such as safety assessment, are largely acquired during pre-training. LLMs can generalize the guidance for handling queries embedded in the training response distribution even without supervision from explicit instruction-response mappings.\nThe gap in refusal rates between RT and IT models shrinks with model scale. Our experiments using Gemma-2-9B and Gemma-2-2B models reveal that the difference in safety performance between RT and IT models diminishes as the size of the base model increases. While smaller models like Gemma-2-2B show a noticeable gap\u2014with IT models outperforming RT models in RR\u2014larger models such as Llama-3.1-8B and Gemma-2-9B exhibit minimal differences. In some cases, the larger RT models achieve refusal rates comparable to their IT counterparts. This suggests that larger models have a greater capacity to internalize and generalize safety behaviors from limited response supervision. Please refer to Appendix C for the detailed numerical report."}, {"title": "7 IN-CONTEXT RESPONSE LEARNING", "content": "Motivation. We further validate our hypothesis\u2014that establishing an appropriate output space alone can enable LLMs to behave as desired chat assistants\u2014in an in-context learning setting. To this end, we test whether untuned base LLMs can helpfully and safely respond to user queries given only response demonstrations.\nExperimental setup. We remove instruction-response mappings from URIAL (Lin et al., 2024), which consists of 4 instruction-response pairs including one pair of unsafe instruction and refusal. We refer to this new version as URIAL-R. We then evaluate these models using two different base"}, {"title": "8 CONCLUSION", "content": "In this paper, we explore the role of establishing an output space in transforming pre-trained LLMs into instruction-following and safe assistants. We propose Response Tuning (RT), a method that focuses solely on response space supervision by eliminating the instruction-conditioning step in instruction tuning. Our extensive experiments demonstrate that RT models, trained only on responses without paired instructions, effectively respond to a wide range of user queries and responsibly handle unsafe requests by generalizing embedded behavioral guidance in the response space. These findings suggest that many of the capabilities required for alignment are already inherent in pre-trained models and can be activated by establishing a proper response space. Our work emphasizes the impact of output space supervision in alignment and highlights the potential of inherent capabilities in LLMs."}, {"title": "LIMITATIONS & FUTURE WORK", "content": "Our study focuses on the core objectives of LLM alignment\u2014instruction-following and safety. Future work could explore controlling response distribution to achieve more complex alignment objectives, such as mitigating sycophancy or social bias (Perez et al., 2023; Sharma et al., 2024). Additionally, our work relies on empirical results from the ablation study; direct investigations into the inherent capabilities of pre-trained LLMs, such as extracting their semantic features (Bricken et al., 2023; Templeton et al., 2024), could further clarify the role of the alignment stage and potentially improve efficiency. Moreover, future research might adopt analytical approaches for automatic selection or fine-grained control over the response distribution to achieve better alignment."}, {"title": "ETHICS STATEMENT", "content": "Our study involves human evaluations to evaluate instruction-following LLMs. The evaluators were hired in compliance with local laws and were paid appropriate compensation. The authors manually reviewed the LLM responses flagged by OpenAI moderation API and confirmed that these pose no harm to human evaluators. In addition, evaluators had the right to immediately stop the evaluation if they wished, and were encouraged to discuss any discomfort with the authors. This work is dedicated to understanding the LLMs safety alignment stage. While we publicly release the codes for safety evaluations, we decide not to release the refusal judge validation set to prevent potential misuse of unsafe or illegal information."}, {"title": "ACKNOWLEDGEMENTS", "content": "We thank the members of the UNIST Language and Intelligence Lab for their helpful comments.\nSeokhyun An is supported by the Korea Presidential Science Scholarship. This work was supported\nby grants from the Institute of Information & Communications Technology Planning & Evaluation\n(IITP), funded by the Ministry of Science and ICT (MSIT) of the Korea government: No.RS-2020-\nII201336 (Artificial Intelligence Graduate School Support Program at UNIST) and IITP-2024-RS-\n2024-00360227 (Leading Generative AI Human Resources Development). We also acknowledge\nthe 2022 Research Fund (1.220140.01) of UNIST."}, {"title": "APPENDIX", "content": "A EVALUATION SETUP\nA.1 HUMAN EVALUATION\nHuman participants. We employ three undergraduate students at a university where the official language is English. To prevent potential harm to the human evaluators, we manually review the LLM responses flagged by OpenAI Moderation API and confirm that these pose no harm to the human evaluators (400 out of 22,540 of the responses (1.77%) are flagged). Additionally, the human evaluators can stop the evaluation at any time and are encouraged to contact the authors immediately if they experience any discomfort.\nResponse acceptability evaluation. Table 4 and Figure 7 present the evaluation guidelines and annotation interface, respectively. Human raters are given two models' responses at once and are asked to rate each response independently by choosing one of three ratings: Acceptable (Excellent), Acceptable (Sufficient), or Not Acceptable. The order of the model responses is internally randomized at each turn to avoid potential evaluation bias.\nResponse preference evaluation. The preference evaluation is conducted simultaneously with the acceptability evaluation. Evaluators are instructed to choose the response they find more helpful. The annotation interface is shown in Figure 7.\nA.2 AUTOMATIC EVALUATIONS\nResponse quality evaluation. We use the test instructions and the LLM judge from the JustEval benchmark (Lin et al., 2024). For models without safeguards, we perform only the regular evaluation using 800 instructions. The evaluation conducted in Section 7 involves safety measures, so we also use the safety evaluation suite. The evaluation prompt can be found in their official implementation.4\nPairwise preference evaluation. We use the \u2018weighted_alpaca_eval_gpt4_turbo\u2019 judge from AlpacaEval 2.0 (Li et al., 2023) for the automatic preference evaluation and report length-controlled win rates (Dubois et al., 2024). The evaluation prompt can be found in their official repository.5\nCore capabilities evaluation. We measure the core capabilities of the models as follows:\n\u2022 MMLU (Hendrycks et al., 2021): We use the script from the open-instruct repository (Ivison et al., 2023) for evaluation. Exact-match accuracy is reported in a zero-shot setting.\n\u2022 OpenbookQA (Mihaylov et al., 2018): We evaluate using the Language Model Evaluation Harness (1m-eval) package (Gao et al., 2024), reporting exact-match accuracy in a zero-shot setting.\n\u2022 HellaSwag (Zellers et al., 2019): We evaluate with the 1m-eval package, measuring exact-match accuracy in a zero-shot setting.\n\u2022 ARC (Clark et al., 2018): We use the 1m-eval package to measure exact-match accuracy in a zero-shot setting.\n\u2022 GSM8K (Cobbe et al., 2021): We evaluate using the 1m-eval package. Following the setup of Dubey et al. (2024), we use 8-shot demonstrations in multi-turn chat format and report exact-match accuracy.\n\u2022 PIQA (Bisk et al., 2020): We use the 1m-eval package for evaluation, measuring exact-match accuracy in a zero-shot setting.\nSafety evaluation. We evaluate the safety of models by measuring the refusal rates for unsafe instructions and false refusal rates for benign instructions using multiple safety benchmarks. For"}, {"title": "B EXPERIMENTAL SETUP", "content": "Response refinement. We use Llama-3.1-70B-Instruct (Dubey et al., 2024) with our refinement prompt. This prompt can be found in Table 5.\nResponse in-context learning. The simplified template of URIAL (Lin et al., 2024), URIAL-R, and zero-shot template prompt used for the evaluation can be found in Tables 6, 7 and 8, respectively. We use urial1kv4 prompt in their official repository as a base URIAL prompt.7 Full version of URIAL-R prompt can be found in our code repository. The generation of LLM is truncated by the response marker of URIAL (```)."}, {"title": "C FULL EXPERIMENTAL RESULTS", "content": "The evaluation results are presented in the following tables or figures:\n\u2022 Human evaluation results for response acceptability: See Table 10.\n\u2022 Human evaluation results for model preference: See Figure 8.\n\u2022 Core capabilities evaluation results: See Table 11.\n\u2022 GPT-4 response quality evaluation results: See Table 12.\n\u2022 GPT-4 preference evaluation results: See Figure 9.\n\u2022 Preference evaluation results for models trained using refined responses: See Table 13.\n\u2022 Safety evaluation results: See Tables 14 and 15."}, {"title": "D DATA EXAMPLES", "content": "Examples of training data are presented in the following tables:\n\u2022 Refined responses: See Table 16.\n\u2022 Contextual refusals: See Table 17."}, {"title": "E MODEL OUTPUT EXAMPLES", "content": "Examples of responses generated by the IT and RT models are presented in the following tables:\n\u2022 Llama-3.1-8B (Dubey et al., 2024): See Table 18 and 19 (for the model trained using refined responses).\n\u2022 Gemma-2-9B (Riviere et al., 2024): See Table 20 and 21 (for the model trained with refusals).\n\u2022 Mistral-7B-v0.3 (Jiang et al., 2023): See Table 22.\n\u2022 Gemma-2-2B (Riviere et al., 2024): See Table 23."}]}