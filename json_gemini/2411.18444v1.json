{"title": "Is my Meeting Summary Good?\nEstimating Quality with a Multi-LLM Evaluator", "authors": ["Frederic Kirstein", "Terry Ruas", "Bela Gipp"], "abstract": "The quality of meeting summaries generated\nby natural language generation (NLG) systems\nis hard to measure automatically. Established\nmetrics such as ROUGE and BERTScore have\na relatively low correlation with human judg-\nments and fail to capture nuanced errors. Re-\ncent studies suggest using large language mod-\nels (LLMs), which have the benefit of better\ncontext understanding and adaption of error\ndefinitions without training on a large number\nof human preference judgments. However, cur-\nrent LLM-based evaluators risk masking errors\nand can only serve as a weak proxy, leaving hu-\nman evaluation the gold standard despite being\ncostly and hard to compare across studies. In\nthis work, we present MESA, an LLM-based\nframework employing a three-step assessment\nof individual error types, multi-agent discus-\nsion for decision refinement, and feedback-\nbased self-training to refine error definition un-\nderstanding and alignment with human judg-\nment. We show that MESA's components en-\nable thorough error detection, consistent rating,\nand adaptability to custom error guidelines. Us-\ning GPT-40 as its backbone, MESA achieves\nmid to high Point-Biserial correlation with hu-\nman judgment in error detection and mid Spear-\nman and Kendall correlation in reflecting er-\nror impact on summary quality, on average\n0.25 higher than previous methods. The frame-\nwork's flexibility in adapting to custom error\nguidelines makes it suitable for various tasks\nwith limited human-labeled data.", "sections": [{"title": "1 Introduction", "content": "Meeting summaries have become integral to pro-\nfessional environments (Zhong et al., 2021; Hu\net al., 2023; Laskar et al., 2023), serving as refer-\nences, updates for absentees, and reinforcements\nof key topics discussed. The integration of summa-\nrization services into established digital meeting\nplatforms (e.g., Zoom\u00b9, Microsoft Teams\u00b2, Google\nMeet\u00b3) further underscores their growing relevance.\nThe evaluation of generated summaries remains an\nongoing problem (Kirstein et al., 2024b) and is\ntypically solved through costly, time-consuming\nhuman assessment. Consequently, an automatic\nevaluator is necessary, which would, if providing\ninsights along the scoring, also enable sophisti-\ncated techniques such as feedback-based summary\nrefinement (Kirstein et al., 2024a) and reinforce-\nment learning from AI feedback (Lee et al., 2023).\nEstablished automatic metrics such as ROUGE\n(Lin, 2004), BERTScore (Zhang et al., 2020), and\nBARTScore (Yuan et al., 2021) exhibit a relatively\nlow correlation with human judgment. These count-\nand model-based metrics often fail to reliably de-\ntect errors, leading to error masking (Kirstein et al.,\n2024c), and lack sensitivity to error impact, result-\ning in inaccurate reflection of summary quality in\nscore (Kirstein et al., 2024a).\nRecently, Large language models (LLMs) have\nbeen proposed as evaluators for text summarization\n(Liu et al., 2023a,b; Wang et al., 2024), assigning\nLikert scores based on predefined guidelines. How-\never, these approaches face limitations in meeting\nsummarization contexts. Current annotation guide-\nlines do not cover typical errors in meeting sum-\nmaries, e.g., structure presentation, coreference is-\nsues (Kirstein et al., 2024c), resulting in oversight\nand insufficient quality assessment. Moreover, the\nsubjective nature of existing guidelines, e.g., 'in-\nformativeness' (Liu et al., 2023b) may lead to in-\nconsistent interpretations by LLMs, resulting in\nunreliable evaluations (Kirstein et al., 2024a).\nWe introduce the meeting summary assessor\n(MESA), a multi-stage LLM-based framework that\nmimics the human evaluation approach (see Fig-\nure 1). MESA operates on three levels: error-"}, {"title": "2 Methodology", "content": "Key weaknesses of meeting summarization evalu-\nators include error type confusion (Kirstein et al.,\n2024a), oversight of error instances (Kirstein et al.,\n2024c), and risk of self-inconsistency (Wei et al.,\n2024a). To address these, we develop MESA\nthrough comparative experiments between tra-\nditional approaches and promising alternatives.\nOur findings indicate that the most reliable, self-\nconsistent, and thorough setup combines error-type\nspecific single-aspect evaluators with multi-agent\ndiscussion in a three-stage scoring process (see\nFigure 1). Experiments use GPT4 backbones, gen-\nerating verbose confidence scores (0-10) (Geng\net al., 2024) and chain-of-thought (CoT) (Wei et al.,"}, {"title": "2.1 Error types and dataset", "content": "We assess the error types redundancy (RED), inco-\nherence (INC), language (LAN), omission (OM),\ncoreference (COR), hallucination (HAL), structure\n(STR), and irrelevance (IRR). The definitions (see\nAppendix C) are based on Kirstein et al. (2024a),\ncombining total and partial omission into one.\nWe use the QMSum Mistake dataset (Kirstein\net al., 2024a), comprising 170 samples from aca-\ndemic (ICSI (Janin et al., 2003)), business (AMI\n(Mccowan et al., 2005)), and parliament meetings,\nsummarized by language models (LED (Beltagy\net al., 2020), DialogLED (Zhong et al., 2022),\nPegasus-X (Phang et al., 2022), GPT-3.5, and Phi-3\n(Abdin et al., 2024)) and human-annotated for er-\nrors. Four annotators update the human annotation\nscores (Likert scale, 0 to 5) and reasoning traces\nto align with our modified definitions, following\nthe annotation process detailed in Appendix D.2.\nWe achieve a high inter-annotator agreement of\n0.793 (Krippendorff's alpha (Krippendorff, 1970),\ncomplete agreement stated in Appendix D.3), indi-\ncating strong reliability. Statistics on the QMSum\nMistake dataset are listed in Appendix D.1."}, {"title": "2.2 Challenge I: error type confusion", "content": "Error-type definitions are nuanced (Appendix C),\nrequiring careful consideration during detection.\nPrompting models to consider multiple error types\nsimultaneously (multi-aspect) risks definition con-\nfusion (Kamoi et al., 2024). Literature suggests\nrestricting detection to one error type at a time\n(single aspect), using multiple model instances for\ncomprehensive coverage (Kirstein et al., 2024a).\nSingle-aspect error-type assessment leads to a\nmore reliable and comprehensive evaluation.\nMulti-aspect approaches often assign uniform\nscores across error types, provide superficial rea-\nsoning (e.g., \"it misses details about decision mak-\ning\"), and occasionally confuse error definitions,\nleading to false detections. In contrast, single-\naspect approaches demonstrate a more thorough\nunderstanding of individual error types, identifying\na broader range of errors. However, the single-\naspect approach may become oversensitive, assign-\ning overly bad scores to minor errors, aligning with\nrecent findings (Kirstein et al., 2024a)."}, {"title": "2.3 Chalenge II: error instance oversight", "content": "A direct assessment of error types may miss crit-\nical instances, affecting scoring accuracy (Kamoi\net al., 2024). We propose a three-step evaluation\npipeline to address the risk of oversight and have\na more thorough assessment process consisting of\nidentifying potential error instances, rating the er-\nror severity for each instance, and assigning a score\nbased on the observations for the currently assessed\nerror type (see Figure 1). Each step is carried out\nby an LLM instance informed by the result of the\nprevious step.\nThree-step assessment offers more thorough er-\nror instance identification and sensitive scoring.\nComparing single-step and three-step evaluation\napproaches reveals notable improvements in error\ndetection and scoring with the three-step method.\nUsing the single-aspect setup as the backbone, we\nobserve that the three-step approach more effec-\ntively detects non-obvious error instances, such as\nparaphrased repetitions. Balanced accuracy scores\n(Table 1, definition in Appendix E) show an im-\nprovement in detecting all error types with an aver-\nage improvement of ~3.5% on average.\nHowever, this increased sensitivity and larger\nnumber of detections can lead to overly strict as-\nsessments, particularly for subjective error types\n(e.g., irrelevance). We conclude that the three-step\napproach offers a more comprehensive evaluation\nbut requires adjustment, e.g., through in-context\nsamples, to better align with human judgment.\nWhile offering more comprehensive evaluations,\nthe three-step approach requires fine-tuning, poten-\ntially through in-context samples, to better align\nwith human judgment."}, {"title": "2.4 Challenge III: inconsistent scoring", "content": "To address score fluctuations in LLM-based as-\nsessments (Wei et al., 2024a), we explore a multi-\nagent debate protocol (MADP) (Liang et al., 2023).\nIn MADP, different models (agents) collaborate\nthrough a natural language exchange to solve a"}, {"title": "2.5 Resulting MESA architecture", "content": "The derived MESAarchitecture combines single-\naspect, three-step evaluation using single-model\nMADP for thorough assessment. Individual error-\ntype Likert scores are combined using a weighted\nsum, following the idea of (Liu et al., 2023a):\n$$impact = \\frac{\\sum_{n} s_n (c_n i_n)}{\\sum_{n} (c_n i_n)}$$\nwhere $s_n$ is the Likert score, $c_n$ the scaled con-\nfidence score (0-1) reported by the LLM, and $i_n$\nan importance parameter (default: 1.0; OM, HAL,\nIRR: 1.1; REP, INC, LAN: 0.9). Errors such as OM,\nHAL, and IRR are prioritized as they significantly\naffect summary accuracy and introduce biases, un-\ndermining the summary's trustworthiness. REP,\nINC, and LAN primarily influence readability and\noccur less frequently in LLM-generated summaries\n(Kirstein et al., 2024c), warranting a slightly lower\nweight. The impact score, describing how large\nthe impact of all errors is on the summary quality\n(none: 0 to highly impacted: 5), is converted to a\nquality score (1 to 10) using:\n$$quality = 1 + (\\frac{5 - impact}{5}) * 9$$\nAn optional self-training mechanism inspired\nby self-teaching (Wang et al., 2024) and feedback\ntechniques (Kirstein et al., 2024a) is introduced to\naddress overly harsh scoring. This mechanism uses\nGPT4 as a judge (Zheng et al., 2024) to evaluate\nthe quality of the reasoning traces on completeness,\noverlap with human reasoning, and logic. For the\nscore differences, we report labels ranging from\n\"no difference\" to \"major difference\" for score dis-\ncrepancies, with \"critical disagreement\" for con-\nflicting error observations. A second GPT4 judge\nis tasked to detect patterns in the per-sample feed-\nback and provides a consolidated report for each\nerror type on what should be considered or treated\ndifferently during evaluation. This report is then\nused in the following three-step assessment, being\nappended to the original task describing prompt to\nsteer the detection and evaluation behavior."}, {"title": "3 Experiments", "content": "3.1 Setup\nWe compare MESA with established metrics using\nthe modified QMSum Mistake dataset and the eight\nerror types: omission (OM), repetition (REP), inco-\nherence (INC), coreference (COR), hallucination\n(HAL), language (LAN), structure (STR), and irrel-\nevance (IRR). We use the MESA setup described\nin Section 2.5 with and without MADP (Multi-n,\nSingle-n), with n iterations of self-training (0, 1).\nBaseline metrics include:\n\u2022 ROUGE (Lin, 2004), the most common,\ncount-based metric, assessing n-gram overlap\nbetween generated and reference summaries.\nWe report unigrams, bigrams, and the longest\ncommon sequence.\n\u2022 BERTScore (Zhang et al., 2020), a model-\nbased metric measuring the contextual sim-\nilarity between generated and reference texts,\nreflecting semantic and syntactic similarity.\nWe report the rescaled F score5.\n\u2022 A modified version of the LLM-based G-Eval-\n4 (Liu et al., 2023a) prompted with our eight\nevaluation criteria and access to the transcript.\n3.2 Analysis and discussion\nOur analysis focuses on three aspects of evalua-\ntion: error masking, sensitivity to error impact, and\ncloseness to human ratings. We conclude that the\nthree-stage detection in MESA demonstrates signif-\nicant improvements over the best current approach,\nG-Eval-4, showing the highest correlation with hu-\nman judgment on both pure error detection (avg.\ngap: 0.1) and error sensitivity (avg. gap: 0.15). The\nself-teaching loop further enhances MESA's perfor-\nmance, increasing correlation (avg. gap increase:"}, {"title": "4 Related Work", "content": "Meeting summarization evaluation faces sig-\nnificant challenges with traditional metrics like\nROUGE (Lin, 2004) and BERTScore (Zhang et al.,\n2020). These metrics correlate relatively poorly\nwith human judgment, potentially masking or\nrewarding certain error types (e.g., QuestEval\n(Scialom et al., 2021) favors missing information).\nLLM-generated summaries expose these limita-\ntions further, leading to minimal metric score dif-\nferences despite substantial qualitative variations\n(Kirstein et al., 2024a). Our work formalizes the\nerror-type focused evaluation concepts by Kirstein\net al. (2024a) into a thorough detection framework.\nLLMs as summary evaluators have shown\npromising results, with approaches like GPTScore\n(Fu et al., 2024), G-EVAL (Liu et al., 2023a), and\nself-taught evaluators (Wang et al., 2024) demon-\nstrating positive correlation with human judgments.\nFor meeting summarization specifically, single-\nevaluator metrics such as AUTOCALIBRATE (Liu\net al., 2023b) and FACTSCORE (Min et al., 2023)\nare recently explored but still lag in reliability and\nalignment with human judgment (Kirstein et al.,\n2024a). Persistent challenges include difficulty de-\ntecting specific error types (e.g., omission) and han-\ndling subjective assessments (Kirstein et al., 2024a).\nOur work continues research of LLM-based met-\nrics by further developing existing objective error\ndefinitions (Kirstein et al., 2024a), implementing\nan LLM-based single-aspect evaluator, and incor-\nporating a refinement process inspired by the self-\nteaching technique (Wang et al., 2024)."}, {"title": "5 Final Considerations", "content": "In this paper, we introduced MESA, an LLM-\nbased single-aspect evaluation framework for meet-\ning summarization using a three-step evaluation\npipeline and multi-agent discussion paradigm. We"}]}