{"title": "ReWind: Understanding Long Videos with Instructed Learnable Memory", "authors": ["Anxhelo Diko", "Tinghuai Wang", "Wassim Swaileh", "Shiyan Sun", "Ioannis Patras"], "abstract": "Vision-Language Models (VLMs) are crucial for applications requiring integrated understanding textual and visual information. However, existing VLMs struggle with long videos due to computational inefficiency, memory limitations, and difficulties in maintaining coherent understanding across extended sequences. To address these challenges, we introduce ReWind, a novel memory-based VLM designed for efficient long video understanding while preserving temporal fidelity. ReWind operates in a two-stage framework. In the first stage, ReWind maintains a dynamic learnable memory module with a novel read-perceive-write cycle that stores and updates instruction-relevant visual information as the video unfolds. This module utilizes learnable queries and cross-attentions between memory contents and the input stream, ensuring low memory requirements by scaling linearly with the number of tokens. In the second stage, we propose an adaptive frame selection mechanism guided by the memory content to identify instruction-relevant key moments. It enriches the memory representations with detailed spatial information by selecting a few high-resolution frames, which are then combined with the memory contents and fed into a Large Language Model (LLM) to generate the final answer. We empirically demonstrate ReWind's superior performance in visual question answering (VQA) and temporal grounding tasks, surpassing previous methods on long video benchmarks. Notably, ReWind achieves a +13% score gain and a +12% accuracy improvement on the MovieChat-1K VQA dataset and an +8% mIoU increase on Charades-STA for temporal grounding.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities at human language processing. However, these models are limited to text-based inputs and, therefore, oblivious to real-world, multi-sensory information. To address this limitation, researchers are actively developing Multimodal LLMs (MLLMs) capable of processing signals from multiple and diverse modalities, including images, video, and audio. This emerging field holds immense potential for applications such as visual question answering (VQA), real-time interfaces for autonomous agents, and generating detailed scene descriptions for the visually impaired.\nRecent research in MLLMs has predominantly concentrated on Vison-Language Models (VLMs), which typically combine pre-trained LLMs with visual encoders that encode and feed to them visual information. However, existing VLMs face two major challenges in processing long videos. First, their self-attention mechanisms require substantial memory that scales quadratically with the number of tokens, making long video processing computationally intensive. Second, these models struggle to effectively model temporal dependencies over extended sequences. To address these challenges, recent efforts have proposed using memory modules to enhance the capability of VLMs. However, current memory modules often serve as storage units and lack the ability to discern and retain information pertinent to the task or user instructions. Moreover, these models tend to compress temporal information heavily, sacrificing the fidelity of the temporal dynamics and overlooking critical details in the video's narrative. Additionally, current models rely on fixed dense spatial representations per frame, increasing memory requirements: by treating all frames equally, they store unnecessary details for non-essential moments, increasing memory demands and limiting the model's ability to focus on critical events for accurate video comprehension.\nTo address these long video challenges, we introduce"}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Short Video Understanding", "content": "Recent VLMs have explored various architectural approaches for video understanding. Dual-stream architectures, exemplified by Video-LLaMA and VideoChat process different modalities separately. The former processes both audio and visual information separately using Q-Formers. The latter processes video using specialized embedding models and a perception toolkit for mixed modalities. In contrast, single-stream approaches like Video-ChatGPT employ spatiotemporal pooling to capture the overall video context. Video-LLaVA utilizes a LanguageBind module to map multimodal inputs into a shared space. Mirasol3B proposes a decoder-only model adapted to handle multimodal input, representing them in disentangled spaces. ChatUniVi takes a unique approach by introducing a unified visual representation through dynamic visual tokens for both images and videos."}, {"title": "2.2. Long Video Understanding", "content": "Recent works have proposed diverse solutions to address the challenges pertinent to long video understanding. Memory-based approaches include MovieChat, which employs a dual memory module with a FIFO queue for short-term memory and a consolidation module for long-term memory, and MA-LMM, which introduces a hierarchical memory module. TimeChat incorporates timestamps and transcribed speech for time-aware encoding. However, these approaches significantly compress temporal information, compromising the understanding of event dynamics. Alternative approaches focus on efficient frame representation. LLaMA-VID efficiently represents each frame with only two tokens. VTimeLLM introduces temporal-focused training and uses only the class tokens as frame representations. Yet, both VTimeLLM and LLAMA-VID process frames in isolation, failing to capture coherent temporal representations.\nUnlike previous works that either significantly compress temporal information or process frames in isolation, ReWind distinguishes itself by proposing a novel memory-based architecture with a read-perceive-write cycle that selectively stores instruction-relevant visual information while enabling efficient processing of long videos and maintaining temporal fidelity. As opposed to approaches that maintain fixed dense representations, ReWind employs an adaptive frame selection mechanism that enriches memory representations with detailed spatial information only for instruction-relevant key moments."}, {"title": "3. Method", "content": "ReWind enables efficient long-video understanding through a novel memory-based architecture that maintains temporal fidelity while selectively storing instruction-relevant information. As shown in Fig. 2 (a), the architecture implements this through two-stage processing. Stage-1, namely read-perceive-write cycle, comprises: (1) a vision encoder, (2) a text encoder for instruction processing, (3) a instruction-aware perceiver that bridges visual features and LLM understanding, and (4) a memory module with learnable read and write operations for efficient information storage. Stage-2, the Selection, comprises a dynamic frame selection (DFS) mechanism that enriches memory representations with detailed spatial information for key moments. These two stages work in concert to enable the LLM to generate responses based on both the instruction and video content. We explain Stage-1 components in Sections 3.1 and 3.2, and the DFS in Section 3.3. Finally, we detail the LLM input formation and the training strategy in Sections 3.4 and3.5."}, {"title": "3.1. Visual Feature Extraction", "content": "To process long videos under GPU memory constraints, ReWind divides input video V containing T frames into N sub-clips S = S1,S2,...,SN, each with F frames (N = T/F). For each frame $f_{ij}$ in sub-clip $s_i$, a pre-trained ViT-G/14 encoder from EVA-CLIP extracts visual features as a sequence of tokens $P_{ij}$."}, {"title": "3.2. Instructed Memory Architecture", "content": "At the core of ReWind lies its novel read-perceive-write cycle that enables progressive video understanding while maintaining temporal fidelity. This cycle orchestrates the interaction between a long-term memory bank for storing distilled video representations, an instruction-aware temporal perceiver for temporal representation construction, and learnable read-write functions for memory interaction, as illustrated in Fig. 3.\nTo effectively process long videos, ReWind's memory module selectively stores instruction-relevant information from incoming frames while enabling progressive information accumulation. The module centers on a long-term memory bank M and learnable read-write functions that bridge memory content with perceiver features. The read operation, using learnable queries QR, first retrieves historical context from M. These read queries then initialize the perceiver's queries for instruction-guided visual feature extraction from ViT outputs. Finally, learnable write queries Qw distill the perceiver's output through cross-attention for efficient storage in M. Additionally, original visual features"}, {"title": "3.2.1. Read Operation", "content": "The read operation aims to facilitate dynamic, context-aware feature extraction. This interface enables continuous interaction between the feature extraction process and the evolving memory content in M. Specifically, as the memory gets populated with information from previously processed video segments, the read interface uses a fixed number $N_R$ (i.e., 32) of read queries $Q_R$ to actively retrieve relevant context through a cross-attention mechanism between them and the contents of M as depicted in Fig. 2 (a). This retrieval process enables the feature extraction pipeline to remain informed by the most recent knowledge stored in the memory. These context-enriched read queries then guide the perceiver's processing of incoming frames, ensuring that feature extraction maintains awareness of previously stored temporal information."}, {"title": "3.2.2. Perceive Operation", "content": "The perceive operation, performed by a perceiver block, bridges visual features and the LLM's understanding through instruction-aware temporal modeling. As illustrated in Figure 2 (b), the design of perceiver allows for effective integration of instruction-guided features with historical context. As such, it utilizes a set of $N_Q$ learnable queries, $Q$, to project $P_{ij}$ into a latent space that LLM can understand. These learnable queries guide the extraction of relevant information from the visual features.\nA crucial aspect of ReWind's design is the synergistic relationship between the perceiver block and the memory module. The learnable $Q$ in the perceiver block share the same weights and are initialized with the current content of the read queries $Q_R$ obtained by the cross attention between $Q_R$ and the contents of M (note that this implies $N_Q = N_R$). This creates a continuous pipeline, allowing the feature extraction process to dynamically interact with the memory and access relevant context. To further enhance this process, the perceiver block incorporates the textual embedding of the user instruction, denoted as I. This embedding is obtained by encoding the input text query using a pre-trained BERT encoder. I is then appended to the visual features $P_{ij}$ to form extended representations, denoted as"}, {"title": "3.2.3. Write Operation", "content": "The write operation efficiently distills and stores the perceiver's frame-level output $Q_{ij}$ in memory. While these outputs capture rich spatial and contextual information, their sheer number of queries impedes processing long videos. To address this, ReWind's learnable writing mechanism compresses the visual information into a more efficient representation. This mechanism utilizes a set of learnable write queries, $Q_W$, to distill the scene information into a much smaller number of tokens (e.g., 2 tokens per frame). Specifically, ReWind employs cross-attention between $Q_W$ and $Q_{ij}$ to generate compact per-frame representations $Q_W$. These representations are then stored in the memory bank M in temporal order, enabling the progressive construction of temporally coherent video representations. Additionally, the original visual features $P_{ij}$ for each frame are stored in a separate feature buffer, preserving the detailed spatial information for later use (see Section 3.3). This feature buffer is a simple storage container and does not impact computational resources."}, {"title": "3.3. Dynamic Frame Selection", "content": "While M efficiently stores a compressed video representation, certain instructions demand high spatial resolution at specific moments. ReWind addresses this through a Dynamic Frame Selection (DFS) mechanism that identifies instruction-relevant key frames using memory contents M"}, {"title": "3.4. Large Language Model", "content": "The input to the LLM is constructed by concatenating M with the dense representations 2, separated by a special token $\\tau$: < $m_0, m_1,...,\\tau, \\hat{Z}$ >. The role of $\\tau$ is purely to separate the memory content with progressive temporal information from the DFS frames where the spatial information is prioritized. The video content is then combined with the text instruction and given in input to the LLM."}, {"title": "3.5. Training", "content": "Instruction tuning has been a crucial training strategy for VLMs, especially for the QA tasks, as demonstrated from previous works. Inspired by this, our training strategy is divided into two stages.\nMultimodal Pretraining Stage. During the initial stage, we conduct standard multimodal alignment, keeping the network components except the perceiver frozen. This phase aims to empower our ReWind to effectively capture semantic visual information without any compromise in the performance of the overall pipeline. Specifically, it involves contrastive learning utilizing the SigLIP loss between perceiver projections and caption encodings from BERT.\nInstruction Tuning Stage. The second training stage engages the memory module, the DFS, and the LLM (fine-tuned using LoRA). This phase employs the instruction-tuning strategy on multimodal instruction-tuning datasets, aiming to integrate all network components seamlessly for the VQA and temporal grounding tasks."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Experimental Setup and Datasets", "content": "Model Settings. ReWind's architecture is built upon the EVA-02 vision encoder (ViT-G/14) and a 7B-parameter LLaMA-2 LLM. The perceiver block, illustrated in Fig. 2 (b), consists of 8 sequential layers. Additionally, we utilize 32 queries for reading and perceiving information and two write queries to ensure efficient memory storage. The DFS mechanism selects 64 frames in the first selection phase and then refines this to 8 representative frames. These selected frames are then pooled into 32 tokens per frame before being integrated with the memory content.\nTraining Setup and Data. We pretrain ReWind on 100K video-caption pairs randomly selected from the WebVid2.5M and Panda70M datasets. This stage involves 10K steps with a batch size of 64, using the AdamW optimizer and cosine scheduling. The learning rate is set to 1e-4 with 500 warmup steps. For instruction tuning, we combine multimodal instruction data from VideoChatGPT with the same 100,000 video-caption pairs used in the pretraining stage. All frames are resized to 224\u00d7224 pixels. During this stage, ReWind is trained for 100,000 steps with a batch size of 64, a learning rate of 5e-5, and 2,000 warmup steps, using the same optimizer and scheduler as in pretraining. We utilize LoRA for the LLM with a rank of 64 and alpha of 32. For temporal grounding tasks, ReWind undergoes additional fine-tuned on DiDemo and ActivityNet datasets with manually annotated QA pairs with temporal boundaries for an extra 15K steps using the same optimizer and learning rate. Remarkably, our model can obtain great results while being trained on only 8\u00d7V100 GPUs. Further details regarding the data and training setup"}, {"title": "4.2. Datasets and Evaluation", "content": "Long Video. We evaluate ReWind's performance on two tasks: VQA and temporal grounding. For VQA, we use the MovieChat-1K test set, with a video average length of 9.13 minutes. We assess VQA performance using three metrics: accuracy, score, and generation quality, determined by comparing the generated answer to the ground truth (GT) using GPT-3.5. Accuracy measures the exact matches between answers and GT, while the score measures their proximity in meaning with a score from 0 to 5. Generation quality is evaluated using the protocol proposed in based on five metrics: correctness of information (CI), detailed orientation (DO), contextual understanding (CU), temporal understanding (TU), and consistency (CO). Each metric is assigned a score from 0 to 5 by GPT-3.5 by comparing the generated answer and the GT. For temporal grounding, we use Charades-STA. We measure recall at various thresholds (30-70%) and mean IoU (mIoU) to compare the predicted time intervals with the GT.\nShort Video. We evaluate ReWind's performance on short-video benchmarks using the VideoChatGPT dataset and generation quality evaluation protocol."}, {"title": "4.3. Results on Long Videos", "content": "VQA. The MovieChat-1K dataset is a challenging long-video benchmark with an average video length of 9.13 minutes. It contains 1,000 videos, each with multiple open-ended questions in two settings: global and breakpoint."}, {"title": "4.4. Results on Short Videos", "content": "To further assess ReWind's capabilities, we evaluate its performance on the VideoChatGPT QA test set, which features open-ended questions with more detailed answers. Utilizing the generation evaluation protocol, the results are presented in Table 3. ReWind achieves a higher overall average score (AVG) than all previous short and long-term"}, {"title": "5. Ablation", "content": "Core Mechanisms. Table 4 presents an ablation of ReWind's core components the memory module, and the DFS mechanism on long videos. We establish a baseline model that uses 64 uniformly sampled frames and incorporates the perceiver block as an adapter layer, with each frame encoded using 32 tokens. We then progressively incorporate the memory and DFS to complete ReWind's architecture. Note that when we add the components, the video is processed at 1 fps, and each frame is encoded with 2 tokens to align with our design. The results demonstrate that memory and DFS significantly contribute to ReWind's performance on long videos. To assess the effectiveness of these components on shorter videos, we conduct a similar ablation using the VideoChatGPT dataset, which consists of short videos, and report the findings in Table 5. Notably, combining memory and DFS leads to substantial improvements over the baseline, even when applied to short videos.\nPerceiver. In our architectural design, the perceiver layer is conditioned on the text and past information through reading queries. We validate the effects these elements have on the perceiver in Table 6. Particularly, we start with ReWind without DFS and experiment with different conditions."}, {"title": "5.1. Qualitative Results", "content": "Fig. 5 provides qualitative examples showcasing ReWind's ability to comprehend long videos while preserving fine-grained details. We pose two types of questions to ReWind: (1) a comprehensive description of the entire video content, where ReWind effectively captures the overall narrative and key events of the video, and (2) a question about the chang-"}]}