{"title": "ENHANCING GRAMMATICAL ERROR DETECTION USING BERT WITH CLEANED LANG-8 DATASET", "authors": ["Rahul Nihalani", "Kushal Shah"], "abstract": "This paper presents an improved LLM based model for Grammatical Error Detection (GED), which is a very challenging and equally important problem for many applications. The traditional approach to GED involved hand-designed features, but recently, Neural Networks (NN) have automated the discovery of these features, improving performance in GED. Traditional rule-based systems have an F1 score of 0.50-0.60 and earlier machine learning models give an F1 score of 0.65\u20130.75, including decision trees and simple neural networks. Previous deep learning models, for example, Bi-LSTM, have reported F1 scores within the range from 0.80 to 0.90. In our study, we have fine tuned various transformer models using the Lang8 dataset rigorously cleaned by us. In our experiments, the BERT-base-uncased model gave an impressive performance with an F1 score of 0.91 and accuracy of 98.49% on training data and 90.53% on testing data, also showcasing the importance of data cleaning. Increasing model size using BERT-large-uncased or RoBERTa-large did not give any noticeable improvements in performance or advantage for this task, underscoring that larger models are not always better. Our results clearly show how far rigorous data cleaning and simple transformer-based models can go toward significantly improving the quality of GED.", "sections": [{"title": "1 Introduction", "content": "Grammatical Error Detection is an important part of Natural Language Processing and helps in detecting errors in written text. The technology is very critical to the betterment of Grammatical Error Correction systems, which are basically not only error detectors but also grammatical mistake correctors. This tool will be of immense help for second language learners in guiding them to write more accurately and with increased confidence. Despite the fact that much progress has been realized over the years, early approaches were largely based on rule-based and statistical methods. For example, in rule-based systems, most of the applications in early NLP relied on predefined grammar rules in detecting errors. In contrast, statistical methods use probabilistic models in identifying anomalies in text; Kaneko et al. used error- and grammaticality-specific word embeddings in their 2018 work Kaneko et al. [2017]. Furthermore, a 2018 study by Rei and S\u00f8gaard on Bi-LSTM models for sentence and token labeling exemplifies an application of statistical methods in GEDRei and S\u00f8gaard [2018]. Deep learning and neural networks, and the more recent invention of Transformer-based models, have played a huge part in changing GED into one that could yield more accurate and context-sensitive error detection.\nDespite these developments in GED, there are still some issues in this domain. The first one is related to the quality of the training data. Most of the existing datasets are noisy and contain inconsistencies, which will decrease the per-formance of the GED model. Besides, larger models perform well in many NLP tasks but sometimes can't be applied easily in GED. In particular, this will involve research into whether these huge models really hold out performance against their smaller, more efficient counterparts within GED."}, {"title": "2 Methodology", "content": "For this paper, the Lang8 dataset used was downloaded from Google Research Datasets. Initially, the dataset consisted of 23,72,119 rows. To download the dataset, a form was filled up, which was provided by the repository. Then the repository provided a run.sh script file to run it on the raw data to generate the final tokenized dataset. Though, with some problems in the execution of the script during tokenization, the final dataset was at 23,50,982 rows. The Lang-8 dataset contains two columns: '0' and '1'. Column '0' contains sentences that are incorrect, and column '1' holds the corrected versions corresponding to the sentences in column '0'.\nThe cleaning process involved the following steps:"}, {"title": "2.1 Data Collection and Cleaning", "content": "For this paper, the Lang8 dataset used was downloaded from Google Research Datasets. Initially, the dataset consisted of 23,72,119 rows. To download the dataset, a form was filled up, which was provided by the repository. Then the repository provided a run.sh script file to run it on the raw data to generate the final tokenized dataset. Though, with some problems in the execution of the script during tokenization, the final dataset was at 23,50,982 rows. The Lang-8 dataset contains two columns: '0' and '1'. Column '0' contains sentences that are incorrect, and column '1' holds the corrected versions corresponding to the sentences in column '0'.\nThe cleaning process involved the following steps:"}, {"title": "2.1.1 Removing Similar Sentences:", "content": "First, all sentences which were exactly identical in column '0' and column '1' (i.e. sentences which were grammat-ically correct) were deleted. In this way, the number of sentences was decreased from 2,350,982 to 1,359,624. This was done because cleaning mainly needs to be applied to grammatically incorrect sentences."}, {"title": "2.1.2 Text Normalization:", "content": "The text in both columns was then normalized, which involved mapping all characters available in the Unicode standard onto their equivalent in the ASCII standard, replacing punctuation marks with spaces. Now, sentences which had"}, {"title": "2.1.3 Space Removal:", "content": "Extra spaces were removed from the sentences in both columns. This concerned spaces at the beginning and at the end of sentences, as well as multiple spaces between words. After the above step, all sentences which had become identical in both columns were removed, and 1,323,190 were retained."}, {"title": "2.1.4 Lower-casing:", "content": "All sentences were then converted to lowercase. The aim was that the same word in different cases would not be considered as different words. Any sentences that had become identical in both columns after lower-casing were removed, leaving 1,251,300."}, {"title": "2.1.5 Handling Contraction:", "content": "First, contractions in the sentences were expanded; for example, \"can't\" became \"cannot\". De-duping the remaining pairs whereby identical sentences on both columns were discarded resulted in 1,251,257 sentences."}, {"title": "2.1.6 Punctuation Removal:", "content": "There are many reasons why a sentence can be considered to be grammatically incorrect, one of which is incorrect punctuation. In our work on developing an algorithm for GED, we did not want to focus on this reason. Hence, we removed the punctuation and deleted all rows where the resulting sentence was same in column 0 and column 1, leaving 1,182,692 sentences. Note that the punctuation was removed only for this cleaning step, and restored for further processing."}, {"title": "2.1.7 Sentence Length and Levenshtein Distance Filtering:", "content": "The Levenshtein distance, a measure of the difference between two strings, between each pair of columns '0' and '1' sentences, was computed. Sentences that had a Levenshtein distance of 0, representing identical sentences, were dropped. It also computed the length difference between sentences in columns '0' and '1'. Sentences were then filtered by their Levenshtein distance, length difference, and lengths of the sentences in columns '0' and '1'. Only sentences with a Levenshtein distance between 7 and 42 and lengths less than 101 characters were kept, leaving 227,527 sentences. This is because wanted our final dataset to have sentences which are neither too close nor too far from their grammatically correct versions."}, {"title": "2.1.8 Normalized Levenshtein Distance Filtering:", "content": "The Levenshtein distance was then normalized by the length of the sentences in columns '0' and '1'. Sentences were filtered now, on the basis of these normalized Levenshtein distances. Only sentences with a normalized Levenshtein distance between 0.08 & 0.5 were kept. This retained 217,018 sentences. We finally created a dataset of 200,000 sentences from this set Nihalani and Shah [2024]. To clarify, the sentences in column 0 of this dataset are grammatically incorrect, and the sentences in column 1 are their corrected versions."}, {"title": "2.2 Model Selection and Training", "content": "In this study, we used the following pretrained models: bert-base-uncased, bert-large-uncased, roberta-base, and roberta-large. They were selected because of their good performance in many NLP tasks, which also include the task of grammatical error detection. Each transformer model was fine-tuned to the cleaned Lang8 dataset and, in turn, evaluated in how well each predicts a sentence as grammatically correct or otherwise. The dropout rate used in this study was 0.65 for the last layer of the encoder and the classifier part of the model.\nWe used AdamW with a learning rate of 2e-5, epsilon of 1e-8, and weight decay of 0.2. Scheduler would have a warm-up of 0 steps and a total of epochs times length of the train dataloader steps. Number of epochs was set to 4. These were for choosing the parameters to optimize training.\nThe model was trained using a laptop GPU [NVIDIA RTX3050], and validation was done per epoch. Measurements taken for each epoch included training accuracy, training loss, validation accuracy, and validation loss. Another important part in training is the gradient clipping to prevent it from reaching large values that can disrupt training."}, {"title": "3 Results", "content": null}, {"title": "3.1 Performance of BERT-base-uncased", "content": null}, {"title": "3.1.1 Test using cleaned Lang8 dataset", "content": "Model was trained on different sets of Cleaned Lang8 plus Discarded Lang8 (Sentences Removed during Cleaning process) and was Tested on Cleaned Lang8.\nResults of table 2 show variation in the performance of BERT-base-uncased depending on the composition of the training set. In this table, the test set of size 20,000 sentences was taken from the cleaned Lang8 set. Batch Sizes mentions that how many sentences were taken from Cleaned and Discarded Lang8 set respectively for training/fine-tuning. For example, 18k + 2k means that 18,000 sentences were taken from Cleaned Lang8 set and 2,000 sentences were taken from Discarded Lang8 set. We can see that the accuracy and other metrics are much higher when the training dataset contains higher proportion of sentences from the cleaned dataset.\nThe F1-score, precision, and recall also changed when different training sets were used, hence putting more emphasis on the strong impact of the training data composition in the model's performance. Training and validation times remained quite consistent across different training sets. This clearly shows the importance of data cleaning for this task. A minor aberration happens when we take 8k cleaned and 12k discarded sentences for our training, which needs further investigation."}, {"title": "3.1.2 Test using discarded Lang8 dataset", "content": "Model was trained on various sets of Cleaned Lang8 plus Discarded Lang8 (Sentences Removed during Cleaning process) and was Tested on Discarded Lang8 sentences."}, {"title": "3.1.3 Weight Watcher", "content": "While we obtained good test performance on training the BERT-base-uncased model using the cleaned Lang8 dataset, the training accuracy is still considerably larger than the testing accuracy, indicating over-fitting. To address this, we used the WeightWatcher tool to analyse the extent to which each layer of our BERT model is undergoing over-fitting Martin et al. [2021]. We then froze different layers of the model, and did the fine-tuning again to check the results.\nThe results are shown in Table 4.\nThe first row in the table shows five layers with appreciable changes in alpha values given by Weight Watcher. These five layers were frozen one by one during training, and the model was trained five times by freezing one layer at a time. For all five layers, the results were more or less similar, which clearly indicates that freezing these layers didn't affect the performance of the model significantly. The model returned a validation F1-score of 0.84, a training accuracy of about 96%, and a validation accuracy of about 84% across the different configurations. Besides, precision and recall"}, {"title": "3.2 Comparison of various BERT-like models", "content": "We compared various BERT-like models and the results are shown in Table 5."}, {"title": "3.2.1 BERT-base-uncased", "content": "For the BERT-base-uncased model, training was done with 20,000 cleaned sentences, while another 20,000 cleaned sentences were used for testing. In this case, it achieved a training accuracy of 98.09% and a test accuracy of 87.45%. The F1-Score was 0.87, the Precision was 0.87, and Recall was 0.97. The training time taken was 0:26:09, and the time taken to carry out validation was 0:02:07. When the model is trained on a larger dataset of 180k sentences, it achieved a training accuracy of 98.49% and the test accuracy improves to 90.53%. F1-score, Precision and Recall were 0.91, 0.91 and 0.91 respectively. This suggests that the model benefits from having more training data."}, {"title": "3.2.2 BERT-large-uncased", "content": "The BERT-large-uncased model achieved a test accuracy of 87.94% when trained and validated on 20,000 cleaned sentences each. The training time taken was 12:00:28, and the time taken to carry out testing was 0:06:47 with F1-score, Precision and Recall of 0.88 each."}, {"title": "3.2.3 ROBERTa-base", "content": "The RoBERTa-base model was trained and validated on 20,000 cleaned sentences each to obtain a test accuracy of 86.72%. The F1-score, Precision and Recall were 0.87 each. It performed almost similar to BERT-base-uncased 20k model with a slight decrease in test accuracy."}, {"title": "3.2.4 ROBERTa-large", "content": "The validation accuracy result for the ROBERTa-large model was quite close to that of the BERT-large-uncased model with a slight increase in validation accuracy, at 89.07%, for cleaned 20,000 training and testing sentences. Its training time was 11:02:18, with its testing time being 0:51:46."}, {"title": "3.3 Inference on Generative and Fine-tuned Models", "content": "After having done extensive analysis using BERT-like models, we also tried out two major generative models to see how their performance compares with that of BERT-like models. For this purpose, we chose 500 sentences from the cleaned Lang8 dataset, out of which 255 were grammatically correct and 245 were grammatically incorrect. We could not do for a larger dataset since generative models have a per token API cost, which can become quite large for larger datasets. The results are shown in Table 6.\nThe inference on 500 sentences was done without fine-tuning using the GPT-4 model API provided by OpenAI OpenAI et al. [2024] and it returned 119 TPs and 232 TNs. This means that the model can identify a reasonable number of grammatically correct and incorrect sentences correctly without any fine-tuning.\nInterestingly, the Llama-3-70B-instruct model by Meta AI [2024] performed very similarly to the GPT-4 model, re-turning a count of 114 TPs and a count of 235 TNs when used for inference on the exact same 500 sentences.\nWhen used for inference on those same 500 sentences, the BERT-base-uncased model that had been trained on 20k sentences returned 232 TPs and 207 TNs, and that with 180k sentences returned a count of 255 TPs and 242 TNs. On a larger set of 5000 sentences, it scored 2529 as a count of TPs and 2441 as a count of TNs. Clearly, this shows that the BERT-base-uncased model performs way much better than the generative models on the task.\nWhen inference was done on the same set of 500 sentences, BERT-large-uncased returned 232 TPs and 208 TNs almost similar to 20k version whereas ROBERTa-base 20k returned a count of 237 TPs and 202 TNs and with almost similar results, RoBERTa-large 20k returning a count of 238 TPs and 209 TNs.\nThe BERT-base-uncased 180k model showed the highest validation accuracy, showing that transformer model perfor-mance can be significantly improved by fine tuning using larger datasets. Generative models GPT-4 and Llama-3-70B-instruct did quite reasonably without fine-tuning, but they were still far behind fine-tuned BERT like models. And among BERT like models, BERT-base-uncased performed similar to BERT-large-uncased. This is very interesting, for it again proves that bigger models do not always give better results; sometimes they need to be fine-tuned more or need more data to train. We did not train larger models, such as BERT-large-uncased and RoBERTa-large, on larger datasets because even to fine-tune these models on 20,000 sentences, it took around 12 and 11 hours respectively.\nResults from this study underpin the importance of cleaning and preprocessing the data for high model performance. The highest validation accuracy seen with the BERT-base-uncased 180k model trained on the cleaned dataset suggests that the cleaning process was very helpful in improving model performance. The results also indicated that fine-tuning of the pre-trained model BERT-base-uncased is extremely effective in grammatical error detection."}, {"title": "4 Conclusion", "content": "Among all the models that were trained, the highest accuracy was obtained using the BERT-base-uncased model, which, after being trained on 180k sentences of the cleaned Lang8 dataset, returned a high validation accuracy of 90.53% with robust performance in the identification of grammatically correct and incorrect sentences.\nThese findings further prove that fine-tuning a pre-trained model, like BERT-base-uncased, on a cleaned and pre-processed dataset goes a long way in detecting grammatical errors. This proves the importance of the cleaning and pre-processing steps toward attaining high model performance. From these results, it seems that growing the models in size is not always helpful to gaining better performance, and larger models might require more careful tuning or additional training data. These findings have key implications for the development of more effective and efficient systems for grammatical error detection.\nIn the future, different experiments could involve different pre-trained models, larger datasets, and various other clean-ing and preprocessing strategies. Other strategies for preventing over-fitting by multitask learning could be applied. This study also showed that further investigation on the performance of larger models and generative models in gram-matical error detection is needed."}]}