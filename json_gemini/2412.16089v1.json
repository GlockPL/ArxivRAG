{"title": "The Evolution of LLM Adoption in Industry Data Curation Practices", "authors": ["Crystal Qian", "Michael Xieyang Liu", "Emily Reif", "Grady Simon", "Nada Hussein", "Nathan Clement", "James Wexler", "Carrie J. Cai", "Michael Terry", "Minsuk Kahng"], "abstract": "As large language models (LLMs) grow increasingly adept at processing unstructured text data, they offer new opportunities to enhance data curation workflows. This paper explores the evolution of LLM adoption among practitioners at a large technology company, evaluating the impact of LLMs in data curation tasks through participants' perceptions, integration strategies, and reported usage scenarios. Through a series of surveys, interviews, and user studies, we provide a timely snapshot of how organizations are navigating a pivotal moment in LLM evolution. In Q2 2023, We conducted a survey to assess LLM adoption in industry for development tasks (N=84), and facilitated expert interviews to assess evolving data needs (N=10) in Q3 2023. In Q2 2024, we explored practitioners' current and anticipated LLM usage through a user study involving two LLM-based prototypes (N=12). While each study addressed distinct research goals, they revealed a broader narrative about evolving LLM usage in aggregate. We discovered an emerging shift in data understanding\u2014from heuristic-first, bottom-up approaches to insights-first, top-down workflows supported by LLMs. Furthermore, to respond to a more complex data landscape, data practitioners now supplement traditional subject-expert-created \u201cgolden datasets\" with LLM-generated \u201csilver\u201d datasets and rigorously validated \u201csuper golden\u201d datasets curated by diverse experts. This research sheds light on the transformative role of LLMs in large-scale analysis of unstructured data and highlights opportunities for further tool development.", "sections": [{"title": "1. Introduction", "content": "As large language models (LLMs) continue to advance, their improved reasoning capabilities, enhanced summarization techniques, and growing context windows enable them to process and generate insights from complex and voluminous data more effectively than ever before [Touvron et al. (2023a), Team et al.(2023), Xiao et al.(2023), Liu et al.(2024c), Dunivin(2024), Liu et al.(2023b), Zheng et al.(2024)]. These advancements present a significant opportunity to improve data curation and analysis workflows, particularly for those working with unstructured, text-based datasets.\nAt the same time, the complexity of text-based data has also grown. Modern foundation models increasingly rely on unstructured text data throughout their pipelines, including data for pre-training, fine-tuning, human feedback, and evaluation [Touvron et al.(2023a), Touvron et al.(2023b), Team et al.(2023), Groeneveld et al.(2024)]. With data coming from increasingly diverse sources, such as LLM-generated content, curating it-ensuring its quality, coherence, and relevance through iterative refinement and evaluation\u2014becomes even more critical and challenging, as reported by recent work [Muller et al.(2019), Kuo et al.(2024), Han et al.(2023), Freitas and Curry(2016), Liu et al.(2024b)].\nMotivated by the potential of emerging LLM technology to address these issues, we set out to investigate how those who curate and analyze unstructured, text-based datasets\u2014a population we refer to as data practitioners are adapting to these changes. Our research unfolded in three stages:"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Data Practitioners and the Importance of Data Quality", "content": "In recent years, the role of data practitioners has expanded significantly, encompassing a wide range of tasks from data collection and cleaning [Kandel et al.(2012a), Kandel et al. (2012b), Epperson et al.(2024)] to transformation [Drosos et al.(2020)], visualization [Ruddle et al. (2024), Liu et al. (2019), Alencar et al.(2012)], and analysis [Wongsuphasawat et al.(2019), Kandel et al.(2012a), Collins et al.(2009), Liu et al.(2023b)].\nIn this work, we focus on data practitioners with first-hand experience in data curation, whose responsibilities include ensuring the quality of unstructured, largely text-based data. High-quality data is integral for"}, {"title": "2.2. Tools and Techniques for Data Curation and Analysis", "content": "Existing research has investigated practitioners' workflows and tools. Many data science professionals interact with data in tabular formats, using tools such as Google Sheets or Microsoft Excel [Birch et al.(2018)]. They may also write code to perform custom analyses, commonly by using Python scripts or notebooks such as Google Colab or Jupyter Notebook [Chattopadhyay et al.(2020), Kery et al.(2019), Tabard et al.(2008)].\nThere are also bespoke tools and techniques developed for specific stages of data work. For instance, the machine learning community has utilized crowd workers to acquire and label data [Yuen et al. (2011), Quinn and Bederson (2011)], with recent improvements through mechanisms like weak supervision [Ratner et al.(2020), Mintz et al.(2009)] to enhance data quality. In addition, Wrangler [Kandel et al.(2011)], Profiler [Kandel et al.(2012b)], and AutoProfiler [Epperson et al.(2024)] assist practitioners in evaluating, cleaning, and preparing data. To support exploratory data analysis [Tukey(1977), Wongsuphasawat et al.(2019)], where practitioners aim to discover new insights, researchers have proposed various methods and systems. These include computing features [Lara and Tiwari(2022)], calculating distributions [Gebru et al.(2021), Pushkarna et al.(2022)], clustering data into semantically relevant slices and categories [Kucher and Kerren(2015), Viswanathan et al.(2023)], and enabling interactive visualization, exploration, and comparison of the data [Brath et al.(2023), Reif et al.(2019), Wang et al.(2024)]."}, {"title": "2.3. Leveraging LLMs for Data Work", "content": "As LLMs have become more salient, there is a growing trend of integrating them into tools for data work. LLMs can be used to interactively cluster datasets [Viswanathan et al. (2023)], explain and label these clusters [Wang et al.(2023a)], qualitatively code and analyze data [Gao et al.(2024), Chew et al.(2023), De Paoli(2024)], and even expand existing datasets by generating synthetic examples [Wu et al.(2021), Yuan et al.(2022), Liu et al.(2023a)]. For instance, TopicGPT [Pham et al.(2024)] and LLooM [Lam et al.(2024)] are LLM-enabled topic modeling tools that create high-level, human-understandable topics for datasets. Furthermore, researchers have proposed leveraging LLMs to evaluate the performance of models (often LLMs themselves), a practice known as \u201cLLM-as-a-judge\u201d [Zheng et al.(2024)], along with tools that visualize results [Kahng et al.(2024), Kahng et al.(2025)]. This approach can be applied not only to assessing general-purpose LLMs but also to evaluating specific aspects such as safety, factuality, coherence, fluency, or other custom evaluation criteria [Inan et al.(2023), Kim et al.(2024)]. Despite the recent emergence of LLM-focused tools, there is limited research examining their adoption in industry data work. Therefore, in this study, we utilized two LLM-based prototypes as design probes to explore how LLMs could potentially transform the workflows of data practitioners."}, {"title": "3. Exploratory Survey", "content": "To measure LLM tool adoption in development workflows, we conducted a survey across many teams and organizations at Google in Q2 2023\u00b2."}, {"title": "4. Formative Study: Expert Interviews", "content": "While the survey revealed limited LLM adoption across the company, our firsthand experiences in LLM development within the research organization underscored emerging challenges in managing the complex data ecosystems both created and utilized by LLMs. To better understand these challenges and the experiences of analyzing text-based datasets, we conducted interviews with industry practitioners.\u00b3"}, {"title": "4.1. Participants", "content": "Using company-internal user lists for data tools, we recruited 10 participants from the research organization (N=10, 4 female, 6 male), described. The recruitment criteria prioritized sampling participants from a variety of backgrounds and teams that work with text-based datasets. Most were involved in projects related to the development of foundation models. Six participants (U1-U6) were practitioners who directly worked with text datasets, handling tasks such as dataset collection, curation for model training, and evaluation for safety and labeling. The remaining four participants (D1-D4) were developers focused on building tools to support the understanding and analyzing of text-based datasets within the organization, largely for the purpose of developing and evaluating large language models."}, {"title": "4.2. Interview Protocol", "content": "We conducted one-on-one, semi-structured video interviews with participants, each lasting approximately 30 minutes. We followed a protocol inspired by prior studies on data analysis practices [Kaur et al.(2020), Li et al.(2023), Wang et al. (2019)], focusing on:\n\u2022 Use cases: Background, use case, product impact, and research questions\n\u2022 Tools and techniques: Awareness and usage of existing tools and pipelines, decision-making processes, advantages and limitations, and data analysis methods\n\u2022 User challenges: Bottlenecks and unaddressed concerns"}, {"title": "4.3. Findings", "content": "In Table 2, we summarize participants' reported processes, including the tools they use, tasks they perform, and challenges they face. Here, we discuss the key findings from these interviews."}, {"title": "4.3.1. Participants increasingly prioritize data quality.", "content": "Participants unanimously reported data quality-defining, finding, and identifying high-quality data\u2014as their biggest challenge. Identifying, understanding, and addressing low-quality data have become essential tasks:\n\u201cData, historically, has been around volume we've had this big paradigm shift [to quality].\u201d \u2014D2\n\"Quality is the big obstacle. . . [You need] a lot of high-quality data. . . there's no shortcut.\u201d -U6"}, {"title": "4.3.2. Participants favor flexible, customizable tooling for evaluating data quality.", "content": "Our participants reported performing most of their data exploration using spreadsheets and Python notebooks, which are both flexible, customizable interfaces. These reported practices are consistent with previous research [Herman(2019), Pirolli and Card(2005), Gilpin et al. (2018), Caliskan et al.(2017)].\nInspecting data visually in spreadsheets is a universal practice. All participants indicated that they evaluate their data by visually scanning it in spreadsheet form, examining a handful of examples to validate their understanding."}, {"title": "4.3.3. Bespoke tools have yet to gain widespread adoption.", "content": "Participants are aware of standalone tools that provide specific data insights, such as classifiers for safety and toxicity [Bellamy et al.(2018)] or tools for data and model interpretability [Tenney et al.(2020), Amershi et al.(2015), Wexler et al.(2020)]. However, they find these tools too specialized for their needs, with no guarantee that these tools will be useful:"}, {"title": "4.3.4. There's an opportunity for improved tooling and workflows.", "content": "Participants are exploring ways to refine their workflows and are open to adopting new workflows beyond their current usage of spreadsheets and notebooks:\n\u201cNot having an easy-to-use-tool is a major bottleneck... Every time [that I make changes to data], I have to write a custom Colab to ingest the new fields.\u201d -U2"}, {"title": "4.4. There was little adoption of LLMs in practitioner workflows at this time.", "content": "In our sample of 12 participants, including those who piloted the study, only one participant reported regularly incorporating LLMs into their workflow\u2014this usage was limited to programmatically accessing LLMs in a Colab notebook for specific rating tasks. It is possible that developers of LLMs, being more aware of their limitations, may be less likely than others to adopt them. However, the finding that developers were not actively incorporating LLMs into their workflows aligned with the broader organizational trends we found in the exploratory survey."}, {"title": "5. Design Probes", "content": "The expert interviews occurred during a transitional time when practitioners were beginning to address challenges related to increasing data complexity and LLM development, but were not adopting these technologies at scale themselves. Following the interviews, the trends around LLM usage began to shift. Newly released tools and methods demonstrated the increasing use of LLMs for in data curation [Zheng et al.(2024), Inan et al.(2023), Reif et al.(2024)]. In addition to direct prompting interfaces (e.g. ChatGPT, Gemini), numerous bespoke LLM-based tools emerged [Wang et al.(2024), Liu et al. (2024c), Parnin et al.(2023), Ma et al.(2024b), Ma et al.(2024a), Liu et al.(2024b), Fok et al.(2024)], many addressing challenges identified in our formative interviews such summarization and categorization. Finally, as industry practices evolved and the risks around generative Al were better understood, restrictions on generative AI usage were relaxed, enabling practitioners to integrate LLMs into their workflows.\nThis motivated our follow-up user study in Q3 2024 to explore how practitioners' perspectives on LLM adoption had evolved. Our goal was not only to understand how LLMs were currently being incorporated into existing workflows, but also to design for future adoption patterns. The expert interviews had revealed a common reliance on spreadsheets and Colab for data-related tasks. To address this, we developed two design probes that integrate LLM capabilities directly into these widely used tools: spreadsheets (e.g., Google Sheets) and Python notebooks (e.g., Colab\u2075).\nDesign goals: Our design probes aimed to leverage LLMs to address the user challenges identified in Table 2:"}, {"title": "5.1. Spreadsheet Integration", "content": "Given the widespread usage of spreadsheets found in Section 4.3.2, we developed an Apps Script application\u2076 that enables LLM prompting within spreadsheet cells. This application introduces a \"RUN_PROMPT\" function that sends a text prompt to an LLM model. A separate sheet in the spreadsheet contains customization parameters for an API: model name (e.g., gemini-1.5-pro), temperature value, and API key."}, {"title": "5.2. Computational Notebook Integration", "content": "For the second probe, we provided a Colab notebook with built-in libraries for LLM prompting. Similarly to the spreadsheet probe, participants can configure the model, default temperature, and API key through form fields.\nFigure 2 shows the example notebook. The library includes a \u201crun_classifier\" function that accepts a Pandas dataframe (i.e., df) and an instruction. The function calls the LLM and returns the dataframe with an additional column containing the LLM's outputs. Since Python notebooks offer greater flexibility than spreadsheets. We provide two additional features:\n\u2022 Summative analysis: Users can query the LLM with an entire dataset.\n\u2022 Controlled generation: This feature allows structured outputs (e.g. yes or no) for tabular queries.\u2077 In the spreadsheets probe, controlled generation can only be approximated with the inclusion of instructions in the prompt such as Please output only \u201cyes\u201d or \u201cno\u201d."}, {"title": "6. User Study With Design Probes", "content": "We then conducted a user study employing both prototypes as design probes. Our primary goals were to 1) explore how practitioners perceive and use LLMs to address the challenges identified in our formative research, and 2) explore the opportunities and challenges associated with incorporating LLM-prompting interfaces into practitioners' workflows."}, {"title": "6.1. Participants", "content": "We recruited 12 participants (N=12; 5 female, 7 male) who work with text-based datasets within Google (Table 3). To gain multiple insights per product area, we used a two-step recruitment process, encouraging participants to refer colleagues working on relevant tasks. This sample was carefully curated to include industry experts across six distinct product areas within the company. We categorize these participants into three roles:\n\u2022 Technical roles (T1-T4): Engineers and model developers who create and evaluate models for products.\n\u2022 Analytical and operational roles (A1-A5): Domain experts, ethics researchers, and project leads who develop policies around products, primarily focused on safety.\n\u2022 Client-facing roles (C1-C3): User experience researchers and survey experts who assess product usability.\nDuring screening, participants evaluated their familiarity with relevant tools (spreadsheets, Colab, Python) on a five-point scale to provide context for their usage patterns (Table 3, Tool Familiarity). Python and Colab usage were less common in client-facing roles but prevalent in technical and analytical roles. Notably, Colab was utilized by some participants without extensive Python experience."}, {"title": "6.2. User Study Protocol", "content": "We conducted individual sessions with the participants via video conferencing. At the beginning of the session, each participant received a dedicated copy of both the spreadsheet and notebook design probes (1, 2, and 3), which contained an excerpt of 100 entries from the Chatbot Arena Conversation Dataset.\u2078\nEach hour-long session began with a brief interview to understand the participant's use case and background, followed by an introduction and tutorial on the design probes. Participants then shared their screens for real-time observation. They explored and explained their current approaches to tasks identified from our formative study, such as summative analysis, categorization, and numerical analysis. Discussions focused on existing workflows, the current and potential role of LLMs, and how interfaces like those in the design probe might fit into their workflow."}, {"title": "6.3. Results", "content": ""}, {"title": "6.3.1. Design Goal 1: Improve Productivity", "content": "Accuracy Participants were not entirely convinced that LLMs had significantly improved their accuracy. They cited anecdotal evidence suggesting that LLMs performed comparably to humans, with some instances of higher agreement, though this might be partially attributed to LLMs' self-consistency [Wang et al.(2023b)]."}, {"title": "6.4. Design Goal 2: Allow Customization", "content": "We found that the open-ended, flexible nature of LLMs allowed them to be used across many different applications, listed below."}, {"title": "6.5. Design Goal 3: Integration Across Tools and People", "content": "Many participants' teams had already independently developed LLM-based tooling prior to the study, such as prompting interfaces within Python notebooks similar to the one in our design probe. Participants noted this as a recent trend that had emerged over the last six to twelve months. However, these tools were largely used by developers only; for example, a developer might run a prompt in a Python notebook, download the output to a different data format, and share the data file with non-technical members of their team.\nParticipants across various roles found the spreadsheet prototype valuable for reduce such existing inefficiencies in collaboration."}, {"title": "6.6. Other Limitations", "content": "The sheets prototype had a few limitations, such as a lack of immediate summative capabilities due to the cell-based default of the the function input.\u2079 Participants tried to run queries such as \u201cWhat are the key themes in the dataset?\u201d to extract summative insights, not fully grasping that the LLM only had context within the cell, not the entire sheet. Scalability was posed another challenge. Participants reported not conducting analyses at scale within spreadsheets due to latency, which would limit usage of the Sheets-based prototype."}, {"title": "6.7. Emerging Dataset Hierarchies", "content": "Traditionally, \"golden datasets,\" meticulously labeled by human experts, have been the sole standard for model training and evaluation. However, the capabilities of LLMs have enabled more sophisticated tiers of datasets. We discovered two new types of datasets from our study:"}, {"title": "6.8. Barriers to adoption", "content": "In this section, we discuss participants' reported barriers and reservations concerning the adoption of LLMs."}, {"title": "7. Discussion", "content": ""}, {"title": "7.1. Emerging Workflow Trends", "content": "As the nature of data evolving, so is its interpretation. Our research highlights a shift in how practitioners approach the understanding of their datasets."}, {"title": "7.1.1. From proxy measures to LLM-powered direct insights.", "content": "In our formative studies, a tool developer remarked:"}, {"title": "7.1.2. From bottom-up aggregation to top-down extraction.", "content": "Traditionally, data analyses were performed using a bottom-up approach. Practitioners would first label and categorize individual data points, and then aggregate them to identify trends. LLMs are now enabling a reversal of this process, allowing practitioners to gain high-level insights from the start. For instance, in the work of R2 and R3, when the goal was to extract actionable insights from customer surveys, they now identified themes using LLMs and then returned to the raw data to extract quotes and evidence that validated these themes. The top-down approach is more efficient, as practitioners only need to focus on extracting individual data points when granular analysis is needed."}, {"title": "7.1.3. Expanded scope for data practitioners.", "content": "LLMs are transforming the way humans engage with dataset understanding. While certain tasks may be automated, especially data gathering and manual coding, experts reported that they were using LLMs to expand the scope of their work."}, {"title": "7.2. Limitations", "content": "This study was conducted within the context of a single company, utilizing specific internal infrastructures and particular cultural and operational practices. While our study utilized a diverse population across many company organizations, and the findings aligned with prior research [Kandel et al.(2012a)], further work is needed to validate their generalizability. For example, the organization's emphasis on developing and utilizing foundation models could have influenced participants' perspectives, as those working closely with these models are likely to possess a higher-than-average level of familiarity regarding LLMs' limitations. Thus, future research could aim to replicate these findings across different organizational contexts to assess their broader applicability. Additionally, while the small sample size for the expert interviews and user studies was sufficient to meet our qualitative research goals, a larger sample would capture more varied perspectives and reduce potential biases, strengthening the robustness of the results.\nThe scope of this work was constrained to individuals primarily involved in data curation, which may not capture the full range of experiences across the spectrum of data-centric roles. Future research should explore the perspectives of data workers and crowd workers, whose work also involves text-based datasets.\nWith the rapid advancements in LLM capabilities and evolving regulatory frameworks, data practitioners' perspectives and the challenges identified in this study may quickly shift. Future work should continue to provide snapshots over an extended period of time to provide deeper insights into LLMs' sustained utility and evolution."}, {"title": "7.3. Future Work and Directions", "content": "This work opens up several promising directions for future research."}, {"title": "Opportunities and limitations of single LLM queries.", "content": "Opportunities and limitations of single LLM queries. Further research is needed to fully understand the potential and limitations of leveraging LLMs to directly identify categories in a top-down manner. Recent advances in long context window LLMs, such as Google's Gemini 1.5 model families [Google(2024)], enable LLMs to process large amounts of input at once. As these models evolve, users may expect LLMs to perform tasks like clustering and labeling all data points in a single query. However, it remains unclear whether this is practical, as current generation long-context models may still struggle with \u201clost in the middle\u201d issues, where attention is unevenly distributed across the input [Liu et al.(2024a)]."}, {"title": "Workflows combining query types.", "content": "Despite advances in LLM capabilities, we believe an iterative workflow will likely remain essential. Expressing complex user needs clearly in a single query is inherently challenging, suggesting that query refinement will be key. Future research should explore how to support users in efficiently iterating based on imperfect results, breaking down tasks into manageable components, and integrating multiple small tasks into higher-level user goals. This shift could impact our understanding of sensemaking, traditionally a bottom-up process, potentially transforming how users approach exploratory data analysis with a more top-down approach."}, {"title": "Addressing responsibility challenges in silver datasets.", "content": "The growing use of silver datasets-those curated by users via LLMs\u2014raises concerns about their quality and bias. As silver datasets are created and curated by people using LLMs, these datasets need to be validated, similar to how LLM outputs are validated in Responsible Al efforts. Future research could explore ways to validate classification results, conduct error analysis, audit for potential biases and stereotypes, and ensure diversity maintained in such datasets."}, {"title": "Beyond spreadsheet or notebooks.", "content": "Although our study used spreadsheets and notebooks as design probes, future work could explore hybrid tools that combine the strengths of both. This could involve embedding notebooks within spreadsheets, vice versa, or developing new web-based tools. Future work could prototype and evaluate solutions tailored to user needs based on our findings."}, {"title": "Extending to multimodal datasets.", "content": "While this work focused on text datasets, the findings could extend to multi-modal datasets, including images and audio. Foundation models like LLMs can augment and profile unstructured data across different modalities beyond text. For instance, users might ask, \"What is the resolution of this image?\u201d or \u201cIs there bias present in the image?\u201d However, modality-specific factors-such as humans' ability to scan images more quickly than text-may make LLMs less desirable for certain tasks. Further research is needed to better understand these nuances."}, {"title": "Evolving paradigms.", "content": "We anticipate that the current emphasis on creating small, high-quality, and non-biased datasets will remain a focus for the foreseeable future. The current approach to refining the existing \u201cgolden\" dataset paradigm has resulted in a complex landscape that includes variations such as silver and super-golden datasets. Looking ahead, we envision two directions that could shape the future of dataset development."}, {"title": "8. Conclusions", "content": "This work is the culmination of multiple checkpoints of work assessing LLM adoption in industry data curation tasks. By the time that our final user study took place- just six months after finding evidence that LLMs had not yet been widely adopted\u2014 we had set out to explore whether industry data practitioners would be open to using LLMs for dataset understanding tasks. However, it quickly became clear that the question was not if practitioners were using LLMs, but rather, how. We observed a rapidly growing reliance on LLMs for a wide variety of tasks, such as classification, summarization, explanation, and outlier detection, especially in cases where efficiency is prioritized. We also discovered that LLMs were enabling practitioners to move away from heuristics-based, bottom-up data aggregation and toward insights-first, top-down analyses, marking a fundamental transformation in how practitioners engage with their data.\nThe adoption of LLMs in data curation signifies not just an incremental improvement, but rather, a paradigm shift. As we navigate the complexities of this new landscape, it is essential to harness the transformative potential of LLMs while staying aware of their limitations. As LLMs play an increasingly integral role in data curation and analysis, clear definitions and evaluation frameworks for data quality become essential. Human oversight in defining, evaluating, and upholding data quality standards remains crucial as AI-driven insights grow more widespread."}]}