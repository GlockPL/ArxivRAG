{"title": "B-STAR: MONITORING AND BALANCING\nEXPLORATION AND EXPLOITATION IN SELF-TAUGHT\nREASONERS", "authors": ["Weihao Zeng", "Yuzhen Huang", "Lulu Zhao", "Yijun Wang", "Zifei Shan", "Junxian He"], "abstract": "In the absence of extensive human-annotated data for complex reasoning tasks,\nself-improvement \u2013 where models are trained on their own outputs \u2013 has emerged\nas a primary method for enhancing performance. Recently, the approach to self-\nimprovement has shifted toward a more dynamic, online fashion through iterative\ntraining. However, the critical factors underlying the mechanism of these iterative\nself-improving methods remain poorly understood, such as under what conditions\nself-improvement is effective, and what are the bottlenecks in the current iterations.\nIn this work, we identify and propose methods to monitor two pivotal factors\nin this iterative process: (1) the model's ability to generate sufficiently diverse\nresponses (exploration); and (2) the effectiveness of external rewards in distinguish-\ning high-quality candidates from lower-quality ones (exploitation). These factors\nare inherently dynamic throughout the iterative process, yet prior research rarely\ndiscusses their evolution \u2013 leaving unclear why models often stagnate after only\na few iterations. Using mathematical reasoning as a case study, we begin with a\nquantitative analysis to track the dynamics of exploration and exploitation, dis-\ncovering that a model's exploratory capabilities rapidly deteriorate over iterations,\nand the effectiveness of exploiting external rewards diminishes as well. Motivated\nby these findings, we introduce B-STAR, a Self-Taught Reasoning framework\nthat autonomously adjusts configurations across iterations to Balance exploration\nand exploitation, thereby optimizing the self-improving effectiveness based on the\ncurrent policy model and available rewards. Our experiments on mathematical\nreasoning, coding, and commonsense reasoning demonstrate that B-STAR not only\nenhances the model's exploratory capabilities throughout training but also achieves\na more effective balance between exploration and exploitation, leading to superior\nperformance. Crucially, this work deconstructs the opaque nature of self-training\nalgorithms, providing interpretable insights into their dynamics and highlighting\ncurrent limitations to guide future research.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models possess advanced reasoning capabilities such as mathematical problem-\nsolving (Cobbe et al., 2021), coding challenges (Chen et al., 2021) or commonsense reasoning\n(Clark et al., 2018). However, the challenge of acquiring extensive, high-quality human-curated\ndatasets remains a significant barrier to further enhancing these reasoning abilities. As tasks grow in\ncomplexity, the reliance on human-generated data becomes increasingly unsustainable, necessitating\nalternative approaches to training.\nTo tackle this issue, methods rooted in the concept of \"self-improvement\" (Huang et al., 2022),\nsuch as STaR (Zelikman et al., 2022), RFT (Yuan et al., 2023), and ReST (Gulcehre et al., 2023;\nSingh et al., 2023), provide more cost-effective and scalable solutions. Self-improvement follows an\niterative process where the model generates responses, from which the better are selected to create"}, {"title": "2 MONITORING EXPLORATION AND EXPLOITATION IN SELF-IMPROVEMENT", "content": "Given a pre-trained model Po and a training set D = {(xi, Yi)}^N_{i=1}, where xi denotes the training\nquery and yi represents the response, the goal of self-improvement is to iteratively generate high-\nquality responses from the current model and update the model with these self-generated data. Let T\nrepresent the total number of iterations, with the model at the start of the t-th iteration denoted as\nPt\u22121. In the first iteration, Po is typically fine-tuned on the initial dataset D, then each subsequent\niteration involves three critical steps generally (Yuan et al., 2023; Gulcehre et al., 2023):\n(1) Generating (Sampling): For each query xi, the model Pt\u22121 generates K candidate responses,\nforming a new, self-generated dataset.\n(2) Rewarding (Verifying): A reward function r(x, y) is applied to score and select the high-quality\nresponses from the self-generated dataset. This reward can be binary and utilize additional supervision,\nfor example, in problem-solving tasks in the math and code domains, it is common to use the final\nanswer matching or the result of passing unit tests as binary feedback (Yuan et al., 2023; Chen et al.,\n2022). In a more sophisticated case, r(x, y) can be parameterized by a reward model outputting\ncontinuous scores, using outcome-based reward models (ORMs) (Li et al., 2022) or process-based\nreward models (PRMs) (Uesato et al., 2022; Lightman et al., 2023; Havrilla et al., 2024).\n(3) Improving: The selected dataset is then used to update Pt\u22121, producing Pt. To differentiate the\ngeneration model M from the reward model, M is also referred to as the policy model following RL\nliterature. In reasoning tasks that we are going to focus on in this paper, SFT loss is commonly used\nin the Improving step due to its robustness and scalability (Pang et al., 2024; Dubey et al., 2024), as\nmore sophisticated RL losses can be unstable to optimize and scale up. When SFT loss is adopted,\nthe Rewarding step aims to reject low-quality responses and use the remaining high-quality data\nfor training, thus this process is also referred to as rejection fine-tuning (RFT, Yuan et al. (2023)).\nIn Appendix C, we provide detailed explanations of self-improvement concepts, including reward\nfunctions and RFT."}, {"title": "2.1 BACKGROUND: SELF-IMPROVEMENT", "content": "Given a pre-trained model Po and a training set D = {(xi, Yi)}_{i=1}^N, where xi denotes the training\nquery and yi represents the response, the goal of self-improvement is to iteratively generate high-\nquality responses from the current model and update the model with these self-generated data. Let T\nrepresent the total number of iterations, with the model at the start of the t-th iteration denoted as\nPt-1. In the first iteration, Po is typically fine-tuned on the initial dataset D, then each subsequent\niteration involves three critical steps generally (Yuan et al., 2023; Gulcehre et al., 2023):\n(1) Generating (Sampling): For each query xi, the model Pt\u22121 generates K candidate responses,\nforming a new, self-generated dataset.\n(2) Rewarding (Verifying): A reward function r(x, y) is applied to score and select the high-quality\nresponses from the self-generated dataset. This reward can be binary and utilize additional supervision,\nfor example, in problem-solving tasks in the math and code domains, it is common to use the final\nanswer matching or the result of passing unit tests as binary feedback (Yuan et al., 2023; Chen et al.,\n2022). In a more sophisticated case, r(x, y) can be parameterized by a reward model outputting\ncontinuous scores, using outcome-based reward models (ORMs) (Li et al., 2022) or process-based\nreward models (PRMs) (Uesato et al., 2022; Lightman et al., 2023; Havrilla et al., 2024).\n(3) Improving: The selected dataset is then used to update Pt-1, producing Pt. To differentiate the\ngeneration model M from the reward model, M is also referred to as the policy model following RL\nliterature. In reasoning tasks that we are going to focus on in this paper, SFT loss is commonly used\nin the Improving step due to its robustness and scalability (Pang et al., 2024; Dubey et al., 2024), as\nmore sophisticated RL losses can be unstable to optimize and scale up. When SFT loss is adopted,\nthe Rewarding step aims to reject low-quality responses and use the remaining high-quality data\nfor training, thus this process is also referred to as rejection fine-tuning (RFT, Yuan et al. (2023)).\nIn Appendix C, we provide detailed explanations of self-improvement concepts, including reward\nfunctions and RFT."}, {"title": "2.2 THE CRITICAL FACTORS - EXPLORATION AND EXPLOITATION", "content": "To maximize the gains from self-improvement training and fundamentally enhance the model's ability\nusing its own outputs, the key is to achieve scalable self-improvement training, where the model's\nperformance scales with increased computational investment in the training algorithm. However, all\nprevious works show quick saturation after merely 3-5 self-improvement iterations (Singh et al., 2023;\nWu et al., 2024), hypothesizing that the model's own outputs can only lead to limited gains. In this\nwork, we seek to dive deeper into the currently opaque process of self-improvement, to understand\nthe critical factors that determine whether self-improvement succeeds or fails.\nIntuitively, for a certain iteration of training, we argue that two high-level conditions must be met for\nthe model to make progresses: (1) Diverse Exploration for High-Quality Responses: When multiple\ncandidates are sampled from the model, a portion of them must be high-quality responses. This\nis particularly important for queries where the model fails to produce satisfactory outputs using\ngreedy decoding. Such diversity enables the discovery of responses that cannot be reached through\ngreedy decoding. (2) Effective Reward Function Discrimination: The reward function r(x, y) must\nreliably distinguish high-quality candidates from lower-quality ones. We refer to the two conditions\nas exploration and exploitation respectively, and we provide their analogies to RL in Appendix G.\nIf either of these conditions is unmet \u2013 such as when the model produces responses overly similar\n(i.e. lack of diversity), or when the reward function fails to identify high-quality responses \u2013 the\nself-improvement will be limited on the gains.\nBoth exploration and exploitation are dy-\nnamically influenced by the policy model during the self-improvement process. On one hand, after\nmultiple iterations, the policy model may overfit the task, failing to explore diverse responses and\ninstead generating highly similar outputs (i.e., a lack of exploration). Training on these highly\nsimilar responses is unlikely to yield significant improvements. On the other hand, if the model\ngenerates excessively diverse responses, resulting in a distribution that deviates significantly from\nthe training data distribution of the reward model, it becomes challenging for the reward model to\nreliably distinguish high-quality responses (i.e., a lack of exploitation). Thus, maintaining a dynamic\nbalance between exploration and exploitation is essential throughout the self-improving process.\nHowever, such dynamics are rarely discussed in prior research, Next, we propose methods to quantify\nexploration and exploitation, enabling us to monitor their dynamics during training and deepen our\nunderstanding of the mechanisms underlying self-improvement."}, {"title": "Quantifying Exploration and Exploitation.", "content": "In this work, we mainly focus on complex problem-\nsolving tasks such as math and coding domains, where the correctness of the responses can be\neasily verified on labeled datasets. This property facilitates the quantification of exploration and\nexploitation, for which we detail the metrics below:\n\u2022 Exploration: Pass@K, which measures whether there is at least one correct response during K\nsampled candidates, is a straightforward metric for assessing exploration, as it directly reflects\nwhether the model is able to explore correct solutions. However, Pass@K can be noisy, as it only\ncounts a single correct response, while it is desirable to assess whether the model can explore\nmultiple correct response as well. To this end, we propose to track Pass@K-S as well, which\nmeasures whether there are at least S unique correct responses among K sampled candidates.\nPass@K-S serves as a more stable proxy to exploration than Pass@K. Pass@K is essentially\nPass@K-1 following such a definition. Besides Pass@K and Pass@K-S, we also track diversity\nof the generations using Distinct Equations proposed by Wu et al. (2024), which measures the\nproportion of unique equations among all correct generated responses.\n\u2022 Exploitation: Best-of-K accuracy measures whether the top one response ranked by the reward\nfunction is correct, directly reflecting how well the reward can potentially select a good response."}, {"title": "2.3 DYNAMICS OF EXPLORATION AND EXPLOITATION - A CASE STUDY IN MATHEMATICAL\nPROBLEM SOLVING", "content": "In this section, we perform a case study to analyze the dynamics of exploration and exploitation in a\nmathematical reasoning task. Specifically, we follow Singh et al. (2023) to adopt MATH (Hendrycks\net al., 2021) training set as the training data, evaluating on the test split of MATH. We also conduct\nevaluation on the test split of GSM8K, with experimental results shown in the Appendix A.2.\nAs introduced in \u00a72.1, we adopt the online RFT training framework in our implementation.\nWe use Mistral-7B (Jiang et al., 2023) as our base model. We run the SFT baseline on MATH for\n3 epochs, and use the checkpoint at the first epoch as the initial checkpoint to run self-improving\ntraining. We experiment with two different types of reward functions:\n\u2022 Answer: we just match the predicted answer with the ground-truth final answer and keep the\nresponses with correct answers, following (Singh et al., 2023):\n$$r = \\mathbb{1}(a = a^*),$$\n\u2022 Answer + PRM: we train a process reward model (PRM) using the approach in Wang et al. (2024b)\nbased on Mistral-7B. Then we combine the final answer reward and the PRM reward as:\n$$r = \\mathbb{1}(a = a^*) + r_{prm}(x, \\hat{y}),$$\nwhere a, a* are the predicted answer and the ground-truth answer respectively, \\mathbb{1}(\u00b7) is the indicator\nfunction, \u0177 is the predicted solution. rprm(\u00b7) is the PRM score. Since PRM is designed to score\nevery step of the solution, we choose the minimal score across all steps as the score for the entire\nsolution, following Lightman et al. (2023). We only keep responses with r > \u03c4 to train the model,\nand \u03c4 is a threshold. We found \u03c4 = 0 is a good hyperparameter in our early trials with different\nthresholds, thus we keep \u03c4 = 0 in all the experiments on dynamics analysis."}, {"title": "3 B-STAR \u2013 BALANCED SELF-TAUGHT REASONERS", "content": "In this section, we firstly introduce a metric to evaluate the interplay effect between exploration and\nexploitation. Next, we analyze its relationship with different configurations. Finally, we present our\ncomplete algorithm, which automatically adjusts the exploration and exploitation abilities."}, {"title": "3.1 BALANCE SCORE", "content": "\u00a72 highlights the importance of a model's exploration and exploitation capabilities for self-\nimprovement, emphasizing their continuous evolution during training. Now we seek a metric\nthat could capture the interplay of these two factors. Intuitively, we hope the model can explore a"}, {"title": "3.2 DYNAMIC OR STATIC \u2013 ON THE CONFIGURATIONS OF EXPLORATION AND EXPLOITATION", "content": "The balance score provides a straightforward signal on how we should manipulate exploration\nand exploitation to be optimal. In this work, we mainly focus on adjusting some hyperparameter\nconfigurations to control exploration and exploitation. Here, we conduct a preliminary analysis to\ninvestigate whether the optimal configurations \u2013 which maximize the averaged balance score \u2013 are\nstatic or should be dynamically changing. Specifically, we obtain the policy model checkpoints and\nreward model checkpoints from different iterations of online RFT with the final Answer + PRM\nreward run, with 500 steps between each iteration. We then apply different configurations to these\ncheckpoints, and compute the average balance scores on 600 randomly sampled MATH training\nqueries. Below we detail the configurations we study.\nSampling temperature, an important configuration influencing explo-\nration, is first examined for its impact on balance score. With the reward threshold fixed at 0.0 and a\nsample size of 32, we adjust the sampling temperature to 0.5, 0.7, 0.9, and 1.1. In Figure 4(a) we show\nthe optimal temperature we obtained that maximizes the average balance score at different iterations\n(training steps). In this particular setting, lower temperatures are preferred in the beginning while\nhigher temperatures are better later on. This phenomenon can be explained by the model's shifting\nlimitations during training. In the early stages, the model's ability to generate correct solutions is\nlimited, so lower temperatures help sample more accurately (Yang et al., 2023). As training advances,\nthe challenge shifts to preserving diversity in the generated solutions, requiring higher temperatures\nto ensure broader sampling.\nWe investigate the impact of reward thresholds on balance score, which\ndetermines how the reward exploits the responses. A high reward threshold indicates that the reward\nmodel strictly exploits responses, whereas a low threshold suggests more relaxed selection criteria. In\nour experiments, we fix the sampling temperature at 1.0 and the sample size at 32, while varying the\nreward threshold and selecting responses that exceed the threshold. Figure 4(b) presents the optimal\nthreshold that maximizes the average balance score. It indicates that a higher threshold is preferred in\nthe beginning, but it may need to decrease as training progresses. Intuitively, this suggests that more\nrigorous filtering should be applied in the beginning, when the model is weaker, and the threshold\nshould be relaxed slightly as the model becomes stronger."}, {"title": "3.3 B-STAR", "content": "Building on the findings from $3.2 that the optimal configurations to maximize balance score is\ndynamically changing, we propose B-STAR, short for Balanced Self-Taught Reasoners, a method that\nmaximizes the average balance score by dynamically adjusting configurations to balance exploration\nand exploitation. We also include related work on dynamically adjusting configurations in the\nAppendix F. Specifically, we adjust temperature and threshold automatically at every iteration, to\nmaximize the average balance score. Notably, we only need to compute the balance score on a small\nsubset of training queries to decide the balanced configurations, thus incurring negligible additional\ncosts compared to the baselines. For example, we only use 600 MATH queries in our experiments.\nOur B-STAR algorithm is illustrated in Figure 2 and summarized in Algorithm 1. There are other\nconfigurations affecting balance score, such as the number of responses drawn per query (sample\nsize) that will influence the exploration. In our preliminary trials in Figure 4(c), larger sample size\ntends to be generally helpful. Therefore, we just use the maximum sample size according to our\ncomputing budget and fix it, and mainly focus on dynamic adjustments of temperature and threshold\nin this work."}, {"title": "4 MAIN EXPERIMENTS", "content": "We evaluate B-STAR's effectiveness in enhancing self-improvement for mathematical problem-\nsolving, coding challenges and commonsense reasoning. Our evaluation compares several baseline\nmethods: STaR/ResT-EM (Zelikman et al., 2022; Singh et al., 2023), Iterative RFT, and Online\nRFT (Shao et al., 2024). The STaR/ResT-EM approach involves multiple iterations, where each\niteration samples from the latest policy model but resets and retrains the model from scratch. In\ncontrast, Iterative RFT builds on each previous iteration by inheriting the checkpoint and resuming\ntraining from that point. Online RFT goes a step further by not only inheriting the checkpoint but\nalso the optimizer state and learning rate schedule, ensuring a smoother transition across iterations.\nAdditionally, we implement these methods with two types of reward function: \"without RM\" (Answer)\nand \"with RM\" (Answer+PRM), as described in \u00a72.3. We also provide detailed experimental setup in\nAppendix B.\nFor mathematical problem-solving, we use the Mistral-7B model as our base and conduct SFT on the\nMATH training dataset for three epochs. The first epoch serves as the starting point for the model's\nself-improvement phase. During this phase, we use Pass@1, Pass@K-S, and Reward@K-S metrics\nto track changes in the performance and model's exploration and exploitation capabilities. On coding\nchallenges, we follow Singh et al. (2023) and adopt the APPS (Hendrycks et al., 2021) dataset for\nboth training and testing. We use Llama-3-8B (Dubey et al., 2024) as our base model, keeping\nthe rest of the settings consistent with those of the math domain. We do not apply reward models\nto the coding task and instead use unit tests as the binary reward, which means only the sampling\ntemperature is automatically adjusted in B-STAR.\nFor commonsense reasoning, following Pang et al. (2024), we conduct experiments on ARC-\nChallenge (Clark et al., 2018), a dataset consisting of multiple-choice science questions designed\nto evaluate commonsense reasoning beyond mathematics and coding challenges. We start with the\nMistral-7b-instruct model and omit the SFT stage due to the absence of the CoT data for this dataset.\nOther configurations, such as sample size and temperature, are the same as those used in the coding\ntasks. The ground-truth answer serves as the binary reward, and we report only Pass@1 results\nfor the ARC-Challenge dataset. Given the constrained response space inherent to multiple-choice"}, {"title": "4.1 SETUP", "content": "We evaluate B-STAR's effectiveness in enhancing self-improvement for mathematical problem-\nsolving, coding challenges and commonsense reasoning. Our evaluation compares several baseline\nmethods: STaR/ResT-EM (Zelikman et al., 2022; Singh et al., 2023), Iterative RFT, and Online\nRFT (Shao et al., 2024). The STaR/ResT-EM approach involves multiple iterations, where each\niteration samples from the latest policy model but resets and retrains the model from scratch. In\ncontrast, Iterative RFT builds on each previous iteration by inheriting the checkpoint and resuming\ntraining from that point. Online RFT goes a step further by not only inheriting the checkpoint but\nalso the optimizer state and learning rate schedule, ensuring a smoother transition across iterations.\nAdditionally, we implement these methods with two types of reward function: \"without RM\" (Answer)\nand \"with RM\" (Answer+PRM), as described in \u00a72.3. We also provide detailed experimental setup in\nAppendix B.\nFor mathematical problem-solving, we use the Mistral-7B model as our base and conduct SFT on the\nMATH training dataset for three epochs. The first epoch serves as the starting point for the model's\nself-improvement phase. During this phase, we use Pass@1, Pass@K-S, and Reward@K-S metrics\nto track changes in the performance and model's exploration and exploitation capabilities. On coding\nchallenges, we follow Singh et al. (2023) and adopt the APPS (Hendrycks et al., 2021) dataset for\nboth training and testing. We use Llama-3-8B (Dubey et al., 2024) as our base model, keeping\nthe rest of the settings consistent with those of the math domain. We do not apply reward models\nto the coding task and instead use unit tests as the binary reward, which means only the sampling\ntemperature is automatically adjusted in B-STAR.\nFor commonsense reasoning, following Pang et al. (2024), we conduct experiments on ARC-\nChallenge (Clark et al., 2018), a dataset consisting of multiple-choice science questions designed\nto evaluate commonsense reasoning beyond mathematics and coding challenges. We start with the\nMistral-7b-instruct model and omit the SFT stage due to the absence of the CoT data for this dataset.\nOther configurations, such as sample size and temperature, are the same as those used in the coding\ntasks. The ground-truth answer serves as the binary reward, and we report only Pass@1 results\nfor the ARC-Challenge dataset. Given the constrained response space inherent to multiple-choice"}, {"title": "4.2 RESULTS", "content": "Main Results. Table 1 provides a comprehensive comparison of B-STAR with various self-\nimprovement methods, including Rest-EM, Iterative RFT, online RFT, and their reward model\nvariants in mathematical problem-solving, coding challenges and commonsense reasoning. The re-\nsults show that B-STAR consistently achieves higher Pass@1 scores across all scenarios, highlighting\nits ability to effectively steer the model toward correct responses via greedy decoding. Moreover,\nB-STAR demonstrates significantly better Pass@K-S values, reflecting an enhanced exploration\ncapacity that facilitates the generation of a wider range of high-quality responses. Notably, online\nRFT outperforms predominantly offline methods like Rest-EM, illustrating that dynamic, on-policy\napproaches strike a more effective balance between learning efficiency and performance gains.\nAs shown in Figure 1, all the self-improvement methods exhibit significant performance improvement\nafter the first iteration. However, as the number of iterations increases, the growth trends of other\nbaseline methods slows down and eventually stagnate. In contrast, B-STAR maintains a substantial\ngrowth rate and consistently outperforms other baselines. This suggests that balancing exploration\nand exploitation is crucial for achieving stable and efficient self-improvement, further validating the\ngeneralizability of B-STAR.\nMoreover, in Figures 5 and 6, we present the dynamic evolution of B-STAR and Online RFT\nthroughout the self-improvement process in mathematical problem-solving. Figure 6(c) highlights\nB-STAR's effectiveness in balancing exploration and exploitation-related configurations, resulting in\na markedly higher balance score. This optimal balance drives the consistently higher Pass@K-S and\nReward@K-S scores observed in Figures 5 and 6 across both datasets. These results suggest that\nB-STAR not only encourages the model to generate a diverse range of accurate responses but also\nefficiently incorporates feedback from the reward model.\nConfiguration Adjustments by B-STAR. Table 2 presents the configurations automati-\ncally adjusted by B-STAR at various training stages, along with the resulting balance scores. Early\nin training, B-STAR employs a lower sampling temperature, typically around 0.5, which gradually\nincreases as training advances. This gradual increase promotes cautious exploration during the\ninitial phases, allowing the model to stabilize before expanding to broader sampling in later stages.\nConcurrently, B-STAR enforces stricter reward thresholds at the start, relaxing them as the model\nbecomes more proficient. These strategies, as discussed in Section 3.2, enable B-STAR to maintain\nan effective balance between exploration and exploitation throughout the training process."}, {"title": "5 DISCUSSION", "content": "In this paper, we reveal the importance of the balance between exploration and exploitation in self-\nimproving training, and we propose B-STAR, a novel algorithm to strike a better balance and achieve\nsuperior performance on mathematical reasoning, coding, and commonsense reasoning. While this\npaper adopts a simple method that manipulates exploration and exploitation through hyperparameter"}, {"title": "A.1 EXPERIMENT SETUP", "content": "Datasets We use the MATH dataset for training and validate the model's mathematical reasoning\nability using test sets from both the MATH (Hendrycks et al., 2021) and GSM 8K (Cobbe et al., 2021)\ndatasets. For the MATH dataset, we follow previous settings (Lightman et al., 2023; Wang et al.,\n2024b; Sun et al., 2024) by using a subset of 500 representative problems (MATH500) as our test\ndata. We uniformly sample an additional 500 problems for validation and use the remaining 4,000\nproblems from the MATH test set along with the original 7,500 training problems as our training\ndata.\ndetails For SFT, we use Mistral-7B (Jiang et al., 2023) as the base model with a\nlearning rate of 5e-6, a batch size of 128, and train for 3 epochs. After the first epoch, the model\n(denoted as Po) is used as the starting point for self-improvement. We then proceed with 9 iterations,\nwhere each iteration consists of 500 training steps with a batch size of 128. At the beginning of each\niteration, we sample 32 candidate responses for each query, using a temperature of 1.0.\nFor the Process Reward Model (PRM), we automatically generate process annotations following\nthe MATH-Shepherd approach (Wang et al., 2024b). Using the SFT model trained for 1 epoch, we\nsample 15 responses for each query in the training set. The SFT model trained for 3 epochs serves as\nthe completer, decoding 8 solutions per step to annotate the sampled data. This process results in\napproximately 270 K process reward annotations. We train the reward model using the Mistral-7B\nbase, with a learning rate of 2e-6, for 2 epochs. During rewarding, we use the lowest step score in the\nsolution as the PRM Reward, normalize it to a range of [-1, 1] and combine it with a sparse reward to\nform the final reward score. We set the reward threshold to 0.0, selecting only those responses with\nfinal reward scores exceeding this threshold."}, {"title": "A.2 RESULTS ON GSM8K", "content": "In Figure 7, we present the changes in Pass@1, Diversity, Pass@K-S, and Reward@K-S metrics on\nGSM8K. Our results indicate that incorporating PRMs leads to additional improvements on GSM8K.\nHowever, these improvements are less pronounced on MATH. We hypothesize that this is due to the\ndifficulty of MATH problems, which may exceed the discriminative capabilities of the 7B reward\nmodel."}, {"title": "B EXPERIMENT SETUP FOR MAIN EXPERIMENTS", "content": "For mathematical problem-solving, we largely maintain the experimental setup from \u00a72.3. We use the\nMistral-7B model as our base and conduct SFT on the MATH training dataset for three epochs. The\nfirst epoch serves as the starting point for the model's self-improvement phase. We set the number\nof samples per iteration (N) to 67,500 and feed 11,500 MATH training queries (M) per iteration.\nWe set sample size to 64 for all methods. We vary temperature from 0.5 to 1.2 in 0.1 increments\nand reward threshold from -1.0 to 1.0 in 0.1 increment. Additionally, finer increments for both the\ntemperature and reward threshold are explored in Appendix D. Throughout the self-improvement\nprocess, we use Pass@1, Pass@K-S, and Reward@K-S metrics to track changes in the performance\nand model's exploration and exploitation capabilities."}, {"title": "B.1\nMATHEMATICAL PROBLEM-SOLVING", "content": "For mathematical problem-solving, we largely maintain the experimental setup from \u00a72.3. We use the\nMistral-7B model as our base and conduct SFT on the MATH training dataset for three epochs. The\nfirst epoch serves as the starting point for the model's self-improvement phase. We set the number\nof samples per iteration (N) to 67,500 and feed 11,500 MATH training queries (M) per iteration.\nWe set sample size to 64 for all methods. We vary temperature from 0.5 to 1.2 in 0.1 increments\nand reward threshold from -1.0 to 1.0 in 0.1 increment. Additionally, finer increments for both the\ntemperature and reward threshold are explored in Appendix D. Throughout the self-improvement\nprocess, we use Pass@1, Pass@K-S, and Reward@K-S metrics to track changes in the performance\nand model's exploration and exploitation capabilities."}, {"title": "B.2 CODING CHALLENGES", "content": "On coding challenges, we follow Singh et al. (2023) and adopt the APPS (Hendrycks et al., 2021)\ndataset for both training and testing. To balance the number of responses per question, we sample 5\nresponses per question from the original APPS training set forming a dataset with 13K examples. We\nuse Llama-3-8B (Dubey et al., 2024) as our base model, keeping the rest of the settings consistent\nwith those of the math domain. For baselines, we uniformly sample 32 candidate responses per query\nwith a temperature of 0.4. For B-STAR, we explore temperatures 0.4 to 1.1 in 0.1 increment to\ndetermine the optimal configuration. We set the number of samples per iteration (N) to 13,500 and\nfeed 2627 APPS training queries (M) per iteration. We do not apply reward models to the coding\ntask and instead use unit tests as the binary reward, which means only the sampling temperature is\nautomatically adjusted in B-STAR."}, {"title": "B.3 COMMONSENSE REASONING", "content": "For commonsense reasoning, following Pang et al. (2024), we conduct experiments on ARC-\nChallenge (Clark et al., 2018), a dataset consisting of multiple-choice science questions designed\nto evaluate commonsense reasoning beyond mathematics and coding challenges. We start with the\nMistral-7b-instruct model and omit the SFT stage due to the absence of the CoT data for this dataset.\nOther configurations, such as sample size and temperature, are the same as those used in the coding\ntasks. The ground-truth answer serves as the binary reward, and we report only Pass@1 results\nfor the ARC-Challenge dataset. Given the constrained response space inherent to multiple-choice\nquestions, Pass@K and Pass@K-S metrics (where K > 1) yield no additional insights and are\ntherefore excluded."}, {"title": "C DETAILS OF SELF-IMPROVEMENT", "content": "A fixed reward function r(x, y) is a predefined, static function that does not adapt based on the\ntraining process or model parameters. For instance, binary feedback (e.g., whether a"}]}