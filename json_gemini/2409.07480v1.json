{"title": "EEG-LANGUAGE MODELING FOR PATHOLOGY DETECTION", "authors": ["Sam Gijsen", "Kerstin Ritter"], "abstract": "Multimodal language modeling constitutes a recent breakthrough which leverages advances in large language models to pretrain capable multimodal models. The integration of natural language during pretraining has been shown to significantly improve learned representations, particularly in computer vision. However, the efficacy of multimodal language modeling in the realm of functional brain data, specifically for advancing pathology detection, remains unexplored. This study pioneers EEG-language models trained on clinical reports and 15000 EEGs. We extend methods for multimodal alignment to this novel domain and investigate which textual information in reports is useful for training EEG-language models. Our results indicate that models learn richer representations from being exposed to a variety of report segments, including the patient's clinical history, description of the EEG, and the physician's interpretation. Compared to models exposed to narrower clinical text information, we find such models to retrieve EEGs based on clinical reports (and vice versa) with substantially higher accuracy. Yet, this is only observed when using a contrastive learning approach. Particularly in regimes with few annotations, we observe that representations of EEG-language models can significantly improve pathology detection compared to those of EEG-only models, as demonstrated by both zero-shot classification and linear probes. In sum, these results highlight the potential of integrating brain activity data with clinical text, suggesting that EEG-language models represent significant progress for clinical applications.", "sections": [{"title": "1 Introduction", "content": "Medical neuroimaging such as electroencephalography (EEG) has not yet benefited to the same extent as other domains from the considerable advances deep learning has brought about. While EEG sees widespread clinical use for pathology detection, in particular for epilepsy [1, 2] as well as sleep disorders [3], available annotated data is scarce. As the impressive scaling properties of deep learning are now well described [4, 5], self-supervised learning (SSL) is a promising direction by enabling pretraining with unlabeled data and thereby increasing available training sample sizes [6, 7]. Various such approaches have shown initial success when applied to EEG. These include methods relying on data-augmentations [8, 9], the temporal ordering of EEG data [10], as well as masking and reconstruction [11]. However, these are hindered by the difficulty of creating appropriate data augmentations and, especially reconstruction techniques, by low signal-to-noise. Thus, progress in the medical context has lagged, likely further exacerbated by the modality displaying high similarity between pathologies."}, {"title": "2 Related work", "content": "\u2022 Self-supervised learning with EEG data. SSL with EEG data has been predominantly applied to emotion recognition [18, 19], motor imagery [20, 21], sleep staging [9, 21], as well as pathology detection. For the latter application, the temporal order of EEG crops was used initially to demonstrate label-efficient representation learning [10]. Contrastive learning, combined with larger EEG encoders trained on multiple datasets, further improved pathology detection [8]. In this case, the model was trained for invariance to a set of data augmentations, based on the SimCLR work in computer vision [7]. Whereas most work uses convolutional and recurrent architectures, recent studies have explored"}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Experimental Setup", "content": ""}, {"title": "3.1.1 EEG-language pretraining", "content": "Here we detail the setup for pretraining EEG-language models (ELMs). Whereas vision-language models are typically trained by aligning a 2D image with a short caption [13, 12], EEG-language modeling is confronted with long EEG time series and multi-paragraph medical reports. To overcome this, we employ text segmentation and time series cropping to create multiple non-overlapping samples per modality and subject. Next, we propose sub-unit alignment by pretraining on these cropped samples. In addition to considerably increasing sample size, this enables the extension of successful approaches in vision-language models. Specifically, we compare two strategies in combination with sub-unit alignment. First, EEG and text representations may be projected using neural networks to a new, shared latent space prior to alignment (as in CLIP; [13, 12]), denoted henceforth as ELMe,l. Alternatively, the EEG embeddings may be projected into the output space of the language model (as in M-FLAG by [17]), denoted as ELM\u012e.\nFor EEG-language pretraining we assume the paired input $(x_{e,i}, x_{l,i})$. Here $x_{e,i} \\in R^{c \\times s}$ denotes one or a batch of crops of EEG signal with e channels and s time samples belonging to EEG recording i. Meanwhile, neural signals of recording i as well as patient information is described in $x_{l,i}$, which represents a natural language text report. The main goal is to train the EEG encoder function $f_e$, which projects a crop of EEG signal into a vector of lower dimensionality. Following pretraining, this encoder function $f_e$ can be used for downstream applications such as pathology detection.\nDropping the recording subscript i for brevity, each pair $(x_e, x_l)$ is projected into the vectors $e \\in R^d$ and $l \\in R^d$ respectively. For every $x_e$, text of the associated report is sampled according to $z_l = z_l(x_l)$, where $z_l$ represents the language sampling function detailed below. First, both the EEG crop $x_e$ and text $x_l$ are encoded into vectors $h_e$ and $h_l$. For ELMe,l, we use projectors $g_e$ and $g_l$ to yield vectors e and l, whereas for ELM\u012e the text embedding are not projected:\n$e = g_e (f_e (x_e))$ (1)\n$l=\\begin{cases} g_l (f_l (x_l)) & \\text{if ELM}_{e,l} \\\\ f_l (x_l) & \\text{if ELM}_l \\end{cases}$ (2)\nTo enable multimodal pretraining, the projectors $g_e$ and $g_l$ map e and l to a shared latent space with identical dimensionality d. For ELM\u012e, this is achieved by having $g_e$ project to the native dimensionality of the text encoder $f_l$.\nAs paired medical EEG data and clinical reports are scarce, training the text encoder function $f_l$ from scratch is unlikely to be successful. Furthermore, employing an existing language model and finetuning the model during multimodal pretraining can lead to training instability and collapse of the latent space [16, 17]. To prevent resulting information"}, {"title": "3.1.2 EEG-only self-supervised learning", "content": "We compare the representations learned by EEG-language pretraining to those obtained via EEG-only pretraining. First, we employ multiple methods that train for invariance to data augmentations. This is achieved by sampling data augmentations for each EEG crop $x_e$, resulting in two differing data views {$x_e, x_e'$}. It is important the data augmentations do not destroy the semantic information in $x_e$. Training to align the embeddings of these views while preventing collapse has been shown to yield data representations useful for downstream tasks [7, 8, 9]. We implement the following methods (Appendix A.5): Bootstrap-Your-Own-Latent (BYOL; [41]), Variance-Invariance-Covariance Regularization (VICReg; [42]), and Contrast with the World Representation (ContraWR; [9]). Additionally, we compare against methods using the temporal ordering of EEG crops: Relative Positioning (RP; [10]), Temporal Shuffling (TS; [10]), Contrastive Predictive Coding (CPC; [10])."}, {"title": "3.2 Datasets and preprocessing", "content": "\u2022 TUEG. The Temple University Hospital (TUH) EEG Corpus is the largest available corpus of hospital EEG data with varying montages, channel counts, and sampling frequencies (n=26846 [15]). For each patient, one or more EEG sessions are provided, each of which contains one or more recordings. For most of the dataset, no labels are available beyond patient age and sex. However, each EEG session is associated with a natural-language clinical report, containing information about the patient such as demographics and clinical history, as well as the physician's impression of EEG recording and clinical status.\n\u2022 TUAB. The TUH Abnormal EEG corpus is a subset of TUEG which was manually labeled by clinicians indicating whether the EEG displays pathological abnormalities [43]. This enables the binary classification task of predicting the"}, {"title": "3.2.1 Text processing", "content": "In order to categorize the textual content in the clinical reports, we employed regular expressions matching for commonly- occurring headings (an overview is provided in Appendix E.2). These enabled the segmentation of individual reports into their respective headings with associated text paragraphs, providing insight into which information in physician reports is encoded in the EEG. We cluster headings into four categories. First, the clinical history cluster contains demographic information in terms of patient age and sex, as well as a brief description of relevant current and/or past pathology and symptoms. The record description cluster includes the physician's observations of the EEG traces, which describes both normal and abnormal features, often in terms of oscillatory brain activity. The medication cluster contains the patient's current medication information. Finally, the interpretation cluster summarizes a physician's thoughts, often including the impression of whether the EEG is normal or pathologically abnormal, as well as a clinical correlation. To investigate whether EEG-language models can learn richer representations by being exposed to a larger variety of text, we also train models by sampling text from all four aforementioned clusters. In order to categorize the textual content in the clinical reports, we employed regular expressions matching for commonly-occurring headings (an overview is provided in Appendix E.2). These enabled the segmentation of individual reports into their respective headings with associated text paragraphs, providing insight into which information in physician reports is encoded in the EEG. We cluster headings into four categories. First, the clinical history cluster contains demographic information in terms of patient age and sex, as well as a brief description of relevant current and/or past pathology and symptoms. The record description cluster includes the physician's observations of the EEG traces, which describes both normal and abnormal features, often in terms of oscillatory brain activity. The medication cluster contains the patient's current medication information. Finally, the interpretation cluster summarizes a physician's thoughts, often including the impression of whether the EEG is normal or pathologically abnormal, as well as a clinical correlation. To investigate whether EEG-language models can learn richer representations by being exposed to a larger variety of text, we also train models by sampling text from all four aforementioned clusters.\nDue to the heterogeneity of the clinical reports, we further test the utility of summarizing the pathological status indicated by the clinical report using a large language model (LLM). Due to the sensitive nature of the clinical reports, we use the Llama-3 8B model [44] locally and instruct for the production of a single-sentence summary of a report, which should include whether the EEG was deemed abnormal and for which reasons (Appendix E.2).\nLanguage encoding. Given a sampled paragraph from a clinical report or the LLM-generated summary, we encode this text by relying on the embedding of a special [cls] token which aggregates the representations across all tokens. As such, given a clinical report $x_l$, the transformation function $z_l$ corresponds to text segmentation or summarization yielding l. Following tokenization, we embed into the [cls] token using $f_l$. The resulting text embedding $h_l$ may be used for multimodal pretraining."}, {"title": "3.2.2 EEG processing", "content": "From the EEG dataset, recordings longer than 2.5 hours were omitted to filter out a small subset of very long, potentially overnight recordings. For training efficiency, only the first 45 minutes of a recording were used. Any recording files shorter than 70 seconds were also omitted.\nEEG preprocessing. EEG data received minimal preprocessing (using MNE [45]). First, the first 10 seconds were removed to reduce the impact of set-up artefacts. Afterwards, a bandpass filter of 0.1-49 Hz was applied and all recordings were resampled to 100 Hz. To reduce the impact of signal artefacts, all EEG signals had their amplitude clipped to \u00b1 800 \u00b5V. As a large majority of recordings used an average-reference (AR) or linked-ear reference (LE), we only used these recordings and standardized them via transformation to the 20-channel Temporal Central Parasagittal (TCP) montage. To enable fair comparisons between methods, the optimal crop-length out of {5,10,20,30,60} seconds was determined without data-leakage through training and evaluation on subsets of the training data only (Appendix A.2). Based on these results, we used 20 and 60 second crops for EEG-only and EEG-language modeling respectively."}, {"title": "3.2.3 Data subsampling", "content": "TUEG contains considerably more abnormal than normal EEGs. As vision-language models have been shown to be sensitive to imbalanced classes [46], we subsample the data to create approximately equal class representation. To do so, we rely on the LLM summaries of reports, which facilitated report classification based on regular expressions due to reoccurring phrasing. If any data of a subject was present in the retrieval or TUAB test set, all their data was excluded"}, {"title": "3.2.4 Retrieval", "content": "To investigate the information represented in the learned embeddings resulting from EEG-language training, we perform retrieval analyses. Given a medical report describing the patient and their EEG recording, we probe the ability to recover the patient's EEG by rank-ordering candidate EEG based on embedding similarity. This analysis is also performed in opposite direction, by retrieving the associated report given an EEG recording. As reports refer to an entire recording, EEG embeddings of single crops are averaged. Given that reports generally consist of multiple paragraphs, embeddings of single paragraphs are averaged too. This procedure yields one EEG and report embedding per patient recording, which we use for rank-ordering based on cosine similarity.\nThe top-K retrieval accuracy, which scores whether the patient's EEG or report has a rank equal or better than K, is plotted in Figure 2. Many models perform considerably above chance level, indicating the successful generalization of learned multimodal EEG-language information. For both EEG and report retrieval, ELMe,l models tend to outperform ELM models. However, this discrepancy in performance is particularly prevalent for report retrieval. This is likely due to omission of a text projection head in ELM\u012e, which may therefore lack the flexibility to appropriately separate the EEG reports in latent space. This is likely exacerbated by the current application domain: while we employ a language model finetuned for the medical domain, EEG-relevant text will only represent a small minority of the training data. Therefore, due to the similarity of the clinical reports, they may span only a relatively small part of the latent space. Meanwhile, ELMe,l performs similarly for EEG and report retrieval."}, {"title": "3.2.5 Pathology classification", "content": "Next, we study the learned representations in their relevance to clinical pathology. To do so, we first train linear probes to detect pathology on the representations of pretrained models on TUAB under varying amounts of labels (Table 2; Appendix C). Models are trained on single EEG crops, across which we average predictions to obtain a recording-level prediction. We find EEG-language pretraining yields large improvements for pathology detection over EEG-only pretraining, with multimodal models being particularly effective at small sample sizes: at 1% of exposed labels, performance increases reach 7.7% balanced accuracy and 8.4% AUROC. In general, models with the best retrieval performance, also tend to perform better pathology detection. Nevertheless, relative performance differences between EEG-language models are smaller than for retrieval, suggesting that all models obtained representations useful for the narrower task of binary pathology detection.\nLanguage-independent effects of multimodal pretraining. Given the broad outperforming of ELMs compared to EEG-only models, especially for ELMe,l, we further investigate whether the general setup of multimodal pretraining provides inherent benefits. EEG recordings are split into multiple crops, which in turn are all aligned to the same clinical report during pretraining. It follows that EEG crops of a single recording are indirectly aligned to one another to some extent (Figure 1C). We investigated this hypothesis by shuffling reports between patients prior to pretraining. We find that while embeddings of single EEG crops of an untrained encoder are only minimally more similar within-subject than between-subject (ratio of ~1.1x), this effect is much more pronounced after pretraining ELMe,l on correctly paired reports (~6.3x), and even more so after pretraining on shuffled reports (~15.7x; figure 3). Linear probing reveals that training ELMe, on shuffled reports clearly boosts pathology detection over using an untrained encoder and manages to almost match EEG-only pretraining without the need for augmentations (mean accuracies of 73.70%, 81.04%, 83.69%). On the contrary, the ratios for ELM are close to 1 after training using paired and shuffled reports, with the latter resulting in decreased pathology detection accuracy."}, {"title": "Zero-shot pathology detection", "content": "Next, we investigate the unique ability of multimodal language modeling to leverage the language modality to perform 'zero-shot' classification. Without any explicit labels for downstream training, EEG may be classified by computing its similarity in latent space to text prompts representing the candidate classes. As suggested by [12], we create a prompt ensemble over 21 variations of the phrasing \"The EEG is {normal, abnormal}\" (Appendix C). Results in table 3 indicate that, despite a small dataset, EEG-language models can reach high levels of zero-shot pathology detection. The best models outperform all linear probes at 1% labels and even match EEG-only models at 100% labels. The clinical history, medication, and description models perform poorly, which follows from these models not being exposed to the explicit phrasing indicating the EEG status as normal or abnormal per se. Their performance can likely be improved by designing appropriate prompts.\nNotably, while the ELMe,l models trained on either the interpretation cluster or all clusters both perform well with high consistency, training on LLM summaries resulted in highly variable scores. As the LLM-generated text was considerably more uniform with repetitive phrasing across reports, the lack of variability in combination with limited data may have lead to unstable language representations of the text projector. Meanwhile, we observe the opposite pattern for ELM\u012e, where LLM summaries enabled the only consistently high-performing zero-shot classifier. As with the report retrieval analysis, the fixed text representations of a language model which is not finetuned for EEG is likely inadequate to reliably separate between diverse descriptions of pathological and normal EEG. Meanwhile, the rigid LLM-generated text may have aided in this scenario by consistently yielding divergent text representations with which normal and abnormal EEG may be aligned. It must be noted that the current set-up may have favoured the LLM-generated summaries as they were used to subsample the dataset into a balanced subset. However, in this case it is likely this filtering step also aided other models by omitting some EEGs for which the pathology status was uncertain."}, {"title": "4 Discussion", "content": "The current work presents a first application of multimodal pretraining using natural language and functional brain data in a medical context. Our findings indicate that EEG-language pretraining provides better representations than EEG-only SSL. The richest representations appear to be obtained via a combination of contrastive learning and exposure to a variety of textual information. Specifically, combining information regarding the patient's clinical history and medication as well as the physician's description and interpretation of the EEG considerably improved retrieval performance. Such multimodal models were also found to be capable of zero-shot pathology detection. Using linear probing, sizable performance improvements over EEG-only SSL were observed, with the largest gains in contexts with few annotated samples.\nSome considerations of the current study deserve mention. Due to clear data privacy concerns, no additional paired EEG-report datasets are publicly available, which currently prevents assessing the generalizability of our results across datasets. Although great care was taken to prevent data leakage and no model development involved any of the evaluation data, future work is required to properly study generalizability and scaling behavior as demonstrated in [22, 11]. While the retrieval analyses suggest that certain models learn richer data representations, a lack of annotations hindered a more detailed assessment of their utility for downstream tasks. Future research could benefit from annotations for specific pathologies, enabling more precise model comparisons. Additionally, we observed lower pathology detection scores for EEG-only SSL than a previous study [8], despite using the same data augmentations. Their work pretrained larger models on multiple datasets to output sequential representations. However, many such adaptations to the EEG encoder or its training could also be applied to EEG-language modeling. Finally, although several models displayed accurate zero-shot pathology detection, the variability in results may be due to the challenges of language modeling with limited data. Further research is needed to explore additional inductive biases or regularization of the text projector to address this issue."}, {"title": "5 Acknowledgments and Disclosure of Funding", "content": "This research was funded by the Deutsche Forschungsgemeinschaft (DFG) through FOR 5187 (project number 442075332). Additional support was provided by the DFG through the following projects: CRC 1404 (project number 414984028), TRR 265 (project number 402170461), and RU 5363 (project number 459422098).\nThe data used in this study was provided by the Neural Engineering Data Consortium at Temple University. For further details about this data, please access the following URL: https://isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml.\nThe authors declare no competing interests."}, {"title": "A Training Details", "content": "In this section, we provide further detailed information of the model training. Unless stated otherwise, ablation and hyperparameter analyses were performed on a data subset consisting of 5000 and 500 EEG recordings divided into a training and test set respectively. To prevent data leakage, this data had no overlap with the patients used for evaluation of the main results."}, {"title": "A.1 Optimization", "content": "All models are trained using the LARS optimizer [47] with a cosine decay learning rate schedule over 50 epochs, with a warm-up of 4 epochs. The base learning rate is set to 0.3 for EEG-only and 0.01 for EEG-language models, scaled with the batch size (0.2 \u00d7 BatchSize/256 [41]). We use a weight-decay parameter of 1 \u00d7 10\u22124. Models were trained on either an Nvidia Geforce GTX 3090 or Tesla V100 GPU and require less than 24GB of memory. Training took approximately 9 hours for EEG-language modeling or 18 hours for EEG-only modeling due to data augmentations. We used CUDA v12.3 and PyTorch v1.12.1."}, {"title": "A.2 EEG Encoder", "content": "We use a CNN architecture with a residual stream as the EEG encoder for all analyses (Figure 4). The model uses parallel convolutions, involving reflection padding and 1D-convolutions with kernel sizes {4, 8, 16} with 32 filters each. These outputs are concatenated, resulting in a 96 dimensional representation and 747K trainable parameters. We compare input lengths of EEG crops varying from 5 to 60 seconds. This presents a trade-off where longer crops result in a greater information content per crop, while reducing the total sample size. As EEG-only pretraining relies on data augmentations, this introduces an additional influence of crop length. Specifically, longer crop lengths likely make the pretraining task easier, as augmentations introduce relatively lesser distortion due to the greater information content. We therefore compare performance of different crop lengths for both EEG-language and EEG-only pretraining. As the EEG encoder progressively downsamples the signal, we adjust the pooling layers to the input length. These adjustments are shown in Table A.2. For EEG-language pretraining we evaluate zero-shot pathology detection, while for EEG-only pretraining we are required to compare the performance of a linear probe. Results are shown in Figure 5. Due to computational resources, we only compare crop lengths for BYOL and ELM\u2081 as representations of EEG-only and EEG-language modeling. We observe that for EEG-only pretraining an intermediate crop-length of 20 seconds performs best, which matches the findings by [8]. Meanwhile, zero-shot pathology detection is found to be insensitive to crop lengths of at least 10 seconds, with 60 second crops scoring highest, while the shortest crop length consistently leads to unstable text representations and chance-level performance.\nFor the EEG projector, we use a linear layer with an output dimension of 512 followed by batch normalization, exponential linear units, and a final linear layer with output size 256."}, {"title": "A.3 Language Encoder", "content": "We compare three pretrained language models in their ability to perform zero-shot pathology detection following EEG-language pretraining (Table 5). We find that MedCPT performs best [39], which is trained using contrastive learning with 255 million user click logs from PubMed.\nFor the text projector of ELMe,l, we use a linear layer with output size 1024 followed by batch normalization, rectified linear units, and a final linear layer with output size 256 and batch normalization."}, {"title": "A.4 Temperature parameter", "content": "For ELMe,1, the softmax operation used in the loss computation includes a temperature hyperparameter 7. We compare zero-shot pathology detection for multiple values. We observe poor performance for low temperature values, but stable zero-shot classification for higher parameter values. We set 7 = 0.3 for all further analyses."}, {"title": "A.5 EEG-Only Pretraining", "content": "We implement the following three methods for EEG-only SSL:\nBootstrap-Your-Own-Latent. BYOL relies on two encoder models: an online and a target network [41]. During pretraining, the online network is trained to predict the target model's output. Meanwhile, the weights of the target network are updated using a moving average of the weights of the online network, which has been empirically shown to prevent collapse of the latent space. For alignment, l2 normalization is applied to the EEG embeddings {h,h\"} and the mean square distance is minimized. We adopt the recommended parameter value for the exponential moving average [41]. The projection head is a 2-layer non-linear MLP with a hidden dimension of width 256 and an output dimension of 32.\nVariance-Invariance-Covariance Regularization. VICReg allows for the use of a single encoder model and prevents collapse by applying two explicit regularization terms to each of the embedding batches {h, h\"} [42]. The 'variance' term maintains the standard deviation (computed batch-wise) of every embedding dimension above a threshold, thereby avoiding a trivial solution. In addition, latent collapse is avoided through the 'covariance' term which decorrelates pairs of embedding dimensions. The method minimizes the mean square distance between {h, h\"}. Hyperparameters are set to their recommended values [42]. The projection head is a 2-layer non-linear MLP with a hidden dimension of width 256 and an output dimension of 256.\nContrast with the World Representation. ContraWR was proposed to improve augmentation-based SSL for EEG [9]. The method, which is contrastive in nature, maximizes similarity between {h,h\"} while preventing collapse by minimizing similarity with 'negative' samples. ContraWR forms a negative representation by aggregating across all negative batch elements, aiming to compensate for the low signal-to-noise of EEG data by creating a more reliable negative contrast. It relies on a triplet loss based on Info-NCE [49]. We also here set the hyperparameters to the values recommended by the authors [9]. The projection head is a 2-layer non-linear MLP with a hidden dimension of width 256 and an output dimension of 32.\nRelative Positioning. Pairs of EEG crops are sampled and assigned binary labels based on their temporal proximity [10]. Crops close in time are labeled positive, while those far apart are labeled negative. We use the same EEG encoder as for all other methods to create representations and use the suggested contrastive module to compute the element-wise absolute difference between representations. A logistic regression model then predicts the label. The method is trained using binary logistic loss. For all methods by [10], we use the hyperparameters reported to work best on TUAB, including between-subject sampling of EEG crops.\nTemporal Shuffling. An extension of Relative Positioning by sampling triplets of EEG crops. The task is to determine whether the crops are in temporal order or shuffled [10]. The contrastive module concatenates absolute differences between representations. As with Relative Positioning, a logistic regression model is used for prediction, and the method is trained end-to-end using binary logistic loss.\nContrastive Predictive Coding. This method uses an autoregressive encoder to summarize a sequence of EEG crops into a context vector [10]. The task is to predict which future crop actually follows the context, among negative samples. A bilinear model is used for prediction at each future step. The method is trained end-to-end using the InfoNCE loss."}, {"title": "A.5.1 Data augmentations", "content": "For EEG-only pretraining, we adapt the data augmentations proposed by Mohsenvand et al.[8], which were found to perform well on the TUAB dataset. For a given EEG crop, we apply the same augmentation to each channel. Parameters are sampled independently for each EEG crop and uniformly from the ranges displayed in table 6. Augmentations are visualized for a single EEG channel in figure 7."}, {"title": "B Details on Data Subsampling", "content": "To alleviate class imbalance in the TUEG dataset, we perform data subsampling. We rely on the LLM summaries of reports, which were more consistent in their phrasing regarding the normal or abnormal status. This allowed for a more reliable classification using regular expressions. All reports for which no clear classification was made were omitted. 5015 reports in the potential training set were classified as normal, which were associated with 7526 EEG recordings. For our 'pretrain' data subset, we subsampled the abnormal EEGs to match the amount of normal EEG recordings. This resulted in 7526 abnormal EEG recordings, with 6770 reports. Although only a minor subset of these preliminary classifications was manually verified, it is important to note that this process was solely to alleviate severe class imbalance and was not used for further analysis."}, {"title": "C Classification", "content": "To study the predictive capability of learned representations after pretraining, we train linear probes and perform zero-shot classification.\nLinear probe\nFor linear evaluation, we train logistic linear regression models using 10-fold cross validation for each pretrained model using sklearn [51]. We perform grid-search over 45 logarithmically-spaced values for L2 regularization between 10-6 and 105 via a validation set.\nZero-shot classification\nFor zero-shot pathology detection, we perform an ensemble over 21 binary prompts, listed in Table 7. Prompt ensembling was shown to improve performance [12], but we employ it here also as the limited data is likely to lead to less stable representations, which may lead to sensitivity to phrasing. To inspect whether results are sensitive to changes to the prompt set, we perform a post-hoc analysis using the held-out test set that iteratively leaves one prompt out of the ensemble (Figure 8). We observe that results are consistent across such reduced prompt sets, except for the ELM model trained on the clinical history or interpretation clusters, although neither model reaches competitive performance. This set was only initially verified on the training set to enable model- and parameter-comparisons using zero-shot performance. Tuning is likely to enable further performance improvements, although the flexibility of the zero-shot approach may introduce severe risk of overfitting on the TUEG dataset."}, {"title": "D Post-hoc investigation of data leakage", "content": "To maximize the amount of data available in this data-scarce setting, the TUAB training set was included during pre- training. We investigate whether this gave a disproportionate advantage to linear probes trained on ELM representations by repeating the \"1% labels\" context using unseen subjects as follows: Given only the TUAB test set, we train linear probes using 10-fold cross validation (times five random seeds), each time splitting 10-20-70% of the test set into train/validation/test. This gives the same labeled sample size as 1% of the TUAB training set without relying on samples seen during pretraining. As seen in Table 8, results are highly similar, strongly suggesting that the advantage of ELMs is not due to the inclusion of the TUAB training set in the pretraining set."}, {"title": "E Additional visualisations", "content": ""}, {"title": "E.1 EEG embeddings", "content": "We provide t-SNE (complexity=40, [52]) visualisations of the averaged EEG embeddings per subject after pretraining. These are post-hoc plots for which we use models trained on the entire pretraining subset and display embeddings of hold-out TUAB patients. Multimodal modeling, specifically for ELMe,l, shows the clearest visual separation between abnormal and normal EEGs, which is in line with the linear probing results."}, {"title": "E.2 Report and content segmentation", "content": ""}]}