{"title": "Syllabus: Portable Curricula for Reinforcement Learning Agents", "authors": ["Ryan Sullivan", "Ryan P\u00e9goud", "Ameen Ur Rahmen", "Xinchen Yang", "Junyun Huang", "Aayush Verma", "Nistha Mitra", "John P. Dickerson"], "abstract": "Curriculum learning has been a quiet yet crucial component of many of the high- profile successes of reinforcement learning. Despite this, none of the major re- inforcement learning libraries directly support curriculum learning or include curriculum learning implementations. These methods can improve the capabilities and robustness of RL agents, but often require significant, complex changes to agent training code. We introduce Syllabus, a library for training RL agents with curriculum learning, as a solution to this problem. Syllabus provides a universal API for curriculum learning algorithms, implementations of popular curriculum learning methods, and infrastructure for easily integrating them with distributed training code written in nearly any RL library. Syllabus provides a minimal API for each of the core components of curriculum learning, dramatically simplifying the process of designing new algorithms and applying existing algorithms to new environments. We demonstrate that the same Syllabus code can be used to train agents written in multiple different RL libraries on numerous domains. In doing so, we present the first examples of curriculum learning in NetHack and Neural MMO, two of the premier challenges for single-agent and multi-agent RL respectively, achieving strong results compared to state of the art baselines.", "sections": [{"title": "Introduction", "content": "Curricula have been a core component of many of the successes of reinforcement learning (RL). AlphaGo  was trained with self-play, AlphaStar used a novel league training method to achieve grandmaster level play in Starcraft II , and GT Sophy  was taught to outrace professionals in Gran Turismo with manually curated race track sections. Curriculum learning is most valuable in environments with large task spaces where, for any given RL agent, many tasks will be too challenging to learn and many will be too easy to provide a useful learning signal. In these settings it is critical to prioritize tasks that teach general skills that can transfer to new tasks and accelerate learning. This problem is further highlighted in environments with infinite or evolving task spaces. Open-endedness research seeks to co-evolve agents and environments in order to create more complex tasks, leading to more complex agent behavior"}, {"title": "Background", "content": "Curriculum Learning has been studied in the context of deep supervised learning for many years . More recently, it has been studied explicitly as a method to improve the capabilities and generalization of deep reinforcement learning agents. Curriculum learning encompasses a wide range of methods targeted at changing the distribution of data used to train an agent. Their goal is to increase the asymptotic performance or sample efficiency of RL agents on a single environment or range of tasks by sampling tasks that provide maximal learning value.  and  present more thorough taxonomies and surveys of existing curriculum learning methods.\nMany diverse methods fall under the broad definition of curriculum learning. Exploration bonuses like curiosity  or novelty  modify the reward function to induce a curriculum by incentivizing the agent to explore unseen section of the state space. Methods like self-play  or league training  create an implicit curriculum by training the opponents in a multiplayer game . Progressively more capable opponents lead to progressively more difficult tasks for an agent. Most task-based methods follow the general approach of proposing tasks that are challenging yet solvable, or tasks which the agent is recently performing well on"}, {"title": "Related Work", "content": "Mainstream RL libraries do not include curriculum learning algorithms, but there are research libraries for curriculum learning and unsupervised environment design. One of these is the Dual Curriculum Design library , which incorporates multiple UED Methods in a single repository. It includes implementations of PLR , PAIRED , Robust PLR, REPAIRED , and ACCEL . Watts is another curriculum learning library focused on open-endedness with implementations of POET and PAIRED . It atomizes components of the open-ended framework into modules to create new methods by sharing and combining components from different methods. They also compare algorithms on a suite of common benchmarks and explore different combinations of modules to form new algorithms.\nRecently, Jax has become a popular choice for writing fast deep learning libraries that can be run end- to-end on hardware accelerators . Minimax provides Jax-based implementation of the UED algorithms in the DCD library, leading to significantly faster training. JaxUED refactors the UED algorithms in Minimax into single-file implementations for faster prototyping, inspired by CleanRL . Both of these libraries are intended to be used with Jax-based environments, which allow you to run the entire training loop on hardware accelerators. However, they provide only moderate performance benefits to the complex CPU-based games that motivate open-endedness research at the cost of inconvenient code constraints. These libraries also suffer from the same lack of portability as previous curriculum learning systems.\nSyllabus distinguishes itself from previous works by making minimal assumptions on the training code, rather than providing its own training system that intermingles RL and curriculum learning. By defining a simple, uniform API for interfacing with a curriculum learning method, Syllabus makes it possible to add these algorithms to nearly any RL system with minimal code changes. This also means that all of the algorithmic details are contained in a single file, rather than distributed across the algorithm code, task generator, and other infrastructure. Finally, Syllabus provides the only general-purpose infrastructure for synchronizing curricula across CPU-based environments in multiple processes."}, {"title": "Design Philosophy", "content": "Syllabus aims to simplify the process of developing new curriculum methods, combining them with varying RL algorithms, and applying them to new challenging domains. It is also built to be compatible with many different RL and multiprocessing libraries. These unique goals and restrictions motivate the following key points of our design philosophy:\n1.  Syllabus should be agnostic to the choice of reinforcement learning framework.\n2.  Syllabus should be general enough to support any form of curriculum learning."}, {"title": "Syllabus APIs", "content": "Syllabus designates responsibility for maintaining sampling distributions over the task space to a separate Curriculum class, and implements task swapping through a TaskWrapper, limiting curriculum code to two locations in the code base. Users can also define a TaskSpace that represents the full range of tasks in an environment that can be used to train agents. These are the main components that users need to modify to develop or apply curriculum learning methods, so each API designed to be simple and general enough to support future use cases."}, {"title": "Curriculum API", "content": "In Syllabus, a Curriculum is responsible for maintaining a distribution over the task space and implementing a sampling function for selecting tasks. Automatic curriculum learning methods require feedback from the RL process to update their sampling distribution. The Curriculum API provides multiple options for receiving updates from different sources. A Curriculum can be manually updated from the main training process, or automatically receive updates through Syllabus's synchronization infrastructure. This system will automatically send feedback to the curriculum after each step, episode, or completed task, depending on its requirements. The multiprocessing infrastructure is explained"}, {"title": "Agent API", "content": "A subset of the Curriculum API is the Agent API, which allows us to define curricula over co-players in multiagent games. These algorithms store co-players and load them at a later time based on some sampling criteria, enabling advanced multiagent curricula. The currently implemented algorithms are intended for two-player zero-sum games like Chess, but the API itself supports many-agent general-sum games. They are both forms of self-play  where the opponent is a copy of the online policy.\n1.  Fictitious Self Play - trains a single agent and maintains a history of past copies of the agent  as opponents. FSP uniformly samples an opponent from this history to prevent strategic cycles that occur in pure self play.\n2.  Prioritized Fictitious Self Play - like FSP, PFSP maintains a history of past opponents to sample during training. PFSP selects the previous opponent with the highest win-rate against the current agent. This prevents the curriculum from spending a disproportionate amount of time on opponents that the agent already performs well against.\nSyllabus supports the simultaneous use of opponent-based (e.g. FSP, PFSP) and task-based (e.g. DR, PLR) curricula through the DualCurriculum wrapper. This wrapper extends the Curriculum API, allowing the user to sample from a joint task space and update both curricula at once. This API allows users to experiment with different joint curricula with minimal changes to the training code."}, {"title": "Task Space API", "content": "Defining the task space of the environment is one of the core design challenges of applying curriculum learning to a new domain. In most benchmark environments, tasks are in a low-dimensional discrete or continuous space. In more challenging environments, the space of possible tasks might be a combination of discrete and continuous variables, or a complex predicate system such as in Neural"}, {"title": "Task Interface A\u03a1\u0399", "content": "In unsupervised environment design, we study underspecified POMDPs (UPOMDPs), which have free configuration variables that need to be chosen to produce a fully specified POMDP . In multi-task environments these free variables are the task. Syllabus supports UPOMDPS by accepting a \"new_task\" argument in the reset function of the standard Gym API. However, most environments do not support this behavior by default. We provide a TaskWrapper that accepts a new task in its reset function, reconfigures the environment with the new task, then resets the environment for the next episode. This even allows us to add multi-task capabilities to single-task environments.\nThe Task Interface can also define an optional environment-specific progress metric and encode the current task into the observation space for task-conditional policies. In the simplest case, the progress metric can be a binary value that is 1.0 when the task is completed and 0.0 while it is not. This allows Syllabus to train agents with the Learning Progress curriculum, or other similar methods."}, {"title": "Multiprocessing Infrastructure", "content": "Syllabus's multiprocessing infrastructure is designed to separate curriculum logic from multiprocess- ing logic, and provide interoperability with many different forms of distributed RL infrastructure. It uses a bidirectional sender-receiver model in which the curriculum sends tasks and the environment sends feedback from playing the provided task. The curriculum synchronization wrapper adds multiprocessing functionality to a Curriculum and an environment synchronization wrapper adds the same functionality to the environment without changing the Gym interface. The environment synchronization wrapper automatically sends feedback to the curriculum after each step, episode, or completed task depending on the curriculum method. You can also update the curriculum with training metrics directly from the main learner process. All updates are batched to reduce multiprocessing overhead, and task sampling is buffered to prevent delays at the start of each episode. Crucially, adding Syllabus's synchronization to existing RL training code requires only a few lines of code, shown in Figure 2.\nThe user-facing Curriculum and environment code follows our design goals stated in section 4, while the multiprocessing infrastructure is engineered to ensure stability and reduce the risk of bugs."}, {"title": "Integration Experiments", "content": "We demonstrate Syllabus's versatility by applying it to strong baselines in four different domains using four separate RL libraries. Each of these libraries has a different design philosophy, software architecture, and multiprocessing layer. Despite this, Syllabus can be easily applied to all of them with only a few lines of library-agnostic code. We provide full details for these experiments in section 12 and a summary of the results in section 7. The data for all of these experiments is publicly available on Weights & Biases and the code is available at github.com/RyanNavillus/Syllabus.\nCart Pole in RLLib - Cart Pole is a simple toy environment mainly used to debug RL implemen- tations . It initializes a cart to a random starting point along a 2D track and tasks the agent with balancing a pole for as long as possible. It is not a multitask environment, so we use Syllabus's task wrapper to make the cart's initialization range a configurable option. This experiment demonstrates how Syllabus's Task Interface can add multitask functionality to singleton environments, and that Syllabus integrates easily with RLLib's Ray-based multiprocessing infrastructure .\nProcgen in CleanRL - Procgen is a collection of procedurally generated arcade game environments designed to test the generalization of RL agents . In this environment, we use curriculum learning to select the environment seed for each episode, following prior work . CleanRL provides a single-file implementation of PPO, which allows direct access to training code and makes integrating Syllabus trivial. We also use Procgen to validate our PLR implementation by comparing directly to the original implementation by Jiang et al. [2021a].\nNeural MMO in PufferLib - Neural MMO 2.0 is a complex multi-agent simulation inspired by massively multiplayer online games . Agents can collect resources, learn skills, trade goods, and fight non-player characters or other agents. It has a predicate task space which allows users to define objectives in Python code. The baseline for the 2023 Neural MMO competition was written in PufferLib because it supports complex action spaces and multiagent environments where agents can die, which complicates learning code . We show that Syllabus can be used in this environment with 128 agents and a massive task space.\nNetHack in Moolib - NetHack is a popular text-based dungeon-crawler released in 1987, and adapted into an RL environment by K\u00fcttler et al. [2020]. It's a complex, procedurally generated game in which winning or \"ascending\" can take more than 50,000 steps for human players. Ascending requires players to solve puzzles using common sense, knowledge of mythology, and game-specific tricks, all while collecting equipment, fighting monsters, and scavenging for food. NetHack remains one of the hardest benchmarks for online RL methods, which lag behind hand-crafted symbolic agents and behavior cloning baselines . Importantly, NetHack can be simulated extremely quickly, shifting the training bottleneck from data collection to policy optimization and inference. To address this, NetHack baselines use specialized training libraries like TorchBeast , Sample Factory , and MooLib . SampleFactory and Moolib use asynchronous PPO (APPO), which create separate copies of the policy for action inference and optimization. Moolib in particular creates servers for policies and envi- ronments and communicates via remote procedure calls (RPCs), allowing it to scale to many"}, {"title": "Results", "content": "Our Cart Pole experiments use a simple curriculum that increases the initialization range of the cart over the course of training. This causes the cart to begin in more precarious positions. We compare this to an agent trained only with the maximum initialization range. The curriculum learning agent initially learns a strong policy, but converges to a weaker policy than the agent trained solely on the maximum range. Since the single-task agent easily converges to a strong policy, there is little reason to use curriculum learning.\nWe show in Figure 4 that the training curves for our Procgen experiments almost precisely match the reference implementation for Prioritized Level Replay from Jiang et al. [2021a]. We train agents on 200 seeds and evaluate them on the full task distribution periodically over the course of training.\nIn Neural MMO, we see in Figure 4c that prioritized level replay over environment seeds performs comparably to uniform random sampling. Prioritized Level Replay may perform poorly in a self-play setting where value predictions are non-stationary with respect to the opponent's policy. In this context, task-based curricula which do not select tasks based on the current model may be more effective. Additional experiments using a manually designed curriculum of progressively harder reward functions can be found in section 12, which we find outperforms the default survival reward in Neural MMO on several metrics.\nIn NetHack, we also see that Prioritized Level Replay performs similarly to Domain Randomization. This is unsurprising because a single seed of NetHack can diverge significantly in just a few steps, so"}, {"title": "Discussion and Future Work", "content": "Multi-domain research is crucial to developing generalizable and robust RL methods. Curriculum learning is most valuable in challenging, open-ended environments that approach the complexity of the real world. In practice, the complexity of these domains necessitates custom learning infrastructure. NetHack agents are trained in libraries that are specially designed to maximize hardware utilization, taking advantage of NetHack's fast simulation speed. Neural MMO agents are trained in PufferLib because it has native support and optimizations for the complexities of multiagent environments. Syllabus is designed for this reality of training agents in complex environments. Isolating curriculum learning code from reinforcement learning code allows us to apply curriculum learning to complex environments without developing or adapting new infrastructure for each environment. We hope this will improve the reproducibility of curriculum learning research and help to push it away from toy environments and toward domains that challenge modern RL methods.\nMuch of the recent infrastructure work in unsupervised environment design has been written in Jax. Jax allows environments and training code to be parallelized on hardware accelerators, producing experimental results hundreds of times faster than equivalent CPU-based environments. Jax is a powerful tool for conducting fast research, but it also enforces strict requirements on how code is written, slowing down development, and incentivizing simplistic environments. For this reason, it will be challenging for Jax-based simulations to reach the complexity of even CPU-based research environments, much less the professionally developed videogames that have historically been bench- marks for reinforcement learning. Jax has empowered curriculum learning research and led to new discoveries using simpler environments, and we hope that Syllabus will similarly enable curriculum learning in complex domains.\nSyllabus is under continuous development as we add more features, benchmark more methods, and provide support for more general approaches to curriculum learning. In the near future, we plan to implement more open-ended algorithms including PAIRED  and OMNI"}, {"title": "Appendix: Optimization", "content": "As a consequence of the choice to use a separate multiprocessing system from the RL training loop, Syllabus incurs some unavoidable computational costs. Specifically, receiving and sending information in the environments decreases the effective steps per second of each environment, while sampling and sending tasks in the actor process increases the computational load on the main process. We perform experiments on the NetHack Learning Environment  to demonstrate the effect of this choice on overall steps per second. We evaluate with a minimal curriculum that always returns the same task to isolate the impact of our multiprocessing infrastructure.\nNote that this is a worst case test for several reasons. Typically in distributed reinforcement learning, environments are vectorized and stepped together, such that N environments step at the speed of the single slowest environment. Here we run each environment independently, so they are not bottlenecked by vectorization. The NLE is an extremely fast environment with large observations, which stresses the multiprocessing communication bandwidth. Finally, RL training with larger architectures is usually bottle-necked by policy optimization rather than environment iteration time."}, {"title": "Testing", "content": "We use pytest to continuously test and benchmark the performance of new additions to Syllabus. Each curriculum is smoke tested and benchmarked on several environments. The multiprocessing infrastructure is evaluated to ensure that every sampled task is received by the environments and every environment updated is processed by the curricula, as well as several other safeguards. We also use unit tests for task spaces and core curriculum features. Finally, we compare the performance of our algorithm implementations against the original implementations or original paper results whenever possible."}, {"title": "Experiments", "content": "This section outlines the details of our experimental setup for each environment. All of the code for these experiments is also open-sourced on GitHub. Reinforcement learning research typically compares training returns to evaluate agents, but this is not valid when using curriculum learning. Curriculum learning modifies the training task distribution, meaning that higher returns may indicate easier tasks or tasks with larger return scales rather than better agent performance. The metric that we use in place of training returns depends on the environment, so we describe each along with the experimental results below."}, {"title": "Procgen", "content": "Our Procgen experiments use the same ResNet architecture and hyperparameters as previous work [Cobbe et al., 2020b, Jiang et al., 2021a]. We use the exact same model architecture, PPO hy- perparameters, and PLR options as Jiang et al. [2021a] to reproduce their results with Syllabus's implementation of PLR. We experiment on a subset of 10 Procgen environments: Bigfish, Bossfight, Caveflyer, Chaser, Climber, Dodgeball, Fruitbot, Leaper, Ninja, and Plunder. We train 5 agents on dif- ferent seeds per environment and evaluate them on the full distribution of seeds for 10 episodes every 16,384 environment steps. We compute normalized returns using the maximum and minimum return values for each environment listed in Cobbe et al. [2020b] according to the formula $r_n = \\frac{r - r_{min}}{r_{max} - r_{min}}$.\nThis allows us to weigh each environment equally while aggregating returns, such as in Figure 4."}, {"title": "Neural MMO", "content": "Our Neural MMO agents use the exact architecture and hyperparameters provided in the starter kit for the 2023 Neural MMO Competition Suarez et al. [2024]. These agents are trained with self-play, so as the agent improves so do its opponents. We rely on events and achievements that are not zero-sum to determine how proficient each agent is in the environment, but these metrics are not completely independent from the quality of opponents.\nThe sequential curriculum consists of 5 stages, each of which uses domain randomization over a subset of the task space. The curriculum progresses to the next stage whenever the agent achieves a mean episodic return of 0.75 averaged over the past 1000 episodes. Each agent successfully made it to the final curriculum stage by the end of training.\nFor each individual task, we assign a threshold. The agent is gets a reward of 1.0 for completing the task, which is distributed as the agent makes progress on the task. For example, if we task the agent with surviving for 100 timesteps, after 50 timesteps it will have a cumulative reward of 0.5. After surviving for 150 timesteps, the agent will have a cumulative reward of 1.0 because we stop assigning reward after the threshodl is reached."}, {"title": "NetHack", "content": "Our NetHack agents are trained using the open source Moolib code provided by Hambro et al. [2022a]. Moolib implements a version of Asynchronous Proximal Policy Optimization (APPO) . It uses the standard Chaotic Dwarf Sample Factory baseline from the NetHack Challenge [Hambro et al., 2022b]. We make minor modifications to the environment definition to add Syllabus's synchronization wrapper, and add our curriculum object in the main experiment code.\nFigure 11 shows that although our PLR agents achieve similar scores to the Domain randomization agents, they score higher on several key metrics that are important to progress in the game. Score in NetHack is not the true objective of the game, but it is correlated with survival time and several good behaviors, so it is important to consider several metrics when evaluating agents. The true objective of the game is to \"ascend\" by descending through the 50 procedurally generated dungeon levels, acquiring the Amulet of Yendor, and offering it to the player's deity."}, {"title": "Code Examples", "content": null}, {"title": "RLLib", "content": null}, {"title": "Stable Baselines 3", "content": null}, {"title": "Documentation", "content": "Syllabus is documented both in code and with a dedicated documentation website. This includes details on each curriculum algorithm and warnings about common pitfalls that users might run into when configuring them."}, {"title": "Limitations", "content": "Syllabus defines a completely separate multiprocessing pathway to send data to the curriculum. When curricula require observations, rewards, dones, or infos from the environment, this will send the same information as the RL training multiprocessing, potentially leading to bandwidth or processing bottlenecks. We demonstrate in section 10 that this does not significantly impact performance in the cases we've tested, but there may be systems where this limitation becomes noticeable. Syllabus has not been tested on multi-node infrastructure, though the Ray backend should allow it to function in any Ray distributed training loop. It is also possible to explicitly support multi-node systems by implementing new versions of the synchronization wrappers that use RPC calls. Syllabus algorithms can not be run entirely on hardware accelerators, meaning that Jax-based environments and RL code can not be fully parallelized while using Syllabus.\nSyllabus also does not currently implement any exploration bonuses, or advanced multiagent algo- rithms beyond self-play. Exploration bonuses typically train additional neural network to predict some measure of novelty  and use them to compute additional reward components."}]}