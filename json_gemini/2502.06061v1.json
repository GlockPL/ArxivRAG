{"title": "ONLINE REWARD-WEIGHTED FINE-TUNING OF FLOW MATCHING WITH WASSERSTEIN REGULARIZATION", "authors": ["Jiajun Fan", "Shuaike Shen", "Chaoran Cheng", "Yuxin Chen", "Chumeng Liang", "Ge Liu"], "abstract": "Recent advancements in reinforcement learning (RL) have achieved great success in fine-tuning diffusion-based generative models. However, fine-tuning continuous flow-based generative models to align with arbitrary user-defined reward functions remains challenging, particularly due to issues such as policy collapse from overoptimization and the prohibitively high computational cost of likelihoods in continuous-time flows. In this paper, we propose an easy-to-use and theoretically sound RL fine-tuning method, which we term Online Reward-Weighted Conditional Flow Matching with Wasserstein-2 Regularization (ORW-CFM-W2). Our method integrates RL into the flow matching framework to fine-tune generative models with arbitrary reward functions, without relying on gradients of rewards or filtered datasets. By introducing an online reward-weighting mechanism, our approach guides the model to prioritize high-reward regions in the data manifold. To prevent policy collapse and maintain diversity, we incorporate Wasserstein-2 (W2) distance regularization into our method and derive a tractable upper bound for it in flow matching, effectively balancing exploration and exploitation of policy optimization. We provide theoretical analyses to demonstrate the convergence properties and induced data distributions of our method, establishing connections with traditional RL algorithms featuring Kullback-Leibler (KL) regularization and offering a more comprehensive understanding of the underlying mechanisms and learning behavior of our approach. Extensive experiments on tasks including target image generation, image compression, and text-image alignment demonstrate the effectiveness of our method, where our method achieves optimal policy convergence while allowing controllable trade-offs between reward maximization and diversity preservation.", "sections": [{"title": "1 INTRODUCTION", "content": "Generative models have achieved remarkable success in producing high-fidelity data across various domains, including text and images (Ouyang et al., 2022a; Esser et al., 2024). Among these, continuous flow-based models have gained attention for their more concise and flexible design of the ODE-based denoising process to model complex data distributions (Tong et al., 2024a; Lipman et al., 2023). However, fine-tuning such flow-based models to align with arbitrary user-defined reward objectives remains challenging. While recent advancements in reinforcement learning (RL) have demonstrated considerable success in fine-tuning diffusion-based generative models (Rafailov et al., 2023), their application to continuous flow-based models remains underexplored. The primary challenges lie in lacking computationally efficient divergence measurement and tractable methods to avoid overoptimization (Black et al., 2024) and policy collapse in fine-tuning continuous-time flow matching models. Particularly, traditional policy gradient methods (Black et al., 2024) struggle in this continuous domain due to the high computation cost of computing the exact ODE likelihood for continuous normalizing flows (CNFs), which requires solving intricate transport dynamics and tracking probability flows with model divergence calculation across time (See App. B.2.4)."}, {"title": "2 RELATED WORKS", "content": "Conditional Flow Matching Conditional Flow Matching (CFM) (Lipman et al., 2023; Tong et al., 2024a) and related methods have been pivotal in transforming simple data distributions into complex, task-specific ones. Lipman et al. (2023) and Tong et al. (2024a) focus on training flow-based models by conditioning the generative flow on known distributions, allowing for more precise control over the generative process. However, while these models achieve great success in generation tasks, how to fine-tune FM models to fit arbitrary user-defined objectives has not yet been widely studied.\nFine-tuning From Human Feedback Recent works typically leverage DPO (Rafailov et al., 2023) or policy gradient RL methods (Black et al., 2024; Fan et al., 2023) to achieve RLHF (Ouyang et al., 2022a). However, since the likelihood of the generative policy cannot be easily calculated in continuous-time flow models, none of these approaches can be easily adapted to fine-tune flow matching models. Additionally, DPO (Rafailov et al., 2023) relies heavily on a filtered dataset, which does not work with a general reward model, while DPOK (Fan et al., 2023) relies on KL divergence, which is computationally inefficient and lacks effective evidence lower bound (ELBO) in FM.\nFine-Tuning Generative Models with RL RL has been increasingly adopted to fine-tune generative models, allowing them to adapt to specific downstream tasks by optimizing for user-defined reward objectives. Black et al. (2024); Fan et al. (2023); Lee et al. (2023) used RL to fine-tune diffusion models to align generated data with task-specific rewards. Lee et al. (2023) proposed an offline reward-weighted method, while Black et al. (2024) proposed an online version without divergence regularization, which either failed to converge into optimal policy or collapsed into an extremely greedy policy without diversity (Black et al., 2024). Other reward-weighted methods like ReFT, ReST (Huguet et al., 2024; G\u00fcl\u00e7ehre et al., 2023) are almost limited to the offline settings with given datasets and lack theoretical analysis of the convergent behavior. Importantly, how to adopt online RL methods to fine-tune FM models has not yet been widely studied in both theory and practice."}, {"title": "3 PRELIMINARIES", "content": "3.1 FLOW MATCHING MODELS\nFlow Matching (FM) is a promising technique for training generative models by transforming a simple initial distribution $p(x_0)$ like Gaussian noises, into a complex target distribution $p(x_1)$, which represents real-world data (Esser et al., 2024). This transformation works continuously over time, governed by a time-dependent vector field $u_t(x)$ and the flow ordinary differential equation (ODE): $\\frac{dx}{dt} = u_t(x)$. The goal of FM is to learn a vector field that transports samples from the base distribution to the target distribution by minimizing the difference between the learned vector field and the true dynamics of the probability flow. This process can be formalized by the Flow Matching loss: $L_{FM}(\\theta) = E_{t~U(0,1),x~p_t(x)}[||v_{\\theta}(t, x) - u_t(x)||^2]$.\nHowever, the original FM objective is generally intractable because the time-varying distribution $p_t(x)$ and the true vector field $u_t(x)$ are often intractable. To address this, Conditional Flow Matching (CFM) has been proposed (Lipman et al., 2023), which simplifies the task by conditioning the flow on target samples to derive a tractable objective. The CFM loss is defined as follows:\n$L_{CFM}(\\theta) = E_{t~U(0,1),x_1~q(x_1),x~p_t(x|x_1)}[||v_{\\theta}(t, x) - u_t(x | x_1)||^2]$                                                                                                                 (1)\nwherein $u_t(x | x_1)$ becomes tractable by defining explicit conditional probability paths from $x_0$ to $x_1$, such as OT-paths (Tong et al., 2024a) or linear interpolation paths (Lipman et al., 2023). In this paper, we adopt OT-paths for all experiments based on TorchCFM (Tong et al., 2024b) and Diffusers (von Platen et al., 2022).\n3.2 REWARD WEIGHTED REGRESSION\nReward Weighted Regression (RWR) (Peters & Schaal, 2007; Black et al., 2024) is an approach used in RL for policy optimization. In this framework, actions that yield higher rewards are assigned greater importance, and the policy is updated by re-weighting actions based on the rewards they produce. The key idea behind RWR is to adjust the probability of selecting actions proportionally to their associated rewards, encouraging the agent to focus on high-reward actions.\nSpecifically, the policy $\\pi(a | s)$ is updated by re-weighting the actions via $\\pi_{new}(a | s) \\propto \\pi_{old}(a | s) \\cdot exp(\\frac{1}{\\tau} * r(s, a))$, where $r(s, a)$ is the reward for taking action $a$ in state $s$, and $\\tau$ is a temperature parameter that controls the trade-off between exploration and exploitation. Higher rewards lead to higher probabilities for the corresponding actions, allowing the agent to improve its policy over time by favoring actions that maximize cumulative rewards: $\\pi^* = arg \\max E_{\\pi}[\\sum_{t=0}^{T} \\gamma^t r(s_t, a_t)]$.", "3.1": null}, {"title": "4 METHODS", "content": "4.1 PROBLEM STATEMENT\nIn this paper, we aim to fine-tune continuous flow-based models using a reward-weighted RL method (Peters & Schaal, 2007; Peng et al., 2019) to optimize a user-defined reward function $r(x_1)$. Specifically, our objective is to find the optimal generative policy model, denoted as $\\pi^*$, which maximizes the expected reward throughout data generation:\n$\\pi^* = arg \\max_{\\pi} J(\\pi) = arg \\max_{\\pi} E_{x_1~\\pi(x_1)} [r (x_1)]$                                                                                                              (2)\nwhere the policy $\\pi$ refers to the parameterized flow matching data generative model $p_{\\theta}(x_1)$, determining how data points (e.g., images) are generated, which is learned solely from scalar rewards, without ground-truth labels or filtered dataset. In fine-tuning tasks, we may also consider constrained optimization, where the distance between the pre-trained model and fine-tuned model will be constrained (Ouyang et al., 2022a). Considering the computational inefficiency of KL in continuous flow models, we instead propose a Wasserstein-2 distance to achieve that.\n4.2 FINE-TUNING FLOW MATCHING MODELS WITH OFFLINE REWARD-WEIGHTING\nTo find the optimal data generation policy in equation 2, in this section, we extend the standard CFM framework by introducing a reward-weighting mechanism to focus the model's learning on more", "4.1": null, "4.2": null}, {"title": "4.3 FINE-TUNING FLOW MATCHING MODELS WITH ONLINE REWARD-WEIGHTING", "content": "As discussed, to mitigate the online-offline gap and achieve better performance, we can instead adopt an online RL algorithm, where the fine-tuned FM models are used as a data generation/sampling policy to continuously generate new training data and explore high-value regions, inducing what we call the Online Reward-Weighted Conditional Flow Matching (ORW-CFM) method:\n$L_{ORW-CFM}(\\theta) = E_{t~U[0,1],x_1~p_{\\theta}(x_1),x~p_t(x|x_1)} [w (x_1) ||v_t(x; \\theta) - u_t (x | x_1)||^2]$,                                                                                                                (5)\nIn the online setting, the model can adaptively sample from the current learned data distribution, and fine-tune itself based on the newly generated samples, inducing an online policy iteration that promotes more efficient exploration of the high-reward areas in the data space. In general, the data distribution learned by the ORW-CFM method follows Theorem 2 (Proof in App. C.3):\nTheorem 2 (Online Reward Weighted CFM). Let $q (x_1)$ be the initial data distribution, and $w (x_1)$ be a non-negative weighting function integrable with respect to $q (x_1)$, and proportional to user-defined reward $r(x_1)$. Assume that at each epoch $n$, the model $v_{\\theta}$ perfectly learns the distribution implied by the ORW-CFM loss in equation 5. Then, the learned data distribution after $N$ epochs is given by:\n$\\tilde{q}_{\\theta}^N(x_1) = \\frac{w(x_1)^N q(x_1)}{Z_N}$ (6)\nwhere $Z_N = \\int_X w(x_1)^N q(x_1) dx_1$ is the normalization constant ensuring $\\tilde{q}_{\\theta}^N(x_1)$ is a valid probability distribution."}, {"title": "4.4 WASSERSTEIN-2 REGULARIZATION IN FLOW MATCHING", "content": "To address the policy collapse problem in the ORW-CFM method, we introduce Wasserstein-2 (W2) regularization, which helps maintain diversity by bounding the distance between the current learned model and a pre-trained reference model. This regularization induces an exploration-exploitation trade-off, allowing the model to avoid focusing solely on high-reward regions at the expense of diversity. In general, we can write the Wasserstein-2 Distance as follows (Arjovsky et al., 2017):\nDefinition 4.1 (Wasserstein-2 Distance). Given two probability measures $\\mu$ and $\\nu$ on $R^n$, the squared Wasserstein-2 distance between $\\mu$ and $\\nu$ is defined as:\n$W_2(\\mu, \\nu) = \\inf_{\\gamma \\in \\Pi(\\mu,\\nu)} \\mathbb{E}_{(x,y)\\sim\\gamma} [||x - y||^2 dy(x, y)]$                                                                                           (8)\nwhere $\\Pi(\\mu, \\nu)$ denotes the set of all couplings of $\\mu$ and $\\nu$; that is, all joint distributions $\\gamma$ on $R^n \\times R^n$ with marginals $\\mu$ and $\\nu$.\nHowever, it is intractable to directly introduce $W_2$ distance into our loss function. Therefore, we instead derive and introduce its upper bound as our distance regularizer (Proof in App. C.4):\nTheorem 3 (W2 Bound for Flow Matching). We consider two flow matching models parameterized by $\\theta_1$ and $\\theta_2$, inducing time-evolving data distributions $p^{\\theta_1}_t(x)$ and $p^{\\theta_2}_t(x)$, respectively. The models define vector fields $v^{\\theta_1}(t, x)$ and $v^{\\theta_2}(t, x)$ that transport an initial distribution $p_0(x)$ to final distributions $p^{\\theta_1}_1(x)$ and $p^{\\theta_2}_1(x)$ at time $t=1$. Assume that $v^{\\theta_2}(t, x)$ is Lipschitz continuous in $x$ with Lipschitz constant $L$. Then, the squared Wasserstein-2 distance between the distributions $p^{\\theta_1}_1$ and $p^{\\theta_2}_1$ induced by the flow matching models at time $t=1$ satisfies:\n$W_2^2(p_1^{\\theta_1}, p_1^{\\theta_2}) < e^{2L} E_{\\chi\\sim p_0^{\\theta_1}} \\int_0^1 [|| v^{\\theta_1}(s, x) - v^{\\theta_2}(s, x)||^2] ds$                                                  (9)\nSince the integral can be approximated by Monte Carlo sampling, this bound allows us to constrain the discrepancy between the learned and reference models, effectively preventing the model from collapsing into a greedy policy. By controlling the Wasserstein-2 distance, the model maintains a balance between exploring new areas of the data space and exploiting known high-reward regions in a constrained neighborhood from the pre-trained model, thus preserving diversity."}, {"title": "4.5 FINE-TUNING FLOW MATCHING MODELS WITH W2 REGULARIZATION", "content": "The offline version of the reward-weighted method can be considered as the first step of the online version (Black et al., 2024). For simplicity, we only derive the ORW-CFM method with W2 distance regularization (i.e., ORW-CFM-W2 method) in this section, and left the offline version (i.e., RW-CFM with W2 distance regularization, namely RW-CFM-W2 method) in App. C.5 for ease of reading.\nTo further improve the stability of the ORW-CFM method and prevent policy collapse, we incorporate W2 regularization into our method, which has not been widely studied (Black et al., 2024). This regularization introduces a balance between exploration and exploitation by penalizing divergence from the reference model, thus maintaining diversity in the learned model. The W2 distance bound can be directly incorporated into the ORW-CFM loss, inducing an ORW-CFM-W2 method:\n$L_{ORW-CFM-W2} = E_{t~U(0,1),x_1~q(x_1;\\theta_1),x~p_t(x|x_1)} [w (x_1) ||v_t(x; \\theta_{ft}) - u_t (x | x_1)||^2 + \\alpha * ||v_{\\theta_t}(x, t) - v_{\\theta_{ref}}(x, t)||^2]$,                 (10)\nwherein $\\alpha$ is a trade-off coefficient, $v_{\\theta_{ft}} (x, t) = v_t(x; \\theta_{ft})$ and $v_{\\theta_{ref}} (x, t)$ is the reference model. Then, the learned data distribution after $n$ epochs follows the Theorem 4 (Proof in App. C.6):\nTheorem 4. Under the online RL setup with ORW-CFM-W2 loss, the data distribution after n epochs evolves according to the following rules:\n$q_0^n(x_1) = q_0(x_1) \\times [w(x_1) q_0^{n-1} (x_1) exp(-\\beta D^{n-1} (x_1))]$, (11)\nwhere, $D^{n-1}(x_1) = E_{t,x~p_t(x/x_1)} [||v^{\\theta_{n-1}}(t,x) - v_{\\theta_{ref}}(t,x)||^2]$, $\\beta = \\gamma \\alpha$, $\\gamma > 0$ is a scaling constant, $\\theta_{n-1}$ denotes the fine-tuned model parameters after epoch $n-1$.\nThe W2 regularization term prevents the model from solely collapsing into high-reward regions and ensures diversity in the learned policy (See cases where $\\alpha > 0$ in Figs.3, 4 and 11), also inducing the exploration-exploitation trade-off.\nIn practice, we often use an exponential function or Boltzmann distribution to formulate the weighting function as $w(x_1) = exp(\\tau *r(x_1))$, where $\\tau$ is an entropy coefficient that controls the entropy of the induced policy (Haarnoja et al., 2018). Thus, we can derive the induced data distribution for the exponential case under the ORW-CFM-W2 loss (Proof in App. C.7):\nTheorem 5. Given $w (x_1) = exp(\\tau *r (x_1))$, the induced data distribution after N epochs of training under ORW-CFM-W2 Loss as equation 10 is:\n$\\tilde{q}_0^N (x_1) \\propto exp [\\tau N r (x_1) - \\beta \\sum_{n=1}^{N} D^{n-1}(x_1)] q(x_1)$                                                                                                                               (12)\nwhere, $D^{n-1}(x_1) = E_{t,x~p_t(x/x_1)} [||v^{\\theta_{n-1}}(t,x) - v_{\\theta_{ref}}(t,x)||^2]$, $\\beta = \\gamma \\alpha$, $\\gamma > 0$ is a scaling constant, $\\theta_{n-1}$ denotes the model parameters after epoch $n - 1$.\nBased on this, we can derive the learning behavior of ORW-CFM-W2 under two limiting cases:\nCase 1 (Dominant Regularization Term: $\\alpha \\rightarrow \\infty$ or $r=0$). If $D^{n-1} (x_1)$ grows rapidly with N or $\\alpha$ is large, since $\\beta \\propto \\alpha$, the regularization term dominates, namely $\\beta \\sum_{n=1}^{N} D^{n-1} (x_1) \\gg \\tau *r (x_1)$. The induced distribution is heavily penalized for deviations from the reference model, potentially leading to a distribution similar to the initial distribution $q (x_1)$ (See $\\tau = 0$ in Fig. 2).\nCase 2 (Dominant Reward Term: $\\alpha = 0$ or $\\tau \\rightarrow \\infty$). If $D^{n-1} (x_1)$ grows slowly with N or $\\alpha$ is small, since $\\beta \\propto \\alpha$, the reward term dominates the regularization term, namely $\\tau * r (x_1) \\gg \\beta \\sum_{n=1}^{N} D^{n-1} (x_1)$. The induced distribution $\\tilde{q}_0^N(x_1)$ concentrates on the $x_1$ maximizing $r (x_1)$, similar to the case without considering $D^{n-1} (x_1)$ as Lemma 1 (See $\\alpha=0$ in Fig. 3).\nThough the Boltzmann distribution is commonly used for the weighting function to handle the normalization constant Z, the derivation is similar to the exponential form. For completeness, we provide the derivation of the Boltzmann form weighting function in App. C.8 for ease of reading.\nInterestingly, we can interpret our ORW-CFM-W2 method from the perspective of policy iteration in RL (Sutton & Barto, 1998), which can connect our method to KL-constrained policy optimization (Ouyang et al., 2022a; Peng et al., 2019). See App. C.9 for more detailed discussion."}, {"title": "5 EXPERIMENTS", "content": "We empirically evaluate our ORW-CFM-W2 method in equation 10 by fine-tuning small-scale FM models with a U-net architecture from TorchCFM (Tong et al., 2024a) and large-scale FM models like Stable Diffusion 3 (SD3) (Esser et al., 2024) from diffusers (von Platen et al., 2022) for reproducibility. In all experiments, we use the exponential weighting function described in Theorem 5. We also explore the impact of each component of our method and conducted detailed analyses of convergent behavior in various hyperparameter settings. More experimental details can be found in App. D and E with our Pseudocode in App. H. In general, we focus on the following questions:\n\u2022 Optimal Policy Convergence: Can our online RL methods converge to an optimal generative data distribution that maximizes various user-defined reward objectives as we derived in Theorem 2 and Lemma 1 while the offline baseline method converges to a sub-optimal?\n\u2022 Reward-Diversity Trade-off: Does W2 regularization effectively prevent policy collapse by maintaining a balance between reward maximization and diversity as Theorem 5?\n\u2022 Stable and Controllable Policy Optimization: Can our method achieve a stable and controllable fine-tuning process of FM models in different settings, and control the convergent behavior of learned policy by adjusting entropy coefficient $\\tau$ and W2 coefficient $\\alpha$?\n5.1 TARGET IMAGE GENERATION\nAt first, we evaluate our ORW-CFM method, with and without W2 regularization, by fine-tuning a pre-trained model of MNIST (LeCun et al., 1998) to generate only even numbers using reward signals. The reward function is defined as $r (x_1) = P_{even} (x_1) - P_{odd} (x_1)$, calculated by a pre-trained binary classifier. An optimal expected return of 1 indicates the model exclusively generates even numbers.\nFigs. 2 and 3 demonstrate that our method quickly reaches near-optimal performance across different settings of the entropy coefficient $\\tau$ and regularization coefficient $\\alpha$. From Fig. 2, increasing $\\tau$ leads to a more greedy, reward-maximizing policy at the cost of diversity, consistent with the theoretical limits in Theorem 5 and Lemma 1, while $\\tau$ = 0 remains a similar performance as the pre-trained model. Besides, from Fig. 3, W2 regularization controlled by $\\alpha$ remains diverse by controlling how far the", "5.1": null}, {"title": "5.2 IMAGE COMPRESSION", "content": "The effect of W2 regularization and exploration-exploitation trade-off is particularly evident in the image compression task using the model pre-trained in CIFAR-10, as shown in Figs. 4. In Fig. 4, we see that the introduction of W2 regularization prevents the policy from collapsing into an extremely greedy solution. As $\\alpha$ increases, the model explores optimal solutions within a constrained neighborhood of the pre-trained model, preserving diversity while still optimizing the reward.\nThe reward-distance trade-off curve in Fig. 4 further highlights this balance. Varying $\\alpha$ provides an explicit trade-off between maximizing the reward and minimizing the divergence from the pre-trained model. As $\\alpha$ decreases, the policy becomes more exploitative, potentially collapsing into a Delta distribution, as described in Lemma 1. Conversely, increasing $\\alpha$ encourages broader exploration of the data space, ensuring a more diverse generative policy. Therefore, proper W2 regularization effectively balances exploration and exploitation, avoiding policy collapse."}, {"title": "5.3 TEXT-IMAGE ALIGNMENT", "content": "Aligning Large-Scale Flow Matching Models To further validate our method's effectiveness, we evaluate our ORW-CFM-W2 method on fine-tuning Stable Diffusion 3 (SD3) (Esser et al., 2024). Besides, we adopt LoRA (Hu et al., 2022) for efficient fine-tuning. As shown in Figure 5, we compare our method against RAFT (Dong et al., 2023) and ReFT (Huguet et al., 2024) on spatial relationship prompts used in DPOK (Fan et al., 2023). Our approach demonstrates better positional relationship control while maintaining image quality and semantic coherence. On challenging prompts like \"a banana on the left of an apple\" and \"a cat on the left of a dog\", our method consistently generates images with correct object placement and natural appearances, outperforming previous approaches in terms of both alignment accuracy and generation quality. Results on these complex compositional prompts further demonstrate our approach's capability in handling multiple semantic constraints while maintaining generation diversity. See App. A for more details and additional results."}, {"title": "6 CONCLUSION", "content": "In this paper, we present the first theoretically-grounded online RL framework for fine-tuning flow matching models - Online Reward-Weighted Conditional Flow Matching with Wasserstein-2 Regularization (ORW-CFM-W2) without requiring calculations of ELBO, exact likelihood or KL divergence, which are intractable in ODE-based flow matching models. Our work provides the first theoretical analysis of convergent behavior in flow matching online fine-tuning, proving that unregularized methods inevitably converge to a Delta distribution (Lemma 1), inducing policy collapse that maximizes rewards at the cost of diversity. This theoretical discovery motivates our second major contribution - the derivation of a tractable Wasserstein-2 regularization that can be directly computed from vector fields, providing an efficient way to prevent collapse while maintaining diversity. Extensive experiments across target image generation, image compression, and text-image alignment demonstrate that our method achieves optimal convergence while maintaining stable learning, with ablation studies validating both the online reward-weighting mechanism for optimal convergence and W2 regularization for diversity preservation. See App. I for more discussions."}, {"title": "A TEXT-IMAGE ALIGNMENT EXPERIMENTS OF STABLE DIFFUSION 3", "content": "In this section, to further empirically demonstrate our performance on fine-tuning large-scale generative models like the stable diffusion series (Esser et al., 2024), we conduct extensive experiments on fine-tuning Stable Diffusion 3 (SD3) (Esser et al., 2024), which adopts a flow matching architecture (Lipman et al., 2023; Esser et al., 2024). For reproducibility, we built our code on the open-sourced diffuser codebase (von Platen et al., 2022), and adopted LoRA (Hu et al., 2022) and fp16 precision to reduce GPU memory requirements. Unless otherwise specified, we use a = 1,\u03c4 = 0.1 for all experiments in this section.\nTo verify the effectiveness of our method, we primarily use the raw CLIP score (i.e., logit) (Radford et al., 2021a) as our rewards for text-image alignment for SD3 fine-tuning. We also demonstrate our method's adaptability by extending it to various other text-image alignment reward models, including HPS-V2 (Wu et al., 2023), Pick Score (Kirstain et al., 2023), and Alpha Clip (Sun et al., 2024). Notably, our method learns entirely from self-generated data (Dong et al., 2023; Shumailov et al., 2024) without requiring manually collected (Ouyang et al., 2022a) or filtered datasets (Rafailov et al., 2023), while effectively preventing policy collapse through our theoretically-derived W2 regularization (See App. C.4)."}, {"title": "A.1 TEXT-IMAGE ALIGNMENT WITH SPATIAL UNDERSTANDING", "content": "In the first experiments, we aim to improve the model's understanding of spatial relationships between objects, a challenging task that requires both semantic understanding and precise positional control.\nTo demonstrate the effectiveness of our method, we use the raw CLIP score (i.e., logits) (Radford et al., 2021a; Xu et al., 2023; Dong et al., 2023) directly as our text-image alignment reward model,"}, {"title": "A.2 ABLATION STUDIES AND POLICY COLLAPSE ANALYSIS", "content": "In the second part of SD3 fine-tuning experiments, to empirically validate our theoretical analysis of policy collapse in online RL fine-tuning as Lemma 1 and demonstrate the effectiveness of our W2 regularization via ablation, we further fine-tune SD3 models to align out-of-distribution (OOD) prompt \"a cat in the sky\". This setup provides an ideal test case for examining both the policy collapse phenomenon and our method's ability to maintain diversity while optimizing rewards in different online methods. The ablation results have been concluded in Figure 8.\nEffectiveness of Online Reward Weighting (ORW-CFM) The ablation studies in Figure 8 demonstrate that our ORW-CFM method, even without W2 regularization (a = 0), achieves better semantic alignment than other online methods like RAFT (Dong et al., 2023) and ReFT (Huguet et al., 2024). This superior performance validates our theoretical framework for online reward-weighted fine-tuning of flow matching models (See Lemma Optimal Convergent Behavior in Lemma 1). The method successfully optimizes for the desired semantic content, achieving higher-quality generations that better match the prompt \"cat in sky\" compared to baseline approaches."}, {"title": "A.3 ADAPTABILITY ACROSS DIFFERENT REWARD MODELS", "content": "In addition to the three reward functions in Section 5, to demonstrate the broad applicability and reward-agnostic nature/property of our methods, we further evaluate our performance using three different text-image alignment reward models: HPS-V2 (Wu et al., 2023), Pick Score (Kirstain et al., 2023), and Alpha Clip (Sun et al., 2024). Using the challenging Spatial prompt \"a train on top of a surfboard,\" we examine how our method adapts to different reward signals/models while maintaining generation quality and semantics coherence.\nIn general, the results shown in Figure 9 demonstrate several key strengths of our approach:\nReward Adaptability: Our method shows consistent performance improvement across all three reward models, successfully positioning trains on surfboards while maintaining realistic scene composition. This empirically validates our theoretical framework's ability to optimize arbitrary reward functions without requiring reward-specific modifications (Dong et al., 2023) or filtered datasets (Rafailov et al., 2023). The high-quality generations across different reward models demonstrate that our W2 regularization (Theorem 3) provides effective stabilization regardless of the underlying reward mechanism (It is worth mentioning that appropriately increasing the a value can be used to adjust the distance from the reference model, which is also demonstrated in Figure 4)."}, {"title": "A.4 COMPOSITIONAL AND COMPLEX SEMANTIC UNDERSTANDING", "content": "In the fourth experiments of fine-tuning SD3 models (Esser et al., 2024), to further validate our method's capability in handling complex semantic relationships and demonstrate the practical benefits of our theoretical framework, we evaluate ORW-CFM-W2 on a set of challenging compositional prompts that test multiple aspects of semantic understanding. These experiments not only demonstrate our method's effectiveness on large-scale flow matching models like SD3, but also empirically validate our theoretical predictions about controlled optimization and diversity preservation. We use CLIP"}, {"title": "C PROOFS", "content": "C.1 INDUCED DATA DISTRIBUTION\nIn practice", "function": "n$\\mathbb{L"}, {"x_1)": 26, "is": "n$\\mathbb{P"}, {"distribution": "n$\\mathbb{P"}, {"namely": "nr(x"}]}