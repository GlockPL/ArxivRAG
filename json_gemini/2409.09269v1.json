{"title": "Guiding Vision-Language Model Selection for Visual Question-Answering Across Tasks, Domains, and Knowledge Types", "authors": ["Neelabh Sinha", "Vinija Jain", "Aman Chadha"], "abstract": "Visual Question-Answering (VQA) has become a key use-case in several applications to aid user experience, particularly after Vision-Language Models (VLMs) achieving good results in zero-shot inference. But evaluating different VLMs for an application requirement using a standardized framework in practical settings is still challenging. This paper introduces a comprehensive framework for evaluating VLMs tailored to VQA tasks in practical settings. We present a novel dataset derived from established VQA benchmarks, annotated with task types, application domains, and knowledge types, three key practical aspects on which tasks can vary. We also introduce GoEval, a multimodal evaluation metric developed using GPT-4o, achieving a correlation factor of 56.71% with human judgments. Our experiments with ten state-of-the-art VLMs reveals that no single model excelling universally, making appropriate selection a key design decision. Proprietary models such as Gemini-1.5-Pro and GPT-40-mini generally outperform others, though open-source models like InternVL-2-8B and CogVLM-2-Llama-3-19B demonstrate competitive strengths in specific contexts, while providing additional advantages. This study guides the selection of VLMs based on specific task requirements and resource constraints, and can also be extended to other vision-language tasks\u00b9.", "sections": [{"title": "1. Introduction", "content": "Visual Question-Answering (VQA) [1] is a task of answering a question q about an image I correctly. With the evolution of research, this field has been presented with significant challenges in terms of the nature of the problem. For example, question q can be about the image directly [13, 14, 39], or outside the scope of the image where external knowledge may be required [25, 31]. The images I can be a photograph, a mathematical chart [19,26], a document's screenshot [27], or more.\nDedicated methods [14, 16,39] have long existed to solve different challenges in VQA. But, with the advancement of Vision-Language models (VLMs) [2, 10, 23, 30, 33] in multimodal research, several applications have started adapting them for VQA, and most of them [10, 18, 33, 34] perform better than dedicated methods. This is because by leveraging pre-training [21,37] on vast multimodal datasets [7, 8, 25-27, 35], VLMs can effectively generalize across various types of images, including photographs, charts, and documents, making them highly adaptable to multiple VQA tasks without additional modification. Additionally, VLMs can also incorporate external knowledge beyond the image content, enabling more robust responses to questions with broader context in open-domain settings.\nWith this shift comes the next question: Which VLM to utilize for a given VQA-based requirement? For example, can the model that is the best in knowledge-based VQA also understand mathematical charts better than the others? The complexity of this question stems from two different directions - the number of VLMs that are being proposed these days, and the diverse nature of tasks in VQA that the VLMS are expected to perform. New VLMs are being released almost every month now [2, 9, 10, 15, 22, 23, 34, 36], and they all are different in their model architecture, training data, prompts, training strategy, size, etc. Due to this, they can possess different strengths and weaknesses. Additionally, users often face technical and business constraints like compute or memory limitations, data privacy risk, associated cost of inference, compliance and regulatory risks, or associated costs of inference, which may lead them to favor specific VLMs over others. For example, one may favor Gemini Flash over Pro due to affordability concerns over cost per input and output token. Second, tasks may vary drastically, and in multiple ways. They can range from types such as Chart Question-Answering [26] or Document Understanding [27], to application domains like Science, Politics, or Sports, and the type of knowledge required, encompassing areas like Geographical Information, Mathematical Reasoning, and beyond. For an application that can fall into one or multiple such categories, how to identify best suited VLM? How to compare them meaningfully using a standardized approach? These are gaps in existing literature. Technical reports of VLMs provide benchmarks and comparison, but they are very theoretical and limited.\nTo bridge this gap, we propose this end-to-end framework which provides a standardized paradigm to evaluate VLMs for visual question-answering in practical settings. First, we develop a evaluation framework on coming up with a structure that allows this comparison over factors that define practical usage. Our framework allows comparison of VLMs under three separate aspects: task type, application domain, and knowledge type. Multiple datasets like VQAv2 [13], OK-VQA [25], ChartQA [26], etc., provide task instances to train and evaluate models based on different task types, but none of them label their instances on other practical aspects. Our framework proposes a dataset derived from these existing benchmarks where tasks are also classified and labeled with their application domains, and type of knowledge required to carry out the task successfully, as shown in Figure 1. Each task can have multiple tags associated with the above three aspects. Further, recent works [17, 24] have also identified that comparing LLM-generated text with traditional evaluation metrics for open-domain QA [17] as well as Natural Language Generation [24] is challenging and not representative with human evaluations. Because these generative models are trained on large datasets, they may produce correct answers that are unfairly evaluated if they differ from the gold-standard labels. This challenge would also transfer to VLMs. To mitigate this, we also propose GoEval, which is a multimodal evaluation metric based on GPT-4o [30], and shows superior correlation with human judgement than previously used metrics when used with VLM generated outputs. Our framework is for VQA, but can be adapted to other vision-language tasks.\nIf application developers utilize VLMs for VQA, some might prefer openly available VLMs [10, 22, 34] due to compliance and data privacy requirements, some may need small VLMs [10,34] which can be hosted on-device or using less resources, and some can have other business or economic constraints. Further, how an application plans to use VLMs can also vary. They can be adapted to align closer to downstream requirements [6], in a customized pipeline [12], or directly without any modifications [18]. To accommodate all these needs, we evaluate 10 variations of 8 different VLMs using our method, and identify the best models for different tasks amongst the currently existing VLMs, allowing users to select the best alternative as per their requirement. With our analysis, we establish that the VLM performances aren't highly correlated, and models indeed have different strengths and weaknesses. They perform well in some categories, and struggle in others. But the struggling VLM's performance can be compensated by another VLM which is strong in that category, making appropriate selection a crucial step.\nIn summary, we aim to address the following research questions (RQs): (1) How to compare VLMs for different types of VQA tasks under practical application requirements? (2) How to appropriately evaluate VLM outputs for VQA that align closely with human judgements? (3) As per current state-of-the-art, which VLM is suited for which application, depending on various external usage constraints?\nOur key contributions are as follows:\n(i) We release a dataset covering VQA tasks classified into three aspects: task types, application domains, and knowledge type, enabling VLM comparison based on different practical requirements.\n(ii) We propose GoEval, which is a multimodal evaluation metric based on GPT-40 [30], and aligns closer to human judgements for visual question-answering.\n(iii) We analyze 10 variants of 8 state-of-the-art VLMs of different sizes, families, using our framework to compare their performance under practical settings."}, {"title": "2. Experimental Dataset Creation", "content": "In this section, we discuss our dataset creation steps we followed in detail, which we propose to utilize for evaluating VLMs for VQA."}, {"title": "2.1. Source Datasets", "content": "To be able to evaluate VLMs on a wide variety of QA tasks, our experimental dataset is created from five standard datasets - VQAv2 [13], OK-VQA [25], A-OKVQA [31], ChartQA [26] and DocumentVQA [27]. VQAv2 is an extensive VQA benchmark, while OK-VQA and A-OKVQA are used primarily for knowledge-based VQA, where the answer to the question doesn't lie within the scope of the image. ChartQA and DocumentVQA are taken to evaluate the performance of VLMs on questions based on mathematical graphs and charts, and documents, respectively. From the test split of each dataset $D_{test}$, we take max($D_{test}$, 1145) task instances, sampled randomly without replacement. We include the image, question, and all reference answers. 1145 was used as is the minimum size of test set among the five datasets, thus keeping equal contributions from all datasets. Thus, our final experimental set contains 5725 task instances, with equal contributions from each dataset."}, {"title": "2.2. Instance Tags", "content": "Using the dataset mentioned above only allows us to differentiate based on task types. But there are more ways in which a practical application can be classified. In order to achieve a multi-dimensional comparison that can be helpful for users, we also classify our experimental dataset in two more aspects - application domains, and knowledge types. This is inspired from a recent work [32], but adapted to suit this task. Application domain is the field a task belongs to, like History, Science, Sports, etc., and the knowledge type is a specific type of knowledge required, like Geographical, Common sense, etc. Both of them are different factors. For example, one can show a crest of a Football team, and ask where is the home stadium of this team located. This is a Sports domain question, but requires a Visual and Geographical Knowledge. Thus, both aspects are necessary.\nOur initial set of tags are crafted manually, with an aim to achieve a broad spectrum of application domains, and reasoning types. They are specified in Table 1, and cover a wide range of domains and knowledge types. We tag each of our task instances in the dataset with one or more application domains and knowledge types using the method outlined in Figure 2 and discussed below."}, {"title": "2.3. Generating Instance Tags", "content": "Creating the tags for task type is straightforward - we map ChartQA to 'Chart Understanding', DocumentVQA to 'Document Understanding', A-OKVQA and OKVQA to 'Knowledge-based Visual Question-Answering' and VQAv2 to 'Visual Question-Answering'.\nFor application domains and knowledge types, we utilize gpt-3.5-turbo [5, 28]. To correctly generate instance tags, we require key features of the image and question, and also need to eliminate less useful information from the image to avoid confusions in tag generation. To achieve this, following a recent work [12], we generate captions of images from VIT-GPT22, describing key information of the image, and object tags, containing all the objects in the image from Azure Computer Vision API\u00b3. Using these two as the descriptor of the image, and the question, we prompt gpt-3.5-turbo to get the application domains and knowledge type. The prompt used for this task is given in Table 7 in the Appendix A (in supplementary material). After this, we post-process the tags by removing entries which don't belong to the any of the entries in Table 1. If all tags are removed for a task instance, we add \"Other\" by default. In the last step, we manually go through each of the classifications and ensure that they are correctly tagged, and replacing any erroneous tag. The final set contains questions, images, candidate answers, task type, application domains and knowledge type of all 5725 candidates.\nFrom the instance tags, we remove the tags for which number of instances are less than 300 from our analyses. Please note, we don't remove the task instances entirely, as they may contain other tags included in the study, but just not consider those tags in reporting our results in Section 4 due to less number of instances. This gives a final set of 5 task types, 14 application domains, and 9 knowledge types, which are shown in Figure 3. We also report the mean, max and std. of number of tags per instance, along with caption length and number of object tags per instance in Table 2.\nWe use this dataset for rest of the analysis in this paper, and also release it publicly, so that they can be used by the research community in further research. Find the link to access it in footnote of Page 1."}, {"title": "3. GoEval: A VQA Evaluation Metric", "content": "With the shift from extractive to generative models for QA, the limitations of evaluation metrics based on lexical matching have been exposed, particularly when the answers are correct, but not do not appear in the set of gold answers [17]. A recent work [17] evaluated traditional VQA metrics against GPT-based evaluation for open-domain QA."}, {"title": "3.1. Validating GoEval", "content": "To validate GoEval, we first generate outputs on our experimental dataset using Gemini-1.5-Pro [33], a state-of-the-art VLM. Then, we manually evaluate all the answers on the validation set, marking 0 for incorrect answer and 1 for correct answer. These are the human evaluation results.\nWe compare BERTScore precision, recall, F1-score, and six variants of GoEval using GPT-40 and GPT-40 mini [30], with and without using reference answers, and with and without using images with human evaluations in terms of accuracy and Kendall's Tau. The exact prompts that we use are outlined in Table 8 of Appendix A (in the supplementary material). When we don't use the image, we also alter the template text by a little to not ask model to refer the image. Results are detailed in Table 3.\nFrom the results, we can clearly see that using GoEval with reference answers, images, together provides best alignment with human judgement. Without using the image (-R, -I), the performance proves to be very weak, depicting that existing metrics that don't utilize the image will perform poorly on VQA. Moreover, the differences between models are largely influenced by whether they use all available information. For example, performance of GoEval and GoEval-mini with image but without reference only has a difference of $\\Delta \\tau$ = 11, but as soon as the reference answers are added, $\\tau$ increased by 20 units. This shows that all these information are necessary for a successful evaluation. When used, GoEval shows high accuracy and Kendall's Tau correlation with human evaluation, and GoEval-mini marginally outperforms GoEval."}, {"title": "4. Comparative Evaluation of VLMs", "content": "We evaluate state-of-the-art (SOTA) VLMs in practical settings using our framework. We use GoEval-mini for all the evaluations, since it demonstrated maximum correlation with human judgements, and is more cost-efficient.\nFor the VLMs, we use InternVL-2 1B and 8B [9, 10], PaliGemma-3B [4], Qwen-2-VL 2B and 7B [2, 34], LlaVa-1.6-Mistral-7B [22], CogVLM2-Llama-3-19B [15, 36], Gemini-1.5 Flash and Pro [33], and GPT-40-Mini [30]. The rationale behind choosing these models is to have sufficient diversity to allow users to choose appropriate VLM based on other constraints.\nAll models except Gemini-1.5 and GPT-40 are open sourced, which gives freedom to customize the models as desired, and host it in-house. It takes away the privacy and regulatory risk of sending data to a third-party. It also reduces operational and opportunity cost factor, as these APIs have high associated cost and rate limits. In-house hosting allows a relatively fixed cost rather than a variable cost with usage. Out of these open-sourced VLMs, we have taken smaller models in 1B-3B range too, which can be used in resource-constrained environments and on-device AI, if desired. Larger models may give better performance, but for more resource. If an application expects SOTA performance, doesn't have constraints of API cost or rate limits, and doesn't want any constraints of hosting the models, Gemini-1.5 and GPT-40 models can be utilized directly.\nFor all the open-sourced models, we use HuggingFace implementation with the image and question in a prompt recommended by the model card, since we want to evaluate all scenarios unformly. For Gemini models, we use the Gemini APIs, and for GPT-4o-mini, we use the OpenAI API. The results are discussed in the following subsections. More details about the artifacts used are given in Table 5 of the Appendix A.1 (in supplementary material)."}, {"title": "4.1. Correlation Between VLM Outputs", "content": "Our first hypothesis was that all VLMs don't perform similar with all task instances. They have their own strengths and weaknesses. To establish that, we evaluate the correlation between outputs of different models for all task instances, and document the results in Figure 4.\nFrom the figure, most of the correlations are blue, resembling low correlation between outputs. This shows that the performance difference between VLMs is not just in terms of a global statistic, but also differs for individual task instances. Thus, some VLMs that might be great on a subset of tasks, may be poor on another subset.\nThe highest correlation is observed between Gemini-1.5 Flash and Pro, followed by Qwen-2-VL 2B and 7B, and InternVL-2-1B and 8B. This behavior can be explained by the fact that these VLMs are part of the same family. They might have similar training data, training strategy, and architecture. Some open-source VLMs also exhibit higher correlations (light blue) with the three proprietary models-Gemini-1.5-Pro, Gemini-1.5-Flash, and GPT-40-mini-though these correlations remain relatively low.\nSince task types, application domains and knowledge types are key factors on which tasks can be differentiated in practical settings, we move to analyzing the performance on those factors in the following subsections."}, {"title": "4.2. Evaluation on Task Types", "content": "We evaluated VLMs on four different task types - Chart Understanding using ChartQA dataset [26], Document Understanding using DocVQA dataset [27], Knowledge-based VQA using A-OKVQA [31] and OK-VQA [25], and general VQA using VQAv2 [13]. The performance of different VLMs across these task types is summarized in Table 4.\nFrom the table, the closed models, which are believed to be SOTA, outperform the smaller, open-sourced models. This is expected considering those models are larger. Within them, we identify a pattern that Gemini-1.5-Pro performs significantly better than GPT-4o-mini when visual inference and extracting information from image is more important, like interpreting documents and charts. On the other hand, where knowledge and comprehension is critical with tricky questions using standard images, GPT-40-mini outperforms Gemini 1.5-Pro. Therefore, based on the nature of the problem, one of these models can be chosen. If cost of processing is a factor, Gemini-1.5-Flash can be chosen with approximately 7-10% performance loss.\nAmong the open-sourced models, InternVL-2 shows promising results with both 1B and 8B models given their size, and can be chosen if open-sourced models are needed for their advantages, based on resource availability. CogVLM-2-Llama-3-19B also competes closely with Gemini-1.5-Flash in Chart and Document understanding tasks. Llava-1.6-Mistral-7B performs acceptable in knowledge-based VQA and VQA, but the performance degrades drastically in the other two categories, where visual abilities are critical, exposing its limitations in that area. Qwen-2-VL variants and PaliGemma-3B surprisingly prove to be weak in all tasks."}, {"title": "4.3. Evaluation on Application Domains", "content": "We evaluate the VLMs on all application domains which had more than 300 task instances, and the results are shown in Figure 5.\nIf the VLMs would be similar in all application domains, their result will be perfectly circular. But, we see that most of the VLM performance graphs are not circular and have aberrations in multiple categories, which establish that VLMs have weaknesses and strengths. In most cases, the weakness of one of few VLMs is compensated in the strength of others. So, choosing a VLM wisely as per the application need can help mitigating the weaknesses.\nAmong the closed models, GPT-40-mini proves to be the best in four categories - Nature, Nutrition and Food, Social Media and Sports, and Gemini-1.5-Pro proves to be the best in all other categories. GPT-40-mini doesn't even remain second best in some categories like Mathematics, Economics, Law, and is outperformed by many open-sourced models as well in these categories. Thus, if one uses this model in these, they may not achieve results even close to best possible. Gemini-1.5-Flash remains strong with a performance deficit compared to Pro, but in Social Media tasks, it almost matches Pro. Therefore, while appropriate model should be selected based on requirement, and Gemini 1.5-Pro generally looks to be the best overall choice.\nIn the open-sourced model category, InternVL-2-8B and CogVLM-2-Llama-3-19B are the best possible choices. CogVLM-2-Llama-3-19B generally performs well in more academic topics like Mathematics, Computer Science, law, Government and Politics, but suffers a lot of performance degradation in more social topics like Nature, Nutrition and Food, Social Media, Sports. So, it may be a decent choice for the earlier, but not for the latter categories. InternVL-2-8B also shows similar traits, but the difference is relatively less. For academic topics, these models prove particularly strong, sometimes even outperforming some of the the closed models. Llava-1.6-Mistral-7B is one model that shows exactly opposite trait than this, being limited in academic topics as compared to social topics, but its performance remains at a gap compared to the two previously mentioned models. Qwen-2 variants and PaliGemma show weak results in all domains, like in task types. InternVL-2-1B remains the best choice if small-sized model is required, with decent results using 1B parameters."}, {"title": "4.4. Evaluation on Knowledge Types", "content": "Similar to application domains, we evaluate all VLMs on all knowledge types where number of task instances are greater than 300. We demonstrate the results using a similar radar chart in Figure 6.\nIn the closed models category, the SOTA performance is again shared by Gemini-1.5-Pro and GPT-40-mini. But unlike in application domains, GPT-40-mini competes more strongly with Gemini-1.5-Pro on several knowledge types. Visual, Social and Commonsense knowledge is where the advantage of GPT-40-mini over Gemini-1.5-Pro is maximum, again depicting its strength in social knowledge types. It's still considerably weak in temporal, scientific, and mathematical knowledge, falling behind even some of the open-sourced small models. Gemini-1.5-Pro will still be the better choice overall, but different models can be selected based on the results obtained for specific knowledge types. Gemini-1.5-Flash again follows similar trend, with slight performance difference from Pro.\nComparing the open-sourced models, InternVL-2-8B and CogVLM-2-Llama-3-8B are the best choices here as well, but here we can see that InternVL-2-8B outperforms the latter in all knowledge types. InternVL-2-1B continues to be the best overall choice if small models are required, and Qwen-2 variants and PaliGemma continue to prove to be least effective. Therefore, InternVL-2-8B is the best overall choice for open-sourced models."}, {"title": "4.5. Overall Analysis and Recommendations", "content": "In subsections 4.2, 4.3 and 4.4, we evaluated all vision-language models under three different aspects of task types, application domains and knowledge types. We also discussed their strengths and weaknesses in those categories and recommended different VLMs to use under different requirements. In this section, we will take a higher-level look considering everything together.\nWe identified that Gemini-1.5-Pro and GPT-40-mini, both are strong models. In general, GPT-40-mini is found to be weak in analytical tasks, which belonged to Mathematics or Economics domain, or scientific or mathematical knowledge types. It not only falls behind Gemini-1.5-Pro in these tasks, but also behind open models like InternVL-2-8B or CogVLM-2-Llama-3-19B. However, it is strong is social and topical tasks like Nature, Sports, Nutrition and Food domains. Gemini-1.5-Pro is a strong model across multiple usages. It is either the best, or comes close second or third best. However, this model is expensive to use. Therefore, if cost is a significant factor, Gemini-1.5-Flash can be considered as a decent alternative, which performs in similar trends as the Pro, but at a performance deficit of around 7-10%. Despite this difference, its overall performance is very strong, considering that it's significantly cost efficient in comparison to Pro.\nInternVL-2-8B and CogVLM-2-Llama-3-19B are strong open-sourced models. Due to size differences, resource availability also contributes in deciding which model to use. CogVLM-2-Llama-3-19B is better at more academic tasks, that belong to domains like History, Law, Computer Science, etc., or knowledge types like Temporal, Scientific or Mathematical Knowledge. InternVL-2-8B is a more overall capable model, proving to be more suited in broader application requirements. It is generally the best or close second-best in in open-sourced models, and also outperforms Gemini-1.5-Flash in many categories. In some cases, it outperforms GPT-40-mini as well. Possessing superior performance in addition to other advantages of open-sourced model makes it a strong choice. One should consider it if open-souced models are required and sufficient resources are available to host the model. It can also be aligned for downstream tasks to improve performance.\nAmong small models suited for on-device AI, resource-efficient environments, InternVL-2-1B proves is strongest overall, significantly outperforming models like Qwen2-VL-2B and PaliGemma-3B in all categories.\nThe Qwen-2-VL variants and PaliGemma-3B didn't prove fit for use in our experimental settings, being very weak on all categories. LlaVa-1.6-Mistral-7B also performs average, similar to InternVL-2-1B, but is weak in all aspects compared to InternVL-2-8B, a similar sized model."}, {"title": "5. Conclusion", "content": "In this paper, we propose a comprehensive framework for evaluating Vision-Language Models (VLMs) across a wide spectrum of visual question-answering (VQA) tasks. The framework enables a rigorous comparison of VLMs based on specific application requirements. We introduce a novel evaluation paradigm that classifies VQA tasks according to three key dimensions: task types, application domains, and knowledge types. To facilitate this, we release a dataset derived from established VQA benchmarks, and annotated 4 four task types, 22 application domains, and 15 knowledge types for classification.\nAdditionally, we present a new evaluation metric, GoEval, which demonstrates a correlation factor of 56.71% with human judgments, making it a superior choice compared to traditional metrics. GoEval leverages the capabilities of GPT-40 for evaluation, incorporating both visual and textual information to deliver more nuanced performance assessments.\nThrough extensive experimentation with 10 state-of-the-art VLMs, we find that model performance varies significantly across different categories, with no single VLM emerging as universally optimal. Notably, proprietary models such as Gemini-1.5-Pro and GPT-40-mini achieve the highest performance overall, while open-source models like InternVL-2-8B and CogVLM-2-Llama-3-19B, exhibit competitive results in specific scenarios. Our findings offer practical insights for selecting VLMs based on task-specific requirements and resource constraints, establishing a standardized framework for VLM evaluation and selection in downstream tasks. Furthermore, this evaluation framework can be extended to other vision-language tasks beyond VQA, fostering advancements in multimodal research."}, {"title": "A. Appendix", "content": "In this appendix, we provide additional details and results related to our work. Implementation details can be found in Section A.1. Table 7 provides prompts designed to classify tasks by domain (e.g., \"Anthropology,\" \"Computer Science\") and by the type of knowledge required (e.g., \"Commonsense Knowledge,\" \"Visual Knowledge\"). Table 8 presents the prompts used in the GoEval to verify whether a candidate answer is correct, with or without reference answers, both for visual and text-only evaluation. Additionally, Table 9 details some qualitative examples using the best overall model that we found - Gemini-1.5-Pro. At last, Table 10 and 11 contain quantitative results on all application domains and knowledge types respectively, including the categories that were excluded from the study of the main paper."}, {"title": "A.1. Implementation Details", "content": "In this subsection, we will discuss more around implementation details. Table 5 contains all the model cards, which contain exact details about how we implemented all the VLMs, and the recommended prompt templates that we used in our evaluation."}, {"title": "A.2. Using this Work to Select VLM", "content": "The prerequisite to using this work is to lay down the problem statement and its scope along with other system parameters that should include, but shouldn't be limited to resource availability, data availability, system constraints, resource or data processing budget, acceptable performance bounds, etc.\nStart with finding the task type, application domain and reasoning type closest to your requirement from Table 4, 10 and 11. Next, from your design constraints, identify some sets of VLMs acceptable for your solution. For example, if using on-device AI, you might only be able to use either small VLMs, or closed model accessible by APIs, depending on your acceptable performance bounds, and regulatory aspects of being able to share data across, having the inference budget for using APIs, and so on. Refer to Section 4 for a more detailed discussion around which models will fit which needs. Between those, check the performance of the subset of categories and subset of models, and choose the best model.\nThese models provide a comparison on a uniform foundation so that a comparative analysis can be done. These models can further be customized as desired for best outputs as per other design parameters and needs."}]}