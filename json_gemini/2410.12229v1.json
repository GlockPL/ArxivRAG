{"title": "Comprehending Knowledge Graphs with Large Language Models for Recommender Systems", "authors": ["Ziqiang Cui", "Yunpeng Weng", "Xing Tang", "Fuyuan Lyu", "Dugang Liu", "Xiuqiang He", "Chen Ma"], "abstract": "In recent years, the introduction of knowledge graphs (KGs) has significantly advanced recommender systems by facilitating the discovery of potential associations between items. However, existing methods still face several limitations. First, most KGs suffer from missing facts or limited scopes. This can lead to biased knowledge representations, thereby constraining the model's performance. Second, existing methods cannot effectively utilize the semantic information of textual entities as they typically convert textual information into IDs, resulting in the loss of natural semantic connections between different items. Third, existing methods struggle to capture high-order relationships in global KGs due to their inefficient layer-by-layer information propagation mechanisms, which are prone to introducing significant noise. To address these limitations, we propose a novel method called CoLaKG, which leverages large language models (LLMs) for knowledge-aware recommendation. The extensive world knowledge and remarkable reasoning capabilities of LLMs enable them to supplement KGs. Additionally, the strong text comprehension abilities of LLMs allow for a better understanding of semantic information. Based on this, we first extract subgraphs centered on each item from the KG and convert them into textual inputs for the LLM. The LLM then outputs its comprehension of these item-centered subgraphs, which are subsequently transformed into semantic embeddings. Furthermore, to utilize the global information of the KG, we construct an item-item graph using these semantic embeddings, which can directly capture higher-order associations between items. Both the semantic embeddings and the structural information from the item-item graph are effectively integrated into the recommendation model through our designed representation alignment and neighbor augmentation modules. Extensive experiments on four real-world datasets demonstrate the superiority of our method.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of web applications has led to an increasingly critical issue of information overload. Recommender systems address this problem by modeling user preferences based on historical data and providing personalized recommendations. Collaborative filtering (CF) [9, 10, 22], as one of the most classic and efficient methods, has been extensively employed in existing recommender systems. However, CF-based methods exclusively rely on user-item collaborative signals, often suffering from the data sparsity issue for users who have not interacted with a sufficient number of items [25]. To address such data sparsity issue, recent studies [32, 41, 43] have incorporated knowledge graphs (KGs) as external knowledge sources into recommendation models, achieving significant progress. Typically, these methods capture diverse and higher-order semantic relationships between items by"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Knowledge-aware Recommendation", "content": "Existing knowledge-aware recommendation methods can be categorized into three types [8]: embedding-based methods, path-based methods, and GNN-based methods. Embedding-based methods [3, 28, 43] enhance the representations of users and items by leveraging the relations and entities within the KGs. Notable examples include CKE [43], which integrates various types of side information into a collaborative filtering framework using TransR [19] for structural knowledge. Another example is DKN [28], which improves news representations by combining textual embeddings of sentences and knowledge-level embeddings of entities. Path-based methods leverage KGs to explore long-range connectivity [13, 34, 42]. For example, Personalized Entity Recommendation (PER) [42] treats a Knowledge Graph (KG) as a heterogeneous information network and extracts meta-path-based latent features to represent the connectivity between users and items along various types of relational paths. MCRec [13] constructs meta-paths and learns the explicit representations of meta-paths to depict the interaction context of user-item pairs. Despite their effectiveness, these approaches heavily rely on domain knowledge and human effort for meta-path design. Recently, GNN-based methods have been proposed, which enhance entity and relation representations by aggregating embeddings from multi-hop neighbors [29, 32, 33]. For instance, KGAT [32] employs graph attention mechanisms to propagate embeddings and utilizes multi-layer perceptrons to generate final recommendation scores in an end-to-end manner. Similarly, KGIN [33] adopts an adaptive aggregation method to capture fine-grained user intentions. Additionally, some methods [31, 40, 41, 47]"}, {"title": "2.2 LLMs for Recommendation", "content": "In light of the emergence of large language models and their remarkable achievements in the field of NLP, scholars have begun to explore the potential application of LLMs in recommender systems [4, 6, 37, 45]. Due to the powerful reasoning capabilities and extensive world knowledge of LLMs, they have been already naturally applied to zero-shot [11, 12, 30] and few-shot recommendation scenarios [2, 18]. In these studies, LLMs are directly used as a recommendation model [17, 46], where the output of LLMs is expected to offer a reasonable recommendation result [38]. However, when the dataset is sufficiently large, their performance often falls short of that achieved by traditional recommendation models. Another line of research involves leveraging LLMs as feature extractors. These methods [1, 14, 15, 21, 21, 35, 36, 48] generate intermediate decision results or semantic embeddings of users and items, which are then input into traditional recommendation models to produce the final recommendations. Unlike existing methods, our approach aims to leverage the extensive knowledge base and reasoning capabilities of LLMs to understand KGs and transform them into semantic embeddings, thereby addressing existing issues in KG-based recommender systems and enhancing recommendation performance."}, {"title": "3 Preliminaries", "content": "User-Item Interaction Graph. Let \\(U\\) and \\(V\\) denote the user set and item set, respectively, in a recommender system. We construct a user-item bipartite graph \\(G = {\\{(u, Y_{uv},v)|u \\in U,v \\in V\\}\\) to"}, {"title": "4 Methodology", "content": "In this section, we introduce our proposed method CoLaKG in detail. An overview of our method is illustrated in Figure 2. For each item, we extract a subgraph centered on it from the KG. The LLM then comprehends this subgraph and converts it into a semantic embedding, thereby fully utilizing the local information of the KG. Additionally, we enhance the representation of each item by aggregating the most semantically similar neighbors based on the semantic similarity between items, leveraging the global information of the KG. Furthermore, we generate semantic embeddings for users based on their preferences derived from the KG. Finally, these semantic embeddings are integrated with the ID representations of the recommendation model, resulting in enhanced representations of both items and users, thereby improving the performance of the recommendation model."}, {"title": "4.1 KG Comprehension with LLMs", "content": "KGs have been widely utilized in recommender systems to provide semantic information and model latent associations between items. However, KGs are predominantly manually curated, leading to missing facts and limited knowledge scopes. Additionally, the highly structured nature of KGs poses challenges for effectively utilizing textual information. Existing methods often transform textual entities and relations into IDs, resulting in a significant loss of semantic information. Recently, the rapid emergence of LLMs has provided a promising approach to addressing these issues. Their extensive knowledge and reasoning abilities enable them to complete missing entities and expand the knowledge scopes of KGs. Furthermore, their inherent text comprehension abilities facilitate the effective utilization of textual entities. Leveraging these advantages, we propose the use of LLMs to enhance the understanding and refinement of KGs for improved recommendations."}, {"title": "4.1.1 Item-Centered KG Subgraph Comprehension", "content": "Equipping LLMs to comprehend KGs presents certain challenges, as LLMs cannot directly interpret non-textual graph data. Consequently, KGs must be converted into a textual format. However, due to the vast number of entities in a KG, inputting the entire KG into an LLM is impractical. To address this, we initially extract the subgraph of the KG centered on each item. This approach enables the effective utilization of local KG information for each item. Note that we also consider the utilization of global KG information, which will be discussed in Section 4.1.2.\nFirst, we represent the first-order KG subgraph centered on each item (i.e., ego network [20]) using triples. Specifically, given an item \\(v \\in V\\), we use \\(T_1 = {\\{(v, r, e)|(v, r, e) \\in G_k \\}\\} \\) to denote the set of triplets where \\(v\\) is the head entity. In the context of recommendation, the first-order neighboring entities of an item in a KG are usually attributes. Therefore, we use \\(e\\) to represent these attribute entities to distinguish them from those item entities \\(v\\). During generating triples, in cases where the attribute or relation is absent, the term \u201cmissing\u201d is employed as a placeholder. Next, we consider the second-order relations in KGs. The number of entities in an ego network centered on a single entity increases exponentially with the growth of the radius. However, the input length of an LLM is strictly limited. Consequently, including all second-order neighbors associated with the central item in the prompt becomes impractical. To address this issue, we adopt a simple but effective strategy, random sampling, to explore second-order connections of \\(v\\). Let \\(E_v = \\{e | (v, r, e) \\in T\\}\\) denote the set of first-order connected neighbors of \\(v\\). For each \\(e \\in E_v\\), we randomly sample \\(m\\) triples from the set \\(T_e\\) to construct the triples of second-order connections, denoted as \\(T_m\\). Here, \\(T_e = {\\{(e, r, v') | (e, r, v') \\in G_k, v' \\neq v\\}\\} \\) represents the set of triples where \\(e \\in &\\) is the head entity. We fix \\(m\\) to 10 in our paper and do not perform hyperparameter exploration due to cost considerations associated with the LLM.\nAfter converting first-order and second-order relationships into triples, we transform these triples into textual form. For first-order relations, we concatenate all the first-order triples in \\(T\\) to form a single text, denoted as \\(D_v\\). For second-order relations, we use a template to transform the second-order triples \\(T_m\\) into coherent sentences \\(D'\\), facilitating the understanding of the LLM. In addition to \\(D_v\\) and \\(D'\\), we have carefully designed a system prompt \\(I_v\\) as the instruction to guide the generation. By combining \\(I_v\\), \\(D_v\\), and \\(D'\\), we obtain the prompt, which is shown in Figure 3. The prompt enables the LLM to fully understand, complete, and refine the KG, thereby generating the final comprehension for the \\(v\\)-centered KG subgraph. This process can be formulated as follows:\n\\[C_v = \\text{LLM}_S(I_v, D_v, D').\\]"}, {"title": "4.1.2 Semantic-Relational Item-Item Graph Construction", "content": "This section introduces the utilization of global KG information. Items that are distant in the KG can still have close semantic associations."}, {"title": "4.2 User Preference Comprehension", "content": "The introduction of KGs allows for the expansion of user-item bipartite graphs and enables us to understand user preferences from a knowledge-driven perspective. Given a user \\(u\\), we first extract the subgraph corresponding to user \\(u\\) from the user-item bipartite graph, denoted as \\(B_u\\). For each item \\(v \\in B_u\\), we extract its first-order KG subgraph and represent it as a set of triples, denoted as \\(T_v\\). We then concatenate all triples in \\(T_v\\) to form a single text, denoted as \\(D_v\\). The detailed approach is the same as described in Section 4.1.1. Subsequently, we represent user \\(u\\) with all items the user has interacted with in the training set and the corresponding knowledge triples \\(D_v\\):\n\\[D_u = \\bigoplus_{v \\in G_u} {\\text{name} : D_v},\\]\nwhere \\(\\bigoplus\\) denotes concatenation operation, and name denotes the text name of item \\(v\\). Additionally, we have meticulously designed a system prompt, denoted as \\(I_u\\), to serve as an instruction for guiding the generation of user preferences. By combining \\(D_u\\) and \\(I_u\\), we enable the LLM to comprehend the user preference for \\(u\\), which can be formulated as:\n\\[C_u = \\text{LLM}_S(I_u, D_u).\\]\nFurthermore, we also utilize the text embedding function \\(P\\) to transform the textual answers \\(C_u\\) into embedding vectors \\(s_u\\), which can be expressed as:\n\\[s_u = P(C_u).\\]"}, {"title": "4.3 Representation Alignment and Neighbor Augmentation", "content": ""}, {"title": "4.3.1 Cross-Modal Representation Alignment", "content": "In a traditional recommendation model, each item and user is associated with an ID embedding. Let \\(e_v \\in R^d\\) represent the ID embedding of item \\(v\\) and \\(e_u \\in R^d\\) represent the ID embedding of user \\(u\\). In addition to these ID embeddings, we also obtain the semantic embedding \\(s_v \\in R^{d_s}\\) w.r.t. the comprehension of \\(v\\)-centric KG subgraph, and the semantic embedding \\(s_u \\in R^{d_s}\\) w.r.t. the comprehension of user \\(u\\)\u2019s preference. Since ID embeddings and semantic embeddings belong to two different modalities and typically possess different embedding dimensions, we employ an adapter network to align the semantic embeddings with the ID embedding space. Specifically, the adapter networks consist of a linear map and a non-linear activation function, which are formulated as:\n\\[\\hat{s}_v = \\sigma(W_1 s_v); \\ \\hat{s}_u = \\sigma(W_2 s_u),\\]\nwhere both \\(W_1 \\in R^{d \\times d_s}\\) and \\(W_2 \\in R^{d \\times d_s}\\) are are weight matrices, \\(\\sigma\\) represents the non-linear activation function ELU [5].\nNote that during the training process, we fix \\(s_v\\) and \\(s_u\\), training solely the corresponding projection parameters \\(W_1\\) and \\(W_2\\), and the parameters of the recommendation model. The benefits of this method are two-fold. Firstly, by preserving \\(s_v\\) and \\(s_u\\), we can utilize the rich semantic information they already contain, which can guide the recommendation model to converge more effectively during the initial stage of training. Secondly, the number of parameters in \\(s_v\\) and \\(s_u\\) is typically much greater than those in the recommendation model's ID embeddings due to their large dimensions. Consequently, altering these parameters would significantly affect the updates to the ID embeddings and slow down the gradient update process, leading to an unstable training procedure.\nAfter mapping the representations to the same space, we need to fuse the representations of the two modalities, leveraging both the collaborative signals and the semantic information to form a complementary representation. To achieve this, we employ a straightforward mean pooling technique to fuse their embeddings,"}, {"title": "4.3.2 Item Representation Augmentation with Semantic-related Neighbors", "content": "For each item, we have obtained its semantic-related items from the constructed item-item graph in Section 4.1.2. To fully utilize these neighbors, we propose to aggregate their information to enhance the representations. Considering the varying contributions of different neighbors to the central item, we employ the attention mechanism for weighted aggregation of representations.\nSpecifically, for item \\(v_i\\) and its top-\\(k\\) neighbor set \\(N_k(v_i)\\), we first compute attention coefficients that indicate the importance of item \\(v_j \\in N_k(v_i)\\) to item \\(v_i\\) as follows:\n\\[w_{ij} = a(Ws_{v_i} || Ws_{v_j}).\\]\nHere, \\(W \\in R^{d_a \\times d}\\) is a learnable weight matrix to capture higher-level features of \\(s_{v_i}\\) and \\(s_{v_j}\\), \\(||\\) is the concatenation operation, \\(a\\) denotes the attention function: \\(R^{d_a} \\times R^{d_a} \\rightarrow R\\), where we adopt a single-layer neural network and apply the LeakyReLU activation function following [26]. Note that the computation of attention weights is exclusively dependent on the semantic representation of items, as our objective is to calculate the semantic associations between items, rather than the associations present in collaborative signals. In addition, we employ the softmax function for easy comparison of coefficients across different items:\n\\[a_{ij} = \\text{softmax}_j(w_{ij}).\\]\nThe attention scores \\(a_{ij}\\) are then utilized to compute a linear combination of the corresponding neighbor embeddings. Finally, the weighted average of neighbor embeddings and the embedding of item \\(v_i\\) itself are combined to form the final output representation for item \\(v_i\\):\n\\[\\hat{h}_{v_i} = \\sigma\\left(h_{v_i} + \\bigoplus_{j \\in N_k(v_i)} a_{ij}h_{v_j}\\right).\\]\nwhere \\(\\sigma\\) denotes the non-linear activation function."}, {"title": "4.4 User-Item Modeling", "content": "Having successfully integrated the semantic information from the KG into both user and item representations, we can use them as inputs for traditional recommendation models to generate prediction results. This process can be formulated as follows:\n\\[\\hat{Y}_{uv} = F(h_u, \\hat{h}_v),\\]\nwhere \\(\\hat{Y}_{uv}\\) is the predicted probability of user \\(u\\) interacting with item \\(v\\), \\(h_u\\) is the representation for user \\(u\\), \\(\\hat{h}_v\\) is the augmented representation for item \\(v\\), and \\(F\\) denotes the function of the recommendation model.\nSpecifically, we select the classic model, LightGCN [10], as the architecture for our recommendation method due to its simplicity and effectiveness. The trainable parameters of original LightGCN are only the embeddings of users and items, similar to standard"}, {"title": "4.5 Model Training", "content": "Our approach can be divided into two stages. In the first stage, we employ the LLM to comprehend the KGs, generating corresponding semantic embeddings for each item and user, denoted as \\(s_v\\) and \\(s_u\\), respectively. In the second stage, these semantic embeddings are integrated into the recommendation model through an adapter network to enhance its performance. Only the second stage necessitates supervised training, where we adopt the widely-used Bayesian Personalized Ranking (BPR) loss:\n\\[\\mathcal{L} = \\sum_{(u,v^+,v^-) \\in O} -\\text{lno}(\\hat{y}_{uv^+} - \\hat{y}_{uv^-}) + \\lambda ||\\Theta||^2.\\]\nHere, \\(O = \\{(u,v^+,v^-)|(u,v^+) \\in R^+, (u,v^-) \\in R^- \\}\\) represents the training set, \\(R^+\\) denotes the observed (positive) interactions between user \\(u\\) and item \\(v\\), while \\(R^-\\ indicates the sampled unobserved (negative) interaction set. \\(\\sigma(\\cdot)\\) is the sigmoid function. \\(||\\Theta||\\) is the regularization term, where \\(\\lambda\\) serves as the weight coefficient and \\(\\Theta\\) constitutes the model parameter set."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Settings", "content": ""}, {"title": "5.1.1 Datasets", "content": "We conducted experiments on four real-world datasets, including three public datasets (MovieLens\u00b9, MIND\u00b2, Last-FM\u00b3), and one industrial dataset (Fund). The statistics for these datasets are presented in Table 1. These datasets cover a wide range of application scenarios. Specifically, MovieLens is a well-established benchmark that collects movie ratings provided by users. MIND is a large-scale news recommendation dataset constructed from user click logs on Microsoft News. Last-FM is a well-known"}, {"title": "5.1.2 Evaluation Metrics", "content": "To evaluate the performance of the models, we employ widely recognized evaluation metrics: Recall and Normalized Discounted Cumulative Gain (NDCG), and report values of Recall@k and NDCG@k for k=10 and 20, following [10, 32]. To ensure unbiased evaluation, we adopt the all-ranking protocol. All items that are not interacted by a user are the candidates."}, {"title": "5.1.3 Baseline Methods", "content": "To ensure a comprehensive assessment, we compare our method with ten baseline methods, which can be divided into three categories: classical methods (BPR-MF, NFM, LightGCN), KG-enhanced methods (CKE, RippleNet, KGAT, KGIN, KGCL, KGRec), and LLM-based methods (RLMRec).\nBPR-MF [22] employs matrix factorization to model users and items, and uses the pairwise Bayesian Personalized Ranking (BPR) loss to optimize the model.\nNFM [9] is an advanced factorization model that subsumes FM [23] under neural networks.\nLightGCN [10] facilitates message propagation between users and items by simplifying GCN [16].\nCKE [43] is an embedding-based method that uses TransR to guide entity representation in KGs to enhance recommendation performance.\nRippleNet [27] automatically discovers users\u2019 hierarchical interests by iteratively propagating users\u2019 preferences in the KG.\nKGAT [32] designs an attentive message passing scheme over the knowledge-aware collaborative graph for node embedding fusion.\nKGIN [33] adopts an adaptive aggregation method to capture fine-grained user intentions."}, {"title": "5.1.4 Implementation Details", "content": "We implement all baseline methods according to their released code. The embedding size d for all recommendation methods is set to 64 for a fair comparison. All experiments are conducted with a single V100 GPU. We set the batch size to 1024 for the Last-FM dataset and 4096 for the other datasets to expedite training. The Dropout rate is chosen from the set {0.2, 0.4, 0.6, 0.8} for both the embedding layer and the hidden layers. We employ the Adam optimizer with a learning rate of 0.001. The maximum number of epochs is set to 2000. The number of hidden layers for the recommendation model L is set to 3. For the LLM, we select DeepSeek-V2, a robust large language model that demonstrates exceptional performance on both standard benchmarks and open-ended generation evaluations. For more detailed information about DeepSeek, please refer to their official website4. Specifically, we utilize DeepSeek-V2 by invoking its API5. To reduce text randomness of the LLM, we set the temperature \\(\\tau\\) to 0 and the top-p to 0.001. In addition, We fix the sampled number \\(m\\) to 10 and do not perform hyperparameter exploration due to cost considerations. For the text embedding model \\(P\\), we use the pre-trained sup-simcse-roberta-large6 [7]. We use identical settings for the baselines that also involve LLMs and text embeddings to ensure fairness in comparison."}, {"title": "5.2 Comparison Results", "content": "We compare 10 baseline methods across four datasets and run each experiment five times. The average results are reported in Table 2. Based on the results, we make the following observations:\n\u2022 Our method consistently outperforms all the baseline models across all four datasets. The performance ceiling of traditional methods (BPR-MF, NFM, LightGCN) is generally lower than that of KG-based methods, as the former rely solely on collaborative signals without incorporating semantic knowledge. However, some KG-based methods do not perform as well as LightGCN, indicating that effectively leveraging KG is a challenging task.\n\u2022 Among the KG-based baselines, KGCL and KGRec stand out the most. Both models incorporate self-supervised learning on top of general KG-based recommendation frameworks. During training, they jointly optimize the recommendation task and KG-based self-supervised tasks. However, they face challenges such as missing facts and difficulty in understanding semantic information. Additionally, they are unable to model higher-order associations of items within the KG. In contrast, our method does"}, {"title": "5.3 Ablation Study", "content": "In this section, we demonstrate the effectiveness of our model by comparing its performance with four different versions across all four datasets. The results are shown in Table 3, where \u201cw/o So\u201d denotes removing the semantic embeddings of items, \u201cw/o su\u201d denotes removing the semantic embeddings of users, \u201cw/o \\(N_k(v)\\)\u201d"}, {"title": "5.4 Hyperparameter Study", "content": "In this section, we investigate the impact of the hyperparameter \\(k\\) of \\(N_k(v)\\) on Recall@20 and NDCG@20 across four datasets. Here, \\(k\\) represents the number of semantically related neighbors, as defined"}, {"title": "5.5 Robustness to Varying Degrees of Sparsity", "content": "One of the key functions of KGs is to alleviate the issue of data sparsity. To further examine the robustness of our model against users with varying levels of activity, particularly its performance with less active users, we sort users based on their interaction frequency and divide them into four equal groups. A lower group ID indicates lower user activity (01 being the lowest, 04 the highest). We analyze the evaluation results on two relatively sparse datasets, Last-FM and MIND, as shown in Figure 5. By comparing our model with three representative and strong baseline models, we observe that our model consistently outperforms the baselines in each user group. Notably, the improvement ratio of our model in the sparser groups (01 and 02) is higher compared to the denser groups (03 and 04). For the group with the most limited data (Group 01), our model achieves the most significant lead. This indicates that the average improvement of our model is primarily driven by enhancements in the sparser groups, demonstrating the positive impact of CoLaKG in addressing data sparsity."}, {"title": "5.6 Case Study", "content": "In this section, we conduct an in-depth analysis of the rationality of our method through two real cases. In the first case, we present the movie \u201cApollo 13\u201d and its five semantically related neighbor items in the item-item graph identified by our method. The first three movies belong to the same genre as \u201cApollo 13\u201d, making them 2-hop neighbors in the KG. In contrast, the other two movies, \u201cTop Gun\u201d and \u201cStar Trek\u201d, do not share any genre or other attributes"}, {"title": "6 Conclusion", "content": "In this paper, we analyze the limitations of existing KG-based recommendation methods and propose a novel approach, CoLaKG, to address these issues. CoLaKG comprehends item-centered KG subgraphs to obtain semantic embeddings for both items and users. These semantic embeddings are then used to construct a semantic relational item-item graph, effectively leveraging global KG information. We conducted extensive experiments on four datasets to validate the effectiveness and robustness of our method. The results demonstrate that our approach significantly enhances the performance of recommendation models."}]}