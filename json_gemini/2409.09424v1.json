{"title": "NBBOX: Noisy Bounding Box Improves Remote Sensing Object Detection", "authors": ["Yechan Kim", "SooYeon Kim", "Moongu Jeon"], "abstract": "Data augmentation has seen significant advance- ments in computer vision to improve model performance over the years, particularly in scenarios with limited and insufficient data. Currently, most studies focus on adjusting the image or its features to expand the size, quality, and variety of samples during training in various tasks including object detection. However, we argue that it is necessary to investigate bounding box transforma- tions as a model regularization technique rather than image-level transformations, especially in aerial imagery due to potentially inconsistent bounding box annotations. Hence, this letter presents a thorough investigation of bounding box transformation in terms of scaling, rotation, and translation for remote sensing object detection. We call this augmentation strategy NBBOX (Noise Injection into Bounding Box). We conduct extensive experiments on DOTA and DIOR-R, both well-known datasets that include a variety of rotated generic objects in aerial images. Experimental results show that our approach significantly improves remote sensing object detection without whistles and bells and it is more time-efficient than other state-of-the-art augmentation strategies.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, remote sensing object detection has achieved remarkable success primarily due to the advance- ments in deep learning. Particularly, modern deep learning architectures such as convolutional neural networks and trans- formers have significantly enhanced the capabilities of object detection in Earth vision. These models have millions or billions of parameters to learn, requiring tremendous training data to avoid the over-fitting problem. Compared to natural scenes (e.g. ImageNet [1] or MS-COCO [2]), datasets for overhead imagery usually lack diversity and quantity due to the expense of data collection. To overcome this limitation, data augmentation plays a significant role as an implicit regularization strategy for model learning.\nExisting visual data augmentation methods can be mainly divided into three categories: (a) image manipulation (e.g. [3], [4], [5], [6]), (b) feature transformation (e.g. [7], [8]), and (c) generative image synthesis (e.g. [9], [10], [11]). For (a), geometric transformations (such as flipping and shearing), sharpness transformations, noise distribution, etc are consid- ered the simplest augmentations. More advanced strategies in image manipulation contain image erasing and image mix. The image erasing like [3], [4] randomly selects a sub-region in an image and deletes its contents, while the image mix like [5], [6] combines two or more images (or patches) as a single image during training. Meanwhile, (b) aims to directly ma- nipulate features in the latent space rather than that raw input image. For example, [7] applies augmentations to the extracted features and encourages the model to be consistent for both the original and augmented features. [8] leverages the synergy be- tween feature normalization and data augmentation to improve the robustness and accuracy of models. Unfortunately, both are hard to directly apply to rotated object detection. Finally, (c) produces samples with generative models based on GAN [9] or Diffusion [11]. Such methods can generate high-quality (for GAN) or high-fidelity (for Diffusion) samples, but these require significant computational resources.\nWhile most prior work for data augmentation concentrates on adjusting the image or its features, only few have focused on the deformation of bounding boxes for object detection. However, [12] discovers the presence of low-confidence anno- tations in current overhead object detection datasets: there is a mismatch between the minimum enclosing box (i.e. optimal) and the actual annotation. Thus, we argue that investigating bounding box transformation is necessary to boost the model performance for remote sensing object detection, by enabling robust training in potentially inaccurate bounding boxes.\nIn this letter, we propose a simple but efficient data augmen- tation method named NBBOX (Noise Injection into Bounding Box) for remote sensing object detection. Intuitively, our method adds noise to bounding boxes with geometric transfor- mations during training (not for test/deployment). Particularly, this work thoroughly investigates bounding box transformation in terms of scaling, rotation, and translation. To demonstrate the effectiveness of our approach, extensive experiments are conducted on DOTA [13] and DIOR-R [14], both datasets that contain rotated and densely-placed objects.\nThe main contributions of this work are as follows:\n\u2022 To the best of our knowledge, we first investigate the bounding box transformation to improve remote sensing object detection. With our investigation, we provide a useful recipe for adding noise to bounding boxes.\n\u2022 It is worth noting that our method is easy to plug in and merely increases the computational complexity compared to other state-of-the-art data augmentation strategies.\n\u2022 Extensive experimental results demonstrate that NBBOX can boost remote sensing object detection without whis- tles and bells, against implicit box label noise."}, {"title": "II. METHODOLOGY", "content": "In this section, we present a simple, but efficient data augmentation strategy namely NBBOX (Noise Injection into Bounding Box) for remote sensing object detection. Our method can be easily integrated into existing detection frame- works with minimal modifications. More precisely, our aug- mentation method can be seamlessly added to any data pre- processing pipeline without severe computational burden. The proposed approach is straightforward as depicted in Fig. 1. For data augmentation, our training scheme randomly adds noise to oriented bounding boxes with the three simplest geometric transformations: scaling, rotation, and translation."}, {"title": "A. Background", "content": "Training models under label noise is challenging and re- quires careful consideration to learn resilient and generalized models. Most research attention has been given to the clas- sification task, but recently a few have dedicated to object detection (e.g. [15], [16], [17], [18] for natural scenes and [19], [20], [21] for remote sensing imagery).\nFor natural images, [15] first analyzes the effects of var- ious label noise types on object detection and introduces a per-object co-teaching framework to address the noise label issue. [16] suggests a learning strategy of alternating between rectifying noise and training the model to handle label noise concerning category and bounding box. [17] proposes to address label noise by adopting meta-learning via exploiting a few clean labels. Previous studies consider classification and localization simultaneously, whereas [18] employs multiple instance learning to deal solely with inaccurate bounding boxes. We believe that it is reasonable and practical to pursue this research direction as imprecise bounding box labels are more prevalent than noisy category labels in practice.\nFor remote sensing images, a few work has been proposed to address noisy labels in object detection, inspired by research for natural scenes. As seen in Fig. 2, there is a discrepancy between the actual annotation and the minimum enclosing box, which might cause difficulties in model training due to inaccurate and low-consistent bounding boxes. [19] designs two kinds of loss functions to mitigate mislabels for both classification and localization. On the other hand, [20], [21] focus on correcting incomplete annotations such as missing bounding boxes due to misalignment of GPS sensors. The main goal of our work is to resolve localization errors inherent in the existing bounding box labels. In other words, we consider a generic object detection scenario, where missing bounding boxes or incorrect class labels of the ground truth are uncommon. Instead, we assume that the bounding box labels may not be optimal due to the annotators' mistakes or dilemma situations such as how to rotate a bounding box for a circular object on overhead imagery during the annotation process. In particular, we study a data augmentation strategy based on bounding box transformation for more robust localization."}, {"title": "B. Overview of the proposed approach", "content": "The detail of NBBOX is presented in Algorithm 1. In our algorithm, there exist two types of input: I is the input image and L = {(Bi, c\u2081)}^N_{i=1} is the corresponding labels of N objects for object detection, where Bi = (xc, yc, w, h, 0) and ci correspond to the bounding box label and the one-hot encoded category label for i-th object, respectively. (xc, yc) is the center point, while w and h represent the width and height of the bounding box. O denotes the rotated angle of the box.\nMoreover, there are three kinds of hyper-parameters to tune. s = (smin, Smax), r = (rmin, rmax), and t = (tmin, tmax) determine the ranges of translation, scaling, and rotation, respectively, where \u2200k, sk \u2208 R \u2227 rk \u2208 R \u2227 tk \u2208 Z. Here, Z is the set of all integers whereas R is for all real numbers. randint(a, c) ~ Pint selects and returns any specific integer c such that a < b < c. randfloat (a, c) ~ Pfloat generates any real number value within the closed interval [a, c). We simply choose the uniform distributions for all random functions used in our algorithm. In other words, $P(b; a, c) = \\frac{1}{c-a+1}$ and $P(b; a, c) = \\frac{1}{c-a}$ are used for the probability mass (discrete) function Pint and the probability density (continuous) function Pfloat, respectively. One might question why t has a domain of integer values while s and r have domains of real numbers. This is because translation operations are conducted on a pixel basis, while scaling and rotation factors can be the real number."}, {"title": "C. Advanced options for translation and scaling", "content": "As demonstrated in Algorithm 1, the bounding box may be translated by different values along both the x-axis and y-axis. Besides, the aspect ratio may not be maintained for scaling operations. This phenomenon would not be recommended for training on certain data scenarios. For instance, it would not be desirable to modify the aspect ratio of bounding boxes in an extreme situation where most objects are expected to have a square size. Hence, we alleviate this by providing additional parameters bools \u2208 {True, False} and boolt \u2208 {True, False} in our NBBOX algorithm. Here, bools and boolt are boolean values to be set by users. One can effortlessly modify our algorithm with bools and boolt as follows.\nFor scaling, line 4 should be replaced with:\n$\\alpha, \\beta \\leftarrow \\text{randfloat}(S_{\\text{min}}, S_{\\text{max}}), \\text{randfloat}(S_{\\text{min}}, S_{\\text{max}})$\nSw \u2190 a if bools is True, otherwise a;\nSh \u2190 a if bools is True, otherwise \u03b2,\nwhere bools represents whether the bounding box is propor- tionally resized to keep the width and height ratio unchanged. Likewise, for translation, line 10 should be replaced with:\n$\\alpha, \\beta \\leftarrow \\text{randint}(t_{\\text{min}}, t_{\\text{max}}), \\text{randint}(t_{\\text{min}}, t_{\\text{max}})$\ntx\u2190a if boolt is True, otherwise a;\nty a if boolt is True, otherwise \u03b2,\nwhere boolt denotes whether the center point is shifted equally along both the x and y directions."}, {"title": "III. EXPERIMENTS", "content": "In this section, we show our data augmentation strategy NBBOX improves the performance of remote sensing object detection. We first briefly introduce the experimental settings and implementation details. We then provide test results and analysis of experiments with the proposed method."}, {"title": "A. Experimental setup", "content": "Detection models. Traditionally, deep learning-based object detection models are typically separated into one-stage and two-stage methods. Nowadays, the distinction between anchor- based and anchor-free methodologies tends to be more empha- sized when classifying detector models. In this work, we adopt Faster R-CNN [22], RetinaNet [23], and FCOS [24] which are representative models in anchor-based two-stage, anchor- based one-stage, and anchor-free one-stage detection studies, respectively. For all models, Imagenet-pretrained ResNet-50 [25] is used for feature extraction, and FPN-1x [26] is added as a neck to improve multi-scale invariance.\nTraining details. We implement our hypothesis using Py- Torch and MMRotate [27]. All the models are trained for 25 epochs. For a fair comparison with other approaches, all the experiments are resumed with weights pre-trained for five epochs without any augmentation per each {detector, data} combination. SGD is used for optimization with momentum of weight 0.9, weight decay 1e-4, learning rate 1e-3, and batch size 8. Learning rate warm-up is used over the first 500 iterations, where the learning rate linearly increases from zero to 33% of 5e-3. L2-norm gradient clipping is utilized with a maximum norm of 35. We apply the following pre- processing operations for training: \u2018RResize', \u2018RRandomFlip', 'Normalize', and 'Pad', while for testing, 'MultiScaleFlipAug' is also used. For the sake of brevity and due to time limitations, we only conduct the parameter search on DIOR-R and scale the best configurations to DOTA in this work.\nEvaluation metric. To objectively verify performance, all experiments are repeated five times, and the evaluation results are reported as \u2018mean \u00b1 standard deviation'. For each experi- ment, we use mAP (mean Average Precision) at IoU(y, y)=0.5 (i.e. mAP@50) to measure the accuracy of the detected bound- ing boxes (y) against the ground truth boxes (y) following [2]. This metric evaluates both the precision and recall of the model across different classes as:\n$MAP = \\frac{1}{C}\\sum_{i=1}^{C} AP_i$,\nwhere C is the number of categories and AP\u00bf is the average precision for category i. See [28] for detailed calculation."}, {"title": "B. Datasets", "content": "This work conducts experiments on two widely recognized datasets for remote sensing object detection: DOTA (v1) [13] and DIOR-R [14]. Both have diverse real-world categories, high resolution, and detailed annotations. DOTA includes 15 object categories such as 'car', 'ship', and 'harbor'. It contains 2,806 aerial images of varying resolutions with 188,282 ori- ented bounding box labels. Compared to DOTA, DIOR-R has 20 object categories including 192,512 instances over 23,463 images for rotated object detection. We conduct experiments"}, {"title": "C. Impact of scaling, rotation, and translation in NBBOX", "content": "This subsection studies the model performance when ex- posed to varying levels of scaling, rotation, and translation of bounding boxes. For this, we consider combinations of the following parameters: Smin \u2208 {0.5, 0.8, 0.9, 0.95, 0.99}, Smax \u2208 {1, 1.01, 1.05, 1.1, 1.2, 1.5}, bools \u2208 {True, False}, {rmin, rmax} \u2208 {\u00b10.01, \u00b10.1, \u00b10.5, \u00b11, \u00b12, \u00b13, \u00b14, \u00b15, \u00b110}, and {tmin, tmax} \u2208 {\u00b11, 2, 3, 4, 5, \u00b110, \u00b120, \u00b130}, boolt \u2208 {True, False}. As indicated in Fig. 3 and Table I, following hyper-parameters are recommended:\n\u2022 Smin \u2208 {0.95, 1} \u2227 Smax \u2208 {1, 1.01};\n\u2022 {rmin, rmax} = {\u00b10.01} & tmin = \u00b11, tmax \u00b12.\nFor rotation, we need to use fairly small parameter values, presumably as rotating bounding boxes can easily harm fore- ground content in aerial imagery, especially for tiny objects.\nIsotropic scaling and translation. When scaling with pre- serving aspect ratio (i.e. bool=True), all models show a slight increase in mAP compared to when it is not (i.e. bool=False) as shown in Table II. Besides, in the case of translation noise, the models perform slightly better when isotropic translation is applied (i.e. bool\u0165=True) compared to when it is not (i.e. boolt=False) as indicated in Table II. Thus, experimental results on DIOR-R in Table II suggest that isotropic scaling and translation may be important. However, it is vital to note that the performance observed in this experiment might not be consistently replicable across different datasets."}, {"title": "D. Comparison with state-of-the-art methods", "content": "In this subsection, we compare our method with easy-to- get data augmentation methods based on image manipulation (image/patch erasing and mix) as illustrated in Table IV. We also compare ours with RandRotate and RandShift to show the difference (Our method differs as it only transforms bounding boxes while RandRotate and RandShift rotate or shift both the image and the bounding boxes). For a DOTA dataset, we follow the best parameters across three detectors on DIOR-R by hard voting. Additionally, we train and test only on Faster R-CNN, the best architecture on DIOR-R in our previous experiments. Table IV shows that our NBBOX achieves the best performance among the state-of-the-art aug- mentation methods. Though our method, when applied solely to DOTA, achieved competitive performance, ranking second behind RandRotate, its performance significantly improves when integrated with RandRotate. Such is true of RetinaNet on DIOR-R with MosaicMix. Besides, our method is faster than other image manipulation methods. Table V indicates the time required per each training epoch on Faster R-CNN for each augmentation. (e.g. for one epoch, ours vs MosaicMix: 9.63min vs 21.24min on DIOR-R). Due to the page limit, detailed settings are described in the supplementary material."}, {"title": "IV. CONCLUSION", "content": "In this letter, we have proposed a novel data augmentation method named NBBOX for remote sensing object detection. This work thoroughly investigates the effect of bounding box transformation with scaling, rotation, and translation for remote sensing object detection. Extensive experiments on DOTA and DIOR-R demonstrate that noise injection into bounding boxes surpasses existing state-of-the-art augmenta- tion strategies based on image manipulation. Besides, NBBOX is easy to integrate with modern deep-learning frameworks and more time-efficient than other augmentation methods. In the future, we will extend our method with self-supervised learn- ing such as multiple instance learning for further improvement."}]}