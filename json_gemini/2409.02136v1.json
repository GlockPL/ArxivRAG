{"title": "Large Language Models versus Classical Machine Learning: Performance in COVID-19 Mortality Prediction Using High-Dimensional Tabular Data", "authors": ["Mohammadreza Ghaffarzadeh-Esfahani", "Mahdi Ghaffarzadeh-Esfahani", "Arian Salahi-Niri", "Hossein Toreyhi", "Zahra Atf", "Amirali Mohsenzadeh-Kermani", "Mahshad Sarikhani", "Zohreh Tajabadi", "Fatemeh Shojaeian", "Mohammad Hassan Bagheri", "Aydin Feyzi", "Mohammadamin Tarighatpayma", "Narges Gazmeh", "Fateme Heydari", "Hossein Afshar", "Amirreza Allahgholipour", "Farid Alimardani", "Ameneh Salehi", "Naghmeh Asadimanesh", "Mohammad Amin Khalafi", "Hadis Shabanipour", "Ali Moradi", "Sajjad Hossein Zadeh", "Omid Yazdani", "Romina Esbati", "Moozhan Maleki", "Danial Samiei Nasr", "Amirali Soheili", "Hossein Majlesi", "Saba Shahsavan", "Alireza Soheilipour", "Nooshin Goudarzi", "Erfan Taherifard", "Hamidreza Hatamabadi", "Jamil S. Samaan", "Thomas Savage", "Ankit Sakhuja", "Ali Soroush", "Girish Nadkarni", "Ilad Alavi Darazam", "Mohamad Amin Pourhoseingholi", "Seyed Amir Ahmad Safavi-Naini"], "abstract": "This study aimed to evaluate and compare the performance of classical machine learning models (CMLs) and large language models (LLMs) in predicting mortality associated with COVID-19 by utilizing a high-dimensional tabular dataset.\nWe analyzed data from 9,134 COVID-19 patients collected across four hospitals. Seven CML models, including XGBoost and random forest (RF), were trained and evaluated. The structured data was converted into text for zero-shot classification by eight LLMs, including GPT-4 and Mistral-7b. Additionally, Mistral-7b was fine-tuned using the QLoRA approach to enhance its predictive capabilities.\nAmong the CML models, XGBoost and RF achieved the highest accuracy, with F1 scores of 0.87 for internal validation and 0.83 for external validation. In the LLM category, GPT-4 was the top performer with an F1 score of 0.43. Fine-tuning Mistral-7b significantly improved its recall from 1% to 79%, resulting in an F1 score of 0.74, which was stable during external validation.\nWhile LLMs show moderate performance in zero-shot classification, fine-tuning can significantly enhance their effectiveness, potentially aligning them closer to CML models. However, CMLs still outperform LLMs in high-dimensional tabular data tasks.", "sections": [{"title": "Introduction", "content": "The rapid advancement of large language models (LLMs) has revolutionized their practical applications across various domains, including medicine. These sophisticated models, trained on vast datasets, excel in a wide array of natural language processing tasks, demonstrating remarkable adaptability in assimilating specialized information from diverse medical fields (1). While primarily designed for next-word prediction, LLMs have emerged as powerful, evidence-based knowledge assistants for healthcare providers, offering valuable insights and support in clinical decision-making processes (2\u20134). While their main training centers on predicting the next word, LLMs can act as evidence-based knowledge helpers for healthcare providers, offering valuable insights and assistance (5).\nIn medical and clinical practice, machine learning models, particularly classical machine learning (CML) models, have gained significant traction in predicting patient outcomes, prognoses, and mortality rates. These models typically employ supervised and unsupervised learning methods, which primarily utilize structured data (6). However, clinical datasets often present a complex interplay of structured and unstructured information, with clinical notes serving as prime examples of the latter. Traditionally, patient information management via machine learning has followed a two-step approach: transforming unstructured textual data into a structured format, followed by training CML models on these structured datasets. This process, however, often leads to potential information loss and introduces complexities in model deployment, hindering practical application in clinical settings (7).\nWhile the efficacy of LLMs in handling unstructured text is well documented (8), their performance in handling structured data and their comparative effectiveness against CML models"}, {"title": "Methods", "content": "The study was approved by the Institutional Review Board (IRB) of Shahid Beheshti University of Medical Sciences (IR.SBMU.RIGLD.REC.004 and IR.SBMU.RIGLD.REC.1399.058). The IRB exempted this study from informed consent. Data were pseudonymized before analysis; patients' confidentiality and data security were prioritized at all levels. The study was completed under the Helsinki Declaration (2013) guidelines. During the generation of LLM responses, using the OpenAI API and Poe Web interface, we opted out of training on OpenAI and used no training-use models in Poe to maintain the data safety of patient information.\nThe objective of this research is to evaluate the efficacy of CMLs in comparison to LLMs, utilizing a dataset characterized by high-dimensional tabular data. We employed a previously compiled dataset and focused our experimental efforts on the task of classifying COVID-19 mortality. As illustrated in Figure 1, the primary experiment encompasses the following:\nAssessment of the performance of seven CML models on both internal and external test sets\nThe assessment of eight LLMs and two pretrained language models on the test set.\nAssessment of a trained LLM's performance on both internal and external tests.\nAdditionally, we investigate the performance of models necessitating training (CML and trained LLM) across varying sample sizes, coupled with an elucidation of model prediction mechanisms through SHAP analysis."}, {"title": "2.3 Study Context, Data Collection, and Dataset", "content": "This study was conducted as part of the Tehran COVID-19 cohort, which included four tertiary centers with dedicated COVID-19 wards and ICUs in Tehran, Iran. The study period was from March 2020 to May 2023 and included two phases of data collection. The protocol and results of the first phase have been published previously. The four COVID-19 peaks during this period covered the alpha, beta, delta, and Omicron variants.\nAll admitted patients with a positive swab test during the first two days of admission or those with CT scans and clinical symptoms were included in the study. A medical team collected the patients' symptoms, comorbidities, habitual history, vital signs at admission, and treatment protocol through the hospital information system and reviewed the medical records. Laboratory values during the first and second days of admission were collected and sorted from the hospitals' electronic laboratory records via Python (Python Software Foundation, Python Language Reference, version 4. Available at: http://www.python.org). Patients with a negative PCR result in the first two days of admission or with one missing clinical record in the HIS were excluded.\nThe dataset included the records of 9,134 patients with COVID-19. The data were filtered to include demographic information, comorbidities, vital signs, and laboratory results collected at the time of admission (first two days)."}, {"title": "2.4. Data Preprocessing", "content": "The features in the dataset were divided into categorical and numerical categories. To address the missing values in the numerical features, we used an iterative imputer from the scikit-learn library. This method employs iterative prediction for each feature, considering the multiple imputation by"}, {"title": "2.4.3 Oversampling", "content": "To address the issue of class imbalance in our dataset, we employed the synthetic minority oversampling technique (SMOTE), a widely used method in machine learning, particularly for medical diagnosis and prediction tasks (17). By applying SMOTE, we mitigated dataset imbalances, resulting in a more robust and reliable analysis for predicting mortality. SMOTE works by creating synthetic samples for the minority class instead of simply duplicating existing samples. It selects samples from the minority class and their nearest neighbors and then generates new synthetic samples by interpolating between these samples and their neighbors. This approach not only increases the number of samples in the minority class but also introduces new data points, improving dataset diversity. In our experiments, the SMOTE technique was applied to the training set (X_train), increasing the number of samples from 6118 to 9760. Similarly, for the test set (X_test), the sample size increased from 1530--2470, and for the external set (X_ex), it increased from 1409--248."}, {"title": "2.4.4 Preparing Data for the LLM", "content": "To prepare the data for input into the LLM, we completed all the previous steps for feature selection and sampling, but normalization was not performed. As shown in Figure 1, we converted the dataset into text. We categorized the dataset features into symptoms, past medical history, age, sex, and laboratory data. For symptoms and medical history, we considered only positive data. For age, we added 'the patient's age is' before the age number. For sex, we used 'male' and 'female.' We used the normal range of laboratory data to classify the data into the normal range, higher than the normal range, and lower than the normal range. For example, if blood pressure and oxygen saturation were higher than the normal range, we used the sentence \u2018blood pressure and oxygen saturation are higher than the normal range.' We considered only laboratory data that were higher"}, {"title": "2.5 CML Predictive Performance", "content": "We employed five CML algorithms: logistic regression (LR), support vector machine (SVM), decision tree (DT), k-nearest neighbor (KNN), random forest (RF), multilayer perceptron neural network (MLP), and XGBoost. The hyperparameters were optimized via a grid search and cross-validation. The full details of training and hyperparameters are provided in Supplementary Section 1."}, {"title": "2.6 LLM Predictive Performance", "content": "We utilized open-source and proprietary LLMs to test their predictive power on clinical texts transformed from tabular data. First, we tested different prompts to determine the most efficient prompt to use, as well as the temperature (between 0.1 and 1). These prompts are listed in Supplementary Table S1. We then sent clinical text and commands, received the unstructured output, and extracted the selected outcome, which could be either \"survive\" or \"die.\" We used different sessions for each prediction, limiting the memory of the LLM to remembering previous generations.\nWe tested open-source, open-weight models of Mistral-7b, Mixtral 8 \u00d7 7 B, Llama3-8b, and Llama3-70b via the Poe Chat Interface. OpenAI models, including GPT-3.5T, GPT-4, GPT-4T, and GPT-40, were utilized via the OpenAI API. We also tested the performance of two pretrained language models, BERT (18) and ClincicalBERT (19), which are fine-tuned versions of BERT on medical text. A list of all LLMs and times of use, as well as model parameters, is available in Supplementary Table S2."}, {"title": "2.6.1 Zero-Shot Classification", "content": "Zero-shot classification is an approach in prompt engineering in which the prompt is given to the model without any training. This approach is used in transfer learning, where a model used for different purposes is employed instead of fine-tuning a new model, thereby reducing the cost of training the new model. To perform zero-shot classification, we used eight different LLMs and two LMs. We provided each patient's history as input to predict whether the patient would die or survive and then stored the results."}, {"title": "2.6.2 Fine-tuning LLM", "content": "We fine-tuned one of the open-source LLMs, Mistral-7b-Instruct-v0.2, which is a GPT-like large language model with 7 billion parameters. It is trained on a mixture of publicly available and synthetic data and can be used for natural language processing (NLP) tasks. It is also a decoder-only model that is used for text-generation tasks. Fine-tuning an LLM is usually considered time-consuming and expensive; recently, several methods have been introduced to reduce costs. We implemented the QLoRA fine-tuning approach to optimize the LLM while minimizing computational resources (20).\nThe model was configured for 4-bit loading with double quantization, utilizing an \"nf4\" quantization type and torch.bfloat16 compute data type. A 16-layer model architecture with Lora attention and targeted projection modules was employed. We used the PEFT library to create a LoraConfig object with a dropout rate of 0.1 and task type 'CAUSAL_LM'. The training pipeline, established via the transformer library, consisted of 4 epochs with a per-device batch size of 1 and gradient accumulation steps of 4. We utilized the \"paged_adamw_32bit\" optimizer with a learning rate of 2e-4 and a weight decay of 0.001. Mixed-precision training was conducted via fp16, with a maximum gradient norm of 0.3 and a warm-up ratio of 0.03. A cosine learning rate scheduler was employed, and training progress was logged every 25 steps and reported to TensorBoard. This methodology, which combines QLoRA with the bits and bytes library, enables efficient enhancement of our language model while significantly reducing resource requirements, demonstrating superior performance across various instruction datasets and model scales."}, {"title": "2.6 CML and LLM Performance on Different Sample Sizes", "content": "To investigate the influence of training sample sizes on model performance, we conducted a series of experiments using varying sample sizes: 20, 100, 200, 400, 1000, and 2476. Multiple models were trained using these sample sizes, and their performance was evaluated on the basis of the F1 score and accuracy metrics via an internal test set. The objective of this exploration was to gain valuable insights into the correlation between the volume of training data and the accuracy of predictive models."}, {"title": "2.7 Evaluation and analysis", "content": "The accuracy of the outputs was assessed by comparing them against a ground truth that categorized outcomes as either mortality or survival. Outputs from the LLM were similarly classified. If an LLM initially produced an undefined result, the prompt was repeatedly presented up to five times to elicit a defined prediction; these instances are documented in Supplementary Table S1. We evaluated the models' performance via five critical metrics: specificity, recall, accuracy, precision, and F1 score. To optimize our models, we employed a grid search strategy with accuracy as the primary criterion. We further employed cross-validation to ensure the robustness and reliability of our model selection process. Additionally, the area under the receiver operating characteristic curve (AUC) was used to illustrate the predictive capacity of each model."}, {"title": "2.9 Explainability", "content": "In our study, we employed SHAP (SHapley Additive exPlanations) values to examine both the total (global) and individual (granular) impacts of features on model predictions. We normalized the numerical data via a standard scaler and adopted a model-agnostic methodology. This approach involved employing XGBoost as the explainer model, which was chosen for its robust performance, as demonstrated in prior research and our own findings. SHAP values provide a clear, quantitative assessment of how each feature influences individual predictions, enhancing transparency in the model's decision-making process.\nFor our analysis, we used the test set for each model, generated SHAP values for every prediction, and computed the mean and standard deviation of the absolute SHAP scores. We then converted SHAP scores from a range of 0 to 1 into \"global impact percentages\" by dividing each feature's score by the total score of all features and multiplying by 100. We calculated the average impact percentages for both CMLs and LLMs by first averaging the SHAP scores and then determining the impact percentages. To compute the standard deviation of the impact percentages, we adjusted the average standard deviation of CML/LLM via a multiplication factor derived from the ratio of the impact score to the SHAP mean. The global impact percentage represents the proportion of each feature's impact on the predicted class across the entire dataset. A violin plot visually represents the variability of each input feature's effect on the output."}, {"title": "Results", "content": "Our study initially included a dataset of 9,057 patients, with a mean age of 58.40 \u00b1 19.81 years and a male-female ratio of 1.19. The overall mortality rate in this group was 25.11% (N=1818). We utilized an internal validation test set and an external validation set comprising 2,470 and 2,248 participants, respectively, each with a mortality rate of 50%. Additionally, the validation set for"}, {"title": "3.1 Classic Machine Learning Predictive Performance", "content": "XGBoost and RF were the top-performing models in terms of accuracy, achieving scores of 86.28% and 86.52%, respectively, as detailed in Table 2. These models also excelled in precision, recall, specificity, and F1 scores, all surpassing 85%. The MLP also delivered an acceptable performance, with an accuracy of 75.87%. When the models were applied to the external validation set, a slight decline in the AUC of 2\u20135% was observed. Supplementary Figures S1 and S2 depict the performance of the CMLs on the internal validation test set and the external validation set, respectively. SVM, KNN, and DT showed consistent performance across both validation sets, confirming their reliability in generalizing to unseen data."}, {"title": "3.2 LLM: Zero-shot classification and fine-tuned Mistral-7b", "content": "Table 2. The zero-shot classification results showed variability among the models, with GPT-4 outperforming the other models by achieving an accuracy of 0.62 and an F1 score of 0.43 and recording the highest recall at 0.28 among the LLMs. Generally, LLMs exhibited low recall rates, predominantly classifying predictions as \"mortality.\" The open-source models, including Llama-3-70B, Llama-3-8B, Mistral-7b-Instruct, and Mistral-8x7b-Instruct-v0.1, had F1 scores ranging from 0.03 (Mistral 7b) to 0.15 (L Llama3-8B and Llama3--70b). Notably, the gpt-\u00b0model showed limited effectiveness, with an F1 score of 0.01, indicating a challenge in distinguishing between true positives and true negatives. The pretrained language models \u2013 BERT and ClinicalBERT also labeled all outcomes as dies, failing to provide predictive power.\nFine-tuning Mistral-7b significantly improved its performance, increasing the F1 score from 0.03 to 0.74 in the internal test set and to 0.69 in the external test set. This fine-tuned version also demonstrated a high recall rate of 78.98%, a substantial increase from 1% in zero-shot classification, showing its ability to accurately identify a greater proportion of actual survival instances. This consistency between internal and external validations highlights the generalizability of the fine-tuned Mistral-7b in mortality prediction. The detailed results are presented in Supplementary Figure S4. Also, Figure 2 sets out the ROC curves and AUC scores for internal and external validation of COVID-19 mortality prediction models."}, {"title": "3.3 Comparing models on different training sample sizes", "content": "To evaluate the impact of training sample size on model efficacy, experiments were conducted across various sample sizes. Figure 3 shows that the performance of all CMLs increased as the size of the training set increased. XGBoost demonstrated the strongest performance across all categories: small (100 samples), medium (400--1000 samples), and full training set sizes (2476 samples). Notably, the MLP neural network and SVM exhibited the most significant performance improvements, with accuracies increasing from 55% with 20 training samples to 73% and 77%, respectively.\nIn contrast, while the zero-shot performance of GPT-4 reached an F1 score of 0.43, CMLs still surpassed both zero-shot classification and fine-tuned LLMs in predicting COVID-19 mortality. During the fine-tuning of Mistral-7b, notable performance degradation occurred in scenarios with small training sizes, leading to a loss of broader model understanding, an effect termed \"negative transfer.\""}, {"title": "3.4 Explainability: Impact of Features on Prediction", "content": "As shown in Supplementary Figure S5, while the global impact of features among CMLs exhibits similar patterns, with many of the top 10 impactful features being consistent, the granular impact differs significantly. For example, in the context of O2 saturation levels in patients, XGBoost, RF, DT, and MCP consider both high (increasing mortality risk) and low (increasing survival chance) levels to be significant, whereas KNN and LR focus only on low saturation levels. According to Figure 4.a, the most influential features are age (11.18%) and O2 saturation (9.89%), followed by LOC (4.83%), lymphocyte count (4.79%), dyspnea (3.76%), and sex (3.68%).\nConversely, the influence of features in LLMs, particularly in lower-performing models such as Mistralb-7b and GPT4o, appears less coherent, as illustrated in Supplementary Figure S6. This inconsistency contributes to noise in the average feature impact among LLMs (Figure 4.d). Nonetheless, age (6.58%) and O2 saturation (5.51%) remained the most significant features, with a series of laboratory tests, including neutrophil count, PT, ALP, MCV, K, Na, ESR, and Cr, revealing impacts in the 4%--5% range.\nWhen comparing the top performers among CMLs and LLMs\u2014XGBoost and GPT4\u2014the patterns of global (Fig. 4.b and Fig. 4.e) and granular (Fig. 4.c and Fig. 4.f) impacts diverge, with XGBoost displaying more specific impacts and GPT4 showing broader ranges of impact."}, {"title": "Discussion", "content": "Our study reveals a notable performance gap between CML models and LLMs in predicting patient mortality via tabular data. RF and XGBoost emerged as the top CML performers, achieving over 80% accuracy and an F1 score of 0.86. In contrast, the best-performing LLM, GPT-4, achieved 62% accuracy and an F1 score of 0.43 in zero-shot classification. This disparity highlights the challenges LLMs face when dealing with purely tabular data. Notably, increasing our sample size from 5,000 patients in our previous study to 9,000 patients in this study significantly improved the performance of CML models. The AUC of RF improved from 0.82 to 0.94, underscoring the importance of large and diverse datasets in realizing the full potential of CMLs in medical tasks.\nLLM performance heavily relies on the knowledge embedded within model weights, the complexity of input data, and the table-to-text transformation technique. Our approach, which uses a simple prompt and transformation to resonate with current clinical use, achieved results comparable to those of similar studies, with F1 scores of 0.50\u20130.60 across different medical tasks using LLMs such as the GPT-4 or GPT-3.5 (10,11,21). However, in line with many previous studies, we found that CMLs can outperform this zero-shot performance with even fewer than 100 training samples (10,21).\nGiven the performance gap between CMLs and LLMs, researchers have explored two main approaches for improving LLM performance: pipeline improvements and fine-tuning. Previous studies have shown that LLMs can close the gap in CML performance via pipeline improvements such as prompt engineering techniques (XAI4LLM), few-shot approaches (XAI4LLM, EHR-CoAgent, TabLLM), multiple runs of LLM to double-check results (EHR-CoAgent), the addition of a tree-based explainer alongside the LLM (XAILLM), or novel LLM-based text-to-table"}, {"title": "Conclusion", "content": "The efficacy of LLMs versus CML approaches in medical tasks appears to be contingent upon data dimensionality and data availability. In low-dimensional scenarios with limited samples, LLM-based methodologies may offer superior performance; however, as dimensionality increases and diverse sample sizes become available, CML techniques tend to outperform the zero-shot capabilities of LLMs. Notably, fine-tuning LLMs can substantially enhance their pattern recognition and logical processing, potentially achieving performance levels comparable to those of CMLs. The potential of LLMs to process both structured and unstructured data may outweigh marginally lower performance metrics than CMLs do. Ultimately, the choice between LLMs and CMLs should be guided by careful consideration of task complexity, data characteristics, and clinical context demands, with further research warranted to elucidate the precise conditions under which each methodology excels."}, {"title": "Conflict of interest declaration", "content": "All the authors declare that they have no financial or nonfinancial conflicts of interest related to this work."}, {"title": "Abbreviations", "content": "LLM: Large Language Model\nCML: classical machine learning model\nLR: Logistic regression\nSVM: Support vector machine\nDT: decision tree\nKNN: k-nearest neighbor\nRF: random forest\nXGBoost: Extreme Gradient Boosting\nMLP: multilayer perceptron\nZero-shot classification: ZSC\nLASSO: least absolute shrinkage and selection operator\nSMOTE: synthetic minority oversampling technique\nQLORA: quantized low-lanking adaptation\nMICE: Multiple Imputation by Chained Equations\nReLU: rectified linear unit"}]}