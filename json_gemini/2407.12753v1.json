{"title": "LookupViT: Compressing visual information to a limited number of tokens", "authors": ["Rajat Koner", "Gagan Jain", "Prateek Jain", "Volker Tresp", "Sujoy Paul"], "abstract": "Vision Transformers (ViT) have emerged as the de-facto choice for numerous industry grade vision solutions. But their inference cost can be prohibitive for many settings, as they compute self-attention in each layer which suffers from quadratic computational complexity in the number of tokens. On the other hand, spatial information in images and spatio-temporal information in videos is usually sparse and redundant. In this work, we introduce Lookup ViT, that aims to exploit this information sparsity to reduce ViT inference cost. Lookup ViT provides a novel general purpose vision transformer block that operates by compressing information from higher resolution tokens to a fixed number of tokens. These few compressed tokens undergo meticulous processing, while the higher-resolution tokens are passed through computationally cheaper layers. Information sharing between these two token sets is enabled through a bidirectional cross-attention mechanism. The approach offers multiple advantages (a) easy to implement on standard ML accelerators (GPUs/TPUs) via standard high-level operators, (b) applicable to standard ViT and its variants, thus generalizes to various tasks, (c) can handle different tokenization and attention approaches. Lookup ViT also offers flexibility for the compressed tokens, enabling performance-computation trade-offs in a single trained model. We show Lookup ViT's effectiveness on multiple domains - (a) for image-classification (ImageNet-1K and ImageNet-21K), (b) video classification (Kinetics400 and Something-Something V2), (c) image captioning (COCO-Captions) with a frozen encoder. Lookup ViT provides 2\u00d7 reduction in FLOPs while upholding or improving accuracy across these domains. In addition, Lookup ViT also demonstrates out-of-the-box robustness and generalization on image classification (ImageNet-C,R,A,O), improving by up to 4% over ViT.", "sections": [{"title": "1 Introduction", "content": "Images and videos, the cornerstones of modern visual communication, possess an inherent characteristic: their information content is often sparse and exhibits significant redundancy. However, Vision Transformers (ViTs) [13], despite their dominance across multiple vision tasks, do not exploit this redundancy and attend to every token in a homogenized way. This leads to quadratic computational complexity with respect to image size, hindering its applicability in real-time situations. To bridge this gap, there is a pressing need to efficiently compress visual information into a smaller, more computationally manageable set of tokens. Such representations would unlock the potential of ViTs for resource-constrained scenarios while preserving their flexibility and performance advantages which led to their widespread adoption in the field of computer vision.\nSeveral architectures aim to address the computational burden of ViTs by thoughtfully reducing the number of tokens. Token pruning methods retain a subset of tokens [15,32,39], while token pooling techniques combine similar tokens for a more compact representation [5,29]. These mechanisms rely on heuristics derived from attention scores or feature similarities and might require additional task-specific adjustments. While these techniques offer valuable benefits, they may necessitate further fine-tuning based on the application. In contrast, we propose a novel Lookup ViT block to replace the vanilla ViT block, which intrinsically acts as a compression module. This design eliminates the need for post-processing or extensive fine-tuning. Furthermore, our method preserves the general structure of the ViT architecture, thus allowing further optimization and adaptations using existing approaches like token pruning or merging.\nCompression modules like TokenLearner [31] and Perceiver [21] have also been explored in the literature. TokenLearner utilizes vanilla ViT blocks for a significant portion of the network depth, compressing a large number of tokens to a smaller set (e.g., 8 or 16) at later stages. This reliance on ViT blocks incurs a substantial computation and heavily limits the the full utilization of compression module within the network. Perceiver, on the other hand, devises an asymmetric information flow directly from image pixels to a small set of latent representations iteratively throughout the network. Moreover, for these network architectures, it is non-trivial to extract multiple models with the same parameters, to exhibit a compute-performance trade-off between extracted models. Lookup ViT distinguishes itself by offering a scalable, computationally efficient block that can be seamlessly repeated like standard ViT blocks. Its bidirectional cross-attention mechanism facilitates a richer exchange of information between the compressed and original tokens, enhancing representational power.\nIn this paper, we corroborate that for innately redundant modalities like vision, condensing relevant spatial (and temporal) information from original tokens to a much smaller set can still sustain performance while significantly lowering the computational requirements, by maintaining an effective exchange of information between the two token sets. Figure 1b indicates Lookup ViT's ability to scale to large image sizes efficiently, by processing only relevant information, compared to vanilla ViT blocks, which scales quadratically in the number of original image tokens. We denote the smaller compressed set of tokens as compressed tokens, which \"look\" at the larger original set of tokens, which we call lookup tokens. The information exchange between these tokens happens in every LookupViT block in three key steps, as shown in Figure 2 - (i) cross attention to transfer relevant information from the lookup tokens to the compressed tokens (shown in Figure 1a), (ii) self-attention amongst the compressed tokens, and (iii) information transfer from the compressed tokens to the lookup tokens using shared attention weights, computed in the first step. While the compressed tokens communicate through self-attention, the lookup tokens communicate among themselves only via the compressed tokens. This technique avoids the quadratic scaling, while ensuring that the lookup latent representations get richer along the layers.\nLookup ViT's intrinsic design naturally supports flexibility in terms of token compression and variable image or token size. By adjusting the down-sampling ratio between compressed and lookup tokens, the cost- performance trade-off can be tailored to match specific application require- ments. This multi-resolution nature allows for extraction of compute-efficient high-performing models during infer- ence, with the same parameter space. To validate Lookup ViT's efficacy, we show results on multiple benchmarks like image and video classification, and image captioning. Notably, due to the information bottleneck, LookupViT also shows out-of-the-box robustness to image corruptions. The key contributions of this work are\nEfficient Token Compression: Lookup ViT introduces a novel Multi-Head Bidirectional Cross-attention (MHBC) module that enables effective informa- tion flow with significant computational savings."}, {"title": "2 Related Works", "content": "Since the introduction of the Vision Transformer (ViT), a multitude of works have endeavored to improve its efficiency and scalability.\nMulti-scale and Hierarchical Features: Early studies such as [14, 27, 36] utilized non-overlapping patches with multi-scale or hierarchical features, achiev- ing notable success in both image and video domains [1]. Concurrently, [30] proposed hierarchical designs for efficient training and inference across these modalities. These approaches pushed accuracy boundaries, but often at the ex- pense of added architectural complexity. For instance, MViTv2 [25] decomposes relative position embedding and residual pooling, while CSWin [12] integrates cross-shaped windows within a hierarchical framework. This creates a trade-off between enhanced accuracy and the potential loss of ViT's inherent simplicity and scalability. LookupViT's compressed and lookup tokens has some parallels with the convolution-based OctConv's [8] low and high frequency features. How- ever Lookup ViT restricts heavy processing to compressed tokens, and enjoys scalability of Transformers.\nToken Merging and Sampling: Another prominent research direction involves token merging and pruning. [5,15,32,39] aim to reduce redundant tokens through merging, sampling, or pruning. For example, [5] uses similarity to groups and merge tokens, while [15] employs adaptable token sampling. While valuable, these techniques often introduce heuristics and generally function as post-processing steps. Furthermore, they can face challenges when extending to modalities beyond images, such as videos or multi-modal data. In contrast, Lookup ViT emphasizes intrinsic compression through its core architecture, replacing the ViT block. Importantly, Lookup ViT remains harmonious with the potential application of token merging or sampling for further optimization.\nToken compression: Instead of merging tokens, [29] learns a smaller number of M patches from the original N patches in ViT using a learnable weight matrix. Similarly, TokenLearner [31] compressed all ViT tokens into a smaller set of 8-16 tokens and performing self-attention within this reduced set, but after a certain number of vanilla ViT layers. Perceiver [21] proposes learning a small set of tokens directly from the pixel space using iterative unidirectional cross-attention. These two methods are most closely related to our work. However, TokenLearner's compression achieves optimal performance only when processing at least 50-75% of the network with ViT blocks, leading to no reduction in computation for a significant number of layers. In contrast, Lookup ViT can be trained entirely"}, {"title": "3 Lookup ViT Methodology", "content": "In this section, we discuss the Lookup ViT framework in detail, starting with a high- level architectural discussion, and then focusing on specific design choices. We also discuss its applicability to downstream tasks and Multi-Resolution flexibility. We conclude this section with an analysis of the improved computational complexity.\nAn overview of the Lookup ViT architecture is presented in Figure 3. Similar to the ViT architecture, it comprises of a stack of LookupViT blocks. First, an input RGB image (or video) is divided into non-overlapping patches. These patches are then passed through a convolutional layer to generate feature embeddings. Positional embeddings are then added to construct the input tokens a process identical to the standard ViT architecture [13]. Unlike vanilla ViT, the core idea here is - to compress visual information into a smaller number of tokens, focusing heavy computation exclusively on those tokens.\nA fixed number of tokens \\(M (\\ll N)\\), which we name as the compressed tokens are sampled from the input tokens, using bilinear interpolation. Computationally intensive processing is performed on the compressed tokens, analogous to a standard ViT block, while exchanging information with the original tokens through asynchronous Multi-Head Bidirectional Cross-Attention (MHBC). The process unfolds as follows - (1) Information Gathering: Compressed tokens use cross-attention to \"look\" at the original tokens (termed lookup tokens) and gather relevant information. (2) Representation Refinement: Compressed tokens exchange information amongst themselves, updating their representations. (3) Global Context Infusion: The lookup tokens utilize the processed, information- rich compressed tokens, to update their own representations, reusing the attention weights calculated during Information Gathering for efficiency.\nDuring this entire process, the lookup tokens are forced to gather information only by interacting with the compressed tokens, thus reducing computational complexity. Additionally, the lookup tokens pass through a MLP block with a smaller projection dimension \\( (D/q) \\) compared to the vanilla model projection \\((pD)\\), which is applied on the compressed tokens, where D represents the trans- former embedding dimension \\(((p, q) = (4, 2))\\). This optimization further reduces computations. The LookupViT block's ability to achieve performance compara- ble to the baseline, despite this substantial MLP bottleneck, demonstrates the effectiveness of the information exchange between compressed and lookup tokens."}, {"title": "3.1 Overall Architecture", "content": "An overview of the Lookup ViT architecture is presented in Figure 3. Similar to the ViT architecture, it comprises of a stack of LookupViT blocks. First, an input"}, {"title": "3.2 Input Tokenization", "content": "The construction of lookup token embeddings similar to standard ViT [13] tokenization strategy. Given an input image \\(X \\in R^{h\\times w\\times c}\\), it is passed through a convolutional layer to obtain lookup features \\(F_{l} \\in R^{h_{1}\\times w_{1} \\times D}\\). A\u02bblearnable lookup positional embedding \\(F_{l,pos} \\in R^{h_{1}\\times w_{1} \\times D}\\) is added to this feature map. These tokens are then significantly downsampled to a fixed shape - \\((h_{p}, w_{p})\\), which constitute the compressed tokens. This can be summarized as below -\n\\(F_{p} \\leftarrow T(F_{l}, (h_{p}, w_{p}))\\) (1)\n\\(F_{l} \\leftarrow F_{l} + F_{l,pos}\\) (2)\n\\(F_{p} \\leftarrow F_{p} + F_{p,pos}\\)\n\\(F_{l,pos} \\leftarrow T(F_{l,pos}, (h_{p}, w_{p}))\\)\nThe operator \\(T(x, s)\\) bilinearly resizes x to shape s. The lookup and compressed token grids have sizes \\((h_{l}, \\omega_{l})\\) and \\((h_{p}, w_{p})\\), and D is the embedding dimension. These feature maps \\(F_{l}\\) and \\(F_{l}\\) are then spatially flattened to \\(z^{l}\\) and \\(z^{l}\\):\n\\(z^{p} = [F_{p}(0,0),..., F_{p}(h_{p}-1,w_{p}-1)]\\)\\(z^{p}\\in R^{h_{p}.w_{p}\\times D}\\) (3)\n\\(z^{l} = [F_{l}(0,0),..., F_{l}(h_{l} -1,w_{l} -1)]\\)\\(z^{l}\\in R^{h_{l}.w_{l}\\times D}\\) (4)"}, {"title": "3.3 Lookup ViT Block", "content": "The \\(k^{th}\\) LookupViT block consumes the compressed tokens \\(z^{k-1}\\) and lookup tokens \\(z^{k-1}\\) from its previous block, facilitates information exchange between the two token sets, and passes the updated representations to the next block. The novel architectural design here is the asynchronous Multi-Head Bidirectional Cross-attention (MHBC). Intuitively, in the first layer, the lookup tokens main- tain a richer image representation than the compressed tokens. However, after multiple passes through the Lookup ViT block, the compressed tokens accumulate relevant compressed image information, thus making them suitable for down- stream tasks. This happens through iterative communication between the lookup and compressed tokens in every LookupViT block (Algorithm 4). This can be summarized into three key steps -\nInformation Gathering: In this step, there is a unidirectional information flow from the lookup to the compressed tokens through MHBC1\u2192p. The compressed tokens are used as query (Q) and lookup tokens as key-value (K, V). Algorithm 1 presents this part of the proposed MHBC module. Additionally, we store the attention weights A computed in this step to be re-used while sharing information in the reverse direction.\nRepresentation Refinement: After the information extraction step, the com- pressed tokens go through a vanilla ViT block (self-attention followed by MLP), as illustrated in Algorithm 3. The MLP dimension upscaling factor p is kept equal to 4, as in vanilla ViT. But this computation happens on the smaller compressed token set. This step allows internal information sharing between compressed tokens to update their representation.\nGlobal Context Infusion: The information gathering along with the ViT based processing enriches the compressed token features, as they contain a compressed global representation of the image. While the lookup tokens do not directly share information amongst themselves, they are notified about the global information"}, {"title": "3.4 Training and Token Utilization for Downstream Applications", "content": "In Lookup ViT, we maintain two sets of tokens throughout the network - N lookup tokens and M compressed tokens. For classification, we can apply the classifier to either or both token sets. Empirically, we've found that enforcing classification loss on both heads yields the best performance. We use global average pooling on the respective token sets, followed by two separate classifiers. The joint loss function is then optimized with equal weights.\nAlthough the training loss is applied independently to both token sets, we find that during inference, the classifier on the compressed tokens is sufficient. However,"}, {"title": "3.5 Computational Complexity", "content": "Let \\(C_{x}\\) denote the computation of a procedure x. Then, given the feature dimension D, number of lookup tokens N, number of compressed tokens \\(M(\\ll N)\\), MLP upscaling factor \\(p = 4\\) (on compressed tokens) and downscaling factor \\(q = 2\\) (on lookup tokens), the computational complexity of the vanilla ViT and Lookup ViT blocks can be represented as follows (neglecting smaller terms).\n\\(C_{ViT} = 2N^{2}D + 12ND^{2}\\) (5)\n\\(C_{LookupViT} = (3NM + 2M^{2})D + (4N + 15M) D^{2}\\) (6)\nNotice that we get rid of the quadratic dependence on the number of lookup tokens N and reduce the attention and linear projection computations individually. Since the number of compressed tokens \\(M(\\ll N)\\) stay constant at a user-specified value, the attention reduction factor grows quickly, enabling scalability for usage at higher resolutions. Typically, for an image resolution of 384, we use N = 576 and M = 25, which shows superior performance than the vanilla model, while simultaneously reducing FLOPs by a factor greater than 3."}, {"title": "4 Results", "content": "Implementation Details: As ViTs are prone to overfit more as compared to CNNs, they either need pre-training on large datasets like JFT [34] or augmen- tation based training frameworks like DeIT [35] or AugReg [33]. Due to the ease of implementation and adaptability to other tasks that we pursue in this work, we build our implementation on top of [33]. We implement Lookup ViT in JAX [6] within the Big Vision repository [4]. We adopt the exact training settings as in [33] (like learning rate, training epochs, etc) without performing any parameter sweeps. We also train TokenLearner [31], another state-of-the-art token compression technique, on the same repository for fair comparison, with 16 tokens for all experiments. TokenLearner1/2 denotes their compression module is applied half-way through the network, which the authors recommend.\nImage classification: We evaluate Lookup ViT on image classification while - (a) training from scratch on ImageNet-1k [11], and (b) finetuning on ImageNet-1k from a ImageNet-21k pre-trained model. The popular benchmark ImageNet- 1k has 1.28 million training images and 50,000 validation images across 1,000"}, {"title": "5 Ablations", "content": "The ablations performed in this section use a B/16 model with a compressed token size 5 \u00d7 5 trained from scratch on ImageNet-1k. This model, with all the components in place, reaches a top-1 classification accuracy of 79.1. We discuss the component-wise importance of Lookup ViT model in Table 5.\nNo Lookup Tokens: We consider constructing the compressed tokens by aggresively downsampling the image features through convolution, while having no information support through the higher resolution lookup tokens, i.e. no MHBC1\u2192p or MHBCp\u21921. The compressed tokens go through only the vanilla ViT. This leads to much lower performance, indicating that ViT, by itself, doesn't work well with very limited tokens without additional information exchange.\nNo MHBCp\u21921: From the previous step, we now add MHBCl\u2192p, which facilitates information transfer from lookup to compressed tokens, while still not updating the lookup tokens. This leads to a slight increase in performance as compared to the previous setup, as the lookup tokens are not updated at all with global information. However, this step involves construction of compressed tokens using a parameter-free resize rather than a convolutional downsampling, which enables use of same model across different compressed token sizes.\nNo Lookup/Compressed Loss: Next, we add MHBCp\u21921, a source of information exchange from the compressed to the lookup tokens, with loss computation still only on the compressed tokens. This leads to a further 8.5% increase in accuracy, thus justifying the need for the bidirectional MHBC. We also consider the case where we add loss on the lookup logits only, but not on the compressed, and this leads to an equivalent performance, indicating the near equal capability of compressed and lookup tokens.\nRandom Compressed Tokens: We also experiment with random learnable compressed tokens in the first layer, instead of resizing from lookup tokens. We observe a ~1% performance drop, thus showing the effectiveness of parameter-free resize operation for constructing compressed tokens."}, {"title": "6 Conclusions", "content": "In this work, we present a novel Lookup ViT architecture, which efficiently com- presses sparse and redundant visual information to fewer tokens. By efficiently combining lower and higher resolution tokens with bidirectional cross-attention, Lookup ViT achieves a significant reduction in FLOPs while upholding perfor- mance of ViT. Its effectiveness is demonstrated on diverse vision tasks, like image and video classification, image captioning, as well as it generalizability and robustness to visual corruptions.\nFuture work includes extending our model to dense prediction tasks like object detection and semantic segmentation, as well as scaling to larger model sizes."}, {"title": "A Appendix", "content": "In this section, we present detailed arguments indicating the robustness of our method, supported by additional results and visualisations."}, {"title": "A.1 Robustness on ImageNet family of datasets", "content": "The ImageNet family of datasets provides a comprehensive suite for evaluating the robustness of vision models. ImageNet-A assesses performance on real-world, unmodified images that are typically misclassified by models, gauging their abil- ity to handle naturally occurring challenges. ImageNet-C introduces common image corruptions like blur and noise, measuring resilience to various degrada- tions. ImageNet-R applies artistic styles to the original images, testing a model's ability to generalize across diverse visual renditions. ImageNet-O presents out-of- distribution samples from classes not found in the standard ImageNet-1k dataset, evaluating a model's robustness to unfamiliar objects and scenes. Together, these datasets offer a multi-faceted assessment of a vision model's performance, span- ning natural challenges, degradations, artistic variations, and out-of-distribution generalization.\nWe further analyse results on ImageNet-C in a greater detail here. ImageNet-C consists of 15 corruption types applied across five severity levels. In Table 6, we compare Lookup ViT's performance under these corruptions with a vanilla ViT"}, {"title": "A.2 Performance Analysis on Something-Something-V2", "content": "In Table 4 of the main text, we demonstrate that on the Something-Something V2 dataset [16], the performance improvements due to regularization when using the ViViT Factorised Encoder [1] model do not translate to the ViViT- Base model. In this section, we extensively try to enhance the vanilla ViViT- Base model. Table 7 lists the performance of the ViViT-Base model, when employed with different initialisation and regularisation strategies. The model is initialised using either a Kinetics 400 or a ImageNet21k pretrained checkpoint. We analyse these variants both in presence and absense of regularisation parameters"}, {"title": "A.3 Comparison with other efficient networks on ImageNet-1k", "content": "While we compare our method against three key architectures - ViT, Token Learner and Perceiver, in the main paper, we further contrast the performance of Lookup ViT against some more techniques in Table 8. Since some of these methods report results using different training frameworks (ViT/DeIT/DeIT3 we report relative gains in accuracy along with relative computational savings for a fair comparison."}, {"title": "A.4 Few-shot Transfer Results", "content": "In this section, we compare the generalization properties of LookupViT as compared to ViT, through few shot evaluations on standard image datasets like"}, {"title": "A.5 Attention Maps across Image Sizes and Primary Token Count", "content": "Figure 6 depicts the attention maps computed by the Lookup ViT-B/16 model trained on ImageNet-1k for different image resolutions and number of primary tokens. Each row is annotated with the corresponding values for these two parameters on the left. Each row represents the image, followed by the layerwise attention maps, averaged over all the attention heads, as well as over the primary tokens.\nAs the image resolution goes up, the cross-attention maps become finer in the sense that their representation power goes up. This is consistent with vanilla ViT models. However, the number of primary tokens being another choice to be made, there are two things at play in LookupViT. With a constant primary token count and increasing image resolution, the down-sampling ratio goes up and thus the information bottleneck becomes more stringent. A weak signal of the argument can be seen in Figure 6, where the attention maps for (384, 3) look \"stronger\" than those for (512, 3). However, the increasing accuracy trend with resolution for all patch sizes, as seen in Figure 1b (of paper), indicates that this effect is well subdued.\nAnother interesting detail to note here is the identification of salient objects in the early layers itself. This allows the later layers to concentrate on the relevant regions. Analogous to ViT, information is repurposed across tokens in the later layers for easier internal computation, which may not be otherwise intuitive or aligned with the image [9]. This partially explains the artifacts in the attention maps, and works from literature [9] can mitigate them for better visualization."}, {"title": "A.6 Attention Maps on Something-Something-V2 Video Classification", "content": "Figure 7 depicts the attention maps computed by Lookup ViT for some of the video inputs from the Something-Something-v2 dataset. In the case of images,"}]}