{"title": "IMPROVING EMBEDDING ACCURACY FOR DOCUMENT\nRETRIEVAL USING ENTITY RELATIONSHIP MAPS AND\nMODEL-AWARE CONTRASTIVE SAMPLING", "authors": ["Thea Aviss"], "abstract": "In this paper we present APEX-Embedding-7B (Advanced Processing for Epistemic eXtraction), a\n7-billion parameter decoder-only text Feature Extraction Model, specifically designed for Document\nRetrieval-Augmented Generation (RAG) tasks. Our approach employs two training techniques that\nyield an emergent improvement in factual focus: (1) Pre-convergence interrupted fine-tuning using\nStructured Entity Relationship Maps as training data input: designed to shift the model's attention\nand create a bias towards factual content rather than semantic style - this enhances plain text perfor-\nmance despite not being directly trained for it; and (2) Model-Aware Contrastive Sampling, creating\na balanced and evenly distributed collation map of hard and soft negatives directly informed by the\nbase model's competency. This combined methodology yields significant improvements, enhancing\nplain text query/document pair retrieval to achieve an absolute rank@1 accuracy of 90.86% (an in-\ncrease of 6.26% compared to the next leading model) in our evaluation, and reducing training data\ninput context size by an average of 37.71% compared to plain text for both queries and document\ntexts. Based on our evaluations, our model establishes a new state-of-the-art standard in text feature\nextraction for longer context document retrieval tasks.", "sections": [{"title": "Introduction", "content": "In AI-driven document retrieval, integrating external knowledge to enhance language model responses is essential.\nRetrieval-Augmented Generation (RAG) improves this process by combining document retrieval with text generation,\naiming for more accurate and relevant outputs [1]. However, the effectiveness of RAG systems relies heavily on the\nquality of text embeddings used for information retrieval. Poor embeddings can introduce inaccuracies and increase\nthe likelihood of factually incorrect or misleading responses, commonly referred to as hallucination[2].\nTo address these challenges, we present APEX-Embedding-7B, a 7-billion parameter model designed to enhance\ntext embeddings for document retrieval tasks [9, 11]. Our model uses structured entity relationship maps during\ntraining, focusing on key factual and relational content rather than overall semantic style. This approach ensures\ngreater accuracy in retrieval tasks, particularly for industries with large and complex datasets, such as the property\nmarket.\nFor example, in property management, legal compliance, and real estate transactions, where extensive documentation\nis required-ranging from contracts and property deeds to market analysis reports\u2014retrieving accurate and relevant\ninformation is critical. Our proposed model can efficiently create embeddings to allow accurate searches through vast\ncorpora of documents, retrieving precise data such as zoning laws, transaction histories, or market trends, reducing\nthe risk of factual errors and hallucinations. This is particularly useful when a RAG system needs to generate property\nlistings, valuation reports, or legal advice based on detailed document archives.\nBy advancing text feature extraction in RAG systems, our proposed model provides a powerful solution for improv-\ning retrieval accuracy, ensuring factual precision, and streamlining information extraction in large document-heavy\nindustries such as real estate."}, {"title": "Proposed Methodology", "content": "Our method for improving document retrieval accuracy in Retrieval-Augmented Generation (RAG) systems leverages\ntwo key techniques: pre-convergence interrupted fine-tuning using Structured Entity Relationship Maps and Model-\nAware Contrastive Sampling. These techniques work together to enhance factual precision without sacrificing the\nmodel's general capabilities.\n2.1 Pre-convergence Interrupted Fine-tuning with Structured Entity Relationship Maps\nThis method includes fine-tuning the model with Structured Entity Relationship Maps to emphasise factual content.\nThe structured maps are synthetic representations generated from real documents, capturing key entity relationships\nto shift the models attention bias toward factual data rather than linguistic style.\nHowever, we stop the fine-tuning process before convergence for a critical reason: we want to bias the hidden state\nrepresentations of the model toward factual content without diminishing its ability to work with plain text. By inter-\nrupting the process early, we prevent the model from overfitting to the structured data and becoming overly reliant on\nthe entity maps. Instead, this approach influences the model to implicitly prioritise factual information when working\nwith both structured inputs and plain text during general use.\nThe hypothesis behind our methodology can be validated by work such as Jin et al. (2024)[12]'s investigation of how\nrepresentations of formal program semantics emerge during training. Similar to their findings that a model can learn\nto interpret underlying structures without explicit supervision throughout, we aim to influence our model's internal\nrepresentations toward factual embeddings. By stopping early in training, we ensure that the model maintains its\ngeneral ability to handle plain text, while instilling a stronger focus on extracting critical factual details, resulting in\nimproved embeddings for data-heavy document retrieval tasks.\nIn addition, recent work in parameter-efficient fine-tuning highlights the benefits of early stopping or selective updates\nto balance specialisation and generalisation. Houlsby et al.[18] demonstrates that limiting parameter updates in fine-\ntuning can prevent overfitting while still improving model performance on task-specific objectives. Similarly, Wang\net al.(2022)[19] shows that aligning fine-tuning strategies closely with pre-training objectives can yield significant\nperformance gains in few-shot scenarios, supporting the idea that early-stopping helps the model focus on factual\naccuracy while avoiding excessive reliance on structured inputs.\n2.2 Model-Aware Contrastive Sampling\nIn our method, Model-Aware Contrastive Sampling improves retrieval accuracy by selecting and curating negative\nexamples based on the base model's existing embedding capabilities and reducing the need for large batch sizes\nduring training. This approach primarily ensures that the negative examples used during training are tailored to the\nmodel's strengths and weaknesses, rather than being selected arbitrarily or generated uniformly.\nFirst, embeddings for queries and documents are generated using the base model[13]. We then compute cosine\nsimilarity[7] scores between these embeddings, allowing us to rank negative examples based on their similarity to\nthe query. This ranking enables the classification of negative samples into two categories:\nSoft negatives: These are documents with higher cosine similarity to the query but are factually incorrect or irrelevant.\nThese examples are more challenging for the model, as they appear semantically close to the positive document but\ncontain key factual differences.\nHard negatives: These are documents with lower cosine similarity to the query, and while still factually incorrect,\nthey are easier for the model to differentiate. Introducing these examples ensures diversity in the training process and\nprevents overfitting to closely related examples.\nWe then condense a large batch of query-document pairs into smaller, well-balanced training batches consisting of one\nquery, one positive document, five soft negatives, and five hard negatives throughout the training data and each set and\ndistributed evenly during collation across the training data to reduce stochastic gradient noise. This balanced sampling\napproach helps the model improve its ability to make precise factual distinctions, which is especially important in\nindustries like property management, where document retrieval requires high factual accuracy.\nIn contrast, Wang et al. 2024 [11] do not use the base model's performance to inform negative sampling. Their\nmethod rests on generating synthetic data for contrastive learning tasks, where negative examples are chosen based on\npredefined data augmentation strategies rather than the model's own embedding quality or performance. Their focus is\non generating diverse synthetic query-document pairs for tasks like semantic similarity, with uniform sampling across\nthese datasets. Similarly, NV-Embed [15] uses a separate fine-tuned encoder-based model to select hard negatives"}, {"title": "Model Architecture", "content": "APEX-Embedding-7B is a decoder-only feature extraction model, based on 'SFR-Embedding-2-R' [13], which itself\nbuilds on E5-mistral-7b-instruct [11] and Mistral-7B-v0.1 [9]. Similarly, the NV-Embed-V2 model [15] is also based\non Mistral-7B.\nWhile both models share the same underlying base, APEX-Embedding-7B differentiates itself by leveraging structured\nentity relationship maps during training to improve the factual accuracy of embeddings, especially in document-heavy\nretrieval tasks. In contrast, NV-Embed focuses on improving general-purpose embeddings by removing the causal\nattention mask and introducing a latent attention layer for better sequence pooling.\nUnlike NV-Embed, which modifies the attention mechanism to enhance representation, APEX-Embedding-7B retains\nthe causal attention mask but interrupts fine-tuning pre-convergence to balance between factual emphasis and main-\ntaining strong performance on plain text tasks. This allows APEX-Embedding-7B to focus more on structured input\nwhile still excelling in retrieval tasks that require precise factual extraction."}, {"title": "Dataset Generation", "content": "To train APEX-Embedding-7B, we constructed a custom dataset consisting of tens of thousands of curated document\nexcerpts and full-text pages, drawn from a variety of publicly available sources. These documents include a mix of\nplain text general knowledge paragraphs, RAG corpus documents, and full legal contracts, such as property leases.\nEach document page is tokenised to a maximum length of 4096 tokens, matching the context window size of the base\nmodel.\nBased on each processed document, query text is synthetically generated using GPT-40[3] (prompts can be found\nin section 9.1). After this, each document and query in the training set is then represented by a Structured Entity\nRelationship Map, which serves as the core input format during training. These maps are also generated using GPT-40\nusing a specialised prompt. The maps focus on key entities and their relationships within the text, ensuring that the\nmodel learns to prioritise factual content over stylistic or irrelevant details.\nThe structure of an Entity Relationship Map is as follows:\n{\n \"topic\": \"\",\n \"cross_reference_tags\":\"\",\n \"entities\": [{\n \"entity_type\": \"\",\n \"entity_name\": \"\",\n \"attributes\": {},\n }],\n}\nEach resulting sample in the dataset includes the following components:\nQueries: Synthetic user queries are generated based on labeled data from the corpus. These queries are meant to\nsimulate real-world information retrieval tasks. These are represented structured as entity relationship maps.\nCorpus Documents: The documents are either full pages or relevant excerpts from the corpus, represented using\nStructured Entity Relationship Maps. These entity maps serve as a direct transformation of the document's content,\nfocusing on key entities and their relationships rather than stylistic or semantic features."}, {"title": "Applying Model Aware Contrastive Sampling", "content": "Relevant Document Mappings: This mapping directly connects each query to one or more relevant documents from\nthe corpus. This is critical for training the model to retrieve the correct documents during contrastive learning.\nEach sample within the dataset is structured as follows:\n{ \"queries\": { \"query_id_1\": \"query_map_1\", ... },\n \"corpus\": { \"document_id_1\": \"document_map_1\", ... },\n}\nThis structure supports the model's ability to distinguish between relevant and irrelevant documents in a retrieval\ntask, enhancing its performance in document-heavy industries like real estate, where factual precision in selecting the\ncorrect document from a pool is critical.\nOur approach maintains the model's pre-trained capability to generate high-quality plain text embeddings while en-\nhancing its ability to handle structured entity relationship maps, as described in Section 2.1. This leads to improved\nfactual content prioritization in document retrieval tasks.\nAs explained in Section 2.2, our use of Model-Aware Contrastive Sampling is central to this improvement. Both our\nmethod and Wang et al. [11] utilise the InfoNCE (Noise-Contrastive Estimation) loss function [14]. However, unlike\nWang et al.[11], who rely on synthetic hard negative samples generated via predefined prompting strategies, our\napproach selects soft and hard negatives informed by the base model's performance. By using negative examples from\nthe batch itself and tuning their selection based on cosine similarity[7] to the query, we train the model to recognise\nmore nuanced factual distinctions, not just stark contrasts. This approach results in significantly improved embedding\nquality and retrieval accuracy, particularly in scenarios requiring high factual precision.\nOur collation strategy is an essential part of the Model-Aware Contrastive Sampling methodology, designed to optimise\ntraining efficiency by reducing the memory footprint during batch processing. Rather than treating this as a distinct\nprocess, collation leverages the same precomputed embeddings and similarity-based selection of negative samples but\nfocuses specifically on how these are organized into smaller, more memory-efficient micro-batches.\nTo begin, embeddings for all queries and corpus documents are precomputed using the base model, creating a compre-\nhensive collation map that includes cosine similarity[7] scores, positive matches, and ranked soft and hard negatives\nfor each query. This map is not only used for sampling hard and soft negatives, but also to inform how these samples\nare distributed across micro-batches.\nThe core of this collation strategy is to compress large, resource-heavy batches into smaller units while maintaining\nthe diversity and challenge necessary for effective contrastive learning. By carefully balancing hard and soft negatives\nwithin each micro-batch, the training process avoids overfitting and reduces stochastic gradient noise, all while working\nwithin the constraints of limited GPU memory.\nThis structured distribution of challenging samples ensures that training remains both effective and efficient, with batch\nsizes kept minimal without sacrificing the richness of the training data. In doing so, the model is exposed to a wide\nvariety of contrastive examples, allowing it to refine its understanding of factual distinctions, even when operating\nwith compressed micro-batches.\nExample Collation Map Entry (positive rank@1 match)\n}\n\"Hb4ZYSq-2g0Xgg\": {\n\"positives\": [\n\"bZjh0CwdCNixvw\"\n],\n\"soft_negatives\": [\n\"SDBU_xkhIwcz3w\",\n...x5\n],\n\"hard_negatives\": [\n\"zdGv_8yIIKRSAA\",\n...x5\n],\n\"positive_match_similarity\": 0.93701171875,\n\"positive_rank\": 0"}, {"title": "Experimental Data and Results", "content": "We initially conducted experiments with various batch sizes using our training dataset split to evaluate the pre-trained\nbase model and recorded the resulting accuracy and lowest positive match rank seen during evaluation.\nThe data demonstrates that as the batch size increases with full batch sampling for negatives, the final accuracy of\nthe pre-trained model decreases and the lowest positive match rank seen during evaluation increases. This indicates\nthat larger batch sizes provide a better representation of the model's shortcomings and offer a higher ceiling for\nimprovement during further training However due to GPU Memory limitations we will have to optimise this to give\nus the same improvement potential with much smaller batch sizes.\nBase Model Evaluation Process\u00b9\nCosine Similarity\nFor each query embedding q and document embedding $d_i$, cosine similarity[7] is:\n$cosine\\_similarity(q, d_i) = \\frac{q.d_i}{\\left | q \\right | \\left | d_i \\right |}$\nCosine similarity[7] measures the cosine of the angle between two vectors in the embedding space, in this case between\nthe query embedding q and the document embedding $d_i$. It gives a value between -1 and 1, where 1 means the vectors\nare perfectly aligned (most similar) and -1 means they are diametrically opposed (least similar). This similarity score\nis essential for ranking the relevance of documents to a given query in retrieval tasks.\nDocument Ranking\nAfter queries and document texts are generated by the base model we perform the following. For each query j, we\ndefine a set of candidate documents, denoted as $D_j$. We calculate the similarity between the query j and each document\n$d_i$ in $D_j$ using cosine similarity:\n$similarity_j = {cosine\\_similarity(q_j, d_i) | i \\in D_j}$\nThe documents are then sorted based on their similarity scores to produce a ranked list, ranked_docsj. The rank of the\npositive (relevant) document, $d_{pos}$, within this list is defined as:\npositive_rank = rank of $d_{pos}$ in ranked_docsj\nAccuracy Calculation\nThe accuracy is the proportion of queries where the positive document is ranked first:\n$Accuracy = \\frac{\\Sigma_{j=1}^{n} 1 (positive\\_rank_j = 1)}{n}$\nHere, $1(positive\\_rank_j = 1)$ is an indicator function that returns 1 if the positive document is ranked first for query j,\nand 0 otherwise. The total number of queries is denoted by \u043f.\nOur Evaluation process in section 7 follows a very similar procedure."}, {"title": "Determining the Smallest Effective Batch Size", "content": "Following the completion of the Model-Aware Contrastive Sampling process, the next task is to reorganise the col-\nlation map to identify the smallest effective batch size for training. This step relies on redistributing the challenging\nsamples (i.e., soft and hard negatives) across the batches, ensuring that each batch has a balanced proportion of positive\nexamples while maintaining the difficulty level seen in larger batches. The goal is to retain the same level of challenge\nin smaller batches while optimising memory usage.\nTo maintain consistency across training, the aim is to ensure that each batch reflects the base model's observed perfor-\nmance distribution, meaning each batch should contain a similar mix of rank 1 correct predictions and failed queries.\nThis ensures that the model is exposed to a representative sample of both successful and challenging cases in every\ntraining step.\nTo determine the smallest effective batch size, we apply the following process:\n$b_{min} = min \\{ b \\in N | (\\frac{N_{queries}}{[\\frac{N_{queries}}{b}]}) \\land (\\frac{N_{positives}}{[\\frac{N_{queries}}{b}]}) > 1 \\}$\nWhere:\n\u2022 Nqueries: Total number of queries in the dataset.\n\u2022 Npositives: Total number of queries where the positive document is ranked at 0 (i.e., positive_rank = 0 or\n rank@1 if non-zero indexed).\n\u2022 $[\\frac{N_{queries}}{b}]$: Number of batches for a given batch size b.\n\u2022 bmin: The smallest batch size b that guarantees a balanced distribution of positive ranks across all batches.\nNumber of Batches and Positives per Batch\nOnce the smallest batch size bmin is found, the total number of batches is:\n$num\\_batches = \\frac{N_{queries}}{b_{min}}$\nThe number of queries with positive_rank = 0 (i.e., positive matches) per batch is:\n$pos\\_per\\_batch = max \\{ 1, (\\frac{N_{positives}}{num\\_batches}) \\}$\nPercentage of Positive Matches per Batch\nThe percentage of queries with positive_rank = 0 (i.e., positive matches rank@1) in each batch i is:\n$Percentage = \\frac{Positive Matches in Batch i}{Total Queries in Batch i} \u00d7 100$\nThe challenging samples are distributed evenly across batches to ensure that each batch maintains the same level of\ndifficulty as observed in larger batches. This approach guarantees that each batch contains a balanced mix of positive\nand negative samples, which promotes more robust learning, reduces the risk of over-fitting, and minimises stochastic\ngradient noise-issues that typically arise in smaller, unbalanced batches. This method effectively condenses the\nlearning effectiveness of very large batches into much smaller, memory-efficient batches.\nThis collation strategy presents an effective approach to training with smaller batch sizes by embedding the model's\npre-training performance into the training process. By ensuring each batch is both challenging and representative, the\nstrategy enhances training efficiency while reducing the memory footprint. This enables the training of large-scale\nmodels with more efficient resource usage. The balanced exposure to diverse data points helps improve the model's\naccuracy and generalisation, contributing to its ability to handle complex queries with limited computational resources."}, {"title": "Training", "content": "For training, We employed 4-bit QLoRA (Quantised Low-Rank Adaptation) techniques [4] to enable efficient fine-\ntuning while preserving the integrity of the pre-trained weights. This approach allowed for resource-efficient training\nby modifying only a small number of parameters, reducing the computational burden without compromising the\nmodel's capacity. Our custom training loop was implemented using the PEFT (Parameter-Efficient Fine-Tuning) [5]\nand Transformers [6] libraries.\nThe training process was configured with the following hyperparameters and loss function:\n\u2022 Effective Batch Size: 10\n\u2022 Gradient Accumulation Steps: 2\n\u2022 Loss Function: Noise-Contrastive Estimation (InfoNCE) with a temperature of 0.07\n\u2022 Learning Rate: 2e-5\n\u2022 LORA Configuration:\n Rank (r): 8\n Alpha: 16\n Applied to all linear layers: q_proj,k_proj,v_proj,o_proj, gate_proj, up_proj, down_proj\nThe training ran for 545 steps, with the CUDA memory reserved maxing out at 38GB. The total training time was\napproximately 13 hours on a single 40GB NVIDIA A100 GPU.\nValidation was conducted every 100 steps to monitor model performance and ensure the model did not overfit to the\nstructured entity relationship maps. The training process was stopped early after peak accuracy was reached, as further\ntraining risked degrading plain text performance due to overfitting on the structured data.\nA key aspect of the training duration was the frequent validation checks, which significantly extended the training time.\nIf the validation process had been optimized, the total training time could have been reduced.\nIn contrast to our pre-convergence interrupted fine-tuning, which halts early to balance factual accuracy and generali-\nsation, the NV-Embed model [15], employs a two-stage contrastive instruction-tuning process. The first stage focuses\nspecifically on retrieval tasks, utilising in-batch negatives and hard negatives generated by a separate encoder based\nmodel to refine the model's ability to differentiate between closely related documents. In the second stage, NV-Embed\nis fine-tuned on a broader range of tasks, including classification and clustering, without the use of in-batch negatives,\nto enhance its generalization across multiple language domains. Additionally, while our model applies the same in-\nstruction across both queries and documents to streamline training and improve factual precision, NV-Embed adapts\ndifferent instructions for retrieval and non-retrieval tasks. This choice allows NV-Embed to handle diverse language\ntasks but may trade off retrieval-specific performance optimization, where our model remains focused."}, {"title": "Observations on Convergence", "content": "One notable observation is that if the model were allowed to fully converge on the structured entity relationship\nmaps, the improvement in evaluation accuracy using a strucural entity map version of our evaluation dataset would be\nmarginal, only exceeding the base model's plain text accuracy by 1.93%. The base model achieved 84.6% accuracy\non plain text, whereas the model trained to convergence on the entity relationship maps improved this to 86.53%.\nThis outcome helps to further validate our methodology and hypothesis that interrupting training before full conver-\ngence on entity relationship maps refocuses the model on key factual differentiators[18], rather than solely optimising\nfor semantic similarities. By halting training early, we allow the model to generalise better to plain text while still\nbenefiting from the structured data, effectively balancing the model's ability to handle factual content across different\ntypes of input."}, {"title": "Evaluation", "content": "In evaluating our embedding model and other leading embedding models for comparison, we aimed to ensure that\nthe evaluation methodology reflected real-world use within a Retrieval-Augmented Generation (RAG) application as\nclosely as possible.\nOur evaluation dataset consists of 1,500 previously selected queries and their respective documents, unseen to our\nmodel and split before training. These documents have a context length spread representative of the training dataset\n(<4096 tokens). The documents cover a wide range of topics, including: General knowledge, Legal texts, Property\nmarket-related content (e.g., leases and contracts). For each query and document, embeddings were generated itera-\ntively via last token pooling, with the exception of proprietary models from OpenAI and Google, which were accessed\nvia API.\nIn an accurate RAG system, it is crucial that the top-ranked retrieved document is the correct one, to a) Prevent factual\ninaccuracies and potential misinformation in responses, and b) Reduce the likelihood of hallucination if the document\ntext is passed to a Large Language Model (LLM). Therefore our evaluation methodology centers on testing the quality\nand factual alignment of the vector embeddings within a rigorous framework.\nOur evaluation process is both comprehensive and exacting. For each query in our dataset, we compare its embedding\nagainst all 1,500 document embeddings in the evaluation dataset. This exhaustive approach ensures that no challenging\nsamples are overlooked in our analysis. The comparison between each query embedding and all document embeddings\nis carried out using cosine similarity [7], a metric chosen for its proven efficacy in exhaustively measuring the similarity\nbetween high-dimensional vectors [7]. Following the calculation of similarities, we sort the entire list of documents in\ndescending order of similarity scores for each query [7]. We then scrutinise the document at rank 1 (highest similarity)\nto ascertain whether it corresponds to the correct document for the given query. Our criteria are stringent: if the\ntop-ranked document is indeed the correct match, we assign a score of 1; otherwise, it receives a score of 0. This\nbinary scoring system reflects our emphasis on precise, top-ranking retrieval. To obtain our final metric, we calculate\nan average accuracy across the entire dataset. This is achieved by summing the binary scores and dividing by the total\nnumber of queries, yielding a comprehensive measure of our model's performance.\nThis approach ensures that our evaluation closely mirrors the performance requirements of real-world RAG applica-\ntions."}, {"title": "Conclusion", "content": "In this paper, we presented APEX-Embedding-7B, a language model optimised for document retrieval in RAG. By\nintegrating pre-convergence interrupted fine-tuning with Structured Entity Relationship Maps and Model-Aware Con-\ntrastive Sampling, we enhanced both factual precision and training efficiency. These methods allowed us to focus the\nmodel on critical content without over-fitting, and efficiently manage training with reduced memory usage.\nAPEX-Embedding-7B has demonstrated a significant improvement in rank@1 retrieval accuracy, surpassing existing\nstate-of-the-art models by 6.26% (achieving 90.86%), while also reducing input context size by 37.71% during training.\nThese results highlight its potential to enhance retrieval accuracy in industries like property management and legal\nservices, where factual accuracy is crucial and also demonstrates practical steps to improve training efficiency.\nFuture work could explore scaling these techniques for larger models and expanding their application to other domains,\nas well as optimising training processes further for even greater efficiency. The model's context window can also be\nexpanded with techniques such as RoPE scaling and rotation [8] (section 5.2 [11]). We believe our approach offers\na promising path forward for improving document retrieval systems that require both high precision and practical\nscalability."}, {"title": "Appendix", "content": "Query Generation2:\n\"\"\"\nUser Query Generator v2.0\nPurpose: Expertly analyze the text excerpt given and output a query or question sentence that\nincludes references to the task at hand and type of document:\\n\n\nPlease fulfill this purpose with precision and factual accuracy. You must write the query without\nany \"document name\"... It should be framed as a question, with detail.\n\"\"\"\nStructured Entity Relationship Map\n\"\"\"RAG Metadata Preparation System v2.0\\n\n\nPurpose: Expertly analyze the text excerpt given and output the following JSON:\\n\n\n{\n \"topic\": \"\",\n \"cross_reference_tags\": \"\",\n \"entities\": [{\n \"entity_type\": \"\",\n \"entity_name\": \"\",\n \"attributes\": {},\n }],\n}\nThere must be no detail lost or omitted, and please intracately capture the factual structure of\nthe entity relationships within. Details that would otherwise be lost in a general \"semantic\"\nunderstanding must be highlighted here.\nPlease fulfill this purpose with precision and factual accuracy.\n\"\"\"\nAfter this, all generations undergone human quality control and editing.\nFeature Extraction Prompts\nThis instruction is used for both queries and documents/passages during end-use of the model.\n\"Instruction: Please perform a RAG search based on the following. Text: {query_or_passage}\""}]}