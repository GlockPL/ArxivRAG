{"title": "Combating Spatial Disorientation in a Dynamic Self-Stabilization Task Using AI Assistants", "authors": ["Sheikh Mannan", "Hannah N. Davies", "Paige Hansen", "Paul DiZio", "Vivekanand Pandey Vimal", "Nikhil Krishnaswamy"], "abstract": "Spatial disorientation is a leading cause of fatal aircraft accidents. This paper explores the potential of AI agents to aid pilots in maintaining balance and preventing unrecoverable losses of control by offering cues and corrective measures that ameliorate spatial disorientation. A multi-axis rotation system (MARS) was used to gather data from human subjects self-balancing in a spaceflight analog condition. We trained models over this data to create \"digital twins\" that exemplified performance characteristics of humans with different proficiency levels. We then trained various reinforcement learning and deep learning models to offer corrective cues if loss of control is predicted. Digital twins and assistant models then co-performed a virtual inverted pendulum (VIP) programmed with identical physics. From these simulations, we picked the 5 best-performing assistants based on task metrics such as crash frequency and mean distance from the direction of balance. These were used in a co-performance study with 20 new human subjects performing a version of the VIP task with degraded spatial information. We show that certain AI assistants were able to improve human performance and that reinforcement-learning based assistants were objectively more effective but rated as less trusted and preferable by humans.", "sections": [{"title": "1 INTRODUCTION", "content": "Maintaining spatial awareness and orientation is critical in domains like piloting, spaceflight, and even driving. Spatial disorientation has been and continues to be a leading cause of fatal aircraft incidents [7, 15] and occurs when sensory information (e.g. from the visual, somatosensory, and vestibular systems) is erroneous, which can lead to unrecoverable crashes, injury, or loss of life [15, 30].\nAn Al agent in this situation could potentially use numerical signals to track the pilot and vehicle's positioning in the relevant orientational plane(s), detect if there is a risk of losing control [10, 58, 59], and even alert the pilot to make corrective maneuvers. However, there is a record, particularly in high-risk domains like aviation, of either over-trust or under-trust in highly automated systems. For instance, Sadler et al. [43] demonstrated that pilots' trust in the recommendations of an automated system correlated with the level of transparency (such as justification) in the recommendation. The shared autonomy literature indicates that even when an agent knows an optimal strategy, failing to comply with a suboptimal strategy its human partner insists on may have a negative effect on trust and lead to disuse of the system [17, 22, 31, 44].\nIn this paper, we hypothesize that when attempting to balance themselves under disorienting conditions, humans will be more receptive to assistance from an Al whose strategy to regain balance is more human-like, even if that strategy is objectively less optimal than a less human-like strategy. We establish a novel task of AI assistance in regaining balance in disorienting situations using a documented, realistic simulation of vehicle control in a spaceflight analog condition. In this scenario, subjects are deprived of gravitational cues and use a joystick to self-balance while seated in a multi-axis rotation system (MARS) programmed to behave like an inverted pendulum [34, 51, 52]. We took data of human subjects attempting to keep the MARS balanced and used it to train digital twins of \"pilots\" that exemplified performance characteristics of humans of various proficiency levels. We then trained multiple \"assistant\" models that, due to different data and techniques, demonstrate performance strategies that may align with or differ drastically from those of humans. The pilot and assistant models were placed in a co-performance simulation with a virtual inverted pendulum (VIP) programmed with identical physics to the MARS. Assistants attempted to help the pilot models keep the VIP balanced and avert crashes by offering corrective cues when the pilot risked a destabilizing loss of balance. To test transfer to a different environment, we ran assistants with digital twins trained over data from humans performing an instance of the VIP task directly. Finally, the best-performing assistant models were deployed in a human-subject study in the VIP environment to demonstrate the feasibility of AI assistance as a countermeasure in disorienting situations.\nFollowing a discussion of related literature (Sec. 2), in Sec. 3 we describe the MARS and VIP tasks, the human performance data collected from each, and how performance is evaluated in these tasks. Sec. 4 describes the training of different reinforcement learning models using an analogous environment to the MARS/VIP tasks, and deep learning models using human performance data, as well as the selection of \"digital twin\" models of different human proficiencies. Sec. 5 presents the evaluation methods and results for two studies: the high-throughput digital twins study wherein 21 candidate assistant models made corrective suggestions to the different digital twins of human pilots, and a human-subject study wherein the five best performing assistants from the digital twins study engaged in co-performance and co-training in the VIP task with real humans. In Sec. 6 and Sec. 7, we discuss our findings and their implications for human-agent collaboration and trust when performing real-time situated tasks, and directions for future work.\nOur novel contributions are: 1) a novel task of AI assistance in disoriented self-balancing, a challenging action-learning task with well-controlled parameters; 2) an assessment of different reinforcement learning and deep learning models' abilities to prevent destabilizing loss of control in this task using a high-throughput digital twins setting; 3) a human-agent co-performance study with the best-performing AI assistants involving human-in-the-loop (HITL) Al training, to demonstrate transfer to real humans."}, {"title": "2 RELATED WORK", "content": "Spatial disorientation and balance. An inverted pendulum (with center of mass above the pivot point) is a common model of human upright balance in the study of postural dynamics [40]. Panic et al. [34] and Vimal et al. [53] explored the relevance of the MARS balancing task to the perception of gravitational cues during unstable vehicle control. Subjects were strapped into a MARS device programmed with inverted pendulum (IP) dynamics and instructed to stabilize themselves about the direction of balance (DOB) using a joystick. Because the risk of spatial disorientation-related accidents is heightened when visual information is limited [25, 46], subjects were blindfolded. Typically, humans rely on gravitational cues when balancing, which are detected by the vestibular and somatosensory systems as participants tilt away from the gravitational vertical, however in spaceflight conditions gravitational cues are not reliable. To create a disorienting spaceflight analog condition, Panic et al. [33] and Vimal et al. [51, 52] placed participants in the Horizontal Roll Plane, where they were always perpendicular to the gravitational vertical and no longer tilting relative to it. 90% of participants reported spatial disorientation and in data 100% exhibited characteristic positional drifting [52]. Participants showed minimal learning and frequent \"crashes\" (reaching pre-programmed \u00b160\u00b0 boundaries, after which the MARS automatically reset to the DOB).\nAl algorithms. Tasks like IP balancing are well-known use cases in reinforcement learning [2, 5, 13] that serve as demonstration benchmarks for newer continuous control algorithms like SAC [16] or DDPG [23]. These have demonstrated proficiency at solving non-linear control tasks like IP balancing using reward signals extracted from observation of applied environmental physics. However, due to the nature of environmental physics input vs. sensorimotor input, we hypothesize that they learn to perform the task very differently from humans. Here we explore the application of different RL and deep learning algorithms to AI assistance in disoriented balancing.2\nEmbodiment. Seminal literature operationalizes embodiment as a two-way process between brain and body/environment [47, 50]. Most modern approaches treat embodiment primarily at a surface level, such as the richness of visual [3, 4, 18, 19] or interoceptive [1] representation of an agent's form, or in terms of the physical form an agent takes, wherein the actions it is capable of are conditioned upon its articulators [11, 36, 37]. We adopt a definition of embodiment akin to Ziemke's structural coupling [60], where a system embodied in an environment takes environmental states as input, which changes the condition of the system [38]. However, the types of information a system is exposed to, such as through different types of sensors, also condition the relations the system develops between itself and its environment [8], such that exposure to different types of data (or different ways of measuring the same underlying environmental state) may mean that different inputs and modeling strategies cause a model to learn different policies within the same action space, and thus may learn to perform the same task equally well through potentially radically different strategies. Therefore we will speak of different \"embodiments\u201d of the task problem space as reflected through these different strategies."}, {"title": "3 MARS AND VIP TASKS", "content": "Fig. 2 compares the MARS and VIP paradigms. Both can be configured in challenging but non-disorienting modes, with standard sensory information; or difficult and disorienting modes, with degraded information. In both disorienting conditions, subjects show the same characteristic drifting and lack of learning when compared to the coherent conditions.\nThe MARS task is described in Sec. 2. In the data we use here (described below), MARS dynamics were governed by = $k_psind$, where @ is degrees deviation from the DOB and pendulum constant $k_p$ = 600\u00b0/s2.\nIn the virtual inverted pendulum (VIP) paradigm, an analog to the MARS programmed with the same physics, subjects balance a visually simulated circular array of dots (random dot kinematogram, RDK) which rolls in the plane of the display screen. This is visually rendered for humans and can be directly actuated by an algorithm. In the disorienting VIP condition (similar to the Horizontal Roll Plane in the MARS), the RDK is 50% coherent: alternating subsets of dots displace coherently across consecutive frames while the other half jump randomly. This eliminates configural displacement cues relative to the upright DOB while providing low-level retinal motion cues. Similar performance degradations occur between MARS upright vs. supine and VIP 100% vs. 50% coherence conditions, even with practice, exhibited by an increased number of crashes accompanied by more frequent destabilizing actions (Fig. 2)."}, {"title": "3.1 Data Collection", "content": "MARS. Wang et al. [58] released MARS human performance data from 34 healthy adults (18 female, 16 male). Each subject experienced two experimental sessions on consecutive days, each consisting of 20 100-sec. trials where they attempted to balance themselves with minimal oscillations while blindfolded. The data contains angular positions and velocities, and joystick deflections sampled at 50 Hz.\nVIP. The VIP data consists of 31 healthy adults (22 female, 9 male). Subjects took part in 12 30-sec. trials in one session in the disorienting condition, with the same goal as the MARS task. Angular pos./vel. and joystick deflections were sampled at 200 Hz."}, {"title": "3.2 Performance Evaluation", "content": "An ideal performer in both of these tasks would be one that immediately rotates to the balance point and stays there with little to no motion. Calculable metrics from the collected data include number of crashes (excursions beyond \u00b160\u00b0), proportion of destabilizing deflections (% destab.-see Fig. 2), mean/standard deviation of angular position 0, average magnitude of velocity (\u00b5|Mag|vel), and root-mean-square (RMS) velocity. Lower metric values usually mean improvement, e.g., fewer crashes, more time spent near the DOB, less oscillation, slower motion, smaller deflections, etc.\nMARS. Vimal et al. [55] used a Bayesian Gaussian Mixture method and the aforementioned features to cluster subjects into 3 statistically distinct groups that represent Proficient, Somewhat-Proficient, and Not-Proficient performance (hereafter Good, Medium and Bad). We uses these clusters to split the MARS data into training subsets and to characterize digital twin performance in the task.\nVIP. Plotting VIP subjects' RMS velocity vs. crash frequency revealed a positive linear relationship (r = .73). Based on the number of crashes in the 12th trial and crash reduction between Trials 1 and 12 (i.e., final performance and overall improvement), we assigned participants relative rankings and divided them in tertiles to mirror the Good/Medium/Bad MARS classification. VIP participants typically exhibited more crashes, destabilizing actions, and RMS velocity compared to MARS participants of the equivalent proficiency. These factors and differences in sample rate and environment allow us to test the transfer of AI assistants to novel digital twins."}, {"title": "4 MODEL TRAINING", "content": "Our goals in model training were both to train Al models capable of independently performing an IP balancing task parameterized with MARS physics and to create digital twins of humans that replicate different kinds of participant performance in the task. In some cases these two categories overlapped, leading to a testable hypothesis: that a model that performs well at the task may also be able to assist a \"pilot\" (real or simulated) in performing the task better."}, {"title": "4.1 Reinforcement Learning Models", "content": "We trained reinforcement learning-based models that learn directly from exposure to environmental physics using a custom variation of Gymnasium's classic-control Pendulum environment [49]. This included 1) a problem space bounded at \u00b160\u00b0 from the DOB, like the MARS/VIP task; 2) a random starting point for the inverted pendulum within the newly defined problem space; 3) a custom reward function\u00b3 given by Eq. 1, to encourage small continuous adjustments like those of Good MARS participants [52, 55].\nr=\\begin{cases}1-(\\frac{\\theta}{30})^{2}+(\\frac{w}{.1})^{2}+ .01d^{2}), & \\text{if } \\theta < 30\u00b0 \\cup \\theta > 30\u00b0\\\\0,& \\text{if }30\u00b0 \\leq \\theta \\leq 30\u00b0\\end{cases}\n\nThe RL algorithms were directly exposed to environmental physics, unlike the DL models which received only implicit physics through the human performance data. The default SAC and DDPG implementations routinely converged to an optimal strategy that manifests as rotating immediately to the DOB and holding position there. We also trained and evaluated behavior cloning (BC) [42] and adversarial inverse RL (AIRL) [14] using Good MARS participant data, to teach the models strategies closer to what humans would execute, in terms of replicating behavior or uncovering implicit reward functions in the data.\nRL models take the current angular position and velocity to predict the next joystick deflection (winSize = 0.0, future = 0.0, cf. Fig. 4). We used Stable-Baselines3's SAC and DDPG implementations [39], and trained them with the default MLP policy, BC, or AIRL. Gaussian distributed noise was added to the action space to encourage exploration as the IP is considered an under-actuated task [21]. We trained 5 RL models: 1) SAC & DDPG each with the standard policy; 2) SAC & DDPG each trained using BC; 3) AIRL implemented with a SAC-based generator model."}, {"title": "4.2 Deep Learning Models", "content": "To replicate human-like real-time performance of the MARS task, we trained Multilayer Perceptron (MLP), Vanilla Recurrent (RNN), Long-Short Term Memory (LSTM) [20], and Gated Recurrent Unit (GRU) [9] network architectures over actions made by humans in the actual MARS data. Architectures were trained using different window sizes (0.0s, just the current timestep-MLP models only; and 0.2s, 0.3s, and 0.5s). Training data was also split into Good, Medium, and Bad proficiencies and individual models were trained on data of a specific proficiency. An additional set of models was trained using a combination of 1) Good & Medium and 2) Good, Medium & Bad proficiency data, to see if models could learn strategies employed by certain proficiency groups in scenarios that were not experienced by the others. In total, we trained 40 individual DL models, all of which, even when they successfully avoid crashes in solo task performance, demonstrate suboptimal strategies with human-like oscillation and intermittent deflections. These behavioral differences show the differences in how the RL and DL models learn to situate themselves in (or \"embody\") the problem space."}, {"title": "4.2.1 Selecting Representative Pilots", "content": "As the same performance characteristics are exemplified in both MARS & VIP, we identified models that most closely approximate performance categories from Vimal et al. [55], reflected in Table 1.\nAll DL models were made to perform the VIP task (3 \u00d7 30s trials), with angular position, velocity, and joystick deflection recorded at each timestep. We extracted performance features shown in Table 1. Since the data distribution could not be assumed to be spherical, these features were used in k-means clustering (k = 3) to approximate the split into Good, Medium, or Bad groups. Following Vimal et al. [52], the cluster of models that displayed higher oscillations and greater average magnitude of deflections was considered Bad while the cluster that displayed smaller, more intermittent actions was considered Good (with the remainder considered Medium). We then took the models in each cluster that were trained over the equivalent data subset (e.g., models in the Good cluster trained over Good data, m.m.), and used the VIP performance characterization technique from Sec. 3.2 to identify which model best exemplified the characteristics of each proficiency group: Good-LSTM trained over Good data with a window size of 0.2s; Medium-GRU trained over Medium data with a window size of 0.3s predicting 0.1s into the future; Bad-MLP trained over Bad data with a window size of 0.5s. That each exemplar used a different architecture also suggests that the human subjects exhibited different strategies in performing the MARS task, to different effects.\nModels trained over MARS data were found to also exemplify characteristics of different proficiencies when compared to participants in the VIP task, suggesting a level of generalizability between the two different environments. The selected architectures were then retrained using data from 3 VIP participants of each proficiency group to produce digital twins of VIP pilots. Table 1 shows the performance characteristics of each pilot exemplar model. All other models that were trained over Good MARS data were reserved to act as candidate assistants, for a total of 21.4"}, {"title": "5 EVALUATION", "content": "We perform two evaluations: 1) A high-throughput evaluation of pilot digital twins in co-performance of the VIP task with candidate assistants; 2) A human subject study of human co-performance of the VIP task with the best-performing assistants from the digital twin study.\nThe 4 major components of the evaluation pipeline include: 1) VIP 2) Crash predictor; 3) Assistant; 4) Pilot. The VIP component is PyVIP, a Python implementation for easy integration of ML models. The Crash predictor is a trained instance of the best crash prediction architecture reported in Wang et al. [58]: a stacked GRU trained over inputs like those in Sec. 4 that predicts the likelihood of a crash occurring. Due to the crash predictor's high false positive rate, we added a crash probability threshold of 0.8 where only highly imminent danger would permit assistant suggestions.5 The assistant would provide suggestions when either 1) crash probability is greater than the threshold and angular distance from the DOB exceeds 12\u00b0, or 2) angular distance from the DOB exceeds 15\u00b0. The Assistant observes task performance and makes suggestions when certain conditions are met. The Pilot may be a digital twin or an actual human that controls the VIP. We used 6 digital twins-each of the architectures mentioned in Sec. 4.2.1, trained over both MARS and VIP data. Humans control the VIP with a joystick."}, {"title": "5.1 Digital Twins Study", "content": "In these experiments, the pilot has an 80% probability of accepting and executing an assistant suggestion instead of its own next action. If accepted, the pilot makes suggested deflections with a noise of U(-.05, .05) added to simulate human imprecision, after a.4 +\nU(-.05, .05)s delay to simulate reaction time.6\nWe ran each evaluation for 3 30-sec. trials. Data was sampled at 200 Hz with each sample comprising of the angular position and velocity of the VIP, joystick deflection, crash probability, pilot and assistant's joystick deflections, which entity's deflection was performed, and whether the deflection made was destabilizing. 468 individual digital twin trials were collected, or 3.9 hours of data."}, {"title": "5.1.1 Results", "content": "Table 2 shows performance differences between the digital twins when unaided and when aided by different assistants. Following the performance evaluation from Sec. 3.2 (where lower metric values signal improvement), SAC-AIRL is the overall strongest assistant for digital twins, decreasing crashes, % destabilizing deflections, and RMS velocity to a statistically significant level (all p < 0.0001 according to a paired two-tailed t-test).\nRL models are generally better assistants than DL models over human data. Interestingly, MLP-GMB-0 (MLP trained over all proficiencies with no window) decreased crashes as much as SAC-AIRL,"}, {"title": "5.2 Human Subject Study", "content": "Results from the high-throughput digital twins setting indicated that the 5 assistant models shown in Table 2 had a statistically significant effect on one or more metrics in co-performance with digital twins trained over both MARS and VIP data. These models-3 RL-based models and 2 models trained over human data-were included as candidate assistants in the human subject study. This gave us a robust but tractable sample of assistants to assess in co-performance of the VIP task with real human subjects.\nWe recruited 20 healthy adult subjects (6 female, 13 male, 1 non-binary). Each subject participated in 2 experimental sessions separated by approximately one week. In Session 1, subjects 1) attempted to balance a 50% coherent PyVIP RDK (3 \u00d7 30s trials); 2) controlled the RDK with assistance from an AI model, rendered as left/right arrows indicating the direction of suggested deflection (3 \u00d7 30s); 3) watched the same AI control the RDK while providing directional suggestions via the joystick (3 \u00d7 30s). Participants were randomly assigned one of the candidate assistant models during Session 1-subjects were grouped into fours and each group received assistance from a single type of architecture. Subjects were not told which type of model they were receiving assistance from. Between sessions, each assistant model was fine-tuned using data from Task 3 in Session 1. Episodes (consisting of input window and predicted action) where the direction of agent-predicted deflection conflicted with the direction of human deflection were stored. These human-in-the-loop (HITL) disagreement samples were used to fine-tune the model: the actor networks of the SAC and DDPG were fine-tuned using behavior cloning over the new data, the SAC-AIRL model was updated using AIRL over the new data, and the deep learning models underwent standard fine-tuning.\nIn Session 2, subjects 1) undertook Task 1 as in Session 1 (solo RDK balancing-3 \u00d7 30s); 2) undertook AI-assisted balancing as in Session 1 Task 2 but with a different assistant model (3 \u00d7 30s); 3) undertook AI-assisted balancing with the version of their Session 2 Task 2 assistant fine-tuned with data from Session 1 subjects who interacted with that model type (3\u00d730s). In Session 2 Task 2, subjects were assigned a non-fine-tuned model of a different architecture and in Session 2 Task 3 they were given an instance of that same model fine-tuned with HITL data from Session 1. Participants were not informed that the Session 2 Task 3 model was fine-tuned on Session 1 human data.\nFinally, subjects took a survey, based on Muir [28], about their solo performance, how AI assistance changed their performance, and the level of trust they had in the assistant from each task."}, {"title": "5.2.1 Results", "content": "We first assessed whether subjects displayed any adaptation to the balancing task within or across sessions that might confound apparent performance improvements due to AI assistance. Following Vimal et al. [51], in which participants in the disorienting MARS task showed minimal learning across consecutive days, we take performance in Session 1 Task 1 vs. Session 2 Task 1 (which were separated by approximately 1 week) as a baseline \"no learning\" condition in which participants lost familiarity with the task. We then compare performance differences between Session 1 Task 1 vs. Session 2 Task 1 and between Session 2 Task 1 vs. Session 2 Task 2. If the performance differences between Session 2 Task 1 and Session 2 Task 2 are similarly non-significant compared to the performance differences between Session 1 Task 1 and Session 2 Task 1, this indicates that no significant adaptation to the task occurred between tasks within Session 2, and likewise is unlikely to have occurred between Session 2 Task 2 and Session 2 Task 3; therefore apparent differences in Session 2 Task 2 and Session 2 Task 3 performance are likely to attributable to the nature of the AI assistance received.\nWe computed a score for each participant in each task of interest, given by Eq. 2,\ns = (\\frac{60 - C}{60})+(1-\\frac{p_D}{100})+(1-\\frac{p_A}{100})+((\\frac{R}{maxr})-(\\frac{C}{maxc})), (2)\nwhere C is the count of crashes over the task (3 \u00d7 30s trials), pD is the percentage of deflections that were destabilizing, pA is the percentage of deflections that were anticipatory (see Fig. 2), R is the task-level count of recoveries from beyond 20\u00b0 away from the DOB to within 20\u00b0 of the DOB, and maxr and max are the maximum number of recoveries and crashes in the data, respectively.\nBecause these scores are not normally distributed, we ran a Wilcoxon Signed-Rank test between Session 1 Task 1 and Session 2 Task 1 scores, and between Session 2 Task 1 and Session 2 Task 2 scores. No statistically significant differences were found between either pairing, with similar p-values (.2627 between Session 1 Task 1 and Session 2 Task 1, and .3681 between Session 2 Task 1 and Session 2 Task 2). This indicates that there was no adaptation to the task significant enough to confound performance differences attributable to AI assistance.\nFig. 5 shows the absolute difference in performance metrics between human solo VIP performance and 3 versions of AI-assisted performance for each model type: using the original model weights in Session 1 (5a), the non-fine-tuned assistant from Session 2 (5b), and the Session 2 assistant fine-tuned on data from humans in Session 1 who interacted with the assistant of the same architecture (5c). We see that although, like in the digital twin studies, the RL assistants in Session 1 were better at reducing the absolute number of crashes than DL assistants, this distinction often vanished or reversed after the assistant models were fine-tuned on participant data and then re-evaluated in Session 2. In Session 2 Task 3, the DL models fine-tuned on Session 1 data were often now better on average at reducing metrics associated with velocity and oscillation such as RMS velocity, velocity magnitude, and standard deviation of position. This effect is absent in Session 2 Task 2, where subjects were assisted by a different type of model."}, {"title": "6 DISCUSSION", "content": "Like in the digital twins study, the SAC-AIRL assistant often helped the human subjects reduce crashes and oscillations. This is more pronounced in versions fine-tuned on HITL data, suggesting that models with a more human-like strategy contribute to this effect.\nAssistance from the DDPG shows a strong tendency to increase RMS velocity and velocity magnitude values, and this is actually more so after HITL fine-tuning. This discrepancy is reflected in the number of disagreement episodes logged for each model type (Table 3). Human subjects registered a much higher mean number of disagreements with the DDPG model\u2014and RL models more generally-than with the DL models. This further indicates that the DDPG and RL models behave in ways that may contradict human intuition and/or physical instinct. Through their data and training, DL models are embodying the problem space and performing the task in a more human-like way, including transitioning from destabilizing to corrective and anticipatory deflections at distances from the DOB that align with human behaviors (cf. Fig. 3).\nDue to different human reaction times, it is not possible to know exactly when human subjects followed assistant suggestions, but we can calculate a heuristic estimate based on instances where a subject deflects in the direction suggested by the AI within a threshold of"}, {"title": "7 CONCLUSION AND FUTURE WORK", "content": "In this paper, we established a novel task of AI assistance in helping humans maintain balance in a disorienting condition. We first explored the space of possible AI assistance models using a high-throughput digital twins setting. The top performing models from this experiment were then used in a human-subject study to assess both performance impact and participants' attitudes toward different assistants.\nGiven certain data and training methods, Als that were capable of performing IP balancing alone were also able to assist real humans in reducing crashes and oscillation. SAC-AIRL, which learns rewards implicit in human performance data, appeared to be an effective disorientation countermeasure in both the digital twins and human subject studies, by apparently embodying the problem space in a way that incorporates both physics and human signals. Although RL models on average make better assistants than DL models trained over human data, they do so by suggesting actions that often diverge significantly from the apparent model of the task captured in human actions. In the human subject studies: human subjects empirically perceived the RL models as performing the task incorrectly, and models that learned and embodied a human-like strategy through pretraining over human data, then were fine-tuned over more human data in the subject study, were able to significantly reduce factors related to oscillation and velocity.\nPalmer et al. [32] illustrate trust dimensions in the use of autonomous or automated systems, such as robustness (handling perturbations/deviations appropriately), benevolence (supporting mission and operator), and dynamism (negotiating changes in environment). While our assistants' suggested actions may be appropriately corrective (benevolent), respond to pilot-induced perturbations such as ignoring cues (robust), and transfer between the MARS and VIP tasks (dynamic), they need to also be understandable in terms of the pilot's internal model of the situation to avoid corrections directly opposed to what the pilot expects (cf. the DDPG vs. MLP and LSTM in Fig. 5c). Our findings indicate that humans are in fact more receptive to assistance from an AI that demonstrates a more human-like, even if objectively suboptimal, balancing strategy.\nFuture study may investigate fine-tuning on data from a specific participant rather than an aggregate sample, to uncover person-specific patterns in task performance, or an investigation of modeling techniques that can account for the fact that human behavior is likely to change over time to account for assistance received from an Al agent, including one which is trained in real-time using live human feedback. Subsequent research may also involve transfer to more complicated conditions, like orientation in multiple roll planes or flight simulators, as well as investigating the transfer of AI assistance in the high-throughput VIP to the physical MARS.\nThere also remains the question of how to deliver an AI assistant's cues to a human pilot. In this work we rendered visual indicators on the screen, but other modalities may include aurally rendered tones or vibrotactile cues (as in [54]) to indicate the direction and magnitude of the corrective action, or linguistic instructions. In a previous experiment Mannan and Krishnaswamy [26] presented evidence toward the utility of language understanding in task performance, and a multi-variable examination of intervention method and timing is another avenue of future study."}]}