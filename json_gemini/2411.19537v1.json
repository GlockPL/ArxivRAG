{"title": "Deepfake Media Generation and Detection in the Generative AI Era: A Survey and Outlook", "authors": ["Florinel-Alin Croitoru", "Andrei-Iulian H\u00eeji", "Vlad Hondru", "Nicolae C\u0103t\u0103lin Ristea", "Paul Irofti", "Marius Popescu", "Cristian Rusu", "Radu Tudor Ionescu", "Fahad Shahbaz Khan", "Mubarak Shah"], "abstract": "With the recent advancements in generative modeling, the realism of deepfake content has been increasing at a steady pace, even reaching the point where people often fail to detect manipulated media content online, thus being deceived into various kinds of scams. In this paper, we survey deepfake generation and detection techniques, including the most recent developments in the field, such as diffusion models and Neural Radiance Fields. Our literature review covers all deepfake media types, comprising image, video, audio and multimodal (audio-visual) content. We identify various kinds of deepfakes, according to the procedure used to alter or generate the fake content. We further construct a taxonomy of deepfake generation and detection methods, illustrating the important groups of methods and the domains where these methods are applied. Next, we gather datasets used for deepfake detection and provide updated rankings of the best performing deepfake detectors on the most popular datasets. In addition, we develop a novel multimodal benchmark to evaluate deepfake detectors on out-of-distribution content. The results indicate that state-of-the-art detectors fail to generalize to deepfake content generated by unseen deepfake generators. Finally, we propose future directions to obtain robust and powerful deepfake detectors. Our project page and new benchmark are available at https://github.com/CroitoruAlin/biodeep.", "sections": [{"title": "1 INTRODUCTION", "content": "Deepfake media comprises image, video or audio files that are digitally altered or generated from scratch with AI tools in order to impersonate real or non-existent people. The recent groundbreaking progress of generative AI methods [1]\u2013[6] has enabled the creation of realistic deep-fake media with very little effort [7]-[18]. Unfortunately, the generated deepfake media can be used by scammers to spread misinformation on social media platforms to achieve large-scale political manipulation, and to deceive individuals or companies into financial frauds.\nIn an age where misinformation can quickly spread through social media platforms, deepfakes pose a critical threat to public trust and democracy, especially due to their growing online exploitation. A recent analysis of the fraud trends indicates that the number of fraud cases based on deepfakes registered a 10\u00d7 increase in 2023, with respect to 2022\u00b9. Another recent study found that about 70% of people are unable to distinguish between a real and a deepfake voice\u00b2. The growing quality and quantity of deepfakes raise significant concerns, particularly regarding online fraud and manipulation. To prevent the spread of deepfake media, researchers have developed a broad range of unimodal [19]\u2013[23] or multimodal [24]\u2013[26] methods for deepfake detection. However, deepfake detectors trained on media generated with a certain set of AI tools typically fail on deepfakes generated with a distinct set of tools [20]\u2013[22]. This has led to a relentless race to develop more powerful and robust deepfake detectors.\nTo this end, we conduct a comprehensive survey on the recent developments in deepfake media generation and detection. We first define a set of deepfake categories, which are determined based on the procedure used to generate the deepfake content. We identify both domain-agnostic and domain-specific deepfake types, and explain what kind of deepfake media belongs to each category. We next build a taxonomy of deepfake generation and detection methods, which creates a multi-perspective hierarchical categorization based on the considered media types, the employed architectures and the targeted tasks. As shown in Figure 1, we first divide contributions by task, into generation and detection. For each task, we identify the employed architectures. For deepfake generation, we find that the most popular architectures are Generative Adversarial Networks (GANs) [8], [14]\u2013[16], [27], [28] and denoising diffusion models [11]\u2013[13], [18], [29]\u2013[31]. To detect deepfakes, the"}, {"title": "2 RELATED SURVEYS", "content": "Several attempts have been made to survey deepfake detection and generation. In Table 1, we gather related surveys and illustrate the tasks, domains, methods and other aspects covered by the gathered surveys. Some surveys only cover the generation part [43], [45], while others are particularly focused on detection [40], [41], [44], [50]. Many surveys consider only one input media type, e.g. video [40], [42], [43], [45] or audio [44], [50]. There are a few surveys [41], [46], [47] that cover all media types (image, video, audio and multimodal), but only Masood et al. [46] and Patel et al. [47] address both detection and generation tasks. Although the surveys of Masood et al. [46] and Patel et al. [47] are comprehensive, they do not cover the most recent developments, such as diffusion models and vision transformers.\nIn summary, we find that existing surveys are either outdated or limited in terms of coverage, including only specific tasks (generation or detection) or media types (image, audio or video). In contrast, we conduct an extensive survey of current literature, covering both generation and detection, as well as all deepfake media types. Moreover, we create a multi-level taxonomy to ease the navigation through the current deepfake literature, providing direct links to the referenced papers. To our knowledge, our survey is the first to propose a novel benchmark to test the generalization capacity of deepfake detectors to out-of-distribution data."}, {"title": "3 DEEPFAKE TYPES", "content": "To date, a number of alternative procedures have been employed to generate deepfakes. In order to simplify the task of producing realistic deepfakes, one commonly used procedure is to only alter a certain aspect of a media file, e.g. modifying the identity of a person in an existing video, or changing the emotion of a speech, while preserving the speech content and the speaker's identity. By employing recent and powerful generative models [3], [52], generating deepfake content from scratch has also become prevalent. We further categorize the deepfake content according to the procedure employed to obtain the respective deepfake type. We illustrate the identified categories in Figure 2 and present them in detail below. Interestingly, we identify deepfake categories that are domain-agnostic (which have been applied to all media types) and domain-specific (which have only been applied to a certain media type).\nIdentity swapping. Deepfakes based on identity swapping imply replacing the identity information of a target person with that of a source person, while preserving identity-agnostic attributes, such as facial expressions. In the visual domain, this kind of deepfake is often referred to as face swapping, while in the audio domain, it is known as voice conversion or voice swapping. Voice conversion seeks to change the timbre and prosody of a speaker with those of another speaker, while preserving the content of the speech.\nExpression/emotion swapping. In contrast to identity swapping, expression or emotion swapping involves altering the facial expression/emotion without changing the identity information. In the image domain, this task is known as facial expression swapping. In the video domain, the task is also known as face reenactment, and it implies altering the facial movement, which might often require facial motion capturing technology. In the audio domain, emotion swapping is the task of changing the emotion of speech, while retaining the speech content and the speaker's identity.\nFacial attribute manipulation. Producing deepfake image"}, {"title": "4 DEEPFAKE GENERATION", "content": "In Figure 1, we first divide deepfake research by task, into deepfake generation methods and deepfake detection methods. We organize our literature review according to this split, discussing generative methods in the current section. We find that the most prominent approaches for deepfake generation are based on GANs or diffusion models. In some domains, such as video and audio, transformer-based methods are also very popular. Less frequently encountered methods are based on Variational Autoencoders (VAEs), Neural Radiance Fields (NeRF), 3D Morphable Models (3DMMs) and CNNs. Some models are only applied to specific media types, e.g. NeRF and 3DMMs are typically applied in the video domain. A number of studies use hybrid models, combining GANs with VAEs on the one hand, or CNNS with RNNs on the other. We further structure our presentation according to the media type. For each media type, we divide the surveyed studies according to the underlying architectures. Moreover, we provide tutorials for the most important generative frameworks in the supplementary."}, {"title": "4.1 Image", "content": "4.1.1 GAN-based methods\nFace synthesis. Creating realistic faces is essential for deep-fake generation, and GANs are widely employed to achieve this [5], [6], [54]\u2013[60]. Shen et al. [54] introduce a third model into the traditional adversarial framework, tasked with determining whether the generated images retain the identity from a reference image. This approach enables the method to perform conditional generation. Conditional generation is also the focus of Xu et al. [60]. Their approach synthesizes high-quality 3D heads with control over the camera poses and other facial attributes. StyleGAN [56]\u2013[58] improves the quality of the synthesized images by changing the generator architecture, and leveraging a mapping network to map the usual Gaussian vector to an intermediary latent space. The generative network adopts these new latents at different scales in the architecture via AdaIN [61] layers. Fu et al. [59] demonstrate that StyleGAN is also effective for generating full body images. Sauer et al. [5] extend the StyleGAN model, presenting a method that leverages Projected GAN training [62], progressive growing and classifier guidance [63], unlocking image synthesis at a resolution of 1024\u00d71024. Different from StyleGAN, Esser et al. [6] utilize GANs to learn a perceptually rich codebook, representing images as a sequence of codebook entries. This approach enables the use of transformers for training.\nFace swapping. One of the most widely-used methods for generating deepfakes is face swapping. This technique involves replacing the face in a target image with that of another individual, sourced from a different image. The key challenge lies in seamlessly integrating the source face, while maintaining non-identity-specific attributes, such as facial expressions and lighting conditions. Thanks to their well-known capacity of generating realistic images, GANs [7]\u2013[10], [27], [55], [64]\u2013[69] are widely adopted in face swapping frameworks.\nIn GAN-based face swapping pipelines, the generator is usually conditioned on identity information from the source image and the attributes extracted from the target image [9], [10], [65], [66], [68]\u2013[73]. The work of Bao et al. [65] is one of the earliest contributions in this direction. The authors employ a face recognition model to extract an identity embedding from the source image. The attributes of the target image are extracted by a neural network trained to minimize the Euclidean distance between the target and generated images, applying a lower weight when the identities in the target and source images differ. The concatenated representations are processed by a generator that is trained in an adversarial setting. Li et al. [69] advance the previous framework by introducing a multi-level attribute encoder that is trained in a self-supervised manner,"}, {"title": "4.1.2 Diffusion-based methods", "content": "Text-to-image. Diffusion models are effectively applied in text-to-image generation [2]\u2013[4], [89], utilizing large language models to encode textual descriptions that guide image creation. This capability allows users to generate counterfeit content featuring public figures simply by including their names in the text description used as input for generation. One of the most popular methods for text-to-image generation is Stable Diffusion [3], which leverages the latent space of a vector quantized (VQ) GAN [6] to perform the diffusion processes. SDXL [4] scales up the Stable Diffusion architecture, improving the quality and text fidelity of the generated images.\nPersonalized generation. Although text-to-image diffusion models allow deepfake content generation of public figures, some results do not accurately replicate the identity of the person. Thus, these models might have limited application in deepfake generation. However, there is another direction of research [11]\u2013[13], [29], [30], [90]\u2013[110] focused on generating images that contain a specific identity or concept depicted in an image or a set of images given as input. Such methods are more likely to be employed in deepfake generation. We can distinguish these contributions into two main approaches, those that perform test-time fine-tuning [90], [94], [97], [100], [102], [111]\u2013[113] and those that leverage large-scale datasets and learn how to incorporate the additional images offline [11]\u2013[13], [29], [30], [91]\u2013[93], [95], [96], [98], [99], [102]\u2013[110].\nTest-time fine-tuning approaches use different components to integrate and learn the new identity. Some works introduce a new text token for the identity and learn to embed it [97], [100], [111]. Other approaches [94] either use low-rank adaptation (LoRA) [114] or directly fine-tune the weights of the denoising network [90], [102], [112], [113]. Overall, test-time fine-tuning methods yield impressive results in terms of identity preservation, but their main disadvantage is the expensive optimization, which significantly increases the generation time. To this end, many works address the efficiency issue, to some extent. For example, Chen et al. [113] try to incorporate the knowledge of multiple subject-specific models into a single model. Ruiz et al. [112] leverage a HyperNetwork architecture to predict the network weights from a face image. Subsequently, they use these weights as a starting point for test-time fine-tuning, reaching faster convergence than previous work [90]. Despite these advancements, test-time fine-tuning methods still suffer from high generation times.\nIn contrast to test-time fine-tuning methods, the approaches that harness offline training [11]\u2013[13], [29], [30], [91]\u2013[93], [95], [96], [98], [99], [102]\u2013[109] are faster in terms of generation time, but their primary issue is identity preservation. Therefore, solving the latter problem constitutes the priority of these works. Zhao et al. [12] propose an identity preservation loss for which they construct a better estimation of the original image given the predicted noise, at training time. The same idea is studied by Liu et al. [13], who improve the estimation even further. Peng et al. [92] employ an identity loss, but only for certain noise levels. Other methods [11], inspired by the GAN literature, employ the ArcFace model as identity embedding extractor for better identity representations. Similarly, Li et al. [109] improve representations by stacking multiple embeddings when multiple images are available. Lastly, Wang et al. [30] decouple the generation of background and identity-related content by training two separate denoising networks, out of which only one knows to generate images of a given person.\nTools. The most popular tools for personalized generation are LoRA-based variants of Stable Diffusion [3] and SDXL [4], that are specialized on particular public personalities, e.g. Elon Musk\u00b3 or Alan Turing\u2074. Different from these options, another powerful tool is Midjourney5. In contrast to the LoRA-based methods, Midjourney is not popular for personalized generation, but for text-to-image synthesis. However, given the quality of its generative results, Midjourney is a popular tool for creating counterfeit images."}, {"title": "4.1.3 Other methods", "content": "Unlike previous methods centered around generative models, some approaches rely on alternative techniques. Bitouk et al. [115] identify the closest match in terms of lighting and pose from a large set of face images, and perform face replacement using key point alignment. Korshunova et al. [116] use a multi-resolution CNN in the VGG feature space, aligning target and source images to minimize cosine distance between corresponding patches. Wang et al. [117] propose an encoder-decoder architecture with a 3D identity extractor and a Semantic Facial Fusion module to enhance resolution and preserve identity. In contrast, other works combine generative methods to produce higher-quality images. Bao et al. [118] introduce CVAE-GAN, a method which combines VAEs with GANs. The generator and the encoder are trained with an adversarial objective, but also with a mean feature matching objective and a pixel-wise reconstruction loss, respectively. Li et al. [119] merge diffusion models and GANs by representing identity in Stable Diffusion through the latent space of StyleGAN, integrating latent codes into the U-Net via cross-attention layers."}, {"title": "4.2 Video", "content": "4.2.1 GAN-based methods\nThe early works for generating deepfake videos employ conventional GAN models for face swapping and reenactment, which are applied frame by frame [14], [120]. Nevertheless, these methods are usually part of more complex frameworks which have zero-shot capabilities, either involving more steps [14] or enhanced architectures [120]. Gao et al. [121] introduce a face reenactment GAN, focusing on generating videos of talking heads. The facial landmarks, expressions and head poses are extracted from both source and target frames to fit a face 3DMM and obtain predefined keypoints.\nTo depart from the conventional paradigm and improve the video generation using GANs, subsequent works leverage the latent space of StyleGAN2 [57]. A consistent number of methods divide the latent space in which they operate into two: one for content and one for motion [28], [122], [123]. While some utilize an RNN for sampling the motion trajectory [122], [123] and employ two discriminators, one for individual frames and one for the video sequence, Skorokhodov et al. [28] compute the motion embeddings with 1D convolutional layers and use only one video discriminator. Oorloff et al. [16] take a different approach by encoding both source and target frames, fusing their latent representations, then generating the output frame, while also utilizing multiple latent spaces [124], [125]. Yu et al. [126] treat videos as continuous-time signals and, with the help of an Implicit Neural Representation [127], they map an input signal (pixel coordinates and time) to RGB values in order to generate the corresponding video. Brook et al. [128] propose to generate multiple consecutive frames in low-resolution, and then increase their resolution with a super-resolution network. This approach ensures that training long video sequences is feasible."}, {"title": "4.2.2 Diffusion-based methods", "content": "Latent diffusion models [3] use a cross-attention mechanism that facilitates conditioning diffusion models for image generation. However, the main challenge in generating deepfake videos with diffusion models is employing a conditioning mechanism, while achieving temporal cohesion. The studies of Ho et al. [129] and Blattman et al. [130] represent the stepping stones in adopting diffusion models for video generation. Their methods extend diffusion models in several ways. The architectural changes applied on the U-Net mainly consist of replacing 2D convolutions with 3D convolutions, and appending additional self-attention layers for temporal attention. In a subsequent work, Blattmann et al. [131] demonstrate the benefits of using a large curated dataset for training a video generator. Wu et al. [132] introduce a one-shot method for editing a video given a text prompt. Inspired by Ho et al. [129], a text-to-image diffusion model is extended to an additional dimension (time) by Wu et al. [132], where the added self-attention layers operate on the current frame and the previous two frames.\nNewer diffusion-based video generation methods [133]\u2013[135] depart from the U-Net architecture and adopt a transformer-based one, namely ViT, which provides an innate mechanism for both spatial and temporal attention. This allows longer videos to be generated. Additionally, Guo et al. [134] introduce a plug-and-play module that can be integrated into a text-to-image diffusion model to induce the ability to generate videos. Inspired by this module, Wang et al. [136] present a method for text-to-video generation composed of several stages, in which a ControlNet is applied to improve guidance. Different from previous studies employing Stable Diffusion as the base model, Singer et al. [137] ground their work on DALLE-2 [138], while Ho et al. [139] utilize Imagen [89]. Nevertheless, similar architectural changes are implemented, where the network is extended to support the temporal dimension.\nAn important line of research is represented by portrait animation, in which a video is generated from a source frame and various conditional inputs. Most works in this area [140]\u2013[143] aim to apply a sequence of facial expressions over the image. Two different approaches are used to condition the diffusion model. One is based on intermediate representations of the facial expressions, such as facial key-points [140], [141], [143], and the other is based on directly encoding the frames containing the target facial movements [142].\nCurrently, the ability of the video generation methods based on diffusion modeling is not satisfactory, often requiring quality enhancements at a later stage in the pipeline. For example, super-resolution models are sometimes employed to increase video resolution [136], [137], [139], while the frame rate is usually increased through frame interpolation [131], [136], [137], [141]."}, {"title": "4.2.3 Other methods", "content": "Transformers represent the most popular architectural choice for video generation. For instance, Rochow et al. [144] leverage cross-attention blocks to guide the generation (using encoded facial keypoints and expressions), while Villegas et al. [145] apply the attention mechanism on frames to generate longer and coherent videos.\nAn alternative choice for video generation consists of employing some VQ autoencoder, either variational [146] or standard [147]. Similar to diffusion models, the generation process is carried out in the latent space of the autoencoder, which is lower dimensional. Within this vector space, a transformer is used to generate video tokens [147]\u2013[150]. Unlike other related approaches, Jiang et al. [149] carefully design the latent space such that it is decomposed into an appearance and a pose representation, respectively.\nA few methods harness the 3D space for face reenactment. In this context, warping is often employed, which involves computing a flow field between the source frame and the driving frame, then applying it on the former frame. In the same context, NeRF models [151] are used to generate novel views of 3D face models. For example, Thies et al. [152] obtain a coarse 3D representation from the source frame using a traditional graphics pipeline, and then feed it to a neural network to obtain a neural texture, a high-dimensional embedding space, from which a Deferred Neural Renderer (based on U-Net) generates the target image. Thies et al. [153] and Yang et al. [154] synthesize faces by applying a deformation transfer between two 3DMM-based intermediate representations of the source and driving video frames, the mouth being further refined through warping. Zhang et al. [155] also apply warping based on dense landmarks, while Li et al. [156] combine warping with NeRF. Finally, to increase the performance, some works adopt pre-training strategies that involve masking the input and then reconstruct the signal [145], [147]."}, {"title": "4.3 Audio", "content": "4.3.1 GAN/VAE-based methods\nA number of text-to-speech models employ popular generative frameworks, such GANs and VAEs, either alone or in combination with more recent developments in the field. Kim et al. [157] propose an end-to-end TTS framework that augments variational inference with normalizing flows and uses an adversarial training procedure to enhance the representation potential. The method of Casanova et al. [158] constructs on the previous model, introducing new procedures, such as the concatenation of language embeddings with the ones of the input characters to allow training in a multilingual fashion.\nIn [159], the authors introduce new modules to develop NaturalSpeech, another VAE-based TTS. A differentiable durator is used to improve the duration prediction, a memory mechanism simplifies the waveform reconstruction, and a bidirectional prior/posterior module improves the prior from text, while simplifying the posterior from speech.\nLee et al. [160] present a GAN-based vocoder that improves the generator by introducing anti-aliased feature representation and periodic non-linearities, delivering state-of-the-art results and robustness for out-of-distribution scenarios, such as novel languages and speakers."}, {"title": "4.3.2 Transformer-based methods", "content": "A few recent methods employ transformers to obtain competitive generation performance. FastSpeech [161] introduces a transformer-based model that speeds up speech synthesis by parallelizing Mel-spectrogram generation through a feed-forward architecture. A length regulator is used to match the length of the hidden states with the length of the Mel-spectrograms, and a duration predictor provides the duration for the phonemes. Jiang et al. [162] reuse the length regulator and the duration predictor from FastSpeech, adding separate modules for content, timbre and prosody modeling. A global timbre encoder is used to extract a global timbre vector, while a latent code language model fits the prosody distribution.\nWang et al. [163] proposed VALL-E, a framework that uses intermediate representations consisting of audio codec codes instead of Mel-spectrograms. A pre-trained neural codec model generates acoustic codes that are used alongside corresponding phoneme sequences during training, allowing the neural language model to extract both speaker information and content. SPEAR-TTS [164] removes the necessity to supply the transcripts of audio prompts by decoupling the generation of semantic tokens and the acoustic tokens.\nYang et al. [165] introduce UniAudio, a hierarchical transformer framework that learns both inter-frame and intra-frame correlations separately, reducing the computational complexity. It supports the generation of multiple types of audio by employing LLM-style next token prediction and tokenization via a universal neural codec."}, {"title": "4.3.3 Diffusion-based methods", "content": "Following the success of diffusion models in vision [1], several generation methods adopted the diffusion modeling framework to generate deepfake audio. Huang et al. [166] present FastDiff-TTS, a conditional diffusion model that follows the architectural design proposed by Ren et al. [167]. The authors employ time-aware location variable convolutions for long-term dependency modeling and a noise schedule predictor for sampling acceleration. ProDiff [168] is another framework with an architecture inspired by Ren et al. [167], which uses a denoising model with a parametrization that directly predicts the clean data, halving the number of diffusion steps via knowledge distillation.\nThe audio encoder/decoder, the phoneme encoder and the duration and pitch predictors proposed by Tan et al. [159] are reused in NaturalSpeech 2 [169], alongside a diffusion model that learns to predict latent representations conditioned on the input text. To promote zero-shot generation, a speech prompting mechanism helps the diffusion model and the duration and pitch predictors to follow prosody, style and speaker identity from the supplied audio prompt. The encoder/decoder and the duration predictor are further used in NaturalSpeech 3 [170], where, in contrast to previous studies [159], [169], each of the following speech attributes are independently generated by a novel factorized diffusion model: duration, content, prosody and acoustic details.\nDu et al. [171] introduce UniCATS, a framework capable of performing speech editing tasks, where speech is synthesized by taking into account both preceding and following contexts. UniCATS can achieve this with a contextual VQ-diffusion-based acoustic model and a contextual vocoder.\nYang et al. [172] aim to solve learning problems specific to expressive TTS, a novel task that tries to control the speaking style of the synthesized speech. The proposed framework uses ROBERTa [173] to extract the style representation from a natural language prompt."}, {"title": "4.4 Multimodal", "content": "4.4.1 Transformer-based methods\nRecent advancements in talking face generation focus on improving the synchronization of facial movements with speech, while maintaining natural motion and emotional consistency [174]\u2013[177]. These approaches address challenges such as lip-sync accuracy [175]\u2013[177], motion stability [174], [177] and speaker-specific styles [174], [175], aiming for realistic human-video synthesis. Jang et al. [174] introduce a system that combines talking face generation with TTS, addressing the challenge of generating natural head poses and maintaining consistent speech patterns even with varying facial motions. Their approach leverages a motion sampler and a conditioning method to ensure fluidity in both aspects. Building on the idea of synchronizing audio and visual elements, Cheng et al. [175] propose VideoReTalking, a method designed to edit real-world talking head videos for perfect lip-sync and emotional consistency. In a similar fashion, Wang et al. [176] develop a one-shot talking face generation framework. They introduce an audio-visual correlation transformer, which improves lip-sync accuracy by mapping audio to dense motion fields through phoneme and keypoint representations. Addressing another challenge in the field, Ling et al. [177] tackle the issue of lip motion jitter in speech-driven talking face generation. Their solution, StableFace, identifies key problems such as noise in the 3D face representation and mismatches between training and inference stages. To address emotion-agnostic talking head generation, Gan et al. [178] propose emotional adaptation for audio-driven talking-head. The method enhances emotion-agnostic talking-head models by adding three lightweight adaptations: deep emotional prompts, an emotional deformation network, and an emotional adaptation module."}, {"title": "4.4.2 Diffusion-based methods", "content": "Several frameworks are designed to generate high-quality audio-driven portrait animations, aiming to achieve realism and synchronization [18], [31], [179]\u2013[183]. AniPortrait [179] transforms audio into photorealistic animations by extracting 3D facial meshes and head poses, allowing for flexible facial motion editing. Building on this, V-Express [180] focuses on precisely synchronizing lip movements with audio, while maintaining control over facial identity and background through progressive training techniques. EchoMimic [181] offers another solution by integrating audio and facial landmarks using a denoising U-Net architecture, which stabilizes and enhances the natural flow of portrait videos. Similarly, Stypu\u0142kowski et al. [31] propose an autoregressive diffusion model to achieve realistic talking heads with smooth, expressive movements and accurate lip-sync. Xu et al. [18] also use a diffusion-based framework to improve lip-sync accuracy and motion diversity, employing a hierarchical audio-driven visual synthesis module. In a similar direction, VASA [182] produces talking faces, capturing synchronized lip movements and dynamic expressions using a diffusion-based model in a latent facial space, enabling real-time interactions with high realism. Distinctly, EMO [183] generates expressive talking head videos without relying on 3D models, excelling in natural transitions and seamless identity preservation."}, {"title": "4.4.3 Other methods", "content": "Recent advancements in talking head generation leverage different models, such as NeRF [184], [185], GANs [186]\u2013[188], RNNs [189] [191], VAEs [192] or CNNs [193], to address the challenges of synchronization, realism, and efficiency. SyncTalk [184] is a NeRF-based approach which focuses on speech-driven video generation. SyncTalk enhances synchronization between lip movements, facial expressions and head poses by using a face-sync controller for precise lip-sync, a head-sync stabilizer for natural head movements, and a portrait-sync generator to integrate the generated head with the torso. Similarly, GeneFace++ [185] builds on NeRF technology to produce real-time talking face videos with arbitrary speech audio. By improving audio-lip synchronization using pitch contour analysis and incorporating a fast motion-to-video renderer, GeneFace++ offers a robust and efficient solution.\nIn the context of GANs, Text2Video [186] presents an approach for synthesizing videos directly from text, reducing reliance on audio-driven models. Using a phoneme-pose dictionary and a GAN-based architecture, the method achieves high-quality video synthesis with just one minute of training data. HeadGAN [187] is developed for head reenactment and editing from a single reference image. It integrates 3DMMs for real-time reenactment at approximately 20 FPS. Furthermore, it incorporates audio features for enhanced mouth movement accuracy. Wang et al. [188] introduce TalkLip, a speech-to-lip generation model that enhances lip-speech intelligibility by incorporating a lip-reading expert to penalize incorrect outputs.\nFor gesture generation, RNN-based frameworks such as hierarchical audio-to-gesture (HA2G) [189] introduce ways to generate co-speech gestures. HA2G extracts multi-level audio features using a hierarchical audio learner. Another RNN-based model [190] offers a real-time pipeline to generate personalized photorealistic talking-head animations. This model operates at over 30 FPS and follows a three-stage process: extracting deep audio features, predicting facial dynamics and head motions with an auto-regressive model, and rendering high-fidelity faces through image-to-image translation. Some RNN-based methods [194], [195] rely on audio-visual cues for realistic face synthesis. Peng et al. [194] present a speech-driven 3D face animation model that separates speech content and emotion using an Emotion Disentangling Encoder, while Zhong et al. [195] introduce a two-stage framework for audio-driven person-generic talking face video generation. Both approaches apply RNNs on top of CNN features. The CNN-based DisCoHead [193] offers an unsupervised approach to disentangling head motion from facial expressions. By applying geometric transformations to isolate head motion and using speech audio for facial expressions, DisCoHead efficiently generates realistic talking heads."}, {"title": "5 DEEPFAKE DETECTION", "content": "The second part of our taxonomy illustrated in Figure 1 comprises deepfake detection methods. The taxonomy clearly indicates that most deepfake detectors are based on CNN architectures. However, with the recent advent of vision and audio transformers, a large body of work on deepfake detection is now based on multi-head attention. To boost detection performance, a considerable number of studies employ hybrid models, combining CNNs with transformers or RNNs, respectively. Less prevalent architectures in deepfake detection are graph and recurrent neural networks. We organize our subsequent presentation of deepfake detection methods according to the input domain. The reviewed articles are further separated according to the employed architectures."}, {"title": "5.1 Image", "content": "5.1.1 CNN-based methods\nConvolutional nets are the most prevalent type of architecture for deepfake detection [19", "22": [196], "223": "."}]}