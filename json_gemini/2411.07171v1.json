{"title": "Anytime Sequential Halving in Monte-Carlo Tree Search", "authors": ["Dominic Sagers", "Mark H.M. Winands", "Dennis J.N.J. Soemers"], "abstract": "Monte-Carlo Tree Search (MCTS) typically uses multi-armed bandit (MAB) strategies designed to minimize cumulative regret, such as UCB1, as its selection strategy. However, in the root node of the search tree, it is more sensible to minimize simple regret. Previous work has proposed using Sequential Halving as selection strategy in the root node, as, in theory, it performs better with respect to simple regret. However, Sequential Halving requires a budget of iterations to be pre-determined, which is often impractical. This paper proposes an anytime version of the algorithm, which can be halted at any arbitrary time and still return a satisfactory result, while being designed such that it approximates the behavior of Sequential Halving. Empirical results in synthetic MAB problems and ten different board games demonstrate that the algorithm's performance is competitive with Sequential Halving and UCB1 (and their analogues in MCTS).", "sections": [{"title": "Introduction", "content": "Monte-Carlo Tree Search (MCTS) [13,7] is a search algorithm used for different sequential decision-making problems. It has been thoroughly studied within the context of game playing agents, but also seen use in other planning, optimization, and control problems [4]. The algorithm consists of four strategic steps, each of which can be implemented using a variety of different strategies [4,19]. Strategies for the selection step tend to use Multi-Armed Bandit (MAB) algorithms, which balance exploration (sampling actions that are less explored) with exploitation (more deeply searching actions that appear more promising).\nThe most commonly used selection strategy is UCB1 [2], which focuses on minimizing cumulative regret. Sequential Halving (SH) [12], which focuses on simple regret, may be argued to be a more suitable choice in MCTS [16,6]. Integrations of SH into MCTS have been described in research on partially observable games [15], variants of MCTS that take additional guidance from scores learned through online or offline learning [9], and the state-of-the-art Gumbel AlphaZero and MuZero [8], which also integrate deep neural networks."}, {"title": null, "content": "Running through the four strategic steps of MCTS once is referred to as an iteration, and MCTS typically runs multiple iterations, after which it returns a final decision (e.g., move to play or action to take). It is common to use either a time budget, where MCTS keeps running iterations until it runs out of time, or an iteration budget, where it runs a predetermined number of iterations. SH requires the number of iterations that can be executed to be known in advance, which means that MCTS using SH as selection strategy does not have the anytime property. The algorithm cannot be terminated at any arbitrary point in time and be expected to have the quality of its final decision smoothly increasing as processing time increases (barring pathological cases [14]). When dealing with known games (for which the average number of iterations per unit of time could be measured) and a fixed per-move time budgets, a reasonable approximation of a fixed iteration budget may be calculated and used. However, when dealing with particularly large and varied sets of games [18], automatically generated games that must be played quickly in the context of an evolutionary search [20], or agents that automatically manage their time in an intelligent manner [11,3], the lack of anytime property can be more problematic.\nThis paper proposes anytime SH: a MAB algorithm with the anytime prop-erty, which can be used as selection strategy in (the root node of) MCTS. Its design was heavily inspired by the original SH, with anytime SH essentially being the original SH turned inside out. While we leave formal analyses of bounds on regret for future work, empirical results in synthetic MAB problems as well as a diverse set of ten board games demonstrate that anytime SH performs compet-itively with UCB1 (or UCT in games) as well as SH (only used in root node in games) in practice, whilst in contrast to SH-retaining the anytime property."}, {"title": "Background", "content": "The Multi-armed Bandit (MAB) problem assumes an environment in which an agent repeatedly chooses one from a set of K arms to pull at time steps $t = 1,2,...$. Each of the arms is associated with a stationary, unknown probability distribution, and whenever the agent pulls an arm with index i at time t, it receives a reward sampled from the corresponding distribution with (unknown) mean $\u00b5_i$. The most common measure of performance in MAB problems is the cumulative regret $R(T) = \\sum_{t=1}^{T} (\u03bc^* \u2013 \u03bc_{i_t})$, which accumulates the regret of not having always picked the optimal arm (with the highest mean $\u03bc^*$) over a sequence of T time steps. The simple regret $r(T) = \u03bc^* \u2013 \u03bc_{i_{T+1}}$, which solely measures the regret of a single choice made after a period of T exploration steps, is an alternative measure of performance [1,5,10]. Simple regret is a more appropriate measure of performance when it is only an ultimate single decision that matters, with all prior decisions simply serving as a learning or training phase.\nUCB1 [2] is a common MAB algorithm, used as the selection strategy in the canonical UCT [13] variant of MCTS. It is designed primarily to minimize cumulative regret. At any give time step t, UCB1 selects an arm $j_t$ to pull"}, {"title": "Anytime Sequential Halving in Monte-Carlo Tree Search", "content": "using $j_t = argmax_{1 \\le j \\le K} \\bar{X_j} + C \\sqrt{\\frac{ln(\\sum_{k=1}^{K} n_k)}{n_j}}$, where $\\bar{X_j}$ is the average reward collected from previous pulls of arm j so far, C is a tunable hyperparameter, which affects the balance between exploration and exploitation, and $n_j$ is the number of times an arm j has been pulled so far.\nSequential Halving (SH) [12] is a different MAB algorithm, designed to opti-mize for simple regret rather than cumulative regret. It assumes prior knowledge of the budget of arm pulls (or time steps) T, as well as the fixed number of arms K. It calculates how many times the number of arms can be halved before nar-rowing down to a single arm as $B = [log_2 K]$, and operates in B separate phases. The total budget T gets spread out evenly over these B phases. Within every phase, the algorithm distributes the number of arm pulls uniformly over all re-maining arms (in the first phase, this is the full set of all K arms). After every phase, the worst-performing half of all arms get discarded. Algorithm 1 provides pseudocode for SH, and Fig. 1 gives a visual depiction of the algorithm.\nOutside of affecting rewards that are collected and information that is gath-ered by an agent, decisions made in earlier time steps have no effect on later time steps in MAB problems. In contrast, games are examples of sequential decision-making problems, where actions taken (moves played) in early states affect which states the agent transitions into. Monte-Carlo Tree Search (MCTS) [13,7] can handle this sequential nature by gradually building up and travers-ing through a search tree, with nodes representing different states, connected by the actions that lead to transitions between the states. When traversing the tree, MCTS views the problem of selecting a branch from each node it reaches as a MAB problem. After running many iterations, it will use the information it has collected from simulations to make a final decision as to which move to play. Because only this final move selection truly matters any choices made for tree traversal during the search itself only result in fictitious rewards it can be argued that algorithms that optimize for simple regret (such as SH) are more suitable than ones that optimize for cumulative regret (such as UCB1) in the root node [10,16,6]. Outside of the root node, the argument in favor of simple regret minimization still holds due to the minimax structure of zero-sum games. However, simultaneously there is an argument to be made in favor of cumulative regret minimization, as this leads to a more focused best-first search that wastes less time on seemingly uninteresting parts of the search space in particular for"}, {"title": "Time-Based and Anytime Sequential Halving", "content": "As an initial step towards an anytime variant of SH, we consider Time-based SH. Standard SH requires a budget of iterations to be known in advance, because it spreads this number of iterations evenly over all of its phases. If every iteration takes approximately the same amount of time, a simple but effective algorithm with a time budget could simply divide the time budget over the phases instead of the iteration budget. This is a trivial variant of SH, but not one that we have seen in prior literature. It does not have the anytime property, as it still expects the time budget to be predetermined.\nWe propose Anytime SH as an anytime variant of the algorithm, which works as follows. It starts operating like the standard SH, albeit with an assumed bud-get of iterations that is only the bare minimum required for a valid, \"complete\" run of SH. In the first phase it allocates exactly one iteration to each arm. Then it halves the arms, allocating two additional iterations to each remaining arm, and so on, until only a single arm remains. We refer to this as one pass of the"}, {"title": "Anytime Sequential Halving", "content": "Algorithm 2 provides pseudocode for the algorithm, and Fig. 2 visualises two passes of the algorithm."}, {"title": "Experiments", "content": "This section describes the experiments used to evaluate the novel Anytime SH algorithm.\u00b9 The algorithm is compared to appropriate baselines in synthetic MAB problems (Subsection 4.1), and in a set of ten different board games (Subsection 4.2). In the board games, the algorithm is not used standalone, but as selection strategy in the root node for MCTS."}, {"title": "Synthetic Multi-Armed Bandit Problems", "content": "For an evaluation of the raw MAB algorithms, without the added complexity of sequential decision making and combining with tree search in games, we con-struct a set of 100 synthetic MAB problems as follows. For every MAB problem, we sample ten different means $\u00b5_1,..., \u03bc_{10}$ for $K = 10$ arms from a normal dis-tribution with mean 0 and unit variance. Within each problem, when an arm i is pulled, a reward is drawn from another normal distribution with mean $\u03bc_i$ and unit variance. The following algorithms are compared on this suite of problems:\nUCB1: UCB1 [2] using an exploration constant $C = \\sqrt{2\\frac{-1}{t}}$, as described in Section 2.\nBase SH: standard Sequential Halving [12], as described in Section 2.\nTime SH: the trivial Time-based variant of SH, as described in Section 3.\nAnytime SH: the novel algorithm as described in Section 3.\nEvery algorithm was run on each MAB problem for up to 5000 milliseconds (or 186,500 iterations in the case of Base SH) using Python 3, on an i7-10700k Intel CPU.  For each time budget listed in Table 1 (or iteration budget for Base SH), the simple regret incurred by each algorithm running with that budget is recorded for each of the 100 problems, and aver-aged over those 100 problems. Fig. 3 depicts 95% confidence intervals for these averaged simple regret measures."}, {"title": "Board Games", "content": "The second experiment evaluates the performance of Anytime SH, when used as selection strategy in the root node of MCTS, in a set of ten different board games (listed in Table 2). Three different agents are considered in this experiment:"}, {"title": "Discussion", "content": "The results for the MAB problems in Fig. 3 show that Anytime SH performs on par with the naive time-based SH and the standard SH in terms of simple regret. In this setting, adjusting the algorithm to introduce the anytime property does not harm performance. UCB1 appears to have better performance for some bud-gets, which is somewhat surprising as this algorithm is not necessarily designed"}, {"title": null, "content": "to optimize simple regret. However, for many budgets, UCB1 has substantially greater variance in its performance level than the three SH-based algorithms do.\nWhen averaging results over all ten board games (Fig. 4), we find that (i) H-MCTS (which is not an anytime algorithm) tends to perform better than the other two algorithms, in particular for low iteration budgets, and (ii) anytime SH appears to be possibly weaker than UCT by a slight margin for low iteration budgets, but evenly-matched or slightly stronger for medium and large iteration budgets. At the level of individual games, these trends can differ (see Table 2). The results in the game of Gomoku stand out in particular, with H-MCTS having particularly high win percentages of around 75% against UCT for all budgets, and ranging between approximately 70% and 90% against Anytime SH. While a sample size of ten games (with 150 plays per game, per pair of agents) is no less than is customary in the literature, it is still small enough for a single game with relatively extreme results such as Gomoku to have an outsized effect on a plot that averages over the games like Fig. 4. Especially in the direct matchup against H-MCTS (the left subfigure), Anytime SH would be substantially closer to H-MCTS if the results for Gomoku were excluded (and, plausibly also if the experiment were extended with more other games, if Gomoku is an outlier)."}, {"title": "Conclusion and Future Work", "content": "This paper proposed Anytime Sequential Halving: an algorithm designed in an effort to approximate the behavior of the standard Sequential Halving (SH) algorithm, whilst having the anytime property. Experiments in a set of synthetic Multi-Armed Bandit problems show that Anytime SH performs on par in terms of simple regret with the standard SH, as well as a trivial time-based variant of SH, which can handle time-based budgets (but is not truly anytime). While the performance in terms of simple regret remains unchanged, Anytime SH does bring the benefit of having the anytime property. When used as selection strategy for the root node in Monte-Carlo Tree Search (MCTS) for game playing, Anytime SH appears to perform slightly below the Hybrid MCTS baseline (which has the downside of not being an anytime algorithm). Its performance level is competitive"}, {"title": null, "content": "with UCT, appearing to be possibly slightly worse for low search budgets, but slightly better for higher budgets.\nWe see potential for ample future work to further improve Anytime SH. The Anytime SH algorithm as proposed in this paper was kept simple and straightfor-ward. We expect that there is likely room to improve the algorithm, in particular by more carefully investigating how it should behave in situations where its or-dering of arms changes between different passes of the algorithm (a situation that cannot occur in the standard SH). When Anytime SH \"changes its mind\" about ordering of any pair of arms, one of them will have had fewer iterations so far than it should have had in hindsight (fewer iterations than the standard SH would have allocated if the current total number of iterations were the full budget). In our implementation, we ignore this, but it is worth investigating if correcting for this would be worthwhile. This could potentially make the algo-rithm more adaptive to the level of complexity of a problem than the standard SH is, which always halves sets of arms at specific intervals regardless of how many arms are close or far from each other in terms of expected rewards. Fur-thermore, it would be interesting to investigate how the algorithm interacts with different values of hyperparameters, such as the exploration constant still used for UCB1 in nodes below the root node in MCTS."}]}