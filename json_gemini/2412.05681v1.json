{"title": "LEVERAGING TIME-SERIES FOUNDATION MODEL FOR\nSUBSURFACE WELL LOGS PREDICTION AND ANOMALY\nDETECTION", "authors": ["Ardiansyah Koeshidayatullah", "Abdulrahman Al-Fakih", "SanLinn Ismael Kaka"], "abstract": "The rise in energy demand has made it important to have suitable subsurface storage., Achieving\nthis requires detailed and accurate subsurface storage characterization, which often relies on the\navailability and quality of borehole well log data. However, obtaining a complete suite of well-log\ndata and ensuring high data quality is costly and time-consuming. In addition, missing data in well\nlog measurement is common due to various reasons, such as borehole conditions and tool errors.\nWhile different machine learning and deep learning algorithms have been implemented to mitigate\nthese issues with varying degrees of success, they often fall short in capturing the intricate, nonlinear\nrelationships and long-term dependencies inherent in complex well log sequences. Furthermore,\nprevious Al-driven models usually require retaining when new datasets are introduced and are\nlimited to deployment in the same basin. Here, for the first time, we explored and evaluated the\npotential applications of the time-series foundation model, which leverages transformer architecture\nand a generative pre-trained approach to predict and detect anomalies in borehole well log data.\nHere, we fine-tuned and adopted TimeGPT architecture to analyze well log data. Our goal is to\nforecast different key log responses from various locations and detect anomalies in the logs with high\naccuracy. Our proposed model shows high-performance predictions, achieving correlation coefficient\n(R2up to 87%) and a mean absolute percentage error (MAPE) as low as 1.95%). In addition, the\nmodel's capability, with a zero-shot approach, to detect anomalies in these logs has proven capable of\nidentifying subtle yet critical deviations that could indicate drilling hazards or unexpected geological\nformations with an overall 93% accuracy. Our model has demonstrated a significant advancement in\nboth predictive accuracy and computational efficiency through its capability to perform zero-shot\ninference via a fine-tuning approach. Its application in well logs prediction not only enhances\noperational decision-making but also minimizes risks associated with subsurface exploration.", "sections": [{"title": "1 INTRODUCTION", "content": "Well logs are critical records in the oil and gas industry, capturing detailed measurements of subsurface properties\nalong the depth of a wellbore. Not only does it provide a mathematical and physical measurement or representation of\nthe rock properties in the subsurface, but also other surrounding information such as pressure or temperature can be\nrecorded and deduced from well log data. These logs are essential for characterizing geological formations, estimating"}, {"title": "2 GENERATIVE AI FOR WELL LOG ANALYSIS", "content": "The introduction of generative AI and transformer-based models for well-log data analysis and interpretation have\nopened new possibilities for not only generating synthetic data as pseudo-logs but also the efficiency, accuracy, and\nscope of well-log predictions. One of the first attempts at applying generative modeling to well-log data was through the\nautoregressive model. A study by [15] showed that the deep learning autoregressive model surpasses human accuracy\nand with a more consistent interpretation in performing multiple petrophysical evaluations. Another work by [15]\nleveraged conditional variational autoencoders for predicting sonic log responses and effectively considered uncertainty\ninherited from the measured data. The versatility of generative AI is further demonstrated by the work of [24]in\nperforming data imputation for missing well-log data and achieving high accuracy and matches with the geological\nfeatures. Furthermore, [1] have successfully implemented sequence-based GAN to provide realistic synthetic well\nlogs data and also fill the missing values with high accuracy. In addition, [2] enhanced anomaly detection in well log\ndata through the application of ensemble GANs.Recent works have also combined both transformer models and deep\nneural networks to enhance well-log interpretation and data processing, outperforming interpretation based on physical\nequations [18, 34]. While studies have highlighted the potential of generative AI, including time-series foundation\nmodels, in well-log data analysis, the applications remain limited. Hence, further exploration and implementation of\nsuch models are required to better understand the actual potential and limitations of generative modeling and foundation\nmodels for advancing well-log interpretation."}, {"title": "3 METHODOLOGY", "content": ""}, {"title": "3.1 Dataset", "content": "The datasets used in this study includes detailed well log from the North Sea Dutch region. The well logs include GR\n(Gamma Ray), DT (Sonic), NPHI (Neutron), and RHOB (Density) data, and cover a depth range of 1925-2065 meters,\nwith a total of 6553 data points. These wells were selected due to their extensive logging data, which is pivotal for\ntraining and evaluating accurate machine-learning models."}, {"title": "3.2 Computing Environment", "content": "The analysis was conducted on a powerful workstation with 4x RTX A5500 24GB and RAM of 128 GB at the AI\nlab, King Fahd University of Petroleum and Minerals, Saudi Arabia. Python and the NixtlaClient API were used\nfor forecasting and anomaly detection, along with a workflow specifically designed for handling time-series data for\nsubsurface applications."}, {"title": "3.3 Model Architecture", "content": "The proposed model architecture is developed by adopting and fine-tuning the TimeGPT architecture [10]. It is built\nupon the GPT-2 architecture, which consists of multiple layers of Transformer blocks whereby each block includes\nmulti-head self-attention mechanisms and position-wise feedforward networks [28]. The input to the model is a sequence\nof historical time series values, which are first embedded into a higher-dimensional space. Positional encodings are\nthen added to the embedded sequence to incorporate the temporal order of the data [31]. This is particularly useful in\ntime series forecasting, where understanding both short-term and long-term dependencies is crucial. Overall, the key\ncomponents of our proposed model are the following (Figure 1):\n1. Input Embedding Layer: It consists of two components, time series data embedding and positional encoding.\nThe embedding process is performed by transforming the time series data into a higher-dimensional space.\nThis involves converting each data point in the sequence into a dense vector representation, capturing its\ntemporal and contextual information. Since the transformer does not inherently understand the order of the\ninput data, positional encodings are added to the input embeddings. This step introduces information about the\nposition of each data point in the sequence, which is crucial for understanding the temporal relationships.\n2. Multi-Head Self-Attention Mechanism: The first procedure is to calculate the self-attention score. The\nself-attention mechanism allows the model to focus on different parts of the sequence when making predictions.\nEach element of the sequence is transformed into three vectors: Query (Q), Key (K), and Value (V). The\nmodel then calculates attention scores by taking the dot product of Q with K, normalizing them, and using\nthese scores to weigh the V vectors. This process helps the model focus on the most relevant parts of the\nsequence. Finally, our model uses multiple attention heads to capture different aspects of the time series. Each\nhead processes the data differently, allowing the model to consider various relationships within the sequence\nsimultaneously. The outputs of all attention heads are then concatenated and linearly transformed to form the\nfinal output.\n3. Feed-Forward Neural Network (FFN): After the attention mechanism, the output is passed through a\nfeed-forward neural network. This network consists of two linear layers with a ReLU activation in between.\nThe FFN processes each position in the sequence independently, further transforming the data and introducing\nnon-linearities.\n4. Residual Connections and Layer Normalization: To combat the vanishing gradient problem and help the\nmodel learn more efficiently, our proposed model includes residual connections that bypass the attention\nand feed-forward layers, adding the original input back to the output of these layers. In addition, after each\nattention and feed-forward block, layer normalization is applied. This helps stabilize the training process by\nnormalizing the outputs, ensuring that the scale of the data remains consistent.\n5. Output Layer: The final output layer predicts the next value(s) in the time series. This layer can be adjusted\nto output a single value (univariate forecasting) or multiple values (multivariate forecasting), depending on the\nspecific task.\n6. Conformal Prediction: Our model applies conformal prediction to provide uncertainty estimates through\nconformal prediction techniques. This allows the model to output prediction intervals, offering a confidence\nmeasure for the forecasts."}, {"title": "3.4 Evaluation Metrics", "content": ""}, {"title": "3.4.1 Well Log Prediction", "content": "The performance of the models was assessed using several key metrics: R2 (Coefficient of Determination), Mean\nAbsolute Error (MAE), Root Mean Squared Error (RMSE), Mean Squared Error (MSE), and Mean Absolute Percentage\nError (MAPE). These metrics provide complementary insights into the model's accuracy and predictive capabilities."}, {"title": "Correlation Coefficient (Pearson's Correlation Coefficient)", "content": "The correlation coefficient measures the strength and direction of the linear relationship between two variables:\n$R^2 = \\frac{cov(x, y)}{\\sigma_x \\sigma_y}$                                                                         (1)\nwhere cov(x, y) is the covariance between variables X and Y, and $\\sigma_x$ and $\\sigma_y$ are the standard deviations of X and Y,\nrespectively. It ranges from 0 to 1, where a higher R\u00b2 indicates a better fit, meaning the model explains a larger portion\nof the variance in the data."}, {"title": "Mean Absolute Error (MAE)", "content": "The mean absolute error measures the difference between the predicted and actual values. It provides an easy-to-interpret\nmeasure of prediction accuracy, with lower MAE values indicating better performance:\n$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y_i}|$                                                                                                 (2)\nIn which $y_i$ is the actual value, $\\hat{y_i}$ is the predicted value, and n is the number of data points."}, {"title": "Root Mean Squared Error (RMSE)", "content": "The RMSE penalizes larger errors more than MAE and is often used to compare different models:\n$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}$                                                                                      (3)"}, {"title": "Mean Squared Error (MSE)", "content": "The MSE provides the mean of the squared differences between actual and predicted values. Though it is less\ninterpretable due to its squared units, it is widely used as a loss function in model optimization:\n$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$                                                                                       (4)"}, {"title": "3.4.2 Well Log Anomaly Detection", "content": "To comprehensively evaluate the effectiveness of the models employed in this study, several key metrics were employed.\nThese metrics offer insights into various facets of the model's predictive accuracy and reliability, particularly in the\ncontext of anomaly detection in well log data. The following metrics were used:"}, {"title": "Accuracy (Acc)", "content": "Accuracy quantifies the proportion of correctly predicted instances (both true positives and true negatives) out of all\ninstances. It provides an overall indication of the model's correctness, as depicted in equation (5):\n$Accuracy = \\frac{True Positives (TP) + True Negatives (TN)}{Total Instances}$                                                                             (5)"}, {"title": "Receiver Operating Characteristic - Area Under the Curve (ROC-AUC)", "content": "This metric evaluates the model's capacity to differentiate between positive and negative classes across all potential\nthresholds. The ROC-AUC is represented by the area under the ROC curve, which plots the true positive rate against\nthe false positive rate. A higher ROC-AUC signifies superior model performance."}, {"title": "Matthews Correlation Coefficient (MCC)", "content": "The MCC offers a balanced measure that accounts for both true and false positives and negatives. It is particularly\neffective for assessing imbalanced datasets and provides a holistic assessment of binary classification quality, as depicted\nin equation (6):\n$MCC = \\frac{(TP \\times TN) - (FP \\times FN)}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$                                                                              (6)"}, {"title": "Confusion Matrix (CM)", "content": "The CM offers a comprehensive breakdown of true positives, false positives, true negatives, and false negatives. It\nserves as a crucial tool for visualizing and assessing the performance of a classification algorithm. The CM provides\ninsights into recall and precision, aiding in the evaluation of model performance."}, {"title": "4 RESULTS AND INTERPRETATION", "content": ""}, {"title": "4.1 Exploratory Data Analysis", "content": "Data analytics were conducted to understand the nature of the dataset and establish correlations between the input\nparameters. Based on the heatmap (Figure 2), key correlations between variables were revealed. A strong positive\ncorrelation (0.84) was observed between the RHOB and NPHI variables, followed by a moderate correlation (0.73)\nbetween the RHOB and GR. In contrast, the strongest negative correlation (-0.6) was observed between the NPHI and\nDT variables, as well as between the RHOB and DT (-0.55). The ILD variable exhibits weak correlations with others,"}, {"title": "4.2 Well-Log Prediction", "content": "In this study, various key log responses, including Gamma Ray (GR), Density (RHOB), Neutron (NPHI), Resistivity\n(ILD), and Sonic (DT), were evaluated to understand the potential applications of our proposed time-series foundation\nmodel. Among the different well-log data, the highest performance was obtained from forecasting GR, with the R2\nvalues achieving 87% (Figure 4 and Table 1). In contrast, the model did not perform well in forecasting the RHOB logs,"}, {"title": "4.3 Well-Log Anomaly Detection", "content": "Two different experiments were conducted for anomaly detection, using 90% and 99% confidence intervals. The model\ndemonstrated consistent accuracy across the five different log responses, achieving 92% for the 99% confidence interval\nand 89% for the 90% interval (Figure 5 and Table 2). It is also noteworthy that the ROC-AUC values were similar\nacross both log responses and confidence intervals, with values around 0.51 (Table 2)."}, {"title": "5 DISCUSSIONS", "content": ""}, {"title": "5.1 Time-Series Foundation Models in Geosciences", "content": "Generative AI models, including foundation models, are beginning to find applications in geosciences such as reservoir\nmodeling, seismic data augmentation, and production optimization [9, 13]. These models assist in analyzing large\nvolumes of geological and geophysical data, offering valuable insights to aid decision-making in reservoir engineering,\ndrilling, and production management. However, the specific application of foundation models in these domains remains\nunderexplored suggesting potential for future research.\nIn the broader context of time series forecasting, foundation models like TimeGPT have demonstrated superior\nperformance. Their ability to handle multivariate forecasting tasks and support exogenous variables makes them"}, {"title": "5.2 Limitations and Future Recommendations", "content": "While the time-series foundation model represents a significant advancement in the field of time-series forecasting, it is\nnot without its limitations. These challenges need to be addressed to enhance the model's performance and applicability\nacross different domains. This discussion will outline the current limitations of TimeGPT and provide recommendations\nfor future research and development.\nThe Transformer architecture, which underpins TimeGPT architecture, is known for its computational intensity,\nparticularly in terms of memory usage and training time. The self-attention mechanism in Transformers scales\nquadratically with the length of the input sequence, making it challenging to apply TimeGPT to long time series\ndatasets without significant computational resources [31]. Due to its high capacity, foundation models are susceptible to\noverfitting, especially when trained on small or noisy datasets. Although techniques like dropout and early stopping are\nemployed to mitigate this risk, overfitting remains a concern, potentially leading to poor generalization of unseen data\n[12]. This limitation is particularly relevant in non-stationary time series where patterns can change over time, and the\nmodel may struggle to adapt to these shifts.\nTo address the computational complexity of GPT-based time-series foundation models, future research could focus on\ndeveloping more efficient variants of the Transformer architecture. Techniques such as sparse attention mechanisms or\nlow-rank approximations could reduce the computational burden without sacrificing accuracy [29, 33]. To mitigate the\nrisk of overfitting and improve the model's adaptability to non-stationary time series, future iterations of time-series\nfoundation models could incorporate adaptive learning mechanisms. For example, integrating online learning techniques\nthat update the model as new data arrives could help the model maintain its performance in dynamically changing\nenvironments. Regularization strategies, such as ensemble learning or Bayesian methods, could also be explored to\nenhance the model's robustness against overfitting [20]."}, {"title": "6 CONCLUSIONS", "content": "The integration of time series forecasting and anomaly detection into well log analysis marks a significant advancement\nin subsurface exploration and drilling operations. These approaches enable more accurate predictions and provide early\nwarnings of potential issues, enhancing decision-making and reducing operational risks. The case studies highlighted"}]}