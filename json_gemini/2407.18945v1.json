{"title": "COGNARR ECOSYSTEM: FACILITATING GROUP COGNITION AT SCALE", "authors": ["John C. Boik"], "abstract": "Human groups of all sizes and kinds engage in deliberation, problem solving, strategizing, decision making, and more generally, cognition. Some groups are large, and that setting presents unique challenges. The small-group setting often involves face-to-face dialogue, but group cognition in the large-group setting typically requires some form of online interaction. New approaches are needed to facilitate the kind of rich communication and information processing that are required for effective, functional cognition in the online setting, especially for groups characterized by thousands to millions of participants who wish to share potentially complex, nuanced, and dynamic perspectives. This concept paper proposes the CogNarr (Cognitive Narrative) ecosystem, which is designed to facilitate functional cognition in the large-group setting. The paper's contribution is a novel vision as to how recent developments in cognitive science, artificial intelligence, natural language processing, and related fields might be scaled and applied to large-group cognition, using an approach that itself promotes further scientific advancement. A key perspective is to view a group as an organism that uses some form of cognitive architecture to sense the world, process information, remember, learn, predict, make decisions, and adapt to changing conditions. The CogNarr ecosystem is designed to serve as a component within that architecture.", "sections": [{"title": "1 Introduction", "content": "Consider a small group of people in a room who wish to understand each other's perspectives on some situation or problem. The group might also want to deliberate and collaboratively develop and decide upon a favored course of action. If discourse is reasonably healthy (e.g., honest and respectful), this type of small-group setting can allow participants to share rich, even nuanced and complex information about their beliefs, desires, intentions, concerns, and observations. In turn, rich communication can facilitate effective decision making. But how can rich information be shared in a group that numbers in the hundreds, thousands, or even millions of participants? In the large-group setting, which typically involves online interaction, how can each person's potentially complex, nuanced, and dynamic perspective be heard and understood by the group as a whole? The CogNarr (Cognitive Narrative) ecosystem, proposed here, seeks to answer these questions.\nParticipants of a group engage in a cognitive process that can involve sensing, deliberation, and decision making. One can attribute that process to individual group members, but also to the group as a whole. The distinction between cognition in individuals and in groups is not as crisp as once believed. Lyon and colleagues list thirteen hallmarks of basal cognition, including sensing, remembering, learning, anticipating (predicting), and problem solving. These hallmarks, listed later in full, are not unique to humans\u2014all thirteen appear in individuals across species, even in plants and unicellular organisms. And they are not unique to individuals they appear also in groups across species. Watson and Levin suggest that an organism can be defined as a collective of cooperating entities that displays a cognitive architecture. The collective might consist of cells, bees, or humans, for example. The organism's cognitive architecture integrates information and coordinates action\u2014the core functionalities of cognition-regardless of whether it is implemented by \u201cchemical, gene-regulatory, bio-electrical, neural, ecological, or social interactions.\" Although not specifically mentioned by the authors, we can add \u201celectrical\" to the list.\nThus, a coordinated group of humans in the CogNarr setting can be understood as a cognitive organism that employs a cognitive architecture. Moreover, CogNarr itself can be viewed as a component of that architecture. Experience tells us that the cognitive process of any particular group can range from highly functional to highly dysfunctional. The question at hand, then, is what CogNarr design would favor the former and discourage the latter?\nThis concept paper contributes a novel vision as to how recent scientific developments in cognitive science, artificial intelligence, natural language processing, and related fields might be scaled and applied to large-group cognition, using an approach that itself promotes further scientific advancement. That vision, the CogNarr ecosystem, is still in its incubation phase and the design of CogNarr is still incomplete. The vision is presented here as a first, introductory step forward. The CogNarr ecosystem is conceived to be an extensible set of technologies, resources, applications (apps), tools, libraries, and services that facilitates group cognition, especially for large groups, and especially with regard to deliberation, strategizing, collaborative problem solving, and decision making. The current paper provides a high-level overview of the CogNarr ecosystem and a companion paper dives deeper into technical aspects of meaning representations for user input.\nThere is need for this project. While humans today can communicate with potentially millions of others via social media platforms, those platforms were not designed to support functional group cognition, at least not in the sense described here. If the purpose of a tool or platform is to facilitate group cognition, then each of Lyon's thirteen hallmarks of cognition quite reasonably becomes a driver of design. So too do topics such as transparency, power-sharing, privacy protection, inclusiveness, governance, and information quality. Apart from social media, several online tools and platforms have been developed in recent years to facilitate group deliberation and decision making. While these can be useful, there is still great room and need for more comprehensive designs, such as CogNarr's, that better reflect our science-based understanding of cognition and that are more capable of processing the volume and richness of information that is natural in the large-group setting.\nThis project is timely. Societies are under stress due to a host of unsolved or inadequately solved problems, including climate change, biodiversity loss, pollution, economic and political instabilities, and wealth and power inequalities, including poverty. These and other problems exacerbate human suffering and can lead to social unrest. Indeed, the very existence of multiple, serious, unsolved or inadequately solved problems might be a symptom of long-standing dysfunctional group cognition. Given that each unsolved problem can have local, regional, national, and global impacts, groups of all sizes and kinds are challenged. New tools that facilitate functional group cognition might increase our capacity to address and solve problems, prevent and recover from harm, and identify and act on shared purpose.\nDespite its incubation status, the CogNarr ecosystem is often described here as if it already exists. This literary device avoids some repetition of phrases such as \u201ccould be\" or \"would be\". The intent, however, is not to unnecessarily fix or limit the ecosystem design. As the project develops, that design would likely change to some degree from what is presented here. Importantly, the ecosystem could exist. That is, the project is practical in the sense that a functional proof of concept and minimum viable product could be developed using existing technologies. A fully functional CogNarr ecosystem would benefit from new scientific advances and emerging technologies.\nTo further the scientific effort, to help ensure that the technology is put to safe and best use, and to maximize transparency and build and maintain trust within a population of users, the intention is to develop and implement the ecosystem within an open source framework, under an appropriate governance system. Third-party enhancements resting on top of the core stack, and second- and third-party provision of services (such as consulting and training) are possible.\nThe CogNarr concept is a continuation of ideas presented in , which as a body of work addresses two questions: (1) Out of all conceivable designs for economic, financial, governance, legal, educational, and other core societal systems, which designs might best serve the common good? and (2) By which viable strategies might new designs be developed, tested, implemented, and monitored? In posing these questions, societal systems are viewed as central components of a society's cognitive architecture. The connection to the present work is that here the concept of cognitive architecture is applied to the more modest but still ambitious goal of developing a tool to facilitate group cognition, where the group does not necessarily implement a suite of societal systems. The same principles apply, however, and central questions remain the same. Which designs for group cognitive architectures are fit for purpose, and how can we best implement new and better designs?\nThe CogNarr project is in the spirit of ecosystems of intelligence, as envisioned by , although there are major differences between that work and this one. While the idea of shared narratives reflecting shared generative models (of beliefs) is central to both, this paper takes the concept literally-sharing belief models, conceived of as narratives, about specific problems or situations, within specific groups. In contrast, Friston et al. appear to aim toward a domain-general collective intelligence that exists as a global web of generative models and intelligent systems, with"}, {"title": "2 Cognition", "content": "Active inference, a Bayesian description of cognition and self-organization, plays a central inspirational as well as technical role in the CogNarr project. This section provides a brief overview of active inference to give the reader a feel for how cognition is conceptualized in the CogNarr ecosystem, including the purpose of cognition, how learning occurs, why action and prediction are viewed as part of cognition, and how action and prediction relate to beliefs. Communicating beliefs and predictions is a core focus of the CogNarr ecosystem. This section also briefly describes the hallmarks of basal cognition and the components of group cognitive architectures, alerting the reader to question how each hallmark is addressed in the CogNarr ecosystem and how functional rather than dysfunctional cognition is favored.\nActive inference is a normative principle underwriting cognition in biological and artificial intelligent agents. As a brief sketch, consider two entities, an intelligent agent and its world. The agent, a small entity relative to the massive world, cannot know everything about the world. In particular, the agent cannot know the world's (hidden) states and its generative process, \\(P_w\\) in the figure. The world's generative process is a joint probability function of states, agent actions, and outcomes. The agent can only know its own actions and what it perceives with its senses, which are noisy and partial observations of the world's true states. The agent can, however, use its generative model, \\(P_a\\) in the figure, to estimate states of the world and predict how the world might respond to an action or series of actions (a policy for action) that the agent entertains. The agent uses its generative model to select new actions that it believes will most likely achieve its aims.\nStates of the agent and world only influence each other via observations and actions. The agent and its world are said to be statistically separated by a Markov blanket comprised of observations and actions, as shown in Figure 1. The state of the world is conditionally independent of the agent's internal state, given agent actions, and the internal state of the agent is conditionally independent of the world's state, given agent observations. The situation"}, {"title": "2.1 Active Inference", "content": "in Figure 1 can be modeled, for example, by using a partially observed Markov decision process (POMDP).\nAgent cognition can be described as an idealized four-part cyclic process of predict \u2192 act \u2192 sense \u2192 learn, simplified here to be sequential and non-hierarchical. Notably, actions are viewed as a component of cognition. Some actions serve to gather information (thereby providing epistemic gain) while others are goal oriented (thereby directly affecting conditions). Given epistemic and goal-oriented motivations, an agent acts to reduce its uncertainty about achieving and maintaining those preferred conditions that (it expects will) bestow it with continued existence, wellbeing, and homeostasis. Said differently, it acts in order to thrive, where the term encompasses notions of wellbeing, resilience, and sustainability discussed by Albarracin et al. In short, an agent favors choices that ensure the greatest resolution of uncertainty, under the constraint that preferred outcomes are realized.\nPrediction (anticipation) can result from a logical assessment of a situation. For a human at least, it can also result from intuition, the capacity to recognize and assess patterns without engaging logical faculties or even conscious thought, or perhaps operating at the fringe of conscious thought. As such, an agent's generative model regarding some situation can be more or less formal, so to speak. Regardless, if an agent's generative model is faulty, the risk of making inaccurate predictions and/or poor policy choices tends to increase. Learning is the agent's process of updating and improving its generative model to reduce predictive errors and uncertainty. Ultimately, the predict \u2192 act \u2192 sense \u2192 learn cycle is an optimization process, where the objective function being minimized is expected variational free energy. Because free energy is bounded above by (Bayesian) surprise, it can be minimized by minimizing surprise. Surprises occur when outcomes do not agree with predictions and when uncertainty exists about whether preferred states are or will be achieved.\nThe above description of active inference applies to agents that are individuals, such as an individual robot or human, as well as to agents that are collectives, such as a group of humans. Gu\u00e9nin\u2013Carlut, for example, uses the active inference framework to examine City-States, which he views as intelligent biological entities."}, {"title": "2.2 Functional/Dysfunctional Cognition and Cognitive Architectures", "content": "Agents can and do take actions that are antithetical to thriving. Some such actions are mistakes or miscalculations that an agent might learn from. Others can form a pattern of behavior that poorly serves or even harms the agent. Thus we can reasonably speak of functional versus dysfunctional cognition, relative to the normative principle of minimizing both predictive error and the uncertainty of achieving and maintaining conditions that support thriving. Dysfunctional cognition can arise from inadequacies related to any or all of the thirteen hallmarks of basal cognition identified by summarized below.\n1. Valence (attraction, repulsion, neutrality)\n2. Sensing\n3. Discrimination (ability to identify opportunities and challenges)\n4. Memory storage and recall\n5. Learning\n6. Problem solving (behavior selection, adaptability, abstract thinking)\n7. Communication (with others of the same or different kind)\n8. Motivation (teleonomic striving, implicit and explicit goals)\n9. Anticipation (prediction, forecasting, expectancy)\n10. Attention (oriented response, focus)\n11. Self-identity (distinguishing self from non-self)\n12. Normativity (error detection, behavior correction, value assignment)\n13. Intention (directedness toward an object, belief and desire)"}, {"title": "3 Story Graphs", "content": "From an active inference perspective, group cognition rests on the communication of (potentially dynamic and evolving) member beliefs, and consensus building with respect to those beliefs. As described by Albarracin et al. for a generic group in the normative setting, \u201cgroup members can be seen as actively and implicitly aligning their beliefs and expectations through dialogue and interactions, thereby enhancing their ability to predict each other's actions and intentions, and thereby coming to perceive and act in the world in similar ways.\" We can refer to a person's beliefs and expectations as a belief model, and equate it with \\(P_a\\) in Figure 1. As such, a belief model is an agent's internal model of how the world works, relative to some situation. Two central tasks of CogNarr are to record a user's belief model-externalize it, make it explicit and transparent and then to share it with others in order to facilitate group cognition.\nMost humans do not conceive of their own beliefs in terms of models, however. Rather, humans tend to experience their beliefs and make sense of the world largely through narratives. These can be internal narratives that a person constructs, adjusts, and recites to himself or herself, or social narratives that are shared within a group. In the active inference context, Albarracin et al. consider social scripts, which are widely-supported prescriptions about how one is to behave in various social settings, or what is important in those settings. Bouizegarene et al. consider shared narratives conceived of more broadly. Social scripts and shared narratives help humans to generate more accurate predictions about the world and to coordinate social behavior.\nCogNarr builds on the human propensity to use narratives in sense-making and, more generally, in cognition, by transforming internal narratives of a particular type\u2014those that reflect or resemble belief models-into explicit, external belief models, which are then communicated. This type of narrative is an account of events that conveys, for example, a user's beliefs about what happened, why it happened, what it means, what might happen in the future, what the user wishes would happen, and what action should be taken, if any."}, {"title": "3.1 Rich Stories", "content": "Given the role of narratives in sense-making, CogNarr refers to the externalized belief models as narratives, or more simply, as stories. Users might be encouraged to \u201cshare your story,\u201d for example, which seems more appealing than \"share your belief model.\" But both phrases amount to the same thing. One might expect that stories would be conveyed through written text, as is common outside of CogNarr. But written text is unsuitable for the task at hand. CogNarr is designed to facilitate cognition in the large-group setting, and cognition at scale requires that stories exist in a format that is amenable to automated computational assessment. No user or committee is going to read, say, 10,000 written stories in order to digest and make sense of their contents. Further, computers are not proficient at assessing the meaning of written text, especially when that text is lengthy and reasonably complex, and where its meaning must be understood at a deep level (for example, where the logical implications of passages must be understood). Thus, a story in CogNarr is not represented as text but rather as a special type of computational graph, called a story graph, which can be understood by both humans and computers. Story graphs are the core innovation on which the CogNarr ecosystem is based. CogNarr uses story graphs to curate, understand, and reason with high-quality information about human experience, beliefs, and expectations.\nCreating a story graph takes some effort on the part of a user, although CogNarr is designed to make that process as easy and efficient as possible. Further, creating a story graph is a foreign task that must be learned, compared to creating a written document, which most people are familiar with. So, why would someone want to spend time and energy creating a story graph? The reason is simply that doing so allows and facilitates group cognition at scale. If one wants their potentially lengthy, complex, or nuanced input to be heard and understood in the large-group setting-that is, if one wants to contribute input that is substantially more complex than a yes/no vote, or a selection from a set of options-then he or she might be willing to create their input in a format that serves their goals.\nThe remainder of this section describes the purpose and components of stories shared as story graphs, introduces some computational models that could be used to conduct inference, describes how story graphs are constructed and the tools used for construction, and describes the process of editing story graphs, which can occur in rounds. Supporting these processes is a backend computational system, which hereafter is referred to as the system.\nThe CogNarr ecosystem facilitates individual and group cognition by helping a person to tell a potentially rich (e.g., nuanced, complex, involved) story that conveys his or her experience, beliefs, and expectations about a situation. A story can communicate a user's:\n\u2022 Experience, understandings, views, questions, and concerns regarding a situation, including explanations about what a situation means and how and why it came to be.\n\u2022 Predictions about about how a situation might unfold in the future, given a series of actions or inactions, and predictions about what might be learned through action.\n\u2022 Beliefs about the understandings, intentions, motivations, and desires of pertinent actors.\n\u2022 Uncertainties about beliefs.\n\u2022 Proposed solutions to a political, economic, environmental, technical or other kind of problem, including proposed strategies to address and/or avoid a problem.\nStories can be short, say, the equivalent of a paragraph in length, or much longer, say, the equivalent of a book chapter. Let us assume that an average story is the equivalent of one to several pages of text in length.\nHereafter, the term situation refers to the situations, events, circumstances, and/or problems that form the topic of a story. The user is sometimes called a participant, or group member. He or she might be called a storyteller when creating a story or a reader when viewing one.\nCogNarr apps can be designed to support a wide range of use cases in a wide range of domains for a wide range of groups. These include use cases for individuals, apart from a group, and for small groups. The focus of this paper, however, is on use cases that involve deliberation, strategizing, collaborative problem solving, and/or decision making in the large-group setting. These are among the most challenging of use cases, and addressing them is a core purpose of the CogNarr project.\nThe act of sharing stories and interacting about stories helps a group to better understand the beliefs, concerns, desires, and expectations of its members. Interactions can take several forms. For example, group members and the system can pose questions or offer feedback to a storyteller. A storyteller can edit his or her story as desired, and can cite, comment"}, {"title": "3.2 Story Components and Purpose", "content": "In the CogNarr ecosystem, a completed story exists simultaneously as:\n1. A collection of passages. A passage is written text, perhaps one or a few sentences in length, conveying a set of closely related thoughts. A passage also includes associated metadata and artifacts. The collection of passages is considered the source of record, from which all else is derived.\n2. A meaning representation of each passage, called a story graph fragment.\n3. A meaning representation of the integrated set of passages, called a story graph.\n4. A set of translations of the story graph, in whole or in part, into natural language, alternative meaning representations, and/or computational programs, called models.\nThe purpose of the story graph is to: (a) represent a narrative in a format that is less ambiguous than natural language; (b) serve as a format that is readable and understandable by both humans and computers; (c) provide a structure on which certain kinds of inference or analysis can be applied (e.g., graph matching); and (d) serve as an interlingua-an abstract natural-language-independent representation suitable for translation into: (i) multiple natural languages; (ii) one or more models, or arguments of models; and (iii) alternative meaning representations that support specific models.\nNumerous meaning representations have been developed over the years to support natural language processing and inference. Most of these are graph-based. The reason for graphs is that they excel at representing structured information, and both humans and computers are adept at understanding information in graph form. CogNarr adopts a graph-based meaning representation for the same reasons. Hereafter, for convenience, the terms story graph and meaning representation are sometimes used interchangeably. That is, a story in story graph form is an instantiation of the CogNarr meaning representation. The design of that representation is the same for story graphs and story graph fragments, except that fragments might employ a more limited set of features."}, {"title": "3.3 Models", "content": "A story graph can be translated into one or more models. The purpose of a model is to conduct inference or analysis, compute with uncertainties, and/or generate predictions. Generate predictions means to instantiate a storyteller's beliefs regarding predictions. As an example, if a storyteller conveys a belief of 100 percent certainty that the next 30 days in the story world will be sunny, then the predictions generated by a model will be a series of 30 forecasts, all of which are for sun. If a storyteller believes that for each of the next 30 days there is a 50 percent chance of rain, then the predictions will be a series of forecasts, about half of which are for rain. If the model were executed a very large number of times, almost exactly half of all predictions would be for rain.\nTwo characteristic models are:\n\u2022 A logic model that interprets formulas of classical or non-classical logic. Classical logic, extensions of classical logic, and non-classical logics include first-order logic, temporal logic, natural logic, probabilistic logic, and annotated logic.\n\u2022 A probabilistic model that is written in a universal probabilistic programming language (PPL), such as the Julia package Gen."}, {"title": "3.4 Story Graph Views", "content": "The CogNarr meaning representation is likely to be more full-featured and sophisticated than AMR or other common meaning representations. Thus, a long and complex story is likely to be represented as a large and complex story graph. A typical CogNarr user is not likely to be familiar with natural language inference or the details of meaning representations, and so might have limited use of viewing a raw meaning representation, especially for long stories. On the other hand, a user might very well want to verify that his or her story is being understood by the system as expected. For example, suppose a storyteller enters the sentence \u201cSomeone shot the friend of the actress who was on the balcony.\" Perhaps the storyteller knows what happened, but the system might be confused as to who was on the balcony, the friend, the actress, or both? The system might interpret the sentence in a way that the storyteller does not intend.\nIf the storyteller suspects that there is more than one interpretation of a sentence, or more than one way to combine a fragment into a story graph, or has some other concern about how the story graph is being constructed, then he or she might want to verify that the system understands the story as intended. There are several ways that verification can be done. One is to view a potentially simplified version of a story graph fragment or story graph. Views can be tailored to meet the immediate needs of the user. They can be simple, containing superficial, informal, summarized, and/or partial information, or they can contain detailed information. A fully detailed view is the underlying meaning representation itself."}, {"title": "3.5 Uncertainty", "content": "Assuming that a story is translated into a probabilistic model, the model will be executed once every time a user instructs CogNarr to generate an example outcome (unless a saved example is requested or provided by default). In contrast, the model would be executed multiple times, perhaps very many times, if a user instructs CogNarr to generate a distribution over possible outcomes. Using the example already given for a 50 percent chance of rain, if a user asks CogNarr how many of the next 30 days are likely to be rainy, according to the story, CogNarr would execute the model a large number of times and return the answer 15, if rounded to a whole number. Obviously, for simple problems like this that have an analytic answer, CogNarr might not need to execute the probabilistic model at all to produce an answer. It could execute a more efficient model. As another example, suppose that a story has a character, Alice, who usually rides her bike to work, but sometimes drives her car. If a user instructed CogNarr to generate a set of possible outcomes, in some (i.e., in some futures that the storyteller conceives of), Alice will ride her bike to work. In others, she will drive her car.\nIf a story has certain outcomes, such as the sunny day example previously given, predictions will be static. Each execution of the model produces the same outcome. Such a story is deterministic. The same is true for a story that does not include any uncertainty information, inferred, implied, or explicit. Unless otherwise instructed, the system would understand the lack of certainty information to mean that the storyteller has complete certainty about outcomes.\nAs in the examples with Alice and weather forecasting, a probabilistic model might be very simple. In an uncomplicated short story containing only one story-line branch, the model might represent not much more than flips of a (fair or"}, {"title": "4 Story Construction and editing", "content": "A story is constructed one passage at a time, as illustrated in Figure 3. Each passage is translated into a story graph fragment, and the fragment is merged into the growing story graph. The story graph, in turn, can be translated into code for one or more models, into alternative meaning representations, or into various natural languages. For a story graph translated into model code, one can think of the story graph as providing a semantic description of code, perhaps even detailed elements of code, like algebraic equations. If a logic model is employed, a story graph can be understood, roughly, as a set of formulas within a logical-form language.\nWhen creating a story graph fragment, a storyteller can input sentences as well as metadata and artifacts. In initial CogNarr versions, artifacts would likely be limited to tables and charts. Sketch, import, and other tools assist in their creation. The information in artifacts is not just for human readers; it is integrated into the models and so also into"}, {"title": "4.1 Story Construction", "content": "the meaning and predictions of a story. Over time, more advanced versions of CogNarr could include images or other objects as artifacts.\nMetadata are instructions or comments that help the system to understand or process the meaning of input text. Metadata could indicate:\n\u2022 That a passage has some rhetorical relation to another passage, such as being an explanation or providing contrast.\n\u2022 That a passage describes code, an algebraic function, or domain information.\n\u2022 Where or how a passage is to fit into the larger story, including syuzhet instructions.\n\u2022 That a passage is intended only for a specific model.\n\u2022 That the ambiguity of a sentence should be resolved in a particular way.\nTo resolve ambiguity, metadata can include markup tags or symbols within a sentence that indicate the intended semantics. In some cases, markup could be as simple as use of brackets to indicate scope (e.g., \"Someone shot the friend of [the actress who was on the balcony]\").\nAs mentioned, the set of passages becomes the source of record from which all else derives. Consider for a moment the simple case where a passage is only text, as entered into a text box. In this and more complicated cases, it is useful to have a single source of record. If multiple sources exist (e.g., if passages as well as story graph fragments and the story graph can be edited), then information in the various sources might conflict. At the very least, the new information in each edit would need to propagate across sources, in multiple directions. With passages as the single source of record, edits propagate in one direction only (i.e., downward in Figure 3). Also, the user is always editing his or her own input, rather than editing derivations that the system makes as it propagates information from one source to another."}, {"title": "4.2 Tools for Story Graph Construction", "content": "Story graph construction is assisted by tools and guided by system feedback. Passages are automatically translated into story graph fragments, and when a translation is not possible, the system might ask questions or offer suggestions. Current text-to-graph parsing methods employ neural and statistical methods, commonsense resources such as knowledge graphs, syntactic parsers and/or PTLMs. Current graph-to-code methods also employ PTLMs.\nMerging a fragment into a story graph is automatic, guided as necessary by user-supplied metadata and questions and suggestions offered by the system. To verify that the system understands the intended meaning of input, the storyteller can view the story graph, as mentioned, can view predictions and outcomes, and/or can translate the story into natural language. An advanced version of CogNarr might generate animations or other multimedia as part of story playback. Current graph-to-text generation methods employ PTLMs or other kinds of neural networks."}, {"title": "4.3 Why Story Graphs and Fragments?", "content": "One might wonder why both story graph and model representations are necessary. Why not convert natural language directly into probabilistic or other kinds of models, if that is the goal? There are several reasons. First, a model might represent only a portion of a story graph. Second, humans are likely to grasp the meaning of a graph more easily and faster than the meaning of code. Coding requires special skills, whereas understanding a story graph view generally does not. Third, some forms of computational processing are more efficient or simpler on graphs, compared to code."}, {"title": "4.4 Editing in Rounds", "content": "In some cases, a group will want to edit stories over a series of rounds, as illustrated in Figure 4. In each round:\n1. Users edit their stories, with possible feedback from the system.\n2. The system processes all stories and reports back to the group. For example, it reports on similarities and differences between stories.\n3. The group considers the meaning of stories and engages in dialogue and story feedback.\nAs rounds progress, ideally the group comes to better understand the perspectives of its members, and, in the decision-making case, hone in on a well-supported, favored decision. That decision is itself in the form of a narrative. In other words, the narratives of the group ideally evolve toward one or a few central narratives that best represent the shared understanding of the group (and potentially, also any divergence of understanding). It is worth noting that a selected decision could, in some cases, be in the form of a policy that outlines a series of actions. That policy could include if-then statements.\nReporting by the system in each round helps the group to better understand the range of stories, the characteristics and qualities of stories, and the evolution of stories during rounds. By pointing out commonalities and differences among stories (for example, based on quality, content, time frames, and predictions), clustering stories by similarities, and reporting other characteristics of the set of stories, the system helps draw attention toward potentially salient features. Additional approaches to focus group attention are possible, and their development is left for future work.\nThe three-step process illustrated in Figure 4 makes group cognition explicit and transparent. Further, it generates a history of narrative evolution that can be recalled and used for learning and evaluation. The quality of the group cognitive process itself could be evaluated. The history also serves as a record of minority opinions, which along with majority opinions could be evaluated by the group in hindsight. For example, evaluation could assess the accuracy of predictions made by different subgroups."}, {"title": "4.5 Story Repository", "content": "As a result of the editing, feedback, and assessment processes described previously, story ambiguity is minimized. Most finished stories would provide rich, high-quality, curated information about human experiences and beliefs. Stories that are low quality could readily be identified. If CogNarr is successful, over time the accumulated volume of high-quality narratives would become massive. Such data would be very valuable to academic research (in fields such as sociology, governance, psychology, and cognition). And it would be valuable to the system itself, which could use it for continuous learning. As the data repository grows, so too would the system's capacity to understand and analyze both stories and strategies for action. Data privacy, data ownership, and data control are critically important concerns. Addressing these topics is left for future work."}, {"title": "5 Storytellers and Active Inference", "content": "The preceding sections have introduced both active inference and stories. Now we take a look at storyteller relationships in the context of active inference. These relationships suggest ways that active inference naturally occurs in the CogNarr setting, and as well suggest ways that active inference models could be used to assist users.\nReferring back to Figure 1, active inference is a normative process that occurs between an agent, which we now label Self, and its world. The world can include other agents, and we now label it Other. In this case, Self is trying to understand Other and Other is trying to understand Self. From an active inference perspective, interactions between Self and Other that involve honest communication can help agents develop common beliefs and common expectations about what is likely to happen. Several studies have framed belief-sharing and cooperation between agents, including cooperative communication, in an active inference context.\nIn the CogNarr setting, multiple Self-Other relationships can be identified. Some examples are illustrated in Figure 5. A storyteller tries to understand a group. The system tries to understand a storyteller, the group, and the set of all groups. A group tries to understand a storyteller. And a group tries to understand the world, in part though member narratives and, in the decision-making case, in part by choosing actions, implementing them in the world, and then sensing and evaluating outcomes.\nIn all these cases, an agent is choosing actions based on beliefs and predictions. Some actions are intended to gain information and some are intended to alter conditions. Those human-human and human-world interactions illustrated in Figure 5 that are informal, not part of a mathematical model, are examples of active inference in the wild. Fox calls this implicit active inference.\nFormal active inference can occur in CogNarr too. As mentioned, an active inference model could be used to orchestrate system-initiated interactions that are intended to assist and guide storytellers. In this case, the system uses an active inference model to refine its understanding of a story and storyteller, with the goal of improving story quality. Among other things, the system tries to reduce its uncertainty about events and characters in a story, and reduce any discrepancies between the story content and rules or guidelines that the storyteller is asked to follow. The system's preferred conditions include high-quality stories that are appropriate for the situation at hand. As an example, storytellers in a group might be asked to stay on topic, and the system might occasionally remind a user of this if he or she drifts too far away.\nTo assist and guide users, the system should be capable of asking intelligent questions about a story, the kind of questions that a thoughtful and respectful human active-listener would ask. These include questions about who the characters are, where the story is going, what the storyteller intends, and what pieces of information are missing that, if obtained, would most reduce story ambiguity or improve clarity. To ask such questions, the system must be able to understand a story in its current form and predict not only what might be gained if certain new information were added or existing information altered, but also how its prompts or questions might best achieve the aim of eliciting a desired change in information. In order to learn"}]}