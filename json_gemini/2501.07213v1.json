{"title": "Multi-face emotion detection for effective Human-Robot Interaction", "authors": ["Mohamed Ala Yahyaoui", "Mouaad Oujabour", "Leila Ben Letaifa", "Amine Bohi"], "abstract": "The integration of dialogue interfaces in mobile devices has become ubiquitous, providing a wide array of services. As technology progresses, humanoid robots designed with human-like features to interact effectively with people are gaining prominence, and the use of advanced human-robot dialogue interfaces is continually expanding. In this context, emotion recognition plays a crucial role in enhancing human-robot interaction by enabling robots to understand human intentions. This research proposes a facial emotion detection interface integrated into a mobile humanoid robot, capable of displaying real-time emotions from multiple individuals on a user interface. To this end, various deep neural network models for facial expression recognition were developed and evaluated under consistent computer-based conditions, yielding promising results. Afterwards, a trade-off between accuracy and memory footprint was carefully considered to effectively implement this application on a mobile humanoid robot.", "sections": [{"title": "INTRODUCTION", "content": "The rapid advancement of technology in recent years has accelerated research in robotics, with a particular emphasis on humanoid robots. Designed to resemble humans in body, hands, and head, humanoid robots are increasingly capable of sophisticated interactions with people, including recognizing individuals and responding to commands. This human-like form and behavior make them particularly well-suited for applications in human-computer interaction, serving as effective platforms for studying and improving user engagement and interaction dynamics. Current examples of humanoid robots include Honda's ASIMO (Hirose and Ogawa, 2007), known for its advanced mobility and dexterity; Blue Frog Robotics' Buddy (Peltier and Fiorini, 2017), designed for social interaction and domestic assistance; and Aldebaran Robotics' NAO (Gouaillier et al., 2009), recognized for its versatility in research and educational settings. These robots showcase the diversity of roles humanoid robots can play, from companionship and entertainment to education and beyond.\nDeveloping emotional intelligence in robots is relevant as they increasingly participate in social settings. Indeed, beyond performing physical tasks, enhancing robots' ability to perceive, interpret and respond to human needs is essential for effective social Human-Robot Interaction (HRI) and Human-Robot Collaboration (HRC).\nIn the realm of social robotics, integrating sensors such as microphone for \"mouth\" or camera for 'eyes' into the humanoid robot, enables the robot to capture human emotions in real-time, and to adapt its response and behavior accordingly (Justo et al., 2020; Olaso et al., 2021; Palmero et al., 2023). This capability enhances their utility in various applications and facilitates engagement and intuitive interaction experiences between robots and humans. Detecting emotions from camera starts with face detection, which involves identifying and locating human faces within images or video frames. This process includes pre-processing images, extracting distinct facial features, classifying regions as faces or non-faces, refining detection accuracy, and handling variations in lighting, occlusions, poses and scales. Face emotion recognition (FER) employs computer vision and machine learning techniques to analyze human emotions from face.\nOften, emotion recognition systems deals with only one user while he is communicating with a machine. However, multiple users can communicate simultaneously with it. Multi-face emotion recognition is particularly valuable across various scenarios. For instance, at a comedy club, it provides real-time feedback to comedians, manages lighting and sound, interacts with the audience, and detects disruption.\nIn this work, we present a complete facial emotion recognition interface and its deployment in a mobile humanoid robot. The proposed interface can display emotions from multiple individuals in real-time within an advanced user interface. To achieve this, several deep neural network models have been developed and evaluated under the same conditions. Then a tradeoff between system accuracy and model size have been considered in order to implement the optimal solution into a humanoid robot. The model's performance and its confidence interval also guided this choice of solution.\nThe remainder of this paper is structured as follows. Section 2 reviews the state of the art related to emotion detection for Human-Robot Interaction (HRI) and Facial Emotion Recognition (FER) systems. Section 3 presents the design and implementation of the proposed emotional interface, detailing the multi-face detection, emotion recognition system, and the graphical user interface. Section 4 describes the integration of the facial emotion recognition system into the Tiago++ humanoid robot, highlighting the processes of face tracking and real-time emotion detection. Section 5 outlines the experimental setup and presents the results, including performance metrics, model comparisons, and user interaction analysis. Finally, Section 6 concludes the paper with a discussion of the findings, limitations of the current approach, and potential directions for future work."}, {"title": "RELATED WORK", "content": "Although emotions have been investigated in the context of HRI, it remains a significant challenge. In this section, we report recent research in HRI as well as FER systems."}, {"title": "Emotion detection for HRI", "content": "In social robotics, emotion detection is mimicked by robots to interact naturally and harmoniously with humans. Several studies have focused on implementing facial emotion recognition in robots. For instance, the study (Zhao et al., 2020) applied facial emotion recognition on three datasets: FER2013, FERPLUS and FERFIN. The system was implemented on a NAO robot, which responds with actions based on the detected emotions. However, this study has some limitations, as it does not provide details on the robot's implementation. Additionally, the research (Dwijayanti et al., 2022) integrated a facial detection system with a facial emotion recognition system and implemented it in a robot. They also explored automatic detection of the distance between the camera of the robot and the person. One drawback is that the robot is stationary, so mobility is not considered. The study (Spezialetti et al., 2020) serves as a survey of emotion recognition research for human-robot interaction. It reviews emotion recognition models, datasets, and modalities, with a particular emphasis on facial emotion recognition. However, it does not include any research utilizing deep learning models for facial emotion recognition."}, {"title": "Facial emotion recognition", "content": "Deep learning has revolutionized computer vision tasks, including Facial Emotion Recognition (FER), with numerous studies proposing various methodologies to achieve high classification accuracy using well known benchmark datasets (Farhat et al., 2024; Goodfellow et al., 2013; Letaifa et al., 2019; Mollahosseini et al., 2017; Justo et al., 2021; Lucey et al., 2010).\nSeveral recent studies have proposed innovative approaches for FER. Farzaneh et al. (Farzaneh and Qi, 2021) introduced the Deep Attentive Center Loss (DACL) method, which integrates an attention mechanism to enhance feature discrimination, showing superior performance on RAF-DB and AffectNet datasets. Similarly, Pecoraro et al. (Pecoraro et al., 2022) proposed the LHC-Net architecture, which employs a multi-head self-attention module tailored for FER tasks, achieving state-of-the-art results on FER2013 with lower computational complexity. In another work, Han et al. (Han et al., 2022) presented a triple-structure network model based on MobileNet V1, which captures inter-class and intra-class diversity features, demonstrating strong results on KDEF, MMI, and CK+ datasets. Fard et al. (Fard and Mahoor, 2022) introduced the Adaptive Correlation (Ad-Corre) Loss, which improved performance on AffectNet, RAF-DB, and FER2013 datasets when applied to Xception and ResNet50 models. Other notable contributions include the Segmentation VGG-19 model (Vignesh et al., 2023), which enhanced FER on FER2013 using segmentation-inspired blocks, and the DDAMFN network by Zhang et al. (Zhang et al., 2023), which incorporated dual-direction attention to achieve excellent results on AffectNet and FERPlus. Lastly, in our recent work, we introduced EmoNeXt (El Boudouri and Bohi, 2023), a deep learning framework that has set new state-of-the-art benchmarks on the FER2013 dataset. EmoNeXt integrates a Spatial Transformer Network (STN) for handling facial alignment variations, along with Squeeze-and-Excitation (SE) blocks for channel-wise feature recalibration. Additionally, a self-attention regularization term was introduced to enhance compact feature generation, further improving accuracy.\nThis brief review shows that many FER models have focused exclusively on improving accuracy. As a result, today's leading models can reach memory sizes in the order of gigabytes, which poses challenges for deployment in memory-constrained environments, such as the robots."}, {"title": "THE EMOTIONAL INTERFACE", "content": "One of the challenges in the domain of emotion detection for HRI, is the simultaneous detection of emotions from multiple faces, which is useful where robots interact with groups of people."}, {"title": "Multi-face detection", "content": "We choose the Haarcascade classifier, proposed by Paul Viola and Michael Jones in their seminal paper (Viola and Jones, 2001), as a highly effective method for face detection. Other notable methods include the Histogram of Oriented Gradients (HOG) combined with Support Vector Machines (SVM) and deep learning approaches such as the Multi-task Cascaded Convolutional Networks (MTCNN). While these methods have shown promising results in various applications, the Haarcascade classifier is particularly advantageous for real-time scenarios.\nThe general principle of the Haarcascade approach is illustrated in Figure 1. This machine learning-based method involves training a cascade function using a large dataset of positive (face) and negative (non-face) images. The classifier relies on Haar features, which are similar to convolutional kernels, to extract distinguishing characteristics from images. Each Haar feature is a single value calculated by subtracting the sum of pixels under a white rectangle from the sum of pixels under a black rectangle. To efficiently compute these features, the concept of integral images is utilized, reducing the calculation to an operation involving just four pixels, regardless of the feature's size.\nDuring training, all possible sizes and positions of these features are applied to the training images, resulting in over 160,000 potential features. To select the most relevant features, the AdaBoost algorithm is utilized, which iteratively adjusts the weights of misclassified images and selects features with the lowest error rates, thereby creating a strong classifier from a combination of weak classifiers. Despite the high initial number of features, this process narrows it down significantly (e.g., from 160,000 to around 6,000).\nFor detection, the image is scanned with a 24x24 pixel window, applying these selected features. To enhance efficiency, the authors introduced a cascade of classifiers. This means that features are grouped into stages, and if a window fails at any stage, it is immediately discarded as a non-face region. This hierarchical approach ensures that only potential face regions undergo the full, more complex evaluation process, allowing for real-time face detection with high accuracy."}, {"title": "Emotion recognition system", "content": "Pretrained deep learning models have demonstrated exceptional effectiveness for feature extraction across various domains (Palmero et al., 2023). In our emotion recognition system (Figure. 2), we leverage a pretrained convolutional neural network (CNN) model to apply transfer learning using the FER2013 dataset (Goodfellow et al., 2013). Specifically, we utilize pretrained CNN models, initially trained on the ImageNet dataset which encompass millions of images from various categories (Deng et al., 2009). This extensive training enables these models to extract highly relevant and general visual features through their convolutional layers. These layers detect fundamental elements such as edges, textures, and shapes, which are essential for understanding facial structures. We utilize these convolutional layers to process our input images, leaving out the top portion of the model, specifically the fully connected layers initially designed for the ImageNet classification tasks. Instead, by passing our facial images through the pretrained model's convolutional layers, we generate a feature stack that encapsulates essential visual information. This feature stack, representing a rich set of features extracted from the images, is then flattened into a format suitable for further processing. Subsequently, we introduce additional fully connected layers tailored to the FER2013 dataset to recognize and classify seven distinct emotions: anger, disgust, fear, happiness, sadness, surprise, and neutrality."}, {"title": "Graphical interface", "content": "The graphical interface of our emotion recognition system integrates multiple advanced technologies to provide a seamless and responsive user experience. Upon launching the application, the interface is built using the Tkinter library, creating a user-friendly graphical environment. The system activates the webcam through the OpenCV library, capturing a live video feed for real-time analysis. Captured video frames undergo face detection using the HaarCascade classifier, a robust method for identifying faces under various lighting conditions and angles (see description in subsection 3.1).\nOnce a face is detected, the region of interest is extracted and subjected to preprocessing to ensure compatibility with the model's input size. The processed image is then fed into a pretrained CNN model that have been fine-tuned on the FER2013 dataset. This model analyzes the facial image to predict the user's emotional state, categorizing it into distinct emotions such as anger, fear, disgust, happiness, sadness, surprise, and neutrality. The predicted emotion is then displayed on the graphical interface, providing immediate feedback to the user."}, {"title": "THE HUMANOID ROBOT", "content": "The Tiago robot, developed by PAL Robotics (Pages et al., 2016), is a humanoid mobile robot. Its modular design allows for customization to meet specific needs. In this section, we outline our approach to equipping the Tiago++ model of the robot with face emotion recognition capabilities. By using the Robot Operating System (ROS) for communication and processing, and integrating a Tkinter-based GUI for real-time visualization, we enhance the ability of the robot to interact with humans. This implementation is divided into two primary tasks: face tracking and emotion detection, each described in the following subsections."}, {"title": "Face Tracking Integration on Tiago++ Robot", "content": "We implemented a face tracking module on the Tiago robot by integrating ROS with a Tkinter-based GUI application. The process begins with initializing a ROS node named Tiago FER and setting up essential publishers and subscribers to facilitate communication between the robot and the software. We use the CvBridge library to convert images from ROS format to OpenCV format. Meanwhile, the MediaPipeRos instance processes these images to detect regions of interest (ROI) for face tracking. The application's main loop receives images from the robot's camera through the /xtion/rgb/image ROS topic, processes these images to detect faces, and generates commands to adjust the robot's yaw and pitch. These commands, which control head movements, are published to the head_controller/increment/goal topic using the IncrementActionGoal message type, enabling the robot to track the detected faces. These steps are outlined in the diagram generated by ROS, as shown in Figure 4."}, {"title": "Emotion detection and GUI display on Tiago++ screen", "content": "Following face tracking, the processed images are analyzed to predict emotions. The detected emotions are displayed on a Tkinter GUI, which features a canvas for image display and progress bars to visualize emotion scores. The processed images and emotion data are published back to the /imagesBack ROS topic. Additionally, incremental commands for torso movements are sent to the /torso_controller/safe_command topic using the Joint Trajectory message type, allowing the robot to dynamically respond to detected emotions (see Figure 4."}, {"title": "EXPERIMENTS AND RESULTS", "content": "Developing a human-robot interface for FER involves detecting faces and emotions, implementing the user interface, and integrating it into the robot platform. The robot's camera captures images of individuals interacting with it, processes these images to detect emotions, and then displays the detected emotions on the user interface. This interface is visible on the tablet mounted on the robot's chest. Several challenges are to be addressed, particularly focusing on the accuracy of the models and the feasibility of implementing them on the robot."}, {"title": "Face emotion detection", "content": "In this work, we fine-tuned several pretrained models from the Keras library, initially trained on the ImageNet 1000K dataset. These models were selected based on their strong performance in the ImageNet classification task and their ability to generalize well for FER tasks. We applied transfer learning, as explained in subsection 3.2, to the following models: MobileNet (Howard et al., 2017), DenseNet201 (Huang et al., 2017), ResNet152V2 (He et al., 2016b), ResNet101 (He et al., 2016a), Xception (Chollet, 2017), EfficientNetV2-B0 (Tan and Le, 2021), InceptionResNetV2 and InceptionV3 (Szegedy et al., 2017), VGG16 and VGG19 (Karen, 2014), and ConvNeXt (from Tiny to XLarge version) (Liu et al., 2022).\nFor training, we consistently used data augmentation techniques such as rotation, shift, zoom, horizontal flip and adjustments in brightness and contrast to improve the model's robustness. Additionally, Random Erasing was used to simulate occlusions, while resizing and recropping variations improved robustness to differences in face positioning. The models were optimized using Adam with a learning rate of 0.0001, combined with strategies like EarlyStopping and ReduceLROnPlateau to prevent overfitting and dynamically adjust the learning rate."}, {"title": "Confidence interval", "content": "Accuracy is an estimate of the performance of a system, and its reliability depends on the number of tests conducted that is in our case the number of emotions to be recognized. The measurement of the confidence interval is introduced to assess the trustability of our recognition rate. In (Zouari, 2007), the successes are modeled by a binomial distribution. If N is the number of tests and P is the recognition rate, then the confidence interval [P-, P+] at x% is:\n$P \\pm Z_x \\sqrt{\\frac{P(1-P)}{N} + \\frac{Z_x^2}{4N^2}} \\over{1+\\frac{Z_x^2}{N}}$\nwith z95% = 1.96 and z98% = 2.33. This means that there is a x% chance that the rate falls within the interval [P-, P+]."}, {"title": "User interface development", "content": "Our emotion recognition application features an intuitive and user-friendly graphical interface designed for both single-face and multi-face emotion detection. The interface allows users to utilize their device's camera to capture live video streams, which are then processed in real-time to detect and classify facial expressions. For single-face emotion recognition, the application highlights the detected face and displays the identified emotion with corresponding confidence levels. In multi-face scenarios, the interface efficiently detects multiple faces within the same frame, assigning emotions to each detected face individually. The results are visually presented using bounding boxes and emotion labels directly on the video feed, providing clear and immediate feedback.\nAdditionally, the interface includes progress bars for the detected emotion, visually representing the confidence level of each prediction. An avatar further enhances user interaction by imitating the predicted emotion in real-time, offering an engaging and dynamic way to understand the results. This comprehensive and interactive interface ensures that users can easily interpret the emotion detection outcomes, making the application practical for various real-world settings, including human-robot interaction and affective computing."}, {"title": "FER deployment on Tiago++ robot", "content": "The Tiago++ is a humanoid mobile robot with constrained resources (CPU, memory, and storage). Besides interacting with humans, the robot must concurrently perform critical tasks such as navigation and detection, which are also resource-intensive. Consequently, for deploying our application on the Tiago++ robot, it is essential to select a model not only based on its test accuracy but also on the memory footprint of the model. The Tiago++ robot has a maximum capacity of about 150 MB for model files to ensure real-time inference without disrupting other processes running on the robot. According to Table 1 and the previous subsection, EfficientNetV2-B0 stands out with a good balance between accuracy (70.00%) and model size (139 MB), meeting the robot's constraints.\nTo illustrate the system's effectiveness, we conducted two sets of experiments. In the first set, a single participant interacted with the robot, displaying a range of emotions. The system's ability to accurately detect the face and classify the emotional state of the participant in real-time was meticulously observed and documented. In the second set, two participants were present simultaneously, engaging in various interactions with the robot. This scenario tested the system's robustness in detecting multiple faces"}, {"title": "CONCLUSIONS", "content": "In this paper, we presented a facial emotion detection interface implemented on a mobile humanoid robot. This interface is capable of displaying emotions from multiple individuals in real-time video. To achieve this, we developed and evaluated several deep neural network models under consistent conditions, carefully considering factors such as model size and accuracy to ensure compatibility with both personal computers and mobile robots like the Tiago++.\nWhile our system demonstrates strong performance, it is important to note the limitations of relying solely on facial expressions for emotion detection, particularly in contexts where communication may be impaired. Emotions are complex and multifaceted, often requiring the integration of multiple modalities for more accurate recognition. Therefore, future work will focus on incorporating additional modalities, such as voice, text, gestures, and biosignals, to enhance the performance and reliability of emotion recognition systems. Additionally, we will focus on optimizing large models used in FER tasks to ensure their efficiency for deployment on the Tiago++ robot, considering the balance between model size and accuracy."}]}