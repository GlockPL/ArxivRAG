{"title": "Content Moderation by LLM: From Accuracy to Legitimacy", "authors": ["Tao Huang"], "abstract": "One trending application of LLM (large language model) is to use it for content moderation\nin online platforms. Most current studies on this application have focused on the metric of\naccuracy - the extent to which LLM makes correct decisions about content. This article\nargues that accuracy is insufficient and misleading, because it fails to grasp the distinction\nbetween easy cases and hard cases as well as the inevitable trade-offs in achieving higher\naccuracy. Closer examination reveals that content moderation is a constitutive part of plat-\nform governance, the key of which is to gain and enhance legitimacy. Instead of making\nmoderation decisions correct, the chief goal of LLM is to make them legitimate. In this\nregregard, this article proposes a paradigm shift from the single benchmark of accuracy to-\nwards a legitimacy-based framework of evaluating the performance of LLM moderators.\nThe framework suggests that for easy cases, the key is to ensure accuracy, speed and trans-\nparency, while for hard cases, what matters is reasoned justification and user participation.\nExamined under this framework, LLM's real potential in moderation is not accuracy im-\nprovement. Rather, LLM can better contribute in four other aspects: to conduct screening\nof hard cases from easy cases, to provide quality explanations for moderation decisions, to\nassist human reviewers in getting more contextual information, and to facilitate user partic-\npation in a more interactive way. Using normative theories from law and social sciences to\ncritically assess the new technological application, this article seeks to redefine LLM's role\nin content moderation and redirect relevant research in this field.", "sections": [{"title": "1 Introduction", "content": "LLM (large language model) can be used in a wide range of scenarios. One emergent area of its application\nis content moderation. Within a span of less than two years, such topic has generated considerable scholarly\ndiscussion (see [42]; [46]; [57]; [93]; [73]; [86]; [66]; [12]; [68]; [8]). Most studies so far have deemed\nLLM promising in this application field and the technology has been hailed as \u201cthe greatest change to the\ndynamics of content moderation in at least a decade\u201d ([112]).\nEven though researchers have explored LLM's contribution in different types or aspects of moderation\ntasks, one conclusion shared by most of them is that the new tool, through appropriate design and use, could\nbe more accurate in detecting and classifying violating content than its predecessors. In other words, the\nexisting scholarship on LLM moderation has centered upon the metric of accuracy the extent to which\nLLM makes correct decisions of moderation. LLM's superiority in accuracy derives from its technical"}, {"title": "2 The Accuracy Discourse", "content": "LLM moderation is gaining momentum. A simple Google Scholar search shows that more than a thousand\npapers have discussed this topic in the past two years.\u00b9 Researchers and developers have growing interest\non using this technology to moderate content on digital platforms and communities. Most existing studies\non LLM moderation have focused on the metric of accuracy. Accuracy refers to the capability of making\ncorrect decisions. It is usually measured by the percentage of correct decisions (true positives plus true\nnegatives) among the total number of cases.\nThere are also metrics that are akin to accuracy, such as recall and precision: some may stress the ability of\nidentifying true positives and others may stress the ability of identifying true negatives ([50], 21). And trade-\noffs are sometimes inevitable between the two sides ([95], 45). But the general goal of all AI moderation\ntools is to reduce the two types of errors and increase the ratio of correct ones. This article uses accuracy in\nthis broad sense, referring to the capability of making correct (true) decisions and avoiding erroneous (false)\nones.\nAmong the 1220 results in Google Scholar after searching \u201cLLM\u201d and \u201ccontent moderation\", 844 of them\ncontain \"accuracy\u201d, constituting roughly two thirds. Generally, studies to date reported positive appraisals re-\ngarding LLM's accuracy performance in moderation tasks, such as identifying and classifying content. ([46],\n2) found that in most cases, ChatGPT's zero-shot accuracy is higher than that of MTurk (crowd-workers of\nannotation on Amazon). ([88], 9) attested that \"LLMs can perform on par or better than traditional ML\nmodels in cyberbullying detection. ([65]) revealed that LLMs exhibit a high degree of accuracy in detecting\nfake news. ([28]) tested the fine-tuned LLMs' performance in fact-checking and discovered that \u201cLLMs\ncan reliably assess the relationships between social media posts and verified claims, offering performance\ncomparable to human evaluations\u201d. Similar work of testing LLM's ability in fact-checking has been done by"}, {"title": "3 The Unique Features of LLM Moderation", "content": "AI moderation tools have been widely used by online platforms and communities to manage content, as\nhuman moderation is impossible to scale and costly to operate. Several methods have been adopted for\nvarious circumstances, such as hashing ([51], 4) and blacklisting ([95], 41). Currently, the most widely\nused is the technique of machine learning (ML). Since LLM is one sub-category of ML, this article uses\n\"traditional ML\" to refer to the technique of ML before the emergence of LLM. To know what differences\nLLM could bring, we should first take a glimpse at the basic characteristics of the traditional ML method\nand its limits."}, {"title": "4 Why Accuracy is NOT Enough", "content": "The accuracy discourse is parochial and misleading. Taking accuracy as our primary or even exclusive\nmetric of evaluating and designing LLM moderation tools misunderstands the function of accuracy and its\nposition in the governance system.\nThere are four cases against the accuracy discourse. First, it is impossible to achieve perfect accuracy in\npractice, and trying to do so is dangerous. Second, accuracy has both individual and systematic aspects, but\nthe current discourse stresses the former while ignoring the latter. Third, the discourse failed to recognize\nthe distinction between easy cases and hard cases, which should be taken differently in platform governance.\nAnd fourth, focusing exclusively on the substantive result of decisions overlooks other aspects of content\nmoderation that are also critical for governing the platforms. This part will illustrate these four points in\nturn."}, {"title": "4.1 Can we make all decisions right?", "content": "The accuracy discourse, as reflected in most studies about LLM moderation so far, holds that the goal of\ndesigning and incorporating LLM into moderation is to achieve high percentage of correct decisions \u2013 the\nhigher, the better. If accuracy is the benchmark of evaluation, then the ideal state is that all moderation\ndecisions will be made correctly; in other words, LLM can perfectly tell whether an online post violates the\ncommunity rules. That is the aspiration that LLM developers should pursue.\nSuch discourse has one assumption behind: that there exists, and we can identify, an objective or consensu-\nally endorsed standard of determining right decisions (true positives and true negatives) from errors (false\npositives and false negatives). That assumption, however, does not stand. Determining whether certain"}, {"title": "4.2 Two levels of accuracy", "content": "Current studies on LLM moderation uses accuracy in the individual case level, aiming for making correct\ndecisions in each and every individual case. This perspective ignores that accuracy can also be measured\nin the holistic or system level. System accuracy is not the aggregate of the accuracy scores of individual\ncases. Rather, it refers to the general performance of the whole moderation system, including metrics like\nconsistency, predictability, and the fairness of error distribution across different groups of users, different\ncategories of content, and different periods of time. Two insights can be gained by taking the system level\nof accuracy into account.\nFirst, the perspective from system level further corroborates why perfect accuracy is not a worthwhile goal.\nOne can reasonably anticipate that the moderation of one case to be accurate. But things will get much\nharder if the number of cases arises to millions or billions, and the time for moderating each case is very\nlimited (sometimes only seconds for a post) ([35], 549). Moderation also takes costs, no matter who does the\njob - human or AI. The factors of time, cost, and scale at the system level renders moderation a managerial\nenterprise that must constantly make tradeoffs under limited resources. Errors are to be managed by consid-\nering costs, not to be avoided at all costs. That's why people have widely accepted that it is"}, {"title": "4.3 Easy cases vs. hard cases", "content": "The foregoing analysis revealed several tensions within the practice of content moderation: that accuracy is\nan important metric for moderation but it is also parochial and misleading; that good governance requires\nmaking correct decisions but the scale and resource limit render such task extremely challenging; that con-\ntent moderation is individually similar to judicial adjudication but systematically more akin to administrative\nmanagement. Evelyn Douek is right that perfect accuracy is impossible and \u201csystem thinking", "[i]n most\ncases, identification is a simple matter. The post either clearly violates our policies or it doesn't. Other times,\nidentification is more difficult": [81], "identify and remove a large amount of violating content\u2014often, before anyone sees it\u201d, and when": "echnology misses something or needs more input\u201d, human reviewers will intervene to make their judgment\ncalls ([82]). Such practice reveals that platforms must make, and already have made the strategic choice of\ntreating different cases differently.\nSecond, easy cases and hard cases have also been distinguished in legal systems, for similar reasons (see gen-\nerally [55]; [37]). In most instances, we get highly determinate answers regarding what the law prescribes\n([97], 423): consider our behaviors adjusted according to the law, the quick and clear advice we receive\nfrom lawyers during consultation, and cases that are settled or finalized at trial level ([97], 412-3; [41], 597;\n[110], 225). Sometimes the result to a case is unclear or contested so that value judgments are needed ([41],\n596): think about the cases that are appealed or those with dissenting opinions from the deciding judges. In\nsome sense, the institutional design of litigation costs, hierarchies of courts, and judicial writing of dissents"}, {"title": "4.4 Content moderation is a part of governance", "content": "The accuracy discourse views content moderation as an isolated process, a process in which platform rules\nare mechanically applied to individual cases. Framed in this way, content moderation is like a syllogistic\ngame. But content moderation is never simply the application of rules or the adjudication of disputes. Rather,\nit is, and must be taken as, a constitutive part of the governance system of platforms. Perceiving content\nmoderation as a component of governance will highlight many more metrics other than accuracy."}, {"title": "5 A Legitimacy-Based Framework for Content Moderation", "content": "This article is not the first endeavor that proposes a framework for normatively guiding the content modera-\ntion practice. For instance, ([74], 96) has devised a framework on the basis of two metrics, risk and accuracy.\nThis framework failed to notice that risk is not the only factor of distinguishing moderation cases and ac-\ncuracy is merely one of the several elements that matter. ([94], 1038) has compared the relative strengths\nand weaknesses of machine and human moderators, covering factors such as contextual adaptability, speed,\nand consistency. However, their work overlooks procedural legitimacy and the disparate implications of dif-\nferent cases. Based on the previous discussion, this part introduces a framework that distinguishes between\neasy and hard cases on the one hand, and covers both substantive and procedural aspects of legitimacy on\nthe other.\nScholars have proposed various benchmarks or principles as indexes of legitimacy, such as transparency,\ndemocratic participation, and rule of law ([6], 5; [103], 2-4). Legitimacy has both substantive and procedural\naspects ([19], 379). Substantive legitimacy evaluates the content of decisions \u2013 whether they are correct, fair,\nor conforming to some high values or principles ([85], 676-7). Another crucial component of legitimacy\nis procedural justice, which refers to the approach, manner, and process in which rules are enacted and\nenforced [106]. Justice Marshall has succinctly summarized the two aspects as \u201cthe prevention of unjustified\nor mistaken deprivations and the promotion of participation and dialogue by affected individuals\u201d . This is\nalso the case for content moderation. Users care about both the outcome of their cases and how they have\nbeen treated in the process. The public holds serious concern over how the platforms' moderation system\nperforms as well as to what extent it remains accountable.\nFew existing studies of LLM moderation have drawn a distinction between easy cases and hard cases. One\nexception is ([8], 2), which argues that LLM can automate moderation decisions on clear cases, so that\nhuman moderators can focus on borderline cases that require more expertise. This article further elaborates\non this distinction and suggests different metrics for evaluating the legitimacy of the two groups separately.\nBefore introducing the metrics, one practical issues needs to be addressed: how to separate them \u2013 how to\nscreen the hard cases from the easy ones? This might be a challenging task since \u201cit is not clear how to know\nin advance which areas are safely bulk and which are more controversial, as the landscape of controversy\nchanges over time", "severe, viral,\nnuanced, novel and complex": [81], "account": "n1) Complexity of facts and contexts. Disagreement about fact is one type of causes that makes a case hard\n([58], 93). Lack of clear and comprehensive understanding of the contextual facts of a case may inhibit\njudges or moderators from reaching a clear answer. The Facebook Oversight Board, for example, stated in\nthe famous moderation decision about the suspension of Trump's account, that because Facebook refused to\nprovide detailed information about how the platform's technical design amplified Trump's inciteful posts, the\nBoard could not fully assess whether the removal of account is a necessary measure in that case ([22]). One\npertinent factor here is the category of content. Some content generally requires less contextual knowledge,\nsuch as spam, child porn, and IP infringing materials. Other categories of content may be highly contextual\nand culturally dependent, such as hate speech, which is more likely to make a case hard.\n2) The vagueness of platform rules. Sometimes the moderation rules may be too vague to dictate a clear\nor singular result. Ambiguity of rules is more complicated than non-existence of rules. When there are no\nrules applicable for a case, we can resort to the principle that those which are not prohibited are permitted\n([100], 484-5). But when a rule is so ambiguous in its applicability or that it dictates multiple reasonable,"}, {"title": "5.2 Easy cases: making correct, fast, and transparent decisions", "content": "Easy cases are those with clear answers: either the post has violated platform rules or not, and the type of\nviolation is obvious. Moderating this type of cases is the routine job of moderators. From the legitimacy\nperspective, moderating easy cases must serve two goals: for users, cases should be resolved in a correct\nand timely manner; for platforms, the communicative space must be regulated fairly, efficiently, and openly.\nThese are the goals that are not only achievable but also indispensable for gaining trust for the moderation\npractice. From these goals we can derive three legitimacy metrics for moderating easy cases:\n1) Accuracy. Users expect cases to be decided accurately, and platforms also anticipate right moderation\nin order to gain public trust and to build amicable communities. Accuracy should be measured at both\nindividual-case level and system level. The former refers to the aggregate percentage of correctly decided\ncases, while the latter requires the distribution of accuracy \u2013 or, viewed in the opposite angle, errors \u2013 to be\nfair across different groups of people, categories of content, and periods of time.\n2) Speed. One salient difference between online moderation and offline speech adjudication is the velocity\nand virality of communications in the former context. In the online world, it is more appropriate to say"}, {"title": "5.3 Hard cases: enabling justification and participation", "content": "Hard cases defy clear and consensual answers. For the users, especially the losing party, to accept and\nrespect the decisions of hard cases, the key is not the substantive result, but the process of reaching that\nresult. On the one hand, the contentious decisions must be justified to be accepted; on the other hand, users\nand stakeholders must be fairly treated in the moderation process.\n1) Justification. The more contentious the case, the more it needs to be justified. Jurists like Hart and\nDworkin shared the view that for hard cases, adjudicators need to justify their decisions ([29], 80). Feldman\nargued, in his proposal of establishing the Facebook Oversight Board, that \u201c[when] there is no magic-bullet\nsolution to balancing competing values\n[t]he advantage enjoyed by real-life constitutional courts is\nthat they openly address difficult cases, and so derive credit and legitimacy from being principled", "cases": "while the latter refers to the disclosure\nof moderation details in a systematic, statistical, and holistic manner, the former evaluates the quality of\nexplanation in individual cases. Explanation has been recognized as an important factor of legitimacy: for\nexample, empirical findings show that \u201cusers who did receive some sort of explanation from moderators\nregarding their removal were less likely to have posts removed in the future\u201d ([61], 71). In addition, reading\nexplanations of moderation decisions is an educational opportunity for users to learn and internalize the\ncommunity norms [59].\nTo be sure, explanations should not aim for gaining wide approval on the merit of the decisions. But refrain-\ning from the ambition of achieving consensus on substantive results does not mean that all the substantive\nreasonings are equally persuasive and equally acceptable. Not all explanations can be qualified as justifi-\ncations ([27], 1288). Justificatory effect entails that the explanations provided must reach certain level of\nsubstantive quality. For example, how the decisions address the facts and rules of the case as well as the\ncontext of the controversy, how the decisions respond to users' concern and the public expectation, and how\nthe decisions approach pressing issues like the borrowing of human rights norms into private moderation\ncontext, are all important aspects for measuring the substantive legitimacy of the explanations.\n2) Participation. If justification (reason-giving) earns substantive legitimacy for moderating hard case, then\nparticipation secures the procedural side of legitimacy. That participation is a central tenet of procedural\njustice is \u201cas old as the law\u201d ([99], 308). Scholars found that people would deem algorithms more acceptable\nwhen they are more informed about how the algorithms work and afforded more control in their work ([83],\n4, 20). This corresponds with the general finding that", "when they play a meaningful role in the process\u201d ([60], 1548).\"\n    },\n    {\n      \"title\": \"6 LLM Moderation under the Framework\",\n      \"content\": \"The main argument supporting the accuracy discourse and its sanguine tone can be summarized as this: be-\ncause of LLM": "superior contextual understanding achieved during their extensive pretraining, it is capable\nof making more accurate judgments in moderation tasks, many of which involve contexts and nuances. This\npart will argue that accuracy is not LLM's major field of contribution to content moderation. For easy cases,\nreplacing traditional ML models with LLMs may bring some accuracy bonus, but LLMs also generate addi-\ntional cost and latency; and as easy cases do not involve much contextual complexity, traditional ML models\nalready perform reasonably well in this category. For hard cases, accuracy is not an important metric, since\nthe standard of what is accurate becomes blurred here. Rather, what matters is the quality of justification for\nthe decisions as well as the procedural justice offered by the decisionmaking process."}, {"title": "6.1 Reassessing LLM's accuracy capability", "content": "Examining LLM under the framework proposed by this article and comparing its performance with other\ntypes of moderators, we can see that LLM's superior capability in accuracy comes with significant limita-\ntions and costs.\nFirst, even though LLM has achieved impressive improvement in accuracy, it cannot take the place of human\nexperts in moderating hard cases. It is true that LLM's exposure to large corpus of language equips it with\nmore contextual knowledge and more proficiency in recognizing linguistic variations. This capability is\na competitive advantage compared to the target-trained ML models, the knowledge of which is limited\nto the specific training datasets. And some studies indicated that LLM's accuracy score is higher than the\noutsourced human moderators ([46], 2). But LLM cannot, at least for the short run, reach the level of experts.\nAccording to OpenAI, LLM and ordinary human moderators (with light training) perform equally well in\nlabeling accuracy; but both are outperformed by expert moderators that are well-trained ([111]). Expert\nmoderators, with systematic training, sufficient decision time, and organizational and financial support from\nthe platforms, are the best performers in accuracy \u2013 actually, expert labeling has commonly been used as\nthe"}, {"title": "6.2 LLMs' real potential in content moderation", "content": "If accuracy is not the major field of LLM's contribution, then what areas can LLM play a role in? Recog-\nnizing LLM's limited use in accuracy does not dictate its dim prospect in content moderation. As this part\nwill argue, LLM can make significant contributions to content moderation and platform governance in at\nleast four aspects: to conduct screening of hard cases from easy cases, to provide quality explanations for\nmoderation decisions, to assist human reviewers in getting more contextual information, and to facilitate\nuser participation in a more interactive way.\n1) Distinguishing easy and hard cases is crucial since they should be assigned with different resources and\nstrategies. LLM can help the task of differentiation. One simple way is judging by disagreement. For\nexample, human moderators and LLM can moderate the same content simultaneously; if the two disagree,\nthen it is likely to be a hard case ([8], 4). Another approach is to ask LLM to generate confidence scores for\ncases ([10]): if the confidence score is low, that means the model may not be certain about the result of the\ncase in this scenario, the case is probably hard. To facilitate LLM's capacity on this task, we can fine-tune\nLLM with an annotated dataset that containing easy and hard cases. Due to costs consideration, however,\nplatforms may not choose to screen all the cases through LLM. Rather, they can either use LLM to screen\nonly those cases which have been appealed by users, or those which had already been marked as uncertain\nby traditional ML models. Using LLM as a second screener can supplement the first reviewer with more\ncontextual knowledge; and instead of replacing all the work of traditional ML models, using the two tools\nin a collaborative way is also more financially sustainable.\nResearchers found that LLM exhibits satisfactory performance in pre-filtering content, that is, to remove\nclearly non-violative content from the queue of moderation, as well as to escalate clearly violative content\nfor human review ([8], 11). LLM can also self-evaluate their answers by measuring the level of confidence\n([109], 3). These findings corroborate LLM's technical capacity in distinguishing hard cases from easy ones.\nIn practice, LLM's confidence score can be adjusted according to changing contexts such as the need to\naddress emergencies or crises \u2013 this is another testimony of LLM's adaptability. LLM can also provide\nrationales for its screening. This is especially important for the influential headline cases, for explaining\nwhy they have been escalated for further review is necessary to address the public concerns.\n2) As a type of generative AI, LLM owns one defining feature: the capacity of generating text content.\nUnlike traditional ML models which moderate content in an opaque way, LLM is iterative by design. Such\nfeature, supported by its extensive training, makes it capable of providing high-quality explanations for\nmoderation decisions, enhancing transparency of the process ([86], 7-8). LLM can provide not only reasons\nof decisions to users but also possible ways of revising their posts to make them conforming with the content\npolicies. Such informative process is dialogic and can even starts before the content has been made public"}, {"title": "7 Technical and Legal Implications", "content": "Broadening our discourse on LLM moderation from accuracy to legitimacy is the first step. To better facili-\ntate LLM's contributatory role in content moderation and platform governance, endeavors from both technical\nand legal/policy fields are needed. This part sketches some technical and legal issues that may arise from\nthe paradigm shift. Due to limit of space, the discussion below is only introductory. Instead of offering firm\nconclusions or detailed suggestions, this part aims to point directions for future work.\nOn the technical side, there are two urgent issues that await researchers to explore. One is bias and arbi-\ntrariness, the other is latency and cost. Bias and arbitrariness are, to some extent, an inherent feature of ML\nmodels, because the probabilistic and statistical approach they rely upon necessarily contains randomness\n([49], 8, 17). Apart from the models themselves, arbitrariness and bias can also be introduced in pretraining\nand fine-tuning processes. The datasets for pretraining, though large and diverse, cannot be comprehensive\nenough to cover all knowledge. Developers must select and curate the datasets. Thus, the datasets may\nbe more influenced by some social viewpoints than others, as well as reflect biases of the selectors. Be-\nsides, biases can be produced during fine-tuning because the process \u201cmight overgeneralize alleged patterns\nand... falsely associates those patterns with positive or negative labels"}, {"title": "8 Conclusion", "content": "Using LLM for content moderation is an exciting field to work on. Generative AI and LLM constitute one of\nthe most revolutionary technical breakthroughs in recent years \u2013 and their applications have reshaped many\nindustries. Realizing LLM's full potential in moderation tasks depends upon locating this technology within\nthe governance structure of online platforms and communities. The currently dominant discourse on LLM\nmoderation focuses on accuracy \u2013 how to deploy the tool to increase the ratio of correct decisions. However,\nthis article argues that accuracy is not the best area of contribution by LLM. LLM's accuracy advantage as\ncompared to traditional AI and ordinary human moderators has been largely offset by its weakness in cost\nand latency. Rather, LLM can make meaningful contributions in other aspects, such as distinguishing hard\ncases from easy ones and providing interactive channels for user participation. Moving from accuracy to\nlegitimacy, we can get a clearer picture of LLM's role in moderation and governance.\nContent moderation is an important case of application of Generative AI and LLM. It is a recent instance of\nusing technological advances to improve our social lives. The critical analysis offered by this article affirms\nthe necessity of combining technical explorations with normative inquiries from the socio-legal perspec"}]}