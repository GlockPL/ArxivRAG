{"title": "Towards Symbolic \u03a7\u0391\u0399 \u2013 Explanation Through Human Understandable Logical Relationships Between Features", "authors": ["Thomas Schnake", "Farnoush Rezaei Jafari", "Jonas Lederer", "Ping Xiong", "Shinichi Nakajima", "Stefan Gugler", "Gr\u00e9goire Montavon", "Klaus-Robert M\u00fcller"], "abstract": "Explainable Artificial Intelligence (XAI) plays a crucial role in fostering transparency and trust in AI systems, where traditional XAI approaches typically offer one level of abstraction for explanations, often in the form of heatmaps highlighting single or multiple input features. However, we ask whether abstract reasoning or problem-solving strategies of a model may also be relevant, as these align more closely with how humans approach solutions to problems. We propose a framework, called Symbolic XAI, that attributes relevance to symbolic queries expressing logical relationships between input features, thereby capturing the abstract reasoning behind a model's predictions. The methodology is built upon a simple yet general multi-order decomposition of model predictions. This decomposition can be specified using higher-order propagation-based relevance methods, such as GNN-LRP, or perturbation-based explanation methods commonly used in XAI. The effectiveness of our framework is demonstrated in the domains of natural language processing (NLP), vision, and quantum chemistry (QC), where abstract symbolic domain knowledge is abundant and of significant interest to users. The Symbolic XAI framework provides an understanding of the model's decision-making process that is both flexible for customization by the user and human-readable through logical formulas.", "sections": [{"title": "1. Introduction", "content": "Machine learning (ML) algorithms have increasingly become part of everyday life in both private and public sectors, playing crucial roles in data analysis, prediction, and automation. However, alongside the growing adoption of ML algorithms, there are increasing concerns about the potential risks associated with these models [12, 24]. A key step towards creating safer and trustworthy artificial intelligence (AI) systems is to understand their underlying decision-making strategies, particularly whether they align with human expectations on how problems should be tackled at an abstract level [88, 133, 15, 110, 111]. Since most models are large, highly nonlinear, and involve complex feature interactions, their predictions tend to remain opaque unless equipped with explanation capabilities.\nIn this work, we address the case where the model is not interpretable by design, and interpretability is acquired by means of so-called 'post-hoc' explanation strategies. In recent years, many different post-hoc explanation methods have been proposed, encompassing first-order [6, 5, 78, 112, 96], second-order [36, 79, 59, 127], and higher-order [99, 32] feature interactions. Notably, previous research has mostly focused on first-order explanation methods, which have been applied successfully across many disciplines (see e.g. [65, 66, 69, 101, 104, 37, 4, 55, 124, 10, 58]).\nFor inference, ML models typically do not rely on individual input features, such as a single image pixel or a word in a sentence. Instead, they often rely on complex interactions between various input features to make predictions. This complexity necessitates the development of more abstract explanations to represent these intricate relationships. Such explanations would offer insights that extend beyond individual feature contributions, providing a more holistic view of how different feature interactions influence the model's predictions. Attempts to provide such abstract explanations, which go beyond feature-wise explanations, have been proposed recently in [1, 21, 22]. The goal is to provide explanations that are more human-understandable and closely aligned with human intuition regarding problem solving strategies. The human-understandable explanatory features range from concepts that the model has learned [1] to activation patterns that are relevant for the prediction [21].\nAnother approach to generating abstract explanations is to attribute relevance to the logical relationships between features, rather than to the features themselves. This type of abstraction is intuitive and human-readable, as it mirrors the human tendency to reconsider the logical reasoning behind a prediction. For instance, when we read a sentence, we understand its meaning not just through individual words but also through the grammatical rules that govern their combination. To build trust in the model, an explanation method should be able to recover such grammatical rules, reflecting the underlying logical structure of the model's prediction strategy.\nThere exist already different explanation methods that attribute the relevance of logical relationships include logical conjunctions (\u2227) [59, 32, 36, 92, 132] and logical disjunctions (V) [125]. Additionally, Some ML models are designed to encode these logical relationships, making them inherently more interpretable [35, 22]. However, there is no post-hoc explanation framework that attributes relevance to logical formulas that are composed of a functionally complete set of logical connectives, which express any logical relationship between features.\nWe propose an explanation framework that specifies relevance values for both conjunctive relationships (\u2227) between input features and the absence of features (\u00ac). This enables us to express all possible logical relationships between features. These logical formulas, which we refer to as queries, are configurable and can be tailored to answer any question a user might have about the model's predictions. Furthermore, we provide a method for automatically generating queries that best describe the model's prediction strategy.\nOur framework relies on decomposing the model's prediction into the relevance values of different feature subsets. This decomposition can be defined either as a propagation-based [5, 84, 99] or a perturbation-based [78, 79, 14] approach. The relevance value of a logical formula (query) is computed by filtering the feature subsets where the query holds true and summing their corresponding relevance values. To identify the query that most accurately represents the model's prediction strategy, we search for the query, represented as a binary vector indicating the truth values for feature subsets, that most closely matches the model decomposition. Since the resulting explanatory features are composed of symbolic expressions, we refer to our novel framework as Symbolic XAI.\nFigure 1 previews the insights that we can gain about a model's decision-making strategies using Symbolic XAI. An in-depth analysis of these experiments is provided in Section 4.\nIn Figure 1a), we analyze the sentence \"That was not bad\", which has a positive sentiment. Classical XAI, specifically first-order relevance values achieved using Layer-wise Relevance Propagation (LRP), provides high relevance values for the words \"not\" and \"bad\". In contrast, Symbolic XAI assigns high relevance values to the conjunctive relationship between \"not\" and \"bad\". However, when evaluating the queries \u201cnot\u2227bad\u201d and \u201c\u00acnot \u2227 bad\u201d, which measure the relevance of \"not\" and \"bad\" in the absence of their counterpart, both have low relevance values. This shows that the words \"not\" and \"bad\" marginally influence the prediction without their counterpart, which is a detail that classic \u03a7\u0391\u0399 fails to reveal.\nIn Figure 1b), we see that Classical XAI identifies the mouth as having a high contribution to the prediction, while the eyes are considered to be less important. Symbolic \u03a7\u0391\u0399 reveals that the interaction between the mouth and eyes are less important, whereas the mouth alone, in the absence of the eyes, has a higher contribution. This suggests that the model focuses primarily on the first-order concepts and disregards the interactions between them. Without using Symbolic XAI, this information remains elusive.\nWhen considering the relevance values of single atoms in 1c), the oxygen atoms have the highest variation compared to all other atoms. Despite this variation, no clear trend is visible. In Symbolic XAI, we observe that the pairwise interactions between the oxygen and hydrogen atoms play a crucial role. This result aligns with chemical intuition, as hydrogen moves from one oxygen atom to the other during the reaction. This demonstrates the flexibility of constructing various logical formulas within the Symbolic XAI framework and obtaining their relevance for the prediction. Such a result would not be possible with a rigid explanation method that only considers a fixed set of explanatory features.\nIn summary, the contributions of this paper are:\n1. We introduce a novel Symbolic XAI framework, designed to compute the relevance values of logical formulas composed of a functionally complete set of logical connectives for the model's prediction. This framework is integrated into existing explanation approaches, specifically perturbation-based or propagation-based methods.\n2. We propose a query search strategy aimed at identifying the most expressive queries that accurately reflect the decision-making processes of the models.\n3. We demonstrate the practical usefulness of our framework across three different application domains, providing diverse use cases, such as sentiment analysis, facial expression recognition (FER), and molecular energy prediction in quantum chemistry."}, {"title": "2. Background and Related Work", "content": "Throughout this manuscript, we assume that we are given an ML model f, which takes X = (xI)IEN as input to obtain the prediction f(X). Here, N is the set of feature indices and the model f maps the input X to a scalar value. In the following, we outline several families of explanation methods.\n2.1. From first- to higher-order feature relevance\nFirst-order feature relevance. First-order explanation methods consider the impact of individual features on the model's predictions [2, 5, 84]. For each feature index I \u2208 N, we obtain a relevance value RI to quantify the contribution of xI to the prediction f(X). Popular first-order feature methods include Layer-wise Relevance Propagation (LRP) [5, 84, 85, 97], Integrated Gradients (IG) [112], SHAP [78], and others [130, 13, 93, 89].\nIn [78], many of these methods are classified as additive explanation methods, where the sum of feature contributions reconstruct the prediction, i.e., \u2211I RI = f(X).\nIn our framework, it is also possible to obtain the relevance values of individual features by formulating them as a logical formula. In fact, we observe that some first-order explanation methods, such as Occlusion Sensitivity [13, 130] and SHAP [78], are special cases of our methodology.\nSecond-order feature relevance. Second-order explanation methods consider the impact of pairwise feature interactions on the model's predictions. These feature interactions can either be represented as tuples of two features (I, J) \u2208 N \u00d7 N, where \u00d7 is the Cartesian product, or as sets of two features {I, J} \u2286 N. The difference between these representations is that for tuples, the order of the features matters [127, 39], whereas for sets, it does not [59, 79, 36].\nIn both cases, we obtain a value RIJ, which specifies the relevance of the features I and J to the prediction f(X). The interpretation of RIJ varies by applications. For instance, in BiLRP [36], RIJ expressed by design the similarity between pixels in a similarity model f, and can be extended to other applications (e.g., [65, 37]). When explaining graph neural networks, RIJ describes the relevance of edges in the input graph [127, 39]. In other contexts, the relevance value RIJ quantifies the joint contribution of the features xI and xJ to the prediction f(X) [59, 79].\nSome of these methods are considered additive [36, 79, 59], meaning that the sum of the relevance values of feature pairs equals the model's prediction. For sets of two features, this is expressed as \u2211IJ\u2208N:I<J RIJ = f(X).\nIn our framework, we can also attribute relevance values to sets of two features expressed by logical formulas, specifically using a logical conjunction (\u2227) between the features.\nHigher-order feature relevance. Higher-order explanation methods aim to explain how interactions among multiple features collectively influence a model's predictions, which generalizes the notion of second-order feature relevance. As before, this class of explanation methods can be subdivided into two types: sequences of features [99] and feature sets [32, 48, 44, 13]. In the case of sequences of features, we consider the relevance score RW for a fixed-length sequence W = (I, J, ...) \u2208 N \u00d7 N \u00d7 .... For feature sets, we consider the relevance score RS for each set of feature indices {I, J, ...} = S \u2286 N. The distinction is that for sequences, the order of the features matters, whereas for sets, it does not.\nHigher-order explanation methods find applications in various domains, such as understanding infection chains in disease prediction [125] or analyzing how chains of atoms contribute to specific properties of molecules [125, 99].\nSome of these methods are additive [99, 32, 44]. For instance, in [99], the sum over all feature sequences equals the model's prediction, i.e., \u2211W\u2208N\u00d7N\u00d7... RW = f(X). Similarly, in [32], the sum over all sets of a fixed size k equals the model's prediction, i.e., \u2211S\u2286N:|S|=k RS = f(X).\nNote that in the Symbolic XAI framework, we can also attribute relevance values to higher-order interactions within a set of features by composing a logical formula that describes a logical conjunction (\u2227) between the features."}, {"title": "2.2. Explainable AI beyond first-, second-, and higher-order feature relevance", "content": "A branch of research dating back to the 1990s, known as rule extraction algorithms for deep neural networks [116, 117, 52], aims to summarize the decisions made by neural networks using rule-based algorithms, offering potentially greater interpretability. For example, in [117], the authors approximate different parts of a neural network using Boolean functions and then express the entire network as a logical formula. In Section 3.3, we also present logical formulas that articulate the model's prediction strategy. However, rather than approximating the components of the model, we propose a search algorithm that identifies logical formulas that are comparable to a specific decomposition of the model's prediction.\nIn reinforcement learning, a fundamental principle is that the model employs a strategy (policy) to determine an action [113]. In [28], these policies are represented as symbolic abstractions, which correspond to the contextual information on which decision are based. Since these abstractions are human readable, this approach increases the interpretability of these models. While the use of symbolic abstractions as explanatory features is similar to our framework, our approach applies to any ML model and do not consider reinforcement learning.\nNeural-symbolic learning is an interdisciplinary approach that combines neural networks and symbolic AI to address complex problems that require both symbolic reasoning and statistical learning [25, 26]. In [35], the authors attempt to explain these models for a better interpretation of their decisions. Unlike our framework, which is applicable to any neural networks, their method is restricted to specific neural-symbolic model architectures.\nIn [22], the authors propose a model architecture that is able to explain its predictions by first-order logical formulas. The visual format of their explanatory logical formulas is similar to the queries used in our Symbolic XAI approach. However, their explanation framework is specific to a particular model architecture, whereas ours is more versatile and not tied to any specific architecture.\nIn [91], it has been shown that the output score of a deep neural network can be decomposed into the effects of several interactive concepts. Each interactive concept represents a conjunctive relationship (\u2227) between a set of input features. The authors further extend the conjunctive interaction to include the disjunctive interaction (V). The interactive concepts are then used in [92] and [132] to derive optimal baseline values of input features and to analyze the generalization power of deep neural networks, respectively. It is known that the logical connectives \u2227 and V are not functionally complete [3]. In contrast, our framework employs the connectives \u2227, \u00ac, achieving functional completeness.\nThe authors in [1] propose a propagation-based method to identify specific parts of a neural network that correspond to particular concepts, such as 'eyes' or 'snout' in an image of a dog. The method generates heatmaps for these model-specific concepts, offering a more interpretable way to understand the model's decision-making process. This approach is similar to our work in two ways: First, for computing the relevance of a concept, they mask particular neurons in the model during propagation that correspond to that concept. Similarly, we mask different neurons in the propagation-based approach to obtain a decomposition of the model's prediction (see Section 3.1). Second, they provide a novel abstraction of the explanatory features by considering the relevance of concepts, while we attribute relevance to logical formulas.\nIn [21], the authors describe a technique for extracting relevant subspaces that represent different factors influencing the model's predictions. These subspaces are disentangled from each other, providing insights into the complexity of the prediction task. While their approach focuses on abstract explanatory features through relevant subspaces, our approach considers the relevance of the abstraction of logical formulas to understand the model's predictions."}, {"title": "3. Our Approach", "content": "In this section, we introduce Symbolic XAI (Symb-XAI), an explanation framework that attributes relevance values to logical formulas, also referred to as queries, which express different logical relationships between input features.\nThroughout the rest of this manuscript, we abbreviate the notations for sets, i.e., omit the braces {} for sets with only a few elements, such as {I} or {I, J}, and instead write I or IJ, respectively. We also recall the specification of the ML function f and its input sample X = (xI)IEN from Section 2. Other notations used in this work are summarized in Table A.3 in the Appendix.\nPrevious studies, as discussed in Section 2, explored computing the joint relevance of features for the prediction, which corresponds to the conjunctive relationship (\u2227) between features. Methodologically, there is a straightforward way to calculate such relevance. Assume a function A that measures the relevance values of sets of single features I and J, as well as sets of feature pairs IJ. In order to extend A to measure the relevance of the conjunction between two features, I\u2227J, we can utilize the inclusion-exclusion principle [95]:\n\\(A(I\u2227J) = A(I) + A(J) \u2013 A(IJ)\\)\nOur goal is to extend the relevance function A to any logical formula that expresses logical relationships between features. Specifically, we aim to include the absence of features, described by the logical negation (\u00ac). By combining this with the logical connective \u2227, we can attribute relevance to any logical formula, since \u2227 and \u00ac are known to be functionally complete [3].\nWe base our framework on a decomposition of the model's prediction into detailed components (multi-order terms), a common approach in cooperative game theory [44, 49]. The decomposition of a model's prediction and the different approaches for specifying it are discussed in Section 3.1. Moreover, we provide a general definition for attributing relevance values to logical queries, i.e., extending the relevance function A beyond Equation (1), as outlined in Section 3.2. To gain a comprehensive understanding of which logical relationships are relevant for the model's prediction, we develop a search method to identify the queries that best describe the underlying prediction strategies. This is discussed in Section 3.3."}, {"title": "3.1. Multi-order model decomposition", "content": "As a first step to attribute relevance values to logical formulas, we aim to decompose the model's prediction f(X) into components that uniquely depend on subsets of features L\u2286N. Such a decomposition is formally given by:\n\\(f(x) = \u2211_{L\u2286N} \u03bc_L,\\)\nwhere the contribution term \u03bcL expresses the contribution of the feature interactions in subset L to the prediction. It depends only on the elements in L and does not involve any variables outside of L. We refer to this model decomposition in Equation (2) as multi-order decomposition, and the terms (\u03bcL)L\u2286N as the multi-order terms. We consider \u03bc = (\u03bcL)L\u2286N to be a vector depending on subsets L. We also see a description of the multi-order terms in Figure 2a).\nIn principle, there is no straightforward way to define \u03bc. Therefore, we investigate two different ways to obtain such multi-order model decomposition, each from the perspective of popular explanation approaches.\nPropagation-based approach to define \u03bc. LRP for graph neural networks (GNN-LRP) [99] is an explanation method designed to obtain higher-order explanations for graph neural networks (GNNs), but it is also applicable to other network architectures. This method considers walks W = (I, J, . . . ), which are ordered sequences of input features I, J, . . . \u2208 N, and assigns them a relevance score RW. For more details on how RW is specified, particularly for architectures different from GNNs, such as Transformer models, we refer to Appendix C. As discussed in Section 2, RW expresses the relevance of higher-order interactions between the corresponding features in W. We can use these terms to define the multi-order model decomposition.\nApproach 1. We define the multi-order terms \u03bcL within the propagation-based approach as the sum over all walks that encompass every feature in the subset L. Formally, this is expressed as:\n\\(\u03bc_L = \u2211_{W: set(W)=L} R_W\\)\nwhere set(W) represents the set of all indices within the walk W.\nThis approach ensures that the multi-order terms effectively decompose the prediction f(X), as described in Equation (2). This property holds true because the sum of the relevance values RW for all walks equals the model's prediction f(X) (see e.g. [99]).\nPerturbation-based approach to define \u03bc. An alternative way to specify the multi-order terms in Equation (2) is to use a perturbation-based approach [14, 130, 79, 44, 32]. This approach estimates the model's prediction by focusing on smaller areas of the input, perturbing the features outside the area of interest."}, {"title": "3.2. Calculating relevance of logical formulas", "content": "We aim to compute the relevance of any logical relationships between input features using the multi-order decomposition in Equation (2). A visual description of the problem formulation and how to attribute relevance to a query, can be seen in Figure 4.\nTo understand the advantage of using the multi-order terms from Section 3.1 for relevance attribution of logical formulas, we motivate it with the conjunctive relationship between features, as illustrated in Equation (1). Assuming the relevance A of feature sets Lis measured using Occlusion Sensitivity [130, 13], i.e., A(L) = f(X) \u2212 f(XN\\L), we obtain:\n\\(A(I \u2227 J) = \u2211_{L\u2286N} \u03bc_L \u00b7 1(I \u2208 L \u2227 J \u2208 L)\\)\nwhere \u03bcL represents the multi-order terms from the perturbation-based approach. This is a well-established result and can be verified, for instance, in [49], Section 2. Our goal is to extend the use of multi-order terms \u03bc to define relevance A for more general logical formulas. Additionally, we incorporate a weight for each term \u03bcL to enforce desired properties in the relevance function A.\nDefinition of queries. We explicitly define what a logical formula, called query, is composed of in our explanation framework.\nDefinition 1. A query q is a sequence of symbols, where each symbol is either referencing to a set of feature indices S, or a logical connective of conjunction \u2227 or negation \u00ac. An example is q' = (S1, \u2227, \u00ac, S2), where S1 and S2 are referencing to feature sets. We denote Q as the set of all well-formed queries, meaning q \u2208 Q is a readable combination of symbols, referring each to logical connectives or feature sets. For a query q we abbreviate the notation by omitting the parentheses and commas. In the case of the exemplary q', we write q' = S1 \u2227 \u00acS2.\nDefinition of query relevance. We define the relevance attribution of queries using the multi-order terms in Equation (2).\nDefinition 2. The relevance of a query q \u2208 Q is given by:\n\\(\u0391(\u03b7, \u03bc, q) = \u2211_{L\u2286N} \u03b7_L \u00b7 \u03bc_L \u00b7 \u03bb_L(q)\\)\nHere, \u03b7 = (\u03b7L)L\u2286N is a given weight vector, which introduces customized weights of the relevance attribution, and \u03bb(q) = (\u03bbL(q))L\u2286N is a filter vector, corresponding to the query q, that maps each subset L \u2286 N to a Boolean value.\nTo avoid heavy notation overhead, we do not always specify \u03bc and \u03b7 in the arguments of A when it is clear from the context which weight vector and multi-order terms are being used. We provide a visual description of the relevance attribution of logical formulas in Figure 2a).\nIn the following Sections 3.2.1 and 3.2.2, we provide more details on the filter and weight vectors, respectively.\n3.2.1. Specifying the filter vector of a query\nWe provide a more detailed view on how to obtain a filter vector \u03bb(q) from a query q. An visual description how to specify the filter vector is shown in Figure 5. For an example of the filter vector, see Figure 2a).\nTo reduce ambiguity when specifying symbols of subsets S in a query, we explicitly add in this subsection the prefix 'q:' in front of the query in the argument of the filter vector, such as \u03bb(q : S) for the filter vector of the query q = S.\nThe query of feature presence. We aim to estimate the relevance of the presence of features S, specified by the query q = S. The filter vector \u03bb(q : S) is defined by:\n\\(\u03bb_L(q : S) = 1(S \u2229 L) for L\u2286N\\)\nThis means that for the relevance of q = S using Equation (6), we keep all \u03bcL where S and L intersect. We note that the computation of first-order SHapley Additive exPlanations (SHAP) [78] or Occlusion Sensitivity [130, 13] is equivalent to query relevance of single feature presence, i.e., q = I, with different weighting vectors \u03b7. More details on the weighting function are given in Section 3.2.2."}, {"title": "3.2.2. Specifying weight vectors", "content": "The choice of the weight vector \u03b7 is custom and depends on the user's needs. We aim to offer a straightforward strategy for defining \u03b7 with the consideration of desirable properties of the explanation.\nOcclusion values of queries. One way to specify the weight vector for queries is by setting it to unity, i.e.,\n\\(\u03b7_L = 1 for L\u2286N\\)\nFor the first-order query q = I, such a weight vector leads to the so-called Occlusion Sensitivity [130] or PredDiff [13] method. We therefore call it the occlusion value of queries.\nThis weighting allows for calculating the relevance of some queries without explicitly computing the multi-order terms in Equation (2), which can be computationally expensive. This result is known for the relevance of feature presence, as it is equivalent to Occlusion Sensitivity [130]. We also observed a similar result for logical conjunction in Equation (1) and feature absence in Equation (7). Thus, occlusion values of queries are often computationally efficient.\nShapley values of queries. In classical explanation tasks, it is desirable for the explanation method to exhibit the conservation property. In the context of single-feature explanation methods such as SHAP [78], which we can obtain by attributing the queries q = I, this means that the sum of all feature attributions equals the model's prediction, i.e., \u2211I A(I) = f(X).\nWe now show that there is a weighting function \u03b7 which enforces the conservation property for any set of queries (qk)k=1:\nLemma 1. Let (qk)k=1 be a set of queries with, such that for every subset L \u2286 N we have \u2211k \u03bbL(qk) > 0. If we set the weight vector to be\n\\(\u03b7_L = [\u2211_{k=1}^{M}\u03bb_L(q_k)]^{-1}\\)\nwe get: \u2211k=1 A(\u03b7, \u03bc, qk) = f(X).\nThe proof of this lemma can be found in Appendix B.\nNote that when applying Lemma 1 to the query set (qI)I\u2208N of feature presence qI = I, we obtain the weight vector:\n\\(\u03b7_L = \\frac{1}{L}\\)\nIt is known[44] that the relevance A(qI) with this weight vector coincides with the Shapley value of the feature I. This motivates us to call the relevance A(qk), with \u03b7 given by Lemma 1, the Shapley value of query qk within the set of queries (qk)k=1."}, {"title": "3.3. Automatic specification of expressive queries", "content": "In practice, the query specifications are not always given by the user. The options for queries in Q are large and to find a query that properly expresses the model's prediction is challenging.\nIn this subsection, we aim to describe a search algorithm that expresses and summaries best the prediction strategy of the model. We provide a visual description the query search in Figure 2b).\nProblem formulation. Our framework is based on the multi-order decomposition of the prediction f(X) into the terms \u03bcL, as given in Equation (2). To summarize the prediction strategy of f(X), we aim to find the query q\u2217 for which the filter vector \u03bb(q\u2217) is an adequate description of the multi-order terms \u03bc. This suggests that we need to identify a measure of similarity between \u03bb(q\u2217) and \u03bc. In our case we consider the correlation between \u00b5 and q\u2217, i.e.,\n\\(q^* = arg max corrn (\u03bb(q), \u03bc)\\)\nwhere Q is the set of all queries, as specified in Section 3.2.1, and corr\u03b7 is the weighted correlation by the weighting vector \u03b7. The explicit definition of corr\u03b7 is given in Appendix E. Note that, due to the computational complexity, we compute in practice the multi-order terms \u03bcL only for subsets L that contain at most 4 features."}, {"title": "4. Showcasing the Application of SymbXAI in Different Application Domains", "content": "In the following sections, we demonstrate the use of our SymbXAI framework for selected data and models from the domains of natural language processing, computer vision, and quantum chemistry.\n4.1. Usage in natural language processing\nIn the field of natural language processing (NLP), ML models play a crucial role in learning from text data. These models are employed across a diverse range of applications, such as information extraction [62], text generation [57], and sentiment analysis [109, 128].\nIn the following sections, we demonstrate the usefulness of our framework in the NLP domain, particularly for gaining insights into the predictions of sentiment analysis models. The objective of sentiment analysis is to determine the sentiment expressed in a given sentence or paragraph, such as a movie review conveying an opinion about a film. In Section 4.1.1, we compare our framework's performance with ground-truth annotations. Additionally, in Section 4.1.2, we illustrate how the flexibility of modeling queries within our framework significantly impacts common evaluation metrics for explanation methods. Finally, in Section 4.1.2, we show how to automatically identify expressive queries that best represent the underlying prediction strategies when predicting sentiment from sentences with common grammatical constructions.\nDatasets. The SST dataset [109] comprises 11,855 sentences extracted from movie reviews. Each sentence is subdivided into human-readable subsentences, which are categorized into one of five labels: very negative, negative, neutral, positive, or very positive. This dataset is referred to as SST-5. The annotations of the subsentences were generated by reading each subsentence individually and indicating their associated sentiments.\nAnother version of this dataset, referred to as SST-2, consists of a subset of sentences from SST-5 with compressed label information. Sentences with a positive or very positive label in SST-5 are labeled positive in SST-2. Similarly, sentences with a negative or very negative label in SST-5 are labeled negative in SST-2. Neutral sentences are omitted, and the fine-grained sentiment information of subsentences is also omitted in SST-2.\nThe Movie Reviews dataset [129, 31] contains movie reviews with binary sentiment labels: positive or negative. Each review is accompanied by rationale annotations, which are specific parts of the review that support the sentiment label. These annotations are based on the full-text analysis of the reviews, depending on the context of the whole text.\nIt is essential to highlight the difference in annotation methods between the SST-5 and Movie Reviews datasets. The key distinction lies in the presence or absence of contextual information in these annotations. The annotations for the Movie Reviews dataset are given with the awareness of the whole paragraph, i.e., the context of the subsentences. In contrast, the annotations in the SST-5 dataset lack this context information, meaning they are self-contained and autonomous, not relying on the rest of the sentence. We demonstrate how our framework can capture this contextual aspect.\nExperimental setup. We will use our proposed SymbXAI framework in the NLP domain to interpret predictions of a popular Transformer model, namely BERT [30], which was trained for the sentiment analysis task. Further details about the model can be found in Appendix F.2.\nOur evaluation encompasses a thorough examination and interpretation of the model's predictions on two sentiment analysis datasets: Movie Reviews [129, 31] and Stanford Sentiment Treebank (SST) [109]."}, {"title": "4.1.1. Assessing the relevance of queries against ground-truth values", "content": "We evaluate the quality of the relevance values produced by our framework by comparing them with human annotations.\nIn our experiments, we employ the BERT model, which is pre-trained on the SST-2 dataset, and evaluate its performance on SST-5 to analyze the model's understanding in a more complex setting. We utilize the relevance A(q) for the queries q = S and q = S \u2227 \u00acS, which consider the presence of the subsentence S with and without surrounding features, respectively.\nThe qualitative outcome of the query relevance on a sentence from SST-5 is visualized in Figure 6a), where positive, negative, and neutral sentiments are represented by the colors red, blue, and white, respectively. It is worth noting that since the model has been trained on only two labels, positive and negative, it lacks familiarity with the nuances of neutral sentiment. The results obtained from the query contribution A(S\u2227 \u00acS) demonstrate a stronger alignment with the true labels compared to those derived from the query contribution A(S). This phenomenon can be attributed to the fact that, during the process of generating ground-truth annotations for the SST-5 dataset, annotators were constrained to evaluate individual phrases in isolation, lacking the information to consider the complete context.\nWe illustrate a comparable quantitative observation in Figure 6b). In this experiment, we evaluate the model's competence in previewing the sentiment of each subsentence from the SST-5 dataset, spanning from very negative to very positive. Alongside its mostly accurate identification of positive and negative sentiments, our model demonstrates an understanding of neutral sentiment, even though it was not explicitly trained for it. Furthermore, when the context is disregarded, i.e., A(S\u2227 \u00acS), a more evident correlation emerges between the true labels and the signs of the relevance scores, quantitatively underscoring that this query relevance reflects the method of assigning human annotations to subsentences.\nFigure 6c) provides a quantitative representation of similar findings for the Movie Reviews dataset. We investigate the model's ability to understand the positivity or negativity of the movie rationales. In this figure, we show the relevance distributions of the positive and negative rationales in different contextual settings for the queries. As shown, the model is capable of accurately assigning positive and negative relevance scores to positive and negative rationales, respectively, especially when the full context is taken into account. This underscores that the contribution measure A(S), which incorporates the full context, aligns more closely with human annotations. Further results on the Movie Reviews dataset can be found in Appendix G."}, {"title": "4.1.2. Evaluating SymbXAI via input flipping", "content": "We demonstrate the performance of our framework using the well-established input flipping strategy [96, 14"}]}