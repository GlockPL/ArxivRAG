{"title": "Towards Symbolic \u03a7\u0391\u0399 \u2013 Explanation Through Human Understandable Logical Relationships Between Features", "authors": ["Thomas Schnake", "Farnoush Rezaei Jafari", "Jonas Lederer", "Ping Xiong", "Shinichi Nakajima", "Stefan Gugler", "Gr\u00e9goire Montavon", "Klaus-Robert M\u00fcller"], "abstract": "Explainable Artificial Intelligence (XAI) plays a crucial role in fostering transparency and trust in AI systems, where traditional XAI approaches typically offer one level of abstraction for explanations, often in the form of heatmaps highlighting single or multiple input features. However, we ask whether abstract reasoning or problem-solving strategies of a model may also be relevant, as these align more closely with how humans approach solutions to problems. We propose a framework, called Symbolic XAI, that attributes relevance to symbolic queries expressing logical relationships between input features, thereby capturing the abstract reasoning behind a model's predictions. The methodology is built upon a simple yet general multi-order decomposition of model predictions. This decomposition can be specified using higher-order propagation-based relevance methods, such as GNN-LRP, or perturbation-based explanation methods commonly used in XAI. The effectiveness of our framework is demonstrated in the domains of natural language processing (NLP), vision, and quantum chemistry (QC), where abstract symbolic domain knowledge is abundant and of significant interest to users. The Symbolic XAI framework provides an understanding of the model's decision-making process that is both flexible for customization by the user and human-readable through logical formulas.", "sections": [{"title": "1. Introduction", "content": "Machine learning (ML) algorithms have increasingly become part of everyday life in both private and public sectors, playing crucial roles in data analysis, prediction, and automation. However, alongside the growing adoption of ML algorithms, there are increasing concerns about the potential risks associated with these models [12, 24]. A key step towards creating safer and trustworthy artificial intelligence (AI) systems is to understand their underlying decision-making strategies, particularly whether they align with human expectations on how problems should be tackled at an abstract level [88, 133, 15, 110, 111]. Since most models are large, highly nonlinear, and involve complex feature interactions, their predictions tend to remain opaque unless equipped with explanation capabilities.\nIn this work, we address the case where the model is not interpretable by design, and interpretability is acquired by means of so-called 'post-hoc' explanation strategies. In recent years, many different post-hoc explanation methods have been proposed, encompassing first-order [6, 5, 78, 112, 96], second-order [36, 79, 59, 127], and higher-order [99, 32] feature interactions. Notably, previous research has mostly focused on first-order explanation methods, which have been applied successfully across many disciplines (see e.g. [65, 66, 69, 101, 104, 37, 4, 55, 124, 10, 58]).\nFor inference, ML models typically do not rely on individual input features, such as a single image pixel or a word in a sentence. Instead, they often rely on complex interactions between various input features to make predictions. This complexity necessitates the development of more abstract explanations to represent these intricate relationships. Such explanations would offer insights that extend beyond individual feature contributions, providing a more holistic view of how different feature interactions influence the model's predictions. Attempts to provide such abstract explanations, which go beyond feature-wise explanations, have been proposed recently in [1, 21, 22]. The goal is to provide explanations that are more human-understandable and closely aligned with human intuition regarding problem solving strategies. The human-understandable explanatory features range from concepts that the model has learned [1] to activation patterns that are relevant for the prediction [21].\nAnother approach to generating abstract explanations is to attribute relevance to the logical relationships between features, rather than to the features themselves. This type of abstraction is intuitive and human-readable, as it mirrors the human tendency to reconsider the logical reasoning behind a prediction. For instance, when we read a sentence, we understand its meaning not just through individual words but also through the grammatical rules that govern their combination. To build trust in the model, an explanation method should be able to recover such grammatical rules, reflecting the underlying logical structure of the model's prediction strategy.\nThere exist already different explanation methods that attribute the relevance of logical relationships include logical conjunctions (^) [59, 32, 36, 92, 132] and logical disjunctions (V) [125]. Additionally, Some ML models are designed to encode these logical relationships, making them inherently more interpretable [35, 22]. However, there is no post-hoc explanation framework that attributes relevance to logical formulas that are composed of a functionally complete set of logical connectives, which express any logical relationship between features.\nWe propose an explanation framework that specifies relevance values for both conjunctive relationships (\u06f8) between input features and the absence of features (\u00ac). This enables us to express all possible logical relationships between features. These logical formulas, which we refer to as queries, are configurable and can be tailored to answer any question a user might have about the model's predictions. Furthermore, we provide a method for automatically generating queries that best describe the model's prediction strategy.\nOur framework relies on decomposing the model's prediction into the relevance values of different feature subsets. This decomposition can be defined either as a propagation-based [5, 84, 99] or a perturbation-based [78, 79, 14] approach."}, {"title": "2. Background and Related Work", "content": "Throughout this manuscript", "N": "I"}, {"title": "3. Our Approach", "content": "In this section, we introduce Symbolic XAI (Symb-XAI), an explanation framework that attributes relevance values to logical formulas, also referred to as queries, which express different logical relationships between input features.\nThroughout the rest of this manuscript, we abbreviate the notations for sets, i.e., omit the braces \\{\\} for sets with only a few elements, such as \\{I\\} or \\{I, J\\}, and instead write I or IJ, respectively. We also recall the specification of the ML function $f$ and its input sample $X = (x_I)_{I\\in N}$ from Section 2. Other notations used in this work are summarized in Table A.3 in the Appendix.\nPrevious studies, as discussed in Section 2, explored computing the joint relevance of features for the prediction, which corresponds to the conjunctive relationship (^) between features. Methodologically, there is a straightforward way to calculate such relevance. Assume a function $A$ that measures the relevance values of sets of single features $I$ and $J$, as well as sets of feature pairs $IJ$. In order to extend $A$ to measure the relevance of the conjunction between two features, $I\\land J$, we can utilize the inclusion-exclusion principle [95]:\n$A(I\\land J) = A(I) + A(J) - A(IJ)$ (1)\nOur goal is to extend the relevance function $A$ to any logical formula that expresses logical relationships between features. Specifically, we aim to include the absence of features, described by the logical negation (\u00ac). By combining this with the logical connective \u039b, we can attribute relevance to any logical formula, since \u039b and \u00ac are known to be functionally complete [3].\nWe base our framework on a decomposition of the model's prediction into detailed components (multi-order terms), a common approach in cooperative game theory [44, 49]. The decomposition of a model's prediction and the different approaches for specifying it are discussed in Section 3.1. Moreover, we provide a general definition for attributing relevance values to logical queries, i.e., extending the relevance function $A$ beyond Equation (1), as outlined in Section 3.2. To gain a comprehensive understanding of which logical relationships are relevant for the model's prediction, we develop a search method to identify the queries that best describe the underlying prediction strategies. This is discussed in Section 3.3.\n3.1. Multi-order model decomposition\nAs a first step to attribute relevance values to logical formulas, we aim to decompose the model's prediction $f(X)$ into components that uniquely depend on subsets of features $L \\subset N$. Such a decomposition is formally given by:\n$f(X) = \\sum_{L\\subset N} \\mu_L$ (2)\nwhere the contribution term $\\mu_L$ expresses the contribution of the feature interactions in subset $L$ to the prediction. It depends only on the elements in $L$ and does not involve any variables outside of $L$. We refer to this model decomposition in Equation (2) as multi-order decomposition, and the terms $(\\mu_L)_{L \\subset N}$ as the multi-order terms. We consider $\\mu = (\\mu_L)_{L \\subset N}$ to be a vector depending on subsets $L$. We also see a description of the multi-order terms in Figure 2a).\nIn principle, there is no straightforward way to define $\\mu$. Therefore, we investigate two different ways to obtain such multi-order model decomposition, each from the perspective of popular explanation approaches.\nPropagation-based approach to define $\\mu$. LRP for graph neural networks (GNN-LRP) [99] is an explanation method designed to obtain higher-order explanations for graph neural networks (GNNs), but it is also applicable to other network architectures. This method considers walks $W = (I, J, ...)$, which are ordered sequences of input features $I, J, ... \\in N$, and assigns them a relevance score $R_W$. For more details on how $R_W$ is specified, particularly for architectures different from GNNs, such as Transformer models, we refer to Appendix C. As discussed in Section 2, $R_W$ expresses the relevance of higher-order interactions between the corresponding features in $W$. We can use these terms to define the multi-order model decomposition.\nApproach 1. We define the multi-order terms $\\mu_L$ within the propagation-based approach as the sum over all walks that encompass every feature in the subset $L$. Formally, this is expressed as:\n$\\mu_L = \\sum_{W: set(W)=L} R_W$ (3)\nwhere $set(W)$ represents the set of all indices within the walk $W$.\nThis approach ensures that the multi-order terms effectively decompose the prediction $f(X)$, as described in Equation (2). This property holds true because the sum of the relevance values $R_W$ for all walks equals the model's prediction $f(X)$ (see e.g. [99]).\nPerturbation-based approach to define $\\mu$. An alternative way to specify the multi-order terms in Equation (2) is to use a perturbation-based approach [14, 130, 79, 44, 32]. This approach estimates the model's prediction by focusing on smaller areas of the input, perturbing the features outside the area of interest."}, {"title": "3.1. Multi-order model decomposition", "content": "Formally, we consider the input sample $X_S$, which is equivalent to the original input $X$ for features in the subset $S$, i.e., $(X_S)_I = x_I$ for $I \\in S$. The rest of $X_S$ is perturbed, for example, by setting it to a constant value or using an inpainting method [78, 13]. The prediction $f(X_S)$ of the original model $f$ on this modified input $X_S$ estimates the model's prediction for the smaller input area. Additionally, if all information is perturbed, $S = \\{\\emptyset\\}$, we assume that the model predicts zero, i.e., $f(X_{\\emptyset}) = 0$. We can use this approach to specify the multi-order decomposition of the model's prediction.\nApproach 2. We define the multi-order terms $\\mu_L$ within perturbation-based approach as the Harsanyi dividend of $f(X_C)$, given by:\n$\\mu_L = \\sum_{S \\subset L} (-1)^{|L|-|S|} f(X_S)$ (4)\nThe Harsanyi dividends are common in cooperative game theory [54].\nThe Harsanyi dividend specifically measures the part of the contribution that arises from the dependencies among all features in $L$. Denoting the input features by $I, J,...,$ the first two orders of the Harsanyi dividends are given as:\n$\\mu_I = f(X_I)$,\n$\\mu_{IJ} = f(X_{IJ}) - f(X_I) - f(X_J)$\nIt is well known that Harsanyi dividends ensure that the multi-order terms effectively decompose the prediction, as desired in Equation (2) (see e.g. [44]).\nIn fact, the two approaches to specify $\\mu$ in Equations (3) and (4) are related. In the context of higher-order propagation-based explanation methods, the common way to specify the relevance of a subset of features $S$ is by summing over all walks that can be composed with features in $S$, i.e., $R_S = \\sum_{W \\in S \\times S \\times S...} R_W$. This concept was proposed in [99] and further developed with computational acceleration in [125].\nWe can now use the subgraph relevance to connect the two approaches. If we replace $f(X_S)$ with $R_S$ in Equation (4) and then compute the Harsanyi dividends of $R_S$, we obtain exactly the multi-order terms in Equation (3). This implies that it is methodologically consistent to use the relevance of subsets $R_S$ to estimate the model's prediction on a smaller area of the input, as used in the context of perturbation-based method. We provide a proof of this finding in Appendix D.\nShowcasing multi-order terms on synthetic data. Figure 3 shows multi-order terms generated from the predictions of sentiment classification and FER models. For the sentence \"not very good\", Figure 3a) reveals relatively high values for the multi-order terms $\\mu_{\\text{not very good}}$ (first row) and $\\mu_{\\text{not good}}$ (third row), while the other terms play weaker roles. This indicates that the model finds the combination of \"not\" and \"good\" to be significant for the prediction, highlighting the negation of a positive word.\nIn the FER experiment shown in Figure 3b), the multi-order terms $\\mu_{\\text{rest}}$ (fifth row) and $\\mu_{\\text{mouth rest}}$ (second row) have the highest values. This suggests that the 'rest' segment, which includes all pixels except those corresponding to the 'eyes' and 'mouth', significantly influences the model's prediction, particularly when combined with the 'mouth' segment. This shows that the incorporation of the 'rest' segment is crucial when prediction the emotion of this image.\nA challenge when working with the multi-order terms is that their number increases exponentially with the number of input features. This makes it particularly difficult to draw conclusions about the prediction strategies solely by examining the values of these terms. This challenge motivates us to summarize the multi-order terms into human-understandable explanations: the relevance values of logical formulas. In the following sections, we will elucidate how to specify these logical formulas and calculate their relevance values."}, {"title": "3.2. Calculating relevance of logical formulas", "content": "We aim to compute the relevance of any logical relationships between input features using the multi-order decomposition in Equation (2). A visual description of the problem formulation and how to attribute relevance to a query, can be seen in Figure 4.\nTo understand the advantage of using the multi-order terms from Section 3.1 for relevance attribution of logical formulas, we motivate it with the conjunctive relationship between features, as illustrated in Equation (1). Assuming the relevance A of feature sets $L$ is measured using Occlusion Sensitivity [130, 13], i.e., $A(L) = f(X) - f(X_{N\\backslash C})$, we obtain:\n$A(I \\land J) = \\sum_{L\\subset N} \\mu_L \\cdot 1(I \\in L \\land J \\in L)$ (5)\nwhere $\\mu_L$ represents the multi-order terms from the perturbation-based approach. This is a well-established result and can be verified, for instance, in [49], Section 2. Our goal is to extend the use of multi-order terms $\\mu$ to define relevance A for more general logical formulas. Additionally, we incorporate a weight for each term $\\mu_L$ to enforce desired properties in the relevance function A.\nDefinition of queries. We explicitly define what a logical formula, called query, is composed of in our explanation framework.\nDefinition 1. A query $q$ is a sequence of symbols, where each symbol is either referencing to a set of feature indices $S$, or a logical connective of conjunction $\\land$ or negation $\u00ac$. An example is $q' = (S_1, \\land, \u00ac, S_2)$, where $S_1$ and $S_2$ are referencing to feature sets. We denote $Q$ as the set of all well-formed queries, meaning $q \\in Q$ is a readable combination of symbols, referring each to logical connectives or feature sets. For a query $q$ we abbreviate the notation by omitting the parentheses and commas. In the case of the exemplary $q'$, we write $q' = S_1 \\land \u00acS_2$.\nDefinition of query relevance. We define the relevance attribution of queries using the multi-order terms in Equation (2).\nDefinition 2. The relevance of a query $q \\in Q$ is given by:\n$A(\\eta, \\mu, q) = \\sum_{L \\subset N} \\eta_L \\cdot \\mu_L \\cdot \\lambda_L(q)$ (6)\nHere, $\\eta = (\\eta_L)_{L \\subset N}$ is a given weight vector, which introduces customized weights of the relevance attribution, and $\\lambda(q) = (\\lambda_L(q))_{L \\subset N}$ is a filter vector, corresponding to the query $q$, that maps each subset $L \\subset N$ to a Boolean value.\nTo avoid heavy notation overhead, we do not always specify $\\mu$ and $\\eta$ in the arguments of A when it is clear from the context which weight vector and multi-order terms are being used. We provide a visual description of the relevance attribution of logical formulas in Figure 2a).\nIn the following Sections 3.2.1 and 3.2.2, we provide more details on the filter and weight vectors, respectively.\n3.2.1. Specifying the filter vector of a query\nWe provide a more detailed view on how to obtain a filter vector $\\lambda(q)$ from a query $q$. An visual description how to specify the filter vector is shown in Figure 5. For an example of the filter vector, see Figure 2a).\nTo reduce ambiguity when specifying symbols of subsets $S$ in a query, we explicitly add in this subsection the prefix 'q:' in front of the query in the argument of the filter vector, such as $\\lambda(q: S)$ for the filter vector of the query $q = S$.\nThe query of feature presence. We aim to estimate the relevance of the presence of features $S$, specified by the query $q = S$. The filter vector $\\lambda(q: S)$ is defined by:\n$\\lambda_L(q: S) = 1(S \\cap L)$ for $L\\subset N$\nThis means that for the relevance of $q = S$ using Equation (6), we keep all $\\mu_L$ where $S$ and $L$ intersect. We note that the computation of first-order SHapley Additive exPlanations (SHAP) [78] or Occlusion Sensitivity [130, 13] is equivalent to query relevance of single feature presence, i.e., $q = I$, with different weighting vectors $\\eta$. More details on the weighting function are given in Section 3.2.2."}, {"title": "3.2.2. Specifying weight vectors", "content": "The query of feature absence. Conversely, we aim to estimate the relevance of the absence of features S, specified by the query $q = \u00acS$. The filter vector $\\lambda(q : \u00acS)$ is defined by:\n$\\lambda_L(q : \u00acS) = 1(S \\subseteq L^C)$ for $L\\subset N$\nwhere $L^C = N \\backslash L$ is the complement set of $L$. When computing the relevance of the query $q = \u00acS$ in Equation (6), we only sum over those $\\mu_L$ where S and L are disjoint.\nTo the best of our knowledge, the relevance of the absence of features has not been proposed before. When using the perturbation-based approach for specifying $\\mu$, as in Equation (4), the attribution of the absence of the feature $I$, i.e., $q = \u00acI$, with a weight vector $\\eta = 1$ in Equation (6), yields:\n$A(q : \u00acI) = f(X_{N\\backslash I})$ (7)\nwhich is the model's prediction when the feature I is absent.\nLogical conjunction between queries. To build more complex queries, we aim to estimate the relevance of the conjunction (\u039b) between queries $q_1, q_2 \\in Q$, i.e., $q = q_1 \\land q_2$. The filter vector $\\lambda(q: q_1 \\land q_2)$ is defined by:\n$\\lambda_L(q: q_1 \\land q_2) = \\lambda_L(q : q_1) \\cdot \\lambda_L(q: q_2)$ for $L\\subset N$\nThis means $\\lambda_L(q: q_1 \\land q_2)$ is true if and only if both $\\lambda_L(q : q_1)$ and $\\lambda_L(q: q_2)$ are true. This rule is applied recursively until we reach the cases that $q_1$ and $q_2$ either express the query of feature presence or absence, as described above.\nWe specified the filter function for queries which are composed of feature subsets, the logical connectives $\u00ac$ and \u039b, which enables us to obtain the query relevance for any query $q \\in Q$.\n3.2.2. Specifying weight vectors\nThe choice of the weight vector $\\eta$ is custom and depends on the user's needs. We aim to offer a straightforward strategy for defining $\\eta$ with the consideration of desirable properties of the explanation.\nOcclusion values of queries. One way to specify the weight vector for queries is by setting it to unity, i.e.,\n$\\eta_L = 1$ for $L\\subset N$\nFor the first-order query $q = I$, such a weight vector leads to the so-called Occlusion Sensitivity [130] or PredDiff [13] method. We therefore call it the occlusion value of queries.\nThis weighting allows for calculating the relevance of some queries without explicitly computing the multi-order terms in Equation (2), which can be computationally expensive. This result is known for the relevance of feature presence, as it is equivalent to Occlusion Sensitivity [130]. We also observed a similar result for logical conjunction in Equation (1) and feature absence in Equation (7). Thus, occlusion values of queries are often computationally efficient."}, {"title": "3.3. Automatic specification of expressive queries", "content": "Shapley values of queries. In classical explanation tasks, it is desirable for the explanation method to exhibit the conservation property. In the context of single-feature explanation methods such as SHAP [78], which we can obtain by attributing the queries $q = I$, this means that the sum of all feature attributions equals the model's prediction, i.e., $\\sum_I A(I) = f(X)$.\nWe now show that there is a weighting function $\\eta$ which enforces the conservation property for any set of queries $(q_k)_{k=1}^M$:\nLemma 1. Let $(q_k)_{k=1}^M$ be a set of queries with, such that for every subset $L \\subset N$ we have $\\sum_{k=1}^M \\lambda_L(q_k) > 0$. If we set the weight vector to be\n$\\eta_L = (\\sum_{k=1}^M \\lambda_L(q_k))^{-1}$\nwe get: $\\sum_{k=1}^M A(\\eta, \\mu, q_k) = f(X)$.\nThe proof of this lemma can be found in Appendix B.\nNote that when applying Lemma 1 to the query set $(q_I)_{I \\in N}$ of feature presence $q_I = I$, we obtain the weight vector:\n$\\eta_L = \\frac{1}{|L|}$\nIt is known [44] that the relevance $A(q_I)$ with this weight vector coincides with the Shapley value of the feature $I$. This motivates us to call the relevance $A(q_k)$, with $\\eta$ given by Lemma 1, the Shapley value of query $q_k$ within the set of queries $(q_k)_{k=1}^M$.\n3.3. Automatic specification of expressive queries\nIn practice, the query specifications are not always given by the user. The options for queries in Q are large and to find a query that properly expresses the model's prediction is challenging.\nIn this subsection, we aim to describe a search algorithm that expresses and summaries best the prediction strategy of the model. We provide a visual description the query search in Figure 2b).\nProblem formulation. Our framework is based on the multi-order decomposition of the prediction $f(X)$ into the terms $\\mu_L$, as given in Equation (2). To summarize the prediction strategy of $f(X)$, we aim to find the query $q^*$ for which the filter vector $\\lambda(q^*)$ is an adequate description of the multi-order terms $\\mu$. This suggests that we need to identify a measure of similarity between $\\lambda(q^*)$ and $\\mu$. In our case we consider the correlation between $\\mu$ and $q^*$, i.e.,\n$q^* = \\arg \\max_{q\\in Q} corr_{\\eta}(\\lambda(q), \\mu)$ (8)\nwhere $Q$ is the set of all queries, as specified in Section 3.2.1, and $corr_{\\eta}$ is the weighted correlation by the weighting vector $\\eta$. The explicit definition of $corr_{\\eta}$ is given in Appendix E. Note that, due to the computational complexity, we compute in practice the multi-order terms $\\mu_L$ only for subsets $L$ that contain at most 4 features."}, {"title": "4. Showcasing the Application of SymbXAI in Different Application Domains", "content": "In the following sections, we demonstrate the use of our SymbXAI framework for selected data and models from the domains of natural language processing, computer vision, and quantum chemistry.\n4.1. Usage in natural language processing\nIn the field of natural language processing (NLP), ML models play a crucial role in learning from text data. These models are employed across a diverse range of applications, such as information extraction [62], text generation [57], and sentiment analysis [109, 128].\nIn the following sections, we demonstrate the usefulness of our framework in the NLP domain, particularly for gaining insights into the predictions of sentiment analysis models. The objective of sentiment analysis is to determine the sentiment expressed in a given sentence or paragraph, such as a movie review conveying an opinion about a film. In Section 4.1.1, we compare our framework's performance with ground-truth annotations. Additionally, in Section 4.1.2, we illustrate how the flexibility of modeling queries within our framework significantly impacts common evaluation metrics for explanation methods. Finally, in Section 4.1.2, we show how to automatically identify expressive queries that best represent the underlying prediction strategies when predicting sentiment from sentences with common grammatical constructions.\nDatasets. The SST dataset [109] comprises 11,855 sentences extracted from movie reviews. Each sentence is subdivided into human-readable subsentences, which are categorized into one of five labels: very negative, negative, neutral, positive, or very positive. This dataset is referred to as SST-5. The annotations of the subsentences were generated by reading each subsentence individually and indicating their associated sentiments.\nAnother version of this dataset, referred to as SST-2, consists of a subset of sentences from SST-5 with compressed label information. Sentences with a positive or very positive label in SST-5 are labeled positive in SST-2. Similarly, sentences with a negative or very negative label in SST-5 are labeled negative in SST-2. Neutral sentences are omitted, and the fine-grained sentiment information of subsentences is also omitted in SST-2.\nThe Movie Reviews dataset [129, 31] contains movie reviews with binary sentiment labels: positive or negative. Each review is accompanied by rationale annotations, which are specific parts of the review that support the sentiment label. These annotations are based on the full-text analysis of the reviews, depending on the context of the whole text.\nIt is essential to highlight the difference in annotation methods between the SST-5 and Movie Reviews datasets. The key distinction lies in the presence or absence of contextual information in these annotations. The annotations for the Movie Reviews dataset are given with the awareness of the whole paragraph, i.e., the context of the subsentences. In contrast, the annotations in the SST-5 dataset lack this context information, meaning they are self-contained and autonomous, not relying on the rest of the sentence. We demonstrate how our framework can capture this contextual aspect.\nExperimental setup. We will use our proposed SymbXAI framework in the NLP domain to interpret predictions of a popular Transformer model, namely BERT [30], which was trained for the sentiment analysis task. Further details about the model can be found in Appendix F.2.\nOur evaluation encompasses a thorough examination and interpretation of the model's predictions on two sentiment analysis datasets: Movie Reviews [129, 31] and Stanford Sentiment Treebank (SST) [109]."}, {"title": "4.1.1. Assessing the relevance of queries against ground-truth values", "content": "We evaluate the quality of the relevance values produced by our framework by comparing them with human annotations.\nIn our experiments, we employ the BERT model, which is pre-trained on the SST-2 dataset, and evaluate its performance on SST-5 to analyze the model's understanding in a more complex setting. We utilize the relevance A(q) for the queries q = S and q = S ^ \u00abS, which consider the presence of the subsentence S with and without surrounding features, respectively.\nThe qualitative outcome of the query relevance on a sentence from SST-5 is visualized in Figure 6a), where positive, negative, and neutral sentiments are represented by the colors red, blue, and white, respectively. It is worth noting that since the model has been trained on only two labels, positive and negative, it lacks familiarity with the nuances of neutral sentiment. The results obtained from the query contribution $A(S \\land \u00acS)$ demonstrate a stronger alignment with the true labels compared to those derived from the query contribution A(S). This phenomenon can be attributed to the fact that, during the process of generating ground-truth annotations for the SST-5 dataset, annotators were constrained to evaluate individual phrases in isolation, lacking the information to consider the complete context.\nWe illustrate a comparable quantitative observation in Figure 6b). In this experiment, we evaluate the model's competence in previewing the sentiment of each subsentence from the SST-5 dataset, spanning from very negative to very positive. Alongside its mostly accurate identification of positive and negative sentiments, our model demonstrates an understanding of neutral sentiment, even though it was not explicitly trained for it. Furthermore, when the context is disregarded, i.e., $A(S \\land \u00acS)$, a more evident correlation emerges between the true labels and the signs of the relevance scores, quantitatively underscoring that this query relevance reflects the method of assigning human annotations to subsentences.\nFigure 6c) provides a quantitative representation of similar findings for the Movie Reviews dataset. We investigate the model's ability to understand the positivity or negativity of the movie rationales. In this figure, we show the relevance distributions of the positive and negative rationales in different contextual settings for the queries. As shown, the model is capable of accurately assigning positive and negative relevance scores to positive and negative rationales, respectively, especially when the full context is taken into account. This underscores that the contribution measure A(S), which incorporates the full context, aligns more closely with human annotations. Further results on the Movie Reviews dataset can be found in Appendix G."}, {"title": "4.1.2. Evaluating SymbXAI via input flipping", "content": "We demonstrate the performance of our framework using the well-established input flipping"}]}