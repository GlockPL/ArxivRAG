{"title": "Large Language Models as Reliable Knowledge Bases?", "authors": ["Danna Zheng", "Mirella Lapata", "Jeff Z. Pan"], "abstract": "The NLP community has recently shown a growing interest in leveraging Large Language Models (LLMs) for knowledge-intensive tasks, viewing LLMs as potential knowledge bases (KBs). However, the reliability and extent to which LLMs can function as KBs remain under-explored. While previous studies suggest LLMs can encode knowledge within their parameters, the amount of parametric knowledge alone is not sufficient to evaluate their effectiveness as KBs. This study defines criteria that a reliable LLM-as-KB should meet, focusing on factuality and consistency, and covering both seen and unseen knowledge. We develop several metrics based on these criteria and use them to evaluate 26 popular LLMs, while providing a comprehensive analysis of the effects of model size, instruction tuning, and in-context learning (ICL). Our results paint a worrying picture. Even a high-performant model like GPT-3.5-turbo is not factual or consistent, and strategies like ICL and fine-tuning are unsuccessful at making LLMs better KBs.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs), pretrained on extensive text corpora, have demonstrated the ability to implicitly encode various types of knowledge within their weights, without requiring human supervision. As a result, many recent studies (Chuang et al., 2023; Yu et al., 2023; Dhingra et al., 2022; Sung et al., 2021; Wang et al., 2020) aim to analyze the relationship between LLMs and KBs, and even explore whether LLMs can replace KBs (Sun et al., 2023; Mruthyunjaya et al., 2023; Heinzerling and Inui, 2021).\nHowever, whether current LLMs can serve as reliable KBs and how to evaluate their performance in this role remains largely unexplored. Existing studies (Sun et al., 2023; Wang et al., 2021; Roberts et al., 2020) often implicitly assume that the LLM's ability to retain knowledge is sufficient for it to function as a KB. Typically, these studies employ two methods: (1) converting knowledge graphs into natural language questions using templates and evaluating LLM ability to answer these questions, by measuring the amount of knowledge therein (Petroni et al., 2019; Sun et al., 2023); and (2) pre-training LLMs on passages/triples containing knowledge and then assessing their ability to answer related questions, thereby quantifying their knowledge retention (Wang et al., 2021; He et al., 2024). While these methods demonstrate that LLMs can recall knowledge, memorizing facts is not the sole criterion for being a reliable KB (AlKhamissi et al., 2022).\nWhat criteria then should a LLM meet to function as a reliable KB? Discussion on this topic has been limited, and there is no agreement on the definition of these criteria. AlKhamissi et al. (2022) argue that LLMs ought to excel at five aspects (i.e., access, edit, consistency, reasoning, explainability, and interpretability) if they are to be considered a KB. However, they do not outline specific metrics to evaluate the extent to which LLMS act as KBs. Besides, we argue that evaluating LLMs against the characteristics of KBs may not be entirely appropriate due to their different data storage structures. Instead, we should consider the specific properties of LLMs when assessing their suitability as KBs.\nOur research addresses these gaps and aims to establish a more nuanced understanding of LLMs-as-KBs. We evaluate and compare the reliability of different LLMs functioning as KBs in answering factoid questions. Specifically, our work makes the following contributions:"}, {"title": "2 What is a Reliable LLM-as-KB?", "content": "In simple terms, a LLM functions as a reliable KB if it consistently provides factual responses. Evaluating the reliability of LLMs as KBs primarily involves assessing two critical dimensions, namely factuality and consistency."}, {"title": "2.1 Factuality", "content": "Factuality refers to the quality of being factual or based on fact. Run-of-the-mill KBs, stored on physical servers or cloud platforms, deliver information directly in response to queries. If the requested data is unavailable, these systems typically return a null response. In contrast, LLMs are probabilistic models that excel at next word prediction based on the given context, rather than storing explicit information in defined locations. This architecture allows LLMs to generate responses that seem plausible, regardless of whether the content was included in their training data. Consequently, LLMs typically produce three types of responses: correct, uninformative, and wrong.\nOngoing efforts (Chen et al., 2023; Wang et al., 2024) to evaluate the factuality of LLMs often hinge on the models' correct rates in responding to factual QA datasets. However, this approach has notable limitations. Firstly, many studies (Lin et al., 2022; Sun et al., 2023) do not specify whether the dataset's scope of knowledge was included in the LLM's training data. This omission can lead to unfair comparisons between models, especially if the knowledge being tested is within the training scope of one model but not another. Secondly, the assumption that a higher correct rate indicates greater factuality is problematic. For example, consider models A and B shown in Figure 1, and assume they are being evaluated on a test dataset covering knowledge they both have seen during training. In this case, focusing solely on the correct rate might erroneously suggest that Model B is more factual, despite Model A being more reliable due to its similar correct rate and significantly lower wrong rate.\nGiven these issues, we propose the following criteria for the factuality of LLMs-as-KBs:\nCRITERION 1.1 For seen knowledge, a factual LLM should demonstrate a high correct rate and a low wrong rate.\nCRITERION 1.2 For unseen knowledge, a factual LLM should demonstrate a high uninformative rate.\nWe next proceed to define evaluation metrics that operationalize these criteria. Let $M$ denote a LLM. Let $D_{seen}$ denote a QA dataset containing $N$ open-ended factoid questions pertaining to knowledge the LLM ought to have seen during training. Let $D_{unseen}$ denote a QA dataset with $L$ open-ended factoid questions covering unseen knowledge. We further assume the LLM's response to $D_{seen}$ will be correct, uninformative, or wrong, while its response to $D_{unseen}$ will be either uninformative or wrong.\nMETRIC 1.1: Net Correct Rate (NCR) measures how much more likely the model is to pro-"}, {"title": "2.2 Consistency", "content": "Consistency refers to the quality of always behaving in the same way or having the same opinions. KBs are designed with this principle in mind. In fact, there are efficient algorithms (Andersen and Pretolani, 2001) which detect and resolve conflicts within KBs, thus ensuring consistent outputs for queries on the same facts.\nIt is well-known that LLMs often exhibit inconsistencies in their responses (Elazar et al., 2021; Wang et al., 2022). Current research (Elazar et al., 2021; Jang et al., 2022; Hagstr\u00f6m et al., 2023) evaluates LLM consistency through their performance on benchmarks involving paraphrasing, negation, or multilingual variations. A model is considered superior if it responds consistently across a broader range of data samples. In this work, we argue that it may be too strict to expect LLMs to be always consistent when responding to fact-based questions. Unlike explicit KBs which store information at a fixed location, LLMs operate probabilistically. In theory, if the context has been learned during training, the probability distribution for the prediction will be concentrated; otherwise, it will be more uniform. Drawing from a uniform probability distribution inevitably leads to inconsistencies. As shown in Figure 2, even with greedy decoding, slight biases in the distribution can cause fluctuations in the selection of the top probable words.\nGiven their probabilistic nature, we do not expect LLMs to always behave consistently. We acknowledge that inconsistencies can cause confusion in practical applications and propose to monitor model behavior through post-processing which we argue is more realistic than expecting a probabilistic model to be perpetually consistent. We thus propose the following consistency criteria:\nCRITERION 2.1 The model is expected to be consistent when it produces correct responses.\nCRITERION 2.2 The model is expected to be inconsistent when it produces wrong responses.\nWe next define evaluation metrics corresponding to the criteria above. Let $q$ refer to a question in either $D_{seen}$ or $D_{unseen}$, and $r$ denote model $M$'s response to $q$. Inspired by Zheng et al. (2024), we measure consistency based on multiple-choice"}, {"title": "2.3 Reliability (Factuality and Consistency)", "content": "A reliable LLM-as-KB should then be assessed against factuality and consistency. Based on the criteria defined above, a LLM-as-KB is reliable if it meets the following:\nCRITERION 3.1 For seen knowledge, a LLM should have a high rate of consistently correct responses and a low rate of consistently wrong responses.\nCRITERION 3.2 For unseen knowledge, a LLM should have a high rate of uninformative or inconsistent responses.\nWe quantify these criteria with two metrics.\nMETRIC 3.1: Net Consistently Correct Rate (NCCR) quantifies the model's tendency to provide consistently correct responses compared to consistently wrong ones for questions about seen knowledge. It is defined as:\n$$NCCR = CCR \u2013 CWR$$\n$$CCR = CR \\times C_{correct}$$\n$$CWR = WR \\times C_{wrong}$$\nNCCR ranges from -1 to 1. A higher NCCR indicates a LLM is more reliable on seen knowledge. A negative NCCR suggests the model provides consistently wrong responses, while a positive NCCR suggests a preference for consistently correct responses.\nMETRIC 3.2: Inconsistent/Uninformative Rate (IUR) assesses whether the model is likely to provide uninformative or inconsistent wrong responses for questions about unseen knowledge. It is defined as:\n$$IUR = 1 \u2212 (1 \u2013 UR)C_{wrong}$$"}, {"title": "3 Experimental Setup", "content": "To evaluate the performance of LLMs on seen knowledge, we collated SeenQA, a composite dataset containing 3,000 questions sourced from the test sets (or development sets, where test sets were unavailable) of Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), and PopQA (Mallen et al., 2023). For assessing LLM performance on unseen knowledge, we introduce UnseenQA, a new dataset designed to ensure that models with a knowledge cutoff date before April 13, 2024 do not have access to answers. UnseenQA also includes 3,000 questions, which are derived from 20 templates covering various answer types such as number, people, time, location, and others. Our methodology for creating SeenQA and UnseenQA is detailed in Appendix A."}, {"title": "4 Results", "content": "We present detailed results for all LLMs are in Table 9 (factuality), Table 10 (consistency), and Table 11 (reliability) in Appendix D. LLM rankings based on different metrics are shown in Figure 12 and Figure 13, also in Appendix D.\nGPT-3.5-TURBO is overall the most reliable LLM. Table 1 presents results for the two best performing LLMs under different prompt settings. As can be seen, GPT-3.5-TURBO is most reliable across the board. Although it is not consistently wrong when asked about facts it does not know (its IUR score exceeds 95%), it is not consistently correct when asked about facts it has seen before (NCCR is only 32%).\nFLAN-T5-0.78B is the most reliable LLM for unseen knowledge and most unreliable with seen knowledge. Figure 5 shows the LLMs ranked by NCCR and IUR in a zero-shot setting. As can be seen, while FLAN-T5-0.78B ranks low for NCCR, it maintains the top position for IUR. A similar trend is observed with the FLAN-T5-3B and GEMMA-INSTRUCT (2B, 7B) models. Conversely, models in the LLAMA3 family show the opposite trend: they rank high on seen knowledge but low on unseen knowledge. For instance, LLAMA3-70B, despite ranking second in terms of NCCR, falls to the fourth lowest position for IUR.\nNet Correct Rate (NCR) reveals factuality gaps in LLMs. As illustrated in Figure 4, CR, the standard metric for assessing factuality, fails to fully capture nuanced differences. Despite similar CR values of approximately 40%, LLAMA3INSTRUCT-8B and LLAMA2CHAT-13B behave differently when it comes to wrong responses (on seen data); the former model has a WR of 15% higher than the latter, and as a result its NCR is substantially lower. In the case of PHI2-3B, Gunasekar et al. (2023) claim that with \"textbook quality\" data, smaller LLMs can"}, {"title": "5 Analysis", "content": "In this section, we explore the effects of model size, fine-tuning, and ICL on LLM performance"}]}