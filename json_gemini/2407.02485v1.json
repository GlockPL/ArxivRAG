{"title": "RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs", "authors": ["Yue Yu", "Wei Ping", "Zihan Liu", "Boxin Wang", "Jiaxuan You", "Chao Zhang", "Mohammad Shoeybi", "Bryan Catanzaro"], "abstract": "Large language models (LLMs) typically utilize the top-k contexts from a retriever\nin retrieval-augmented generation (RAG). In this work, we propose a novel instruc-\ntion fine-tuning framework RankRAG, which instruction-tunes a single LLM for\nthe dual purpose of context ranking and answer generation in RAG. In particular,\nthe instruction-tuned LLMs work surprisingly well by adding a small fraction\nof ranking data into the training blend, and outperform existing expert ranking\nmodels, including the same LLM exclusively fine-tuned on a large amount of\nranking data. For generation, we compare our model with many strong baselines,\nincluding GPT-4-0613, GPT-4-turbo-2024-0409, and ChatQA-1.5, an open-sourced\nmodel with the state-of-the-art performance on RAG benchmarks. Specifically,\nour Llama3-RankRAG significantly outperforms Llama3-ChatQA-1.5 and GPT-4\nmodels on nine knowledge-intensive benchmarks. In addition, it also performs\ncomparably to GPT-4 on five RAG benchmarks in the biomedical domain without\ninstruction fine-tuning on biomedical data, demonstrating its superb capability for\ngeneralization to new domains.", "sections": [{"title": "1 Introduction", "content": "Retrieval-augmented generation (RAG) (Lewis et al., 2020; Izacard & Grave, 2021; Lin et al., 2024;\nWang et al., 2024) is a widely used technique for customizing large language models (LLMs) to handle\nlong-tail knowledge (Mallen et al., 2023; Asai et al., 2024b), provide up-to-date information (Kasai\net al., 2023), and adapt to specific domains and tasks (Xiong et al., 2024) without modifying the\nmodel weights. In general, a dense embedding based retriever (Karpukhin et al., 2020; Lin et al.,\n2023; Wang et al., 2022) first retrieves top-k chunked contexts from a collection documents or external\ndatabase for a given question. Then, LLM reads the top-k contexts to generate the answer.\nHowever, the current RAG pipeline has the following limitations: i) LLMs are not good at reading too\nmany chunked contexts (e.g., top-100) even with the long-context window, not only due to efficiency\nreasons, but also because a shorter list of top-k (e.g., 5, 10) contexts usually leads to higher accuracy\nof generation (e.g., see Table 5 in Xu et al., 2024b). ii) Given a small k, one needs a mechanism to\nensure the high recall of relevant contents. Relying solely on a retrieval model may be inadequate\ndue to challenges in learning effective local alignments across the entire embedding space to support\naccurate matching (Luan et al., 2021). In practice, a separate ranking model (Nogueira et al., 2020;\nGlass et al., 2022; Ma et al., 2023) that cross-encodes question and candidate context can work better\nthan a dense embedding-based retriever for obtaining the most relevant top-k contexts from top-N\ncandidates (N\u226b k). iii) However, the zero-shot generalization capability of the expert ranking model\ncan be relatively limited compared to the versatile LLM itself."}, {"title": "2 Related Work", "content": "Retrieval-augumented generation (RAG) has been established for knowledge-intensive NLP\ntasks (Lewis et al., 2020; Borgeaud et al., 2022; Izacard et al., 2023; Izacard & Grave, 2021).\nIn the standard process, a standalone dense-embedding-based retriever (e.g., Karpukhin et al., 2020)\nfirst retrieves relevant information from an external corpus, which the LLM then utilizes in the\ngeneration process. To improve this pipeline, recent research has focused on aligning retrievers to\nthe needs of LLMs for generation (Shi et al., 2024; Lin et al., 2024), designing multi-step retrieval\nprocesses (Trivedi et al., 2023; Jiang et al., 2023; Jeong et al., 2024; Shao et al., 2023), or filtering\nirrelevant contexts (Wang et al., 2023c; Yoran et al., 2024; Xu et al., 2024a). To improve generation,\nseveral studies have designed instruction-tuning methods dedicated to enhancing the search (Ma et al.,\n2023; Zhu et al., 2024; Muennighoff et al., 2024) and RAG capability of LLMs (Liu et al., 2024; Lin\net al., 2024; Luo et al., 2023; Asai et al., 2024a; Wang et al., 2024).\nAlthough strong retrievers have been introduced (e.g., Lin et al., 2023; Yu et al., 2022; Wang et al.,\n2022, 2023a; Lee et al., 2024), one potential approach to improve retriever is optimizing it along with\nLLM in an end-to-end manner (e.g., Guu et al., 2020; Shi et al., 2024; Sachan et al., 2021; Izacard\net al., 2023). However, this requires surrogate loss for optimization and complicates the training\npipeline, especially when the embedding database needs to be re-indexed frequently due to the update\nof the embedding model (i.e., retriever).\nRanking serves as an intermediate step to improve the quality of information retrieval (Mitra et al.,\n2018), and has been applied to RAG pipeline for improving generation quality (Glass et al., 2022;\nRam et al., 2023). However, these methods still rely on an additional moderate-sized model (e.g.\nBERT, T5) for ranking, which is often insufficient to capture the relevance between query and contexts\nand may lack the zero-shot generalization capability. Although recent studies have demonstrated the\nstrong ability of LLMs at ranking tasks (Khalifa et al., 2023; Qin et al., 2024; Sun et al., 2023), how\nto harvest this ability for the RAG pipeline remains underexplored."}, {"title": "3 Preliminaries", "content": "In this section, we first introduce the preliminaries of retrieval-augmented generation as well as the\nproblem setup. Then we present the limitations in the current RAG pipeline, which motivates the\nproposed RankRAG method."}, {"title": "3.1 Problem Setup", "content": "In retrieval-augmented generation, a collection of documents or contexts (e.g. Wikipedia) is given,\nproviding the grounded knowledge. Given a question q, the retriever R (e.g., a parameterized\nembedding model) first retrieves top-k contexts C = {c1,\u2026\u2026,ck} that are most relevant to the\nquestion. Subsequently, the language model produces the final answer where the answer can either\nbe a short phrase or a long sentence, depending on the type of the target task. Our focus is on\nautoregressive language models (OpenAI, 2022, 2023; Meta-AI, 2024), which is the most common\narchitectures for LLMs."}, {"title": "3.2 Limitation of Current RAG Pipelines", "content": "Before formally introducing RankRAG, we would like to first pinpoint several limitations of the\ncurrent \"retrieve-then-generate\" pipeline with large language models.\nLimited Capacity of Retriever. Current RAG systems usually employ sparse retrieval (e.g.\nBM25 (Robertson et al., 2004)) or moderate-size (e.g. BERT-based) embedding model (Karpukhin\net al., 2020; Lin et al., 2023; Wang et al., 2022) as the retriever R, mainly due to efficiency con-\nsideration as there are often millions of, if not more, documents need to be indexed. These models\nencode questions and documents independently and calculate the similarity between question and\ndocuments using vector similarity metrics. However, the limited capacity of embedding model and\nindependent processing of query and documents constrain the ability to estimate textual relevance\nbetween question q and documents d, reducing their effectiveness in new tasks or domains, verified\nby both theoretical (Menon et al., 2022) and empirical (Luan et al., 2021; Thakur et al., 2021) studies.\nTrade-off of Picking Top-k Contexts. Although the state-of-the-art long-context LLM can take\nmany retrieved contexts as input for answer generation, the performance quickly saturates with\nincreased k in practice. For example, Xu et al. (2024b) finds the optimal number of chunked context\nk is around 10 for long document QA tasks. As illurstrated in Figure 1, we perform evaluation\non ChatQA-1.5 (Liu et al., 2024), one of the strongest RAG model with open weights, and find\nthe saturation of accuracy when k = 10. In general, a smaller k often fails to capture all relevant\ninformation, compromising the recall, given the limited expressibility of retriver. In contrast, a larger\nk improves recall but at the cost of introducing irrelevant content that hampers the LLM's ability to\ngenerate accurate answers (Yoran et al., 2024; Yu et al., 2023b)."}, {"title": "4 RankRAG", "content": "To address the limitations mentioned in the previous section, we propose the RankRAG method\nto enhance the LLM's ability for retrieval-augmented generation. Specifically, we instruction-tune\nthe LLM to simultaneously capture the relevance between the question and context and utilize the\nretrieved context for answer generation. The details are introduced as follows."}, {"title": "4.1 Stage-I: Supervised Fine-Tuning (SFT)", "content": "It is observed that general instruction-tuning or supervised fine-tuning (SFT) often significantly\nimproves the ability of LLMs to follow instructions, thus improving zero-shot results on various\ndownstream tasks (Wei et al., 2022; Ouyang et al., 2022). As such, we follow existing works (Chung\net al., 2024; Wang et al., 2024; Liu et al., 2024) to first leverage SFT on a blend of high quality\ninstruction following datasets, including: i) a private crowd-sourced conversational dataset and\npublic conversation datasets: OpenAssistant (K\u00f6pf et al., 2023), Dolly (Conover et al., 2023), and\nSODA (Kim et al., 2023), ii) a long-form QA dataset ELI5 that requires elaborate answers (Fan et al.,\n2019), iii) LLM-generated instructions: Self-Instruct (Wang et al., 2023b) and Unnatural Instructions\n(Honovich et al., 2023), iv) FLAN and Chain-of-thought datasets (Chung et al., 2024).\nThere are overall 128K SFT examples in total. We make sure that there is no overlap between SFT\ndata and data from evaluation tasks. For each sample in the instruction-following dataset, we take the\nmulti-turn conversational format, use the previous turns of conversation between the user and the\nassistant as the context, and compute the loss only at the last response from the assistant."}, {"title": "4.2 Stage-II: Unified Instruction-Tuning for Ranking and Generation", "content": "The Stage-I SFT enpowers the LLMs with basic instruction-following capabilities; however, their\nperformance on RAG tasks often remains suboptimal, as the LLMs are not optimized for extracting\nanswers from retrieved context for a given question. Although recent studies (Lin et al., 2024; Liu\net al., 2024; Zhang et al., 2024) enhance the RAG capability of LLM by instruction tuning it on\ncontext-rich generation tasks, these approaches can still be ineffective with poor initial retrieval\nresults. RankRAG instruction tunes the LLM for both retrieval-augmented generation and context\nranking. In particular, the context ranking capability is crucial to obtain more relevant top-k context\nwith imperfect retriever.\nTo achieve this goal, the instruction tuning blend of Stage-II consists the following five parts:\n1) SFT data from Stage-I. This part is included to maintain LLM's instruction-following capability.\n2) Context-rich QA data. We first follow Liu et al. (2024) to leverage multiple context-rich QA tasks\nto enhance the LLM's capability of using context for generation. The training blend we use consists of:\ni) standard QA and reading comprehension datasets: DROP (Dua et al., 2019), NarrativeQA (Ko\u010disk\u1ef3\net al., 2018), Quoref (Dasigi et al., 2019), ROPES (Lin et al., 2019), NewsQA (Trischler et al.,\n2017), TAT-QA (Zhu et al., 2021), which contains a question, a golden context and an answer. ii)\nconversational QA datasets: HumanAnnotatedConvQA and SyntheticConvQA open-sourced by Liu\net al. (2024), which contains a conversation between user and assistant, as well as one background\ndocument. The model needs to generate an answer given the conversation history and document.\n3) Retrieval-augmented QA data. In addition to the above QA datasets used in Liu et al. (2024),\nwe add two datasets with not only gold context but also the top-retrieved context using BM25. Note\nthat, it is crucial to improve LLM's robustness over irrelevant context at generation. Being aware of\nthis, we consider two QA tasks, namely SQUAD (Rajpurkar et al., 2016) and WebQuestions (Berant\net al., 2013). For each question with the answer, we combine the gold context with the top-retrieved\ncontexts using BM25, ensuring a total of five contexts. Note that some retrieved contexts may not\ncontain the answer, and could be the \u201chard-negative\u201d contexts.\n4) Context ranking data. To empower LLMs with ranking capabilities, we use the popular\nMS MARCO passage (context) ranking dataset (Bajaj et al., 2016). We treat the gold query-passage"}, {"title": "4.3 RankRAG Inference: Retrieve-Rerank-Generate Pipeline", "content": "As RankRAG incorporates an additional reranking step, the inference pipeline for each question\nis modified as a retrieve-rerank-generate pipeline, described as follows: (1) the retriever R first\nretrieves top-N contexts from the corpus. (2) the RankRAG model calculates the relevance score\nbetween the question and retrieved N contexts as the probability of generating the answer as True\nusing the prompt in Table 1, then reranks contexts to only retain top-k (k < N) contexts, which are\nthen used as the input for the generation step. (3) The top-k contexts, along with the question, are\nconcatenated and fed back into the RankRAG model to generate the final answer.\nEfficiency Discussion. We are aware that the addition of a reranking step introduces extra processing\ntime. In practice, for each question, denote the time for indexing and retrieval as t\u2081, the time for using\nLLM to calculate the relevance score as t2 and the time for generation as t3, then the ratio of added\ntime overhead is \\frac{N*t_2}{t_1+t_3}. In practice, calculating relevance typically requires generating just one token\nand involves much shorter inputs compared to the generation step with top-k contexts. We provide\nefficiency study in \u00a75.5."}, {"title": "5 Experiments", "content": "In this section, we conduct comprehensive experiments on a variety of knowledge-intensive NLP\ntasks to demonstrate the zero-shot capabilities of RankRAG."}, {"title": "5.1 Experiment Setup", "content": "Tasks and Datasets. We consider 3 types of tasks in experiments: (1) Open-domain QA (OpenQA),\nwhich includes NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), PopQA (Mallen et al.,\n2023), HotpotQA (Yang et al., 2018) and 2WikimQA (Ho et al., 2020). The first three are single\nhop QA tasks, while the last two are multi-hop QA datasets. For NQ, TriviaQA, and HotpotQA,\nwe use the split from KILT benchmark (Petroni et al., 2021) 2. (2) Fact verification, where we\nuse FEVER (Thorne et al., 2018) from KILT benchmark. (3) Conversational QA (ConvQA), we\nconsider three datasets including Doc2Dial (Feng et al., 2020), TopiOCQA (Adlakha et al., 2022)\nand INSCIT (Wu et al., 2023), which have long documents that cannot be fitted directly into LLMs\nthus necessitates retrieval and ranking. The detailed dataset information is in Appendix A.1.\nBaselines. We consider the following baselines: (1) Baseline LLMs without RAG, where we con-\nsider LLMs trained with proprietary data including InstructGPT (Ouyang et al., 2022), PaLM\n2 (Anil et al., 2023), FLAN-LaMDA (Longpre et al., 2023), GLaM (Du et al., 2022), Claude\n2 (Anthropic, 2023), Mixtral-8x22B-Instruct (Mistral, 2024), DeepSeek-V2 Chat (DeepSeek, 2024)\nand only use the official reported results. We also consider two ChatGPT-series models, namely\nGPT-3.5-turbo (gpt-3.5-turbo-0613) (OpenAI, 2022) and GPT-4 (gpt-4-0613) (OpenAI, 2023).\n(2) Baselines with retrieval, we evaluate models augmented with retrieval. Specifically, we include\nAtlas (Izacard et al., 2023) and Raven (Huang et al., 2023), two RAG models based on encoder\ndecoder LMs. For decoder-only models, we consider Self-RAG (Asai et al., 2024a), RECOMP (Xu\net al., 2024a), InstructRetro (Wang et al., 2024), RePlug (Shi et al., 2024), RA-DIT (Lin et al., 2024),\nLlama-3-instruct (Meta-AI, 2024) and ChatQA-1.5 (Liu et al., 2024). We also list the result of RAG\npipelines using InstructGPT (175B parameters) as the backbone including GenRead (Yu et al., 2023a),\nRetrieve-read (Lazaridou et al., 2022) and ReFeed (Yu et al., 2024), but mainly for reference. Other\nreported numbers are directly comparable if they follow the standard zero-shot settings.\nEvaluation Metrics. For OpenQA datasets, we use Exact Match (EM) as the main metric but also\nreport Accuracy for TriviaQA and PopQA and F1 score for HotpotQA and 2WikimQA as it is used\nin several studies (Asai et al., 2024a; Mallen et al., 2023). For FEVER, we use accuracy as the metric.\nFor ConvQA datasets, we follow (Liu et al., 2024; Wang et al., 2024) to use F1 score as the metric.\nImplementation Details. We use Llama3 8B and 70B (Meta-AI, 2024) as the backbone in our main\nexperiments. For the two-stage instruction tuning, we set the batch size to 128 and train the model for\n1000 steps with learning rate 5e-6 in Stage-I. Then, we reduce the learning rate to 3e-7 for 8B and 2e-7\nfor 70B model, set the batch size to 64, and train the model for 3300 steps (around 1 epoch). We use\nthe Adam optimizer (Kingma & Ba, 2014) with \u03b2\u2081 = 0.9 and \u1e9e2 = 0.98. During the inference stage,\nwe use the December 2018 Wikidump as the corpus index for NQ, TQA, HotpotQA, 2WikimQA, and\nuse the December 2020 Wikidump for PopQA, following (Asai et al., 2024a). By default, we follow\n(Wang et al., 2024; Lin et al., 2024; Liu et al., 2024) to use the Dragon retriever (Lin et al., 2023) as\ndefault and retrieve top-N (100 for 8B and 30 for 70B) documents for ranking, but RankRAG can be\nadapted to various retrievers and different N (see \u00a7 5.3 and 5.5). To ensure a fair comparison, we test\nthe performance of k \u2208 {5, 10, 20} and report the best performance for baselines. For generation, we\nkeep temperature T = 0 and set the maximum number of generated token to be 32 for OpenQA, 128\nfor ConvQA and 8 for others. Training RankRAG-8B uses 32 NVIDIA A100 GPUs for 10 hours\n(4 hours for Stage-I and 6 hours for Stage-II finetuning), while training RankRAG-70B uses 128\nNVIDIA A100 GPUs for 16 hours (4 hours for Stage-I and 12 hours for Stage-II Finetuning).\nData Contamination Issues. One possible issue for the zero-shot evaluation is the test set contami-\nnation, where some of the task-specific examples overlap with the instruction fine-tuning data (Oren\net al., 2024). To address this issue, we have performed a string match-based analysis where we do not\nobserve any overlap between the train data and data from target tasks."}, {"title": "5.2 Main Experiments", "content": "Table 2 presents results of RankRAG and baselines. The findings are summarized as follows:\nRankRAG outperforms existing RAG methods. With 8B scale, RankRAG consistently outperforms\nChatQA-1.5 8B, one of the most recent open-sourced model with state-of-the-art performance on\nmany RAG benchmarks. RankRAG 8B is also competitive when compared with baseline models\nwith much more parameters. For example, it significantly outperforms InstructRetro (5\u00d7 parameters),\nRA-DIT 65B (8\u00d7 paramters), and even outperforms Llama3-instruct 70B (8\u00d7 parameters) on NQ\nand TriviaQA tasks. With more parameters, RankRAG 70B outperforms the strong ChatQA-1.5 70B\nmodel, and largely outperforms previous RAG baselines with InstructGPT as the underlying LLM.\nRankRAG demonstrates larger improvement on more challenging datasets. We observe that\nthe performance gains of RankRAG over baselines are more pronounced for more challenging QA\ndatasets. For example, on long-tailed QA (PopQA) and multi-hop QA (2WikimQA) tasks, we\nachieve more than 10% improvement over ChatQA-1.5. These findings suggest that in challenging\nOpenQA datasets where top documents from retrievers are less relevant to the answer, context ranking\neffectively enhances performance. In this work we focus on improving single-time retrieval for QA\ntasks. How to effectively combine multi-round RAG pipelines (Jiang et al., 2023; Khattab et al.,\n2022; Jeong et al., 2024) with RankRAG is an interesting avenue of future work."}, {"title": "5.3 Ablation Studies", "content": "Effect of Designed Components. Table 3 shows the ablations of RankRAG with Llama3 8B as the\nbackbone on nine general-domain datasets. Overall, we observe all of the proposed components\ncontribute to the final performance. Removing context ranking hurts performance on all tasks,\njustifying its efficacy in selecting the most relevant contexts for the target question. Besides, the\nretrieval-augmented QA (RQA) and retrieval-augmented ranking (RAR) designed for instruction fine-\ntuning improve outcomes on most tasks by helping the model explicitly pinpoint relevant contexts.\nOn the contrary, the RAFT method used in (Lin et al., 2024) treats each retrieved context separately\nduring instruction finetuning, which yields suboptimal results when compared to RankRAG with the\nsame training data.\nPerformance with Different LLMs. Table 4 reports the performance of RankRAG and the most"}, {"title": "5.4 Experiment on Domain-specific RAG Benchmarks", "content": "To demonstrate that RankRAG\ncan adapt to specialized domains,\nwe conduct experiments on Mi-\nrage (Xiong et al., 2024), a re-\ncently introduced RAG bench-\nmark for the biomedical field.\nWe follow Xiong et al. (2024)\nto employ MedCPT (Jin et al.,\n2023) as the retriever R with\nMedCorp as the corpus D."}, {"title": "5.5 A Closer Look at the Ranking Module", "content": "As the context ranking serves as a core step in RankRAG, we take a closer look at this component.\nAll the studies are done using Llama3-8B as the backbone."}, {"title": "6 Conclusion", "content": "In this work, we introduce a new RAG framework, RankRAG, which instruction-tunes a single LLM\nfor both ranking and answer generation. We find that the instruction tuned LLMs can outperform\nexisting expert ranking models by only adding a small fraction of ranking data into the training blend.\nWe compare our RankRAG with the state-of-the-art RAG models on comprehensive knowledge-\nintensive benchmarks and demonstrate RankRAG significantly outperform all of them on nine\ngeneral-domain and five biomedical benchmarks for RAG."}, {"title": "A Dataset Description", "content": "The information for 14 datasets used in RankRAG is listed as follows."}, {"title": "A.1 Main Experiments", "content": "\u2022 NQ (Kwiatkowski et al., 2019) is a widely used question-answering dataset constructed with\nWikipedia. The questions are constructed from the Google search engine, and the answers are\nidentified as text spans in the Wikipedia article.\n\u2022 TriviaQA (Joshi et al., 2017) is a challenging QA dataset containing question-answer pairs from\ntrivia enthusiasts and independently gathered evidence documents.\n\u2022 PopQA (Mallen et al., 2023) is an entity-centric QA dataset concentrated on long-tail entities.\nFor PopQA, we follow (Asai et al., 2024a) to use the long-tail subset, consisting of questions on\n1399 rare entities whose monthly Wikipedia page views are less than 100.\n\u2022 HotpotQA (Yang et al., 2018) is a multi-hop QA dataset, where the goal is to answer complex\nquestions that require understanding and linking information from multiple documents.\n\u2022 2WikimQA (Ho et al., 2020) is also a multi-hop QA designed to test machine understanding\nacross two different Wikipedia entities, evaluating the ability of systems to handle cross-lingual\nand cross-cultural retrieval and question answering.\n\u2022 FEVER (Thorne et al., 2018) is a fact verification dataset aimed at supporting research into the\nautomatic verification of factual claims. It consists of claims that are manually verified against\nevidence from Wikipedia, providing a benchmark for fact-checking systems.\n\u2022 Doc2Dial (Feng et al., 2020) is a document-grounded conversational QA dataset covering four\ndomains: DMV, SSA, VA, and Student Aid. Each sample comprises a dialogue where a user poses\nqueries regarding the document, and an agent responds those questions. The average document\nlength is around 101K words.\n\u2022 TopiOCQA (Adlakha et al., 2022) is grounded on the whole Wikipedia. It incorporates topic\nswitching and requires the agent to search the entire Wikipedia for answers to user questions.\n\u2022 INSCIT (Wu et al., 2023) is also grounded on the whole Wikipedia. It studies the case where\nuser questions are under-specified and require clarification."}, {"title": "A.2 Biomedical Benchmarks", "content": "\u2022 MMLU-med (Hendrycks et al., 2021) is a subset of six tasks related to biomedicine, including\nanatomy, clinical knowledge, professional medicine, human genetics, college medicine, and\ncollege biology. It contains 1089 questions in total.\n\u2022 MedQA (Jin et al., 2021) is collected from the US Medical Licensing Examination, contaiing\n1273 four-option multiple-choice questions focused on real-world scenarios from professional\nmedical board exams.\n\u2022 MedMCQA (Pal et al., 2022) includes multiple-choice questions derived from Indian medical\nentrance exams, covering 2400 healthcare topics across 21 medical subjects. We use the 4,183-\nquestion development set from MedMCQA, as the test set lacks provided ground truths.\n\u2022 PubmedQA (Jin et al., 2019) is a biomedical research QA dataset consisting of 1000 manually\nannotated questions based on PubMed abstracts. Answers in PubMedQA are structured as\nyes/no/maybe to reflect the validity of the questions.\n\u2022 BioASQ (Tsatsaronis et al., 2015) includes 618 questions constructed from biomedical litera-\nture without providing the ground truth snippets, challenging RAG systems to infer answers\nindependently."}, {"title": "B Data Blending Details for Ranking-enhanced Instruction Finetuning", "content": "The dataset blending ratio for Stage-II is as follows:\n\u2022 Drop: 0.069"}, {"title": "C Prompt Formats of Instruction Tuning", "content": "C.1 Stage I: Supervised Fine-tuning\nThe format template of LLM inputs in stage-I is as follows:\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user's questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nUser: {Question 1}\nAssistant: {Answer 1}\nUser: {Latest Question}\nAssistant:\nC.2 Stage-II: Unified Instruction-Tuning for Ranking and Generation\nThe format template of LLM inputs in stage-II are as follows:\n1) Context-rich QA data\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user's questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nPassage: {(Gold) Passage containing relevant context for QA}\nUser: {Question 1}\nAssistant: {Answer 1}\nUser: {Latest Question}\nAssistant:\nWe tailor specific user instructions for various dataset types. For instance:\nFor datasets requiring short answers (such as DROP, NarrativeQA, Quoref, ROPES, SQuAD1.1,\nSQUAD2.0, NewsQA), we use: \"Answer the following question with a short span.\"\nFor datasets that necessitate long answers (such as Synthetic_ConvQA), we instruct: \"Please give a\nfull and complete answer for the question.\"\nFor datasets involving arithmetic calculations or number extraction from the context (such as TAT-\nQA), we specify: \"Answer the following question with a number from the context or through math\narithmetic.\"\nFor datasets that may require both short and long answers (such as TAT-QA-Others), we direct:\n\"Answer the following question with a short span, or a full and complete answer.\"\n2) Retrieval-augmented QA data\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user's questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nPassage 1: {(Shuffled) Passage 1}\nPassage 2: {(Shuffled) Passage 2}\nPassage 3: {(Shuffled) Passage 3}\nPassage 4: {(Shuffled) Passage 4}\nPassage 5: {(Shuffled) Passage 5}\nUser: {Question}\nAssistant:\n3) Context ranking data\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user's questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nPassage: {Passage 1}\nUser: {For the question <question>, access whether the passage is relevant to the\nquestion. Return True if relevant, otherwise False. }\nAssistant:\n4) Retrieval\u2022 narrativeqa: 0.09\n\u2022 quoref: 0.026\n\u2022 ropes: 0.026\n\u2022 Squad (Retrieval-augmented QA): 0.09\n\u2022 Squad (Retrieval-augmented Ranking): 0.02\n\u2022 WebQuestions (Retrieval-augmented QA): 0.09\n\u2022 WebQuestions (Retrieval-augmented Ranking): 0.02\n\u2022 newsqa: 0.09\n\u2022 tatqa-arithmetic: 0.15\n\u2022 tatqa-others: 0.08\n\u2022 ConvQA: 0.2\n\u2022 MS MARCO ranking: 0.15\n\u2022 ConvQA ranking: 0.03\n\u2022 SFT: 0.2\nThe ratio for each dataset is further normalized to ensure the total ratio equals to 1."}, {"title": "D Prompt Formats of Target Tasks", "content": "D.1 Context Ranking\nNQ/TriviaQA/HotpotQA/PopQA:\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user's questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nPassage: {Passage}\nUser: For the question , access whether the passage is relevant to the\nquestion. Return True if relevant, otherwise False. }\nAssistant:\nFEVER:\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user's questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nPassage: {Passage}\nUser: {For the claim , access whether the passage is relevant to the\nclaim. Return True if relevant, otherwise False. }\nAssistant:\nDoc2dial, Inscit, TopiocQA:\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user's questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nPassage: {Passage}\nUser: {Question 1}\nAssistant: {Answer 1}\nUser: {For the question , access whether the passage is relevant\nto the question. Return True if relevant, otherwise False. }\nAssistant:\nD.2 RAG\nNQ/TriviaQA/HotpotQA/PopQA:\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user's questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nPassage 1: {Rerank Top Passage 1}\nPassage 2: {Rerank Top Passage 2}\nPassage 3: {Rerank Top Passage 3}\nPassage 4: {Rerank Top Passage 4}\nPassage 5: {Rerank Top Passage 5}\nUser: {Question}. Answer the above question with a short phrase.\nAssistant:\nFever:\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user's questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nPassage 1: {Rerank Top Passage 1}\nPassage 2: {Rerank Top Passage 2}\nPassage 3: {Rerank Top Passage 3}\nPassage 4: {Rerank Top Passage 4}\nPassage 5: {Rerank Top Passage 5}\nUser: Answer the following question with True or False. Is the claim '' correct?\nAssistant:\nDoc2dial, Inscit, TopiOCQA:\nSystem: This is a chat between a user and an artificial intelligence assistant."}, {"title": "E Additional Experiment Results", "content": "E.1 Ranking Performance Using DPR and Contriever as Retrievers R\nTable 8 shows the ranking performance of RankRAG-8B using DPR (Karpukhin et al., 2020) and\nContriever (Izacard et al., 2022) on three datasets. There are consistent performance gains for all\ntasks, indicating that RankRAG can apply to many popular retrieval models to improve the quality of\nretrieved contents.\nTable 8: Answer Recall Comparison Before and After Ranking on 3 Representative Datasets.\nContriever\nR@5 R@10 R@20\n67.60% 75.24% 80.67%\n75.32% 80.18% 84.70%\nContriever\nR@5 R@10 R@20\n81.95% 86.76% 90.08%\n88.71% 90.05% 92.59%\nContriever\nR@5 R@10 R@20\n60.61% 65.54% 69.90%\n65.11% 68.41% 71.77%\nE.2 RAG Performance with Different k\nWe also show the performance of RankRAG with different context size k in figure 6. From the result,\nwe observe that different from the trend of vanilla RAG approaches (without ranking), k = 5 already\nworks well for most datasets. This effectiveness stems from the reranking step, which prioritizes the\nmost relevant contexts at the top, reducing the necessity to include additional contexts."}]}