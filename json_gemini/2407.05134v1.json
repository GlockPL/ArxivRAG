{"title": "Solving for X and Beyond: Can Large Language Models Solve Complex Math Problems with More-Than-Two Unknowns?", "authors": ["Kuei-Chun Kao", "Ruochen Wang", "Cho-Jui Hsieh"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in solving math problems, a hallmark of human intelligence. Despite high success rates on current benchmarks; however, these often feature simple problems with only one or two unknowns, which do not sufficiently challenge their reasoning capacities. This paper introduces a novel benchmark, BeyondX, designed to address these limitations by incorporating problems with multiple unknowns. Recognizing the challenges in proposing multi-unknown problems from scratch, we developed BeyondX using an innovative automated pipeline that progressively increases complexity by expanding the number of unknowns in simpler problems. Empirical study on BeyondX reveals that the performance of existing LLMs, even those fine-tuned specifically on math tasks, significantly decreases as the number of unknowns increases with a performance drop of up to 70% observed in GPT-4. To tackle these challenges, we propose the Formulate-and-Solve strategy, a generalized prompting approach that effectively handles problems with an arbitrary number of unknowns. Our findings reveal that this strategy not only enhances LLM performance on the BeyondX benchmark but also provides deeper insights into the computational limits of LLMs when faced with more complex mathematical challenges.", "sections": [{"title": "1 Introduction", "content": "Mathematical problem-solving is a fundamental aspect of human intelligence, necessitating both language comprehension and reasoning skills. Recently, LLMs pretrained on extensive web-scale datasets, have exhibited exceptional abilities in addressing a variety of complex tasks. Consequently, mathematical challenges are frequently employed to benchmark the reasoning abilities of LLMs. Studies have shown that these models demonstrate human-level efficacy in solving math problems,\n aided by diverse prompting techniques include in-context learning (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022) and the integration of external computational tools (Gao et al., 2022; Chen et al., 2022; Liu et al., 2023; He-Yueya et al., 2023).\nExisting math datasets (see Table 6) used to evaluate LLMs often consist of algebraic problems involving only one or two unknown variables. While current results on these datasets are promising, their simplicity masks the true capabilities and limitations of these models. For instance, GPT-4 (Achiam et al., 2023) achieves a 98% success rate on the GMS8K (Cobbe et al., 2021) dataset, suggesting that performance on these benchmarks is nearing saturation. This highlights the need for the development of more complex problem sets designed to rigorously stress test LLMs and provide a more accurate measure of their performance.\nWhile quantifying the complexity of these math problems is multi-dimensional, one common measure is the number of unknowns required to solve the problem. Problems with more unknowns involve larger systems of equations, reflecting more complex relationships between the quantities, and thus demanding more sophisticated solving methods. However, creating datasets that include problems with multiple unknowns presents significant challenges, as it is difficult for humans to manually develop a sufficient number of these complex problems from scratch. As a result, existing math datasets are dominated by problems with at most two unknowns (Cobbe et al., 2021; Koncel-Kedziorski et al., 2016, 2015; Roy and Roth, 2018).\nThis paper tackles the aforementioned challenge systematically, by presenting three key contributions: (1) the development of a multi-unknown math benchmark, (2) an empirical study assessing the performance of current LLMs on this new benchmark, and (3) the introduction of a specialized prompting strategy designed to enhance the ability of LLMs to solve multi-unknown problems."}, {"title": "C1: BeyondX - The first multi-unknown algebraic benchmark", "content": "To efficiently generate a large corpus of multi-unknown problems, we developed a novel pipeline that automatically expands existing problems to N unknowns. This pipeline operates on three key ideas: (1). Scenario Expansion: New problems are derived by extending the scenarios (such as financial calculations or grocery shopping) of existing simpler problems, ensuring contextual relevance. (2). Progressive Extrapolation: We add unknowns incrementally one at a time progressing from problems with two unknowns to three, four, and so on. This step-by-step approach makes the problem generation process significantly more manageable. (3). Decomposed Problem Generation: Instead of creating an entire problem at once, we decompose the process. The LLM is carefully instructed to first introduce new unknown variables into the base scenarios, generate the corresponding equations, translate these equations into semantic statements, and finally integrate them into the comprehensive problem statement."}, {"title": "C2: Existing LLMs struggles with increasing unknowns", "content": "Utilizing our BeyondX benchmark, we conduct a comprehensive evaluation of current LLMs, which includes both general-purpose models like GPT-3.5 (Brown et al., 2020), GPT-4, Gemini-Pro (Team et al., 2023), and Mistral-7B (Jiang et al., 2023) as well as models specifically fine-tuned on mathematical problems (WizardMath (Luo et al., 2023), OpenMath (Toshniwal et al., 2024), and MetaMath (Yu et al., 2023)). Our findings reveal a significant drop in performance as the number of unknowns in problems increases a staggering ~ 70% degradation on GPT-4 for instance.\n\u2022 This marked decline indicates that current benchmarks may overstate the algebraic capabilities of these models.\n\u2022 Additionally, despite efforts to fine-tune LLMs on previous math corpus, they still struggle with multi-unknown problems.\n\u2022 Even sophisticated prompting strategies, which utilize detailed natural language explanations, fail to substantially aid LLMs in overcoming these more complex challenges."}, {"title": "C3: Formulate-and-Solve A prompting method to tackle multi-unknown problems", "content": "Traditional prompting methods for LLMs typically do not account for the complexity inherent in systems of equations, potentially limiting the math-solving capabilities of these models. Addressing whether the observed performance drop in LLMs is primarily due to inadequate prompting strategies forms a critical part of future investigation. As the initial step, we propose Formulate-and-Solve, an automated prompting method that generalizes to an arbitrary number of unknowns. This strategy refines current approaches by integrating general math-solving principles to automatically craft relevant multi-unknown in-context examples for LLMs.\nOur empirical evaluations demonstrate that Formulate-and-Solve outperforms traditional prompting methods on both standard algebra problem datasets and our more challenging BeyondX dataset. Importantly, our findings suggest that while the inherent limitations of LLMs contribute to their underperformance on complex problems, inadequate prompting strategies are a substantial bottleneck. By enhancing these strategies, Formulate-and-Solve not only improves LLM performance but also provides clearer insights into the actual computational limitations of current models when faced with advanced mathematical challenges."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Math Word Problem Generation", "content": "Early research on math word problem (MWP) generation relied heavily on pre-defined structures, including domain knowledge, equations, and text templates (Nandhini and Balasundaram, 2011; Williams, 2011; Polozov et al., 2015). More recently, researchers began using pre-trained models fine-tuned on equation-to-MWP examples (Wang et al., 2021). Studies on using LLMs for MWP generation are scarce. Existing work includes evaluating GPT-3's ability to mimic specific problem types (Zong and Krishnamachari, 2023) and using GPT-4 to improve readability in existing problems (Norberg et al., 2023). However, these approaches are limited to replicating existing problem structures, such as the number of unknowns or equation templates. Our work focuses on how to expand existing one or two unknown problems into more complex multiple unknowns."}, {"title": "2.2 Math Word Problem Solver", "content": "Mathematical reasoning skills are crucial for intelligent systems, leading to a surge in research. In the past, studies focused on how statistical and deep learning NLP models could solve arithmetic"}, {"title": "2.3 Math Reasoning with LLMs", "content": "Many prompting techniques have emerged to unlock the reasoning abilities of LLMs (Qiao et al., 2022). Chain-of-Thought (CoT) Prompting (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022) was proposed to generate the reasoning steps before submitting the answer. Later, several other works (Nye et al., 2021; Zhou et al., 2022; Drozdov et al., 2022; Wang et al., 2023) also proposed different approaches to utilize LLMs to solve reasoning tasks by allowing intermediate steps or planning first before solving. All of these methods allow LLMs to process all steps without using any external tools or refinements. For incorporating external tools, Programming-of-Thought (PoT) prompting (Chen et al., 2022; Gao et al., 2022) utilizes LLMs with code pretraining to write a program as a rationale that explains the reasoning process. Equation-of-Thought (EoT) (Liu et al., 2023; He-Yueya et al., 2023) prompting tackles MWPs by converting them into linear equation systems, which are then solved by a symbolic solver. Although PoT and EoT tried to use external tools to solve MWPs, they did not consider the scenario of multiple unknown variables.\nRecent LLMs advancements for math reasoning involve various training approaches. One method focuses on pretraining data specifically designed for math, such as Minerva (Lewkowycz et al., 2022), Llemma (Azerbayev et al., 2023), and DeepSeekMath (Shao et al., 2024). Another approach involves generating synthetic questions and answers that mimic existing benchmarks. For example, the WizardMath series (Luo et al., 2023) that improves mathematical reasoning in Mistral 7B (Jiang et al., 2023) with problems sourced primarily from GSM8K and MATH (Hendrycks et al., 2021) via output from closed-source LLMs. MetaMath (Yu et al., 2023) and MMIQC (Liu and Yao, 2024) focus on expanding existing questions in GSM8K and MATH. MetaMath rewrites questions in various ways, while MMIQC combines existing math pretraining data such as Open Web-Math (Paster et al., 2023) with question-answer variations from MetaMath. The Mammoth series (including Mammoth2) (Yue et al., 2023, 2024) uses curated instruction tuning datasets (MathInstruct, WebInstrcut) with reasoning rationales for training. The OpenMathInstruct (Toshniwal et al., 2024) series utilizes synthetic instruction data from open-source LLMs with strong math reasoning abilities."}, {"title": "3 Automatic Generation of Multi-Unknown Algebra Problems via Progressive Expansion", "content": ""}, {"title": "3.1 Challenges for Constructing Multi-Unknown Datasets", "content": "Generating new problems with LLMs. Creating correct, diverse, and solvable math problems manually is an exceptionally laborious task. The complexity of this task increases with the addition of each unknown, as more unknowns require consideration of additional relationships within the problem scenario. To automatize this process, we employ LLMs to generate the problems, with human verifiers subsequently ensuring the quality and solvability of these problems.\nLimitations of naive generation. Directly prompting LLMs to generate multi-unknown algebra problems has often resulted in poor quality outputs. Firstly, generating problem scenarios from scratch tends to produce a narrow range of problem types, as evidenced by the lack of diversity reported in Table 16. Secondly, attempting to generate all relevant relationships and corresponding equations in a single step frequently leads to violations of problem constraints, rendering many problems unsolvable as detailed in Table 17."}, {"title": "3.2 Generating New Problems via Progress Expansion", "content": "Pipeline overview. To address the aforementioned challenges, we introduce a novel approach called Progressive Expansion, which applies a divide-and-conquer strategy: (1). Scenario Diversification: We begin by expanding existing simpler problems to increase scenario diversity. This leverages the rich variety of simpler problem scenarios as a foundation for more complex questions. (2)."}, {"title": "3.3 Constructing the Benchmark", "content": "Seed problems. We select ALG514 (Kushman et al., 2014) and DRAW-1K (Upadhyay and Chang, 2017) as the foundational seed problems to expand. These datasets are particularly suitable as they include full solutions with oracle equation sets.\nStatistics. With this generation process, we selected a total of 464 problems. Specifically, there are 194 problems with three unknowns, 158 problems with four unknowns, and 112 problems with five unknowns. In addition, since our generated dataset is expanded from the existing dataset, it contains various topics or subjects including moving objects, liquids, interest, distance, and geometry."}, {"title": "4 Benchmarking existing LLMs and Prompting Methods", "content": ""}, {"title": "4.1 LLMs for solving multi-unknown algebra problems", "content": "To evaluate the performance of various LLMs on BeyondX, we consider the Zero-shot-CoT prompting method (details in Section 6.1) and test the performance of both General-Purpose LLMs (GPT-3.5, GPT-4, Gemini-Pro, Mistral-7B) and Mathematically fine-tuned LLMs (WizardMath, OpenMath, MetaMath)."}, {"title": "4.2 Prompting Methods", "content": "Figure 2a demonstrates that state-of-the-art LLMs cannot solve multi-unknown problems with Zero-shot-CoT prompting. To investigate whether this issue can be mitigated with better prompting methods, we evaluated nine existing prompting methods using GPT-3.5, categorized into three types:\nZero-shot. Zero-shot-CoT (Kojima et al., 2022) and Plan-and-Solve (Wang et al., 2023) prompting.\nFew-shot with manual demonstrations. CoT (Wei et al., 2022), PoT (Gao et al., 2022), EoT (Liu et al., 2023), and Declarative (He-Yueya et al., 2023) prompting.\nFew-shot with automatic demonstrations. Analogical (Yasunaga et al., 2023), Auto-Zero-shot-CoT.\nIn Figure 2c, we observe zero-shot and few-shot CoT prompting methods seem inadequate when solving multi-unknown problems. We find that while CoT correctly sets up the equations, it fails to accurately solve the system of equations. Additionally, even though some prompting methods like PoT, EoT, and Declarative use external tools as a calculator and equation solver, they manually design their demonstration for simpler problems and fail to generalize to more complex multiple unknown scenarios. Although some methods construct demonstrations automatically from the problem context (Analogical, Auto-Zero-shot-CoT), they still suffer from poor performance. Since LLMs themselves do not have enough capability to solve multi-unknown problems, the generated demonstrations are often of low quality. This raises concerns about prompt engineering requiring \"human-in-the-loop\" solutions with domain knowledge integrated through instructions.\nTherefore, in the next section, we will go through a detailed formulation of Formulate-and-Solve prompting. Our method can significantly bridge the gap as shown in Figure 2c."}, {"title": "5 Automatic Solver of Algebra Problems", "content": "To investigate whether the observed performance drop is primarily due to inadequate prompting strategies or the limitation of LLMs. we develop Formulate-and-Solve, an automated prompting method designed for LLMs to solve math problems with an arbitrary number of unknowns. We also show that our method performs competitively to state-of-the-art algorithms even for non-algebra problems in Appendix A.1.\nA major challenge in applying the prompting method to multi-unknown problems is the scarcity of hand-crafted demonstrations. Traditional examples with a single unknown do not scale well to more complex, multi-unknown scenarios, necessitating automated demo generation. Furthermore, while LLMs can be guided by prompts to solve these systems of equations, they often require external tools due to their limited ability to independently solve and explicitly formulate these problems into a system of equations.\nTo overcome these limitations, we propose Formulate-and-Solve, a framework that incorporates a set of principles to instruct LLMs in generating demonstrations automatically. This framework empowers LLMs to translate problems into equations and subsequently utilize external tools to solve them. The overall pipeline is illustrated in Figure 3 and we include the actual prompts used in each step in Appendix 15."}, {"title": "6 Experimental Results", "content": ""}, {"title": "6.1 Experimental Setting", "content": "Dataset. Our experiments are conducted on five algebra problem sets, including existing widely-used ones (ALG514, DRAW-1K, AsDiv, HMWP) and the proposed BeyondX benchmark. AsDiv consists of a wide range of math problems and we only take the algebra problem subset. Also, since HWMP is a Chinese dataset, we use GPT-4 to translate the problem into English. We find that while most translation results effectively convey the intended meaning. The five datasets differ in size and complexity, as shown in Table 1. We also report the average number of unknowns in each dataset. Note that we split the proposed dataset into three subsets, correspond to problems with 3 (BeyondX_3), 4 (BeyondX_4), and 5 (BeyondX_5) unknowns, while the problems in all other datasets have \u2264 2 unknowns.\nModels. For experiments in this section, we utilize GPT-3.5, GPT-4, and Gemini-Pro as representatives of general-purpose LLMs. We also opt to exclude open-sourced LLMs, as they typically struggle with multi-unknown problems due to their limited capacity to process and follow long prompts. The versions of these models and system prompts we used for the experiments are listed in Appendix D.1.\nBaselines. We compare Formulate-and-Solve with three types of prompting baselines: (1) Zero-shot. We include Zero-shot-CoT (Kojima et al., 2022) and Plan-and-Solve (PS) (Wang et al., 2023) prompting. The former appends \u201cLet's think step by step\u201d to the prompt without any demo. The latter appends \u201cLet\u2019s first understand the problem and devise a plan to solve the problem. Then, let\u2019s carry out the plan and solve the problem step by step\u201d to the prompt without any demo. (2) Few-shot with manual demonstrations. CoT (Wei et al., 2022) creates eight hand-crafted natural language examples as demonstrations. PoT (Gao et al., 2022) creates eight hand-crafted Python code examples as demonstrations and uses programming tools to get the final answer. EoT (Liu et al., 2023) creates eight hand-crafted equation examples as demonstrations and uses symbolic solvers to obtain the final answer. Declarative (DR) (He-Yueya et al., 2023) creates three hand-crafted examples with principles as demonstrations and uses symbolic solvers to obtain the final answer. (3) Few-shot with automatic demonstrations. (Yasunaga et al., 2023) proposed Analogical prompting (AG), designed to automatically guide LLMs to self-generate relevant examples as demonstrations before proceeding to solve the problem. We come up with another naive method: selecting examples from the dataset and employing Zero-shot-CoT (Kojima et al., 2022) to generate examples as demonstrations. We refer to this method as Auto-Zero-shot-CoT (AZ)."}, {"title": "6.2 Main Results", "content": "To investigate which reasoning methods and models better solve multi-unknown problems, we summarize the performance of different prompting methods using GPT-3.5 in Table 2 and Tables 3. We also evaluate and compare various prompting methods with GPT-4 and Gemini-Pro in Appendix A.3."}, {"title": "7 Discussion and Analysis", "content": "Ablation study. We analyze the significance of each component within in Formulate-and-Solve through an ablation study. We assess five variations: (1) Use only a system of equations as a rationale for reasoning. (2) Remove the instruction before demonstrations. (3) Remove demonstrations after the instruction. (4) Use an LLM instead of a symbolic solver to solve a system of equations. (5) Formulate-and-Solve. We randomly select 60 problems for each unknown from ALG514, DRAW-1K, and BeyondX to evaluate each variation. The results are provided in Table 4.\nWe observe that performance decreases significantly when instruction or demonstration is removed, highlighting its role in guiding the LLM. Interestingly, instruction has a greater impact than demonstration. Replacing the symbolic solver with an LLM also leads to a large decrease in accuracy. These findings confirm that all elements in Formulate-and-Solve contribute significantly to solving multi-unknown problems.\nNext, we verify whether Formulate-and-Solve maintains its effectiveness for problems with one or two unknowns. For this, we compare it with baselines on commonly used algebra datasets containing one or two unknowns, with the results presented in Table 3. The results demonstrate that Formulate-and-Solve again achieves the best performance. Compared with other automatic few-shot methods such as AZ, the performance gap is considerably smaller (5.2% on average) for one or two unknowns than for multiple unknowns (48.4% on average). This is likely because it is easier to generate a high-quality demo for problems with one or two unknowns. Also, we cannot see a big difference between the zero-shot and few-shot CoT in this experiment since the manual few-shot demonstrations that are commonly used in previous work are beneficial for solving arithmetic problems, not algebra problems.\nError analysis. We delve deeper into the primary challenges that LLMs encounter when solving multi-unknown problems using Formulate-and-Solve. To gain a quantitative understanding of model failures, we conduct an error analysis in Formulate-and-Solve with GPT-3.5 on BeyondX. We collect all instances where predictions were incorrect and annotate the main reasons for these mispredictions. The error types include: (E1) generating too few or too many equations, (E2) producing the correct number of equations but with incorrect content, and (E3) generating responses in the wrong format, preventing the extraction of the equation system.\nAs illustrated in Table 5, the most common error is E2 (incorrect equation). This indicates that current LLMs equipped with prompting methods still struggle to accurately formulate multi-unknown equations in some cases. Besides, 37.7% of the errors occur due to the wrong format of the response, and 17.4% of the errors arise when LLMs fail to align relevant information correctly with the equations, resulting in either too few or too many equations. The detailed qualitative analysis of the error examples is in Appendix D.3."}, {"title": "8 Conclusion", "content": "We introduce BeyondX, the first benchmark for evaluating LLMs on multi-unknown problems. Our analysis reveals a significant performance drop in LLMs and existing prompting methods when faced with such problems. To address this, we propose Formulate-and-Solve, a novel prompting method that leverages instruction, automatic demonstrations and a system of equations. Experiments demonstrate the effectiveness of Formulate-and-Solve in tackling multi-unknown problems."}, {"title": "Limitations", "content": ""}, {"title": "Scope of Benchmark", "content": "Although our automatic generation method can decrease the labor-intensive data collection process, our method still needs to be expanded from high-quality problems with low unknowns. Besides, we figure that some types or topics of the problems cannot be extended to multiple unknown problems. And, our benchmark is limited to English questions and data. We look forward to future benchmarks on a broader domain or modality and other languages."}, {"title": "Models and Reasoning Methods", "content": "Although we experiment with many representative models and reasoning methods in this paper, we acknowledge that this does not cover all models and frameworks. Besides, we acknowledge that our approach falls short on more straightforward arithmetic datasets since our method is more suitable for algebra datasets. Further research is required to explore new problem-solving methods for general math reasoning tasks, including different modalities."}, {"title": "Algorithm 1 Formulate-and-Solve Reasoning Algorithm", "content": "Require: question $Q$, instruction $I$, reasoning module $R$, symbolic solver $S$, finalize module $F$\n1: function AUTO DEMO($I$, Question, $K$)\n2:  while $K \\neq 0$ do\n3:   $D \\leftarrow R(I + D + Question)$\n4:   $K \\leftarrow K - 1$\n5:  end while\n6:  return $D$  $\\triangleright D$ is a demo\n7: end function\n8: $D \\leftarrow$ AutoDemo()\n9: $p \\leftarrow I + D + Q$  $\\triangleright p$ is a input prompt\n10: $eq \\leftarrow R(p)$\n11: if $S(eq)$ then $\\triangleright$ Equation System is solvable\n12:  $ans \\leftarrow F(Q + eq + S(eq))$\n13: else\n14:  $ans \\leftarrow F(Q + eq)$\n15: end if\n16: return $ans$   $\\triangleright$ Return the Answer"}, {"title": "A Further Experiment", "content": ""}, {"title": "A.1 Generalization to common arithmetic datasets", "content": "We analyze the generalizability of the Formulate-and-Solve framework to other common arithmetic datasets, such as GSM8K, SVAMP, AddSub, SingleEq, and MultiArith where some problems can be seen as single unknown problems on GPT-3.5. From Table 7, we can see that our method can still perform a comparable performance to other existing custom and manual prompting methods for arithmetic tasks since these datasets are much easier than multiple unknown datasets and allow only minimal room for improvement. Besides, our method is more generalized not only to single unknown problems but also can deal with multiple unknown problems automatically."}, {"title": "A.2 Experiments different mathematical models on BeyondX", "content": "We further evaluate seven different existing mathematical models fine-tuned on Mistral-7B on BeyondX under Zero-shot-CoT setting. As shown in Figure 4, the results indicate that these open-source LLMs are still struggling with more complex mathematical reasoning tasks in multiple unknown problems. There is still a significant amount of effort during pretraining or supervised fine-tuning to instill enough multiple unknown knowledge and the way of solving multiple unknown system of equations into the models' parameters to close the gap."}, {"title": "A.3 Experiments different models on Formulate-and-Solve", "content": "We further assess the performance of Formulate-and-Solve across various base models, such as GPT-4 and Gemini-Pro. The results of the general algebra dataset and the multi-unknown dataset are shown in Table 13 and Table 12, and we also illustrate the performance curve of GPT-4 and Gemini-Pro on different numbers of unknown in Figure 5a and Figure 5b. The findings remain as GPT-3.5 and our method outperforms a large gap among other methods. Additionally, we observe that the performance of the Gemini-Pro generally falls between that of GPT-3.5 and GPT-4 across various datasets and prompting methods."}, {"title": "A.4 Experiments different shots of demonstrations on Formulate-and-Solve", "content": "We analyze the effect of varying the number of automatic generated exemplars (K) in our approach on GPT-3.5. Here, we show three variations with K = 3, 5, and 8. In Table 11, we observe that LLM demonstrates consistent performance under single or double unknown in different datasets. When K is bigger, on average, performance improves."}, {"title": "B Full Instruction", "content": "In this section, we show the full instructions in Section 3 and Section 5."}, {"title": "B.1 Automatic Generation of Multiple Unknown Algebra Problems", "content": "We can see the full instructions in Table 14."}, {"title": "B.2 Automatic Solver of Algebra Problems", "content": "We can see the full instructions in Table 15."}, {"title": "C Detail Studies of Automatic Generation of Multi-Unknown Algebra Problems", "content": ""}, {"title": "C.1 Construction Steps", "content": "Starting from the 2 unknown problems in our seed dataset ALG514 and DRAW-1K, we use Zero-shot prompting with GPT-4 (gpt-4-0613) to generate an initial demonstration using the instruction in Table 14, which is then manually refined. The LLM then iteratively creates additional demonstrations (approximately five) based on the problem, the system of equations, and the existing demonstrations. Combining this information, the LLM generates a new problem with N+1 unknowns and its corresponding system of equations. Finally, we use GPT-4 to solve these newly generated problems, discarding low quality ones (where GPT-4 provides incorrect answers) for the next round."}, {"title": "C.2 Quality Validation", "content": "We recruit 12 raters to validate whether the generated problems are reasonable and whether they are consistent with the generated system of equations. We show the ratio of problems that are marked as"}, {"title": "D Detail Studies of Automatic Solver of Algebra Problems", "content": ""}, {"title": "D.1 Model Hyperparameters", "content": "The hyperparameters for the experiments for studying Formulate-and-Solve and other prompting methods are set to their default values to ensure consistency in our experiment. Table 9 details the specific generation parameters for the various LLMs we evaluate."}, {"title": "Algorithm 1 Formulate-and-Solve Reasoning Algorithm", "content": "Require: question $Q$, instruction $I$, reasoning module $R$, symbolic solver $S$, finalize module $F$\n1: function AUTO DEMO($I$, Question, $K$)\n2:  while $K \\neq 0$ do\n3:   $D \\leftarrow R(I + D + Question)$\n4:   $K \\leftarrow K - 1$\n5:  end while\n6:  return $D$  $\\triangleright D$ is a demo\n7: end function\n8: $D \\leftarrow$ AutoDemo()\n9: $p \\leftarrow I + D + Q$  $\\triangleright p$ is a input prompt\n10: $eq \\leftarrow R(p)$\n11: if $S(eq)$ then $\\triangleright$ Equation System is solvable\n12:  $ans \\leftarrow F(Q + eq + S(eq))$\n13: else\n14:  $ans \\leftarrow F(Q + eq)$\n15: end if\n16: return $ans$   $\\triangleright$ Return the Answer"}, {"title": "Instruction for Automatic Generation of Multiple Unknown Algebra Problems", "content": "1. Write down each variable meaning in a system of equations by understanding the relation between a system of equations and a math word problem.\n2. Introduce a new variable based on the existing variable meaning. Then, assign a value to the new variable based on the solution of the existing system of equations.\n3. Add new terms and new equations to the existing system of equations to generate a solvable three-unknown system of equations.\n4. Introduce new statements that are related to the existing math word problem or modify the existing statement to rewrite the problem into a three-unknown math word problem based on the new system of equations and the same original scenario.\n5. Formulate and rephrase the statements and scenario into a coherent and reasonable math word problem."}, {"title": "Instruction for Automatic Solver of Algebra Problems", "content": "1. Determine what the question is asking.\n2. Write down the relevant information in simple statements.\n3. Assign symbols (must be an alphabetic character e.g., x, y, z etc.) to unknown values that must be found.\n4. Determine how the statements relate to each other mathematically.\n5. Give the equations only here, with each equation on a new line."}, {"title": "MWP", "content": "Suppose you invested 10,000 dollars, part at 6% annual interest and the rest at 9% annual interest. If you received 684 dollars in interest after one year, how much did you invest at 6% annual interest and at 9% annual interest?"}, {"title": "System of Equations", "content": "6.0 * 0.01 * x + 9.0 * 0.01 * y = 684.0\nx + y = 10000.0\n(x = 7200.0; y = 2800.0)"}, {"title": "Variable Meaning", "content": "x = invested at 6% annual interest; y invested at 9% annual interest"}, {"title": "MWP", "content": "Suppose you invested 12,000 dollars in three different ways: part at 6% annual interest, part at 9% annual interest, and the rest at 4 % annual interest. After one year, you received 764 dollars in interest. Also, the amount invested at 6% annual interest plus the amount invested at 9% annual interest equals five times the amount invested at 4% annual interest. How much did you invest at each rate?"}, {"title": "System of Equations", "content": "6.0 * 0.01 * x + 9.0 * 0.01 * y + 4.0 * 0.01 * z = 764.0\nx + y + z = 12000.0\nx + y = 5.0 * z\n(x = 7200.0; y = 2800.0; z = 2000.0)"}, {"title": "Variable Meaning", "content": "x = invested at 6% annual interest; y = invested at 9% annual interest; z = invested at 4% annual interest"}, {"title": "MWP", "content": "Suppose you invested 14", "ways": "part at 6% annual interest, part at 9% annual interest, part at 4% annual interest, and the rest at 3% annual interest. After one year, you received 824 dollars in interest. Also, the amount invested at 6% annual interest plus the amount invested at 9% annual interest equals five times the amount invested at 4% annual interest. The amount invested at 6% annual interest plus the amount invested at 9% annual interest plus the amount invested at 4% annual interest equals"}]}