{"title": "Solving for X and Beyond: Can Large Language Models Solve Complex\nMath Problems with More-Than-Two Unknowns?", "authors": ["Kuei-Chun Kao", "Ruochen Wang", "Cho-Jui Hsieh"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in solving\nmath problems, a hallmark of human intelligence. Despite high success rates on current\nbenchmarks; however, these often feature simple problems with only one or two unknowns,\nwhich do not sufficiently challenge their reasoning capacities. This paper introduces a\nnovel benchmark, BeyondX, designed to address these limitations by incorporating prob-\nlems with multiple unknowns. Recognizing the challenges in proposing multi-unknown prob-\nlems from scratch, we developed BeyondX using an innovative automated pipeline that pro-\ngressively increases complexity by expanding the number of unknowns in simpler problems.\nEmpirical study on BeyondX reveals that the performance of existing LLMs, even those fine-\ntuned specifically on math tasks, significantly decreases as the number of unknowns increases\nwith a performance drop of up to 70% observed in GPT-4. To tackle these challenges,\nwe propose the Formulate-and-Solve strategy, a generalized prompting approach that effec-\ntively handles problems with an arbitrary number of unknowns. Our findings reveal that this\nstrategy not only enhances LLM performance on the BeyondX benchmark but also provides\ndeeper insights into the computational limits of LLMs when faced with more complex mathe-\nmatical challenges.", "sections": [{"title": "1 Introduction", "content": "Mathematical problem-solving is a fundamental\naspect of human intelligence, necessitating both\nlanguage comprehension and reasoning skills. Re-\ncently, LLMs pretrained on extensive web-scale\ndatasets, have exhibited exceptional abilities in ad-\ndressing a variety of complex tasks. Consequently,\nmathematical challenges are frequently employed\nto benchmark the reasoning abilities of LLMs.\nStudies have shown that these models demonstrate\nhuman-level efficacy in solving math problems,"}, {"title": "C1: BeyondX - The first multi-unknown algebraic benchmark.", "content": "To efficiently generate a large\ncorpus of multi-unknown problems, we developed\na novel pipeline that automatically expands existing\nproblems to N unknowns. This pipeline operates\non three key ideas: (1). Scenario Expansion: New\nproblems are derived by extending the scenarios\n(such as financial calculations or grocery shopping)\nof existing simpler problems, ensuring contextual\nrelevance. (2). Progressive Extrapolation: We\nadd unknowns incrementally one at a time\nprogressing from problems with two unknowns to\nthree, four, and so on. This step-by-step approach\nmakes the problem generation process significantly\nmore manageable. (3). Decomposed Problem Gen-\neration: Instead of creating an entire problem at\nonce, we decompose the process. The LLM is\ncarefully instructed to first introduce new unknown\nvariables into the base scenarios, generate the corre-\nsponding equations, translate these equations into\nsemantic statements, and finally integrate them into\nthe comprehensive problem statement."}, {"title": "C2: Existing LLMs struggles with increasing\nunknowns.", "content": "Utilizing our BeyondX benchmark,\nwe conduct a comprehensive evaluation of cur-\nrent LLMs, which includes both general-purpose\nmodels like GPT-3.5 (Brown et al., 2020), GPT-\n4, Gemini-Pro (Team et al., 2023), and Mistral-\n7B (Jiang et al., 2023) as well as models specifi-\ncally fine-tuned on mathematical problems (Wiz-\nardMath (Luo et al., 2023), OpenMath (Toshniwal\net al., 2024), and MetaMath (Yu et al., 2023)). Our\nfindings reveal a significant drop in performance\nas the number of unknowns in problems increases\n\u2014 a staggering ~ 70% degradation on GPT-4 for\ninstance.\n\u2022 This marked decline indicates that current\nbenchmarks may overstate the algebraic capa-\nbilities of these models.\n\u2022 Additionally, despite efforts to fine-tune\nLLMs on previous math corpus, they still\nstruggle with multi-unknown problems.\n\u2022 Even sophisticated prompting strategies,\nwhich utilize detailed natural language expla-\nnations, fail to substantially aid LLMs in over-\ncoming these more complex challenges."}, {"title": "C3: Formulate-and-Solve A prompting method\nto tackle multi-unknown problems.", "content": "Traditional\nprompting methods for LLMs typically do not ac-\ncount for the complexity inherent in systems of\nequations, potentially limiting the math-solving ca-"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Math Word Problem Generation", "content": "Early research on math word problem (MWP)\ngeneration relied heavily on pre-defined struc-\ntures, including domain knowledge, equations, and\ntext templates (Nandhini and Balasundaram, 2011;\nWilliams, 2011; Polozov et al., 2015). More re-\ncently, researchers began using pre-trained models\nfined-tuned on equation-to-MWP examples (Wang\net al., 2021). Studies on using LLMs for MWP\ngeneration are scarce. Existing work includes eval-\nuating GPT-3's ability to mimic specific problem\ntypes (Zong and Krishnamachari, 2023) and us-\ning GPT-4 to improve readability in existing prob-\nlems (Norberg et al., 2023). However, these ap-\nproaches are limited to replicating existing prob-\nlem structures, such as the number of unknowns or\nequation templates. Our work focuses on how to\nexpand existing one or two unknown problems into\nmore complex multiple unknowns."}, {"title": "2.2 Math Word Problem Solver", "content": "Mathematical reasoning skills are crucial for in-\ntelligent systems, leading to a surge in research.\nIn the past, studies focused on how statistical and\ndeep learning NLP models could solve arithmetic"}, {"title": "2.3 Math Reasoning with LLMs", "content": "Many prompting techniques have emerged to un-\nlock the reasoning abilities of LLMs (Qiao et al.,\n2022). Chain-of-Thought (CoT) Prompting (Wei\net al., 2022; Kojima et al., 2022; Wang et al., 2022)\nwas proposed to generate the reasoning steps be-\nfore submitting the answer. Later, several other\nworks (Nye et al., 2021; Zhou et al., 2022; Droz-\ndov et al., 2022; Wang et al., 2023) also proposed\ndifferent approaches to utilize LLMs to solve rea-\nsoning tasks by allowing intermediate steps or plan-\nning first before solving. All of these methods\nallow LLMs to process all steps without using\nany external tools or refinements. For incorporat-\ning external tools, Programming-of-Thought (PoT)\nprompting (Chen et al., 2022; Gao et al., 2022)\nutilizes LLMs with code pretraining to write a pro-\ngram as a rationale that explains the reasoning pro-\ncess. Equation-of-Thought (EoT) (Liu et al., 2023;\nHe-Yueya et al., 2023) prompting tackles MWPs\nby converting them into linear equation systems,\nwhich are then solved by a symbolic solver. Al-\nthough PoT and EoT tried to use external tools to\nsolve MWPs, they did not consider the scenario of\nmultiple unknown variables.\nRecent LLMs advancements for math reason-\ning involve various training approaches. One\nmethod focuses on pretraining data specifically\ndesigned for math, such as Minerva (Lewkowycz\net al., 2022), Llemma (Azerbayev et al., 2023), and\nDeepSeekMath (Shao et al., 2024). Another ap-\nproach involves generating synthetic questions and\nanswers that mimic existing benchmarks. For ex-\nample, the WizardMath series (Luo et al., 2023)\nthat improves mathematical reasoning in Mistral\n7B (Jiang et al., 2023) with problems sourced\nprimarily from GSM8K and MATH (Hendrycks\net al., 2021) via output from closed-source LLMs.\nMetaMath (Yu et al., 2023) and MMIQC (Liu and\nYao, 2024) focus on expanding existing questions"}, {"title": "3 Automatic Generation of\nMulti-Unknown Algebra Problems via\nProgressive Expansion", "content": ""}, {"title": "3.1 Challenges for Constructing\nMulti-Unknown Datasets", "content": "Generating new problems with LLMs. Creat-\ning correct, diverse, and solvable math problems\nmanually is an exceptionally laborious task. The\ncomplexity of this task increases with the addi-\ntion of each unknown, as more unknowns require\nconsideration of additional relationships within the\nproblem scenario. To automatize this process, we\nemploy LLMs to generate the problems, with hu-\nman verifiers subsequently ensuring the quality and\nsolvability of these problems.\nLimitations of naive generation. Directly\nprompting LLMs to generate multi-unknown al-\ngebra problems has often resulted in poor quality\noutputs. Firstly, generating problem scenarios from\nscratch tends to produce a narrow range of prob-\nlem types, as evidenced by the lack of diversity\nreported in Table 16. Secondly, attempting to gen-\nerate all relevant relationships and corresponding\nequations in a single step frequently leads to vi-\nolations of problem constraints, rendering many\nproblems unsolvable as detailed in Table 17."}, {"title": "3.2 Generating New Problems via Progress\nExpansion", "content": "Pipeline overview. To address the aforemen-\ntioned challenges, we introduce a novel approach\ncalled Progressive Expansion, which applies a\ndivide-and-conquer strategy: (1). Scenario Diver-\nsification: We begin by expanding existing simpler\nproblems to increase scenario diversity. This lever-\nages the rich variety of simpler problem scenarios\nas a foundation for more complex questions. (2)."}, {"title": "3.3 Constructing the Benchmark", "content": "Seed problems. We select ALG514 (Kushman\net al., 2014) and DRAW-1K (Upadhyay and Chang,\n2017) as the foundational seed problems to expand.\nThese datasets are particularly suitable as they in-\nclude full solutions with oracle equation sets.\nStatistics. With this generation process, we se-\nlected a total of 464 problems. Specifically, there\nare 194 problems with three unknowns, 158 prob-\nlems with four unknowns, and 112 problems with\nfive unknowns. In addition, since our generated\ndataset is expanded from the existing dataset, it con-\ntains various topics or subjects including moving\nobjects, liquids, interest, distance, and geometry."}, {"title": "4 Benchmarking existing LLMs and\nPrompting Methods", "content": ""}, {"title": "4.1 LLMs for solving multi-unknown algebra\nproblems", "content": "To evaluate the performance of various LLMs on\nBeyondX, we consider the Zero-shot-CoT prompt-\ning method (details in Section 6.1) and test the per-\nformance of both General-Purpose LLMs (GPT-3.5,\nGPT-4, Gemini-Pro, Mistral-7B) and Mathemati-\ncally fine-tuned LLMs (WizardMath, OpenMath,\nMetaMath)."}, {"title": "4.2 Prompting Methods", "content": "Figure 2a demonstrates that state-of-the-art LLMs\ncannot solve multi-unknown problems with Zero-\nshot-CoT prompting. To investigate whether this\nissue can be mitigated with better prompting meth-\nods, we evaluated nine existing prompting methods\nusing GPT-3.5, categorized into three types:\nZero-shot. Zero-shot-CoT (Kojima et al., 2022)\nand Plan-and-Solve (Wang et al., 2023) prompting.\nFew-shot with manual demonstrations. CoT (Wei\net al., 2022), PoT (Gao et al., 2022), EoT (Liu\net al., 2023), and Declarative (He-Yueya et al.,\n2023) prompting.\nFew-shot with automatic demonstrations. Analog-\nical (Yasunaga et al., 2023), Auto-Zero-shot-CoT.\nIn Figure 2c, we observe zero-shot and few-shot\nCoT prompting methods seem inadequate when\nsolving multi-unknown problems. We find that\nwhile CoT correctly sets up the equations, it fails\nto accurately solve the system of equations. Ad-\nditionally, even though some prompting methods\nlike PoT, EoT, and Declarative use external tools\nas a calculator and equation solver, they manually"}, {"title": "5 Automatic Solver of Algebra Problems", "content": "To investigate whether the observed performance\ndrop is primarily due to inadequate prompting\nstrategies or the limitation of LLMs. we develop\nFormulate-and-Solve, an automated prompting\nmethod designed for LLMs to solve math prob-\nlems with an arbitrary number of unknowns. We\nalso show that our method performs competitively\nto state-of-the-art algorithms even for non-algebra\nproblems in Appendix A.1.\nA major challenge in applying the prompting\nmethod to multi-unknown problems is the scarcity\nof hand-crafted demonstrations. Traditional ex-\namples with a single unknown do not scale well\nto more complex, multi-unknown scenarios, ne-\ncessitating automated demo generation. Further-\nmore, while LLMs can be guided by prompts to\nsolve these systems of equations, they often re-\nquire external tools due to their limited ability to\nindependently solve and explicitly formulate these\nproblems into a system of equations.\nTo overcome these limitations, we propose\nFormulate-and-Solve, a framework that incorpo-"}, {"title": "6 Experimental Results", "content": ""}, {"title": "6.1 Experimental Setting", "content": "Dataset. Our experiments are conducted on five\nalgebra problem sets, including existing widely-\nused ones (ALG514, DRAW-1K, AsDiv, HMWP)\nand the proposed BeyondX benchmark. AsDiv\nconsists of a wide range of math problems and\nwe only take the algebra problem subset. Also,\nsince HWMP is a Chinese dataset, we use GPT-4\nto translate the problem into English. We find that\nwhile most translation results effectively convey\nthe intended meaning. The five datasets differ in\nsize and complexity, as shown in Table 1. We also\nreport the average number of unknowns in each\ndataset. Note that we split the proposed dataset\ninto three subsets, correspond to problems with 3\n(BeyondX_3), 4 (BeyondX_4), and 5 (BeyondX_5)\nunknowns, while the problems in all other datasets\nhave \u2264 2 unknowns."}, {"title": "6.2 Main Results", "content": "To investigate which reasoning methods and mod-\nels better solve multi-unknown problems, we sum-\nmarize the performance of different prompting\nmethods using GPT-3.5 in Table 2 and Tables 3. We\nalso evaluate and compare various prompting meth-\nods with GPT-4 and Gemini-Pro in Appendix A.3."}, {"title": "7 Discussion and Analysis", "content": "Ablation study. We analyze the significance of\neach component within in Formulate-and-Solve\nthrough an ablation study. We assess five vari-\nations: (1) Use only a system of equations as a\nrationale for reasoning. (2) Remove the instruction\nbefore demonstrations. (3) Remove demonstrations\nafter the instruction. (4) Use an LLM instead of a\nsymbolic solver to solve a system of equations. (5)\nFormulate-and-Solve. We randomly select 60 prob-\nlems for each unknown from ALG514, DRAW-1K,\nand BeyondX to evaluate each variation. The re-\nsults are provided in Table 4.\nWe observe that performance decreases signif-\nicantly when instruction or demonstration is re-\nmoved, highlighting its role in guiding the LLM.\nInterestingly, instruction has a greater impact than\ndemonstration. Replacing the symbolic solver with\nan LLM also leads to a large decrease in accu-\nracy. These findings confirm that all elements"}, {"title": "8 Conclusion", "content": "We introduce BeyondX, the first benchmark for\nevaluating LLMs on multi-unknown problems. Our\nanalysis reveals a significant performance drop in\nLLMs and existing prompting methods when faced\nwith such problems. To address this, we propose\nFormulate-and-Solve, a novel prompting method\nthat leverages instruction, automatic demonstra-\ntions and a system of equations. Experiments\ndemonstrate the effectiveness of Formulate-and-\nSolve in tackling multi-unknown problems."}, {"title": "Limitations", "content": ""}, {"title": "Scope of Benchmark.", "content": "Although our automatic generation method can de-\ncrease the labor-intensive data collection process,\nour method still needs to be expanded from high-\nquality problems with low unknowns. Besides, we\nfigure that some types or topics of the problems\ncannot be extended to multiple unknown problems.\nAnd, our benchmark is limited to English questions\nand data. We look forward to future benchmarks on\na broader domain or modality and other languages."}, {"title": "Models and Reasoning Methods.", "content": "Although we experiment with many representative\nmodels and reasoning methods in this paper, we\nacknowledge that this does not cover all models\nand frameworks. Besides, we acknowledge that our\napproach falls short on more straightforward arith-\nmetic datasets since our method is more suitable\nfor algebra datasets. Further research is required to\nexplore new problem-solving methods for general\nmath reasoning tasks, including different modali-\nties."}, {"title": "A Further Experiment", "content": ""}, {"title": "A.1 Generalization to common arithmetic\ndatasets", "content": "We analyze the generalizability of the Formulate-\nand-Solve framework to other common arithmetic\ndatasets, such as GSM8K, SVAMP, AddSub, Sin-\ngleEq, and MultiArith where some problems can\nbe seen as single unknown problems on GPT-3.5.\nFrom Table 7, we can see that our method can\nstill perform a comparable performance to other\nexisting custom and manual prompting methods\nfor arithmetic tasks since these datasets are much\neasier than multiple unknown datasets and allow\nonly minimal room for improvement. Besides, our\nmethod is more generalized not only to single un-\nknown problems but also can deal with multiple\nunknown problems automatically."}, {"title": "A.2 Experiments different mathematical\nmodels on BeyondX", "content": "We further evaluate seven different existing math-\nematical models fine-tuned on Mistral-7B on Be-\nyondX under Zero-shot-CoT setting. As shown in\nFigure 4, the results indicate that these open-source\nLLMs are still struggling with more complex math-\nematical reasoning tasks in multiple unknown prob-\nlems. There is still a significant amount of effort\nduring pretraining or supervised fine-tuning to in-"}, {"title": "A.3 Experiments different models on\nFormulate-and-Solve", "content": "We further assess the performance of Formulate-\nand-Solve across various base models, such as GPT-\n4 and Gemini-Pro. The results of the general al-\ngebra dataset and the multi-unknown dataset are\nshown in Table 13 and Table 12, and we also illus-\ntrate the performance curve of GPT-4 and Gemini-\nPro on different numbers of unknown in Figure 5a\nand Figure 5b. The findings remain as GPT-3.5 and\nour method outperforms a large gap among other\nmethods. Additionally, we observe that the perfor-\nmance of the Gemini-Pro generally falls between\nthat of GPT-3.5 and GPT-4 across various datasets\nand prompting methods."}, {"title": "A.4 Experiments different shots of\ndemonstrations on Formulate-and-Solve", "content": "We analyze the effect of varying the number of\nautomatic generated exemplars (K) in our approach\non GPT-3.5. Here, we show three variations with\nK = 3, 5, and 8. In Table 11, we observe that LLM\ndemonstrates consistent performance under single\nor double unknown in different datasets. When K\nis bigger, on average, performance improves."}, {"title": "B Full Instruction", "content": "In this section, we show the full instructions in\nSection 3 and Section 5."}, {"title": "B.1 Automatic Generation of Multiple\nUnknown Algebra Problems", "content": "We can see the full instructions in Table 14."}, {"title": "B.2 Automatic Solver of Algebra Problems", "content": "We can see the full instructions in Table 15."}, {"title": "C Detail Studies of Automatic Generation\nof Multi-Unknown Algebra Problems", "content": ""}, {"title": "C.1 Construction Steps", "content": "Starting from the 2 unknown problems in our seed\ndataset ALG514 and DRAW-1K, we use Zero-shot\nprompting with GPT-4 (gpt-4-0613) to generate\nan initial demonstration using the instruction in Ta-\nble 14, which is then manually refined. The LLM\nthen iteratively creates additional demonstrations\n(approximately five) based on the problem, the sys-\ntem of equations, and the existing demonstrations.\nCombining this information, the LLM generates\na new problem with N+1 unknowns and its cor-\nresponding system of equations. Finally, we use\nGPT-4 to solve these newly generated problems,\ndiscarding low-quality ones (where GPT-4 provides\nincorrect answers) for the next round."}, {"title": "C.2 Quality Validation", "content": "We recruit 12 raters to validate whether the gener-\nated problems are reasonable and whether they are\nconsistent with the generated system of equations.\nWe show the ratio of problems that are marked as"}, {"title": "D Detail Studies of Automatic Solver of\nAlgebra Problems", "content": ""}, {"title": "D.1 Model Hyperparameters", "content": "The hyperparameters for the experiments for study-\ning Formulate-and-Solve and other prompting\nmethods are set to their default values to ensure\nconsistency in our experiment. Table 9 details\nthe specific generation parameters for the various\nLLMs we evaluate."}]}