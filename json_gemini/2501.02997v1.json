{"title": "CALM: Curiosity-Driven Auditing for Large Language Models", "authors": ["Xiang Zheng", "Longxiang Wang", "Yi Liu", "Xingjun Ma", "Chao Shen", "Cong Wang"], "abstract": "Auditing Large Language Models (LLMs) is a crucial and challenging task. In this study, we focus on auditing black-box LLMs without access to their parameters, only to the provided service. We treat this type of auditing as a black-box optimization problem where the goal is to automatically uncover input-output pairs of the target LLMs that exhibit illegal, immoral, or unsafe behaviors. For instance, we may seek a non-toxic input that the target LLM responds to with a toxic output or an input that induces the hallucinative response from the target LLM containing politically sensitive individuals. This black-box optimization is challenging due to the scarcity of feasible points, the discrete nature of the prompt space, and the large search space. To address these challenges, we propose Curiosity-Driven Auditing for Large Language Models (CALM), which uses intrinsically motivated reinforcement learning to finetune an LLM as the auditor agent to uncover potential harmful and biased input-output pairs of the target LLM. CALM successfully identifies derogatory completions involving celebrities and uncovers inputs that elicit specific names under the black-box setting. This work offers a promising direction for auditing black-box LLMs. Our code is available at https://github.com/x-zheng16/CALM.git.", "sections": [{"title": "1 Introduction", "content": "The development of Large Language Models (LLMs) represents a significant advancement in artificial intelligence, allowing machines to produce human-like text with impressive fluency and understanding of context (Radford et al. 2019). These models have wide-ranging applications, from facilitating natural language comprehension to generating creative content, solidifying their importance in education, industry, and research (Xu et al. 2024). However, the considerable capabilities of LLMs also bring about significant concerns, particularly regarding their potential to generate toxic or hallucinative outputs (Wallace et al. 2019; Zou et al. 2023). The complex and often incomprehensible internal processes on which these models base their decisions further complicate the challenge of ensuring their safe and responsible use (Wei, Haghtalab, and Steinhardt 2024).\nAuditing LLMs is an essential and promising step in managing risks they may expose (Rastegarpanah, Gummadi, and Crovella 2021). The auditing process is closely linked to red teaming (Hong et al. 2024), a strategy traditionally used to test systems by subjecting them to adversarial challenges. While red teaming is focused on identifying risks through adversarial prompts crafted by the internal red team, auditing involves systematically evaluating a target LLM's behavior based on ethical and safety standards established by external auditors or stakeholders (M\u00f6kander et al. 2023). In this paper, we refer to auditing to assess and monitor the target LLM's alignment and compliance over time. The aim is to uncover and monitor undesirable behaviors before and after the target LLM is widely deployed. However, current auditing methods often face challenges in dealing with the black-box nature of LLMs, especially when access to the model's parameters is restricted, for example, when the target LLM is offered as services in the cloud.\nThere are various undesired behaviors that LLMs might exhibit, such as producing toxic content, stereotypes, discrimination, and leaking private information (Mazeika et al. 2024). Generally, we can formulate the auditing objective that captures specific undesired behaviors as a multivariate function r(s, o), where s represents the audit prompt and o represents the response from the target LLM. For instance, r(s, o) can measure whether the output o is legally and ethically toxic, biased, sensitive, or private. In this work, we focus on two specific auditing objectives: generating specific suffixes (e.g., names of senators) and toxic completions about celebrities. Maximizing the auditing objectives can uncover toxic and sensitive behaviors of the target LLM. Moreover, adopting such an auditing objective makes it easy to adapt to auditing new undesired behaviors for specific auditors and stakeholders.\nThe auditing methods previously used for black-box LLMs have primarily relied on manually created prompts (Yu et al. 2024; Zhang et al. 2024). While useful, these methods have limitations in exploring these models' vast and complex input space. Manually crafted prompts cannot cover the full range of potential outputs. Moreover, these methods struggle to identify rare but potentially harmful outputs, making it challenging to uncover infrequent yet possibly catastrophic behaviors in the models. Research has shown that harmful behaviors in LLMs can be rare and context-dependent, which presents significant challenges for traditional auditing methods that may be unable to detect these rare cases.\nTo tackle these challenges, we propose a novel black-box auditing approach: Curiosity-Driven Auditing for Large Language Models (CALM). CALM is designed to operate in a black-box setting, where the auditor cannot directly access the target LLM's parameters. CALM employs intrinsically motivated Reinforcement Learning (RL) (Zheng et al. 2024b) to finetune an audit LLM to generate diverse audit prompts that can induce specific responses from the target LLM, such as derogatory comments or factual errors about celebrities. The intuition behind CALM is that by leveraging curiosity-driven exploration, the auditor can efficiently navigate the vast and discrete prompt space to uncover specific behaviors that might remain hidden. We leverage the policy cover theory (Agarwal et al. 2020) to design the token-level intrinsic bonus in the token embedding space for estimating the novelty of each token s\u1d62 in the audit prompt S\u209c = [s\u2081, s\u2082, ..., s\u209c] at the audit LLM's each generation step. Intuitively, the token-level intrinsic bonus for each token s\u1d62 represents the sparsity of each token s\u1d62 in the token embedding space. By intrinsically rewarding the sparse token, the audit LLM is encouraged to explore the novel regions in the token embedding space (i.e., generate novel audit prompts) before it receives any external rewards (i.e., induces the target LLM to produce any specific behaviors), instead of sticking to the small explored region (i.e., generating repetitive and meaningless audit prompts).\nWe evaluate CALM through comprehensive experiments, maximizing the two auditing objectives across multiple LLMs. Our experimental results demonstrate the effectiveness of CALM in identifying a variety of problematic behaviors, from generating derogatory content related to public figures to producing sensitive names. We provide examples of the audit prompt s generated by the audit LLM and the induced response o from the target LLM in Section 1 and Table 2. Surprisingly, we find that even finetuning a relatively small transformer-based model like GPT-2 can discover the undesired behaviors of larger LLMs like Llama-3-8B. We attribute this success to CALM's curiosity-driven exploration. These findings highlight the potential risks LLMs pose and underscore the importance of curiosity-driven RL-based black-box LLM auditing.\nThe main contributions of this paper are as follows:\n\u2022 We present CALM, a novel approach to auditing black-box LLMs that utilizes intrinsically motivated RL to finetune an audit LLM to efficiently discover undesired behaviors of the target LLM in the black-box setting.\n\u2022 We design a novel token-level intrinsic bonus based on the policy cover theory to encourage the audit LLM to explore the token embedding space efficiently.\n\u2022 We validate the effectiveness of CALM through extensive experiments, showcasing its ability to uncover subtle and harmful behaviors in LLMs across multiple tasks, including inverse suffix generation and toxic completion."}, {"title": "2 Related Work", "content": "Algorithmic auditing. Algorithmic auditing has become crucial for ensuring the development and deployment of artificial intelligence systems, especially for complex models such as LLMs operating in high-stakes environments (Vecchione, Levy, and Barocas 2021). Auditing involves systematically evaluating a model's behavior to ensure it meets ethical and safety standards, identifying potential biases, and assessing compliance with legal and regulatory requirements (Casper et al. 2024). Traditional auditing methods often rely on static datasets and predefined benchmarks, which may not capture the full range of behaviors in complex models like LLMs. Recent work has emphasized the importance of dynamic and adaptive auditing strategies to explore the model's behaviors and uncover hidden risks effectively.\nLLM-assisted red teaming. LLM-assisted red teaming is a proactive method for stress-testing black-box AI systems, such as LLMs, by simulating adversarial scenarios with a red-team LLM to find the weaknesses of the target LLM (Deng et al. 2022; Perez et al. 2022; Casper et al. 2023; Hong et al. 2024). Unlike traditional red teaming techniques that usually involve human adversaries manually testing the system, LLM-assisted methods leverage pre-trained LLMs to automate the process. The red-team LLM is instructed to generate diverse adversarial inputs. This technique is especially effective in identifying edge cases and failure modes that may not be found through conventional testing or fuzzing methods."}, {"title": "3 Preliminaries", "content": "Our CALM includes two essential components: 1) interaction with the target LLM and 2) reinforcement fine-tuning of the audit LLM. To better illustrate our method, we first introduce the notations and definitions involved in these two essential components.\nInteraction with the target LLM. In the context of CALM, we model the target LLM as a stochastic black-box function that generates outputs in response to the user prompt. Let s\u209c denote an input prompt, a sequence of tokens s\u209c = [s\u2081, s\u2082, ..., s\u209c], where each s\u1d62 belongs to a predefined vocabulary, and T is the length of the sequence. The target LLM, denoted as a stochastic function f (reflecting the top-k or top-p decoding strategies commonly employed in modern LLMs), maps this input prompt to an output sequence O_N = [o\u2081, o\u2082,..., o_N] of length N, such that o ~ f(s). Our goal is to identify specific input-output pairs [s, o] where the output o exhibits undesirable or harmful behaviors (e.g., producing toxic or sensitive content) while having no access to the target LLM's internal parameters.\nReinforcement fine-tuning of the audit LLM. The process of generating the next token in an LLM can be naturally modeled as a partially observable Markov Decision Process (POMDP), where each token generation is treated as an action, and the previously generated tokens constitute the observable state. In CALM, we denote the tunable audit LLM as \u03c0. At each step t, the audit LLM predicts the next token s\u209c based on the initial prompt z and the sequence of previously generated tokens s\u209c\u208b\u2081 = [s\u2081, s\u2082, ..., s\u209c\u208b\u2081]. Formally, the audit LLM updates its policy \u03c0(s\u209c|z, s\u209c\u208b\u2081) sequentially: at step one, s\u2081 is sampled via s\u2081 ~ \u03c0(\u00b7|z), and at step two, the next token is generated as s\u2082 ~ \u03c0(\u00b7|z, [s\u2081]). This formulation allows us to utilize modern RL algorithms like Proximal Policy Optimization (PPO) (Schulman et al. 2017) to fine-tune the audit LLM by maximizing expected rewards."}, {"title": "4 Curiosity-Driven Auditing", "content": "In this section, we provide details about CALM. We begin by analyzing previous auditing methods' shortcomings, then formulate the regularized auditing objective for CALM. Finally, we explore the design of the extrinsic auditing objective and the token-level intrinsic bonus.\nProblems of previous auditing method. Auditing LLMs traditionally depends on methods that require full access to the model's internal parameters (i.e., white-box methods) or rely on hand-crafted prompts in a black-box setting. While white-box gradient-based methods are effective in auditing LLMs, they are impractical in scenarios where the model's architecture and parameters are inaccessible, such as when auditing an LLM-powered service deployed in the cloud. Estimating gradients at each token position in the black-box setting (i.e., zero-order gradient) is computationally expensive and often infeasible for LLMs. To avoid gradient estimation in black-box scenarios, hand-crafted prompts are proposed. However, the reliance on hand-crafted prompts presents significant limitations. These prompts typically require extensive expert knowledge, are labor-intensive to create, and may fail to uncover potential vulnerabilities. Additionally, they tend to be narrow in scope, which restricts the exploration in the vast input space of LLMs, leaving many harmful behaviors undetected. As a result, there is an urgent need for efficient auditing methods that can function in black-box settings and effectively explore the input-output pairs of the target LLM to uncover undesirable behaviors.\nOur approach. We propose finetuning an audit LLM via intrinsically motivated RL to address the above problems. Specifically, We finetune an audit LLM to automate audit prompt generation. This audit LLM is reinforced by maximizing our novel regularized auditing objective to generate prompts more likely to elicit harmful outputs from the target LLM, thereby reducing reliance on human-crafted prompts. The regularized auditing objective consists of a primary auditing objective and an intrinsic objective that serves as a regulator. We also design curiosity-driven exploration bonuses based on the policy cover theory to encourage the audit LLM's exploration in the target LLM's prompt space.", "subsections": [{"title": "4.1 Regularized Auditing Objective", "content": "To effectively explore the input space and identify harmful behaviors, CALM employs intrinsically motivated RL for fine-tuning the audit LLM. The audit LLM, acting as an RL-based agent, aims to maximize a composite objective that includes both extrinsic and intrinsic rewards. The extrinsic reward, such as detecting harmful output behaviors, corresponds to the primary auditing objective r(s, o). The token-level intrinsic reward r_E (s), instead, encourages exploration by assigning token-level bonuses to novel or rarely encountered states. The optimization objective for the audit LLM in CALM is thus a regularized auditing objective as follows:\nmax JA(s) + \u03bb_I J_I(s) \u2013 \u03bb_KL J_KL(s),\n\u03c0\nExtrinsic Intrinsic KL Penalty\nwhere:\n\u2022 J_A(s) is the extrinsic objective with the auditing objective as the extrinsic reward. For the sake of simplicity, we also call J_A(s) the (expected) auditing objective.\n\u2022 J_I(s) is the intrinsic objective to encourage the audit LLM to explore in the token embedding space.\n\u2022 J_KL(s) is the Kullback-Leibler (KL) divergence term utilized in reinforcement fine-tuning, ensuring the fine-tuned audit LLM does not deviate excessively from its reference model.\nThe hyperparameters \u03bb_I and \u03bb_KL control the trade-offs between these objectives.\nSelection of auditing objectives. Following the previous work(Jones et al. 2023), we adopt two auditing objectives: inverse suffix generation and toxic completion. In inverse suffix generation, the audit LLM is tasked with creating suffixes that can evoke specific celebrities' names, akin to inverse engineering principles. The auditing objective for inverse suffix generation is then\nr(s, o) = Any(name in NameSet for name in o).  (2)\nIn toxic completion, the audit LLM generates subtle adversarial prompts targeting specific celebrities to provoke the target LLM into producing toxic content about them. The primary auditing objective for toxic completion is thus\nr(s, o) = NonToxic(s) & Toxic(o).  (3)\nWe present the implementation details of the toxicity classifier Toxic() in the experiment setup.\nThe audit LLM \u03c0 induces a prompt distribution P = \u03a0_1^T\u03c0(s_t| s_{t-1}) and a token distribution P^\u03c0(s) = (1 - \u03b3) \u03a3_{t=0}^T\u03b3^t P(s_t = s|z, \u03c0) with a discount factor \u03b3. The extrinsic objective J_A([s, o]) = E_{s~P^\u03c0,o~f(\u00b7|s)}r(s, o) is the expected reward based on the target LLM's response under the induced prompt distribution. Similarly, the intrinsic objective is defined as J_I(s) = E_{s~P^\u03c0} R_I(s), where R_I(s) is the token-level intrinsic bonus measures the novelty of the token s in the token embedding space T = \u211d^m, where m is the dimension of token embedding vector. We use the embedding layer h = \u03a6(OneHot(s)) of the audit LLM as the encoder to convert the token s into its embedding representation h, where OneHot() is the one-hot function that converts the discrete token s to a one-hot vector based on the predefined vocabulary of the audit LLM, and \u03a6 is the embedding layer of the audit LLM. Note that we do not require to know the embedding layer of the target LLM, and the intrinsic objective J_I(s) only involves the token s in the audit prompt s."}, {"title": "4.2 Token-Level Intrinsic Bonus", "content": "The design rationale of the intrinsic bonus is to measure the novelty of the state. There are various intrinsic motivation techniques to design the intrinsic bonus for each token, including knowledge-based and data-based intrinsic motivation methods(Zheng et al. 2024a). The key difference between knowledge-based and data-based intrinsic motivation methods is that knowledge-based intrinsic bonuses are estimated with all the agent's historical experiences. In contrast, data-based intrinsic motivation methods only concern the agent's current experience sampled by the latest policy. In this work, we adopt policy-cover-based intrinsic motivation, which belongs to knowledge-based intrinsic motivation.\nWe now discuss how to design the token-level intrinsic bonus R_I(s) based on the policy cover theory. To design a practical intrinsic objective, we leverage the concept of policy cover \u03c1(s) and define \u03c1(s) as a weighted sum of all historical token distributions. The intrinsic objective is designed to maximize the deviation of the current policy from the policy cover, thereby encouraging the agent to explore novel regions in the prompt space. The formal intrinsic objective of policy cover is as follows (Agarwal et al. 2020):\nJ_I(s) = \u2211_s"}]}, {"title": "Algorithm 1: CALM", "content": "Initialize the audit LLM \u03c0\u2080(s\u1d62|z, S\u1d62\u208b\u2081), the value function V(s), the step counter t = 0, the policy update step counter l = 0, the total policy update steps TotalSteps, the length of the audit prompt T, the length of the output of target LLM N, the audit objective r(s, o), and the initial prompt set {z} for the audit LLM according to the audit task.\nwhile l <TotalSteps do\nCollect samples {s\u209c = [s\u2081, s\u2082, ...s_T], o_N} with s_t ~ \u03c0_{\u03b8_l} (\u00b7|z, S_{t-1}) and o_N ~ f(\u00b7|s_T)\nCompute the auditing reward r(s, o) via Equation (2) or Equation (3)\nCompute the intrinsic bonus R_I(s) via Equation (6)\nCompute the advantage A(s_{t-1}, s_t) via Generalized Advantage Estimator (Schulman et al. 2016)\nCompute the policy loss L_\u03b8 via PPO\nUpdate the audit LLM's parameters \u03b8 via stochastic gradient ascent step on L_\u03b8\nUpdate the value function V(s_t) via regression\nend\nwhere P_\u03c0^(\u03c0)(s) is the token distribution induced by the current policy \u03c0, h = \u03a6(OneHot(s)) is the token embedding of the token s as stated in the previous subsection.\nThe intrinsic bonus at the l-th optimization iteration can be derived from Equation (4) based on the Frank-Wolfe Algorithm (Frank, Wolfe et al. 1956) as follows:\nR_I(s) = \u2207J_I(s) = \u2207\n\nPlease refer to Appendix A for details on utilizing the Frank-Wolfe Algorithm to derive the intrinsic bonus. To avoid directly estimating P_\u03c0(h) and \u03c1_l(h), which is challenging, we approximate the inverse of the policy cover 1/P_t(s) using the prediction error of a random neural network (Burda et al. 2019). The final policy-cover-based intrinsic bonus is then\nR_I(s) = ||\u03c6_1(h) \u2013 g_1(h)||||\u03c6_2(h) \u2013 g_2(h)||,\nwhere \u03c6_1 and \u03c6_2 are encoders trained to predict the outputs of two fixed random networks g_1 and g_2, respectively. Note that the parameters of \u03c6_2 are reinitialized after computing the prediction errors for the latest batch of audit prompts at each update step. This policy-cover-based intrinsic bonus can be considered a modified version of the prediction-error-based intrinsic bonus. Our design encourages the audit LLM to explore novel regions of the token space effectively."}, {"title": "5 Experiments", "content": "To evaluate the effectiveness of CALM, we conducted a series of experiments designed to assess its ability to uncover harmful behaviors in target black-box LLMs. Our experiments demonstrate how CALM can efficiently generate audit prompts that elicit undesirable outputs from the target LLM even when the model parameters are inaccessible."}, {"title": "5.1 Experiments Setup", "content": "We first detail the experimental setup, including the audit LLM backbone, RL backbone, the toxicity classifier's implementation details, and the baseline methods selection.\nAudit LLM and RL backbones. In our experiments, we adopt GPT-2 as the audit LLM, fine-tuning only its last two transformer blocks to balance adaptability and computational efficiency. GPT-2 is lightweight and has the essential text generation ability. We use PPO, a modern on-policy RL algorithm, as the RL backbone for reinforcement fine-tuning of the audit LLM. Our implementation runs on an Nvidia A6000 GPU (48G), which provides the necessary computational power for handling the high dimensionality of the LLM's input and output spaces.\nImplementation of the toxicity classifier. To assess the output generated by the target LLMs, we implement a simple toxicity classifier. This classifier checks if the output contains any Not-Safe-For-Work (NSFW) words. The decision to use this approach, rather than a more complex neural classifier, stems from several essential considerations. Neural classifiers, while powerful, are known to be vulnerable to adversarial attacks. These classifiers can be easily exploited by subtle manipulations of the input text that remain undetected by the model. For instance, attackers might intentionally alter the wording or structure of a sentence in ways that circumvent detection while retaining the toxic meaning. By contrast, our word-based classifier is more transparent and less prone to such exploitation. It directly checks for specific problematic terms, making it robust against attempts to evade detection through adversarial attacks. Although this approach is straightforward, it is effective for our study, where the primary goal is to detect overtly toxic language reliably. Furthermore, the word list used in our classifier is based on well-established criteria from previous research, ensuring that it covers a broad spectrum of commonly recognized toxic terms. For details on the specific words included in this list, please refer to Appendix B.\nSelection of baselines. We adapt two LLM-assisted red teaming methods named RL (Perez et al. 2022) and CRT (Hong et al. 2024) as our baselines. For justification of this selection, please refer to Appendix C."}, {"title": "5.2 Inverse Suffix Generation", "content": "In this section, we provide a detailed analysis of the audit LLM's ability for inverse suffix generation, as shown in Figure 1 and Figure 2. We focus specifically on comparing the performance of CALM and RL methods across various language models in the inverse suffix generation task.\nPerformance of the audit LLM. Figure 1 illustrates the convergence behavior of the audit LLM when auditing various target black-box LLMs, specifically GPT-2, Dolly-2-7B, Llama-2-7B, and Llama-3-8B, for the inverse suffix generation task. The results show that both CALM and RL methods converge towards the auditing objective as the number of queries increases. This convergence indicates that the RL-based auditing method effectively adapts to the task, improving performance over time and successfully generating the desired suffixes. Figure 2 further offers insight into the L0 norm of the NameSet coverage, which measures how well each method covers the desired set of names during the generation process. A key observation is the difference in variance between our method, CALM, and RL methods. Specifically, CALM exhibits consistently lower variance, mainly when applied to the Llama-3-8B model. This lower variance suggests that CALM not only achieves better overall coverage but does so with more excellent stability and reliability compared to the vanilla RL method. The reduced variance in CALM's performance is particularly significant for complex models like Llama-3-8B, where stable and consistent results are crucial for effective auditing.\nAblation study on intrinsic rewards. Here, we conduct an ablation study to analyze the effect of intrinsic rewards on the performance of the audit LLM when auditing the Llama-3-8B model in the inverse suffix generation task with a larger intrinsic coefficient \u03bb = 100. The results are presented in Figure 3, which illustrates the model's behavior across three metrics, including Auditing Objective, LO Norm of Set Coverage, and Entropy of Set Coverage.\nThe left subfigure in Figure 3 depicts the growth of auditing objectives as the number of queries increases. Incorporating intrinsic rewards facilitates a gradual improvement in the auditing objective over time, suggesting an enhancement in the model's capacity to explore the large token embedding space. The middle subfigure in Figure 3 portrays the LO Norm of Set Coverage, which assesses the model's effectiveness in encompassing the desired output set. The learning curve's rapid convergence signifies the intrinsic rewards' efficacy in guiding the model to explore and cover the related output space efficiently. Although the curve tends to be stable beyond the initial phase, it still grows gradually, indicating that the model continues to explore the prompt space. The right subfigure in Figure 3 illustrates the entropy of the token distribution, offering insights into the diversity of the model's outputs. Initially, the entropy is high, indicating that the model explores diverse possible outputs. As the number of queries increases, the entropy gradually decreases, suggesting that the model becomes more focused on specific outputs over time. Moreover, the relatively stable entropy observed in the later stages implies that the intrinsic rewards allow the model to balance exploration and exploitation, enabling it to concentrate on the most relevant outputs without completely sacrificing diversity."}, {"title": "5.3 Toxic Completion Task", "content": "The toxic completion task is a critical benchmark for assessing the ability of auditing methods to identify potential toxic outputs induced from the target LLM. We analyze the results of CALM in the senator-related toxic completion task in this section to show its effectiveness."}, {"title": "Performance of the audit LLM", "content": "Figure 4 highlights the consistently superior performance of CALM compared to the baseline methods, RL and CRT, across all tested models in the senator-related toxic completion task. Notably, CALM outperforms the baselines by significant margins, exceeding their results by over 35% and 50% in the GPT-2 and LLAMA3 models, respectively. In contrast, the baseline methods, RL and CRT, exhibit significantly lower peak performance across the models, with none reaching the efficacy of CALM. This underscores the limitations of current LLM-assisted red teaming approaches in black-box auditing tasks. Furthermore, the sentence-level diversity score introduced in CRT detrimentally impacted the performance of vanilla PPO in this context, highlighting the critical importance of our token-level intrinsic bonus for enhancing audit efficacy.\nIn addition to delivering superior performance, CALM demonstrates significantly faster convergence. As illustrated in Figure 4, CALM achieves over 80% in the auditing objective for Llama-3-8B with approximately 1.5 \u00d7 10\u2074 queries. Remarkably, it attains a 50% accuracy rate with just 1 \u00d7 10\u2074 queries, significantly faster than the baseline methods. This rapid convergence is a crucial advantage, allowing CALM to reach higher performance more efficiently. Moreover, CALM exhibits greater stability, with consistently lower variance in its results than RL and CRT, which are prone to more pronounced fluctuations."}, {"title": "Limitations", "content": "In this paper, we adopt the lightweight GPT-2 as the audit LLM backbone for CALM. AS CALM introduces a general intrinsically motivated auditing framework with a flexible auditor backbone, we believe a more powerful auditor backbone will enhance CALM's performance."}, {"title": "6 Conclusion", "content": "We proposed CALM that uses intrinsically motivated RL to finetune an audit LLM to uncover harmful and biased input-output pairs of the target black-box LLMs. CALM successfully identified toxic completions involving celebrities and uncovered inputs that elicited specific names under the black-box setting. The experimental results showed that CALM outperformed existing baselines and efficiently generated concerning input-output pairs that exhibit illegal, immoral, or unsafe behaviors from the target LLMs."}]}