{"title": "A WAVE IS WORTH 100 WORDS: INVESTIGATING\nCROSS-DOMAIN TRANSFERABILITY IN TIME SERIES", "authors": ["Xiangkai Ma", "Xiaobin Hong", "Wenzhong Li", "Sanglu Lu"], "abstract": "Time series analysis is a fundamental data mining task that has made encouraging progress in\nmany real-world scenarios. Supervised training methods based on empirical risk minimization\nhave proven their effectiveness on specific tasks and datasets. However, the acquisition of well-\nannotated data is costly and a large amount of unlabeled series data is under-utilized. Due to\ndistributional shifts across various domains and different patterns of interest across multiple tasks.\nThe problem of cross-domain multi-task migration of time series remains a significant challenge. To\naddress these problems, this paper proposes a novel cross-domain pretraining method based on Wave\nQuantization (termed as WQ4TS), which can be combined with any advanced time series model and\napplied to multiple downstream tasks. Specifically, we transfer the time series data from different\ndomains into a common spectral latent space, and enable the model to learn the temporal pattern\nknowledge of different domains directly from the common space and utilize it for the inference\nof downstream tasks, thereby mitigating the challenge of heterogeneous cross-domains migration.\nThe establishment of spectral latent space brings at least three benefits, cross-domain migration\ncapability thus adapting to zero- and few-shot scenarios without relying on priori knowledge of\nthe dataset, general compatible cross-domain migration framework without changing the existing\nmodel structure, and robust modeling capability thus achieving SOTA results in multiple downstream\ntasks. To demonstrate the effectiveness of the proposed approach, we conduct extensive experiments\nincluding three important tasks: forecasting, imputation, and classification. And three common\nreal-world data scenarios are simulated: full-data, few-shot, and zero-shot. The proposed WQ4TS\nachieves the best performance on 87.5% of all tasks, and the average improvement of the metrics on\nall the tasks is up to 34.7%.", "sections": [{"title": "Introduction", "content": "Time series analysis has broad real-world applications, including imputation of missing data Karmitsa et al. [2022],\npower consumption detection for production equipment Wang et al. [2022b], and weather forecasting Schultz et al.\n[2021]. Traditional end-to-end deep learning models have made significant progress in time series analysis Zhou et al.\n[2023], Wang et al. [2022a], Zhou et al. [2022b], Seyfi et al. [2022], Li et al. [2022], Jeon et al. [2022], with many\npopular deep neural network architectures being applied to time series modeling, such as Linear-based Zeng et al. [2023],\nYi et al. [2023b], CNN-based Wu et al. [2023], Wang et al. [2023], RNN-based Shi et al. [2015], Transformer-based Nie\net al. [2023], Zhou et al. [2022a, 2021], Liu et al. [2022], and GNN-based models Wu et al. [2020]. In addition,\nwork on signal processing before the backbone based on the numerical characterization of the series has driven the\ndevelopment of time series forecasting, such as Seasonal-Trend Decomposition Zhou et al. [2022a], Wu et al. [2021],\nWang et al. [2023], which improves the efficiency of backbone in representation extraction by capturing the complex\npatterns from original series in advance. Recently, we have witnessed the remarkable success of pre-trained foundation"}, {"title": "Related Work", "content": "We introduce the related works in terms of LLM for TS, attempts for the unified pre-trained Model, tokenization strategy\nin TS, and spectrum analysis in TS."}, {"title": "Cross Domain Migration in TS", "content": "Deep neural networks trained on one domain can be poor at generalizing to another domain due to the issue of domain\nshift, that is, domain shift problem Wang et al. [2020c], Zhao et al. [2020], Zhang et al. [2020], Oza et al. [2021].\nDomain adaptation (DA) methods attempt to mitigate the harmful effect of domain shift by aligning features extracted\nacross source and target domains Hong et al. [2020], Wang et al. [2020b]. Existing approaches mainly focus on\nclassification tasks, where a classifier learns a mapping from a learned domain-invariant latent space to a fixed label\nspace using source data. Consequently, the classifier depends only on common features across domains, and can be\napplied to the target domain Wilson and Cook [2018]. Unlike DA, This paper is devoted to design a novel framework to\nestablish cross-domain connectivity between different TS datasets, enabling the knowledge learned by the backbone\nnetwork from different complex sequence patterns to be migrated to multiple downstream tasks, which is known as\nUnsupervised Cross-domain Migration (UCM).\nRecently, lots of research efforts Oza et al. [2021], Vibashan et al. [2021] are devoted on unsupervised domain adaptation.\nThis task aims to transfer knowledge learned from a labeled source domain to a different unlabeled target domain, and\nmost approaches focus on aligning distributions of source and target domain and learning domain-invariant feature\nrepresentations. There are mainly two levels for UDA methods: domain-level Bousmalis et al. [2017], Tzeng et al.\n[2017] and category-level Kang et al. [2019], Du et al. [2021], Li et al. [2021]. Domain-level UDA mitigates the\ndistribution divergence between the source and target domain by pulling them into the same distribution at different\nscale levels. Besides, some works Du et al. [2021], Li et al. [2021] focus on the fine-grained category-level label\ndistribution alignment through an adversarial manner between the feature extractor and two domain-specific classifiers.\nFinally, One prevailing paradigm aims at minimizing statistical distribution measures to mitigate the distribution shift\nproblem between the source and target domains Chen et al. [2019], Wang et al. [2020a], Wu and Ng [2022].\nIn light of successes in related fields, domain adaptation techniques have been introduced to time series tasks Jin\net al. [2022], Ragab et al. [2022], He et al. [2023], Cai et al. [2021], Ragab et al. [2021]. To generate accurate input\npairs, CDTrans Xu et al. [2022] designs a two-way center-aware labeling algorithm to produce pseudo labels for target\nsamples. Along with the pseudo labels, a weight-sharing triple-branch transformer framework is proposed to apply\nself-attention and cross-attention for source/target feature learning and source-target domain alignment, respectively.\nRecent approach Jin et al. [2022] proposes a shared-attention model with domain-adaptive capabilities that predicts\nfuture sequences using local representations over different time periods by extracting domain-invariant features and\nmodeling domain-relevant attributes in conjunction with domain-specific features in order to appropriately approximate\nthe data distribution of the respective domain. AdaTime Ragab et al. [2022] develops a bench marking evaluation\nsuite to systematically and fairly evaluate different domain adaptation methods on time series data. Specifically, we\nstandardize the backbone neural network architectures and bench marking datasets, while also exploring more realistic\nmodel selection approaches that can work with no labeled data or just a few labeled samples. RainCoat He et al.\n[2023] as a model for analyzing both closed-set and generalized-set data for complex time series, addresses feature\nand label shifts by considering both temporal and frequency features, aligning them across domains, and correcting\nfor misalignments to facilitate the detection of private labels. Existing model Cai et al. [2021] designs intra- and\ninter-variable sparse attention mechanisms to extract correlation-structured time-series data considering time lags and\nutilize correlation-structured alignment to guide the transfer of knowledge from the source domain to the target domain.\nSLARDA Ragab et al. [2021] designs a self-supervised learning module that utilizes forecasting as an auxiliary task to\nimprove the transferability of the source features, and proposes a novel autoregressive domain adaptation technique"}, {"title": "Tokenization strategy in TS", "content": "Motivated by the successful application of transformers in NLP Vaswani et al. [2017] and pre-trained foundation models\nRadford and Narasimhan [2018], Brown et al. [2020], Touvron et al. [2023a,b], transformer-based models are viewed as\nequally promising in time series. Due to the characteristics of time series, initial approaches Godfried et al. [2020],\nKitaev et al. [2020a], Zhou et al. [2021], Liu et al. [2021], Wu et al. [2021], Woo et al. [2022b], Zhou et al. [2022a]\ngenerally followed the Point as Token design, that is each sampled time point acts as a separate token. There are two\nlimitations of this strategy: firstly, computing global dependencies between any time points leads to high computational\ncomplexity; secondly, there is a serious information redundancy in the global dependencies captured by the attention\nmechanism, considering that the time series is continuously varying, which leads to two neighboring tokens having\nnearly the same numerical distribution. Therefore, approaches at the initial stage of time series analysis have focused\non reducing computational complexity by mitigating information redundancy.\nFor instance, Reformer Kitaev et al. [2020b] designed the locally sensitive hashing self-attention to reduce computational\ncomplexity. Informer Zhou et al. [2021] proposes the ProbSparse self-attention mechanism to efficiently replace the\ncanonical self-attention. Pyraformer Liu et al. [2021] introduces the pyramidal attention module which reduces\ncomputational complexity by constraining the maximum length of the signal traversing paths. Autoformer Wu et al.\n[2021] designs the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies\ndiscovery and representation aggregation at the sub-series level. ETSformer Woo et al. [2022b] proposes the novel\nexponential smoothing attention and frequency attention to replace the self-attention mechanism in vanilla Transformers,\nthus improving both accuracy and efficiency. FEDformer Zhou et al. [2022a] avoids the high overhead of computation\non the time domain by calculating the dependencies between individual bands in the frequency domain, while Fourier\nTransform has been used to ensure that individual bands have a global view.\nSubsequently, the representative PatchTST Nie et al. [2023] proposed a Patch as Token strategy and channel-independent\ndesign to demonstrate the effectiveness of the transformer in time series. Based on this, the recent iTransformer Liu\net al. [2023b] proposes the Series as Token strategy, that is the whole series is regarded as the token, and the attention\nmechanism is used to capture the dependencies between the sequences of different channels, which is surprisingly\nintuitive and effective in data domains with a very large number of channels and complex dependency relationships\nbetween them. The above studies have shown that the reasonable tokenization strategy can drastically improve\nthe performance of Transformer-based models compared to the complex and tedious model structure improvement.\nHowever, the existing tokenization strategy cannot be well applied to cross-domain migration. Since TS datasets from\ndifferent domains and sampling settings exhibit diverse periodic patterns, it is difficult to identify a unified sub-series\nspan that matches all TS data domains. existing tokenization strategies are unable to establish a unified framework\nto adapt to the potential TS data domains, and the pre-training phase is unable to adequately learn cross-domain\nrepresentational information, which limits the generalization ability and scalability of the model. Therefore, The ideal\ntokenization should be insensitive to the mathematical characteristics of different data domains, which motivates us to\npropose Wave as Token as the general compatible cross-domain migration strategy."}, {"title": "LLM for TS", "content": "The adaptation of pre-trained LLMS for time series analysis has attracted attention, exploiting their superior capabilities\nin sequence representation learning. OneFitsAll Zhou et al. [2023] first attempted to apply the pre-trained GPT2\nRadford et al. [2019] to the time series downstream tasks, and achieved comparable performance to the state-of-the-art\nmethods by freezing the pre-trained self-attention and feed forward structures to maximize the retention of pre-training\ninformation, and only fine-tuning the layer norm. In contrast, the recent TimeLLM Jin et al. [2023] completely freezes\nthe model parameters of the pre-trained Llama Touvron et al. [2023a], aligning both time series and natural language\nmodalities by introducing textual prototypes. The task settings and dataset priori information are fed into the pre-trained\nLlama as hard-prompt by the pre-trained Embedder, thus fully activating the inference capability of the foundation\nmodel on the time series forecasting task. TEMPO Cao et al. [2023] designs a prompt pool based on seasonal-trend\ndecomposition to generate specific prompts for each sub-component, and incorporates LoRA Hu et al. [2022] to achieve\nefficient fine-tuning of LLM. TEST Sun et al. [2023] establishes a TS embedding method applicable to LLMs by using"}, {"title": "Spectrum Analysis in TS", "content": "Over the past two decades, spectrum analysis techniques based on Fast Fourier Transform (FFT) and Wavelet Transform\n(WT) has been widely used in diverse model structures for time series analysis Finder et al. [2022], Fang et al.\n[2023], Zhang et al. [2023], Guo et al. [2023] to improve the performance of learning directly from the time domain.\nSpecifically, transformer-based models use spectral analysis to reduce the complexity of self-attention mechanisms, e.g.,\nAutoformer Wu et al. [2021] captures the periodic pattern information from original series based on FFT and establishes\nthe Auto-Correlation mechanism at the sub-series level for learning dependencies and representation aggregation.\nFEDformer Zhou et al. [2022a] generates a set of mixed frequency components by Fourier analysis and designs a\nfrequency enhanced attention mechanism, which exploited the sparse representation of the spectrogram and achieved\nlinear complexity. Besides, the MLP-based FreTS Yi et al. [2023b] captures a complete global view of the original\nsignal by operating on the spectral components obtained by frequency-transformation, and overcomes the information\nbottleneck of MLP on time series by ensuring that MLP focuses on the key part frequency components. In addition,\nthe FourierGNN Yi et al. [2023a] proposes a novel architecture that uniformly captures inter-series (spatial) dynamics\nand intra-series (temporal) dependencies by performing matrix multiplication in Fourier space. Finally, researchers in\nrepresentation learning have noted the significance of establishing consistency constraints in the temporal-frequency\nspace. For example, TF-C Zhang et al. [2022b] designs a self-supervised pre-training strategy based on time-frequency\nconsistency, that is temporal- and frequency- representations learned from the same TS samples should be closer in\nthe temporal-frequency space than different TS samples. In addition, CoST Woo et al. [2022a] establishes a more\neffective connection by mapping the embedded features in the time domain space to the frequency domain. Based on\nthe advantages of descriptive spectrum analysis, this paper designs the Wave Quantize tokenization strategy."}, {"title": "Methodology", "content": "Establishing the generic and compatible cross-domain migration framework between different time series domains\nhas the following challenges: (1) time series data with diverse domain background knowledge tend to exhibit different\ncharacteristics; (2) Datasets from different sources may exhibit variations, even in the same domain background; (3)\nThe same data instances can also face the challenge of cross-domain migration due to a priori features such as sampling\nrate.\nThe proposed Wave Quantize Module solves challenge-1 through the designed common spectral latent space. In\naddition, the designed strategy ensures that arbitrary series data can be embedded to equal-length groups of tokens\n(number of tokens equal to series length), thus solving challenge-2. This ensures that no hyperparameter setting tricks\nare utilized in the model to adapt to potential data domains, even if these domains are completely different from each\nother in terms of structural features (e.g., channel number, sequence length) are completely different. Finally, we\nsolve challenge-3 by ensuring that each token contains TS pattern information within a localized window through the\nfinite-length basis functions and bridges the difference in the distribution of pattern information from different domains\nwithin the A-dimensional space formed by basis functions, where X is the size of the proposed wavebook.\nSpecifically, the basic idea is illustrated in Fig. 1. Motivated by VQVAE van den Oord et al. [2017], we design a\nfine-grained tokenization strategy to standardize the original series before the backbone, project input series from\ndiverse domains into a common latent space, and generate a set of tokens. Subsequently, the TS pattern knowledge of\ndifferent domains will be learned directly by backbone from the common spectral latent space, and used for multiple\ndownstream tasks. Specifically, a set of orthogonal basis functions is designed to form the latent space, where each\nbasis function ${A_i|1 \\leq i \\leq \\lambda}$ is a finite-length waveform with attenuation, thus ensuring that the energies of the basis\nfunctions are confined to a local window. We define this set of orthogonal basis functions as the \"Wavebook\" in Fig. 11.\nWe perform the inner product operation between the original series and basis function by continuously sliding the\nwindow with the step equal to 1, thus calculating the fluctuation pattern similarity between the basis function and each\nsegment of the input series in the local window. Repeating for all A basis functions so that each segment of the local"}, {"title": "Framework Overview", "content": "We propose a framework called WQ4TS to build a cross-domain migration time series general model, which is illustrated\nin Fig. 1. It consists of four parts: wave quantize module, time series tokenization, cross-domain pre-training, and\nfine-tuning for domain migration. Wave quantize module construction extracts a set of orthogonal basis functions from\nmultiple TS domains to form the common spectral latent space. Based on the wavebook, a tokenization mechanism\nprojects the input time series from diverse domains into tokens in the common spectral latent space to mitigate their\nheterogeneity. Subsequently, in the cross-domain pre-training phase, the comprehensive time series pattern knowledge\nof different domains is extracted from the tokens to train a backbone Transformer encoder to form a time series unified\nframework.\nTo guarantee that the model can simultaneously learn latent representations from diverse TS data domains with different\nstatistical features and temporal pattern, we adopt two promising designs: 1) Encoder-only design. Since TS data\noften exhibit the complexity of multiple patterns superimposed Wang et al. [2023], Cao et al. [2023], we adopt an\nencoder-only design with excellent generalization capabilities for representation learning, which has been proven\ncompetent by the SOTA Transformer-based models Nie et al. [2023], Liu et al. [2023b]. 2) channel independent. The\nmultivariate time series sample with n channels was regarded as n separate univariate series, which is utilized as the\nexemplary case to simplify the methodology and was shown effective in the literature Zhou et al. [2023], Jin et al.\n[2023]. Besides, the specific description of the important symbols involved in the method is shown in Table 2.\nThe overview of the proposed WQ4TS architecture is illustrated in Fig. 1. The time series from multiple TS domains\nis represented as $X = (x_1,...,x_\\lambda) \\in \\mathbb{R}^\\lambda$ with l time steps, and WQ4TS is utlized to predict future series $Y =\n(x_{l+1},...,x_{l+c}) \\in \\mathbb{R}^c$ with c time steps. The common space $V^\\lambda$ will be formed by the proposed A-dimensional Wave\nQuantize Module, which comprises a set of orthogonal basis functions. Subsequently, in Time Series Tokenization,\naccording to proposition 2, any sub-series from the original series can be transformed into a group of coordinates in $V^\\lambda$,\nand the bijection relation is satisfied between the sub-series and the coordinates. Furthermore, proposition 3 gives the\nsufficient-necessary condition and construction method for orthogonal wavelet bases. In Cross-domain Pre-training\nphase, the sub-series at the timestep-j are converted to $token_j = (P_{1,j}, P_{2,j},...,P_{\\lambda,j}) \\in \\mathbb{R}^\\lambda$, from diverse TS domains.\nSubsequently, simple full-parameter Fine-tuning for Domain migration is designed since the common spectral latent\nspace reduces the distance between the source and target domains."}, {"title": "Wave Quantize Module", "content": "This section presents the theoretical foundations of the wave quantize module, which is the fine-grained tokenization\nstrategy that makes the establishment of the general compatible cross-domain migration framework possible. Among\nit, proposition 2 theoretically proves the bijective projection relation established by the wavebook, and proposition 3\nproposes the sufficient-necessary condition for the construction of the wavebook. According to these propositions, we\nconstruct the wavebook and eventually form the common spectral latent space. For all propositions described in the\ncurrent section, the thorough proof procedure and the background of the wavebook will be provided.\nWe introduce the following notations: $F(t)$ is the specific basis function, $H(w)$ and $G(w)$ refer as the low-pass filter\nand band-pass filter of $F(t)$, respectively.\nDefinition 1. We define the wavebook as a set consisting of several basis functions, as ${F_{j,k}(t) = 2^{j/2}F(2^jt -\nk), (j, k) \\in \\mathbb{Z}^2}$, which constitutes a set of standard orthonormal basis (O.N.B) and forms the finite series space $L^2(\\mathbb{R})$\nif and only if $F(t)$ is the orthogonal wavelet.\nDefinition 2. Based on the proposed wavebook, there must exist the unique coefficients sequence ${c_{j,k}; (j, k) \\in \\mathbb{Z}^2} e\nl^2(\\mathbb{Z})$. For $\\forall f(t) \\in L^2(\\mathbb{R})$, we have $f(t) = \\sum_{j \\in \\mathbb{Z}}\\sum_{k \\in \\mathbb{Z}}C_{j,k} \\cdot F_{j,k}(t)$, where $f(t)$ denotes the univariate time series\nand ${c_{j,k}}$ contains the coordinates of $f(t)$ in space $\\mathbb{Z}^2$. Specifically, the bijection is satisfied between variables $L^2(\\mathbb{R})$\nand $12(\\mathbb{Z})$ in two spaces $f(t)$ and ${C_{j,k}}$."}, {"title": "Framework Overview", "content": "We propose a framework called WQ4TS to build a cross-domain migration time series general model, which is illustrated\nin Fig. 1. It consists of four parts: wave quantize module, time series tokenization, cross-domain pre-training, and\nfine-tuning for domain migration. Wave quantize module construction extracts a set of orthogonal basis functions from\nmultiple TS domains to form the common spectral latent space. Based on the wavebook, a tokenization mechanism\nprojects the input time series from diverse domains into tokens in the common spectral latent space to mitigate their\nheterogeneity. Subsequently, in the cross-domain pre-training phase, the comprehensive time series pattern knowledge\nof different domains is extracted from the tokens to train a backbone Transformer encoder to form a time series unified\nframework.\nTo guarantee that the model can simultaneously learn latent representations from diverse TS data domains with different\nstatistical features and temporal pattern, we adopt two promising designs: 1) Encoder-only design. Since TS data\noften exhibit the complexity of multiple patterns superimposed Wang et al. [2023], Cao et al. [2023], we adopt an\nencoder-only design with excellent generalization capabilities for representation learning, which has been proven\ncompetent by the SOTA Transformer-based models Nie et al. [2023], Liu et al. [2023b]. 2) channel independent. The\nmultivariate time series sample with n channels was regarded as n separate univariate series, which is utilized as the\nexemplary case to simplify the methodology and was shown effective in the literature Zhou et al. [2023], Jin et al.\n[2023]. Besides, the specific description of the important symbols involved in the method is shown in Table 2.\nThe overview of the proposed WQ4TS architecture is illustrated in Fig. 1. The time series from multiple TS domains\nis represented as $X = (x_1,...,x_\\lambda) \\in \\mathbb{R}^\\lambda$ with l time steps, and WQ4TS is utlized to predict future series $Y =\n(x_{l+1},...,x_{l+c}) \\in \\mathbb{R}^c$ with c time steps. The common space $V^\\lambda$ will be formed by the proposed A-dimensional Wave\nQuantize Module, which comprises a set of orthogonal basis functions. Subsequently, in Time Series Tokenization,\naccording to proposition 2, any sub-series from the original series can be transformed into a group of coordinates in $V^\\lambda$,\nand the bijection relation is satisfied between the sub-series and the coordinates. Furthermore, proposition 3 gives the\nsufficient-necessary condition and construction method for orthogonal wavelet bases. In Cross-domain Pre-training\nphase, the sub-series at the timestep-j are converted to $token_j = (P_{1,j}, P_{2,j},...,P_{\\lambda,j}) \\in \\mathbb{R}^\\lambda$, from diverse TS domains.\nSubsequently, simple full-parameter Fine-tuning for Domain migration is designed since the common spectral latent\nspace reduces the distance between the source and target domains."}, {"title": "Wave Quantize Module", "content": "This section presents the theoretical foundations of the wave quantize module, which is the fine-grained tokenization\nstrategy that makes the establishment of the general compatible cross-domain migration framework possible. Among\nit, proposition 2 theoretically proves the bijective projection relation established by the wavebook, and proposition 3\nproposes the sufficient-necessary condition for the construction of the wavebook. According to these propositions, we\nconstruct the wavebook and eventually form the common spectral latent space. For all propositions described in the\ncurrent section, the thorough proof procedure and the background of the wavebook will be provided.\nWe introduce the following notations: $F(t)$ is the specific basis function, $H(w)$ and $G(w)$ refer as the low-pass filter\nand band-pass filter of $F(t)$, respectively.\nDefinition 1. We define the wavebook as a set consisting of several basis functions, as ${F_{j,k}(t) = 2^{j/2}F(2^jt -\nk), (j, k) \\in \\mathbb{Z}^2}$, which constitutes a set of standard orthonormal basis (O.N.B) and forms the finite series space $L^2(\\mathbb{R})$\nif and only if $F(t)$ is the orthogonal wavelet.\nDefinition 2. Based on the proposed wavebook, there must exist the unique coefficients sequence ${c_{j,k}; (j, k) \\in \\mathbb{Z}^2} e\nl^2(\\mathbb{Z})$. For $\\forall f(t) \\in L^2(\\mathbb{R})$, we have $f(t) = \\sum_{j \\in \\mathbb{Z}}\\sum_{k \\in \\mathbb{Z}}C_{j,k} \\cdot F_{j,k}(t)$, where $f(t)$ denotes the univariate time series\nand ${c_{j,k}}$ contains the coordinates of $f(t)$ in space $\\mathbb{Z}^2$. Specifically, the bijection is satisfied between variables $L^2(\\mathbb{R})$\nand $12(\\mathbb{Z})$ in two spaces $f(t)$ and ${C_{j,k}}$."}, {"title": "Time Series Tokenization", "content": "Tokenizing multiple time series domains and establishing their cross-domain connectivities is a challenging task. We\npropose a time series tokenization approach to map the input time series from diverse domains into tokens in the\ncommon spectral latent space following the \u201cwave as token\u201d principle, which is described as follows.\nBased on proposition 3, the amplitude sequence $A \\in \\mathbb{R}^{2m}$ is utilized to discretely inscribe the orthogonal wavelet.\nSpecifically, A defines the amplitude values of $2m$ points that contain information about the waveforms of the specified\northogonal wavelet function, where m serves as the precision of the discrete sequence describing the information in A,\nand the spacing between any two neighboring points is denoted as $step$ which equals to $m/(2^m-1)$\nWe denote the central frequency of the orthogonal wavelet as $f_c$ and the size of the wavebook as $\\lambda$. The set of scale\nfactors is calculated by:\n$S = {S_i = (2\\cdot f_c \\cdot \\lambda)/i, i \\in [1, 2, ..., \\lambda]}$,\nwhere $(m \\cdot S_i) \\in \\mathbb{Z}$. For each scale factor $S_i$, the downsampling coordinate $W_i$ and basis function $A_i \\in \\mathbb{R}^{m \\cdot S_i}$ in\nwavebook can be sampled by:\n$W_i = {W_{i,j} = \\frac{(2^m - 1) \\cdot j}{mS_i} , j \\in [1, ..., m \\cdot s_i]}$,\n$A_i = A[W_i] = A[W_{i,1},..., W_{i,m \\cdot s_i}] \\in \\mathbb{R}^{m \\cdot s_i}$"}, {"title": "Cross-domain Pre-training", "content": "We describe the fine-grained cross-domain pre-training strategy as follows. In each epoch, variables from multiple\ndomains are randomly shuffled with the batch to ensure that the unified general cross-domain model can simultaneously\nexhibit satisfactory generalization ability across diverse time series domains. Specifically, the backbone encoder accepts\nthe tokens from diverse time series domains as input and learns the dependencies between segments via the attention\nmechanism. Besides, each token vector serves as the fluctuation pattern similarity, thus containing sufficient semantic\ninformation. Notably, since all tokens come from the unified common embedding space, it alleviates the heterogeneity\nof TS multi-domains. These advantages ensure that the proposed model is advantageous in multi-domain pre-training\nphase.\nTo fully stimulate the inference capability of WQ4TS in downstream tasks, we design forecasting and classification\nas multi-tasks for different downstream tasks. Besides, in the cross-domain pre-training phase, a set of adaptive\ndynamic weights was designed to balance the gradients from different time series domains, considering differences in\ngeneralization across time series domains and the dataset size. Specifically, we define the aggregate loss function as\n$Loss = \\sum_{i \\in \\mathbb{Z}}(a_i \\cdot Loss_i)$, where $Loss_i$ is defined as the loss value of specific domain\u017c, and multi-task weightings\n${a_i; i \\in \\mathbb{Z}}$ are automatically calculated by the learnable strategy Kendall et al. [2018] which considering the\nhomoscedastic uncertainty of each task."}, {"title": "Cross-domain Migration", "content": "To validate the domain migration capability, WQ4TS is pre-trained in multiple TS source domains, it contains two\nnatural advantages: (1) the knowledge learned by the model in the pre-training phase has the same modality information"}, {"title": "Architecture of WQ4TS", "content": "The architecture of WQ4TS consists of a Tokenization operation (Section. 3.3)", "follows": "n$T^0 = Tokenization(X)$", "W^V": "are\nserved as query", "components,\nFlatten": "mathbb{R"}, {"Linear": "mathbb{R}^{\\lambda l} \\rightarrow \\mathbb{R}^C$, where the output of the Output"}]}