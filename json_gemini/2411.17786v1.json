{"title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation via Feature Caching", "authors": ["Emanuele Aiello", "Umberto Michieli", "Diego Valsesia", "Mete Ozay", "Enrico Magli"], "abstract": "Personalized image generation requires text-to-image generative models that capture the core features of a reference subject to allow for controlled generation across different contexts. Existing methods face challenges due to complex training requirements, high inference costs, limited flexibility, or a combination of these issues. In this paper, we introduce DreamCache, a scalable approach for efficient and high-quality personalized image generation. By caching a small number of reference image features from a subset of layers and a single timestep of the pretrained diffusion denoiser, DreamCache enables dynamic modulation of the generated image features through lightweight, trained conditioning adapters. DreamCache achieves state-of-the-art image and text alignment, utilizing an order of magnitude fewer extra parameters, and is both more computationally effective and versatile than existing models.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in text-to-image generation, fueled by the development of diffusion models [11, 31], have enabled high-quality and diverse image generation from textual descriptions. Diffusion models [26, 28] gradually transform random noise into images through a sequence of denoising steps, conditioned on the input text prompt.\nAn active area of research is personalizing these models, enabling the generation of novel images of a reference subject in various contexts, while maintaining flexibility for text-based editing. Early personalization techniques [1, 7, 9, 27, 33, 35, 38, 38], such as the seminal DreamBooth [27] relied on fine-tuning (FT) the generative model for each reference subject. However, these approaches are often impractical for many use cases due to costly test-time FT, which can take several minutes per subject. To address this, FT-free (i.e., zero-shot) personalized image generation methods have emerged to eliminate test-time optimization. These FT-free approaches can be broadly categorized into two families: encoder-based methods and reference-based methods, each with distinct drawbacks."}, {"title": "2. Background and Related Work", "content": "Personalized image generation aims at generating images containing a specific subject. This task has been widely studied, resulting in two main approaches: fine-tuning methods, which require test-time finetuning on multiple subject reference images, and finetuning-free (zero-shot) methods, which learn a generalizable conditioning mechanism to generate reference subjects without the need for further optimization.\nFinetuning-based Personalization DreamBooth [27] finetunes the entire U-Net with reference images while introducing a regularization loss to mitigate overfitting. On the other hand, Custom Diffusion [13] only finetunes the K and V projections for the cross-attention blocks of the U-Net. Text-based personalization methods optimize single (like"}, {"title": "3. Method", "content": "Given a pretrained text-to-image generative model ee and an image containing a reference subject Iref, the goal of personalized sampling is to generate novel images containing the reference subject in various contexts while maintaining textual control. We propose DreamCache, a novel approach for extracting conditioning signals from Iref and guiding of the image generation process. This method leverages a pretrained diffusion model, conditioning adapters that are pretrained with a synthetic dataset, and a feature cache from the reference image. Sample outputs generated by DreamCache are shown in Fig. 2, with a method overview in Fig. 3.\nAt the core of DreamCache, we utilize the denoiser in the pretrained diffusion model to extract multi-resolution features from Iref by caching the activations of a few selected layers. To improve generalization, we cache features using a forward pass with a null text prompt. When personalized sampling is performed, the cached features are processed by adapters to act as conditioning signals, modulating the denoiser features of the image under generation at corresponding layers. These adapters, once pretrained on a synthetic dataset, enable zero-shot personalized generation with any new reference image, requiring no further finetuning once its features are cached.\nIn the following, we detail the three main aspects of DreamCache, namely i) how to cache reference features (Sec. 3.1); ii) how to condition the diffusion model on the cached features for personalized sampling (Sec. 3.2); iii) how to train the adapters used for model conditioning (Sec. 3.3)."}, {"title": "3.1. Caching Reference Features", "content": "To extract information from the reference image for personalized sampling, we perform a forward pass through the denoiser of the diffusion model at a single timestep. We select t = 1, the least noisy timestep, to obtain clean features that are optimal for conditioning the personalized generation process. Additionally, we remove the text conditioning to decouple visual content of the reference image from the text caption, thus also eliminating the need for user-provided captions for reference images. This contrasts with methods such as JeDi [40], which are sensitive to caption content.\nDuring the forward pass, activations are computed for all layers of the denoiser, but only a subset is cached. Based on our experiments with the Stable Diffusion U-Net, we"}, {"title": "3.2. Conditioning on Cached Reference Features", "content": "We propose a novel conditioning adapter mechanism composed of: i) a cross-attention block between the cached features and the features of the image under generation; ii) a concatenation operation between the features from the self-attention block of the original U-Net denoising backbone and those of the cross-attention block; and iii) a projection layer. A block diagram is shown in Fig. 3 (right).\nOmitting the layer subscript for clarity, the conditioning adapter mechanism is expressed mathematically as follows:\nq = W_{h, k}c = W_K h_{ref}, V_c = W_V h_{ref},\n$$a_c = \\text{softmax}\\left(\\frac{q k^T}{\\sqrt{d}}\\right)$$\na = W_{proj} ([a; a_c]),\nwhere h \u2208 \u211d^{N\u00d7d} are the current d-dimensional features of the N-pixel image under generation, and href are the cached reference features from Eq. (2). WQ, WK, WV, and Wproj are learnable projection matrices, whose training is described in Sec. 3.3. The concatenation [a; ac] \u2208 \u211d^{N\u00d72d} combines the output of self-attention a \u2208 \u211d^{N\u00d7d} and cross-attention ac \u2208 \u211d^{N\u00d7d}. The concatenation operation allows a flexible information fusion without explicit alignment constraints, compared to other approaches in similar works (see Sec. 4.3). The learnable projection matrix Wproj reduces the dimensionality of the concatenated features back to \u211d^{N\u00d7d} to maintain compatibility with the original backbone.\nOverall, the approach proposed for the adapter enriches feature representations used in the diffusion process of the image under generation by allowing the model to leverage both primary and conditioning-based contextual information from the cache."}, {"title": "3.3. Training the Conditioning Adapters", "content": "The additional parameters introduced in Sec. 3.2 to process the cached features must be trained on a large and varied dataset to ensure they generalize for any reference subject. Collecting paired data for this training process would be prohibitively expensive, as it requires multiple images of the same subject in different contexts. To address this, we draw inspiration from the recently proposed synthetic data generation pipeline in BootPIG [23] to construct our training data. First, we utilize a large language model (Llama 3.2 [6]) to generate captions for potential target images. Each caption is used to generate an image via Stable Diffusion [26]. We then use the Segment Anything Model (SAM) [12] and Grounding DINO [15] to accurately segment the reference subject based on the text caption and generate a foreground mask of the main object in the caption.\nWe treat the Stable Diffusion-generated image as the target image, the foreground object pasted on a white background as the reference image, and the LLM-generated caption as the textual prompt during our training pipeline. Compared to BootPIG, our pipeline employs open-source models, making it more accessible. We will release our synthetic dataset to facilitate reproducibility and further research, since similar datasets, including BootPIG's [23], have not been publicly released. Additional details on the dataset and its statistics can be found in the Supp. Mat.\nWe train the newly introduced adapters' parameters (WQ, WK, WV, and Wproj) with the standard score matching loss [32] using both the text-conditioned noisy input and the cached reference features:\n$$L_{diffusion} = E_{x_0,\\epsilon,c_T, I_{ref},t} [||\\epsilon - \\epsilon_{\\theta}(x_t, c_T, H_{FC}, t)||^2]$$\nwhere x0 is the target image, cr is the text prompt generated by the large language model, \u03f5 is Gaussian noise, and t is the diffusion timestep sampled uniformly from 1,..., T. The noisy image at timestep t, xt, is obtained by gradually adding noise to x0 during the forward diffusion process. The function \u03f5\u03b8 represents the modified denoising model that incorporates the conditioning adapters."}, {"title": "4. Experimental Results", "content": "In this section, we present our experimental results, including quantitative and qualitative comparisons, an ablation study, and an analysis section that visualizes the behavior of the newly introduced cross-attention mechanism in the adapters.\nImplementation Details We evaluate our method on two versions of Stable Diffusion (SD) [26], specifically versions 1.5 and 2.1, to ensure fair comparison with state-of-the-art methods across different backbones. As described in the ablation study, our caching and conditioning mechanism is applied to the middle layer and every second layer of"}, {"title": "4.1. Zero-Shot Personalization", "content": "We compare DreamCache against state-of-the-art methods for finetuning-based and zero-shot personalization."}, {"title": "4.2. Inference Time Evaluation", "content": "The computational efficiency of our method is compared to the reference-based method BootPIG [23] and encoder-"}, {"title": "4.3. Ablation Studies", "content": "We validate our design choices through a series of studies, examining different conditioning mechanisms, evaluating our feature caching approach, and analyzing the impact of synthetic dataset scaling."}, {"title": "4.4. Visualizing Reference Impact", "content": "Finally, we analyze how the cross-attention mechanism in DreamCache impacts image generation by visualizing cached reference feature influence. Attention map visualizations at different resolutions are provided in Fig. 5. Specifically, attention maps between the query from the current generation and the key derived from reference features reveal a highly localized focus on the subject, without interference from background elements. This mechanism models correspondences effectively, integrating reference information into the generated image."}, {"title": "5. Discussion and Conclusions", "content": "In this paper, we proposed DreamCache, a novel approach to personalized text-to-image generation that uses feature caching to overcome the limitations of existing methods. By caching reference features from a small subset of layers of the U-Net only once, our method significantly reduces both computational and memory demands, enabling efficient, real-time personalized image generation. Unlike previous approaches, DreamCache avoids the need for costly finetuning, external image encoders, or parallel reference processing, making it lightweight and suitable for plug-and-play deployment. Our experiments demonstrate that DreamCache achieves state-of-the-art zero-shot personalization with only 25M additional parameters and a fast training process. While DreamCache is a promising direction towards more efficient personalized generation, it has some limitations. Although effective for single-subject personalization, our approach may require adaptation for complex multi-subject generation where feature interference can occur. Additionally, certain edge cases, such as highly abstract or stylistic images, may challenge the caching mechanism's capacity to accurately preserve subject details. To address these challenges, future work may explore adaptive caching techniques or multi-reference feature integration."}]}