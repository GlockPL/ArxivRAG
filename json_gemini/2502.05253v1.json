{"title": "LLMs Can Teach Themselves to Better Predict the Future", "authors": ["Benjamin Turtel", "Danny Franklin", "Philipp Schoenegger"], "abstract": "We present an outcome-driven fine-tuning framework that enhances the forecasting capabilities of large language models (LLMs) without relying on human-curated reasoning samples. Our method leverages model self-play to generate pairs of diverse reasoning trajectories and probabilistic forecasts for a set of diverse questions that resolve after the models' knowledge cutoff date. We then rank pairs of these reasoning traces by their distance to the actual outcomes before fine-tuning the model via Direct Preference Optimization (DPO). On a separate test set, our approach increases prediction accuracy of Phi-4 14B and DeepSeek-R1 14B by between 7-10% over a base model and a DPO fine-tuned control model with randomized labels, bringing them on par with forecasting capabilities of much larger frontier models like GPT-40.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated remarkable capabilities in a wide range of areas, often approaching or exceeding human performance. One area where human performance has not yet been surpassed is judgemental forecasting [1], where probabilistic forecasts are assigned to future events. Successful forecasts by top-performing human forecasters include substantial reasoning about facts of the world, various trends, and competing pieces of evidence [2], making it a great place to study model reasoning capabilities in a messy real-world environment.\nMoreover, forecasting is a central task in decision-making across sectors as diverse as finance, policy, and law. It is central to inform resource allocation, manage risks, and plan organizational decisions. Modern LLMs have already been shown to conduct financial analysis [3], evaluate the impact of events on time series [4], and improve climate policy decision-making [5]. This makes improving LLMs' forecasting abilities potentially impactful and wide-ranging.\nThere has been some work explicitly looking to apply and boost the forecasting capabilities of LLMs. Such work has relied on aggregation [6], retrieval of news as well as fine-tuning [7], and ranked-based context retrieval [8], among other approaches [9]. While most of these systems improve performance to varying degrees, many share a common methodological limitation: They are frequently reliant on human-curated data such as up-to-date crowd forecasts or output curation, and often fail to have the models learn from resolved outcomes. Human outputs are slow and costly to procure, making it difficult to have models continually learn from them and improve.\nIn this paper, we propose a new approach to improving LLM forecasting performance that sidesteps the use of human inputs above and beyond real-world resolutions and enables the model to directly learn from actual outcomes and self-play. Self-play, where models compete against themselves, has previously been used in AlphaGo Zero to achieve superhuman performance [10], as well as more recent fine-tuning approaches like Self-Play fine-tuNing (SPIN) [11]. By allowing the model to produce reasonings and forecasts by itself on a large number of forecasting questions, this provides us with a large data set that we can then use for further training. As such, we do not rely on human-written forecasting rationales or predictions and instead only use model-generated reasoning, making this straightforwardly scalable.\nFurther, our approach uses Direct Preference Optimisation (DPO) [12], a reward-free method entirely bypassing the need for a separate reward model, to instead learn a reward signal from sets of ranked reasoning pairs [13] drawn from the self-play outputs. This allows DPO to capitalize on relative rankings between forecasts, enabling the model to learn from the entire set of generated samples without the need for manual curation. Even when forecasts are individually suboptimal, DPO trains the model to discern subtle differences in quality and systematically correct biases through pairwise comparisons. By contrast, Supervised Fine-Tuning (SFT) relies on human-curated examples and treats selected forecasts as fully correct, which can lead to the discarding of potentially valuable information; DPO overcomes this limitation by learning from all samples, thereby enhancing the robustness and efficiency of the fine-tuning process.\nOur work follows up on recent advances made by DeepSeek's release of R1 [14], which demonstrated the power of reinforcement learning in deterministic contexts like mathematics and coding. We move the focus to real-world forecasting, which is inherently noisy and relies on calibrated predictions rather than simple binary correctness. This requires our models to learn from noisy probabilistic outcomes, which, if successful, promises widespread applicability.\nTo achieve this, we draw on a large dataset of resolved prediction market questions from Polymarket, where the model-restricted to a historical cutoff date-generates multiple reasoning traces and probabilistic forecasts through self-play. We then rank these pairs of rationales based on their proximity to the resolved outcome (for instance, ranking a 5% prediction higher than a 10% prediction if the event resolved to \"No\") before fine-tuning our model on them and testing the model on a separate test set. This ensures that the model does not simply learn whether a forecast predicted that an event would or would not occur, but instead enables it to draw directly from the full set of forecasts needed for a well-calibrated forecasting model (see Figure 1).\nOur results on a temporally held-out test set of questions resolving after December 25, 2024 show that"}, {"title": "Method", "content": "Our approach consists of six main steps: 2.1) Collection and preprocessing of forecasting data, 2.2) News collection, 2.3) Synthetic training data generation through base model self-play, 2.4) Resolution-driven re-ranking, 2.5) Direct Preference Optimization (DPO) fine-tuning, and 2.6) Forecasting test-set questions.\nFor this pipeline, we used two models for self-play and for the final forecasting process: Phi-4 [15] and DeepSeek-R1-Distill-Qwen-14B [14]. Both models are small (at 14B parameters) but have shown strong performance on general science and coding benchmarks, sometimes rivalling GPT-40 [17, 18, 14]. DeepSeek-R1-Distill-Qwen-14B is a distilled model derived from Gwen2.5-14B [19] fine-tuned with the reasoning patterns from DeepSeek-R1 [14]. Throughout this paper, we refer to these models as Phi-4 14B and DeepSeek-R1 14B respectively."}, {"title": "Data", "content": "We collected a total of 12,100 binary outcome forecasting questions from the prediction market Polymarket. We excluded all outcomes with ambiguous resolutions and partitioned the data as follows: our training set included 9,800 questions that all resolved between July 1 and December 15, 2024, and our test set included 2,300 questions that all resolved between December 25, 2024 and January 23, 2025. We also collected the final outcomes for all questions, recording as '0' all outcomes that did not happen and as '1' all outcomes that did happen.\nTo evaluate the accuracy of our probabilistic forecasts in this paper, we calculate Brier scores. For each forecasting question with a predicted probability $p_i$ and an actual outcome $o_i \\in \\{0, 1\\}$, the Brier score is defined as\n$BS = \\frac{1}{N} \\sum_{i=1}^{N} (p_i - o_i)^2$,\nwhere N is the total number of forecasting questions. A lower Brier score indicates higher forecasting accuracy."}, {"title": "News Collection", "content": "We collected news via the NewsCatcher API 14 days prior to question resolution. Our approach was drawn from [7] in that we generated search queries via GPT-40 and then integrated external news retrieval services like Newscatcher to aggregate and process the output. These news articles were then used as further input in Sections 2.3 and 2.6."}, {"title": "Model Self-Play Data Generation", "content": "We then instructed the base models to provide reasoning and a final probabilistic forecast for each question. For Phi-4 14B, we employed a scratchpad prompt [20], while we used a zero-shot prompt for DeepSeek-R1 14B as <think> tags are already present in the model output generation. The prompt included a summary of news from Section 2.2 along with the appropriate scratchpad or zero-shot prompt depending on the model. We ran all queries with a temperature of 1. In total, we generated a pair of reasoning traces for each question [21]. We first generated a single reasoning and then re-ran this process up to four times to arrive at a second, non-identical forecast. If all subsequent predictions were identical, we removed the full set of forecasts. Overall, we obtained 18,854 reasoning traces for the 9,427 forecasting questions that had non-constant forecasts."}, {"title": "Resolution-Driven Re-Ranking", "content": "For each question, we paired up reasoning-outcome pairs and ranked them based on the proximity of the probabilistic forecast (ranging from 0% to 100%) to the ground truth (0 or 1). Formally, for each question with ground truth $o \\in \\{0, 1\\}$, let the probabilistic forecasts from two reasoning traces be denoted by $p_1$ and $p_2$ (with $p_i \\in [0,1]$). We then define a ranking metric as\n$r(p, o) = |p - o|$,\nwhich measures the absolute difference between the forecast and the actual outcome. For example, if a pair consists of reasonings with 4% and 8% predictions respectively i.e. $p_1 = 0.04$ and $p_2 = 0.08$ with a ground truth of 0, then\n$r(0.04, 0) = 0.04$ and $r(0.08, 0) = 0.08$.\nSince 0.04 < 0.08, the reasoning trace resulting in the 4% prediction is ranked above that of the reasoning resulting in the 8% forecast. Notably, the squared error metric of the Brier score naturally mitigates overconfidence by penalizing large deviations more heavily. Pairs that resulted in identical forecasts (i.e. $p_1 = p_2$) were removed prior to this stage. In total, we used the full set of 18,854 reasoning traces for the 9,427 forecasting questions for our re-ranking.\nMoreover, to control for the possibility that information provided via the news aggregation at this step might influence the rankings, we also fine-tuned a second set of models via the same process, but with the ranking of labels randomised. These control models allow us to test whether the learning is attributable to the models learning from the higher-accuracy forecasting rationales."}, {"title": "Direct Preference Optimization Fine-Tuning", "content": "We then fine-tuned Phi-4 14B and DeepSeek-R1 14B using the preference pairs from Section 2.3. We use Direct Preference Optimization (DPO) to optimise model outputs against self-play derived and outcome-driven preferences without the need to train a separate reward model. The DPO loss was minimised using a LORA adapter (rank=16, alpha=32, dropout=0.05, target_modules=\"all-linear\", no bias) on top of the base model, which was held in 4-bit quantisation, using a batch size of 2 (with 4 gradient accumulation steps) and gradient checkpointing enabled. Training leveraged the AdamW optimiser with a linear learning rate scheduler (5e-5 base rate), beta=0.1, and BF16 mixed precision. We used 8 H100 GPUs for training. For Phi-4 14B, we found a plateau at the fifth epoch, while this occurred at the fourth epoch for DeepSeek-R1 14B (see Figure 3)."}, {"title": "Forecasting Test Set Questions", "content": "Finally, we test every model against a held-out test set of 2300 questions. Importantly, this test set begins 10 days after the final outcome in the training set, so our fine-tuned models have not been exposed to any news that might inform outcomes in the test set.\nWe do this with three versions of each model: the original base model, the fine-tuned model with correct outcomes for DPO ranking, and a control fine-tuned model with randomized outcomes for DPO ranking. This allows us to distinguish between learning that happened due to exposure to new information (for example, the news articles shared in prompts) versus learning by optimising for reasoning processes that lead to more accurate forecasts.\nTo generate our final forecasts, we used the following prompts shown in Figure 4, derived from Halawi et al. [7]. Our prompts drew on expert persona prompting [22], based on structured analytic techniques [23] and Tetlock-style superforecasting [2], as well as more structured instructions, aiming to improve forecasting accuracy over a na\u00efve assistant prompt."}, {"title": "Results", "content": "For all results below, we call the fine-tuned model 'Fine-Tune', the base model 'Base', and the fine-tuned model with randomized labels the 'Control'. We find substantial improvements in forecasting accuracy for both Phi-4 14B and DeepSeek-R1 14B fine-tunes, heavily outperforming the ignorance benchmark of a Brier score of 0.25 (arrived at by predicting 50% on every question) and improving upon the base and control models (see Figure 5).\nFor Phi-4 14B, the fine-tuned model achieved a mean Brier score of 0.200 (SD = 0.218; 95% CI [0.191, 0.209]), outperforming both the randomized-label control model (M = 0.214, SD = 0.186; 95% CI [0.206, 0.221]) and the base model (M = 0.221, SD = 0.189; 95% CI [0.214, 0.229]). Similarly, DeepSeek-R1 14B attained a mean Brier score of 0.197 (SD = 0.218; 95% CI [0.188, 0.206]) after fine-tuning, surpassing both its randomized-label control (M = 0.212, SD = 0.202; 95% CI [0.204, 0.220]) and base counterparts (M = 0.211, SD = 0.201; 95% CI [0.204, 0.220]).1\nWe conduct independent samples t-tests between the fine-tuned versions of the models and both the base and control models, as well as the frontier model benchmark set by GPT-40. We find that for both Phi-4 14B and DeepSeek-R1 14B, the fine-tuned model is statistically significantly more accurate than both the base and control models at p < 0.05. This also holds after adjusting the p-values for multiple comparisons via"}, {"title": "Conclusion", "content": "Large language models can enhance their forecasting capabilities through self-play, generating reasoning traces that enable outcome-based fine-tuning without relying on human-curated data. By pairing these traces and ranking them by their proximity to actual outcomes, the models learn to refine their probabilistic forecasts, outperforming base models and matching the performance of larger frontier models."}]}