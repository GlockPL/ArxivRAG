{"title": "EVALUATING ROBUSTNESS OF REWARD MODELS FOR MATHEMATICAL REASONING", "authors": ["Sunghwan Kim", "Dongjin Kang", "Taeyoon Kwon", "Hyungjoo Chae", "Jungsoo Won", "Dongha Lee", "Jinyoung Yeo"], "abstract": "Reward models are key in reinforcement learning from human feedback (RLHF) systems, aligning the model behavior with human preferences. Particularly in the math domain, there have been plenty of studies using reward models to align poli- cies for improving reasoning capabilities. Recently, as the importance of reward models has been emphasized, RewardBench is proposed to understand their be- havior. However, we figure out that the math subset of RewardBench has different representations between chosen and rejected completions, and relies on a single comparison, which may lead to unreliable results as it only see an isolated case. Therefore, it fails to accurately present the robustness of reward models, leading to a misunderstanding of its performance and potentially resulting in reward hacking. In this work, we introduce a new design for reliable evaluation of reward models, and to validate this, we construct REWARDMATH, a benchmark that effectively represents the robustness of reward models in mathematical reasoning tasks. We demonstrate that the scores on REWARDMATH strongly correlate with the results of optimized policy and effectively estimate reward overoptimization, whereas the existing benchmark shows almost no correlation. The results underscore the potential of our design to enhance the reliability of evaluation, and represent the robustness of reward model. We make our code and data publicly available.", "sections": [{"title": "1 INTRODUCTION", "content": "Mathematical reasoning stands as a crucial test-bed for assessing artificial intelligence (Lake et al., 2017). Solving math problems demands multi-step reasoning, involving capabilities such as abstract conceptualization and logical reasoning (Staub & Stern, 1997; Cresswell & Speelman, 2020). \u03a4\u03bf enhance these reasoning capabilities of large language models (LLMs), several methods have been proposed, including prompting methods (Wei et al., 2022; Chen et al., 2023; Wang et al., 2023) and training with large and high-quality datasets (Yu et al., 2023; Toshniwal et al., 2024). Recently, many studies have increasingly focused on using reward models for re-ranking or applying reinforcement learning (RL) algorithms (Lightman et al., 2024; Wang et al., 2024b; Luo et al., 2024).\nBehind the success of LLMs, such as ChatGPT (OpenAI, 2023a) and Claude (Bai et al., 2022), reinforcement learning from human feedback (RLHF) has been instrumental in aligning with human preferences. It enhances not only instruction following abilities (Ouyang et al., 2022) and safety (Dai et al., 2024) but also reasoning capabilities like code generation (Shojaee et al., 2023; Chae et al., 2024) and mathematical reasoning (Luo et al., 2023; Sun et al., 2024). These improvements in LLMs depend on the quality of reward models (RMS) (Touvron et al., 2023), and Shao et al. (2024) also emphasize the importance of building robust RMs for improving reasoning capabilities.\nDespite the crucial role of reward models, research has often focused on evaluating policy models (i.e. post-RLHF models) rather than reward models themselves (Dubois et al., 2024; Zheng et al.,"}, {"title": "2 PRELIMINARIES", "content": "In this section, we first categorize three types of reward models that are widely used and outline two distinct policy optimization methods. We then discuss the robustness of reward models, emphasizing reward overoptimization as a critical challenge that impacts the effectiveness of policy optimization."}, {"title": "2.1 REWARD MODEL", "content": "Generative Reward Model Given the remarkable capabilities of LLMs, these models demon- strate the potential to effectively replace human annotators in assessing various tasks (Gilardi et al., 2023; Huang et al., 2023a). With the growing interest in the LLM-as-a-judge (Zheng et al., 2024), recent studies have attempted to use LLMs as reward models (Luo et al., 2023; Yuan et al., 2024b). In this work, we use two main approaches of the generative RM: (1) conducting pairwise com- parisons to determine win / lose between two responses (Li et al., 2023; Kim et al., 2024), and (2) providing a score for a single response through direct assessment (Cui et al., 2023; Kim et al., 2024).\nClassifier-based Reward Model Using annotated preference data $D = {(x, y_c, y_r)}^M_{i=1}$, the classifier-based reward model is trained to assign higher reward to the chosen completion $y_c$ over the rejected completion $y_r$. This training process involves maximizing the log-likelihood under the Bradley-Terry (BT) model (Bradley & Terry, 1952) for preference estimation:\n$L_{reward} = -E_{(x,y_c,y_r)~D} [log (\u03c3(r_\u03b8(x, y_c) \u2013 r_\u03b8(x,y_r)))],$ (1)\nwhere \u03c3(\u00b7) denotes the sigmoid function. In general, the reward model is obtained by replacing the final output layer of causal language model with a linear head to predict a scalar.\nProcess Reward Model (PRM) Uesato et al. (2022) and Lightman et al. (2024) propose the pro- cess reward model (PRM), which predicts the correctness of each intermediate step $s_i$ in a solution. The PRM is trained with the following objective function:\n$L_{pointwise} = \\sum_{i=1}^{K} \u0177_{s_i} log ys_i + (1 \u2013 \u0177s_i) log(1 \u2013 y_{s_i}),$ (2)\nwhere \u0177si is the correctness label of $s_i$, and $ys_i$ is the sigmoid score of $s_i$ assigned by PRM."}, {"title": "2.2 POLICY OPTIMIZATION METHOD", "content": "Best-of-n Sampling (BoN) Best-of-n (BoN) sampling is an inference-time method used to op- timize the responses generated by a policy model (Nakano et al., 2021; Stiennon et al., 2020). In practice, we generate n completions from the policy model and select the completion with the highest proxy RM score. To evaluate the degree of optimization, the Kullback-Leibler (KL) diver- gence of BoN is defined analytically: $KL_{bon} = log n - \\frac{n-1}{n}$ (Stiennon et al., 2020).\nProximal Policy Optimization (PPO) Proximal Policy Optimization (PPO) (Schulman et al., 2017), a commonly used online RL algorithm, is employed to update the policy $\u03c0_\u03b8$ with a reward model $r_\u03c6$ in RLHF (Ouyang et al., 2022; Bai et al., 2022; Zheng et al., 2023). PPO aims to maximize the expected reward, which is adjusted by a KL penalty term to ensure that the optimized policy $\u03c0_\u03b8$ does not deviate significantly from the reference policy $\u03c0_{ref}$:\n$max_{\u03c0_\u03b8} E_{x~D,y~\u03c0_\u03b8(.\\x)} [r_\u03c6(x, y)] \u2013 \u03b2D_{KL} [\u03c0_\u03b8(y|x) || \u03c0_{ref}(y|x)],$ (3)\nwhere \u03b2 is a scaling factor for the KL penalty."}, {"title": "2.3 THE ROBUSTNESS OF REWARD MODEL", "content": "The success of RLHF depends on the quality of the reward model, which significantly influences the effectiveness of policy optimization (Touvron et al., 2023). Since the policy model is optimized based on a proxy reward rather than the true reward (i.e. human evaluation), the discrepancy be- tween these rewards may result in overfitting to spurious correlations, a phenomenon known as reward overoptimization (Gao et al., 2023; Coste et al., 2024; Yang et al., 2024; Rafailov et al., 2024). This issue impedes the improvement of the policy model and complicates the checkpoint selection (Gao et al., 2023; Coste et al., 2024; Rame et al., 2024). In this work, we argue that the robustness of a reward model should be evaluated based on how effectively it provides signals from which a policy can learn. To validate the benchmark for evaluating robustness of reward model, we conduct experiments to determine whether performance on the benchmark correlates with that of the optimized policy and whether the benchmark can detect overoptimization in reward models."}, {"title": "3 DESIGNING A RELIABLE BENCHMARK", "content": "The robustness of reward models is a key in RLHF systems. To build a robust reward model, it is crucial to develop a reliable benchmark that can accurately reflect the robustness of reward models. However, in RewardBench (Lambert et al., 2024), which is a widely-used benchmark for reward models, the math domain (i.e. math-prm) does not fully take this into consideration. First, math- prm is constructed based on PRM800K dataset (Lightman et al., 2024), and it was recently revealed that approximately 20% of the annotations in PRM800K are incorrect, even though it is human- annotated. Moreover, RewardBench consists of pairs of human-annotated chosen solutions and rejected solutions annotated by unaligned GPT-4, which are evaluated by comparing the rewards be- tween the chosen and rejected solutions. When solving math problems, as Hendrycks et al. (2021) and Sun et al. (2024) mentioned, humans often skip certain steps and rely on mental calculations, rather than writing out a complete step-by-step solution, which results in a significant difference compared to machine-generated solutions. This discrepancy impedes the reliability of evaluation. Finally, there can be countless incorrect solu- tions to a single mathematical problem, so simply comparing with a single incorrect solution is not sufficient to assess the robustness of reward models, as these solutions represent only isolated cases. As a result, we believe it is difficult to figure out whether RMs with high scores on RewardBench are genuinely robust or vulnerable to reward hacking. Therefore, we introduce REWARDMATH, a benchmark that can more reliably evaluate the robustness of RMs on mathematical reasoning."}, {"title": "3.2 REWARDMATH DATASET", "content": "The design philosophy of RewardMATH is to caution against a hasty generalization, which occurs when conclusions are drawn from a sample that is too small or consists of too few cases. To ac- curately measure the robustness of reward model, it is reasonable to compare m correct solutions against n incorrect solutions. However, since collecting correct solutions demands significant human resources, we initially focus on gathering n incorrect solutions to compare against a single correct solution. Based on MATH500, REWARDMATH consists of a total of 483 problems, each com- prising 1 correct solution and 9 incorrect solutions. The construction of both correct and incorrect solutions is as follows:\nCorrect Solution (Chosen) MATH500 includes human-annotated solutions, which often skip many intermediate steps, making it difficult for language models to understand and vulnerable to reward hacking. Hence, we first convert the human-annotated solutions from MATH500 into step- by-step machine-generated solutions. We prompt GPT-4, using 4 carefully crafted exemplars for"}, {"title": "3.3 REWARDMATH SCORING", "content": "For each problem, we infer 10 solutions in total-1 correct solution and 9 incorrect solutions\u2014and then assign a true classification label when a reward of chosen solution is higher than all rewards of rejected solutions. While RewardBench involves a simple binary classification task comparing chosen and rejected solutions at a 1:1 ratio, where a random model achieves a result of 50%, RE- WARDMATH has a 1:9 ratio of chosen to rejected solutions, meaning a random model would achieve a result of 10%. Furthermore, considering only whether the reward of chosen solution is the highest can be fairly strict, we also utilize Mean Reciprocal Rank (MRR), where higher ranks for the chosen solution lead to higher scores. The MRR is calculated using the formula: $MRR = \\frac{1}{d} \\sum_{n=1}^{d} \\frac{1}{rank_n},$ where d is the total number of problems, and $rank_n$ is the rank of the chosen solution for each problem. In pairwise comparison of generative reward models, the rank of the chosen solution is determined by the number of the rejected solutions that win the chosen solution."}, {"title": "4 EVALUATION ON REWARDBENCH AND REWARDMATH", "content": "We conduct our experiments using three types of reward models that widely-used in mathematical reasoning task, i.e. generative reward models, classifier-based reward models, and process reward models. For generative reward models, we employ a series of large language models, GPT-3.5- turbo/4/4O (OpenAI, 2023a;b), Claude-3.5-Sonnet/3-Opus, Prometheus-7B/8x7B (Kim et al., 2024), and LLaMA3-8B/70B (AI@Meta, 2024), which are the off-the-shelf LLM-as-a-judge. To determine whether the reward models ranked at the top of the REWARDBENCH leaderboard are truly robust, we adopt top-ranked classifier-based reward models (Wang et al., 2024a; Cai et al., 2024; Yuan et al., 2024a; Dai et al., 2024; Liu & Zeng, 2024; Yang et al., 2024), as well as available open-source PRMs (Wang et al., 2024b; Sun et al., 2024; Xia et al., 2024). PRMs require an aggregation function to combine step-level rewards in order to obtain a solution-level reward. While most prior works simply multiply the rewards of all steps to calculate the solution-level reward (i.e. prod), we utilize the geometric mean as the aggregation function to minimize the influence of the number of steps."}, {"title": "4.2 EVALUATION RESULTS", "content": "The ability of LLM-as-a-judge as an evaluator on mathematical reasoning. According to the results from RewardBench in Table 1, LLM-as-a-judges, especially GPT-4 or Prometheus-2-7B, appear capable of serving as reward models. However, rather than the results from RewardBench, which only evaluates limited cases, the results of direct assessment on REWARDMATH present that LLMs tend to fall short as reward models, with most scoring close to 0, except for the GPT-4 family. To understand the reason behind this, we assign a true classification label even when the reward of the chosen solution is equal to the rewards of the rejected solution (i.e. Acc. (w/ tie)). Consequently, we observe a significant improvement across all LLM judges. This suggests that most LLMs fail to distinguish details between correct and incorrect solutions, simply assigning the same scores to all. Interestingly, most generative reward models demonstrate better performance in the pairwise comparison.\nHigh scores on RewardBench do not guar- antee the robustness of reward models. As demonstrated in Table 2, rankings on Reward- Bench do not translate to the same level of perfor- mance on REWARDMATH. Specifically, Oasst- rm-2.1-pythia-1.4b, which is one of the top- ranked models in RewardBench, faces challenges in REWARDMATH, scoring lower than Beaver- 7b-v2.0-reward, the lowest-ranked model in Re- wardBench. However, Internlm2-7b-reward, which ranks lower than Oasst-rm-2.1-pythia-1.4b in RewardBench, shows relatively strong perfor- mance in REWARDMATH, suggesting that it is genuinely a robust reward model for mathemati- cal reasoning. Additionally, PRMs typically tend to achieve high scores on RewardBench due to an advantageous aggregation function (i.e. prod), but when the step bias is removed by using geo- metric mean (i.e. geo mean) as aggregation func- tion, it is revealed that most of them struggle even in RewardBench. Similar to the classifier-based RMs, the performance of PRMs on RewardBench does not carry over to REWARDMATH, with Math-Shepherd-Mistral-7B, a top-ranked PRM in RewardBench, notably ranking the lowest in RE- WARDMATH."}, {"title": "5 THE FUTURE DIRECTION FOR A RELIABLE BENCHMARK", "content": "In this section, we discuss promising directions to improve the reliability of benchmark for reward models. First, we explore the structure of a reliable benchmark using RewardBench and REWARD- MATH. Then, from the perspective of reward overoptimization, we verify which benchmark can effectively represent the robustness of reward models."}, {"title": "5.1 RELIABILITY OF BENCHMARK", "content": "Comparing the results of RewardBench and REWARDMATH in optimizing the poli- cies. To determine whether a reward model is robust, we can assess the performance of the optimized policy. Therefore, an ideal benchmark is one where the results of re- ward model on the benchmark can effectively represent the performance of the policy model optimized by the reward model. To explore this further, we optimize two pol- icy models, MetaMATH-Mistral-7B and WizardMath-7B-v1.1, using the BoN sampling with reward models and evaluate them on both the in-distribution dataset and the out-of-distribution datasets."}, {"title": "5.2 THROUGH THE LENS OF REWARD OVEROPTIMIZATION", "content": "We now discuss whether REWARDMATH effectively represents the robustness of reward models by evaluating how well it estimates reward overoptimization. The robust reward model should provide effective signals for policy learning, which are resilient to reward overoptimization."}, {"title": "5.2.1 EXPERIMENTAL SETUP", "content": "To examine the phenomenon of reward overoptimization, we analyze the relation between the true reward (i.e. human evaluation) and the degree of optimization, quantified by the KL divergence between the optimized and initial policies. We apply two types of optimizations: BoN sampling and reinforcement learning via PPO. For BoN sampling, we generate n responses on MATH500 dataset and approximate the degree of optimization by $KL_{bon} = log n - \\frac{n-1}{n}$ (Stiennon et al., 2020). For PPO, policy is trained over one epoch on the training set of MATH dataset.\nSince accessing the true rewards requires human annotators, prior work often relies on a gold RM to approximate true rewards or uses win-rates evaluated by LLM-as-a-judge. In mathematical reasoning, where hu- man preference can be measured by accuracy (i.e. pass@1), we assess the true rewards from two perspectives: (1) gold reward via the gold RM, and (2) oracle reward, which represents human preferences (i.e. pass@1). We assume Internlm2-7B-reward, which performs well on both Reward- Bench and REWARDMATH, as the gold RM. Depending on whether the reward model is trained using synthetic preference data, we take two approaches to both gold and oracle rewards as below:\nSynthetic Setup We conduct experiments under a synthetic setup following Gao et al. (2023) and Coste et al. (2024), where responses are scored using a gold RM instead of human annotators. First, we train Mistral-7B-v0.1 using MetaMATH dataset as the initial policy model, after which we collect correct and incorrect solutions generated by the policy model. These solutions are scored by the gold RM to generate 65K synthetic preference data. Then, we train proxy RMs based on Mistral-7B-v0.1 while varying the amount of data. For the policy optimization, we use BoN sampling and PPO to investigate reward overoptimization.\nNon-synthetic Setup Unlike in the synthetic setup, we use open-source classifier-based reward models and PRMs as proxy reward models. We aim to observe whether the performance of the reward model on RewardBench and REWARDMATH can predict reward overoptimization. We only use BoN sampling as the policy optimization method for MetaMATH-Mistral-7B and WizardMATH-7B-v1.1, due to potential instabilities in PPO that may arise from discrepancies between the reference model and the base model of the RM."}, {"title": "5.2.2 RESULTS", "content": "Evaluating the robustness of reward models via reward overoptimization. Typically, a robust proxy reward model trained to capture human preferences should exhibit increasing gold rewards as\nKLBON \u2248 4.55 nats"}, {"title": "5.3 DISCUSSION ON DEVELOPING EFFECTIVE RLHF SYSTEMS", "content": "Benchmarks serve as critical milestones in advancing artificial intelligence. In this work, we argue that a benchmark for reward models should reliably assess their robustness, where a robust RM indicates a model that provide useful signals to enable effective policy learning. Through extensive experiments, we confirm that our reliable benchmark design, which mitigates the risk of reward hacking and employs one-to-many comparisons, accurately reflects the robustness of reward models. While this work marks a significant step forward, there is still room for improvement. We validate our design in mathematical reasoning tasks, where human preferences can be clearly defined by correctness, making it easier to gather multiple rejected completions. Since the reward models can be applied to a wide range of tasks, a crucial next step is to extend our design to cover all of them. We hope that advancing this line of research will provide a promising path toward developing more trustworthy and effective RLHF systems."}, {"title": "6 RELATED WORK", "content": "Evaluating Reward Models. The success of RLHF depends on the robustness of the reward model in capturing human preferences. The assessment of reward models pri- marily relies on downstream evaluation, validating their efficacy by observing performance enhance- ments in the optimized policy. However, these evaluation approaches are questionable due to numerous ad-hoc choices in the policy optimization process, in- cluding the selection of the RL algorithm, computational resources, and hyperparameters. Recently, to understand the behavior of the reward models and directly observe its performance, Lambert et al. (2024) proposed RewardBench, a benchmark that evaluates by comparing the reward between chosen and rejected completions. In this work, we address the limitations in both quality and evaluation approach (i.e. one-to-one comparisons) of RewardBench in the math domain and demonstrate the effectiveness of our proposed reliable benchmark design.\nMathematical Reasoning of LLMs. The mathematical reasoning capabilities of LLMs play a major role in evaluating artificial intelligence of these models. To strengthen the mathematical reasoning capability of LLMs, researchers often train the models with large and high-quality datasets, and apply sophisticated prompt en- gineering, tailored for step-by-step reasoning in mathematics. Furthermore, they also seek to address the weakness of LLMs, such as its limitations in precise calculation and algorithmic processing, by incorporating external tools like Python interpreters and calculators. Although these tool-augmented methods present promising results, this work focuses on the intrinsic capability of LLMs to solve math problems, without relying on external tools. Recently, many studies have ex- plored the use of reward models for reasoning tasks through two main approaches: using reward models as verifiers to re-rank outputs during inference, and applying RL algorithms during training to improve reasoning abilities. Therefore, we investigate ways to further enhance mathematical reasoning abilities in a RLHF system through comprehensive evaluation of reward models."}, {"title": "7 CONCLUSION", "content": "In this work, we suggest a new design for reliable evaluation of reward models: (1) mitigating the risk of reward hacking and (2) employing a one-to-many comparison. To validate our design, we propose REWARDMATH, a benchmark that effectively represents the robustness of reward models in mathematical reasoning tasks. Our extensive experiments demonstrate that the performance on REWARDMATH has a strong correlation with the performance of the optimized policy, whereas the existing benchmark shows no correlation. Furthermore, we also confirm that REWARDMATH can effectively estimate the reward overoptimization, a critical concern in RLHF systems. While we utilize a one-to-many comparison due to resource limitations, a crucial next step may be to employ many-to-many comparisons for a more thorough assessment. We hope that this work, which aims to establish a reliable benchmark for evaluating reward models, paves the way toward the development of a more trustworthy RLHF system."}]}