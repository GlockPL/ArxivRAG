{"title": "Prompts Are Programs Too! Understanding How Developers Build Software Containing Prompts", "authors": ["JENNY T. LIANG", "MELISSA LIN*", "NIKITHA RAO*", "BRAD A. MYERS"], "abstract": "The introduction of generative pre-trained models, like GPT-4, has introduced a phenomenon known as prompt engineering, whereby model users repeatedly write and revise prompts while trying to achieve a task. Using these Al models for intelligent features in software applications require using APIs that are controlled through developer-written prompts. These prompts have powered Al experiences in popular software products, potentially reaching millions of users. Despite the growing impact of prompt-powered software, little is known about its development process and its relationship to programming. In this work, we argue that some forms of prompts are programs, and that the development of prompts is a distinct phenomenon in programming. We refer to this phenomenon as prompt programming. To this end, we develop an understanding of prompt programming using Straussian grounded theory through interviews with 20 developers engaged in prompt development across a variety of contexts, models, domains, and prompt complexities.\nThrough this study, we contribute 14 observations about prompt programming. For example, rather than building mental models of code, prompt programmers develop mental models of the FM's behavior on the prompt and its unique qualities by interacting with the model. While prior research has shown that experts have well-formed mental models, we find that prompt programmers who have developed dozens of prompts, each with many iterations, still struggle to develop reliable mental models. This contributes to a rapid and unsystematic development process. Taken together, our observations indicate that prompt programming is significantly different from traditional software development, motivating the creation of tools to support prompt programming. Our findings have implications for software engineering practitioners, educators, and researchers.", "sections": [{"title": "1 Introduction", "content": "\u201cI suspect that machines to be programmed in our native tongues\u2014be it Dutch, English, American, French, German, or Swahili\u2014are as damned difficult to make as they would be to use.\"\n-Edsger Dijkstra, 1979\nThe introduction of generative pre-trained models (e.g., GPT-4 [9], Dall-E [16])-also known as foundation models (FMs)\u2014have drastically changed how software is built by developers. AI program-ming assistants that generate code (e.g., GitHub Copilot [21]) have aided developers across a variety of tasks such as writing significant portions of code, learning new APIs and programming languages, writing tests [47], and information seeking behaviors [33] thereby improving the productivity of programmers [61, 83, 84]. Recently, instruction-tuned [57] large language models (LLMs), such as ChatGPT [2], have assisted developers with a broader range of software development-related tasks via writing natural language prompts. This includes resolving code issues, developing new features, refactoring, and writing configuration files [19].\nThe rising prominence of prompts has ushered in a new phenomenon known as prompt engi-neering, whereby model users repeatedly write and revise natural language prompts to achieve a task. These prompts enable more intelligent features when integrated in popular code-based software applications [58], such as Google Search [4] and Microsoft Office [3], reaching potentially millions of users [1]. As of January 2024, engineered prompts have also underpinned the creation of over 3 million custom versions of ChatGPT for a specific task, known as GPTs [5].\nDespite the budding impact of prompt-powered software, little is known about the prompt engineering process and its relationship to programming. To the best of our knowledge, two such studies in software engineering currently exist. One is Dolata et al. [27]'s study of 52 freelance developers on the challenges of developing solutions based on generative AI. Their interview focuses on the positive and negative experiences with generative AI, project uncertainties, and views on freelancing. The other is Parnin et al. [58]'s study of 12 professional software developers integrating generative AI into products. Their interview topics include the participant's motivation for using AI in the product, major tasks for building the generative AI application, prompt engineering, testing, tooling, challenges, learning related skills, and concerns with AI. While these studies offer interesting preliminary insights on prompt development, they are constrained to solely freelance and professional software development, limiting the generalizability of the results to other types of programming. Further, these studies do not place targeted focus on the process of developing prompts in relation to programming activities. We address the gaps in this literature by following a more systematic and rigorous qualitative methodology-known as Straussian grounded theory [23]-and performing maximum variation sampling [69] to recruit a diverse sample of programmers with varying programming contexts, models, roles, application domains, and prompt complexities to understand the process of using prompts as part of creating software applications.\nIn this work, we argue that some forms of prompts are programs. We view the development of such prompts and other natural language-powered software as a distinct phenomenon in programming. We refer to it as prompt programming, following prior work [36]. We note that the concept of programming using natural language is not novel; in 1979, Edsger Dijkstra discussed a phenomenon where one could \u201cinstruct [a machine] in our native tongues\" [25]. This vision has only recently been realized, with the advent of FMs providing the means for technical feasibility and widespread use of such programs.\nWe argue that prompt programming is a phenomenon that warrants its own study. Programming as a discipline is broad but highly nuanced\u2014the sociotechnical circumstance of the programming task drastically influences the nature of the programming. For example, the programming process of end-users who write software for personal use (i.e., end-user programming [43]) is fundamentally"}, {"title": "2 Related Work", "content": "We discuss related work on prompt engineering (see Section 2.1 and human aspects of software engineering for AI (see Section 2.2). Due to the quickly evolving landscape of prompt development and FMs, our work is up-to-date as of September 2024."}, {"title": "2.1 Prompt Engineering", "content": "Prior work has conducted empirical studies of prompt engineering. Some works have focused on chatbot development with end users through prompts [80-82]. In a user study of 10 people who used a tool to create chatbots through prompts, Zamfirescu-Pereira et al. [82] identified several challenges of prompt development, including opportunistic prompt design approaches, a lack of systematic testing, and writing prompts that did not generalize. Jiang et al. [36] evaluated a prompt development tool on 11 users from diverse roles, including designers, content strategist, and front-end developers. Problems participants faced included the prompt easily breaking and fixating on examples as well as having difficulty evaluating large amounts of text.\nOther work has investigated creating tools to facilitate prompting across a variety of contexts. This includes the creation of a prompt pattern catalogue to solve common problems encountered when conversing with a language model (LLM) [74], such as question refinement. A separate catalogue was created for software engineering tasks [75], such as specification disambiguation. Other tools include interfaces for prompting, such as PromptAid [54], PromptMaker [36], and PromptIDE [68]. These tools included separate views to view the dataset, iterate the prompt, track prompt performance, and search for prompts.\nPerhaps most related to our study are studies that investigate prompt engineering in the context of software engineering. Dolata et al. [27] conducted a study with 52 freelance developers on their experience with developing prompt-powered software. The participants enumerated a number of challenges, including having difficulty identifying the source of incorrect responses, budget constraints, and unrealistic client expectations. Meanwhile, Parnin et al. [58] ran an interview study with 12 professional programmers who developed product copilots. They found that participants"}, {"title": "2.2 Human Aspects of Software Engineering for Al", "content": "Numerous works have studied the human aspects of building ML-enabled systems. This domain offers some preliminary insights on the challenges of developing prompt programs with FMs. This form of software development also involves working with and wrangling non-deterministic, opaque neural models [31]. This is similar to prompt programming, as developers must also contend with nontransparent, stochastic FMs. Prior work describes the development of models as highly experimental [31, 55, 56, 71, 76]. Literature also stresses the importance of collecting high-quality data for the domain [12, 27, 31, 55, 56, 62] as well as the use of quantitative metrics, like accuracy, precision, and recall, to measure model performance [56, 71].\nHowever, developing ML components introduces unique challenges for developers. Models can be difficult to debug due to its non-determinism [31]. Multiple sources have identified the issue of training-serving skew, when training data does not generalize to production data [55, 56, 71]. \u03a4\u03bf reduce this gap, models are re-trained with new data over time [55]. Additionally, expertise across a variety domains, such as software engineering and data science, are distributed across roles [31, 56, 71]. In an interview study with 45 practitioners, Nahar et al. [56] found this could introduce collaboration challenges between different roles, such as unclear model requirements, handling evolving data, and inadequate datasets. Finally, several works note the lack of AI literacy can make requirements elicitation, communication, and collaboration with clients challenging [26, 56, 76].\nThis body of literature serves as a foundation for understanding prompt programming. We extend this body of work by comparing building ML-enabled systems to prompt programming and understanding what aspects of this prior literature applies to prompt programming."}, {"title": "3 Methodology", "content": "Since there is limited literature on how developers create prompt programs, we use a qualitative approach to develop our understanding by exploring the process of developing prompt-powered software. In particular, we use the grounded theory methodology, which allows qualitative re-searchers to develop a novel exploratory explanation (called a \u201ctheory\u201d) for a particular domain. There are three popular approaches in grounded theory: Glaserian/classical [32], Straussian [23], and constructivist [18] methodologies, which vary in their procedures and epistemology [22]. We present our results in Section 4 and Section 5.\nOur approach is based on Straussian rounded theory [23], which involves defining a research question, developing theoretical sensitivity towards the phenomenon via a broad literature review, generating a theory through simultaneous data collection and analysis, and performing a focused literature review to contextualize the results [22, 23]. In this section, we first discuss our grounded theory process (Section 3.1). We then describe our participants (Setion 3.2) and interview protocol (Section 3.3), and close with a discussion of the limitations to our method (Section 3.4)."}, {"title": "3.1 Grounded Theory Process", "content": "Below, we describe our grounded theory process for the study. It contains five main stages: defining a research question, developing theoretical sensitivity, generating the grounded theory, triangulating with literature, and validating the theory. We describe this process in further detail below.\nDefining a research question. Our grounded theory process began with defining a research question. We study the phenomenon where developers write a program using natural language, rather than pure code: How do programmers develop programs that are natural language prompts? Prompts can be used in a variety of contexts, such as interacting with ChatGPT in conversation [19], but are not programs. Thus, we apply the definition of a prompt program described in Section 1.\nDeveloping theoretical sensitivity. Before the study, the researchers should develop an intuition of what is occurring in the data towards the phenomenon being studied, known as theoretical sensitivity [22]. This can be done through professional experience and literature reviews so the researcher can extract insights from the data; however, the researcher should not be constrained by prior knowledge while developing the theory [23].\nWe entered the study with some theoretical sensitivity since two authors are software engineering and AI researchers and have developed multiple prompt programs themselves. Thus, following prior work [53], we consider our background and adapt our literature review strategy to be a broad but lightweight review of prompting. The first author reviewed foundational prompting papers in machine learning venues [17, 73] and a notable survey paper on prompting techniques in natural language processing (NLP) [51]; in addition, that researcher reviewed two empirical studies on how people develop prompts [27, 82] from human-computer interaction (HCI) and software engineering venues. We used this literature review in combination with the authors' existing domain expertise to develop questions for the interview protocol (see Section 3.3).\nGenerating the grounded theory. We conducted 60-minute interviews with a diverse set of 20 developers who had developed prompt programs (see Table 1) over Zoom. Interviews were audio recorded and transcribed; recordings were deleted afterwards. Two authors were present for the first 6 interviews to become familiar with the data; the first author conducted the remaining interviews. To develop the initial theory, three authors first independently performed line-by-line coding on the first two interviews in separate codebooks to become further sensitized to the data. Each code contained a description of the code as well as observations from the interviews. They then reconvened to merge the individual codebooks by identifying codes with similar concepts and merging them into a shared codebook. The remaining codes were then discussed and added or removed to the codebook based on unanimous vote. The first author then coded the next interview; two authors reviewed the codes for agreement. We identified five instances of disagreement. These disagreements were then discussed and resolved. Next, the three authors performed axial coding, grouping the emerging codes into preliminary categories upon unanimous vote.\nFor the remaining interviews, the authors individually open coded the interview transcripts, making note of any new codes that emerged in the shared codebook. Following the open coding, a memo for each interview was created to record notable parts of the interview to further refine the emerging theory. The authors met regularly to discuss the new codes and observations. New codes and observations were added to the shared codebook upon unanimous vote and the categories were further refined as more data was collected.\nAs the theory developed, we performed maximum variation sampling [69] to obtain a diverse set of participants that could challenge or extend the theory. We recruited participants through snowball sampling within the authors' social networks and in online open-source communities who met the definition of creating a prompt program (see Section 1). Participants were recruited based on"}, {"title": "3.2 Participants", "content": "A summary of our participants is in Table 1. The inclusion criterion was developers who had created at least one prompt program before, according to the definition presented in Section 1. Our participants were men (N = 17) and women (N = 3) from a broad range of roles in technology, including Machine Learning Engineer, Senior Software Engineer, Ph.D. Student, and Principal Data and Applied Scientist. All participants had prior programming experience (median 8.5 years) and had written a prompt program before (median 10 programs). Participants also regularly used foundation models: 10 participants reported using them more than once daily, 5 participants reported using them once daily, and 5 participants reported using them weekly."}, {"title": "3.3 Interview Protocol", "content": "The interviews were 60 minutes long and were conducted over Zoom. The procedure was approved by our Institution's Review Board. Participants were compensated with a $20 Amazon gift certificate. To begin, the participant completed a demographic and background survey with questions such as gender, number of years of programming experience, and number of prompt programs developed. We follow best practices in reporting gender in HCI [64]. Then, the interviewer presented the participant with the definition of a prompt program and asked the participant to recall the most recent time they wrote a prompt program. If the participant had access to the prompt, they retrieved the prompt and any associated history with the prompt. The participant then provided a brief overview of the prompt. To re-familiarize the participant with their prompt and their process, the participant discussed the prompt's design choices (i.e., a decision made in how to implement the prompt). Next, they discussed the challenges they faced while developing the prompt. Finally, the participant discussed the overall process they used to develop the prompt.\nTo develop the interview protocol, we identified several themes to explore: data, requirements, design, implementation, evaluation, debugging, and deployment based on the broad literature review and the authors' prior experience developing prompt programs (see Section 3.1). We then"}, {"title": "3.4 Threats to Validity", "content": "Below, we discuss the threats to validity of this study.\nInternal validity. Participants could have memory biases that could introduce errors in their recounting of their prompt design choices and development experience. To reduce this threat, we asked participants to recount their most recent prompt that met the definition of a prompt program. When possible, we asked participants to review their prompt to ground their answers. Further, the authors' own previous experiences with prompt development could also introduce confirmation biases of the developed theory. Overall, we reduced the limitations of this study by performing triangulation with prior literature and validating the theory with study participants and a professional who have engaged in prompt programming.\nExternal validity. Using snowball sampling and recruiting within the authors' social networks may introduce sampling bias, so our sample may not be representative of all developers creating prompt programs. Additionally, self-selection bias could influence the results. Interviews were conducted in English and could cause less representation from non-native English speakers. Also, some participants were unable to disclose all details of their process due to company policy. Thus, the results from this study may not fully generalize beyond our sample of participants."}, {"title": "4 Properties of Prompt Programming", "content": "We find that prompt programming is the interaction between three entities: the developer, the foundation model, and the prompt. To write a prompt, a programmer uses their prior experience with the FM to construct a mental model of how the FM might perform on the task. This mental"}, {"title": "4.1 Results", "content": "4.1.1 Programmer. During prompt programming, the programmer spends significant effort devel-oping a mental model of how the FM might behave on the task.\nProgrammers develop a mental model about the FM's behavior on the prompt to pre-dict how it may behave. Technology users form mental representations of their interactions with devices, which can be developed through hands-on experience with the technology [67]. Understanding the ML model's abilities is important but challenging for developing ML-enabled systems [12, 26, 31, 76], including with FMs [10, 34, 36, 77]. Just as developers form mental models of code by exploring the codebase or talking to colleagues [46], participants described developing a mental model of the FM's behavior on the prompt by running the prompt. All participants developed their mental model by observing the FM's performance, either by examining individual examples (P6, P7, P8, P9, P10, P12, P14, P15, P16, P17, P18, P19, P20) or looking at metrics (P4, P5, P7, P8, P9, P11, P14, P15, P16, P18, P19). Based on these observations, the participant formed a hypothesis or belief about the model's behavior and updated the prompt accordingly. This often included creating a new explicit instruction, \u201cguideline\u201d (P8), \u201crule\u201d (P14), or \u201cmandate\u201d (P20) in the prompt to address the observation (see Section 5.1.3), as noted by Zamfirescu-Pereira et al. [81].\nThe programmer's mental model was the accumulated beliefs about the FM's behavior. These beliefs could differ or even contradict between individuals. For instance, while some participants (P1, P4, P5, P6, P9, P15, P16, P18, P20) included examples in the prompt for few-shot learning [17] to achieve better results, others (P7, P13, P19) believed the inclusion of examples caused the prompt to overfit to those examples: \u201c[we did not] use few-shot prompting because...it over-indexes on the examples. After a while, you start seeing very repetitive and monotonous output\u201d (P7). This overfitting of examples has been found in NLP literature [51] and has also been noted in practice [36].\nProgrammers' mental models are not reliable. Mental models are known to be incomplete and inaccurate [67]. Due to the lack of transparency of neural models [12] like FMs in prompt engineering [54], participants mentioned they were not always confident about their developed mental models (P2, P4, P5, P6, P8, P9, P10, P15, P17, P18, P19): \u201cIt's a black box model. Nobody knows what's going on inside. It's a science of faith\u201d (P17). Additionally, the stochastic nature of FMs [15] made it difficult for participants to predict the model's behavior, as found in prior work [36]: \u201cI have never been confident [in predicting an FM's behavior]...and I don't think I ever will\u201d (P10). Some participants (P1, P3, P9, P13, P19) had the FM generate explanations to understand its behavior. Whereas in NLP literature, these explanations are not faithful to the FM (i.e., accurately reflecting the model's actual reasoning process) [35], participants felt that it helped them understand the model's behavior: \u201cYou can reason about [the LLM] if you can see [its] thought process\u201d (P19).\nProgrammers use external knowledge sources and prior experience with FMs to develop their mental model. Prior to prompt programming, participants (P1, P2, P4, P11, P12, P15, P16, P18) developed their mental model of the FM through \u201cprompt intuition\u201d (P15) (i.e., prior experience"}, {"title": "4.1.2 Foundation Model", "content": "The FM on which the prompt is run influences how the prompt is written.\nEach FM has its own set of qualities and capabilities. Participants ran their prompts on models that varied in capabilities (see Table 1). Some qualities were explicit or obvious; this included the types of input the model accepted (e.g., images and text (P9)), output generated (e.g., \u201cGPT-3.5 model's JSON output [mode]\u201d (P7)), context window size (P2, P3, P8, P19), prompt formatting requirements (P6), generation speed (P4, P11, P15, P19), and the privacy of the model (P4).\nSome capabilities were latent and were discovered by interacting with the model, as noted in prior work [10, 36]. Literature suggests FMs as having latent capabilities, as GPT's performance has improved on some tasks and regressed on others over time [20, 52]. Participants found some differences between FMs to be qualitative: \u201c[One] model was capturing some instructions in the prompt in a much more focused manner, whereas the [other] was a little more lax\u201d (P15). In our sample, participants (P1, P4) noted a quality difference in open-source models (e.g., Llama [70]) compared to closed-source models (e.g., GPT-4 [9]): \u201cGetting [the prompt] to work on a smaller model was very frustrating when with a zero-shot prompt, GPT-4 would almost achieve a hundred percent performance\u201d (P4). Participants (P1, P9) also noted some FMs were unable to complete the task: \u201c[The model was] generating garbage, and [after 30 iterations] I concluded the model is trash\u201d (P1)."}, {"title": "4.1.3 Prompt", "content": "Prompts are sensitive to small details. Additionally, since prompts are a product of the programmer's observations about model behavior as well as the qualities and capabilities of the model, we observe that prompts can be fragile.\nMinute details in the prompt matter. Participants reported developing several techniques to influence the FM into generating the desired output. In addition to using known, high-level techniques from literature (e.g., assigning personas, clearly describing the task, selecting a prompting strategy, and providing data context) [7, 49], participants described a range of subtle strategies they employed to obtain the desired output. This included specific phrasing for clarity or generality (P5, P13, P14, P16, P20); repeating phrases for emphasis (P4, P6, P10, P20); avoiding negations (P2, P12); or adding specific characters, like emojis, to encourage their appearance (P6, P10). Other participants noted strategies to visually format the prompt for organization purposes. This included using numbered or bulleted lists (P8, P9, P10, P16) as well as new lines and spacing (P1, P3, P5, P11, P12, P18). Other participants used techniques to emphasize specific parts of the prompt, including bolding (P10, P18) and capitalization (P7, P10, P12, P20).\nParticipants (P10, P12) also converged on creative details to include in the prompt for better performance. For instance, to prevent the FM from leaking prompts, one participant \u201cbullied GPT\u201d"}, {"title": "4.2 Implications", "content": "Are the \u201cbest practices\u201d best practices? In our study, participants' mental models on FMs conflicted, such as whether few-shot learning [17] was beneficial to prompt performance. While including examples is listed within the official prompting guide made by OpenAI [7], some of our participants (P7, P13, P19) were skeptical of this practice after observing the FM. This underscores the importance"}, {"title": "5 Prompt Programming Process", "content": "We found that the prompt programming process exhibited significant differences across each of the software development activities (see Figure 2). We identified six types of activities in prompt programming: requirements, design, implementation, debugging, data curation, and evaluation. We first present these results (see Section 5.1) and conclude with its implications (see Section 5.2)."}, {"title": "5.1 Results", "content": "5.1.1 Requirements. Requirements underpin the construction of prompt programs. Similar to prior work [28, 58], participants noted the existence of functional and non-functional requirements, such as usability (P13, P15), reliability (P7, P19), latency (P4, P11, P15, P19), safety (P7, P10, P13, P19),"}, {"title": "5.1.2 Design", "content": "Complex or lengthier prompts can be decomposed into into multiple components.\nPrompt programs can be composed and decomposed. Participants decomposed prompt pro-grams into smaller components based on the complexity of the prompt program. For a single prompt, participants described decomposing the prompt into sub-components, such as into subsections (P1, P2, P5, P6, P7, P8, P13, P20). One participant noted \u201cit is important to structure the prompt in a way that makes sense to you. Otherwise, you're not able to maintain it going forward\u201d (P13).\nPrompts can be composed or \u201cchained\u201d together for greater capabilities [10, 51, 77, 78]. Partici-pants chained multiple prompts that depended on each other (see Table 1) for complex tasks: \u201cWe know what the complex task looks like, and then our job is to break that down...Evaluating each step independently of the others can help for complex tasks\u201d (P9). Each prompt had its own responsibility: \u201cif you are creating a robot, the motion of the hand in any scenario could be a responsibility to one agent. The movement of the legs could be given as a responsibility to another agent... Sometimes, they have to communicate with each other to perform a synchronous action\u201d (P19). Similar to pure code systems, prompts acted as a single module with its own concern [24] which could be composed into a more complex system. Yet, composing multiple prompt programs posed challenges (see Section 4.1.3), causing coupling as the prompts were modified to work together. This is similar to engineering ML-enabled systems, whose components have tight coupling [71]. Participants tried to address this challenge by allowing APIs, such as LangChain (P4, P9, P10) or LangGraph (P19) to handle orchestration, as noted in prior work [27, 77]."}, {"title": "5.1.3 Implementation", "content": "Prompt programming involves constant \u201cexperimentation [compared to] traditional software development\u201d (P15) (see Figure 3) and was described as a \u201ctrial and error\u201d (P3, P7, P10, P15) process by participants.\nPrompt programming is rapid and unsystematic. Many participants (P1, P4, P6, P7, P8, P12, P13, P14, P15, P19, P20) described a process of starting with a basic prompt for the task to test"}, {"title": "5.1.4 Debugging", "content": "Prompts can have defects or undesired outputs, which requires debugging from the programmer. Our participants observed a range of defects, such as hallucinations (P9, P14, P15, P17, P19) or not following directions (P2, P3, P4, P6, P8, P9, P10, P12, P18, P19, P20).\nFault localization is not certain. To fix errors, participants engaged in debugging behaviors. However, debugging prompts is challenging [27, 36]. The stochastic and black-box nature of FMs made it impossible to ascertain the defect's source: \u201cComing from a software engineering background, you...want to set breakpoints and debug, looking at the results step by step. There's no such mechanism for prompts\" (P13). Thus, one challenge was knowing how to change the prompt to address the defect [54]: \"I don't even know what to change in my prompt to get there\u201d (P5). Thus, participants engaged in shotgun debugging by trying random changes to the prompt to fix the defect (P2, P7, P8, P10, P15, P18): \u201cIn most cases, it is like hit and trial. So maybe I will try a different prompt. I will try it with a different example\u201d (P18). Prior experience and intuition were thus helpful in debugging (P2, P4, P15, P16). Participants also described strategies to gain more confidence in understanding what could fix the error. This included having the FM generate reasoning (P1, P3, P9, P13, P19); testing small, incremental changes (P12); and restricting instructions to specific parts of the prompt (P14, P20). However, participants reported that fixes did not always fix the error (P3, P4, P5, P6, P15, P16, P18, P20), as it could introduce new errors (P3, P4, P7) as documented in prior work [58].\""}, {"title": "5.1.5 Data curation", "content": "To develop prompt programs, programmers must develop new, high-quality datasets, a process referred to as \u201cdata curation\u201d [62]. This became a central activity.\nProgrammers need to find representative data for the task. The shift to FM-based com-ponents put greater emphasis on finding high-quality data, consistent with prior work [12, 26, 31, 34, 55, 56, 71]. To evaluate the prompt (see Section 5.1.6) and provide examples for few-shot learning [17], data curation became important. Since many of the participants' tasks were custom or specific, using existing benchmarks was not sufficient except in general and common tasks such as jailbreaking (P19). Participants created their own datasets by mining data from the internet or from the participant's organization, as well as annotating data (P1, P3, P4, P5, P6, P7, P8, P11, P13, P14, P15, P17, P18, P20). The challenge was finding representative data for the task that worked in practice, as prompt programmers needed \u201cto do [their] best to predict how people are going to use [the application]\u201d (P20). Datasets did not necessarily work in practice due to training-serving skew (i.e., when training data does not generalize to production data) [55, 56, 71]: \"You can test with your own dataset, but at the end of the day, you're still not completely sure how good the output is\u201d (P7).\nParticipants took care in capturing the full diversity of the input distribution by dividing the inputs into categories and finding examples for each category (P1, P8, P9, P15, P19, P20). Prior work indicates the non-determinism of ML models could pose difficulties in evaluation [34, 71]-participants addressed this by paraphrasing the inputs (P16, P18, P19, P20). Despite their best efforts, the datasets participants created did not capture the full distribution of inputs. Participants relied on several methods to obtain more data, such as by using an FM to generate examples (P12, P18, P19); having colleagues stress test, \u201cdata bash\u201d (P13), or red team [28] the prompt (P7, P10, P13, P17, P19); and deploying the prompt for user feedback (P7, P10, P13, P15, P17, P19).\nParticipants struggled with creating representative datasets, as found in prior studies [12]. Despite the emphasis on safety and fairness in literature [12, 31], the prompt program work less well on culturally diverse audiences upon deployment (P7, P10, P13). This reflects prior research in NLP, which found that FMs and datasets align with certain demographics (i.e., Western, college-educated, and younger populations) more than others [39, 60, 63]. Additionally, participants found aspects of manual data annotation challenging. Some participants (P3, P13) described challenges on determining labels for the dataset. Participants noted instances of low-quality data annotations:"}, {"title": "5.1.6 Evaluation", "content": "Prompt programmers engage in frequent evaluation of the prompt program.\nEvaluating prompt programs requires assessing qualitative constructs. Many participants' tasks often involved custom tasks that did not have a clear", "58": ".", "71": "while prompt engineering includes manual inspection [30", "54": ".", "quantify the concept of good": "P10) for outputs. For example", "tool": "I would just sit in the chair and talk to my phone for like 10 minutes just to fine-tune and see, \u2018Okay, does it ask me the right question? Does the summary make sense?\u201d", "P15)": "It is extremely hard to realize"}]}