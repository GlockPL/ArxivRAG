{"title": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning", "authors": ["Wenkai Yang", "Shuming Ma", "Yankai Lin", "Furu Wei"], "abstract": "Recent studies have shown that making a model spend more time thinking through longer Chain of Thoughts (CoTs) enables it to gain significant improvements in complex reasoning tasks. While current researches continue to explore the benefits of increasing test-time compute by extending the CoT lengths of Large Language Models (LLMs), we are concerned about a potential issue hidden behind the current pursuit of test-time scaling: Would excessively scaling the CoT length actually bring adverse effects to a model's reasoning performance? Our explorations on mathematical reasoning tasks reveal an unexpected finding that scaling with longer CoTs can indeed impair the reasoning performance of LLMs in certain domains. Moreover, we discover that there exists an optimal scaled length distribution that differs across different domains. Based on these insights, we propose a Thinking-Optimal Scaling strategy. Our method first uses a small set of seed data with varying response length distributions to teach the model to adopt different reasoning efforts for deep thinking. Then, the model selects its shortest correct response under different reasoning efforts on additional problems for self-improvement. Our self-improved models built upon Qwen2.5-32B-Instruct outperform other distillation-based 32B 01-like models across various math benchmarks, and achieve performance on par with QwQ-32B-Preview.", "sections": [{"title": "1 Introduction", "content": "Recently, System-2 thinking [38] has become an important research area for enhancing the reasoning capabilities of Large Language Models (LLMs). Unlike previous System-1 thinking systems [39, 31] that perform fast thinking, such a slow thinking system aims to increase the test-time compute of LLMs to make them think more thoroughly before responding to a question. OpenAI' ol model [25] has demonstrated a promising potential in this direction. By incentivizing the model to employ longer internal Chain of Thoughts (CoTs) [36] for thinking, 01 shows human-like reasoning capabilities, including searching, reflecting, backtracking, and re-exploring in its reasoning process, and achieves outstanding performance on complex reasoning tasks [13, 29].\nSubsequently, a series of follow-up studies [27, 32, 11] have been proposed to imitate and explore 01-like thinking systems. These studies try to scale the number of reasoning tokens of LLMs either by distilling from existing ol-like models [14, 22, 23] or reinforcement learning [6, 15], and gain significant improvements compared to earlier reasoning models [39, 31]."}, {"title": "2 Related Work", "content": "LLM Reasoning Leveraging the Chain-of-Thought (CoT) technique [36, 44], LLMs have demonstrated impressive performance on various reasoning tasks [30, 29, 31]. CoT enables LLMs to decompose the entire problem into several sub-goals and then reason step-by-step to achieve a more reliable answer. Among various LLM reasoning tasks, mathematical reasoning has become one of the most widely studied and important tasks. Current work on LLM math reasoning primarily focuses on: synthesizing large-scale and diverse math data [42, 41, 17], constructing challenging math reasoning benchmarks [8, 9], training powerful process reward models (PRMs) [19, 34, 32], and designing more effective alignment or reinforcement learning algorithms to improve math reasoning capabilities of LLMs [16, 12, 5].\nTest-Time Scaling Recently, scaling test-time compute of LLMs has shown significant potential for further improving their reasoning performance [1, 2]. Existing test-time scaling studies can be divided into several categories: (1) Sampling-based scaling aims to increase the number of individual reasoning paths of LLMs when solving a given problem. Then the most reliable answer is selected from all the generated options using mechanisms such as majority voting [35], weighted"}, {"title": "3 The Impact of Scaling Efforts on the Effectiveness of Test-Time Scaling", "content": "Though 01-like models has proven to be much more effective on reasoning tasks than previous System-1 thinking models, we are curious about the scaling process behind the these 01-like models. That is, we want to explore that: How effective has their scaling achieved compared to their corresponding System-1 thinking models(e.g., QwQ-32B-Instruct v.s. Qwen2.5-32B-Instruct)?\nWe first conduct a preliminary analysis on several existing typical 01-like models along with their corresponding System-1 thinking models. Specifically, we choose ol-mini [25] v.s. gpt-40/40-mini [24], Gemini2.0-Flash-Thinking-Exp.-1219 [11] v.s. Gemini2.0-Flash-Exp. [10], and QwQ-32B-Preview [27] v.s. Qwen2.5-32B-Instruct [26] as our experimental models. We calculate the accuracy and the average number of generated tokens of each model on two typical benchmarks: MATH500 [19]: 500 high school math competition problems across various subjects, sampled from MATH benchmark [13]; AIME2024: 30 challenging problems from the American Invitational Mathematics Examination (AIME). To address the issue of token counts not being directly comparable due to the different tokenizers used by different models, we standardize by using Qwen2.5 tokenizer to tokenize the reasoning completions of different models and then calculate the number of tokens."}, {"title": "3.2 Deeper Explorations on The Scaling Process of CoT Length", "content": "The above analysis still faces a problem that the base models of different o1-like models are not identical, making it unfairly to compare the impacts of scaled CoT lengths on test-time scaling effects of different models. Therefore, we conduct experiments on LLaMA3.1-8B-Instruct [21] and Qwen2.5-32B-Instruct [26] to fairly investigate the impact of the scaling efforts on the effectiveness of test-time scaling. Specifically, we first use three system prompts (refer to Figure 2), corresponding to different levels of reasoning effort (\u201cLow\u201d, \u201cMedium\u201d and \u201cHigh\u201d), to prompt QwQ-32B-Preview to generate solutions of different numbers of tokens for the same set of math problems sampled from NuminaMath [17]. We then filter out the problems that can be answered correctly under all three reasoning efforts, along with the corresponding three reasoning paths of different lengths. However, we find that QwQ-32B-Preview has relatively poor instruction-following abilities, reflected in that the length distributions of the generated responses for the same question does not closely match the specified system prompts. Therefore, for a given problem, we further reorder the three responses based on their lengths and keep them if their pairwise length difference consistently exceeds 300 tokens. The length is determined either by LLaMA3.1 tokenizer or Qwen2.5 tokenizer depending"}, {"title": "3.3 Analysis on The Adverse Effects of Excessive Length Scaling", "content": "Here, we take a deeper step to explore why training on longer CoTs leads to a decline in model's reasoning performance. We randomly selected 100 problems from the tag model's training set along with their responses under three different reasoning efforts. We first follow the existing study [3] to use gpt-40 to determine the total number of reasoning rounds contained in each response. Each reasoning round is defined as a complete reasoning process or verification process that contains a final answer. Besides, we further use gpt-40 to determine the number of reasoning rounds that contain erroneous steps or wrong final answers. The utilized prompt is shown in Appendix B. We visualize the average number of reasoning rounds and the average number of erroneous reasoning rounds on each problem under each reasoning effort in Figure 5.\nFirst, we can see that the number of reasoning rounds consistently increases from low reasoning effort to high reasoning effort. It could lead to the overthinking issue in reasoning models [3], where redundant tokens are used to answer the question through repetitive verification. This, however, should not affect the accuracy if all reasoning rounds are correct. Unfortunately, we observe a pattern that the number of erroneous reasoning rounds also increases when the reasoning effort becomes higher. Training the model on more wrong steps would bring adverse effects to the model's reasoning abilities, which can explain why scaling with high reasoning effort leads to worse results. Therefore, we can conclude that while including a certain incorrect and reflective steps can help the model learn to correct errors during inference, an excess of erroneous steps can have a detrimental impact on model's learning."}, {"title": "4 Thinking-Optimal Test-Time Scaling", "content": "The above analysis reveals that excessively increasing the response lengths of LLMs can result in negative consequences. This motivates us that an optimal approach to achieve test-time scaling is allowing the model to determine by itself the number of tokens needed to solve each problem. Specifically, for a simple question, if the model can provide a correct answer within a certain number of tokens, further extending the CoTs becomes suboptimal, as it may introduce unnecessary overthinking or even additional erroneous steps into the reasoning process. Conversely, the model should be encouraged to use more tokens for difficult problems if additional reasoning effort can help it to obtain a more reliable and accurate answer.\nThus, we propose a Thinking-OPtimal Scaling (TOPS) strategy aiming to achieve more effective and efficient test-time scaling for LLM reasoning. We define a System-2 thinking-optimal response as the shortest correct response that the model can generate using System-2 thinking: fewer tokens may lead to wrong answer while more tokens causes overthinking. Then, our method includes three stages: Format Imitation, Reasoning Effort-Conditioned Generation, and Self-Improvement.\nFormat Imitation First, we require a small set of o1-like responses for a cold start, enabling the model to learn the format of System-2 thinking patterns, including searching, reflecting, verification, backtracking, etc. Preliminary results in Section 3.2 and previous studies [22, 14] have shown that a small number of o1-like responses is sufficient for the model to effectively learn the format of System-2 reasoning. Such a small set of seed data can be manually human-written or generated by existing o1-like models. However, different from previous studies [22, 23] that use a fixed length distribution of seed samples (i.e., directly generated by existing o1-like models), which may not be a thinking-optimal distribution for our base model, we instead create the seed data containing responses under different reasoning efforts (i.e., different length distributions). Specifically, we define a small number of seed problems as \\(P_s\\), and our goal is to curate a seed dataset \\(D_s = D_{s_1} \\cup \\dots \\cup D_{s_n}\\),"}, {"title": "Reasoning Effort-Conditioned Generation", "content": "We then use the tag model to generate the solutions on a large number of additional math problems \\(P_a\\) under different reasoning efforts \\(e_i\\):\n\\[Y_{e_i} \\sim \\pi(C_i, x\\vert\\theta_{tag}), \\quad x \\sim P_a,\\]\nwhere \\(\\pi(\\cdot\\vert\\theta_{tag})\\) denotes the output distribution of the tag model. We select the shortest correct solution \\(y_{sc}\\) among all generations \\(\\{Y_{e_1},, Y_{e_n} \\}\\) as the thinking-optimal response for problem \\(x\\), and obtain a thinking-optimal self-improvement dataset \\(D_{TOP} = \\{(x, y_{sc})\\vert x \\sim P_a\\}\\).\nSelf-Improvement After obtaining the thinking-optimal dataset determined by the model itself, we can use it to train the base model, enabling the base model to achieve better self-improvement on System-2 thinking. Specifically, we perform Supervised Fine-Tuning (SFT) to the base model on \\(D_{TOP}\\):\n\\[\\theta_{TOP} = \\underset{\\theta}{\\text{arg max}} \\mathbb{E}_{(x, y_{sc})\\sim D_{Top}}[P(y_{sc}\\vert x, \\theta)].\\]"}, {"title": "5 Experiments and Analysis", "content": "Base Model We perform our Thinking-Optimal Scaling strategy mainly on Qwen2.5-32B-Instruct, as it serves as an appropriate base model for exploration on test-time scaling according to previous works [27, 22, 23].\nDatasets First, we directly use the model Qwen2.5-32B-Tag created in Section 3.2 as the tag model for reasoning effort-conditioned generation. As mentioned before, this tag model is trained on the seed data that contains 1.3K problems from a subset of NuminaMath, and totally 3.9K responses under three types of reasoning efforts generated by QwQ-32B-Preview. We then use this tag model to generate responses on an additional subset of NuminaMath containing extra 50K problems under"}, {"title": "5.2 Main Results", "content": "The results of each model are displayed in Table 2. Besides the accuracy, we also report the number of CoT tokens used by each model on each dataset for reference."}, {"title": "5.3 Results of Iterative Self-Improvement", "content": "To further enhance the reasoning performance of our model on challenging problems, we perform iterative self-improvement on Qwen2.5-32B-TOPS. Specifically, we select additional 4500 MATH problems [13] (which have not appeared in the previously used problems) and the problems from AIME1983-2023. On each problem, we sample 8 responses from Qwen2.5-32B-TOPS. Then, we select the shortest correct response among 8 responses as the chosen response. One iterative self-improvement approach is to further supervised fine-tune Qwen2.5-32B-TOPS on the dataset composed of all chosen responses (shortest correct responses), resulting in Qwen2.5-32B-TOPS-Iter-SFT. Besides, we can also perform preference optimization. Specifically, if there are responses with incorrect final answers, we select the longest incorrect response as the rejected response to improve reasoning capability. Additionally, we include preference pairs where the rejected response is the shortest wrong response if there exists a wrong response that is shorter than the shortest correct response, to avoid causing the model to underthink. After obtaining the preference dataset, we perform Direct Preference Optimization (DPO) [28] on Qwen2.5-32B-TOPS to get Qwen2.5-32B-TOPS-Iter-DPO. Detailed experimental settings are in Appendix C.\nThe results of iterative self-improvement are in Table 2. As we can observe, further SFT is mainly effective in shortening the CoT lengths but does not necessarily improve reasoning performance. Preference optimization improves both the efficiency and the effectiveness, resulting in a reasoning model that is comparable to QwQ-32B-Preview."}, {"title": "6 Conclusion", "content": "In this work, we explore a potential issue under current pursuit of test-time scaling. Through empirical analysis on mathematical reasoning tasks, we first demonstrate that overly long CoTs can negatively impact the model's reasoning performance in certain domains, emphasizing the need for an optimal length distribution during CoT length scaling. To tackle this, we propose a Thinking-Optimal Scaling strategy, which leverages a small set of seed data to teach LLMs to adopt varying levels of reasoning effort to perform System-2 thinking. Then, our approach allows models to identify the shortest correct response for self-improvement, leading to a more efficient and effective System-2 thinking. Experimental results show that our self-improved and further iteratively self-improved models, based on Qwen2.5-32B-Instruct, outperform existing distillation-based 01-like models and achieve performance that is on par with QwQ-32B-Preview across various math benchmarks."}, {"title": "Impact Statement", "content": "This work aims to explore the effectiveness and limitations of ol-like test-time scaling. Our goal is to enhance both the efficiency and effectiveness of test-time scaling in a more thinking-optimal way. These findings highlight the importance of adaptive reasoning efforts and provide a promising direction for more effectively enhancing LLM reasoning capabilities."}, {"title": "A Details on Estimating The Number of Tokens in Hidden CoT of o1-mini by Qwen2.5 Tokenizer", "content": "In the preliminary analysis in Section 3.1, we use Qwen2.5 tokenizer to calculate the number of tokens in the CoTs generated by each model for a fair comparison. However, o1-mini does not expose the internal CoTs to users, but only shows the summary parts S, along with the number of reasoning tokens \\(n_{ol}^{s}\\) and total number completion tokens \\(n_{ol}^{c}\\) (the sum of reasoning tokens and summary tokens) measured by 01-mini tokenizer. Therefore, we choose to estimate the number of tokens in its hidden CoTs measured by the Qwen2.5 tokenizer using S, \\(n_{ol}^{s}\\) and \\(n_{ol}^{c}\\). Specifically, we denote the number of tokens of the summary part measured by Qwen2.5 tokenizer as \\(n_{qwen}^{s}\\), then the estimated number of tokens of hidden CoT by Qwen2.5 tokenizer can be calculated as \\(n_{qwen}^{c} = \\frac{n_{ol}^{s}}{n_{ol}^{c}}n_{qwen}^{s}\\)."}, {"title": "B User Prompt for gpt-40 to Determine the Number of (Erroneous) Reasoning Rounds", "content": null}, {"title": "C Experimental Settings", "content": null}, {"title": "C.1 Training Settings in Format Imitation", "content": "In the format imitation stage, we perform SFT on the base model Qwen2.5-32B-Instruct on a small subset of seed data containing 1.3K problems sampled from NuminaMath along with responses with varying lengths for each problem. The statistics of the seed data is shown in Table 1. In SFT stage, the learning rate is \\(1 \\times 10^{-5}\\), the batch size is 32, the number of epochs is 3."}, {"title": "C.2 Training Settings in Self-Improvement", "content": "In the self-improvement stage, we perform SFT on Qwen2.5-32B-Instruct on the curated thinking-optimal dataset for 2 epochs. The learning rate is \\(1 \\times 10^{-5}\\), and the batch size is 96."}, {"title": "C.3 Training Settings in Iterative Self-Improvement", "content": "In the iterative self-improvement stage, for Qwen2.5-32B-TOPS-Iter-SFT, the learning rate is \\(1 \\times 10^{-6}\\), the batch size is 32, and we set the training epoch to 1. For Qwen2.5-32B-TOPS-Iter-DPO, the learning rate is \\(5 \\times 10^{-7}\\), the batch size is 32, the training epoch is 3."}, {"title": "C.4 Evaluation Settings", "content": "For all o1-like models, we set the decoding temperature to 1.0 and average the results over 5 random seeds for each evaluation experiment. The maximum generation length is 16,384."}]}