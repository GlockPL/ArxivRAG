{"title": "Poetry2Image: An Iterative Correction Framework for Images Generated from Chinese Classical Poetry", "authors": ["Jing Jiang", "Yiran Ling", "Binzhu Li", "Pengxiang Li", "Junming Piao", "Yu Zhang"], "abstract": "Text-to-image generation models often struggle with key element loss or semantic confusion in tasks involving Chinese classical poetry. Addressing this issue through fine-tuning models needs considerable training costs. Additionally, manual prompts for re-diffusion adjustments need professional knowledge. To solve this problem, we propose Poetry2Image, an iterative correction framework for images generated from Chinese classical poetry. Utilizing an external poetry dataset, Poetry2Image establishes an automated feedback and correction loop, which enhances the alignment between poetry and image through image generation models and subsequent re-diffusion modifications suggested by large language models (LLM). Using a test set of 200 sentences of Chinese classical poetry, the proposed method-when integrated with five popular image generation models-achieves an average element completeness of 70.63%, representing an improvement of 25.56% over direct image generation. In tests of semantic correctness, our method attains an average semantic consistency of 80.09%. The study not only promotes the dissemination of ancient poetry culture but also offers a reference for similar non-fine-tuning methods to enhance LLM generation.", "sections": [{"title": "1 Introduction", "content": "Text-to-image generation combines natural language understanding with image generation models, which synthesize realistic images conditioned on natural language descriptions. When text-to-image generation models deal with prompts requiring professional knowledge, such as Chinese classical poetry, they are prone to losing key elements or causing semantic confusion. It is challenging to accurately describe the precise meaning of poetry as illustrated in Fig. 1."}, {"title": "2 Related Works", "content": "2.1 Text-to-Image Generation\nText-to-image generation is the task of synthesizing images conditioned on natural language prompts. Recent advancements in diffusion models (Sohl-Dickstein et al., 2015; Dhariwal and Nichol, 2021; Song et al., 2020) have significantly improved the quality of text-to-image generation, such as Dreambooth (Ruiz et al., 2023) and DALL-E 3 (Betker et al., 2023). Despite their impressive visual quality, these models struggle with complex prompts, which tend to generate images lacking core semantic elements and cause semantic confusion (Feng et al., 2022; Lian et al., 2023; Bar-Tal et al., 2023). Some recent studies (Xie et al., 2023; Yang et al., 2023; Lian et al., 2023) incorporate bounding boxes as conditional controls to the diffusion process. Several recent papers (Huang et al., 2023; Xu et al., 2024; Fang et al., 2023) leverage image understanding feedback, which builds a general-purpose reward model to refine diffusion models for text-image alignment. Despite their progress, there are two limitations in handling image generation with complex prompts: (i) open-loop generation in a single iteration cannot guarantee the alignment between generated images and prompts; (ii) these methods result in additional training costs. To address these issues, we introduce a training-free cyclic self-correction framework to enhance the alignment of images with complex prompts.\n2.2 Text-Guided Image Editing\nText-guided image editing synthesizes images from a given image and text descriptions. Classic image editing aims at fine-grained manipulation by in-painting masked regions while keeping the remaining areas. Studies (Avrahami et al., 2022; Meng et al., 2021) show that using user-generated masks for spatial editing in image generation is a straightforward yet effective method. Another method (Balaji et al., 2022; Hertz et al., 2022) focuses on predicted masks for spatial editing, demonstrating that manipulating image-text cross-attention masks is also effective. Evolved from spatial editing, text-guided image editing (Brooks et al., 2023; Kawar et al., 2023) accepts direct commands, allowing editing without regional masks. Despite some progress, these works mainly focus on diffusion models but often suffer from complicated prompts or image understanding. Recent advancements have demonstrated the capabilities of incor-"}, {"title": "3 Method", "content": "In this section, we introduce the iterative correction framework for poetry generation, as shown in Fig. 2. Compared to common texts, poetry is semantically implicit. Firstly, the implicit semantic elements should be extracted. Secondly, an image semantic error correction mechanism should be established to alleviate potential semantic inconsistencies in the images.\n3.1 Extract Implicit Semantics Based on LLM\nDataset construction should consider the meanings of poems, completeness of translation annotations, and cultural popularity. We consider rhetorical techniques involved in literature with semantic implicit features such as metaphor, personification, hyperbole, and allusion. Then, we allocate 200 well-known sentences with their modern Chinese translations, keyword annotations, and phrase explanations from the largest public platform of Chinese classical poetry, GuShiWen.com\u00b9."}, {"title": "Algorithm 1 Key Elements Extraction", "content": "Input: Poetry p; Poetry Database S\n1: dmin \u2190 0\n2: Pfind = \u00d8 // Query list initialization\n3: for i = 1 to N do\n4: d = Fsimularity(p, S[i])\n5: if d \u2264 dmin then\n6: dmin \u2190 d\n7: Pfind.append(S[i])\n8: end if\n9: end for\n10: tfind = FTranslation (Pfind)\n11: Nfind = FAnnotation(Pfind)\n12: Ekey = LLMextract(Pfind, t find, N find)\n13: Initial image: Porigin = Diff (t find)\nOutput: Key elements Ekey; Initial image Porigin\nData extension involves processing the dataset to match the input features required for detection and ensuring generality for prompts from different image generation models. Key elements of the poetry are extracted along with their translations, appreciations, and annotations, to facilitate monitoring of the elements completeness in the generated images. To automate the extraction process and achieve high extraction accuracy, we use GPT-4 for key element extraction, and design prompts for the LLM, as illustrated in Fig. 3."}, {"title": "Algorithm 2 Image Feedback Correction", "content": "Input: Key elements Ekey; Initial image Porigin\n1: key elements detection: E' \u2190 0\n2: Pfeedback\u2190 Porigin\n3: while E' \u2260 Ekey do\n4: L' = OVD(Pfeedback)\n5: L\" = LLMsuggester (L', Ekey)\n6: L'\" = LLMtransform(L\")\n7: Pfeedback = Diff(L'\")\n8: E' \u2190 OVD(Pfeedback)\n9: end while\n10: return Pout \u2190 Pfeedback\nOutput: Final generated image Pout\nL represents a list of intermediate calculation results. E represents a list of key elements in the semantics of poetry.\nFor all the bounding boxes in the image, the element labels are compared with the results from the LLM Extractor to determine whether a bounding box need to be retained or modified. This is discussed in the following scenarios:\n1. Retain: Keep the bounding box unchanged if it is included in the LLM Extractor's result, which is key elements.\n2. Remove: Delete the bounding box based on the LLM analysis of the poetic imagery.\n3. Add: If an key element from the LLM Extractor's result is missing in the current generated image, the LLM selects a new rectangular area and adds the missing key element label based on the poetry translation."}, {"title": "Evaluation", "content": "Evaluation involves the elemental completeness and the semantic consistency in the poetry generated image, and we establish an image-text consistency evaluation model, as shown in Eq. 1.\narg maxs,e \u03b1()+\u03b2()\n\u03b1 + \u03b2(1)\nis the quantitative measure for assessing generated images of ancient poems, considering semantic features s and key elements e, with upper thresholds se and ee respectively. Linear parameters a and \u03b2 dictate the focus: \u03b2 = 0 evaluates semantic compliance, while a = 0 evaluates key elemental completeness."}, {"title": "4 Experiment", "content": "4.1 Key Elements Extraction\nIn our image generation process, the initial stage uses LLM Extractor to semantically extract key elements from the database corpus. The accuracy of LLM Extractor is crucial to the subsequent process and needs to be evaluated in detail.\nSettings. We select a dataset of 200 Chinese poems with implicit semantics and manually annotated them to establish a benchmark for element extraction. The poems are then processed using five LLMs: GPT-4-Turbo (Achiam et al., 2023), GPT-3.5-Turbo (Brown et al., 2020), Claude-3 (Anthropic, 2024), GLM-4 (Zeng et al., 2023), and"}, {"title": "4.2 Verification of Self-Correcting Cycles Across Different Generation Models", "content": "Settings. We evaluate our image error correction method using elemental completeness and semantic consistency. Poetry2image is applied to five text-to-image generation models to validate the effect: DALL-E-3 (Peebles and Xie, 2023), CogView3 (Zheng et al., 2024), Midjourney, Wenxin Yige,"}, {"title": "4.3 Comparison of Iteration Rounds", "content": "Settings. To ensure the improvement in elemental completeness and ascertain the maximum efficacy of our method, we conduct an iterative comparison experiment. Observation points are established within the automated process, sequentially recording improvements in elemental completeness as the number of image iteration rounds increased."}, {"title": "4.4 Ablation Experiment", "content": "Settings. In order to verify the effect of the initial generation on correction, we perform an ablation experiment, removing additional information such as translations and annotations, and directly utilize the original text of the poems for generation.\nResult. The results of the experiment show in Tab. 4. The completeness of the initial generation remains largely unchanged after eliminating additional information, while after detection and correction the elemental completeness decreases by approximately 11%. This is because most elements are derived directly from the poem's text, so the initial generation's completeness is unaffected. However, images generated directly based on poems lack much of the additional semantics from the additional information, due to factors such as lack of stylistic richness, which reduces the completeness of the picture elements, and subsequent modifications can be much less effective."}, {"title": "5 Discussion", "content": "5.1 The Influence of the Number of Key Elements in Poetry\nTo evaluate the performance of Poetry2Image in processing Chinese classical poetry with different numbers of key elements, we design a series of experiments and use elemental completeness as the evaluation indicator.\nThe experimental results, as illustrated in Tab. 5, indicate that with fewer key elements, such as 3, the initial generation covers most elements, resulting in minimal improvement in overall elemental completeness. As the number of key elements increases, the initial generation's missing rate escalates. However, Poetry2Image compensates for this by completing elements more rapidly than they are missed, resulting in a 15% to 20% improvement in elemental completeness. Specifically, for information-intensive poems containing up to six elements, the elemental completeness improvement rate reaches 23.73%. This demonstrates Poetry2Image's efficacy in improving elemental completeness. However, when the number of elements exceeds seven, the image fails to achieve the desired elemental completeness, posing a challenge in balancing the aesthetics of the image with the improvement of elemental completeness.\n5.2 The Influence of Poetry Language Types\nTo further assess the generalizability and applicability of Poetry2Image, we extended its application to multilingual poetry. We test Poetry2Image on 100 classical Japanese and English poems representing diverse linguistic and cultural backgrounds.\nThe results, as detailed in Appendix A, demonstrate that our method is effective not only with Chinese classical poetry but also with Japanese and English poetry.This confirms the wide applicability of Poem2Image and provides insights into generating images of multilingual poetry."}, {"title": "6 Limitations", "content": "The limitations of Poetry2Image stem from the intrinsic characteristics of poetry, as illustrated in Fig. 6. The poems corresponding to the images can be found in Appendix B. When the genre of the poem is lyrical or didactic, the key elements in the sentence are scarce or abstract, so both the initial image and the corrected image fail to capture the key elements used for generation, leading to unsatisfactory correction results. In addition, when dealing with proper nouns such as historical personal names (e.g. 'Zhou Yu') in the poems, the elements cannot be recognized and understood by the OVD and diffusion models, resulting in suboptimal correction results."}, {"title": "7 Conclusion", "content": "We propose Poetry2Image, an iterative correction framework that integrates image generation, error correction and feedback. This framework enhances image generation quality for specialized texts like Chinese classical poetry and addresses core issues such as element loss and semantic confusion. Our method is adept at element-rich or multi-lingual poems and is compatible with other image generation models.Additionally, our approach provides a reference for similar non-fine-tuning methods to enhance LLM generation."}, {"title": "Appendix", "content": "A Results of Poetry Image Correction in Multiple Languages\nPoetry examples in different languages and test results of Poetry2Image are shown below.\n1. Japanese Haiku: The moon in the water; Broken and broken again, Still it is there.\n2. American English Poetry: On the beach at night alone, As the old mother sways her to and fro singing her husky song, As I watch the bright stars shining, I think a thought of the clef of the universes and of the future.\n3. British English Poetry: O wild West Wind, thou breath of Autumn's being Thou, from whose unseen presence the leaves dead Are driven, like ghosts from an enchanter fleeing, Yellow, and black, and pale, and hectic red.\nInitially, for Japanese poetry, we chose the renowned haiku of Matsuo Basho for analysis. Our method accurately identified the metaphor of a 'broken moon in the water' and appropriately adjusted the image from a moon in the sky to reflect this. Subsequently, for English poetry, we tested poems by Whitman and Shelley. The results indicate that our method effectively interprets and corrects metaphors such as 'old mother' and 'ghosts'.\nB Poetry Text for Generating Images\nPoetry Text for Generating Fig. 5.\nPoetry a: Singing loudly in front of the wine, life is short and the days pass by quickly.\nPoetry b: The capital is filled with nobles in fine cars and beautiful clothes, but you are extremely talented but your face is haggard.\nPoetry c: The sparse shadows of plum blossoms are reflected obliquely in the clear water, and the faint fragrance of plum blossoms is drifting in the hazy moonlight.\nPoetry d: The Xianglu Peak is covered with purple haze under the sunlight, and from a distance you can see a waterfall hanging in front of the mountain like white silk.\nPoetry e: I am facing a cup of sad wine, thousands of miles away from home. I have a lot of thoughts, thinking about the unrest on the border, the unfinished work, and I don't know when I can return to my hometown.\nPoetry f: I stopped the carriage just because I loved the maple forest in the evening. The frost-stained maple leaves are more beautiful than the bright flowers in February.\nPoetry Text for Generating Fig. 6.\nPoetry a: Without the help of the east wind, Jiangnan would have been a ruin; the beautiful Erqiao would have been locked up in the Tongque Tower forever.\nPoetry b: The people back then are no longer around, but the Yishui River is still as cold today.\nC System Prompt Setup of Extractor and Suggester in Our Method\nWe use a recognition method based on open vocabulary detector to detect key elements of poems and an automatic iterative correction framework to generate images through secondary diffusion. The system prompt setup of our extractor and suggester are shown below."}]}