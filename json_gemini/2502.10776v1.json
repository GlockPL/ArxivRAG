{"title": "A Distillation-based Future-aware Graph Neural Network for Stock Trend Prediction", "authors": ["Zhipeng Liu", "Peibo Duan", "Mingyang Geng", "Bin Zhang"], "abstract": "Stock trend prediction involves forecasting the future price movements by analyzing historical data and various market indicators. With the advancement of machine learning, graph neural networks (GNNs) have been extensively employed in stock prediction due to their powerful capability to capture spatiotemporal dependencies of stocks. However, despite the efforts of various GNN stock predictors to enhance predictive performance, the improvements remain limited, as they focus solely on analyzing historical spatiotemporal dependencies, overlooking the correlation between historical and future patterns. In this study, we propose a novel distillation-based future-aware GNN framework (DishFT-GNN) for stock trend prediction. Specifically, DishFT-GNN trains a teacher model and a student model, iteratively. The teacher model learns to capture the correlation between distribution shifts of historical and future data, which is then utilized as intermediate supervision to guide the student model to learn future-aware spatiotemporal embeddings for accurate prediction. Through extensive experiments on two real-world datasets, we verify the state-of-the-art performance of DishFT-GNN.", "sections": [{"title": "I. INTRODUCTION AND RELATED WORK", "content": "The stock market is a financial investment platform where numerous corporations and investors engage in trading [1], [2]. As of 2022, the global market capitalization increased from $54.6 trillion to $94.69 trillion over the past decade\u00b9. With the advancement of artificial intelligence, deep learning (DL) techniques provide more opportunities for investors to increase their wealth through stock investment [3], [4]. For an auxiliary investment tool, it is non-trivial to make profitable investment decisions and trading strategies within the volatile market [5].\nTypically, DL-based stock trading methods are based on the fundamental hypothesis that a stock's future dynamics can be revealed from its historical patterns [6]\u2013[10]. Thus, it is essential to perform a meticulous analysis of latent temporal dynamics within historical stock price indicators, as exemplified by methods based on recurrent neural networks (RNN) [11], [12], as well as its variants LSTM or GRU [13]\u2013[15]. Drawing upon the temporal dynamic feature extraction capabilities of RNN-based methodologies, graph neural networks (GNNs) construct stock graphs that facilitate the incorporation of explicitly relational (spatial) dependencies between stocks, such as industry and shareholding information [16]\u2013[19]. In this framework, the nodes denote individual stocks with attributes derived from the aforementioned RNN-based models, while the edges represent the inter-stock relationships. Recent studies have found that relying solely on explicit relationships can lead to biased and incomplete aggregation of relational features. As a result, there is growing interest in investigating implicit relationships using DL-based graph representation methods to better understand stock interactions [20]\u2013[24].\nGoing beyond the above mentioned studies, we further observe that stock data exhibits highly non-stationary characteristics, making it more challenging to forecast compared to other time series data. As shown in Fig.1, an annual report from a corporation showing better-than-expected profits can cause distribution shift between historical and future data, which results in sub-optimal performance in contemporary stock predictors that rely exclusively on historical price indicators, with their impact becoming apparent only in hindsight. Thus, we are motivated to believe that not only are the features in historical stock data essential for analysis, but also the correlation between distribution shifts of historical and future data. For further proof, please refer to the supplementary material provided due to space constraints\u00b2.\nTo this end, we propose a novel general Distillation-based Future-aware GNN framework (DisFT-GNN), to capture the correlation between historical and future patterns for stock trend prediction. In theory, DisFT-GNN can be integrated with any GNN-based stock predictor, enhancing their predictive performance. The contributions in this study are summarized as follows: i) Unlike existing knowledge distillation-based time series forecasting methods [25], [26], which utilize the teacher model to produce historical pattern-related soft labels or features for guiding the training of a lightweight student model, DisFT-GNN introduces a novel teacher model that has undergone converged training to generate high-level, future-aware representations, which serve as intermediate supervision, guiding the student model in learning correlations between historical and future distributions. ii) As the future prices can be influenced by various factors and exhibit different patterns. We introduce a novel attention-based multi-channel feature fusion method in the teacher model, which models the diverse distribution shifts between historical and future data. iii) Through extensive experiments on two real-world datasets from the American stock market, our proposed DisFT-GNN consistently boosts current state-of-the-art models, achieving an average improvement of up to 5.41%."}, {"title": "II. PROBLEM FORMULATION", "content": "We formulate stock trend prediction as a binary node classification task. A dynamic stock graph at trading day t can be defined as $G_t = {V, X_{[t-L+1,t]}, A_t}$, where $V = {v_1, v_2, ..., v_N }$ is the set of N individual stocks, $X_{[t-L+1,t]} \\in R^{N \\times L \\times M}$ represents M price indicators over the historical L trading days. $A_t \\in R^{N \\times N}$ is the normalized relation matrix representing the relationship between stocks. Mathematically, the problem is formulated as:\n$\\hat{y}_{[t+1,t+T]} = f(g(X_{[t\u2212L+1,t]}, A_t; \\Theta_g); \\Theta_f)$.\nIn (1), g(\u00b7) is a GNN-based model with parameters $\\Theta_g$, aiming to capture spatiotemporal features between stocks; f(\u00b7) is the prediction layer with parameters $\\Theta_f$; $\\hat{y}_{[t+1,t+T]} = {\\hat{y}^{t+1}_{[t+1,t+T]}, \\hat{y}^{t+2}_{[t+1,t+T]}, ..., \\hat{y}^{t+T}_{[t+1,t+T]}}$ is the set of output binary variables. For $\\forall r \\in [t + 1,t +T]$, $\\hat{y}_n^r = 1$ predicts that the n-th stock will increase, and 0 otherwise. For $u_n \\in V$, the ground-truth label is defined as follows:\n$y_n^r = \\begin{cases}\n1 & \\text{if } p^r_n > p^t_n + \\delta\\\\\n0 & \\text{else}\n\\end{cases}$,\nwhere $p_n^\\tau$ is the stock close price at trading day $\\tau$, $\\delta$ is the hyperparameter. Notably, when $T = 1$ and $\\delta = 0$, the problem is the next-trading day stock trend prediction [27]."}, {"title": "III. METHODOLOGY", "content": "The overview of DisFT-GNN is illustrated in Fig.2. DisFT-GNN contains two training processes: training a teacher model and a student model, respectively. First, the teacher model encodes the historical stock graph and future trend information, generating historical spatiotemporal and high-level future embeddings. Subsequently, a novel attention-based multi-channel feature fusion method is proposed to integrate these embeddings to generate future-aware spatiotemporal representations, which depict the diverse historical-future distribution shifts. The representations are then fed into the prediction module to obtain predicted results, which are compared with the ground truth to optimize the teacher model. Subsequently, the future-aware spatiotemporal representations from the teacher model serve as intermediate supervision to guide the student model in learning historical-future distribution correlations.\nBefore introducing the details of the proposed method, we first introduce several general components for stock prediction.\nSpatiotemporal GNN module. Existing stock prediction methods follow a fundamental hypothesis that a stock's future prices can be inferred from its historical patterns (temporal dependence) [13] and the behavior of related stocks (spatial dependence) [20]. For $u_n \\in V$, the spatiotemporal embeddings can be calculated as $h_n^t = ST(X_{[t-L+1,t]}, A_t)$, specifically,\n$s_n^t = Temporal(X_{[t-L+1,t]})$, \n$h_n^t = Spatial(s_v^k \\in N_n)$,\nwhere $N_n$ denotes the set of stocks related to $u_n$. It is noted that ST(\u00b7) can be any spatiotemporal GNN model.\nFuture Trend Encoder. It is a feed-forward network (FNN) to encode the stock future trends, generating a novel future trend embedding, $q_n^{t+}$. Since future trend information can intuitively reflect prediction results, it is unnecessary to design intricate modules to analyze future price indicators.\n$q_n^{t+} = ReLU(FFN(f_{[t:t+T]}))$,\nwhere $f_{[t:t+T]} \\in {0, 1}$ represents the future trend of $u_n$ over the following T trading days."}, {"title": "B. Future-aware Teacher Model Training", "content": "1) Future-aware Spatiotemporal Encoding: First, the teacher model utilizes a future trend encoder that encodes stock future trends into novel high-level embeddings, $q_n^{t+} = ReLU(FFN(f_{[t:t+T]})), q_n^{t+} \\in R^{D_f}$, and a spatiotemporal GNN module generating historical spatiotemporal embeddings, $p_n^t = ST^{(T)}(X_{[t-L+1,t]}, A_t), p_n^t \\in R^{D_p}$. Subsequently, to model the diverse historical-future distribution shifts, we propose a novel attention-based multi-channel fusion method, integrating $q_n^{t+}$ and $p_n^t$ into high-level future-aware spatiotemporal representations, $h_n^{t+} \\in R^D$.\nSpecifically, we perform multiple vector-matrix-vector (VMV) multiplications [20] to generate a multi-channel result, with each channel representing a potential correlation between historical and future distributions, $V = p_n^tF^{[1:D]}q_n^{t+}$, where $F^{[1:D]} \\in [!]R^{D \\times D_p \\times D_f}$ is the trainable parameter, and D is the number of channels. Next, we utilize the attention mechanism to assess the importance of each channel by assigning an attention score, i.e., $Q = q_n^{t+}W_Q, K = p_n^tW_K$. Finally, similar to Transformer [28], we conduct a scaled dot-product attention operation for feature fusion, which is formulated as,\n$h_n^{t+} = Attention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V$,\nwhere Softmax(\u00b7) is the softmax function, $K_a$ is the scaling factor and $d$ is the attention temperature coeffcient.\n2) Prediction and Optimization: After obtaining $h_n^{t+}$, to forecast the stock trend, $h_n^{t+}$ is fed into the prediction module, which is a feed-forward network with a softmax function.\n$\\hat{y}_n^{[t+1,t+T]} = Prediction(h_n^{t+}) = Softmax(FNN(h_n^{t+}))$\nwhere $y_n^{[t+1,t+T]} \\in {0,1}$ is the predicted result and the softmax function generates a probability distribution over classes.\nFinally, parameters are learned by minimizing the cross entropy loss (CEL), $L_p = CEL(y_n^{[t+1,t+T]}, \\hat{y}_n^{[t+1,t+T]})$."}, {"title": "C. Distillation-based Student Model Training", "content": "To enable the student model with the capability to deduct the future patterns based on historical spatiotemporal embeddings, we utilize the teacher model, which has undergone converged training, to supervise the training of the student model, which includes two phases.\nFirst, the student model encodes the historical stock graph, generating historical spatiotemporal representations, $h_n^t = ST^{(S)}(X_{[t-L+1,t]}, A_t)$. Subsequently, $h_n^t$ is fed into the shared prediction module with the same parameters as the teacher model for prediction,\n$\\hat{y}_n^{[t+1,t+T]} = Prediction(h_n^{t})$.\nSecond, to infer future patterns based on $h_n^{t}$, the student model is trained by minimizing the distillation loss between $h_n^{t}$ and $h_n^{t+}$: $L_d = MSE(h_n^{t}, h_n^{t+})$, where MSE(\u00b7) is the mean square error function and $h_n^{t+}$ is the future-aware spatiotemporal representation distilled from the teacher model. Note that $h_n^{t}$ and $h_n^{t+}$ have the same dimensions. However, since the representations are highly non-linear, it is necessary to focus more on the nonlinear dependence between $h_n^{t}$ and $h_n^{t+}$. Thus, the Hilbert-Schmidt Independence Criterion (HSIC) [29], [30] is utilized as the distillation loss,\n$L_d = HSIC(h_n^t, h_n^{t+}) = (D \u2212 1)^{\u22122}tr(KHLH)$,\nwhere $K, L \\in R^{D \\times D}$ are kernel matrices, and $K_{i,j} = k(h_n^t, h_n^t), L_{i,j} = l(h_n^{t+}, h_n^{t+})$. $H = I \u2212 D^{\u22121}11^T$, where I is an identity matrix and 1 is an all-one column vector.\nThe final objective of the student model is to generate future patterns based on the historical stock data while forecasting the stock trends as accurately as possible. Specifically,\n$L = L_p + \\lambda L_d$,\nwhere $L_p = CEL(y_n^{[t+1,t+T]}, \\hat{y}_n^{[t+1,t+T]})$ and $\\lambda$ is the hyper-parameter striking a balance between two terms."}, {"title": "IV. EXPERIMENTS", "content": "1) Datasets: To verify the effectiveness of the proposed DishFT-GNN, we conduct extensive experiments using two datasets from American stock indices, i.e., S&P 100 and NASDAQ 100, spanning from January 1, 2019, to September 30, 2023. All the datasets are divided into three parts: 85% for training, 7.5% for validation and 7.5% for testing. To ensure that both datasets contain continuous trading records, we eliminate stocks with missing data, such as those affected by suspensions. Thus, 96 and 94 stocks are selected from S&P 100 and NASDAQ 100, respectively. Additionally, industry data is utilized to represent explicit stock relationships.\n2) Experimental Setting: To assess the performance of DishFT-GNN, we conduct the experiments with seven GNN stock prediction methods for comparison, i.e., GCN [31], GAT [32], TGC [16], ADGAT [20], MGAR [21], VGNN [22] and MDGNN [19]. DishFT-GNN is integrated with each stock predictor to boost predictive performance. Following previous studies [33], [34], we use accuracy (ACC) and Matthews correlation coefficient (MCC) as two metrics.\nParameters of all models are trained using Adam optimizer [35] on a single NVIDIA RTX 4070Ti GPU. In our experiments, T and \u03b4 are set to 20 and 4%, respectively. T is set to 0.5 The learning rate is set to 5e-4 and the batch size is set to 64. We independently repeated each experiment five times and reported the mean and standard deviation."}, {"title": "V. CONCLUSION", "content": "We propose a novel distillation-based framework named DishFT-GNN for stock trend prediction. To address the issue that contemporary stock prediction methods fail to extract rich future patterns from historical stock knowledge, we introduced a teacher model trained to generate informative historical-future correlations, supervising the student model's learning. Specifically, the teacher model first extracted historical spatiotemporal and future embeddings. We then proposed a novel attention-based multi-channel fusion method to mine the association between historical and future distributions by integrating both embeddings. Finally, the fused representation was then utilized as intermediate supervision to guide the student GNN to learn future-aware spatiotemporal representations for accurate prediction. Extensive evaluations on real-world data showcase the effectiveness of our proposed method."}]}