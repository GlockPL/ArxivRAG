{"title": "Efficient Training of Transformers for Molecule\nProperty Prediction on Small-scale Datasets", "authors": ["Shivesh Prakash"], "abstract": "Abstract-The blood-brain barrier (BBB) serves as a protective\nbarrier that separates the brain from the circulatory system, regulating\nthe passage of substances into the central nervous system. Assessing\nthe BBB permeability of potential drugs is crucial for effective drug\ntargeting. However, traditional experimental methods for measuring\nBBB permeability are challenging and impractical for large-scale\nscreening. Consequently, there is a need to develop computational\napproaches to predict BBB permeability. This paper proposes a GPS\nTransformer architecture augmented with Self Attention, designed\nto perform well in the low-data regime. The proposed approach\nachieved a state-of-the-art performance on the BBB permeability\nprediction task using the BBBP dataset, surpassing existing models.\nWith a ROC-AUC of 78.8%, the approach sets a state-of-the-art by\n5.5%. We demonstrate that standard Self Attention coupled with GPS\ntransformer performs better than other variants of attention coupled\nwith GPS Transformer.\nKeywords-Blood-Brain Barrier Permeability, Molecule Property\nPrediction, Transformers, Geometric Learning", "sections": [{"title": "I. INTRODUCTION", "content": "The blood-brain barrier acts as a protective shield, sepa-\nrating the brain from the circulatory system of the body. Its\nprimary function is to restrict the passage of solutes from the\nbloodstream into the central nervous system, where neurons\nreside [1].\nFor drugs to effectively act on the brain, they must be\ncapable of crossing the blood-brain barrier. Conversely, drugs\nintended for peripheral action should exhibit limited ability to\npenetrate this barrier in order to avoid undesired effects on the\ncentral nervous system. Therefore, it is crucial to determine the\nblood-brain barrier permeability of potential drugs. However,\nexperimental methods for measuring brain-blood permeability\nare challenging, time-consuming, expensive, and impractical\nfor large-scale screening of chemicals [2].\nThe blood-brain barrier's highly selective nature enables en-dothelial cells to tightly regulate the central nervous system's\nhomeostasis [3]. This regulation is vital for proper neuronal\nfunction and provides protection against toxins, pathogens,\ninflammation, injury, and disease. However, the restrictive\nproperties of the blood-brain barrier pose a significant obstacle\nto drug delivery into the central nervous system. Consequently,\nextensive research has been conducted to develop methods\nfor modulating or circumventing the blood-brain barrier to\nenhance therapeutic delivery [1].\nThe process of brain entry for compounds is multifaceted\nand influenced by various factors. Lipophilic drugs, for in-\nstance, can passively diffuse across the blood-brain barrier,\nwith their ability to form hydrogen bonds playing a role. In\ncontrast, polar molecules typically face difficulties in crossing\nthis barrier, although active transport mechanisms can facilitate\ntheir penetration [4]. Local hydrophobicity, ionization profile,\nmolecular size, lipophilicity, and flexibility are among the\nimportant parameters influencing blood-brain barrier perme-\nation [5].\nFor compounds targeting the central nervous system, it is\ncrucial to achieve brain penetration, ensuring the site of action\nis reached effectively [6]. Conversely, compounds designed\nfor peripheral targets should minimize blood-brain barrier\npenetration to reduce potential central nervous system-related\nside effects. Therefore, selecting compounds with appropriate\nbrain penetration properties is a critical consideration during\nthe drug discovery phase [6].\nIn this paper we focus on producing more powerful results\nusing smaller datasets, thus increasing the computational effi-\nciency.\nThe contributions of this paper can be summarized as:\n\u2022\n\u2022\nA GPS Transformer architecture similar to GPS [7] and\nGPS++ [8] paired with Self Attention which is able to\nperform well in the low-data regime is proposed.\nWith this approach a new state-of-the-art for the task\nof predicting Blood-Brain Barrier permeability using the\nBBBP dataset by Wu, Ramsundar, Feinberg, et al. [9]\nwas established, and to the best of our knowledge this\nis the first work to use Self Attention coupled with the\nGPS Transformer to improve models in this domain. A\n78.8 ROC-AUC was achieved, beating the current state-\nof-the-art [10] by 5.5."}, {"title": "II. RELATED WORK", "content": "The paper by Miao, Xia, Chen, et al. [11] proposes a Deep\nLearning method for predicting the blood-brain barrier perme-\nability based on clinical phenotypes data. The method aims\nto overcome the limitations of existing prediction approaches\nthat rely on physical characteristics and chemical structure of\ndrugs, which are typically applicable only to small molecule\ncompounds that passively diffuse through the BBB.\nThe Deep Learning method leverages clinical phenotypes\ndata to train a predictive model. Clinical phenotypes refer to\nobservable characteristics or traits associated with drugs, such\nas drug side effects and drug indications. By incorporating\nthis information into the prediction model, the method aims\nto capture more comprehensive and complex mechanisms of\nBBB penetration beyond passive diffusion.\nMiao, Xia, Chen, et al. [11] validate their Deep Learning\nmethod using three datasets. The validation results demon-\nstrate that their approach outperforms existing methods in"}, {"title": null, "content": "terms of prediction accuracy. Specifically, the average accuracy\nachieved by their method is 0.97, the area under the receiver\noperating characteristic curve (AUC) is 0.98, and the F1 score\nis 0.92. These performance metrics indicate a high level of\naccuracy in predicting BBB permeability. By incorporating\nclinical phenotypes data into the predictive model, the method\ndemonstrates superior performance compared to existing phys-ical, chemical, and supervised learning approaches [11].\nJain and Shanmuganathan [12] in their paper address the\nlimitations of existing models used to predict blood-brain\nbarrier (BBB) permeability by exploring the role of inflamma-\ntion in influencing BBB permeability. Inflammation, measured\nthrough acute phase C-reactive protein (CRP) levels, has been\nfound to have a direct correlation with BBB permeability. A\nthreshold of 2.5 \u00b5g/ml of CRP is established, above which\nBBB impairment is expected.\nTo overcome the challenges of existing models, a custom-built dataset of 281 molecules is created, and a machine\nlearning approach is employed. Initially, different models such\nas multi-layer perceptron regression (MPR) and ensemble\nmodels are tested, but they struggle with outliers and lack the\nability to predict descriptors for future drugs. Finally, a fully\nconnected neural network (FCNN) is chosen as the predictive\nlogBB model, which performs well in terms of learning the\ndistribution and achieving lower error levels.\nThe research also incorporates a neuroinflammation model,\nwhich considers CRP levels alongside the logBB model. Al-\nthough CRP levels do not directly correlate with logBB values,\nthe model explores the second and third-order feature deriva-\ntives that show significant correlations. A quadratic polynomial\nregression model is used to determine this correlation. The\nsoftware developed for this research includes preprocessing,\nthe predictive logBB model, and the neuroinflammation model,\nproviding a continuous logBB value that represents BBB\npermeability based on the patient's inflammation level. This\napproach aims to improve the accuracy of predicting BBB\npermeability for drugs and better understand the impact of\ninflammation on BBB function.\nThe study by Saber, Mhanna, and Rihana [13] focuses on\npredicting drug permeability across the blood-brain barrier\n(BBB) using in silico models. The researchers compare the\nperformance of sequential feature selection (SFS) and genetic\nalgorithms (GA) in selecting relevant molecular descriptors\nto enhance the accuracy of permeability prediction. Five\ndifferent classifiers are trained initially using eight molecular\ndescriptors, and then SFS and GA are separately applied to\nchoose the descriptors for each algorithm.\nThe results show that both SFS and GA improve the\naccuracy of the classifiers, but GA outperforms SFS. The\nhighest accuracy of 96.23% is achieved with GA, specifically\nwith a fitness function based on the performance of a support\nvector machine. The study highlights the significance of the\npola surface area (PSA) of drugs in crossing the BBB. GA\nconsistently selects the PSA and the number of hydrogen bond\ndonors as the most relevant descriptors, providing better results\ncompared to using other features.\nThe findings suggest that GA is a more robust approach for\nselecting relevant descriptors in predicting BBB permeability."}, {"title": null, "content": "The selected classifiers demonstrate a good balance in predict-ing BBB+ and BBB- drugs. Accurate in silico BBB models\nare crucial for early-phase drug discovery, reducing the need\nfor extensive in vitro testing and saving time and resources.\nThe paper by Yuan, Zheng, and Zhan [14] describes the im-portance of predicting the permeability of compounds through\nthe blood-brain barrier (BBB) for drug discovery targeting\nthe brain. It discusses the use of computational methods,\nparticularly support vector machine (SVM), in predicting BBB\npermeability. Different types of descriptors, such as molecular\nproperty-based descriptors (1D, 2D, and 3D descriptors) and\nfragment-based descriptors (fingerprints), have been utilized in\nSVM models. The selection of descriptors greatly influences\nthe performance of the SVM model.\nThe paper aims to develop a new SVM model by com-bining molecular property-based descriptors and fingerprints\nto improve the accuracy of BBB permeability prediction. The\nresults indicate that the proposed SVM model outperforms\nexisting models in predicting BBB permeability. The paper\nalso provides an overview of the blood-brain barrier and the\ncomplex factors that influence compound penetration into the\nbrain. Additionally, it discusses the classification of quan-titative and qualitative BBB permeability prediction models\nand highlights SVM as a superior method for qualitative\nprediction. The paper concludes that the combined use of\nproperty-based descriptors and fingerprints improves the ac-curacy of SVM models and suggests that similar approaches\nmay enhance computational predictions for other molecular\nactivities in the future.\nHowever, our work explores how we find a simple modi-fication to a General, Powerful, Scalable Graph Transformer\n(GPS) Ramp\u00e1\u0161ek, Galkin, Dwivedi, et al. [7] and improve\nupon the performance for the task of blood-brain barrier per-meability while showing the effectiveness of simple methods\nwithin the graph setting."}, {"title": "III. DATA", "content": "We perform our experiments on the dataset 'BBBP: Binary\nlabels of blood-brain barrier penetration (permeability)' which\nis a classification dataset available as a part of MoleculeNet [9]\nwhich contains 2,050 molecules and each molecule comes with\na name, label, and SMILES string. The label is a boolean\ninteger which denotes if a given compound can pass through\nthe blood-brain barrier or not.\nThe Blood-brain barrier penetration (BBBP) dataset comes\nfrom a study by Martins, Teixeira, Pinheiro, et al. [15] on\nthe modeling and prediction of the barrier permeability using\na Bayesian approach. For this well-defined target scaffold\nsplitting is also recommended by Wu, Ramsundar, Feinberg,\net al. [9].\nIn the paper by Martins, Teixeira, Pinheiro, et al. [15], a\nunique approach based on Bayesian statistics, in conjunction\nwith state-of-the-art machine learning techniques, was em-ployed to create a robust model suitable for real-world drug re-search applications. The dataset used for the Bayesian analysis\nconsisted of 1970 carefully curated molecules, making it one\nof the largest, but still small, datasets used in similar studies."}, {"title": null, "content": "Various configurations of Random Forests and Support Vector\nMachines, coupled with different combinations of chemical\ndescriptors, were evaluated. To assess the performance of the\nmodels, a 5-fold cross-validation process was employed, and\nthe best-performing model was further tested on an indepen-dent validation set.\nThe model proposed by Martins, Teixeira, Pinheiro, et al.\n[15] achieved an impressive overall accuracy of 95%, as\nmeasured by a mean square contingency coefficient ($\\phi$) of\n0.74. Furthermore, this model exhibited a high capacity for\npredicting blood-brain barrier (BBB) positives, with an accu-racy of 83%, and a remarkable accuracy of 96% in determining\nBBB negatives. These findings highlight the efficacy of the\ndeveloped model in accurately predicting the permeability of\nmolecules across the blood-brain barrier, based on experiments\nby Martins, Teixeira, Pinheiro, et al. [15]."}, {"title": "IV. METHODS", "content": "The proposed approach closely follows the framework\nof General, Powerful, Scalable Graph Transformer (GPS)\nby Ramp\u00e1\u0161ek, Galkin, Dwivedi, et al. [7] and that of GPS++\nby Masters, Dean, Klaser, et al. [8]. This paper explores how\nthese frameworks could be used to learn from low amounts of\ndata. Thus, the BBBP dataset was used which contains only\n2050 molecules."}, {"title": "A. GPS Block", "content": "Masters, Dean, Klaser, et al. [8] proposed that at each\nlayer, the features are updated by aggregating the output of\nan MPNN layer with that of a global attention layer which is\ndescribed by the equations below. Note that the edge features\nare only passed to the MPNN layer, and that residual connec-tions with batch normalization [16] are omitted for clarity.\nBoth the MPNN and GlobalAttn layers are modular, i.e.,\nMPNN can be any function that acts on a local neighborhood\nand GlobalAttn can be any fully-connected layer.\ncomputed as\n$\\begin{aligned}\nX^{l+1}, E^{l+1} &= \\text{GPS} (X^l, E^l, A), \\\nX_M^{l+1}, E^{l+1} &= \\text{MPNN} (X^l, E^l, A), \\\nX_G^{l+1} &= \\text{GlobalAttn} (X^l), \\\nX^{l+1} &= \\text{MLP} (X_M^{l+1} + X_G^{l+1})\n\\end{aligned}$\nwhere $A \\in \\mathbb{R}^{N \\times N}$ is the adjacency matrix of a graph with\nN nodes and E edges; $X^l \\in \\mathbb{R}^{N \\times d_e}$, $E^l \\in \\mathbb{R}^{E \\times d_e}$ are the\n$d_e$-dimensional node and edge features, respectively; MPNN\nand GlobalAttn are instances of an MPNN with edge\nfeatures and of a global attention mechanism at the $l$-th layer\nwith their corresponding learnable parameters, respectively;\nMLP is a 2-layer MLP block."}, {"title": "B. GPS++ Block", "content": "Ramp\u00e1\u0161ek, Galkin, Dwivedi, et al. [7] proposed the GPS++\nblock to be defined as follows for layers $l > 0$:\n$\\begin{aligned}\nX^{l+1}, E^{l+1}, g^{l+1} &= \\text{GPS++} (X^l, E^l, g^l, B) \\\nY^l, E^{l+1}, g^{l+1} &= \\text{MPNN} (X^l, E^l, g^l), \\\nZ^l &= \\text{BiasedAttn} (X^l, B), \\\n\\forall i : X^{l+1} &= \\text{FFN} (Y^l + Z^l)\n\\end{aligned}$"}, {"title": null, "content": "In GPS++ [8] which is analogous to MPNN modules as\nshown by Gilmer, Schoenholz, Riley, et al. [17] and Battaglia,\nHamrick, Bapst, et al. [18], and Bronstein, Bruna, Cohen, et\nal. [19]. They indicate that their MPNN variant maximizes the\nexpressivity of the model and increases the generalizability of\nthe model."}, {"title": "C. Our Architecture", "content": "The proposed block is defined as follows which is similar\nto the construction of GPS++ [8].\n$\\begin{aligned}\nX^{l+1}, E^{l+1}, g^{l+1} &= \\text{GPS++} (X^l, E^l, g^l, B) \\\nY^l, E^{l+1}, g^{l+1} &= \\text{MPNN} (X^l, E^l, g^l), \\\nZ^l &= \\text{Attn} (X^l, B), \\\n\\forall i : X^{l+1} &= \\text{FFN} (Y^l + Z^l)\n\\end{aligned}$\nwhere $X, E, g$, and B defined in Equation 13.\nWe do not use the MPNN variant proposed by GPS++ [8] in\nour architecture which increases the overfitting considering the\nsizes of the datasets we work with. Our approach also incor-porates Attn which is defined as the standard attention block\nfrom Vaswani, Shazeer, Parmar, et al. [20]. We also find that\nfor smaller graph datasets standard Attention, modules work\nbetter than Attention variants like Biased self-attention [21]\nwhich was used by GPS++ [8]. The Feed Forward Network is\ndefined simply as a stack of MLP layers and also uses Drop\nGNN [22]. These architectural choices are also summarized\nin Figure 1.\nWith these changes in the Architecture of GPS++, our\napproach outperforms GPS++ on smaller datasets and sets a\nnew state-of-the-art on the BBBP dataset achieving 78.8 ROC-AUC, beating the current state-of-the-art [10] by 5.5."}, {"title": "V. EXPERIMENTAL SETUP", "content": "This section explains the implementation details of the\nexperiments and our proposed model."}, {"title": "A. Sampling", "content": "There is a class imbalance in the dataset, meaning that not\nall classes have a similar number of images. For this reason,\nwe follow a stratified sampling strategy during data loading\nto ensure each batch contains 50\u00b15% instances of each label\nclass."}, {"title": "B. Model backbone", "content": "Throughout this work GPS [7] and GPS++ [8] is used\nas the architecture backbone due to their success in using\nMPNNs and Transformers. Our model backbone is standard\nself-attention [20] which we demonstrate works well for the\ntask of molecule property prediction. Since the BBBP dataset\ndoes not contain a large amount of data, other transformer-based models did not show great results for this task whereas\nstandard self-attention model with regularization and augmen-tation techniques, was able to generalize well and achieved\nbetter results."}, {"title": null, "content": "$\\begin{aligned}\nX^0 &= \\text{Dense}([X_{\\text{atom}} \\vert X_{\\text{LapVec}} \\vert X_{\\text{LapVal}} \\vert X_{\\text{RW}} \\vert X_{\\text{Cent}} \\vert X_{\\text{3D}}]) && \\in \\mathbb{R}^{N \\times d_{\\text{node}}} \\\nE^0 &= \\text{Dense}([E_{\\text{bond}} \\vert E_{\\text{3D}}]) && \\in \\mathbb{R}^{M \\times d_{\\text{edge}}} \\\ng^0 &= \\text{Embedd}_{\\text{global}} (0) && \\in \\mathbb{R}^{d_{\\text{global}}} \\\nB &= B_{\\text{SPD}} + B_{\\text{3D}} && \\in \\mathbb{R}^{N \\times N}\n\\end{aligned}$"}, {"title": "VI. RESULTS", "content": "We report all results and compare them against previous\nmodels and a random baseline (equivalent to making a guess)\nin Table I. The performance of models is calculated using\nthe metrics that are typical for a molecular property prediction\nproblem, ROC-AUC. Additionally, it is found that using any of\nthe other variants of GPS++ is detrimental for this task and the\nmodel starts heavily overfitting even with our design changes,\naugmentation, and regularization. We also observe that our\narchitectural changes over GPS++ [8] improves GPS++ by\n6.1%."}, {"title": "C. Baseline model", "content": "We established a naive baseline with random guesses. The\nbaseline model we chose was training GPS++ [8] without any\ndesign modifications. This gets to a ROC-AUC of 72.7 on\nthe BBBP dataset. To train this model, we employ standard\naugmentations using AugLiChem [23] and train the network\nfor 300 epochs with a batch size of 256."}, {"title": "D. Loss function", "content": "The final model prediction is formed by global sum-pooling\nof all node representations and then passing it through a\nMLP. The regression loss is the mean absolute error (L1 loss)\nbetween a scalar prediction and the ground truth."}, {"title": "E. Code", "content": "Our code is in PyTorch 1.10 [24] and we also pro-vide TensorFlow code. We use a number of open-source\npackages to develop our training workflows. Most of our\nexperiments and models were trained with PyTorch Geo-metric [25]. Our hardware setup for the experiments in-cluded either four NVIDIA Tesla V100 GPUs or a TPUv3-8 cluster. We utilized mixed-precision training with Py-Torch's native AMP (through torch.cuda.amp) for mixed-precision training and a distributed training setup (through torch.distributed.launch) which allowed us to ob-tain significant boosts in the overall model training time. Our\ncode to reproduce the results along with Tensorboard logs of\nthe training runs are available at 1."}, {"title": "VII. CONCLUSION", "content": "This research has provided valuable insights into the chal-lenges and opportunities associated with drug delivery across\nthe blood-brain barrier. By understanding the intricate mech-anisms of brain entry and the parameters influencing blood-brain barrier permeation, this paper have paved the way for\nthe development of novel strategies in the field of therapeutic\ndelivery. The introduction of GPS Transformer architecture,\ncombined with Self Attention, has demonstrated significant\nadvancements in predicting blood-brain barrier permeability\nusing limited data. Notably, the approach has surpassed the\ncurrent state-of-the-art performance, as evidenced by achieving\nan impressive ROC-AUC of 78.8. This breakthrough opens\nup new possibilities for efficient and effective drug discovery\nand development, ultimately aiming to improve treatments for\ncentral nervous system disorders. The findings contribute to\nthe bigger vision of enhancing patient care by enabling tar-geted drug delivery to the brain, thus revolutionizing common\npractices and offering hope for transformative advancements\nin neurological medicine."}]}