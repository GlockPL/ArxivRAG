{"title": "Paid with Models: Optimal Contract Design for Collaborative Machine Learning", "authors": ["Bingchen Wang", "Zhaoxuan Wu", "Fusheng Liu", "Bryan Kian Hsiang Low"], "abstract": "Collaborative machine learning (CML) provides a promising paradigm for democratizing advanced technologies by enabling cost-sharing among participants. However, the potential for rent-seeking behaviors among parties can undermine such collaborations. Contract theory presents a viable solution by rewarding participants with models of varying accuracy based on their contributions. However, unlike monetary compensation, using models as rewards introduces unique challenges, particularly due to the stochastic nature of these rewards when contribution costs are privately held information. This paper formalizes the optimal contracting problem within CML and proposes a transformation that simplifies the non-convex optimization problem into one that can be solved through convex optimization algorithms. We conduct a detailed analysis of the properties that an optimal contract must satisfy when models serve as the rewards, and we explore the potential benefits and welfare implications of these contract-driven CML schemes through numerical experiments.", "sections": [{"title": "1 Introduction", "content": "Training a state-of-the-art machine learning (ML) model is a Herculean task due to the requirement of an enormous amount of data and computational resources. The exorbitant cost often precludes budget-constrained small parties from training a model on their own, resulting in a high industrial concentration where top-performing models are owned by big firms (AI Index Steering Committee 2024). In this regard, collaborative machine learning (CML) provides a promising crowdsourcing paradigm. The advent of CML schemes like federated learning (McMahan et al. 2017; Kairouz et al. 2021; Sheller et al. 2020; Nguyen et al. 2022) allows participants to join their resources for model training and share the training cost that would otherwise be insurmountable at an individual level. Despite their great potential, such schemes might not make economic sense. As is shown by Karimireddy, Guo, and Jordan (2022), catastrophic freeriding can occur when profit-maximizing parties in a collaboration have the ability to observe each other's data collection costs. This issue can be mitigated through the role of a scheme coordinator who conducts model training on the parties' behalf and rewards models with modified accuracy levels based on the parties' contributions. Practically, this could be achieved through the design of contracts, where the scheme coordinator acts as the principal and each participating party of the scheme acts as the agent.\nPrior to our work, there has been a line of research that resorts to contract theory to address the incentive issue in collaborative machine learning (Kang et al. 2019; Ding, Fang, and Huang 2020; Karimireddy, Guo, and Jordan 2022; Liu et al. 2023), but most of them focus on using money as the reward for the collaboration. Karimireddy, Guo, and Jordan (2022) attends to the administration of models with different accuracy levels as rewards, while their primary focus is on the case where the scheme coordinator can directly observe each party's data collection costs. However, in reality, the cost of contribution is typically private information known only to the contributing party. For instance, consider a CML scheme where private computing firms pull together their GPUs for the training of a language model for code generation. Each firm could face a different vendor price and incur dissimilar maintenance cost of the chips. As another example, consider the CML scheme where investment firms join their privately curated data for the training of an investment model. To gather the data, each firm needs to recruit analysts, the overheads of which are usually determined by conditions of the local labor market and the firm's own incentive policies. The differences in the operating environments cause the parties of a CML scheme to have a wide range of per-unit contribution costs. While the scheme coordinator can be an expert in the domain field, thereby possessing some general information about the process, it remains challenging for them to gauge the exact costs borne by the parties. Even if the parties willingly inform the coordinator of their costs, the coordinator cannot verify the truthfulness of these reports without incurring significant auditing expenses. Worse still, a rent-seeking party may cheat by misreporting their cost if it leads to higher profits being gained from the scheme. This information asymmetry results in what is known as a principal-agency problem in economic literature (see Mas-Colell, Whinston, and Green 1995; Laffont and Martimort 2002; Bolton and Dewatripont 2004 for a comprehensive treatment of the subject).\nIn the presence of private information, optimal contract design with models as the rewards poses unique challenges that distinguish it from its economic counterparts. For one,"}, {"title": "2 Problem Setup", "content": "Collaborative Machine Learning. We consider a typical setting of collaborative machine learning where budget-constrained parties contribute their resources, such as data and GPUs, to collectively train a model through the mediation of a scheme coordinator. We capture the dependency between contributed resources and model accuracy through the function $a(\\cdot)$, and make the standard assumptions that more contribution leads to better model performance, $a'(\\cdot) > 0$, and that the same amount of contribution has diminishing marginal returns as the total contribution increases, $a''(\\cdot) < 0$. To ensure that the framework is well-grounded economically, we additionally represent the dependency between accuracy of a model and the economic profit gained by the participant through a weakly concave and increasing valuation function $v(\\cdot)$, with $v'(\\cdot) > 0$ and $v''(\\cdot) \\leq 0$. In the computing firm example mentioned in the introduction, the accuracy function captures the fact that the use of more GPUs enables more iterations of model training to be undertaken within the given time span; the valuation function reflects the fact that improved performance of the language model reduces human overheads\u2014enabling a firm to hire fewer software engineers for the same amount of work. In the investment firm example, $a(\\cdot)$ captures the fact that more data leads to a fuller picture of the market landscape, which in turn boosts the predictive accuracy of the trained model; $v(\\cdot)$ embodies the fact that a model's predictive accuracy directly affects the quality of a firm's investment decisions and better decisions lead to more revenues being earned by the firm. To consolidate the intuition, we henceforth use data consistently throughout the paper as the resources contributed by parties in the collaboration, whereas the analysis can also apply readily to other resource types, like GPUs or computation devices.\nPrincipal-Agent Problem. There are two distinct features of the CML setting mentioned above: Firstly, there is a conflict of interests between the scheme coordinator, who aims to maximize the performance of the collectively trained model by encouraging data contribution, and the scheme participant, who hopes to receive a good-performing model by contributing as little data as possible since collection is costly. Secondly, the scheme participant possesses information advantage over the scheme coordinator, as the per-unit data collection cost is directly known by the party themself but may not be observed by the coordinator likely due to costly verification or auditing process. This opens up the possibility that a party might cheat by misreporting their cost so that they can be compensated better by the coordinator. To facilitate the formalization of the model, we introduce the following notation: Let $I$ denote the total number of distinct types of private per-unit data collection costs in the population\u2014henceforth called private types or types; we denote the per-unit data costs by $c_i$, $i = 1,..., I$, and order them decreasingly, $c_1 > ... > c_I$, for analytical tractability. Hence, a type-1 party incurs the highest per-unit cost while contributing data to the CML scheme. We let $\\mathbf{n} = (n_1,...,n_I)$ be a vector that counts the numbers of parties of each type in the collaboration and $N = \\sum n_i$ denote the total number of participants of the CML scheme. Here, we assume the coordinator observes $N$ but not $\\mathbf{n}$. She nevertheless has the general knowledge that $\\mathbf{n}$ follows a multinomial distribution $Mul(N, \\mathbf{p})$ with probabilities $\\mathbf{p} = (p_1, ..., p_I)$ for the types. We denote the model reward received by a type-$i$ party by $r_i$ measured in model accuracy, their data contribution by $m_i$ measured in amount, and their reservation utility by $f_i$ measured in monetary terms and defined as the highest profits they could achieve by not participating in the CML scheme.\nModel as the Reward. When we use models as the rewards for the CML scheme, a natural budget constraint we need to abide by is\n$\\Vert \\mathbf{r}(\\mathbf{n}) \\Vert_\\infty \\leq a\\left(\\sum_{i=1}^I n_i m_i\\right), \\forall \\mathbf{n} \\in Mul(N, \\mathbf{p}),$\nwhere $\\mathbf{r}(\\mathbf{n}) = (r_1(\\mathbf{n}), ..., r_I(\\mathbf{n}))$ is a vector with elements specifying the model rewards received by different types of parties under the realized combination $\\mathbf{n}$. In words, for every possible combination of types, the maximal model accuracy reward assigned to the parties cannot surpass the accuracy of the collectively trained model. Fixing the contributions $m_i, i = 1,..., I$ from each type of parties, we see that this upper bound is dependent on $\\mathbf{n}$. Consequently, the model"}, {"title": "3 Optimal Contracting in CML", "content": "With the notation in place, we are now ready to formalize the optimal contracting problem in CML.\nCoordinator's Objective. The objective of the coordinator reflects the goal of the CML scheme. Here we assume that she aims to maximize the expected model accuracy,\n$\\mathbb{E}_{\\mathbf{n}\\sim Mul(N,\\mathbf{p})} \\left[ a\\left(\\sum_{i=1}^I N_i m_i\\right)\\right].$\nFor readability, we henceforth abbreviate $\\mathbb{E}_{\\mathbf{n}\\sim Mul(N,\\mathbf{p})}$ to $\\mathbb{E}$ unless otherwise stated.\nParticipant's Utility Function. Each participant is assumed to be a von Neumann-Morgenstern utility maximizer (von Neumann and Morgenstern 1944), who has the following utility function:\n$U_i(r_i, m_i) = \\mathbb{E}_{n_i\\geq 1}[v(r_i)] - c_i m_i.$\nRecall that $r_i$ denotes the stochastic model reward (measured in accuracy), which takes on different values for different $\\mathbf{n}$; $m_i$ is the amount of data contribution; and $c_i$ denotes the private per-unit data cost. $v(\\cdot)$ reflects the profit mechanism that maps a model accuracy level to a pecuniary amount. The expectation operator $\\mathbb{E}_{n_i\\geq 1}$ codifies the information advantage possessed by the type-$i$ participant\u2014when they decide to participate, they know in advance that there is at least one party making the type-$i$ commitment. The aim of each participant is to maximize their expected net profit, represented by the utility function in (1).\nIndividual Rationality. To ensure the contract is well designed, it must pass the first test that it gives parties of different types enough incentives to join the CML scheme. Formally, this requires that a party upon choosing the contract option designed for their type cannot be made worse off than them not participating in the CML scheme. This is known as the individual rationality (IR) conditions. To formalize the idea, we should specify the reservation utility (a.k.a. opportunity cost) for each type of parties. Here we define a party's reservation utility to be the utility level they achieves by training a model on their own, which amounts to solving the following optimization problem:\n$f_i \\triangleq \\underset{m\\geq 0}{\\text{max}} \\ u(m) = v(a(m)) - c_i m.$\nWe make the following observation upon solving this problem, the proof of which can be found in Appendix A.\nProposition 1. Let $m_i$ denote the data contribution a type-i party is willing to commit to when training a model on their own. If $c_i \\leq c_j$, then $m_i \\geq m_j$, and $f_i \\geq f_j$.\nThe proposition states that when training a model alone, a party with lower data cost is willing to utilize more data and will end up with a better model and generate higher profits. We can write the IR conditions as follows:\n$\\mathbb{E}_{n_i\\geq 1}[v(r_i)] - c_i m_i \\geq f_i, \\forall i.$\nUsing this formulation, readers familiar with the economic literature could also interpret $f_i$ as the fixed cost and $c_i$ the variable cost of joining the CML scheme for type-$i$ parties.\nIncentive Compatibility. Designing a contract $\\mathcal{C}$ in the presence of hidden information seems highly complicated, as participants could lie about their private costs. Luckily, with the aid of the revelation principle (Myerson 1981, 1982; see Mas-Colell, Whinston, and Green 1995 for a lucid explanation on the concept), we could confine the design space to one contract option per private type, with each party choosing the option designed for their type upon signing the contract. In the CML setting, this translates to designing a contract option that stipulates a required data contribution $m_i$"}, {"title": "4 Contracting with Observable Costs", "content": "We conduct an analysis of the complete information scenario, as it offers insights on two fronts. Firstly, it helps establish a welfare benchmark, against which we could evaluate the welfare loss incurred by the existence of hidden information. Secondly, it models particular CML settings, in which it is relatively cheap to obtain or elicit the private cost information. When the coordinator can observe a party's type, the IC conditions become redundant, as the coordinator can directly contract parties based on their costs. The problem thus reduces significantly to\n$\\underset{\\{r_i,m_i\\}_{i=1}^I}{\\text{max}} a\\left(\\sum_{i=1}^I n_i m_i\\right)$\ns.t. $v(r_i) - c_i m_i - f_i \\geq 0, \\forall i$;\n$r_i \\leq a\\left(\\sum_{i=1}^I n_i m_i\\right), \\forall i$.\nNote that we drop the expectation operators because the coordinator now fully knows the number of parties present for each type. Consequently, the rewards also become deterministic. Solving the problem leads to the following proposition.\nProposition 2. Under the complete information scenario, the optimal strategy for the principal is to offer the best model to all participating parties and require them utilize"}, {"title": "5 Contracting with Private Costs", "content": "When data costs are private information of the parties, it could have serious implications on the CML scheme without contract. As we demonstrate in Appendix B, a complete collaboration failure could occur where all parties contribute nothing to the scheme. In other cases, equilibrium does not exist, making the learning outcome unpredictable. Designing a contract helps address these issues but solving the problem defined by (3) and (4) directly is difficult due to the non-convexity of the constraints and the enormous number of choice variables. Luckily, the problem can be simplified on two fronts to improve its tangibility. Firstly, we transform the original problem into a convex constrained optimization problem with respect to first moments (termed the first-moment problem), which significantly reduces the number of choice variables we need to optimize with. We then derive a mapping from the solution of the first-moment problem to one that elegantly solves the original problem. Secondly, we conduct constraint analysis of the first-moment problem, further removing redundant constraints and delineating the properties an optimal contract should satisfy.\nFirst-moment problem\nThe key to converting the original problem into a first-moment problem lies in the relaxation of the budget constraint. The following provides a necessity result."}, {"title": "Simplified First-Moment Problem", "content": "With the above simplifications, the first-moment problem becomes\n$\\underset{\\{(t_i,m_i\\}_{i=1}^I}{\\text{max}} \\mathbb{E}_{\\mathbf{n}\\sim Multi(N,p)}\\left[ a\\left(\\sum_{i=1}^I N_i m_i\\right)\\right]$\ns.t.\n$t_1 - C_1 m_1 - f_1 = 0$;\n$t_i \\leq \\mathbb{E}_{n_i\\geq 1} \\left[v\\left(a\\left(\\sum_{i=1}^I n_i m_i\\right)\\right)\\right], \\forall i \\in I$;\n$t_i - C_i m_i = t_{i-1} - C_i m_{i-1}, \\forall i \\in \\{2,...,I\\}$;\n$m_i \\geq m_{i-1}, \\forall i \\in \\{2, ..., I\\}$;\n$t_i - C_i m_i - f_i \\geq 0, \\forall i \\in \\{2, . . ., I\\}$.\nWe keep the inequality of the budget constraint for type-I parties so that the resulting problem is still convex (cf. Appendix B). The set of constraints in (7) fully specifies model rewards as a function of contributions:\n$t_i = \\begin{cases}\nf_1 + C_1 m_1 & \\text{if } i = 1; \\\\\nf_1 + C_1 m_1 + \\sum_{k=2}^i C_k (m_k - m_{k-1}) & \\text{if } i > 1.\n\\end{cases}$\nThe problem defined by (6) and (7) is convex and can be solved by numerical optimization methods, such as the trust-region interior-point algorithm, which works by staying away from the boundary of the feasible region defined by the inequality constraints and weakening the barrier effects as the estimate of the solution gets increasingly accurate (Nocedal and Wright 2006)."}, {"title": "6 Experiments", "content": "To gain numerical insights into optimal contract design, we conduct a series of experiments with specified forms of the accuracy function and the valuation function. Following Karimireddy, Guo, and Jordan (2022), we adopt the standard generalization bound (Mohri, Rostamizadeh, and Talwalkar 2018) as the accuracy function, expressed as follows:\n$a(m) := \\text{max}\\left\\{0, a_{opt} - \\frac{\\sqrt{2k(2 + \\text{log}(m/k))}+4}{\\sqrt{m}}\\right\\}.$\nwhere $m$ measures the quantity of data used for model training; $a_{opt}$ is the optimal accuracy achievable by the model, and $k$ captures the difficulty of the learning task. Arguably, this choice of $a(m)$ itself is not concave due to its piecewise nature, but this non-concavity can be addressed via the concavity of each piece of the function. We set $k = 1$ and $a_{opt} = 1$ for the experiments. We assume a constant return to the model accuracy, $v(x) := 100x$, so a model with perfect accuracy is worth 100 in monetary terms.\n6.1 Two-type Case\nWe first focus on the case where there are two private types of parties, $I = \\{1,2\\}$, to analyze how contextual factors such as the total number of participants $N$ and the type distribution $p$ affect the design of optimal contracts. We specify the per-unit data cost for the high-cost type as $c_1 = 0.02$ and that for the low-cost type as $c_2 = 0.01$. In this setting, both types would have initial incentives to train a model on their own without the CML scheme-the high-cost type would use 715.6 units of data and obtain a model valued at 69.6 in monetary terms, and the low-cost type would use 1148.5 units of data to achieve a model reward worth 75.6. The top panel of Figure 2 depicts the optimal contract for the CML scheme under incomplete information for varied probability of high-cost type $p_1 \\in [0, 1]$ and total number of participants $N \\in [2, 100]$, the solutions of which are obtained by solving the corresponding first-moment problems. Under the incentivized CML scheme, both types contribute more data and obtain better models than their reservation levels. For contract design, ceteris paribus, a higher probability of the high-cost type in the population makes it less favorable to create distinct contract options for the types-making a pooling contract more likely to be optimal from the coordinator's perspective. In contrast, the total number of participants has a relatively marginal effect. When all other factors are held constant, a larger participant pool leads to greater differentiation between the options in a separating contract. To gauge the welfare implications of the information asymmetry, we calculate the information cost and information rent under incomplete information. The information cost, $\\Delta v(a_{max})$, is defined as the expected difference between the value of the collectively trained model under incomplete information and that under complete information:\n$\\Delta v(a_{max}) = \\mathbb{E}[t - v\\left(a\\left(\\sum_{i=1}^I n_i m^{complete}_i\\right)\\right)],$\nwhere $t$ is the"}, {"title": "7 Conclusion", "content": "In this work, we consider optimal contract design for CML with models as the rewards. We convert the original non-convex problem of optimizing with reward distributions into one solvable through convex constrained optimization algorithms. Our constraint analysis establishes the necessary conditions for an optimal contract. We further demonstrate the framework through numerical experiments, showing its ability to overcome high cost of model training and improve participant welfare. Our findings highlight that optimal contract design is a viable tool for democratizing future technology in an incentive-driven economy. Future research could explore relaxing the distribution assumption to improve scalability. For a detailed discussion on future research directions, we refer interested readers to Appendix B."}]}