{"title": "Robust Domain Generalization for Multi-modal\nObject Recognition", "authors": ["Yuxin Qiao", "Keqin Li", "Junhong Lin", "Rong Wei", "Chufeng Jiang", "Yang Luo", "Haoyu Yang"], "abstract": "In multi-label classification, machine learning en-\ncounters the challenge of domain generalization when handling\ntasks with distributions differing from the training data. Existing\napproaches primarily focus on vision object recognition and\nneglect the integration of natural language. Recent advancements\nin vision-language pre-training leverages supervision from exten-\nsive visual-language pairs. This allows learning across diverse\ndomains and enhances recognition in multi-modal scenarios,\nshowcasing superior transfer learning capabilities in methods like\nCLIPood. However, CLIPood has several limitations: differences\nin the utilized loss, loss of generality in evaluating only a single\nbackbone, and neglect of class-aware visual fusion.\nTo address these, we propose this paper that infers the actual\nloss based on the implementation, broadens evaluations to larger\nvision-language backbones, and introduces Mixup-CLIPood with\na novel mix-up loss for enhanced class-aware visual fusion.", "sections": [{"title": "I. INTRODUCTION", "content": "In multi-label classification, machine learning applications\ninevitably encounter the challenge of domain generalization\n[2], [22]. This challenge arises when confronting new tasks\nwith distributions that differ from those encountered during\ntraining. While large-scale pretrained models and carefully\ncrafted transfer learning algorithms are readily accessible,\nthese current approaches are predominantly tailored for tasks\nfocused on pure vision object recognition, neglecting the\nincorporation of natural language [20], [21].\nDiverging from conventional methods that rely on learning\nfrom images and encoded labels, contemporary progress in\nvision-language pre-training aims to harness naturally oc-\ncurring supervision derived from extensive visual-language\npairs [2], [4], [18], [19]. This innovative approach allows for\nlearning across diverse domains and enhances the recognition\nof concepts within a multi-modal scenario. As a result, vision-\nlanguage pretrained models demonstrate impressive transfer\nlearning capabilities, outperforming models trained exclusively\non images and encoded labels. This highlights a promising\ngeneralization, like CLIPood [3].\nHowever, there exist several limitations in CLIPood [3].\nFirst, we observed a difference between the actual loss utilized\nin the official implementation and the one described in the\npaper of CLIPood [3]. Second, CLIPood [3] restricts its\nconsideration to the use of a single type of backbone, not eval-\nuating the generalization ability of CLIPood comprehensively.\nThird, CLIPood [3] neglects the fusion of class-aware visual\ninformation, focusing solely on cross-modal fusions and pure\ntext fusions. This oversight may impede the model's overall\ngeneralization capability.\nTo rectify these limitations, we present this paper. Regarding\nthe disparity between the actual loss and the one detailed\nin the paper, we strive to infer the actual loss based on\nimplementations, conducting a comparison with the stated loss\nin Section III-B. To address the limited evaluation scope,\nwe broaden our experiments to include two additional larger\nvision-language backbones. Regarding the oversight of class-\naware visual knowledge fusion, we introduce a novel mix-up\nloss as shown in Fig. 1. Our contributions can be summarized\nin three folds:\n\u2022 We address the incongruity between the actual loss and\nthe one documented in the paper. Through a meticulous\nanalysis of implementations, we deduce the actual loss\nand compare it with the documented loss.\n\u2022 We expand our experiments to encompass two larger\nvision-language backbones. This comprehensive evalua-\ntion provides a more robust assessment of the method's\nperformance.\n\u2022 We propose Mixup-CLIPood with a novel mix-up loss\nto enhance the previous model's generalization ability\nby incorporating class-aware visual information during\ntraining."}, {"title": "II. RELATED WORK", "content": "Multi-modal Learning. Multi-modal learning has garnered\nsignificant attention in recent years, propelled by the rise of\npowerful neural network architectures such as transformers\n[16] and vision transformers (ViT) [17], [24]. Noteworthy\nexamples include CLIP [4], UNITER [18], and ALIGN [19],\nwhich harness transformer-based architectures to concurrently\nprocess both text and image modalities. For our foundational\nvision-language model, we employ CLIP. The field continues\nto progress with ongoing research, particularly in the domain\nof domain generalization. Representative multi-modal domain\ngeneralization methods encompass CoOp [20], CoCoOP [21],\nand CLIPood [3]. In this paper, we adopt CLIPood [3] as the\ncornerstone of our proposed method.\nDomain Generalization. Current DG methods aim to learn\ndomain-invariant representations and are categorized into three\ntypes: domain alignment [10], [11], meta-learning [12], [13],\nand augmentation strategies [14], [15]. For domain alignment,\n[10] enhances the conditional invariance of learned features\nby incorporating an entropy regularization term, leading to\nimproved classifier generalization. [11] iteratively segregates\nsamples into latent domains through clustering. Concerning\nmeta-learning, [12] proposes a model-agnostic training pro-\ncedure that simulates domain shift during training, whereas\n[13] applies meta-learning to single-domain generalization.\nRegarding augmentation strategies, [14] introduces a novel\nregularization term for adversarial data augmentation de-\nrived from the information bottleneck principle, while [15]\npresents a unique style hallucination module to generate style-\ndiversified samples crucial for generalization. In this paper, we\nuse a novel mix-up loss for better generalization, which is an\naugmentation-based technique."}, {"title": "III. METHOD", "content": "In this study, our focus centers on addressing the zero-\nshot generalization challenge inherent in the vision-language\npretrained model CLIP [4]. Commencing with a pretrained\nCLIP model, we initially adapt it using labeled (source) data\ndenoted as S = (x, y). Inspired by [2], our objective is to\nachieve a robust generalization of this model to previously\nunseen (target) data represented by T = {(xt,yt)} through\ncarefully tailored finetuning operations. Although the source\nand target data occupy the same label space, they originate\nfrom distinct distributions, encapsulated by the inequality\nP(x,y) != P(xt, yt).\nOur approach adheres to the established protocols outlined\nin CLIPood [3], involving finetuning on the visual model of\nCLIP [4] while leveraging its text encoder to generate text\nembeddings. To elaborate, for each class c within the label\nspace, we construct a text prompt employing the format \"a\nphoto of a [CLASS]\", with the \"[CLASS]\" token dynamically\nreplaced by the corresponding class name c. Subsequently,\nthe constructed text prompt undergoes processing in the text\nencoder to yield the text embedding specific to class c."}, {"title": "B. Calibration on CLIPood", "content": "For each training sample (x, y), the Margin Metric Softmax\n(MMS) loss in CLIPood [3] is used to finetune the vision\nmodel of CLIP [4], and this loss is represented as Eq. 1 in the\npaper of CLIPood:\n$L_{paper} = -log(\\frac{exp(Sim(I_x, T_y)/\\tau)}{\\sum_{c=1}^{C} exp((Sim(I_x, T_c) + 0.3(1 - Sim(T_y, T_c)))/\\tau)})$ (1)\nIn this equation, $I_x$ represents the embedding of image x,\nwhereas $T_y$ and $T_c$ denote the embeddings of label y and class\nc respectively. C stands for the total number of categories.\nThe function $Sim(\\cdot, \\cdot)$ measures the similarity between two\nembeddings, and $\\tau$ serves as the temperature parameter.\nHowever, after carefully reviewing the official implementa-\ntion of CLIPood [3], we found the actual loss used in the\nfinetuning is not consistent with the formula in the paper.\nBased on the official implementation, we deduce the actual\nloss as:\n$L_{actual} = -log(\\frac{exp(-(I_xT_y - 0.3)/\\tau)}{\\sum_{c=1}^{C} exp(-(I_xT_c - 0.3T_yT_c)/\\tau)})$ (2)\nHere $I_x$ denotes the embedding of image x, while $T_y$ and $T_c$\nrepresent the embeddings of label y and class c respectively."}, {"title": "C. Class-aware Feature Fusion", "content": "The MMS loss within CLIPood [3] places a heightened\nfocus on cross-modal feature fusion to enhance generalization.\nIt achieves classification by comparing visual features with\ntext embeddings generated through the text prompts introduced\nin Section III-A, thereby leveraging knowledge from the text\nmodality to improve image-text alignment.\nHowever, certain limitations persist within the MMS loss\nframework. First, the cross-modal adaptation overlooks the\nfusion of class-aware visual information. Specifically, while\ncross-modal fusions like $I_xT_c$ and pure text fusions such as\n$T_yT_c$ exist, there is a lack of class-aware image feature fusions.\nThis limitation restricts the model's generalization ability in\ntasks pertinent to visual information. Second, practical im-\nplementation involving mini-batch stochastic gradient descent\n(SGD) [6] raises concerns, especially when dealing with a\nsmall batch size. In such scenarios, the limited number of\nsamples may fail to ensure domain invariance between the\nsource and target domains in the latent space. One paper\n[2] proposed a framework to address the potential invariance\nby using discrimination from one side to update its weak\naugmentor, while employing discrimination from the other\nside to optimize its strong augmentor. Inspired by [2], [5],\nwe propose the integration of a cross-modal mix-up loss to\naddress the latent space issue.\nRandomly draw \u03b7 from Beta(0.2, 0.2) and another training\nsample (x', y') from the dataset, we build the mixed sample\n$(x_m, y_m)$ as:\n$x_m = \\eta x + (1 - \\eta)x'$, (3)\n$y_m = \\eta y + (1 - \\eta)y'$. (4)\nCorrespondingly, the mixed image embedding and text em-\nbedding can be represented as:\n$I_{xm} = \\eta I_x + (1 - \\eta)I_{x'}$, (5)\n$T_{ym} = \\eta T_y + (1 - \\eta)T_{y'}$. (6)\nDiverging from conventional mix-up approaches found in prior\nworks [5], [23], [25], which focus on achieving convex linear\ncombinations between input and output, our method enhances\nclass consistency by comparing the outputs of the original\nand mixed samples. Furthermore, our proposed mix-up loss\nplaces a greater emphasis on modality fusion and interactions,\nspecifically tailored for addressing the zero-shot multi-modal\ngeneralization problem of this paper. The novel mix-up loss\nis then computed as follows:\n$L_{mix} = -(\\frac{exp(-(I_xT_c - 0.3T_yT_c)/\\tau)}{\\sum_{j=1}^{C} exp(-(I_xT_j - 0.3T_yT_j)/\\tau)} +\\frac{exp(-(I_{xm}T_c - 0.3T_{ym}T_c)/\\tau)}{\\sum_{j=1}^{C} exp(-(I_{xm}T_j - 0.3T_{ym}T_j)/\\tau)}||_1)$ (7)\nNow we introduce the overall objective of finetuning the\nMixup-CLIPood model, which is a combination of MMS loss\nand mix-up loss as follows:\n$L_{total} = L_{actual} + \\lambda L_{mix}$, (8)\nwhere is the trade-off parameter in this objective and is set\nto 0.1 in the implementation."}, {"title": "IV. EXPERIMENTS", "content": "Three datasets are used in the experiments. Photo-Art-\nCartoon-Sketch (PACS) [7] is a widely used dataset for do-\nmain generalization, which consists of four domains, namely\nPhoto (1,670 images), Art Painting (2,048 images), Cartoon\n(2,344 images), and Sketch (3,929 images), and each domain\ncontains seven categories. VLCS [8] is a dataset of large\nimages that is popular in the field of out-of-distribution clas-\nsification, with 5 classes (bird, car, chair, dog, and person)\ndistributed equally across 4 domains (Caltech101, LabelMe,\nSUN09, and VOC2007). Office-Home [9] is a medium-size\ndataset containing four domains as Art, Clipart, Product, and\nReal World. Each domain includes 65 classes and the total\nnumber of images is 15,500."}, {"title": "B. Evaluation metrics and protocols", "content": "Adhering to the protocols outlined in CLIPood [3], we\nformulate four tasks for each dataset. In each task, one domain\nserves as the target data for final inference, abstaining from\nparticipation in the training/adaptation processes. Meanwhile,\nthe remaining three domains function as the source data\ninvolved in the finetuning processes of CLIP [4]. Besides the\ndefault backbone ViT-B/16 [17] applied by CLIPood [3], we\nalso adopt another two backbones ViT-B/32 [17] and ViT-L/14\n17], which are larger than ViT-B/16. The evaluation metric\nemployed is the accuracy percentage on each respective target\ndomain."}, {"title": "C. Results", "content": "Our findings, detailed in Table I for PACS [7], Table II for\nVLCS [8], and Table III for Office-Home [9], yield valuable\ninsights and conclusions:\n\u2022 Robust Generalization Across Backbones: Mixup-\nCLIPood exhibits robust generalization capabilities\nacross various backbones. In the majority of tasks,\nCLIPood significantly outperforms CLIP, underscoring\nthe effectiveness of the proposed MMS loss in enhancing\ndomain generalization.\n\u2022 Effectiveness of Mix-up Loss: Our introduced mix-up\nloss proves to be highly effective. Across most tasks, our\nmethod demonstrates a substantial performance improve-\nment compared to CLIPood. This suggests the efficacy\nof our proposed Mixup-CLIPood in addressing the limi-\ntations of existing methods and enhancing generalization\nin multi-modal scenarios."}, {"title": "D. Analysis and discussions", "content": "In Fig. 2 we list two accuracy curves based on our adopted\nBACKBONES, tested using the VLCS dataset and the Office-\nHome dataset, respectively. The results showed that the accu-\nracy increased as the epochs increased and reached stable.\nNotably, the performance varies across different backbone\nmodels. For instance, the ViT-L/14 model demonstrates a\nremarkable starting accuracy of approximately 89%, which\nimpressively surpasses the 90% threshold upon further testing.\nIn contrast, its counterpart, the ViT-B/16, exhibits a more\nmodest trajectory, achieving an 81% accuracy after undergoing\n10 epochs of training with the test data. This comparison\nhighlights the distinct capabilities and learning efficiencies of\nthe respective models under study."}, {"title": "V. CONCLUSION", "content": "This study presents a significant advancement in robust\ndomain generalization for multi-modal object recognition. Our\napproach, which integrates a novel mix-up loss and extends the\nevaluation to larger vision-language backbones, has demon-\nstrated superior performance across various datasets. The\nmeticulous experiments conducted, as detailed in Tables Table\nI, II, and III, have been instrumental in establishing the efficacy\nof our proposed method. These experiments not only validate\nthe robustness of our approach across different backbones but\nalso underscore the utility of the mix-up loss in enhancing\ndomain generalization capabilities in multi-modal scenarios.\nThe results obtained from these comprehensive experiments\nsuggest that our method can effectively bridge the gap in\ndomain generalization tasks, addressing the limitations of\nexisting models like CLIPood. By incorporating class-aware\nvisual information and extending the evaluation framework,\nour study sets a new benchmark in the field and opens av-\nenues for future research in multi-modal learning and domain\ngeneralization."}, {"title": "CONTRIBUTION", "content": "Yuxin Qiao and Keqin Li initiated addressing the limitations\nof CLIPood by proposing a novel mix-up loss function. Yuxin\nidentified inconsistencies between actual loss results and those\ndocumented in a previous paper, which Keqin confirmed\nthrough thorough research. Together, they developed the theo-\nretical framework and tested various loss functions to resolve\nthe issues.\nJunhong Lin, Yuxin Qiao, and Wei Rong evaluated the\nmix-up loss function's reasonableness, with Junhong providing\nvaluable suggestions on complex dataset results. Rong Wei and\nChufeng Jiang continuously revised the loss function, ensuring\nrobustness and accuracy. Chufeng and Keqin handled data\ncollection and analysis, conducting pre-test experiments. Yang\nLuo and Yuxin designed iterative versions of the proposed\nfunctions. Haoyu Yang and Chufeng replicated experimental\nresults and implemented error analysis.\nAll authors contributed to interpreting the results and draw-\ning meaningful insights, which were instrumental in fine-\ntuning the proposed approach."}]}