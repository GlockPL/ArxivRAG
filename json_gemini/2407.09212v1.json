{"title": "Generating SROI- Ontologies via Knowledge Graph Query Embedding Learning", "authors": ["Yunjie He", "Daniel Hernandaz", "Mojtaba Nayyeri", "Bo Xiong", "Yuqicheng Zhu", "Evgeny Kharlamov", "Steffen Staab"], "abstract": "Query embedding approaches answer complex logical queries over incomplete knowledge graphs (KGs) by computing and operating on low-dimensional vector representations of entities, relations, and queries. However, current query embedding models heavily rely on excessively parameterized neural networks and cannot explain the knowledge learned from the graph. We propose a novel query embedding method, AConE, which explains the knowledge learned from the graph in the form of SROI description logic axioms while being more parameter-efficient than most existing approaches. AConE associates queries to a SROI description logic concept. Every SROI concept is embedded as a cone in complex vector space, and each SROI relation is embedded as a transformation that rotates and scales cones. We show theoretically that AConE can learn SROI axioms, and defines an algebra whose operations correspond one-to-one to SROI description logic concept constructs. Our empirical study on multiple query datasets shows that AConE achieves superior results over previous baselines with fewer parameters. Notably on the WN18RR dataset, AConE achieves significant improvement over baseline models. We provide comprehensive analyses showing that the capability to represent axioms positively impacts the results of query answering.", "sections": [{"title": "1 Introduction", "content": "Knowledge Graphs (KGs) such as Wikidata [22], Freebase [5], and YAGO [19] represent real-world facts as sets of triples of the form (s, p, o) which encode atomic assertions as p(s, 0). Graph database engines can store and query KGs efficiently using query languages such as SPARQL [1] that can express a variety of queries that result of combining atomic queries called triple patterns.\n\nThe first step of querying KGs is answering triple patterns with the stored triples. However, when querying incomplete KGs, some triples are not explicitly available in the triple store and thus are not included in the answers, nor into the intermediary results of an answer. To provide plausible answers beyond what is known, these missing triples must be inferred. Figure 1 shows an example, where the available triples (L.Messi, playsFor, ArgentinaNFT), and (ArgentinaNFT, teamWon, WorldCup) should suggest that triple (L.Messi, athleteWon, WorldCup) is a missing triple. KG embedding methods can predict these missing triples [6, 27, 14] by learning how to embed entities and relations into vector representations, which can be points or more complex geometric objects. These methods use these embeddings to answer triple patterns by computing plausibility scores by applying geometric operations.\n\nQuery embedding methods [17, 29, 16] go beyond querying triple patterns. They provide plausible answers to queries that combine triple patterns into first-order logic queries with logical connectives (e.g., negation (\u00ac), conjunction (A), and disjunction (V)). However, current query embedding approaches are restricted to queries with a single unquantified variable and are called tree-form queries because their computation graph is a tree [18]. The tree-form queries correspond to the SROI description logic concepts that do not include the symbols T (the concept for all entities) nor\u22a5 (the concept for no elements), nor concept names (e.g., Athlete or Team), but nominals (i.e., concepts with a unique element like {C.Ronaldo}).\n\nExample 1. The query asking for the birthplaces of the athletes who have won either the World Cup or the Europe Cup, but do not play in the same team as C.Ronaldo can be expressed as the SROI-concept\n\nC = birthPlaceOf . (\u00ac(teamMate\u00af.{C.Ronaldo}) \u03a0\n  (athleteWon.{WorldCup} \n    athleteWon. {EuroCup})),"}, {"title": "2 Related Work", "content": "Logical patterns in knowledge graphs. KG embedding methods aim to learn KG representations that capture latent structural and logical patterns [6, 27, 21, 20, 23]. In particular, RotatE [20] captures a broad range of logical patterns, such as symmetry, inversion, and composition, among others. While these KG embedding methods excel at predicting links, they cannot answer first-order logical queries. BoxE [2] and Box\u00b2EL [10] embed description logic concepts, but are limited to reduced description logics (EL and EL++).\n\nQuery answering. Path-based [26, 12], neural [8, 17, 29, 16, 11], and neural-symbolic [3, 31, 30] methods have been developed to answer (subsets of) queries. Among these methods, geometric and probabilistic query embedding approaches [8, 17, 29, 16] provide an effective way to answer tree-form queries over incomplete and noisy KGs. This is done by representing entity sets as geometric objects or probability distributions, such as boxes [17], cones [29], or Beta distribution [16], and performing neural logical operations directly on them. The Graph Query Embedding (GQEs) [8] was first proposed to answer only conjunctive queries via modeling the query q as single vector q through neural translational operators. However, modeling a query as a single vector limits the model's expressiveness in modeling multiple entities. Query2Box [17] remedies this flaw by modeling entities as points within boxes. This allows Query2Box to predict the intersection of entity sets as the intersection of boxes in vector space. ConE [29] was proposed as the first geometry-based query embedding method that can handle negation via embedding the set of entities (query embedding) as cones in Euclidean space.\n\nAll of the above query embedding methods commonly apply multi-layer perceptron networks for selecting answer entities of atomic queries by relation and performing logical operations. Such"}, {"title": "3 Preliminaries", "content": "The Description Logic SROI. We next present the standard SROI syntax and semantics and assume standard semantics as defined in Baader et al. [4]. For these definitions, we assume three pairwise disjoint sets C, R, and I, whose elements are respectively called concept names, relation names, and individual names.\n\nDefinition 1 (SROI Concept Descriptions). SROI concept descriptions C and relation descriptions R are defined by the following grammar\n\nC ::= T | A | {a}|\u00acC | C\u03a0C | \u2203R.C\nR::= \u00acr\n\nwhere the symbol T is a special concept name, and symbols A, a, and r stand for concept names, individual names, and relation names, respectively. Concept descriptions {a} are called nominals.\n\nGiven two concept descriptions C and D, the expression C \u2286 D is a concept-axiom. Given the relation descriptions R, S, R1, ..., Rn (with n \u2265 1), the expressions R1 \u2218 ... \u2218 Rn-1 \u2286 Rn, Disj(R, S), Trans(R), Ref(R), Irref(R), Sym(R), and Asym(R) are relation-axioms. Given two individual names a, b \u2208 I, a concept description C and a relation description p, a(C) is a concept-assertion and p(a, b) is a relation-assertion.\n\nWe write C = D as an abbreviation for two axioms C \u2286 D and D \u2286 C, and likewise for p\u2081 = p2. We write, \u22a5, C \u2293 D, \u2200p.C as abbreviations for \u00acT, \u00ac(\u00acC \u03a0 \u00acD) and \u00ac\u2203p.\u00acC, respectively.\n\nAn SROI knowledge base (or ontology) K is a triple (R, T, A) where R is a finite set of relation-axioms called the RBox, T is a finite set of concept-axioms called the TBox, and A is a finite set of assertions called the ABox.\n\nAn interpretation I is a pair (\u0394\u00b9, \u00b7\u00b9) consisting of a set \u0394\u00b9, called the domain, and a function \u00b7\u00b9 such that we have for each individual name a \u2208 I, an element a\u00b9 \u2208 \u0394\u00b9; for each concept name A \u2208 C, a subset A\u00b9 \u2286 \u0394\u00b9; and for each relation name r\u2208 R, a relation r\u00b9 \u2286 \u0394\u00b9\u00d7 \u0394\u00b9. An interpretation I is a model of a knowledge base K if and only if all the axioms and assertions in K are satisfied according to the standard semantics defined in Baader et al. [4]. Given two knowledge bases K\u2081 and K2, K1 entails K2, denoted K1 |= K2, if and only if every model I of K\u2081 is also a model of K2.\n\nKnowledge graphs and queries. We next define knowledge graphs in terms of SROI- knowledge bases, and tree-form queries in terms of SROI concepts.\n\nDefinition 2 (Knowledge Graph). A knowledge graph G is a SROI knowledge base K whose RBox is empty, and its T contains a unique axiom T \u2286 {a1} \u222a ... \u222a {an}, where {a1,..., an} is the set of all individuals names occurring in the ABox. This axiom is called domain-closure assumption.\n\nDefinition 3 (Tree-form query). Given a knowledge graph G, and an individual name x that does not occur in G, a tree-form query q is an assertion C(x) where C is a SROI concept description. The answers to query q, are all individuals a occurring in G such that G |= C(a).\n\nInformally, the query answering task over incomplete data consists of predicting answers to a query q over a knowledge graph pattern G given only a subset G' \u2286 G. Knowledge graph embeddings to generalize the knowledge on G' to predict answers on G."}, {"title": "4 SROI concepts and Cone Algebra", "content": "In this section, we present an algebra of cones in the complex plane, called the Cone Algebra, we show the correspondence between this algebra and SROI concepts, and we identify the subset of the SROI axioms that are expressible with this algebra.\n\nDefinition 4 (Cone). A cone C(\u03b1, \u03b2) is a region in the complex plane C determined by a pair of angles (\u03b1, \u03b2) \u2208 R\u00b2 as follows:\n\nC(\u03b1, \u03b2) = {e^{i\u03b8} : \u03b1 \u2264 \u03b8 and \u03b8 < \u03b2}.\n\nThe empty cone, denoted C\u22a5, is the cone such that \u03b1 > \u03b2. A singleton cone with angle \u03b1, denoted C\u03b1, is a cone such that \u03b1 = \u03b2. A proper cone, denoted C\u03b1\u2192\u03b2, is a cone where \u03b1 + 2\u03c0 > \u03b2. A full cone, denoted CT, is the cone such that \u03b1 + 2\u03c0 < \u03b2.\n\nNotice that e^{i\u03b8} = e^{i\u03b8+2k\u03c0}, for every natural number k. Thus, the same cone can be determined with multiple combination of angles. Notice that the intersection is not closed on the set of cones. Indeed, the region C(0, 3/2\u03c0) \u2229 C(\u03c0, 7/4\u03c0) is not a cone. This hinders the use of cones to represent SROI concepts. To overcome this limitation, we can embed concepts on sets of cones.\n\nDefinition 5 (Multicone algebra). A multicone MC(C) is a region determined by a set of cones C as follows:\n\nMC(C) = \u222a C(\u03b1,\u03b2)\n C(\u03b1,\u03b2) \u2208C\n\nThe multicone algebra is the algebra over the set of multicones defined by the binary operations \u2229 and \u222a that are defined as the set operations over the multicone regions.\n\nProposition 1. The multicone algebra is a field with sum \u222a and multiplication \u2229, where the identity of \u222a is MC\u22a5 = MC({C\u22a5}), the identity is \u2229 is MCT = MC({CT}), and the additive and multiplicative inverse of a multicone MC(C), denoted MC(C), is the multicone MC+ \\ MC(C).\n\nDefinition 6 (Rotation algebra). Given a triple (\u03b8, \u03b3, \u03b4) \u2208 R\u00b3 where 0 \u2264 \u03b3 \u2264 2\u03c0, a rotation R\u3008\u03b8, \u03b3, \u03b4\u3009 is a function that maps every singleton and proper cone C(\u03b1, \u03b2) to the cone C(\u03b1', \u03b2') such that\n\n\u03b2' + \u03b1' = \u03b8(\u03b2 + \u03b1),\n\u03b2' \u2013 \u03b1' = \u03b3(\u03b2 \u2013 \u03b1) + \u03b4,\n\nand that maps the empty and the full cone to themselves:\n\nR(\u03b8, \u03b3, \u03b4) (C\u22a5) = C\u22a5,  R(\u03b8, \u03b3, \u03b4)(CT) = CT.\n\nThe rotation parameters \u03b8, \u03b3, and \u03b4 are called the rotation angle and the aperture factor, and the aperture adding. We call aperture-multiplicative and aperture-additive rotations to rotations of the respective forms R(\u03b8, \u03b3, 0) and R(\u03b8, 1, \u03b4).\n\nWe call rotation algebra to be the algebraic structure whose ground set is the set of rotations over cones, and has a binary operation \u2218 denoting function composition (i.e., (f \u2218 g)(x) = g(f(x)))."}, {"title": "5 Expressing Logical Patterns", "content": "In this section, we present how logical patterns are expressed in the multicone embedding space. Given the relation names r1, r2, r3 \u2208 R, correspond to the following SROI axioms:\n\nr1 \u2286 r2 (role containment), r1 \u2218 r2 \u2286 r3 (composition),\nTrans(r1) (transitivity), r1 = r2\u207b\u00b9 (inverse),\nSym(r1) (symmetry), Asym(r1) (asymmetry).\n\nRole containment pattern. Given two rotations R& = R(\u03b81,1, \u03b41) and S& = R(\u03b82,1, \u03b42) with \u03b81 \u2260 \u03b82 and \u03b41 \u2260 \u03b42, it never hold that R& \u2286 S&. Indeed, we can always consider a cone with a sufficiently small aperture angle (e.g., a singleton cone) to show that this inclusion does not hold. On the other hand, for rotations R& = R(\u03b81,1, \u03b41) and S& = R(\u03b82, 1, \u03b42) the inclusion can hold, even with distinct values for \u03b81 and \u03b82.\n\nProposition 4. Given two rotations R& = R(\u03b81,1, \u03b41) and S& = R(\u03b82, 1, \u03b42), the axiom R \u2286 S holds if \u03b82 + \u03b42/2 > \u03b81 + \u03b41/2 and \u03b82 - \u03b42/2 < \u03b81 - \u03b41/2.\n\nProposition 5. Given two rotations R& = R(\u03b81, \u03b31, \u03b41) and S& = R(\u03b82, \u03b32, \u03b42), the axiom R \u2286 S holds if \u03b31 \u2264 \u03b32 and \u03b41 \u2264 \u03b42.\n\nComposition and transitivity patterns. The rules for composition and transitivity are a corollary of Proposition 4 and Proposition 5. For space limitations we omit the conditions for the Transitivity pattern. The conditions can be obtained from equivalence of axioms Trans(R) and RRR.\n\nCorollary 6. Given three rotations R\u2081 = R(\u03b81,1, \u03b41), R\u2082 = R(\u03b82, 1, \u03b42), and R\u2083 = R(\u03b83, 1, \u03b43), the axiom R1 \u2218 R2 \u2286 R3 holds if |\u03b83 - (\u03b81+\u03b82)| < \u03b43 - (\u03b41 + \u03b42), \u03b41 \u2265 0, \u03b42 \u2265 0, and \u03b43 \u2265 0.\n\nCorollary 7. Given R\u2081 = R(\u03b81, \u03b31, \u03b41), R\u2082 = R(\u03b82, \u03b32, \u03b42), and R\u2083 = R(\u03b83, \u03b33, \u03b43), the axiom R1 \u2218 R2 \u2286 R3 holds if \u03b31\u03b32 \u2264 \u03b33, \u03b41 + \u03b42 - \u03b43 \u2264 0, \u03b41 \u2265 0, and \u03b42 \u2265 0."}, {"title": "6 Tree-form Query Answering with AConE", "content": "To accommodate the learning of logical patterns in query answering over KGs, we propose a new model, AConE, which embeds KGs according to the multicone embedding (Definition 7). Our approach distinguishes itself from ConE [29], which represents cone embeddings using two-dimensional vectors in real space. We reframe these embeddings in the complex plane. By doing so, we introduce an inductive bias that facilitates the learning of logical patterns. This bias is achieved by combining the embeddings with relational rotations on cones, enhancing the model's ability to capture complex relationships and patterns. In AConE, the embedding of a tree-form query q = C(x) is parameterized by a pair q = (hu, hL) \u2208 Cd, where d is the embedding dimension, |hu|2 = 1, |hL|2 = 1 with |\u00b7|2 being the L2 norm. The vectors hu and hL represent the counterclockwise upper and lower boundaries of the cone, such that\n\ne^{i\u03b8u} = e^{i(\u03b8ax+\u03b8ap)},  e^{i\u03b8L} = e^{i(\u03b8ax-\u03b8ap)}, (1)\n\nwhere \u03b8ax \u2208 [-\u03c0, \u03c0)d represents the angle of the symmetry axis of the cone and \u03b8ap \u2208 [0, 2\u03c0]d represents the cone aperture.\n\nFor each index j with 1 \u2264 j \u2264 d, we write [v]; for the value of the j-th component of a vector v. The i-th component of q represents the proper cone C([\u03b8L]j, [\u03b8u]j) if [\u03b8ap]j > 0 and [\u03b8ap]j < 2\u03c0, the full cone CT if [\u03b8ap]j = 2\u03c0, and the singleton cone C[\u03b8ax], if \u03b8ap = 0. Notice that we do not provide an encoding for the empty cone because interesting queries are satisfiable. The queries are thus modeled as multidimensional cones according to Definition 7, and each entity answering the query is modeled as a vector h* = e^{i\u03b8*} in the cone parametrized with the query embedding.\n\nRelational Rotation. Given a set of entities S \u2286 I and a relation r\u2208 R, the transformation operator for r selects the entities S' = {e' \u2208 I : G |= r(e,e'), e \u2208 S}. To this end, we model r with a vector r = (ru, rL) \u2208 C2\u00d7d encoding a counterclockwise rotation on query embeddings about the complex plane origin such that |ru| = 1 and |rL| = 1. The rotation r transforms a query embedding q = (hu, hL) into a query embedding P\u2203(q, r) = q' = (hu', hL') where\n\nhu' = hu \u00b7 ru,  hL' = hL \u00b7 rL, (2)\n\nwhere \u00b7 is the Hadamard (element-wise) product (i.e., for each component j, [hu']j = [hu]j [ru]j and [hL']j = [hL]j [rL]j). If a pair ([ru]j, [rL]j) is seen as a cone, with axis angle [\u03b8ax,r]j and aperture angle [\u03b8ap,r]j, then the rotation corresponds to the aperture-additive rotation R\u3008[\u03b8ax,r]j, 1, [\u03b8ap,r]j\u3009 (see Definition 6). That is, the axis and aperture angles of q' are \u03b8ax' = \u03b8ax + \u03b8ax,r, and \u03b8ap' = \u03b8ap + \u03b8ap,r. Figure 3 illustrates how does AConE model the logical patterns described in Section 5 through the example on a single dimension.\n\nIntersection. The intersection of a set of query embeddings Q = {q1, ..., qn}, denoted P\u2229(Q) = q' is defined with the permutation-invariant functions SemanticAverage and CardMin [29], which calculate the center \u03b8ax' and the aperture \u03b8ap' of the resulting cone encoded by q' as follows:\n\n\u03b8ax' = SemanticAverage({qk}k=1^n),\n\u03b8ap' = CardMin({qk}k=1^n).\n\nSemantic Average is expected to approximate the axis of the cone resulting from the intersection of the input cones. CardMin predicts the aperture \u03b8ap' of the intersection set such that [\u03b8ap'] i should be no larger than the aperture of any cone qi \u2208 Q, since the intersection set is the subset of all input entity sets. Both functions, Semantic Average and CardMin, use a neural network to improve the results. We extend the details of them in Appendix D.\n\nUnion. Given a set of query embeddings Q = {q1,..., qn}, the union operator P\u222a(Q) returns the set Q. Intuitively, for each k where 1 \u2264 k \u2264 d, the set Q encodes a multicone embedding for the query (q1\u222a...\u222aqn)(x), which represents the multicone MC(C1, ..., Cn) where Ci is the cone encoded in [qi]k.\n\nNotice that multicones cannot be further used as input of other operations because all AConE operations are applied on cones. To compute queries, we thus follow [17], translating queries into disjunctive normal form, so we only perform the disjunction operator in the last step in the computation graph.\n\nNegation. The negation of a cone q = (hu, hL), denoted P\u00ac(q), is the cone q' that contains the entities in the complement of q, that is, q' = (hL, hu)."}, {"title": "6.2 Optimization", "content": "Learning Objective. Given a set of training samples, our goal is to minimize the distance between the query embedding q and the answer entity vector h*, while maximizing the distance between q and its negative samples. Thus, we define our training objective as\n\nL = - log \u03c3(\u03b3 - dcomb(h*, q)) - 1/k \u03a3 log \u03c3(d(hi, q) - \u03b3), (4)\n\nwhere dcomb is the combined distance defined below, \u03b3 is a margin, h* is a positive entity, and hi is the i-th negative entity, k is the number of negative samples, and \u03c3 represents the sigmoid function.\n\nCombined Distance. Inspired by [29, 17], the distance between q and h* is defined as a combination of inside and outside distances, dcomb(q, h*) = do(q, h*) + \u03bbdi(q, h*):\n\ndo(q, h*) = min{||hu - h* ||1, || hL - h* ||1}, (5)\ndi(q, h*) = min{||hax - h* ||1, ||hu \u2013 hax ||1}, (6)\n\nwhere || \u00b7 ||1 is the L\u2081 norm, hax represents the cone center, and \u03bb \u2208 (0, 1). Note that dcomb can only be used for measuring the distance between a single query embedding and an answer entity vector. Since we represent the disjunctive queries in Disjunctive Normal Form as a set of query embeddings Q, the distance between the answer vector and such set of embeddings is the minimum distance:\n\ndcomb(Q, h*) = min{dcomb (q, h*) : q \u2208 Q}. (7)"}, {"title": "7 Experiments and Analysis", "content": "In this section, we answer the following research questions with experimental statistics and corresponding case analyses. RQ1: how well does AConE improve query answering on incomplete KGs? RQ2: How is the improvement of results related to the capturing of logical patterns?\n\nDataset and Query Structure. For a fair comparison with the baseline models, we use the same query structures and logical query datasets from NELL-QA [26] and WN18RR-QA [9], and the open-sourced framework created by [16] for logical query answering tasks. Figure 4 illustrates all query structures in experiments. We trained our model using 10 specific query structures and tested it on all 14 query structures to evaluate the generalization ability of our model regarding unseen query structures. More experimental details can be found in the supplementary file.\n\nEvaluation Metrics. We use Mean Reciprocal Rank (MRR) as the evaluation metric. Given a sample of queries Q, MRR represents the average of the reciprocal ranks of results, MRR = \u03a3 1/|Q| \u03a3 1/ranki\n\n7.2 RQ1: how well does AConE improve query answering on incomplete KGs?\n\nWe evaluate AConE on two logical query-answering benchmark datasets that include a variety of complex logical patterns: NELL-QA created by [16] and WN18RR-QA by [9]. AConE is compared with various query embedding models that also define query regions as vectors, geometries, or distributions, including GQE [8], Query2Box (Q2B) [17], BetaE [16], ConE [29], and LinE [9]. To ensure a fair comparison, we indeed referenced the results of all baseline models from corresponding papers. In the case of ConE's performance on the WN18RR dataset, we conducted our own experiments due to the absence of previously reported results. This involved rerunning ConE with an optimized search for hyperparameters to generate the relevant data.\n\nMain Results. Table 2 summarizes the performance of all methods on answering various query types. Compared with baselines that can only model queries without negation. Overall, AConE (aperture-additive) outperforms baseline methods on the majority of query types in Figure 4 while achieving competitive results on the others. Specifically, AConE consistently achieves improvements on non-negation queries on all datasets. Furthermore, our model brings notable improvement over baseline models on the WN18RR dataset, where the average accuracy of AConE is 18.35% higher than that of the baseline models.\n\nOn the other hand, we observe that AConE performs closely to the baseline models on answering query types involving negation on dataset NELL, in spite of its consistently good performance on WN18RR. This may be due to two factors: firstly, handling negation is a challenging research question for complex logical query-answering tasks. When it comes to negation queries, all the current models show inferior performance compared to their performance on non-neg queries. Secondly, modeling negation as the complement leads to bias in prediction and high uncertainty in a large number of answers for negation queries. Thus, we leave the task of further improving our model on negation queries for future research.\n\n7.3 RQ2: How is the improvement of results related to the capturing of logical patterns?\n\nTo investigate the influence of logical patterns learning on the query answering model, we provide an ablation analysis considering both model and data:\n\nModel Perspective. We compared the performance of three AConE's variants: AConE (Base), AConE (aperture-multiplicative) and AConE (aperture-additive). They utilize alternative relational rotating transformation strategies for capturing the logical patterns while keeping other modules unchanged. Note that AConE (Base) represents AConE model with any neural relational transformation module. This configuration operates similarly to ConE in terms of its foundational approach. Consequently, we used the results of ConE as a direct representation of \"AConE (Base)\". This decision was based on their operational similarity, ensuring a fair and coherent presentation of our findings."}, {"title": "8 Analysis on Model Parameters", "content": "The acquisition of logical patterns not only enhances the model performance but also alleviates the reliance on excessively-parameterized neural networks in existing methods."}, {"title": "9 Conclusion and Future Work", "content": "In this work, we investigate the important yet understudied impact of logical pattern inference on the query answering task. We propose a new embedding method, the multicone embedding, and study its algebraic structure and capability to express SROI ontologies. We show some limitations of the method, like breaking the tautology \u2203R.(CD) = \u2203R.C \u2203R.D, the bias favoring the commutativity, and the (general) non-existence of inverse rotations.\n\nOur practical method, AConE, is motivated by the multicone algebra, but it has differences. First, multicones consisting of multiple cones are difficult to manage due to the unconstrained number of parameters they require. Hence, we limit the representation of queries to single multicones (i.e., cones). Since this simplification makes the cone intersection no longer closed, we define it as a neural operation returning a single cone. We limit rotations to be aperture-additive because they outperform aperture-multiplicative rotations (see Table 4). Although the experiment datasets had no concept names, multicone embeddings support concept names. We use these datasets to make a fair comparison with the existing methods, which do not support concept names. Since most real datasets include concept names, and concept names are a key component of knowledge representation, we plan to extend our research to datasets with concept names. Furthermore, we think that AConE can be extended for knowledge graphs including also non-empty RBoxes and TBoxes."}, {"title": "Ethics Statement", "content": "The authors declare that we have no conflicts of interest. This article does not contain any studies involving business and personal data."}, {"title": "A Experimental Details", "content": "The source code of AConE, the benchmark datasets, and the instructions for the reproduction of our experimental results are attached as zip files.\n\nDescription on Benchmark Datasets For a fair comparison with the baseline models, we use the same query structures and logical query datasets from NELL-QA [26] and WN18RR-QA [9], and the open-sourced framework created by [16] for logical query answering tasks.\n\nHyperparameters and Computational Resources. All of our experiments are implemented in Pytorch [15] framework and run on four Nvidia A100 GPU cards. For hyperparameters search, we performed a grid search of learning rates in {5\u00d710\u22125, 10-4,5\u00d710\u22124}, the batch size in {256, 512, 1024}, the negative sample sizes in {128, 64}, the regularization coefficient \u03bb in {0.02, 0.05, 0.08, 0.1} and the margin \u03b3 in {20, 30, 40, 50}.\n\nB Classification Process of Test Queries\n\nThis paragraph describes the process for identifying subgroups of test queries with different relation patterns. We first mined rules from the triples-only dataset NELL995 [25] using the rule mining tool AMIE [7]. The rules that are significantly supported by a large amount of data and have body coverages greater than 0.2 are selected. These rules are then categorized into six types: symmetry, inversion, composition, containment, transitivity and other. Each category includes a list of relations that encompass relation patterns. Using the selected relations, the test dataset of the logical query dataset NELL [16] is traversed, and queries are classified into six subgroups: symmetry, inversion, composition, containment, transitivity and other. A query is classified into a subgroup based on the presence of at least one relation from the corresponding relation subgroup. The other subgroup does not include any of these patterns.\n\nC Model Robustness Check\n\nIn this section, we check the model robustness of AConE by running AConE 10 times with different random seeds. The mean performance and standard deviations are reported in Table 8. The slight variances in the results indicate that AConE is robust to the different initializations of model parameters and our reported improvements are not due to randomness."}, {"title": "D Intersection Operator", "content": "In this section, we explain the details of two important components of the intersection operator, SemanticAverage() and CardMin().\n\nSemanticAverage() is expected to compute the semantic center \u03b8'\u03b1x of the input {(\u03b8j,ax, \u03b8j,ap)}j=1^n. Specifically, the computation process is provided as:\n\n[x; y] = [\u03a3 aj o cos(\u03b8j,ax); \u03a3 aj o sin(\u03b8j,ax)], (8)\n\n\u03b8'\u03b1x = Arg(x, y),\n\nwhere cos and sin represent element-wise cosine and sine functions. Arg() computes the argument given x and y. aj \u2208 Rd are attention weights such that\n\n[aj]k = exp([MLP([\u03b8j,ax - \u03b8j,ap/2; \u03b8j,ax+ \u03b8j,ap/2])]k) / \u03a3 exp([MLP([\u03b8m,ax \u2014\u03b8m,ap/2;\u03b8m,ax+\u03b8m,ap/2])]k), (9)\n\nwhere MLP: R2d \u2192 Rd is a multi-layer perceptron network, [\u00b7;\u00b7] is the concatenation of two vectors.\n\nCardMin() predicts the aperture \u03b8'ap of the intersection set such that [\u03b8'ap] i should be no larger than any 0,ap, since the intersection set is the subset of all input entity sets.\n\n[\u03b8ap]i = min{01,ap,..., 0m,ap}\u00b7 \u03c3([DeepSets({(\u03b8j,ax, \u03b8j,ap)}j=1)]i) (10)\n\nwhere DeepSets({(0j,ax, 0j,ap)}j=1) [28] is given by\n\nMLP(\u03a3=1 MLP([\u03b8j,ax \u2014 \u03b8j,ap/2; \u03b8j,ax + \u03b8j,ap/2]))."}]}