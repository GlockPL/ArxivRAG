{"title": "Citekit: A Modular Toolkit for Large Language Model Citation Generation", "authors": ["Jiajun Shen", "Tong Zhou", "Suifeng Zhao", "Yubo Chen", "Kang Liu"], "abstract": "Enabling Large Language Models (LLMs) to generate citations in Question-Answering (QA) tasks is an emerging paradigm aimed at enhancing the verifiability of their responses when LLMs are utilizing external references to generate an answer. However, there is currently no unified framework to standardize and fairly compare different citation generation methods, leading to difficulties in reproducing different methods and a comprehensive assessment. To cope with the problems above, we introduce Citekit, an open-source and modular toolkit designed to facilitate the implementation and evaluation of existing citation generation methods, while also fostering the development of new approaches to improve citation quality in LLM outputs. This tool is highly extensible, allowing users to utilize 4 main modules and 14 components to construct a pipeline, evaluating an existing method or innovative designs. Our experiments with two state-of-the-art LLMs and 11 citation generation baselines demonstrate varying strengths of different modules in answer accuracy and citation quality improvement, as well as the challenge of enhancing granularity. Based on our analysis of the effectiveness of components, we propose a new method, self-RAG SNIPPET, obtaining a balanced answer accuracy and citation quality. Citekit is released at https://github.com/SjJ1017/Citekit.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (OpenAI, 2024; AI@Meta, 2024) nowadays demonstrate strong performance on Question Answering (QA) (Kamalloo et al., 2023) on different scenarios such as Commonsense QA (Talmor et al., 2019), long-form QA (Stelmakh et al., 2023; Min et al., 2020) and Multi-hop QA (Ho et al., 2020; Yang et al., 2018), but they can still inevitably produce hallucinated responses that are non-factual (Huang et al., 2023), nonsensical or irrelevant to the input(Xu et al., 2024b), reflecting the ongoing challenges in ensuring factual accuracy. Given the challenges above, Retrieval Augmented Generation (RAG) (Lewis et al., 2021), which leverages facts from external unstructured knowledge helps to enhance the reliability of LLMs and can be made more faithful and verifiable by generating citations (Gao et al., 2024). Asking models to generate citations can improve the factual correctness of answers (Gao et al., 2023b), and the citations that link to the original references will allow readers to easily verify the source of the response, making the answers of models more verifiable and explainable. Figure 1 shows how citation generation can help users become more assured of the answer. In Figure 1, the answer without citation inconsistently states both 1956 and 1976 as the dates for the passage of right-to-work legislation in Louisiana, leading to uncertainty about the actual timeline. If citations are included, readers can scrutinize the reference to understand clearly why there are 2 different dates to the question.\nGiven the urgent need, ALCE (Gao et al., 2023b)"}, {"title": null, "content": "is the first attempt systematically to develop some basic methods to enable LLMs to generate citations in QA tasks and propose metrics for evaluating the quality of citations, showing there is still room for improvement concerning citation generation. Following ALCE's contribution, there are other methods that either use training (Huang et al., 2024; Li et al., 2024a; Ye et al., 2024) or construct complicated pipelines to enhance the ability of generative models in citing external documents (Zhang et al., 2024; Hennigen et al., 2024; Lee et al., 2023).\nFor example, Fierro et al. (2024) found the black-box generation is not factually faithful, so they use blueprint models to generate plans or blueprints and the output can be traced back to the blueprint. Verifiable Text Generation (VTG) (Sun et al., 2024) uses verifiers, evidence finder, retriever and a simplifier to simplify citations to improve citation quality. Another category related to citation generation is LLM attribution (Jain et al., 2023; Xu et al., 2024a; Gao et al., 2023a), which refers to the capacity of an LLM to generate and provide evidence (Li et al., 2023). For instance, Recitation Augmented Language Models(Sun et al., 2023), learns to sample documents from LLM's self-knowledge and construct a path of attributing passages to generate the final answer.\nDespite a variety of state-of-the-art methods, there are still two problems:\nChallenges in reproducibility and improvement: Different works are distinguished largely on their implementation, making it difficult to reproduce and improve. There are still some works not transparent enough, with inaccessible codes, and different works are realized using different frameworks and in different coding styles, hence the difficulty in generalization and a lack of flexibility.\nNeed for comprehensive and fair comparisons: There is a lack of comprehensive and fair horizontal comparisons between various methods. Previous works have not been examined on new emerging Large Language Models, and since there are now some new metrics like citation granularity, a comprehensive evaluation is needed. Works following ALCE (Sun et al., 2024; Lee et al., 2024; Slobodkin et al., 2024) only compare their methods to ALCE baselines. Others (Asai et al., 2023; Sun et al., 2023) that focus more on answer accuracy are not evaluated on the citation benchmark, which means a lack of horizontal comparisons between SOTAS.\nGiven the problems above, a toolkit that unifies"}, {"title": null, "content": "different methods is crucial for researchers to realize their workflow quickly and facilitate fair and straightforward horizontal comparisons between various methods and efficient improvement and innovation. However, there is still no such open-source toolkit to compare these state-of-the-art methods conveniently and fairly.\nTo this end, we present Citekit, an open-source, extensible, and user-friendly modular toolkit that can be easily used to construct a pipeline for citation generation by combining preset modules or propose an enhanced design by utilizing customized modules and modifying their interconnections accordingly.\nCitekit offers four different types of modules: INPUT, GENERATION MODULE, ENHANCING MODULE, and EVALUATOR, which are combined in a pipeline. Input contains automatic components for loading data and making prompts, and is accessible by other modules. GENERATION MODULE is for response generation, where LLM follows the instructions and uses retrieved documents to generate an answer with citations or explicit reference to the documents for further process, featuring a wide range of LLM's supported and adaptive generation modes, it can satisfy different need for various generation task. ENHANCING MODULE contains some components that can assess, retrieve, plan, and edit, and they can be customized for different tasks and even combined into clusters. EVALUATOR integrates different metrics to evaluate the output of the pipeline and other new metrics could also be defined and utilized. For training-based methods, a parallel data export component can output evaluation results that could be an indicator of the performance of a method and can also be used as training data for supervised learning and Reinforcement Learning (RL). The toolkit also provides significant versatility in customizing new modules to quickly and conveniently realize an improved method. We provide 11 baseline recipes using Citekit to comprehensively and fairly compare these SOTA methods with the latest models Llama3 and GPT-40 across 8 various metrics on answer factuality and citation quality. Our experiments show that despite each method having its special strengths, it is challenging to achieve optimal results in both answer accuracy and citation quality. Therefore, we used Citekit to combine the most effective functional modules, achieving a balance in both answer accuracy and citation quality.\nOur contributions can be summarized as follows:"}, {"title": null, "content": "\u2022 We propose a framework that modularizes the citation tasks, with 4 main modules decomposing citation pipelines into request wrapping, generating, enhancing, and evaluating to unify different methods. The framework contains 14 components and 16 functions to define complicated interconnections between modules and satisfy different needs for citation generation tasks.\n\u2022 We design and complete Citekit, an easy-to-use and extensible toolkit based on our framework to help reproduce, compare, and combine different methods. We pre-defined 11 recipes to cover 11 citation generation paradigms, all derived from state-of-the-art research.\n\u2022 We conduct a comprehensive evaluation and comparison of the existing 11 baselines on 2 SOTA LLMs and propose an improved new method, self-RAG SNIPPET, by combining effective components. Our method achieves balance in both answer accuracy and citation quality, showing the convenience of verifying new methods."}, {"title": "2 System Design", "content": "In this section, we will introduce the design of Citekit and detail distinct functionalities of different modules mentioned in \u00a71, and how they are connected to each other to form an integrated working pipeline of citation generation. We show our design in Figure 2"}, {"title": "2.1 INPUT", "content": "INPUT of a citation generation pipeline or a particular module contains requested and retrieved documents.\nDocuments: In RAG, the input contains some initial documents with knowledge relevant to the question, and the designated documents will be stored for further use and evaluation, each attached automatically with a unique index to trace.\nRequest: The request of a specific module is among three options: (1) from the user's query, such as questions. (2) from the module itself, like demonstrations and instructions for a task-specific LLM. (3) a dynamic data flow that changes in the process, such as the response from the upstream module."}, {"title": "2.2 GENERATION MODULE", "content": "GENERATION MODULE contains a Large Language Model for generating content according to according to the requirements. This module, allows users to load a large language model or use some LLM API for generating responses. The input of GENERATION MODULE is a natural language query and the output is a response from LLM. A GENERATION MODULE supports different frameworks, including huggingface, vllm, fastchat, and APIs like openai API to implement the generation according to the need. To fit into different pipelines, GENERATION MODULE can be called either iterative or direct."}, {"title": "2.3 ENHANCING MODULE", "content": "Works that use one or more external modules to enhance the quality of citations can be classified into four categories: retriever, planner, assessor, and editor, as shown in Table 1. They can be used individually or collaboratively, providing sufficient flexibility for the construction of a citation generation pipeline."}, {"title": "2.3.1 Real Time retriever", "content": "A Real-Time Retriever, utilized to retrieve external documents from a corpus, is helpful when LLMs find it difficult to attribute from existing retrieved documents. The input of the retriever is a query and the output contains some retrieved documents or chunks. In the design of Citekit, documents or chunks returned will be automatically added into the pipeline for later evaluation, with a unique"}, {"title": "2.3.2 Ahead Planner", "content": "Planners will process the query and documents in advance before it is sent to LLMs for generation. Taking the query and relevant documents as input, An ahead planner such as blueprint modules and attributers generate guidance that the GENERATION MODULE can follow to improve the citation quality. The generated information like plans or attributing routes serve to help GENERATION MODULE better understand and extract knowledge, while also making the answer more traceable."}, {"title": "2.3.3 Quality Feedbacker", "content": "A Feedbacker, once defined and plugged into the pipeline, can automatically evaluate the initial answer in the process to guide the modules to generate a better response. The input contains the initial answer, and the output of a feedbacker may be a quantitative value (scorer), or the answer with the highest score by some pre-defined metrics (reranker)."}, {"title": "2.3.4 Output Editor", "content": "An output editor can modify the response for a better citation or answer quality. The input of an editor contains an answer and the output is a new answer edited using information from the data storage or the other feedback from the input. It can either revise the answer, like correct the factual problems in the answer, or modify the citation, including simplifying it to make it more precise (simplifier)."}, {"title": "2.3.5 Extensibility", "content": "In addition to the predefined components in ENHANCING MODULE, researchers can also create a new component by just setting a corresponding prompt template and a calling function that demonstrates the logic of execution. Any module that is inherited from the base class will have the ability to be connected to the pipeline and send output to the target module so the new module can be easily plugged into the pipeline."}, {"title": "2.4 EVALUATOR", "content": "EVALUATOR is a module that evaluates or scores the output. If the EVALUATOR is plugged in, the output of the pipeline will be automatically sent to"}, {"title": null, "content": "it, and other information such as reference answers will also be passed into it for evaluation. EVALUATOR have access to the initial corpus as well as the newly retrieved documents during the whole process. Finally, a result will be returned by the EVALUATOR.\nThere are some predefined metrics that can be set easily, such as ROUGE for answer quality and MAUVE for fluency, citation precision and recall in ALCE benchmark, a citation precision and recall metric with granularity for citation quality, and dataset-specific metrics for answer correctness (e.g. STR-EM for ASQA, claims for ELI5).\nManually defined other metrics are also possible once the evaluation function and the specific data in the pipeline for evaluation are defined, allowing users to implant an existing metric into a pipeline or a new one."}, {"title": "2.5 Pipeline", "content": "A pipeline is a class for managing data flow and organizing different modules and corresponding components. It serves as a runner to start the citation generation process and control the input and output of modules contained in itself.\nFor data management, input (e.g. question to be answered, ground truth for evaluation) and documents retrieved are stored respectively as dictionaries with keys and values in the pipeline, and they are both accessible by modules.\nFor module organization, modules that are connected to the pipeline can take an input and generate an output, and the output will be sent to target a module by predefined conditions.\nCitekit offers more flexible options for more complicated pipelines. For instance, multiple responses can be sent to the next module in parallel for independent processes. They can also be sent iterative for sequential needs. Besides, modules that simply form a sequence can be connected in order and be used like a complete module.\nThe pipeline is also extensible, as users can plug in different modules. Different ways of connection will make the pipeline a sequence, a loop, a tree, or other structures."}, {"title": "3 Usage", "content": ""}, {"title": "3.1 Realization of SOTA method.", "content": "To construct a citation generation pipeline with Citekit, users can use predefined recipes to define some preset modules and combine them. For"}, {"title": "3.2 Customized pipeline modification.", "content": "The Attribute First, then Generate design uses only an ahead planner. If we want to extend the pipeline by plugging in a verifier to ask the GENERATION MODULE to regenerate with new retrieved documents if the statement is not entailed by the documents. We will add a loop after the initial output to GENERATION MODULE. Figure 4 shows an example of the efficiency of improving an existing method."}, {"title": "4 Evaluation", "content": ""}, {"title": "4.1 Baselines and metrics", "content": "We evaluate 11 baselines in total using the state-of-the-art open-source and closed-source Large Language Models, GPT-40 (OpenAI, 2024) and Llama3-8B-Instruct (AI@Meta, 2024), respectively on ASQA dataset. ALCE VANILLA, SNIPPET, and SUMM directly prompt the LLM to generate citations, using full documents, snippets, and summaries respectively. ALCE INTERACT (Gao et al., 2023b) uses document summaries and interactively provides full documents. AAR (Lee et al., 2024) asked the LLM to revise the answer, while VTG (Hennigen et al., 2024) will verify the answer and retrieve more supplementary documents for regeneration if needed. Citation Enhanced (Li et al., 2024b) method retrieves documents after generation, and Recitation Augmented (Sun et al., 2023) sample documents from self-knowledge of LLMs. Attribute First, then Generate (Slobodkin et al., 2024) will provide attributing spans to help the generation, while Blueprint (Fierro et al., 2024) provides some questions to guide the generation. For self-RAG (Asai et al., 2023), we use our prompt"}, {"title": null, "content": "version instead of a trained Model (Appendix A.2), to retrieve documents and generate sentence-by-sentence. To further improve this method on the citation granularity, we use snippets to make the model cite a smaller span as our improved method as self-RAG SNIPPET. We show the improved prompt in Appendix 13\nAs for metrics, we use metrics from ALCE, including fluency, correctness, rouge, citation recall, and precision. Besides, we also evaluate the correct prediction of citation need and the citation granularity."}, {"title": "4.2 Settings", "content": "We set max generated tokens to 500 to avoid too long answers and use \\n as stop token. For Llama3-8B-Instruct, we use the model from huggingface and set the temperature to 0.5. and other configurations by default. For GPT-40, we use the openai API. During our experiment, we used the same prompt for the two models.\nFor retrieving documents relevant to the query, we use 5 documents by default. However, for ALCE SUMM, ALCE SNIPPET, and ALCE INTERACT, we use 10 documents as they show the"}, {"title": null, "content": "short summaries and snippets from the documents. Citation Augmented and self-RAG use real-time retrievers instead of a fixed number of document inputs, and we configured our retrievers to return the top-1 document at a time.\nFor evaluation of citation quality, we adopt a TRUE model (Honovich et al., 2022) to verify if the cited documents could entail the generated statement."}, {"title": "4.3 Results and analysis", "content": "We show the full results on ASQA dataset in Figure 2. We discuss the main results from the experiments below.\nA more advanced model performs better. GPT-40 is generally better than Llama3-8B-Instruct on both citation quality and answer correctness. The performance of citation generation can benefit from the enhanced capabilities of the base model.\nAhead planner can enhance the answer. A planner can enhance the answer on correctness, especially for a more powerful model. GPT-40 is more likely to achieve a better performance via planning.\nOutput editor can significantly improve the citation quality. An output editor can improve citation recall and precision, as well as the correctness of citation prediction. While the ALCE Vanilla presents a citation precision and recall at about 50, a reranker in self-RAG can make the precision and recall achieve 80.\nEnhancing the granularity of citations is still a challenge. Nearly all the baselines presented cite the full documents, resulting in a relatively low granularity. Citation generation based on summary and extraction(ALCE SUMMand ALCE SNIPPET) can cite only a snippet or a summary from the document, but it risks a loss of correctness.\nLLMs can cite internal knowledge better Despite the significant loss of answer quality and correctness, Llama3-8B-Instruct demonstrates better citation quality both on recall and precision when citing the documents sampled from itself, compared to the ALCE-Vanilla baseline that uses external knowledge, demonstrating the prospects of research in this area.\nConsidering both citation quality and answer correctness remains a challenge Our method significantly improves the overall quality of citations while only slightly sacrificing accuracy. It achieves the best balance among all the methods tested."}, {"title": null, "content": "However, the answer accuracy is still lower than the highest-performing method by 7.3 points. Additionally, the citation recall and precision barely exceed 80%. In practical applications, it is still difficult to gain readers' trust. We believe that thanks to the modularity and extensibility of Citekit, this issue will gradually be resolved."}, {"title": "5 Conclusion", "content": "To unify different methods for LLM citation generation and to conduct a comprehensive and fair comparison of existing methods, we propose Citekit a user-friendly, modular, and extensible toolkit. We also present an instance to demonstrate the application of the toolkit, showing the usability and versatility of realizing citation generation pipelines. We conducted experiments on 11 different baselines and found that different modules excel in improving either the answer accuracy or the citation quality, and our approach achieves the best balance between answer accuracy and citation quality. However, generating a citation in fine-grained granularity is still challenging."}, {"title": "6 Limitation", "content": "There are still areas for improvement in our evaluation. (1) We only conduct our experiment on two LLMS, GPT-40 and Llama3-8B-Instruct. For other models, especially smaller ones, whether the different methods would still be effective in improving the performance of citation tasks is unknown. (2) Existing datasets are not designed for citation tasks. For instance, they do not take into account appropriate citation generation based on needs. Building datasets that reflect real citation generation scenarios remains an open problem."}, {"title": "A Implementation Details", "content": "In this section, we describe the implementation details for different baselines. For other baselines, we follow the original prompts and the structure they provided, but for Blueprint and self-RAG, we use In-Context-Learning (ICL) instead of a trained model to complete the sub-task in their design."}, {"title": "A.1 Blueprint Model", "content": "For the Blueprint Model, we use the abstractive model to produce general-purpose questions: the paragraph is the input and the question is the output. We use prompts to make LLMs generate questions. ALCE provides question-answer pairs for ASQA dataset, and in each pair the sub-question shows an aspect of answering the final question. We use these pairs to complete a 2-shot prompt for ICL. For answer generation, we adjust the ALCE prompt to make LLMs answer all the subquestions. We show our prompts in Figure 9, 10"}, {"title": "A.2 Prompt self-RAG", "content": "As for Llama3-8B and GPT-40, there is no trained version for self-RAG, we use prompt to make the LLM retrieve documents and generate, then use an NLI model to evaluate if the document is supportive and the answer is useful, respectively in 3 segments. A reranker will find the best segment and the sentence is add to the answer. Similar to Attribute First, then Generate, We use generated sentences as prefix to complement the sentence-by-sentence iterative generation. We show our prompts in Figure 11, 12"}, {"title": "B Case Study", "content": ""}, {"title": "B.1 Answer quality improvement", "content": "We will describe how the answer quality could be improved by using a planner. In ASQA, the question is to some extent ambiguous and requires multiple short answers to cover different aspects. As in Figure 5, a planner can give guidance by a blueprint or properly highlighting and clustering useful spans."}, {"title": "B.2 Citation quality improvement", "content": "Although we ask the model to cite a minimum set of documents, LLMs still tend to overcite. Since the ASQA dataset contains rare multi-hop questions, most of the statements only need one document as a citation. An editor, such as a simplifier can"}, {"title": "B.3 Granularity improvement", "content": "To improve granularity, the answer should cite the minimum number of spans from the documents. Most of the methods use document-level citation, and in our metrics of granularity, we assume all the spans in one document are cited for document-level citation. In ALCE-SNIPPET, LLM only cites a snippet from the document, hence a high score of granularity. Figure 7 shows how ALCE VANILLAworks to cite a span, not a document."}]}