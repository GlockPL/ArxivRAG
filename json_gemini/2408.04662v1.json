{"title": "Citekit: A Modular Toolkit for Large Language Model Citation Generation", "authors": ["Jiajun Shen", "Tong Zhou", "Suifeng Zhao", "Yubo Chen", "Kang Liu"], "abstract": "Enabling Large Language Models (LLMs) to generate citations in Question-Answering (QA) tasks is an emerging paradigm aimed at enhancing the verifiability of their responses when LLMs are utilizing external references to generate an answer. However, there is currently no unified framework to standardize and fairly compare different citation generation methods, leading to difficulties in reproducing different methods and a comprehensive assessment. To cope with the problems above, we introduce Citekit, an open-source and modular toolkit designed to facilitate the implementation and evaluation of existing citation generation methods, while also fostering the development of new approaches to improve citation quality in LLM outputs. This tool is highly extensible, allowing users to utilize 4 main modules and 14 components to construct a pipeline, evaluating an existing method or innovative designs. Our experiments with two state-of-the-art LLMs and 11 citation generation baselines demonstrate varying strengths of different modules in answer accuracy and citation quality improvement, as well as the challenge of enhancing granularity. Based on our analysis of the effectiveness of components, we propose a new method, self-RAG SNIPPET, obtaining a balanced answer accuracy and citation quality. Citekit is released at https://github.com/SjJ1017/Citekit.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) (OpenAI, 2024;\nAI@Meta, 2024) nowadays demonstrate strong per-\nformance on Question Answering (QA) (Kamalloo\net al., 2023) on different scenarios such as Com-\nmonsense QA (Talmor et al., 2019), long-form QA\n(Stelmakh et al., 2023; Min et al., 2020) and Multi-\nhop QA (Ho et al., 2020; Yang et al., 2018), but they\ncan still inevitably produce hallucinated responses\nthat are non-factual (Huang et al., 2023), nonsensi-\ncal or irrelevant to the input(Xu et al., 2024b), re-\nflecting the ongoing challenges in ensuring factual\naccuracy. Given the challenges above, Retrieval\nAugmented Generation (RAG) (Lewis et al., 2021),\nwhich leverages facts from external unstructured\nknowledge helps to enhance the reliability of LLMs\nand can be made more faithful and verifiable by\ngenerating citations (Gao et al., 2024). Asking\nmodels to generate citations can improve the fac-\ntual correctness of answers (Gao et al., 2023b),\nand the citations that link to the original references\nwill allow readers to easily verify the source of the\nresponse, making the answers of models more veri-\nfiable and explainable. Figure 1 shows how citation\ngeneration can help users become more assured of\nthe answer. In Figure 1, the answer without cita-\ntion inconsistently states both 1956 and 1976 as the\ndates for the passage of right-to-work legislation\nin Louisiana, leading to uncertainty about the ac-\ntual timeline. If citations are included, readers can\nscrutinize the reference to understand clearly why\nthere are 2 different dates to the question.\nGiven the urgent need, ALCE (Gao et al., 2023b)\nis the first attempt systematically to develop some\nbasic methods to enable LLMs to generate cita-\ntions in QA tasks and propose metrics for evaluat-\ning the quality of citations, showing there is still\nroom for improvement concerning citation gener-\nation. Following ALCE's contribution, there are\nother methods that either use training (Huang et al.,\n2024; Li et al., 2024a; Ye et al., 2024) or construct\ncomplicated pipelines to enhance the ability of gen-\nerative models in citing external documents (Zhang\net al., 2024; Hennigen et al., 2024; Lee et al., 2023).\nFor example, Fierro et al. (2024) found the black-\nbox generation is not factually faithful, so they use\nblueprint models to generate plans or blueprints\nand the output can be traced back to the blueprint.\nVerifiable Text Generation (VTG) (Sun et al., 2024)\nuses verifiers, evidence finder, retriever and a sim-\nplifier to simplify citations to improve citation qual-\nity. Another category related to citation generation\nis LLM attribution (Jain et al., 2023; Xu et al.,\n2024a; Gao et al., 2023a), which refers to the ca-\npacity of an LLM to generate and provide evidence\n(Li et al., 2023). For instance, Recitation Aug-\nmented Language Models(Sun et al., 2023), learns\nto sample documents from LLM's self-knowledge\nand construct a path of attributing passages to gen-\nerate the final answer.\nDespite a variety of state-of-the-art methods,\nthere are still two problems:\nChallenges in reproducibility and improve-\nment: Different works are distinguished largely on\ntheir implementation, making it difficult to repro-\nduce and improve. There are still some works not\ntransparent enough, with inaccessible codes, and\ndifferent works are realized using different frame-\nworks and in different coding styles, hence the\ndifficulty in generalization and a lack of flexibility.\nNeed for comprehensive and fair compar-\nisons: There is a lack of comprehensive and fair\nhorizontal comparisons between various methods.\nPrevious works have not been examined on new\nemerging Large Language Models, and since there\nare now some new metrics like citation granularity,\na comprehensive evaluation is needed. Works fol-\nlowing ALCE (Sun et al., 2024; Lee et al., 2024;\nSlobodkin et al., 2024) only compare their methods\nto ALCE baselines. Others (Asai et al., 2023; Sun\net al., 2023) that focus more on answer accuracy\nare not evaluated on the citation benchmark, which\nmeans a lack of horizontal comparisons between\nSOTAS.\nGiven the problems above, a toolkit that unifies\ndifferent methods is crucial for researchers to re-\nalize their workflow quickly and facilitate fair and\nstraightforward horizontal comparisons between\nvarious methods and efficient improvement and\ninnovation. However, there is still no such open-\nsource toolkit to compare these state-of-the-art\nmethods conveniently and fairly.\nTo this end, we present Citekit, an open-source,\nextensible, and user-friendly modular toolkit that\ncan be easily used to construct a pipeline for ci-\ntation generation by combining preset modules\nor propose an enhanced design by utilizing cus-\ntomized modules and modifying their interconnec-\ntions accordingly.\nCitekit offers four different types of modules:\nINPUT, GENERATION MODULE, ENHANC-\nING MODULE, and EVALUATOR, which are\ncombined in a pipeline. Input contains automatic\ncomponents for loading data and making prompts,\nand is accessible by other modules. GENERA-\nTION MODULE is for response generation, where\nLLM follows the instructions and uses retrieved\ndocuments to generate an answer with citations or\nexplicit reference to the documents for further pro-\ncess, featuring a wide range of LLM's supported\nand adaptive generation modes, it can satisfy dif-\nferent need for various generation task. ENHANC-\nING MODULE contains some components that can\nassess, retrieve, plan, and edit, and they can be cus-\ntomized for different tasks and even combined into\nclusters. EVALUATOR integrates different metrics\nto evaluate the output of the pipeline and other new\nmetrics could also be defined and utilized. For\ntraining-based methods, a parallel data export com-\nponent can output evaluation results that could be\nan indicator of the performance of a method and\ncan also be used as training data for supervised\nlearning and Reinforcement Learning (RL). The\ntoolkit also provides significant versatility in cus-\ntomizing new modules to quickly and conveniently\nrealize an improved method. We provide 11 base-\nline recipes using Citekit to comprehensively and\nfairly compare these SOTA methods with the latest\nmodels Llama3 and GPT-40 across 8 various met-\nrics on answer factuality and citation quality. Our\nexperiments show that despite each method having\nits special strengths, it is challenging to achieve op-\ntimal results in both answer accuracy and citation\nquality. Therefore, we used Citekit to combine the\nmost effective functional modules, achieving a bal-\nance in both answer accuracy and citation quality.\nOur contributions can be summarized as follows:"}, {"title": "System Design", "content": "In this section, we will introduce the design of\nCitekit and detail distinct functionalities of differ-\nent modules mentioned in \u00a71, and how they are\nconnected to each other to form an integrated work-\ning pipeline of citation generation. We show our\ndesign in Figure 2\n2.1 INPUT\nINPUT of a citation generation pipeline or a par-\nticular module contains requested and retrieved\ndocuments.\nDocuments: In RAG, the input contains some\ninitial documents with knowledge relevant to the\nquestion, and the designated documents will be\nstored for further use and evaluation, each attached\nautomatically with a unique index to trace.\nRequest: The request of a specific module is\namong three options: (1) from the user's query,\nsuch as questions. (2) from the module itself, like\ndemonstrations and instructions for a task-specific\nLLM. (3) a dynamic data flow that changes in the\nprocess, such as the response from the upstream\nmodule.\n2.2 GENERATION MODULE\nGENERATION MODULE contains a Large Lan-\nguage Model for generating content according to\naccording to the requirements. This module, al-\nlows users to load a large language model or use\nsome LLM API for generating responses. The\ninput of GENERATION MODULE is a natural lan-\nguage query and the output is a response from\nLLM. A GENERATION MODULE supports dif-\nferent frameworks, including huggingface, vllm,\nfastchat, and APIs like openai API to implement\nthe generation according to the need. To fit into\ndifferent pipelines, GENERATION MODULE can\nbe called either iterative or direct.\n2.3 ENHANCING MODULE\nWorks that use one or more external modules to\nenhance the quality of citations can be classified\ninto four categories: retriever, planner, assessor,\nand editor, as shown in Table 1. They can be used\nindividually or collaboratively, providing sufficient\nflexibility for the construction of a citation genera-\ntion pipeline.\n2.3.1 Real Time retriever\nA Real-Time Retriever, utilized to retrieve external\ndocuments from a corpus, is helpful when LLMs\nfind it difficult to attribute from existing retrieved\ndocuments. The input of the retriever is a query\nand the output contains some retrieved documents\nor chunks. In the design of Citekit, documents\nor chunks returned will be automatically added\ninto the pipeline for later evaluation, with a unique"}, {"title": "EVALUATOR", "content": "2.4 EVALUATOR\nEVALUATOR is a module that evaluates or scores\nthe output. If the EVALUATOR is plugged in, the\noutput of the pipeline will be automatically sent to\nit, and other information such as reference answers\nwill also be passed into it for evaluation. EVALU-\nATOR have access to the initial corpus as well as\nthe newly retrieved documents during the whole\nprocess. Finally, a result will be returned by the\nEVALUATOR.\nThere are some predefined metrics that can be\nset easily, such as ROUGE for answer quality and\nMAUVE for fluency, citation precision and recall in\nALCE benchmark, a citation precision and recall\nmetric with granularity for citation quality, and\ndataset-specific metrics for answer correctness (e.g.\nSTR-EM for ASQA, claims for ELI5).\nManually defined other metrics are also possible\nonce the evaluation function and the specific data\nin the pipeline for evaluation are defined, allowing\nusers to implant an existing metric into a pipeline\nor a new one.\n2.5 Pipeline\nA pipeline is a class for managing data flow and\norganizing different modules and corresponding\ncomponents. It serves as a runner to start the cita-\ntion generation process and control the input and\noutput of modules contained in itself.\nFor data management, input (e.g. question to be\nanswered, ground truth for evaluation) and docu-\nments retrieved are stored respectively as dictionar-\nies with keys and values in the pipeline, and they\nare both accessible by modules.\nFor module organization, modules that are con-\nnected to the pipeline can take an input and gener-\nate an output, and the output will be sent to target\na module by predefined conditions.\nCitekit offers more flexible options for more\ncomplicated pipelines. For instance, multiple re-\nsponses can be sent to the next module in parallel\nfor independent processes. They can also be sent\niterative for sequential needs. Besides, modules\nthat simply form a sequence can be connected in\norder and be used like a complete module.\nThe pipeline is also extensible, as users can plug\nin different modules. Different ways of connection\nwill make the pipeline a sequence, a loop, a tree, or\nother structures."}, {"title": "Usage", "content": "3 Usage\n3.1 Realization of SOTA method.\nTo construct a citation generation pipeline with\nCitekit, users can use predefined recipes to de-\nfine some preset modules and combine them. For\nAttribute First, then Generate pipeline, users can\nsimply use a list to indicate the interconnection and\nthe last module will output the answer.\nTo run the entire pipeline, the user should spec-\nify certain entries from the dataset as input, and\ndesignate document entries as the initially stored\ndocuments. As shown in Figure 3, we use several\nlines of code to complete the method quickly.\n3.2 Customized pipeline modification.\nThe Attribute First, then Generate design uses only\nan ahead planner. If we want to extend the pipeline\nby plugging in a verifier to ask the GENERATION\nMODULE to regenerate with new retrieved docu-\nments if the statement is not entailed by the docu-\nments. We will add a loop after the initial output\nto GENERATION MODULE. Figure 4 shows an\nexample of the efficiency of improving an existing\nmethod."}, {"title": "Evaluation", "content": "4 Evaluation\n4.1 Baselines and metrics\nWe evaluate 11 baselines in total using the state-\nof-the-art open-source and closed-source Large\nLanguage Models, GPT-40 (OpenAI, 2024) and\nLlama3-8B-Instruct (AI@Meta, 2024), respec-\ntively on ASQA dataset. ALCE VANILLA, SNIP-\nPET, and SUMM directly prompt the LLM to gen-\nerate citations, using full documents, snippets, and\nsummaries respectively. ALCE INTERACT (Gao\net al., 2023b) uses document summaries and inter-\nactively provides full documents. AAR (Lee et al.,\n2024) asked the LLM to revise the answer, while\nVTG (Hennigen et al., 2024) will verify the answer\nand retrieve more supplementary documents for re-\ngeneration if needed. Citation Enhanced (Li et al.,\n2024b) method retrieves documents after genera-\ntion, and Recitation Augmented (Sun et al., 2023)\nsample documents from self-knowledge of LLMs.\nAttribute First, then Generate (Slobodkin et al.,\n2024) will provide attributing spans to help the gen-\neration, while Blueprint (Fierro et al., 2024) pro-\nvides some questions to guide the generation. For\nself-RAG (Asai et al., 2023), we use our prompt\nversion instead of a trained Model (Appendix A.2),\nto retrieve documents and generate sentence-by-\nsentence. To further improve this method on the\ncitation granularity, we use snippets to make the\nmodel cite a smaller span as our improved method\nas self-RAG SNIPPET. We show the improved\nprompt in Appendix 13\nAs for metrics, we use metrics from ALCE, in-\ncluding fluency, correctness, rouge, citation recall,\nand precision. Besides, we also evaluate the correct\nprediction of citation need and the citation granu-\nlarity.\n4.2 Settings\nWe set max generated tokens to 500 to avoid too\nlong answers and use \\n as stop token. For Llama3-\n8B-Instruct, we use the model from huggingface\nand set the temperature to 0.5. and other configu-\nrations by default. For GPT-40, we use the openai\nAPI. During our experiment, we used the same\nprompt for the two models.\nFor retrieving documents relevant to the query,\nwe use 5 documents by default. However, for\nALCE SUMM, ALCE SNIPPET, and ALCE IN-\nTERACT, we use 10 documents as they show the\nshort summaries and snippets from the documents.\nCitation Augmented and self-RAG use real-time\nretrievers instead of a fixed number of document\ninputs, and we configured our retrievers to return\nthe top-1 document at a time.\nFor evaluation of citation quality, we adopt a\nTRUE model (Honovich et al., 2022) to verify if\nthe cited documents could entail the generated state-\nment.\n4.3 Results and analysis\nWe show the full results on ASQA dataset in Fig-\nure 2. We discuss the main results from the experi-\nments below.\nA more advanced model performs better.\nGPT-40 is generally better than Llama3-8B-\nInstruct on both citation quality and answer cor-\nrectness. The performance of citation generation\ncan benefit from the enhanced capabilities of the\nbase model.\nAhead planner can enhance the answer. A\nplanner can enhance the answer on correctness,\nespecially for a more powerful model. GPT-40 is\nmore likely to achieve a better performance via\nplanning.\nOutput editor can significantly improve the\ncitation quality. An output editor can improve cita-\ntion recall and precision, as well as the correctness\nof citation prediction. While the ALCE Vanilla\npresents a citation precision and recall at about 50,\na reranker in self-RAG can make the precision and\nrecall achieve 80.\nEnhancing the granularity of citations is still\na challenge. Nearly all the baselines presented cite\nthe full documents, resulting in a relatively low\ngranularity. Citation generation based on summary\nand extraction(ALCE SUMMand ALCE SNIPPET)\ncan cite only a snippet or a summary from the\ndocument, but it risks a loss of correctness.\nLLMs can cite internal knowledge better De-\nspite the significant loss of answer quality and cor-\nrectness, Llama3-8B-Instruct demonstrates better\ncitation quality both on recall and precision when\nciting the documents sampled from itself, com-\npared to the ALCE-Vanilla baseline that uses ex-\nternal knowledge, demonstrating the prospects of\nresearch in this area.\nConsidering both citation quality and answer\ncorrectness remains a challenge Our method sig-\nnificantly improves the overall quality of citations\nwhile only slightly sacrificing accuracy. It achieves\nthe best balance among all the methods tested.\nHowever, the answer accuracy is still lower than\nthe highest-performing method by 7.3 points. Ad-\nditionally, the citation recall and precision barely\nexceed 80%. In practical applications, it is still dif-\nficult to gain readers' trust. We believe that thanks\nto the modularity and extensibility of Citekit, this\nissue will gradually be resolved."}, {"title": "Conclusion", "content": "5 Conclusion\nTo unify different methods for LLM citation gener-\nation and to conduct a comprehensive and fair com-\nparison of existing methods, we propose Citekit a\nuser-friendly, modular, and extensible toolkit. We\nalso present an instance to demonstrate the appli-\ncation of the toolkit, showing the usability and\nversatility of realizing citation generation pipelines.\nWe conducted experiments on 11 different base-\nlines and found that different modules excel in\nimproving either the answer accuracy or the ci-\ntation quality, and our approach achieves the best\nbalance between answer accuracy and citation qual-\nity. However, generating a citation in fine-grained\ngranularity is still challenging."}, {"title": "Limitation", "content": "6 Limitation\nThere are still areas for improvement in our evalua-\ntion. (1) We only conduct our experiment on two\nLLMS, GPT-40 and Llama3-8B-Instruct. For other\nmodels, especially smaller ones, whether the differ-\nent methods would still be effective in improving\nthe performance of citation tasks is unknown. (2)\nExisting datasets are not designed for citation tasks.\nFor instance, they do not take into account appro-\npriate citation generation based on needs. Building\ndatasets that reflect real citation generation scenar-\nios remains an open problem."}, {"title": "Implementation Details", "content": "A Implementation Details\nIn this section, we describe the implementation\ndetails for different baselines. For other baselines,\nwe follow the original prompts and the structure\nthey provided, but for Blueprint and self-RAG, we\nuse In-Context-Learning (ICL) instead of a trained\nmodel to complete the sub-task in their design.\nA.1 Blueprint Model\nFor the Blueprint Model, we use the abstractive\nmodel to produce general-purpose questions: the\nparagraph is the input and the question is the output.\nWe use prompts to make LLMs generate questions.\nALCE provides question-answer pairs for ASQA\ndataset, and in each pair the sub-question shows\nan aspect of answering the final question. We use\nthese pairs to complete a 2-shot prompt for ICL.\nFor answer generation, we adjust the ALCE prompt\nto make LLMs answer all the subquestions. We\nshow our prompts in Figure 9, 10\nA.2 Prompt self-RAG\nAs for Llama3-8B and GPT-40, there is no trained\nversion for self-RAG, we use prompt to make the\nLLM retrieve documents and generate, then use an\nNLI model to evaluate if the document is supportive\nand the answer is useful, respectively in 3 segments.\nA reranker will find the best segment and the sen-\ntence is add to the answer. Similar to Attribute\nFirst, then Generate, We use generated sentences\nas prefix to complement the sentence-by-sentence\niterative generation. We show our prompts in Fig-\nure 11, 12"}, {"title": "Case Study", "content": "B Case Study\nB.1 Answer quality improvement\nWe will describe how the answer quality could\nbe improved by using a planner. In ASQA, the\nquestion is to some extent ambiguous and requires\nmultiple short answers to cover different aspects.\nAs in Figure 5, a planner can give guidance by a\nblueprint or properly highlighting and clustering\nuseful spans.\nB.2 Citation quality improvement\nAlthough we ask the model to cite a minimum set of\ndocuments, LLMs still tend to overcite. Since the\nASQA dataset contains rare multi-hop questions,\nmost of the statements only need one document\nas a citation. An editor, such as a simplifier can"}]}