{"title": "FactorGCL: A Hypergraph-Based Factor Model with Temporal Residual Contrastive Learning for Stock Returns Prediction", "authors": ["Yitong Duan", "Weiran Wang", "Jian Li"], "abstract": "As a fundamental method in economics and finance, the factor model has been extensively utilized in quantitative investment. In recent years, there has been a paradigm shift from traditional linear models with expert-designed factors to more flexible nonlinear machine learning-based models with data-driven factors, aiming to enhance the effectiveness of these factor models. However, due to the low signal-to-noise ratio in market data, mining effective factors in data-driven models remains challenging. In this work, we propose a hypergraph-based factor model with temporal residual contrastive learning (FactorGCL) that employs a hypergraph structure to better capture high-order nonlinear relationships among stock returns and factors. To mine hidden factors that supplement human-designed prior factors for predicting stock returns, we design a cascading residual hypergraph architecture, in which the hidden factors are extracted from the residual information after removing the influence of prior factors. Additionally, we propose a temporal residual contrastive learning method to guide the extraction of effective and comprehensive hidden factors by contrasting stock-specific residual information over different time periods. Our extensive experiments on real stock market data demonstrate that FactorGCL not only outperforms existing state-of-the-art methods but also mines effective hidden factors for predicting stock returns.", "sections": [{"title": "Introduction", "content": "In the domain of stock investment, the factor model has long been a cornerstone for explaining and predicting stock returns, which employs specific variables, known as factors, to elucidate fluctuations in stock prices [9, 12]. This method, prevalent in both academia and industry, has demonstrated a strong ability to explain and predict stock returns. Consequently, establishing an effective factor model is of paramount importance in stock investment.\nThe factor model explains stock returns by utilizing various factors, including fundamental, technical, and macroeconomic indicators. Specifically, in a factor model, stocks are described by factors and their corresponding factor exposures, which represent the impact of factors on stocks. In traditional factor models, these factors are designed based on expert practical experience. For instance, the well-known Fama-French model [11] employs three manually designed factors: market, size, and value. However, these humandesigned factors, while effective, are limited in number and may not sufficiently explain stock returns. For example, as illustrated in Figure 1, stock price trends across different industries exhibited high correlations that could not be adequately explained by industry-specific factors based on prior human experience. Moreover, most existing factor models are linear, explaining stock returns through a linear combination of factors weighted by factor exposures. However, recent studies [19, 1, 4, 16] have identified complex nonlinear relationships between factors and stock returns in real markets. This discrepancy highlights the limitations of linear factor models in capturing actual market behavior. Consequently, a core issue in current factor model research is how to mine more effective factors that are applicable to real market behavior.\nRecent advancements in machine learning (ML) have introduced a new perspective for factor model research [18, 33, 14]. ML-based approaches can learn complex and nonlinear market patterns in a data-driven manner and construct more market-adaptive models [22, 40, 8]. However, the low signal-to-noise ratio in stock market data may complicate the learning process for ML-based factor models.\nTo address this obstacle, we propose a hypergraph-based factor model with temporal residual contrastive learning (FactorGCL) that supplements human-designed prior factors with data-driven hidden factors, thereby enhancing the effectiveness of factor models in predicting stock returns. Our model utilizes a hypergraph, a generalized graph structure, to better capture high-order relationships among stock returns and factors. Specifically, stocks are treated as nodes in the hypergraph, factors are represented as hyperedges, and the mining of hidden factors is framed as a hyperedge generation task. As shown in Figure 2, FactorGCL employs a cascading residual hypergraph architecture where stock returns are decomposed into three components: prior beta, hidden beta, and individual alpha. Each component is extracted from the residuals after removing the influence of the previous component. Additionally, we propose a temporal residual contrastive learning method to guide the model's learning process by contrasting individual stock residuals across different time periods.\nIn summary, the contributions of our work are as follows:\n\u2022 We propose FactorGCL, a novel factor model that utilizes a hypergraph structure to capture high-order nonlinear relationships among stock returns and factors. It employs a cascading residual hypergraph architecture to mine hidden factors, supplementing human-designed prior factors, thereby enhancing the prediction of stock returns.\n\u2022 We design a self-supervised learning method called temporal residual contrastive learning. This method enhances the model's ability to extract effective and comprehensive hidden factors by contrasting stock-specific residuals across different time periods to better guide the mining of hidden factors.\n\u2022 We conduct extensive experiments on real stock market data. The results demonstrate that our method not only surpasses existing state-of-the-art baselines in stock trend prediction but also mines effective hidden factors for predicting stock returns."}, {"title": "Related Work", "content": "Factor models are widely utilized in stock investments. The original factor model, the capital asset pricing model (CAPM) [32, 30, 24], attributes differences in stock returns to varying exposures to a single market factor. Later, in a seminal work [11], it was observed that firm value and size also contribute to explaining expected stock returns, and proposed the Fama-French three-factor model. With the advancement of machine learning, some machine learningbased factor models have emerged. [19] proposed a nonlinear factor model based on neural networks to model possible interactions between different factors. [14] proposed a latent dynamic factor model using a conditional autoencoder network to capture non-linearity in return dynamics, demonstrating that the nonlinear factor model outperforms other leading linear methods. Furthermore, [10] introduced a probabilistic factor model based on variational autoencoders to better extract effective factors from the market data with high noise levels.\nHypergraphs have proven to be an efficient approach for modeling high-order correlations among data. [39] first introduced hypergraph learning as a propagation process on hypergraph structures. [13] further advanced this concept by developing the hypergraph convolutional neural network using deep learning methods for data representation learning. Hypergraphs have also been widely applied in the field of stock return prediction[21, 15, 31], [29] initially applied the hypergraph neural network to learn stock price evolution based on stock relationships. Subsequently, [28] improved hypergraph neural network for stock trend prediction by incorporating ranking loss. [37] designed a concept-oriented graph framework to mine hidden concepts for stock trend forecasting. Additionally, [36] developed a dynamic hypergraph for stock selection problem using a transformer-based pretraining mechanism.\nThis work is also related to contrastive learning, a promising class of self-supervised methods that leverage the semantic dependencies of sample pairs to capture the essence of data [6, 23, 20, 5]. Contrastive learning has been widely used in various applications. For instance, [26] introduced Contrastive Predictive Coding to learn useful representations for predicting future data. [25] proposed a unsupervised deep graph structure learning method based on contrastive learning. In the financial domain, [17] introduced a contrastive learning method for multi-granularity stock data and used it as a regularization term to improve stock trend prediction tasks."}, {"title": "Preliminaries", "content": "In this section, we first introduce the basic concepts of hypergraph convolutional neural networks, and then formally describe the research problem."}, {"title": "Hypergraph Convolutional Neural Network", "content": "A hypergraph generalizes a graph by allowing an edge, termed a hyperedge, to connect two or more nodes. This structure enables the hypergraph to capture the group-wise correlations beyond pair-wise connections. Formally, a hypergraph is defined as G = (V, E, W), where V and E denote the sets of vertices and hyperedges, respectively, and W is a diagonal matrix assigning weights to the hyperedges. The pair (V, E) in a hypergraph can be represented by an incidence matrix \\(H\\in R^{|V|\\times|E|}\\), where H(i,j) indicates the connection between the i-th vertex V(i) and the j-th hyperedge E(i), defined as:\n\\(H(i,j) =\\begin{cases}1, & \\text{if } V(i) \\in E(j)\\\\0, & \\text{if } V(i) \\notin E(j)\\end{cases}\\)\nThe hypergraph convolutional neural network (HyperGCN) [13] extends graph convolutional networks to hypergraphs, enabling the capture of high-order relationships inherent in hypergraph structures. The core of HyperGCN involves a message propagation rule that updates node features by aggregating information from their connected hyperedges, which are influenced by all nodes connected by those hyperedges. Formally, the node features at the (l +1)-th layer of HyperGCN, \\(e^{(l+1)} \\in R^{|V| \\times d_{l+1}}\\, are computed using the formula:\n\\[e^{(l+1)} = \\sigma(D_V^{-\\frac{1}{2}}HWD_E^{-1}H^T D_V^{-\\frac{1}{2}}e^{(l)}W)\\]\nwhere \\(\\sigma\\) is a non-linear activation function, \\(D_V \\in R^{|V|\\times|V|}\\) and \\(D_E \\in R^{|E|\\times|E|}\\) are diagonal matrices representing node degrees and hyperedge degrees, respectively. \\(W \\in R^{|E|\\times|E|}\\) is a diagonal matrix representing hyperedge weights, \\(w \\in R^{d_l \\times d_{l+1}}\\) is a learnable weight matrix, with \\(d_l\\) representing the dimensions of node features at the l-th layer."}, {"title": "Problem Formulation", "content": "Given N stocks in cross-section of the stock market, and a set of K factors, the traditional linear factor model calculates expected stock returns as a linear combination of factors weighted by factor exposures:\n\\[Y_t = \\sum_{k=1}^{K} \\beta_t^{(k)} z_t^{(k)} + a_t\\]\nwhere \\(Y_t = \\frac{price_{t+1}-price_t}{price_t} \\in R^N\\) denotes the future returns of N stocks at trading day t, \\(\\beta_t \\in R^{N\\times K}\\) is the factor exposure matrix of stocks at trading day t, \\(\\beta_t^{(k)} \\in R^N\\) represents the k-th factor exposure of stocks at trading day t, \\(z_t \\in R^K\\) is the vector of K factor returns, and \\(a_t \\in R^N\\) denotes the idiosyncratic returns of the stocks.\n[19] extends the linear factor model to a nonlinear version, where the stock returns are calculated using a nonlinear function h of factor returns and factor exposures:\n\\[Y_t = h(\\beta_t, z_t) + A_t\\]\nTypically, factor returns and stock individual returns are estimated using factor exposures and historical stock returns. For example, in a linear factor model, factor returns \\(z\\) are estimated using the slopes from the linear regression on historical data, while individual returns a are estimated using the residuals. Therefore, we set \\(z_t = f_z(\\beta_t, x_t)\\) and \\(a_t = f_a(\\beta_t, x_t)\\), where \\(x_t \\in R^{N\\times T\\times D}\\) represents the historical data of stocks at trading day t, with T being the length of historical data and D being the feature dimension of each stock's data.\nFinally, the task in this work is to learn a nonlinear factor model based on given factor exposures Bt and historical stock market data xt, for predicting future stock returns.\n\\[\\hat{Y}_t = h(\\beta_t, z_t) + a_t = f_\\beta(\\beta_t, x_t) + f_a(\\beta_t, x_t)\\]\nIn the following sections, we simplify our notation by omitting the time subscript t. Unless otherwise specified, all references to w and b pertain to the weights and biases of linear layers, respectively, and will not be further elaborated."}, {"title": "Methodology", "content": "This section introduces the design of FactorGCL. We first design a cascading residual hypergraph architecture for our model, which extracts prior beta, hidden beta, and individual alpha components to predict stock returns. Next, we propose a temporal residual contrastive learning method to guide the model in extracting effective and comprehensive hidden factors."}, {"title": "Cascading Residual Hypergraph Architecture", "content": "As previously mentioned, we utilize hypergraphs to construct a nonlinear factor model. Inspired by [37], we design a cascading residual hypergraph architecture to better extract hidden factors. As illustrated in Figure 3, we decompose the predicted stock returns into three components: prior beta, hidden beta, and individual alpha. Specifically, we first extract stock features from the raw data and then use the prior beta module to extract the representations of prior factors. Next, we mine hidden factors from the residuals after removing the prior beta information using the hidden beta module. Finally, we extract individual alpha from the residuals after removing both prior and hidden beta information. The final prediction of our model is obtained by summing these three components. The detailed design of our architecture is as follows.\nGiven the raw sequential market data \\(x \\in R^{N\\times T\\times D}\\), the feature extractor \\(\\Theta_{feat}\\) encodes the stock temporal feature \\(e_s \\in R^{N\\times H}\\) to capture rich temporal information. This process is defined as \\(e_s = \\Theta_{feat}(x)\\), where H represents the dimension of the feature embeddings. To capture long-term dependencies in sequential data, we utilize a gated recurrent unit with a batch normalization as the feature extractor, using the hidden state at the last time step as the stock feature embeddings.\nTo leverage expert knowledge from given K prior factors \\(\\beta \\in R^{N\\times K}\\), , we employ a hypergraph convolutional neural network (HyperGCN) to model the nonlinear relationships among stock returns and these factors. Specifically, we represent stocks as nodes in the hypergraph and factors as hyperedges. Stocks exposed to the same factor are connected by the same hyperedge, with the incidence matrix representing factor exposures. We posit that the information propagation mechanism in the HyperGCN effectively captures the nonlinear influence of factors on stocks. This process, illustrated in Figure 4, comprises the following steps:\n\u2022 Message extraction: Applies a transformation matrix to the each node features to extract expressive information.\n\u2022 Message aggregation: Aggregates the information from stock nodes connected by the same hyperedge, representing the shared information of the corresponding factor.\n\u2022 Message sharing: Integrates the node embeddings with the shared factor information as the influence of factors on stocks.\nFormally, given the stock feature embeddings es output by the feature extractor, we build a hypergraph Gp with node features es and incidence matrix B. We then calculate the prior beta embeddings by applying the HyperGCN to the hypergraph Gp, expressed as:\n\\[e_p = \\Theta_{prior}(e_s, \\beta)\\]\\[= \\sigma(D_V^{-\\frac{1}{2}}BWD_E^{-1}B^TD_V^{-\\frac{1}{2}}e_sW_p)\\]\nwhere \\(\\sigma\\) is the LeakyReLU activation function, Dn and De are the degree matrices of the nodes and hyperedges, respectively, and W = I is the identity matrix. The resulting prior beta embeddings \\(e_p \\in R^{N\\times H}\\) represent the influence of prior factors on stocks."}, {"title": "Hidden Beta Module", "content": "As previously mentioned, factors based on human prior knowledge may not adequately capture stock returns. To address this, we designed a hidden beta module to extract hidden factors that supplement these prior factors. Specifically, we regard the extraction of hidden factors as a hyperedge generation task: after removing the prior factor information, the hidden beta module generates new hyperedges from the residual embeddings and constructs a new hypergraph to model the nonlinear influence of these hidden factors on stocks.\nFormally, we first calculate the residual embeddings by subtracting the prior factor embeddings from the stock feature embeddings, i.e., \\(e_r = e_s - e_p\\). Next, we construct M learnable vectors \\(\\{c^{(i)}\\}_{i=1}^M\\), where \\(c^{(i)} \\in R^H\\), referred to as hidden factor prototypes. Hidden factors are mined by calculating the similarity between the residual embeddings and the hidden factor prototypes: \\(\\beta_h^{(i,j)} = Sigmoid(e_r^{(i)} \\cdot c^{(j)})\\) where \\(\\beta_h \\in R^{N \\times M}\\) is the hidden factor exposure matrix.\nSimilar to the prior beta module, we construct a hypergraph Gh with node features er and incidence matrix \\(\\beta_h\\), and then extract the influence of hidden factors on stocks by applying the HyperGCN to the hypergraph Gh:\n\\[e_h = \\Theta_{hidden}(e_r, \\beta_h) = HyperGCN(e_r, \\beta_h)\\]\nwhere \\(e_h \\in R^{N \\times H}\\) is the hidden beta embeddings. Note that we generate \"soft\" hyperedges \\(\\beta_h\\) in the hidden beta module, with values ranging between [0, 1], enhancing the flexibility of the hidden factors."}, {"title": "Individual Alpha Module", "content": "In addition to the influence of prior and hidden factors, the idiosyncratic information of the stock itself, or alpha, also significantly impacts stock returns. The individual alpha module handles the residual embeddings after removing the prior and hidden factor embeddings to capture the stock-specific information. We calculate the individual alpha embeddings \\(e_a \\in R^{N \\times H}\\) by applying a linear layer with a LeakyReLU activation function:\n\\[e_a = LeakyReLU(W_a(e_s - e_p - e_h) + b_a)\\]\nWe obtain the model's prediction by performing a linear mapping on the embeddings output by the prior beta module, hidden beta module, and individual alpha module, and then summing them up. Additionally, we design a multi-label prediction that requires the model to predict stock returns over multiple forward periods. This approach aims to enhance the robustness and reliability of our model by ensuring its predictive power extends across different future time frames.\n\\[\\hat{y}^{(l)} = w_{ep}^{(l)}e_p + w_{eh}^{(l)}e_h + w_{ea}^{(l)}e_a + b\\]\nwhere \\(\\hat{y}^{(l)} \\in R^N\\) represents the predicted stock returns of the l-th forward prediction period."}, {"title": "Temporal Residual Contrastive Learning", "content": "As previously mentioned, data-driven factor models face the challenge of a low signal-to-noise ratio in market data. Specifically, the hidden factors mined through such models encounter two main issues:\n\u2022 Effectiveness: The hidden factors extracted from historical data should remain consistently effective in the future. However, factors extracted by data-driven approaches are prone to overfitting market noise, thus lacking effectiveness in predicting future stock returns.\n\u2022 Comprehensiveness: The hidden factors should supplement prior factors to provide a comprehensive description of stock returns. Nonetheless, market noise complicates factor mining, making the model more likely to extract simplistic factors while neglecting others, thereby failing to adequately represent stock returns.\nTo address these issues, we have developed a selfsupervised contrastive learning method for FactorGCL, termed temporal residual contrastive learning. The motivation behind this method is as follows: for a factor model with effective and comprehensive factors, after removing all factor information, the residual, represented by the alpha embeddings ea, should contain only idiosyncratic information unique to each stock, independent of other stocks. Based on this intuition, we draw inspiration from [26] and design a cross-temporal contrastive learning approach at the stock node level. As illustrated in the Figure 5, given the same prior and hidden factors, we use our model to calculate alpha embeddings based on both past and future market data. We then treat the past and future alpha embeddings of the same stock as positive pairs, and the embeddings of different stocks as negative pairs. By training the model with a cascading residual hypergraph architecture to extract temporally consistent alpha embeddings through a contrastive learning objective function, our model can be guided to mine hidden factors that are both effective and comprehensive.\nFormally, given future data \\(x' \\in R^{N \\times T' \\times D}\\), with T' being the length of future data, we use the prior factor exposure \\(\\beta\\) and hidden factor exposure \\(\\beta_h\\) extracted from historical data to calculate the future alpha embedding \\(e_a'\\):\n\\[e'_s = \\Theta_{feat}(x')\\]\\[e'_a = e'_s - \\Theta_{prior}(e'_s, \\beta) - \\Theta_{hidden}(e_r, \\beta_h)\\]\nwhere \\(\\Theta_{feat}, \\Theta_{prior}, and \\Theta_{hidden}\\) represent the feature extractor, prior beta module, and hidden beta module for the future data, respectively. Note that \\(\\Theta_{prior} and \\Theta_{hidden}\\) share parameters with \\(\\Theta_{prior}\\) and \\(\\Theta_{hidden}\\), respectively.\nIn this context, we employ the InfoNCE loss function from [26] as the contrastive learning loss function, formulated as follows:\n\\[L_{CL} = -\\frac{1}{N} \\sum_{i=1}^{N} log \\frac{exp (sim(\\rho(e_a^{(i)}), \\rho(e'^{(i)}_a)) / \\tau)}{\\sum_{j=1}^{N} exp (sim(\\rho(e_a^{(i)}), \\rho(e'^{(j)}_a)) / \\tau)}\\]\nwhere \\(\\rho(x) =\\) is a 2-layer MLP with LeakyReLU activation functions, and sim(x, y) represents the cosine similarity function, \\(\\tau\\) is the temperature parameter.\nOur objective function consists of two parts. The first part is the mean squared error (MSE) over multiple forward periods, which aims to minimize the prediction error. The second part is the contrastive learning loss, which guides the model to mine hidden factors that are both effective and comprehensive. The overall objective function is:\n\\[L_{mse} = \\frac{1}{N \\cdot L} \\sum_{l=1}^{L} \\sum_{i=1}^{N} ((\\hat{y}^{(l)} (i) - y^{(i,l)})^2\\]\n\\[L = L_{mse} + \\gamma L_{CL}\\]\nwhere L is the number of forward prediction periods, \\(y^{(i,l)}\\) is the true return of the i-th stock at the l-th forward period, and \\(\\gamma\\) is a hyperparameter that balances the contributions of the mean squared error loss and the contrastive learning loss."}, {"title": "Experiments", "content": "In this section, we present a series of experiments to demonstrate the effectiveness of our proposed method in real-worldstock markets. Our discussion is structured around the following key research questions:\n\u2022 RQ1: How does our method compare to existing stock trend prediction methods in terms of performance?\n\u2022 RQ2: What impact does each module in our model have on its overall performance?\n\u2022 RQ3: How does varying the number of hidden factors affect the model's performance?\n\u2022 RQ4: Can our method achieve higher investment profits in simulated investment scenarios?\nWe conduct our experiments on the China A-shares market, utilizing a dataset spanning from 01/01/2014 to 06/30/2023. This dataset includes 5028 stocks, excluding suspended or otherwise abnormal stocks, and is constructed from a sequence of market data comprising day-level price-volume data (high, open, low, close, volume-weighted average price, and trading volume). In detail, the cross-sectional standardized future stock returns with multiple periods (\\(\\Delta t = 1, 5, 10, 20\\)) are used as labels, and the future return is calcu\\(price_{t+\\Delta t}-price_t\\), where price_t lated by the formula \\(Y_t = \\frac{price_t}{price_t}\\) is the volume-weighted average price at trading day t. The length of historical squence data x is T = 60, and the length of future data x' is T' = 20. We adopt secondary industry factors as the prior factors (83 industries, if a stock belongs to the industry, the value of corresponding factor exposure is 1, otherwise 0).\nWe follow the temporal order to split the dataset into training set, validation set and test set, where the time length is 5 years: 1 year: 2 years, and adopt a rolling method for training and testing. The overall test period is from 01/01/2020 to 06/30/2023. The other details about the experiment are provided in the supplementary material\u00b9.\nIn the experiment, we compare our proposed model with competitive baselines on the stock trend prediction task. In order to evaluate the performance of the compared methods, we adopt the information coefficient (IC) as metric, which are the widely-used evaluation metrics in finance. Besides, we also report the information ratio of information coefficient (ICIR) to evaluate the stability of prediction.\nsummarizes the performances of all compared methods on the test dataset. Our method achieves the highest IC and ICIR among all the compared methods. From the experimental results, we have the following observations:\n\u2022 FactorGCL can achieve better results than existing stock trend prediction methods like ALSTM, SFM and STHAN-SR, which illustrates the effectiveness of the proposed method for stock trend prediction.\n\u2022 Moreover, compared with some baselines which can also extract the hidden relations from market data, like HIST, FactorVAE and CI-STHPAN, our model can mine the hidden factors more effectively and comprehensively, to achieve higher prediction performance.\nTo verify the effectiveness of different modules in our framework, we build four variants of proposed model by removing the prior beta module, hidden beta module, individual alpha module, and contrastive learning loss, respectively.  lists the IC of these variants on the test dataset. The results show that each module in our model contributes to the overall performance of our model."}, {"title": "Investment Simulation", "content": "To further evaluate the profitability of our method in the real stock market, we conduct an investment simulation. Specifically, we adopt a simple stock selection strategy, referred to as the TopK strategy. This strategy involves investing in the TopK stocks with the highest predicted scores each trading day and selling them after holding for \\(\\Delta t\\) days, where \\(\\Delta t\\) represents the model's prediction period. In the simulation backtest, we select stocks from the CSI 300 and CSI 500 indexes, which are representative of large-cap and mid to small-cap stocks, respectively, providing a balanced and comprehensive evaluation of our investment strategy across different market segments. We use the equally weighted CSI300 and CSI500 portfolio as the benchmark, and set TopK = 30, \\(\\Delta t\\) = 10, and the transaction cost to 0.3%.\nWe present the cumulative return (CR) curves of all compared methods in  and report the annualized return (AR), information ratio (IR), and return over maximum drawdown (RoMaD) of cumulative excess return (CER) relative to the benchmark in , and the meaning of these metrics can be found in the supplementary material. The investment simulation results show that our method achieves the best performance across all metrics, indicating that our model can achieve profitable investments in the real market."}, {"title": "Conclusion", "content": "In this paper, we propose a novel hypergraph-based factor model with temporal residual contrastive learning (FactorGCL), which leverages both human-designed prior factors and data-driven hidden factors to predict stock returns. Specifically, our model follows a cascading residual hypergraph architecture, in which the hidden factors are extracted from the residual information after removing the prior factor information. To enhance the effectiveness and comprehensiveness of the hidden factors, we design a temporal residual contrastive learning method that contrasts stock-specific residuals embeddings over different time periods. Our extensive experiments demonstrate that our proposed FactorGCL outperforms existing state-of-the-art baselines in terms of predictive accuracy and investment profitability, and the ablation study further verifies our method can effectively extract hidden factors to improve the model's performance. In the future, we plan to apply the hypergraph-based factor model to risk factor mining and portfolio optimization."}]}