{"title": "PSLF: A PID Controller-incorporated Second-order Latent Factor Analysis Model for Recommender System", "authors": ["Jialiang Wang", "Yan Xia", "Ye Yuan"], "abstract": "A second-order-based latent factor (SLF) analysis model demonstrates superior performance in graph representation learning, particularly for high-dimensional and incomplete (HDI) interaction data, by incorporating the curvature information of the loss landscape. However, its objective function is commonly bi-linear and non-convex, causing the SLF model to suffer from a low convergence rate. To address this issue, this paper proposes a PID controller-incorporated SLF (PSLF) model, leveraging two key strategies: a) refining learning error estimation by incorporating the PID controller principles, and b) acquiring second-order information insights through Hessian-vector products. Experimental results on multiple HDI datasets indicate that the proposed PSLF model outperforms four state-of-the-art latent factor models based on advanced optimizers regarding convergence rates and generalization performance.", "sections": [{"title": "I. INTRODUCTION", "content": "In a recommender system, each user and item interaction can be labeled as a rating, describing the user's preference for an item. However, with the rapid development of users and items scale, it is difficult for each user to rate all items, and likewise, it is difficult for each item to be rated by all users. High-dimensional and incomplete (HDI) rating matrices are widely adopted to represent this high-dimension and sparse interaction behavior in recommender system [1-22, 108, 109].\nEach row of the HDI matrix represents a user, each column represents each item, and the matrix's values, the ratings $r_{u,i}$, describe the preference for user u to item i. Albeit the interaction behaviors are sparse, i.e., the HDI matrix includes many null elements, we can extract latent knowledge from these sparse interaction behaviors to obtain their data representation.\nThe latent factor analysis (LFA) is a graph representation learning model used for representing HDI data. The LFA model assumes that each user's and each item's features can be mapped into a low-rank latent factor matrix (aka embedding space). The rating related to user u and item i can be obtained via the inner product between u's latent factor vector and i's latent factor vector. Hence, the learning objective of the LFA model is bi-linear and non-convex.\nTo achieve optimal representation ability of the LFA model while balancing multiple aspects such as computation complexity, storage complexity, and missing data prediction performance. There are three types of optimization-based LFA models [23], i.e., first-order-based LFA [24-28], adaptive gradient-based LFA [29-32], and second-order-based LFA [33-39, 73]. Among them, the second-order-based LFA outperforms the first-order-based LFA and adaptive gradient-based LFA in representation ability since it incorporates curvature information of the LFA model's objective function at a given point.\nCommonly the second-order optimizer solve the linear system as follows:\n$g_E (X) + H_E (X) \\Delta X = 0,$\nwhere $X \\in \\mathbb{R}^d$ is the decision parameter vector, $\\Delta X \\in \\mathbb{R}^d$ denotes the incremental vector, $g_E(X) \\in \\mathbb{R}^d$ and $H_E(X) \\in \\mathbb{R}^{d \\times d}$ denotes the gradient vector and Hessian matrix w.r.t objective function $E(X)$, respectively. However, the second-order optimizers, e.g., Newton-type methods, require $O(d^2)$ to compute and store $H_E(X)$, and $O(d^3)$ to compute $H_E(X)$'s inverse [33-39, 73]. Although the second-order optimizers theoretically converge faster compared to first-order optimizers, the expensive computational and storage costs hinder the practicality of second-order optimizers in handling large-scale data. It has been found that we can obtain the incremental vector $\\Delta X$ by computing the Hessian-vector product across multiple conjugate gradient iterations. This means that we do not need to manipulate the Hessian matrix $H_E(X)$ and its inverse directly, thus mitigating the curse of dimensionality. However, the Hessian-vector-based LFA model requires multiple iterations to converge in some data scenarios [33-39, 73].\nAs in previous research [41-58], incorporating the principle of PID controller to refine stochastic gradient and learning error, which can accelerate the training process due to acquiring gradient estimation with more accuracy without sacrificing generalization ability. Their ideas are described as:"}, {"title": "II. PRELIMINARIES", "content": "The notations of this paper are summarized in Table I.\n\n\nA. Problem Statement\nThe related definitions of this paper are given as follows:\nDefinition 1: An HDI Matrix. Given two entry sets $U$ and $I$, a target matrix $R\\in \\mathbb{R}^{\\mid U \\mid \\times \\mid I \\mid}$ organized by multiple tuples, e.g., $(u, i, r)$, each tuple denotes the interaction behavior between $u \\in U$ and $i \\in I$. Let set $K$ denote the known tuples and set $M$ denote the missing tuples in $R$, respectively. If and only if the scale of $\\mid K \\mid << \\mid M \\mid$, the $R$ is the so-called HDI matrix.\nDefinition 2: An LFA Model. Given $U$, $I$ and $K$, a standard LFA model aims to construct a low-rank estimation of $R$, i.e., $R \\approx \\hat{R} = X_U X_I^T$, where $X_U \\in \\mathbb{R}^{\\mid U \\mid \\times f}$ and $X_I \\in \\mathbb{R}^{\\mid I \\mid \\times f}$ denote the latent matrix w.r.t $U$ and $I$, respectively, among them symbol $f$ denotes the low-rank dimension of latent space.\nTo optimize $\\hat{R}$, the objective function of a basic LFA model can be expressed in a form that depends on a single latent factor element, as shown below:\n$E(X) = \\frac{1}{2\\mid K \\mid} \\sum_{(u,i) \\in K} (r_{u,i} - \\sum_{d=1}^{f} X_{u,d} X_{i,d})^2 + \\frac{\\lambda}{2} (\\sum_{d=1}^{f} X_{u,d}^2 + \\sum_{d=1}^{f} X_{i,d}^2)$ s.t.\\forall u \\in U,\\forall i \\in I,\\forall d \\in \\{1,...,f\\},$\nwhere $r_{u,i}$ denotes the $(u, i, r)$-th tuple, $X \\in \\mathbb{R}^{(\\mid U \\mid + \\mid I \\mid) \\times f}$, a unified decision parameter, contains $X_U$ and $X_I$, $X_{u,d}$ denotes the $u$-th row and $d$-th column of $X_U$, the same definition as $X_{i,d}$, bi-linear term $\\sum_{d=1}^{f} \\sum_{d=1}^{f} X_{u,d} X_{i,d}$ denotes the estimation w.r.t $r_{u,i}$, and $\\lambda$ is a hyperparameter controlling the effect of Tikhonov regularization term.\nC. A second-order-based Latent Factor Analysis Model\nAn SLF model and other second-order-based graph representation learning models are desired to solve below equation (4) iteratively with a second-order optimization method:\n$\\arg \\min_X g_E (X) + H_E (X) \\Delta X,$\nwhere $g_E(X) \\in \\mathbb{R}^{(\\mid U \\mid + \\mid I \\mid) \\times f}$ denotes the gradient of function $E(X)$, $H_E(X) \\in \\mathbb{R}^{((\\mid U \\mid + \\mid I \\mid) \\times f) \\times ((\\mid U \\mid + \\mid I \\mid) \\times f)}$ represents the Hessian matrix of $E(X)$, $\\Delta X \\in \\mathbb{R}^{(\\mid U \\mid + \\mid I \\mid) \\times f}$ is the increment. Most SLF models adopt the Hessian-free optimization method, computing double Jacobian-vector products in each conjugate gradient iteration, hence, avoiding the manipulation of $H_E(X)$ and its inverse [33-40, 73].\nD. PID Controller Principles: Refining Learning Error\nBased on prior research [41-58], a representative learning model incorporating the PID controller has been developed to refine errors, adhering to the principles of a PID controller. This approach aims to enhance the understanding of learning errors and gradient estimation, thereby improving convergence rate. The main concept of the PID-incorporated LF model is to capture refinement errors, as summarized by Eq. (5) and depicted in Fig. 1.\n$e_{u,i} = r_{u,i} - \\sum_{d=1}^{f} X_{u,d} X_{i,d}$\n$\\tilde{e}_i = K_P e_i + K_I \\sum_{k=1}^{t} e_k + K_D (e_i - e_{i-1}),$\nwhere $e_{u,i}$ is the refinement error of $e_i$, constant t represents the t-th epoch, $K_P$, $K_I$, and $K_D$ are hyperparameters that govern the impact of the proportional, integral, and derivative terms, respectively."}, {"title": "III. A PID-INCORPORATED SLF MODEL", "content": "A. Reformulation\nTo simplify the analysis process of the second-order optimization-based LFA model, this section firstly considers the LF model's loss function of Eq. (1) and maps the bi-linear term $\\sum_{d=1}^{f} X_{u,d} X_{i,d}$ into an implicit function $\\sigma(\\cdot)$. The reformulation form is as follows:\n$\\sigma(X) = \\sum_{d=1}^{f} X_{u,d} X_{i,d}$\n$E(X) = \\frac{1}{2 \\mid K \\mid} \\sum_{(u,i) \\in K} L(\\sigma(X)) = \\sum_{(u,i) \\in K} (r_{u,i} - \\sigma(X))^2,$\nwhere $\\sigma(X)_{u,i}$ is a latent mapping value related to entry u and i, $L(\\sigma(X))$ denotes the loss function of Eq. (3).\nB. Gauss-Newton Approximation\nDue to the non-convex nature of loss function $L(\\sigma(X))$, its Hessian matrix $H_L(\\sigma(X))$ is indefinite. Hence, we adopt the Gauss-Newton matrix $G_L(\\sigma(X))$, a semi-definite matrix, to approximate its Hessian matrix $H_L(\\sigma(X))$. Based on prior research [33-39, 73], the Gauss-Newton matrix $G_L(\\sigma(X))$ of $L(\\sigma(X))$ can be derived by computing the gradient of implicit function $\\sigma(X)$. The approximation details are given as follows:\n$G_L (\\sigma(X)) = J_{\\sigma} (X)^T IJ_{\\sigma} (X),$"}, {"title": "C. Hessian-vector Product", "content": "In each conjugate gradient iteration, the Hessian-vector product can be derived as follows:\n$\\omega_L (\\sigma(X)) = G_L (\\sigma(X)) v = J_{\\sigma} (X)^T J_{\\sigma} (X)v,$\nwhere $\\omega_L(\\sigma(X)) \\in \\mathbb{R}^{(\\mid U \\mid + \\mid I \\mid) \\times f}$ denotes Hessian-vector product, and $v \\in \\mathbb{R}^{(\\mid U \\mid + \\mid I \\mid) \\times f}$ denotes the conjugate direction in each conjugate gradient iteration. Note that the initial v is a negative gradient vector, i.e., $v^0=-g_E(X)$. The details of the Jacobian-vector product $J_{\\sigma}(X)v$ calculating rules are given as:\n$J_{\\sigma} (X) = \\mathbb{R} \\{(\\frac{\\partial \\sigma(X)}{\\partial X})_{u,(u,i) \\in K}\\}$\n$\\frac{\\partial \\sigma(X)}{\\partial X} = \\sum_{d=1}^{f} \\frac{\\partial (X_{u,d} X_{i,d})}{\\partial X} =\\begin{cases} V_{u,d} X_{i,d} + X_{u,d} V_{i,d} & (u,i) \\in K \\\\0 & o.w. \\end{cases}$\nwhere $\\mathbb{R}{\\{\\cdot\\}}$ denotes the $\\mathbb{R}$-operator [33-40, 73]. Then, the Jacobian matrix $J_{\\sigma}(X)$ can be derived as follows:\n$\\frac{\\partial \\sigma(X)}{\\partial X} = (\\frac{\\partial}{\\partial X_{u,(u,i) \\in K}} \\sigma(X))_{u,(u,i) \\in K}$\nCombining Eq. (9) and Eq. (10), the $L(\\sigma(X))$'s element form Hessian-vector product can be derived as follows:\n$\\begin{cases} \\forall u \\in U,d=1~f: \\\\ \\omega_E (\\sigma(X)) = \\sum_{i \\in K_u} X_{i,d} \\sum_{d=1}^{f} (V_{u,d} X_{i,d} + X_{u,d} V_{i,d}) \\\\ \\forall i \\in I, d = 1 ~ f: \\\\ \\omega_E (\\sigma(X)) = \\sum_{u \\in K_i} X_{u,d} \\sum_{d=1}^{f} (V_{u,d} X_{i,d} + X_{u,d} V_{i,d}) \\end{cases}$\nwhere $K_u$ and $K_i$ denote the subset w.r.t u and i, respectively.\nD. Tikhonov Regularization Term Incorporation\nFollowing the computation rules of the $\\mathbb{R}$-operator, the Tikhonov regularization term of the objective function's Hessian-vector product vector $\\omega_{\\tau}(X) \\in \\mathbb{R}^{(\\mid U \\mid + \\mid I \\mid) \\times f}$ in each conjugate gradient step can be calculated as:\n$\\omega_{\\tau} (X) = H_\\tau (X) v - H_L (\\sigma(X)) v\\\\ \\Rightarrow\\begin{cases} \\forall u \\in U,d=1~f: \\\\ \\omega_{\\tau} (X)_{u,d} = \\lambda v_{u,d} |K_u|, \\\\ \\forall i \\in I,d=1~f: \\\\ \\omega_{\\tau} (X)_{i,d} = \\lambda v_{i,d} |K_i|, \\end{cases}$\nwhere $K_u$ denotes the volume of set $K_u$, the same as $K_i$. Then, the element form of the Hessian-vector product of Eq. (1) is as follows:\n$\\omega (X) \\approx G_E (X) v \\approx \\omega_L (\\sigma(X)) + \\omega_\\tau (X),$\nwhere $\\omega_E(\\sigma(X)) \\in \\mathbb{R}^{(\\mid U \\mid + \\mid I \\mid) \\times f}$ denotes the Hessian-vector product of Eq. (3). Its single element dependent form can be expanded as:\n$\\begin{cases} \\forall u \\in U,d = 1~ f : \\\\ \\omega_E (X) = \\sum_{i \\in K_u} X_{i,d} \\sum_{d=1}^{f} (V_{u,d} X_{i,d} + X_{u,d} V_{i,d}) + \\lambda v_{u,d} |K_u| \\\\ \\forall i \\in I,d=1~ f \\\\ \\omega_E (X) = \\sum_{u \\in K_i} X_{u,d} \\sum_{d=1}^{f} (V_{u,d} X_{i,d} + X_{u,d} V_{i,d}) + \\lambda v_{i,d} |K_i| \\end{cases}$"}, {"title": "E. Damping Term Incorporation", "content": "Considering the bilinear and non-convex nature of the objective function $E(X)$, its curvature approximation cannot be fully trusted. Adding a large enough damping term in the curvature matrix can avoid ill-conditioned issues and improve convergence performance [33-39, 73]. The single-factor form's Hessian-vector product vector $\\omega_E(\\sigma(X))$ incorporating the damping term can be expanded as follows:\n$\\omega (X) \\approx (G_E (X) + \\gamma I) v$\n$\\Rightarrow\\begin{cases} \\forall u \\in U,d=1~f: \\\\ \\omega_E (X)_{u,d} = \\sum_{i \\in K_u} X_{i,d} \\sum_{d=1}^{f} (V_{u,d} X_{i,d} + X_{u,d} V_{i,d}) + \\lambda v_{u,d} |K_u| + \\gamma V_{u,d} \\\\ \\forall i \\in I, d = 1 ~ f: \\\\ \\omega_E (X)_{i,d} = \\sum_{u \\in K_i} X_{u,d} \\sum_{d=1}^{f} (V_{u,d} X_{i,d} + X_{u,d} V_{i,d}) + \\lambda v_{i,d} |K_i| + \\gamma V_{i,d} \\end{cases}$\nwhere $\\gamma$, the damping term coefficient, is regarded as the regularization term of the curvature matrix $G_E(X)$, balancing the first-order approximation and second-order approximation.\nF. Gradient Estimation via PID Controller Principle\nBefore initiating the conjugate gradient method, it is necessary to compute the negative gradient $-g_E(X)$ as the initial conjugate direction vector $v$. Integrating the principles of a PID controller, represented by Eq. (5), we derive the estimated negative gradient $-g_E(X)$ for Eq. (3) as follows:\n$\\begin{cases} \\forall u \\in U,d=1~f: \\\\ -g_E (X) = \\sum_{i \\in K_u} (e_i X_{i,d} - \\lambda X_{u,d}) \\\\ \\forall i \\in I,d=1~f: \\\\ -g_E (X) = \\sum_{u \\in K_i} (e_i X_{u,d} - \\lambda X_{i,d}) \\end{cases}$\nG. Update Rule\nBased on the above analysis, the increment vector can be derived via multiple conjugate gradient computations. At t-th epoch, the proposed method's update rule is as follows:\n$\\Rightarrow \\Delta X  \\quad (G_E (X)^t + \\gamma I) \\Delta X^t + g_E (X)^t \\leq \\tau,$\n$X^{t+1} = X^t + \\Delta X^t,$\nwhere constant $\\tau$ denotes the tolerance in the conjugate gradient that terminates its iterations [33-39, 73]."}, {"title": "IV. EXPERIMENTS", "content": "In this section, all experiment details are given.\n\nDatasets. Four open-access benchmark HDI datasets are involved in this section, i.e., MovieLens 1M (ML-1M) [99], IMDB [101], Personality [103], and Learning to Sets (LTS) [105]. The details of all benchmark datasets are summarized in Table II.\nEvaluation metric. We adopt the root mean square error (RMSE) to evaluate the prediction performance of the proposed method and other competitors. The lower the RMSE value, the better prediction performance for missing elements in the HDI matrix [59-98, 100, 102, 104, 106, 108, 109].\n$\\textrm{RMSE} = \\sqrt{ \\frac{\\sum_{(u,i) \\in \\Omega}(r_{u,i} - \\sum_{d=1}^{f} X_{u,d} X_{i,d} )^2}{\\mid \\Omega \\mid} }$"}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed a PID controller-incorporated SLF model to improve the performance of representing HDI interaction data. The incorporation of the PID controller principles significantly enhances prediction accuracy by refining learning error estimation, as evidenced by lower RMSE values across multiple datasets. Additionally, this approach accelerates the convergence rate, resulting in reduced training time and fewer iterations compared to the vanilla second-order-based latent factor model and state-of-the-art optimizer-based latent factor models. Experimental results demonstrate that the proposed model not only achieves superior generalization performance but also offers better computational efficiency. Overall, the integration of a PID controller with second-order-based latent factor provides a robust and efficient solution for representing HDI data, outperforming existing state-of-the-art optimizer-based latent factor models."}]}