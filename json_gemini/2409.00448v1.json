{"title": "PSLF: A PID Controller-incorporated Second-order\nLatent Factor Analysis Model for Recommender\nSystem", "authors": ["Jialiang Wang", "Yan Xia", "Ye Yuan"], "abstract": "A second-order-based latent factor (SLF) analysis model demonstrates superior performance in graph representation\nlearning, particularly for high-dimensional and incomplete (HDI) interaction data, by incorporating the curvature information of the\nloss landscape. However, its objective function is commonly bi-linear and non-convex, causing the SLF model to suffer from a low\nconvergence rate. To address this issue, this paper proposes a PID controller-incorporated SLF (PSLF) model, leveraging two key\nstrategies: a) refining learning error estimation by incorporating the PID controller principles, and b) acquiring second-order\ninformation insights through Hessian-vector products. Experimental results on multiple HDI datasets indicate that the proposed\nPSLF model outperforms four state-of-the-art latent factor models based on advanced optimizers regarding convergence rates and\ngeneralization performance.", "sections": [{"title": "I.\nINTRODUCTION", "content": "In a recommender system, each user and item interaction can be labeled as a rating, describing the user's preference for an\nitem. However, with the rapid development of users and items scale, it is difficult for each user to rate all items, and likewise, it\nis difficult for each item to be rated by all users. High-dimensional and incomplete (HDI) rating matrices are widely adopted to\nrepresent this high-dimension and sparse interaction behavior in recommender system [1-22, 108, 109].\nEach row of the HDI matrix represents a user, each column represents each item, and the matrix's values, the ratings $r_{u,i}$,\ndescribe the preference for user u to item i. Albeit the interaction behaviors are sparse, i.e., the HDI matrix includes many null\nelements, we can extract latent knowledge from these sparse interaction behaviors to obtain their data representation.\nThe latent factor analysis (LFA) is a graph representation learning model used for representing HDI data. The LFA model\nassumes that each user's and each item's features can be mapped into a low-rank latent factor matrix (aka embedding space). The\nrating related to user u and item i can be obtained via the inner product between u's latent factor vector and i's latent factor vector.\nHence, the learning objective of the LFA model is bi-linear and non-convex.\nTo achieve optimal representation ability of the LFA model while balancing multiple aspects such as computation complexity,\nstorage complexity, and missing data prediction performance. There are three types of optimization-based LFA models [23], i.e.,\nfirst-order-based LFA [24-28], adaptive gradient-based LFA [29-32], and second-order-based LFA [33-39, 73]. Among them,\nthe second-order-based LFA outperforms the first-order-based LFA and adaptive gradient-based LFA in representation ability\nsince it incorporates curvature information of the LFA model's objective function at a given point.\nCommonly the second-order optimizer solve the linear system as follows:\n$g_{E} (X) + H_{E} (X) \\Delta X = 0,$\\documentclass{article}\\usepackage{amsmath}\begin{document}\n\nwhere $X \\in \\mathbb{R}^{d}$ is the decision parameter vector, $\\Delta X \\in \\mathbb{R}^{d}$ denotes the incremental vector, $g_{E}(X) \\in \\mathbb{R}^{d}$ and $H_{E}(X) \\in \\mathbb{R}^{d \\times d}$ denotes the\ngradient vector and Hessian matrix w.r.t objective function E(X), respectively. However, the second-order optimizers, e.g.,\nNewton-type methods, require $O(d^{2})$ to compute and store $H_{E}(X)$, and $O(d^{3})$ to compute $H_{E}(X)$'s inverse [33-39, 73]. Although\nthe second-order optimizers theoretically converge faster compared to first-order optimizers, the expensive computational and\nstorage costs hinder the practicality of second-order optimizers in handling large-scale data. It has been found that we can obtain\nthe incremental vector $\\Delta X$ by computing the Hessian-vector product across multiple conjugate gradient iterations. This means\nthat we do not need to manipulate the Hessian matrix $H_{E}(X)$ and its inverse directly, thus mitigating the curse of dimensionality.\nHowever, the Hessian-vector-based LFA model requires multiple iterations to converge in some data scenarios [33-39, 73].\nAs in previous research [41-58], incorporating the principle of PID controller to refine stochastic gradient and learning error,\nwhich can accelerate the training process due to acquiring gradient estimation with more accuracy without sacrificing\ngeneralization ability. Their ideas are described as:"}, {"title": "II.\nPRELIMINARIES", "content": "The notations of this paper are summarized in Table I."}, {"title": "A. Problem Statement", "content": "The related definitions of this paper are given as follows:\nDefinition 1: An HDI Matrix. Given two entry sets U and I, a target matrix $R \\in \\mathbb{R}^{|U|\\times |I|}$ organized by multiple tuples, e.g., (u,\ni, r), each tuple denotes the interaction behavior between u\u2208U and i\u2208I. Let set K denote the known tuples and set M denote the\nmissing tuples in R, respectively. If and only if the scale of $|K|<<|M|$, the R is the so-called HDI matrix.\nDefinition 2: An LFA Model. Given U, I and K, a standard LFA model aims to construct a low-rank estimation of R, i.e.,\n$\\hat{R}\\approx R = X_{U}X_{I}^{T}$, where $X_{U} \\in \\mathbb{R}^{|U| \\times f}$ and $X_{I} \\in \\mathbb{R}^{|I| \\times f}$ denote the latent matrix w.r.t U and I, respectively, among them symbol f denotes\nthe low-rank dimension of latent space.\nTo optimize $\\hat{R}$, the objective function of a basic LFA model can be expressed in a form that depends on a single latent factor\nelement, as shown below:\n$E(X) = \\frac{1}{2} \\sum_{(u,i) \\in K} (r_{u,i} - \\sum_{d=1}^{f} X_{u,d} X_{i,d})^{2} + \\frac{\\lambda}{2}(\\sum_{u=1}^{|U|} \\sum_{d=1}^{f} X_{u,d}^{2} + \\sum_{i=1}^{|I|} \\sum_{d=1}^{f} X_{i,d}^{2}),$\n$\\forall u \\in U, \\forall i \\in I, d \\in \\{1,...,f\\},$\\documentclass{article}\\usepackage{amsmath}\begin{document}\n\nwhere $r_{u,i}$ denotes the (u, i, r)-th tuple, $X \\in \\mathbb{R}^{(|U|+|I|) \\times f}$, a unified decision parameter, contains $X_{U}$ and $X_{I}$, $X_{u,d}$ denotes the u-th row\nand d-th column of $X_{U}$, the same definition as $X_{i,d}$, bi-linear term $\\sum_{d=1}^{f} \\sum_{d=1}^{f} X_{u,d} X_{i,d}$ denotes the estimation w.r.t $r_{u,i}$, and $\\lambda$ is a\nhyperparameter controlling the effect of Tikhonov regularization term."}, {"title": "C. A second-order-based Latent Factor Analysis Model", "content": "An SLF model and other second-order-based graph representation learning models are desired to solve below equation (4)\niteratively with a second-order optimization method:\n$\\arg \\min g_{E}(X) + H_{E}(X)\\Delta X,$\\documentclass{article}\\usepackage{amsmath}\begin{document}\n\nwhere $g_{E}(X) \\in \\mathbb{R}^{(|U|+|I|) \\times f}$ denotes the gradient of function E(X), $H_{E}(X) \\in \\mathbb{R}^{((|U|+|I|) \\times f) \\times ((|U|+|I|) \\times f)}$ represents the Hessian matrix of E(X),\n$\\Delta X \\in \\mathbb{R}^{(|U|+|I|) \\times f}$ is the increment. Most SLF models adopt the Hessian-free optimization method, computing double Jacobian-vector\nproducts in each conjugate gradient iteration, hence, avoiding the manipulation of $H_{E}(X)$ and its inverse [33-40, 73]."}, {"title": "D. PID Controller Principles: Refining Learning Error", "content": "Based on prior research [41-58], a representative learning model incorporating the PID controller has been developed to refine\nerrors, adhering to the principles of a PID controller. This approach aims to enhance the understanding of learning errors and\ngradient estimation, thereby improving convergence rate. The main concept of the PID-incorporated LF model is to capture\nrefinement errors, as summarized by Eq. (5) and depicted in Fig. 1.\n$e_{u,i}^{'} = r_{u,i} - \\sum_{d=1}^{f} X_{u,d} X_{i,d}$\n$\\widetilde{e^{t}} = K_{p}e^{'} + K_{I} \\sum_{k=1}^{t} e^{k} + K_{D} (e^{t} - e^{t-1}),$\\documentclass{article}\\usepackage{amsmath}\begin{document}\n\nwhere $e_{u,i}^{'}$ is the refinement error of $e_{u,i}$, constant t represents the t-th epoch, $K_{P}$, $K_{I}$, and $K_{D}$ are hyperparameters that govern the\nimpact of the proportional, integral, and derivative terms, respectively."}, {"title": "III. A PID-INCORPORATED SLF MODEL", "content": "To simplify the analysis process of the second-order optimization-based LFA model, this section firstly considers the LF\nmodel's loss function of Eq. (1) and maps the bi-linear term $\\sum_{d=1}^{f} X_{u,d}X_{i,d}$ into an implicit function $\\sigma(\\cdot)$. The reformulation form\nis as follows:\n$\\sigma(X) = \\sum_{(u,i) \\in K} \\sum_{d=1}^{f} X_{u,d} X_{i,d}$\n$L(\\sigma(X)) = \\frac{1}{2} \\sum_{(u,i) \\in K} (r_{u,i} - \\sigma(X))^{2},$\nwhere $\\sigma(X)_{u,i}$ is a latent mapping value related to entry u and i, $L(\\sigma(X))$ denotes the loss function of Eq. (3)."}, {"title": "B. Gauss-Newton Approximation", "content": "Due to the non-convex nature of loss function $L(\\sigma(X))$, its Hessian matrix $H_{L}(\\sigma(X))$ is indefinite. Hence, we adopt the Gauss-\nNewton matrix $G_{L}(\\sigma(X))$, a semi-definite matrix, to approximate its Hessian matrix $H_{L}(\\sigma(X))$. Based on prior research [33-39,\n73], the Gauss-Newton matrix $G_{L}(\\sigma(X))$ of $L(\\sigma(X))$ can be derived by computing the gradient of implicit function $\\sigma(X)$. The\napproximation details are given as follows:\n$G_{L} (\\sigma(X)) = J_{\\sigma} (X)^{T} J_{\\sigma} (X),$"}, {"title": "C. Hessian-vector Product", "content": "In each conjugate gradient iteration, the Hessian-vector product can be derived as follows:\n$\\omega_{\\sigma} (\\sigma(X)) = G_{L} (\\sigma(X))v$\n$= J_{\\sigma} (X)^{T} J_{\\sigma} (X)v,$\nwhere $\\omega_{\\sigma}(\\sigma(X)) \\in \\mathbb{R}^{(|U|+|I|) \\times f}$ denotes Hessian-vector product, and $v \\in \\mathbb{R}^{(|U|+|I|) \\times f}$ denotes the conjugate direction in each conjugate\ngradient iteration. Note that the initial v is a negative gradient vector, i.e., $v^{0}=-g_{E}(X)$. The details of the Jacobian-vector product\n$J_{\\sigma}(X)v$ calculating rules are given as:\n$J_{\\sigma} (X) = \\mathcal{R} \\{\\frac{\\partial \\sigma(X)_{u,i}}{\\partial (X)}\\};$\n$=\\sum_{d=1}^{f} \\left[V_{u,d} X_{i,d} + X_{u,d} V_{i,d} \\right]$\nwhere $\\mathcal{R}\\{\\cdot\\}$ denotes the R-operator [33-40, 73]. Then, the Jacobian matrix $J_{\\sigma}(X)$ can be derived as follows:\n$J_{\\sigma} (X) = \\{\\frac{\\partial \\sigma(X)}{\\partial (X)}\\} \\newline (u,i) \\in K$\nCombining Eq. (9) and Eq. (10), the $L(\\sigma(X))$'s element form Hessian-vector product can be derived as follows:\n$\\begin{cases}\n\\forall u \\in U, d=1 \\sim f:\\newline \\omega_{L} (\\sigma(X)) = \\sum_{i \\in K_{u}} X_{i,d} (V_{u,d} \\sum_{d=1}^{f} V_{u,d} X_{i,d} + X_{u,d} V_{i,d}) \\newline \\forall i \\in I, d = 1 \\sim f:\\newline \\omega_{L} (\\sigma(X)) = \\sum_{u \\in K_{i}} X_{u,d} (V_{i,d} \\sum_{d=1}^{f} V_{u,d} X_{i,d} + X_{u,d} V_{i,d})\n\\end{cases}$\nwhere $K_{u}$ and $K_{i}$ denote the subset w.r.t u and i, respectively."}, {"title": "D. Tikhonov Regularization Term Incorporation", "content": "Following the computation rules of the R-operator, the Tikhonov regularization term of the objective function's Hessian-\nvector product vector $\\omega_{\\tau}(X) \\in \\mathbb{R}^{(|U|+|I|) \\times f}$ in each conjugate gradient step can be calculated as:\n$\\omega_{\\tau} (X) = H_{\\tau}(X)v \\newline = H_{\\tau} (\\sigma(X))v \\newline \\Rightarrow \\newline \\begin{cases} \\forall u \\in U, d=1 \\sim f: \\newline \\omega_{\\tau}(X)_{u,d} = \\lambda V_{u,d} |K_{u}|, \\newline \\forall i \\in I, d=1 \\sim f: \\newline \\omega_{\\tau}(X)_{i,d} = \\lambda V_{i,d} |K_{i}|, \\end{cases}$\nwhere $|K_{u}|$ denotes the volume of set $K_{u}$, the same as $|K_{i}|$. Then, the element form of the Hessian-vector product of Eq. (1) is as\nfollows:\n$\\omega_{E} (X) \\approx G_{E} (X) v \\newline \\approx \\omega_{L} (\\sigma(X)) + \\omega_{\\tau} (X),$\nwhere $\\omega_{E}(\\sigma(X)) \\in \\mathbb{R}^{(|U|+|I|) \\times f}$ denotes the Hessian-vector product of Eq. (3). Its single element dependent form can be expanded as:\n$\\begin{cases}\n\\forall u \\in U, d = 1 \\sim f : \\newline \\omega_{E} (X)_{u,d} = \\sum_{i \\in K_{u}} X_{i,d} (V_{u,d} \\sum_{d=1}^{f} V_{u,d} X_{i,d} + X_{u,d} V_{i,d}) + \\lambda V_{u,d} |K_{u}|, \\newline \\forall i \\in I, d=1 \\sim f: \\newline \\omega_{E} (X)_{i,d} = \\sum_{u \\in K_{i}} X_{u,d} (V_{i,d} \\sum_{d=1}^{f} V_{u,d} X_{i,d} + X_{u,d} V_{i,d}) + \\lambda V_{i,d} |K_{i}|, \\end{cases}$"}, {"title": "E. Damping Term Incorporation", "content": "Considering the bilinear and non-convex nature of the objective function E(X), its curvature approximation cannot be fully\ntrusted. Adding a large enough damping term in the curvature matrix can avoid ill-conditioned issues and improve convergence\nperformance [33-39, 73]. The single-factor form's Hessian-vector product vector $\\omega_{E}(\\sigma(X))$ incorporating the damping term can\nbe expanded as follows:\n$\\omega_{E} (X) \\approx (G_{E} (X) + \\gamma I)v \\newline \\Rightarrow \\begin{cases} \\forall u \\in U, d=1 \\sim f: \\newline \\omega_{E} (X)_{u,d} = \\sum_{i \\in K_{u}} X_{i,d} (V_{u,d} \\sum_{d=1}^{f} V_{u,d} X_{i,d} + X_{u,d} V_{i,d}) + \\lambda V_{u,d} |K_{u}| + \\gamma V_{u,d} \\newline \\forall i \\in I, d=1 \\sim f: \\newline \\omega_{E} (X)_{i,d} = \\sum_{u \\in K_{i}} X_{u,d} (V_{i,d} \\sum_{d=1}^{f} V_{u,d} X_{i,d} + X_{u,d} V_{i,d}) + \\lambda V_{i,d} |K_{i}| + \\gamma V_{i,d} \\end{cases}$\nwhere $\\gamma$, the damping term coefficient, is regarded as the regularization term of the curvature matrix $G_{E}(X)$, balancing the first-\norder approximation and second-order approximation."}, {"title": "F. Gradient Estimation via PID Controller Principle", "content": "Before initiating the conjugate gradient method, it is necessary to compute the negative gradient $-g_{E}(X)$ as the initial conjugate\ndirection vector v. Integrating the principles of a PID controller, represented by Eq. (5), we derive the estimated negative gradient\n$-g_{E}(X)$ for Eq. (3) as follows:\n$\\begin{cases} \\forall u \\in U, d=1 \\sim f:\\newline -g_{E} (X)_{u,d} = \\sum_{i \\in K_{u}} (e'_{u,i} X_{i,d} - \\lambda X_{u,d}) \\newline \\forall i \\in I, d=1 \\sim f:\\newline -g_{E} (X)_{i,d} = \\sum_{u \\in K_{i}} (e'_{u,i} X_{u,d} - \\lambda X_{i,d}) \\end{cases}$"}, {"title": "G. Update Rule", "content": "Based on the above analysis, the increment vector can be derived via multiple conjugate gradient computations. At t-th epoch,\nthe proposed method's update rule is as follows:\n$\\Delta X \\leftarrow \\begin{array}{c} Mutiple Conjugate Gradient Steps \\newline ((G_{E} (X)^{t} + \\gamma I) \\Delta X^{'} + g_{E} (X)^{'} \\le \\tau, \\newline X^{t+1} = X^{t} + \\Delta X^{'}, \\end{array}$\nwhere constant $\\tau$ denotes the tolerance in the conjugate gradient that terminates its iterations [33-39, 73]."}, {"title": "IV. EXPERIMENTS", "content": "In this section, all experiment details are given."}, {"title": "A. General Settings", "content": "Datasets. Four open-access benchmark HDI datasets are involved in this section, i.e., MovieLens 1M (ML-1M) [99], IMDB\n[101], Personality [103], and Learning to Sets (LTS) [105]. The details of all benchmark datasets are summarized in Table II.\nEvaluation metric. We adopt the root mean square error (RMSE) to evaluate the prediction performance of the proposed\nmethod and other competitors. The lower the RMSE value, the better prediction performance for missing elements in the HDI\nmatrix [59-98, 100, 102, 104, 106, 108, 109].\n$RMSE = \\sqrt{\\frac{\\sum_{(u,i) \\in \\Omega} (r_{u,i} - \\sum_{d=1}^{f} X_{u,d} X_{i,d})^{2}}{|\\Omega|}}$"}, {"title": "B. Comparison Results", "content": "The comparison results on four open-access benchmark datasets are summarized in Table IV. From Table IV, we can make\nthe following observations:\n(1) Refining learning error via incorporating the principle of a PID controller can improve prediction accuracy for\nthe HDI matrix's missing data. As shown in Table IV, the average RMSE values for M5 are 0.85082, 0.77026, 0.86237, and\n0.81855 on D1-D4, respectively. For M4, the average RMSE values are 0.85132, 0.77096, 0.86382, and 0.81962, respectively.\nIt is noteworthy that a lower RMSE value indicates better generalization ability for the LFA model in this paper. The PID-\nincorporated SLF model M5 exhibits lower RMSE values on D1-D4 compared to the vanilla SLF model. Specifically, on D1-\nD4, M5's average RMSE values are lower than M1-M4 0.06%, 0.09%, 0.17%, and 0.13%, respectively.\n(2) Incorporating the principle of a PID controller can accelerate the convergence rate. For instance, from Table IV,\nM5's average time consumption and training epochs are 29 seconds and 36 epochs on D1, respectively, which are about 9.38%\nand 29.41% lower than M4's 32 seconds and 51 epochs, respectively. On D3, M5's time cost and iterations are 101 seconds and\n43 epochs, respectively, which are about 28.87% and 61.95% lower than M4's 142 seconds and 113 iterations, respectively.\nSimilar results can be observed across the other datasets as well, as shown in Table IV."}, {"title": "C. Summary", "content": "In this section, we compared the proposed PSLF model with advanced optimizer-based LFA models in terms of generalization\nand convergence rate. Empirical results demonstrate that the PSLF model outperforms its competitors in predicting missing data\nwithin the HDI matrix and achieving faster convergence rates."}, {"title": "V. CONCLUSION", "content": "In this paper, we proposed a PID controller-incorporated SLF model to improve the performance of representing HDI\ninteraction data. The incorporation of the PID controller principles significantly enhances prediction accuracy by refining learning\nerror estimation, as evidenced by lower RMSE values across multiple datasets. Additionally, this approach accelerates the\nconvergence rate, resulting in reduced training time and fewer iterations compared to the vanilla second-order-based latent factor\nmodel and state-of-the-art optimizer-based latent factor models. Experimental results demonstrate that the proposed model not\nonly achieves superior generalization performance but also offers better computational efficiency. Overall, the integration of a\nPID controller with second-order-based latent factor provides a robust and efficient solution for representing HDI data,\noutperforming existing state-of-the-art optimizer-based latent factor models."}]}