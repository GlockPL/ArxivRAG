{"title": "R.I.P. -: A Simple Black-box Attack on Continual Test-time Adaptation", "authors": ["Trung-Hieu Hoang", "Duc Minh Vo", "Minh N. Do"], "abstract": "Test-time adaptation (TTA) has emerged as a promising solution to tackle the continual domain shift in machine learning by allowing model parameters to change at test time, via self-supervised learning on unlabeled testing data. At the same time, it unfortunately opens the door to unforeseen vulnerabilities for degradation over time. Through a simple theoretical continual TTA model, we successfully identify a risk in the sampling process of testing data that could easily degrade the performance of a continual TTA model. We name this risk as Reusing of Incorrect Prediction (RIP) that TTA attackers can employ or as a result of the unintended query from general TTA users. The risk posed by RIP is also highly realistic, as it does not require prior knowledge of model parameters or modification of testing samples. This simple requirement makes RIP as the first black-box TTA attack algorithm that stands out from existing white-box attempts. We extensively benchmark the performance of the most recent continual TTA approaches when facing the RIP attack, providing insights on its success, and laying out potential roadmaps that could enhance the resilience of future continual TTA systems.", "sections": [{"title": "1. Introduction", "content": "This age-old statement holds in modern machine learning (ML) research, where the real-world environment, like a river, is constantly changing, necessitating adaptation to continual domain shifts [7, 18, 41]. Recently recognized continual Test-Time Adaptation (TTA) [48, 49] is a powerful tool for addressing this need by allowing the model parameters to change at the test time. Nonetheless, much like the man who is no longer the same after stepping into the river, it is uncertain whether the adapted model will evolve in a better or worse direction. For instance, under extended time horizons [17, 40] or challenging testing streams [8, 43, 53], recent studies raise a critical concern that TTA models can be collapsed, resulting in predictions that are confined to a single set of categories regardless of the input after several iterations. As such, unforeseen risks are inevitable in TTA. Deepening the understanding of these risks, including how they occur and how to prevent them, is crucial for ensuring a reliable, and trustworthy real-world TTA deployment.\nWhile some risks may naturally arise during a TTA process [17, 40, 43, 53], we delve into a more perilous situation where such systems are particularly vulnerable to malicious samples, which attackers could exploit to degrade the performance of a system intentionally. This threat is known as adversarial model attack [9, 45, 50]. To our best knowledge, only a limited number of prior studies investigated this threat on continual TTA models [2, 39, 51]. Unfortunately, all resort to the white-box setting (assuming access to parameters of a victim model), which is impractical to implement. The gap here urges us to extend the concept of black-box [12, 38] attack to continual TTA that is not only more realistic but also a pioneering work in this area.\nTTA updates a ML model using testing samples available at test time. Thus, an attack algorithm functions by modifying that batch of testing samples [2, 39, 51]. Theoretically speaking, the sampling process is manipulated. We aim to find a simple yet dangerous sampling operator, opening the ability to attack a TTA method effortlessly. By extending Gaussian Mixture Model Classifier (GMMC) [17], a handy theoretical model for understanding TTA, we first discover one such sampling strategy that can easily fool TTA to converge undesirably, under a condition. This condition is not hard to meet in almost every modern continual TTA methods [6, 17, 49, 53] that requires a random image transformation [42] to be applied during training at test time. The idea behind augmentation is straightforward, based on the fact that the semantics of an image are unchanged under mild random image transformations. These augmentation strategies generate more samples that favor the adaptation efficacy [16, 27, 55]. Yet, this good practice becomes problematic when incorrectly predicted samples are augmented and used for adaptation. Ultimately, it turns into the \"Achilles' heel\"\u00b9 of TTA that one can exploit to attack, even within the black-box constraints. Aside from the attack scenario, the discussion here is still relevant if a general user unintentionally queries samples that fall into this corner test case.\nThis leads us to develop Reusing of Incorrect Predictions (RIP), the first black-box attack algorithm to make a TTA model prone to collapse. RIP is illustrated by a simple binary classification task in Fig. 1. Here, the attacker intentionally picks incorrect predictions in previous TTA steps (highlighted in red) and reuses them in the subsequent testing batches. With random augmentation applied, incorrectly predicted samples and their augmented variants are used for TTA. We discover that under RIP, the decision boundary of a victim class is erroneously shifted, penetrated, and dominated by nearby classes. Over time, a TTA model can be collapsed in this way. Undoubtedly, RIP is a straightforward attack algorithm that does not require any specialized expertise. The contributions of this work are:\n\u2022 Through a theoretical model, we discovered a threat in which data augmentation in TTA and i.i.d. sampling assumption violation can make a model collapse (Sec. 3).\n\u2022 Inspired by this observation, Reusing of Incorrect Predictions (RIP) - the first black-box attack algorithm targeting continual TTA methods is proposed (Sec. 4).\n\u2022 Through extensive experiments, we confirm the vulnerability of many recent continual TTA methods (Sec. 5).\n\u2022 A series of ablation studies verifies the root causes of vulnerability that can help to mitigate RIP attack (Sec. 6).\nVisit the Appendix for a summary of the related work."}, {"title": "2. The Continual TTA Procedure", "content": "This section presents the major notations and describes key components living in a continual TTA method.\n\n2.1. Continual Test-time Adaptation\nNotations. We focus on a ML classifier $f_t : \\mathcal{X} \\rightarrow \\mathcal{Y}$, parameterized by $\\theta_t \\in \\Theta$ (parameter space) that maps an input image $x \\in \\mathcal{X}$ to a class-label $y \\in \\mathcal{Y}$. A TTA method continuously modifies $f_t$ at each time step $t \\in \\mathcal{T}$. Let the capital letters $(\\mathcal{X}_t, \\mathcal{Y}_t) \\in \\mathcal{X} \\times \\mathcal{Y}$ denote a pair of random variables with the joint distribution $P_t(x, y) \\in \\mathcal{P}_\\Theta$, $t \\in \\mathcal{T}$. In practice, $\\mathcal{X}_t$ is in the form of a batch of $B$ testing samples. The superscript such as $X_t^{(i)}$ is used to denote the i-th realization of a random variable, when necessary. The covariate shift [41] is assumed: $P_t(x)$ and $P_{t'}(x)$ could be different but $P_t(y|x) = P_{t'}(y|x)$ holds $\\forall t \\neq t'$. At $t = 0$, $\\theta_0$ is a source model trained on labeled data from distribution $P_0$.\nThe Continual TTA Procedure. At $t > 0$, the continual TTA process follows the diagram provided in Fig. 2:\n(i) $\\mathcal{X}_t$ is sampled from distribution $P_t$. While i.i.d. sampling is typically assumed, this work discusses a critical point: \"What if this assumption fails to hold?\"\n(ii) The pseudo-label predictor (Sec. 2.4) guesses the label for $\\mathcal{X}_t$ based on $\\theta_{t-1}$ (from the previous step):\n$\\widehat{\\mathcal{Y}}_t = f_{t-1}(\\mathcal{X}_t)$.\n(1)\n(iii) Besides returning $\\widehat{\\mathcal{Y}}_t$ (Eq. 1) as a prediction, pseudo-labels are used for minimizing an objective function (Sec. 2.2), adapting itself $f_{t-1} \\rightarrow f_t$ (Sec. 2.3).\n2.2. Loss Functions\nWith only unlabeled data $(\\mathcal{X}_t)$ available at test time, a pseudo label [22] ($\\mathcal{Y}_t$ as in Eq. 1) is introduced for each $\\mathcal{X}_t$. As a shorthand notation, we omit $t$ and denote the following probability vectors $p$ and $q$. Here, $p_{\\widehat{y}} = Pr{\\mathcal{Y}_t = \\widehat{y}}$ and $q_{\\widehat{y}} = Pr{f_t(\\mathcal{X}_t) = \\widehat{y}}$, for $\\widehat{y} \\in \\mathcal{Y}$ represent the conditional probability of the pseudo-label predictor and the model assign label $\\widehat{y}$ for a given input sample (i.e., the intermediate model output after softmax and before argmax). Existing loss functions in the field can be classified into two groups:\nAugmenting-free Loss Functions: In the most basic form, pioneering studies [34, 48] adopt the Entropy (Ent) loss:\n$L_{Ent}(q) = -\\sum_{\\widehat{y} \\in \\mathcal{Y}} q_{\\widehat{y}} \\log (q_{\\widehat{y}})$.\n(2)"}, {"title": "2.3. Model Update", "content": "TTA with Mean Teacher Update. To achieve a stable optimization process, the main (teacher) model $f_t$ are updated indirectly through a student model $f_t'$ with parameter $\\theta_t'$ [6, 8, 46, 48, 53]. With $L_{CLS}$ as a placeholder for the loss function (Sec. 2.2), and a regularizer $R$, the student model ($f_t'$) is first updated with a generic optimization operator $Optim$, followed by an Exponential Moving Average (EMA) update of the teacher model parameter $\\theta_{t-1}$:\n$\\theta_t' = Optim_{\\theta' \\in \\Theta} E_{P_t} [L_{CLS} (\\mathcal{Y}_t, \\mathcal{X}_t; \\theta')] + \\lambda R(\\theta'),$\n(6)\n$\\theta_t = \\alpha \\theta_{t-1} + (1 - \\alpha)\\theta'_t,$\n(7)\nwith $\\alpha \\in (0,1)$ - the EMA update rate, and $\\lambda \\in \\mathbb{R}^+$ - the coefficient of the regularization term are hyper-parameters.\nSource Model Weights Ensemble Update: Suggested in ROID [30] to avoid self-training and mean teacher update [46]. The accumulated model is updated via:\n$\\theta_t = \\alpha \\theta_0 + (1 - \\alpha)\\theta'_t,$\n(8)\nwith $\\theta_0$ is the parameter of the source model.\n2.4. Pseudo-label Predictor\nThe pseudo-label of augmented samples ($\\widehat{\\mathcal{Y}}_t$) in Eq. 5 can be predicted in two ways. Let denote $f_t$ be the model updated with EMA in Eq. 7 (teacher model). We have:\n\u2022 Pseudo-labels from Teacher Model: The earliest work - COTTA [49] proposes the following strategy:\n$\\widehat{\\mathcal{Y}}_t = f_{t-1}(x_t)$.\n(9)\n\u2022 Pseudo-labels from Student Model: This strategy is used in almost every follow-up study after CoTTA, (e.g., RMT [6], ROID [30], ROTTA [53] or TRIBE [43]):\n$\\widehat{\\mathcal{Y}}_t = f'_{t-1}(x_t)$.\n(10)"}, {"title": "3. A Risk in TTA with Data Augmentation", "content": "By inspecting a toy example of a simple theoretical model (Sec. 3.1), this section delves into our first findings on a sampling strategy, namely Incorrectly Prediction Sampling (IPS) (Sec. 3.2) that negatively impacts a TTA method.\n3.1. Augmented Gaussian Mixture Model Classifier\nIn [17], Hoang et al. introduces a simple yet representative Gaussian Mixture Model Classifier (GMMC) that replicates the behavior of a real-world continual TTA model for a theoretical analysis. However, GMMC fails to account for the role of the random image augmentation operator, a common practice in modern continual TTA. This study extends GMMC along this line and introduces an inspection, shedding light on the introduction of our attack algorithm.\nTo replicate the Aug operator in Eq. 5 on GMMC, we employ Additive White Gaussian Noise (AWGN) that can be directly applied to the data used for optimizing GMMC:\n$\\mathcal{X}'_t = Aug(\\mathcal{X}_t) = \\mathcal{X}_t + \\delta, \\quad \\delta \\sim \\mathcal{N}(0, \\sigma)$.\n(11)\nThe level of data augmentation can be controlled by $\\sigma$. A larger value of $\\sigma$ corresponds to a stronger augmentation scheme, and $\\sigma = 0$ means no data augmentation is applied.\nWhile AWGN is relatively simple\n3.2. Incorrect Prediction Sampling on GMMC\nThe Shifting-Boundary Effect. Since random augmentation can generate variations around an original sample, the optimal decision boundary optimized on a combination of is straightforward as they are likely to be incorrectly predicted, either false positives or negatives (Fig. 3-middle).\nIncorrect Prediction Sampling (IPS). The shifting boundary effect gives an idea of a sampling operator that $\\mathcal{X}_t$ is only selected if it comes from a victim class $(\\mathbf{y}_a)$ and its pseudo-label is incorrectly predicted (i.e., $\\widehat{\\mathcal{Y}}_t \\neq \\mathcal{Y}_t > \\mathcal{Y}_t = \\mathbf{y}_a$). We name this strategy as IPS. When $\\mathcal{X}_{t}$'s are not i.i.d. sampled from $P_t$, but IPS operator is applied instead, an attacker can easily modify the decision boundary of a TTA model. Fig. 4 illustrates the shifting boundary effect caused by IPS. Due to the expansion of incorrect predictions of the victim class via augmentation and TTA model update, its decision boundary is penetrated by the nearby classes.\nNumerical Simulation. To empirically confirm the case of IPS and Aug on GMMC, we carry out a numerical simulation in Fig. 5, with $T = 120$ adaptation steps (See the Appendix for setup details). The collapse is observed when most predictions converge to a single label, no matter what the input data is (see Def. 1). In Fig. 5a, the model is collapsed when IPS is applied while this is not the case in Fig. 5b. When removing Aug in Fig. 5c, the boundary can be shifted, but a total collapse is not observed. In sum, the collapse only happens when the two conditions are met: IPS is performed and Aug operator is in place. Slowing down the update $\\alpha$ (Eq. 7) helps mitigate the effect of RIP (Fig. 5d). We will return back to this discussion in Sec. 6.5."}, {"title": "4. Reusing Incorrect Predictions (RIP) Attack", "content": "Inspired by IPS, this section establishes the threat model and our RIP attack, an algorithm introducing this threat.\n4.1. Threat Model\nThe threat model in this study focuses on two main aspects: first, making a TTA model collapsed as the main objective and second, black-box as the constraint for the attack.\nCollapsing Attack Objective. Collapsing attack on continual TTA is first introduced in this study. Its primary objective is to make a continual TTA model that tends to ignore some categories in $\\mathcal{Y}' \\subset \\mathcal{Y}$. This is called model collapse, and Definition 1 restates its mathematical definition in [17]:\nDefinition 1 (Model Collapse). A model is said to be collapsed from step $\\tau \\in \\mathcal{T}, \\tau < \\infty$ if there exists a non-empty subset of categories $\\mathcal{Y}' \\subset \\mathcal{Y}$ such that $Pr{\\mathcal{Y}_t \\in \\mathcal{Y}'} > 0$ but the marginal $Pr{\\widehat{\\mathcal{Y}}_t \\in \\mathcal{Y}'}$ converges to zero in probability:\n$\\lim_{t \\rightarrow \\tau} Pr{\\widehat{\\mathcal{Y}}_t \\in \\mathcal{Y}'} = 0$.\nAs it is irrecoverable once collapsed, the only remedy would be resetting all parameters back to $\\theta_0$. Tab. 1 compares the existing continual TTA attack objectives [51] and our TTA model collapsing TTA attack (columns 2-3).\nMetric for Collapsing Attack. We simply compute the average class-wise testing error among all categories and report the increment versus the same model without attack.\nBlack-box Continual TTA Model Attack. In a TTA black-box attack, an attacker can only interact with queries and model responses [38]. Before and during the adaptation, accessing the model architecture and parameters, the operating TTA algorithm, gradient information, optimizer state, etc., or queries from other users are strictly forbidden at any time. We define the terminology of black-box TTA attack as any attack algorithm following these constraints.\n4.2. Reusing Incorrect Prediction Attack Algorithm\nAttack Description. The idea of Reusing Incorrect Prediction (RIP) is as simple as IPS. RIP capitalizes on the vulnerability of TTA in Sec. 3 by intentionally reusing incorrect predictions from one victim class ya in subsequent adaptation steps. All mispredicted samples from ya in the previous steps are accumulated and reused. A severe do-"}, {"title": "5. Continual TTA Methods Under RIP Attack", "content": "5.1. Experimental Setup\nContinual TTA Task, Dataset and Methods. The effect of RIP attack is evaluated on the image classification task, with three benchmarks including CIFAR10 \u2192 CIFAR10-C, CIFAR100 \u2192 CIFAR100-C, and ImageNet \u2192 ImageNet-C [14]. The following continual TTA methods ($f_t$ in Alg. 1) are studied: COTTA [49], EATA [34], RMT [6], ROTTA [53], ROID [30], TRIBE [43], and PeTTA [17].\nAttack Scenario. We employ RIP attack follows Alg. 1, with $T_a = 500$ rounds, $B = 64$ and $D_a$ is a set of images corrupted by impulse noise from each dataset. The choice of corruption here is arbitrary and B follows prior studies. Results in the Appendix show that other options also perform well. In following experiments, we compute the error of each class independently, after every 25 adaptation step, and report the average value among all classes. For a robust estimation, we repeat the attack 10 times (trials), each with a different victim attack label ya, randomly selected and averaged across trials. For all compared TTA methods, we use the default set of hyper-parameters from their authors.\n5.2. Vulnerability of Existing TTA Methods\nThe vulnerability of many existing TTA methods under RIP attack is confirmed in Tab. 2. Overall, the average testing error increment is observed in all datasets. To qualitatively observe this effect on CIFAR-10-C [14], we visualize the testing error after every 25 steps. Two scenarios are considered: under RIP attack (Fig. 6a), and no attack (Fig. 6b). Surprisingly, COTTA [49] or EATA [34] - the earliest methods have the best resilience to RIP. The following Sec. 6 conducts ablation studies to explain when a TTA method fails or thrives. As a teaser for their resilience, EATA does not involve training with augmented sample Aug, CoTTA is a simple method that uses the teacher model for predicting the pseudo-labels (Eq. 9). However, we note that while their limitations and assumptions do exist, motivating the development of many subsequent methods [8, 53]. The behavior of real TTA methods matches the risk (Sec. 3). Additional attack results are provided in the Appendix."}, {"title": "6. Analyzing the Causes of Vulnerability", "content": "There are multiple living components inside a continual TTA. This section introduces a baseline method (Sec. 6.1) and a series of ablation studies, isolating those factors to evaluate the risk of existing design choices: loss function (Sec. 6.2), level of augmentation (Sec. 6.3), pseudo-label generator (Sec. 6.4) and model update scheme (Sec. 6.5).\n6.1. Baseline Continual TTA Method\nWe employ a baseline continual TTA method based on Eq. 5, 7, closed to COTTA [49]. This simple model updates the linear parameters of batch normalization [19, 48]. For simplicity, the ablation studies are conducted on CIFAR-10-C [14] - 10 trials averaged. Without specifically noted, the $L_{CE}$ with augmentation and update rate $\\alpha = 0.99$ are used.\n6.2. Effects of the Loss Function Choices\nSetup. We explore the effect of the loss function on the robustness of a TTA method under RIP attack. Different choices of loss functions in Sec. 2.2: $L_{Ent}$ (Eq. 2), $L_{CE}$ (Eq. 3, augmentation), $L_{RMT}$ [6], and $L_{SLR}$ (Eq. 4) are used as $L_{CLS}$ in the baseline method's update step (Eq. 6).\nResults. Fig. 6c introduces the testing errors. The effect of RIP on each loss function choice is different. Notably, all loss functions that involve augmented samples (Eq. 5) as discussed in Sec. 2.2, such as $L_{CE}$ and $L_{RMT}$, are more susceptible to RIP attack. In contrast, simpler loss functions like $L_{Ent}$ and $L_{SLR}$ perform well. $L_{RMT}$, falling between these two categories of functions, is intermediate.\n6.3. Effects of the Level of Augmentation\nSetup. To confirm the suspicions of augmented samples, we investigate the correlation between the level of data augmentation and the tolerance of the baseline model to RIP. Following a prior study [49], random color jitter, affine transformations, and horizontal flipping are applied. The level of augmentation varies from 1 to 5 (5 is the strongest, and the typical level used in practice-[49]).\nResults. The effect of the RIP attack on the baseline algorithm is visualized in Fig. 6d. Unsurprisingly, the damage of RIP is correlated with the level of augmentation used, well explained by the shifting boundary effect in Sec. 3.2.\n6.4. Effects of the Pseudo-label Predictor\nSetup. Previous ablation studies identified the use of loss functions computed on augmented samples as the main cause of RIP vulnerability. Sec. 2.4 presents two choices of model for pseudo-label predictors: teacher (Eq. 9) and student (Eq. 10). They are investigated in this section.\nResults. The experimental result in Fig. 6e shows that the pseudo labels predicted by the student model make continual TTA methods more vulnerable to RIP attach, compared to the teacher model. This can be explained by the improved accuracy of pseudo-labels for adaptation, as stated in [46].\n6.5. Effects of the Model Update Rate\nSetup. Extending the observation in Sec. 6.4, we study the effect of the update rate $\\alpha$ (Eq. 7) on the resilience of the baseline model under RIP attack. Various values are chosen from $\\alpha = 0$ (no mean teacher update) and $\\alpha = 1$ (no TTA).\nResults. Fig. 6f plots the testing error with increasing value of $\\alpha$. The slower the update rate, the better the model can mitigate RIP attack. However, it cannot be eliminated."}, {"title": "7. Discussions and Conclusions", "content": "RIP Attack Defense. Although this is not the primary focus of this study, we still investigate some techniques: the source replay [6, 25], the contrastive loss [6, 20], and the source model weight ensemble (Eq. 8). Fig. 7 visualizes their effect. Constraining the model's divergence from the source model appears to help preserve the resistance of the baseline TTA method, but it does not fully eliminate the risk. Eliminating Aug operator could remove the risk, but it comes at the cost of reduced performance. While using the teacher model for pseudo-label prediction can make the attack more difficult, there is a trade-off as noted in [39].\nLimitations of RIP Attack. Though it may not be costly to execute, RIP attack still requires either collecting a small labeled dataset or a manual step to find incorrect predictions. From Sec. 5, 6, RIP attack can only succeed if a random augmentation operator is used. Although this is not a criterion in prior studies [2, 51], we recognize that these attempts, including RIP, require continuous sample submissions, occupying the testing stream for a prolonged period.\nFuture Work. The introduced RIP is relatively simple, only utilizing the final predicted label as the feedback signal for the attack purpose. An elaborated version of it can be developed by exploiting the output probability or the confidence score associated with each prediction. The ability to defend RIP attack is only briefly discussed, necessitating the development of an efficient RIP defense mechanism.\nConclusions. Orthogonal to prior TTA attack studies, Reusing of Incorrect Prediction (RIP) draws our attention to an untouched concern: \u201cContinual TTA is vulnerable to an intriguingly simple black-box algorithm\u201d. This study confirms the risk on recent continual TTA methods and highlights that the use of augmentation is correlated to RIP's vulnerability. As the key mechanism, TTA makes a model more confident in their predictions after each adaptation step, which is undesirably, also magnifies the errors caused by incorrect pseudo-labels. This becomes a backdoor for attackers to intentionally collapse a continual TTA model."}, {"title": "A. Related Work", "content": "Continual Test-time Adaptation (TTA). Under the circumstance of testing data distribution diverged [41], Test-Time Adaptation (TTA), a domain generalization technique [56], enhances the performance of a machine learning (ML) model by enabling its parameters to change through test-time training [26, 44]. Fundamentally, TTA encourages the ML model to be more confident in their predictions by minimizing the prediction entropy [24, 33-35, 48, 48]. Observing that the distribution may not happen once, but can be changed continuously, later studies extend the TTA approach to multiple shifts setting [17, 40, 49]. Towards real-world TTA, recent research studies TTA also investigate the scenarios where the label distribution is non-i.i.d., or temporally correlated [8, 30, 35, 53]. These studies, on the one hand, address major failure modes of TTA on challenging testing streams, are far more complicated than earlier methods such as [34, 49]. This necessitates further investigation into their reliability and trustworthiness during deployment, with RIP attacks being on this line of inquiry.\nAdversarial Attacks and Defenses in ML. An adversarial attack aims to degrade the performance of a victim ML model by manipulating the input data, causing it to produce false predictions [9, 32, 45, 50]. Those attempts can be classified into main categories: white-box, black-box, and gray-box [52], based on the attacker's knowledge of the victim system. The white-box attack assumes the model is fully accessible, allowing gradient-based algorithms to craft the adversarial samples [9, 45]. Meanwhile, the black-box attack strictly restricts these favors and repetitively queries the model to adjust the strategy [38]. A black-box attack is significantly more realistic. Adversarial defense techniques are also explored to counteract attack attempts. Straightforwardly, a trained ML model to detect and filter adversarial samples [23, 36, 50]. More advanced techniques can make the model more robust to such perturbations during training [10, 37] or be more adaptive at test time [4].\nContinual TTA Attack. While adversarial attack and continual TTA are two active research areas, only a limited number of prior studies investigate this risk on continual TTA methods. Poisoning [2] or distribution invading [39, 51] attacks are among the first studies uncovering the risk of continual TTA being manipulated by injecting malicious testing samples. Even though they are the closest to our work, both fall into the white-box attack category that assumes the attacker has complete knowledge of the underlying model. In contrast, our RIP attack neither makes any assumptions about the operating TTA algorithm nor the adaptation process, and we simply audit the model outputs to modify the attack scheme. To the best of our knowledge, we are the first to introduce an implementable black-box attack algorithm targeting a continual TTA method.\nComparison of RIP Versus Previous Continual TTA Attack Studies. Besides studying a black-box continual TTA attack for the first time, this work is orthogonal to the prior studies [2, 39, 51] in multiple aspects. Firstly, the investigation here primarily focuses on the risk during self-training (data augmentation, loss function, model update, and pseudo-label predictor) utilized for the TTA process, not the risk in intercepting the batch normalization [19] statistics like [39, 51] to create false predictions. The long-term effect on a TTA model after the attack is not also well investigated. Secondly, this work targets the recent continual TTA algorithms, mean-teacher model update [46], not the traditional single-domain TTA approaches [33, 35, 48]. Thirdly, the attack action of RIP is inherently natural, as it simply reuses images from the testing distribution without requiring any image-level modifications via adversarial training and gradient descent update [9, 45]."}, {"title": "B. Additional RIP Attack Results", "content": "In Alg. 1, all the input parameters of RIP attack have been introduced. Within the main text, the effectiveness of RIP has been demonstrated on a variety of different continual TTA methods (varying ft(x)) (Sec. 5.2). While the number of attack steps Ta can be monitored by determining whether the model performance is saturated, this section provides additional RIP attack results to complete the discussion. Specifically, we study RIP attack with varying victim class Ya (Sec. B.1), labeled attack dataset Da (Sec. B.2) and testing batch size B (Sec. B.3).\nB.1. Varying Victim Classes\nWe provide the confusion matrices of the baseline continual TTA method (Sec. 6.1) under RIP attack in Fig. 8. Here, the confusion matrix of this model at the 1st, 100th, and the 400th adaptation step is visualized. From this figure, we can see the effect of RIP attack with three different choices of the victim class ya. It is hard to see in the main tables and figures since they provide the averaged value across all 10 selected classes (or trials, as defined in the main text). Nevertheless, the observation is similar, regardless of the selected victim class. Furthermore, this figure also showcases the \"shifting boundary effect\" (Fig. 4 and Sec. 3.2) on a real image dataset. A transition in the model predictions on the victim class to the most misclassified class is consistently observed in all the choices of the victim class.\nB.2. Varying Choices of Labeled Attack Dataset\nRIP attack requires a dataset with labels Da to perform (for checking the correctness of the model predictions). In"}, {"title": "C. Implementation Details", "content": "C.1. Random Data Augmentation Operator\nFollowing prior work [49], the image-level data augmentation Aug() investigated in this study is composed of a series of operators: random horizontal flipping, affine transformation, color jittering, additive Gaussian noise, blurring by a Gaussian kernel. Sec. 6.3 introduces five level of augmentations. They are created by varying the degree of randomness and the strength of each component augmentation operator. Specifically, we keep the probability for random horizontal flipping at 0.5 and the kernel size of 3, standard deviation of 0.005 for the Gaussian blur at all levels. Other components' parameters at each level are detailed in Tab. 3. For illustration, in Fig. 10, the effect of augmentation on an image in ImageNet [5] at each level is provided.\nC.2. The Numerical Simulation on Gaussian Mixture Model Classifier (GMMC)\nFor the GMMC simulation mentioned in our analysis, a toy dataset is constructed with $N=1,000$ data points, drawn from a mixture of two Gaussian distributions: $(\\mu_0, \\mu_1) = (-1.0, 2.0)$ and $\\sigma_0 = \\sigma_1 = 1.0$. We simulate a TTA process with $T_a = 120$ steps. The level of augmentation in the Additive White Gaussian Noise (AWGN) opearator is set to $\\sigma = 0.2$. Visit [17] for further discussions on GMMC.\nC.3. Continual Test-time Adaptation Methods\nSource Model and Dataset. RobustBench [3] and torchvision [29] provide the initial models (fo) trained on the source distributions. From RobustBench, the model with checkpoint Standard and Hendrycks2020AugMix_ResNext [15] are adopted for CIFAR10-C and CIFAR-100-C experiments, respectively. The ResNet50 [13] model pre-trained on ImageNet V2 (specifically, checkpoint ResNet50_Weights.IMAGENET1K_V2 of torchvision) is used for ImageNet-C experiments.\nUpdated Parameters. Following prior studies [6, 48, 49, 53], $\\theta_t$ - the linear parameters of batch norm layers [19] are updated at each adaptation step $t$.\nOptimizer. Adam [21] optimizer with learning rate equal to 1e-3, and $\\beta = (0.9, 0.999)$ is selected as a universal choice for all experiments.\nC.4. RIP Attack Trials\nAs mentioned in the experimental setup, the average performance across 10 RIP attack trials is reported. For each trial, all settings are kept the same, except for the victim class ya. This class is uniformly sampled without replacement from all possible classes. For reproducibility, we list the index of the victim classes (ya) used in our experiments:\n\u2022 CIFAR-10-C: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 (all classes).\n\u2022 CIFAR-100-C: 3, 8, 29, 48, 56, 67, 71, 88, 91, 96.\n\u2022 ImageNet-C: 91, 323, 392, 583, 630, 637, 643, 707, 864, 952.\nC.5. Computing Resources\nExperiments are conducted on a computer cluster with an Intel(R) Core(TM) 3.30GHz Intel Core i9-9820X CPU, 128 GB RAM, and 4\u00d7NVIDIA Quadro RTX 5000 GPUs."}]}