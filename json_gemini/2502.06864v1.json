{"title": "Knowledge Graph-Guided Retrieval Augmented Generation", "authors": ["Xiangrong Zhu", "Yuexiang Xie", "Yi Liu", "Yaliang Li", "Wei Hu"], "abstract": "Retrieval-augmented generation (RAG) has emerged as a promising technology for addressing hallucination issues in the responses generated by large language models (LLMs). Existing studies on RAG primarily focus on applying semantic-based approaches to retrieve isolated relevant chunks, which ignore their intrinsic relationships. In this paper, we propose a novel Knowledge Graph-Guided Retrieval Augmented Generation (KG2RAG) framework that utilizes knowledge graphs (KGs) to provide fact-level relationships between chunks, improving the diversity and coherence of the retrieved results. Specifically, after performing a semantic-based retrieval to provide seed chunks, KG2RAG employs a KG-guided chunk expansion process and a KG-based chunk organization process to deliver relevant and important knowledge in well-organized paragraphs. Extensive experiments conducted on the HotpotQA dataset and its variants demonstrate the advantages of KG2RAG compared to existing RAG-based approaches, in terms of both response quality and retrieval quality.", "sections": [{"title": "Introduction", "content": "Recently, large language models (LLMs) (Li et al., 2024; Ren et al., 2024; Touvron et al., 2023; Brown et al., 2020) have achieved remarkable success across a broad range of real-world tasks, including question answering (Sen et al., 2023), writing assistance (Calamo et al., 2023), code generation (Cheng et al., 2024), and many others (Kaddour et al., 2023; Wu et al., 2023). However, hallucinations (Xu et al., 2024b; Liu et al., 2024a) in the generated responses becomes a critical challenge, which often results from containing outdated information or lacking domain-specific knowledge. Retrieval-augmented generation (RAG) (Gao et al., 2023; Fan et al., 2024) has emerged as a feasible solution to mitigate hallucinations by retrieving relevant knowledge from provided documents and incorporating it into the prompts of LLMs for response generation.\nExisting studies in RAG (Lewis et al., 2020; Yu, 2022; Purwar and Sundar, 2023; Gao et al., 2023; Ziletti and D'Ambrosi, 2024), as shown in Fig. 1, employ keyword-based or semantic-based approaches to retrieve documents or chunks having the highest similarities to user queries. However, these retrieved chunks can be homogeneous and redundant, which fails to provide the intrinsic relationships among these chunks and cannot further activate the reasoning abilities of LLMs. Furthermore, the retrieved chunks are often directly concatenated in the order of their similarity scores and fed into LLMs as part of the prompts. Such a practice can lead to isolated pieces of information, limiting the utility of LLMs in generating comprehensive and reliable responses.\nKnowledge graphs (KGs) (Auer et al., 2007; Ji et al., 2022), as structured abstractions of real-world entities and their relations, can be expected to effectively supplement existing semantic-based RAG approaches by integrating structured factual knowledge. Knowledge within a KG, represented in the form of triplets (head entity, relation, tail entity), is naturally linked through overlapping entities. A simplified workflow for utilizing KGs in RAG is shown in Fig. 1, where relevant triplets are retrieved to augment the context for response generation in LLMs, providing fact-level relationships among chunks and highlighting important facts that may be missed by semantic-based approaches.\nShed light by such insights, in this paper, we propose a novel Knowledge Graph-Guided Retrieval Augmented Generation framework, called KG2RAG. Specifically, we first perform chunking and KG-chunk association during the offline processing of the provided documents, establishing linkages between chunks and a specific KG to capture the fact-level relationships among these chunks. Based on the chunks and the KG, KG2RAG employs KG-enhanced chunk retrieval, which consists of a semantic-based retrieval and graph-guided expansion. The semantic-based retrieval prepares several seed chunks using embedding and ranking techniques (Nussbaum et al., 2024; Li and Li, 2024). These seed chunks are then used to extract a relevant subgraph from the association KG, onto which we can apply graph traversal algorithms to include the chunks containing overlapped or related entities and triplets. Such a design of graph-guided expansion provides a greater diversity of retrieved chunks and a comprehensive knowledge network.\nAfter that, we incorporate a post-processing stage named KG-based context organization in KG2RAG. On one hand, the KG-based context organization serves as a filter to retain the most relevant information contained in the subgraph, thereby enhancing the informativeness of the retrieved chunks. On the other hand, it serves as an arranger to organize the chunks into internally coherent paragraphs with the knowledge graph as a skeleton. These semantically coherent and well-organized chunks are fed into the LLMs along with user queries for response generation.\nWe conduct a series of experiments on the widely-used HotpotQA (Yang et al., 2018) dataset and its newly constructed variants to mitigate the impacts of prior knowledge on LLMs. We adopt a distractor and a fullwiki setting, comparing KG2RAG with several RAG-based approaches. The experimental results demonstrate that KG2RAG consistently outperforms baselines in terms of both response quality and retrieval quality. Moreover, we conduct an ablation study to highlight the effectiveness of different modules in KG2RAG. The constructed dataset and source code are released at https://github.com/nju-websoft/KG2RAG to further promote the development and application of KGs in RAG."}, {"title": "Methodology", "content": "An overview of the workflow of KG2RAG is illustrated in Fig. 2. In the following subsections, we provide more details following the workflow of KG2RAG, including document offline processing (Sec. 2.1), KG-enhanced chunk retrieval (Sec. 2.2), and KG-based context organization (Sec. 2.3)."}, {"title": "Document Offline Processing", "content": "Following the existing studies in RAG (Lewis et al., 2020; Gao et al., 2023; Fan et al., 2024), all documents are first split into n chunks based on the structure of sentences and paragraphs given a predefined chunk size, which can be given as $D = {C_1,...,C_n}$. These chunks can be further processed, for example, by adding relevant context (Jiang et al., 2023; Eibich et al., 2024), extracting meta-information (Mombaerts et al., 2024) (e.g., title, abstract), and generating corresponding questions (Ma et al., 2023; Wang et al., 2024b).\nSince these chunk-enhancing techniques are orthogonal to the proposed method in this paper, we recommend referring to the original paper for more details. Hereafter, we continue to denote the processed chunks as $D = {c_1, . . ., C_n}$.\nTo capture the rich fact-level relationships among these chunks, we associate them with a KG, which can be implemented via the following approaches. In cases where a KG is available, such as in WebQSP (Yih et al., 2016) and CWQ (Talmor and Berant, 2018), the chunk-KG association can be performed through entity and relation recognition and linkage algorithms (Zhao et al., 2023; Tian et al., 2024). Another approach involves directly extracting multiple entities and relations from the chunks to form subgraphs, which can be used to combine into a complete graph. In this paper, to avoid reliance on existing KGs, we adopt the latter approach, implementing it by providing appropriate prompts (refer to Fig. 3) to LLMs.\nAfter this process, we provide linkages between chunks and a specific KG, which can be given as\n$G = {(h, r,t, c) | c \u2208 D}$,\nwhere h, r, and t denote the head entity, relation, and tail entity, respectively, and e denotes the chunk that derives the triplets. Note that the chunk-KG association process is query-independent, which implies that it can be performed offline, only needs to be constructed once for all documents, and supports incremental updates for new documents. As"}, {"title": "KG-enhanced Chunk Retrieval", "content": "Given the chunks D and the associated KG G, the proposed KG2RAG suggests a two-stage retrieval process, including semantic-based retrieval and graph-guided expansion.\nDuring the semantic-based retrieval process, the semantic similarities between a user query q and all the chunks can be measured as\n$S = {s(q, c) | c \u2208 D}$,\nwhere the similarity function s(\u00b7) employs an embedding model (Nussbaum et al., 2024; Li and Li, 2024) to transfer the query and chunks into high-dimensional representations, followed by computing their cosine similarity.\nThe chunks with the top-k highest similarities to the query are selected as the retrieved chunks, denoted by $D_q$. These retrieved chunks can be integrated into the prompts as context and fed into LLMs for RAG. As discussed in Sec. 1, relying solely on semantic-based retrieval may result in isolated chunks, missing crucial factual knowledge and the intrinsic connections among the chunks. To tackle this, we regard the retrieved chunks $D_q$ as seed chunks, and propose a graph-guided expansion process.\nDuring communication and thinking processes, people often connect one event to others as these events involve the same entities, such as persons and places. For example, Capitol Hill, Washington, D.C. connects our impressions of Barack Obama, Donald Trump, and Joe Biden, as they all delivered their presidential inaugural speeches there in 2013, 2017, and 2021, respectively. Shed light by such insights, KG2RAG suggests linking one chunk to other chunks through the overlapping or connected entities that they contain for retrieved chunk expansion.\nSpecifically, given the retrieved chunks $D_q \u2286 D$ and the KG $G = {(h, r, t, c) | c \u2208 D}$, we first get the relevant subgraph of $D_q$ as follows:\n$G = {(h, r, t, c) | c \u2208 D_q} \u2286 G$.\nAfter that, we traverse the m-hop neighborhood of $G_q$ to get the expanded subgraph $G_r$, which can"}, {"title": "KG-based Context Organization", "content": "After the KG-enhanced chunk retrieval, KG2RAG incorporates a post-processing stage before response generation of LLMs, motivated by the following two considerations.\nFirstly, the number of expanded chunks through the graph-guided expansion is tied to the triplets contained in the expanded subgraph, which can be too large, potentially exceeding the context length and introducing noise that may obscure helpful information. Secondly, inspired by human reading habits and previous studies (Li, 2023; Liu et al., 2024b), providing semantically coherent and well-organized materials as context makes positive impacts on the understanding and generation performance of LLMs. As a result, we propose a KG-based context organization module in KG2RAG, which serves as both a filter and an arranger to meet these requirements.\nSpecifically, we first calculate the semantic similarities between the expanded chunks with the user query, according to Eq. (2). Based on these similarities, the expanded subgraph $G_r$ can be transformed into an undirected weighted graph as follows:\n$U = {(h \u2194 t,rel : r, src : c, weight : s(q, c))|(h,r,t,c) \u2208 G}$,\nwhere h t represents an undirected edge, attached with the corresponding relation and the source chunk as meta information. We reuse the semantic similarities calculated in Sec. 2.2 to save computing resources.\nDue to the cohesive nature of knowledge, $U_q^m$ can naturally be divided into p connected components, denoted by $B_i, 1 < i < p$, where nodes within each connected component $B_i$ represent entities from the KG. Note that multiple edges may connect a pair of nodes due to redundant knowledge, which promotes us to generate the maximum spanning tree (MST) of each connected component"}, {"title": "Conclusion", "content": "In this paper, we propose KG2RAG, a novel framework designed to enhance the performance of RAG through the integration of KGs. We introduce linkages between chunks and a specific KG, which help in providing fact-level relationships among these chunks. Consequently, KG2RAG suggests performing the KG-guided chunk expansion and the KG-based context organization based on seed chunks retrieved by semantic-based retrieval approaches. Through these processes, the retrieved chunks become diverse, intrinsically related, and self-consistent, forming well-organized paragraphs that can be fed into LLMs for high-quality response generation. We compare KG2RAG with existing RAG-based approaches, demonstrating its superior performance in both response quality and retrieval quality. An ablation study is also conducted to further confirm the contributions of KG-guided chunk expansion and KG-based context organization, indicating that these two modules collaboratively enhance the effectiveness of KG2RAG."}, {"title": "Limitations", "content": "Retrieval-augmented generation (RAG) is a systematic engineering framework that can be refined from multiple perspectives, including query rewriting (Xiao et al., 2023), retrieval optimization (Eibich et al., 2024), multi-turn dialogue (Yao et al., 2023) and so on (Gao et al., 2023). KG2RAG only focuses on the part of retrieval optimization and aims to perform KG-guided retrieval expansion and KG-based context organization to enhance RAG with the structured factual knowledge from KGs, without optimizing other modules. However, the proposed KG2RAG is orthogonal and compatible with the aforementioned modules. In the future, we will develop KG2RAG into a plug-and-play tool that can be easily integrated with other approaches, thereby better facilitating the research community."}, {"title": "A Additional Experimental Results", "content": "We conduct additional experiments on two different datasets to confirm the effectiveness and generality\nof KG2RAG in various scenarios.\nAs shown in Table 7, KG2RAG maintains superi-\nority on the widely-used MuSiQue dataset (Trivedi\net al., 2022) in response F1 score, response exact\nmatch (EM) rate, and retrieval F1 score."}, {"title": "Efficiency Analysis", "content": "We compare the KG construction cost of KG2RAG\nwith two other KG-enhanced RAG approaches:\nLightRAG (Guo et al., 2024) and GraphRAG (Edge\net al., 2024). The results, as summarized in Ta-\nble 9, demonstrate that KG2RAG is more efficient\nin terms of token cost, the number of LLM calls,\nand time cost.\nWe calculate the average retrieval time and\ngeneration time of KG2RAG compared to Ligh-\ntRAG and GraphRAG. The results in Table 10 in-\ndicate that KG2RAG requires less time for both\nretrieval and response generation than LightRAG"}]}