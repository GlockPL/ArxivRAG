{"title": "Jailbreaking with Universal Multi-Prompts", "authors": ["Yu-Ling Hsu", "Hsuan Su", "Shang-Tse Chen"], "abstract": "Large language models (LLMs) have seen rapid development in recent years, revolutionizing various applications and significantly enhancing convenience and productivity. However, alongside their impressive capabilities, ethical concerns and new types of attacks, such as jailbreaking, have emerged. While most prompting techniques focus on optimizing adversarial inputs for individual cases, resulting in higher computational costs when dealing with large datasets. Less research has addressed the more general setting of training a universal attacker that can transfer to unseen tasks. In this paper, we introduce JUMP, a prompt-based method designed to jailbreak LLMs using universal multi-prompts. We also adapt our approach for defense, which we term DUMP. Experimental results demonstrate that our method for optimizing universal multi-prompts outperforms existing techniques. Our code is publicly available on GitHub\u00b9.", "sections": [{"title": "1 Introduction", "content": "In earlier stages of NLP, adversarial attacks primarily targeted the vulnerabilities of fine-tuned models on specific downstream tasks (Jin et al., 2019; Li et al., 2020; Alzantot et al., 2018). However, with the advent of large language models (LLMs) such as Meta's LLaMA family (Touvron et al., 2023) and OpenAI's GPT series (OpenAI, 2023), which are trained on massive datasets and capable of generalizing across a broad spectrum of tasks without the need for fine-tuning, the landscape has shifted. These models have demonstrated remarkable versatility and applicability across various domains (Zhao et al., 2023; Touvron et al., 2023; OpenAI et al., 2023), gaining significant influence in recent years. The prevalence of instruction tuning and Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) has further enhanced LLMs' ability to generate human-preferred responses, contributing to their widespread release as valuable tools and assistants.\nHowever, despite their utility, the pretraining datasets used for these models may contain unsafe content, which can lead to undesirable behavior when exposed to malicious user inputs (Ganguli et al., 2022; Zou et al., 2023). To mitigate this, safety alignments (Ziegler et al., 2019; Rafailov et al., 2023; Ji et al., 2023) have been incorporated into the development of LLMs to reject unethical or harmful requests.\nTo explore the blind spots of LLMs, early works attempted attacks using handwritten resources (Shen et al., 2023; Shah et al., 2023; Wei et al., 2023a). Due to the inefficiency of these approaches, numerous attacks have proposed automated processes, with one of the most prominent being GCG (Zou et al., 2023), which searches for adversarial suffixes using gradient information. Unfortunately, since this strategy optimizes without considering the naturalness of the text, it can easily be detected as a malicious request by defense techniques (Alon and Kamfonas, 2023), thereby reducing its effectiveness in such situations.\nTo bypass the issues mentioned above, there has been increasing research focused on creating human-readable prompts. We categorize these works into two types. The first type involves assisting with a set of pre-crafted prompts. A representative work in this category is (Liu et al., 2023; Yu et al., 2023), which investigates evolutionary algorithms to search for the most effective jailbreak prompt by iteratively editing pre-crafted templates. However, these approaches heavily rely on a set of handcrafted prompts and cannot function without human-curated resources. The second type does not rely on pre-crafted prompts. A notable example is BEAST (Sadasivan et al., 2024), which uses beam search decoding to find the best suffixes. While the approaches previously mentioned"}, {"title": "2 Related Work", "content": "As more robust large language models have been released, the concept of jailbreaking has emerged. Researchers have attempted to craft prompts, such as DAN (Do Anything Now) (Shen et al., 2023), that describe the characteristics of the model and try to persuade it to act accordingly. However, such works (Shen et al., 2023; Shah et al., 2023; Wei et al., 2023a) are resource-intensive and require significant human effort, making them inefficient.\nTo reduce human effort in attacking, most research focuses on automatically generating adversarial prompts. The earliest works primarily concentrate on white-box attacks (Zou et al., 2023; Guo et al., 2021; Shin et al., 2020). Despite their success in achieving high attack success rates on widely-used models, these methods still suffer from high perplexity issues, which can easily be detected by defenses such as perplexity filters (Alon and Kamfonas, 2023).\nTo mitigate this situation, several works aim to search for human-readable prompts. Notable works, such as AutoDAN (Liu et al., 2023), apply genetic algorithms to produce new attack samples, while GPTFuzzer (Yu et al., 2023), motivated by software testing techniques, generates new samples by manipulating different operators. Both of these methods heavily rely on external handcrafted resources. Another approach to finding readable inputs is directly rephrasing (Chao et al., 2023; Mehrotra et al., 2023). Inspired by notable prompting techniques (Wei et al., 2022; Yao et al., 2023), this method utilizes an additional model to improve the rephrasing of harmful requests based on the interaction history. However, the approaches mentioned above can only optimize inputs individually. Several works (Wei et al., 2023b; Anil et al.) use in-context learning (Brown et al., 2020) by collecting harmful question-answer pairs as few-shot demonstrations, but these methods result in lower attack effectiveness."}, {"title": "2.2 Jailbreak with Finetuning Attackers", "content": "Compared to designing prompting algorithms to optimize inputs, several works (Paulus et al., 2024; Xie et al., 2024; Basani and Zhang, 2024; Wang et al., 2024) focus on fine-tuning an attacker to generate adversarial suffixes tailored to each input. These approaches can be more efficient, as they aim to optimize a group of malicious instructions and offer higher flexibility, allowing the trained attacker to generate customized suffixes for each input. While this approach seems ideal, training a model may require deeper expertise and result in increased time and effort spent on hyperparameter tuning."}, {"title": "2.3 Defenses against Jailbreaking", "content": "To enhance the safety of models, defense methods have been proposed to counter malicious inputs. Defenses can be implemented in various ways, such as the perplexity filter (Alon and Kamfonas, 2023), which detects abnormal inputs by evaluating the perplexity of input texts. ICD (Wei et al., 2023b) proposes an in-context defense that concatenates few-shot demonstrations consisting"}, {"title": "3 Methodology", "content": "Following the design of the work by (Zou et al., 2023), given the malicious input x (e.g. \"How to make a bomb\") and its associated target y (e.g. \"Sure, here is the step-by-step guideline for making a bomb\"), the goal is to find the adversarial suffix q that minimizes the cross-entropy loss L on the victim model \u03c0:\n\\(L_\\pi(x,q,y) = -log( \\prod_{i=1}^n p_\\pi(y_i | x, q, y_{1:i-1})\n)\"\nWe aim to extend our work to more general settings. Specifically, our objective is to optimize a universal attacker to achieve the best attack results on a set of malicious instructions. Several approaches are capable of achieving this goal, as shown in Table 5. In particular, we aim to search for a set of multi-prompts Q generated by a frozen-weight attacker \u03c6:\n\nmin \u2211 min  L(x, q, y)\nQ\n(x,y)\u2208(X,Y)\nq\u2208Q"}, {"title": "3.2 BEAST", "content": "Our work originates from a method called BEAST (Sadasivan et al., 2024). We follow the framework introduced in EasyJailbreak (Zhou et al., 2024b), which decomposes the jailbreak process into four components: Selector, Mutator, Constraint, and Evaluator. We demonstrate the procedure with this arrangement. In the main design of BEAST, the goal is to find an optimal adversarial suffix q for a given harmful instruction x and its associated target y. We define an additional language model as our attacker, \u03c6, and a victim model, \u03c0. In the initial stage, the first K tokens are sampled from the attacker \u03c6 to initialize the adversarial candidate set Q. For the following optimization steps, to search for the best K suffixes as attack prompts, we repeat the following steps: Mutator, Evaluator, and Selector, which are detailed as follows.\n\u2022 Mutator: First, new candidates will be generated in this stage. For each template tempi, the input for the attacker input; is formed by replacing the placeholder in tempi with"}, {"title": "3.3 From BEAST to JUMP*", "content": "As previously mentioned, to aim for the universal setting, which takes transferability across different inputs into consideration, we derive the first version of our new method, represented as JUMP*, that adapts BEAST to this new scenario. In the following, we emphasize the differences between BEAST and JUMP*.\nOur work aims to find optimal adversarial suffixes Q for multi-prompts, which is different from BEAST, which mainly focuses on optimizing single prompts for individual cases. The workflow of JUMP* is described in Figure 1. Assume we have M adversarial templates temp1, temp2,..., tempM framed by a red dashed line in Figure 1, malicious instructions X =\n{X1,X2,...,Xd}, and the associated target strings Y = {Y1, Y2, ..., Yd} from a training dataset. We divide the entire training process of JUMP* into the following stages: Initialization, Selector\u00b9, Mutator, Evaluator, and Selector\u00b2, which are detailed as follows."}, {"title": "3.4 Adding perplexity constraints to JUMP*", "content": "In some cases, user prompts containing unusual content may be easily detected as abnormal inputs by defenders. To enhance the stealthiness of jailbreak prompts, we incorporate a Constraint step between the Mutator and Evaluator stages in JUMP*, and name this final version JUMP. This version applies a sampling mechanism on each beam to obtain smaller sets of candidates with lower perplexities. The new set beam' is sampled from beam with probability \\(P(beam'):\n\nPP(beam') = {\\frac{e^{s_k}}{\\sum_t e^{s_t}}\t | k \u2208 1, . . ., Nc },\n\nwhere sk =  \\frac{1}{ppl_k}"}, {"title": "4 Attack Experiments", "content": "Our experiments are primarily conducted on AdvBench, which originates from the work of (Zou et al., 2023). This dataset contains two fields: goal and target. The goal column stores harmful instructions, while the target column contains the presumed prefix of the jailbroken response. We use the same train and test sets following the settings in AdvPrompter (Paulus et al., 2024)."}, {"title": "4.2 Victim Models", "content": "We choose from a diverse range of chat models, including recent popular open-source models such as the Llama family (Llama-2-7b-chat-hf and Llama-3-8b-instruct) (Touvron et al., 2023) from Meta, Mistral-7b-instruct (Jiang et al., 2023), Vicuna-7b (Zheng et al., 2023), and Gemma-7b-it (Mesnard et al., 2024), released by Google. We also conduct transfer attacks on closed-source models from OpenAI, including GPT-3.5, GPT-4, and GPT-40 (OpenAI et al., 2023)."}, {"title": "4.3 Evaluation Metrics", "content": "The experiment results are assessed using three types of metrics: String Matching, Llama Guard, and Perplexity.\n\u2022 String Matching (S) (Zou et al., 2023): It determines whether the response generated by victim models constitutes a jailbreak by detecting refusal patterns, such as \"I cannot fulfill your request\", \"I apologize\"."}, {"title": "4.4 Comparing Methods", "content": "Design an algorithm to train an attack model to generate adversarial suffixes. The procedure consists of two steps: in the query step, beam search is used to find the optimal suffix, and in the training step, the attacker is fine-tuned on the suffix.\n\u2022 AutoDAN (Liu et al., 2023): Utilize an evolutionary algorithm to optimize a set of handcrafted prompts.\n\u2022 GPTFuzzer (Yu et al., 2023): Motivated by software testing, they design multiple operations in seed selection and mutation to explore the combinations of different operators at each stage.\n\u2022 JUMP*: The first version of our proposed method focuses on finding a set of universal multi-prompts. The method is an extension of a previous work called BEAST (Sadasivan et al., 2024).\n\u2022 JUMP: An improved version of JUMP*, which integrates the Constraint step into the training pipeline.\n\u2022 JUMP++: The enhanced version of JUMP, which is initialized with our newly designed prompts."}, {"title": "4.5 Results and Discussions", "content": "Our work derives from BEAST, which focuses on finding adversarial suffixes using a beam search decoding process. In their original design, they aim to optimize a new prompt for each test case, which does not generalize to unseen data. A simple way to address this issue is to find a universal"}, {"title": "4.5.2 Trade-offs Between ASR and Perplexity", "content": "As previously mentioned, we found that optimizing with a universal single prompt is less effective. Therefore, we further developed our method, JUMP*, which attempts to find a set of multiprompts. The results in Table 1 show that, in most cases, we achieve better results than the baseline, AdvPrompter, at the cost of sacrificing the naturalness of context.\nWe further apply a perplexity constraint to JUMP*. The new version of our method, called JUMP, attempts to strike a balance between perplexity and ASR. Unfortunately, despite the satisfactory results from the previous experiment, we observe a significant performance drop after adding the perplexity constraint, as shown in Figure 2. We find that, after adding the constraint, the ASR@10 drops by more than 10 percentage points on both Llama models. Additionally, we discover that adjusting the temperature in the probability distribution during the Constraints step can indeed reduce perplexity, but it also penalizes the ASRs.\nTo mitigate the issues, we aim to improve our method by incorporating additional handcrafted assistance during initialization. We randomly select a sample template from the set of seed prompts proposed in AutoDAN. Then, we duplicate the sampled template to form the initial adversarial set. We compare this setting (denoted as JUMP initialized with AutoDAN) with the configurations shown in Figure 2, and the results in Table 9 demonstrate that the new approach effectively alleviates the trade-offs. In most cases, we achieve better ASRs with lower perplexity when using the additional initialization."}, {"title": "4.5.3 From JUMP to JUMP++", "content": "Encouraged by the previous results, we aim to design our own handcrafted templates and use them to assist with our training process. We demonstrate the effectiveness of the enhanced version, which we name JUMP++, in Table 2. From the experimental results, we observe that, although we achieve slightly inferior performance on models that are comparably vulnerable, such as Vicuna7b and Mistral-7b, our method, JUMP++, outperforms the rest of the models, including those that are harder to attack, such as the Llama models."}, {"title": "4.5.4 Sensitivity to different choices of initial templates", "content": "Since we utilize additional human resources in the JUMP++ setting, this may raise a concern: it is uncertain how much our beam search algorithm contributes to JUMP++, and some may suspect that the good performance comes from the carefully crafted prompts rather than JUMP itself. To clarify this, we tested three baseline methods, each initialized with three different initial prompts. The results, shown in Figure 3, demonstrate that our method, JUMP++, can generalize well when initialized with templates proposed in AutoDAN and JUMP++. Compared with the two baselines, we outperform AutoDAN in most cases. On the other hand, when compared with GPTFuzzer, although we achieve better results on Llama3-8b, we perform worse on Llama2-7b.\nOverall, we achieve comparable results, suggesting that there is room for improvement in the sensitivity to the choice of initial prompts for JUMP++. Additionally, we observe that all baselines tend to perform best when initialized with our designed prompt, indicating the success of our handcrafted prompts."}, {"title": "4.5.5 Transfer Attack Results", "content": "Our method, JUMP, depends on calculating losses on affirmative strings when assessing attack candidates in the Evaluator step. This may lead to a problem, as it is not always possible to access full model outputs, especially when dealing with proprietary models such as the GPT series released by OpenAI (OpenAI et al., 2023). To address this issue, we conduct transfer attack experiments on these closed-source models. We use two open-source models as proxies: Llama2-7b and Llama3-8b. The results, shown in Table 3, compare our method, initialized with JUMP++ and AutoDAN, to other baselines. We found that our method, initialized with templates from AutoDAN, achieves the best transfer results."}, {"title": "5 Defenses against Individual Attacks", "content": "We found that our framework can also adapt to defense scenarios. In this situation, we optimize multiple defense prompts for adversarial samples generated from individual attacks, which is similar to the concept in adversarial training (Madry et al., 2017)."}, {"title": "5.2 Comparing Methods", "content": "Attack each case without applying any defense.\n\u2022 SmoothLLM (Robey et al., 2023): A non-training defense approach involves adding random perturbations, such as Insert, Swap, and Patch, to input text in order to recover models tricked into generating jailbroken results.\n\u2022 DUMP: Our proposed defense method which aims to find a set of universal defense prompts."}, {"title": "5.3 Results", "content": "In the defense experiment, we attempt to defend against AutoDAN (Liu et al., 2023), a prominent attack mentioned previously. We compare our training-based method, DUMP, to other configurations. The results, shown in Table 4, demonstrate that DUMP outperforms both the no-defense scenario and SmoothLLM, highlighting its effectiveness in defending against individual attacks. We also showcase our ASR curves versus the number of queries in Figure 4. We observe that DUMP"}, {"title": "6 Conclusion", "content": "In this work, we explored attacking currently prominent models with multi-prompts in a general setting. Our experiments demonstrate that JUMP can achieve high performance, both in the setting without perplexity control (JUMP*) and in the one assisted by our designed prompts after adding constraints (JUMP++). We also adapted our approach for defense and achieved significant results."}, {"title": "7 Limitations", "content": "Though our experimental results may seem promising, there is still room for improvement. Currently, JUMP still cannot generate readable prompts while maintaining its efficiency. On the other hand, though JUMP++ can successfully mitigate the"}, {"title": "A Appendix", "content": "In our paper, we introduce the first version of our method, JUMP*, an algorithm that optimizes adversarial multi-prompts, focusing on attacking LLMs to generate jailbroken responses on a set of malicious data. The details of JUMP* are shown in Algorithm 1. To address the issue of readability, we propose the final version, JUMP, which integrates perplexity control into JUMP*. The details of the Constraints step are shown in Algorithm 2. Our implementation for inference using the optimized adversarial set is showcased in Algorithm 3."}, {"title": "A.2 Detail Settings in Experiments", "content": "We run our experiments on an Intel Xeon Gold 6226R CPU with an NVIDIA RTX A6000 GPU. The environment is configured with Python 3.10.8. For the settings of each process in the universal jailbreak attack, we set the time limit to 150,000 seconds. In the defense setup, we also set the same time limit for optimizing defense multi-prompts in DUMP."}, {"title": "A.2.2 Baseline Attacks", "content": "We categorize all the baselines into two types: model-based and prompt-based. For model-based methods, they focus on fine-tuning attackers to generate adversarial suffixes. The baseline we use in our experiments is AdvPrompter, utilizing their official implementation4. For prompt-based baselines such as AutoDAN and GPTFuzzer, we utilize their official code56, and all of them are transformed into the multi-prompt setting, similar to JUMP, which splits data into batches and optimizes each candidate with the corresponding batch in the adversarial set.\nFor the settings in the JUMP* experiments, we set the number of selected candidates K in the Selector step to be 6 and the beam size Ne in the Mutator to be 50. The number of instructions in a single batch is initialized to 20. We use 50 initial templates to form the adversarial set.\nWe didn't apply perplexity constraints in the JUMP* experiments. Instead, we added constraints in the experiments with the extended method,"}, {"title": "A.2.3 Baseline Defenses", "content": "Time limit Setup In the defense experiments, we try different scenarios against individual attacks. For the individual attack experiments, a time limit of 300 seconds is allocated per attack case under the no-defense scenario, and 480 seconds in other defense experiments (SmoothLLM, DUMP) to account for the additional runtime required for the defense process.\nAdversarial Data for Training In the defense experiments, we specifically choose AutoDAN as the attacker. Our method, DUMP, is a training-based defense method. We use handcrafted templates released in their official implementation, randomly replacing instructions from the training set as the set of adversarial samples used for training defense prompts.\nHyperparameter Settings We select several scenarios to compare with our method, DUMP. A simple baseline we choose is SmoothLLM, which applies random perturbations to inputs. For readability, we set the perturbation percentage q to 5%. We use the reimplemented version of the method\n7. For the settings in the individual attack experiments, in the No Defense scenario, we directly feed the adversarial sample to the victim model. In the other scenarios, we protect victim models by augmenting adversarial inputs into multiple options and choosing the one that is less harmful. The number of augmented data is set to be 50 in both the SmoothLLM and DUMP settings."}, {"title": "A.3 Supplementary Materials", "content": "Comparison with Beam Search-Based Approaches We compare our method, JUMP, with beam search-based approaches and organize them into the table shown in Table 5. The analysis of their pros and cons is presented in Table 6.\nASR Curves of Different Methods Across Various Models We have depicted ASR curves from baseline methods for each trial across all models on"}]}