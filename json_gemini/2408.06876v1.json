{"title": "Decision-Focused Learning to Predict Action Costs for Planning", "authors": ["Jayanta Mandi", "Marco Foschini", "Daniel H\u00f6ller", "J\u00f6rg Hoffmann", "Sylvie Thiebaux", "Tias Guns"], "abstract": "In many automated planning applications, action costs can be hard to specify. An example is the time needed to travel through a certain road segment, which depends on many factors, such as the current weather conditions. A natural way to address this issue is to learn to predict these parameters based on input features (e.g., weather forecasts) and use the predicted action costs in automated planning afterward. Decision-Focused Learning (DFL) has been successful in learning to predict the parameters of combinatorial optimization problems in a way that optimizes solution quality rather than prediction quality. This approach yields better results than treating prediction and optimization as separate tasks. In this paper, we investigate for the first time the challenges of implementing DFL for automated planning in order to learn to predict the action costs. There are two main challenges to overcome: (1) planning systems are called during gradient descent learning, to solve planning problems with negative action costs, which are not supported in planning. We propose novel methods for gradient computation to avoid this issue. (2) DFL requires repeated planner calls during training, which can limit the scalability of the method. We experiment with different methods approximating the optimal plan as well as an easy-to-implement caching mechanism to speed up the learning process. As the first work that addresses DFL for automated planning, we demonstrate that the proposed gradient computation consistently yields significantly better plans than predictions aimed at minimizing prediction error; and that caching can temper the computation requirements.", "sections": [{"title": "Introduction", "content": "Automated planning generates plans aimed at achieving specific goals in a given environment. However, in real-world environments some information is hard to access and to specify directly in a model, e.g., in a transportation logistics planning domain Helmert [2014], to find route-cost"}, {"title": "Background", "content": "We use the STRIPS formalism Fikes and Nilsson [1971] and define a planning problem as a tuple (P, A, so, g, c), where P is a set of propositions, A is a set of actions, so \u2286 P is the initial state, gCP the goal definition, and c : A \u2192 Ro is the cost function mapping each action to its (positive) costs. We identify a state s C P with the set of propositions that hold in it; propositions that are not included in s are assumed to be false. A state s is a goal state if and only if s g. The functions"}, {"title": "From Planning to Learning", "content": "Predict-then-Optimize problem. In domains like travelling or delivery services, the costs of actions are hard to specify at design time, because they depend on the current situation, e.g., regarding weather or traffic. However, one can estimate these costs using contextual features that are correlated with the costs. In this case, predicting the costs using ML methods is a natural choice. When the ground truth action costs are unknown, we employ a trained ML model M to predict the action costs from features X. The trainable parameters, denoted as w, are estimated using a set of past observations, used as a training dataset for the ML model. To obtain a feasible plan in this setting, the action costs are first predicted using ML, followed by the generation of a plan optimized with respect to the predicted costs. This pipeline is commonly referred to as the Predict-then-Optimize problem formulation in the literature [Elmachtoub and Grigas, 2022, Mandi et al., 2020]. We present a schematic diagram illustrating Predict-then-Optimize in the context of planning problems in Figure 1.\nVector representation. State-of-the-art ML architectures, including neural networks, represent the data in matrix and vector form. As we will be using neural networks as the predictive model, we will introduce a vector based notation of the action costs and the solution. Consider the left side of Figure 2. It shows the illustration of a simple planning problem and an optimal solution as defined before.\nWe create a vector representation of a plan by storing the number of times each action occurs in this plan. Since this discards the orderings of the actions in the plan, more than one plan might map to the same vector. We refer to this as the action count vector by \u03c0. More formally, let m = |A| be the number of possible actions a\u017c in the model, and A = (a0, a1, ..., am\u22121) a sequence containing the actions of A in an arbitrary but fixed ordering. Given some plan p = (po,..., pn), we define the action count vector \u03c0 = (00,..., 0m\u22121) with oi = \u22117=01(ai = pj).\nWe need a similar vector representation for action costs, which we denote by C. We define C = (c(ao), c(a1), ..., c(am\u22121)), where a\u017c is the ith element of A. Hereafter, we will use \u03c0*(C) to denote the action count vector of an optimal plan with respect to C. Observe that both vectors C and \u03c0*(C) have the same length. An example of C, \u03c0*(C) and A is given on the right of Figure 2. With this notation, we can represent the training dataset as {(X, CK)}=1\nN\nRegret. In the predict-then-optimize problem, we need to distinguish between the ground truth action costs that we want to learn and the predicted action costs. We will denote them as C and \u0108, respectively. Let \u03c0*(C) and \u03c0*(C) be optimal action count vectors with respect to C and \u0108 respectively. Using vector notation, the cost of executing \u03c0*(C) can be expressed as CT \u03c0*(C). Importantly, when a plan that was created using the predicted costs is actually executed in practice, the actual costs, C, is revealed, and the efficacy of the plan is evaluated with respect to C. Hence the real cost of executing \u03c0*(\u0108) is C\u00af\u03c0*(\u0108), e.g. the real cost times the action count vector. The quality of a predicted cost in a predict-then-optimize problem is evaluated based on regret. Regret measures the difference between the realized cost of the solution, made using the predicted cost and the true optimal cost, which is obviously not known a priori. It can be expressed in the following form:\nregret(\u0108, C) = C*\u03c0*(\u0108) \u2013 C\u00af \u03c0* (C)  \\ (1)"}, {"title": "Decision-Focused Learning", "content": "In a predict-then-optimize setup, the final goal of predicting the cost is to make a planning solution with zero or low regret. The motivation of DFL is to directly train an ML model to predict \u0108 in a manner that minimizes regret. We are particularly interested in gradient descent training, a widely utilized method for training neural networks. In gradient descent training, the neural network is trained by computing the gradient of the loss function. Modern neural network frameworks like TensorFlow Abadi et al. [2016] and PyTorch Paszke et al. [2017] compute this gradient automatically by representing the set of all neural network layers as a computational graph Baydin et al. [2018]. However, in DFL, as the final loss is the regret; this would require computing the derivative the regret and hence of plan \u03c0*(C) with respect to \u0108. Firstly, one cannot rely on automatic differentiation to compute this derivative as the planning problem is solved outside the neural network computational graph. Moreover, the planning process is not a differentiable operation, since slight changes in action costs either do not affect the solution or change the solution abruptly, just like in combinatorial optimisation Mandi et al. [2023]. So, the derivative of the planning solution is either zero or undefined.\nTo obtain gradients useful for DFL for different classes of optimization problems, numerous tech- niques have been proposed. For an extensive analysis of existing DFL techniques, we refer readers to the survey by Mandi et al. [2023]. In this work, we focus on the seminal and robust 'Smart Predict then Optimize' (SPO) technique Elmachtoub and Grigas [2022], which has demonstrated success in implementing DFL across various applications, including e.g. power systems Chen et al. [2021] or antenna design Chai et al. [2022].\nSmart Predict-then-Optimize (SPO). SPO is a DFL approach which proposes a convex upper- bound of the regret. This upper-bound can be expressed in the following form, as shown below:\nSPO+ = $(C \u2013 2\u0108) + 2\u0108\u00af \u03c0*(C) \u2013 CT \u03c0*(C) \\(2)\nwhere $(C) = max\u201e{C\u315c\u3160}. However, to minimize the SPO+ loss in gradient-based training, a difficulty arises because it does not have a gradient. It is easy to verify that \u2013\u03c0*(C) is a subgradient of ((-C), allowing to write the following subgradient of SPO+ loss\n\u2207 SPO\u2081 = 2(\u03c0*(C) \u2013 \u03c0*(2\u0108 \u2013 C)) \\(3)\nThis subgradient is used for gradient-based training in DFL."}, {"title": "DFL in the Context of Planning", "content": "A classical planning problem is essentially a compact definition of a huge graph akin to a finite automaton. The objective is to find a path in this graph from a given initial state to a goal state without"}, {"title": "Regret Evaluation in the Presence of Negative Action Costs", "content": "We highlight that while the ground truth action costs are positive, the predicted cost, returned by the ML model, might turn negative. one could use a Relu activation layer to enforce the predicted costs \u0108 to be non-negative. This can be formulated by using an element-wise max operator.\nrelu(\u0108) = max(\u0108, 0)  \\ (4)\nWe can naively use relu, by feeding the planning system with action costs after setting the negative ones to zero. We refer to it as thresholding. However, this approach may yield a subpar plan by turning all negative predictions into zeros, losing the relative ordering of negative action costs.\nNext, we propose an improved method for transforming all action costs into positive values before feeding them to the planning system. Our idea is to add a scalar value to each element of the cost vector, if any element in it is negative. We implement this by adding the absolute value of the smallest action cost to the cost vector. For a given \u0108, it can be computed as follows:\nc = min(0, min(\u0108))|  \\(5)\nwhere min(C) is the minimum value in the cost vector \u0108. Eq. (5) ensures that if all the elements in \u0108 are positive, the value of c is 0. The action count vector obtained after this transformation, can be expressed in the following form:\n\u03c0* min+(C)= \u03c0*(\u0108 + c)  \\ (6)\nwhere c is defined in Eq. (5). We refer to this approach as add-min. We will evaluate which among these two approaches would be suitable for evaluating regret in the presence of negative action costs."}, {"title": "Training in the Presence of Negative Action Costs", "content": "The second challenge is associated with training an ML model in the DFL paradigm. As mentioned before, DFL involves computing the planning solution with the predicted action costs during the training of the ML model. As the action costs might turn negative, it also requires finding a planning solution over negative action costs. We emphasise that while training with the SPO method, (2\u0108 \u2013 C) can turn negative, even if we ensure that both \u0108 and C are positive.\nDuring evaluation, as we mentioned earlier, our aim is to create a planning solution with negative action costs. However, this objective differs during training. In the DFL paradigm, the primary focus of solving the planning problem during training is to produce a useful gradient for training. This concept is reflected in Equation 3; where the gradient does not include a planning solution for the predicted action cost \u0108; rather, it considers a solution for (2\u0108 \u2013 C), as it yields a suitable gradient for training.\nIt is obvious that the thresholding or add-min approach introduced for evaluation of regret can also be used while training. Computing the SPO subgradient using thresholding would result in the following:\nVrelu\nSPO+\n= 2(\u03c0*(C) \u2013 \u03c0elu (2\u0108 \u2013 C))  \\(7)\nOn the other hand, computing the SPO subgradient through the add-min approach would yield the following:\nmin+\nSPO+\n= 2(\u03c0*(C) 2(\u03c0*(C) \u2013 \u03c0min+ (2\u0108 \u2013 C))  \\(8)\nNote that we do not have to change \u03c0*(C) as it does not have any negative element in it."}, {"title": "Explicit Training Penalty", "content": "We highlight that when training the ML model using Eq. (7) or Eq. (8), the conversion of negative action costs to non-negative ones occurs outside the gradient computational graph. Consequently, the ML model does not receive feedback indicating the necessity of such corrective measures before computing the regret. This limitation motivates us to explore alternative gradient computation techniques that not only make the ML model aware of the need for such corrective actions but also has no impact when there are no negative predictions.\nWe propose to add a penalty function in the loss function if any element of the vector (2\u0108 \u2013 C) is negative.\nSPO+P\n= SPO+ + X 1T relu(C \u2013 2\u0108)  \\(9)\nwhere, A signifies the weight assigned to the penalty function, 1 denotes a vector of ones with the same dimension as C. So, 1T relu(C \u2013 2\u0108) is the sum of all non-zero elements in C \u2013 2\u0108. In this formulation, 1T relu(C \u2013 2\u0108) will be zero only if 2\u0109(ai) < c(ai) for all actions a\u00bf. In Eq. (9), the second term can be viewed as a regularizer that penalizes predicting 2\u0108 < C, as for such predictions we have to make the transformation of the cost vector before feeding it to the planning system. To train with the SPO O+p loss, we can use the subgradient \u2207 (7) or mp (8) for the SPO+\npart; we will denote the respective loss functions as SPO+relu and SPO-"}, {"title": "From Loss to Gradient Computation", "content": "The subgradient of SPO+p in Eq. (9) can be expressed in the following form:\nSPO+P\n= \u2207 SPO+ \u2212 2 X I<0(2\u0108 \u2013 C)  \\ (10)\nWe use an indicator function I<0, which outputs a vector with elements equal to 1 for actions ai if 2\u0109(ai) < c(ai). For instance, if we use min Pas the SPO+ subgradient, \u2207PO+P takes the following form:\nmin+\nSPO+P\n= 2(\u03c0* (C) \u2013 \u03c0min+ (2\u0108 \u2013 C)) \u2013 2 x I<0(2\u0108 \u2013 C)\n=\n=\n2 (\u03c0*(C) \u2013 (min + (2C \u2013 C) +\n= 2\n((\u03c0*(C) \u2013 \u5143*(2\u0108 \u2013 C)))\n1 I<0 (2\u0108 \u2013 C))\n-  \\\\(11)\nwhere \u5143*(2\u0108 \u2013 C) is defined as follows:\n*(2C \u2013 C) = min+(2C \u2013 C) + 1 I<0(2\u0108 \u2013 C) \\(12)\nUsing the SPO methodology, we can use Eq. (11) as a subgradient to minimize SPO+min+.\nThe vector \u5143*(2\u0108 \u2013 C) increments the count of any action a\u017c where 2\u0109(ai) < c(ai) by 1 after obtaining a solution with add-min. In other words, these actions are executed once more. In this way it penalizes for the need to correct the predicted cost by selecting any action a\u017c for which 2\u0109(ai) < c(ai). We highlight that there might be no solution to the original planning problem corresponding to the vector representation \u5143*(\u0108). Since we added actions apart from the solution returned by the planning system, there might not even be an executable permutation of the actions represented in the vector.\nIntuitive interpretation of the subgradient. The motivation behind introducing the subgradient formulation (11) is that we can associate an intuitive interpretation to it. The intuition behind the subgradient (11) is that the action count vector \u5143*(2\u0108 \u2013 C) increases the count of action a\u017c if 2\u0109(ai) < c(ai); which makes the corresponding elements in the subgradient vector 2(\u03c0*(C) *(2\u0108 \u2013 C)) negative. As the ML model is updated using the opposite of the subgradient, the corresponding action costs are increased in the next iteration. So, in this way we incentivize the model to avoid predicting 2\u0109(ai) < c(ai)."}, {"title": "Scaling up DFL for Planning Problems", "content": "As reported by Mandi et al. [2023], DFL comes with substantial computational costs. This is due to the fact that DFL requires solving the planning problem with the predicted (action) costs while training the underlying ML model. This means that we need to solve a planning problem repeatedly, which is computationally expensive. This computational burden poses a significant challenge in applying DFL to real-world planning problems, often resulting in long training times. In this section, we present some strategies to tackle this crucial issue."}, {"title": "Use of Planning Techniques to Expedite Training", "content": "As DFL involves repeatedly solving the planning problem during training, one strategy to expedite training is to use planning techniques without optimality guarantees or even solutions to relaxed planning problems (as usually done when computing planning heuristics). The advantage of this is that it is easier and faster to solve. Although such solutions may not be identical to the optimal ones, they can still provide a useful gradient (11). Note that the gradient computation in DFL is computed across a batch of training instances. In such cases, the exact optimal solution with the predicted action costs might not be necessary to determine the direction of the gradient update. A non-optimal solution, reasonably close to the true solution, often provides a good gradient direction and suffices for gradient computation.\nFor integer linear problems (ILPs), Mandi et al. [2020] observed that solving their linear relaxation is sufficient for obtaining informative DFL gradients. In planning, we have several options towards approximating the optimal solution: we can either use planning algorithms that are bounded optimal, those without optimally guarantee, or even use solutions to relaxed planning problems. This leads us to the following settings, where both plan quality and computational effort decrease:\n\u2022 opt - Use an optimal planning system to get an optimal solution.\n\u2022 boundn - Use an algorithm that guarantees a solution not worse than n times the optimal plan.\n\u2022 no-bound \u2013 Use a planning system without optimality guarantees.\n\u2022 h - Return a solution to a relaxation of the planning problem as usually done to compute heuristics in planning.\nIn our experiments, we use an A* search and the admissible LM-Cut heuristic Helmert and Domshlak [2009] for optimal planning (opt). For bounded optimal planning (boundn), we combined LM-Cut with a weighted A* search. In the latter setting, the heuristic value is multiplied with a factor, which practically leads to finding solutions more quickly, but comes at the cost of losing the guarantee of finding an optimal solution. However, solutions are guaranteed to be bounded optimal.\nFor planning without optimality guarantees (no-bound), using a non-admissible heuristic is usually the better option to find plans more quickly. In our experiments, we combine a Greedy Best First Search (GBFS) with the hFF heuristic Hoffmann and Nebel [2001]. hFF internally finds a solution to the so-called delete-relaxed (DR) planning problem, which ignores the delete effects in the original planning problem. This simplifies the problem and makes it possible to find a solution in polynomial time (while finding the optimal solution is still NP-hard). The heuristic estimate is then the costs of the solution to the DR problem.\nFor the last option (h), we need to choose a heuristic that computes not only a heuristic value, but also a relaxed plan, because we need one to compute the gradient as discussed above. Since hFF internally computes a DR solution, it is well suited for our setting and we can use the DR solution as well as the heuristic estimate computed by the hFF heuristic for our learning process."}, {"title": "Use of Solution Caching to Expedite Training", "content": "As shown by Mulamba et al. [2021], an alternative approach to tackle the scalability of DFL is to replace solving an optimization problem with a cache lookup strategy, where the cache is a set of feasible solutions and acts as an inner approximation of the convex-hull of feasible solutions. How this cache is formed is crucial to the success of this approach."}, {"title": "Experimental Evaluation", "content": "In this section, we first describe our benchmark set and the system setup. We come to the results afterwards. The code and data have been made publicly available [Mandi et al.].\nExperimental Setup"}, {"title": "Benchmark Set", "content": "For our experiments we need domains with meaningful action costs that have impact on solution quality (otherwise we will not be able to measure the impact of our methods). Further, to have a wide range of solving techniques available we want to stay in the standard classical planning (i.e., non-temporal) setting. We use a problem generator to generate problems of different sizes. In the Rovers domain, meeting these requirements required some adjustments. Next, we detail the domains, their source, and (if necessary) modifications we made.\nShortest path. This domain models an\u00d7n grid environment an agent needs to navigate through. Each node is connected to its top and right nodes. The objective is to find a path starting from the bottom left cell to the top right cell with minimal costs. This domain is particularly interesting for our experiments, because it is a widely used benchmark in DFL [Elmachtoub and Grigas, 2022, Mandi et al., 2023, Tang and Khalil, 2022]. In these works, the problem is solved using an LP solver. We include this to have a direct comparison to existing DFL methods.\nTransport. In this domain we use the standard domain and generator Seipp et al. [2022] from the generator repository\u00b9. Each transport problem instance revolves around the task of delivering p number of packages using t number of trucks. We consider a n\u00d7n grid for the transport problem, within which both pickup and delivery operations occur. We denote each transport problem instance"}, {"title": "Generation of Training Data", "content": "While we adopt the planning problems from planning benchmark domains, we synthetically generate the action costs. Such synthetic data generation processes are common in DFL literature. We follow the synthetic data generation process exactly as done by Elmachtoub and Grigas [2022]. We generate a set of pairs of features and action costs {(Xk, CK)}=1 for training and evaluation. The dimension of Ck is equal to the number of actions, |A|, which is specific to the planning problem. The dimension of X is 5. and each X is sampled from a multivariate Gaussian distribution with zero mean and unit variance, i.e., X\u3047 ~ N(0, I5) (I5 is a 5 \u00d7 5 identity matrix). To set up a mapping from Xk to Cr, first, a matrix B \u2208 R|A|\u00d75 is constructed, and then C\u0118 is generated according to the following formula:\nc*(ai) = ((BX)+3)+1 \\ (13)\nwhere ck(ai) is the the cost of action i in instance k, the parameter Deg parameter signifies the extent of model misspecification, and & is a multiplicative noise term sampled randomly from the uniform distribution. Note that the action costs generated in this manner are always positive if Deg is a even number. Furthermore, since the action costs are random numbers sampled from a continuous distribution, it is highly improbable that two feasible plans will have exactly identical execution costs. Therefore, in this scenario, we do not encounter the phenomenon of multiple non-unique solutions.\nElmachtoub and Grigas [2022] use a linear model to predict the cost vector from features. The higher the value of Deg, the more the true relation between the features and action costs deviates from the linear model and the larger the errors of the linear predictive model. Such model misspecification is a common phenomenon in ML, because the in practise the data generation process is not observable. In our experiments, we will report result with Deg being 4."}, {"title": "Planning and Learning Setup", "content": "Similarly to Elmachtoub and Grigas [Elmachtoub and Grigas, 2022], we will also use a linear model to predict the action costs from features. We use PyTorch [Paszke et al., 2017] to implement the linear predictive model and train it by minibatch stochastic gradient descent [Goyal et al., 2017, Robbins"}, {"title": "Results", "content": "RQ1: Does training with a relu activation layer impact regret? One can enforce the predictions to be non-negative by adding relu as a final activation layer. However, when the predictive model does not fully represent the data generation process, imposing non-negativity constraint using relu may distort the predictions resulting in higher prediction as well as decision errors. To investigate whether using relu affects the regret, we consider the Shortest path problem, which is a widely used benchmark in DFL."}, {"title": "How to evaluate regret given that planning system does not allow negative costs?", "content": "RQ2: How to evaluate regret given that planning system does not allow negative costs? As we will not be using relu activation layer in the final layer, the predictions generated by the ML model can turn negative, even though the groundtruth action costs are positive. As action costs with negative values, are not supported by a planner; we will be using thresholding (??) or add-min (6) to solve the planning problem with negative predicted action costs. We again consider the shortest path problem. This time we again use the LP solver for training. However, during evaluation, we compute regret using both the LP solver and a planner, allowing to compare the true regret with the regret obtained by a planner. We want to find out which method, thresholding or add-min, gives a regret measure closest to the LP regret. We see in Table 3 that add-min regret demonstrates greater fidelity to true LP regret. Note that thresholding regret shows significant deviations, particularly evident SPO+ . Hence in our latter experiments, we will use add-min regret to evaluate the regret of predicted action costs. With the evaluation protocol set, we now focus on DFL learning methods."}, {"title": "Training With and Without Explicit Penalty", "content": "RQ3: How do the proposed SPO subgradients perform? Now that we had a direct comparison with related work on the Shortest path domain, we evaluate our methods on the Transport and Rovers domain known from the planning literature. So, in this case, we use the FD planner for DFL training as well as evaluation. We seek to answer whether adding the explicit training penalty results in lower regret. As DFL training requires repeatedly solving a planning problem for every training instance, we restrict ourselves to planning problems that are fast to solve. We consider small-size planning problems, which can be solved quite fast (within 0.25 seconds).\nIn our experiments, we set \u5165 (9) to be 1. We report the result in Table A3. SPO+relu loss performs very poorly, as its regret is much higher than MSE. This is due to the fact that turning negative costs to zero without considering their values causes loss of information. SPO+min+ performs much better. However, even in some cases its regret is higher than MSE. On the other hand, SPO+relu and SPO+min, which add explicit training penalty if 2\u0109(ai) < c(a\u2081) for a action ai, are able to improve SPO+relu and SPO+min+. It is interesting to note that the difference between relu and min+ is insignificant after adding explicit training penalty. This experiment suggests both SPO+relu and SPO+min+ are effective DFL approaches for predicting action costs in planning problems."}, {"title": "Optimal Planning Versus Non-Optimal Planning", "content": "RQ4: Can we use non-optimal planning for DFL training? As DFL requires solving the planning problem repeatedly while training, which creates a considerable computational burden when"}, {"title": "Optimal Planning versus Solution Caching", "content": "RQ5: Can we use solution caching to speed up training? Next we investigate whether solution caching, as implemented by Mulamba et al. [2021] in the context of DFL for optimization, is effective for planning problems too. We initialize the cache with all the solutions present in the training data. We experiment with p = 10% and 20%. We can see in Table 5, the training time of caching faster compared to opt due to they solve the planning is solved for only p% of instances using the predicted action costs. While p = 10% does not consistently outperform MSE regret, p = 20% produces regret lower than MSE for all instances. This indicates for large planning instances, use of solution caching with p = 20% could prove to be a useful approach."}, {"title": "Conclusion", "content": "In this work, we investigated for the first time how we can use techniques from DFL in the planning domain. More specifically for the case of predicting action costs from correlated features and historic data, we showed how the SPO technique, which places no assumptions on the solver used, can also be used for planning problems. Other DFL techniques which work with a black-box solver Pogancic et al. [2020], Niepert et al. [2021] are now equally applicable.\nWe proposed an implementation of DFL which accounts for the fact that planners do not support negative action costs. Our findings suggest that imposing non-negativity through relu leads to an increase in regret, both for model trained with MSE and DFL loss. Moreover, training with an explicit penalty for correcting negative action costs before solving the planning problem yields significant improvements. Indeed, our DFL approach always leads to lower regret than when training to minimize MSE of predicted action costs. While using sub-optimal plans did not consistently lead to lower-than-MSE regret, a moderate amount of caching was able to reduce computation cost significantly.\nFuture work includes reducing computational costs further, as well as DFL for state-dependent action cost prediction or other action components; for which SPO and related techniques is insufficient."}, {"title": "Appendices", "content": ""}, {"title": "Details of Transport Problem Instances", "content": ""}, {"title": "Details of the Rovers Problem Instances", "content": ""}, {"title": "Explanation of Negative Predicted Action Costs", "content": "Negative predictions of action costs may occur when the predictive model is misspecified. For instance, if the actual relationship between C and X is C = 2X2 \u2013 4X + 3 and we fit a linear model like \u0108 = \u03b1x + \u03b2, the predicted model might have a = 10 and \u03b2 = -15. Consequently, for X < 1.5, the predicted \u0108 is negative. It might also happen due to very high value outliers. An example of negative predictions due to high positive outlier values: Suppose the true model is C = 2X + 2 but C values corresponding to high & are affected by high noise. This might turn the predicted slope to be steeper, e.g., \u0108 = 4X \u2013 1. Consequently, for X < 0.25, \u0108 < 0.\nIn Figure Ala, we empirically demonstrate why there might be negative predicted action cost even if all the true action costs are positive. For this demonstration, the data is generated synthetically following the procedure of the SPO paper Elmachtoub and Grigas [2022]. The value of the Deg parameter is 4, so the true relationship between C and X is non-linear. However, a linear model is fit on the dataset by minimizing mean square error loss. As shown in the numerical example, the predicted \u0108 becomes negative when C is low. Figure Alb presents a scatterplot comparing predicted action costs to true action costs when the predictive model uses ReLU activation as the final layer. The prediction quality is poor, as all negative \u0108 values are indiscriminately set to zero, ignoring the magnitude of the negative values in absolute terms."}, {"title": "Solving Time", "content": ""}]}