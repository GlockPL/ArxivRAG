{"title": "Decision-Focused Learning to Predict Action Costs for\nPlanning", "authors": ["Jayanta Mandi", "Marco Foschini", "Daniel H\u00f6ller", "J\u00f6rg Hoffmann", "Sylvie Thiebaux", "Tias Guns"], "abstract": "In many automated planning applications, action costs can be hard to specify.\nAn example is the time needed to travel through a certain road segment, which\ndepends on many factors, such as the current weather conditions. A natural way\nto address this issue is to learn to predict these parameters based on input features\n(e.g., weather forecasts) and use the predicted action costs in automated planning\nafterward. Decision-Focused Learning (DFL) has been successful in learning\nto predict the parameters of combinatorial optimization problems in a way that\noptimizes solution quality rather than prediction quality. This approach yields better\nresults than treating prediction and optimization as separate tasks. In this paper, we\ninvestigate for the first time the challenges of implementing DFL for automated\nplanning in order to learn to predict the action costs. There are two main challenges\nto overcome: (1) planning systems are called during gradient descent learning,\nto solve planning problems with negative action costs, which are not supported\nin planning. We propose novel methods for gradient computation to avoid this\nissue. (2) DFL requires repeated planner calls during training, which can limit the\nscalability of the method. We experiment with different methods approximating\nthe optimal plan as well as an easy-to-implement caching mechanism to speed\nup the learning process. As the first work that addresses DFL for automated\nplanning, we demonstrate that the proposed gradient computation consistently\nyields significantly better plans than predictions aimed at minimizing prediction\nerror; and that caching can temper the computation requirements.", "sections": [{"title": "Introduction", "content": "Automated planning generates plans aimed at achieving specific goals in a given environment.\nHowever, in real-world environments some information is hard to access and to specify directly\nin a model, e.g., in a transportation logistics planning domain Helmert [2014], to find route-cost\nPreprint. Under review."}, {"title": "Background", "content": "We use the STRIPS formalism Fikes and Nilsson [1971] and define a planning problem as a tuple\n$(P, A, s_0, g, c)$, where P is a set of propositions, A is a set of actions, $s_0 \\subseteq P$ is the initial state,\n$g \\subseteq P$ the goal definition, and $c : A \\rightarrow \\mathbb{R}_0^+$ is the cost function mapping each action to its (positive)\ncosts. We identify a state $s \\subseteq P$ with the set of propositions that hold in it; propositions that are not\nincluded in s are assumed to be false. A state s is a goal state if and only if $s \\subseteq g$. The functions"}, {"title": "From Planning to Learning", "content": "Predict-then-Optimize problem. In domains like travelling or delivery services, the costs of\nactions are hard to specify at design time, because they depend on the current situation, e.g., regarding\nweather or traffic. However, one can estimate these costs using contextual features that are correlated\nwith the costs. In this case, predicting the costs using ML methods is a natural choice. When the\nground truth action costs are unknown, we employ a trained ML model M to predict the action\ncosts from features X. The trainable parameters, denoted as w, are estimated using a set of past\nobservations, used as a training dataset for the ML model. To obtain a feasible plan in this setting,\nthe action costs are first predicted using ML, followed by the generation of a plan optimized with\nrespect to the predicted costs. This pipeline is commonly referred to as the Predict-then-Optimize\nproblem formulation in the literature [Elmachtoub and Grigas, 2022, Mandi et al., 2020]. We present a\nschematic diagram illustrating Predict-then-Optimize in the context of planning problems in Figure 1.\nVector representation. State-of-the-art ML architectures, including neural networks, represent the\ndata in matrix and vector form. As we will be using neural networks as the predictive model, we\nwill introduce a vector based notation of the action costs and the solution. Consider the left side of\nFigure 2. It shows the illustration of a simple planning problem and an optimal solution as defined\nbefore.\nWe create a vector representation of a plan by storing the number of times each action occurs in this\nplan. Since this discards the orderings of the actions in the plan, more than one plan might map to\nthe same vector. We refer to this as the action count vector by \u03c0. More formally, let m = |A| be the\nnumber of possible actions a\u017c in the model, and A = (a0, a1, ..., am\u22121) a sequence containing the\nactions of A in an arbitrary but fixed ordering. Given some plan p = (po,..., pn), we define the\naction count vector \u03c0 = (00,..., 0m\u22121) with oi = $\\sum_{j=0}^{n}1(a_i = p_j)$.\nWe need a similar vector representation for action costs, which we denote by C. We define C =\n(c(ao), c(a1), ..., c(am\u22121)), where a\u017c is the ith element of A. Hereafter, we will use \u03c0*(C) to\ndenote the action count vector of an optimal plan with respect to C. Observe that both vectors C\nand \u03c0*(C) have the same length. An example of C, \u03c0*(C) and A is given on the right of Figure 2.\nWith this notation, we can represent the training dataset as $\\{(X, C_k)\\}_{k=1}^{N}$\nRegret. In the predict-then-optimize problem, we need to distinguish between the ground truth\naction costs that we want to learn and the predicted action costs. We will denote them as C and\n$\\hat{C}$, respectively. Let \u03c0*(C) and \u03c0*($\\hat{C}$) be optimal action count vectors with respect to C and $\\hat{C}$\nrespectively. Using vector notation, the cost of executing \u03c0*($\\hat{C}$) can be expressed as $C^T\u03c0*(\\hat{C})$.\nImportantly, when a plan that was created using the predicted costs is actually executed in practice,\nthe actual costs, C, is revealed, and the efficacy of the plan is evaluated with respect to C. Hence\nthe real cost of executing \u03c0*($\\hat{C}$) is $C^T\u03c0*(\\hat{C})$, e.g. the real cost times the action count vector. The\nquality of a predicted cost in a predict-then-optimize problem is evaluated based on regret. Regret\nmeasures the difference between the realized cost of the solution, made using the predicted cost and\nthe true optimal cost, which is obviously not known a priori. It can be expressed in the following\nform:\n$regret(\\hat{C}, C) = C^T\u03c0*(\\hat{C}) \u2013 C^T \u03c0* (C)$\n(1)"}, {"title": "Decision-Focused Learning", "content": "In a predict-then-optimize setup, the final goal of predicting the cost is to make a planning solution\nwith zero or low regret. The motivation of DFL is to directly train an ML model to predict $\\hat{C}$ in a\nmanner that minimizes regret. We are particularly interested in gradient descent training, a widely\nutilized method for training neural networks. In gradient descent training, the neural network is\ntrained by computing the gradient of the loss function. Modern neural network frameworks like\nTensorFlow Abadi et al. [2016] and PyTorch Paszke et al. [2017] compute this gradient automatically\nby representing the set of all neural network layers as a computational graph Baydin et al. [2018].\nHowever, in DFL, as the final loss is the regret; this would require computing the derivative the regret\nand hence of plan \u03c0*($\\hat{C}$) with respect to $\\hat{C}$. Firstly, one cannot rely on automatic differentiation to\ncompute this derivative as the planning problem is solved outside the neural network computational\ngraph. Moreover, the planning process is not a differentiable operation, since slight changes in\naction costs either do not affect the solution or change the solution abruptly, just like in combinatorial\noptimisation Mandi et al. [2023]. So, the derivative of the planning solution is either zero or undefined.\nTo obtain gradients useful for DFL for different classes of optimization problems, numerous tech-\nniques have been proposed. For an extensive analysis of existing DFL techniques, we refer readers to\nthe survey by Mandi et al. [2023]. In this work, we focus on the seminal and robust 'Smart Predict\nthen Optimize' (SPO) technique Elmachtoub and Grigas [2022], which has demonstrated success in\nimplementing DFL across various applications, including e.g. power systems Chen et al. [2021] or\nantenna design Chai et al. [2022].\nSmart Predict-then-Optimize (SPO). SPO is a DFL approach which proposes a convex upper-\nbound of the regret. This upper-bound can be expressed in the following form, as shown below:\n$SPO^+ = \\phi(C \u2013 2\\hat{C}) + C^T\u03c0*(\\hat{C}) \u2013 C^T \u03c0*(C)$\n(2)\nwhere $\\phi$(C) = $max_\u03c0 \\{C^T\u03c0\\}$. However, to minimize the $SPO^+$ loss in gradient-based training, a\ndifficulty arises because it does not have a gradient. It is easy to verify that \u2013\u03c0*(C) is a subgradient\nof ((-C), allowing to write the following subgradient of $SPO^+$ loss\n$\\nabla SPO_+ = 2(\u03c0*(C) \u2013 \u03c0*(2\\hat{C} \u2013 C))$\n(3)\nThis subgradient is used for gradient-based training in DFL."}, {"title": "DFL in the Context of Planning", "content": "A classical planning problem is essentially a compact definition of a huge graph akin to a finite\nautomaton. The objective is to find a path in this graph from a given initial state to a goal state without"}, {"title": "Regret Evaluation in the Presence of Negative Action Costs", "content": "We highlight that while the ground truth action costs are positive, the predicted cost, returned by the\nML model, might turn negative. one could use a Relu activation layer to enforce the predicted costs\n$\\hat{C}$ to be non-negative. This can be formulated by using an element-wise max operator.\n$relu(\\hat{C}) = max(\\hat{C}, 0)$\n(4)\nWe can naively use relu, by feeding the planning system with action costs after setting the negative\nones to zero. We refer to it as thresholding. However, this approach may yield a subpar plan by\nturning all negative predictions into zeros, losing the relative ordering of negative action costs.\nNext, we propose an improved method for transforming all action costs into positive values before\nfeeding them to the planning system. Our idea is to add a scalar value to each element of the cost\nvector, if any element in it is negative. We implement this by adding the absolute value of the smallest\naction cost to the cost vector. For a given $\\hat{C}$, it can be computed as follows:\n$c = |min(0, min(\\hat{C}))|$\n(5)\nwhere min($\\hat{C}$) is the minimum value in the cost vector $\\hat{C}$. Eq. (5) ensures that if all the elements in\n$\\hat{C}$ are positive, the value of c is 0. The action count vector obtained after this transformation, can be\nexpressed in the following form:\n$\u03c0*_{min^+}(C)= \u03c0*(\\hat{C} + c)$\n(6)\nwhere c is defined in Eq. (5). We refer to this approach as add-min. We will evaluate which among\nthese two approaches would be suitable for evaluating regret in the presence of negative action costs."}, {"title": "Training in the Presence of Negative Action Costs", "content": "The second challenge is associated with training an ML model in the DFL paradigm. As mentioned\nbefore, DFL involves computing the planning solution with the predicted action costs during the\ntraining of the ML model. As the action costs might turn negative, it also requires finding a planning\nsolution over negative action costs. We emphasise that while training with the SPO method, $(2\\hat{C} \u2013 C)$\ncan turn negative, even if we ensure that both $\\hat{C}$ and C are positive.\nDuring evaluation, as we mentioned earlier, our aim is to create a planning solution with negative\naction costs. However, this objective differs during training. In the DFL paradigm, the primary focus\nof solving the planning problem during training is to produce a useful gradient for training. This\nconcept is reflected in Equation 3; where the gradient does not include a planning solution for the\npredicted action cost $\\hat{C}$; rather, it considers a solution for $(2\\hat{C} \u2013 C)$, as it yields a suitable gradient\nfor training.\nIt is obvious that the thresholding or add-min approach introduced for evaluation of regret can also\nbe used while training. Computing the SPO subgradient using thresholding would result in the\nfollowing:\n$\\nabla^{relu}_{SPO_+} = 2(\u03c0*(C) \u2013 \u03c0^{relu} (2\\hat{C} \u2013 C))$\n(7)\nOn the other hand, computing the SPO subgradient through the add-min approach would yield the\nfollowing:\n$\\nabla^{min+}_{SPO_+} = 2(\u03c0*(C) \u2013 \u03c0^{min+} (2\\hat{C} \u2013 C))$\n(8)\nNote that we do not have to change \u03c0*(C) as it does not have any negative element in it."}, {"title": "Explicit Training Penalty", "content": "We highlight that when training the ML model using Eq. (7) or Eq. (8), the conversion of negative\naction costs to non-negative ones occurs outside the gradient computational graph. Consequently,\nthe ML model does not receive feedback indicating the necessity of such corrective measures before\ncomputing the regret. This limitation motivates us to explore alternative gradient computation\ntechniques that not only make the ML model aware of the need for such corrective actions but also\nhas no impact when there are no negative predictions.\nWe propose to add a penalty function in the loss function if any element of the vector $(2\\hat{C} \u2013 C)$ is\nnegative.\n$SPO_{+P} = SPO_+ + \u03bb 1^T relu(C \u2013 2\\hat{C})$\n(9)\nwhere, \u03bb signifies the weight assigned to the penalty function, 1 denotes a vector of ones with the\nsame dimension as C. So, $1^T relu(C \u2013 2\\hat{C})$ is the sum of all non-zero elements in $C \u2013 2\\hat{C}$. In this\nformulation, $1^T relu(C \u2013 2\\hat{C})$ will be zero only if $2\\hat{c}(a_i) < c(a_i)$ for all actions $a_i$. In Eq. (9), the\nsecond term can be viewed as a regularizer that penalizes predicting $2\\hat{C} < C$, as for such predictions\nwe have to make the transformation of the cost vector before feeding it to the planning system. To\ntrain with the $SPO_{+P}$ loss, we can use the subgradient $\\nabla^{relu} _{SPO_{+P}} (7)$ or $\\nabla^{min+} _{SPO_{+P}}$ (8) for the $SPO_+$ part; we will denote the respective loss functions as $SPO_{+}^{relu}$ and $SPO_{+}^{min+}$ .\nFrom Loss to Gradient Computation\nThe subgradient of $SPO_{+P}$ in Eq. (9) can be expressed in the following form:\n$\\nabla SPO_{+P} = \\nabla SPO_+ \u2013 2 \u03bb I_{<0}(2\\hat{C} \u2013 C)$\n(10)\nWe use an indicator function $I_{<0}$, which outputs a vector with elements equal to 1 for actions ai\nif $2\\hat{c}(a_i) < c(a_i)$. For instance, if we use $\\nabla^{min+} _{SPO_{+}}$as the $SPO_+$ subgradient, $\\nabla^{min+}_{SPO_{+P}}$ takes the\nfollowing form:\n$\\nabla^{min+} _{SPO_{+P}} = 2(\u03c0^* (C) \u2013 \u03c0^{min+} (2\\hat{C} \u2013 C)) \u2013 2 \u03bb I_{<0}(2\\hat{C} \u2013 C)$\n$= 2(\u03c0^* (C) \u2013 (\u03c0^{min+} (2\\hat{C} \u2013 C) + 1 I_{<0}(2\\hat{C} \u2013 C))$\n$= 2(\u03c0^*(C) \u2013 \u03c0^* (2\\hat{C} \u2013 C))$\n(11)\nwhere $\u5143^*(2\\hat{C} \u2013 C)$ is defined as follows:\n$\u03c0^*(2\\hat{C} \u2013 C) = \u03c0^{min+}(2\\hat{C} \u2013 C) + \u03bb I_{<0}(2\\hat{C} \u2013 C)$\n(12)\nUsing the SPO methodology, we can use Eq. (11) as a subgradient to minimize $SPO_{+}^{min+}$.\nThe vector $\u03c0^*(2\\hat{C} \u2013 C)$ increments the count of any action a\u017e where $2\\hat{c}(a_i) < c(a_i)$ by 1 after\nobtaining a solution with add-min. In other words, these actions are executed once more. In this\nway it penalizes for the need to correct the predicted cost by selecting any action a\u017c for which\n$2\\hat{c}(a_i) < c(a_i)$. We highlight that there might be no solution to the original planning problem\ncorresponding to the vector representation \u5143*($\\hat{C}$). Since we added actions apart from the solution\nreturned by the planning system, there might not even be an executable permutation of the actions\nrepresented in the vector.\nIntuitive interpretation of the subgradient. The motivation behind introducing the subgradient\nformulation (11) is that we can associate an intuitive interpretation to it. The intuition behind the\nsubgradient (11) is that the action count vector $\u03c0^*(2\\hat{C} \u2013 C)$ increases the count of action a\u017c if\n$2\\hat{c}(a_i) < c(a_i)$; which makes the corresponding elements in the subgradient vector $2(\u03c0^*(C) \u2013$\n$\u03c0^*(2\\hat{C} \u2013 C))$ negative. As the ML model is updated using the opposite of the subgradient, the\ncorresponding action costs are increased in the next iteration. So, in this way we incentivize the\nmodel to avoid predicting $2\\hat{c}(a_i) < c(a_i)$."}, {"title": "Scaling up DFL for Planning Problems", "content": "As reported by Mandi et al. [2023], DFL comes with substantial computational costs. This is due\nto the fact that DFL requires solving the planning problem with the predicted (action) costs while\ntraining the underlying ML model. This means that we need to solve a planning problem repeatedly,\nwhich is computationally expensive. This computational burden poses a significant challenge in\napplying DFL to real-world planning problems, often resulting in long training times. In this section,\nwe present some strategies to tackle this crucial issue."}, {"title": "Use of Planning Techniques to Expedite Training", "content": "As DFL involves repeatedly solving the planning problem during training, one strategy to expedite\ntraining is to use planning techniques without optimality guarantees or even solutions to relaxed\nplanning problems (as usually done when computing planning heuristics). The advantage of this is\nthat it is easier and faster to solve. Although such solutions may not be identical to the optimal ones,\nthey can still provide a useful gradient (11). Note that the gradient computation in DFL is computed\nacross a batch of training instances. In such cases, the exact optimal solution with the predicted\naction costs might not be necessary to determine the direction of the gradient update. A non-optimal\nsolution, reasonably close to the true solution, often provides a good gradient direction and suffices\nfor gradient computation.\nFor integer linear problems (ILPs), Mandi et al. [2020] observed that solving their linear relaxation\nis sufficient for obtaining informative DFL gradients. In planning, we have several options towards\napproximating the optimal solution: we can either use planning algorithms that are bounded optimal,\nthose without optimally guarantee, or even use solutions to relaxed planning problems. This leads us\nto the following settings, where both plan quality and computational effort decrease:\n\u2022 opt - Use an optimal planning system to get an optimal solution.\n\u2022 boundn - Use an algorithm that guarantees a solution not worse than n times the optimal plan.\n\u2022 no-bound \u2013 Use a planning system without optimality guarantees.\n\u2022 h - Return a solution to a relaxation of the planning problem as usually done to compute heuristics in planning.\nIn our experiments, we use an A* search and the admissible LM-Cut heuristic Helmert and Domshlak\n2009] for optimal planning (opt). For bounded optimal planning (boundn), we combined LM-Cut\nwith a weighted A* search. In the latter setting, the heuristic value is multiplied with a factor, which\npractically leads to finding solutions more quickly, but comes at the cost of losing the guarantee of\nfinding an optimal solution. However, solutions are guaranteed to be bounded optimal.\nFor planning without optimality guarantees (no-bound), using a non-admissible heuristic is usually\nthe better option to find plans more quickly. In our experiments, we combine a Greedy Best First\nSearch (GBFS) with the hFF heuristic Hoffmann and Nebel [2001]. hFF internally finds a solution\nto the so-called delete-relaxed (DR) planning problem, which ignores the delete effects in the original\nplanning problem. This simplifies the problem and makes it possible to find a solution in polynomial\ntime (while finding the optimal solution is still NP-hard). The heuristic estimate is then the costs of\nthe solution to the DR problem.\nFor the last option (h), we need to choose a heuristic that computes not only a heuristic value, but\nalso a relaxed plan, because we need one to compute the gradient as discussed above. Since hFF\ninternally computes a DR solution, it is well suited for our setting and we can use the DR solution as\nwell as the heuristic estimate computed by the hFF heuristic for our learning process."}, {"title": "Use of Solution Caching to Expedite Training", "content": "As shown by Mulamba et al. [2021], an alternative approach to tackle the scalability of DFL is to\nreplace solving an optimization problem with a cache lookup strategy, where the cache is a set of\nfeasible solutions and acts as an inner approximation of the convex-hull of feasible solutions. How\nthis cache is formed is crucial to the success of this approach."}, {"title": "Experimental Evaluation", "content": "In this section, we first describe our benchmark set and the system setup. We come to the results\nafterwards. The code and data have been made publicly available [Mandi et al.].\nExperimental Setup\nBenchmark Set\nFor our experiments we need domains with meaningful action costs that have impact on solution\nquality (otherwise we will not be able to measure the impact of our methods). Further, to have a\nwide range of solving techniques available we want to stay in the standard classical planning (i.e.,\nnon-temporal) setting. We use a problem generator to generate problems of different sizes. In the\nRovers domain, meeting these requirements required some adjustments. Next, we detail the domains,\ntheir source, and (if necessary) modifications we made.\nShortest path. This domain models an\u00d7n grid environment an agent needs to navigate through.\nEach node is connected to its top and right nodes. The objective is to find a path starting from the\nbottom left cell to the top right cell with minimal costs. This domain is particularly interesting for our\nexperiments, because it is a widely used benchmark in DFL [Elmachtoub and Grigas, 2022, Mandi\net al., 2023, Tang and Khalil, 2022]. In these works, the problem is solved using an LP solver. We\ninclude this to have a direct comparison to existing DFL methods.\nTransport. In this domain we use the standard domain and generator Seipp et al. [2022] from\nthe generator repository\u00b9. Each transport problem instance revolves around the task of delivering p\nnumber of packages using t number of trucks. We consider a n\u00d7n grid for the transport problem,\nwithin which both pickup and delivery operations occur. We denote each transport problem instance\nRovers. This domain describes the task of a fleet of Mars rovers, each equipped with a (probably\ndifferent) set of sensors. They need to navigate and gather data (e.g. rock samples or pictures). The\ndata then needs to be send to the lander. Our domain is based on the one from the 2006 International\nPlanning Competition. However, the domains from the different competition tracks did not directly\nfit our needs: MetricTime contains durative actions, Propositional and QualitativePreferences do not\ninclude action costs. We created a model based on the MetricSimplePreferences track and made the\npreferences normal goals. To get integer costs, we multiplied the included action costs by 10 and\nrounded them afterwards to integers.\nFor our domains, we generated two groups of problem instances: small-sized instances that can be\nsolved within 0.25 seconds, and large-sized instances that take 0.5-1 seconds to solve."}, {"title": "Generation of Training Data", "content": "While we adopt the planning problems from planning benchmark domains, we synthetically generate\nthe action costs. Such synthetic data generation processes are common in DFL literature. We follow\nthe synthetic data generation process exactly as done by Elmachtoub and Grigas [2022]. We generate\na set of pairs of features and action costs $\\{(X_k, C_k)\\}_{k=1}^{N}$ for training and evaluation. The dimension\nof $C_k$ is equal to the number of actions, |A|, which is specific to the planning problem. The dimension\nof $X_k$ is 5. and each $X_k$ is sampled from a multivariate Gaussian distribution with zero mean and\nunit variance, i.e., $X_\u3047 \u223c N(0, I_5)$ ($I_5$ is a 5 \u00d7 5 identity matrix). To set up a mapping from $X_k$ to\n$C_r$, first, a matrix $B \u2208 \\mathbb{R}^{|A|\u00d75}$ is constructed, and then $C_\u0118$ is generated according to the following\nformula:\n$c^K(a_i) = ((((BX_k)+3)_{+})^4 )^{\\frac{Deg}{4}} + \\epsilon^k_{+}$\n(13)\nwhere $c^k(a_i)$ is the the cost of action i in instance k, the parameter Deg parameter signifies the\nextent of model misspecification, and $&^k_+$ is a multiplicative noise term sampled randomly from the\nuniform distribution. Note that the action costs generated in this manner are always positive if Deg is\na even number. Furthermore, since the action costs are random numbers sampled from a continuous\ndistribution, it is highly improbable that two feasible plans will have exactly identical execution costs.\nTherefore, in this scenario, we do not encounter the phenomenon of multiple non-unique solutions.\nElmachtoub and Grigas [2022] use a linear model to predict the cost vector from features. The higher\nthe value of Deg, the more the true relation between the features and action costs deviates from the\nlinear model and the larger the errors of the linear predictive model. Such model misspecification is a\ncommon phenomenon in ML, because the in practise the data generation process is not observable.\nIn our experiments, we will report result with Deg being 4."}, {"title": "Planning and Learning Setup", "content": "Similarly to Elmachtoub and Grigas [Elmachtoub and Grigas, 2022], we will also use a linear model\nto predict the action costs from features. We use PyTorch [Paszke et al., 2017] to implement the linear\npredictive model and train it by minibatch stochastic gradient descent [Goyal et al., 2017, Robbins"}, {"title": "Results", "content": "In this section, we will present key insights from our empirical evaluation. After training the ML\nmodel, we report percentage regret on the test data, which is computed as follows:\n$\\frac{1}{N_{test}} \\sum_{k=1}^{N_{test}} \\frac{C_k^T \u03c0* (\\hat{C}_k) \u2013 C_k^T \u03c0*(C_k)}{C_k^T \u03c0*(C_k)}$\n(14)\nFor each set of experiments, we run 5 experiments each time with different seeds and report average\nand standard deviation of percentage regret in the tables. We confirmed that all the models converge\nwithin 20 epochs. We report results after the 20th epoch.\nEvaluating Regret for Planning Problems\nRQ1: Does training with a relu activation layer impact regret? One can enforce the predictions\nto be non-negative by adding relu as a final activation layer. However, when the predictive model\ndoes not fully represent the data generation process, imposing non-negativity constraint using relu\nmay distort the predictions resulting in higher prediction as well as decision errors. To investigate\nwhether using relu affects the regret, we consider the Shortest path problem, which is a widely used\nbenchmark in DFL."}, {"title": "How to evaluate regret given that planning system does not allow negative costs?", "content": "RQ2: How to evaluate regret given that planning system does not allow negative costs? As we\nwill not be using relu activation layer in the final layer, the predictions generated by the ML model\ncan turn negative, even though the groundtruth action costs are positive. As action costs with negative\nvalues, are not supported by a planner; we will be using thresholding (??) or add-min (6) to solve the\nplanning problem with negative predicted action costs. We again consider the shortest path problem.\nThis time we again use the LP solver for training. However, during evaluation, we compute regret\nusing both the LP solver and a planner, allowing to compare the true regret with the regret obtained\nby a planner. We want to find out which method, thresholding or add-min, gives a regret measure\nclosest to the LP regret. We see in Table 3 that add-min regret demonstrates greater fidelity to true LP\nregret. Note that thresholding regret shows significant deviations, particularly evident $SPO^+$. Hence\nin our latter experiments, we will use add-min regret to evaluate the regret of predicted action costs.\nWith the evaluation protocol set, we now focus on DFL learning methods."}, {"title": "Training With and Without Explicit Penalty", "content": "RQ3: How do the proposed SPO subgradients perform? Now that we had a direct comparison\nwith related work on the Shortest path domain, we evaluate our methods on the Transport and Rovers\ndomain known from the planning literature. So, in this case, we use the FD planner for DFL training\nas well as evaluation. We seek to answer whether adding the explicit training penalty results in lower\nregret. As DFL training requires repeatedly solving a planning problem for every training instance,\nwe restrict ourselves to planning problems that are fast to solve. We consider small-size planning\nproblems, which can be solved quite fast (within 0.25 seconds).\nIn our experiments, we set \u5165 (9) to be 1. We report the result in Table A3. $SPO^{relu}_+$ loss performs\nvery poorly, as its regret is much higher than MSE. This is due to the fact that turning negative costs\nto zero without considering their values causes loss of information. $SPO^{min+}_+$ performs much\nbetter. However, even in some cases its regret is higher than MSE. On the other hand, $SPO^{relu}_{+\\P}$ and\n$SPO^{min+}_{+\\P}$, which add explicit training penalty if $2\\hat{c}(a_i) < c(a_i)$ for a action ai, are able to improve\n$SPO^{relu}_+$ and $SPO^{min+}_+$. It is interesting to note that the difference between relu and min+ is\ninsignificant after adding explicit training penalty. This experiment suggests both $SPO^{relu}_{+\\P}$ and\n$SPO^{min+}_{+\\P}$ are effective DFL approaches for predicting action costs in planning problems."}, {"title": "Optimal Planning Versus Non-Optimal Planning", "content": "RQ4: Can we use non-optimal planning for DFL training? As DFL requires solving the\nplanning problem repeatedly while training, which creates a considerable computational burden when"}, {"title": "Optimal Planning versus Solution Caching", "content": "RQ5: Can we use solution caching to speed up training? Next we investigate whether solution\ncaching, as implemented by Mulamba et al. [2021] in the context of DFL for optimization, is effective\nfor planning problems too. We initialize the cache with all the solutions present in the training data.\nWe experiment with p = 10% and 20%. We can see in Table 5, the training time of caching faster\ncompared to opt due to they solve the planning is solved for only p% of instances using the predicted\naction costs. While p = 10% does not consistently outperform MSE regret, p = 20% produces regret\nlower than MSE for all instances. This indicates for large planning instances, use of solution caching\nwith p = 20% could prove to be a useful approach."}, {"title": "Conclusion", "content": "In this work, we investigated for the first time how we can use techniques from DFL in the planning\ndomain. More specifically for the case of predicting action costs from correlated features and historic\ndata, we showed how the SPO technique, which places no assumptions on the solver used, can also\nbe used for planning problems. Other DFL techniques which work with a black-box solver Pogancic\net al. [2020", "2021": "are now equally applicable.\nWe proposed an implementation of DFL which accounts for the fact that planners do not support\nnegative action costs. Our findings suggest that imposing non-negativity through relu leads to an\nincrease in regret, both for model trained with MSE and DFL loss. Moreover, training with an\nexplicit penalty for correcting negative action costs before solving the planning problem yields\nsignificant improvements. Indeed, our DFL approach always leads to lower regret than when training\nto minimize MSE of predicted action costs."}]}