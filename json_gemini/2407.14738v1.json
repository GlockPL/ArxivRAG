{"title": "Flatness-aware Sequential Learning Generates Resilient Backdoors", "authors": ["Hoang Pham", "The-Anh Ta", "Anh Tran", "Khoa D. Doan"], "abstract": "Recently, backdoor attacks have become an emerging threat to the security of machine learning models. From the adversary's perspective, the implanted backdoors should be resistant to defensive algorithms, but some recently proposed fine-tuning defenses can remove these backdoors with notable efficacy. This is mainly due to the catastrophic forgetting (CF) property of deep neural networks. This paper counters CF of backdoors by leveraging continual learning (CL) techniques. We begin by investigating the connectivity between a backdoored and fine-tuned model in the loss landscape. Our analysis confirms that fine-tuning defenses, especially the more advanced ones, can easily push a poisoned model out of the backdoor regions, making it forget all about the backdoors. Based on this finding, we re-formulate backdoor training through the lens of CL and propose a novel framework, named Sequential Backdoor Learning (SBL), that can generate resilient backdoors. This framework separates the backdoor poisoning process into two tasks: the first task learns a backdoored model, while the second task, based on the CL principles, moves it to a backdoored region resistant to fine-tuning. We additionally propose to seek flatter backdoor regions via a sharpness-aware minimizer in the framework, further strengthening the durability of the implanted backdoor. Finally, we demonstrate the effectiveness of our method through extensive empirical experiments on several benchmark datasets in the backdoor domain. The source code is available at https://github.com/mail-research/SBL-resilient-backdoors", "sections": [{"title": "1 Introduction", "content": "Swift progress in the field of machine learning (ML), especially within the realm of deep neural networks (DNNs), is revolutionizing various aspects of our daily lives across different domains and applications from computer vision to natural language processing tasks [5, 60, 61, 64, 71]. Unfortunately, as well-trained models are now considered valuable assets due to the significant computational resources, annotated data, and expertise spent to create them, they are becoming appealing targets for cyber attacks [6,7,45]. Prior studies have shown"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Backdoor Attacks", "content": "In backdoor attacks, the adversary aims to manipulate the output of the victim model to a specific target label with input having pre-defined triggers [11, 19, 42,62]. The backdoor injection process can be done by poisoning data [10,24] or maliciously implanting a backdoor during training [21,80]. Gu et al [24] first investigated backdoor attacks in deep neural networks and proposed BadNets. It injects the trigger into a small random number of inputs in the training set and re-labels them into target labels. After that various backdoor attacks focus on designing the triggers. In particular, Chen et al [10] leverage image blending in design trigger while Barni et al [4] use sinusoidal strips. WaNet [56] trains a generator to create input-aware triggers. LIRA [13, 14] jointly learns trigger generator and victim model to launch imperceptible backdoor attacks. Besides data attacks, some works [21,80] perturb the weights of a pre-trained clean model to inject a backdoor."}, {"title": "2.2 Backdoor Defenses", "content": "In general, backdoor defense methods can be divided into pre-training [68,70], in-training [39, 79], and post-training [32, 40, 44, 46, 81] stages. In pre-training and in-training defenses, the defender assumes the dataset is poisoned, and thus leverages the models' distinct behavioral differences on the clean and poisoned samples to remove the manipulated data or avoid learning the backdoor during training. Most defensive solutions perform post-training defenses since it can be more challenging to alter or, in some cases, not possible to participate in the training process. Post-training defenses assume the defender has access to a small set of benign samples for backdoor removal [31, 40, 44,72,81] and can be roughly categorized into fine-tuning based defenses [40,44,81] and pruning-based defenses [8,44,72]. Pruning-based defenses prune neurons [44,72] or weights [8] to remove backdoor-contaminated components in the model. However, these methods either cause non-trivial drops in benign accuracy or their effectiveness depends on the network architectures [81], significantly reducing their utility and consistency. On the other hand, fine-tuning based defenses leverage the catastrophic forgetting phenomenon of DNNs [2,34], when a backdoored model is fine-tuned on clean data, for backdoor removal. In addition, fine-tuning is also a common step in numerous practical ML systems to adapt a pre-trained model to better align with the user's needs. This paper focuses on developing a novel backdoor learning approach that can enable existing backdoor attacks to be resistant to conventional fine-tuning processes in practical applications and advanced fine-tuning defenses."}, {"title": "2.3 Continual Learning", "content": "Continual Learning (CL) is a learning paradigm where the model learns a sequence of tasks. When tasks arrive, the model has to preserve previous knowledge"}, {"title": "2.4 Mode Connectivity and Sharpness-Aware Minimization", "content": "Loss landscape has been investigated to understand the behavior of DNNs [18, 22, 38]. Hochreiter et al [29] show that flat and wide minima generalize better than sharp minima. Recently, SAM [18] and its variants [37,47,78] improve generalization by simultaneously minimizing both the loss value and loss sharpness. This property is leveraged to mitigate forgetting in CL methods [12,52]. Besides, Mode Connectivity [16, 22], a novel tool to understand the loss landscape, postulates that different optima obtained by gradient-based optimization methods are connected by simple low-error path (i.e., low-loss valleys). Mirzadeh et al [51] observe that there exists a low-error path connecting multi-task and continual learning minima when they share a common starting point. Motivated by this observation, the works in [43,51] propose methods to guide the model towards this connectivity region."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Threat Model", "content": "We adopt the commonly-used backdoor-attack setting where the attacker trains a model and provides it to the victim [13,41]. Since training large-scale neural networks is empirical, data-driven, and resource-extensive, it is generally cost-prohibitive for end-users, who consequently turn to third-party MLaaS platforms [63] for model training, or simply clone pre-trained models from public sources such as Hugging Face. This practice opens up opportunities for training-control backdoor attacks, a serious security threat to victim users.\nAttacker's Capability. The attacker has full control of designing the triggers, poisoning training data, and the model training schedule.\nAttacker's Goal. The attacker aims to implant a backdoor into the model and bypass post-training defense methods, especially fine-tuning defenses."}, {"title": "3.2 Conventional Backdoor Learning", "content": "We consider supervised learning of classification tasks where the objective is to learn a mapping function $f_\\theta : \\mathcal{X} \\rightarrow \\mathcal{C}$ with the input domain $\\mathcal{X}$ and the set of class labels $\\mathcal{C}$. The task is to learn the parameters $\\theta$ from training dataset $\\mathcal{D} = \\{(\\mathbf{x}_i, y_i) : \\mathbf{x}_i \\in \\mathcal{X}, y_i \\in \\mathcal{C}, i = 1, 2, ..., N\\}$ using a standard classification loss $\\mathcal{L}$ such as Cross-Entropy Loss. The most common training scheme for backdoor attacks uses data poisoning to implant backdoors, where the classifier is trained on $\\mathcal{D}_p$ a mixture of clean and poisoned data from $\\mathcal{D}$. The general procedure to generate poisoned data is to transform a clean training sample $(\\mathbf{x}, y)$ into a backdoor sample $(\\mathcal{T}(\\mathbf{x}), \\eta(y))$ with some backdoor injection function $\\mathcal{T}$ and target label function $\\eta$. Backdoor training manipulates the behavior of $f$ so that: $f(\\mathbf{x}) = y, f(\\mathcal{T}(\\mathbf{x})) = \\eta(y)$.\nIt is well-established that such training can cause the models to converge to the backdoor regions. However, empirical evidence [44] (see also our Figure 2) suggests that even a simple fine-tuning process with a small set of clean data can lead the model to an alternative local minimum that is free of backdoor while preserving the model's performance on clean data."}, {"title": "3.3 Proposed SBL Framework", "content": "This paper views backdoor learning through the lens of continual learning (CL): we re-formulate the attack and defense as the CL tasks. More precisely, the attacker aims to develop resilient backdoors that remain even after the models undergo fine-tuning defenses at the user's site - this can be regarded as reducing catastrophic forgetting in CL; while the defender strives to relocate the models away from the backdoor region without compromising performance on clean data - leveraging catastrophic forgetting to remove backdoors.\nTo challenge the effectiveness of fine-tuning defenses, our key idea is to simulate this defense mechanism during the training phase of backdoor learning to familiarize our models with clean-data fine-tuning, which reduces the effect of forgetting during any subsequent fine-tuning defenses. In particular, we split the training data $D_p$ into two sets $D_0$ and $D_1$, where $D_0$ is a combination of clean and poisoned data while $D_1$ contains only clean samples. Then, we divide backdoor training into two consecutive tasks: first to learn the backdoor, then familiarize it with fine-tuning. In the first step, we learn a backdoored model $\\theta_{B_0}$ on $D_0$ by utilizing Sharpness-Aware Minimization (SAM) [18,37] on the loss $\\mathcal{L}^{SAM}(D_0; \\theta)$. Since a flatter loss landscape is known to reduce catastrophic forgetting [12, 52], this training strategy will seek a flat backdoor loss landscape, consequently limiting the model's ability to forget backdoor-related knowledge"}, {"title": "3.4 On the working mechanism of our method SBL", "content": "The proposed method SBL is designed based on our intuition that to neutralize the effect of fine-tuning defenses, we can train the model so that it converges to backdoored regions having flat loss landscapes. Flatness then can cause the fine-tuned model in Step 1 (usually with small learning rates) to still be trapped in the region of backdoor knowledge, which makes our attack resilient to fine-tuning defenses. Here, we provide further heuristic explanations based on observations from continual learning and mode connectivity, and confirm with empirical evidence that the behaviors of our approach closely align with these intuitions. Additional analysis in terms of Taylor expansions is given in the Appendix.\nWe regard the Algorithm 1 of SBL as a two-step procedure: multi-task (MT) training (Step 0) followed by continual learning (CL) (Step 1). More precisely, SBL first trains the backdoored model $\\theta_{B_0}$ on both clean and poisoned data (MT), then fine-tunes $\\theta_{B}$ with clean data and a tiny learning rate to obtain $\\theta_B$ (CL). Denote $\\theta_f$ of the model obtained with a fine-tuning defense afterward.\nFirst, we calculate the losses and accuracies of models trained with SBL along various connectivity paths and compare them to those obtained with conventional backdoor learning (CBL) models. We perform experiments on two settings: ResNet18 model on CIFAR-10 and GTSRB, with BadNets as the base attack. In Figure 2, the first column visualizes the loss and accuracy on the clean and poisoned test sets, where we linearly interpolate between a backdoored and fine-tuned model in the CBL setup. While the clean loss and accuracy remain unchanged, the poison loss gradually increases and the corresponding ASR decreases to nearly zero on the fine-tuned model. This indicates that with CBL, fine-tuning can effectively push the poisoned model out of the backdoor-affected area. On the other hand, the persistently high ASR and low poison losses in"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experiment Setting", "content": "Datasets. We use three benchmark datasets in backdoor research, namely CIFAR-10 [35], GTSRB [30], and ImageNet-10 for the experiments. We follow [33] to select 10 classes from ImageNet-1K [65]. We divide the training set into three subsets: mixed set $D_0$ (poisoned and benign samples), clean set $D_1$, and defense set with portion 85% - 10% - 5%, respectively.\nModels. We use the same classifier backbone ResNet18 [27] for all datasets. We also employ different architectures (in ablation studies): VGG-16 [67] and ResNet-20, a lightweight version of ResNet. We use SGD optimizer for training"}, {"title": "4.2 Backdoor Performance", "content": "We verify the effectiveness of SBL by comparing it against the standard backdoor training using different fine-tuning defenses. We conduct the experiments on three datasets, including CIFAR-10, GTSRB, and Imagenet-10 with 10% poisoning rate, and with ResNet18. In Step 1, we train the backdoored model $\\theta_{B_0}$ obtained in Step 0 with different CL methods. The results are presented in Table 1, 2, and 3, respectively. Due to the space limit, we present the SBL's experiments with Anchoring in the Appendix.\nIn the CIFAR-10 and GTSRB settings, for conventional backdoor training, fine-tuning with a lower learning rate (0.005) can better preserve the model's utility but fails to effectively mitigate the backdoor. In contrast, fine-tuning with a larger learning rate (0.01) can effectively eliminate backdoors but comes at the cost of sacrificing clean-data performance. Both FT-SAM and NAD can maintain high clean accuracy while effectively mitigating backdoors. On the other hand, SBL significantly enhances the backdoor's resilience against all the previously mentioned defensive methods. With different CL techniques, SBL effectively circumvents these defenses."}, {"title": "4.3 Ablation Studies", "content": "Architecture Ablation. We evaluate SBL's effectiveness on different network architectures, including VGG-16 and ResNet-20 (a lightweight version of ResNet). The experiments are conducted on CIFAR-10 and GTSRB with various attacks while SBL uses EWC. Tables 4 and 5 show the backdoor's performance against various fine-tuning defenses. We observe similar effectiveness in SBL with these network architectures, confirming that SBL can successfully enhance the durability of the backdoored models against finetuning defenses.\nRole of Continual Learning. As discussed in Section 3.3, the CL design of SBL helps learn a backdoored model that is resistant to catastrophic forgetting"}, {"title": "5 Conclusion", "content": "In this paper, we approach backdoor attacks and defenses as continual learning tasks with a focus on mitigating backdoor forgetting. We introduce a novel backdoor training framework that can significantly intensify the resilience of the implanted backdoors when these models undergo different fine-tuning defenses. This framework splits the backdoor learning process into two steps with a sharpness-aware minimizer. This collaboration traps the poisoned model in the backdoor regions that are difficult for existing fine-tuning defenses to find alternative backdoor-free minima. We conduct extensive experiments on several benchmark backdoor datasets to demonstrate the effectiveness of our framework, compared to traditional backdoor learning. Our work exposes the existence of another significant backdoor threat against fine-tuning defenses, and we urge researchers to develop countermeasures for this type of attack."}, {"title": "6 Experimental Setup", "content": ""}, {"title": "6.1 Datasets", "content": "We use three benchmark datasets, namely CIFAR-10 [35], GTSRB [30], and ImageNet-10 for our experiments. These datasets are widely used in various previous works, in both backdoor attacks and defenses.\nCIFAR-10: The dataset consists of 60,000 color images in 10 classes with the size 32 x 32. CIFAR-10 is divided into a training set and a test set with 50,000 and 10,000 images, respectively. In training, we use random crop, and random horizontal flip as augmentation operators while no augmentation is applied during evaluation.\nGTSRB: The German Traffic Sign Recognition Benchmark - GTSRB consists of 60,000 images with 43 different classes and size varies from 32 x 32 to 250 x 250. The training set includes 39.209 images while the test set has 12,630 samples. We resize all images to 32 x 32 pixels. In training, we only augment samples with the random crop operator and do not use augmentation during testing.\nImageNet-10: We follow [33] to select 10 classes from ImageNet-1K [65]. There are 10,041 images in the training set and 2,600 images in the test set. We resize all images to 64 x 64 pixels. In training, we use random crop, random rotation, and random horizontal flip as augmentation operators while no augmentation is applied during evaluation."}, {"title": "6.2 Training Details", "content": "We divide the training set into three sets: mixed set $D_0$ (poisoned and benign samples), clean set $D_1$, and defense set with portion 85% - 10% - 5%, respectively. In the testing, to generate the poisoning test set, we clone the clean test set, remove all clean samples with labels equal to the targeted class, and then poison all the images and change their labels to the targeted class. We use the same classifier backbone ResNet18 [27] for all datasets. We also do ablation studies on different architectures: VGG-16 [67] and ResNet-20, a lightweight version of ResNet. We use SGD optimizer for training the backdoored model and SAM [18]"}, {"title": "6.3 Detailed Baselines", "content": "Baselines Here, along with original backdoor training, we select several CL techniques to incorporate in our framework to train backdoored model including Naive, EWC [34], Anchoring [80], and AGEM [9].\nOriginal: This is a conventional backdoor training. Particularly, we incorporate mixed dataset $D_0$ and clean data $D_1$ into one to train the backdoored model."}, {"title": "6.4 Further analysis of SBL", "content": "Recall that SBL first trains the backdoored model on both clean and poisoned data to obtain $\\theta_{B_0}$ (MT), then fine-tune this model from $\\theta_{B_0}$ with clean data and a tiny learning rate to achieve $\\theta_B$ (CL). Denote $\\theta_f$ a fine-tuned model from $\\theta_B$.\nOur experiments show that the training method in Step 1 of Algorithm 1 of SBL is able to find low-loss minima for clean data, while still maintaining the performance on poisoned data. This can be heuristically explained using Taylor approximation of the backdoor loss $\\mathcal{L}_o(\\theta_B) := \\mathcal{L}(D_0;\\theta_B)$ as follows. Since $\\theta_{B_0}$ is the optimal weight, we can assume that the gradient vanishes $\\nabla \\mathcal{L}_o(\\theta_{B_0}) \\approx 0$ in the second order approximation:\n$\\mathcal{L}_o(\\theta_B) \\approx \\mathcal{L}_o(\\theta_{B_0}) + (\\theta_B - \\theta_{B_0})^T \\nabla \\mathcal{L}_o(\\theta_{B_0}) + \\frac{1}{2} (\\theta_B - \\theta_{B_0})^T \\nabla^2 \\mathcal{L}_o(\\theta_{B_0}) (\\theta_B - \\theta_{B_0})$\nfrom which we obtain the estimate:\n$\\mathcal{L}_o(\\theta_B) - \\mathcal{L}_o(\\theta_{B_0}) \\leq \\frac{1}{2} \\lambda_{max} ||\\theta - \\theta_{B_0}||^2,$\nwhere $\\lambda_{max}$ is the maximum eigenvalue of $\\nabla^2 \\mathcal{L}_o(\\theta_{B_0})$. Since (i) the change of weight is low ($||\\theta_B - \\theta_{B_0}|| \\approx 0$) due to the tiny learning rate when continuously fine-tuning on clean data in Step 1, and (ii) $|\\lambda_{max}| \\approx 0$ due to the effect of the flatness-aware optimizer SAM [18] that we use in Step 0, Eq. (2) implies that the change in backdoor loss after Step 1 of SBL is small, as we empirically observed in Fig. 2."}, {"title": "7 Supplementary Experimental Results", "content": ""}, {"title": "7.1 Additional Main Results", "content": "Here, we provide additional results when incorporating Anchoring with our framework in three settings. We show results in Table 10, 11, and 12."}, {"title": "7.2 Additional Results with Pruning Defense", "content": "Besides fine-tuning defense methods, we also test our methods with the pruning-based approach, in particular, we sequentially prune filters in the last layers of networks. We perform pruning defense on two datasets CIFAR-10 and GTSRB on three different backbones including ResNet18, VGG-16, and ResNet20 with BadNets data poisoning method. In Figure 4, we visualize the Clean Accuracy (CA) and Attack Success Rate (ASR) of backdoored models trained by Conventional Backdoor Learning (CBL) and our framework Sequential Backdoor Learning (SBL) when pruning each filter or neuron. In general, our SBL can help the backdoored model to be more resistant against pruning defense. Liu et. al., [44] observes that backdoored neurons are dormant for clean inputs, therefore they proposed to prune low-sensitivity neurons to clean data sequentially. Meanwhile, our SBL mitigates this sensitivity through SAM [18] which forces the backdoored model's parameters to be more stable with perturbations."}, {"title": "7.3 Additional Visualizations with Model Interpolation", "content": "In this section, we visualize the loss and accuracy on the clean and poisoned test sets while linearly interpolating between model after Step 0 $\\theta_B$ and back-doored model $\\theta_{B_0}$ in the first column of Figure 5, and backdoored model $\\theta_B$ and fine-tuned model $\\theta_f$ on different architecture settings. Figure 5 one more time confirms that our framework SBL can identify a low-error path connecting the backdoored model to the fine-tuned model."}, {"title": "7.4 Additional Visualizations with Gradient Norm", "content": "Here, we visualize the gradient norm during fine-tuning of our framework SBL compared to CBL in several network architectures and visualize it in Figure 6. With different backbones, we still observe that in the early stage of fine-tuning, the gradient norm values of CBL are substantially higher than SBL which supports CBL poisoned model escape from backdoor regions."}, {"title": "7.5 Effect of Fine-tuning epochs", "content": "We investigate the ASR/CA results of SBL against FT defenses for up to 200 epochs in Fig. 7 (BadNet trigger, ResNet-18/VGG-16, CIFAR-10/GTSRB). We can observe that during fine-tuning, the ASR remains the same (100%) while the CA slightly changes, demonstrating that the model's still trapped in backdoored regions while finding good local minima for clean data."}, {"title": "8 Limitations", "content": "As discussed, this paper aims to improve the resistance of existing backdoor attacks against fine-tuning defenses, thus our empirical evaluation focuses on studying its effectiveness against these tuning defenses. Nevertheless, since our method only augments the existing attacks against fine-tuning defenses, the base attack's effectiveness against other types of defense should be preserved.\nAnother limitation of this work is that our SBL requires the adversary to have full access to the training process. While this is a commonly studied backdoor setting in the literature, there exist other settings, e.g., dataset control only, where the adversary cannot control the training processes in practice. Extending SBL framework to perform an attack in one of these settings would be an interesting future extension of our work.\nFinally, our work emphasizes the potential risks associated with dependence on pre-trained models from third-party sources, emphasizing the critical need to cultivate trust between users and model providers. To safeguard against these potential threats, users are advised to only utilize pre-trained models from reputable providers or actively involved in the training process. Furthermore, we encourage the research community to launch more investigations into this domain to establish better safeguards."}]}