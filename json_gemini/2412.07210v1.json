{"title": "EDIT: A LOCAL-SGD-BASED EFFICIENT DISTRIBUTED TRAINING METHOD FOR LARGE LANGUAGE MODELS", "authors": ["Jialiang Cheng", "Ning Gao", "Yun Yue", "Zhiling Ye", "Jiadi Jiang", "Jian Sha"], "abstract": "Distributed training methods are crucial for large language models (LLMs). However, existing distributed training methods often suffer from communication bottlenecks, stragglers, and limited elasticity. Local SGD methods have been proposed to address these issues, but their effectiveness remains limited to small-scale training due to additional memory overhead and lack of concerns on efficiency and stability. To tackle these issues, we propose EDiT, an innovative Efficient Distributed Training method that combines a tailored Local SGD approach with model sharding techniques to enhance large-scale training efficiency. EDiT performs layer-wise parameter synchronization during forward pass, reducing communication and memory overhead and enabling the overlap of computation and communication. Besides, EDiT employs a pseudo gradient penalty strategy to suppress loss spikes, which ensures training stability and improve performance. Additionally, we introduce A-EDIT, a fully asynchronous variant of EDiT that accommodates heterogeneous clusters. Building on EDiT/A-EDiT, we conduct a series of experiments to validate large-scale asynchronous training for LLMs, accompanied by comprehensive analyses. Experimental results demonstrate the superior performance of EDiT/A-EDiT, establishing them as robust solutions for distributed LLM training in diverse computational ecosystems.", "sections": [{"title": "INTRODUCTION", "content": "Distributed training stands as the cornerstone for deep neural networks (Dean et al., 2012). With the explosive growth of model scale and data volume (Touvron et al., 2023; Bai et al., 2023), techniques such as Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al., 2020) and 3D Parallelism (Narayanan et al., 2021) are precipitated to address the challenges of GPU memory overflow. These approaches rely on synchronous paradigm, which introduces significant communication overhead during the training process (Douillard et al., 2023). Besides, the synchronous paradigm also introduces the straggler problem, where faster workers are idle waiting for the slower ones to catch up. This issue is particularly prevalent in large/heterogeneous clusters (Lian et al., 2018). Lastly, in resource-constrained clusters, there is a compelling need for elastic training (Li et al., 2023). However, synchronous training paradigms struggle in elastic settings, where dynamic scaling of resources disrupts optimal hyperparameters and requires slow redistribution of model parameters.\nThese challenges have spurred significant research into distributed optimization methods. A typical method is Local Stochastic Gradient Descent (a.k.a Local SGD or Local-Update SGD) (Zhang et al., 2016), where each worker independently executes multiple local optimization steps in parallel before averaging model parameters across all workers. Subsequent studies have improved upon this foundational paradigm to improve the performance (Lin et al., 2019; Wang et al., 2019; Douillard et al., 2023). However, existing Local SGD methods are not easily applicable to the training of large language models (LLMs). These methods do not handle model sharding well, preventing their application to models larger than billions of parameters. Moreover, previous applications of Local SGD have focused on small-scale, highly curated datasets (Zhang et al., 2016; Douillard et al., 2023), making their results less transferable to LLM training that relies on vast, noisy datasets\nwhere instability may be introduced during the training process. Although current Local SGD methods can improve overall training efficiency by reducing communication frequency and diminishing the impact of random stragglers, they still struggle with the presence of consistent stragglers within heterogeneous devices (Liu et al., 2024). Additionally, because most existing Local SGD methods synchronize all parameters globally after some local optimizer updates, extra communication overhead are introduced (Sun et al., 2023). Lastly, current Local SGD methods predominantly employ a uniform averaging strategy to synchronize the parameters, failing to fully capitalize on the inherent differences in training progress across diverse workers (Douillard et al., 2023).\nTo address these challenges, we propose a novel Efficient Distributed Training (EDiT) method for large language models. As illustrated in Figure 1, EDiT employs a hierarchical distribution strategy on a two-dimensional device mesh, where all workers are data parallel. Model parameters are fully sharded along the model shard dimension and synchronized along the model sync dimension. With the efficient communication links within the model shard groups and the low-frequency periodic synchronization strategy within the model sync groups, the impact of communication overhead and random stragglers is effectively alleviated. When synchronizing parameters, EDiT operates layer by layer during the forward pass and makes use of a prefetch strategy to overlap computation and communication, thereby reducing the additional communication and GPU memory overhead introduced by parameter synchronization. Additionally, EDiT employs a novel pseudo-gradient penalty method to address the instability caused by workers progressing differently due to the diverse large-scale corpus, and it leverages these differences among workers. Furthermore, we propose an asynchronous variant of the EDiT method named A-EDiT to deal with the consistent stragglers in heterogeneous clusters. We conducted a comprehensive evaluation of our proposed methods on LLM tasks, demonstrating its effectiveness compared to state-of-the-art methods. 1\nOur primary contributions can be summarized as follows:\n\u2022 Engineering Innovation: We introduce EDiT, an efficient large-scale distributed training method that integrates Local SGD with the model sharding strategy. EDiT reduces the impact of stragglers and communication overhead and supports elastic training.\n\u2022 Algorithmic Novelty: EDiT performs layer-wise parameter sync during forward pass to reduce communication and memory overhead. With prefetch strategy, the parameter-sync communication can be further overlapped with computation. Besides, we propose a new pseudo gradient penalty method to improve the training stability and model performance. We also provide a fully asynchronous variant of EDiT, called A-EDiT, to address the challenges of consistent stragglers.\n\u2022 Practical Contributions: We provide a large-scale verification of asynchronous pre-training for LLMs, along with an extensive analysis of convergence, generalization, acceleration, scalability, and stability. This work offers critical insights into optimizing asynchronous distributed LLM training at scale."}, {"title": "RELATED WORK", "content": "One of the early works that proposed the concept of Local SGD was Zhang et al. (2016), establishing the paradigm of parallel multi-step training followed by periodic averaging. Lin et al. (2019) introduced the Post Local SGD method, which starts with standard synchronized training for warm-up before switching to the Local SGD mode. SlowMo (Wang et al., 2019) utilizes a slow momentum to transform model averaging into moving average. DiLoCo (Douillard et al., 2023) demonstrates that the Nesterov optimizer (Nesterov, 1983) is suitable as an outer optimizer. Multi-Level Local SGD (Castiglia et al., 2020) partition the network into disjoint sub-networks and hierarchically synchronizes the models. Wang & Joshi (2019) and Balles et al. (2023) have respectively explored the optimal hyperparameter settings for Local SGD. Shen et al. (2021) advocated for gradually increasing synchronization intervals while decreasing learning rates to optimize model performance. Extensive theoretical analyses of Local SGD have also emerged. Yu et al. (2019), Khaled et al. (2020), Spiridonoff et al. (2020), and Deng et al. (2022) examined convergence rates under various conditions. Gu et al. (2022) found that Local SGD improves generalization with a small learning rate and long training duration. Pan & Song (2023) demonstrated faster convergence by leveraging second-order information.\nResearchers have also explored the combination of Local SGD with asynchronous training paradigms that decouple computation and communication. Early works were predominantly based on the federated learning framework (Xie et al., 2019). FedBuff (Nguyen et al., 2022) updates the server model only after accumulating a certain amount of pseudo gradients. DN-DyLN (Liu et al., 2024) improves the buffer mechanism to employ delayed Nesterov update. TimelyFL (Zhang et al., 2023) dynamically adjusts the local training workload according to the real-time resource situation. Subsequently, several works based on other architectures were also proposed. Gossip-PGA (Chen et al., 2021) incorporates periodic global averaging into the gossip SGD framework (Lian et al., 2017). CO2 (Sun et al., 2023) utilizes Local SGD and asynchronous communication to hide the overhead. A key challenge for asynchronous training is the staled model problem, resulting in inferior performance compared to synchronous training methods (Liu et al., 2024).\nNotably, current All-Reduce-based Local SGD methods (Lin et al., 2019; Wang et al., 2019; Sun et al., 2023) hold complete model parameters on each GPU, making it difficult to handle model sharding for LLM training. Although Sun et al. (2023) claims that they can combine CO2 with ZeRO series optimizers (Rajbhandari et al., 2020), the additional communication introduced degrades CO2 to a synchronized mode, negating the performance gains from periodic synchronization and overlapped communication. Furthermore, the extra parameters and outer momentum further increase memory pressure, limiting their scalability to larger models. In contrast, our proposed EDiT and A-EDiT methods effectively utilize the characteristics of model sharding, leveraging device mesh, layer-wise parameter synchronization, prefetch strategy, and CPU offload to minimize communication and memory overhead, making it more suitable for LLM training."}, {"title": "METHOD", "content": "Our proposed EDiT method integrates model sharding with periodic synchronization to accelerate the training of LLMs. The detailed procedure of EDiT is illustrated in Figure 1 and formally outlined in Algorithm 1 in Appendix. To start with, EDiT builds an $M \\times N$ device mesh across $K$ workers : $M$ model sync groups $G^r = \\{G_1,\\dots,G_M\\}$ with each comprising $N$ workers $G_i = \\{W_{(i,1)}, W_{(i, 2)}, \\cdots, W_{(i,N)} \\}^2$, and $N$ model shard groups $G^s = \\{G_1,\\dots,G_N\\}$ with each comprising $M$ workers $G_i = \\{W_{(1,i)}, W_{(2,i)},\\cdots, W_{(M,i)}\\}$, where $M \\times N = K$. This structured arrangement aims to tailor communication patterns to the diverse capabilities and network latencies inherent in the distributed system. For instance, in a multi-node GPU cluster where intra-node communication is significantly faster than inter-node communication, all GPUs within the same node can be connected as a model shard group, while GPUs of the same rank across different nodes can be connected as a model sync group. Model parameters are sharded uniformly in each model"}, {"title": "PSEUDO GRADIENT PENALTY", "content": "Despite diligent data cleaning efforts, there are still significant amount of low-quality data in the LLM pre-training corpora (Albalak et al., 2024), resulting in training instability manifested as loss spikes. This issue can be addressed by large batch sizes typical of synchronous training regimes, but becomes salient in Local SGD regimes where each worker operates on relatively smaller batches.\nTo tackle this issue, we introduce a novel pseudo gradient penalty strategy at the parameter synchronization stage, as depicted in Figure 2 and Algorithm 2 in Appendix. This strategy consists of anomaly elimination, weighted averaging, and gradient clipping. To illustrate the idea, we use a model sync group $G_m = \\{W_1,\\dots, W_N\\}$ as an example. We begin by computing the pseudo gradients $\\triangle^{(i,l)}_t = \\Theta^{(i,l)}_{t, \\tau} - \\Theta^{(i,l)}_t$ for each worker, where $\\Theta^{(i,l)}_{t, \\tau}$ is the sharded parameters of module $l$ held by worker $W_i$ at outer step $t$ and inner step $\\tau$, and $\\Theta^{(i,l)}_t$ denotes the corresponding synchronized parameters at the beginning of outer step $t$.\nAnomaly elimination. We first eliminate the significantly anomalous workers to reduce their adverse impacts on the overall model performance. Since anomalies cause substantial parameter fluctuations and lead to large pseudo-gradient norms, we use the pseudo-gradient norm as the criterion. Here we utilize an Exponential Moving Average (EMA) z-test method for statistical analysis. Let $G^{(i,l)}_t = ||\\triangle^{(i,l)}_t||_2$ denotes the pseudo gradient norm for the worker $W_i$, then the EMA z-score can be calculated by $z^{(i,l)}_t = \\frac{G^{(i,l)}_t - \\mu^{(i,l)}_t}{\\sigma^{(i,l)}_t}$, where $\\mu^{(i,l)}_t$ and $\\sigma^{(i,l)}_t$ are the EMA mean and standard deviation of $G^{(i,l)}_t$, respectively. A worker $W_i$ with $z^{(i,l)}_t > \\delta$ is identified as an anomaly and its $G^{(i,l)}_t$ will be set to infinity, where $\\delta$ is a threshold, typically set to 3 in practice. Both $\\mu^{(i,l)}_t$ and $\\sigma^{(i,l)}_t$ are updated at each step using an exponential moving average to capture the convergence trend of the gradient norm during the training process:\n\\begin{equation}\n\\mu^{(i,l)}_{t+1} = \\alpha G^{(i,l)}_t + (1 - \\alpha)\\mu^{(i,l)}_t, \\sigma^{(i,l)}_{t+1} = \\sqrt{(1 - \\alpha) (\\sigma^{(i,l)}_t)^2 + \\alpha (G^{(i,l)}_t - \\mu^{(i,l)}_t)^2},\n\\end{equation}\nwhere $\\alpha$ is a weighting coefficient, commonly assigned a value of 0.02 in practical applications. The update of Equation 1 will be skipped if $G^{(i,l)}_t$ is infinite. In the preliminary stage, a warm-up period is set to establish stable values for $\\mu^{(i,l)}_t$ and $\\sigma^{(i,l)}_t$, during which no workers are flagged as anomalies. Notably, to maintain consistent updates within the same module, we compute the pseudo gradient norm for the entire module, and subsequently introduced gradient norm-related operations follow the same procedure. Because this process only introduces one scalar communication in the model shard groups, the overhead is negligible. If all workers are identified anomalous, all the parameters will be effectively rollbacked to the last synchronized parameters $\\Theta^{(i,l)}_t$.\nWeighted averaging. Furthermore, considering that large pseudo gradients may still exert considerable impacts on the overall update direction, we propose to weigh the pseudo gradients of each worker based on the norms, which was similarly demonstrated in Thakkar et al. (2023). The weight assigned to the pseudo gradients corresponding to $W_i \\in G_m$ is calculated by\n\\begin{equation}\nW_{t,i} = \\frac{\\exp(-G^{(i,l)}_t)}{\\sum_j \\exp(-G^{(j,l)}_t)}\n\\end{equation}\nIn this way, a larger pseudo gradient norm leads to stronger suppression, thereby allowing all workers to contribute equally to the update direction and thus increasing the likelihood to find the correct direction. Following that, by performing a weighted summation of all pseudo gradients in $G_m$, we obtain the synchronized pseudo gradients:\n\\begin{equation}\n\\overline{\\triangle}^{(l)}_t = \\sum_j W_{t,j} \\triangle^{(j,l)}_t, W_j \\in G_m\n\\end{equation}\nGradient clip. We then adopt a gradient clip strategy to constrain the update step size. Let $\\overline{G}^{(i,t)} = ||\\overline{\\triangle}^{(i,l)}_t||_2$ denote the synchronized pseudo gradient norm and $\\phi$ denote the threshold, the clip coefficient is computed by\n\\begin{equation}\n\\beta_t = \\min(\\phi / (\\overline{G}^{(i,t)} + \\epsilon), 1),\n\\end{equation}\nwhere $\\epsilon$ is a small positive constant to avoid division by zero. The pseudo gradients are clipped by\n\\begin{equation}\n\\triangle^{(i,l)}_t = \\beta_t\\overline{\\triangle}^{(i,l)}_t.\n\\end{equation}"}, {"title": "ASYNCHRONOUS EDIT", "content": "EDiT requires periodic synchronization at every $\\tau$ inner iterations. However, the fastest worker idles awaiting the peers to finish $\\tau$ iterations even if it completes its own $\\tau$ iterations earlier. As a consequence, the overall training efficiency is pegged to the slowest worker. This issue becomes more pronounced in heterogeneous clusters, where nodes are equipped with diverse devices.\nIntuitively, it would be beneficial to allow different workers to train at their own pace and remove the constraint of fixed-step synchronization. Therefore, we propose an asynchronous variant of the EDiT method, named A-EDiT. The differences are depicted in Figure 3. Herein, we set a fixed time interval $T_{time}$, and let each worker update locally until surpassing this specified time threshold. Then, a parameter synchronization ensues. This modification enables faster workers to undertake more iterations in each inner loop. Theoretically, no worker will wait longer than the single step time of the slowest worker at each parameter synchronization. We empirically verified that A-EDiT achieves faster training in all scenarios with comparable model performance."}, {"title": "EXPERIMENTS", "content": "We consider four different scales of Llama models (Touvron et al., 2023) in our experiments: 350M, 1B, 3B, and 7B. Their specific configurations are detailed in Table 3 in Appendix.\nDatasets We use a new large-scale open-source dataset, FineWeb-Edu (Lozhkov et al., 2024) in our experiments. Additionally, we also utilize an in-house private out of production pre-training dataset, which we will refer to as in-house dataset below.\nBaselines We consider several state-of-the-art methods, including standard mini-batch (Baseline), Post Local SGD (Lin et al., 2019), DiLoCo (Douillard et al., 2023), and CO2/CO2* (Sun et al., 2023). Here CO2* is the memory-efficient version of CO2 that shards extra parameters and outer momentum across workers (Sun et al., 2023).\nTraining Following DiLoCo (Douillard et al., 2023), we use AdamW (Loshchilov & Hutter, 2019) as the inner optimizer and Nesterov momentum (Nesterov, 1983) as the outer optimizer. The models are initialized with \u00b5P (Yang et al., 2021) for efficient hyperparameter search. Synchronization intervals $\\tau$ and $\\tau_{time}$ are set to 128 and 600s, respectively. Experiments are conducted on eight Nvidia A100 GPU nodes with 64 GPUs and an 8 \u00d7 8 device mesh. $\\delta$ is 10 for pseudo gradient clip."}, {"title": "CONVERGENCE AND GENERALIZATION", "content": "We first applied different methods to train the Llama 1B model on the FineWeb-Edu dataset and in-house dataset separately. Here we only compared the best-performing methods, i.e., Baseline, DiLoCo, EDiT, and A-EDiT, on the in-house dataset. The training loss (\u2193) \u00b3 and validation PPL (\u2193) results are shown in Figure 4.\nAs can be seen, our proposed EDiT and A-EDiT both achieve consistently good performance. Specifically, EDiT achieves the lowest training loss on both datasets and achieves the lowest validation PPL on the FineWeb-Edu dataset, even surpassing the Baseline. A-EDiT marginally lags behind the sync version due to the lagging workers, but it still performs better than other methods in most scenarios. Because the in-house dataset contains diverse data types and lower-quality corpora, DiLoCo (Douillard et al., 2023) experienced a noticeable decline in performance. In contrast, EDiT and A-EDiT filtered out low-quality data with the pseudo gradient penalty strategy, achieving results that were nearly comparable to the Baseline.\nWe evaluated the trained models on public benchmarks (Fourrier et al., 2023; OpenCompass Contributors, 2023). Table 1 presents the evaluation results. As can be seen, the models trained with EDiT both achieve the best average performance, and A-EDiT also performs well on the eight evaluation benchmarks. These results demonstrate that both EDiT and A-EDiT exhibit strong convergence and generalization capabilities.\nBesides, we additionally trained the Llama 350M, 3B, and 7B models on the FineWeb-Edu dataset using EDiT, the results in Figure 8 and Table 5 demonstrate that EDiT performs consistently well across different model scales."}, {"title": "ACCELERATION", "content": "We measured the speeds of different methods when training Llama models of four different scales on two A100 nodes. The synchronization interval was set to 5, and the results are the average"}, {"title": "SCALABILITY", "content": "Elastic training is the ability to dynamically adjust the resources in accordance with workload fluctuations. However, varying the resources alters the global batch size and requires additional learning rate tuning. Intuitively, the optimal learning rate of the Local SGD methods may be solely related to"}, {"title": "ABLATION STUDY", "content": "We conducted ablation studies on the pseudo gradient penalty strategy to better understand its capabilities. In this experiment, we employ the in-house dataset as it is of higher diversity and thus serves as an ideal testbed. We individually removed anomaly elimination (w/o AE), weighted averaging (w/o WA), and gradient clip (w/o GC) from EDiT, as well as all three components simultaneously (w/o ALL). The validation PPL results are shown in Figure 7a. It can be observed that without the pseudo gradient penalty strategy (w/o ALL), the PPL curve exhibits noticeable spikes and deviates considerably from the Baseline. Individually removing anomaly elimination, weighted averaging,"}, {"title": "THEORETICAL ANALYSIS", "content": "In this section, we choose SGD (Robbins & Monro, 1951) as the inner optimizer and the outer optimizer for simplicity. Under the framework developed in Wang et al. (2019), we have the following convergence theorem.\nTheorem 1. Suppose that the following assumptions are satisfied:\n1. L is differential and lower bounded, i.e., $L(\\theta^*) > -\\infty$ where $\\theta^*$ is an optimal solution. L is also L-smooth, i.e., $\\forall u, v \\in \\mathbb{R}^n$, we have $L(u) \\leq L(v) + (\\nabla L(v), u - v) + \\frac{L}{2}||u - v||^2$.\n2. At the outer step t and inner step p, $\\forall W_i \\in G_m, m \\in \\{1,\\dots, M\\}$, the algorithm can access a bounded noisy gradient and the true gradient is bounded, i.e., $||g^{(i)}_{t,p}||_{\\infty} \\leq G_{\\infty}, ||\\mathbb{E}[g^{(i)}_{t,p}]||_{\\infty} \\leq G_{\\infty}, \\forall t \\in [T-1] := \\{0,\\dots,T - 1\\}, \\forall p \\in [\\tau - 1[ := \\{0,\\dots,\\tau - 1\\}$.\n3. The noisy gradient is unbiased and the noise is independent, i.e., $\\mathbb{E}[\\zeta^{(i)}_{t,p}] = 0$ and $\\zeta^{(i)}_{t,p}$ is independent of $\\zeta^{(i)}_{t',p'}$, if $t\\neq t'$ or $p\\neq p'$.\n4. The learning rate of the inner optimizer is $\\eta_{t,p} = \\eta / \\sqrt{t\\tau + p + 1}$, and the learning rate of the outer optimizer is $\\nu$.\nThen Algorithm 1 yields\n\\begin{equation}\n\\min_{t\\in[T-1],p\\in[\\tau-1]} \\mathbb{E}[||\\nabla L(\\theta_{t,p})||^2] < \\frac{1}{2\\sqrt{\\tau\\eta} (\\nu T - 1)} \\Bigg(\\frac{L(\\theta_{0,0})}{\\nu} + \\frac{Ln G_{\\infty}^2 \\tau \\phi \\eta^2 (1 + \\ln(\\tau T))}{\\epsilon} + \\frac{Lv \\eta G_{\\infty}^2 \\phi \\eta^2 (1 + \\ln(\\tau T))}{2 \\epsilon^2}\\Bigg),\n\\end{equation}\nwhere the meaning of $\\eta$, $\\phi$ and $\\epsilon$ are listed in Table 7 of Appendix A.4.\nThe proof of Theorem 1 is presented in Appendix A.4. Therefore, the convergence (to the stationary point) rate of EDiT is $O(\\log(T) / \\sqrt{T})$.\nIn this work, we investigate the challenge of training LLMs on large-scale clusters. We analyze the fundamental characteristics of large scale clusters and the limitations of the existing Local SGD-type methods. On this basis, we propose a novel Efficient Distributed Training method for LLMs called EDiT. This method effectively integrates model sharding strategies with tailored Local SGD mechanisms. We propose layer-wise synchronization to achieve overlap of computation and communication and reduce communication and memory overhead. We enhance the convergence and stability of EDiT by introducing a pseudo gradient penalty strategy. We also present an asynchronous variant of EDiT (A-EDIT) to tackle the problem of consistent stragglers in heterogeneous clusters. Extensive experimental results demonstrate the superior capabilities of our proposed methods across multiple dimensions, and the convergence analysis provides a theoretical foundation for our method.\nSeveral potential avenues for future research are identified. First, for the A-EDiT, the stragglers negatively impact the overall performance. Mitigating the impact of these stragglers warrants further investigation. Second, our simulation of elastic training currently entails halting and restarting the training process upon node addition or subtraction. We look forward to a truly elastic framework that can swiftly adjust training resources without disrupting the ongoing training process."}]}