{"title": "Debiased Contrastive Representation Learning for Mitigating Dual Biases in Recommender Systems", "authors": ["Zhirong Huang", "Shichao Zhang", "Debo Cheng", "Jiuyong Li", "Lin Liu", "Guixian Zhang"], "abstract": "In recommender systems, popularity and conformity biases undermine recommender effectiveness by disproportionately favouring popular items, leading to their over-representation in recommendation lists and causing an unbalanced distribution of user-item historical data. We construct a causal graph to address both biases and describe the abstract data generation mechanism. Then, we use it as a guide to develop a novel Debiased Contrastive Learning framework for Mitigating Dual Biases, called DCLMDB. In DCLMDB, both popularity bias and conformity bias are handled in the model training process by contrastive learning to ensure that user choices and recommended items are not unduly influenced by conformity and popularity. Extensive experiments on two real-world datasets, Movielens-10M and Netflix, show that DCLMDB can effectively reduce the dual biases, as well as significantly enhance the accuracy and diversity of recommendations.", "sections": [{"title": "Introduction", "content": "Recommender systems are designed to predict user preferences and recommend items that might be of interest to users. They are widely used in e-commerce (e.g. by Amazon.com) (Shoja and Tabrizi 2019), streaming services (e.g. by Netflix) (Gomez-Uribe and Hunt 2015), social media platforms (Liao et al. 2022), and other online services where personalised content is crucial (Covington, Adams, and Sargin 2016; Xie et al. 2021). In an era of information, recommender systems are increasingly indispensable since they not only assist users in finding content that aligns with their preferences but also offer substantial commercial benefits (Shoja and Tabrizi 2019). For example, e-commerce platforms can significantly increase the transaction probability by recommending products of interest to users.\nTraditional recommender system models operate under the assumption that observational data is generated when a user's preference aligns with the attributes of an item. Various models employing collaborative filtering algorithms have been developed based on this premise (Liang et al. 2018; Zou et al. 2020; Ji et al. 2020). These models predict the likelihood of a user choosing an item by calculating the inner product of their respective embedding. We illustrate these models using the causal graph in Fig. 1 (a), where U and I represent user preference and exposed item, respectively, and both are causes of C, the choice. However, the traditional modelling approach can be biased towards popular items. Popularity bias leads to over-recommendation of certain items to users despite users' lack of prior interaction with similar items, thereby missing the opportunity of recommending users truly interesting items by matching user preferences and item attributes.\nBalanced representation of items in a recommender system is a common approach to addressing popularity bias (Zhao et al. 2023b; Schnabel et al. 2016). For instance, Zhang et al. (Zhang et al. 2021) proposed a novel training and inference paradigm called Popularity-bias Deconfounding and Adjusting (PDA). This method employs do-calculus (Pearl 2009; Cheng et al. 2024) to mitigate the negative effects of popularity bias during the training phase and adjusts predicted item popularity scores during inference for endowing recommendation policy with the desired level of popularity bias. PDA incorporates an item popularity node Z into the traditional recommender model, as illustrated in Fig. 1 (b). Specifically, Z \u2192 I represents that the popularity"}, {"title": "The Proposed DCLMDB Framework", "content": "We first provide the problem definition and then analyse the effects of both popularity bias and conformity bias on the effectiveness of recommendations from a causal perspective. Subsequently, we propose DCLMDB, the debiasing framework based on disentangled contrastive learning designed to simultaneously mitigate the negative impacts of popularity bias and conformity bias in recommender systems. We provide definitions/concepts of causality related to our DCLMDB framework in the Appendix due to the page limit."}, {"title": "Problem Setting and A Causal Analysis", "content": "In a recommender system, there are two main sets: V, the set of users and J, the set of items. We denote a specific user in set V as v. Within set J, we identify two types of items, p and n, where p is an item that user q has chosen, while n are many items that the user q did not choose. Let D denote user behaviour data, which can be represented as a set of triples, i.e., D = {(v, p,n)|p,n \u2208 J,v \u2208 V}. We use C to indicate whether or not a user has chosen an item. Due to the complexity of user-item interplay, the historical data D often does not reflect real user preferences and exposed items. Thus, we need to reconstruct both user preference (denoted as U) and exposed items (denoted as I) from D.\nIn this work, we simultaneously consider both item popularity and user conformity biases. To analyse the dual biases, we propose a new causal graph G, depicted in Fig. 2 (a), which explicates the factors contributing to popularity bias and conformity bias in recommender systems. In the causal graph G, we use Z to represent an item's popularity, which is considered as a latent factor. Traditional methods, such as matrix factorisation, do not explicitly model this aspect, yet it significantly impacts the effectiveness of the recommendation. W denotes conformity influence, a latent factor reflecting the behaviour of other users who choose the item. The relationships between nodes in the causal graph G are represented by edges, which are explained as follows:\n\u2022 (U, I, Z, W) \u2192 C denotes that the four edges pointing toward C from U, I, Z and W respectively, i.e., C is determined by the four factors: U, I, Z and W. Traditional recommendation methods operate under the assumption that a choice C occurs when user preference U matches with attributes of the exposed item I. In this work, we aim to learn the two latent causes of C, Z and W, to account for popularity and conformity biases. Specifically, the edge Z \u2192 C signifies that an item's popularity influences the choice. For example, a \u201cpopular movie\u201d is more likely to be chosen by a user. Similarly, W \u2192 C refers to the effect of user word-of-mouth on choice, e.g., a movie with high positive ratings is more likely to be chosen by a user.\n\u2022 W \u2192 U represents the influence of conformity on a user. This conformity effect can be detrimental to the recommender system as it may not accurately reflect the user's genuine preference. For instance, a user might conform to many other users on an online forum and choose the same movie, even though it is not their preferred type of movie.\n\u2022 Z \u2192 I indicates that the popularity of items affects their exposure. For example, on many online movie ticket websites, merchants can have their movies appear on the homepage recommendation list by purchasing certain traffic services from the website (i.e., increase item exposure). However, this does not reflect the attributes and quality of the items; it is merely a means to increase the sales of movies.\nFrom the causal graph G, we know that Z and W are confounders between (I, C) and (U, C), respectively. Thus, W affects the observed choices through the causal paths W \u2192 C and W \u2192 U \u2192 C. The path W \u2192 U \u2192 C indicates that the conformity effect contributes to a higher prevalence of certain choices, leading to what is known as conformity bias amplification (Gomez-Uribe and Hunt 2015; Chen et al. 2023). This effect is undesirable in a recommender system. Similarly, Z affects the observed choices through Z \u2192 C and Z \u2192 I \u2192 C. The path Z \u2192 I \u2192 C indicates that the popularity of an item increases its exposure, leading to a higher prevalence of popular items in observed choices. This phenomenon results in popularity bias amplification, which is undesirable in a recommender system (Zhang et al. 2021). An effective recommender system should accurately estimate a user's preferences and recommend the appropriate quality items. However, popularity and conformity biases lead to a false reflection of item quality or the user's genuine preference. Hence, they need to be mitigated to enhance a recommender system's effectiveness."}, {"title": "Mitigating the Dual Biases with Causal Inference", "content": "Guided by the proposed causal graph G in Fig. 2 (a) and the causal analysis in the previous section, we aim to design a data-driven method to mitigate both the popularity and conformity biases in recommender systems. As indicated in Fig. 2 (b), to deal with the biases, we need to remove the influence of popularity (Z) and conformity (W) on item exposure (I) and user preferences (U), respectively. To this end, we propose to perform do(I,U), i.e., the do-operation on I and U (Pearl 2009). The \u201cdo\" operator, denoted as $do(X = x)$, or $do(x)$ for short, represents an intervention where X is set to a specific value x intentionally, rather than by observing X naturally occurring at x.\nApplying the do operation on I and U removes all the edges pointing to I and U, i.e., the edges Z \u2192 I and W \u2192 U from G as shown in Fig. 2 (b) by the red crosses. We denote the manipulated graph as $G_{\\underline{U,I}}$. Note that cutting off the edges Z \u2192 I and W \u2192 U from G to obtain $G_{\\underline{U,I}}$ is equivalent to obtaining $P(C | do(U, I))$ from data. Thus, as implied by the manipulated graph $G_{\\underline{U,I}}$, we need to ensure that W and U are independent, as well as Z and I when we learn the four embeddings from D. In the following sections, we will introduce the details of our method for achieving debiased recommendation based on causal manipulation."}, {"title": "Debiased Contrastive Representation Learning", "content": "Based on the above theoretical analysis from the perspective of causal inference, we proposed the novel Debiased Contrastive Learning framework for Mitigating Dual Biases (DCLMDB) as outlined in Fig. 3.\nIt is challenging to learn both latent embeddings Z and W at the same time because, as shown in the causal graph in Fig. 2 (b), both are latent factors encoded within the user-item historical data and are influenced by the interactive nature of user-item information. To tackle this challenge, we employ contrastive representation learning (Schroff, Kalenichenko, and Philbin 2015) to learn the embeddings Z and W derived from the latent spaces of items and users, guided by the proposed causal graph in Fig. 2 (b) to effectively mitigate the dual biases in recommender systems. Contrastive learning reduces the correlation between positive and negative samples in the feature space by adjusting the model such that the distance between the anchor point and positive samples (which are drawn closer together) is smaller than the distance between the anchor point and negative samples (which are pushed further apart).\nFirst, we utilise a normal distribution to initialise the four embeddings: Z, W, U, and I. Then, we impose constraints in our DCLMDB method by ensuring that the learned Z is independent of I and W is independent of U. To achieve these independencies, contrastive representation learning is employed to distance Z (or W) from I (or U), thereby keeping Z (or W) away from the biases in I (or U) during the training phase. In our DCLMDB, I and U are used as negative samples for training Z and W through contrastive learning. We define the following similarity functions to calculate the similarity between the pairs (W, Z), (U, Z), and (W, I):\n$S_{wz} = (w_v, z_p),$\t\t\t(1)\n$S_{uz} = (u_v, z_p),$\t\t\t(2)\n$S_{wi} = (w_v, i_p),$\t\t\t(3)\nwhere $(\u00b7,\u00b7)$ denotes the dot product operation and is used to measure matching scores among the sample pairs $(w_v, z_p)$, $(u_v, z_p)$, and $(w_v, i_p)$. $u_v$ and $w_v$ represent the embeddings of user v, while $i_p$ and $z_p$ are the embeddings of the positive sample of item p (i.e., the user v selected item). We use two triplet loss functions in contrastive learning to maximise $S_{wz}$ and minimise $S_{uz}$ and $S_{wi}$, thereby reducing the distance between the anchor point and the positive samples while increasing the distance between the anchor point and the negative samples. Note that in the two loss functions, $w_v$ and $z_p$ serve as anchor points and positive samples, and $u_v$ and $i_p$ serve as negative samples. The two loss functions are defined as follows.\n$L_u = max(S_{wz} \u2013 S_{uz} + m, 0),$\t\t\t(4)\n$L_i = max(S_{wz} \u2013 S_{wi} + m, 0),$\t\t\t(5)\nwhere m is a hyperparameter. By maximising these distances, we keep $w_v$ (or $z_p$) away from $u_v$ (or $i_p$), and thereby reducing biases in $u_v$ (or $i_p$). Consequently, DCLMDB yields two embeddings, Z and W, such that is as uncorrelated with I as possible, and W is as uncorrelated with U as possible. Our DCLMDB seeks to redirect W and Z away from the biases found in U and I. Both loss functions $L_u$ and $L_i$ are used as regularisation terms in the click prediction task of the recommender system to ensure that Z (or W) and I (or U) remain independent during the training phase.\nHowever, merely distancing (W, Z) from (U, I) does not guarantee the elimination of the dual biases. Since we are distancing them in multiple dimensions, it is not certain whether the direction of their distancing is mitigating or exacerbating the bias. We use the Bayesian Personalised Ranking (BPR) loss function to guide the direction of the optimisation of the two embeddings W and Z, as well as to achieve the main task (click prediction) in the recommender system.\n$L_{BRP} = \\sum_{(v,p,n) \\in D} \\ln \\sigma ((w_v, i_p) \u2013 (w_v, z_n)),$\t\t\t(6)\nwhere $\\sigma (\u00b7)$ is the activation function, and n denotes the negative sample item (i.e., the user q unselected items). Each training instance within the BPR loss function is represented by a ternary (v, p, n). This BPR function acts as the primary loss function in the recommender system, aiding in the click prediction. We utilise the Popularity-based Negative Sampling with Margin (PNSM) strategy (Zheng et al. 2021) for selecting negative sample items. Consequently, the overall loss function of DCLMDB can be articulated as follows:\n$L_{DCLMDB} = \\alpha \u00b7 L_{BPR} + \\beta \u00b7 (L_u + L_i),$\t\t\t(7)\nwhere $\\alpha$ and $\\beta$ stand as hyperparameters. Note that $L_{DCLMDB}$ is a realistic implementation of the causal graph in Fig. 2 (b) that takes into account the reality of the click prediction task. For example, to leverage similar user behaviour for recommendations, we use the inner product operation instead of the distance metric in the latent space.\nTo summarise, we use the embeddings Z and W to separate biased input data, combined with the corresponding user v (i.e., U) and item p (or n) (i.e., I) for click prediction to conform to the causal graph assumptions as shown in Fig. 2 (b). Additionally, $L_u$ (or $L_i$) serves as a regular term to ensure that W (or Z) and U (or I) remain independent.\nBy combining the two contrastive learning loss functions with the BPR function, we obtain our ultimate loss function, $L_{DCLMDB}$. This combination enables us to learn two embeddings, Z and W, such that Z, I, W and U are mutually independent, as shown in the causal graph in Fig. 2 (b). These independences align with the core objective of estimating click probability. Such alignment strengthens the direction of debiasing representation, ensuring that our DCLMDB effectively reduces bias without inadvertently intensifying it. Furthermore, our DCLMDB, being independent of specific data and models, serves as a generalised framework that can be seamlessly incorporated into various mainstream recommendation models."}, {"title": "Experiments", "content": "In this section, we conduct experiments on two real-world datasets to evaluate the performance of DCLMDB against state-of-the-art recommendation methods. The details of the parameter settings for all methods and additional experiments are provided in the Appendix due to the page limit."}, {"title": "Experimental Settings", "content": "Datasets: We utilised two real-world datasets: the Movielens-10M dataset (Harper and Konstan 2015)\nBaseline: We compare DCLMDB with existing causal debiasing methods. Causal methods are often used as supplementary techniques alongside backbone recommendation models. In the experiments, all baseline methods use two commonly used backbone recommendation models, namely MF and LightGCN. We will compare our approach against the following six causal methods:\n\u2022 IPS (Schnabel et al. 2016): IPS addresses popularity bias in models by tackling the imbalance in the long-tail distribution within observational data. Specifically, it assigns the inverse of an item's popularity as its weight, elevating the significance of less popular items and reducing the weight of more popular ones.\n\u2022 IPS-C (Bottou et al. 2013): This method imposes a cap on the maximum value of IPS weights to reduce the variance in the overall weight distribution.\n\u2022 IPS-CN (Gruson et al. 2019): IPS-CN applies normalisation to reduce variance in weight distribution.\n\u2022 CauseE (Bonner and Vasile 2018): CauseE constructs a small unbiased dataset, and employs matrix factorisation on both datasets to derive two sets of embeddings, then applies $L_1$ or $L_2$ regularisation to align these embeddings, enforcing their similarity.\n\u2022 DICE (Zheng et al. 2021): DICE utilises Structural Causal Modeling (SCM) (Pearl 2009) to model user-item interactions as an interplay between \u201cinterest\u201d and \u201cconformity\u201d. It constructs specific training samples based on collision effects, thereby disentangling the embedding of each user and item into separate \u201cinterest\u201d and \u201cconformity\" components to eliminate conformity bias.\n\u2022 DCCL (Zhao et al. 2023a): This study critiques DICE's use of specific data to disentangle \u201cinterest\u201d and \u201cconformity\u201d, suggesting it may introduce noise. DCCL adopts contrastive learning to address data sparsity and disentangle \"interest\u201d and \u201cconformity\u201d."}, {"title": "Correlation Analysis between Popularity and Conformity Biases", "content": "In this section, we conduct experiments to analyze and demonstrate the correlation between popularity and conformity biases, thereby validating the rationality of the causal graph proposed in our experiments."}, {"title": "Experimental Settings", "content": "Parameter Settings: To ensure fair comparisons, we standardised the parameter counts across all methods. For models utilising DICE (Zheng et al. 2021) and DCCL (Zhao et al. 2023a), we set the embedding size to 64, since they comprise two concatenated sets of embeddings. For the other models, we maintained a consistent embedding size of 128. In the DCLMDB method, the hyperparameters $\\alpha$ and $\\beta$ were set to 0.05 and 0.005, respectively, for the MF-based model, and to 0.5 and 0.005 for the GCN-based model. We employed the Adam optimiser for updating model weights, with an initial learning rate of 0.001 and a batch size of 128. All models use BPR (Rendle et al. 2012) loss as the objective function for click prediction. All models were executed on an NVIDIA A100 (40GB RAM) GPU. To assess model performance and validate the effectiveness of our approach, we utilised three widely recognised metrics in the recommendation systems field: Recall, Hit Ratio (HR), and NDCG."}, {"title": "Preliminaries", "content": "In this section, we will introduce the fundamental concepts of causal inference related to our main manuscript.\nCausal graphs use Directed Acyclic Graphs (DAGs) to present causal relationships between variables, where nodes represent variables and edges represent relationships between them. Specifically, there are three classical DAGs to describe causal relationships between variables: chain A \u2192 B \u2192 C, fork A \u2190 B \u2192 C, and collider A \u2192 B \u2190 \u0421.\nIn the causal DAG A \u2192 B \u2192 C, A influencing C through intermediary B. In the causal DAG A \u2190 B \u2192 C, B is referred to as the confounder or common cause of A and C, i.e., B influences both A and C, resulting in an correlation A and C. Note that there is not imply a direct causal relationship between A and C. A \u2192 B \u2190 C is a collider structure, where A and C are independent of each other but jointly influence the collision node B. A and C exhibit correlation when conditioned on B.\nDo-calculus is a derivation system consisting of three derivation rules. Before introducing the rules, we illustrate two special subgraphs. In a causal DAG G with three arbitrarily disjoint sets of nodes X, Y, and Z, we denote by $G_{\\underline{X}}$ the graph obtained by deleting from G all arrows pointing to nodes in X. Likewise, we denote by $G_{X}$ the graph obtained by deleting from G all arrows emerging from nodes in X.\nTheorem 1 (Rules of do-calculus (Pearl 2009)). Let G be DGA associated with a causal model, and let $P(\u00b7)$ stand for the probability distribution induced by that model. For any disjoint subsets of variables X, Y, Z, and W, we have the following rules.\n1. Insertion/deletion of observations:\n$P(y | do(x), z, w)$\n$= P(y | do(x), w) if (Y \\bot Z | X, W)_{G_{\\underline{X}}}}.$\t\t\t(8)\n2. Action/observation exchange:\n$P(y | do(x), do(z), w)$\n$= P(y | do(x), z, w) if (Y \\bot Z | X,W)_{G_{\\underline{Xz}}}.$\t\t\t(9)\n3. Insertion/deletion of actions:\n$P(y | do(x), do(z), w)$\n$= P(y | do(x), w) if(Y \\bot Z | X,W)_{G_{X_{\\underline{z(W)}}}},$\t\t\t(10)\nwhere Z(W) is the set of Z-nodes that are not ancestors of any W-node in $G_{\\underline{X}}$.\nFor example, we perform do(A = a) on the causal DAG A \u2190 B \u2192 C, which denotes the intervention of setting the variable A to be a and cutting off the path B \u2192 A on the causal DAG."}]}