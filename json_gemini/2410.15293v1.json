{"title": "Fractional-order spike-timing-dependent gradient descent for multi-layer spiking neural networks", "authors": ["Yi Yang", "Richard M. Voyles", "Haiyan H. Zhang", "Robert A. Nawrocki"], "abstract": "Accumulated detailed knowledge about the neuronal activities in human brains has brought more attention to bio-inspired spiking neural networks (SNNs). In contrast to non-spiking deep neural networks (DNNs), SNNs can encode and transmit spatiotemporal information more efficiently by exploiting biologically realistic and low-power event-driven neuromorphic architectures. However, the supervised learning of SNNs still remains a challenge because the spike-timing-dependent plasticity (STDP) of connected spiking neurons is difficult to implement and interpret in existing backpropagation learning schemes. This paper proposes a fractional-order spike-timing-dependent gradient descent (FO-STDGD) learning model by considering a derived nonlinear activation function that describes the relationship between the quasi- instantaneous firing rate and the temporal membrane potentials of nonleaky integrate-and-fire neurons. The training strategy can be generalized to any fractional orders between 0 and 2 since the FO-STDGD incorporates the fractional gradient descent method into the calculation of spike-timing-dependent loss gradients. The proposed FO-STDGD model is tested on the MNIST and DVS128 Gesture datasets and its accuracy under different network structure and fractional orders is analyzed. It can be found that the classification accuracy increases as the fractional order increases, and specifically, the case of fractional order 1.9 improves by 155% relative to the case of fractional order 1 (traditional gradient descent). In addition, our scheme demonstrates the state-of-the-art computational efficacy for the same SNN structure and training epochs.", "sections": [{"title": "1. Introduction", "content": "Artificial neural networks (ANNs) have witnessed their wide applications in many aspects of modern science and technology, including tackling the image and signal processing tasks, recommender system development, object detection, and solving the robot motion planning problems [1,2]. As the third-generation ANNs, spiking neural networks (SNNs) have attracted sustained and extensive attention from the artificial intelligence community due to their biological plausibility, computational efficiency, and low power consumption in both the software simulation and hardware emulation [3-13]. In SNNs, distinct from their non-spiking ANNs, the encoding of information is accomplished through the precise timing of a series of asynchronous action potentials-commonly referred to as spikes that traverse the network of interconnected neurons. This mode of operation eschews reliance on the shape and amplitude of electrical analogue potentials, favoring a mechanism that ensures efficient and low-power signal processing [13,14].\nWithin the neuromorphic computing community, there prevails a consensus that synaptic plasticity in SNNs\u2014 characterized by alterations in synaptic weights/efficacy- is predominantly orchestrated by an unsupervised learning protocol predicated on the inherent self-correlation of spike timing, a phenomenon referred to as spike-timing dependent plasticity (STDP) [15]. Concurrently, neuroscientific study has illuminated that supervised learning, a critical framework within non-spiking ANNs, also possesses a biologically plausible counterpart in the form of STDP within SNNs, thus intimating at the expansive utility and integration of this learning paradigm across computational models [13,15-17]. However, the formulation of bio- plausible supervised learning models for SNNs remains a challenging problem due to the intricate spatiotemporal behavior of spiking neurons and the inherently non- differentiable nature of discrete spiking events. Consequently, many high-performance gradient-based supervised learning algorithms, such as the steepest descent method, stochastic gradient descent, and fractional gradient"}, {"title": "2. Method", "content": "Prior to developing our spike-based supervised learning rules, we adopt the nonleaky Integrate-and-Fire (IF) model to describe single neuron dynamics and demonstrate how the temporal activity (quasi-instantaneous firing rate) of a single neuron is related to its mean membrane potential at firing times. The relationship (or the activation function), that is established and justified in Section 2.1, initializes our steps for transforming the backpropagation-based learning rules that update localized states in conventional non-spiking neural networks to spike timing dependent plasticity of synapses in spiking neural networks. In addition, the derivation of the FO-STDGD model relies on the application of fractional gradient descent method in Section 2.2, because the fractional gradient descent method utilizes the nonlocal and hereditary properties of fractional-order derivatives to reinforce the spike-timing dependency of the novel synaptic updating rules. The complete FO-STDGD model is then derived in Section 2.3 by fully employing the activation function and the fractional gradient descent method."}, {"title": "2.1. Nonleaky IF neuron and its activation function", "content": "In the context of a nonleaky IF neuron, the membrane potential undergoes incremental growth as it accumulates the effects of incoming presynaptic spikes within the membrane capacitor. The discretized nonleaky IF neuron model presumes that the neuron can only emit action potentials (spikes) at discrete time instances, specifically at t = k\u2206t, k \u2208 N where k is a natural number and \u2206t =\n1 ms represents the simulation time step size (For simplicity, \u2206t is assumed to be a unity factor and omitted in the subsequent discrete-time models and relevant derivations). The forward propagation of two fully connected layers of SNNs is depicted in Fig. 1, with the membrane potential of the ith neuron in the lth layer at time t being expressed as follows,\n$u_i^{[l]}(t) = u_i^{[l]}(t - 1) + \\sum_j w_{ij}^{[l]}(t)s_j^{[l-1]}(t)$ (1)\nwhere $u_i^{[l]}(t)$ and $u_i^{[l]}(t - 1)$ represent the membrane potentials at two consecutive discrete time points, respectively. $w_i^{[l]}(t) = [w_{i1}^{[l]}(t), w_{i2}^{[l]}(t), ..., w_{in^{[l-1]}}^{[l]}(t)] \\in [R^{n^{[l-1]}}]$ denotes the synaptic weights, $s^{[l-1]}(t) = [s_1^{[l-1]}(t), s_2^{[l-1]}(t),..., s_{n^{[l-1]}}^{[l-1]}(t)]^T \\in [R^{n^{[l-1]}}]$ signifies the output spike vector for $n^{[l-1]}$ neurons in the (l \u2013 1)th layer at time t with $s_j^{[l-1]}(t) \\in {0,1}$. When the membrane potential $u_i^{[l]}(t)$ reaches the firing threshold voltage \u03b8, the postsynaptic spike is promptly generated, and"}, {"title": "3", "content": "simultaneously, $u_i^{[l]}(t)$ is reset to the resting potential (assume $U_{rest}$ = 0). In this context, if we represent the firing time of the ith neuron in the l th layer as $t_{i, tf}^{[l]} \\in {t \\in N| s_i^{[l]}(t) = 1}$, the firing condition for the ith neuron can be articulated as\n$(\\frac{du_i^{[l]}(t)}{dt}|_{t=t_{i,tf}^{[l]^-}} \\ge 0 ) \\land (u_i^{[l]}(t_{i,tf}^{[l]^-}) \\ge \\theta)$  (2)\nwhere \u2227 is the logic \u201cand\u201d operator, and $t_{i,tf}^{[l]^-}$, represents the pre-instantiate of the firing time $t_{i,tf}^{[l]}$. Note that if (2) is satisfied, the neuron fires a spike $s_i^{[l]}(t_{i,tf}^{[l]}) = 1$ and membrane potential is reset $u_i^{[l]}(t_{i,tf}^{[l]+}) = U_{rest}$, where $t_{i,tf}^{[l]+}$ denotes the post-instantiate of the firing time $t_{i,tf}^{[l]}$.\nFig. 1 illustrates the distinctions between our multi-layer- perceptron (MLP) based SNN and traditional non-spiking ANN. Unlike traditional non-spiking neurons with lumped parameters and localized states, the spiking neurons in our study encode spatiotemporal action potentials individually in the discrete-time domain but analyse its error propagation statistically in the continuous-time domain. As nonleaky IF neuron model (1) is described in the discrete time space, the generated spike trains can be expressed using the Kronecker delta function, where $\u03b4(t_{i,tf}^{[l]}) = 1$ if $t = t_{i,tf}^{[l]}$ and otherwise, $\u03b4(t, t_{i,tf}^{[l]}) = 0$. However, to transition the nonleaky IF model from the discrete-time domain to the continuous-time domain, we intentionally propose the temporal average membrane potential $\\hat{u}_i^{[l]}(t)$ and the quasi- instantaneous firing rate $\\hat{s}_i^{[l]}(t)$ by averaging the membrane potentials and action potentials (spikes) within a pre- selected time window \u03c4,\n$\\hat{u}_i^{[l]}(t) = \\frac{1}{\\tau} \\sum_{t_f\\in(t-\\tau,t]}u_i^{[l]}(t_f)$ (3)\n$\\hat{s}_i^{[l]}(t) = \\frac{1}{\\tau} s^{[l]}(t) = \\frac{1}{\\tau} \\sum_{t_f\\in(t-\\tau,t]} \\delta(t_{i,tf}^{[l]})$  (4)\nwhere the two summations on the right side of Equation (4) are equal because $s_i^{[l]}(t_f) = 1$ for $t = t_{i, tf}^{[l]}$ and $s^{[l]}(t)= 0$ otherwise. The time window \u03c4 is a characteristic parameter that statistically regulates the temporal activity of spiking neurons in the continuous-time domain, and is chosen from {\u03c4\u2208 N|1 <\u03c4 < T}, where T is the length of the simulation window for training the SNNs. Moreover, synaptic homeostasis plays a vital role in preserving stable and uniform computational dynamics across different neurons within the same network layer. We assert that $||w_i^{[l]}(t)||_1 < \\theta$ for all l and t \u2208 (0,T), which implies that synaptic homeostasis prevents abrupt changes in membrane potential within a single simulation time step and ensures the refractoriness of each spiking neuron. Consequently, in the forward propagation pathway of multi-layer SNNs, we can show the existence of a nonlinear (piecewise linear)"}, {"title": "4", "content": "activation function that establishes the relationship between $\\hat{s}_i^{[l]}(t)$ and $\\hat{u}_i^{[l]}(t)$ in this study.\nTheorem 1. The temporal average membrane potential and the quasi-instantaneous firing rate of a nonleaky IF neuron satisfies the activation functions $s_i^{[l]}(t) = h(\\hat{u}_i^{[l]}(t))$, i.e.,\n$s_i^{[l]}(t) = \\begin{cases}\n    \u03b3(\\hat{u}_i^{[l]}(t) -b) \\text{ if } (\\hat{u}_i^{[l]}(t) \\ge 0/\\tau)\\\\\n    0      \\text{ otherwise}\n  \\end{cases}$ (5)\nwhere \u03b3 = 0-1, 0 \u2264 b < \u03b8, and \u03b8 denotes the threshold voltage of the nonleaky IF neuron.\nProof. In a simple case, assume that there is only one spike generated by the ith postsynaptic neuron in the lth layer within (t \u2212 \u03c4,t], and suppose that the firing time $t_{i,tf}^{[l]}$ is exactly at t. Thus, $s_i^{[l]}(t) = 1/\u03c4$. Plus, (2) and (3) implies that $\\hat{u}_i^{[l]}(t) \\ge \u03b8/\u03c4$. We then define $b = \\hat{u}_i^{[l]}(t) - s_i^{[l]}(t)/\u03b3$ and immediately have b \u2265 0 for \u03b3 = 0-1. Thereafter, in order to show that b0 for \u03b4 = \u03bb/(2 \u2212 \u03b1). Additionally, \u03b5 is introduced as a small value to circumvent potential singularities arising from $X_k$ being equal to $X_{k\u22121}$. The use of the absolute value within the stepwise difference serves to enhance computational stability. Furthermore, it is noteworthy that a rigorous proof of the convergence of Equation (9) towards the real extremum of a convex function has been provided in a previous work [51][2].\nTo assess the convergence rates of the fractional gradient descent method across different \u03b1 values, we examine an objective function $f(x) = (x - 5)^2$ and employ Equation"}, {"title": "5", "content": "6  (11)\n$\\frac{\\partial L(t, \\tau)}{\\partial w_{ij}^{[l]}(t)} = \\sum_{i=1}^{n^{[o]}} \\frac{\\partial L(t, \\tau)}{\\partial s_i^{[o]}(t)} \\frac{\\partial s_i^{[o]}(t)}{\\partial \\hat{u}_i^{[l]}(t)} \\frac{\\partial \\hat{u}_i^{[l]}(t)}{\\partial s_j^{[l-1]}(t)} \\frac{\\partial s_j^{[l-1]}(t)}{\\partial w_{ij}^{[l]}(t)}$\n= $\\sum_{i=1}^{n^{[o]}} \\frac{\\partial L(t, \\tau)}{\\partial s_i^{[o]}(t)} \\frac{\\partial s_i^{[o]}(t)}{\\partial \\hat{u}_i^{[l]}(t)} \\frac{\\partial \\hat{u}_i^{[l]}(t)}{\\partial w_{ij}^{[l]}(t)}$ =  $\\sum_{i=1}^{n^{[o]}} \\frac{\\partial L(t, \\tau)}{\\partial s_i^{[o]}(t)} h'(u^{[l]}(t))s_j^{[l-1]}(t)$  (12)\nwhere h\u2032(\u00b7) denotes the first derivative of the activation function defined in (5). The second equal signs in Equations"}, {"title": "6", "content": "(11) and (12) are derived from the relationships established in Equations (5) and (1), respectively. In addition, the synaptic weights contingent upon spike-timing are subject to\n$\\frac{\\partial L(t, \\tau)}{\\partial w_{ij}^{[l]}(t)} = \\frac{\\partial L(t, \\tau)}{\\partial \\hat{u}_i^{[l]}(t)} \\frac{\\partial \\hat{u}_i^{[l]}(t)}{\\partial w_{ij}^{[l]}(t)} =  \\frac{\\partial L(t, \\tau)}{\\partial s_i^{[o]}(t)} h'(u^{[l]}(t))s_j^{[l-1]}(t)$    (13)\nwith i = 1,2,..., $n^{[l]}$, j = 1,2, ..., $n^{[l\u22121]}$. From (10), one has $du_i^{[l]}(t)/dw_{ij}^{[l]}(t) = s_j^{[l-1]}(t)$. Then, refer to (8) and (9), the fractional-order gradients in (13) can be determined as\n $\\Delta w_{ij}^{[l], k+1} =   \\frac{\\Gamma(2 - \\alpha)}{\\vert w_{ij}^{[l], (k)}(t) - w_{ij}^{[l],(k-1)}(t) + \\epsilon \\vert^{1-\\alpha}} \\Delta w_{ij}^{[l], k}(t)$ (14)\nwhere $w_{ij, (k)}^{[l]}$(t) is the value of  $w_{ij}^{[l]}(t)$ at the $k^{th}$ iteration, and  $s_{j, (k-1)}^{[l-1]}(t)$ is the value of $s_{j}^{[l-1]}(t)$ at the $(k-1)^{th}$ iteration. Moreover, as we mentioned before, \u03b5 is a small positive value to avoid potential singularities in the numerical computations. Then, combining Equations (13) and (14) yields\n$\\frac{\\partial L(t, \\tau)}{\\partial w_{ij}^{[l]}(t)} |_{(k)} = \\frac{\\partial L(t, \\tau)}{\\partial s_i^{[o]}(t)} h'(u^{[l]}(t))\\frac{s_j^{[l-1]}(t)}{\\frac{\\Gamma(2 - \\alpha)}{\\vert w_{ij}^{[l], (k)}(t) - w_{ij}^{[l],(k-1)}(t) + \\epsilon \\vert^{1-\\alpha}} } \\Delta w_{ij}^{[l], (k-1)}(t)$    (15)\nwhere $\\frac{\\partial L(t, \\tau)}{\\partial s_i^{[o]}(t)} |_{(k-1)} and \\hat{u}_{i}^{[l]}(t)|_{(k-1)}$ are the values of $\\frac{\\partial L(t, \\tau)}{\\partial s_i^{[o]}(t)} and \\hat{u}_{i}^{[l]}(t)$ at the (k \u2013 1)th iteration, respectively.\nParticularly, in the context of image classification tasks, as depicted in Fig. 4, the neurons within the input layer produce Poisson spikes with an arrival rate following"}, {"title": "6", "content": "Poisson's distribution, which is directly proportional to the pixel intensity. The input signal is then transformed into spatiotemporally defined spike events. Simultaneously, the loss function L(t,\u03c4) for image classification tasks can be defined as\n$L(t, \\tau) = \\sum_{i=1}^{n^{[o]}} \\beta_i (r_i^{[o]}(t) \u2013 s_i^{[o]}(t))^2$  (16)\nwhere \u03b2 is a weight factor associated with the loss term for the target neuron, [o] signifies the output layer of the SNN, and $n^{[o]}$ represents the total number of output units. Additionally, $r_i^{[o]}(t)$ denotes the teaching signal in the form of spikes for the ith neuron within the output layer. It is important to note that we consider $r_i^{[o]}(t) = 1$for the target\nAlgorithm 1 (FO-STDGD) Supervised learning of a fully- connected feedforward multi-layer SNN through a fractional-order spike-timing-dependent gradient descent method.\nif updated by Equation (19), converge to the real extreme point $w_{ij}^{*[l]}(t)$ that minimizes the loss function.\nProof. The following proof is given by contradiction. Assume that $w_{ij}^{*[l]}(t)$ converges to a point  $w_{ij}^{[l]}(t)$ that is\n$\\begin{split}\n     w_{ij}^{[l],(k+1)} & =  w_{ij}^{[l],(k)} - \\mu  \\cdot  \\frac{\u2202L}{\u2202w_{ij}} \\\\\n    &=  w_{ij}^{[l],(k)} - \\mu  \\cdot  h^\\prime(u^{[l]}_{i})\\frac{\u2202L}{\u2202s_{i}^{[o]}} \\cdot s_{j}^{[l-1]}  \\cdot  \\frac{\\Gamma(2 - \\alpha)}{\\vert w_{ij}^{[l], (k)} - w_{ij}^{[l],(k-1)}  + \\epsilon  \\vert^{1-\\alpha}}   \\\\\n\\end{split}$ (19)\n$\\lambda  = \\frac{\\mu \\delta}{\\Gamma(2 - \\alpha)} > 0 $. (20)\nwhere \u03bc represents the learning rate, and the loss gradient is determined by Equation (15), or, in the context of image- related tasks, Equation (17).\nTheorem 2. The synaptic weights of a fully-connected feedforward multi-layer SNN, considering the updating rule for the fractional loss gradient as presented in Equation (15),"}, {"title": "7", "content": "Meanwhile, for a sufficient small \u03b4 that satisfies 2\u03b4 < 1/\u03bb1/\u03b1, the convergence of  $w_{ij}^{[l],(k)}$ and the triangular inequality gives following relation.\n$\\begin{split}\n    \\left |  w_{ij}^{[l],(k)}- w_{ij}^{[l]}  \\right | < \\lambda^{1/\\alpha}\\\\\n    \\left |  w_{ij}^{[l],(k-1)} - w_{ij}^{[l]} \\right | < \\lambda^{1/\\alpha}\\\\\n   &  <\\left |  w_{ij}^{[l],(k)}- w_{ij}^{[l]}  \\right | + \\left |  w_{ij}^{[l],(k-1)} - w_{ij}^{[l]} \\right | < 2\\epsilon < \\lambda^{1/\\alpha} (21)\n\\end{split}$\nCombining inequalities in Equations (20) and (21) yields\n$\\left |  w_{ij}^{[l],(k+1)}- w_{ij}^{[l]} \\right | \\geq \\left |   w_{ij}^{[l],(k)} - w_{ij}^{[l],(k-1)}  \\right |$ (22)\nThe inequality expressed in Equation (22) signifies that the sequence {$w_{ij}^{[l],(k)}$},$_{k=1}^{\\infty}$ does not adhere to Cauchy criterion. In a complete vector space, such as R, a non- Cauchy sequence is inherently non-convergent. This incongruity contradicts our initial assumption that $\\lim_{k\\rightarrow \\infty} w_{ij}^{[l],(k)}(t) = w_{ij}^{[l]} \\neq w_{ij}^{*[l]}(t)$.\nRemark 2. Theorem 2 substantiates the convergence of synaptic weights towards the real minimum of the loss function when the update rule (19) is applied. This theoretical assurance promises the establishment of a novel supervised learning model, termed the Fractional-Order Spike Timing Dependent Gradient Descent (FO-STDGD), which is comprehensively delineated as Algorithm 1. The incorporation of a fractional-order loss gradient in the backpropagation process of multilayer SNN, along with the anticipated enhanced convergence efficacy as indicated by the fractional gradient descent method (8) and illustrated in Fig. 2, endows our FO-STDGD with the qualities of potentially expedited convergence rate and higher learning precision in comparison to traditional integer-order learning models."}, {"title": "3. Experiments and discussions", "content": "To assess the FO-STDGD learning model proposed in this work, we initially apply it to the well-established benchmark task of recognizing handwritten digits from the"}, {"title": "9", "content": "structure of the deep SNN consists of an input layer with 784 neurons, the first hidden layers with 500 neurons, the second hidden layer with 150 neurons and an output layer with 10 neurons. Each neuron in the output layer corresponds to a class label (0~9) in MNIST set. The corresponding hyperparameters in the training procedures are set by grid search. For instance, Fig. 5 compares the learning process of the three-layer SNN over 400 iterations under three close but different learning rates. For each iteration, a batch of sample digits is scanned sequentially, with its batch size m = 50."}, {"title": "9", "content": "different \u03b1 values (upper row \u03b1 = 1.0, lower row \u03b1 = 1.5). The study reveals that elevated \u03b1 values correlate with a quicker rise in the membrane potential of target neurons to threshold levels, thereby accelerating spike initiation relative to non-target neurons. This acceleration yields a decrease in the neuronal system's response latency by over 5 milliseconds.\nTo evaluate the generalizability of the training model and demonstrate a performance comparison with other training approaches, we increased the training session to two epochs (2 \u00d7 1200 iterations) and repeated the experiment ten times, each time randomly reselecting the initial values of the synaptic weights and randomly reshuffling the order of training samples. As a result, the average training accuracy, testing accuracy and average training loss at different \u03b1 values are presented in Table 2, with the corresponding standard deviation denoted by error bars in Fig. 9. It is shown that the distribution of the testing accuracy from repeated experiments has a very small standard deviation ( \u03c3(\u03b1 = 1.9) \u2248 0.00102), which suggests an excellent generalizability of our training model.\nIn addition to the three-layer SNN structure (784-500- 150-10), the training experiments are also carried out on a two-layer SNN with its number of hidden neurons varying from 100 to 1300 and the fractional order \u03b1 fixed to 1.9. Fig. 10 displays the classification accuracy of those distinct two-layer SNNs along the training process. In addition to the general pattern of elevated classification accuracy as the number of hidden neurons and training iterations increases, we also find that for SNNs with 1000 and 1300 hidden neurons, the best accuracy of 0.9775 is achieved with 2000 iterations if training is run for only two epochs. It suggests that single-hidden layer SNNs can already show competitive classification accuracy with sufficiently large number of hidden neurons (e.g., 1000), and there is no need to pursue a larger size of network structure."}, {"title": "10", "content": "3.2. Comparison to other models and performance analysis\nPerformance evaluation and comparison of classification models requires consideration of the training strategies employed, such as the network structure, the number of training epochs, and the spike coding schemes (rate-based or temporal-based). The impact of spike coding schemes on classification performance is omitted in this study because training models with training latency (i.e., the time duration to train SNN for each input sample) greater than 40 milliseconds (~70 ms for our simulated SNNs) have been reported to have similar classification accuracies with different spike coding schemes [53]. We thus emphasize two other features of the training models (i.e., network structure and number of epochs) and report our classification accuracy compared to other typical training approaches in Table 3. Specifically, our three-layer SNN network structure consists of two hidden layers each containing 500 and 150 neurons, which is the same network structure used in Tavanaei's BP-STDP [46], and is simpler than the network structures in Lee's backpropagated SNN [35], Mostafa's temporally coded feedforward SNN [41] and Shen's HybridSNN [54]. We find that our FO-STDGD can achieve an accuracy of 0.9764 with two training epochs and boost the accuracy to 0.9843 with ten training epochs. It is reasonable to observe such an improvement in our classification accuracy with the increase of training epochs because most of the wrongly classified samples in the early stages of training can adaptively readjust the synaptic weights of some over-activated excitatory hidden neurons in their later training epochs. Our spike-timing dependent self- tunned synapses can then achieve a similar effect as the dropout regularization technique in non-spiking neural networks and avoid the overfitting appeared in the early training stages. The self-regularization of our trained SNNs can also be observed in Fig. 10, where the rebound of test accuracy during training is gradually weakened as iteration number increases. And favorably, it is found that our three- layer SNN achieves the best classification accuracy among all the three-layer SNNs in Table 3.\nFurthermore, we demonstrate a comparison of our FO- STDGD with other learning approaches in terms of two- layer SNN architectures. It is observed that the performance of our FO-STDGD is much improved compared to the traditional unsupervised learning model (i.e., STDP), as we can identify up to 370 more images (out of 10,000 images in test set) than the STDP scheme [55]. In addition, we find that our model with 400 hidden neurons can achieve an accuracy of 0.9746 (0.9831) within two (ten) training epochs. This result indicates the superiority of our learning model over Comsa's alpha synaptic function approach [34], O'Connor's equilibrium propagation (Eq-Prop) method [56], and Wu's spatio-temporal backpropagation (STBP) approach [36]. It should be noted that Wu's STBP achieved their best accuracy of 0.9848 by introducing a convolutional layer and trained their model for 200 epochs. Meanwhile, we discover that the two-layer SNNs with 700 and 1000 hidden neurons trained by our FO-STDGD also outperform"}, {"title": "10", "content": "the SNNs with similar network structures presented in other's research, such as Mostafa's SNN with 800 hidden neurons and Zhang's SNN with 800 hidden neurons [41,44]. However, we should mention that we did not evaluate the performance of FO-STDGD under much deeper SNNs or spiking convolutional neural networks (SCNNs), because in such cases there is a higher requirement for hardware computing power and hyperparameter conditioning.\nIn Table 3, we list several training models under the use of more complex network structures. For example, Rueckauer proposed a seven-layer spiking SNN to achieve a spectacularly high accuracy of 0.9944 and Shrestha achieved an accuracy of 0.9936 with a SCNN (28x28-12c5- 2a-64c5-2a-100, represents a 6-layer SNN with 28\u00d728 input, followed by 12 convolution filters (5\u00d75), followed by 2\u00d72 aggregation layer, followed by 64 convolution filters (5\u00d75), followed by 2\u00d72 aggregation layer, and finally a dense layer connected to 10 output neurons) [57,58]. Two other impressing SCNNs were proposed by Tavanaei [59] and Kheradpisheh [60], where they achieved high accuracy of 0.9836 and 0.984, respectively. While the performance of our FO-STDGD has not been evaluated on deeper SNN architectures or SCNNs, it is essential to acknowledge the algorithm's robust adaptability. The architectural versatility of FO-STDGD is one of its standout features; the algorithm's design is fundamentally agnostic to network configurations, which enables it to adjust to the diverse and complex architectures inherent to both SCNNs and spiking recurrent neural networks (SRNNs) with efficiency."}, {"title": "3.3. Computational cost analysis", "content": "In evaluating the performance of learning algorithms, computational cost emerges as a crucial metric. This metric, denoted as $C_i$ for the i-th algorithm, is quantified by"}, {"title": "11", "content": "multiplying the mean epochs required to reach a specific error threshold with the algorithm's epoch-wise complexity, O(n). The per-epoch algorithmic complexity for our five evaluated algorithms are summarized in Fig. 11. Formally, $C_i$ is computed as [61]\n$C_i = \\frac{1}{N} \\sum_{j=1}^N \\text{argmin}(f(x) = err_j) \\cdot O(n);$  (23)\nwhere argmin() identifies the epoch at which the minimum error level is first achieved, f(x) delineates the error rate trajectory over epochs, $O(n)$ represents the algorithmic complexity in relation to the number of parameters n, and N signifies the total number of discrete error levels under consideration. By incorporating this metric, we can assess algorithms not just on their accuracy or convergence rate but also on their computational demands, thereby providing a comprehensive analysis of their practical applicability in various settings.\nThe comparative analysis delineated in Fig. 12(a) and (b) reveals that the proposed FO-STDGD outstrips competing schemes in terms of both accuracy and computational efficiency for identification of MNIST dataset. Compared to"}, {"title": "11", "content": "the other four algorithms, FO-STDGD not only attains the zenith of accuracy, surpassing 98.7%, but also showcases a significant decrease in computational costs by a minimum of 60%, a testament to its optimized design. The juxtaposition of FO-STDGD with other contenders such as BP-STDP and Eq-Prop shows that while these algorithms maintain competitive accuracy, they fall behind in computational economy. The STDP and STBP algorithms, while offering moderate accuracy, incurs a disproportionate computational cost, which could be prohibitive in scenarios with limited resources.\nFurthermore, we extend our analysis to the DVS128 Gesture dataset. As elucidated in Figures 12(c) and (d), the FO-STDGD model secures a commendable accuracy of 72%, which notably surpasses the 68.5% achieved by the subsequent Eq-Prop model. Additionally, the FO-STDGD maintains competitive computational efficiency, exhibiting only a marginal increase in cost relative to the BP-STDP algorithm. In essence, the FO-STDGD algorithm's superior performance metrics underscore its potential as the preferred choice for tasks where both precision and cost-effectiveness are paramount. This superior amalgamation of high accuracy with minimized computational cost underscores"}, {"title": "12", "content": "the algorithm's innovative edge and reinforces its suitability for extensive application in real-world computational environments where efficiency is as critical as performance."}, {"title": "4. Conclusion", "content": "In this paper, a spatiotemporally encoded supervised learning model, namely the fractional-order spike-timing- dependent gradient descent (FO-STDGD) algorithm, was developed for deep SNNs. We first derived and verified a nonlinear activation function that correlates the quasi- instantaneous firing rate and the temporal membrane potential of nonleaky IF neurons. The FO-STDGD scheme can adjust its training convergence rate by freely selecting the order of the fractional gradient between 0 and 2. We tested our learning scheme on the MNIST dataset and found that as the order of FO-STDGD increased from 0.1 to 1.9, the classification accuracy and convergence rate improved significantly. In addition, it was demonstrated that our scheme compares favorably with other training approaches as we can achieve higher accuracy (0.9843 for the three- layer SNN and 0.987 for the two-layer SNN) with less training epochs and simpler network structure. Upon evaluation using the DVS128 Gesture dataset, our FO- STDGD model attained an accuracy of 72%, which notably surpasses the performance of four other algorithms.\nThe computational cost analysis further solidified FO- STDGD's standing as an algorithm of high efficacy. It demonstrated an admirable balance of computational thrift and training performance, wherein the algorithm not only improved in accuracy with an increasing order but also did so with a noticeable reduction in the number of training epochs required. Such an outcome highlights the algorithm's proficiency in utilizing computational resources prudently while enhancing learning efficacy. This duality of benefits, when considered alongside the spike-timing dependence of the update rules and the fractional-order gradient's nonlocal influence, positions FO-STDGD as an optimal candidate for future neuromorphic applications. And these promising findings suggest a potential high-efficiency and low-power neuromorphic (hardware) implementation of deep SNNs in future work, paving the way for advancements in both algorithmic development and neuromorphic engineering."}]}