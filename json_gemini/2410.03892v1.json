{"title": "Towards Cost Sensitive Decision Making", "authors": ["Yang Li", "Junier Oliva"], "abstract": "Many real-world situations allow for the acquisition of additional relevant informa-tion when making decisions with limited or uncertain data. However, traditional RL approaches either require all features to be acquired beforehand (e.g. in a MDP) or regard part of them as missing data that cannot be acquired (e.g. in a POMDP). In this work, we consider RL models that may actively acquire features from the environment to improve the decision quality and certainty, while automatically balancing the cost of feature acquisition process and the reward of task decision process. We propose the Active-Acquisition POMDP and identify two types of the acquisition process for different application domains. In order to assist the agent in the actively-acquired partially-observed environment and alleviate the exploration-exploitation dilemma, we develop a model-based approach, where a deep generative model is utilized to capture the dependencies of the features and impute the unobserved features. The imputations essentially represent the beliefs of the agent. Equipped with the dynamics model, we develop hierarchical RL algorithms to resolve both types of the AA-POMDPs. Empirical results demonstrate that our approach achieves considerably better performance than existing POMDP-RL solutions.", "sections": [{"title": "1 Introduction", "content": "Recently, machine learning models for sequential decision making have made significant progress due to the development of reinforcement learning (RL). These models have achieved remarkable success in many application domains, such as games [1, 2, 3], robotics control [4, 5, 6, 7, 8] and medical diagnosis [9, 10, 11, 12]. However, the current RL paradigm is incongruous with the expectation of many real-world decision-making systems. First, for fully-observed Markov decision processes (MDPs), the features at each decision step are assumed to be fully observed. In situations like medical diagnosis, some features, such as MRI, might be expensive to obtain; some features might even pose a risk to the patient, such as X-Ray. Furthermore, acquiring all features at each step may create redundancy, as some features will not change within the adjacent time frames. Therefore, the intelligent decision-making systems are expected to automatically balance the cost of feature acquisition and the improvement of decision by acquiring only the necessary information. Second, for partially-observed Markov decision processes (POMDPs), the observation at each step is determined by an unknown observation model of the environment, thus no additional information (features) may be obtained to improve the decision.\nIn stark contrast to the current RL paradigm, human agents routinely reason over instances with incomplete features and decide when and what additional information to obtain. For example, consider clinicians in intensive care units (ICUs), which have to make sequences of treatment decisions for patients at risk. Typically, all of the (dynamic) patient information is not known, however, and while knowledge of the patient is critical when deciding what treatment decisions to make, due to time/cost/risk constraints the clinician must carefully decide what additional patient attributes (e.g."}, {"title": "2 Active Acquisition Partially-Observed Markov Decision Process", "content": "A discounted AA-POMDP is an environment defined by a tuple $M = (S, A,T,O, R, C, \\gamma)$, where $S$ is the state space and $A = A_f \\cup A_c$ is the joint action space of feature acquisition actions $A_f$ and task control actions $A_c$. $T : S \\times A \\rightarrow S$ represents the state transition kernel, which can be deterministic or stochastic. As in ordinary POMDP, the observation space $O$ is related to $S$ by the observation/emission model $p(o | s', a)$, which defines the probability of observing $o$ when the agent"}, {"title": "3 Methods", "content": "In this section, we first develop a generative model to capture the dependencies of features along the state transition trajectory. The sequential generative model is leveraged afterwards to impute the missing features, which represent the belief of the agent. We then construct the feature acquisition policy and the task control policy in a hierarchical way based on the belief estimation."}, {"title": "3.1 Partially Observed Sequence Modeling", "content": "Let $t \\in {1,..., T}$ denote a time step where state transition happens due to the execution of task action $a_c^{(t)}$ in the environment. At each time step t, our agent has acquired a subset of features $x_v^{(t)}$ from underlying state $s^{(t)}$, and the features $x_u^{(t)}$ remain unobserved to the agent. In order to model the state transitions and estimate the beliefs about the underlying state, we build a generative model to impute the unobserved features conditioned on the observed ones and the action sequence:\n$p(x_{u}^{(1:T)} | x_{v}^{(1:T)}, a_c^{(0:T-1)})$.\nTo simplify notation, we denote $a_c^{(0)}$ as a dummy action that initializes the environment. Note that the conditionals could be evaluated on an arbitrary subset of features since the agent may acquire different features for different instances. The conditionals essentially capture dependencies between the subset of features and across time steps.\nOne way to model the conditional in (3) is to exploit its sequential nature and factorize it by\n$p(x_{u}^{(1:T)} | x_{v}^{(1:T)}, a_c^{(0:T-1)}) = \\prod_{t=1}^T p(x_{u}^{(t)} x_{v}^{(t)} | x_{u}^{(1:t-1)}, x_{v}^{(1:t-1)}, a_c^{(0:t-1)})$. A sequential latent variable $z^{(t)}$ can be introduced to simplify the model as in many sequential VAEs [15, 16, 17, 18]. Please see Fig. 1(a) for an illustration. However, the sequential formulation has several drawbacks: First, the latent variable is updated sequentially, which means the latent only depends on previous time steps, therefore the training signals coming from later time steps cannot be leveraged. Second, due to the limitation of recurrent models, the previous time steps might not have significant influence at the current time step, especially when the episode is long. Third, in order to make a prediction at a distant time step, the model has to unroll the latent multiple times, which could make the error accumulated and result in erroneous predictions.\nIn order to overcome those drawbacks, we draw inspiration from set modeling [19, 20, 21, 22, 23] and Transformer [24, 25, 26] literature and formulate our generative modeling task in (3) as a conditional set generation problem. Specifically, we concatenate the time index with the corre-sponding features and actions as a tuple and then the sequence becomes a permutation invariant set $\\{(t, x_v^{(t)}, x_u^{(t)}, a_c^{(t-1)})\\}_{t=1}^T$. We can then reformulate (3) as\n$p(\\{x_{u}^{(t)}\\}_{t=1}^T | \\{(t, x_v^{(t)}, a_c^{(t-1)})\\}_{t=1}^T) = p(x_u | a_{x_v}),$\nwhere we denote $x_u := \\{x_{u}^{(t)}\\}_{t=1}^T$ and $a_{x_v} := \\{(t, x_v^{(t)}, a_c^{(t-1)})\\}_{t=1}^T$ for notation simplicity. Our Partially Observed Set models for Sequences (POSS) precisely overcomes the shortcomings of the aforementioned sequential generative models. Based on the set formulation, we can now draw samples at arbitrary time points without having to rolling out the sequence step-by-step. During training, later time steps can propagate gradients to early ones and even distant time points can influence each other. Please see Fig. 1(b) for an illustration.\nTo learn the conditional distribution over sets, we employ De Finetti's theorem [27, 28, 29, 21] and introduce a latent variable z. Given the latent variable, the conditionals can be decomposed:\n$p(x_u | a_{x_v}) = \\int \\prod_{t=1}^T p(x_{u}^{(t)} | z, t, x_v^{(t)}, a_c^{(t-1)})p(z | a_{x_v})dz.$"}, {"title": "3.2 Belief State Estimation", "content": "In order to solve the aforementioned AA-POMDP, our agent will need to determine an optimal acqui-sition plan and an optimal task action sequence based solely on the partially observed information. Fortunately, the sequential generative model can impute the missing features and thus estimate the belief about the underlying state.\nAt any specific time step h, suppose the agent has executed task actions $a_c^{(<h)} = \\{a_c^{(i-1)}\\}_{i=1}^h$ resulting in the underlying state $s^{(h)}$, the agent has access to the observation history $o^{(<h)} = \\{x_v^{(i)}\\}_{i=1}^{h-1}$. The acquisition sub-policy will begin with $x_v^{(h)} = \\emptyset$. In the sequential setting $x_v^{(h)}$ shall be updated with each acquisition sub-step (with the acquired feature values from state $s^{(h)}$); in the batch acquisition setting, the observation $x_v^{(h)}$ is updated only once after all specified acquisitions are made. Given the available information, we utilize the sequential generative POSS model (\u00a73.1) to predict the unobserved features for state $s^{(h)}$, i.e., $x_u^{(h)}$. We first sample a latent code from the prior $p(z | \\{(i, x_v^{(i)}, a_c^{(i-1)})\\}_{i=1}^h)$, then pass the latent code through the decoder only for state $s^{(h)}$ to obtain the distribution $p(x_u^{(h)} | z, t, x_v^{(h)}, a_c^{(h-1)})$, to sample the unobserved features $x_u^{(p)}.$"}, {"title": "3.3 Cost Sensitive Reinforcement Learning", "content": "Given the sequential transition model and the belief estimates, we now build the RL agent to solve the AA-POMDP. We decompose the agent into two policies, the feature acquisition policy $\\pi_f$ and the task policy $\\pi_c$, which are then combined in a hierarchical fashion. Both policies take the belief estimation set $b^{(h)}$ as inputs and derive the action distribution in a permutation invariant manner.\nAt any underlying state $s^{(h)}$, we first run the feature acquisition policy $\\pi_f$ to collect information. The features are acquired either in a batch or one-by-one depending on the acquisition setting. In the batch acquisition setting, the acquisition policy is ran once to determine the set of features to be acquired, while in the sequential acquisition setting, the acquisition policy is run multiple times sequentially. The belief estimations are updated after acquiring the features. The task policy $\\pi_c$ is then executed based on the updated belief using the acquired features (see Fig. 2 for illustrations).\nOur goal in the AA-POMDP is to maximize the task reward while minimizing the feature acquisition cost. In the hierarchical setting, we decompose the goal as well. The high-level task policy aims at achieving as high task reward as possible based on the acquired features, while the low-level acquisition policy aims at providing sufficient information and minimizing the acquisition cost.\nTherefore, the reward for the task policy at time step h is defined as $r_c^{(h)} = R(s^{(h)}, a_c^{(h)})$, which is the same as the original task reward. For the acquisition policy, in addition to the acquisition cost, we desire the acquired features to support the task policy. First, the task policy should produce confident action choice based on the acquired features. Therefore, we use the negative entropy of the task policy as a reward to the acquisition policy, i.e., $-Ent(\\pi_c(b^{(h)}))$. Second, the acquired features as inputs to the task policy should lead to a high value estimation indicating high long-term return of the task policy. Therefore, we employ the task value estimation, $V_c(b^{(h)})$, as an additional reward. Third, the acquired features should be indicative of the unobserved features so that the belief estimation is accurate. We hence use the imputation accuracy as one of the acquisition rewards, i.e., $Acc(x_u^{(h)}, x_v^{(h)}, x_u^{(p)})$. For discrete features, the accuracy is evaluated as the average exact mach accuracy of the N belief samples. For continuous features, the accuracy is evaluated as the average negative MSE of the N belief samples. In total, the reward for the acquisition policy at time step h is defined as\n$r_f^{(h)} = -C(s^{(h)}, a_f^{(h)}) - w_e \\cdot Ent(\\pi_c(b^{(h)})) + w_v \\cdot V_c(b^{(h)}) + w_a \\cdot Acc(x_u^{(h)}, x_v^{(h)}, x_u^{(p)})$,\nwhere $w_e$, $w_v$ and $w_a$ are hyperparameters for weighting the corresponding terms. In the batch acquisition setting, all features in $x_v^{(h)}$ are acquired simultaneously, thus the rewards are received immediately after the acquisition. In the sequential acquisition setting, however, each acquisition"}, {"title": "3.4 Implementation", "content": "In this section, we describe several important implementation details. We use PPO [32] for both the acquisition policy and the task policy. The actor and the critic networks are implemented as an ensemble over the belief sets, where the action probabilities and values are averaged over the belief set elements. The sequential generative model is implemented as a VAE with normalizing flow based posterior and prior, of which the base distribution are Gaussian conditioned on the corresponding sets. We use Set Transformers [33] to extract the set representations. The time indexes are embedded using sinusoidal functions as in other Transformer models [34]. For continuous actions and features, we directly concatenate them. For discrete actions and features, we learn their embeddings jointly. During training, we first train the sequential generative model with trajectories obtained by randomly acquired features and random task actions; then we pre-train the task policy with fixed generative model and random acquisitions; finally we train the generative model and both policies jointly."}, {"title": "4 Related Works", "content": "Active Feature Acquisition and Active Perception Active feature acquisition is a relevant field where features are actively acquired with costs to predict the target variable. Previous works [35, 36, 37, 38, 39] have formulated the AFA problem as MDP and have developed various of reinforcement learning approaches to find the optimal acquisition plan for a given instance. [40] further propose a model-based solution by leveraging ACFlow [41] to model the AFA dynamics. The learned dynamics then assists the agent by providing auxiliary information and intrinsic rewards. [42] propose to learn the acquisition policy using augmented data sampled from a pretrained Partial VAE [43]. [44] and [45] instead employ the imitation learning approach guided by a greedy reference policy to learn the acquisition policy. In addition to RL based approaches, [46], [47] and [48] propose decision tree, naive Bayes, and maximum margin based classifiers, respectively, to jointly minimize the misclassification cost and feature acquisition cost. [43], [49] and [50] propose to acquire features greedily using mutual information as the estimated utility. In contrast to the existing AFA works, our setting does not contain a specific target variable; instead, we focus on optimizing the cumulative reward of MDP. Furthermore, our agent learns the feature acquisition policy and the task policy simultaneously. Active perception is a relevant sub-field where a robot with a mounted camera is planning by selecting the best next view [51, 52, 53, 54].\nPOMDP and Temporal Dynamics Modeling Learning in POMDP without access to the environ-ment model is much more difficult than learning in MDP [55]. Many works, therefore, have focused on planning in POMDP with a known environment model [56, 57, 58, 59, 60, 61, 62, 63, 64, 65]. Bayes-Adaptive POMDP [66, 67, 68] instead learn the environment model in a Bayesian fashion by assuming access to an informative prior over the observation model and plan using posterior belief distributions over states. Instead of planing with an environment model, Deep Recurrent Q-Networks (DRQN) [69] and its variants [70] parameterize the value function with a recurrent neural network that takes in the action and observation history. The value function is later learned using the DQN RL algorithm [1]. Deep Variational Reinforcement Learning (DVRL) [16] uses the action and observations history to learns a VAE model, where the latent variable is interpreted as the belief. The A2C RL algorithm [71] is then applied on the latent representation and trained together with the generative model. TD-VAE [72] builds a VAE model to predict the belief state for time points separated by random intervals. Their jumpy state modeling enables the prediction of belief at arbitrary future time without the step-by-step rollout.\nOutside of POMDP literature, there is a number of works that consider jumps when modeling temporal dynamics. [73] and [74] equip recurrent neural network with skip connections, which makes it easier to bridge distant time steps. [75] temporally sub-sample the data with fixed jump interval and build models on the subsampled data. One of the limitations of the subsampling is that the model cannot leverage information contained in the skipped observations. [76] and [77] predict sequences with variable time-skips, by choosing the most predictable future frames as target.\nDue to the feature acquisition, our setting makes the agent observe only a subset of features, thus following a POMDP, but with the difference that the observation is controlled by the agent itself"}, {"title": "5 Experiments", "content": "We evaluate batch acquisition and sequential acquisition scenarios on several benchmark environments. For context, we provide the rewards stemming from a task policy on fully_observed states. Additionally, we also tested a typical POMDP setting where a random subset of features were observed (random*). We vary the cost per acquisition to demonstrate the trade-off between acquisition cost and task reward. We evaluate both the batch acquisition setting and sequential acquisition setting with our proposed cost-sensitive hierarchical PPO (CS-HPPO) (\u00a7 3.3), where the belief state is estimated by POSS (\u00a7 3.1). In order to verify the benefits of our belief estimation, we compare to the variants that replace belief with the observation history, which is a typical practice in POMDP literature [85]. I.e., PPO agents select an action at each step based on either the belief estimation (belief) or the observation history (hist). In-spired by [81], we compare our batch acquisition models to a setting where the action space is the Cartesian product of task control ac-tions and feature acquisition actions (joint*). In this setting, the acquisition action controls what will be observed in next state. For sequential acquisition setting, we also compare to a setting that con-catenates feature acquisition actions and task control actions into a larger action space (concat*). We attempted to evaluate a generic POMDP-RL algorithm, DRQN [69], in the setting of concatenated action space as well, but found that it fails to learn effective acqui-sition policy. Please find more details in Appendix B.\nPartially Observed CartPole First, we evaluate on a modified OpenAI gym CartPole-v1 environment, where the features of a state can be dynamically acquired with a cost. In the batch acquisition setting, the action space contains 16 acquisition actions and 2 task control actions, while in the sequential acquisition setting, the ac-quisition action spaces contains the four measurable features plus a termination action.\nSepsis Simulator This environment simulates a Sepsis patient and the effects of several com-mon treatments [86]. The task is to apply three treatment actions, antibiotic, ventilation and vasopressors, to the ICU patients. Therefore, the task action space is the powerset of the three"}, {"title": "6 Conclusion", "content": "In this work, we study the sequential decision making problems with feature acquisition costs. We present a special MDP named AA-POMDP and identify two types of the feature acquisition settings, batch acquisition and sequential acquisition, which are applicable under different conditions. To help solve the partially observed problem, we develop a sequential generative model to capture the state transitions multiple imputation of the unobserved features. The agent then takes a set of imputed observations as the belief estimation. In order to balance the acquisition cost with the task reward, we propose a hierarchical formulation of the policy, where the low-level policy is responsible for acquiring features and the high-level policy maximizes the task reward based on the acquired feature subsets. The entire framework, including both the generative model and two levels of the policies, is trained jointly. We conduct extensive experiments and demonstrate state-of-the-art performance."}, {"title": "A Partially Observed Set Models for Sequences (POSS)", "content": "As discussed in Sec. 3.1, we formulate the partially observed sequence modeling task (3) as a set modeling task (4) for a set $a_{x_v} := \\{(t, x_v^{(t)}, x_u^{(t)}, a_c^{(t-1)})\\}_{t=1}^T$. According to De Finetti's theorem, there exists a latent code z such that the set elements are conditionally independent conditioned on z, i.e.,\n$p(a_{x_v}) = p(\\{(t, x_v^{(t)}, x_u^{(t)}, a_c^{(t-1)})\\}_{t=1}^T)$\n$= \\int \\prod_{t=1}^T p(t, x_v^{(t)}, x_u^{(t)}, a_c^{(t-1)} | z) p(z)dz$\n$= \\int \\prod_{t=1}^T [p(x_u^{(t)}|t, x_v^{(t)}, a_c^{(t-1)}, z)p(t, x_v^{(t)}, a_c^{(t-1)} | z)] p(z)dz$\n$= \\int \\prod_{t=1}^T [p(x_u^{(t)}|t, x_v^{(t)}, a_c^{(t-1)}, z)] \\prod_{t=1}^T [p(t, x_v^{(t)}, a_c^{(t-1)} | z)] p(z)dz$\n$= \\int \\prod_{t=1}^T [p(x_u^{(t)}|t, x_v^{(t)}, a_c^{(t-1)}, z)] p(\\{(t, x_v^{(t)}, a_c^{(t-1)})\\}_{t=1}^T | z)p(z)dz$\n$= \\int \\prod_{t=1}^T [p(x_u^{(t)}|t, x_v^{(t)}, a_c^{(t-1)}, z)] p(z | \\{(t, x_v^{(t)}, a_c^{(t-1)})\\}_{t=1}^T)p(\\{(t, x_v^{(t)}, a_c^{(t-1)})\\}_{t=1}^T)dz$\n$= \\int \\prod_{t=1}^T [p(x_u^{(t)}|t, x_v^{(t)}, a_c^{(t-1)}, z)] p(z | a_{x_v})p(a_{x_v})dz.$\n(A.1)\nThe equation (1) applies the De Finetti's theorem again. Since $x_u^{(t)}$ contains a subset of features at time step t, the same latent variable z that factors the set element $(t, x_v^{(t)}, x_u^{(t)}, a_c^{(t-1)})$ conditionally independent will also factor $(t, x_v^{(t)}, a_c^{(t-1)})$ conditionally independent.\nDivide both sides with $p(a_{x_v}) := p(\\{(t, x_v^{(t)}, a_c^{(t-1)})\\}_{t=1}^T)$, we have\n$p(x_u|a_{x_v}) = \\int \\prod_{t=1}^T [p(x_u^{(t)}|t, x_v^{(t)}, a_c^{(t-1)}, z)] p(z | a_{x_v})dz.$\n(A.2)\nTo optimize A.2, we resort to the variational approach and optimize a lower bound (6). The prior $p(z | a_{x_v})$ and posterior $q(z | a_{x_v})$ are permutation invariant w.r.t. their inputs $a_{x_v}$ and $a_{x_v}$ respectively. To obtain accurate estimations of the prior and posterior, we utilize normalizing flow based distributions where the base distributions are parameterized as Gaussian distributions with mean and variance derived from $a_{x_v}$, and $a_{x_v}$ using Set Transformers. Due to the permutation invariant architecture of Set transformer, the Gaussian base distribution is permutation invariant; and since the transformations are invertible, the ultimate normalizing flow based distributions are permutation invariant as well. Please see Fig. A.1 for an illustration of our proposed POSS model."}, {"title": "B Experiment", "content": "In this section, we evaluate both the batch acquisition setting and the sequential acquisition setting with our proposed cost-sensitive hierarchical PPO (CS-HPPO). The following are a list of settings we experimented:\nFully Observed In this setting, the agent only needs the task policy $\\pi_c$ since all features will be observed at each time step. The task policy takes the full observation as input and no belief estimation is needed either.\nRandom Acquisition Since there will not be an acquisition policy, we cannot control the number of acquisitions using cost. Instead, we set a fixed budget so that the agent can observe part of the"}]}