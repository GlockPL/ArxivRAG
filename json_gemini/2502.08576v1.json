{"title": "Mapping the Landscape of Generative AI in Network Monitoring and Management", "authors": ["Giampaolo Bovenzi", "Francesco Cerasuolo", "Domenico Ciuonzo", "Davide Di Monda", "Idio Guarino", "Antonio Montieri", "Valerio Persico", "Antonio Pescap\u00e9"], "abstract": "Generative Artificial Intelligence (GenAI) models such as LLMS, GPTs, and Diffusion Models have recently gained widespread attention from both the research and the industrial communities. This survey explores their application in network monitoring and management, focusing on prominent use cases, as well as challenges and opportunities. We discuss how network traffic generation and classification, network intrusion detection, networked system log analysis, and network digital assistance can benefit from the use of GenAI models. Additionally, we provide an overview of the available GenAI models, datasets for large-scale training phases, and platforms for the development of such models. Finally, we discuss research directions that potentially mitigate the roadblocks to the adoption of GenAI for network monitoring and management. Our investigation aims to map the current landscape and pave the way for future research in leveraging GenAI for network monitoring and management.", "sections": [{"title": "I. INTRODUCTION", "content": "BECAUSE of breakthroughs achieved in the last decade, Generative Artificial Intelligence (GenAI) stands as one of the most important stepping stones toward the intelligence era. At its core, GenAI excels in (i) distilling features of complex data distributions (uncovering intricate patterns) and (ii) utilizing these features to generate new, similar, yet distinct data. This contrasts with the usual discriminative Artificial Intelligence (AI) models that focus on analyzing, interpreting, and classifying data to solve specific inference tasks. This twofold ability (i.e., complex analysis and generation) positions GenAI as a crucial technology in advancing both scientific research and industrial applications. Accordingly, GenAI supports tools designed to generate new content-text, images, videos, and more-based on patterns and information learned from large datasets.\nAt a higher abstraction level, such capabilities showcase GenAI as a powerful tool to solve intelligence-level tasks that are common to different domains: content generation, data augmentation, conversational agents and question-answering tools, human-machine interactions, and automation. Noteworthy examples of novel GenAI models are represented by Large Language Models (LLMs), Diffusion Models, and State Space Models (SSMs). To specify, LLMs are language models built on the Transformer architecture, and they are referred to as \"large\" due to their vast number of parameters. Hereinafter, we use the terms \u201cLLM\u201d and \u201cTransformer\u201d synonymously to indicate the AI model [1]. Notable examples for these novel GenAI solutions are represented by GPT and LLAMA for LLM, DALL-E and Stable Diffusion for Diffusion Models, and Mamba for SSM. These models have demonstrated significant commercial value and technical potential. They show notable reasoning, generalization, and emergent abilities in different applications, like text-to-text, text-to-image, and text-to-code. As a consequence of such potential, the global GenAI market stood at just under 45 billion USD at the end of 2023 (doubling its value compared to 2022), and forecasts indicate an impressive growth of \u2248 20 billion USD per-year through 2030 [2].\nThe rapid development of GenAI has been fueled by three main drivers: (i) the availability of large-scale data corpora; (ii) methodological advancements in the AI field, i.e., the shift toward deep and foundational generative models; (iii) technological innovations supporting model building, i.e., high-performance massive Graphics Processing Units (GPUs). Notably, despite these drivers, only a few global stakeholders (to date) are capable of training GenAI models from scratch. Hence, pre-trained large models are beginning to be shared by the open-source part of the GenAI community.\nOn the other side, recent networking research has focused on using Deep Learning (DL) to develop efficient tools for Network Monitoring and Management (NMM) to meet modern Internet traffic needs. In this respect, GenAI can empower intent-based and autonomous networks by automating the translation of user objectives into actionable network policies [3]. This allows networks to self-configure, self-optimize, and self-heal, improving responsiveness and resilience. By leveraging GenAI's predictive capabilities, networks can indeed anticipate traffic patterns and issues, ensuring seamless operation. This reduces manual management complexity, accelerates innovation, and enhances user experience in a dynamically changing digital landscape. However, the full utilization of GenAI for NMM requires shifting from common text, audio, and image generation to network-focused synthetic content-fulfilling the concept of \"AI-generated everything\" [4]. Despite the interest in integrating GenAI into networks and the Internet (trying to echo similar breakthroughs obtained in verticals such as"}, {"title": "A. Contributions and Survey Organization", "content": "This article deepens the technical understanding of GenAI within the context of NMM. Accordingly, the main contributions provided by this manuscript can be summarized as follows:\n\u2022 we discuss the motivation behind our \"GenAI landscape mapping\u201d effort in the field of NMM, highlighting the shared interest in GenAI from different stakeholders, as well as the gap in the (quickly-evolving) scientific literature we aim to fill with our work (Sec. II);\n\u2022 we present a categorization of novel GenAI methods, offering the necessary background to help readers understand the distinctive aspects of NMM-specific research efforts and applications (Sec. III);\n\u2022 we offer a use-case-centric viewpoint, discussing each practical NMM use case and its interplay with GenAI (Sec. IV), along with a model-centric viewpoint (Sec. V) to obtain a nuanced perspective. In addition, for the newly-branded GenAI solutions, we detail the proposed modifications to reference GenAI architectures and their code availability.\n\u2022 we provide a comprehensive view of the public datasets leveraged for GenAI model lifecycle and the available computing platforms that can support and accelerate the design of novel GenAI-based NMM solutions (Sec. VI);\n\u2022 finally, we briefly wrap-up the current GenAI limitations and identify potential methodological/technological enablers for deploying it safely and at scale in the NMM field (Sec. VII)."}, {"title": "II. MOTIVATION OF GENAI IN NETWORK MONITORING AND MANAGEMENT: CONTEXT AND RELATED WORKS", "content": "In this section, we examine the increasing interest from both public and private stakeholders in using GenAI to support NMM processes (Sec. II-A). Next, we discuss related surveys that analyze the impact of GenAI methods in the networking domain II-B. Finally, we outline the positioning and scope of this survey (Sec. II-C)."}, {"title": "A. GenAI in Networking: Context", "content": "The huge and general interest in GenAI solutions also maps to the networking domain, where recent initiatives reflect the endeavors of several private and public stakeholders. Table I provides an overview of this interest, reporting the efforts of different stakeholders in the context of GenAI for NMM.\nFor instance, the current interest in GenAI is witnessed by the recent establishment of IEEE ComSoc Emerging Technology Initiative on Large Generative AI Models in Telecom (GenAINet) [13]. ACM SIGCOMM has already featured several online talks in which experts have discussed the huge interest in the application of GenAI to NMM (and, in general, to networking) [7]. Similarly, the International Telecommunication Union (ITU) via its initiative \"AI for Good\" is showcasing both industry- and academic-oriented viewpoints, as well as first LLM-based challenges [15]. Trending interest is also observed at the IETF, with a first side meeting entirely dedicated to the use of LLMs in networking [14]. It is further witnessed by the latest academic networking conferences and workshops that stably include the application of GenAI among the topics of their call, consistently seeking contributions in this direction (e.g., IEEE GLOBECOM 2024 will feature both dedicated workshops and symposia centered on GenAI).\nAt the governmental level, the EU has launched a strategy for developing GenAI models over the past two years, highlighted by the EIC Accelerator funding program under the Horizon Europe framework aimed at supporting start-ups and small-medium enterprises [11]. Specifically, one of the 2024 challenges, \"Human Centric Generative AI made in Europe\" (50 million EUR budget), aims to promote a European human-centric approach to GenAI, addressing issues like transparency and trust, and seeking to (i) advance foundation language and multimodal frontier models, while also focusing on (ii) smaller foundation models with high performance in specific domains-like the case of this paper.\nNetwork providers have also attempted to capitalize on the benefits of GenAI. For instance, Bell Labs acknowledges the benefits of GenAI, classifies several use cases (in areas such as customer care operations, network design, network performance and optimization, and testing), and envisions their expected role in shaping the future of organizations and functions of Telecom service providers [16]. Huawei has recently launched Net Master [12], an innovative network large model powered by GenAI that aims to enhance the efficiency of network operations and maintenance. This solution is trained using Huawei's Pangu models (i.e., different foundation models tailored to different domains or specific use cases) and it is based on a 50-billion-level corpus and the experience of more than 10k networking experts. According to Ericsson, in the Telecom domain, the integration of GenAI capability to convert natural language to SQL and to execute complex SQL queries enables seamless interaction between users and data, replacing the traditional search process with a more intuitive and conversational experience [10]. This capability allows Telecom companies to empower users to effortlessly access and analyze data, easily supporting data-driven decisions. Telefonica has partnered with Microsoft to integrate Azure GenAI in its digital ecosystem, enhancing its capabilities for key workflows, such as customer identity management or access to network Application Programming Interfaces (APIs) [17]. Similarly, AT&T has launched Ask AT&T a GenAI tool based on OpenAI's ChatGPT, integrated within a secure AT&T-dedicated Azure environment [8]. This tool aims to enhance employees' productivity by translating documents, optimizing network operations, updating legacy software, and improving customer support. On the same line, Cisco proposed its AI Assistant for accessing data at large scale to guide and inform human decision-making and enhance productivity while guaranteeing data protection and privacy [9]. Lastly, TIM is exploring the integration of GenAI across various sectors (e.g., marketing, customer care, network operations). The aim is to support customer service and technical operations through conversational interfaces, enhance document search and summarization, assist in code generation for IT tasks, and improve data analysis with natural language queries [18]."}, {"title": "B. Related Surveys and Overviews on GenAI in the Networking/Telco Domain", "content": "Given the enormous hype surrounding GenAI techniques, a large number of recent surveys and tutorial-style studies aim to analyze and discuss their impact within the wide domain of networking. These works contribute to defining a rich but equally fragmented picture. Indeed, the available studies are characterized by different focuses, scopes, and depths in the provided pictures of the state of the art. Thus, they result in identifying (i) different vertical application fields, use cases, and networking tasks that can benefit from the (rapid) progress in GenAI, as well as (ii) different families of AI tools. Such studies and related aspects are summarized in Tab. II and briefly discussed in the following.\nThe majority of the works aim to analyze the role of GenAI in the fields of the Internet of Things (IoT) and/or cybersecurity [19-22, 29]. For instance, Sai et al. [19] explore the potential of combining GenAI with IoT, which enables the generation of synthetic data that can be used to train DL models to overcome data insufficiency or incompleteness in IoT systems. Ferrag et al. [29] provide a comprehensive survey of LLMs for cybersecurity identifying 9 application fields: threat detection and analysis, phishing detection and response, incident response, security automation, cyber forensics, chatbots, penetration testing, security protocol verification, and security training and awareness. Although the authors offer an in-depth analysis of the potentiality of LLMs for cybersecurity, the potential application fields they identify are not fully centered on networking and do not consider several promising LLM applications in this domain. Hassanin and Moustafa [20] overview the recent progress"}, {"title": "C. Positioning and Survey Scope", "content": "In light of the rich but scattered literature scenario, we position the present work against the existing surveys and overviews in terms of the scope of the applications and tools considered, as well as the provided outcomes of the analyses.\nTo the best of our knowledge, none of the considered studies surveying the impact of recent advancements in GenAI primarily focuses on network monitoring and management. In fact, the studies that are primarily centered on networking [3, 26, 27] share a focus that is slightly close to ours. However, while envisioning the great potential of GenAI in networking, they lack a detailed survey and taxonomization of the current landscape, being aimed at providing only a general overview based on the analysis of a very limited number of works (indeed, these studies reference 15 papers each in their bibliography). On the other hand, the studies that provide a more systematic and in-depth analysis of the literature [23, 25] emphasize different facets of the communication networks, being oriented at capturing telecommunication aspects placed at lower layers in the communication stack\u2014e.g., RAN improvement, mobile-network management, channel state information prediction, prediction-based beamforming. Hence, we believe they provide a view that is complementary to ours.\nIn this survey, we explore 5 use cases: (i) network traffic generation, (ii) network traffic classification, (iii) network intrusion detection, (iv) networked system log analysis, and (v) network digital assistance, which are crucial for network monitoring and management and are mostly overlooked in other such surveys.\nUnlike all the related surveys, we perform an in-depth analysis of each mentioned use case aimed at identifying and providing taxonomies of the solutions proposed in the networking domain. Specifically, we report for each task the adopted GenAI architecture, its public availability, the input fed to the model, and the dataset leveraged for its pre-train or fine-tuning. Indeed, our study is intended for researchers and practitioners interested in capitalizing on the benefits of GenAI for network monitoring and management. Hence, we place a strong emphasis on the reproducibility of the proposals. Therefore, we also contribute to the taxonomization of the models used for each networking application we identify. While centered on the impact of the latest LLM wave, our study does not simply focus on LLM-based generative solutions\u2014such as the majority of similar surveys [3, 20, 23, 26-28]. Instead, we analyze contributions that include the latest achievements based on Diffusion Models and SSMs, which are often overlooked in related surveys. On the other hand, we purposely exclude in our analysis generative algorithms such as GANs, Variational Autoencoders (VAEs), and normalizing-flows. These methods, while significant in past years, are considered less relevant compared to the latest advancements in GenAI."}, {"title": "III. BACKGROUND", "content": "In this section, we first provide a formal description of GenAI (Sec. III-A). Then, we trace the evolution of GenAI models over time, from classic methods (Sec. III-B), proposed since 2013, to the most recent advancements of the present day (Sec. III-C). We end the section by describing the various strategies used to optimize GenAI models to deal with typical NMM use cases (Sec. III-D)."}, {"title": "A. GenAI in a Nutshell", "content": "AI models can be classified into discriminative and generative models, according to the learning objective. The former makes predictions on unseen data by training on labeled data and thus can be used for various inference tasks. In contrast, generative models focus on synthesizing realistic content. From a formal viewpoint, given a set of training samples $x_1,..., x_N$ associated to an unknown data distribution $p_d(x)$, a GenAI technique learns a model to sample new (synthetic) data according to $p_{mod}(x) \\approx p_{da}(x)$. This can be accomplished by either two- or one-step approaches. In the former case, known as Explicit Density Estimation (EDE), the model first learns an explicit distribution $p_{mod}(x) \\approx p_d(x)$ (in a tractable or approximate fashion), which is then used to sample new data. In the latter case, known as Implicit Density Estimation (IDE), the GenAI technique directly learns a model that can sample from $p_{mod}(x) \\approx p_d(x)$ without explicitly defining it.\nThe design and use of GenAI have a long history in NMM: relevant methods include well-known Markov chains (tractable EDE) [32], but also VAEs (approximate EDE) [33], GANS (IDE) [34] and normalizing-flows (tractable EDE) [35]. Conversely, recent applications of generative models are LLMs-based on Transformer (and variants) or selective SSM-and Diffusion Models, which have represented a breakthrough in the realism and complexity of the content generated. Transformer-based models enable parallelization and scalability, enhancing processing speed and contextual understanding through their self-attention mechanisms. Moreover, Diffusion Models offer several advantages over traditional generative models, such as VAEs and GANs, including better mode coverage and stability during training."}, {"title": "B. Classic GenAI Methods", "content": "Figure 2 reports the timeline of the development of GenAI, starting from VAEs (proposed in 2013 by Kingma and Welling [36] at the University of Amsterdam) until the latest models released by OpenAI in the second half of 2024, namely GPT-4O and its successive variants and evolutions (i.e., the lightweight GPT-4O mini, and the reasoning models o1-preview and 01-mini). We recall that this survey focuses only on works that take advantage of the most recent advances in GenAI, specifically from the Transformer architecture onward, which Google proposed in 2017 [37]. One motivation is that these solutions offer improved performance w.r.t. older solutions like VAES, GANs, and normalizing-flows, e.g., Non-linear Independent Components Estimation (NICE) [38].\nMoreover, this choice is justified by the impressive groundbreaking impact of these more sophisticated GenAI architectures across various fields, significantly improving generative tasks such as text generation, image synthesis, and multimodal applications. However, we also report the classic deep generative models-viz., NICE, VAEs, and GANs with their variants and hybridizations for context and completeness. Regarding the latter GenAI architectures\u2014which fall outside our scope-we refer the reader to these prominent surveys for a deeper background and a detailed overview of their usage for networking-related use cases: [19, 22, 25, 39]. The modern era of deep GenAI started with VAEs at the end of 2013, GANs at mid-2014, and NICE at the end of 2014. These models were the first deep neural networks capable of learning generative models for complex data, such as images. In detail, VAEs introduced a structured and probabilistic approach to generative modeling with continuous latent spaces and improved training stability [36], while GANs presented a powerful adversarial framework that excels at generating high-quality and realistic data [40]. Then, NICE was the first model to implement normalizing flows using neural networks, leveraging them as invertible functions to transform data from a complex distribution to a simpler one [38].\nOver time, diverse improvements to VAES, GANs, and NICE have been proposed. Notably, the Conditional GAN (CGAN, 2014) enables controlled data generation by incorporating additional information into the generative process. This allows for a more targeted and context-specific output [41]. Additionally, the Deep Convolutional GAN (DCGAN, 2015) enhances the quality of generated images and improves the stability of the training process [42]. Moreover, hybrid architectures like VAE-GAN (2015) were proposed, integrating the structured latent space of VAEs into the adversarial training of GANs [43]. Lastly, the main evolution of NICE has been the Real Non-Volume Preserving (Real NVP) model\u2014proposed in mid-2016\u2014that incorporates scale transformations, allowing the model to expand or contract regions of data rather than simply rotating or translating them, leading to more accurate and expressive generated contents [44]."}, {"title": "C. Recent Advancements on GenAI", "content": "Focusing on the most recent advancements in GenAI, namely from Transformer onward, we can identify five categories of architecture divided according to the nature of the underlying layers. We identify three variants of the Transformer architecture, namely the (i) full encoder-decoder, the (ii) encoder-only, and the (iii) decoder-only architectures. Additionally, the other two categories are based on (iv) diffusion processes and (v) state-space representations, respectively. From a general perspective, the development of GenAI models has shifted in the last years toward a foundational nature definition [45]. Consequently, specific training strategies are commonly employed.\nTraining Strategies for GenAI Models: Three training strategies can be adopted for GenAI models. In the case of naive (a) Monolithic Training, the model is trained from scratch using a dataset tailored to the specific downstream task. More commonly, the training follows two sequential stages: (i) Pre-Training, where the GenAI model is pre-trained on a large corpus of data consisting of text, images, or other input modalities, in a self-supervised or semi-supervised manner. For instance, during this stage, a text-fed (resp. image-fed) model is instructed to predict masked words and the sequence of sentences (resp. to denoise or reconstruct the original picture). (ii) Fine-Tuning, where the GenAI model is then specialized for specific tasks (possibly by topping/modifying the architecture with task-specific layers). Specifically, the training parameters (or a portion of them) are jointly fine-tuned (exploiting the broader transfer learning concept), tailoring the model for the considered downstream task. Consequently, the training of a model can involve either (b) Pre-Training & Fine-Tuning, i.e., the model is first pre-trained on a large corpus of data (e.g., a networking corpus) and then fine-tuned with a dataset related to the downstream task, or (c) Fine-Tuning Only, i.e., an already pre-trained model is exclusively fine-tuned for the specific downstream task.\nFull Encoder-Decoder (FED): This category includes the GenAI architectures that reflect the typical structure of the Transformer model [37]. The Transformer represents a revolutionary approach to solving sequence processing tasks. This architecture is entirely based on the self-attention mechanism rather than using recurrences\u2014e.g., Recurrent Neural Network (RNN)\u2014or convolutions\u2014e.g., Convolutional Neural Network (CNN). This improvement enables the Transformer to efficiently parallelize computations and significantly reduce training times while achieving state-of-the-art results. In fact, traditional sequence processing models such as RNNs and their variants like Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)\u2014perform computations around symbol positions in input and output sequences. This characteristic results in limited parallelization because it is inherently sequential, becoming a significant bottleneck for longer sequences. To cope with this drawback, the Transformer leverages self-attention mechanisms to model dependencies between different positions in a sequence, regardless of their distance.\nIn general, the Transformer architecture consists of an encoder-decoder structure. The general workflow of a FED is depicted in Figure 3a. The encoder comprises a stack of identical layers, each with two sub-layers, namely a multi-head self-attention and a position-wise fully connected feed-forward network. The decoder is similar to the encoder but includes an additional sub-layer that performs masked multi-head attention over the encoder's output. It also modifies the self-attention sub-layer to prevent positions from attending to subsequent positions, ensuring the auto-regressive property. Both encoder and decoder are designed with residual connections for each sub-layer followed by layer normalization.\nGoing into detail, the Transformer uses multi-head attention to allow the model to learn information from different representation subspaces jointly. In fact, instead of having a single attention function, the model linearly projects queries, keys, and values multiple times with different learned projections and performs the attention function in parallel. This process enhances the model's ability to focus on different parts of the input sequence. Moreover, since the Transformer lacks the inherent sequential order provided by recurrence, it introduces positional encodings to inject information on the position of tokens in the sequence. These encodings are added to the input embeddings at the bottom of the encoder and decoder stacks, enabling the model to understand the sequence order.\nThe Transformer is usually leveraged for sequence-to-sequence tasks, i.e., when the input and the output are both sequences. Examples of applications are translation (from one language to another), summarization (condensation of documents), and text generation (based on given prompts). Notable architectures that fall into this category are XLNet, T5, Gemini, Mistral, and Zephyr.\nEncoder-Only (EO): Compared to FED, EO models only leverage the encoder unit of the FED architecture (as depicted in Figure 3b). EO is designed to model bidirectional relationships between tokens in an input sequence, generating either a vector representation for each token or a single vector summarizing the entire sentence. This architecture is well-suited for tasks focused on text understanding and analysis rather than generation.\nAmong the EO models, BERT [46]\u2014 Bidirectional Encoder Representations from Transformers-is the most representative and has served as the basis for many subsequent advances of this type of architecture. Developed by researchers at Google AI Language, BERT consists of multiple layers of bidirectional Transformer encoders. BERT has been designed with a key innovation: it pre-trains deep bidirectional representations from the unlabeled text by joint conditioning on both left and right contexts in all layers. This procedure allows BERT to capture richer linguistic information, and it contrasts with models like OpenAI GPTs and ELMo (an LSTM-based architecture), which are unidirectional and do not fully leverage the bidirectional context. In detail, the bidirectional training of BERT is achieved through a Masked Language Model (MLM) objective. The MLM randomly masks some tokens in the input sequence and predicts them using the context provided by the remaining tokens on both sides. This method allows BERT to capture the context from both directions. To further enhance its understanding of context and sentence relationships, BERT uses a next-sentence prediction task during pre-training. This involves predicting whether a given sentence B follows sentence A in the original text, allowing the model to learn how sentences relate to each other.\nThe ability of BERT to understand the context from both directions and the effectiveness of its pre-training tasks enables it to achieve superior performance across a wide range of NLP tasks. BERT has been designed for language understanding, i.e., encoding the input text in relation to its context for various subsequent tasks. Examples of applications are text classification (e.g., spam detection), question answering, and text similarity (e.g., semantic search). Other notable architectures in this category are variants of BERT, such as BERTiny, ROBERTa, DistilRoBERTa, and ViT (Vision Transformer).\nDecoder-Only (DO): These models exploit only the decoder component of the FED architecture, as shown in Figure 3c. DO models are designed for autoregressive text generation, predicting the next token based on previous tokens, thereby producing the output one token at a time.\nAmong them, Generative Pre-trained Transformers (GPTs) [47] are the most prominent, spearheading advancements in the field of NLP starting with their first variant named GPT-1. Subsequent improvements of GPTs, including GPT-2, GPT-3, GPT-3.5, GPT-3.5 turbo, GPT-4, and GPT-4O, have dramatically increased the model size and the scale of pre-training data and have included multi-modality (text and images) from GPT-4 onward. GPT-4, with its estimated 1.7 trillion parameters, exemplifies the trend towards larger models and has achieved state-of-the-art results across a wide range of benchmarks without task-specific fine-tuning.\nGPTs are commonly leveraged for autoregressive text generation, i.e., generating text tokens conditioned on the previous token. Examples of applications are text generation, language modeling (e.g., autocompletion), and conversational AI or chatbots (e.g., ChatGPT and Copilot). Notable architectures in this category are Falcon, LLAMA, Phi, Gemma and improvements of GPT-1, from GPT-2 to GPT-40.\nSequential Denoising Process (SDP): Ho et al. [48] proposed a class of generative models, named Diffusion (probabilistic) Models, that describes the process by which particles, information, or other entities spread through a medium over time. In recent years, the Diffusion Model (Figure 4) has found successful applications in computer vision, as well as in audio, bioinformatics, and agent-based systems.\nThe core idea involves defining a forward diffusion process (Figure 4a) that gradually adds noise to the data, transforming them into a simpler distribution, typically Gaussian noise. The corresponding reverse diffusion process (Figure 4b) is then learned to map the noisy data back to the original data distribution. The elegance of Diffusion Models lies in their theoretical foundation, which leverages concepts from Markov chains (when diffusion is performed in discrete time) or stochastic differential equations (when diffusion is performed in continuous time). This foundation allows for a rigorous treatment of the model's behavior and facilitates efficient training and sampling algorithms (through sophisticated sampling acceleration techniques). The resulting models, such as the Denoising Diffusion Probabilistic Model (DDPM) and score-based generative models, have demonstrated remarkable capabilities in generating high-quality synthetic data. Because of the explicit definition of the forward/reverse diffusion process and the objective used to learn them (i.e., a generalized evidence lower-bound [49]), these models fall within the approximate EDE category. In summary, Diffusion Models have been used for high-quality data generation through iterative denoising. Examples of applications include visual and signal data processing tasks, such as image generation, audio synthesis, and video generation, contrasting with previous categories that primarily involve language and textual data-generation tasks. A notable model falling into this category is Stable Diffusion.\nSelective and Structured State Space Models (SSMs): Proposed by Gu and Dao [50], Mamba represents a significant advancement in sequence modeling, introducing a new class of selective and structured SSMs designed to overcome the limitations of existing architectures like Transformers in handling very long input sequences. As depicted in Figure 5, Mamba integrates selective and structured SSMs into a streamlined neural network architecture that avoids traditional attention mechanisms, achieving fast inference and linear scaling with sequence length.\nIn fact, while Transformers have become the backbone of many foundation models due to their effective self-attention mechanism, they suffer from quadratic scaling w.r.t. sequence length, limiting their efficiency on long sequences. Subquadratic-time architectures, including linear attention and structured-only SSMs, have attempted to address these inefficiencies.\nMamba's first core innovation lies in making (a part of) the parameters of SSMs dependent on the input. This mechanism allows the model to selectively propagate or forget information based on the current token, i.e., the model can focus on relevant information or discard irrelevant or outdated information as needed. This selective mechanism enables Mamba to handle discrete modalities effectively, providing a significant advantage over previous SSMs. Secondly, Mamba reduces the number of trainable parameters by assuming a structured form for the SSM matrices defining the information propagation. Thirdly, to maintain efficiency, Mamba employs a (i) hardware-aware algorithm that computes the model recurrently without materializing the expanded state in the GPU memory (leveraging fast memory hierarchies) and (ii) a parallel scan algorithm to accelerate the recursive computation of relevant quantities. This approach ensures linear scaling in sequence length and high throughput on modern hardware.\nIn summary, Mamba integrates selective and structured SSMS into a simplified neural network architecture that omits attention and MultiLayer Perceptron (MLP) blocks. This streamlined design, inspired by previous SSM architectures, offers fast training and inference with high performance in various data modalities, including language, audio, and genomics. Mamba is designed to process and model sequences efficiently, making it ideal for applications that require handling long sequences and achieving high computational efficiency."}, {"title": "D. Optimization Strategies for GenAI", "content": "In this section, we discuss the optimization strategies that have been proposed for GenAI solutions, focusing on those that have been used in NMM use cases. Broadly, optimization strategies can be divided into two categories [51], involving methods for (i) parameter-efficient fine-tuning and (ii) post-training quantization.\nParameter-Efficient Fine-Tuning (PEFT): These methods enhance the adaptation of pre-trained GenAI models to the target downstream task. The primary objective is to minimize the computational resources required for fine-tuning while preserving inference performance.\nAmong recent advances proposed to optimize the fine-tuning step of GenAI solutions, the state-of-the-art approach named Low-Rank Adaptation (LoRA) [52] has been recently leveraged for traffic generation purposes [53]. LoRA capital-izes on the intuition that changes in model weights during adaptation have a low \u201cintrinsic rank\". In other words, the idea is that adapting a model to a new task does not require very complex changes, which can be efficiently represented using fewer adjustments. In detail, LoRA optimizes the fine-tuning by using rank decomposition matrices, specifically targeting the change in dense layers during training while keeping the main pre-trained weights frozen. Thus, this method allows for efficient task adaptation by replacing some model components-i.e., parts of the weight matrices-with small low-rank matrices. This substitution reduces the need to recalculate gradients and memorize optimizer states. The way in which LoRA is designed ensures that there is no extra delay introduced during inference. Moreover, LoRA is compatible with other optimization techniques, such as post-training quantization [54].\nPost-Training Quantization (PTQ): These methods aim to reduce the computational complexity and memory footprint of GenAI models by casting the model parameters into lower precision formats after training. Quantization facilitates faster inference and more efficient deployment on various hardware. One of the most complete methods for enforcing PTQ that has been recently proposed is named GPT-Generated Unified Format (GGUF) [55]. GGUF is a generalized file format that has recently been adopted by [56] to enforce post-training quantization for network digital assistance purposes, namely for networking standards question answering. GGUF has been proposed to reduce\u2014in large LLMs-the precision of the weights and activations of the model by converting real numbers to integers, e.g., 32-bit floating-point to 8-bit. GGUF has been devised with two key features in mind: quantization-aware kernel optimization and extensibility. On the one hand, GGUF does not simply apply quantization to the model weights but also provides kernel optimization functionalities that consider the quantization process. This characteristic is fundamental in avoiding an inference performance decrease due to blind quantization. On the other hand, GGUF has been designed to overcome the limits of its predecessor GGML, which lacks mechanisms to incorporate additional model information or add new features. Therefore, GGUF allows the integration of new features into the file format while ensuring compatibility with models deployed in older GGUF formats, thus preserving backward compatibility for newer versions. In general, GGUF provides several key functionalities, including single file deployment, improved model loading and saving speeds, and intuitive design and detailed information storage that facilitate extensibility. Together, these functionalities enable a more efficient and user-friendly experience in handling LLMs."}, {"title": "IV. CURRENT STATUS OF GENAI IN NETWORK MONITORING AND MANAGEMENT", "content": "This section provides an overview of the current status of Generative Artificial Intelligence (GenAI) in the Network Monitoring and Management (NMM) context. Accordingly, Sec. IV-A outlines the five NMM use cases where GenAI is currently being utilized, along with a description of the benefits GenAI offers for each. Then, Secs. IV-B-IV-F dissect the works that employ GenAI for each use case."}, {"title": "A. Use Cases for GenAI in Network Monitoring and Management", "content": "GenAI is actively used to address NMM use cases in various networking domains. We have identified five key use cases where it is currently employed: (i) Network Traffic Generation, (ii) Network Traffic Classification, (iii) Network Intrusion Detection, (iv) Networked System Log Analysis, and (v) Network Digital Assistance for Documentation & Configuration. Such use cases are summarized in Figure 6 along with the acronyms we use in this work. Below, we provide detailed descriptions for each use case.\nNetwork Traffic Generation (NTG) refers to the process of creating synthetic network data, ranging from the generation of (bi-)flow statistical features or sequence of features extracted from the packets within a (bi-)flow (e.g., packet size, inter-arrival time, and packet direction), to the generation of the entire PCAP trace.\nNTG is crucial for network traffic analysis from various applicative perspectives. It enables the simulation of different"}]}