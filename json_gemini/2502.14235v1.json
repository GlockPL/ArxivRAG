{"title": "OG-Gaussian: Occupancy Based Street Gaussians for Autonomous Driving", "authors": ["Yedong Shen", "Xinran Zhang", "Yifan Duan", "Shiqi Zhang", "Heng Li", "Yilong Wu", "Jianmin Ji", "Yanyong Zhang"], "abstract": "Accurate and realistic 3D scene reconstruction enables the lifelike creation of autonomous driving simulation environments. With advancements in 3D Gaussian Splatting (3DGS), previous studies have applied it to reconstruct complex dynamic driving scenes. These methods typically require expensive LiDAR sensors and pre-annotated datasets of dynamic objects. To address these challenges, we propose OG-Gaussian, a novel approach that replaces LiDAR point clouds with Occupancy Grids (OGs) generated from surround-view camera images using Occupancy Prediction Network (ONet). Our method leverages the semantic information in OGs to separate dynamic vehicles from static street background, converting these grids into two distinct sets of initial point clouds for reconstructing both static and dynamic objects. Additionally, we estimate the trajectories and poses of dynamic objects through a learning-based approach, eliminating the need for complex manual annotations. Experiments on Waymo Open dataset demonstrate that OG-Gaussian is on par with the current state-of-the-art in terms of reconstruction quality and rendering speed, achieving an average PSNR of 35.13 and a rendering speed of 143 FPS, while significantly reducing computational costs and economic overhead.", "sections": [{"title": "I. INTRODUCTION", "content": "Reconstructing realistic, geometrically accurate 3D scenes has long been a key objective in computer vision. Thanks to advancements in technologies like Neural Radiance Fields (NeRF) [1] and 3D Gaussian Splatting (3DGS) [2], generating high-precision 3D models is now more attainable. These technologies greatly enhance the realism of virtual environments and have significant applications in various fields, such as medical imaging [3], surgical navigation [4] and virtual reality [5], [6]. In autonomous driving, the reconstruction technologies can provide precise 3D models of the surroundings, including street, building, and even dynamic object. This capability improves the autonomous system's navigation ability and enables simulation of extreme scenarios, expanding the boundaries of reality while digitizing it.\nTo achieve the high-precision reconstruction of autonomous driving scenes, NeRF is used as the foundational technology, representing the scene as a continuous 3D volume through neural network [7], [8], [9]. While this method generates high-quality outdoor scenes, it comes with the cost of requiring extensive training resources and slower rendering speed. With the emergence of 3DGS, the low-cost, fast-rendering 3D scene reconstruction method quickly captured widespread attention. The native 3DGS is not well suited for handling large outdoor scene with dynamic objects. To adapt this technology for autonomous driving scene reconstruction, existing works of 3DGS put their attention on integrating LiDAR-generated point clouds and using annotated 3D bounding boxes to reconstruct street scenes with dynamic objects [10], [11], [12]. They have successfully separated dynamic objects from static background, achieving promising reconstruction results under low training cost[13].\nHowever, these techniques need expensive LiDAR to generate point clouds and datasets with pre-labeled dynamic vehicle boundaries and trajectories. To mitigate this constraint, we bring Occupancy Prediction Network(ONet) [14], a new approach in autonomous driving perception into the field of 3D scene reconstruction. Since ONet models the real world as voxel grids with semantic information directly, we can eliminate the need for costly LiDAR and resolve the problem of bounding boxes failing to capture unannotated objects[15].\nBased on the considerations, we propose OG-Gaussian, a new solution for autonomous driving scene reconstruction. Our method starts by capturing surround-view images using cameras mounted on the vehicle. Then, we use a Occupancy Prediction Network (ONet) to obtain Occupancy Grid (OG) information about the surrounding world. By leveraging the"}, {"title": "II. RELATED WORKS", "content": "Occupancy Network and applications. Obtaining accurate semantic 3D Occupancy Grids is crucial for downstream tasks, making research on the Occupancy Prediction Network highly valuable[17]. MonoScene [18] completes dense 3D grids from a single RGB image, it ensures spatial consistency via 2D-3D projection and 3D context prior. By extending BEV representation to 3D, TPVFormer [19] proposes a new representation named TPV and combines it with a transformer to predict 3D semantic Occupancy Grids. SurroundOcc [20] utilizes an innovative method to generate dense occupancy labels and employed a 2D-3D attention mechanism to predict Occupancy Grids from multi-camera images. To facilitate future research, Occ3D [21] introduces two benchmark datasets (Occ3D-Waymo and Occ3D-nuScenes) and a new model (CTF-Occ network). As the accuracy of occupancy prediction networks improves, many related applications are also emerging. OccWorld [22] introduces a world model framework based on 3D occupancy space. It demonstrates an effective capability in modeling scene evolution on the nuScenes benchmark. OCC-VO [23] transforms 2D camera images into 3D semantic occupancy to address the depth information challenge in Visual Odometry(VO).\n3D scenes reconstruction for Autonomous Driving. Simulating real-world street scene is essential for developing and testing autonomous driving systems. A prime example is CARLA [24], a well-known open-source driving simulator that's widely used for creating complex 3D environments. But the scenes created by these simulators often lack the realism needed to fully immerse users in real-world environments. To adapt NeRF for unbounded and dynamic field like autonomous driving, [25], [26] improves NeRF to model multi-scale urban scenarios. [27] combines compact multi-resolution ground features with NeRF to achieve high-fidelity rendering. Research in [28], [29] begin to explore handling dynamic scenes with multiple objects, leading to Suds [30] processing scenes into static backgrounds and dynamic objects. While S-NeRF [7] introduces LiDAR as a form of supervision, MARS and EmerNeRF [8], [9] further optimize NeRF for outdoor scene reconstruction to enhance its performance. However, NeRF is limited by its high training costs and slow rendering speed, so attention of industry is gradually shifting toward 3DGS due to its lower training costs and faster rendering speed [2]. [31] uses a transformer to model Gaussian motion from monocular images. [10] parametrizes the entire scene using a set of variable dynamic Gaussians. DrivingGaussian [11] firstly introduces LiDAR point clouds as a prior and incrementally reconstructs the entire scene. Street Gaussians [12] reconstructs dynamic scenes with separate rendering and LiDAR. S3 Gaussian [32] improves this using self-supervised vehicle bounding boxes."}, {"title": "III. PRELIMINARY", "content": "Firstly, we will introduce the classical 3DGS algorithm and 3D Occupancy Grid in the preliminary section.\nClassical 3DGS algorithm. In the first step of reconstruction, we initialize the 3DGS point clouds by placing a Gaussian at each point, other parameters randomly initialized except the center point position. COLMAP [33] is used to generate the Structure from Motion (SfM) point clouds as the prior. After initialization, each Gaussian ellipsoid can be represented by Eq. 1:\n$G(x) = e^{-(x)^T \\Sigma^{-1}(x)}$\n$\\Sigma = RSS^T R^T$\n$G(x)$ is the probability distribution function of the Gaussian, where $x$ is a three-dimensional vector representing a point in space. $\\Sigma$ is the covariance matrix that describes the"}, {"title": "IV. METHOD", "content": "Next, we will provide a detailed explanation of the OG-Gaussian method, as shown in Fig. 2\nA. OG-Gaussian\nIn this section we focus on the basic structure of the OG-Gaussian and introduce how we represent street scene and dynamic vehicles with two different sets of point clouds. We will provide a detailed explanation of them below.\nStreet model. The initial point clouds of the Street model is a set of points in the world coordinate system. According to the information in the preliminary, the parameters of 3D Gaussians can be expressed as the covariance matrix $\\Sigma_s$ and the position vector $\\mu_s \\in R^3$. The matrix $\\Sigma_s$ can be split into the rotation matrix $R_s$ and the scaling matrix $S_s$, this recovery process is:\n$\\Sigma_s = R_sS_sS_s^T R_s^T$\nIn addition to the covariance and position matrices, each Gaussian contains a parameter $\\alpha$ to represent opacity and a set of spherical harmonics coefficients (Eq. 6) to represent the appearance of scenes. $l, m$ in Eq. 6 are referred to the degree and order that define the specific spherical harmonic function. In order to obtain color information of the original view, we also need to multiply the spherical harmonics coefficients with the spherical harmonics basis functions projected from the view direction. To get the semantic information for each Gaussian, we add logit $\\beta_s \\in R^N$ to each point, where N stands for the total number of semantic categories.\n$Z_s = (z_{m,l})_{1:0 < l < l_{max}}$\nDynamic Vehicle model. Autonomous driving scene contains multiple moving vehicles, and we also need to represent"}, {"title": "\u0412. \u041e\u0441\u0441\u0438\u0440\u0430\u043f\u0441y Prior with Surrounding Views", "content": "The original 3DGS generates sparse point clouds as prior with the help of structure-from-motion (SfM). In the task of reconstructing unbounded scenes of streets, it would be difficult to accurately represent dynamic objects and complex street scenes using SfM point clouds directly, this approach would produce some obvious geometric errors and incomplete recoveries. In order to provide accurate initialized point clouds for 3DGS, we convert the results predicted by ONet into an initialized point cloud to obtain accurate geometric information and maintain multi-camera consistency in the surround view alignment.\nSpecifically, we extract the vehicle point clouds based on the semantic information of OGs, and we define the vehicle location information for each timestamp as $\\mu_t$, if $\\mu_{t+1} - \\mu_t \\ge \\mu_{th}$, we can label this vehicle as dynamic, where $\\mu_{th}$ denotes the threshold of the positional offset that determines it to be a dynamic object.\nIn order to generate densified point clouds to represent the dynamic vehicles, we upsample the point clouds of the dynamic objects with a voxel size of 0.05m. We project these points to the corresponding image planes and assign colors to them by querying pixel values. For each initial point $o$ of the dynamic vehicle, we transform its coordinates relative to the camera coordinate system, followed by the projection step described in Eq. 9, where $x_3$ is the 2D pixel of the image, and $K\\in R^{3*3}$ is the internal reference matrix of each camera. $R_t$ and $T_t$ represent the orthogonal rotation matrix and translation vector.\n$x = K[R_t o_d + T_t]$\nFinally, we convert the remaining Occupancy Grids into dense point clouds with position taken from their center coordinates. The specific process for generating initial point clouds of static and dynamic objects can be seen in Fig. 3.\nIn addition to this, we also aggregate the dense point clouds with the point clouds generated by COLMAP in order to deal with distant buildings."}, {"title": "C. Global Rendering via Gaussian Splatting", "content": "To render the entire OG-Gaussian, we aggregate the contribution of each Gaussian to produce the final image. Previous methods represent scenes using neural fields, which require accounting for factors like lighting complexity when composing the scene. Our OG-Gaussian rendering approach is based on 3DGS, enabling high-fidelity rendering of autonomous driving scenes by projecting the Gaussians of all point clouds into 2D image space.\nGiven a rendering timestamp t, we first calculate the spherical harmonics coefficients using Eq. 6. After transforming point clouds from the vehicle coordinate system to the world coordinate system, we combine the street model and the dynamic model into a global model. Using the camera's extrinsic parameters W and intrinsic parameters K, we project point clouds onto a 2D plane and calculate each point's parameters in the 2D space. In the Eq. 10, J is the Jacobian matrix of K, while $\\mu'$ and $\\Sigma'$ represent the position and covariance matrix in the 2D image space.\n$\\mu' = KW\\mu$\n$\\Sigma' = JW\\Sigma W^T.J^T$\nAfter this, we can calculate the appearance color a of each pixel based on the opacity of the points. In Eq. 11, $a_i$ is the product of opacity $\\alpha$ and the probability of the 2D Gaussian, while $a_i$ is the color derived from the spherical harmonic $z$ with a specific view direction.\n$a = \\sum_{i \\in N} \\alpha a_i \\prod_{j=1}^{i-1} (1 - \\alpha_j)$"}, {"title": "V. EXPERIMENT", "content": "A. Experiment Setups\nWe perform 30, 000 iterations on OG-Gaussian, using the same learning rate settings as the original 3DGS. The learning rates for the rotational transformation $\\Delta R_t$ and translational transformation $\\Delta T_t$ are set to 0.001 and 0.005. The Adam optimizer is used for optimization. All experiments are conducted on a machine equipped with an Nvidia A100 GPU.\nDataset. The Waymo Open Dataset [16] is widely used in autonomous driving research due to its high-quality 360-degree panoramic imagery, which offers comprehensive coverage of the vehicle's surrounding[37]. The dataset's quality, diversity, and scale make it a valuable asset for researchers and developers in autonomous driving. It has been widely adopted due to its realistic and challenging dataset, which enables the development and testing of algorithms in conditions that closely resemble real-world driving."}, {"title": "VI. CONCLUSIONS", "content": "We introduce OG-Gaussian, an efficient method that incorporates the Occupancy Grids into 3DGS for reconstructing outdoor autonomous driving scenes. Our approach leverages the prior provided by the Occupancy Grid for scene reconstruction while also separating and reconstructing dynamic vehicles from static street scenes. We achieve performance on par with LiDAR-dependent SOTA while relying solely on camera images. Our method will enable future researchers to reconstruct their autonomous driving scenes quickly and at a low cost, contributing to the advancement of autonomous driving technology."}]}