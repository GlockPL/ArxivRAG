{"title": "NEURFLOW: INTERPRETING NEURAL NETWORKS THROUGH NEURON GROUPS AND FUNCTIONAL INTERACTIONS", "authors": ["Tue M. Cao", "Nhat X. Hoang", "Hieu H. Pham", "Phi Le Nguyen", "My T. Thai"], "abstract": "Understanding the inner workings of neural networks is essential for enhancing model performance and interpretability. Current research predominantly focuses on examining the connection between individual neurons and the model's final predictions, which suffers from challenges in interpreting the internal workings of the model, particularly when neurons encode multiple unrelated features. In this paper, we propose a novel framework that transitions the focus from analyzing individual neurons to investigating groups of neurons, shifting the emphasis from neuron-output relationships to the functional interactions between neurons. Our automated framework, NeurFlow, first identifies core neurons and clusters them into groups based on shared functional relationships, enabling a more coherent and interpretable view of the network's internal processes. This approach facilitates the construction of a hierarchical circuit representing neuron interactions across layers, thus improving interpretability while reducing computational costs. Our extensive empirical studies validate the fidelity of our proposed NeurFlow. Additionally, we showcase its utility in practical applications such as image debugging and automatic concept labeling, thereby highlighting its potential to advance the field of neural network explainability.", "sections": [{"title": "1 INTRODUCTION", "content": "The explainable AI (XAI) field has seen significant advancement in understanding the mechanisms of deep neural networks (DNNs). This field emerges from the growing need in decoding the internal representations, in hope of reverse engineering deep models into human interpretable program. Prior works have initiated on breaking down convolutional neural networks (CNNs) into interpretable neurons, understanding the models in the most fundamental units (Nguyen et al., 2016; Zeiler & Fergus, 2014; O'Mahony et al., 2023; Bykov et al., 2024). Extending further, one can examine the relation between neurons to gain insights on how the model works, within one layer (Vu et al., 2022), and between multiple layers (Cammarata et al., 2020). Ultimately, recent works try to generate circuits (Cammarata et al., 2020; Bykov et al., 2024; Wang et al., 2022c; Conmy et al., 2023) that create exhaustive explanations of how features are processed and evolve throughout the model.\n\nThe majority of existing methods focuses on individual neurons Oikarinen & Weng (2024b);\nLa Rosa et al. (2023a) and their relationship to the model's final predictions Ghorbani & Zou (2020b); Wang et al. (2022b); Ghorbani & Zou (2020a), while giving less attention to exploring and quantifying the relationships and interactions between neurons across different layers. These approaches are not only constrained by scalability challenges arising from the extensive number of neurons, but they also hinder a comprehensive understanding of the underlying mechanisms of DNNs. A notable example is the polysemantic phenomenon Mu & Andreas (2020); O'Mahony et al.(2023); Olah et al. (2020), where a single neuron is activated by several unrelated concepts. This phenomenon complicates the task of associating each neuron with a distinct feature and hampers the interpretation of how a model processes concepts based on the relationships among neurons. Drawing inspiration from human inference, which synthesizes information from a variety of sources, we contend that, in addition to individual neuron encoding multiple concepts (as demonstrated in prior studies O'Mahony et al. (2023); Olah et al. (2020)), groups of neurons within each layer also collectively encode the same concept. Furthermore, the decision-making process in neural networks is shaped not solely by the interactions between individual neurons, but rather by interactions among neuron groups.\n\nThis study seeks to explore the roles and interactions of neuron groups in shaping and developing concepts, enabling the execution of specific tasks. Due to the complex connections between large number of neurons, identifying those functions and there interactions is a daunting task. To overcome this, we demonstrate that for a particular task, only a subset of neurons-referred to as core concept neurons\u2014play a crucial role as influential and concept-defining elements in neural networks. These neurons, when deactivated, significantly alter the associated concepts.\n\nFocusing on core concept neurons allows us to view the intricate network in a simplified way, revealing the most important interactions between the groups of neurons. Therefore, we propose Neur-Flow framework that (1) identifies core concept neurons, (2) clusters these neurons into groups, and (3) investigates the functions and interactions of these groups. To enhance interpretability, we represent each neuron group by the set of visual features it encodes (i.e., named as neuron group's concept). Focusing on classification models, we construct, for each class of interest, a hierarchical tree in which nodes represent neuron groups (defined by the concepts they encode), and edge weights quantify the interactions between these groups.\n\nOur key contributions are summarized as follows:\n\ni) We introduce an innovative framework that systematically builds a circuit to elucidate the mechanisms by which core concept neuron groups operate and interact to achieve specific tasks. This entire process is automated, necessitating no human intervention or predefined concept labels. To our knowledge, we are the first to employ neuron groups as the fundamental units for explaining the internal workings of deep neural networks.\n\nii) We perform empirical studies to validate the proposed framework, demonstrating the optimality and fidelity of core concept neurons, and the reliability of interaction weights between core concept neuron groups.\n\niii) We provide experimental evidence showing that our framework can be applied to various tasks, including image debugging and automatic neuron concept labeling. Specifically, we confirm the biases found by Kim et al. (2024) on ImageNet (Russakovsky et al., 2015), which have not been proven, by masking the core concept neurons related to the biased features."}, {"title": "2 RELATED WORK", "content": "In an effort to understand the inner mechanism of DNNs, several branches of research have emerged:\n\nConcept based. Kim et al. (2018) show that a model can be rigorously understood by assigning meaning to the activations, referred to as concept activation vectors. Subsequent works (Ghorbani et al., 2019; Zhang et al., 2021) have explored more complex methods for extracting these meanings, however, the relationships between concepts remain understudied. Fel et al. (2023); Kowal et al. (2024) address this limitation by constructing a graph of concepts with edges that quantify the relations. Their main intention is to see the evolution of concepts throughout the network layers. Nevertheless, they are unable to explain which parts of the model are responsible for these concepts.\n\nNeuron based. Nguyen et al. (2016); Mu & Andreas (2020); O'Mahony et al. (2023); Bykov et al. (2024) invest effort in studying the meaning of neurons, in parallel, Vu et al. (2022); Ghorbani & Zou (2020c); Khakzar et al. (2021b) propose different approaches in identifying important neurons to the model output. These researches shed light on the function of individual neurons and their impact on the prediction of the model. Recently, (Cammarata et al., 2020; Bykov et al., 2024; Achtibat et al., 2023) connect the neurons to form circuits that explain the behavior of a model throughout the layers, nevertheless, the circuits are constructed manually. Furthermore, a major limitation of all previous works is that they only analyze one neuron at a time. This approach is"}, {"title": "3 NEURFLOW FRAMEWORK", "content": "Our goal is to explain the internal mechanisms of deep neural networks (DNNs) by investigating how groups of neurons function and interact to encapsulate concepts, thereby performing a specific task. In particular, we focus on the classification problem, exploring how groups of neurons process visual features to identify a class. Given the exponential number of possible neuron groups, we focus only on core concept neurons. In addition, to facilitate human interpretation, we group these neurons through the common visual features they encode. In essence, we propose a comprehensive framework to address the following questions: (i) Which neurons play a crucial role in each layer? (ii) How can these neurons be clustered, and what visual features does each neuron group encapsulate? (iii) How do groups of neurons in adjacent layers interact?\n\nOur problem can be formulated as follows: Given a pretrained classification network $F$ and a dataset $D_e$ composed of exemplars from a specific class $c$, the goal is to construct a hierarchical tree whose vertices represent groups of core concept neurons in each network layer, and the edges capture the relationships between these groups. Figure 1 illutrates the workflow of our framework which comprises the following key components: (1) identifying core concept neurons (Section 3.3), (2) determining inter-layer relationships among neurons (Section 3.4), (3) clustering the core concept neurons into groups, and analyzing the interactions between these neuron groups (Section 3.5)."}, {"title": "3.2 DEFINITIONS AND NOTATIONS", "content": "In this paper, the term neuron refers to either a unit in a linear layer or a feature map in a convolutional layer. As suggested by Cammarata et al. (2020); Bykov et al. (2024); O'Mahony et al. (2023), each neuron is selectively activated by a distinct set of visual features, and by interpreting the neuron"}, {"title": "3.3 IDENTIFYING CORE CONCEPT NEURONS", "content": "Given a neuron $a$, we describe our algorithm for identifying its core concept neuron set $S_a$. This process consists of two main steps: determining $a$'s concept $V_a$ according to Definition 1, and identifying core concept neurons following Definition 3.\n\nFirstly, we generate a set of image patches $D$ by augmenting the original dataset $D_e$, which consists of images that the model classifies as class $c$. Since neurons can detect visual features at different levels of granularity, we divide each image in $D_e$ into smaller patches using various crop sizes, where smaller patches capture simpler visual features and larger patches represent more complex ones. We subsequently evaluate all items in $D$ to identify the top-$k$ image patches that induce the highest activation in neuron $a$, thereby constructing $V_a$.\n\nWith $V_a$ identified, one could determine the core concept neurons through a brute-force search over all possible candidates. However, this naive approach is computationally infeasible. To this end, we define a metric named importance score that quantifies the attribution of a neuron $s_i$ to $a$. The importance score can be intuitively seen as integrated gradients (Sundararajan et al. (2017)) of $a$ to $s_i$ calculated across all elements of $V_a$, calculated as follows:\n\n$T(a, s_i, V_a) = \\sum_{v \\in V_a} \\sum_{n=1}^{N} s_i * \\frac{\\partial \\phi^{l,l-1}(v)}{\\partial \\phi^{l-1,l}(s_{i})}$\n\nwhere $\\phi^{l,l-1}$ is the element of $\\phi^{l,l-1}$ corresponding to neuron $s_i$, $\\phi^{l-1,l}$ depicts the function mapping from the activation vector of layer $l-1$ to the activation of neuron $a$, and $N$ is the step size. Utilizing the importance scores of all neurons in the preceding layer, the set of core concept neurons is identified by selecting the top $\\tau$ neurons that exhibit the highest absolute scores. To justify the use of integrated gradients, we empirically show a strong correlation between the absolute values of $T(a, s_i, V_a)$ and the change in $a$'s concept after knocking out $s_i$, as demonstrated in Section 4. Additionally, we compare our method with other attribution techniques in Appendix D.1."}, {"title": "3.4 CONSTRUCTING CORE CONCEPT NEURON CIRCUIT", "content": "For each class of interest $c$, the neuron circuit $H_c$ is represented as a hierarchical hypertree, with the root $a_c$ being the neuron in the logit layer (ouput) associated with class $c$. The nodes in each layer of the tree $H_c$ are the core concept neurons of those in the layer above, and branches connecting a parent node $a$ and its child $s_i \\in S_a$ represents the contributions of $s_i$ to $a$'s concept.\n\nAs mentioned in (Cammarata et al., 2020; O'Mahony et al., 2023), neurons often exhibit polyse-mantic behavior, meaning that a single neuron may encode multiple distinct visual features. In other words, the visual features within a concept $V_a$ of neuron $a$ may not share the same meaning and can be categorized into distinct groups, which we term semantic groups. We hypothesize that each core concept neuron $s_i$ makes a distinct contribution to each semantic group of neuron $a$. To model this relationship, we represent the interaction between $s_i$ and $a$ through multiple connections, where the $j$-th connection reflects $s_i$'s influence on $V_{a,j}$, the $j$-th semantic group of $a$.\n\nAt a conceptual level, the algorithm for constructing the hypertree $H_c$ proceeds through the following steps: (1) employing our core concept neuron identification algorithm to determine the children of each node in the tree (Section 3.3), (2) clustering the neuron concept of each parent node into semantic groups, and (3) assigning weights to each branch connecting a child node to the semantic groups of its parent. Figure 2 illustrates our algorithm. The complete algorithm for constructing the core concept neuron circuit is presented in Appendix E. We provide a detailed explanation of these steps below.\n\nDetermining semantic groups. Let the concept $V_a$ corresponding to $a$ be composed of $k$ elements $\\{v_a,..., v_a\\}$. For each visual feature $v_a (i=1,...,k)$, we define its representative vector $r(v_i) \\in R^m$ as:\n\nr(v) = [mean($\\phi^{l-1}(v_i)_1$),..., mean($\\phi^{l-1}(v_i)_m$)],\n\nwhere $\\phi^{l,l-1}(v_i) (j = 1,...,m)$ represents the $j$-th feature map and mean($\\phi^{l-1}(v_i)_j$ denotes the average value across its all elements. Next, we use agglomerative clustering (Murtagh & Legendre, 2014) to divide the set $\\{v_1, ..., v_k\\}$ into clusters, where the distance between two visual features $v_n, v_a$ is defined by the distance between their corresponding representative vectors $r(v_n)$, $r(v_a)$. The Silhouette score (Rousseeuw, 1987) is employed to ascertain the optimal number of clusters. The complete procedure is detailed in Algorithm 2.\n\nCalculating edge weight. The weight $w(a, s_i, V_{a,j})$ of the branch connecting a child $s_i$ and its parent $a$'s $j$-th semantic group $V_{a,j}$ is defined as:\n\nw(a, s_i, V_{a,j}) = \\frac{T(a, s_i, V_{a,j})}{\\sum_{s \\in S_a} ||T(a, s, V_{a,j})||},\n\nwhere $T(a, s_i, V_{a,j})$ is the importance score of $s_i$ to $a$ calculated over $V_{a,j}$."}, {"title": "3.5 DETERMINING GROUPS OF NEURONS AND CONSTRUCTING CONCEPT CIRCUIT", "content": "This section describes our algorithms to (1) cluster the set of core concept neurons $S_a = \\{s_1, ..., s_k\\}$ into distinct groups, (2) identifying the concept associated with each group, and (3) quantifying the interaction between the groups.\n\nClustering neurons into groups. As mentioned in the previous section, a single neuron can encode multiple distinct visual features, while several neurons may also capture the same visual feature Cammarata et al. (2020). We hypothesize that, due to the polysemantic nature of neurons (Cam-marata et al., 2020; O'Mahony et al., 2023), a model may struggle to accurately determine whether a concept is present in an input image by relying on a single neuron. As a result, the model pro-cesses visual features not by considering individual neurons in isolation but rather by operating at the level of neuron groups. Intuitively, a group of neurons consists of those that capture similar visual features. This can be interpreted as two neurons belonging to the same group if they share similar semantic concept groups.\n\nBuilding on this intuition, we develop a neuron clustering algorithm based on the semantic groups of each neuron's concept (Figure 3). Specifically, let $V_{s_i}$ represent the concept of neuron $s_i$ (i.e., the primary visual features it encodes), which can be decomposed into several semantic groups $\\{V_{s_i,1},..., V_{s_i,n_i}\\}$ (see Section 3.4), where $n_i$ is the number of semantic groups encoded by $s_i$. For each semantic group $V_{s_i,j}$, we calculate its representative activation vector $r_{s_i,j}$ by averaging the feature maps of all its visual features, i.e., $r_{s_i,j} := mean(\\phi^{l-1}(v_s)) .$ We then apply the agglomerative clustering algorithm to group the semantic groups $V_{s_i,j} (i = 1, ..., k; j = 1, ..., n_i)$, where the distance between any two groups $V_{s_i,u}$ and $V_{s_j,w}$ is determined by the distance of their respective representative activation vectors, $r_{s_i,u}$ and $r_{s_j,w}$. Finally, we assign neurons $s_1,..., s_k$ to the same groups based on their semantic concept groups. Specifically, neurons $s_i$ and $s_j$ are clustered together if there exists a semantic group $V_{s_i,u}$ (of $s_i$) and a semantic group $V_{s_j,w}$ (of $s_j$) belonging to the same group.\n\nFinding neuron group concept automatically. We define the concept associated with a group of neurons as the union of all visual features from the corresponding semantic groups. Specifically, let $\\{V_{G,1},..., V_{G,k}\\}$ represent the semantic groups categorized into a cluster, with their corresponding neurons $\\{S_{G,1},..., S_{G,k}\\}$ grouped together in the same set, denoted as $G$. The concept of this group, denoted as $V_G$, is then defined as the union of the sets $\\{V_{G,1}, . . ., V_{G,k}\\}$, i.e., $V_G := \\cup_{i=1}^k V_{G,i}$. We leverage a Multimodal LLM to automatically assign labels to the concept, thereby eliminating the need for a predefined labeled concept dictionary. Further details on the design of the prompts are provided in the Appendix G.\n\nConstructing concept circuit. For a given class $c$, the concept circuit $C_e$ is a hierarchical tree where each node represents a neuron group concept (NGC), and each edge illustrates the contribution of the child neuron group to its parent. For a node $G$, we denote by $V_G = \\{V_{G,1},..., V_{G,|V_G|}\\}$ the set of semantic groups associated with $G$, and $S_G = \\{s_{G,1},..., s_{G,|S_G|}\\}}$ represent the neurons corre-sponding to the semantic groups in $V_G$, i.e., $s_{G,j}$ is the core concept neuron possesses the semantic group $V_{G,j} (j = 1, ..., |V_G|)$. Let $G_i$ and $G_j$ be a child-parent pair in the tree, then, the relationship between $G_i$ and $G_j (quantified by W(G_i, G_j))$ is represented by two aspects: the number of edges connecting elements of $G_i$ and $G_j$, and the weights of those connecting edges. The more the edges and the higher the edge weights, the stronger the relationship between $G_i$ and $G_j$. Accordingly, we define the weight of branch connecting a child $G_i$ to its parent $G_j$ as sum of the attribution of each neuron in $S_{G_i}$ with each neuron in $S_{G_j}: W(G_i, G_j) := \\sum_{s_{G_j,p} \\in S_{G_j}} \\sum_{s_{G_i,q} \\in S_{G_i}} w(s_{G_j,p}, s_{G_i,q}, V_{G_j,p}).$"}, {"title": "5 APPLICATIONS", "content": "We outlines some applications of NeurFlow. We hypothesize that, as one neuron can have multiple meanings, a DNN looks at a group of neurons rather than individually to determine the exact features of the input. Hence, we propose a metric that assesses a model's con-fidence in determining whether the input contains a specific visual feature. For a group $G$ with core concept neurons $S_G = \\{s_{G,1},\u2026\u2026\u2026,s_{G,|S_G|}\\}$, the metric denoted as $M(v,S_G,D) = exp(\\sum_{s \\in S_G} log(||\\phi_s(v)/ max(\\phi_s,D))||))$, where $v \\in D$ and $max(\\phi_s,D)$ is the highest value of activation of neuron $s \\in S_G$ on dataset $D$. This returns high score when all neurons in $G$ have high activation (indicating high confidence), while resulting in almost zero if any neuron in the group has low activation (indicating low confidence). We can use this metric to determine how similar the features in the input image are to the predetermined neuron groups concept. The specific setup can be found in the Appendix F. Figures 7 and 9 demonstrate the usage of the metric and the concept circuit. We use the term NGC to denote the concept of a neuron group."}, {"title": "5.1 IMAGE DEBUGGING", "content": "We aim to use the concept circuit to identify concepts contributing to false prediction, which we call image debugging. If a concept contributes to a class when it should not, we say that the prediction (or equivalently, the model) is biased by that concept. Kim et al. (2024) propose a framework for detecting biases in a vision model by generating captions for the predicted images and tracking the common keywords found in the captions. With this method, they concluded that the pretrained ResNet50 is biased by \u201cflower pedals\u201d in the class \u201cbee\u201d. However, correlational features do not imply causation and can lead to misjudgments. We verify and enhance the causality of their claim by examining the concept circuit of class \u201cbee\u201d, and conducting experiments on the probabilities of the final predictions with and without neurons that related to \"flowers\". Additionally, we discover that the model also suffers from \"green background\" bias (resemble \u201cleaves\u201d), which is not mentioned in Kim et al. (2024).\n\nFigure 9 shows the process of debugging false positive images. Three different concepts are pre-sented in layer4.2 of ResNet50, representing \u201cpink pedals\u201d, \u201cgreen background\u201d, and \u201cbee\u201d respec-tively (we choose this layer as it has a small set of NGCs, however, our following experiment is consistent for multiple layers and with different classes). We discover that most of the false positive images have high metric score for \u201cpedal\u201d and \u201cgreen background\u201d. To further verify the impact of these biased features, we mask all neurons in the groups of the respective concepts and find that the probability of the predictions are distorted drastically (and predictions is no longer \u201cbee\u201d), as opposed to masking random neurons, which yield negligible changes.\n\nThis implies the dependence on the biased concept. But how do we know that the groups reflect the respective visual features? If these groups indeed represent the visual features, then masking them should hinder the classification probability for images that include those features. We highlight the"}, {"title": "5.2 AUTOMATIC IDENTIFICATION OF LAYER-BY-LAYER RELATIONS", "content": "While automatically discovering concepts from inner representation has been a prominent field of research (Fel et al., 2023), automatically explaining the resulting concepts is often ignored, relying on manual annotations. Bykov et al. (2024) utilize label description in ImageNet dataset to generate caption for neurons, however, these annotations is limited and can not be used to label low level concepts. Drawing inspiration from Hoang-Xuan et al. (2024); Kalibhat et al. (2023), we go one step further and not only use MLLM to label the (group of) neurons but also explain the relations between them in consecutive layers. Thus, we show the prospect of completing the whole picture of abstracting and explaining the inner representation in a systematic manner.\n\nSpecifically, for two consecutive layers, we ask MLLM to describe the common visual features in a NGC, then matching with those of the top NGC (with the highest weights) at the preceding layer. This can be done iteratively throughout the concept circuit, generating a comprehensive explanation without human effort. We use a popular technique (Wei et al., 2022) to guide GPT4-0 (OpenAI, 2024) step by step in captioning and in visual feature matching. Figure 8 shows an example of applying this technique to concept circuit of class \u201cgreat white shark\u201d. We observe that MLLM can correctly identify the common visual features within exemplary images of NGCs. Furthermore, MLLM is able to match the features from lower level NGCs to those at higher level, detailing formation of new features, showing the potential of explaining in automation, capturing the gradual process of constructing the output of the model. The prompt used in this experiment is available in Appendix G."}, {"title": "6 CONCLUSION", "content": "We introduced NeurFlow, a framework that systematically elucidates the function and interactions of neuron groups within neural networks. By focusing on the most important neurons, we revealed relationships between neuron groups, which are often obscured by the inherent complexity of neural network structures. Furthermore, we fully automated the processes of identifying, interpreting, and annotating neuron group circuits using large language models. Our method aims to provide a more efficient and comprehensive approach to the automated interpretation of neural activity and applicability of NeurFlow across a variety of domains, including image debugging and automatic concept labeling."}, {"title": "A NOTATIONS", "content": "We summarize the notations used in this work in Table 1."}, {"title": "B RELATED WORKS SUMMARIZATION", "content": "Table 2 compares our proposed method and existing approaches."}, {"title": "C LIMITATIONS AND DISCUSSION", "content": "While we view NeurFlow as a significant step toward understanding the function and interaction of neuron groups, it is not without limitations. Our approach defines the concept of neurons as the top-k most activated visual features, a common practice in the field (O'Mahony et al., 2023; Mu & Andreas, 2020; Nguyen et al., 2016). However, other researchers have broadened this definition to include concepts spanning a wider range of activation patterns (La Rosa et al., 2023b; Oikarinen & Weng, 2024a). This limitation highlights a promising direction for future research: developing more"}, {"title": "D ABLATION STUDIES", "content": "In this section, we run an ablation study on different choices of attribution method apart form our integrated gradient (IG) approach, verifying that IG-based score is the most suitable for the quantification of edge weights. We assess four additional common pixel attribution methods, including LRP (Bach et al., 2015), Guided Backpropagation (Springenberg et al., 2014), SmoothGrad (Smilkov et al., 2017), Saliency (Simonyan et al., 2014), Gradient Shap (Lundberg, 2017). Notably, Smooth-Grad and Gradient Shap are a follow-up versions of IG. Furthermore, we also evaluate attribution method used in Vu et al. (2022), which also find important neurons and attributing scores to them, referred to as Knockoff (Candes et al., 2018). We run on the same setup as in Section 4 for \u03c4 ranging from 0 to 50. For easier comparison, we report the mean correlations of all values of \u03c4. Figure 11 show the mean correlations across the last 10 layers of ResNet50 (He et al., 2016) and GoogLeNet (Szegedy et al., 2015). The Integrated Gradient consistently yields higher correlations compared to other attribution method, surpassing its follow-up version SmoothGrad, while being comparable with Gradient Shap. Furthermore, Knockoff shows a poor performance in ranking the importance of neurons compared to other attribution methods.\n\nAdditionally, we also assess the running time of each method. Specifically, we recorded the run time of each method on 50 images on CPU (we implement Knockoff on KnockPy library (Spector & Janson, 2021+) which does not run on GPU, hence, we evaluate all others on CPU for a fair comparison) across all layers of GoogLeNet. The results in Figure 10 show that IG maintain a small running time compared to the follow-up method (i.e. SmoothGrad and Gradient Shap), while yielding the best correlations among the attribution methods. Hence, we choose IG-based score to assign the edge weights in NeurFlow."}, {"title": "D.2 NEURON GROUP RELATION WEIGHTS AGGREGATION", "content": "In this experiment, we compare our choice of summing the edge weights with averaging the edge weights in forming W(Gi, Gj) in Section 3.5. Our aim is to verify that: groups of neurons with higher sum of scores will have higher impact on a target neuron, regardless of the number of neurons in the group."}, {"title": "D.3 QUALITATIVE COMPARISON OF IMAGE DEBUGGING WITH NEUCEPT", "content": "We conduct a qualitative experiment to compare the set of critical neurons identified by Vu et al. (2022) (the core concept neuron w.r.t the output logit of the model) and our set in the image de-bugging experiment. Specifically, following the setups in the experiment in section 5.1, we identify the top \u03c4 = 16 core concept neurons at layer 4.2 of ResNet50 for both methods, which are used to determine the top-2 groups of core concept neurons for a given misclassified image. Groups of neurons were identified following the methodology described in section 3.5, where the groups with the highest metric scores (defined in equation 5) are selected. Furthermore, to quantify the contribu-tions of the selected groups to the model output, we mask all of neurons in each groups and measure the changes of probability of the final predictions. The higher the changes, the more \u201ccritical\u201d the groups of neurons. We select three classes, without cherry-picking, namely: Bald Eagle, Great White Shark, and Bee (corresponding to the classes in figure 7, 8, and 9). The results are presented in figure 13, 14, and 15."}, {"title": "D.4 QUANTITATIVE COMPARISON OF CORE CONCEPT NEURONS OF THE MODEL OUTPUT", "content": "We run an experiment to further verify: although our method focuses on the set of core concept neurons w.r.t a specific target neuron, our identified neurons also have strong influence to the perfor-mance of the model.\n\nSpecifically, we evaluate the overlaps between our core concept neurons and the critical neurons (which are specifically designed to find important neurons for the model output) determined by Khakzar et al. (2021a) and Vu et al. (2022), then average the results across all layers of ResNet50 and GoogLeNet of 10 random classes. The numbers of core concept neurons are set to be the same for all three methods. We measure the $F_1$ scores of the overlaps, which are shown in table 3. The"}, {"title": "D.5 QUANTITATIVE COMPARISON OF CORE CONCEPT NEURONS OF A TARGET NEURON", "content": "We assess our method of identifying core concept neurons given a specific target neuron with the method used in Cammarata et al. (2020). In Cammarata et al. (2020), neurons are ranked based on the top neurons with the highest L2 weights connected to the target neuron. Note that this method is not applicable in other experiments since calculating weight magnitude is limited to consecutive layers.\n\nFor this comparison, we identify the top \u03c4 = 16 core concept neurons in two consecutive layers (separated by one convolution layer, as per the setup in Cammarata et al. (2020)) using both methods. We then knock out these core concept neurons to observe how the target neuron's concept is affected. The extent of this change is quantified by the loss function defined in 4, where a lower loss indicates better performance. We randomly selected 100 neurons across 10 different convolution layers from both models and calculated the average difference in losses between the two methods. A negative result indicates our method produces a better loss, while a positive result indicates otherwise.\n\nThe results are summarized in table 4. These findings demonstrate that our method is more effective at identifying core concept neurons. Additionally, gradient-based approaches are more versatile, as they can be applied to non-consecutive layers (e.g., ResNet Block 4.2 \u2192 ResNet Block 4.1 in our experiments), whereas the L2-weight-based approach is limited to consecutive layers."}, {"title": "D.6 DEPENDENCE ON THE CHOICES OF \u03c4", "content": "The trade-off of the parameter \u03c4: In this experiment, we aim to study the choices of parameter \u03c4 on the set of core concept neurons of a model. Specifically, in the experiment \u201cFidelity of core concept neurons\u201d, the choice of \u03c4 can be seen as a trade-off between simplicity (the number of core concept neurons) and performance (the accuracy of the prediction when retaining only the core concept neurons). However, for \u03c4 = 4, 8 the results are vary across our tested models. We conduct additional experiment to highlight that for sufficiently large \u03c4, the results are less dependent on the parameter.\n\nWe evaluate on 10 different labels with the same setups as in the experiment \u201cFidelity of core con-cept neurons\u201d for \u03c4 = 20, 24. The results in figure 16 show that with these higher \u03c4 values, the performance drops of the model become negligible. Furthermore, the differences between retaining for \u03c4 = 20 and \u03c4 = 24 at all layers are minimal, suggesting that the dependence on \u03c4 decreases as we increase the value.\n\nCompleteness of core concept neurons on the output: Additionally, we run an experiment to assess the completeness of NeurFlow in identifying the important neurons for the model\u2019s output. By greedily adding 50% more neurons in each layer, of which the neurons are ranked by the importance scores defined in Khakzar et al. (2021a). The higher the scores, the stronger the influence on the"}]}