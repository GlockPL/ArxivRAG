{"title": "MoM: Linear Sequence Modeling with Mixture-of-Memories", "authors": ["Jusen Du", "Weigao Sun", "Disen Lan", "Jiaxi Hu", "Yu Cheng"], "abstract": "Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state, which leads to suboptimal performance on recall-intensive downstream tasks. Drawing inspiration from neuroscience, particularly the brain's ability to maintain robust long-term memory while mitigating \"memory interference\", we introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with a router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. As a result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linear-complexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM significantly outperforms current linear sequence models on downstream language tasks, particularly recall-intensive tasks, and even achieves performance comparable to Transformer models.", "sections": [{"title": "1 Introduction", "content": "Attention mechanisms have made significant contributions to the field of artificial intelligence, advancing various modalities such as language, vision, audio, video, graphs, and even time series (Achiam et al., 2023; Team, 2023). The Transformer (Vaswani, 2017), known for its ability to capture long-range dependencies, has become a foundational architecture in this space. However, traditional Transformers encounter computational challenges due to their quadratic time complexity, $O(n^2)$, with respect to sequence length n, making it difficult to scale to long sequences. To overcome this limitation, several linear sequence modeling methods have been proposed, including linear attention (Katharopoulos et al., 2020; Qin et al., 2023a; Li et al., 2025), state space modeling (Gu and Dao, 2024; Dao and Gu, 2024), and linear RNNs (Peng et al., 2024; Qin et al., 2024d), which offer $O(n)$ training complexity and $O(1)$ inference complexity. These approaches often reduce the input sequence to a fixed-size hidden space, collapsing the information into a single \"memory state\". While these methods enhance efficiency, they face two main challenges: limited memory capacity and memory interference. When new information overwrites the single fixed-size memory state, previously stored representations may degrade, which negatively impacts its long-term memory performance on recall-intensive tasks.\nWe argue that the strong performance of Transformer models on recall-intensive tasks arises from their ability to avoid \"memory interference\" by maintaining independent key-value caches for each token, thus offering virtually unlimited memory capacity. In contrast, linear sequence modeling relies on extreme compression, consolidating all the input information into a single fixed-size memory state (Katharopoulos et al., 2020; Dao and Gu, 2024). This approach results in limited memory capacity and inherently leads to memory interference issues.\nInterestingly, the human brain has developed mechanisms that enable large memory capacity while reducing memory interference. Neuroscience studies show that in the hippocampus, theta oscilla-"}, {"title": "2 Preliminary", "content": "For notations in this work, we use bold lower-case letters for row vectors (e.g., qt, kt), bold upper-case letters for matrices (e.g., Q, K) and the identical letters represent a row in the matrix, e.g., qt is the t-th row of Q.\nSoftmax Attention\nGiven input $X = [x_1,...,x_n]^T \\in \\mathbb{R}^{n \\times d}$, where n is the sequence length and $x_t \\in \\mathbb{R}^d$ is the t-th input vector with d dimensions. Transformer (Vaswani, 2017) softmax attention computes:\n$Q = XW_Q, K = XW_K, V = XW_V, (1)$\n$O = \\text{softmax}(\\frac{QK^T}{\\sqrt{d}})V, (2)$\nwhere $Q, K, V \\in \\mathbb{R}^{n \\times d}$, $A \\in \\mathbb{R}^{d \\times d}$ is an attention mask, $\\odot$ denotes element-wise production.\nThe training time complexity is $O(n^2)$ due to the $QK^T$ operation. During inference, Transformers store all the k, v calculated as \"KV cache\" which can be viewed as memory states. As the KV cache grows with the input, the Transformers can be considered to separate the memory storage for each input token individually and have an unlimited memory capacity. This also results in a time complexity of $O(n)$ per token during inference.\nLinear Attention\nTo reduce the time complexity of Transformer attention, various optimization techniques have been proposed. Linear Transformers (Katharopoulos et al., 2020) replace the softmax attention mechanism with dot-product of feature maps $\\phi(\u00b7)$:\n$O_t = \\frac{\\sum_{i=1}^n \\phi(q_t)\\phi(k_i)v_i}{\\sum_{i=1}^n \\phi(q_t)\\phi(k_i)}, (3)$\nwhere $q_t, k_t, v_t \\in \\mathbb{R}^d$. While the presence of the denominator may lead to numerical instability (Qin et al., 2024b) and the feature map can utilize an identity function, which we omit for simplicity. In"}, {"title": "3 Method", "content": "Linear sequence models compress the entire sequence data into a fixed-size memory state. Despite numerous efforts to minimize information"}, {"title": "3.1 Motivation", "content": "Linear sequence models compress the entire sequence data into a fixed-size memory state. Despite numerous efforts to minimize information loss\u2014such as introducing gating mechanisms and employing more precise control over memory modifications (Orvieto et al., 2023; De et al., 2024; Beck et al., 2024; Yang et al., 2023; Zhang et al., 2024)---some degradation in this compression process is inevitable. Expanding the memory capacity has been shown to mitigate this issue to some extent, with studies indicating that increasing memory capacity can enhance model performance (Qin et al., 2024d; Peng et al., 2024).\nHowever, previous approaches that simply increased the size of the RNN state, essentially expanding a single memory state, struggled to capture the full spectrum of information within an entire sequence. We propose that this difficulty arises because sequence information is often multifaceted, and a single, expanded memory may not be capable of simultaneously capturing multiple aspects of the data. Inputs that introduce new or orthogonal information may interfere with existing memory content when using a shared memory. Rather than discarding these inputs through gating mechanisms or overwriting the existing memory state, it may be more effective to consider alternative strategies that allow for the preservation of diverse information without interference."}, {"title": "3.2 MoM: Mixture-of-Memories", "content": "To address the challenge outlined above, we propose a novel approach inspired by biological mechanisms for encoding multi-item memory such as theta-gamma oscillations (Lisman and Jensen, 2013), and concepts from Mixture-of-Experts (MoE) (Shazeer et al., 2017), where different experts handle specific tokens. In this approach, we leverage multiple memory states, each of which is selectively updated by different inputs. This increases the memory capacity and enables the model to retain diverse pieces of information by storing various types of inputs in separate memory states.\nIn our framework, the memory states function similarly to the experts in MoE. However, instead of relying on completely separate networks, these modules are individual RNN states embedded within a linear recurrent mechanism. This design allows for the isolation of memory updates while concurrently managing distinct types of information. It is important to note that MoM fundamentally differs from traditional MoE, as we will discuss in Appendix A. Figure 1 provides an overview of the MoM architecture. Below, we introduce the structure of the MoM layer and explain how this"}, {"title": "3.2.1 Router Network", "content": "We use a router to assign inputs to different memory states. Utilizing the top-k concept, each token is routed to the top-k memories based on its importance scores. Specifically, we use a simple linear layer to generate these scores for each input token. After applying a softmax function, we select the top-k scores and normalize them.\n$scorest = TopK(\\text{softmax}(x_tW_g)) \\in \\mathbb{R}^k, (5)$\n$gt = \\frac{scorest}{\\sum scorest} \\in \\mathbb{R}^k, (6)$\nwhere $x_t \\in \\mathbb{R}^d$, k is the top-k number, $W_g \\in \\mathbb{R}^{d \\times M}$ is learnable weight, $g_t$ is the normalized importance scores of the input $x_t$."}, {"title": "3.2.2 Linear Recurrent Memory Module", "content": "After the router network, the input $x_t$ is directed to top-k linear recurrent modules, meaning that the top-k memories are activated while the others remain inactive. For each activated memory module, indexed by m, we perform the following operation:\n1. Key and Value Projections: We project the input $x_t$ to $k_t^m$ and $v_t^m$ using $W_k^m$ and $W_v^m$:\n$k_t^m = x_tW_k^m, v_t^m = x_tW_v^m \\in \\mathbb{R}^d, (7)$\nwhere $W_k^m, W_v^m$ are learnable projection weights for kv of the m-th memory module.\n2. Memory Update: We update the activated memory state using $k_t^m, v_t^m$:\n$M_t^m = M_{t-1}^m + (k_t^m)^Tv_t^m \\in \\mathbb{R}^{d \\times d}, (8)$\nThe equation above represents the simplest form of memory update for clarity. Our approach is flexible and does not rely on a specific memory update mechanism. To enhance performance, we can incorporate mechanisms such as forget gates (Sun et al., 2023):\n$M_t^m = \\gamma M_{t-1}^m + (k_t^m)^Tv_t^m \\in \\mathbb{R}^{d \\times d}, (9)$\nwhere $\\gamma$ is a constant forget gate.\nMore generally, our method can be adapted to incorporate various memory update methods proposed in previous work. Detailed descriptions of these methods are provided in Table 1."}, {"title": "Memory Mixing", "content": "After updating the activated memory states, we perform a weighted sum of these memory states using the importance scores obtained from Equation(6).\n$\\hat{M}_t = \\sum_m g_t^{(m)} M_t^m \\in \\mathbb{R}^{d \\times d}, (10)$\nwhere $M_t^m$ is one activated memory and $g_t^{(m)}$ is the importance score of $M_t^m$.\nWe then obtain the output of the MoM by applying query vector qt to the mixed memory $\\hat{M}_t$:\n$o_t = q_t \\hat{M}_t \\in \\mathbb{R}^d, (11)$\nFinally, the output of the MoM layer is computed by applying an activation function, normalization, and a linear transformation:\n$o_t = RMSNorm(Swish(o_t)) W_o \\in \\mathbb{R}^d, (12)$\nThroughout the recurrent process, only a subset of memory states is activated and updated at each time step, while memory states that are not routed remain inactive and unchanged. When the input passes through the key-value projection layer, it generates multiple sets of keys and values that are fed into different memory modules. This design enables the model to maintain multiple memory states, each preserving distinct pieces of information. By aggregating the activated memories into a comprehensive mixed memory by weighted summation, the query can effectively retrieve information from this mixed memory, which results the \"attention output\" followed by other layers."}, {"title": "3.3 Optimizations for MoM", "content": "Shared Memory. To enhance our model's ability to capture long-term dependencies, we introduce a shared memory mechanism. This shared memory has access to the entire sequence information, allowing it to effectively store and retrieve long-term information. By integrating shared memory into our model, we ensure that it can leverage the complete historical context, resulting in significant improvements in performance and robustness.\n$k_1 = x_tW_k^s, v_1 = x_tW_v^s \\in \\mathbb{R}^d (13)$\n$M_1 = M_{t-1} + (k_1)^Tv_1 \\in \\mathbb{R}^{d \\times d} (14)$\n$M = M_1 + \\sum g^{(m)} M^m \\in \\mathbb{R}^{d \\times d} (15)$\nm\nHardware-efficient Implementation. Chunk-wise parallel form of linear attention is a computational optimization strategy that divides the input sequence into smaller chunks to enable partial parallel and recursive computation during model training (Hua et al., 2022; Yang et al., 2023), which is well supported in open-source frameworks.\nIn the implementation of MoM, mixing the memories before multiplying by the query is mathematically equivalent to firstly multiplying each memory by the query and then mixing the results. This property allows us to reuse the efficient Triton-based operator implementation of all previous linear sequence modeling methods, facilitate the hardware-efficient training as well as inference of MoM."}, {"title": "4 Experiments", "content": "In our experiments, we employ the Gated DeltaNet (Yang et al., 2024) as the memory update mechanism in MoM. The model is configured with four memory states, two of which are activated at each time step, along with a shared memory.\nBaselines. We evaluate MoM against several linear recurrent models and Transformers, including RetNet (Sun et al., 2023), GLA (Yang et al., 2023), Gated DeltaNet (Yang et al., 2024), and Transformer++ (Touvron et al., 2023), which incorporates Rotary Position Embeddings (Su et al., 2024) and GLU (Shazeer, 2020) into the Transformer architecture. To ensure a fair comparison, we train all baseline models from scratch using the exact same number of tokens.\nTraining. We follow the training procedure described by Yang et al. (2023), utilizing the SlimPajama dataset (Soboleva et al., 2023) sampled with 100B tokens and tokenized using the Mistral tokenizer (Jiang et al., 2023). We train models from scratch with parameter sizes of 340M and 1.3B, respectively. For the 340M models, we train on 15B tokens with a batch size of 0.5M tokens. The warmup tokens count is set to 0.25M. We set the hidden ratio of our model to 3 to keep the overall parameter count approximately the same. For the 1.3B models, we train on 100B tokens with a batch size of 2M tokens. We utilized publicly available pretrained weights from Zhang et al. (2024) with exactly same configuration. The warmup tokens count is 1B. We employ AdamW optimizer"}, {"title": "4.2 Main Results", "content": "Linear sequence models, due to their limited memory capacity, often exhibit a significant performance gap compared to Transformer models, especially in recall-intensive tasks where extensive context is crucial. These tasks highlight notable performance differences among various linear models, making them a more accurate benchmark for evaluating a linear model's capabilities in handling contextual information.\nTo thoroughly assess our model's proficiency in such scenarios, we test six recall-intensive tasks following Arora et al. (2024): FDA (Arora et al., 2023), SWDE (Arora et al., 2023; Lockard et al., 2019), SQUAD (Rajpurkar et al., 2018), NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017) and Drop (Dua et al., 2019). These tasks are designed to challenge a model's ability to perform context-based retrieval and comprehension.\nAs shown in Table 2, our proposed approach, benefiting from increased memory capacity and memory mixing mechanism, achieves significant improvements over other linear sequence models. Specifically, our model effectively narrows the performance gap with Transformer models. This improvement underscores the advantage of our method in capturing and utilizing long-range dependencies, thereby enhancing performance on tasks"}, {"title": "4.2.2 Commonsense Reasoning Tasks", "content": "As shown in Table 3, we report the language modeling perplexity and zero-shot performance of commonsense reasoning tasks following (Zhang et al., 2024) which includes WikiText (Merity et al., 2016), LAMBADA (Paperno et al., 2016), ARC-easy, ARC-challenge (Clark et al., 2018), HellaSwag (Zellers et al., 2019), PiQA (Bisk et al., 2020) and WinoGrande (Sakaguchi et al., 2019). The evaluation results are based on the lm-evaluation-harness (Gao et al., 2024).\nExperimental results show that MoM outperforms other linear models and surpassed the Transformer model as well."}, {"title": "4.2.3 Long Context Tasks", "content": "Assessing performance on long-context tasks is crucial for linear models, as it reflects their ability to handle long-range dependencies effectively. We evaluated our model's comprehension of long contexts using the LongBench benchmark (Bai et al., 2024; Contributors, 2023). In Table 4, we present the average results across various categories, including summarization, few-shot learning, synthetic tasks, and code completion, along with the overall mean across all tasks. The complete detailed results are provided in Appendix 7."}, {"title": "4.2.4 Mixed Memory vs. Single Memory", "content": "To validate the effectiveness of our mixed memory mechanism, we compare our MoM model with mixed memories to a baseline model that uses an expanded single memory with the same activated"}, {"title": "4.2.5 Efficiency", "content": "We compare the inference speed and memory usage of MoM and Transformer++ with flash attention in Fig 2. Our analysis demonstrates that MoM exhibits linear complexity, showcasing significant advantages over the Transformer model when handling long sequences. Specifically, MoM's efficient memory update mechanisms allow it to process longer inputs with reduced computational overhead, positioning it as a more scalable solution for large-scale natural language processing tasks."}, {"title": "4.2.6 Training Loss Comparison", "content": "To further assess the learning efficiency of MoM, we compared the training loss curves of MoM with those of other baseline models. As depicted in Figure 3, MoM consistently maintains the lowest loss throughout the entire training phase. Even"}, {"title": "4.2.7 Ablation", "content": "To evaluate the influence of memory hyperparameters in our MoM model, we conducted ablation studies using a 340 million parameter model trained on 15 billion tokens. Our investigation primarily focused on the impact of the number of memories, the number of activations, and the use of shared memory on model performance. The results of these experiments are presented in Table 6."}, {"title": "5 Related Work", "content": "Linear Recurrent Models\nLinear recurrent models, comprising linear attention, linear RNNs, state-space models (SSMs), have garnered significant research interests (Qin et al., 2023b). The advancement of SSMs began with the pioneering work on S4 (Gu et al., 2022), which was later optimized through a diagonalized version (Gupta et al., 2022). Despite their strong performance on the LRA benchmark, these models have faced challenges in language modeling mainly"}, {"title": "A Comparison between MoM and MoE", "content": "While our approach to implementing the Mixture-of-Memories (MoM) draws inspiration from the Mixture-of-Experts (MoE) framework, there are significant differences that distinguish our method from traditional MoE implementations.\n\u2022 Purpose: The MoE was introduced to scale up the number of parameters without significantly increasing computational resources. It address the limitations of dense models in scaling both parameters and computational demands through sparse activation. However, MoM is designed to expand the memory capacity of linear attention models while preserving their linear time complexity. By sparsely activating memories and using weighed summation to create a mixed memory, MoM effectively address the challenge of forgetting historical information in linear attention. Moreover, by separating the memory into distinct states, MoM reduces interference between different pieces of information.\n\u2022 Structure: In conventional MoE, each expert is a separate neural network within the feed-forward network (FFN) layer (Team, 2024). In contrast, in MoM, each memory is an RNN state with unique key-value projection weights to generate different key-value pairs. MoE operates during the channel mixing phase, where each token is processed independently by selected experts. On the other hand, MoM functions during the token mixing phase, where each memory processes different segments of the sequence, preserving inter-token relationships."}, {"title": "B Dataset and Benchmark", "content": "We pretrained model on SlimPajama dataset. For 340M models, we train on a sample of 15B tokens. For 1.3B models, we train on a sample of 100B tokens.\n\u2022 SlimPajama (Soboleva et al., 2023) is a high-quality, optimized subset of the Red-Pajama dataset, designed for large-scale language model training. It includes diverse text sources such as Common Crawl, Wikipedia, books, and GitHub code, with a primary focus on English. The dataset is cleaned, deduplicated, and optimized for efficiency and performance."}, {"title": "For the benchmark, we tested on these tasks:", "content": "For the benchmark, we tested on these tasks:\n\u2022 WikiText (Merity et al., 2016): A dataset consisting of high-quality Wikipedia articles, primarily in English, designed for language modeling tasks with 62 test samples. The text covers a wide range of topics, including history, science, and culture, and is authored by Wikipedia contributors, who come from diverse demographic backgrounds.\n\u2022 LAMBADA (Paperno et al., 2016): An English-language dataset for evaluating contextual understanding in language models with 5153 test samples. It consists of narrative texts sourced from books, requiring models to predict the final word of a passage. The data reflects a mix of literary styles and author demographics.\n\u2022 ARC-Easy & ARC-Challenge (Clark et al., 2018): A set of multiple-choice science questions in English, sourced from standardized exams and educational materials with 2376 and 1172 test samples. The dataset represents the domain of elementary and high school science, with questions authored by educators and test designers. ARC-Easy includes straightforward questions, while ARC-Challenge contains more difficult ones that require advanced reasoning.\n\u2022 HellaSwag (Zellers et al., 2019): An English-language dataset designed for commonsense reasoning, where models must choose the most plausible continuation of a sentence. The text is derived from activity descriptions (e.g., WikiHow), covering everyday scenarios. The dataset was constructed adversarially to be challenging for language models. It has 10003 test samples.\n\u2022 PiQA (Bisk et al., 2020): A dataset focused on physical commonsense reasoning in English with 3084 test samples. The text consists of everyday tasks and scenarios, requiring models to determine the most practical way to perform an action. The data is sourced from crowdsourced descriptions, reflecting a broad range of common human experiences.\n\u2022 WinoGrande (Sakaguchi et al., 2019): A large-scale English dataset for commonsense reasoning, based on the Winograd Schema"}, {"title": "Challenge with 1267 test samples. It tests pronoun resolution in ambiguous contexts, with sentences sourced and refined through crowdsourcing. The dataset aims to reduce annotation biases by diversifying sentence structures and topics.", "content": "Challenge with 1267 test samples. It tests pronoun resolution in ambiguous contexts, with sentences sourced and refined through crowdsourcing. The dataset aims to reduce annotation biases by diversifying sentence structures and topics.\n\u2022 FDA (Arora et al., 2024, 2023): An English-language dataset includes 100 randomly sampled PDF files of FDA 510(k) pre-market notification submissions, each with 16 manually annotated attributes like device classification and predicate device codes. It has 1102 test samples.\n\u2022 SWDE (Arora et al., 2024, 2023; Lockard et al., 2019): An English dataset dataset is used for evaluating information extraction systems. It comprises data from 8 movie websites (e.g., IMDB, Rotten Tomatoes) and 5 university websites (e.g., US News), with each site containing 1063 to 2000 annotated web pages and 1111 test samples.\n\u2022 SQUAD (Rajpurkar et al., 2018): A reading comprehension dataset with English questions based on Wikipedia articles with 2984 test samples. Answers are text segments from the articles or the questions can be unanswerable. SQUAD2.0 adds over 50,000 unanswerable questions to the original 100,000 from SQUAD1.1, requiring systems to both answer questions and identify when no answer is supported.\n\u2022 NQ (Kwiatkowski et al., 2019): A large-scale English dataset for open-domain question answering, where questions are sourced from real Google search queries. Answers are extracted from Wikipedia articles, reflecting general knowledge across various domains. It has 3157 test samples\n\u2022 TriviaQA (Joshi et al., 2017): An English-language question-answering dataset consisting of trivia-style questions with 1688 test samples. The dataset includes both human-generated questions and web-sourced evidence, covering a broad range of topics such as history, science, and entertainment.\n\u2022 DROP (Dua et al., 2019): A reading comprehension benchmark in English that requires discrete reasoning, such as arithmetic and logical operations, over passages. The dataset is sourced from Wikipedia and challenges models to go beyond simple span extraction by performing multi-step reasoning. It has 2087 test samples.\n\u2022 LongBench (Bai et al., 2024) is a bilingual (Chinese and English) benchmark for evaluating long-context understanding in large language models. It covers 21 tasks across six categories, including QA, summarization, and code completion with 4750 test samples."}, {"title": "C Experiments Details", "content": "Our experiments were conducted using 32 NVIDIA A800 GPUs. Training the 340M parameter model required approximately 10 hours, while the 1.3B parameter model took around 6 days. All models were trained and evaluated using a fixed random seed of 42 to ensure reproducibility. The reported results are based on a single run, without aggregation over multiple random seeds."}]}