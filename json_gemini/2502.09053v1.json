{"title": "Game Theory Meets Large Language Models: A Systematic Survey", "authors": ["Haoran Sun", "Yusen Wu", "Yukun Cheng", "Xu Chu"], "abstract": "Game theory establishes a fundamental framework for analyzing strategic interactions among rational decision-makers. The rapid advancement of large language models (LLMs) has sparked extensive research exploring the intersection of these two fields. Specifically, game-theoretic methods are being applied to evaluate and enhance LLM capabilities, while LLMs themselves are reshaping classic game models. This paper presents a comprehensive survey of the intersection of these fields, exploring a bidirectional relationship from three perspectives: (1) Establishing standardized game-based benchmarks for evaluating LLM behavior; (2) Leveraging game-theoretic methods to improve LLM performance through algorithmic innovations; (3) Characterizing the societal impacts of LLMs through game modeling. Among these three aspects, we also highlight how the equilibrium analysis for traditional game models is impacted by LLMs' advanced language understanding, which in turn extends the study of game theory. Finally, we identify key challenges and future research directions, assessing their feasibility based on the current state of the field. By bridging theoretical rigor with emerging AI capabilities, this survey aims to foster interdisciplinary collaboration and drive progress in this evolving research area.", "sections": [{"title": "1 Introduction", "content": "Game theory provides a mathematical framework for analyzing strategic interactions among rational agents and has evolved significantly since its seminal work [Von Neumann and Morgenstern, 2007]. Over the decades, it has established robust methodological foundations, including equilibrium analysis [Nash Jr, 1950] and mechanism design [Vickrey, 1961], which serve as essential analytical tools across disciplines such as economics and computer science.\nWith the rapid advancement of large language models (LLMs), researchers have increasingly explored the intersection of game theory and LLMs. A growing body of work investigates how game-theoretic principles can be used to evaluate and enhance LLMs and how LLMs can contribute to game theory. Specifically, existing studies apply game theory to develop theoretical frameworks for assessing LLMs' strategic reasoning. This approach optimizes their training methodologies and analyzes their societal implications. Key research directions include:\n\u2022 Standardized Game-Based Evaluation: Researchers are constructing benchmark environments, such as matrix games [Akata et al., 2023] and auctions [Chen et al., 2023], to evaluate the strategic reasoning capabilities of LLMs systematically.\n\u2022 Game-Theoretic Algorithmic Innovation: Concepts from cooperative and non-cooperative game theory, such as Shapley Value [Enouen et al., 2024] and max-min equilibria [Munos et al., 2024], are inspiring novel approaches to model interpretability and training optimization.\n\u2022 Societal Impact Modeling: As LLMs transform information ecosystems, new theoretical frameworks are emerging to predict the societal consequences of human-AI interactions [Yao et al., 2024], particularly in domains like advertising markets [Duetting et al., 2024] and content creation [Fish et al., 2024a].\nBeyond these applications, recent research suggests that LLMs can also contribute to game theory by facilitating equilibrium analysis in complex text-based scenarios and extending classical models to more realistic settings.\nExisting surveys [Zhang et al., 2024b; Feng et al., 2024; Hu et al., 2024] primarily examine how game theory can be used to build evaluation environments and assess LLMs' strategic performance. For instance, [Zhang et al., 2024b] classifies studies based on the game scenarios used to test LLM capabilities and methods for improving their reasoning. Meanwhile, [Feng et al., 2024] and [Hu et al., 2024] categorize the core competencies required for LLM-based agents in games, such as perception, memory, role-playing, and reasoning. While these surveys provide valuable insights, they primarily focus on the role of game theory in standardized evaluation frameworks, overlooking its broader potential for advancing LLM development. Moreover, they adopt a unidirectional perspective, treating game theory as a tool for as-"}, {"title": "2 Game Theory for LLM Evaluation", "content": "In this section, we explore the integration of LLMs within the context of game theory, focusing on their evaluation as game players. Behavioral evaluations reveal that LLMs face challenges in identifying optimal actions in classic matrix games, yet they can demonstrate human-like strategies in more complex game scenarios. Several studies have explored methods to enhance LLMs' performance as game players, and two significant points can be identified from that: recursive thinking and auxiliary modules. Finally, we also discuss the role of LLMs in games beyond their function as players."}, {"title": "2.1 Evaluation of LLMs' Behavioral Performance", "content": "Struggles of LLM in Matrix Games. Matrix games are a fundamental concept in game theory. In a matrix game, two players make simultaneous decisions, and the outcomes can be represented by a finite payoff matrix. Recent studies have investigated how LLMs respond to these games by converting them into natural language prompts. Despite significant advancements, their results show that LLMs such as GPT-4 struggle to consistently select the optimal strategy in 2 \u00d7 2 matrix games [Akata et al., 2023; Herr et al., 2024; Lor\u00e8 and Heydari, 2024; Wang et al., 2024].\nFor instance, [Akata et al., 2023] states that LLMs frequently fail to choose the optimal action in coordination games, like the Battle of the Sexes. Similarly, [Lor\u00e8 and Heydari, 2024] examines how contextual framing and utility matrices influence LLM decision-making, revealing significant biases. Furthermore, [Herr et al., 2024] explores the impact of game descriptions, player positioning, and payoffs on LLM performance, highlighting consistent behavioral biases. In more dynamic settings, [Fan et al., 2024] observes that LLMs struggle to predict optimal strategies in ring-network games. Additionally, the TMGBench benchmark, which evaluates LLMs across 144 distinct 2 \u00d7 2 matrix games, further confirms these limitations [Wang et al., 2024].\nThe matrix game is a cornerstone of game theory and is the foundation for more intricate strategic challenges. Studying LLMs' behaviors in such games provides valuable insights into their broader limitations in complex reasoning tasks.\nHuman-like Strategies of LLM in Realistic Game Scenarios. Beyond classic matrix games, numerous studies have analyzed LLM performance in more realistic game settings. While these games feature greater contextual complexity, they are not necessarily more challenging for LLMs. This is because strategic inference based on textual content can sometimes replace explicit computation.\nResearch indicates that LLMs can exhibit strategic behavior in communication-based games. In deception and negotiation games, including Werewolf [Xu et al., 2023; Du and Zhang, 2024] and Avalon [Wang et al., 2023; Lan et al., 2024], LLMs demonstrate behaviors such as deception, trust-building, and leadership-traits typically associated with human strategic thinking. These findings suggest that LLMs can function as sophisticated communication agents in games.\nLLMs have also demonstrated strategic reasoning in economically significant scenarios such as bargaining and pricing games. For instance, [Deng et al., 2024] finds that LLMs possess advanced negotiation skills, while [Fish et al., 2024b] shows that LLM-based pricing agents can autonomously engage in collusion to set prices above competitive levels. In auction contexts, [Guo et al., 2024] finds that LLMs can formulate rational bidding strategies based on historical data, often converging toward a Nash equilibrium. Similarly, [Chen et al., 2023] introduces AucArena, a"}, {"title": "2.2 Enhancing LLMs' Game Performance", "content": "Building on the evaluation of LLMs' performance in various games, numerous studies have explored methods to enhance their strategic reasoning and decision-making. These works address key challenges LLMs face in gameplay and propose general frameworks for improving their capabilities. Below, we outline two significant approaches.\nRecursively Thinking. In games requiring long-term or multi-level reasoning, LLMs often struggle to retain and build upon previous information, leading to suboptimal decision-making. To mitigate this, researchers have developed techniques that encourage LLMs to engage in recursive thinking, enabling them to leverage past information better when formulating strategies.\nFor instance, [Wang et al., 2023] introduces the Recursive Contemplation (ReCon) framework. The framework prompts LLMs to engage in first-order and second-order perspective-taking during Avalon gameplay. This helps them avoid common pitfalls, such as deception. Similarly, [Duan et al., 2024a] proposes a method where LLMs predict future moves in multi-turn games, improving their ability to anticipate opponents' strategies. Additionally, [Zhang et al., 2024a] advances LLM reasoning through k-level rationality, which enhances multi-level thinking and significantly increases their win rates in competitive settings."}, {"title": "2.3 Beyond as a Game Player", "content": "While much of the discussion has been centered on utilizing game-based scenarios to evaluate LLMs, work has also shown that LLM capability in games can, in turn, contribute to game theory. This section explores alternative roles for LLMs within game-theoretic contexts, broadening their applications.\nIn section 2.1, we noted that LLMs often struggle to compute optimal strategies in classical matrix games. However, some studies take an alternative approach by leveraging LLMs' natural language understanding instead of their ability to compute equilibria directly. For example, [Mensfelt et al., 2024] utilizes LLMs to formalize game descriptions into a Game Description Language (GDL), allowing external solvers to process them. Similarly, [Deng et al., 2025] introduces a two-stage framework for translating extensive-form games: first, the LLM identifies the information set, and then it constructs the complete game tree using in-context learning. These studies suggest that LLMs can act as intermediaries in converting natural language into formal game structures, a capability beyond traditional models.\nAdditionally, [Horton, 2023] explores the use of LLMs as substitutes for human participants in behavioral economic experiments. Their findings indicate that LLMs can replicate classic behavioral economics results, providing a scalable and cost-effective alternative for conducting social science research. This underscores the potential of LLMs as valuable tools in experimental economics and social science studies, facilitating large-scale simulations and deeper insights into human decision-making."}, {"title": "3 Game Theory for Algorithmic Innovation", "content": "This section investigates how game-theoretic principles contribute to developing LLMs by informing algorithmic innovation. Game theory has proven instrumental in enhancing our understanding of LLMs, mainly through the use of tools like the Shapley Value and social choice models. These methods offer valuable insights into model interpretability, enabling a deeper understanding of how LLMs process and respond to input. Beyond interpretability, game theory also provides a framework for developing training objectives and evaluation metrics that address key challenges in LLM development, such as model heterogeneity and alignment with human preferences."}, {"title": "3.1 Game Theory for Phenomenological Understanding LLMs", "content": "This line of research applies classical game theory concepts to explain observable phenomena in LLMs, including patterns in text generation and the inherent limitations of training within specific frameworks. Such studies are particularly valuable given that LLMs are often treated as \"black boxes\" due to their proprietary nature and large-scale complexity.\nOne approach connects cooperative game theory to LLMs, as these models perform parallel computations on input tokens and are structured around transformer layers. The Shapley Value [Shapley, 1953], a method for attributing contributions to individual players in cooperative games, has been adapted to assess the influence of specific tokens and layers on LLM-generated outputs. Several studies leverage the Shapley Value to evaluate token significance in prompts [Goldshmidt and Horovicz, 2024; Mohammadi, 2024]. For example, [Mohammadi, 2024] demonstrates that LLMs often assign disproportionately high weights to less informative input components, a behavior strongly correlated with incorrect responses. TokenSHAP [Goldshmidt and Horovicz, 2024] enhances Shapley Value computation using Monte Carlo sampling for efficiency, while TextGenSHAP [Enouen et al., 2024] extends the approach to longer, structured input-output scenarios. [Liu et al., 2023] applies the Shapley Value to multi-prompt learning, identifying the most impactful prompts for ensemble generation. Similarly, [Zhang et al., 2024c] analyzes LLM layer contributions, finding that earlier layers exert a more significant influence on output generation.\nAnother research direction models LLM alignment with diverse human preferences using social choice theory. This framework helps address challenges aligning LLMs with human values and decision-making processes [Mishra, 2023]. For instance, [Conitzer et al., 2024] analyzes the role of Reinforcement Learning from Human Feedback (RLHF) in expressing human preferences, identifying fundamental issues arising from preference conflicts, and advocating for social choice principles in LLM alignment. [Ge et al., 2024] examines RLHF reward modeling as a social choice process, demonstrating that Bradley-Terry-based approaches suffer from intrinsic limitations that violate key axioms. [Qiu, 2024] proposes a representative social choice framework, which extracts a small but representative subset of opinions to manage large-scale preference aggregation effectively."}, {"title": "3.2 Game Theory for Stimulating LLM Algorithms", "content": "In addition to enhancing our understanding of LLMs, game theory plays a crucial role in designing algorithms that improve their capabilities. This section highlights several key challenges in LLM training and illustrates how game theory has been applied to address these issues.\nGeneral Human Preference. Standard reward-based RLHF is limited to capturing only transitive preferences [Munos et al., 2024]. Preference models, however, can express more general preferences by comparing two policies rather than assigning a reward for each response. This introduces new challenges in optimizing an LLM based on preference models. Nash Learning from Human Feedback (NLHF) aims to optimize the von Neumann winner of a game defined by the preference model, offering a feasible and robust direction for policy optimization.\nBased on NLHF, SPO [Swamy et al., 2024] introduces methods to express more complex preferences, such as non-transitive, stochastic, and non-Markovian preferences. SPPO [Wu et al., 2025] designs an algorithm that efficiently implements SPO-like algorithms in large-scale language models. DNO [Rosset et al., 2024] improves LLM optimization using a regression-based objective for more efficient and direct training. INPO [Zhang et al., 2024d] introduces a loss function that can be directly minimized on preference datasets, further reducing the time overhead associated with calculating win rates in NLHF.\nHowever, recent work by [Zhi-Xuan et al., 2024] points out that preference-based approaches oversimplify human values, neglecting their complexity, incommensurability, and dynamic nature. As a result, designing more robust methods for aligning human preferences remains an ongoing scientific challenge.\nHeterogeneity in Human Preferences. Capturing heterogeneity in human-annotated datasets remains a significant challenge in LLM alignment. Ignoring this heterogeneity often results in models that reflect only the preferences of the majority [Fleisig et al., 2023]. Several studies have developed more inclusive training and alignment algorithms using social choice theory [Chakraborty et al., 2024b; Park et al., 2024; Alamdari et al., 2024; Chen et al., 2024a]. [Chakraborty et al., 2024b] demonstrates the impracticality of using a single reward model and proposes the Egalitarian principle to learn preference distributions. [Park et al., 2024] suggests clustering preferences and proposes a scalable, incentive-compatible framework for preference alignment. [Alamdari et al., 2024] employs Borda count and quantile fairness for preference aggregation, ensuring fairness and computational feasibility. [Chen et al., 2024a] introduces a mixture modeling framework for aggregating heterogeneous preferences. Additionally, [Klingefjord et al., 2024] takes a macro perspective to examine the gap between human preferences and training objectives, offering solutions from a philosophical standpoint.\nData Cost Efficiency. Game theory has also been applied to enhance the cost efficiency of LLM training. Collecting a dataset with guaranteed quality and coverage is often challenging, so several works have used the self-play framework to improve data utilization, reducing the amount of data required while maintaining performance. [Chen et al., 2024b] addresses the problem of fine-tuning a model with only a tiny amount of gold-standard data. Drawing from Generative Adversarial Networks [Goodfellow et al., 2020], it allows the LLM to improve the quality of its answers while learning to distinguish between its responses and those of the gold-standard answers, ultimately converging to the distribution of the gold-standard data. [Cheng et al., 2024a; Zheng et al., 2024] models a game between an attacker and a defender, both of which are LLMs. [Zheng et al., 2024] employs the attacker to propose prompts that the defender is less skilled at while the defender continuously improves. [Cheng et al., 2024a] considers a classic game, Adversarial Taboo, to enhance model knowledge acquisition without introducing new data, leading to better performance in experiments. Furthermore, [Zhang and Duan, 2024] improves the efficiency of preference data collection by incorporating an auction model into the LLM fine-tuning process, demonstrating how this approach can enhance fine-tuning efficiency while maintaining strong performance.\nOther Two-Player Game Formulations. In addition to the literature discussed above, several studies have formulated other two-player game models in specific phases of LLMs to enhance particular capabilities. [Chakraborty et al., 2024a; Makar-Limanov et al., 2024; Cheng et al., 2024b] model the interaction between the reward model and the LLM as a two-player game. They aim to address the problem where a static reward model cannot handle the distribution shift of the evolving LLM policy. Their game-theoretic modeling captures the co-evolution of the reward model and the LLM, and equilibrium-solving algorithms are used to provide theoretically guaranteed LLM training methods.\n[Jacob et al., 2023] observes that generative and discriminative answers to a question by the same LLM are often inconsistent. It models the Consensus Game, where these two types of answers act as players seeking a consensus answer. Using equilibrium-solving algorithms, this approach significantly improves the LLM's accuracy across various datasets. Furthermore, [Gemp et al., 2024] models the process of LLMs generating long-text conversations as a sequential game, using game-theoretic tools to enhance the model's ability to understand conversations and develop appropriate responses.\nRemark. Game theory is essential in addressing the challenges in LLM development by offering clear principles for"}, {"title": "4 Game Theory for LLM-Related Modeling", "content": "This section provides an overview of research on game-theoretic models that involve LLMs. The theoretical analysis of these models provides evidence of LLMs' impact on human society. We categorize the literature into three main areas. The first area explores game-theoretic models that include both LLMs and humans, aiming to explain or predict the phenomena resulting from the development of LLMs. The second area examines scenarios where LLMs function as products or platforms. This creates competitive environments that exhibit game-theoretic dynamics like ad auctions. The third area extends classical game theory models, investigating how LLMs' unique capabilities can generalize and refine these models for more complex and realistic settings."}, {"title": "4.1 Competitions between LLM and Human", "content": "This body of work introduces several competition models, treating LLMs as players in the game [Yao et al., 2024; Esmaeili et al., 2024; Taitler and Ben-Porat, 2024]. These models generally arise from a recognition: modern LLMs possess powerful content generation capabilities and, compared to human creators, are characterized by lower costs and faster evolutionary rates.\n[Yao et al., 2024] investigates the impact of LLMs on human creators by proposing a competition model based on the Tullock contest. This model explores the dynamics between human-generated and LLM-generated content, modeling LLMs as cost-free players whose output quality improves as human content quality increases. Through equilibrium analysis, the study concludes that LLMs do not fundamentally conflict with or replace human creators but instead reduce the volume of human-generated content, ultimately pushing out less efficient creators. [Esmaeili et al., 2024] extends this model into a repeated game setting, focusing on how humans can optimize their utility in dynamic competition with AI across various content domains. The study highlights the computational complexity of determining optimal strategies and proposes practical algorithms that offer near-optimal solutions.\n[Taitler and Ben-Porat, 2024] examines the competitive dynamics between LLM-based generative Al and human-operated platforms, such as Stack Overflow, and their implications for social welfare. The study models the revenue-maximization problem of LLMs and uncovers phenomena akin to Braess's paradox: as human users increasingly rely on LLMs, the original platforms suffer from a lack of quality-enhancing data. Furthermore, generative AI models rarely undergo training aimed at quality improvement due to cost-saving incentives. The study also suggests theoretical regulatory frameworks to address these issues.\nThe development of LLMs has led to diverse societal effects, and game theory provides a robust theoretical framework for studying these effects. By employing appropriate models depicting optimal behaviors and equilibrium strategies, we can derive properties with theoretical guarantees."}, {"title": "4.2 Game Scenarios Emerging with LLMS", "content": "This section explores game-theoretic scenarios that arise from LLMs as products or platforms. In these scenarios, LLMs do not participate in the games; instead, they revolve around them.\nAs LLMs gain global attention, industries related to LLMs are creating substantial commercial value. [Laufer et al., 2024] explores the feasibility of fine-tuning general-purpose models as a market service. The study models the bargaining process between generalists developing the model and domain specialists adapting it. By analyzing the sub-game perfect equilibria, the paper demonstrates that a profit-sharing outcome is possible and offers methods for determining Pareto-optimal equilibria. [Sun et al., 2024a] investigates potential economic scenarios in which a platform offers fine-tuning services for LLMs through an auction-like process for multiple groups with different preferences. The study proposes an incentive-compatible payment scheme that guarantees social welfare maximization. [Mahmood, 2024] analyzes the competitive dynamics of LLM deployment, highlighting the value of market information and demonstrating that a first-to-market strategy may be cost-ineffective for all tasks when those tasks are sufficiently similar. [Saig et al., 2024] proposes a pay-for-preference payment with a contract design model to address the potential moral hazard in the current pay-per-token pricing scheme.\nIn addition to their role as commodities, LLMs also offer potential commercial value through advertising revenue, similar to search engines. The emergence of LLMs has made traditional fixed-slot advertisements obsolete, prompting several studies on integrating LLMs into advertising auctions [Feizi et al., 2023]. [Duetting et al., 2024] models a scenario where each advertiser owns an agent LLM and bids to influence the probability distribution of the next token generated. The study derives an auction mechanism that ensures incentive compatibility by modifying the second-price auction. [Dubey et al., 2024] assumes each advertiser provides a fixed advertisement copy, influencing LLM summaries through bidding. Their auction mechanism determines the prominence of each advertiser in the summary and the price they pay, ensuring incentive compatibility. [Hajiaghayi et al., 2024] also assumes each advertiser possesses a document representing their content but models the advertisement insertion process in a Retrieval-Augmented Generation (RAG) framework. The mechanism probabilistically retrieves and allocates ads within each discourse segment of LLM-generated content, optimizing for logarithmic social welfare based on bids and relevance. [Soumalias et al., 2024] investigates a scenario where each advertiser bids via a reward function for LLM-generated content. Their mechanism incentivizes truthful reporting of reward functions and demonstrates operational viability in a tuning-free setting."}, {"title": "4.3 LLM Extending Classic Game Models", "content": "In addition to these two areas, this section examines works that enhance traditional game theory models using LLMs, extending their applicability to more realistic scenarios.\nLLMs' text comprehension and generation capabilities make them valuable tools for aggregating and eliciting opinions. [Lu et al., 2024] explores using LLMs to assist peer review, noting that traditional peer prediction mechanisms are limited to simple reports, such as multiple-choice or scalar numbers. The study proposes peer prediction mechanisms that leverage LLMs' powerful text-processing capabilities to incentivize high-quality, truthful feedback. These mechanisms are shown to distinguish between human-written and LLM-generated reviews in empirical experiments. [Fish et al., 2024a] uses LLMs to address the limitations of traditional social choice theory, which is constrained to choices among a few predetermined alternatives. The study employs LLMs to generate text and extrapolate preferences, providing a method for designing AI-augmented democratic processes with rigorous representation guarantees. [Sun et al., 2024b] investigates how LLMs can provide richer information in traditional auctions. The study introduces the Semantic-enhanced Personalized Valuation in the Auction framework, which leverages LLMs to incorporate bidders' preferences and semantic item information into the valuation process. The framework integrates fine-tuned LLMs with the Vickrey auction mechanism to improve valuation accuracy and bidding strategies."}, {"title": "5 Conclusion and Future Directions", "content": "This survey provides a comprehensive overview of the research progress at the intersection of LLMs and game theory. We summarized the role that game theory has been playing in developing LLMs from three key perspectives: providing standardized game-based evaluation, driving game-theoretic algorithmic innovations, and modeling the societal impact of LLMs. Furthermore, we highlighted the bidirectional relationship between LLMs and game theory, exploring how LLMs influence traditional game models. Building on this review of existing literature, we have identified several promising future directions in the intersection of game theory and LLMs. In the following section, we outline some of these opportunities and challenges with the hope of advancing this multidisciplinary field.\nLLM-based Agents with Comprehensive Game Abilities. Existing research has explored the evaluation of LLM agents across various game scenarios and developed methods to enhance their reasoning capabilities. However, while some of these methods demonstrate general applicability, their validation remains highly scenario-specific. A key future direction is to develop LLM agents proficient in game-theoretic reasoning, capable of applying their knowledge across diverse game settings without explicit customization. Achieving this requires advancements in rule comprehension, external environment modeling, and multi-agent reasoning. Key technical aspects include constructing a game-theoretic corpus, refining fine-tuning strategies, and incorporating tool-learning techniques.\nMoving Beyond Human-Oriented Evaluation Frameworks. Game theory provides well-established evaluation criteria for rationality and strategic reasoning, such as K-level rationality, which has been widely adopted to assess LLM intelligence. However, these evaluation methods were originally designed for human cognition and may not fully capture the reasoning processes of next-token prediction models. To assess LLMs comprehensively from a game-theoretic perspective, it is crucial to move beyond existing human-oriented metrics and develop evaluation frameworks tailored to neural network-based models. This remains an underexplored area with the potential to improve our understanding of LLMs' decision-making significantly.\nTheoretical Understanding of LLMs' Strategic Behavior. The application of game-theoretic concepts, such as the Shapley Value, to understanding LLMs' text generation behavior is still in its early stages. Most studies on LLMs' strategic behavior in real-world scenarios rely on empirical observations rather than systematic theoretical interpretations. For example, [Park et al., 2025] has introduced hypothetical models to explain why LLMs struggle to achieve the performance level of no-regret learners in repeated games. Extending such theoretical investigations to more complex scenarios, including games like Werewolf, Avalon, or bargaining games, is essential. A deeper theoretical understanding of LLM strategic behavior will help to define their capability boundaries and provide insights for further improving their reasoning abilities.\nCapturing Cooperative Games in LLM Optimization. Many studies leveraging game theory for optimizing LLM training, as discussed in Section 3.2, primarily focus on non-cooperative game settings. While non-cooperative approaches are a natural fit, cooperative game-theoretic methods offer additional insights into LLM optimization. For instance, different expert networks can be seen as participants in a cooperative game in the Mixture of Expert models. Adopting suitable payoff allocation mechanisms, such as the Shapley Value or the core concept, could optimize expert selection and task allocation, reducing redundancy while enhancing computational efficiency. Similarly, in ensemble learning and knowledge distillation, different sub-models can be treated as cooperative agents working together to refine decision boundaries or transfer knowledge. Effective reward allocation and weight adjustment strategies could enhance collaboration among sub-models, reducing redundant computation while improving generalization. Integrating cooperative game-theoretic methods into LLM training and optimization could offer new theoretical insights and practical solutions.\nModeling Cooperation Between Multi-LLMs and Humans. As discussed in Section 4.1, previous studies have primarily been focused on modeling competitive interactions between LLMs and humans, yielding valuable insights into their societal implications. However, beyond competition, understanding cooperative dynamics between multiple LLMs and humans remains an important research direction. One key challenge is to design mechanisms that incentivize LLMs to collaborate in fulfilling human-assigned tasks while considering their objectives. A theoretical characterization of LLM agents' goals and behaviors is essential for bridging the gap"}]}