{"title": "Greek2Math Tex: A Greek Speech-to-Text Framework for LaTeX Equations Generation", "authors": ["Evangelia Gkritzali", "Panagiotis Kaliosis", "Sofia Galanaki", "Elisavet Palogiannidi", "Theodoros Giannakopoulos"], "abstract": "In the vast majority of the academic and scientific domains, LATEX has established itself as the de facto standard for typesetting complex mathematical equations and formulae. However, LATEX\u015b complex syntax and code-like appearance present accessibility barriers for individuals with disabilities, as well as those unfamiliar with coding conventions. In this paper, we present a novel solution to this challenge through the development of a novel speech-to-LATEX equations system specifically designed for the Greek language. We propose an end-to-end system that harnesses the power of Automatic Speech Recognition (ASR) and Natural Language Processing (NLP) techniques to enable users to verbally dictate mathematical expressions and equations in natural language, which are subsequently converted into LATEX format. We present the architecture and design principles of our system, highlighting key components such as the ASR engine, the LLM-based prompt-driven equations generation mechanism, as well as the application of a custom evaluation metric employed throughout the development process. We have made our system open source and available at https://github.com/magcil/greek-speech-to-math.", "sections": [{"title": "1 INTRODUCTION", "content": "LATEX is a widely used typesetting system in academia, especially in fields like mathematics, physics, and computer science, due to its ability to produce well-structured documents with precise formatting and complex mathematical notations. However, its intricate syntax often requires significant time and effort to master, posing a barrier to entry for many users. Unlike standard word processors, LATEX uses a markup language to define document structure and formatting, which can be daunting for beginners and inaccessible to those with disabilities. Visually impaired users, for example, may struggle to read and interpret LATEX code using screen readers or other assistive technologies. Similarly, users with motor impairments may find it challenging to input ATEX commands accurately, particularly when dealing with complex mathematical equations. These accessibility barriers limit the inclusivity of LATEX, as well as its adoption among a diverse range of users. Additionally, members of the visually impaired community worldwide have expressed that accessibility challenges present a significant obstacle to pursuing higher education and engaging in research and academia. In Greece, specifically, visually impaired individuals also encounter numerous challenges within educational institutions [7].\nIn light of these challenges, we focus our work on developing alternative approaches to interact with LaTeX that are more intuitive, accessible, and user-friendly. In this paper, we open source an end-to-end speech-to-text system specifically designed to generate LATEX equations (in source code format) based solely on audio input. In this way, the users are now able to verbally dictate mathematical expressions and equations, based on which our proposed system swiftly generates the respective LATEX equation. Our goal is to democratize access to LATEX and enhance the efficiency of mathematical communication for visually impaired people in Greece."}, {"title": "2 RELATED WORK", "content": "In the past, various approaches have been proposed to address the problem of transforming speech to structured mathematical equations, e.g. in MathML or IATEX form. Initial approaches relied on rule-based conditions to generate mathematical equations from speech transcriptions [3, 11]. It is notable that the ASR modules used in the aforementioned systems were often based on Hidden Markov Models (HMM), whose capabilities are limited compared to the current state of the art ASR systems."}, {"title": "3 DATASET", "content": "Given the scarcity of publicly available domain-specific datasets and the limited knowledge of open-source ASR systems regarding both the Greek language and math-related speech nuances, we opted to develop our own task-specific dataset (henceforth denoted as Gr2Tex). It consists of 500 pairs of equations in natural text alongside their corresponding mathematical notation in LATEX form.\nDataset Collection Process: The equations were selected from a variety of sources, including Greek mathematics textbooks, online educational platforms, and custom-generated examples crafted by computer science professionals. Our selection aimed to comprehensively cover mathematical concepts, from basic arithmetic"}, {"title": "4 SYSTEM ARCHITECTURE", "content": "In this section, we provide further information about the architecture of our proposed system. Illustrated in Figure 1, it consists of three primary components; a speech recognition module, a retrieval mechanism, as well as a text generation model. The speech recognition module transcribes speech input into free-form text, while the retrieval component searches a database to return the k most similar equations in natural text from a held-out set of our dataset, along with their respective mathematical forms. Subsequently, the text generation model, in our case GPT-3.5-turbo, is prompted with the k retrieved equation pairs, along with a brief instruction that guides the LLM's behaviour."}, {"title": "4.1 Speech Recognition Component", "content": "Our speech recognition component consisted of an instance of the XLS-R model developed by Meta AI [2]. While it is trained on thousands of hours of audio in multiple languages (including Greek), its performance on the Greek language was insufficient for integration into our proposed end-to-end pipeline. Thus, we underwent a fine-tuning process in order to initially refine the model's ability in successfully transcribing audio in Greek. Regarding the initial fine-tuning process, we collected approximately 26 hours of publicly available Greek speech audio, covering diverse topics such as political debates, sportcasting events and stand-up comedy shows. These recordings were transcribed and segmented into smaller chunks of approximately 3 seconds each. Additionally, we utilized a subset of the Common Voice [1] dataset that contained Greek audio. Finally, we also fine-tuned the model using our custom domain-"}, {"title": "4.2 Equation Generation Component", "content": "Given the rapid advancements in the NLP domain, we chose to use a Large Language Model (LLM) for the equation generation process. However, a notable challenge emerges due to the limited availability of LLMs pretrained on a substantial Greek language corpus, with even fewer trained on math-related data. Our decision to develop a speech-to-LaTeX tool for the Greek language meant that models specifically tailored for code generation, such as StarCoder [5] could not be employed, as their capabilities in Greek are limited. While such models would excel due to the code-like structure of LATEX equations, they lack sufficient training data for Greek language tasks. Thus, we decided to employ OpenAI's GPT3.5-turbo.\nA common strategy when leveraging LLMs is to enhance their capabilities through in-context learning (ICL). In the NLP domain, ICL is a paradigm where an LLM is prompted with a set of instructions alongside a few task-specific demonstrative examples. This allows the model to adapt its output based solely on the provided examples, without the need to update its parameters [4]. Given the lack of a task-specific dataset for Greek, aside from our own Gr2Tex with only 500 examples, ICL was essential for guiding the LLM's responses. We selected demonstrative examples by retrieving the k most semantically similar samples to the query, with k being a hyperparameter optimized during a tuning process. Balancing performance with the cost of using a closed-source LLM like GPT-3.5, we experimented with values of k from two to six, as detailed in Section 5.2."}, {"title": "4.3 Retrieval Mechanism", "content": "Our retrieval mechanism operates on the Gr2TeX dataset (see Section 3), composed of a diverse range of mathematical equations represented in natural language and their corresponding LaTeX forms. We employ the k-NN algorithm in order to identify the k most similar examples to the current query. We experimented with various similarity and distance measures, such as the cosine and euclidean similarity, as well as the Manhattan distance. The results of our exploration, detailing which similarity or distance function yielded better performance, are presented in Section 5.2."}, {"title": "5 EXPERIMENTAL SETUP", "content": ""}, {"title": "5.1 Evaluation Metrics", "content": "We evaluated our system's performance by measuring the Levenshtein distance between the generated equations and ground truth, following preprocessing steps to standardize the equations. This preprocessing involved removing LaTeX-specific formatting and commands, such as dollar signs and equation delimiters, and standardizing mathematical symbols and variables, including replacing Greek letters with Latin equivalents and removing unnecessary punctuation. After preprocessing, the function calculates the Levenshtein distance, which quantifies the minimum number of single-character edits required to transform one string into another. We assessed the effectiveness of this evaluation method, which we denote as EL, by comparing its results to a set of annotated assessments conducted by a group of five individuals. Each pair of"}, {"title": "5.2 Experimental Results", "content": "In this section, we present the outcomes of our experiments and discuss our observations. Table 2 showcases the performance of our proposed system across different values of k and various similarity or distance functions. The first row of the table corresponds to our baseline approach, which did not utilize any in-context learning (ICL) functionality. In this setup, the generative model was prompted solely to produce the LATEX equation for the provided transcribed text sequence. The third column indicates the selected prompt (see Table 2). The fourth and fifth columns of the table indicate the percentage of equations with an EL distance lower than 0.1 or 0.4 from their respective ground truths. The final two columns present the BLEU [8] and chrF [9] scores respectively."}, {"title": "6 WEB APPLICATION", "content": "To address accessibility barriers in formulating mathematical expressions, we integrated Gr2tex into a web application, with the backend developed in FASTAPI and the frontend in React. The interface includes four main buttons: one for recording the mathematical expression, another for playing back the recorded phrase, a third for downloading the audio file, and a fourth for converting speech to LATEX (Figure 2). Clicking the ATEX button transcribes the audio and sends the text to the model that generates the corresponding LaTeX expression. This expression is displayed in a modal window, giving users immediate access to the mathematical representation of their spoken input (Figure 2). The web application is accessible by following the instructions found in our open-source repository."}, {"title": "7 CONCLUSIONS & FUTURE WORK", "content": "In this work, we introduced an end-to-end speech-to-text system designed to generate ATEX equations. The system comprises an ASR module, a text generation component, and an assistive retrieval mechanism. The ASR module is a fine-tuned version of the XLS-R model [2], while the text generation component utilizes GPT3.5. We leverage in-context learning to dynamically enhance the model's performance without further training, using a set of demonstrative examples retrieved for each query from a held-out set. This system represents a significant advancement in translating spoken mathematical expressions into LATEX format, improving accessibility and efficiency in scientific communication. In future work, our objective is to explore more advanced ASR systems and newer LLMs with enhanced capabilities in Greek language processing. Additionally, we plan to investigate more sophisticated retrieval techniques and experiment with elaborate prompting strategies, such as Chain-of-Thought (CoT) prompting."}]}