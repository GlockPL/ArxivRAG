{"title": "VQ-CNMP: Neuro-Symbolic Skill Learning for Bi-Level Planning", "authors": ["Hakan Aktas", "Emre Ugur"], "abstract": "This paper proposes a novel neural network model capable of discovering high-level skill representations from unlabeled demonstration data. We also propose a bi-level planning pipeline that utilizes our model using a gradient-based planning approach. While extracting high-level representations, our model also preserves the low-level information, which can be used for low-level action planning. In the experiments, we tested the skill discovery performance of our model under different conditions, tested whether Multi-Modal LLMs can be utilized to label the learned high-level skill representations, and finally tested the high-level and low-level planning performance of our pipeline.", "sections": [{"title": "1 Introduction", "content": "Planning in the continuous real-world domain is a hard problem. Using state [1, 2, 3], and action abstractions [4] has been shown to address this difficulty by separating high-level decision-making from low-level perception and control, where abstractions showed great success in making fast and effective robotic plans that can be generalized to other environments.\nOur paper, on the other hand, focuses on using learned action and state predicates as an intermediary layer between virtual generalist agents and environments. It is difficult to learn from low-level continuous robotic data as it is complex and noisy, especially in the real world. Furthermore, the data is limited as it is not mostly produced by people and is only collected in controlled lab settings. These two issues make it hard to include raw robotic data in training datasets of multi-purpose large models such as LLMs. However, since their training data (text) includes information about numerous domains, LLMs have (although limited) reasoning capabilities in multiple domains, including reasoning for high-level robotic tasks. We believe abstractions can be used to bridge this gap between real-world environments and generalist virtual agents. State abstraction methods can be the eyes, and action abstractions can be the arms and legs of these embodied virtual generalist agents. In the case of LLMs, Instead of including the robotic data in the training data of the LLM, one can train small abstraction models that could bridge the gap between the agent and its embodiment and the environment in which the embodiment resides. This way, these abstraction models can handle all the low-level information while the agent is capable of high-level reasoning, which can be learned from text much more efficiently. An overview of how a system like this would function is provided in Figure 1.\n[5, 4] showed that it is sufficient to learn symbolic categories for the precondition and the effects for long-horizon planning. [6] built upon this framework, focusing on sharing agent-centric symbols between different tasks. Combining traditional clustering and classification machine learning algorithms, [7, 8] discovered object-centric precondition and effect symbols for generating PDDL descriptions. These works commonly cluster the collected random interactions based on actions and effects prior to the symbol learning procedure. Our work studies learning of discrete skills from demonstrations for long-horizon planning with LLM. In another line work, [9, 10, 11] proposed a bi-level planning schema in which a set of operators and corresponding samplers were previously acquired, allowing the agent to make refined plans. [3] learned symbolic predicates from demonstrations optimizing planning performance. These studies follow the task and motion planning (TAMP) formulation [12] in which the problem is finding the sequence of tasks and the correct motion parameters. These TAMP formulations learn state abstractions on top of high-level predicates such as on(?x, ?y). Utilizing recent and advanced neural network architectures, [2] learned symbols with an encoder-decoder network to minimize effect prediction error for probabilistic PDDL-based planning for single and paired objects. Extending their previous work, [13, 14, 15] utilized the attention layer to learn relational symbols in environments with varying numbers of objects and used these symbols in PDDL-based planning. In our work, we aim to benefit from LLMs for long-horizon planning, benefiting from the impressive capabilities of the visual-language foundational models.\nIn this paper, we propose a novel action abstraction model that can learn discrete high-level skill representations from demonstrations that include low-level variations of skills. It is important to emphasize that high-level skills are learned without losing the low-level information. Importantly, our model is capable of clustering skills in unevenly mixed datasets in an unsupervised manner. We also propose a bi-level planning pipeline that an LLM agent can use to execute high-level plans. After discovering the high-level skills, the model can also be utilized to make low-level plans using a gradient-based approach. The proposed pipeline includes the following steps:\n1. Demonstrations are clustered, and high-level skills are discovered using our model.\n2. Discovered skills are labeled either by an expert or a Multi-Modal LLM.\n3. A new model is trained using self-supervision provided by the discovered high level skill labels.\n4. An external agent is used to make high-level plans using the discovered skills for given goal tasks\n5. Low-level execution plans are made for each step of the high-level plan using our proposed gradient-based planning approach.\nOur contributions are as follows:\n1. A novel skill discovery method that can learn high-level skill representations from an unlabeled demonstration data\n2. A bi-level planning pipeline based on our proposed model"}, {"title": "2 Method", "content": "In this study, we propose a novel neural network model that can learn discrete latent representations from skill demonstrations. The model utilizes the vector quantization approach [16] to learn single discrete vectors from different variations of a high-level skill. For example, taking a sandwich from the refrigerator is a high-level skill, while where in the refrigerator the sandwich is taken from"}, {"title": "2.1 Model", "content": "constitutes the variation or the low-level component of the high-level skill. Given a demonstration dataset that includes different variations of multiple high-level skills, using a vector-quantized autoencoder architecture, different skills get assigned to different vectors in the vector space as training progresses. While learning the skill vector space, the model also learns a distribution for each vector which spans the low-level planning space of each high-level skill. Furthermore, our model is also capable of grouping unlabeled demonstrations into skills since the given demonstrations are given without labels and order.\nWe used the architecture of the Conditional Neural Movement Primitives [17] to leverage its ability to learn latent spaces from continuous movement trajectories and incorporated vector quantization to learn a discrete vector space from given demonstrations. More formally, during training, given a set of demonstrations D = (t, SM(t)) where SM(t) denotes sensorimotor information at time t, at each training iteration a demonstration trajectory, Dj is randomly sampled from the dataset. From this trajectory, a set of points are sampled which can be denoted as (ti, SM(t)) and are fed through the encoder,\nz_i = E((t_i, SM(t_i))|\\theta) \\quad (t_i, SM(t_i)) \\in D_j \\qquad(1)\nwhere zi denotes the latent representation generated by the encoder E with parameters \u03b8 using data point (ti, SM(ti)). Then these representations are averaged,\nz_e = \\frac{1}{n}\\sum_{i=0}^{n} z_i \\qquad(2)\nto get a single latent representation ze. Then similar to [16], the representation is passed through a discretization bottleneck by mapping it to the nearest vector in the skill space,\nz_q = U_k \\quad where \\quad k = argmin_m(||z_e - U_m||) \\qquad(3)\nwhere Um denotes the mth vector in the space. Then the resulting representation zq is concatanated with the target time ttarget (the time step of the outputted data point) and passed through the decoder to generate the output,\n(\\mu_{ttarget}, \\sigma_{ttarget}) = Q((z_q, t_{target})|\\phi) \\qquad(4)\nwhere \u03d5 denotes the parameters of the decoder Q, \u03bcttarget denotes the mean and the \u03c3ttarget denotes the variance of the output. The model overview can be seen in Figure 2. The loss of the model is defined as the following,\nLoss = - log P(SM(t_{target})|\\mu_{ttarget}, \\sigma_{ttarget}) + ||sg(z_e) - z_q||^3 + \\beta * ||z_e - sg(z_q)||^2  \\qquad(5)\nwhich includes both the reconstruction loss (negative-log likelihood) of the CNMP (the term on the left) and the loss terms of Vector Quantization (on the middle and the right). sg denotes the stop gradient operator. Unlike [16], we observed that using \u03b2 values that are not small causes the gradients to explode. We used \u03b2 = 0.25 in our experiments and observed that \u03b2 values as small as 1 were enough to cause the gradients to explode."}, {"title": "2.2 Self-Supervised Fine-Tuning", "content": "While the model can successfully cluster the demonstrations into discrete skills, our experiments showed that the actions the system can generate are not reliable enough to be used in low-level plans. We propose self-supervised fine-tuning using the skills the system discovers to solve this issue. After the initial training is complete and the skills are discovered, i.e., the demonstrations are clustered into skills, training a new model using the discovered skills significantly increases the low-level planning performance. To achieve this, instead of letting the system decide which vector to choose by Euler distance in the vector quantization layer, we used the discovered clusters to assign the demonstrations that were assigned to the same vector in the first training phase to the same vector.."}, {"title": "2.3 Planning", "content": "The overview of the planning system can be seen in Figure 3. For the high-level plan, after the skill space is learned and labeled, the actions are given in a prompt along with the goal and the image of the environment. Since we are not using state abstractions in this work, we used a Multi-Modal LLM (ChatGPT-40) that can make inferences for given images. The prompt also includes the statement that only the given actions can be used, and an ordered list of these actions should be returned.\nFor the low-level plan, we used a gradient-based method similar to the one utilized in [18]. The method uses high-level skill vectors as inputs and passes them through the decoder of the trained model to get an initial prediction of the low-level action. Then, at each iteration, gradient descent is used to get the initial prediction closer to the goal by adjusting the input vector. For practicality, we used end-effector pose data and gripper state as input to the model for the planning experiments, so that the part of the action prediction where the robot makes contact with the object would be close to the object's location which is what differs between the low-level versions of the same high-level skill. For each step of the high-level plan, the vector of that step's skill is given as input to the decoder of our model, along with the target time close to the time the robot makes contact with the object it is interacting with. Since the change between the variations of the skills is where they retrieve the objects from, the time step that the robot makes contact with the object will be close to"}, {"title": "3 Experimental Results", "content": "To test the capabilities of our model, we created a kitchen environment. We selected cooking as the general task and conceptualized skills that would complement it, such as taking ingredients from the cupboards or drawers and dropping them into the pan, taking the oil bottle near the stove and adding oil to the pan, taking the salt near the stove and adding it to the pan where we tried to utilize objects that are frequently used in the real world. Some of these skills can be seen in Figure 4. We collected 100 demonstrations for each skill by changing the objects' locations within reachable/valid positions."}, {"title": "3.1 Skill Discovery", "content": "To test the skill discovery performance of our model, we trained 100 randomly initialized models and analyzed them in batches of 10. The results showed that in 27 of all trials, the skills were clustered perfectly into the 5 vectors that were in the high-level skill space. We further analyzed them in batches of 10 to find a practical way of selecting a model that automatically clustered the demonstrations. In all batches, we observed that two of the three models with the least valued losses in the batch were perfectly clustered. Generally, when the model could not perfectly cluster the skills, this was because multiple skills were mapped to the same vector.\nWe also investigated how the model behaves when the size of the skill space is more or less than the number of skills in the demonstrations. We set the size of the skill space to 3, 10, 20, and 100,"}, {"title": "3.2 Labeling using Multi-Modal LLMs", "content": "To test whether Multi-Modal LLMs can be used for labeling skills, we took image snapshots of the execution of the action generated using the vectors in the learned skill space at different time steps. We gave a prompt to ChatGPT-40 asking what the robot might be trying to achieve along with different combinations of the snapshots. All demonstrations used during the experiments have a size of 150, and we took snapshots every 10 data points. Results can be seen in Table 2. The first 10 columns show the prediction accuracy when one to ten consecutive images are used for prediction. We also experimented with a random number of randomly selected snapshots from the actions seen in the last column. While a general increase in accuracy can be observed as the number of snapshots increases, in some cases, it also caused the LLM to be distracted from the skill, which resulted in predictions that are more about the environment. When we examined the results of both consecutive and random cases, we observed that the predictions were more successful when snapshot combinations that sufficiently demonstrated the skill were used, which ideally equals a number of images before making contact with the interacted object, an image of the contact and several images after making contact. The use of more images only deteriorated the prediction performance."}, {"title": "3.3 Planning Performance", "content": "For high-level planning, we selected the tasks of making stews that contain different ingredients. Since results in Subsection 3.2 showed that using LLMs for labeling is not very reliable, we labeled each skill by hand. We described the skill space in the prompt as retrieving the objects from their positions and adding them to the pan without stating where each object is. and gave the goals similar to \"Given the robotic environment in the image, how can the robot make a stew made of potato, mushroom, and salt?\". We also gave an image of the environment as seen in Figure 5. During testing, we included 10 trials from each possible combination of the ingredients (310 in total). The high-level planning performance of ChatGPT-40 can be seen on the left column of the Table 3."}, {"title": "3.3.2 Effect of Number of Skills in the Skill Space", "content": "Since the \"distractions\" in the environment significantly affected the LLM performance in Subsection3.2, we also tested whether irrelevant information about skills affects the high-level planning performance. To test the effect, we checked three conditions: environment-related actions, environment-unrelated actions, and both. Environment-related actions are relevant to the current setting, such as opening/closing cupboards/drawer and moving the pan to the other stoves. Unrelated actions are irrelevant to the current environment, such as plugging in the phone charger or moving the chair closer to the table. The results can be seen in the Table 3. As seen from the second, third, and last columns, the planning performance drops when high-level skill space includes unneeded skills. The results also showed that performance was affected slightly more by the irrelevant skills."}, {"title": "3.3.3 Environment with Hidden Elements", "content": "One limitation of using the environment image only is that the image may not contain all the information needed to achieve the tasks. Humans also share this limitation, which is overcome by having prior information about the environment or exploring it. We closed the cupboards and drawer to test whether we could achieve the former. We used the image in Figure 6 and included the whereabouts of the objects and the skills for opening and closing the cupboards/drawers to the prompts. Previously, we did not include opening and closing actions to the data mix we used to train our model since their variations are insignificant in execution. With the closed state of the environment and locations of the hidden objects given in the prompt, the system achieved a high-level planning performance of %77.74. When we also included the locations of the other two objects (oil and salt), the planning performance rose to %98.38, which shows that the visual understanding of the LLM is unreliable for this setting. Although we did not use them in this study, this result also shows the importance of state abstractions.\nWe also tested the exploration case. When we did not explicitly state that it could explore the environment first, it made assumptions about the places of the objects in the cupboards and the drawer. It made the plan directly, which resulted in an incorrect outcome. However, when explicitly stated, it first planned actions to open the cupboards and drawer, asked for an updated image of the environment, and made the plan according to the updated environment. Based on all the experiments, it can be stated that the performance of the LLM depends heavily on the quality of the given prompt and the understanding of the environment, which would have to be optimized for the environment. While a general prompt that performs well in every environment might be engineered, the reliability of generalizing to every possible environment would still be questionable."}, {"title": "3.3.4 Low-Level Planning Performance", "content": "Since the skills used in the experiment are mostly independent of each other (the success of one does not depend on the success of the others), we evaluated our model's low-level performance on single-task trials. Since our planning approach functions by bringing the step of the action trajectory close to the object position, we examined each task in two parts: picking up the objects and adding them to the pan. We also compared the low-level planning performance of the model when it is trained self-supervised and unsupervised. The results can be on the Table 4. It can be seen from the Table that the model that trained using self-supervision significantly outperformed the unsupervised one. While the actions produced by the unsupervised model were not precise enough to complete the whole task, in most cases, they could pick up the object since that is what the gradient-based planner optimizes the trajectories for. Another difference we observed between the two models is that, on average, the self-supervised model took significantly fewer iterations to optimize the high-level skill (70 for self-supervised and 400 for unsupervised)."}, {"title": "4 Conclusion, Limitations, Future Work and Discussion", "content": "In this paper, we propose a novel neural network model capable of clustering skills in mixed datasets and can be used for bi-level planning. We tested its ability to discover skills under different conditions, such as uneven dataset mixes and varying sizes of the skill space. We also tested whether LLMs can be used to label the discovered skills. Lastly, we show our system's planning performance. One limitation of our model is that demonstrations must follow the same distribution to be classified as having the same skill. Furthermore, since the system depends on an external agent for reasoning and we don't use state abstractions, its planning performance highly depends on the external agent's perception and high-level planning capabilities. Finally, while the environment image is enough to have information about the initial state for high-level planning, the exact locations of the objects to be interacted with are needed to make the low-level plans. In future work, we plan on experimenting with other LLMs to test their high-level planning and skill labeling performance. We also plan on comparing our approach to other models using larger datasets."}, {"title": "A Experimental Results", "content": null}, {"title": "A.1 Labeling Using Multi-Modal LLMs", "content": "The prompt we used to generate labels for the snapshots can be seen below.\nPrompt: I will provide you with snapshot images of a robot interacting with a single object. Your task is to analyze each image and predict the robot's specific action based solely on visual cues. Return a concise description of the action that best fits the images. Provide only the action description with no additional text or explanations."}, {"title": "A.2 Planning", "content": null}, {"title": "A.2.1 High-Level Planning Performance", "content": "The prompt template we used to make the high-level plans can be seen below. The < ingridients > part is changed between trials, and the ingredients are given as a comma-separated list.\nPrompt: Given the robotic environment in the image, how can the robot make a stew using the following ingredients: < ingredients>? The available actions are:\n{ 1:\"retrieve the object in the right cupboard and add to the pan\", 2:\"retrieve the object in the left cupboard and add to the pan\", 3:\"retrieve the object in the drawer and add to the pan\", 4:\"retrieve the object on the left of the stove and add to the pan\", 5:\"retrieve the object on the right of the stove and add to the pan \", }\nSelect the necessary actions to achieve the goal and return only their corresponding keys in a list, formatted as JSON. No explanation, descriptions, or additional output is needed."}, {"title": "A.2.2 Effect of Number of Skills in the Skill Space", "content": "The skill list used for the relevant actions part can be seen below.\n1: retrieve the object in the right cupboard and add it to the pan,\n2: retrieve the object in the left cupboard and add it to the pan,\n3: retrieve the object in the drawer and add it to the pan,\n4: retrieve the object on the left of the stove and add to the pan\",\n5: retrieve the object on the right of the stove and add it to the pan,\n6: put the pan to top right stove,\n7: put the pan to the top left stove,\n8: put the pan on the bottom right stove,\n9: put the pan to the bottom left stove,\n10: close the right cupboard,\n11: close the left cupboard,\n12: close the drawer,\n13: open the drawer,"}, {"title": "A.2.3 Environment with Hidden Elements", "content": "The prompt used for the part with prior knowledge can be seen below.\nPrompt: Given the robotic environment in the image, how can the robot make a stew using the following ingredients: < ingredients>? The tomato is located in the right cupboard, the mushroom in the left cupboard, and the potato in the drawer. The available actions are: <actions>. Return only the keys of the actions necessary to complete the task in the correct sequence, formatted as a list in JSON. No additional text or explanations are required."}]}