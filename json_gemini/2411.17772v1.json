{"title": "MVBoost: Boost 3D Reconstruction with Multi-View Refinement", "authors": ["Xiangyu Liu", "Xiaomei Zhang", "Zhiyuan Ma", "Xiangyu Zhu", "Zhen Lei"], "abstract": "Recent advancements in 3D object reconstruction have been remarkable, yet most current 3D models rely heavily on existing 3D datasets. The scarcity of diverse 3D datasets results in limited generalization capabilities of 3D reconstruction models. In this paper, we propose a novel framework for boosting 3D reconstruction with multi-view refinement (MVBoost) by generating pseudo-GT data. The key of MVBoost is combining the advantages of the high accuracy of the multi-view generation model and the consistency of the 3D reconstruction model to create a reliable data source. Specifically, given a single-view input image, we employ a multi-view diffusion model to generate multiple views, followed by a large 3D reconstruction model to produce consistent 3D data. MVBoost then adaptively refines these multi-view images, rendered from the consistent 3D data, to build a large-scale multi-view dataset for training a feed-forward 3D reconstruction model. Additionally, the input view optimization is designed to optimize the corresponding viewpoints based on the user's input image, ensuring that the most important viewpoint is accurately tailored to the user's needs. Extensive evaluations demonstrate that our method achieves superior reconstruction results and robust generalization compared to prior works.", "sections": [{"title": "1. Introduction", "content": "Generating 3D assets from a single-view image is a critical task in 3D computer vision [10, 17, 18, 20, 25, 29, 33], offering a broad range of applications such as video games [8], virtual reality [22], 3D content creation [1], and animation [15]. High-fidelity 3D reconstruction models greatly reduce the labor involved in creating 3D digital assets. However, generating high-fidelity 3D assets from a single-view image while preserving consistent surface details is a challenge, especially for complex objects. A core difficulty in developing these models is the limited availability of high-quality 3D data. Creation of such datasets is a complex task that often requires specialized equipment, advanced capture techniques, or intricate 3D modeling processes. Current publicly available 3D asset datasets [2\u20134] lack high-quality textures and contain significant repetitiveness, making them insufficient for training 3D generative models effectively.\nSeveral methods leverage 2D diffusion models to enhance 3D generative models due to the widespread success of diffusion models in image generation. DreamFusion proposes Score Distillation Sampling (SDS) [25], which distills 3D knowledge from 2D diffusions and inspires the advancement of SDS-based 2D lifting methods [21, 25, 32]. Although SDS-based methods can produce highly realistic visual effects, their optimization-based approaches require several hours to generate a refined 3D asset, making them impractical for 3D content creators. Additionally, SDS-based methods often suffer from poor geometry and inconsistencies, such as the \"Janus\" problem. To address these issues, various feed-forward methods [10, 13, 17, 29] have developed that use four views of ground truth as input during training, generating 3D assets through a robust 3D reconstruction network. However, these methods rely on multi-view diffusion models [19, 31] during inference, which may yield inconsistent multi-view outputs. In practical application, the reconstructed model will perform poorly if the training set lacks examples for the current scene. Collecting additional data for new scenarios is costly, making reconstruction very inflexible to apply.\nIn this work, we propose a novel framework for boosting 3D reconstruction with multi-view refinement (MV-Boost) from a single-view image. The method consists of a multi-view refinement strategy and a boosting reconstruction model. The multi-view refinement strategy combines the advantages of the high accuracy of the multi-view generation model and the consistency of the 3D reconstruction model, as our data source. While, the multi-view generation model excels in accuracy but lacks consistency across views, whereas the 3D reconstruction model provides consistent multi-view data, but with lower accuracy. Specifically, given a single-view input image, the multi-view refinement strategy uses a multi-view diffusion model to generate high accuracy multi-views. Then, these multi-view images are sent into a large 3D reconstruction model to produce consistent original 3D representation. By refining multi-view images rendered from original 3D data, the multi-view refinement strategy generates a large-scale multi-view dataset to train a feed-forward 3D reconstruction model. The boosting reconstruction model using the recently proposed Large Multi-View Gaussian Model as our starting point, we introduce LoRA [11] to stabilize the training process. In practical applications, the alignment of the reconstructed 3D model with the input image is one of the key criteria for assessing the quality of the reconstruction. Therefore, we provide an input view optimization process to optimize the corresponding viewpoint image.\nWe conduct extensive experiments on the GSO [6] dataset. Both qualitative and quantitative results demonstrate that our proposed method outperforms related reconstruction methods. In summary, our contributions are:\n\u2022 We present a novel framework for constructing pseudo-ground truth, which harnesses the advantages of high-accuracy multi-view generation to improve overall reconstruction quality.\n\u2022 We propose a sophisticated framework designed to integrate diverse single-view datasets into the 3D reconstruction training. This framework is further refined based on user-provided input images, culminating in a significant enhancement of the reconstruction results.\n\u2022 Results validate that our method could reconstruct high-fidelity 3D reconstruction and achieve new state-of-the-art results."}, {"title": "2. Related Work", "content": "3D Generation through Score Distillation. Learning to generate 3D assets from single images presents a formidable challenge due to the limited availability of 3D-image paired data. DreamFusion [25] introduces Score Dis-"}, {"title": "3. Method", "content": "The overall framework of our network, MVBoost, is illustrated in Figure 2. We first provide an overview of the methods and processes behind diffusion models and 3D generation in Section 3.1. Next, we present our multi-view refinement strategy (Section 3.2), which addresses inconsistencies across views generated by the diffusion model. In Section 3.3, we describe our boosting reconstruction model, which leverages a refined 2D multi-view dataset to achieve high-fidelity 3D reconstruction without relying on any 3D datasets. Finally, in Section 3.4, we explain how we optimize the alignment of the 3D Gaussian Splatting with the corresponding input viewpoint for improved 3D representation."}, {"title": "3.1. Preliminaries", "content": "Diffusion Model. Diffusion Model [5, 9, 28] is a generative model that involves forward and reverse diffusion processes. In forward diffusion, data x0 is incrementally corrupted with Gaussian noise ($\\mathbf{x}_t = a_t\\mathbf{x} + \\sigma_t\\epsilon, t \\in \\{1, ..., T\\}$) to approach a noise distribution. The reverse process, which is the generative phase, uses a neural network to predict and apply denoising steps, progressively recovering the original data as it transitions from t = T back to t = 0.\n3D Generation. Given a single-view image c, we utilize the Multi-View Diffusion Model G to generate multiple views $\\mathcal{C}^\\pi = \\{c^\\pi_i\\}_{i=1}^I$. This process can be represented as\n$\\mathcal{C}^\\pi = \\{c^\\pi_i\\}_{i=1}^I = G(c^\\pi_1, ... , c^\\pi_T; c, T),$\n(1)\nwhere $c^\\pi_1, ..., c^\\pi_T$ are initialized as Gaussian noise, T represents the timestep, and $\\pi_i$ corresponds to the camera pose for each view.\nUsing the multi-view data $\\mathcal{C}^\\pi$ as input, we apply a large 3D reconstruction model $R_\\Theta$ to generate the 3D representation $\\theta$:\n$\\theta = R_\\Theta(\\mathcal{C}^\\pi).$\n(2)\nThe 3D representation $\\theta$ can be either a NeRF [23] or 3D Gaussian Splatting [12]. In our specific implementation, we utilize 3D Gaussian Splatting. We first generate an original 3D Gaussian Splatting denoted as $\\theta$ using a standard 3D asset generation process. Then, we render $\\theta$ from specific viewpoints to obtain multi-view images, represented as $\\mathbf{x}^\\pi$. The rendering process can be be represented as\n$\\mathbf{x}^\\pi = g(\\theta, \\pi).$\n(3)"}, {"title": "3.2. Multi-View Refinement Strategy", "content": "The single-view image dataset can be enhanced using Multi-View Refinement to generate a large, refined multi-view dataset.\nPrevious methods often relied on limited 3D data as ground truth, which constrained their performance. While VFusion3D [7] leverages video diffusion models for multi-view generation, it lacks explicit view consistency constraints, relying only on implicitly learned multi-view relationships, which can result in geometric inconsistencies. We propose a method that creates pseudo-ground truth by combining 3D multi-view consistency with the high accuracy of the diffusion model. The process is as follows:\nIn our forward refinement diffusion, the Gaussian Splatting rendered view $\\mathbf{x}^\\pi$ is progressively corrupted by Gaussian noise, approaching a noise distribution:\n$\\mathbf{x}^\\pi_t = a_t\\mathbf{x}^\\pi + \\sigma_t\\epsilon.$\n(4)\nThe reverse refinement diffusion process predicts the denoising operations using $\\mathcal{X} = \\{\\mathbf{x}^\\pi_i\\}_{i=1}^I$ and the input view c, resulting in the refined multi-view set $\\mathcal{C}'^\\pi = \\{c'^\\pi_i\\}_{i=1}^I$:\n$C' = G(\\mathcal{X}; c,t),$\n(5)\nwhere t = sT and T represents the maximum timestep. The parameter s controls the strength of the noise added during the process, ranging from 0 to 1."}, {"title": "3.3. Boosting Reconstruction Model", "content": "As illustrated in Figure 2, we use the refined multi-view dataset generated from Section 3.2 to boost the reconstruction model eliminating the need for 3D datasets.\nWe input the original multi-view data $\\mathcal{C}^\\pi = \\{c^\\pi_i\\}_{i=1}^I$ into our boosted model to ensure consistency between the training and inference inputs. In our model, we apply LoRA exclusively to the original 3D reconstruction model, $R_\\Theta$, specifically within its Cross-view Self-Attention component, resulting in the boosted model $R_{\\Theta^*}$. By training $R_{\\Theta^*}$ with the refined multi-view dataset $\\mathcal{C}'^\\pi$, we obtain the boosted 3D Gaussian Splatting, $\\theta^* = R_{\\Theta^*}(\\mathcal{C}'^\\pi)$.\nIn summary, we use a multi-view diffusion model along with a refinement strategy to convert a single-view image into a multi-view tuple $(\\mathcal{C}^\\pi, \\mathcal{C}'^\\pi)$. The training process is defined as follows:\n$\\Phi^* = \\underset{\\Phi}{\\arg \\min} \\mathbb{E}_{(\\mathcal{C}^\\pi,\\mathcal{C}'^\\pi),i} [\\mathcal{L}(g(R_{\\Theta^*}(\\mathcal{C}^\\pi), \\pi), C'^\\pi)],$\n(6)"}, {"title": "3.4. Input View Optimization", "content": "To improve the alignment between the generated 3D assets and the input view, we search over the possible camera poses of the 3D assets to find the pose that minimizes the LPIPS loss. Let \u03a0 represent the set of all possible camera poses. The optimal camera pose \u03c0opt is defined as:\n$\\pi_{opt} = \\underset{\\pi}{\\arg \\min} \\mathcal{L}_{LPIPS}(g(\\theta^*, \\pi), c),$\n(7)\nwhere g(\u03b8\u2217, \u03c0) denotes the rendered view from the original 3D Gaussian Splatting \u03b8\u2217 at camera pose \u03c0, and c is the input view.\nSubsequently, we enhance the 3D Gaussian Splatting \u03b8\u2217 by adding a learnable matrix W:\n$\\Theta' = \\Theta^* + W.$\n(8)\nThis produces the refined 3D Gaussian Splatting \u0398\u2191. The matrix W is optimized by minimizing the LPIPS loss between the rendered view at the optimal camera pose \u03c0opt and the input view c:\nW = arg min LPIPS(g(\u0398\u2191, \u03c0opt), c).\nW\n(9)\nDuring this optimization, only the view corresponding to \u03c0opt is optimized, while other views remain frozen. This ensures that the refined 3D Gaussian Splatting \u0398\u2191 aligns more closely with the input view, without affecting the fidelity of the other perspectives."}, {"title": "4. Experiment", "content": "4.1. Implementation Details\nDatasets. We employ ChatGPT to generate over 100k prompts for distinct objects in bulk, and utilize a text-to-image model to transform the prompts into finely crafted object images. We do not rely on any existing image datasets for training, which renders our dataset acquisition nearly cost-free, allowing users to expand the image dataset according to their specific needs. Our sota experiments are conducted on the entire Google Scanned Objects (GSO [6]) dataset, while our ablation studies are carried out on a randomly selected subset of 300 assets from the GSO dataset.\nBaselines. In our research, the Multi-View Diffusion Model utilizes Era3D [14], while the large 3D reconstruction model employs LGM [29] which uses Gaussian Splatting for 3D representation. Our pipeline theoretically does not impose any restrictions on the specific types of multi-view diffusion models, reconstruction models, or 3D representations. Among the models we compare, Triplane-Gaussian and LGM both use 3D Gaussian Splatting, while OpenLRM [10] and VFusion3D [7] employ NeRF, and InstantMesh [34] and CRM [33] utilize mesh representations. To ensure fairness, in the main results (Section 4.2), the comparative experiments of our method against other models are conducted without utilizing Input View Optimization.\nTraining. We train our model using 8 NVIDIA A100 (80G) GPUs for about a day, which, to our knowledge, represents the lowest training cost and shortest training time compared to similar models [7]. Similar to the training of LGM, we resize the images to 256x256 for LPIPS loss to save memory. Both our training and inference input views come from the unrefined multi-view diffusion. All rendered images and supervision images were set against a white background with a resolution of 512x512."}, {"title": "4.2. Main Results", "content": "Quantitative Results. We compare MVBoost with recent open-world feed-forward single/sparse-view to 3D methods, including TriplaneGaussian [36], LGM [29], OpenLRM [10], CRM [33], and InstantMesh [34]. For the comparative methods, we utilize their official implementations. Since input settings differ among the baselines, we evaluate all methods in a unified single-view to 3D setting. For the GSO dataset, we utilize the first thumbnail image as the single-view input. Then, we use the official multi-view diffusion models of each respective model to generate multi-view inputs, thereby simulating the real user environment.\nThe quantitative experiments on the quality of 2D images are illustrated in Table 1. In this study, our method, TriplaneGaussian, and LGM produce 3D representations using 3D Gaussian Splatting, while OpenLRM and VFusion3D use NeRF, and InstantMesh and CRM employ meshes as their 3D representation. We render the corresponding orbiting views of the 3D Gaussian Splatting, NeRF, and mesh outputs, and compare them against the Ground Truth for evaluation metrics. Our method achieves state-of-the-art performance across various metrics of 2D novel view synthesis quality.\nThe quantitative experiments on the quality of 3D geometry are illustrated in Table 2. In this study, our method, TriplaneGaussian, and LGM convert 3D Gaussian Splatting into a mesh following the mesh extraction framework established by LGM. Similarly, OpenLRM and VFusion3D export their NeRF representations as meshes according to their official implementations. Subsequently, all methods are uniformly rescaled to operate at the same mesh scale for the experiments. Even though we choose 3D Gaussian Splatting as our 3D representation, we still achieve state-of-the-art performance across all metrics of geometric quality.\nQualitative Results. Figure 3 visualizes the qualitative results. Compared with other methods, our boosted results show very high fidelity for diverse inputs, including Robots, cartoon characters, and boat images of various subjects."}, {"title": "4.3. Ablation Study and Disscussion", "content": "Our MVBoost correctly models complex geometry (e.g., dinosaur and warrior) and generates the details, such as the ship's sail and halyard, reflecting the great generalization ability of our model.\nTo demonstrate the effectiveness of the multi-view refinement strategy and the input view optimization model, we conduct several ablation studies, which include: (1) qualitative and quantitative comparative experiments on 2D multi-view data before and after refinement; (2) quantitative experiments employing different intensities of refined multi-view supervision; (3) comparative experiments evaluating the Input View Optimization module before and after its implementation.\nFigure 4 illustrates the qualitative experiments conducted with our multi-view refinement strategy. The first row shows the original multi-view images, while the second row presents the refined multi-view images. The restoration of the red sports car's side door highlights the module's ability to enhance texture details. The chandelier and telescope examples demonstrate its effectiveness in preserving geometric consistency. In the keyboard example, the side view, which initially appeared unrealistic, is corrected to a more plausible representation. In the train example (Figure 5), significant viewpoint errors in the original multi-view images are successfully corrected by our method to achieve accurate alignment.\nIn the selection of noise intensity within the multi-view refinement strategy, we conduct quantitative experiments on the GSO dataset, as illustrated in Table 3. The results indicate that the optimal refinement effect is achieved at a strength of 0.95. To further illustrate the efficacy of a noise intensity of 0.95, we employ a reconstruction model supervised with varying noise intensities, subsequently conducting quantitative experiments on image quality using the GSO dataset. The experimental results are presented in the Table 4. D The input view optimization is a post-processing method. All previous quantitative and qualitative experiments are conducted without Input View Optimization. The experimental results with Input View Optimization are shown in Figure 6. On the GSO dataset, the LPIPS loss of the input view decreased from an average of 0.108 to 0.002."}, {"title": "5. Conclusion", "content": "This paper designs a novel framework MVBoost to boost 3D reconstruction. MVBoost combines the advantages of the high accuracy of the multi-view generation model and the consistency of the 3D reconstruction model to generate pseudo-ground truth, as the data source. A feed-forward 3D reconstruction model is trained by synthetic data, showing superior performance in the reconstruction of 3D assets. Followed by a post-processing method, input-view optimization further optimizes the corresponding viewpoints based on the input image, ensuring that the most important viewpoint is accurately tailored to the user's needs. Extensive evaluations on benchmark tasks demonstrate that MVBoost achieves state-of-the-art performance in 3D reconstruction. Our approach provides a new pipeline for 3D reconstruction, and we hope to contribute to the development of the 3D reconstruction community."}, {"title": "6. More Implementation Details", "content": "Training. In our training process, we use a fixed set of six viewpoints (front, front right, right, back, left, front left) for supervision. Our camera model employed orthographic projection. The rank of the LoRA layer in our boosted model is 32.\nMetric. We evaluate both the 2D visual quality and 3D geometric quality of the generated assets. We use the same single view and then employ each model's official multi-view generation process to create their multi-view inputs, simulating real user inference scenarios. For 3D geometric evaluation, we first align the coordinate system of the generated meshes with the ground truth meshes, and then reposition and re-scale all meshes into a cube of size [-1,1]3. We report Chamfer Distance (CD) and F-Score (FS) with a threshold of 0.05, which are computed by all vertexes from the surface uniformly."}, {"title": "7. More Details about Method", "content": "MVBoost generates refined multi-view as pseudo-ground truth through the Multi-View Refinement Strategy. The algorithm details are presented in Algorithm 1. The symbols used in the algorithm are explained and defined in the main paper."}, {"title": "8. Experiment on OpenLRM", "content": "The framework is compatible with various reconstruction models, supporting different types of 3D representations. To further illustrate the versatility of our approach, we boost OpenLRM with multi-view refinement. We employ a dataset of 5k refined multi-view images, integrating LoRA layers with the rank of r = 32 into the self-attention and cross-attention components of the Transformer Decoder in OpenLRM. We train the boosted OpenLRM on 8 NVIDIA A100 (80G) GPUs for half a day. Qualitative results are illustrated in Figure 7, while quantitative outcomes are detailed in Table 5 and Table 6. The boost OpenLRM demonstrates superior performance over the original OpenLRM in terms of geometric and textural details."}]}