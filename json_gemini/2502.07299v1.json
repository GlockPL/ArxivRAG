{"title": "Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification", "authors": ["Zicheng Liu", "Siyuan Li", "Zhiyuan Chen", "Lei Xin", "Fang Wu", "Chang Yu", "Qirong Yang", "Yucheng Guo", "Yujie Yang", "Stan Z. Li"], "abstract": "The interactions between DNA, RNA, and proteins are fundamental to biological processes, as illustrated by the central dogma of molecular biology. While modern biological pre-trained models have achieved great success in analyzing these macromolecules individually, their interconnected nature remains under-explored. In this paper, we follow the guidance of the central dogma to redesign both the data and model pipeline and offer a comprehensive framework, Life-Code, that spans different biological functions. As for data flow, we propose a unified pipeline to integrate multi-omics data by reverse-transcribing RNA and reverse-translating amino acids into nucleotide-based sequences. As for the model, we design a codon tokenizer and a hybrid long-sequence architecture to encode the interactions of both coding and non-coding regions with masked modeling pre-training. To model the translation and folding process with coding sequences, Life-Code learns protein structures of the corresponding amino acids by knowledge distillation from off-the-shelf protein language models. Such designs enable Life-Code to capture complex interactions within genetic sequences, providing a more comprehensive understanding of multi-omics with the central dogma. Extensive Experiments show that Life-Code achieves state-of-the-art performance on various tasks across three omics, highlighting its potential for advancing multi-omics analysis and interpretation.", "sections": [{"title": "1. Introduction", "content": "The advent of large-scale biological datasets has sparked extensive interest in sequence modeling across multiple omics domains-ranging from DNA and RNA to proteins (Smith & Doe, 2023; Li & Wang, 2024; Li et al., 2024; Liu et al., 2024a). These endeavors aim to leverage deep learning to uncover functional elements, predict gene regulation, and accelerate protein engineering. Notably, the successful application of Transformer-based architectures in natural language processing (NLP) (Vaswani et al., 2017; Devlin et al., 2019) has inspired numerous efforts to adapt these methods to biology, giving rise to models for variant effect prediction (Zhou et al., 2022), protein structure elucidation (Jumper et al., 2021a), and more.\nDespite the promise of these paradigms of foundation models in computational biology (Lin et al., 2023b; Nguyen et al., 2024a; Shen et al., 2024), three major obstacles continue to hamper multi-omics modeling: (1) Data Island."}, {"title": "2. Method", "content": ""}, {"title": "2.1. Preliminaries", "content": "Central Dogma links islands. The central dogma of molecular biology describes the flow of genetic information from DNA \u2192 RNA \u2192 Protein. DNA is transcribed into RNA, and RNA is subsequently translated into protein. This process ensures that the information encoded in the genome can be expressed to perform diverse biological functions.\nLet $D \\subset \\{A, T, C, G\\}^*$ be the space of DNA sequences, and let $R \\subset \\{A, U, C, G\\}^*$ be the space of RNA sequences. For proteins, denote $P \\subset \\Sigma^*$, where $\\Sigma$ is the amino acid alphabet (e.g., $\\Sigma = \\{Ala,Arg, Asn, ...\\}$). The central dogma stipulates two fundamental mappings:\n$\\mathcal{T}_{transcribe}: D\\rightarrow R$, $\\mathcal{T}_{translate}: R \\rightarrow P$. (1)"}, {"title": "2.2. Data Pipeline for Life-Code", "content": "Unified to nucleotides. Given an RNA sequence $y \\in R$ or a protein sequence $z \\in P$, we map them back to DNA-level tokens. Specifically: For RNA, assume we have the corresponding genomic DNA region (e.g., from reference genome or aligned reads) and apply $y = \\mathcal{T}_{transcribe}(x)$ in reverse to identify the original $x \\in D$. For protein, we leverage the standard codon translation table. Each amino acid $z_i \\in \\Sigma$ typically maps to one or more codons in $\\{A, T, C, G\\}^3$. We choose the canonical codon (or the most frequent codon) for each amino acid, yielding a surjective function $g : \\Sigma \\rightarrow \\{A, T, C, G\\}^3$. Thus, a protein of length L becomes a DNA-like sequence of length 3L.\nData sampling. Following GenBank (Benson et al., 2012) curation, we sample from the RefSeq database, truncating long genomic segments and padding shorter ones. For CDS-Amino Acid pairs, we extract annotated CDS records and their corresponding protein sequences, then pack multiple short segments into a single training sample with special separators. See Figure 3 for a schematic illustration. This approach enables us to efficiently process both longer genomic segments and multiple shorter CDS-AA pairs in a unified framework. By balancing truncation/padding for DNA sequences and packing for CDS segments, we ensure that each batch contains a diverse mixture of long-range genomic context and coding information, thereby maximizing the model's exposure to various biological signals."}, {"title": "2.3. Life-Code Tokenizer Training", "content": "After mapping all sequences (DNA, RNA, and protein) to a unified DNA-like form, each sample can be viewed as a string in $\\{A, T, C, G\\}^*$. However, to effectively capture both nucleotide-level and codon-level patterns, as shown in Figure 4, our tokenizer incorporates a two-task training paradigm: (i) masked language modeling for nucleotide reconstruction and (ii) protein translation for CDS.\nTokenization with codon-level embeddings. Consider a raw DNA segment of length 3L. We first apply a 1-d convolution (kernel size = 3) together with an \"unfold\" operation (stride = 3) to transform each consecutive triplet $(x_{3j+1}, x_{3j+2}, x_{3j+3})$ into a codon-like representation:\n$e_j = Conv1d(x_{3j+1:3j+3}).$\nThis step effectively condenses triplets-codons into a more compact embedding sequence $\\{e_1,e_2,...,e_L\\} \\in \\mathbb{R}^{L\\times3d}$. We apply a small linear projection or MLP to each $e_j \\in \\mathbb{R}^d$, mapping it into the token embedding space.\nDual pre-training. We jointly pre-train this tokenizer on two objectives: (1) Nucleotide MLM (Reconstruction). For each codon-embedded sequence, we randomly mask a subset of tokens and train the model to reconstruct the original nucleotides. Technically, we design a DNA De-Tokenizer that applies a Fold (stride = 3) and an inverse convolution (De-Convld) to recover the three-base codon, thereby reconstructing the full-length of DNA sequences. Formally, if $\\hat{x}_{3j+1:3j+3}$ is the predicted codon for position j, the loss is:\n$L_{MLM} = \\sum_{j=1}^L - log \\, p(\\hat{x}_{3j+1:3j+3} \\,|\\, x),$ (2)\nwhere $x$ is the masked sequence. This ensures that the tokenizer learns robust nucleotide representations that can reconstruct the original sequence. (2) CDS-to-Protein Translation. For coding regions, each codon corresponds to an amino acid $(z_j \\in \\Sigma)$. We thus include the Amino Acid Translator module, a two-layer MLP layer, that looks up the genetic code table and learns to predict $z_j$ given $e_j$:\n$L_{Trans} = \\sum_{j=1}^L - log \\, p(z_j \\,|\\, e_j).$ (3)\nBy training on paired CDS-amino-acid data, the tokenizer acquires biologically grounded representations, effectively bridging codon-level features to protein-level semantics."}, {"title": "2.4. Bi-Directional Hybrid Model for Long Sequence", "content": "We describe the Life-Code Encoder, which takes the tokenized embeddings as input and models DNA sequences efficiently, respecting double-strand complementarity.\nBi-directional input. To use the double-helix property as Schiff et al. (2024a), we split the embeddings $E = (e_1, e_2, ..., e_n)$ into two parts in the feature dimension:\n$(E^{(+)}, E^{(-)}) = Split(E),$\nwhere $E^{(+)}$ represents the forward strand embedding, and $E^{(-)}$ represents the reverse (complementary) strand. We process these two strands in parallel using the same model weights or parameter-sharing scheme $f_\\theta$:\n$H^{(+)} = f_\\theta(E^{(+)})$, $H^{(-)} = f_\\theta (Reverse(E^{(-)})).$\nFinally, we concatenate or fuse the representations:\n$H = [H^{(+)}, H^{(-)}],$ yielding a representation that encodes both the forward and reverse complement features, where $H \\in \\mathbb{R}^D$.\nHetero Token Mixer. Inspired by recent advances in EVO-style architectures (Nguyen et al., 2024a) and Gated Delta Networks (GDN) (Yang et al., 2024), we design the encoder in Life-Code as the linear-attention Gated DeltaNet mixed with a small proportion of standard multi-head self-attention (MHSA). Concretely, for every twelve encoder layers, eleven employ the GDN update rule-offering linear or near-linear complexity via kernel-based reordering-while one layer applies MHSA. The GDN layer relies on a delta update to maintain memory, reducing the quadratic overhead of na\u00efve Transformers. Meanwhile, the one full-attention layer provides additional global context, which is crucial for capturing fine-grained long-range dependencies. This 11:1 ratio of GDN to dense layers significantly boosts computational efficiency (compared to a pure $O(n^2)$ Transformer) while preserving model expressiveness and long-distance modeling capacity.\nEncoder Pre-training. Once the decoder and translator of our tokenizer weights are fixed (see Section 2.3), we train the Life-Code Encoder (Figure 5) on three tasks: (1) Masked DNA Reconstruction (MLM), (2) CDS-to-Amino-Acid Translation, and (3) Token-Level Knowledge Distillation (KD) from a pretrained Protein LM. The first two objectives match those used to pre-train the tokenizer but are now applied to the encoder output. Specifically, the encoder generates codon embeddings that a DNA De-tokenizer"}, {"title": "2.5. Empirical Consequences", "content": "Biological insights In Figure 6, our analyses reveal that the Life-Code Tokenizer captures biologically meaningful codon relationships in two complementary ways. First, codon usage bias (left figure) indicates that different species, from E. coli to H. sapiens, vary markedly in their preference for certain codons an effect traditionally tied to tRNA abundance and translational efficiency. By visualizing these preferences across amino acids, we observe how each organism clusters around specific high-frequency codons, underscoring the ecological and evolutionary factors shaping codon selection. Second, the t-SNE plot of the tokenizer's learned codon embeddings (right figure) demonstrates that codons translating to amino acids with similar biochemical properties (e.g., hydrophobic or charged) are embedded close together, while the stop codons occupy a separate region. This structural arrangement confirms that the model internalizes codon-amino acid mappings, reflecting both frequency patterns and higher-level attributes like polarity or aromaticity. Together, these findings suggest that the Life-Code Tokenizer not only memorizes sequence data but also discerns and encodes intrinsic biological signals-ranging from species-specific usage preferences to fundamental amino acid groupings.\nTraining efficiency We evaluated the throughput (thousands of tokens processed per second) of Life-Code, EVO, and DNABERT-2 on a single A100 GPU under increasing sequence lengths (with batch size adjusted accordingly). As shown in Figure 7, DNABERT-2 (blue) exhibits a steep drop-off at longer sequences and ultimately runs out of memory (OOM) at 16K \u00d7 2. In contrast, EVO (red) and Life-Code (green) both maintain higher throughput thanks to their hybrid attention mechanisms. Notably, Life-Code achieves the best scalability, retaining above 40K t/s even at 8K \u00d7 4. These results confirm that Life-Code's efficient architecture can accommodate extensive input lengths with minimal degradation in training speed, making it well-suited for large-scale genomic modeling."}, {"title": "3. Experiments", "content": "We first introduce the network architectures and pre-training settings of our Life-Code tokenizer and encoder. Then, we evaluate Life-Code on DNA, RNA, protein, and multi-omics tasks with supervised fine-tuning (SFT) or zero-shot evaluation protocols. All experiments are conducted with PyTorch, transformers library, and NVIDIA A100-80G GPUs."}, {"title": "3.1. Experimental Setup", "content": "Life-Code Tokenizer. With the nucleotide vocabulary, the Life-Code tokenizer contains the following modules: (i) A linear projection from the DNA input to 384-dim implemented by nn.Embedding; (ii) A GDN block (DeltaNet) with 384-dim for global contextual modeling; (iii) A 1-d Convolution with a kernel size of 3 and stride of 1, followed by an UnFold operation to merge every three nucleotide tokens into 768-dim codon embedding. Similarly, we also design the DNA De-Tokenizer and Amino Acid Translator for tokenizer pre-training. As shown in Figure 4, we first pre-train the Life-Code tokenizer by AdamW optimizer for 100,000 iterations with a basic learning rate of $2 \\times 10^{-4}$ adjusted by the Cosine scheduler and a total batch size of 512, as summarized in Table A2. As shown in Table A1, the two pre-training datasets (DNA and DNA-AA pairing) are constructed from NCBI, GenBank, and UniRef50 databases as described in Figure A1 and Appendix A. View Appendix A.2 for details.\nLife-Code Encoder. Similar to the BERT-Large architecture (Devlin et al., 2019), the Life-Code encoder has 24 layers in total with an embedding dim of 1024 and 340M parameters. We enhance the model with three-fold designs: (i) Mixture of DeltaNet (Yang et al., 2025) blocks and Multi-head Self-attention (MHSA) blocks, especially every 11 DeltaNet blocks followed by one MHSA block. (ii) The LLaMA-like macro design (Touvron et al., 2023) with RMSNorm (Zhang & Sennrich, 2019), Layer Scale (Touvron et al., 2021), Rotary Position Embedding (RoPE) (Su et al., 2021), SwiGLU, and FlashAttention to facilitate stable pre-training with long sequences. (iii) During pre-training, we apply the packing strategy (Warner et al., 2024) to build up a long sequence with several CDS, which compromises the gap between different lengths of the reference sequence and the coding sequences. With three pre-training tasks in Eq. 5, the Life-Code tokenizer and encoder are optimized by AdamW optimizer for 1M iterations with the batch size of 256 and the basic learning rate of $1 \\times 10^{-4}$. We adopt 15% random masking in BERT for Masked DNA Reconstruction and the 3-mer span masking for CDS-to-Amino-Acid Translation. As for KD from a pre-trained Protein LM, we adopted ESM2-650M (Lin et al., 2022) and a protein decoder with an output dimension of 1280. During the warmup periods, the maximum sequence length is 1024, with a linear warmup of the learning rate for 10 iterations. After that, the maximum sequence length is set to 4k with the learning rate adjusted by the Cosine Annealing scheduler. View Appendix A.2 for more details."}, {"title": "4. Related Works", "content": "DNA Foundation Models. Recent years have seen rapid progress in DNA foundation models that leverage Transformer-like architectures. Early attempts such as DNABERT (Ji et al., 2021b) treated genomic sequences akin to linguistic tokens, enabling pre-trained contextual representations. Follow-up work DNABERT-2 (Zhou et al., 2023) expanded this paradigm by introducing more efficient training protocols and supporting multi-species data. In parallel, The Nucleotide Transformer (Dalla-Torre et al., 2023) demonstrated the feasibility of scaling up Transformer architectures for human genomics. Beyond Transformers, kernel-based or hierarchical approaches emerged-e.g., HyenaDNA (Nguyen et al., 2024b), which reduces the quadratic complexity of attention for extremely long sequences. Meanwhile, alternative designs like Caduceus (Schiff et al., 2024b) incorporate selective structural priors for long-range DNA modeling, and MxDNA (Qiao et al., 2024) explores adaptive tokenization schemes that automatically discover suitable patterns for genomic data.\nProtein and RNA Advances. On the protein side, AlphaFold series (Jumper et al., 2021a;b) revolutionized structure prediction, catalyzing a surge in protein-based language models such as ESM-2 (Lin et al., 2022), which refines large-scale protein embeddings. Beyond proteins, RNA modeling also gained traction; for instance, (Shen et al., 2024) employed a language-model-based technique for accurate 3D structure prediction. These efforts underscore the trend toward specialized architectures for each biomolecule yet also highlight the desire for integrative multi-omics solutions.\nMulti-Omics Modeling. Recent studies aim to bridge the gap between genomic and protein sequences, pushing beyond single-modality tasks. CD-GPT (Zhu et al., 2024) explicitly connects DNA, RNA, and proteins through the central dogma, while BSM (Xiang et al., 2024) highlights the potential for small but effective models covering genes and proteins simultaneously. Evo (Nguyen et al., 2024a) similarly integrates molecular and genome-scale data with a generalized sequence-modeling approach. Closely related is LucaOne (He et al., 2024), which advocates a unified nucleic acid and protein language for biological representation."}, {"title": "5. Conclusion and Limitations", "content": "We have introduced Life-Code, a method that unifies DNA, RNA, and protein sequences by mapping the latter two back to a DNA-like representation. This design offers straightforward multi-omics integration and maintains biological interpretability via differential encoding of coding versus non-coding regions. To handle the long-range dependencies inherent in genomic data, we employ an efficient symmetric convolution-based architecture. Moreover, knowledge distillation from large protein models allows our approach to scale without excessive resource demands.\nLimitations Experimental results demonstrate that Life-Code achieves competitive performance in protein structure prediction and phenotype analysis. However, limitations remain: refining the modeling of heterogeneous non-coding regions, incorporating post-translational modifications, and addressing the loss of fine-grained information during knowledge distillation. Future work will also focus"}, {"title": "A. Implementation Details", "content": ""}, {"title": "A.1. Pre-training Dataset", "content": "We collect two datasets for pre-training of Life-Code models, i.e., a pure DNA dataset and a DNA-AA pairing dataset, with the collection process shown in Figure A1. As for the DNA dataset, we collect the reference sequences (RefSeq) of multiple species to ensure generalization abilities from the database of the National Center for Biotechnology Information (NCBI) at following the Multi-species Genomes\u00b9 provided by Nucleotide Transformer (Dalla-Torre et al., 2023) and DNABERT2 (Zhou et al., 2024). As for the DNA-AA pairing dataset, we collect the cDNA of coding sequences (CDS) and its corresponding Amino Acids (AA) in the GenBank database at genbank, which aims to model the transcription and translation processes of the central dogma. We also collect some Amino Acids in the UniRef50 database following LucaOne (He et al., 2024) and obtain their corresponding cDNA by reverse translation with online tools. We provide detailed information for used datasets in Table A1."}, {"title": "A.2. Life-Code Tokenizer", "content": "Vocabulary. There are two vocabularies used in Life-Code. The unified vocabulary only uses 4 nucleotides {A, T/U, C, G} of nucleic acid with 5 special tokens, including \u201c[U]\u201d/\u201c[UNK]\u201d, \u201c[PAD]\u201d, \u201c[CLS]\u201d, \u201c[SEP]\u201d, and \u201c[MASK]\u201d for unknown nucleotides, padding tokens, the class token, separator tokens, and masking tokens. Meanwhile, the Life-Code can also use the codon vocabulary (i.e., the 3-mer of 4 nucleotides that constructs 64 codon tokens), which could be merged into 20 amino acids of protein (20 uppercase letters excluding \u201cB\u201d, \u201cJ\u201d, \u201cO\u201d, \u201cU\u201d, \u201cX\u201d, and \u201cZ\u201d). It can only be applied when the length of an input sequence is multiples of 3, i.e., the cDNA of amino acids or matured mRNA (CDS). The pre-trained protein language model employs the amino acid vocabulary of ESM-2 (Lin et al., 2022)."}, {"title": "Tokenizer Network", "content": "As shown in Figure 4, with the nucleotide vocabulary (including 4 nucleic acids and 5 special symbols), the Life-Code tokenizer contains the following modules: (a) A linear projection from 9-dim to 384-dim implemented by nn. Embedding; (b) A GDN block (Gated DeltaNet) with 384-dim for global contextual modeling with linear computational complexity; (c) A 1-d Convolution with a kernel size of 3 and stride of 1, followed by an UnFold operation to merge every three nucleotide tokens into 768-dim codon embedding. Similarly, we design the DNA De-Tokenizer with the symmetrical network as the Life-Code tokenizer: (a) Fold operation with a 1-d Convolution with a kernel size of 3 to unmerge the codon embedding to 384-dim, (b) A linear projection from 384-dim to 9-dim vocabulary to reconstruct the original DNA sequences. We also design the Amino Acid Translator as a two-layer MLP that translates the codon embedding to the corresponding Amino Acid sequences."}, {"title": "Pre-training Settings.", "content": "As shown in Figure 4, we pre-train the Life-Code tokenizer with the DNA de-tokenizer and Amino Acid de-tokenizer by AdamW optimizer for 100,000 iterations (randomly sampled datasets) with a basic learning rate of 2e-4 and a batch size of 512, as detailed in Table A2. We utilize 8 Nvidia A100-80G GPUs with a per-GPU batch size of 8 and a gradient accumulation time of 4."}, {"title": "A.3. Life-Code Encoder", "content": ""}, {"title": "Encoder Architecture.", "content": "As shown in Table A2, the Life-Code encoder has 24 layers in total with the embedding dim of 1024 with the following designs: (1) Mixture of GDN blocks (DeltaNet (Yang et al., 2025)) and multi-head self-attention (MHSA) blocks as a hybrid model, especially every 11 GDN blocks followed by a self-attention block like MiniMax-01 (MiniMax et al., 2025), which could utilize the complementary properties of GDN and MHSA while maintaining efficiency. (2) The model macro design employs pre-norm (Wang et al., 2019a) with RMSNorm (Zhang & Sennrich, 2019), Layer Scale (Touvron et al., 2021), Rotary Position Embedding (RoPE) (Su et al., 2021), SwiGLU (Touvron et al., 2023), and FlashAttention implementations to facilitate training large-scale models stably with long sequences. (3) During pre-training, we apply the packing strategy (Warner et al., 2024) to build up a long sequence with several CDS, which compromises the gap between different lengths of the reference sequence and the coding sequences, as shown in Figure 3."}, {"title": "Pre-training Settings.", "content": "As shown in Figure 5, we further pre-train the Life-Code tokenizer and Encoder with three tasks in Eq. 5 for 1M steps with the batch size of 256 and the basic learning rate of $1 \\times 10^{-4}$. We adopt 15% random masking in BERT for Masked DNA Reconstruction and the 3-mer span masking for CDS-to-Amino-Acid Translation. As for Knowledge Distillation from a pre-trained Protein LM, we adopted ESM2-650M (esm2_t33_650M_UR50D) (Lin et al., 2022) and a protein decoder with the output dimension of 1280. During the warmup periods, the maximum sequence length is 1024, with a linear warmup of the learning rate for 10 iterations. After that, the maximum sequence length is set to 4k with the learning rate adjusted by the Cosine Annealing scheduler (decay to $1 \\times 10^{-6}$). We utilize 8 Nvidia A100-80G GPUs with a per-GPU batch size of 2 and a gradient accumulation time of 16."}, {"title": "A.4. Supervised Fine-tuning", "content": "In most cases, we apply Supervised Fine-tuning (SFT) to transfer pre-trained models to downstream tasks. Following (Nguyen et al., 2024c; Zhou et al., 2024), adding the decoder head (e.g., an MLP head) to a specific downstream task, the linear attention (RNN) or self-attention blocks in the pre-trained encoder models are frozen, while Low-Rank Adaptation (LORA) strategy (Hu et al., 2021) is employed to parameter-efficiently fine-tuning the models by AdamW optimizer with a batch size of 32. For each task, if the benchmark and models have provided hyper-parameters, we follow the official settings, or we choose the best combinations of the basic learning rate {1e-5, 5e-5, 1e-4}, the weight decay {0, 0.01}, the LoRA rank {4, 8, 16, 24, 48}, the LoRA alpha {8, 16, 24, 48, 96}, and the total fine-tuning epoch {5, 10} on the validation set following the GUE benchmark and GenBench (Liu et al., 2024a). Note that the maximum input length will be determined for different tasks since the sequence lengths of downstream tasks vary widely. We report the averaged results over three runs with the optimal settings."}, {"title": "B. Downstream Task Settings and Extensive Comparison Results", "content": ""}, {"title": "B.1. DNA Tasks with Genomics Benchmark", "content": "As proposed by (Gre\u0161ov\u00e1 et al., 2023), three groups of basic genomic tasks are collected as binary classification with top-1 accuracy in the Genomics Benchmark. As for the enhancer prediction, three datasets are provided for identifying enhancer regions in the mouse or human genome. As for the species classification, two datasets are selected for identifying sequences as either coding (exonic) or intergenic (non-coding) and classifying sequences as originating from humans or worms (C. elegans). As for the regulatory elements classification, three datasets are used for classifying sequences as regulatory regions based on Ensembl annotations, identifying open chromatin regions, or identifying non-TATA promoter regions in the human genome. We utilize the fully reproduced results of various DNA models in GenBench (Liu et al., 2024a)."}, {"title": "B.2. DNA Tasks with GUE Benchmark", "content": "As proposed by DNABERT2 (Zhou et al., 2024), the GUE benchmark contains 24 datasets of 7 practical biological genome analysis tasks for 4 different species using Matthews Correlation Coefficient (MCC) as the evaluation metric. To comprehensively evaluate the genome foundation models in modeling variable-length sequences, tasks with input lengths ranging from 70 to 1000 are selected. The following descriptions of the supported tasks are included in the GUE benchmark, where these resources are attached for illustration.\nPromoter Detection (Human). This task identifies human proximal promoter regions essential for transcription initiation. Accurate detection aids in understanding gene regulation and disease mechanisms. The dataset includes TATA and non-TATA promoters, with sequences -249 to +50 bp around the Transition Start Site (TSS) from Eukaryotic Promoter Database (EPDnew) (Dreos et al., 2013). Meanwhile, we construct the non-promoter class with equal-sized randomly selected sequences outside of promoter regions but with TATA motif (TATA non-promoters) or randomly substituted sequences (non-TATA, non-promoters). We also combine the TATA and non-TATA datasets to obtain a combined dataset named all.\nCore Promoter Detection (Human). This task is similar to the detection of the proximal promoter with a focus on predicting only the core promoter region, the central region closest to the TSS, and the start codon. A much shorter context window (center -34 +35 bp around TSS) is provided, making this a more challenging task than the prediction of the proximal promoter.\nTranscription Factor Binding Site Prediction (Human). This task predicts human transcription factor binding sites (TFBS), crucial for gene expression regulation. Data from 690 ENCODE ChIP-seq experiments (161 TF binding profiles in 91 cell lines) (Consortium et al., 2012b) are collected via the UCSC genome browser. TFBS sequences are 101-bp regions around peaks, while non-TFBS sequences match in length and GC content. There are 5 datasets selected from a curated subset of 690, excluding trivial or overly challenging tasks.\nSplice Site Prediction (Human). This task predicts splice donor and acceptor sites, the exact locations in the human genome where alternative splicing occurs. This prediction is crucial to understanding protein diversity and the implications of aberrant splicing in genetic disorders. The dataset (Scalzitti et al., 2021) consists of 400-bp-long sequences extracted from Ensembl GRCh38 human reference genome. As suggested by Ji et al. (2021a), existing models can achieve almost perfect performance on the original dataset, containing 10,000 splice donors, acceptors, and non-splice site sequences, which is overly optimistic about detecting non-canonical sites in reality. As such, we reconstruct the dataset by iteratively adding adversarial examples (unseen false positive predictions in the hold-out set) in order to make this task more challenging."}, {"title": "B.3. mRNA Splicing Tasks", "content": "Following (Zhou et al., 2024; Shen et al., 2024), we evaluate pre-mRNA Splicing Site Prediction as the RNA task, which is a crucial process in eukaryotic gene expression. During splicing, introns are removed from precursor messenger RNAs (pre-mRNAs), and exons are joined together to form mature mRNAs. This process is essential for generating functional mRNAs that could be translated into proteins. Identifying splice sites-the donor sites at the 5' end of introns and the acceptor sites at the 3' end is vital for accurately predicting gene structure and location. Concretely, we regard this task as binary classification of RNA splicing site prediction specifically for acceptor sites and consider two splicing datasets in addition to the Splicefinder dataset (Wang et al., 2019b) used in the GUE benchmark.\nSpliceator dataset. This dataset (Scalzitti et al., 2021) consists of \u201cconfirmed\u201d error-free splice-site sequences from a diverse set of 148 eukaryotic organisms, including humans. The gold standard dataset GS 1 is adopted, which contains an equal number of positive and negative samples, and the F1 score is used as the evaluation metric. We chose three independent test datasets containing the samples from 3 different species of humans, fish (Danio rerio), and fruit fly (Drosophila melanogaster).\nSpliceAI dataset. This dataset (Jaganathan et al., 2019) also constructs a binary classification dataset similar to Spliceator, which utilizes the GTEx (Genotype-Tissue Expression) project for RNA sequencing data from various human tissues and the GENCODE V24lift37 canonical annotation for gene structure information. SpliceAI also references the ClinVar database to evaluate the clinical significance of predicted splicing variants, which contains information on clinically relevant variants and their associations with diseases. This dataset can be regarded as a long-range evaluation and adopts the top-1 AUC-ROC score as the metric."}, {"title": "B.4. Protein and Multi-omic Tasks", "content": "Zero-shot Protein Fitness Prediction. Following EVO (Nguyen et al., 2024a) and protein language models (Lin et al., 2023b), we employ Deep Mutational Scanning (DMS) studies to evaluate the models' abilities for protein tasks, which introduce many mutations to a protein coding sequence and then experimentally measure the effects of these mutations (as fitness scores) on various definitions of fitness (Notin et al., 2022). EVO obtained (DMS) datasets with bacterial (prokaryote) and human (eukaryote) proteins from ProteinGYM at  To adapt this task to nucleotide sequences, EVO proposes to use the wild-type coding sequence and nucleotide mutations reported in the original DMS studies (Notin et al., 2022; Altae-Tran et al., 2021). For generative pre-trained models such as EVO, we rely on likelihood-based scores under the same masking scheme, assessing how well the model anticipates mutations. The model performances of zero-shot function prediction are measured by the strength of Spearman's Rank Correlation Coefficient (SRCC) that correlates the predicted likelihoods with the experimental fitness measurements. Full results of protein fitness prediction are shown in Table 5 and Table A6, in which our Life-Code achieves balancing performances with bacterial and human proteins and surpasses NT-2500M and EVO-7B.\nncRNA-Protein Interactions. Following LucaOne (He et al., 2024), we consider the multi-omics task of ncRNA-protein interactions (ncRPI), which identifies the interaction strengths between non-coding RNAs (e.g., snRNAs, snoRNAs, miRNAs, and IncRNAs) and proteins. Since experimentally identifying ncRPI) is typically expensive and time-consuming, the AI-based ncRPI can be a promising task. LucaOne proposes a binary classification task involving pairs of ncRNA and Amino Acid sequences (20,824 pairs in total) with top-1 accuracy as the metric.\nCentral Dogma Evaluation. To evaluate the modeling of the translation rule in the central dogma, we follow LucaOne (He et al., 2024) to conduct a binary classification task with top-1 accuracy, which determines whether the DNA sequences and the given proteins are correlated. LucaOne collects a total of 8,533 accurate DNA-protein pairs from 13 species in the NCBI"}]}