{"title": "SCFCRC: Simultaneously Counteract Feature Camouflage and Relation Camouflage for Fraud Detection", "authors": ["Xiaocheng Zhang", "Zhuangzhuang Ye", "GuoPing Zhao", "Jianing Wang", "Xiaohong Su"], "abstract": "In fraud detection, fraudsters often interact with many benign users, camouflaging their features or relations to hide themselves. Most existing work concentrates solely on either feature camouflage or relation camouflage, or decoupling feature learning and relation learning to avoid the two camouflage from affecting each other. However, this inadvertently neglects the valuable information derived from features or relations, which could mutually enhance their adversarial camouflage strategies. In response to this gap, we propose SCFCRC, a Transformer-based fraud detector that Simultaneously Counteract Feature Camouflage and Relation Camouflage. SCFCRC consists of two components: Feature Camouflage Filter and Relation Camouflage Refiner. The feature camouflage filter utilizes pseudo labels generated through label propagation to train the filter and uses contrastive learning that combines instance-wise and prototype-wise to improve the quality of features. The relation camouflage refiner uses Mixture-of-Experts(MoE) network to disassemble the multi-relations graph into multiple substructures and divide and conquer them to mitigate the degradation of detection performance caused by relation camouflage. Furthermore, we introduce a regularization method for MoE to enhance the robustness of the model. Extensive experiments on two fraud detection benchmark datasets demonstrate that our method outperforms state-of-the-art baselines.", "sections": [{"title": "Introduction", "content": "Along with the increasing popularity of e-commerce platforms and social media, fraud has increased dramatically. For example, the proliferation of fake reviews on e-commerce platforms may impact both merchants and consumers (Li et al. 2019) and the presence of fraudulent accounts in financial transactions has caused huge property losses for clients(Liang et al. 2021; Ren et al. 2021). In recent years, graph-based fraud detection methods have been widely applied in many practical applications. Researchers usually model entities as nodes and interactions between entities as edges to solve the fraud problem from a graph perspective. Due to the superior representation ability of graph neural networks(GNNs), GNN-based fraud detection methods have attracted extensive attention from industry and academia (Liu et al. 2020; Zhang et al. 2021; Xiang et al. 2023). These methods are based on the assumption that entities with the same goal tend to have \"homophily\" neighborhoods, which means the center nodes rely on information propagated from neighboring nodes in the same class.\nHowever, to avoid being detected by fraud detection systems, fraudsters will try their best to camouflage themselves. Common methods include mimicking the behavioral patterns of benign users and establishing connections with benign users(Hooi et al. 2016; Ge et al. 2018; Liu et al. 2020). The first type of camouflage behavior is called feature camouflage and the second type is called relation camouflage. Figure 1 provides a toy example illustrating the two types of camouflage behavior described above. Fraudsters' camouflage behavior breaks the 'homophily' assumption, where fraudsters may interact with many benign users and have similar characteristics to benign users. Camouflage causes features and connections to be entangled during the propagation process of GNN, making feature learning and structure learning affect each other, which may exaggerate false information(Meng, Ren, and Zhang 2023). Many existing approaches attempt to alleviate the camouflage phenomenon, but most of them only address one type of camouflage and ignore the other. Some methods (Meng, Ren, and Zhang 2023) address both types of camouflage separately and then simply integrate them while neglecting the deeper relationship.\nTo address these issues, we propose SCFCRC, a Transformer-based Fraud Detector that Simultaneously Counteract Feature Camouflage and Relation Camouflage. SCFCRC contains two components: feature camouflage filter and relation camouflage refiner. The feature camouflage filter is to solve the feature camouflage. We first obtain pseudo-labels for each node using the label propagation method, in which only the graph structure information is used and ignoring the original features of the nodes, thus avoiding the negative impact of feature camouflage. The pseudo labels are then used to train the feature camouflage filter. During this period, we use instance-wise and prototype-wise contrastive learning to improve feature quality. Subsequently, the filtered features are sent to the relation camouflage refiner along with the original features. Inspired by the group aggregation strategy(Wang et al. 2023), we perform group aggregation on the filtered features and original features and then convert them into a sequence. We then use the mixture of experts to focus on the substructure of nodes under different relation combinations on the fraud graph, and a manager is developed to guide the training of experts and coordinate the detection results of different experts. Finally, a regularization method for MoE is proposed to improve the robustness of the model.\nIn summary, our contributions are as follows:\n\u2022 We propose a novel fraud detection framework that alleviates the negative effects of feature camouflage and relation camouflage simultaneously.\n\u2022 We leverage mixture of experts to mitigate relation camouflage and propose a regularization method for MoE to improve the collaboration ability between experts. To our best knowledge, this is the first time MoE has been used for fraud detection task.\n\u2022 We conducted extensive experiments and ablation studies to verify the effectiveness of SCFCRC on two real-world datasets. The results show that our approach has made significant progress on all experimental benchmarks, establishing a new state-of-the-art. Our code will be available at Github."}, {"title": "Related Works", "content": "GNN-based Fraud Detection. Recently, many graph-based fraud detectors have been proposed and achieved promising results. CARE-GNN(Dou et al. 2020) and Ri-OGNN(Peng et al. 2021) incorporate a GNN-enhanced neighbor selection module to tackle the camouflage problem in fraud detection. To address the class imbalance in fraud detection scenarios, PC-GNN(Liu et al. 2021) introduces a node sampler and a label-aware neighbor selector to reweight the unbalanced classes. H2-FDetector(Shi et al. 2022) identifies homogeneous and heterogeneous connections while applying separate aggregation strategies for different connection types respectively. GTAN(Xiang et al. 2023) proposes an attribute-driven gated temporal attention network along with risk propagation for learning feature representation of transaction nodes. These GNN-based methods are constrained by the conventional message transmission mode and are unable to effectively tackle the intricate issue of fraudster camouflage. GAGA(Wang et al. 2023) utilizes group aggregation to produce serialized neighborhood information, aggregates multi-hop domain messages using Transformer and enhances the original feature space with learnable encodings. This brings valuable inspiration to our work.\nMixture of Experts. The Mixture-of-Experts(Jacobs et al. 1991) is an ensemble learning method first proposed in the machine learning community. To enhance the model's ability to handle complex visual and speech data, DMoE(Eigen, Ranzato, and Sutskever 2013) extends the MoE structure to the deep neural networks, proposing a deep MoE model composed of multi-layer routers and experts. Afterward, the MoE layers based on different basic neural network architectures have been proposed (Shazeer et al. 2017; Lepikhin et al. 2020; Xue et al. 2022; Zhou et al. 2022), achieving great success in various tasks. Specifically, it first decomposes a task into sub-tasks and then trains an expert model on each sub-task, a gating model is applied to learn which expert is competent and combine the predictions. In this paper, we apply MoE to fraud detection tasks, solving the camouflage problem to improve the performance of the model."}, {"title": "Problem Statement", "content": "We formulate the graph-based fraud detection as a semi-supervised node-level binary classification task. Given a graph $G = \\{V,E,X,Y\\}$, $V = \\{V_1, V_2, ..., V_N\\}$ is the set containing N nodes, $E = \\{A_1, A_2, ..., A_R\\}(R = |E|)$ is the set of edges with a relation $r \\in \\{1, 2, . . ., R\\}$. $A_r$ represents the adjacency matrix, where $A_r$ means that nodes u and v are connected under relation r. $X = \\{X_1,X_2,..., X_N\\}$ is the set of node features, $x_i \\in \\mathbb{R}^d$ is the ith node feature, d is the dimension of feature. In graph-based fraud detection, we consider a semi-supervised situation where only a portion of nodes in G are labeled. Each node $v \\in V$ has a corresponding binary label $y_v \\in \\{0,1\\}$, where 0 represents benign nodes and 1 represents fraud, while other nodes remain unknown. The number of classes is defined as C = 2."}, {"title": "Method", "content": "In this section, we introduce the architecture of the proposed approach first and then present the details of the feature camouflage filter and the relation camouflage refiner. Finally, we introduce the loss function and the training process."}, {"title": "Overview", "content": "The Figure 2 shows the architecture of SCFCRC. Generally, the model includes two components: Feature Camouflage Filter(FCF) and Relation Camouflage Refiner(RCR). The feature camouflage filter focuses on the impact of structure on labels to avoid feature camouflage and outputs filtered features. Relation camouflage refiner aims to refine relation camouflage, which consists of five modules: 1) The label-guided group aggregation generates a sequence of group vectors as the input based on original and filtered features. 2) In the learnable encodings module, the sequence of group vectors is encoded with three types of learnable embeddings. 3) The mixture of experts module for dealing with different relations of input. 4) The management module for guiding the training of experts and combining their ability of detection effectively. 5) The regularized masking for MoE is used to improve the robustness of MoE architectures. Finally, we weightedly combine the outputs of different experts and generate the final prediction of the target node."}, {"title": "Feature Camouflage Filter", "content": "Fraudsters often disguise their true identities by mimicking the features of benign users. Here, we propose the Feature Camouflage Filter, designed to remove the camouflage of features before they are fed into the classifier, to enhance fraud detection. Details are as follows:\nFormally, given a graph G with some nodes labeled, we perform label propagation on it, which ignores features and only considers the graph's structure. Then we send the obtained pseudo label L and graph G into the filter for training. The filter consists of a Multi-Layer Perceptron(MLP) and a GNN, where the MLP is used to output filtered features x':\n$x' = MLP(x)$                                                                                                                     (1)\nThe GNN is used to aggregate neighbor node information for graph representation learning. The neighbor aggregation of node v is as follows:\n$h_v^l = \\sigma(W_g(h_v^{l-1}) \\oplus AGG^{(l)}\\{h_u^{(l-1)}, u \\in N^{(l)}(v)\\}))$                                                                                                                   (2)\nwhere $h_v^{l-1}$ and $h_u^{l-1}$ are the embedding of v and u at layer l . $h_v^{(0)} = x$, a mean aggregator is used for all $AGG^{(l)}$, $W_g \\in \\mathbb{R}^{d_l \\times d_{l-1}}$ is the parameter matrix, $\\oplus$ is activation function, denotes the embedding summation operation. $N^{(l)}(v)$ is the neighbor set of node v at layer l\nFor each node v, its final embedding $z_v = h_v^{(L)}$ is the output of the GNN at the last layer L. We use cross-entropy loss as the loss of GNN:\n$L_{GNN} = \\sum_{v \\in V} - log(y_v\\cdot\\sigma(MLP(z_v)))$                                                                                                                    (3)"}, {"title": "Multi Contrastive Learning", "content": "Inspired by the success of self-supervised contrastive learning (Chen et al. 2020; He et al. 2020), we propose Multi Contrastive Learning to automatically learn how to align features of benign and fraudulent classes, which includes instance-wise and prototype-wise forms.\nInstance-wise Contrastive Learning. To make node features more discriminative, we use an instance-wise contrastive learning objective to cluster the same class and separate different classes of nodes, the objective function is computed as:\n$L_{IC} = \\frac{1}{|B|^2}\\sum_{i=1}^{|B|} \\sum_{j=1}^{|B|} log\\frac{exp(cs(x'_i, x'_j)/\\tau)}{\\sum_{k=1}^{|B|} exp(cs(x'_i, x'_k)/\\tau)} \\mathbb{I}_{y_i=y_j}$                                                                                                                                         (4)\nwhere |B| is the number of nodes in a batch, $cs(\\cdot)$ denotes the cosine similarity function and $\\tau$ controls the temperature.\nPrototype-wise Contrastive Learning. In order to align the filtered feature space with the original feature and avoid excessive discrimination caused by instance-wise contrastive learning, we perform prototype-wise contrastive learning. Prototypes $\\{cen_j\\}_{j=1}^{C}$ denote original feature av-"}, {"title": "Relation Camouflage Refiner", "content": "Label-guided Group Aggregation Traditional graph neural network which relay on message passing mechanisms cannot effectively handle relation camouflage, so we devise a label-guided group(LGA) aggregation strategy to adapt filtered features. Specifically, LGA treats filtered features as additional information and adds them to the aggregation of each hop based on the previous pseudo label. Nodes with the same class label come into the same group and then each group performs aggregation separately:\n$H^\\{k\\} = [h^\\{k\\}_-, h^\\{k\\}_+, h'^\\{k\\}_-, h'^\\{k\\}_+, h^*]^K$                                                                                                                        (6)\n$H_g = \\parallel_{k=1}^K H^\\{k\\}$                                                                                                                                                                                             (7)\nwhere $H_g$ is the sequence of group vectors, $h$ is the average aggregation of features after partitioning according to labels, $h^-$ for negative label nodes, $h^+$ for positive label nodes, $h'^-$ for pseudo-negative label nodes, $h'^+ $for pseudo-positive label nodes, and $h^* $is the aggregation result of masked nodes whose labels are unknown. $H_r$ is the group aggregation results of neighborhood information within K hops under relation r, and $\\parallel$ denotes concatenation operation. We combine the raw feature $h_r$ and filtered feature $h'_r$ and the group vectors $H_r$ together into single sequence $H_{v,r} = [h_r] \\parallel [h'_r] \\parallel H_r$. The next step is to combine all the sequences $H_{v,r}$ as the input feature sequence, which is defined as $H = \\parallel_{r=1}^R H_{v,r}$. Then we get the amount of vectors S = R \u00d7 ((2 \u00d7 C + 1) \u00d7 K + 2), where S is the sequence length.\nLearnable Encodings To harness the structural, relational and label information in the original multi-relation fraud graphs, following previous work(Wang et al. 2023), we introduce three learnable encodings $X_r$, $X_h$, and $X_g$. The difference lies in the change is the length of the embedding sequence caused by the integration of the filtered feature.\n$X_{in} = MLP(X_S) + X_r + X_h + X_g$                                                                                                     (8)\n$H = f_{PEnc}(X_{in})$                                                                                                                                                                          (9)\nwhere $X_{in}$ is the final sequence input, $H \\in \\mathbb{R}^{S \\times d_h}$ denotes the representation vectors, $d_h$ represent the dimension of the encoder, $f_{PEnc}$ refers to public transformer encoder.\nMixture of Experts Module A group of experts is applied to fraud detection, each of which is adept at processing different information of graph structure. Experts share the same structure, which can be smoothly generalized to other scenarios, only the input dimensions accepted by the classifier may differ. Specifically, each expert is implemented with a stack of transformer encoding layers and an MLP classifier that calculates the probability of fraud. The process above is formulated as follows:\n$h_i = AGG(f_{Enc_i}(H))$                                                                                                                                  (10)\n$p_i = softmax(MLP_i(h_i))$                                                                                                                                     (11)\nwhere $f_{enc_i}$ is the ith expert's encoder, $h_i \\in \\mathbb{R}^{2 \\times d_h}$ refers the aggregation of structural information that the ith expert is adept at processing. AGG is concat aggregator. $p_i$ is the probability of fraud predicted by the ith expert, $MLP_i$ is the ith expert's classifier."}, {"title": "Management Module", "content": "This module consists of two components: structure perceptron and manager. The structure perceptron is proposed to generate prior assumptions to guide the manager. The manager is designed to guide experts' training and ensemble the results from all experts.\nStructure Perceptron. Fraudsters try to link multiple benign entities to disguise themselves. According to previous work(Zheng et al. 2017; Kaghazgaran, Caverlee, and Squicciarini 2018), relation camouflage is usually established under a part of the relations of E, not all of them. Based on this, we propose the structure perceptron, which aims to generate a prior assumption under different structures, thereby guiding the manager to focus more on the structure in the non-camouflaged state. Specifically, the structure perceptron module generates the prior assumption $a_g$ based on structures under different relations:\n$scorer_{r_i}(v) = \\beta \\cdot \\frac{1}{|N_{r_i}^{(1)}(v)|} \\sum_{u \\in N_{r_i}^{(1)}(v)} cs(x_u, x_v)+ (1 - \\beta) \\cdot \\frac{1}{|N_{r_i}^{(2)}(v)|} \\sum_{u \\in N_{r_i}^{(2)}(v)} cs(x_u, x_v)$                                                                                               (12)\nwhere $scorer_{r_i}(v)$ and $N_{r_i}(v)$ are respectively the score and neighbor set of node v under relation set $r_i$ that the ith expert focuses on, $\\beta$ is a hyperparameter. We normalize $(scorer_1(v), scorer_2(v), ..., scorer_n(v))$ to get the final prior assumption $a_g$.\nManager. We present a manager to guide the training of experts, which has the same network structure as experts. The manager encodes H and generates attention scores $a_M$:\n$h_M = AGG(f_{Enc_M}(H))$                                                                                                                                                                          (13)\n$a_M = softmax(MLP_M(h_M))$                                                                                                                                          (14)\nwhere $h_M \\in \\mathbb{R}^{2 \\times d_h}$ is representation of manager, $Enc_M$ and $MLP_M$ are the manager's encoder and classifier respectively.\nThe prior assumption $a_g$ and attention score $a_M$ are used to teach the manager to allocate score reasonably and guide the expert's training, the loss function $L_G$, which calculates the logarithmic difference between the prior assumption $a_g$ and the attention scores $a_M$:\n$L_G = D_{KL}(a_G || a_M)$                                                                                                                                                                            (15)\nwhere $D_{KL}(||)$ stands for the Kullback-Leibler divergence."}, {"title": "Regularized Masking for MoE", "content": "Previous MoE-based frameworks heavily relied on the manager's guidance during training and prediction. However, the manager often failed to generate reasonable scores. To address this, we proposed Regularized Masking for MoE(RMMOE). RMMOE mitigates the impact of unreasonable allocation by randomly setting some expert scores to 0 during training and evenly re-distributing these scores to unmasked experts, maintaining a total score of 1. We use KL-divergence to constrain the output before and after the mask to be consistent:\n$L_{RM} = \\sum_{i=1}^{n_e} D_{KL}((a_M)_i * O_i || (a_{Mask})_i * O_i)$                                                                                           (16)\nwhere $n_e$ is the number of experts, $a_{Mask}$ is the attention score after performing the mask operation, $O_i$ is the output of the ith expert."}, {"title": "Learning Objective", "content": "Our training objective includes two parts. First, We minimize the loss of feature camouflage filter L1:\n$L_1 = L_{GNN} + \\lambda_1 L_{IC} + \\lambda_2 L_{PC}$                                                                                                        (17)\nwhere $\\lambda_1$ and $\\lambda_2$ are hyperparameters that control the ratio of $L_{IC}$ and $L_{PC}$.\nThe second objective is to minimize the loss of relation camouflage refiner:\n$L_2 = \\sum_{i=1}^{n_e} (a_M)_i H_{CE}(p_i, Y)$                                                                                                                                             (18)\n$L_2 = L_D + \\lambda_3 L_G + \\lambda_4 L_{RM}$                                                                                                                    (19)\nwhere $L_D$ is the detection loss, $H_{CE}$ refers to the cross-entropy loss function. $\\lambda_3$ and $\\lambda_4$ are hyperparameters that control the ratio of $L_G$ and $L_{RM}$. We perform RMMOE in the training stationary phase and control it by using a hyperparameter $\\delta$. When the training phase is less than $\\delta$, the above loss becomes $L_2 = L_D + \\lambda_3 L_G$."}, {"title": "Experimentation", "content": "Dataset. We conduct experiments on two real-world fraud detection datasets to evaluate the effectiveness of our SCFCRC. The statistic of datasets is shown in Table 1.\n\u2022 YelpChi(McAuley and Leskovec 2013) includes hotel and restaurant reviews filtered (spam) and recommended (legitimate) by Yelp. The nodes in the YelpChi dataset are reviews with 32 handcrafted features, and the dataset includes three relations: R-U-R, R-S-R, and R-T-R.\n\u2022 Amazon(Rayana and Akoglu 2015) collects the product reviews of the Musical Instrument category on Amazon.com, in which nodes are users with 25 handcrafted features and the dataset encompasses three relations: U-P-U, U-S-U, and U-V-U.\nMetrics. We evaluate the detection performance with three widely used and complementary metrics: AUC, AP, and F1-macro. The AUC is the area under the ROC Curve that can evaluate the performance of classification by eliminating the influence of imbalanced classes. The AP is The Area Under the Precision Recall Curve, which focuses more on ranking fraudulent entities than benign ones. The F1-macro is the macroaverage of the two classes of F1 scores. These metrics are bounded within the range [0, 1], with a higher value indicating superior performance. The results presented include the ten-run average and standard deviation obtained on the testing set.\nBaselines. We chose some traditional and improved GNNs as the baseline: GCN(Kipf and Welling 2016), GAT(Velickovic et al. 2017), HAN(Wang et al. 2019), GraphSAGE(Hamilton, Ying, and Leskovec 2017), Graph-SAINT(Zeng et al. 2019), Cluster-GCN(Chiang et al. 2019), and SIGN(Frasca et al. 2020). Besides, some state-of-the-art methods for graph-based fraud detection were used to compare with our approach as follows: CARE-GNN(Dou et al. 2020), RioGNN(Peng et al. 2021), PC-GNN(Liu et al. 2021), FRAUDRE(Zhang et al. 2021), H2-FDetector(Shi et al. 2022), GTAN(Xiang et al. 2023) and GAGA(Wang et al. 2023). In the classical GNN model, the multi-relation graph is amalgamated into a homogeneous graph. We select parameters based on the content of relevant papers.\nExperiment Settings. Our implementations are based on Pytorch and DGL. The size of the training/validation/testing set for all compared methods is set to 0.4/0.1/0.5. The hyperparameters $\\lambda_1/\\lambda_2/\\lambda_3/\\lambda_4$ that control the loss weight are set to 0.1/0.1/0.1/0.3. $\\beta$ is set to 0.5. The masking ratio on YelpChi and Amazon are set to 0.15 and 0.1 respectively. $\\delta$ is set to 0.4. In this paper, we set the number of experts to 4 and the hop count is set to 2. The embedding size on YelpChi and Amazon are set to 32 and 16 respectively, batch size is 512 and 256. The number of layers for each expert and gate is set to 1. The number of public layers on YelpChi and Amazon is set to 2 and 1 respectively. To avoid overfitting, we use dropout mechanism with a dropout rate 0.1. The learning rate is set to 3e-3 and weight decay is 1e-4. All methods are optimized with Adam optimizer."}, {"title": "Overall Detection Results", "content": "The overall performance is shown in Table 2. First of all, it is observed that almost all general GNN models perform weaker than enhanced graph-based fraud detection models. The reason for this is that those general GNN models are based on the homogeneity assumption and cannot handle the noises introduced by camouflage. But SIGN has achieved some performance improvement by aggregating multi-hop neighborhood information, with AUC close to PC-GNN on Amazon and even better performance on YelpChi. This highlights the beneficial nature of multi-hop neighborhood information. Compared to state-of-the-art graph-based fraud detection methods, our proposal significantly outperforms them. Among them, GTAN uses risk embedding and propagation while randomly masking risk features during training, and GAGA introduces a group aggregation module to generate distinguishable multi-hop neighborhood information. Compared with other graph-based fraud detection methods, these two methods make full use of the label information while learning multi-hop neighborhood information, and have substantial improvement overall. But compared with GAGA, SCFCRC achieves better performance improvement on both datasets and achieves 2.37%, 3.34%, and 1% gains in F1-macro, AP, and AUC on YelpChi. The reason behind this can be attributed to the fact that in addition to learning multi-hop neighborhood information and label information, SCFCRC also solves the feature and relation camouflage problem. Overall, the performance shows the effectiveness of SCFCRC."}, {"title": "Ablation Study", "content": "To assess the impact of counteracting the two camouflages on our model's performance, we conducted an ablation study by individually removing the FCF and RCR. The results are shown in Table 2. We can observe that the performance of the two variants decreased significantly. First, When FCF is removed, the performance drops by 0.54%, 1.65%, and 0.98% in terms of AUC, AP, and F1-macro on YelpChi. This demonstrates that FCF can mitigate feature camouflage and supplement more available information. Second, When RCR is removed, we can observe that the performance drops by 0.46% in AUC, 1.33% in AP, and 0.79% in F1-macro on YelpChi. This shows that RCR can effectively refine complex relations structures into multiple relatively simple structures, thereby alleviating the problem of relation camouflage. In addition, Table 3 shows more detailed ablation experiments.\nw/o LIC: When instance-wise contrastive learning is removed, the distinction between features of different classes decreases, resulting in a decrease in detection performance.\nw/o LPC: The ablation experimental results verify the effectiveness of prototype-wise contrastive learning. Detailed visualization will be discussed in next section.\nw/o LG: We conduct an ablation study on both datasets without the guidance loss LG, leading to a significant drop in performance. The \"imbalanced experts\" phenomenon will be discussed in more detail in a subsequent section.\nw/ fixed ag: We initialize the previous hypothesis ag as (0.2, 0.2, 0.2, 0.4) and do not use the score generated by the structure perceptron. F1-macro on YelpChi and Amazon has dropped by 0.16% and 0.24% respectively.\nw/o LRM: The ablation experimental results verify the effectiveness of regularized masking for MoE, and the mask ratio experiment will be discussed in subsequent section."}, {"title": "Visualizati", "content": "We visualize node features after FCF using various training methods, using the YelpChi dataset as an example. We employ the t-SNE(Van der Maaten and Hinton 2008) to map the vector representation into the 2-dimensional space. Due to the extreme class imbalance, we undersample the benign class to keep the number of both classes close for convenient visualization. As shown in Figure 3, we can observe that the original node representation distribution is messy. Without contrastive learning, the training method shows observable clustering in node representations, but fails to distinguish fraud nodes from benign nodes. Instance-wise contrastive learning can significantly improve this phenomenon. Compared with IC, prototype-wise contrastive learning makes"}, {"title": "the validity of our proposed regularized masking method.\nConclusion", "content": "In this paper, we proposed a novel approach to detect fraudsters in the multi-relational graph setting with counteracting camouflage behaviors. In particular, we use label propagation to generate pseudo-labels, which are combined with contrastive learning to obtain camouflage-removed features. To address the relation camouflage, we use Mixture-of-Experts and regularized masking to enhance model robustness. Comprehensive experiments show our proposed method significantly outperforms the state-of-the-art on two public fraud detection datasets. In future work, we plan to explore removing both types of camouflage in an iterative manner, which we believe could achieve more refined de-camouflage effects."}]}