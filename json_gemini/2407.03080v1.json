{"title": "ARTIFICIAL INDUCTIVE BIAS FOR SYNTHETIC TABULAR DATA GENERATION IN DATA-SCARCE SCENARIOS", "authors": ["Patricia A. Apell\u00e1niz", "Ana Jim\u00e9nez", "Borja Arroyo Galende", "Juan Parras", "Santiago Zazo"], "abstract": "While synthetic tabular data generation using Deep Generative Models (DGMs) offers a compelling solution to data scarcity and privacy concerns, their effectiveness relies on substantial training data, often unavailable in real-world applications. This paper addresses this challenge by proposing a novel methodology for generating realistic and reliable synthetic tabular data with DGMs in limited real-data environments. Our approach proposes several ways to generate an artificial inductive bias in a DGM through transfer learning and meta-learning techniques. We explore and compare four different methods within this framework, demonstrating that transfer learning strategies like pre-training and model averaging outperform meta-learning approaches, like Model-Agnostic Meta-Learning, and Domain Randomized Search. We validate our approach using two state-of-the-art DGMs, namely, a Variational Autoencoder and a Generative Adversarial Network, to show that our artificial inductive bias fuels superior synthetic data quality, as measured by Jensen-Shannon divergence, achieving relative gains of up to 50% when using our proposed approach. This methodology has broad applicability in various DGMs and machine learning tasks, particularly in areas like healthcare and finance, where data scarcity is often a critical issue.", "sections": [{"title": "1 Introduction", "content": "Recent advances in deep learning have led to the development of powerful Deep Generative Models (DGMs). These models excel at learning and representing complex, high-dimensional distributions, allowing them to sample new data points realistically. This capability has driven remarkable progress in various domains, including image generation [1], text generation [2], and video generation [3]."}, {"title": "2 Methodology: Generating Artificial Inductive Bias", "content": "Let us assume that we have a tabular dataset composed of N entries {x}\\_i\\^{N}\\_1. N represents the number of samples available and each entry x has a dimensionality of C features. In other words, C represents the number of attributes associated with each data point. Let us also define a DGM as a high-dimensional probability distribution p\\_\\theta, where \\theta represents the learnable parameters of the model. The objective of the DGM is to learn a representation, p\\_\\theta, that closely approximates the true underlying data distribution, denoted by p(x\\_r). Once trained, the DGM can generate new synthetic generated samples x\\_g, by drawing from its learned distribution:\nx\\_g ~ P\\_\\theta.\nIdeally, a well-trained DGM should produce synthetic data x\\_g that are statistically indistinguishable from real data x\\_r. In the prevalent big data setting, characterized by a large number of training samples (N >> C), DGMs with sufficient complexity can effectively capture the underlying data distribution p(x\\_r). This is evidenced by the impressive results achieved in recent research, where high-dimensional synthetic samples are generated using vast amounts of training data [1-3,7]. However, for scenarios with limited training data, which is common in tabular domains, DGMs struggle to accurately represent the complex inter-feature relationships. Consequently, the synthetic samples generated x\\_g deviate significantly from the true data distribution p(x\\_r), leading to high KL and JS divergences between real and synthetic data.\nTo address this challenge, we propose an approach that uses artificially generated inductive biases. Figure 1 illustrates the overall architecture. In the standard big data setting, a DGM p\\_\\theta is directly trained using real data x\\_r generating high-quality synthetic data X\\_g ~ p\\_\\theta. However, when the number of real samples N is limited, the quality of the generated data x\\_g deteriorates. To mitigate this issue, we introduce an artificial inductive bias generator. This module takes the initial synthetic data x\\_g as input and outputs an initial set of weights \\theta\\_0. These weights are then used as the inductive bias to train a second DGM p\\^{}\\_{\\theta} using real data x\\_r. This second DGM generates a new set of synthetic samples, \\^{x\\_g}. Notably, the only distinction between p\\_\\theta and p\\^{}\\_{\\theta} lies in the initial weights: p\\^{}\\_{\\theta} leverages the inductive bias encoded in \\theta\\_0 to potentially achieve faster convergence to a distribution that better resembles p(x\\_r), while p\\_\\theta begins training with random weights. As our simulations will demonstrate, this seemingly minor difference translates into significant improvements in the quality of the generated synthetic data.\nThe proposed approach hinges on two key concepts: the importance of inductive biases and the feasibility of their artificial generation. The importance of inductive biases in supervised learning is well established. The no-free-lunch theorems state that a universally optimal learner does not exist. Consequently, specific learning biases can lead to substantial performance gains for particular problem domains (see [27] and the references therein). Convolutional Neural Networks (CNNs) exemplify this principle. Their inherent inductive bias, the fact that the image information possesses spatial correlation, makes them the preferred architecture for image processing tasks. Similarly, as highlighted in [26], the use of inductive biases is a cornerstone of Deep Learning's success. In scenarios with limited training data, regularizers are commonly employed as inductive biases to prevent overfitting. This underscores the dual role of inductive biases: not only do they contribute to Deep Learning's effectiveness, but they are also crucial in preventing overfitting. However, effective use of inductive biases is often contingent on having specific knowledge about the problem at hand. In the aforementioned example of CNNs, we inherently understand the existence of spatial correlation in images. However, in tabular data, this domain-specific knowledge is often scarce. To address this challenge, recent efforts have focused on designing large models trained on artificially generated data as inductive biases. The underlying"}, {"title": "2.1 Transfer learning", "content": "Transfer learning is a machine learning paradigm that leverages knowledge acquired from a context domain (also called the source domain) to enhance learning performance in a new target domain [30]. This approach aims to improve the learning process in the target domain by capitalizing on the knowledge gained from solving related tasks in the context domain. This technique has demonstrated its efficacy in fields where data scarcity is a common challenge, such as the medical field [31].\nFormally, based on the definition in [30], we can define a domain D by a feature space X and a marginal probability distribution p(x\\_r). Two domains are considered distinct if their feature spaces X\\_1, X\\_2 or marginal probability distributions p(x\\_1), p(x\\_2) differ, i.e., if X\\_1 \u2260 X\\_2 or p(x\\_1) \u2260 p(x\\_2). The core objective of transfer learning is to leverage"}, {"title": "2.1.1 Pre-training", "content": "Pre-training is a frequently adopted strategy for introducing an inductive bias into a model. By leveraging a pre-trained model on a context domain, the target model gains generalizable features that enhance its performance on a target domain. However, while pre-training is a standard in computer vision and natural language processing, achieving similar success with tabular data remains a challenge. This disparity arises from the inherent heterogeneity of the features of the tables, which creates substantial feature space shifts between pre-training and downstream datasets, hindering effective knowledge transfer. Despite these challenges, recent efforts like [37] and [38] explore tabular transfer learning with promising results. Although these studies demonstrate potential, achieving comprehensive parameter transfer in tabular data requires further research to establish best practices and unlock the full potential of pre-training in this domain.\nIn this work, pre-training involves the following steps. First, we train a separate DGM p\\^{}\\_{\\theta\\_pt} using synthetic data x\\_g as training data. Since x\\_g is sampled from the initial DGM, x\\_g ~ p\\_\\theta, we can generate a vast amount of synthetic data."}, {"title": "2.1.2 Model Averaging", "content": "While pre-training can be incorporated with any DGM, our approach focuses on models where the training process is sensitive to initial conditions, such as VAEs. In such cases, it is common to train the DGM p\\_\\theta with multiple initial conditions (seeds) and potentially discard \"bad\" seeds based on a specific metric. We propose using these discarded seeds to create an artificial inductive bias. The simplest implementation involves averaging the model parameters. In this case, our context domains are the different results of each seed, and the target domain is obtained by averaging across the context domains. If we train S different seeds for p\\_\\theta, resulting in S models with parameters \\theta\\_s, we propose using the average of these weights as the inductive bias:\n\\theta\\_0 = \\frac{1}{S} \\sum\\_{s=1}\\^{S} \\theta\\_s\nThis straightforward approach is computationally efficient, requiring only the calculation of the average across the precomputed weights. It assumes that the average model may capture a robust inductive bias, leading to improved performance. Figure 3 summarizes this process."}, {"title": "2.2 Meta-learning", "content": "Traditional machine learning models often rely on large volumes of data to achieve optimal performance in specific tasks. In contrast, meta-learning introduces a distinct paradigm by training algorithms with the ability to \u201clearn to learn\" [41], enabling them to rapidly adapt to new tasks with minimal data. This departure from the conventional requirement of extensive datasets for each new task allows meta-learning algorithms to leverage knowledge gained from addressing numerous related tasks. Through introspective analysis of past experiences, these models dynamically adjust their learning strategies when confronted with novel situations, making them more efficient learners and requiring less data to perform well on tasks with similar characteristics.\nIn this work, we exploit the multi-seed training configuration of certain DGMs. By treating each of the S different seeds obtained after training the DGM as a distinct task, we construct a meta-learning framework."}, {"title": "2.2.1 MAML", "content": "MAML is a prevalent approach within the field of meta-learning [42]. It identifies the initial set of weights denoted by \\theta\\_{MAML} by leveraging various tasks, enabling rapid and data-efficient adaptation to new tasks. This efficiency comes from the ability to fine-tune \\theta\\_{MAML} with minimal data for each new task. However, successful application of MAML requires access to a diverse collection of tasks for effective learning.\nFormally, we can frame the problem by starting with a common single-task learning scenario and transforming it into the meta-learning framework. Consider a task T that consists of an input x sampled from a probability distribution D. For simplicity, we define a task instance T as a tuple comprising a dataset D and its corresponding loss function L. To solve the task T we need to obtain an optimal model parameterized by a task-specific parameter w\\*, which minimizes a loss function L on the data of the task as follows:\nw\\* = arg min E\\_{x~D} [L(D;w)].\nIn single-task learning, hyperparameter optimization is achieved by splitting the dataset D into two disjoint subsets D = D^(t) \u222a D^(v), which are the training and validation sets, respectively. The meta-learning setting aims to develop a general-purpose learning algorithm that excels across a distribution of tasks represented by p(T) [43]. The objective is to use training tasks to train a meta-learning model \\theta\\_{MAML} that can be fine-tuned to obtain w to perform well on unseen tasks sampled from the same task environment p(T). Meta-learning methods utilize meta-parameters to model the common latent structure of the task distribution p(T). Therefore, we consider meta-learning as an extension of hyperparameter optimization, where the hyperparameter of interest \u2013 often called a meta-parameter \u2013 is shared across many tasks.\nIn this work, the distribution of tasks is defined by the set of S training seeds obtained after training the DGM. Given a set of S training seeds following p(T), each task T ~ p(T) is therefore formalized as T = {D, L}. Here, each"}, {"title": "2.2.2 DRS", "content": "Although MAML offers the potential to leverage the underlying structure of learning problems through a powerful optimization framework, it introduces a significant computational cost. Therefore, while we should seek for a trade-off between accuracy and computational efficiency, there is no approach for managing this trade-off. It needs an understanding of the domain-specific characteristics inherent to the meta-problem itself.\nDRS presents an alternative meta-learning approach that circumvents the computational burden associated with bilevel optimization problems. Unlike MAML, DRS trains a model on the combined data from all tasks. This eliminates the need for the complex optimization procedures present in MAML, leading to a more computationally efficient solution. However, it is important to acknowledge that DRS offers an approximation to the ideal solution [49].\nFormally, DRS focuses on the meta-information, denoted by \\theta\\_{meta}, as the initialization of an iterative optimizer used in a new meta-testing task, T\\_s. In this context of meta-learning initialization, a straightforward alternative involves solving the following pseudo-meta problem:\n\u03b8\\_{DRS} = arg min\\_{\\omega} E\\_{Ts~p(T)} [L(D\\*; \u03c9)]."}, {"title": "3 Experiments", "content": "The experiments were carried out on four public datasets obtained from the SDV environment [51], which also implements various data generation models, including the CTGAN implementation we use. The experiment design prioritized datasets with a sufficient number of samples. This allows us to create multiple data splits for various configurations of the different training and validation parameters. This approach comprehensively evaluates the proposed method under different parameter settings.\n\u2022 Adult: The Adult Census Income dataset [52] is a mixed-data dataset extracted from the 1994 U.S. Census. It comprises 32, 561 data points, each described by 14 features that encompass integer, categorical, and binary values. The dataset is used to predict whether an individual's annual income exceeds $50,000. It should be noted that the data set incorporates 13% missing values, concentrated exclusively within two specific variables: \u201cworkclass\" and \u201coccupation\u201d.\n\u2022 News: The News Popularity Prediction dataset [53] consists of 39,644 samples and 58 columns containing information about articles published on the Mashable news blog over two years. The objective is to predict the popularity of an article, measured by the number of social media shares. The dataset is multivariate, including both continuous and categorical variables."}, {"title": "3.2 Validation metrics", "content": "To rigorously evaluate the effectiveness of our proposed method in capturing the real data distribution, we strictly adhere to the validation approach described in [20]. This approach leverages a probabilistic classifier (discriminator) to estimate the ratio of probability densities between the real and synthetic distributions, subsequently calculating the KL and JS divergences. Traditional validation methods often focus on individual data points and the marginal distribution of each separate feature. In contrast, this approach considers the entire data distribution, including complex relationships between features. Additionally, divergences are robust to noise and offer clear interpretations, making them ideal for evaluating the DGM's effectiveness. This provides a comprehensive approach to measuring the discrepancy between two probability distributions, making them suitable for assessing the similarity between real data p(x\\_r) and the distribution of the synthetic data generated by the DGM p\\_\\theta.\nThe discriminator network plays a crucial role in the validation process. This neural network architecture is trained to distinguish between real and synthetic data samples. The network receives two sets of samples as input. The first consists of M samples from the real data distribution p(x\\_r) labeled as class 1, and the second consists of M samples labeled as class 0 from the synthetic data distribution generated by the DGM p\\_\\theta or p\\^{}\\_{\\theta}, depending on the N number of samples in the dataset. During training, the discriminator aims to learn a decision boundary that effectively separates these two sets of samples. This process forces the discriminator to capture the underlying differences between the real and synthetic distributions. Once the discriminator network is trained, it is used to estimate the KL and JS divergences between the real and synthetic probability distributions. This estimation involves using L samples from each distribution and feeding them to the trained discriminator. The output probabilities of the discriminator for these samples are then used to compute the KL and JS divergence metrics."}, {"title": "3.3 Experimental design", "content": "In terms of the experimental design to evaluate the proposed method, and as described in the methodology section, we use the state-of-the-art VAE architecture [15] to train 10 different seeds. Subsequently, we apply methodologies based on transfer learning and meta-learning. For pre-training, we also include results using another state-of-the-art model, CTGAN. It is important to note that while we maintain the default parameters for CTGAN, we adjust the dimensionality of the latent space in the VAE depending on the dataset. This allows the VAE to capture the specific characteristics of each dataset more effectively. Particularly, we use a latent space dimension of 10 for the Adult and Intrusion datasets, 20 for the News dataset, and 15 for the King dataset. We maintain a consistent hidden size of 256 neurons for all VAE models. Regarding the configuration of parameters M and L, we define two different validation configurations for each methodology: a reliable case and a more realistic case. The parameter N remains unchanged, as it reflects the actual number of samples available to train the DGM, which is beyond our control. However, we can vary the number of generated samples for validation, i.e., M and L. Specifically, the results presented for each dataset are as follows."}, {"title": "3.4 Results", "content": "In this section, we present the results obtained from the experiments, focusing on scenarios that promote the validation of reliable synthetic data, characterized by higher values of M and L. The results for scenarios with M = 100 and L = 100, considered unreliable due to their low information content, are presented in the Appendix for comparison purposes. For each database, we present a table summarizing the scenarios defined previously and their respective KL and JS divergence values. The results for each metric are displayed in the following format: mean (std) (lower is better). The code to replicate our results, along with the data used, can be found in our repository.\nTable 1 shows the validation results in terms of divergence obtained for the Adult dataset. We focus primarily on the JS divergence due to its interpretability as a bounded metric (ranging from 0 to 1). The table shows the upper and lower bounds used to assess the efficacy of the proposed methodology in the reliable case of higher validation samples (M = 7500 and L = 1000). These bounds are 0.079 (upper) and 0.331 (lower), highlighting a significant gap and room for improvement in the base VAE model (without any techniques applied). Examining the JS divergence results for the"}, {"title": "4 Conclusions", "content": "This research proposed a novel approach to generate synthetic tabular data using DGMs in the context of limited datasets. Our approach leverages four distinct techniques to artificially introduce an inductive bias that guides the DGM towards generating more realistic and informative synthetic data samples. These techniques encompass two transfer learning approaches: pre-training and model averaging; and two meta-learning approaches: MAML and DRS. To facilitate the application of model averaging, MAML, and DRS, we employ the VAE model from [15] and train multiple instances with different random seeds. This allows us to leverage the ensemble properties of the VAE models for techniques like model averaging and further enables the application of meta-learning algorithms like MAML and DRS. We also used the CTGAN [7] to assess pre-training so that we can compare other architectures of well-known models for synthetic tabular data generation. We used divergence metrics, in particular JS and KL divergences, to compare the real and synthetic data distributions generated. The experimental results consistently demonstrate the effectiveness of our proposed approach in generating high-quality synthetic tabular data, particularly when using transfer learning techniques. These techniques significantly improve the divergence metrics, indicating a closer resemblance between the synthetic and real data distributions. Our approach offers several advantages over existing methods. Firstly, it effectively addresses the challenge of generating realistic synthetic data from small datasets, a common limitation in many real-world applications. Secondly, the use of transfer learning and meta-learning techniques enhances the inductive bias of the DGM, leading to more meaningful and informative synthetic data samples. However, it is also important to acknowledge the trade-offs associated with our methodology. Training VAEs with these techniques requires training multiple VAE models with different random seeds. This can lead to a significant increase in computational cost compared to simpler DGM training methods. While divergence metrics provide a valuable measure of distributional similarity, their ability to reliably assess the improvement in synthetic data quality for specific downstream tasks can be limited, especially with small datasets, as detailed in the Appendix. Nonetheless, our methodology may provide significant gains in JS divergence, of up to 50% according to our experimental results.\nIn conclusion, our proposed approach provides a promising solution for generating high-quality synthetic tabular data from small datasets, particularly when VAEs are employed to apply transfer learning techniques. We believe that this work has the potential to make a significant contribution to the field of synthetic data generation and machine learning applications that rely on small datasets. However, there are several research lines to be addressed. While the current study focuses on VAEs and GANs, investigating the applicability of our framework to other DGM architectures could provide valuable insights. In addition, our current approach does not explicitly incorporate domain knowledge. Future research could explore mechanisms to integrate domain-specific information from an expert into the inductive bias generation process, potentially leading to even more realistic and informative synthetic data. Lastly, although divergence metrics offer a valuable measure of distributional similarity, exploring additional evaluation techniques that assess the quality and usefulness of synthetic data for specific downstream tasks would provide a more comprehensive"}, {"title": "Appendix A Appendix", "content": "To further investigate the impact of sample size on divergence metric performance, we conducted an additional validation experiment using a reduced number of samples for both M and L. Specifically, we set M = 100 and L = 100, representing a scenario with limited data availability. As noted in [20], having enough samples is crucial for accurate distribution comparisons. Therefore, we anticipated that this low sample count scenario would yield less reliable divergence results.\nThe results obtained for this particular validation are presented in Tables 6, 7, 8 and 9. The improvements observed in the previous experiments are not consistent in all datasets under these constrained conditions. Moreover, the divergence values are generally quite small. This suggests that when sample counts are low, the underlying distributions may not be adequately captured, leading to an underestimation of the true disparity between the distributions. Consequently, the divergence may underestimate the actual divergence, resulting in seemingly small divergence values. These findings underscore the importance of having an adequate number of samples when evaluating divergence metrics. With limited data, the reliability of these measures decreases, which can lead to misleading conclusions about similarity between distributions. Therefore, it is essential to consider sample size as a factor when interpreting divergence metric results, particularly in scenarios with limited data availability."}]}