{"title": "AGENTSOCIETY CHALLENGE: DESIGNING LLM AGENTS FOR\nUSER MODELING AND RECOMMENDATION ON WEB PLATFORMS", "authors": ["Yuwei Yan", "Yu Shang", "Qingbin Zeng", "Yu Li", "Keyu Zhao", "Zhiheng Zheng", "Xuefei Ning", "Tianji Wu", "Shengen Yan", "Yu Wang", "Fengli Xu", "Yong Li"], "abstract": "The AgentSociety Challenge is the first competition in the Web Conference that aims to explore\nthe potential of Large Language Model (LLM) agents in modeling user behavior and enhancing\nrecommender systems on web platforms. The Challenge consists of two tracks: the User Modeling\nTrack and the Recommendation Track. Participants are tasked to utilize a combined dataset from Yelp,\nAmazon, and Goodreads, along with an interactive environment simulator, to develop innovative LLM\nagents. The Challenge has attracted 295 teams across the globe and received over 1,400 submissions\nin total over the course of 37 official competition days. The participants have achieved 21.9% and\n20.3% performance improvement for Track 1 and Track 2 in the Development Phase, and 9.1%\nand 15.9% in the Final Phase, representing a significant accomplishment. This paper discusses the\ndetailed designs of the Challenge, analyzes the outcomes, and highlights the most successful LLM\nagent designs. To support further research and development, we have open-sourced the benchmark\nenvironment at https://tsinghua-fib-lab.github.io/AgentSocietyChallenge.", "sections": [{"title": "1 Description", "content": "With the rapid advancement of web technologies, human social interactions are increasingly intertwined between the\nphysical world and cyberspace. The web has evolved from a mere information exchange platform to a dynamic medium\nfor modeling and influencing human behaviors. Web platforms, in particular, provide rich data capturing how users\naccess information, make decisions, and interact with content and other users. These data offer invaluable insights\ninto human dynamics, especially in understanding user intent and optimizing information retrieval (IR) systems for\npersonalized online service. In parallel, large language models (LLMs) have demonstrated exceptional capabilities in\nreasoning and prediction tasks [1, 2, 3], which are now widely applied in user behavior modeling and recommendation\nsystems [4, 5]. LLM-based agents [6, 7] further extend these capabilities by effectively simulating complex, generative\nhuman behaviors [8, 9, 10, 11]. This advancement has profound implications for behavior modeling, paving a new way\nto model user preferences, engagement patterns, and decision-making processes.\nLLM agents possess unique strengths that extend beyond traditional deep learning models. One key advantage is their\ncapability for commonsense reasoning [1, 2, 12, 13], allowing them to make few-shot predictions based on minimal\ndata and effectively addressing data-sparse tasks such as recommendations for users with limited historical records.\nAdditionally, their proficiency in zero-shot role-playing [8, 14] enables them to simulate user behaviors in diverse\ncontexts and adapt to new scenarios with in-context information. This makes them particularly valuable for tasks such\nas simulating context-sensitive and personalized user intent and behaviors. However, there's still a critical challenge in\nbridging these technological advancements with practical, useful tools for information retrieval and recommendation\nsystems. The AgentSociety Challenge is proposed to address this gap by asking participants to leverage the simulation\nand decision-making power of LLM agents to tackle two fundamental IR challenges: simulating user behaviors and\ndeveloping personalized recommendation systems. The challenge consists of two parallel tracks:\nUser Modeling Track. In this track, participants are tasked to design agents to simulate user reviews and star ratings,\nfocusing on simulating user behavior when facing specific items by leveraging their historical actions and accessible\nenvironmental data.\nRecommendation Track. In this track, participants will develop LLM agents that act as a recommendation assistant [14,\n10]. The agent should provide tailored recommendations for users based on historical interactions and retrieved\ninformation.\nAgentSociety Challenge aligns closely with the core themes of the Web Conference by addressing fundamental problems\nin web technologies and user-centered Computing. The competition draws the participation of 295 teams and receives\nover 1,400 submissions over the course of 37 official competition days. Rooted in areas like information retrieval,\nrecommendation systems, and user modeling, this challenge leverages cutting-edge LLM agents to simulate and predict\ncomplex human behaviors in dynamic web-enabled environments."}, {"title": "2 Dataset and Evaluation", "content": "The proposed challenge utilizes three open-source datasets, including Yelp, Amazon, and Goodreads, to evaluate agents.\nThese datasets were carefully chosen for their relevance and diversity in capturing real-world user interactions, making\nthem ideal for building LLM-based agents for user modeling and recommendation.\n\u2022 Yelp Dataset: This dataset [15] includes millions of user reviews, business information, and user feedback\nsuch as ratings and engagement metrics.\n\u2022 Amazon Dataset: This dataset [16] contains millions of reviews for a wide range of products, offering insight\ninto user preferences in the e-commerce context.\n\u2022 Goodreads Dataset: This dataset [17] includes user ratings, reviews, and metadata for books, capturing user\npreferences and interactions in the book community.\nOne key capability of a Web Agent is its ability to acquire information and data from interactive web platforms. To\nadequately assess LLM agents, it is essential to furnish an environment that reflects these intricacies. This environment\nempowers agents to perform tasks such as user modeling and personalized recommendations within an interactive and\nrealistic setting.\nTo that end, we have built a simulator that controls all retrieval actions from LLM agents. The core of the simulator\nused in the challenge is the InteractionTool. This tool constructs an interactive environment comprising a network\nof users, reviews, and items, allowing agents to access historical data as needed. By leveraging these datasets, agents\ncan simulate user behaviors, including generating reviews and ratings and produce personalized recommendations\nbased on contextual information. This configuration enables a thorough evaluation of agent performance in tasks that\nclosely resemble real-world applications. The framework of the simulator is shown as Fig.1.\nUser Modeling Track. In this Track, the performance of participants' agents will be evaluated through quantitative\nmetrics, focusing on the accuracy of user behavior predictions.\nPreference Estimation: The preference estimation accuracy is evaluated through the mean absolute error (MAE) of the\npredicted star ratings compared to the actual ratings. The MAE is defined as:\n$MAE = \\frac{1}{N} \\sum_{i=1}^{N} |\\hat{s}_{ni} - s_{ni}|$"}, {"title": "3 Competition Outcome Analysis", "content": "There are 295 teams registered for the challenge. During the Development Phase, Track 1 received a total of 503\nsubmissions, while Track 2 received 405 submissions. After the Development Phase, the top 20 teams from each track\nadvanced to the Final Phase. During the Final Phase, Track 1 received 262 submissions, while Track 2 received 282\nsubmissions.\nFig. 2 presents the daily submission performance statistics for both tracks during the development and Final Phases.\nOverall, the performance of the submitted agents gradually improved over time. At the beginning of the challenge, we\nprovide an official agent for reference on both tracks. Early in the Development Phase, participants' designed agents are\nprimarily around the baseline. However, as the challenge progressed, several designs emerged that significantly surpassed\nthe performance of the baseline agent. Moreover, we have found that the designed agent-based recommendation methods\ngenerally outperform traditional deep-learning recommendation models (e.g., NCF [21]), highlighting their significant\npotential.\nHere we provide an analysis of the simulated groundtruth used for evaluation. First, we aim to ensure that the simulation\ngroundtruth can objectively assess agents' performance, thereby reducing the risk of overfitting to specific real-world\ntest conditions. Second, we validate that the simulated groundtruth accurately reflects actual user behaviors and\npreferences, demonstrating its potential to enhance traditional methods, such as deep learning-based models.\nCorrelation between simulated and real groundtruth for evaluation. We analyze the correlation between the\nperformance of submitted agents (179 agents on Track 1 and 196 agents on Track 2) during the Final Phase of our\ngenerated simulated and real test groundtruth. The results are shown in Fig. 3, from which it can be observed that\nthere's a strong correlation between the performance on simulated and real groundtruth, with a Pearson correlation\ncoefficient of 0.9739 for Track 1 and 0.9245 for Track 2. This suggests that the generated simulated groundtruth is\nreliable and meaningful, thus the agent performance on the simulated groundtruth can provide a strong prediction of\ntheir real-world performance.\nGeneralization ability of mixed and real groundtruth. We evaluate the agents' performance on the following three\ntypes of datasets: (1) a mixture of simulated and real groundtruth (referred to as \"Mixed Data\"), (2) a sample of real\ngroundtruth (referred to as \"Real Data A\"), and (3) another sample of real groundtruth (referred to as \"Real Data B\").\nWe then compare the agent performance correlation of Mixed Data and Real Data A against Real Data B to assess\ntheir generalization capability. The results on Track 1 and 2 are shown in Fig. 4 and 5, respectively. For Track 1, the\nPearson correlation coefficient between Mixed Data and Real Data B is 0.7516, while the correlation between Real\nData A and Real Data B is 0.7331. In Track 2, the correlation between Mixed Data and Real Data B is 0.7641, whereas\nthe correlation between Real Data A and Real Data B is lower, at 0.5825. The results suggest that the use of mixed\nsimulation-real groundtruth leads to better performance prediction of the agents' real-world capabilities compared to\npurely using real groundtruth. For instance, Agent 2 on Track 2 performs poorly on Real Data A but excels on Real Data\nB. Such a discrepancy can be effectively mitigated when using mixed groundtruth for evaluation. This finding suggests\nthat incorporating simulated groungtruth helps provide a more robust evaluation of the agents' real performance."}, {"title": "4 Case Study", "content": "This section analyzes the top three performing agents from the User Modeling Track (named \u201cASC\u201d, \u201cJiuWen\u201d, \"STDYW\"). Specifically:\nDesign Pattern. All agents follow a multi-stage retrieve-plan-generate pipeline. They employ contextual prompt\nengineering by combining user profiles, item attributes, and historical reviews using platform-specific templates.\nExtracted Features. Each agent employs a unique approach tailored to its objectives, with different strategies for\ngenerating ratings, memory handling, and prompt styles. ASC employs a collaborative filtering core strategy, integrating\na preference alignment engine and using MDILU with similarity search for memory, along with statistical mean\nadjustment for rating logic. In contrast, JiuWen uses an aspect-based analysis strategy, with an aspect extractor as\nits unique module and example-driven memory. Its rating logic is guided by examples, and it employs case-based\nexamples in its prompt style. STDYW, on the other hand, relies on direct LLM generation, with a sanitization module\nand basic DILU memory approach. Its rating logic is based purely on LLM output, and it uses concise instructions for\nits prompts.\nDifferent Competitive Advantages. The distinctive design choices made by each agent lead to notable competitive\nadvantages. ASC's integration of user and item mean ratings with variance is a sophisticated technique for improving\nrating consistency. By adjusting for the statistical variance of the data, ASC minimizes rating fluctuations, helping\nto achieve more stable and reliable user behavior predictions. JiuWen's design excels in its innovative approach to\nextracting aspects from business reviews. By breaking down reviews into specific aspects, such as service quality,\nproduct features, and customer experience, Jiu Wen can align user behaviors with fine-grained features. Another standout\nfeature is STDYW's combination of advanced user-and-item modeling and knowledge mining. By integrating these\ncomponents with sophisticated reasoning capabilities, STDYW enhances personalized and context-aware simulations,\noffering superior predictive accuracy and human-like review generation."}, {"title": "4.2 Top Agents in Recommendation", "content": "In this section, we analyze the designed agents of the top 3 teams on Track 2 (named \u201cbaseline666\u201d, \u201cRecHackers\u201d,\n\"DummyAgent\"), and summarize their key design insights as follows:\nAgentic workflow. All three agents relied on similar information to guide the ranking process, with slight variations\nin implementation. The primary elements for ranking include: (1) the user's historical review data, reflecting past\npreferences; (2) the list of candidate items to be ranked; and (3) detailed item information, helping assess the match\nbetween the items and the user's preferences. Specifically:\nItem-side feature engineering. The first primary innovation in these agents lies in item-side feature engineering.\nEspecially, the baseline666 team applied platform-specific feature extraction, ensuring robust and adaptive rankings\nacross different sources. For Amazon, they extracted features like item ID, name, stars, review count, and description,\namong others. For Yelp, the features were more focused, including item ID, name, stars, and review count. For\nGoodreads, a wider array of features was included, such as authors, publication year, and similar books.\nReview-side feature engineering. Another key point is the feature engineering of review data, involving filtering and\nselecting the most informative reviews to enhance both user and item descriptions. For instance, the DummyAgent\nteam adopted platform-specific strategies. For Yelp, they focused on attributes like \u201cfunny\u201d, \u201ccool\u201d, and \u201cuseful\", along\nwith the review text. For Amazon, they included the publication date and purchase verification. For Goodreads, they\nextracted additional metadata like review date, the number of votes or comments, and reading status, in addition to the\nreview text and rating.\nTo summarize, the key design elements can be categorized into three aspects: First, regarding the agent's workflow, a\nstandard and effective approach involves prompting LLMs with user historical reviews, candidate items, item details,\nand platform-specific information to generate rankings. Second, extracting platform-specific item attributes plays a\ncritical role in enhancing performance. Lastly, prioritizing the most relevant and informative reviews is crucial for\nboosting results."}, {"title": "5 Conclusion", "content": "The AgentSociety Challenge has not only fostered the design of innovative agent-based solutions but also led to\nsignificant improvements in simulating user behaviors and enhancing personalized recommendations. Through the\nchallenge, participants introduced diverse strategies that advanced the accuracy of user behavior simulations and\npersonalized recommendation quality. These outcomes highlight the potential of LLM-driven systems in real-world\nweb platforms and set the stage for continued innovation in user-centered computing."}]}