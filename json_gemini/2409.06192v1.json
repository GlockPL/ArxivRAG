{"title": "NOVI : Chatbot System for University Novice with BERT and LLMs", "authors": ["Yoonji Nam", "Gyeongcheol Shin", "Tae Woong Seo", "Sangji Lee", "JaeEun Im"], "abstract": "To mitigate the difficulties of university freshmen in adapting to university life, we developed NOVI, a chatbot system based on GPT-40. This system utilizes post and comment data from SKKU 'Everytime', a university com-munity site. Developed using LangChain, NOVI's perfor-mance has been evaluated with a BLEU score, Perplexity score, ROUGE-1 score, ROUGE-2 score, ROUGE-L score and METEOR score. This approach is not only limited to help university freshmen but also expected to help various people adapting to new environments with different data. This research explores the development and potential appli-cation of new educational technology tools, contributing to easier social adaptation for beginners and settling a foun-dation for future advancement in LLM studies.", "sections": [{"title": "1. Introduction", "content": "Recently, various enterprises and public institutions, as well as several universities in Korea, have intro-duced chatbots to provide information related to aca-demic administration and IT services. Examples in-clude Sungkyunkwan University's academic administration chatbot 'Kingobot,' Seoul University's IT service chat-bot 'Snubot,' Hanyang University's academic administra-tion chatbot 'Goongmoonyang,' Yonsei University's li-brary materials loan and purchase chatbot 'Tuksoori,' and Chung-Ang University's academic administration chatbot 'CHARLI' [1]. These on-campus chatbots can reduce the workload of staff by addressing frequently asked academic-related inquiries and provide students with necessary infor-mation at any time, thereby offering efficient and highly sat-isfactory services [5].\nHowever, university-provided chatbots are rule-based rather than AI-based, which presents a significant limita-tion. These rule-based chatbots struggle to handle queries that deviate from their predefined rules and scenarios [13]. For instance, Sungkyunkwan University's 'Kingobot' can-not respond to untrained information queries or only pro-vides information that includes similar keywords (see Fig-ure 1). As of now, chatbots cannot answer specific questions and only offer limited functionalities, posing a significant limitation in providing practical information for students' campus life.\nThe drawbacks of these chatbots are particularly criti-cal considering the high demand for practical information about universities, especially among freshmen. Freshmen face confusion and difficulties due to changes in learning methods, expanded areas of activity, and new interpersonal relationships upon entering university [7]. Various studies have shown that freshmen have significantly lower adapta-tion levels to school life compared to other academic years and experience more negative emotions such as depression and anxiety [6]. These negative emotions can persistently impact university life, leading to academic underperfor-mance and dropout [8], making it a priority for universities to provide information to help freshmen adapt.\nAdditionally, the MZ generation, often referred to as the"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Chatbot system for university", "content": "In recent years, there has been increasing interest in uti-lizing AI for enhancing various services in educational in-stitutions, particularly through the development of chatbot systems. Several studies have explored different aspects of implementing AI-driven chatbots to support university op-erations and services.\nNeupane et al. [14] introduced BARKPLUG V.2, a chat-bot system based on a Large Language Model (LLM) and built using Retrieval Augmented Generation (RAG) pipelines. This system is aimed at enhancing the user ex-perience and improving access to university information by providing interactive responses about academic depart-ments, programs, campus facilities, and student resources. The study shows impressive quantitative performance with a mean RAGAS score of 0.96 and high user satisfaction as per the System Usability Scale (SUS) surveys.\nKim, Jung, and Kang [9] presented a chatbot system for handling academic inquiries using natural language pro-cessing (NLP) and open-source software. This system was designed to alleviate the increased workload on university departments during the influx of inquiries from new stu-dents.\nHur et al. [5] developed a university guide chatbot sys-tem based on NLP, utilizing information collected from uni-versity regulations and guidebooks to create a dictionary-style database for responses. This system aimed to stream-line the process of providing information to students about university policies and campus life.\nPark et al. [16] focused on the development of an AI-based university information notification chatbot system. Their study, presented at the KIIT Conference, emphasized the importance of providing timely and accurate informa-tion to students through an intelligent chatbot interface.\nChoi, Cho, and Kim [3] designed a chatbot system to im-prove accessibility to university counseling services during the COVID-19 pandemic. This system aimed to facilitate easier access to support services for students, ensuring they receive timely assistance during challenging times.\nAn [1] explored strategies for constructing an effective chatbot system to guide university academic operations. The study examined various approaches to designing chat-bots that can efficiently handle academic inquiries and pro-vide reliable information to students and staff. This paper proposed a button, scenario, keyword, AI, and hybrid-based chatbot system structure, as well as an SNS-based AI chat-bot system structure, to optimize the interaction and func-tionality of academic chatbots.\nThese studies collectively highlight the diverse applica-tions of Al-driven chatbots in university settings, demon-strating their potential to improve service delivery, enhance"}, {"title": "2.2. Applications in Natural Language Processing (NLP)", "content": "Papineni et al. [15] introduced the BLEU metric, which evaluates the quality of machine translation by comparing the number of tokens in the generated sentence to the num-ber of tokens in the reference (correct) sentence. This metric has been widely adopted for its straightforward approach in assessing translation accuracy. Lavie and Agarwal [2] pro-posed the METEOR metric to address some of the BLEU metric's limitations. METEOR evaluates translations by calculating a score based on explicit word-to-word matches between the machine translation and the reference transla-tion, considering synonyms and stemming.\nIn the domain of text summarization, Lin [12] developed the ROUGE metric, which is used to evaluate the quality of automatic summaries by focusing on n-gram matching between the generated summary and reference summaries. ROUGE has become a standard evaluation metric for sum-marization tasks, providing insights into the coherence and coverage of the summaries produced by models.\nRecent advancements in spam detection have lever-aged transformer-based embeddings and ensemble learn-ing. Ghourabi and Alohaly [4] conducted a study com-paring various spam detection techniques, including tradi-tional methods like TF-IDF and word embeddings against modern transformer-based models such as BERT and GPT. Their findings indicate that transformer-based methods sig-nificantly outperform traditional techniques in classifying spam/ham emails. Similarly, AbdulNabi and Yaseen [17] explored the effectiveness of using BERT for both tokenizer and classifier roles in spam email detection, concluding that BERT-based approaches achieved top performance in this area.\nIn summary, the evolution of evaluation metrics for ma-chine translation and text summarization, alongside the adoption of advanced language models for spam detection, demonstrates significant progress in NLP research. These developments highlight the continuous improvements in model accuracy and efficiency, paving the way for more ro-bust and reliable AI applications."}, {"title": "3. Method", "content": "The overall flow of the system is as above(Figure 2). When a user asks a query, the query is received at the front end and the query is handed over to Flask. Flask delivers the query to RetrievalQA. Then, in the RetrievalQA step, information related to the query is found in the existing trusted knowledge base. The queries and information are then transferred to a pre-fine-tune model, which generates a corresponding answer. The answer is communicated to the user at the front end through Python Flask. In this case, the data for fine-tuning the model is preprocessed as fol-lows. First, we took Raw Data and did topic modeling to extract only data related to school and freshmen. Then, un-"}, {"title": "3.1. Data Collection", "content": "We collected data from three specific boards on Every-time:\nAsk Broly: The Broly board is where users can ask ques-tions related to academics, studies, career paths, and em-ployment to a user nicknamed Broly. This board was the most suitable for our project.\nFreshmen Board: The Freshmen Board is where new students can ask questions about school life, and seniors provide information through their answers.\nInsa Campus Board: Sungkyunkwan University has two campuses: the Humanities and Social Sciences Cam-pus and the Natural Sciences Campus. Since our project targeted freshmen on the Humanities and Social Sciences Campus, we chose only the Insa Campus Board"}, {"title": "3.2. Data Collection Methods", "content": "To collect data, we utilized web crawling techniques on the Everytime community platform, employing the Sele-nium library. The crawling process was as follows: Crawler Setup: Installed ChromeDriver with webdriver_manager and used Selenium's webdriver.Chrome for browser au-tomation.\nLogin: Since Everytime restricts post access to logged-in users, the crawler was programmed to log in automati-cally by entering the user ID and password into the login form and submitting it.\nCollecting Post Lists: The crawler navigated through the pages of each board to collect lists of posts. We set a range of page numbers, accessed each page, and extracted the URLs of the posts.\nCollecting Detailed Post Information: The crawler vis-ited each extracted post URL to retrieve detailed informa-"}, {"title": "3.3. Data Filtering", "content": "The data filtering process, illustrated in figure 3, was conducted as follows:\na. Initial Data Cleaning: During the data integration phase, we removed entries that contained only partial in-formation, such as names with only initials (e.g., \"Prof. KJK\" rather than \"Prof. Jaekwang Kim\"), and handled missing values. Entries with only initials were excluded because they make it difficult to accurately identify the spe-cific course being referred to.\nb. Topic Modeling: The discussion boards included questions and answers unrelated to school life, such as those about politics, society, and private matters. We applied topic modeling techniques to filter out questions not related to academics. This process reduced the total dataset to 18,356 academic-related entries and 5,565 living-related entries.\nc. Manual Labeling: Researchers manually read the questions and labeled them in binary terms to determine whether the answers provided useful information. Due to limitations in human resources, only 12,411 entries were labeled, resulting in 5,128 useful (1) and 7,283 not useful (0) labels."}, {"title": "3.4. Novel Data Filtering Methodology", "content": "After manually labeling the data to determine whether the questions were likely to provide useful information post-topic modeling, we identified inefficiencies in the ex-isting manual labeling process. To address these inefficien-cies, we devised a new data filtering methodology inspired by binary classification tasks used in spam/ham email filter-ing [4]."}, {"title": "3.4.1 Tokenization and Embedding", "content": "We utilized pretrained transformer models, such as GPT and BERT, for tokenizing and embedding the question data. These models were chosen because previous research demonstrated their superior performance.\nKlue-BERT-base: KLUE BERT base is a pre-trained BERT model specifically designed for the Korean language. The developers created KLUE BERT base as part of the Ko-rean Language Understanding Evaluation (KLUE) Bench-mark initiative to enhance the performance of natural lan-guage processing tasks in Korean.\nGPT: GPT-ADA v2 is a state-of-the-art language model developed by OpenAI, designed for efficient and versatile natural language understanding and generation. It excels in various applications such as text completion, summariza-tion, translation, and conversational AI. With improved ac-curacy and response time, GPT-ADA v2 is a powerful tool for developers and businesses."}, {"title": "3.4.2 Classification Models", "content": "Once the data was embedded using BERT and GPT, we employed several classification models, including Support Vector Machines (SVM), LightGBM, and Convolutional Neural Networks (CNN), to perform the classification tasks. We chose the three models because they demonstrated the highest performance in previous studies."}, {"title": "3.4.3 Model Comparison", "content": "We compared the performance of the three best-performing models to classify the dataset, which consisted of 5,128 use-ful (1) and 7,283 not useful (0) labeled entries.\nBy implementing this methodology, future manual label-ing tasks can be significantly streamlined, as this approach provides a clear framework to follow, enhancing both the efficiency and effectiveness of the filtering process."}, {"title": "3.5. LLM", "content": ""}, {"title": "3.5.1 Introduction of LLM", "content": "Large Language Models (LLMs) are newly emerging tools in natural language processing (NLP) that perform func-tions such as understanding human language and generating corresponding responses by training on large-scale datasets. These models are designed based on the Transformer ar-chitecture and consist of trillions of parameters. Currently, prominent models include GPT-4 and Llama3. In this study, we used one of the LLMs, the GPT-40 model, for training and implemented the framework using Langchain. This en-abled us to develop a system capable of generating accurate and efficient responses to natural language questions."}, {"title": "3.5.2 Components of Langchain", "content": "Langchain is an open-source framework that facilitates the efficient construction of LLM-based applications. Langchain provides various components to easily build and manage natural language processing systems. The follow-ing are descriptions of each tool:"}, {"title": "4. Results", "content": ""}, {"title": "4.1. Performance Evaluation Metrics and Test Case Creation", "content": "In this study, the performance of the LLM was evaluated using the following metrics: The evaluation metrics used were BLEU, Perplexity, ROUGE, and METEOR."}, {"title": "4.1.1 BLEU (Bilingual Evaluation Understudy)", "content": "Features: Used to evaluate the quality of machine trans-lation. Measures the lexical similarity between the refer-ence translation and the generated translation. Here, BP is the brevity penalty, a is the n-gram weight, and Pn is the n-gram precision. BLEU score is an indicator mainly used to evaluate the quality of machine translation. It deter-mines the suitability of sentences generated by generative language models by measuring the degree of agreement of words or phrases between machine translation results and human translation results. BLEU's equation is as follows:\nBLEU = BP \\exp(\\sum_{n=1}^{N} a_n log P_n) \\qquad(1)"}, {"title": "4.1.2 ROUGE (Recall-Oriented Understudy for Gist-ing Evaluation)", "content": "Features: Primarily used to evaluate the quality of text summarization. Measures the overlap between the ref-erence summary and the generated summary. Here, Countmatch is the number of matching n-grams between the reference summary and the generated summary, and Count is the number of n-grams in the reference summary.\nThe ROUGE score is the Recall-Oriented Understudy for Gisting Evaluation, which is an indicator of the per-formance evaluation of the text summarization model. ROUGE mainly evaluates task performance such as sum-marization and machine translation, and calculates scores by comparing summarized or translated sentences gener-ated by models with human-made references. As the name suggests, ROUGE calculates scores using Recall and Pre-cision. In general, it can be viewed as a metric evaluated based on n-gram recall. The equation of ROUGE-N for N-gram is as follows:\nROUGE-N = \\frac{\\sum_{s \\in Ref Summaries} \\sum_{gram_n \\in s} Count_{match}(gram_n)}{\\sum_{s \\in Ref Summaries} \\sum_{gram_n \\in s} Count(gram_n)} \\qquad(2)"}, {"title": "4.1.3 Perplexity", "content": "Perplexity = exp(-\\frac{1}{N}\\sum_{i=1}^{N}log P(w_i | w_{<i})) \\qquad(3)"}, {"title": "5. Conclusion", "content": "In this study, we developed the first chatbot for univer-sity freshmen, which utilizes community data to provide information that is more relevant to real life than the offi-cial materials provided by the school. This is expected to greatly help the social adaptation of new generations, es-pecially those who prefer message-based communication, such as the Generation Z.\nDuring the data collection process of this study, we iden-tified major challenges, particularly the inability to distin-guish between comments and replies, which prevented us from including additional questions in the question portion of the dataset. We decided to remove questions with ques-tion marks from comments, but this may have resulted in a loss of useful information. Future research should discuss how to handle the inclusion of additional questions in com-ments.\nIn our performance evaluation, we utilized metrics such as BLEU and ROUGE that have been used in previous stud-ies, but we found that these metrics are not optimized for chatbot evaluation. Testing with human evaluators would have been ideal, but due to copyright issues and resource limitations, we were unable to do so. In addition, we be-lieve that the difference between the length of the answers in the test cases and the length of the answers generated by the model may have contributed to the poor performance. In the future, we may consider utilizing test cases with answers of different lengths or evaluating them through surveys. We plan to ask the 'Everytime' platform to deploy the chatbot and get feedback from real users.\nTo clarify the limitations and strengths of the study, we would like to organize the limitations and strengths in a ta-"}]}