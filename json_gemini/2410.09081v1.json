{"title": "Semantic Environment Atlas for Object-Goal Navigation", "authors": ["Nuri Kim", "Jeongho Park", "Mineui Hong", "Songhwai Oh"], "abstract": "In this paper, we introduce the Semantic Environment Atlas (SEA), a novel mapping approach designed to enhance visual navigation capabilities of embodied agents. The SEA utilizes semantic graph maps that intricately delineate the relationships between places and objects, thereby enriching the navigational context. These maps are constructed from image observations and capture visual landmarks as sparsely encoded nodes within the environment. The SEA integrates multiple semantic maps from various environments, retaining a memory of place-object relationships, which proves invaluable for tasks such as visual localization and navigation. We developed navigation frameworks that effectively leverage the SEA, and we evaluated these frameworks through visual localization and object-goal navigation tasks. Our SEA-based localization framework significantly outperforms existing methods, accurately identifying locations from single query images. Experimental results in Habitat [1] scenarios show that our method not only achieves a success rate of 39.0%-an improvement of 12.4% over the current state-of-the-art-but also maintains robustness under noisy odometry and actuation conditions, all while keeping computational costs low.", "sections": [{"title": "1. Introduction", "content": "Embodied AI technologies, which are becoming in- creasingly ubiquitous in modern life, are proving integral to various applications, including delivery robots, house- hold chore robots, and self-driving cars. The pivotal suc- cess factor in this field has been the development of intel- ligent agents that use RGB sensors to interpret semantic knowledge, particularly through learning-based methods such as reinforcement learning (RL) [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]. These methods, while powerful, introduce a significant challenge: high computational costs.\nAddressing this challenge, this paper introduces the Semantic Environment Atlas (SEA), a novel map type. The SEA is specifically designed to tackle visual localiza- tion and navigation tasks in a computationally efficient manner. The SEA sets itself apart with three distinctive characteristics that collectively enable successful visual navigation.\nThe first distinctive characteristic of the SEA is its ro- bust navigation performance against sensor noise. Sensor noise is a common problem in navigation tasks, which tends to accumulate during sequential decision-making processes. Traditional approaches have tried to mitigate this issue through loop closure, but such solutions are challenging for deep learning-based methods that lack state-space-based noise filtering. Consequently, current navigation methods [16, 21, 15, 24] often assume a noise- less pose sensor-an unrealistic premise in real-world scenarios. In contrast, our method leverages semantic knowledge, enabling it to navigate robustly even with noisy sensors.\nThe second distinctive property of the SEA is its abil- ity to localize the current position using semantic knowl- edge. This capability addresses a key challenge: pre- dicting an object's position with a partially observed map. While recent work [17, 25, 26] has integrated graph-based priors into the metric map to counter this issue, our method takes a step further by incorporating additional semantic knowledge, such as the relationships between objects and places, thereby bolstering localiza- tion performance.\nThe third and final property of the SEA is its adaptabil- ity. Unlike recent methods [15, 16] which do not update"}, {"title": "2. Related Work", "content": "Visual navigation without any map. As a policy net- work, a recurrent neural network (RNN) is a simple method to make an implicit semantic prior [28, 22, 20]. DDPPO [28] has a vanilla RL policy with a CNN back- bone followed by an LSTM as a policy function. Red- Rabbit [22] augments DDPPO with multiple auxiliary tasks, such as predicting agent dynamics, environment states, and map coverage with ObjectNav. Treasure Hunt Data Augmentation (THDA) [20] improves the RL re- ward and model inputs, which result in better generaliza- tion to new scenes. Since an RNN has a difficulty of back- propagating a long sequence, an RNN can be replaced with an explicit structure [15, 16, 21, 24, 17, 25, 23, 29].\nVisual navigation with a metric map. Spatial metric map-based RL methods [15, 16, 21, 24] propose inde- pendent modules for semantic mapping, high-level se- mantic exploration, and low-level navigation. The se- mantic exploration module is learned through RL, yet it is more sample-efficient and generalizes better than end-to-end RL. Active Neural SLAM (ANS) [15] has a hierarchical structure to explore an environment: global and local policies. The global policy constructs a top- down 2D map and estimates a global goal. Given the global goal from the global policy module, a local pol- icy module plans a path to the goal using a simple local navigation algorithm. Semantic exploration [16] is a study that extends ANS. The metric map does not only represent obstacles but draws a semantic map and uses it for navigation to improve performance. This method implicitly learns semantic information for navigation. PONI [21] reduced computational costs in visual naviga- tion by proposing non-interactive learning. Additionally, it improved the navigation performance by learning the encoder by calculating the probability that there is a space or an object beyond the frontier boundary of the current map and then moving to the boundary where the object is likely placed. However, since this method is trained using a top-down map, it is greatly affected by the pose sensor.\nVisual navigation with a graph map. Our work proposes a method to collect semantic priors and use it for nav- igation. Several works have employed semantic priors into a graph to enhance semantic reasoning in visual navigation [17, 25, 23]. Wu et al. [23] tackle the room navigation task using room relationship, while it does not consider the relationship between a room and an ob- ject. Zhang et al. [25] divide a room into several zones to find an object and find the reachability between these zones for navigation. However, the connection between an object and a zone is ambiguous. For example, a bed can exist in any zone in a bedroom. Campari et al. [17] improve performance by building an abstract model in addition to the existing metric map-based methods. Here, the abstract model comprises nodes composed of images and objects, and the connection between nodes is an action taken to navigate between two places. However, actions for moving from one place to another could differ depending on the structure of houses. For example, in one house, the bedroom may be to the right of the living room, and in another, the bedroom may be to the left. Therefore, the structure of the environment is hard to be expressed with the abstract model.\nThe proposed method collects relationships between place clusters and objects using a sequence of observa- tions and uses them in a new environment for navigation."}, {"title": "3. Proposed Method", "content": null}, {"title": "3.1. Problem Statement", "content": "In a given unknown environment, an agent is tasked with traveling to an object specified by its category name"}, {"title": "3.2. Semantic Graph Map", "content": "Inspired by the topological graph map approach out- lined in [18], we construct semantic graph maps, Et, for navigation in unknown environments, as depicted in Figure 2. At each time point t, the semantic graph map in- cludes three types of nodes-place nodes (Vplace), image nodes (Vim), and object nodes (Vob) and corresponding edges: Eim, Eio, and Epi. Each place node, represented as Pi, connects to image nodes with an affinity matrix Aim \u2208 \\mathbb{R}^{N_i \\times N_i} indicating the relational strengths. The object nodes, xi where xi \u2208 \\mathbb{R}^{1\\times D_o}, are similarly linked to image nodes with an affinity matrix Aio \u2208 \\mathbb{R}^{N_i \\times N_o}. The edges Epi connect place and image nodes with an affinity matrix Api \u2208 \\mathbb{R}^{N_p\\times N_i}, illustrating the relation- ships between places and images. The affinity matrices are computed using a multi-layer perceptron (MLP) net- work, which processes the features of nodes to output a scalar similarity value. The semantic graph map is con- structed incrementally as the agent navigates, with the graph at time t being a subset of the graph at time t + 1. This dynamic mapping allows the agent to reason about and navigate through the relationships among objects, images, and places towards the designated goal.\nPlace graph. In visual navigation, accurately identify- ing semantic places, such as living rooms and bedrooms, is crucial. To address this challenge, room navigation methods [23, 30] employ a place recognition algorithm. However, some places can be ambiguous and difficult to classify distinctly. To overcome this issue, a clustering method [31] is applied to train a place encoder, fplace, which groups similar features across similar places. This encoder takes as input image features, object features, and object categories to extract place information, de- fined as vt = fplace(st, of, oat). Here, of represents objects detected from a panoramic RGB sensor s; of is a feature vector of or; and ocat is the object's category. Using a panoramic RGB sensor is advantageous because the recognition of the place is invariant to camera rota- tion. To bring images with similar semantic meanings, such as those from a bedroom, closer together in the metric space, a contrastive loss is employed. The loss function for training the place encoder with a batch of B images is formulated as follows:\nL_{Place} = -\\sum_{i=1}^{B} log \\frac{exp (v_i \\cdot v_i^+/\\varsigma)}{\\sum_{j=0}^{r} exp (v_i \\cdot v_j/\\varsigma)}, (1)\nwhere vi is the query embedding and v\u1d62 are positive place embeddings for location i, sampled from the same place, and vj includes one positive embedding and r negative embeddings from different places, with \\varsigma acting as a tem- perature hyper-parameter. Positive samples are drawn us- ing ground truth place information which includes labels for ambiguous places such as 'other room.' To handle this, we utilize eight specific room labels, described in Section 4.2. We modify the contrastive loss by replacing the positive sample with images generated by randomly rotating the query image, a method we denote as LNear. This approach aims to bring images from nearby loca- tions closer together in the metric space. Subsequently,"}, {"title": "3.3. Localization", "content": "In this study, we test the localization function, Floc, to demonstrate how semantic knowledge can enhance localization accuracy. The input to this function includes the current semantic graph map, Gt, along with source and target images. We employ Graph Neural Networks (GNNs) to encode the graph data effectively and Trans- former [33] decoder networks to extract relevant localiza- tion information. Specifically, Floc utilizes these images to identify corresponding nodes within the graph. It then estimates the distance between these nodes, thereby facilitating precise localization based on semantic rela- tionships captured in the graph."}, {"title": "3.4. Semantic Environment Atlas", "content": "We propose a Semantic Environment Atlas (SEA) that synthesizes semantic graph maps collected from various environments into a unified structure, rather than merely compiling individual maps. This integration facilitates a deeper and more comprehensive understanding of the environments and their interrelationships. The SEA, de- noted as St = {\u0393, R}, comprises two key components: a place reachability matrix (\u0393) and a place-object con- nection matrix (R). he place reachability matrix (\u0393) defines the accessibility between different places, indi- cating possible paths and their navigability. Meanwhile,\ndemonstrating the connectivity between rooms in var- ious training environments. In the first floor plan, the connection between the bedroom and bathroom is cor- rectly identified (\u03b4\u2081(Ac[i, j]) = 1), as indicated by the green check mark. In the second training environment, the connectivity between bedroom and bathroom is not recognized (\u03b4\u2082(Ac[i, j]) = 0), marked by the red cross. When considering N training environments, the reacha- bility (\u0393) between place cluster i and j is calculated as follows:\n\u0393[i, j] = \\frac{\\sum_{n=1}^{N} \u03b4_n (A_c[i, j])}{\\sum_i \u03b4_n (P_i) \u03b4_n (P_j)T}, (2)\nwhere A_c = A_{pi} A_{im} A_{pi}^{T} and \u03b4_n (A_c[i, j]) indicates the existence of a connection between place cluster i and j. The function \u03b4_n (P_i) denotes the presence of the place cluster Pi in the nth scene.\nIt is important to note that the same places are not connected, thus Ac[i, j] = 0 when i = j. Furthermore, to normalize the reachability, we calculate it based on the number of environments in which each cluster appears, rather than the total number of environments. Reacha- bility is set to zero if a cluster does not appear in any scene."}, {"title": "3.5. Global Policy", "content": "The global policy (\u03c0g) utilizes the RGB-D image from the directional camera (st) to determine subgoals (gt) through semantic path planning, which leverages the place relationships and place-object relationships within the SEA. For example, to navigate from a bathroom to a kitchen, the calculated path might be bathroom \u2192 bed- room \u2192 living room \u2192 kitchen. Rather than attempting to locate the kitchen directly from the bathroom, the agent first navigates to intermediary nodes such as the bedroom and the living room, thereby systematically discovering the optimal path.\nCurrent place. To enable the semantic path planning, the agent first determines its current location by encod- ing RGB images and object information using the place encoder (as detailed in Section 3.2). The extracted place feature, derived from the encoded RGB image, object features, and object category, is then compared to place clusters (P) using cosine similarity to identify the near- est place cluster, thereby locating the agent within the environment."}, {"title": "3.6. Local Policy", "content": "The local policy (\u03c0l) processes directional RGB-D sensor data (st) along with local pose sensor readings to navigate the agent towards the designated subgoal gt. It employs the fast marching method (FMM) [34] to compute the shortest path from the agent's current location to the subgoal. This computation makes use of the obstacle channel, which is derived from the top-down map created from the depth component of the RGB-D input. Upon determining the shortest path, the local policy executes a series of deterministic actions to guide the agent along this path. This strategy of navigation has been validated in previous research, demonstrating its effectiveness in various scenarios [15, 16, 21]."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Baselines", "content": "Non-interactive baselines. BC: A baseline for behavior cloning was trained using an RNN-based policy that takes RGB-D, agent pose, and goal object category as inputs.\nEnd-to-end RL baselines. DD-PPO[28]: Standard end- to-end RL with distributed training over several nodes is proposed. Red-Rabbit[22]: Auxiliary tasks that improve sampling efficiency and generalization to pre- viously unseen domains are provided. THDA [20]: RL reward and model inputs are improved, which results in better generalization to new scenes."}, {"title": "4.2. Experimental Settings", "content": "Datasets. We utilized the Habitat simulator [1] to con- duct experiments using the Matterport3D (MP3D)[27] datasets, which feature photorealistic 3D reconstructions of the real world. The standard 61 train / 11 val splits for the ObjectNav configuration, as described in Section3.1, were employed. It should be noted that only the local pol- icy depends on the depth and pose, making the proposed method considerably more practical for use in the real world with noisy pose and depth sensors. The Habitat ObjectNav dataset [1] was used for MP3D experiments, with 21 goal categories (provided in the supplementary Section 1). We utilized 2195 episodes for the test.\nEvaluation metrics. All methods were evaluated us- ing the success rate (Success), success weighted by path length (SPL) [35], and distance to success (DTS). Success is determined by calculating the ratio of successful test episodes to the total number of test episodes. SPL takes into account both the Success and path length. When there are M episodes, SPL = \\frac{1}{M}\\sum_{i=1}^{M} \\frac{l_i}{max(p_i,l_i)} \\gamma_i, where li is the length of the short- est path from goal to target, pi is the length of the path taken by the agent, and \u03b3i is the binary indicator of Success for ith episode. Finally, DTS is the L2 dis- tance (measured in m) between the agent and the suc- cess threshold (1.0m) of the goal object at the end of the episode, as described in [35]."}, {"title": "4.3. Results", "content": "SEA outperforms navigation baselines. Our SEA method sets a new benchmark by surpassing all previous state-of-the-art baseline methods on the MP3D dataset's validation split, as detailed in Table 1. This achievement encompasses end-to-end reinforcement learning (RL), metric map-based baselines, and graph map-based meth- ods, particularly in terms of Success and DTS metrics. Remarkably, SEA demonstrates a staggering 926.3% im- provement in Success compared to the behavior cloning model. Furthermore, SEA outperforms the PONI[21] by 22.64% in Success and by 13.20% in SPL, thanks to its development of more efficient pathways. While PONI [21] is highly reactive and excels in exploration with its frontier-based path generation, it falls short in exploitation; in contrast, SEA uses topological maps for long-term planning, allowing it to create more effective routes through better exploitation. When compared to the SemExp + SI [17] model, which combines an ab- stract model with a semantic exploration approach, SEA increases Success by 12.4%. This is particularly notable given that SEA does not use global pose information and is trained without interactive reinforcement learn- ing. SEA outperforms SemExp[17] in terms of Success but has slightly lower SPL (by 1.4 percentage points) due to its adaptability. This adaptability allows SEA to eventually locate the object, even if it initially follows inefficient routes. In contrast, SemExp [17] relies on a highly accurate metric map and a perfect pose sensor, making it very efficient at following the optimal path. However, if SemExp[17] is led astray onto an incorrect path, it lacks the mechanism to update and correct itself, making it difficult to locate the object. SEA's ability to adapt and correct its course enables higher success rates, even though this sometimes means taking longer and less efficient paths, resulting in lower SPL performance.\nSEA has low computational requirements. The place encoder can be trained within a day using a single GPU. Constructing SEA takes about 10 hours with a single GPU. During the inference, a GPU with 3000 MB mem- ory is enough to run the trained encoder and detector. Our SEA has the lowest cost, three times less than the non-interactive SoTA baseline [21], as demonstrated in Figure 5.\nSEA is robust to pose noises. SEA demonstrates en- hanced robustness to pose sensor noise by utilizing place reachability for long-term planning instead of building a metric map. As shown in Figure 6, SEA experiences a modest performance drop in Success, with a 10% de- crease at a noise level of 10 and only a 0.77% reduction at noise level 20, despite significant pose sensor inter- ference. Here, the noise levels are indicative of real- world scenarios, with noise level 1 mimicking common"}, {"title": "Relation update is effective on adapting to unseen environment", "content": "The effectiveness of adapting to the unseen environment through relation updates is demonstrated in the results of an ablation study presented in the second row from the bottom of Table 1. As new place connec- tions or place-object connections arise during testing, the probability distribution is updated at each step of the current episode. This acquired connection information is utilized solely within the episode and is not stored for future use. The experiment yielded a 17.1% increase in success rate compared to the case where no episodic update was applied. Notably, SPL only improved by 0.7%. This is likely due to the episodic update altering the posterior distribution. The agent initially navigates to the location where the target object is most likely to be found and checks for its presence. If the object is not discovered in the initial place cluster, the agent may become stuck. However, the relation update weakens the connection between the target place cluster and the target object, allowing the agent to replan and reach the next most connected place cluster. As the agent success- fully finds the object by moving to the next place cluster, SPL decreases as the path length of the successful path becomes longer."}, {"title": "5. Visualization", "content": "Construction of SGMs. The process of constructing a SGM is depicted in Figure 8. A SGM integrates place graphs, image graphs, and object graphs. In this represen- tation, image nodes are depicted as circles. Their colors correspond to the respective place clusters. Object nodes are depicted as triangles. The colors of these nodes indicate object categories, matching the colors of the bounding boxes in the panoramic RGB image. For clar- ity, connections between image nodes and object nodes have been omitted in the visualization. It is important to note that top-down maps and the positions of image and object nodes are utilized solely for visualization pur- poses and are not used as input data. Additionally, a supplementary video is available that demonstrates the construction of semantic graph maps, showcasing one episode across 20 training scenes.\nExample visualization of episodes. We provide an exam- ple visualization of an episode in Figure 9. This shows how semantic prior graphs are used in the global pol- icy to perform ObjectNav, where the goal is identified as 'bed'. The navigation process begins with the agent perceiving the bedroom (P12) to be on the left side and the kitchen to be at the front and right side. Based on this initial perception, the agent decides to move left, anticipating that the target location might be there. Upon reaching the subgoal, the agent searches for a bed but does not find one. Consequently, the agent exits the current location and re-evaluates the subgoal, noticing a door on the left and inferring that the target location is likely on the left side. While proceeding towards the subgoal, the agent eventually encounters the target ob- ject, the bed. The agent then formulates a local plan to reach the bed. Finally, after confirming that the location matches the intended destination, the bedroom (P12), the agent presses the stop button. Detailed information and additional examples are provided in the supplementary material."}, {"title": "6. Conclusions and Future Work", "content": null}, {"title": "6.1. Conclusions", "content": "We present SEA, a method for learning semantic re- lationships between places and objects in unknown en- vironments with low computational cost. Our approach identifies the object's location and navigates using place connections. By adapting to the unseen environment through relation updates, SEA achieves state-of-the-art results for ObjectNav in MP3D. Unlike other methods are not robust to noise pose sensor, SEA is robustly navi- gate environment with noisy settings. Our method is the first to not require a global metric map for ObjectNav in large environments like MP3D. We show that incorpo- rating semantic relationships improves localization and navigation tasks."}, {"title": "6.2. Future work", "content": "In addition to the current approach, we propose future directions and still open challenges:\n1. Incorporating language features for 3D objects: Our proposed method relies solely on image features to define objects. Using language features for 3D objects could lead to more general features that can improve object search and correlation graph connections in the metric space.\n2. Agents in dynamic environments: Recognizing changes in the environment, such as a cup being moved from the kitchen to the living room, can aid in task- solving. If an agent maintains memory in the form of a graph, it can adapt much better to dynamic environments compared to using a metric map.\n3. Recognizing physical laws in the environment: To manipulate objects or perform meaningful control tasks, it is necessary to understand the physical laws governing the environment. For example, avoiding small wooden blocks on the floor or considering the center of mass when picking up a tool.\n4. Interactive intelligence: Our method should not only rely on its own intelligence but also interact with humans or other robots in the environment to update the topological map. Further advancements in these areas can lead to more robust and effective navigation and localization in complex environments.\nWe believe that our proposed method is a step towards achieving this goal, and we look forward to future devel- opments and improvements. Additional details can be found in the supplementary material."}]}