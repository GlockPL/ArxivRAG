{"title": "SAFREE: TRAINING-FREE AND ADAPTIVE GUARD FOR SAFE TEXT-TO-IMAGE AND VIDEO GENERATION", "authors": ["Jaehong Yoon", "Shoubin Yu", "Vaidehi Patil", "Huaxiu Yao", "Mohit Bansal"], "abstract": "Recent advances in diffusion models have significantly enhanced their ability to generate high-quality images and videos, but they have also increased the risk of producing unsafe content. Existing unlearning/editing-based methods for safe generation remove harmful concepts from models but face several challenges: (1) They cannot instantly remove harmful or undesirable concepts (e.g., artist styles) without additional training. (2) Their safe generation capabilities depend on collected training data. (3) They alter model weights, risking degradation in quality for content unrelated to the targeted toxic concepts. To address these challenges, we propose SAFREE, a novel, training-free approach for safe text-to-image and video generation, that does not alter the model's weights. Specifically, we detect a subspace corresponding to a set of toxic concepts in the text embedding space and steer prompt token embeddings away from this subspace, thereby filtering out harmful content while preserving intended semantics. To balance the trade-off between filtering toxicity and preserving safe concepts, SAFREE incorporates a novel self-validating filtering mechanism that dynamically adjusts the denoising steps when applying the filtered embeddings. Additionally, we incorporate adaptive re-attention mechanisms within the diffusion latent space to selectively diminish the influence of features related to toxic concepts at the pixel level. By integrating filtering across both textual embedding and visual latent spaces, SAFREE ensures coherent safety checking, preserving the fidelity, quality, and safety of the generated outputs. Empirically, SAFREE achieves state-of-the-art performance in suppressing unsafe content in T2I generation (reducing it by 22% across 5 datasets) compared to other training-free methods and effectively filters targeted concepts, e.g., specific artist styles, while maintaining high-quality output. It also shows competitive results against training-based methods. We further extend SAFREE to various T2I backbones and T2V tasks, showcasing its flexibility and generalization. As generative AI rapidly evolves, SAFREE provides a robust and adaptable safeguard for ensuring safe visual generation.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in Generative AI have significantly impacted various modalities, including text (Brown, 2020), code (Chen et al., 2021), audio (Kreuk et al., 2022; Copet et al., 2024), image (Podell et al., 2023; Ho et al., 2022), and video generation (Ho et al., 2022; Kondratyuk et al., 2023; Yoon et al., 2024; openai, 2024). Generation tools such as DALL\u00b7E 3, Midjourney, Sora, and KLING have seen significant growth, enabling a wide range of applications in digital art, AR/VR, and educational content creation. However, these tools/models also carry the risk of generating content with unsafe concepts such as bias, discrimination, sex, or violence. Moreover, the definition of \"unsafe content\u201d varies according to societal perceptions. For example, individuals with Post-Traumatic Stress Disorder (PTSD) might find specific images (e.g., skyscrapers, deep-sea scenes) distressing. This underscores the need for an adaptable, flexible solution to enhance the safety of generative AI while considering individual sensitivities.\nTo tackle these challenges, recent research has incorporated safety mechanisms in diffusion models. Unlearning methods (Zhang et al., 2023a; Huang et al., 2023; Park et al., 2024; Wu et al., 2024) fine-tune models to remove harmful concepts, but they lack adaptability and are less practical due to the significant training resources they require. Model editing methods (Orgad et al., 2023; Gandikota et al., 2024; Xiong et al., 2024) modify model weights to enhance safety, but they often degrade output quality and make it challenging to maintain consistent model behavior. A promising alternative is training-free, filtering-based methods that exclude unsafe concepts from input prompts without altering the model's original capabilities. However, prior training-free, filtering-based methods encounter two significant challenges: (1) they may not effectively guard against implicit or indirect triggers of unsafe content, as highlighted in earlier studies (Deng & Chen, 2023), and (2) our findings indicate that prompts subjected to hard filtering can result in distribution shifts, leading to quality degradation even without modifying the model weights. Thus, there is an urgent need for an efficient and adaptable mechanism to ensure safe visual generation across diverse contexts.\nThis paper presents SAFREE, a training-free, adaptive plug-and-play mechanism for any diffusion-based generative model to ensure safe generation without altering well-trained model weights. SAFREE employs unsafe concept filtering in both textual prompt embedding and visual latent space, thereby enhancing the fidelity, quality, and efficiency for safe visual content generation. Specifically, SAFREE first identifies the unsafe concept subspace, i.e., the subspace within the input text embedding space that corresponds to undesirable concepts, by concatenating the column vectors of unsafe keywords. Then, to measure the proximity of each input prompt token to the unsafe/toxic subspace, we mask each token in the prompt and calculate the projected distance of the masked"}, {"title": "2 RELATED WORK", "content": "Recent works address vulnerabilities in generative models, including LLMs (Patil et al., 2023; Liu et al., 2024b), VLMs (Zhao et al., 2023), and T2I models (Yang et al., 2024c; Wang et al., 2024; Li et al., 2024c). Cross-modality jailbreaks, like Shayegani et al. (2023), pair adversarial images with prompts to disrupt VLMs without accessing the language model. Tools like Ring-A-Bell (Tsai et al., 2024) and automated frameworks by Kim et al. (2024b) and Li et al. (2024a) focus on model-agnostic red-teaming and adversarial prompt generation, revealing safety flaws. Methods by Ma et al. (2024), Yang et al. (2024b), and Mehrabi et al. (2023) exploit text embeddings and multimodal inputs to bypass safeguards, using strategies like adversarial prompt optimization and in-context learning (Chin et al., 2024; Liu et al., 2024c). These works highlight vulnerabilities in T2I models."}, {"title": "2.2 SAFE T21 GENERATION", "content": "Training-based: Training-based approaches (Lyu et al., 2024; Pham et al., 2024; Zhang et al., 2024) ensure safe T2I generation by removing unsafe elements, as in Li et al. (2024c) and Gandikota et al. (2023), or using negative guidance. Adversarial training frameworks like Kim et al. (2024a) neutralize harmful text embeddings, while works like Das et al. (2024) and Park et al. (2024) filter harmful representations through concept removal and preference optimization. Fine-tuning methods such as Lu et al. (2024)'s cross-attention refinement and Heng & Soh (2023)'s continual learning remove inappropriate content. Latent space manipulation, explored by Liu et al. (2024a) and Li et al. (2024b), enhances safety using self-supervised learning. While effective, they require extensive fine-tuning, degrade image quality, and lack inference-time adaptation. Our SAFREE is training-free, dynamically adapts to concepts, and controls filtering strength w/o modifying weights, offering efficient safety across T2I and T2V models.\nTraining-free: Training-free methods for safe T2I generation adjust model behavior without retraining. These include (1) Closed-form weight editing, like Gandikota et al. (2024)'s model projection editing and Gong et al. (2024)'s target embedding methods, which remove harmful content while preserving generative capacity, and Orgad et al. (2023)'s minimal parameter updates to diffusion models. (2) Non-weight editing, such as Schramowski et al. (2023)'s Safe Latent Diffusion using classifier-free guidance and Cai et al. (2024)'s prompt refinement framework. However, these methods lack robustness and test-time adaptation. Our method is training-free, dynamically adjusts filtering based on prompts, and extends to other architectures and video tasks without weight edits, offering improved scalability and efficiency."}, {"title": "3 SAFREE: TRAINING-FREE AND ADAPTIVE GUARD FOR SAFE TEXT-TO-IMAGE AND VIDEO GENERATION", "content": "Recent approaches (Gandikota et al., 2024; Gong et al., 2024; Lu et al., 2024; Chavhan et al., 2024; Yang et al., 2024a) have demonstrated the effectiveness of weight modification through unlearning or model editing to prevent the generation of harmful (e.g., pornography, self-harm, violence), biased (e.g., racial or social stereotypes, ageism), or otherwise undesirable (e.g., public, copyrighted) visual content in text-to-image generation models. However, these methods have limited flexibility because they 1) require storing individual model weights for each concept to be removed, 2) inherently reduce the backbone model's generative capabilities through unlearning, and 3) necessitate distinct solutions for safe generation across different models (i.e., modified model weights). To address this, we propose SAFREE, a training-free, yet adaptive remedy for safe T2I and T2V generation (See Fig. 2). We first determine the trigger tokens that can potentially induce toxicity based on the proximity between masked input prompt embeddings and toxic concept subspace (Sec. 3.1). Next, we propose projecting the detected undesirable tokens' embeddings into a space orthogonal to the toxic concept subspace, while ensuring they remain within the input embedding space (Sec. 3.2). Additionally, we introduce an adaptive re-attention strategy, called self-validating filtering, in the latent space during the de-noising process, facilitating a robust joint filtering mechanism across both text and visual embedding spaces (Sec. 3.3). Finally, we extend our approach to high-resolution models like SDXL (Podell et al., 2023), DiT-based image diffusion (SD-v3) (Peebles & Xie, 2023), and representative text-to-video generative models such as ZeroScopeT2V (zeroscope, 2024) and CogVideoX-5B (Yang et al., 2024d) (Sec. 3.4)."}, {"title": "3.1 ADAPTIVE TOKEN SELECTION BASED ON TOXIC CONCEPT SUBSPACE PROXIMITY", "content": "Random noise \u20ac0 ~ N(0, I) sampled from a Gaussian distribution can lead to the generation of unsafe or undesirable images in diffusion models, primarily due to inappropriate semantics embedded in the text, which conditions the iterative denoising process (Rombach et al., 2022; Ho & Salimans, 2022). To mitigate this risk, recent studies (Miyake et al., 2023; Schramowski et al., 2023; Ban et al., 2024a) have demonstrated the effectiveness of using negative prompts. In this approach, the model aims to predict the refined noise from \u20ac0 over several autoregressive denoising steps, synthesizing an image conditioned on the input (i.e., the input text prompt). The denoising process of diffusion models, parameterized by 0, at timestep t follows the classifier-free guidance approach:\n\u20act = (1 + \u03c9) \u03b5\u03b8 (zt, p) wee (zt, \u00d8), (1)"}, {"title": "3.2 SAFE GENERATION VIA CONCEPT ORTHOGONAL TOKEN PROJECTION", "content": "We aim to project toxic concept tokens into a safer space to encourage the model to generate appropriate images. However, directly removing or replacing these tokens with irrelevant ones, such as random tokens or replacing the token embeddings with null embeddings, disrupts the coherence between words and sentences, compromising the quality of the generated image to the safe input prompt, particularly when the prompt is unrelated to the toxic concepts. To address this, we propose projecting the detected token embeddings into a space orthogonal to the toxic concept subspace while keeping them within the input space to ensure that the integrity of the original prompt is preserved as much as possible. We begin by formalizing the input space I using pooled embeddings from masked prompts as described in Sec. 3.1, such that I = [P\\0;P\\1;...;P\\N-1] \u2208 RD\u00d7N.\nGiven the projection matrix into input space I formulated by P\u2081 = I(ITI)\u00af\u00b9 IT (derived by Eq. (3)), we perform selective detoxification of input token embeddings based on the obtained token masks that project assigned tokens into P\u2081 and to be orthogonal to Pc:\nPproj = P1(I \u2013 Pc)p,\nPsafe = m Pproj + (1-m) p, (5)"}, {"title": "3.3 ADAPTIVE LATENT RE-ATTENTION IN FOURIER DOMAIN", "content": "Recent literature (Mao et al., 2023; Qi et al., 2024; Ban et al., 2024b; Sun et al., 2024) has demonstrated that the initial noise sampled from a Gaussian distribution significantly impacts the fidelity of T2I generation in diffusion models. To further guide these models in creating content while suppressing the appearance of inappropriate or target concept semantics, we propose a novel visual latent filtering strategy during the denoising process. Si et al. (2024) note that current T2I models frequently experience oversmoothing of textures during the denoising process, resulting in distortions in the generated images. Building on this insight, we suggest an adaptive re-weighting strategy using spectral transformation in the Fourier domain. At each timestep, we initially perform a Fourier transform on the latent features, conditioned on the initial prompt p (which may incorporate unsafe guidance) and our filtered prompt embedding Psafree. The low-frequency components typically capture the global structure and attributes of an image, encompassing its overall context, style, and color. In this context, we reduce the influence of low-frequency features, which are accentuated by our filtered prompt embedding, while preserving the visual regions that are more closely aligned with the original prompt to avoid excessive oversmoothing. Let h(\u00b7) be a latent feature, to achieve this, we attenuate the low-frequency features in h(psafree) by a scalar s when their values are lower in magnitude than those from p:\nF(p) = b FFT(h(p)), F(psafree) = b FFT(h(psafree)), (7)\nFs= { sF(Psafree)i if F(Psafree)i > F(P)i, F(Psafree)i otherwise. (8)"}, {"title": "3.4 SAFREE FOR ADVANCED T2I MODELS AND TEXT-TO-VIDEO GENERATION", "content": "Unlike existing unlearning-based methods limited to specific models or tasks, SAFREE is architecture agnostic and can be integrated across diverse backbone models without model modifications, offering superior versatility in safe generation. This flexibility is enabled by concept-orthogonal, selective token projection and self-validating adaptive filtering, allowing SAFREE to work across a wide range of generative models and tasks. It operates seamlessly with models beyond SD v-1.4, like, SDXL (Podell et al., 2023) and SD-v3 (stabilityai, 2024) in a zero-shot, training-free manner,"}, {"title": "4 EXPERIMENTAL RESULTS", "content": "We use StableDiffusion-v1.4 (SD-v1.4) (Rombach et al., 2022) as the primary T2I backbone, fol-lowing recent work (Gandikota et al., 2023; 2024; Gong et al., 2024). All methods are tested on adversarial prompts from red-teaming methods: I2P (Schramowski et al., 2023), P4D (Chin et al., 2024), Ring-a-Bell (Tsai et al., 2024), MMA-Diffusion (Yang et al., 2024b), and UnlearnDiff (Zhang et al., 2023b). Following Gandikota et al. (2023), we also evaluate models on artist-style removal tasks, using two datasets: one with five famous artists (Van Gogh, Picasso, Rembrandt, Warhol, Caravaggio) and the other with five modern artists (McKernan, Kinkade, Edlin, Eng, Ajin: Demi-Human), whose styles can be mimicked by SD. We extend SAFREE to text-to-video generation, applying it to ZeroScopeT2V (zeroscope, 2024) and CogVideoX (Yang et al., 2024d) with different model backbones (UNet (Ronneberger et al., 2015) and Diffusion Transformer (Peebles & Xie, 2023)). For quantitative evaluation, we use SafeSora (Dai et al., 2024) with 600 toxic prompts across 12 concepts, constructing a benchmark of 296 examples across 5 categories."}, {"title": "4.2 BASELINES AND EVALUATION METRICS", "content": "Baselines. We compare our method with training-free approaches: SLD (Schramowski et al., 2023) and UCE (Gandikota et al., 2024), as well as training-based methods including ESD (Gandikota et al., 2023), SA (Heng & Soh, 2023), CA (Kumari et al., 2023), MACE (Lu et al., 2024), SDID (Li et al., 2024b), and RECE (Gong et al., 2024). Additional details are in the Appendix.\nEvaluation Metrics. We assess safeguard capability via Attack Success Rate (ASR) on adversarial nudity prompts (Gong et al., 2024). For generation quality, we use FID (Heusel et al., 2017),"}, {"title": "4.3 EVALUATING THE EFFECTIVENESS OF SAFREE", "content": "SAFREE achieves training-free SoTA performance without altering model weights. We com-pare different methods for safe T2I generation, extensively and comprehensively evaluating each model's vulnerability to adversarial attacks (i.e., attack success rate (ASR)) and their performance across multiple attack scenarios. As shown in Tab. 1 and Fig. 4, the proposed safeguard SAFREE consistently achieves significantly lower ASR than all training-free baselines, across all attack types. Notably, it demonstrates 47%, 13%, and 34% lower ASR compared to the best-performing counterparts- I2P, MMA-diffusion, and UnlearnDiff, respectively-highlighting its strong resilience against adversarial attacks.\nSAFREE show competitive results against training-based methods. In addition, we compare our approach with training-based methods. Surprisingly, our approach achieves competitive performance against these techniques. While SA and MACE exhibit strong safeguarding capabilities, they significantly degrade the overall quality of image generation due to excessive modifications of SD weights, often making them impractical for real-world applications as they frequently cause severe distortions. Notably, SAFREE delivers comparable safeguarding performance while generating high-quality images on the COCO-30k dataset, all within a training-free framework.\nSAFREE is highly flexible and adaptive while maintaining generation quality. Our SAFREE does not require additional training or model weight modifications (more detailed comparison in later Tab. 4), providing key advantages over other methods (e.g., ESD, SA, and CA) which depend on unlearning or stochastic optimization, thereby increasing complexity. SAFREE allows for dynamic control of the number of filtered denoising steps based on inputs in an adaptable manner"}, {"title": "4.4 EVALUATING SAFREE ON ARTIST CONCEPT REMOVAL TASKS", "content": "As shown in Tab. 2, the proposed SAFREE achieves higher LPIPS\u1ebd and LPIPSu scores compared to the baselines. The latter is likely due to our approach performing denoising processes guided by a coherent yet projected conditional embedding within the input space. As illustrated in Fig. 5, SAFREE enables generation models to retain the artistic styles of other artists very clearly even"}, {"title": "4.5 EFFICIENCY OF SAFREE", "content": "This section compares the efficiency of various methods, including the training-based ESD/CA, which update models through online optimization and loss, and the training-free UCE/RECE, which modify model attention weights using closed-form edits. Our method (SAFREE), similar to SLD, is training-free and filtering-based, without altering diffusion model weights. As shown in Tab. 4, while UCE/RECE offer fast model editing, they still require additional time for model updates. In contrast, SAFREE requires no model editing or modification, providing flexibility for model development across different conditions while maintaining competitive generation speeds. Based on Tab. 1 and Tab. 4, SAFREE delivers the best overall performance in concept safeguarding, generation quality, and flexibility."}, {"title": "4.6 GENERALIZATION AND EXTENSIBILITY OF SAFREE", "content": "To further validate the robustness and generalization of SAFREE, we apply our method to various Text-to-Image (T2I) backbone models and Text-to-Video (T2V) applications. We extend SAFREE from SD-v1.4 to more advanced models, including SDXL, a scaled UNet-based model, and SD-V3, a Diffusion Transformer (Peebles & Xie, 2023) model. Our SAFREE demonstrates strong, training-free filtering of unsafe concepts, seamlessly integrating with these backbones. As shown in Tab. 5, SAFREE reduces unsafe outputs by 48% and 47% across benchmarks/datasets for SD-XL and SD-V3, respectively. We also extend SAFREE to T2V generation, testing it on ZeroScopeT2V (zeroscope, 2024) (UNet based) and CogVideoX-5B (Yang et al., 2024d) (Diffusion Transformer based) using the SafeSora (Dai et al., 2024) benchmark. As listed in Tab. 6, SAFREE significantly reduces a range of unsafe concepts across both models. It highlights SAFREE's strong generalization across architectures and applications, offering an efficient safeguard for generative AI. This is also evident in Fig. 4 right, demonstrating that SAFREE with recent powerful T2I/T2V generation models can produce safe yet faithful (e.g., preserve the concept of 'woman' in CogVideoX + SAFREE) and quality visual outputs. More visualizations for T2I and T2V models are included in the Appendix."}, {"title": "5 CONCLUSION", "content": "Recent advances in image and video generation models have heightened the risk of producing toxic or unsafe content. Existing methods that rely on model unlearning or editing update pre-trained model weights, limiting their flexibility and versatility. To address this, we propose SAFREE, a novel training-free approach to safe text-to-image and video generation. Our method first identifies the embedding subspace of the target concept within the overall text embedding space and assesses the proximity of input text tokens to this toxic subspace by measuring the projection distance after masking specific tokens. Based on this proximity, we selectively remove critical tokens that direct the prompt embedding toward the toxic subspace. SAFREE effectively prevents the generation of unsafe content while preserving the quality of benign textual requests. We believe our method will serve as a strong training-free baseline in safe text-to-image and video generation, facilitating further research into safer and more responsible generative models."}, {"title": "ETHICS STATEMENT", "content": "In recent text-to-image (T2I) and text-to-video (T2V) models, there are significant ethical concerns related to the generation of unsafe or toxic content. These threats include the creation of explicit, violent, or otherwise harmful visual content through adversarial prompts or misuse by users. Safe image and video generation models, including our proposed SAFREE, play a crucial role in mitigating these risks by incorporating unlearning techniques, which help the models forget harmful"}, {"title": "APPENDIX", "content": "In this Appendix, we present the following:\n\u2022 Experiment Setups including our method implementation details (Sec. A.1), baseline implementation details, and evaluation metrics details (Sec. A.1).\n\u2022 Extra visualization of our methods and other baselines, T2I generation with other backbone models (SDXL and SD-v3), video generation results with T2V backbones (ZeroScopeT2V and CogVideoX) (Sec. B).\n\u2022 Limitations and Broader Impact of our proposed SAFREE (Sec. C).\n\u2022 License information (Sec. D)."}, {"title": "A EXPERIMENTAL DETAILS", "content": "We employ StableDiffusion-v1.4 (SD-v1.4) (Rombach et al., 2022) as the main text-to-image generation backbone, following the recent literature (Gandikota et al., 2023; 2024; Gong et al., 2024). We evaluate our approach and baselines on inappropriate or adversarial prompts from multiple red-teaming techniques:\nI2P (Schramowski et al., 2023): contains 4703 unsafe prompts related to seven inappropriate/toxic concepts, including hate, harassment, violence, self-harm, sexual, shocking, and illegal activity. We evaluate our model and baselines by removing only the nudity concept following Gong et al. (2024), as the other concepts are ambiguous to determine their degree.\nP4D (Chin et al., 2024): we use the P4D-N-16 dataset which contains 151 unsafe prompts collected by a red-teaming tool, Prompting4Debugging (P4D). This is obtained by the prompt engineering to generate inappropriate concepts/objects from (safe) SD variants, including SD with negative prompts, ESD (Gandikota et al., 2023), and SLD (Schramowski et al., 2023).\nRing-a-bell (Tsai et al., 2024): we use 79 problematic prompts collected by a model agnostic red-teaming framework, Ring-a-bell, which is built upon a text encoder (e.g., text encoder in CLIP model) and is executed offline without dependency on T2I models.\nMMA-Diffusion (Yang et al., 2024b): is a red-teaming framework that is designed to generate inappropriate attack prompts by considering both textual and visual inputs to bypass defensive approaches for T2I models. We use 1K strong adversarial prompts obtained by this framework.\nUnlearnDiffAtk (Zhang et al., 2023b): we also use 142 adversarial prompts generated by an adversarial prompt attack framework, named UnLearnDiffAtk for unlearned T2I diffusion models (i.e., diffusion models with additional unlearning on unsafe concepts/objects).\nIn addition to evaluating safe T2I generation, we further assess the models' reliability in artist-style removal tasks. Following Gandikota et al. (2023), we employ two datasets: The first includes five famous artists: Van Gogh, Pablo Picasso, Rembrandt, Andy Warhol, and Caravaggio, while the second contains five modern artists: Kellly McKernan, Thomas Kinkade, Tyler Edlin, Kilian Eng, and Ajin: Demi-Human, whose styles have been confirmed to be imitable by SD. Each dataset contains 20 prompts per artist style; therefore, it has 100 prompts in total.\nWe further extend our SAFREE to text-to-video generation. We apply our method to two video generation models, ZeroScopeT2V (zeroscope, 2024) and CogVideoX (Yang et al., 2024d) with different model backbones (UNet and Diffusion Transformer (Peebles & Xie, 2023)). To quantitatively evaluate the unsafe concept filtering ability on T2V, we choose SafeSora (Dai et al., 2024), which contains 600 toxic textual prompts across 12 toxic concepts as our testbed. We further select 5 representative categories within 12 concepts, and thus construct a safe video generation benchmark with 296 examples."}, {"title": "A.2 BASELINES AND EVALUATION METRICS", "content": "Baselines. We primarily compare our method with recently proposed training-free approaches allowing instant weight editing or filtering: variants of SLD (Schramowski et al., 2023) and UCE (Gandikota et al., 2024) and RECE (Gong et al., 2024). UCE and RECE are training-free weight modification methods with a closed-form solution. We note that the editing time of RECE largely varies according to the concept to remove (e.g., several seconds to minutes). In addition, we compare SAFREE with training-based baselines to highlight the advantages of our approach encompassing decent safeguard capability through a training-free framework: ESD (Gandikota et al., 2023), SA (Heng & Soh, 2023), CA (Kumari et al., 2023), MACE (Lu et al., 2024), SDID (Li et al., 2024b).\nHyperparameters. Following Si et al. (2024), we use two different values for 8: 81 = 0.9 and 82 = 0.2 for the first and second stage blocks of the decoder, respectively. \u03b1 = 0.01, \u03b3 = 10 throughout the paper.\nEvaluation Metrics. We measure the Attack Success Rate (ASR) on adversarial prompts in terms of nudity following Gong et al. (2024) to evaluate the safeguard capability of methods. To evaluate the original generation quality of safe generation or unlearning methods, we measure the FID (Heusel et al., 2017), CLIP score, and a fine-grained faithfulness evaluation metric TIFA score (Hu et al., 2023) on COCO-30k (Lin et al., 2014) dataset. Among these, we randomly select 1k samples for evaluating FID and TIFA. In artist concept removal tasks, we use LPIPS (Zhang et al., 2018) to calculate the perceptual difference between SD-v1.4 output and filtered images following Gong et al. (2024). To more accurately evaluate whether the model removes characteristic (artist) \"styles\" in its output while preserving neighbor and interconnected concepts, we frame the task as a Multiple Choice Question Answering (MCQA) problem. Given the generated images, we ask GPT-40 (gpt 40, 2024) to identify the best matching artist name from five candidates. For safe Text-to-Video evaluation metrics, we follow the automatic evaluation via ChatGPT proposed by T2VSafetybench (Miao et al., 2024). We input sampled 16 video frames along with the same prompt design presented in T2VSafetybench to GPT-40 (gpt 40, 2024) for binary safety checking."}, {"title": "B MORE QUALITATIVE VISUALIZATION", "content": "We provide more visualization in this Appendix. We provide visualization of artist concept removal in Figs. 5 to 8, where we remove 'Van Gogh' in the model. Across Figs. 5 to 7, we observe that our SAFREE can effectively remove 'Van Gogh' without updating any model weights while other methods, even for training-based method, still struggle for removing this concept. Meanwhile, SAFREE preserves the quality of the desirable concepts in the given prompts. SAFREE can generate the same subjects/scenes as the base model did but remove the targeted style concepts. Furthermore, as shown in Fig. 8, we test both SAFREE and other baseline methods with text prompts containing other artist concepts. All models removed the 'Van Gogh' concept in their own way. SAFREE successfully preserved other artist styles by maintaining a high similarity to the original SD-1.4 outputs. Meanwhile, other methods like CA/SLD failed to hold the desirable concept. We further show more results by removing the 'nudity' concept in Figs. 9 to 11, and draw a similar conclusion.\nWe further change our diffusion model backbones to more advanced SDXL (Podell et al., 2023) and SD-v3 (stabilityai, 2024), as well as Text-to-Video generation backbone models, ZeroScopeT2V (zeroscope, 2024) and CogVideoX (Yang et al., 2024d). As shown in Fig. 12, our method shows robust-ness across Text-to-Image model backbones, and can effectively filter user-defined unsafe concepts but still preserve the safe concepts in the given toxic prompts. As illusrated in Figs. 13 to 15 and 17 to 19, our SAFREE shows good generalization ability to Text-to-Video settings. It helps to guard against diverse unsafe/toxic concepts (e.g., animal abuse, porn, violence, terrorism) while preserving quality to the remaining desirable content (e.g., building/human/animals)."}, {"title": "C LIMITATION & BROADER IMPACT", "content": "While our SAFREE demonstrates remarkable effectiveness in concept safeguarding and generaliza-tion abilities across backbone models and tasks. We notice that it is still not a perfect method to ensure safe generation in any case. Specifically, our filtering-based SAFREE method exhibits limi-"}, {"title": "D LICENSE INFORMATION", "content": "We will make our code publicly accessible. We use standard licenses from the community and provide the following links to the licenses for the datasets and models that we used in this paper. For further information, please refer to the specific link."}]}