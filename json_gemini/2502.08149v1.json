{"title": "Generalized Class Discovery in Instance Segmentation", "authors": ["Cuong Manh Hoang", "Yeejin Lee", "Byeongkeun Kang"], "abstract": "This work addresses the task of generalized class discovery (GCD) in instance segmentation. The goal is to discover novel classes and obtain a model capable of segmenting instances of both known and novel categories, given labeled and unlabeled data. Since the real world contains numerous objects with long-tailed distributions, the instance distribution for each class is inherently imbalanced. To address the imbalanced distributions, we propose an instance-wise temperature assignment (ITA) method for contrastive learning and class-wise reliability criteria for pseudo-labels. The ITA method relaxes instance discrimination for samples belonging to head classes to enhance GCD. The reliability criteria are to avoid excluding most pseudo-labels for tail classes when training an instance segmentation network using pseudo-labels from GCD. Additionally, we propose dynamically adjusting the criteria to leverage diverse samples in the early stages while relying only on reliable pseudo-labels in the later stages. We also introduce an efficient soft attention module to encode object-specific representations for GCD. Finally, we evaluate our proposed method by conducting experiments on two settings: COCOhalf + LVIS and LVIS + Visual Genome. The experimental results demonstrate that the proposed method outperforms previous state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "While supervised instance segmentation methods (He et al. 2017; Cheng et al. 2022; Jain et al. 2023) have achieved impressive performance, they require large-scale datasets with expensive human annotations. To reduce annotation costs, researchers have investigated semi-supervised learning (SSL) methods (Bellver et al. 2019; Yang et al. 2023; Berrada et al. 2024) that utilize unlabeled images along with small-scale labeled data, as well as weakly-supervised methods (Lan et al. 2021) relying on weak annotations. However, all these methods rely on the closed-world assumption and can recognize only the objects belonging to the classes (i.e., known classes) in the labeled dataset.\nTo address this limitation, researchers have introduced novel category discovery (NCD) (Han, Vedaldi, and Zisserman 2019). Unlike SSL, where the unlabeled images contain only known classes, NCD assumes that the unlabeled data include novel categories. Recently, generalized (novel) category discovery (GCD) (Vaze et al. 2022; Cao, Brbic, and Leskovec 2022) was introduced, further relaxing the assumptions on the unlabeled data. It assumes that the unlabeled data may contain both known and novel classes, making the problem more challenging and realistic. Given labeled and unlabeled data, GCD aims to train a model capable of recognizing both the known classes (e.g., person and car) in the labeled data and the novel categories (e.g., unknown1 and unknown2) discovered from the unlabeled data.\nMost previous works have investigated GCD for curated and balanced image classification datasets (Vaze et al. 2022; An et al. 2023; Zhang et al. 2023; Pu, Zhong, and Sebe 2023; Wen, Zhao, and Qi 2023). Recently, researchers have explored GCD for image classification on imbalanced datasets (Bai et al. 2023; Li et al. 2023; Li, Meinel, and Yang 2023), semantic segmentation (Zhao et al. 2022), 3D point cloud semantic segmentation (Riz et al. 2023), and instance segmentation (Fomenko et al. 2022; Weng et al. 2021). Most of these works leverage semi-supervised contrastive learning and pseudo-label generation regardless of tasks.\nIn this work, we also investigate GCD in instance segmentation. It is worth noting that because the real world contains numerous objects with long-tailed distributions, the instance distribution for each class in instance segmentation datasets is inherently imbalanced. For example, a 'car' appears more frequently than an 'ashtray'. (1) To address this imbalanced distribution, we propose an instance-wise temperature assignment method for contrastive learning. While typical contrastive learning losses treat samples from head and tail classes equally, we aim to emphasize group-wise discrimination for head class samples while focusing on instance-wise discrimination for tail class samples, inspired by (Kukleva et al. 2023). (2) Although relying on reliable pseudo-labels is important (Yang et al. 2022), applying fixed and global reliability criteria to imbalanced data tends to exclude most pseudo-labels for tail classes. Therefore, we propose to utilize class-wise reliability criteria to apply varying thresholds for head and tail classes. Additionally, we dynamically adjust the reliability criteria throughout training to leverage diverse samples in the early stages while focusing on reliable pseudo-labels in later stages. (3) Finally, we introduce an efficient soft attention module based on spatial pooling and depth reduction to effectively encode repre-"}, {"title": "2 Related Works", "content": "Generalized Class Discovery in Image Classification. (Vaze et al. 2022) introduced GCD, which aims to categorize unlabeled images given both labeled and unlabeled data. They trained an embedding network using unsupervised contrastive learning on all the data and supervised contrastive learning on the labeled data. They then applied semi-supervised k-means clustering to assign class or cluster labels to the unlabeled images. Additionally, they proposed a method for estimating the number of novel classes.\n(An et al. 2023) proposed DPN, which utilizes two sets of category-wise prototypes: one for labeled data and the other for unlabeled images based on k-means clustering. For clustering, they assumed prior knowledge of the total number of categories. They discovered novel classes in the unlabeled data by applying the Hungarian algorithm (Kuhn 1955) to the two sets. (Zhang et al. 2023) presented Prompt-CAL, which uses an affinity graph to generate pseudo-labels for unlabeled data. (Pu, Zhong, and Sebe 2023) introduced the DCCL framework, which employs a hyperparameter-free clustering algorithm (Rosvall and Bergstrom 2008) to generate pseudo-labels in the absence of ground-truth cluster numbers. (Wen, Zhao, and Qi 2023) proposed SimGCD, a one-stage framework that replaces the separate semi-supervised clustering in (Vaze et al. 2022) with a jointly trainable parametric classifier. They analyzed the problem of using unreliable pseudo-labels for training a parametric classifier and proposed using soft pseudo-labels.\nRecently, (Bai et al. 2023) introduced GCD for long-tailed datasets to address imbalanced distributions in real-world scenarios. They proposed the BaCon framework, which includes a pseudo-labeling branch and a contrastive learning branch. To handle imbalanced distributions, they estimated data distribution using k-means clustering and the Hungarian algorithm (Kuhn 1955). Concurrently, (Li et al. 2023) proposed ImbaGCD, which estimates class prior distributions assuming unknown classes are usually tail classes. They then generated pseudo-labels for unlabeled images using the estimated prior distribution and the Sinkhorn-Knopp algorithm (Cuturi 2013). Later, (Li, Meinel, and Yang 2023) replaced the expectation-maximization (EM) and Sinkhorn-Knopp (Cuturi 2013) algorithms in (Li et al. 2023) with cross-entropy-based regularization losses to reduce computational costs.\nClass Discovery in Segmentation and Detection. (Zhao et al. 2022) introduced NCD in semantic segmentation. They proposed to find novel salient object regions using a saliency model and a segmentation network trained on labeled data. They then applied clustering to these object regions to obtain pseudo-labels. Finally, they trained a segmentation network using the labeled data and the unlabeled images with clean pseudo-labels from clustering and online pseudo-labels. The clean pseudo-labels were dynamically assigned based on entropy ranking. (Riz et al. 2023) extended the method from (Zhao et al. 2022) to 3D point cloud semantic segmentation. Specifically, they utilized online clustering and exploited uncertainty quantification to generate pseudo-labels.\nIn instance segmentation, (Weng et al. 2021) investigated unsupervised long-tail category discovery. They initially employed a class-agnostic mask proposal network to obtain masks for all objects. They then trained an embedding network using self-supervised triplet losses. Finally, they applied hyperbolic k-means clustering to discover novel categories. (Fomenko et al. 2022) introduced novel class discovery and localization, which can be viewed as GCD in object detection and instance segmentation. They first trained a Faster R-CNN (Ren et al. 2015) or Mask R-CNN (He et al. 2017) using the labeled data and froze the network except for its classification head. Then, they applied the frozen network to unlabeled and labeled images to obtain region proposals. Subsequently, they expanded the classification head to incorporate new classes and trained it using pseudo-labels generated by online clustering based on the Sinkhorn-Knopp algorithm (Cuturi 2013)."}, {"title": "3 Proposed Method", "content": "We first define the GCD problem in instance segmentation, which aims to discover novel classes and learn to segment instances of both known and novel categories, given labeled and unlabeled data. We then present our GCD method in Section 3.2 and the method for training an instance segmentation network in Section 3.3. An overview of the proposed framework during training is illustrated in Figure 1.\n3.1 Preliminaries\nProblem Formulation. We are given a labeled dataset $D^l$ and an unlabeled dataset $D^u$. $D^l$ contains images $\\{I^l\\}$ along with instance-wise class and mask labels $(\\{y^l\\}, \\{M^l\\})$ for known classes $C_k$, while $D^u$ comprises only images $\\{I^u\\}$. Given $D^l$ and $D^u$, GCD in instance segmentation aims to discover novel categories $C_n$ (i.e., $C_k \\cap C_n = \\emptyset$) and to obtain a model capable of segmenting instances of both the known and novel classes $C = C_k \\cup C_n$. Hence, during inference, the network is expected to segment instances of known classes (e.g., person and car) as well as novel categories (e.g., unknown\u2081 and unknown\u2082) given an image $I$. The images in $D^l$ and $D^u$ may contain instances of both the known and novel classes.\nContrastive Learning for GCD. (Vaze et al. 2022) introduced a contrastive learning (CL) method for GCD in bal-"}, {"title": "3.2 Generalized Class Discovery", "content": "Given $D^l$ and $D^u$, we aim to discover novel categories in $D^u$ using the knowledge from $D^l$. To achieve this, we first train an instance segmentation network using $D^l$ and $D^u$ to generate class-agnostic instance masks $M^u$ for all objects in $D^u$. Subsequently, we crop the unlabeled images using $M^u$ and the labeled images using the ground-truth masks $M^l$. Then, we train a GCD model using the cropped unlabeled images $I^u$ and the cropped labeled images $I^l$ with class labels $y^l$ to generate pseudo-class labels $y^u$ for $I^u$.\nClass-Agnostic Instance Mask Generation. Similar to (Fomenko et al. 2022), we first train an instance segmentation network $f_0(\\cdot)$ to obtain class-agnostic instance masks for both known and unseen classes $C = C_k \\cup C_n$. We train a class-agnostic instance segmentation network, the Generic Grouping Network (GGN) from (Wang et al. 2022), using both $D^l$ and $D^u$. We experimentally demonstrate that our GCD method is robust when applied to other class-agnostic instance segmentation methods.\nOnce the training terminates, we apply $f_0(\\cdot)$ to the images $I^u \\in D^u$ to obtain instance masks $M^u$. We then construct an unlabeled object image set $I^u$ by cropping the rectangular regions of $I^u$ based on $M^u$. Similarly, a labeled object image set $I^l$ is prepared by cropping $I^l$ in $D^l$ based on the mask labels $M^l$.\nContrastive Learning for GCD in Instance Segmentation. We propose a contrastive learning method for GCD in instance segmentation by modifying the losses in Eq. (1). While these losses are designed for curated and balanced data, instances in typical instance segmentation datasets are naturally imbalanced (i.e., certain objects appear more frequently than others). To address the long-tail distribution of instances for each class, we propose to adjust the temperature parameters in $L_{rep}^{sup}$ and $L_{rep}^{unsup}$ for each instance based on its likelihood of belonging to head classes.\nFigure 2 visualizes t-SNE projections of two semantically similar classes: 'book' and 'booklet'. Previously, (Kukleva"}, {"title": "3.3 Reliability-Based Dynamic Learning", "content": "We generate pseudo-masks $M^u$ and pseudo-class labels $y^u$ for $D^u$ using the method described in Section 3.2. Subsequently, we train an instance segmentation network $f_s(\\cdot)$ that can segment instances of both known and novel classes using $D^l$ and $D^u$ with pseudo-labels. To address the issues of inaccurate pseudo-labels and imbalanced instance distributions across classes, we propose a reliability-based dynamic learning (RDL) method. It applies different reliability criteria to each class to avoid excluding all samples from tail classes. Additionally, it adjusts these criteria during training to use diverse data in the early stages while relying only on reliable pseudo-labels in the later stages.\nInspired by (Yang et al. 2022), we use holistic stability to measure the reliability of the pseudo-labels. At every fixed number of epochs during training $f_a(\\cdot)$, we save the model at that point in time. We then apply these saved models to object images $I_i \\in I^u$ to compute the probability $q_{ic}^t$ of $I_i$ belonging to class $c$, where $t$ denotes the index of the stored models, ranging from 1 to $T$. Subsequently, we compute the stability $s_i$ of the probabilities by comparing $q_{ic}^t$ from the final model with $q_{ic}^t$ from the intermediate models, as follows:\n$s_i = \\sum_{t=1}^{T-1} \\frac{1}{KL(q_{ic}^T || q_{ic}^t)}$\nwhere $KL(\\cdot||\\cdot)$ represents the Kullback-Leibler divergence, and $q_{ic}^T \\in \\mathbb{R}^{|C|}$. Since a higher $s_i$ indicates greater stability, (Yang et al. 2022) considered pseudo-labels with the lowest $r\\%$ scores unreliable.\nHowever, applying the same criteria to all data may result in categorizing most samples of a certain class as unreliable. Specifically, for imbalanced data, instances of tail classes tend to have lower $s_i$ than those of head classes due to the smaller number of training samples. Additionally, because neural networks tend to first memorize easy samples and then gradually learn harder instances during training (Arpit et al. 2017), difficult samples/classes often have lower $s_i$ than easier ones. To address these issues, we propose to use class-wise reliability criteria, which consider pseudo-labels with the lowest $r\\%$ scores per class as unreliable.\nAdditionally, we gradually increase the portion $r\\%$ of unreliable samples to initially learn from all data and later optimize using only reliable samples. The idea is that, in the early stages, having a larger number of diverse samples is more important than the accuracy of pseudo-labels, while pseudo-label quality becomes more crucial in later stages. Specifically, we first compute $t_i$ for each instance $i$ by finding the rank of $s_i$ among the lowest values within its class. We assign $\\gamma$ to $t_i$ if its proportional rank falls between $\\frac{t-1}{T_{is}}$ and $\\frac{t}{T_{is}}+1$. Then, the reliability-based adjustment weight $\\tau_{is}^t$ is computed as follows:\n$\\tau_{is}^t = 1 - (1 - \\gamma) (\\frac{t}{T_{is}})^2$\nwhere $t$ and $T_{is}$ denote the current epoch and the total number of epochs for training $f_s(\\cdot)$, respectively."}, {"title": "4 Experiments and Results", "content": "4.1 Experimental Setting\nDataset. We conducted experiments in two settings: COCOhalf + LVIS and LVIS + VG, following (Fomenko et al. 2022). In the COCOhalf + LVIS setting, we consider 80 COCO classes as known classes and aim to discover 1,123 disjoint LVIS classes from the total 1,203 LVIS classes. Among the 100K training images in the LVIS dataset (Gupta, Doll\u00e1r, and Girshick 2019), we use 50K images with labels for the 80 COCO classes for $D^l$ and the entire 100K images without labels for $D^u$. We use the 20K LVIS validation images for evaluation.\nIn the LVIS + VG setting, we utilize 1,203 LVIS classes as known classes and aim to discover 2,726 disjoint classes in the Visual Genome (VG) v1.4 dataset (Krishna et al. 2017). We use the entire 100K LVIS training data for $D^l$ and the combined 158K LVIS and VG training images for $D^u$. For evaluation, we use 8K images that appear in both the LVIS and VG validation sets. Although the VG dataset contains over 7K classes, only 3,367 classes appear in both $D^u$ and the validation data. After excluding the 641 classes that overlap with the known classes, we aim to discover 2,726 classes.\nImplementation Details. We use ResNet-50 as the backbone for both $f_a(\\cdot)$ and $f_s(\\cdot)$. For $f_a(\\cdot)$, we apply the SAM to all four stages of the backbone. We train $f_a(\\cdot)$ for 390 epochs with $K=1\\%, \\tau_{min}=0.07, \\tau_{max}=1, \\text{and} \\lambda = 0.35$. For the SAM, we set $M=3, d=D/8, \\omega=0.25\\cdot \\text{stg}, \\alpha =1, s_1=[18/(2^{stg-1})], s_2=[12/(2^{stg-1})], \\text{and} s_3=[8/(2^{stg-1})]$, where stg is the stage index and [] denotes rounding. We train $f_s(\\cdot)$ for 36 epochs with $T = 3$. The experiments were conducted on a computer with two Nvidia GeForce RTX 3090 GPUs, an Intel Core i9-10940X CPU, and 128 GB RAM. The code will be publicly available on GitHub upon publication to ensure reproducibility.\nEvaluation Metric. We first apply the Hungarian algorithm (Kuhn 1955) to find a one-to-one mapping between the discovered classes and the ground-truth novel classes, following (Fomenko et al. 2022). We then calculate mAP.50:.05:.95 for all, known, and novel classes. MAP.50:.05:.95 is computed using mask labels for the COCOhalf + LVIS setting and using bounding box labels for the LVIS + VG setting due to the absence of mask labels in the VG dataset."}, {"title": "5 Conclusion", "content": "Towards open-world instance segmentation, we present a novel GCD method in instance segmentation. To address the imbalanced distribution of instances, we introduce the instance-wise temperature assignment method as well as class-wise and dynamic reliability criteria. The former aims to improve the embedding space for class discovery, and the criteria are designed to effectively utilize pseudo-labels from the GCD model. Additionally, we propose an efficient soft attention module. The experimental results in two settings demonstrate that the proposed method outperforms previous methods by effectively discovering novel classes and segmenting instances of both known and novel categories.\nRegarding limitations, this work assumes the full availability of labeled and unlabeled datasets from the beginning. Thus, it is suboptimal for scenarios where data is provided sequentially, such as in robot navigation. Additionally, we assume prior knowledge of the total number of classes, following most previous works."}]}