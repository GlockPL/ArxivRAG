{"title": "Generating Diverse Negations from Affirmative Sentences", "authors": ["Darian Rodriguez Vasquez", "Afroditi Papadaki"], "abstract": "Despite the impressive performance of large language models across various tasks, they often struggle with reasoning under negated statements. Negations are important in real-world applications as they encode negative polarity in verb phrases, clauses, or other expressions. Nevertheless, they are underrepresented in current benchmarks, which mainly include basic negation forms and overlook more complex ones, resulting in insufficient data for training a language model. In this work, we propose NegVerse, a method that tackles the lack of negation datasets by producing a diverse range of negation types from affirmative sentences, including verbal, non-verbal, and affixal forms commonly found in English text. We provide new rules for masking parts of sentences where negations are most likely to occur, based on syntactic structure and use a frozen baseline LLM and prompt tuning to generate negated sentences. We also propose a filtering mechanism to identify negation cues and remove degenerate examples, producing a diverse range of meaningful perturbations. Our results show that NegVerse outperforms existing methods and generates negations with higher lexical similarity to the original sentences, better syntactic preservation and negation diversity. The code is available in https://github.com/DarianRodriguez/NegVerse.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in natural language processing (NLP) have enhanced various applications such as text generation [42], translation [9] and summarization [2], but handling negation remains a significant challenge [13]. Negations are crucial for reasoning and effective communication, as they express denial, contradiction, and absence. This is especially important in critical fields like biomedicine, where misinterpreting negated conditions can have serious consequences. For example, Large Language Models (LLMs) identifying acute bleeding [32] have misclassified cases with negated phrases, revealing bias and a limited understanding of negations [8, 20].\nDespite their importance, existing literature has established that language models struggle with negated sentences in tasks such as cloze completion, NLI, QA, and classification [1, 13, 20]. For example, the work in [39] found an inverse scaling trend among models such as GPT-J, GPT-3, Flan-T5, GPT-Neo, and OPT (ranging from 125M to 6B parameters), where larger models tend to perform worse on negation tasks and often produce incorrect answers with high confidence. Similarly, [16] and [18] demonstrated that models like BERT, ROBERTa, GPT-2, BART, and T5 frequently generate identical outputs for opposite statements and misinterpret sentences, such as classifying \"The man in the blue shirt is relaxing on the rocks\" as entailing \"A man is not wearing a blue shirt\".\nNegations are also underrepresented in most benchmark datasets, both in terms of frequency and complexity. In particular, the works in [12] and [13] show that general-purpose English corpora, such as reviews, conversations, Wikipedia, and books, contain between 22.6% and 29.9% sentences with"}, {"title": "2 Related Work", "content": "LLMs have excelled in various tasks [2, 9, 17, 42], but they consistently struggle with understanding negated sentences [15], which limits their reasoning abilities [39] and sometimes worsens with the model size. Current solutions, such as syntactic data augmentation using Semgrex patterns [14] and the TINA method [10], aim to enhance LLMs' robustness to negations in textual entailment tasks by augmenting training datasets with grammatically correct negated instances. However, they face errors in complex sentences. Other approaches like [34] generate negated data using tense patterns and keywords, while [8] uses WordNet to create true/false sentences. Nevertheless, these methods are not adaptable across diverse datasets. Polyjuice [43] generates sentence perturbations but produces nonsensical outputs and handles a limited range of negation types.\nThe work in [11] transforms negated sentences into affirmative ones using sentence pairs and back-translation yet it falls short compared to human understanding. Similar to the aforementioned approaches, our goal is to produce new negated sentences to augment the existing datasets. However, in contrast to these methods, our proposed approach generates a wider range of negations \u2013 including verbal, non-verbal, and affixal forms \u2013 from affirmative sentences. It employs an efficient masking strategy to maintain fluency and structural preservation, resulting in outputs that align lexically better with the original sentences and overcome the limitations of earlier methods."}, {"title": "3 Problem Formulation", "content": "We consider a dataset $\\mathcal{D} = \\{(\\mathbf{x}_i, \\mathbf{c}_i, \\mathcal{X}_i)\\}_{i=1}^m$, where $\\mathbf{x}_i$ denotes an affirmative sentence, $\\mathbf{c}_i = \\{c\\}_{i=1}^n$ is the corresponding context vector, and $\\mathcal{X}_i = \\{\\hat{x}_i^{(1)}, \\hat{x}_i^{(2)}, ..., \\hat{x}_i^{(n)}\\}$ the set of all the valid ground-truth negated sentences. The affirmative sentences lack any negation and do not include information guiding the construction of its negation. Each context $c_i$ includes $n$-structured prompts with placeholders denoted as [BLANK], indicating where the negation should be applied within a sentence $\\mathbf{x}_i$. The set $\\mathcal{X}$ contains the respective valid negated sentences corresponding to context $c_i$.\nOur goal is to learn a language generator model $g \\in \\mathcal{G}$, parametrized by a vector $\\theta \\in \\Theta$, that, given an affirmative sentence x and context c, produces a set of negated versions $\\mathcal{X}_{gen}$ that closely approximates the ground truth negated set $\\mathcal{X}$. This is equivalent to solving\n$\\min_{\\Theta \\in \\Theta} E_{\\mathcal{D}} [\\mathcal{L}(\\mathcal{X}, \\mathcal{X}_{gen})]$,\n$\\mathcal{L}(\\mathcal{X}, \\mathcal{X}_{gen}) = \\sum_{x\\in X} l(g_{\\theta}(x, c), x)$\n(1)\nwhere $l: \\Delta^{m-1} \\times \\Delta^{m-1} \\rightarrow R^+$ is the loss function with $\\Delta$ representing the probability simplex and $\\hat{x}_{gen}$ being the output of the generator model given a pair of an affirmative sentence and a context vector, formally defined as $\\hat{x}_{gen} = g(\\theta; x, c)$, with $x \\in \\mathcal{X}$."}, {"title": "4 Neg Verse Data Augmentation Method", "content": "Our model aims to generate negated sentences that meet three key criteria: closeness, quality, and diversity. Closeness ensures the negated sentence minimally differs from the original in structure and meaning. Quality emphasizes grammatical correctness, syntactic accuracy, and coherence. Diversity involves generating a variety of negations across different sentence spans, including verbal, non-verbal, and affixal forms, to enrich the dataset and test multiple negation forms.\nTo maintain closeness, we place [BLANK] tokens at likely negation points in the original sentence x and generate various perturbations for each blank. Our prompt format, adapted from Polyjuice, includes a negation control code, a blank sentence, and outputs separated by the [ANSWER] token, allowing the model to generate up to n possible negations per blank. During training, both individual tokens and entire sentences are masked to teach the model sentence structure and negation patterns. We provide details about the metrics for assessment in Section 5."}, {"title": "4.1 Neg Verse Prompt Format", "content": "We propose a masking strategy that enhances negation generation by strategically placing blanks in sentences, addressing limitations in traditional methods like Polyjuice, which often miss key elements such as main verbs, auxiliary verbs, contractions like \"wasn't, and tense variations. Our approach masks key components, including verbs, adjectives, and specific nouns, to support both verbal and non-verbal negations with flexible granularity, allowing for individual token or subtree masking. We developed the token selection rules, summarized in the Bottom Right Table of Figure 1 based on sentence structure analysis and token functions, covering various aspects of sentence construction like determiners, subjects, objects, adverbs, adjectives, and"}, {"title": "4.2 Neg Verse Prompt-Tuning Process", "content": "We learn the generator using prompt tuning to update only the virtual token embeddings, while keeping a GPT-2 baseline model frozen. To achieve this, we leverage the non-verbal negations from NAN-NLI [40], and the affixal negations from SST-2 [35], as shown in Table 1. Given the underrepresentation of affixal negations in the existing datasets, we develop a new dataset of diverse affixal negations from minimal input [27] using one-shot learning and LLaMA2. During the prompt tuning process, each sentence is masked either by negated parts or entirely, which helps the model learn to handle different spans and levels of context, thereby enhancing its ability to produce accurate negations. The masked sentences are tokenized, padded, and then split into training and validation sets. Additional details on the datasets and hyperparameters are provided in Appendix C.1 and Appendix C.2, respectively."}, {"title": "4.3 Proposed Filtering Mechanism for Degenerate Sentences", "content": "Even though our proposed ap- Algorithm 1 NegVerse Filtering Mechanism\nproach is designed to generate flu-\nent and diverse negations, some of\nthe generated outputs may still con-\ntain errors or nonsensical phrases.\nTo address this, we propose a fil-\ntering process, outlined in Algo-\nrithm 1, that normalizes the orig-\ninal and generated sentences by\nconverting it to lowercase and\nremoving trailing punctuation or\nwhitespace (lines 3-5), removes\nduplicates and uses Levenshtein\ndistance to retain sentences that\nclosely resemble the original (lines\n7-10). We use NegBERT to de-\ntect the negation cues [19], and we\noutput e negations, that were uni-\nformly sampled from the set ex-\ntracted by NegBERT, to increase\nthe diversity in the sets (line 12).\n1: Input: $\\{X_{gen,i}\\}_{i=1}^m$: Generated negated set, $\\{x_i\\}_{i=1}^m$: affir-\nmative sentences, $\\epsilon$: negations sample number, $B = 0.5$:\nLevenshtein distance threshold\n2: for $i \\in [m]$ do\n3: $\\quad x_i \\leftarrow Trim(Lowercase(x_i))$\n4: $\\quad X_{gen,i} \\leftarrow Trim(Lowercase(X_{gen,i}))$\n5: $\\quad \\mathcal{X}_{T,i} \\leftarrow \\emptyset$\n6: $\\quad$ for $x'gen \\in X_{gen, i}$ do\n7: $\\quad\\quad$ if $x'gen \\neq \\text{''}$ then\n8: $\\quad\\quad\\quad d \\leftarrow LevenshteinDistance(x'_{gen},x)$\n9: $\\quad\\quad\\quad \\mathcal{X}_{T,i} = \\begin{cases} \\mathcal{X}_{T,i} \\cup \\{x'_{gen}\\}, & x'_{gen} \\in \\mathcal{X}_{T,i} \\land d < B \\\\ \\mathcal{X}_{T,i}, & O.W. \\end{cases}$\n10: $\\quad\\quad$ end if\n11: $\\quad$ end for\n12: $\\quad \\mathcal{X}_{f,i} = \\{x^{(1)},...,x^{(\\epsilon)}\\}  \\sim Uni (NegBERT(\\mathcal{X}_{T,i}))$\n13: end for\n14: Output: Filtered negation sets $\\{\\mathcal{X}_{f,i}\\}_{i=1}^m$"}, {"title": "5 Empirical Results", "content": "We evaluate our approach on five datasets: the Stanford Natural Language Inference (SNLI) dataset [3], the Semantic Textual Similarity Benchmark (STS) [24], COPA dataset [29], and the SemEval Aspect-Based Sentiment Analysis datasets for both restaurant and laptop domains [28]. We compare NegVerse against Polyjuice [43] and evaluate the generated text using (i) Levenshtein Distance (NLD) [25, 30, 38] that measures the minimal edits required to transform one sentence into another; and (ii) Syntactic Tree Edit Distance (Syntactic), which focuses on surface-level changes [45], to assess closeness. For diversity, we use the Self-BLEU Score [46], and for grammaticality and fluency we use a fine-tuned BERT model, following [44]. The quality of the generated sentences is further evaluated using Perplexity (PPL) [25, 38]. We provide more details and results, including generation examples and degenerate cases of NegVerse, in Appendix C.4.\nWe evaluate the performance of our proposed method, NegVerse, and the baseline Polyjuice across all datasets using the closeness, diversity, and quality criteria. The results are presented in Table 2. We observe that NegVerse outperforms Polyjuice in closeness and text quality for both token and subtree masking criteria. Our method achieves a lower Levenshtein distance, indicating better lexical similarity to the original sentences, and a lower syntactic score,"}, {"title": "6 Conclusions", "content": "In this work, we focus on improving the robustness of LLMs robustness on negated statements by proposing NegVerse, a method capable of generating various types of negations, including verbal, non-verbal, and affixal. We provide new masking rules and propose a filtering mechanism to identify negation cues and remove degenerate examples, producing diverse and in parallel meaningful negated sentences. We experiment with five real-world datasets and NegVerse outperforms existing methods and generates negations with higher lexical similarity to the original sentences, better syntactic preservation, and greater negation diversity. Our empirical results also highlight that the proposed approach can generate negated sentences without specific guidance on blank placement.\nWhile NegVerse excels in preserving syntactic structure and offers a greater variety of negation forms, it still produces some degenerate outputs, particularly when blanks are placed at the end of sentences, leading to grammatically correct but contextually meaningless results. Furthermore, although NegVerse generates a range of affixal negations, certain expected forms are missing. Finally, automated and accurate annotation is essential for the generated negations, as negations can either preserve or invert labels depending on the task."}, {"title": "A Types of Negations.", "content": "There are two main types of negations: morphological and syntactic negations, as outlined in Table 4. Morphological negations create negative expressions by adding affixes to words, either as prefixes or suffixes. A prefixal negation adds prefixes to the beginning of words and includes common prefixes like un- (e.g., unhappy), in-/im-/il-/ir-(e.g., inaccurate, impossible, illegal, irrelevant), dis- (e.g., disagree), and non- (e.g., nonexistent). A suffixal negation adds suffixes to the end of words and includes the common suffix-less (e.g., hopeless, meaningless). These affixes alter the meaning of the base words to convey negation, absence, or opposition [41]. Syntactic negations utilize grammatical structures and specific words to negate a sentence. This typically includes negative particles like not and no (e.g., She is not coming; There is no water), negative pronouns like nobody and nothing (e.g., Nobody knows; Nothing happened), negative adverbs like never and nowhere (e.g., She never comes; They went nowhere), negative determiners like no and neither (e.g., No students passed; Neither option is good), and negative conjunctions like nor and neither...nor (e.g., She didn't call, nor did she email; Neither he nor his friends came)."}, {"title": "B Prompt Design", "content": "In this section, we provide further details on our six rules of the masking strategy, which were outlined earlier in Section 4.1.\nRule 1: The first rule targets verbal negations by selecting verbs (VERB) and auxiliaries (AUX) for masking, as these are key components in forming negations. For instance, in the sentence \"She was eating an apple\", masking \"was\" and \"eating\" allows the model to generate the negation \"She was not eating an apple.\" This approach effectively negates the core actions or states in the sentence, as illustrated in Figure 2.\nRule 2: The second rule targets non-verbal negation by focusing on determiners (DET) with the dependency label det. Determiners like \"the\", \"a\", and \"an\" are crucial for defining noun phrases. The model selects a determiner and the following token for transformation, such as changing \"the man\" to \"no one\", as shown in Figure 2. Unlike Rule 3, which may negate entire phrases, Rule 2 specifically alters the determiner. Other examples include:\nPeter wanted some part of it. \u2192 Peter wanted none of it.\nRule 3: This rule focuses on negating objects (\"obj\") and subjects (\"subj\"), which are essential for defining who is performing an action and what is being acted upon. Negating the subject (\u2018\"subj\"') changes who or what is performing the action. For instance:"}, {"title": "C Additional Experimental Setup Details and Results", "content": "In section 4.2, we provided information about the tuning process of NegVerse by combining the non-verbal negations from NAN-NLI [40], and the affixal negations from SST-2 [35] and the new dataset we generated using LLaMA2. In what follows, we provide more information about these datasets.\nThis dataset is used to evaluate models' capabilities in understanding and processing sub-clausal negation instances in natural language applications. Sub-clausal negation occurs within a clause, rather than negating the entire clause itself. The dataset annotates various aspects of negation, including verbal vs. non-verbal, analytic vs. synthetic, and clausal vs. sub-clausal negation types. Additionally, it captures the constructions used in negation instances, as well as the operations applied"}, {"title": "C.1 Training Data Details", "content": "This dataset is used to evaluate models' capabilities in understanding and processing sub-clausal negation instances in natural language applications. Sub-clausal negation occurs within a clause, rather than negating the entire clause itself. The dataset annotates various aspects of negation, including verbal vs. non-verbal, analytic vs. synthetic, and clausal vs. sub-clausal negation types. Additionally, it captures the constructions used in negation instances, as well as the operations applied"}, {"title": "C.2 Hyperparameters", "content": "We train the model for 31 epochs using the AdamW optimizer, which integrates weight decay directly into the optimization process. The learning rate is set to 2.5 \u00d7 10-2 to strike an optimal balance between training efficiency and convergence speed. A weight decay of 1 \u00d7 10-3 is employed to address overfitting by penalizing large weights, while a batch size of 16 ensures stable gradient updates within memory constraints. Additionally, 24 virtual tokens are used to prompt-tune the model, allowing for focused adaptation on the specific task. Various hyperparameters were tested and monitored through the learning curve analysis, with these settings yielding the best results in terms of stability and performance. Out of the total 124,458,240 parameters in the pre-trained language model, only 18,432 are trainable, all coming from the virtual token embeddings. This represents just 0.0148% of the model's parameters, demonstrating the efficiency of the soft-tuning approach."}, {"title": "C.3 Software and Hardware", "content": "The proposed algorithms and experiments are implemented in Python, utilizing the PyTorch library. The experiments were conducted using a single NVIDIA Tesla A100 GPU. The official implementation of NegBERT can be found at https://github.com/adityak6798/ Transformers-For-Negation-and-Speculation."}, {"title": "C.4 Additional Results", "content": "In Table 7, we provide illustrative examples of how NegVerse and Polyjuice handle verbs and sentence masking differently. When a verb is masked, NegVerse generates various forms of negations, including both contracted and uncontracted versions. In contrast, Polyjuice often introduces unrelated concepts such as \"dining room\", \"Germany\" and \"t-glass\", which are not present in the original sentence and disrupt its overall coherence. Additionally, when entire sentences are masked, NegVerse typically produces outputs that closely resemble the original, while Polyjuice frequently creates entirely different sentences, sometimes altering the sentence type altogether, such"}, {"title": "C.5 Evaluation Metrics", "content": "In this section, we provide further details on the evaluation metrics used in the main manuscript to assess the performance of both the proposed approach and the baseline methods. We consider metrics to examine various aspects of negated text generation, including closeness, fluency, and diversity.\nThis metric measures the average minimum number of edits needed to transform one tokenized sentence into another. The formal definition is provided below:\n$NLD = \\frac{1}{N} \\sum_{i=1}^N \\frac{d(x_i, \\hat{x}_{gen,i})}{\\text{max}(|x_i|, |\\hat{x}_{gen,i}|)}$\nwhere $x_i$ denotes the reference sentence, $\\hat{x}_{gen,i}$ is the generated negated sentence from the model, and n is the total number of sentence pairs. This metric has been widely used in various studies to evaluate the similarity between sentence pairs, particularly in counterfactual evaluations. [25, 30, 38].\nThis metric evaluates the diversity within a set of generated texts by measuring their similarity to each other, as opposed to traditional BLEU, which compares generated texts to reference texts [46]. The Self-BLEU score is calculated as:\n$Self-BLEU = \\frac{1}{m} \\sum_{i=1}^m BLEU (\\hat{x}_{gen, i}, \\mathcal{X}_{gen} \\setminus \\{\\hat{x}_{gen,i}\\})$\nwhere m is the total number of generated sentences, $\\hat{x}_{gen,i}$ is the i-th generated sentence, and $\\mathcal{X}_{gen} \\setminus \\{\\hat{x}_{gen, i}\\}$ represents the set of all generated sentences except $\\hat{x}_{gen,i}$. A lower Self-BLEU score indicates higher diversity, while a higher score suggests more similarity among outputs.\nThis metric evaluates how well a language model predicts a sequence of tokens, with lower perplexity indicating better fluency. It has been widely used for fluency assessment in text generation models like GPT-2 [25, 38]. For a negated sentence $\\hat{x} = (\\hat{z}_1, \\hat{z}_2,..., \\hat{z}_n)$, where n is the sentence length, the perplexity $PPL(\\hat{x})$ is given by:\n$PPL(\\hat{x}) = \\exp \\bigg(-\\frac{1}{n} \\sum_{i=1}^n log p_{\\theta}(\\hat{z}_i | \\hat{z}_{<i}) \\bigg)$\nwhere $log p_{\\theta}(\\hat{z}_i | \\hat{z}_{<i})$ is the log probability of token $\\hat{z}_i$ given the preceding tokens $\\hat{z}_{<i}$."}]}