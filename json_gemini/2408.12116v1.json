{"title": "Geolocation Representation from Large Language Models are Generic Enhancers for Spatio-Temporal Learning", "authors": ["Junlin He", "Tong Nie", "Wei Ma"], "abstract": "In the geospatial domain, universal representation models are significantly less prevalent than their extensive use in natural language processing and computer vision. This discrepancy arises primarily from the high costs associated with the input of existing representation models, which often require street views and mobility data. To address this, we develop a novel, training-free method that leverages large language models (LLMs) and auxiliary map data from OpenStreetMap to derive geolocation representations (LLMGeovec). LLMGeovec can represent the geographic semantics of city, country, and global scales, which acts as a generic enhancer for spatio-temporal learning. Specifically, by direct feature concatenation, we introduce a simple yet effective paradigm for enhancing multiple spatio-temporal tasks including geographic prediction (GP), long-term time series forecasting (LTSF), and graph-based spatio-temporal forecasting (GSTF). LLMGeovec can seamlessly integrate into a wide spectrum of spatio-temporal learning models, providing immediate enhancements. Experimental results demonstrate that LLMGeovec achieves global coverage and significantly boosts the performance of leading GP, LTSF, and GSTF models.", "sections": [{"title": "Introduction", "content": "Geolocation representation models encode geographical coordinates into latent embeddings with enriched geographic contextual information. Such embeddings ensure that similar representations reflect analogous sociodemographic attributes, activity patterns, and climatic characteristics across locations over the globe (Wang et al. 2022; Jean et al. 2019; Lee et al. 2021; Wang, Li, and Rajagopal 2020; Zhang et al. 2021, 2023a; Zhou et al. 2023b; Kim and Yoon 2022).\nThese geolocation representations are naturally suited to enhance spatio-temporal learning because they carry spatial contextual semantics. Previous research has focused only on how geolocation representations can be used for geographic prediction (GP). Specifically, GP tasks are trained on the attributes of some locations and used to predict the attributes of the remaining locations. These attributes include crime rate (Li et al. 2022b; Kim and Yoon 2022), poverty rate (Jean et al. 2016; Chi et al. 2022; Marty and Duhaut 2024), public health (Yeh et al. 2021; Nilsen et al. 2021; Draidi Areed et al. 2022; Chang et al. 2022; Sheehan et al. 2019), and so on. However, these geolocation representations are not used for more complex tasks.\nThe reason why previous approaches do not further extend the complex applications is that they do not achieve global coverage and they have a heavy reliance on expensive input data such as street views, travel patterns, and traffic trajectories (Wang, Li, and Rajagopal 2020; Kim and Yoon 2022; Lee et al. 2021; Zhang et al. 2023a). Although studies have used free and globally available satellite imagery for geolocation representation, their effectiveness has been hampered by the low resolution and absence of important features such as activity patterns (Manvi et al. 2023; Robinson, Hohman, and Dilkina 2017; Head et al. 2017; Jean et al. 2019; Elmustafa et al. 2022; Xi et al. 2022).\nOur objective is to develop a generic and effective geolocation representation method that utilizes only readily accessible global data to improve more spatio-temporal learning tasks: GP, long-term time series forecasting (LTSF), and graph-based spatio-temporal forecasting (GSTF). The latter two are typical spatio-temporal datasets that, given the values of many nodes at historical moments, predict the future values of those nodes. They differ in that the former tends to deal with correlations between nodes by channel-mixing strategies while the latter aggregates spatial connections between nodes by graph neural networks (GNNs).\nRecent advancements have demonstrated the extensive spatio-temporal and human-related knowledge embedded within large language models (LLMs). Some studies have transformed GP tasks as text generation tasks in LLMs (Manvi et al. 2023, 2024), and some have even found that LLMs learn linear representations of space and time across multiple scales (global, country, city) (Gurnee and Tegmark 2023). Inspired by these findings, we explore the potential of LLMs to generate effective geolocation representations.\nIn this paper, we introduce a novel, training-free method that utilizes LLMs and auxiliary map data from OpenStreetMap to derive geolocation representations (LLMGeovec). As illustrated in Fig. 1, our approach first extracts textual descriptions of the coordinates from OpenStreetMap, which provides a sufficient geographic context. LLMs process these descriptions, and the final hidden states of individual tokens are averaged to form the LLMGeovec embedding for each coordinate."}, {"title": "Preliminaries", "content": "In this section, we introduce the definitions of geolocation representation learning, GP, LTSF and GSTF.\nGeolocation Representation Learning. Given a set of nodes $P \\in \\mathbb{R}^{2 \\times N}$, where N represents the number of nodes, and $P_i = (P_{lon}, P_{lat})$ denotes the longitude and latitude of the i-th node. The goal of geolocation representation learning is to construct an effective encoder f that transforms P into geographically informative representations $Z = f(P)$. Here, $Z \\in \\mathbb{R}^{M \\times N}$ with M denoting the dimensionality of the representation.\nGP. Given a set of nodes $P \\in \\mathbb{R}^{2 \\times N}$, each node is associated with geographic attributes such as climatic indicators (e.g., average annual temperature, humidity) and social indicators (e.g., regional average educational attainment, average annual income, poverty rate, crime rate). For a given set of geographic attributes $A \\in \\mathbb{R}^{1 \\times N}$, GP in the context of geolocation representation learning involves training a linear regressor with $Z[:K] = f(P[:K])$ to fit $A[:K]$ using K training samples. The performance of the regressor on the test sets $Z[K:] = f(P[K:])$ and A[K:] is used to measure the prediction task performance and the quality of the encoder f and representations Z.\nLTSF. We consider a historical multivariate time series (MTS) $X \\in \\mathbb{R}^{H \\times N}$, where N_represents the number of nodes (variates) and H is the number of historical time slots. The objective is to predict future values $Y \\in \\mathbb{R}^{F \\times N}$, with F as the number of future time slots. Each value of node i at time slot t is denoted by $X_{t}^{i}$, and their coordinates by $P \\in \\mathbb{R}^{2 \\times N}$.\nGSTF. Different from LTSF, GSTF constructs a weighted adjacency matrix $A \\in \\mathbb{R}^{N \\times N}$, where $A_{ij} = 1/dist(P_i, P_j)$ and $dist(P_i, P_j)$ represents the spatial distance between node $P_i$ and $P_j$. A graph G is then formed based on A. Unlike LTSF, GSTF leverages GNNs to aggregate the features of nodes $X_t$ at t-th time slot, enhancing prediction by incorporating spatial relationships."}, {"title": "LLMGeovec: A Generic Enhancer for Spatio-Temporal Learning", "content": "As depicted in Fig. 1, the proposed LLMGeovec method encapsulates two primary phases: prompt generation and text embedding via LLMs. Initially, we generate geographic descriptions based on specified coordinates leveraging map data. These descriptions are then transformed into embeddings by LLMs. The obtained embeddings can be used for GP, LTSF, and GSTF."}, {"title": "Prompt Generation", "content": "Given a coordinate, we generate universal prompts, intentionally devoid of task-specific data, to enable the effective extraction of geographic knowledge of LLMs. As outlined in Fig. 1, the prompt structure incorporates:\n\u2022 Instruction: Guides LLMs in identifying essential geographic information linked to specific coordinates.\n\u2022 Address: Utilizes reverse-geocoding to detail the hierarchy of location, from local neighborhoods to national identifiers.\n\u2022 Nearby Places: Enumerates the ten nearest points of interest within a 100-kilometer radius, including their names, distances, directions and bearings.\nData sources include OpenStreetMap (Neis and Zipf 2012), with addresses derived through Nominatim's reverse geocoding (Serere, Resch, and Havas 2023) and nearby places via the Overpass API (Olbricht et al. 2011). This approach aligns with and extends previous studies (Manvi et al. 2023, 2024) by focusing on the extraction of generalized geographic information without specifying downstream tasks."}, {"title": "Text Embedding Using LLMs", "content": "With the geolocation prompts generated, we proceed to embed these textual descriptions using LLMs. Recent studies have explored enhancing text embeddings generated by LLMs, typically by modifying attention mechanisms or repeating prompts to circumvent the limitations of decoder-only models (BehnamGhader et al. 2024; Muennighoff 2022; Ma et al. 2024; Wang et al. 2023; Springer et al. 2024). Our structured prompts, particularly with crucial geographic context presented at the end of prompts, allow LLMs to generate sufficiently high-quality geolocation representations without repetition of prompts or modification of models. To be specific, we use the average word embeddings from the last layer of a pre-trained LLM as the text representation, ensuring our LLMGeovec method remains adaptable to the latest LLMs without training. In addition, by avoiding fine-tuning, our method preserves the intrinsic geographic knowledge within LLMs (Zhai et al. 2023; Lin et al. 2023)."}, {"title": "Incorporating LLMGeovec into GP", "content": "Consistent with many previous studies, high-quality geolocation representations can be used for GP with the help of partial region labeling (Wang et al. 2022; Jean et al. 2019; Lee et al. 2021; Wang, Li, and Rajagopal 2020; Zhang et al. 2021, 2023a; Zhou et al. 2023b; Kim and Yoon 2022). This is a direct application of LLMGeovec. To be specific, we will divide the locations into a training set and a test set, and use linear regression in the training set to map geolocation representation to location attributes. This is followed by testing in the test set. It is worth noting that since LLMGeovec achieves global coverage, it can be used in GPs of various scales (global, country, city) and can also be combined with other geolocation representations through feature concatenation."}, {"title": "Incorporating LLMGeovec into LTSF", "content": "In this section, we describe the integration of LLMGeovec with LTSF models. We start by outlining a general LTSF model, which typically consists of a token embedding layer E, an encoder C, and a predictor D (Chen et al. 2023; Li et al. 2023; Liu et al. 2023b; Yi et al. 2024). The embedding layer E projects the the t-th historical record $X_t \\in \\mathbb{R}^{1 \\times \\breve{N}}$ into hidden temporal embeddings $S_t = E(X_t) \\in \\mathbb{R}^{d_t \\times N}$, where $d_t$ is the embedding dimension. Note that there can be a normalization operation in this embedder such as RevIN (Kim et al. 2021) to address the nonstationarity of time series. The encoder C then models the node-to-node and slot-to-slot relationships across H historical time slots, and the predictor D generates predictions $\\hat{Y} \\in \\mathbb{R}^{F \\times N}$ for the future F time slots. This process is formulated as follows:\n$S = {S_0,\\dots, S_t, \\dots, S_H}, S_t = E(X_t), (1)$\n$\\mathit{LOSS}_{LTSF} = min ||D(C(S)) - Y||, (2)$\nwhere the model parameters are updated automatically through gradient descent. In practice, the encoder C can be instantiated by Transformer blocks, convolution, and MLPs, to model either channel dependencies or token correlations. Many state-of-the-art LTSF models follow this architectural template, such as TSMixer (Chen et al. 2023), RMLP (Li et al. 2023), iTransformer (Liu et al. 2023b), and FreTS (Yi et al. 2024). For other Transformer-based architectures that employ token-wise embedding, such as Autoformer (Wu et al. 2021) and Informer (Zhou et al. 2021), we can adapt them with a simple inverting strategy (Liu et al. 2023b).\nFor an LTSF task, we collect the latitude and longitude of each node that generates the time series to construct the node set P. We then select an LLM (e.g., LLaMa3) and use our proposed LLMGeovec to generate the geolocation representation $Z \\in \\mathbb{R}^{M \\times N}$. A two-layer MLP acts as an adapter for LLMGeovec, projecting Z into a low-dimensional space $Z' = Adapter(Z) \\in \\mathbb{R}^{d_s \\times N}$ to align with the LTSF task. This process is described by the following equations:\n$Z' = Adapter(Z), (3)$\n$S' = {S'_1, \\dots, S'_t, \\dots, S'_H}, S'_t = Concat(E(X_t), Z'), (4)$\n$\\mathit{LOSS}_{LTSF} = min ||D(C(S')) - Y||, (5)$\nwhere $S' \\in \\mathbb{R}^{(d_t+d_s) \\times N}$ is formed by concatenating the series embeddings E(Xt) with the geolocation representations Z' along the feature dimension. The parameters of both the adapter and the original components are updated automatically via gradient descent."}, {"title": "Incorporating LLMGeovec into GSTF", "content": "Previous spatio-temporal prediction models often employ GNNs to capture spatial relationships between nodes, aggregating them into node features, which are then input into the temporal modeling component sequentially or alternatively (Shao et al. 2022a,d,c; Wu et al. 2019a; Tang, He, and Zhao 2022; Tang et al. 2022). We refer to the spatio-temporal model as STGNN. Given the graph G, the structural template is framed as follows:\n$S = {S_0,..., S_t, \\dots, S_H}, S_t = E(X_t),$\n$\\hat{S} = STGNN(S, G), (6)$\n$\\mathit{LOSS}_{GSTF} = min||D(\\hat{S}) - Y||,$\nwhere the embedder E projects the node signal to hidden states, and all node states are collected into the STGNN processor to generate the graph representation $\\hat{S}$.\nOn top of them, we first concatenate LLMGeovec into node features (e.g., temporal readings), which are then processed by STGNN. This process is described by:\n$Z' = Adapter(Z), (7)$\n$S' = {S'_1,\\dots, S'_t, \\dots, S'_H}, S'_t = Concat(E(X_t), Z'), (8)$\n$\\hat{S'} = STGNN(S', G), (9)$\n$\\mathit{LOSS}_{GSTF} = min ||D(\\hat{S'}) - Y||, (10)$\nwhere the parameters of both the adapter, the STGNN and D are updated automatically via gradient descent.\nNote that this scheme is applicable for STGNNs with different types of processing methods. Specifically, different models have different instantiations of Eq. 9, e.g., spatio-temporal message passing mechanisms. Both the mainstream time-then-space and time-and-space STGNN family discussed in Cini et al. (2023) can be seamlessly adopted simply by concatenating LLMGeovec into the input of each node."}, {"title": "Numeric Experiments", "content": "We study the effectiveness of LLMGeovec through extensive experiments. We first demonstrate that in geolocation representation models, LLMGeovec performs SoTA in GP tasks at all three scales: city, national, and global, and even outperforms end-to-end supervised training models. We examine two LLMs, LLaMa3 8B and Mistral 8x7B, both of which are able to produce high-quality geolocation representations with LLMGeovec. Moreover, LLMGeovec is seamlessly embedded into various LTSF and GSTF models and directly improves the model performance under various tasks. Notably, in the GSTF tasks, the use of a simple MLP and LLMGeovec outperforms many GNN-based approaches, and LLMGeovec shows great potential as an alternative to time-consuming GNNs. For a detailed description of the models and the datasets (GP, LTSF, GTSF), please refer to Appendix."}, {"title": "LLMGeovec for GP", "content": "To comprehensively validate the quality of LLMGeovec and its effectiveness in GP tasks, we constructed a multi-scale, multi-topic benchmark encompassing a range of scenarios from city-level poverty rates to global population density. Unlike many existing powerful baselines, our approach can generate high-quality geolocation representations for any location without expensive data or extensive training.\nA Multi-scale and Multi-topic GP Benchmark. As illustrated in Table 1, at the global scale, we collect 14 GP tasks. These include three climate indicators such as Annual Air Temperature and 11 social indicators like Population Density and Human Modification (detailed descriptions please refer to Appendix). We utilize 100,000 locations with global coverage, which are generated by Manvi et al. (2024) (Africa: 19,855; Asia: 55,893; Europe: 6,825; North America: 8,440; South America: 5,189; Oceania: 2,049). In line with Manvi et al. (2023, 2024), each GP task is associated with a corresponding GeoTIFF file. For each coordinate, the average value of 12 pixels surrounding the coordinate is taken as the value of the coordinate. Following the protocol of Kim and Yoon (2022), we perform five cross-validation using ridge linear regression implemented in Sklearn (Feurer et al. 2020; McDonald 2009), and the average of mean absolute error (MAE), root mean square error (RMSE) and R2 are reported. On the country and city scales, we use existing benchmarks, including the social indicators in India (Lee et al. 2021) and NYC (Zhou et al. 2023b). We also employ ridge linear regression in Sklearn and report the MAE, RMSE, and R2 on the test sets.\nBaselines. We compare the LLMGeovec generated by LLaMa3 8B (Touvron et al. 2023) and Mistral 8x7B (Jiang et al. 2023). As shown in Tab. 2, we also compare the text embedding generated with Bert-whitening (Su et al. 2021). In the city and country scales, we additionally compare Image-based and GNN-based geolocation representation models."}, {"title": "LLMGeovec for LTSF", "content": "In the previous section, we discussed how to seamlessly embed LLMGeovec into existing LTSF models. Next, we will conduct detailed experiments to verify the effectiveness of LLMGeovec using popular LTSF benchmarks and various models. Due to limited computational resources, we choose LLMGeovec (LLaMa3 8B) with the best performance in the GP to be added to the various models.\nDatasets and models. Following the settings of Zhang et al. (2024a); Wu et al. (2022); Shao et al. (2022a), we select five LTSF datasets from a wide range of domains, including Solar Energy, Global Wind, Global Temperature, Traffic flow, Delivery demand, and air quality. Several representative LTSF models are selected, including both Transformer-based and MLP-based methods. They are iTransformer (Liu et al. 2023b), TSMixer (Chen et al. 2023), RMLP (Li et al. 2023), and Informer (Zhou et al. 2021).\nHyperparameters Settings. We adapt the suggested hyperparameters in Time-Series-Library benchmark (Wang et al. 2024) for all model.\nPerformances of LTSF. As shown in Table 3, LLMGeovec can consistently improve the original performances of different models in almost all scenarios. This effect is noticeable in datasets related to both natural processes and human activities, which demonstrates the generality of LLM-based geolocation representation.\nPerformances Comparisions using different geolocation embeddings. In this section, we compare the effects of two geolocation representations on LTSF models, and for reference, we also add learnable embeddings (Shao et al. 2022b) (e.g., STID) with the same feature dimensions as LLMGeovec. For a fair comparison, all three methods use the same Adapter and model parameters. As shown in Tab. 7, LLMGeovec, which contains richer spatial semantics, achieves the greatest improvement."}, {"title": "LLMGeovec for GSTF", "content": "Finally, we evaluate the effectiveness of LLMGeovec in GSTF tasks and models. As with LTSF, we choose LLaMa3 8B to generate LLMGeovec.\nDatasets and models. We select the large-scale LargeST traffic flow benchmark (Liu et al. 2023a) and the LaDe demand dataset (Wu et al. 2023) for evaluations. Several competitive baselines that are widely adopted in related work are considered, including DCRNN (Li et al. 2017), STGCN (Yu, Yin, and Zhu 2017), ASTGCN (Guo et al. 2019), AGCRN (Bai et al. 2020), GWNET (Wu et al. 2019b), MTGNN (Wu et al. 2020b), and STID (Shao et al. 2022b).\nHyperparameters Settings. We adapt the suggested hyperparameters in LargeST benchmark (Liu et al. 2023a) for all models.\nmodels can benefit from the incorporation of LLMGeovec. This clearly shows that LLMGeovec is able to complement the spatial relationships captured by GNNs with the rich geographic knowledge of LLMs. Surprisingly, the vanilla MLP model equipped with LLMGeovec can achieve comparable performance to the GNN counterparts. This suggests that LLMGeovec can even be used as an alternative to GNNs to provide geographic correlation for temporal models.\nPerformances in zero-shot scenarios. In addition to enhancing various models in full training scenarios for GSTF, LLMGeovec also has the potential for enhancing zero-shot transfer. We compare the performance of the learnable node embedding (STID) introduced by Shao et al. (2022a) and LLMGeovec in zero-shot scenarios. As a reference, we also test the transferability of the baseline GWNET and MLP. The LaDe data is adopted for this experiment. It is evident from Fig. 2 that when models are transferred to a new region in a zero-shot scenario, the learnable embedding harms the performance of MLP significantly because the embedding has adapted to the source data with specific patterns. In contrast, universal LLMGeovec can be generalized to other regions without any adjustment. This indicates that LLMGeovec features intrinsic geolocation knowledge that is generalizable for different regions."}, {"title": "Conclusion and Future Work", "content": "The acquisition of universal geolocation representations to improve downstream tasks has been a long-standing pursuit. This paper presents our first attempt to utilize recent advanced LLMs to extract such representations. By the merit of the geospatial knowledge within LLMs, the extracted embedding from the pre-activated layer achieves global coverage and serves as a generic enhancer for spatio-temporal learning. We demonstrate the effectiveness of embedding in various tasks, including GP, LTSF, and GSTF. Empirical results indicate that LLMGeovec can improve the performances of various models simply by incorporating it into the model input (i.e., feature concatenation). In future work, we are interested to see if larger LLMs (e.g., LLaMa3 70B) can further improve the quality of LLMGeovec. We can adopt LLMGeovec in more challenging spatio-temporal learning tasks, such as spatio-temporal imputation (Nie et al. 2024b; Yuan et al. 2022) and traffic flow generation (Wu et al. 2020a). It is also interesting to explore the possibility of integrating LLM-Geovec into pre-trained foundational architectures for unified spatio-temporal learning tasks (Jin et al. 2023; Yuan et al. 2024; Zhang et al. 2024b)."}]}