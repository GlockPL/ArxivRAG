{"title": "THE CASE FOR CLEANER BIOSIGNALS: HIGH-FIDELITY NEURAL COMPRESSOR ENABLES TRANSFER FROM CLEANER IEEG TO NOISIER EEG", "authors": ["Francesco S. Carzaniga", "Gary Hoppeler", "Michael Hersche", "Kaspar A. Schindler", "Abbas Rahimi"], "abstract": "All data modalities are not created equal, even when the signal they measure comes from the same source. In the case of the brain, two of the most important data modalities are the scalp electroencephalogram (EEG), and the intracranial electroencephalogram (iEEG). iEEG benefits from a higher signal-to-noise ratio (SNR), as it measures the electrical activity directly in the brain, while EEG is noisier and has lower spatial and temporal resolutions. Nonetheless, both EEG and iEEG are important sources of data for human neurology, from healthcare to brain-machine interfaces. They are used by human experts, supported by deep learning (DL) models, to accomplish a variety of tasks, such as seizure detection and motor imagery classification. Although the differences between EEG and iEEG are well understood by human experts, the performance of DL models across these two modalities remains under-explored. To help characterize the importance of clean data on the performance of DL models, we propose BrainCodec, a high-fidelity EEG and iEEG neural compressor. We find that training BrainCodec on iEEG and then transferring to EEG yields higher reconstruction quality than training on EEG directly. In addition, we also find that training BrainCodec on both EEG and iEEG improves fidelity when reconstructing EEG. Our work indicates that data sources with higher SNR, such as iEEG, provide better performance across the board also in the medical time-series domain. This finding is consistent with reports coming from natural language processing, where clean data sources appear to have an outsized effect on the performance of the DL model overall. BrainCodec also achieves up to a 64\u00d7 compression on iEEG and EEG without a notable decrease in quality. BrainCodec markedly surpasses current state-of-the-art compression models both in final compression ratio and in reconstruction fidelity. We also evaluate the fidelity of the compressed signals objectively on a seizure detection and a motor imagery task performed by standard DL models. Here, we find that BrainCodec achieves a reconstruction fidelity high enough to ensure no performance degradation on the downstream tasks. Finally, we collect the subjective assessment of an expert neurologist, that confirms the high reconstruction quality of BrainCodec in a realistic scenario.", "sections": [{"title": "INTRODUCTION", "content": "Collecting high signal-to-noise ratio (SNR) data can prove to be a challenging endeavor in many situations, especially when considering human data. However, noisier signals are sometimes adequate to perform the task at hand. Following this principle, different data modalities can be collected from the same source with varying levels of quality. For instance, the electroencephalogram (EEG)"}, {"title": "RELATED WORK", "content": "Neural audio compression. Neural network models have recently started gaining popularity in the audio compression domain due to their high compression ratios and design flexibility. Most neural compression architectures consist of the encoder-decoder pair of an autoencoder, together with a quantizer to generate discrete representations. VQ-VAE introduced this method-unrelated to the compression objective-by combining variational autoencoders (VAE) with vector quantization (VQ). VQ uses a learnable codebook containing a discrete set of vectors to represent a larger set of input vectors.\nGANs have been shown to be an effective solution to drive the overall neural compressor towards better representations. MelGAN introduced a multi-scale discriminator that consists of three convolutional discriminators that operate on different scales of the waveform. This architecture restricts the discriminators to specific frequency bands so that they learn features of different scales. The learned features can be used to train a generator by minimizing the distance between features of real and synthetic data.\nCombining autoencoder, quantizer, and GAN, SoundStream and EnCodec represent the state-of-the-art audio compression models. They are based on a fully convolutional encoder-decoder network with a residual vector quantizer (RVQ) and a convolutional GAN discriminator, operating on the frequencies of interest of the signal. All components are jointly trained end-to-end by minimizing reconstruction, quantization, as well as perceptual adversarial losses.\nLossless EEG compression. Standard lossless compression algorithms such as gzip, zstd, and lz4 are used routinely to reduce the storage requirements of large EEG collections. Typical compression ratios for these algorithms on EEG are 1.2\u00d7 to 1.5\u00d7. Lossless compression models developed"}, {"title": "BRAINCODEC: QUANTIZED AUTOENCODER NEURAL COMPRESSOR", "content": "This section presents the main contribution of this work, the neural compressor BrainCodec.\nFirst, we outline a typical use case for BrainCodec. Consider an EEG signal $X \\in \\mathbb{R}^{C \\times T}$ having C channels and a duration $T = d \\times f_s$ of d seconds at a sampling frequency of $f_s$. First, each channel of the signal is fed separately to the BrainCodec encoder, which outputs a compressed representation for the given channel. The compressed signal can now be transmitted and stored at a fraction of the cost of the original signal. The BrainCodec decoder reconstructs the original EEG signal from the compressed representation, preserving the relevant information content and producing a high-fidelity result.\nWe now focus on the design of our neural compressor model, specifically adapted to EEG signals. We adopt the basic quantized autoencoder design of SoundStream and EnCodec, and tailor it to the EEG use case by modifying the loss function and the parameters of the architecture. The compressor consists of three components: an encoder, a quantizer, and a decoder. The encoder maps the EEG signal to a latent representation. The quantizer compresses this latent representation to a quantized representation using residual vector quantization (RVQ). Finally, the decoder reconstructs the signal from the RVQ output. We design the compressor to achieve high compression ratios while preserving the information content needed to perform classification on the signal. The model is trained end-to-end together with a discriminator that learns multi-scale features of the input data. We apply multiple losses over both the time and frequency domain to capture different properties of the signal. This allows our compressor to be used in end-to-end classification pipelines as we show in our seizure detection and motor imagery results, providing significant storage savings.\nEncoder and decoder. First, we divide the EEG signal (X) channel-wise into short patches $x_{i,j} \\in \\mathbb{R}^W$, $i \\in \\{1,...,C\\}$, $j \\in \\{1,...,\\frac{T}{W}\\}$, $W = T_W$, with the patch size W in the order of a few seconds. In particular, we choose W to be 4 seconds long at the signal sampling frequency. The patches serve as the input to the encoder. The encoder is composed of a 2D convolutional layer with 1 input channel, F output channels, and a kernel size of (3,1). This ensures that each channel is treated separately, and allows the encoder to work on signals with varying number of channels. The initial layer is followed by N encoder blocks, where N depends on the compression ratio. Each encoder block comprises a residual block as well as a 2D convolutional layer with a kernel size of (K, 1) and a stride of (S, 1) for down-sampling, with K twice the size of S. The number of channels is doubled with each down-sampling layer until there are 256. The encoder blocks are followed by a final 2D convolutional layer with D output channels and a kernel size of (3, 1). We choose F = 16, S = 2, D = 64, and ELU as the activation function. The encoder finally outputs a latent representation $z_{i,j}$ for each input patch. The decoder mirrors the encoder, replacing strided convolutions with transposed convolutions.\nQuantizer. We quantize the latent representation (z) to a compressed representation (zq) through RVQ. A codebook stores a finite set of learnable prototype vectors that are used to represent a larger set of input vectors. When compressing z, the quantizer maps the input vector to the closest pro-"}, {"title": "TRAINING SETUP", "content": "We train BrainCodec following the schema of SoundStream. To help guide BrainCodec towards reconstructed signals apt for downstream classification, we also add a new loss based on the line length, which is widely considered to be a useful feature for EEG classification. We observe that the GAN has a significant effect on the reconstruction fidelity of the signal (see App. C.3); therefore, we train one model with GAN and one without. To avoid the risk of cross contamination between the subjects, we always train the model on one single subject, and test the reconstruction on the remaining subjects. To provide a direct baseline for the performance of BrainCodec in the seizure detection task, we also train a standard EEGWaveNet on the original signal, and then test it using the reconstructed signal.\nValidation. We also provide objective metrics of the reconstruction performance of our BrainCodec in two downstream tasks: the iEEG seizure detection task, and the EEG motor imagery classification task. In particular, for iEEG we train a standard EEGWaveNet classifier on the original signal, and evaluate its performance on the reconstructed signal. For EEG, we train an MI-BMInet classifier on the original signal, and evaluate its performance on the reconstructed signal. For more information on the detailed training regime of BrainCodec refer to App. A.\nOptimizer and setup. We apply the 1-cycle learning rate policy, with a learning rate varying from $10^{-5}$ to $10^{-4}$ for the generator and from $10^{-7}$ to $10^{-6}$ for the discriminator. We further use the weights $\\lambda_t = 1$ and $\\lambda_q = 1$ for the base model. For the GAN model, we choose $\\lambda_t = 0.1$, $\\lambda_s = 1$, $\\lambda_l = 0.1$, $\\lambda_f = 3$, $\\lambda_g = 3$, and $\\lambda_q = 1$. The model is trained with full fp32 precision."}, {"title": "DATASETS", "content": "SWEC IEEG. This short-term iEEG dataset contains 15 subjects, 14 hours of recording, and 104 ictal events. The iEEG signals were recorded intracranially with a sampling rate of either 512 Hz or 1024 Hz. The signals were median-referenced and band-pass filtered between 0.5 and 120 Hz using a fourth-order Butterworth filter, both in a forward and backward pass. All the recordings were inspected by an expert neurologist for identification of seizure onsets and offsets, and to remove channels corrupted by artifacts."}, {"title": "BASELINES", "content": "We compare the performance of BrainCodec with multiple state-of-the-art lossy compression algorithms that have been developed for EEG. We benchmark against the following methods: NL-SPIHT, a classical algorithm; ANN, a deep-learning model; AAC, a wavelet-based model with adaptive arithmetic coding; and CS, a compressed-sensing based deep learning model.\nTo showcase the adaptability of BrainCodec, we test it on three iEEG datasets (SWEC, MC, Treebank), which have a high SNR, and also on multiple EEG datasets (CHB-MIT, BONN, and BCI IV-2a), which are noisier. Finally, we evaluate the generalization capabilities of BrainCodec across datasets and modalities. The full comparison with all the baselines is shown in App. D."}, {"title": "RESULTS", "content": "To evaluate the compression performance in terms of reconstruction fidelity, we report the percentage root-mean-square distortion (PRD):\n$${\\rm PRD} = \\frac{||x-\\hat{x}||_2}{||x||_2} 100,$$ which represents the relative $L_2$-distance between the original and the reconstructed signal. In order to have a comprehensive outlook, we train multiple models at varying compression ratios."}, {"title": "BRAINCODEC CROSS-MODAL COMPRESSION", "content": "Reports from other fields, especially natural language processing, have shown that training with higher-quality data often yields better performance than simply training with more, and more similar, data. We aim to characterise this phenomenon for human iEEG and EEG, where, due to its higher SNR, we consider iEEG to be of higher quality than EEG in the signal processing domain.\nTo evaluate the role of high-SNR data in human EEG signals, we train an instance of BrainCodec on the SWEC iEEG dataset and use it to compress EEG signals. The EEG-trained compressor performs slightly better at lower compression ratios, while the iEEG-trained model becomes competitive and even achieves higher performance at higher compression ratios. Aside from the pure compression advantage, variational autoencoders have been shown to produce bet-"}, {"title": "BRAINCODEC MIXED-MODAL COMPRESSION", "content": "Given the promising results shown by BrainCodec when transferring across modalities, we now investigate the performance of our neural compressor when trained with both modalities \u2014 scalp EEG and intracranial EEG \u2014 at the same time.\nTo evaluate the effect of mixed-modal compression, we train BrainCodec on both the SWEC iEEG dataset and the CHB EEG dataset, for the same overall amount of data as the previous models to keep the evaluation balanced. Figure 3a shows that the median PRD of our mixed model is notably superior to the EEG-only model when compressing EEG signals, indicating that the performance benefits of iEEG training have transferred successfully. In line with the previous cross-modal results, this improvement is more marked at high compression ratios. At the same time, the mixed model also performs on par with the EEG-only model at lower compression ratios, mitigating the drawback we had reported in the previous cross-modal results.\nOn the other hand, we also test mixed BrainCodec on iEEG recordings. In this case, we do not observe any benefit of mixed-modal training in reconstruction fidelity. However, we also do not observe any notable reduction in performance."}, {"title": "BRAINCODEC COMPRESSION PERFORMANCE", "content": "Next, we train BrainCodec exclusively on iEEG data to compress other iEEG datasets. Likewise, we train BrainCodec on EEG data to compress other EEG datasets, ensuring compression is performed within the same modality. We then compare the results with baseline methods."}, {"title": "DOWNSTREAM CLASSIFICATION TASKS", "content": "As another objective measurement of reconstruction quality, we validate the reconstruction fidelity on two downstream classification tasks: the iEEG seizure detection task, and the EEG motor imagery task.\nFirst, we evaluate BrainCodec on the iEEG seizure detection task, by testing a subject-specific EEG-WaveNet classifier on the reconstructed signal. We test the EEG-WaveNet across all subjects with a leave-one-out cross-validation scheme, training for each subject on all seizures but one and testing on the remaining seizure. Even at a 64\u00d7 compression, there is no loss of performance in the seizure detection task. Moreover, the F1-score of the BrainCodec GAN model degrades only by 8% at 256\u00d7 compression, likely due to its better reconstruction of the higher frequencies with respect to the Base model (see App. C.3). Thus, the relevant information content is preserved by BrainCodec while providing significant storage and transmission savings.\nSecond, we evaluate BrainCodec on the EEG motor imagery task using the MI-BMInet classifier. The training and testing setup is analogous to the seizure detection task. Specif-"}, {"title": "DISCUSSION", "content": "In this work, we present BrainCodec, a high-fidelity neural compressor for EEG and iEEG signals. BrainCodec is effective across both modalities and a variety of datasets, indicating that it can successfully replace existing methods and be introduced in any EEG processing pipeline. Compression by BrainCodec up to 64\u00d7 does not affect downstream seizure detection performance as evaluated both by human experts and deep learning models. We also observe that BrainCodec performs better when trained with high SNR iEEG. This performance increase is maintained when compressing the noisier EEG signal, compared to the same model trained on the very same EEG modality. We therefore highlight the importance of training deep learning models on high-quality signals also in the medical domain.\nOverall, BrainCodec is an immediate compression replacement for many EEG and iEEG applications, enabling transmission and storage cost savings in critical clinical environments. We expect the adoption of BrainCodec to increase the feasibility of long-term recordings and wearable devices.\nFurther work is necessary to assess whether the intermediate representations of BrainCodec can be directly used by other deep learning models, to increase performance and provide an additional speed-up. Architectural changes to BrainCodec could be made, for example by utilising decoder models specifically developed for biosignals. Moreover, the RVQ quantization schema of BrainCodec is known to be not necessarily codebook efficient, and improvements are already being developed in the field . On this front, more venues are being explored to replace RVQ with another quantization schema, such as Finite Scalar Quantiza-"}, {"title": "NEURAL COMPRESSOR DETAILS", "content": "We apply multiple losses to capture properties of both the time and frequency domain. The reconstruction loss over the time domain $l_t$ measures the $L_1$-distance between the original signal (x) and the reconstructed signal ($\\hat{x}$):\n$$l_t(x, \\hat{x}) = ||x - \\hat{x}||_1.$$ The reconstruction loss over the frequency domain $l_s$ consists of a linear combination between $L_1$- and $L_2$-distance computed over different scales of the spectrogram. In particular,\n$$l_s(x, \\hat{x}) = \\frac{1}{\\|I\\|} \\sum_{i \\in I} S_i(\\hat{x}) - (||S_i(x) - S_i(\\hat{x})||_1 + \\alpha ||S_i(x) - S_i(\\hat{x})||_2),$$ where $S_i(\\cdot)$ is the output of an STFT with a window size of $2^i$ and a hop length of $2^{i-2}$. We choose $I = \\{5, 6, ..., 11\\}$ and $\\alpha = 1$.\nWe further introduce a new relative line length loss that accounts for large differences in amplitude. Often the amplitude of a seizure sample is much larger than the amplitude of a non-seizure sample. When reconstructing such a seizure sample, a relatively small deviation can cause a large $L_1$- or $L_2$-distance. In these cases, the weight updates are mainly driven by seizure samples, since they contribute much more to the overall loss than non-seizure samples. To address this imbalance, we compute the relative difference in line length of the original and reconstructed signal, which is independent of the amplitude. Formally,\n$$l_l(x, \\hat{x}) = \\sum_{w=0}^{T_W-1} \\sum_{t=1}^{\\frac{W}{S}} \\frac{|x_{t+wS} - x_{t-1+wS}| - |\\hat{x}_{t+wS} - \\hat{x}_{t-1+wS}|}{|x_{t+wS} - x_{t-1+wS}|}$$\nwith $T_W$ windows of size W and stride S. We choose T = 128 and S = 64.\nTo improve the reconstruction of high frequencies, we apply perceptual losses that are based on the MS-STFT discriminator. The discriminator network is trained by minimizing the adversarial loss of the generator $l_g$ as well as the adversarial loss of the discriminator $l_d$. The two losses are defined as follows:\n$$l_g(\\hat{x}) = \\sum_{k=1}^{K} {\\rm mean}(max(0, 1 - D_k(\\hat{x}))),$$ $$l_d(x, \\hat{x}) = \\sum_{k=1}^{K} {\\rm mean}(max(0, 1 - D_k(x))) + {\\rm mean}(max(0, 1 + D_k(\\hat{x}))),$$ where K is the number of discriminators and $D_k$ is the output of the respective discriminator.\nAt the same time, we exploit the multi-scale features learned by the discriminators to compute a feature loss,\n$$l_f(x, \\hat{x}) = \\sum_{k=1}^{K} \\sum_{l=1}^{L} \\frac{||D_k^l(x) - D_k^l(\\hat{x})||_1}{||D_k^l(x)||_1}$$\nwhere K is the number of discriminators, L is the number of layers of a discriminator, and $D_k^l$ is the output of the respective layer.\nWe further add a quantization loss $l_q$ that computes the MSE between the latent representation (z) and its quantized version (zq):\n$$l_q (z, z_q) = \\sum_{c=1}^{C} ||z_c - (z_q)_c||_2,$$"}, {"title": "TRAINING SCHEMA", "content": "For the iEEG compression task, we use the recordings from subject ID1, which have 47 channels for a total duration of 5604 seconds, and contain 13 seizures. We randomly sample 80% of that data for training and use the remaining 20% for validation. For the seizure detection task, we exclude ID1 from the dataset and use the recordings from the remaining 15 subjects of the database.\nFor the leave-one-out cross-validation, we train N classifiers per subject, where N is the number of seizures of the respective subject. We use the original recordings of N \u2013 1 seizures for training, and the reconstructed recording of the remaining seizure for validation. The performance for a subject is the average across N trials. The final performance is the average across all subjects, weighted by the number of seizures. All classifiers are trained for 50 epochs with a learning rate of $3\\cdot 10^{-4}$. We use samples of 5 seconds and a batch size of 128."}, {"title": "ABLATIONS", "content": "The reference schema is an often underappreciated aspect of data collection. To evaluate how BrainCodec's performance varies with different references, we use the unprocessed Brain Treebank dataset and reference it in post-processing. Laplacian refers to referencing each electrode with the mean within its electrode group."}]}