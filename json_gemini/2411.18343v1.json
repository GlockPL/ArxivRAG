{"title": "FreqX: What neural networks learn is what network designers say.", "authors": ["Zechen Liu"], "abstract": "Personalized Federal learning(PFL) allows clients to cooperatively train a personalized model without disclosing their private dataset. However, PFL suffers from Non-IID, heterogeneous devices, lack of fairness, and unclear contribution which urgently need the interpretability of deep learning model to overcome these challenges. These challenges proposed new demands for interpretability. Low cost, privacy, and detailed information. There is no current interpretability method satisfying them. In this paper, we propose a novel interpretability method FreqX by introducing Signal Processing and Information Theory. Our experiments show that the explanation results of FreqX contain both attribution information and concept information. FreqX runs at least 10 times faster than the baselines which contain concept information.", "sections": [{"title": "Introduction", "content": "Personalized Federated Learning(PFL) enables different data owners to cooperatively train a personalized model without sharing their dataset. The regular paradigm is that clients train a local model using their own data and local devices, and then send their trained models to a center server. The server aggregates their models and sends the aggregated model back. Clients and the server repeat these operations until the model convergesMcMahan et al. [2017]. However, this causes PFL to suffer from numeric problems such as Non-IIDLi et al. [2020], personalizationLi et al. [2021a], device heterogeneousXu et al. [2023], Li et al. [2023], lack of fairnessRafi et al. [2024]. These problems urge the researchers to efficiently provide a good explanation for DNNs in PFL."}, {"title": "", "content": "Without an efficient interpretability method, researchers now are adopting re-training Zhang et al. [2023] or alignment-basedZhu et al. [2021], Chen et al. [2023], Li et al. [2021a] methods to extract the desired information from the aggregated model. Lack of interpretability blocks them from designing more explainable and economical algorithms. For fairness FL, especially collaboration fairness, researchers categorize the clients into groupsChaudhury et al. [2022], Rafi et al. [2024] and do retraining to estimate the contribution of each client or use individualized assessment. These methods are costly(such as the Shapley-Value based method with a complexity of O (n!)) or unreality(The basic assumptions are hard to achieve in reality)Rafi et al. [2024].\nSo if we can reveal the content that deep networks have learned in PFL, we can then vigorously advance the resolution of the aforementioned issues. However, PFL raises new limitations for explanation methods: 1) Due to the heterogeneity of the local devices, the cost of the explanation method should be as low as possible to avoid sampling bias or long time waitingXu et al. [2023]. Additionally, the cost will accumulate if an explanation method needs to be performed at each round of aggregation. 2)The explanation results should contain detailed information of what the network learned. Because we need to calculate the contributions of participants. 3)The explanation method should not rely on the knowledge of the entire dataset. Because the training data is separated into different clients and the data can not be shared, each client has little prior knowledge. Although the interpretability of neural networks has been deeply studied, there are still deficiencies in PFL. Neither feature attribution-based methodsSrinivas and Fleuret [2019] nor concept-based methodsFel et al. [2023], Sun et al. [2023], which are the mainstream explanation methods. Feature attribution is poor of detailed information of what the network learned (limitation2) and the other relies on prior knowledge of the entire dataset(limitation3).\nMotivated by these issues, and inspired by signal processingWright et al. [2022], Jianxun Zhao [2018], information bottleneck theory Alemi et al. [2017], Kawaguchi et al. [2023], prototype learningQin et al. [2023a,b] and previous worksLey et al. [2023], Smith [1997], Konforti et al. [2023], we propose a new method called FreqX which satisfies the limitations by explaining how a network classifies the samples layer by layer.\nThere is another significant issue in the interpretability field, the metrics. The two major metrics, Del-Ins Game and Remove And Retrain(ROAR) are both criticized for their different weakness. The ROAR is criticized because it needs to retrain the model, some researchers argueYang et al. [2023a] that it is not faithful. Del-Ins Game will introduce new edge signals into the picture which will frustrate the modelSrinivas and Fleuret [2019]. That's true. See Fig.2 as an example, We start with a green square, and then we remove the pixels from the corners of the square we can get a triangle. What's more, by removing pixels we can also get a little person who is playing basketball! Therefore, the deletion operation in the time domain is not real deletion. It may introduce new information that frustrates the network. We can see in the frequency domain, that the deletion operation introduces new signals with different frequencies into the picture. So, inspired by Fourier transformation, we turn the deletion operation into the frequency domain."}, {"title": "", "content": "a new measure called FreqDIG(Frequency Deletion and Insertion Game) whose deletion is a real deletion that will not introduce extra signals like time domain. We transform the interpretability methods in the time domain into the frequency domain with the equation below:\n$S_f(u, v) = \\sum_x\\sum_y S_t(x,y) e^{-j2\\pi(\\frac{xu}{N} + \\frac{yv}{N})}$ (1)\nWhere $S_f$ means the score of frequency, $S_t$ means the score in the time domain. After transforming the scores into the frequency domain, We observed that the feature frequencies and noise frequencies differed greatly in their effects on the models. As shown in Fig.3 and Fig.6 In the time domain, we need to set more than 30% pixels(which are regarded as important pixels by the interpretability algorithms) to zero to decrease the model performance. However, in the frequency domain, deleting 10% important frequencies will completely change the network's prediction. (The network here is ResNet-50). In contrast, deleting the noise frequency will slightly change the network output, and even increase the confidence of the original judgment. In the third figure, even if only 10 frequencies are deleted, the network's reaction is changed significantly. Therefore, we draw the conclusion that the network is sensitive to frequency. More arbitrarily, the CNN network learned frequency information."}, {"title": "", "content": "Therefore, we turn the key point of interpretability from the time domain into the frequency domain.\nIt is demonstrated that classification networks cluster the samples in hyper-spaceKonforti et al. [2023], and information bottleneck theory demonstrated that the networks can filter the noise and extract the important information. To the best of our knowledge, we are the first team to reveal the filter and extract operations in the original sample space which is more friendly for humans to understand.\nAs shown in Fig.1(a), the network gradually separates two classes by allocating two contrast vectors. As shown in Fig.1(b), after the transformation operations are mapped into the original space, we can see ResNet-50 maps similar concepts to nearby spaces such as \u201criver\u201d(labeled as 1,2), \u201csky\u201d(labeled as 3,4), \"ground\"(labeled as 5,6), and \"grass\" (labeled as 7,8). Based on the transformation, we can turn our explanation result into the feature-attribution-based method. Additionally, the transformation contains concept information. FreqX extracts both two types of information within only 10 minutes.\nOur contributions can be summarized as follows:\n\u2022 We propose a novel explanation method that satisfies the limitations of PFL. To the best of our knowledge, we are the first team that can map the extract/filter operations of a network to the original sample space.\n\u2022 We demonstrate that the proposed method can be converted to existing feature attribution methods and our explanation result contains concept information\n\u2022 We demonstrate that our method can be aggregated into global explanations and applied in PFL."}, {"title": "Related Works", "content": null}, {"title": "Personalized Federated Learning", "content": "Personalized federated learning is challenging because the participants often have different situations including different distributionGao et al. [2022] and computational resourcesXu et al. [2023]. Current methods to handle distribution heterogeneity are to align clients' distributions by adding additional modules such as Batch Normal-izationLi et al. [2021a], Mills et al. [2022], local model or local parametersPillutla et al. [2022], regular term in loss functionHanzely et al. [2020], Li and Zhan [2021]. Or even re-train the model to remove the heterogeneous effectsZhang et al. [2023], Chen et al. [2023], Zhu et al. [2021]. All of them are trying to find the desired/important information that a client needs/provides and remove the noise/unwanted information. However, the lack of interpretability blocks the progress of PFL. Not only the efficiency but also the business model. People find it hard to measure clients' contributions. This makes it difficult to allocate the benefits of PFL outcomesZhou et al. [2021]."}, {"title": "Interpretability of Deep Neural Network", "content": "There are two ways to categorize the interpretability methods. 1) The local explanation and the global explanation. Local explanation focuses on the behavior of a neural network towards an individual sample while global explanation gives an overview of a network's knowledgeAchtibat et al. [2023]. But local explanations can be aggregated to a global explanationMor et al. [2024]. So we propose a local explanation method. 2) Feature-attribution-based and Concept-based explanation. Feature-attribution-based methods focus on \"where\" influences the network instead of \"what\". The most popular feature attribution methods are to generate a saliency map that highlights the relevant features/pixelsSundararajan et al. [2017], Yang et al. [2023a], Smilkov et al. [2017], Shrikumar et al. [2017], Selvaraju et al. [2017], Srinivas and Fleuret [2019], Bhalla et al. [2023], Simonyan et al. [2014], Yang et al. [2023b], Erion et al. [2021]. It is far from enough for PFL. Concept-based methods can provide detailed information but they need a pre-defined concept setKim et al. [2018] or extract the concept from the entire datasetFel et al. [2023], Ghorbani et al. [2019]. Additionally, their costs are highFel et al. [2023], Sun et al. [2023]. These shortcomings make current concept-based methods unsuitable for PFL due to the distributed privacy datasets and the uneven computational resources."}, {"title": "Method", "content": "We first model the neurons in the frequency domain and then propose our formula."}, {"title": "Neuron In Frequency Domain", "content": "Consider a classification dataset D with c classes as an example. We assume that the samples $x_{ci}$ in dataset all has d features and denoted as $x_{ci} = (to, t_1,..., t_j)$. Additionally, we assume that every sample is generated by signal generator $f_c (\\cdot)$. We introduce a basic assumption from generative modelsKingma and Welling [2014], that for every class of samples, they follow a certain distribution. Therefore, We consider each sample as a wave in the time/space domain, consisting of typical samples (those with the highest confidence) plus some noise. Then $X_{ci}$ is replaced with $f_c (t, i)$."}, {"title": "", "content": "With this view, the neuron operations are matched to time-series signal processing. We transform the neuron outputs into the frequency domain:\n$p = \\sigma(\\sum_t x(t) y (t) + B)$ (2)\n$= \\sigma(\\sum_t (\\sum_k X(k)e^{j\\frac{2\\pi kt}{N}}) (\\sum_k Y(k)e^{j\\frac{2\\pi kt}{N}})+B)$ (3)\n$= \\sigma(\\sum_k X (k) Y^* (k) + B)$ (4)\nwhere k means the k \u2013 th frequency. Notice that $X (k) Y^* (k)$ is the mutual energy of two signals. Since the Fourier transform of the real number field is conjugate symmetric, therefore\n$E_{ky} = E_{ky}^*$ (5)\nwhere $E_{xy}$ denotes the mutual energy of the k-th frequency signal. Further, we can give the information-theoretic perspective condition for neuronal activation.\n$\\sum_k E_{ky} + B > 0$ (6)\n$2\\sum_k E_{kxy} + 2B > 0$ (7)\n$\\sum_k E_f^i + \\sum_k E_n^i +2\\sum_k E_{kxy}- 2B > \\sum_k E_f^i + \\sum_k E_n^i$ (8)\nWe define the $E_{ky} < 0$ as noise since it decreases the reaction of the neuron and we denote them as $E_{ny}$. In contrast, we define the $E_{ky} > 0$ as a feature signal and we denote them as $E_{fy}$. We separate them into the two sides of the inequality sign.\n$\\sum_f (E_f^i + E_f^i + 2E_f) + \\sum_n (E_n^i + E_n^i + 2E_n) + 2B > \\sum_f (E_f^i +E_f^i) + \\sum_n (E_n^i + E_n^i)$ (9)\n$\\frac{\\sum(\\sum E_{f xy} + E_f^i + 2 E_f) + \\sum(\\sum E_{n xy} + E_n^i + 2E_n) + 2B}{\\sum(E_f^i + E_n^i)} > \\frac{\\sum(E_f^i + E_n^i)}{\\sum(E_f^i + E_n^i)}$ (10)\n$SNR+1 + \\frac{2B}{\\sum E_n} > \\frac{E_x + E_y}{E_f}$ (11)\n$ SNR> \\frac{E_x + E_y}{\\sum E_n}- 2B -1$ (12)\nWhere SNR denotes the signal-to-noise ratio which is an essential property used in signal processing and information theory."}, {"title": "", "content": "Theorem 0.1 In an unbiased neuron, or a neuron with a negative bias or the bias is regarded as a feature with value 1, if the neuron is activated, the SNR after the compounding of signals is larger than the original ones.\nProof 0.2 According to the definition of mutual energy,\n$E_{xy} < 0$ (13)\n$E_f > 0$ (14)\n$\\sqrt{E_fE_y} > E_{xy}$ (15)\nAnd since energy is larger than 0.\n$E_x+ E_y > 2\\sqrt{E_xEy} > 2 E_{xy}$ (16)"}, {"title": "SNR and Neuron", "content": "In classification tasks, the outputs from the same class become more and more similar as layers go deeperKonforti et al. [2023]. And the prior worksZhou et al. [2018], Sun et al. [2023] demonstrated that the samples with the same label have similar concepts. The neural network will act like an information selector that filters the unimportant informationKawaguchi et al. [2023]. If we regard each layer of neurons as a whole before the output goes through the amplitude limiter. The neurons are performing projective coordinate transformation. As prior work has observedKonforti et al. [2023], the samples with the same label have closer metrics in transformed space. So we define benchmark transformation according to the observation in Konforti et al. [2023]:\nDefinition 1: If the norm of vectors remain unchanged after transformation, we call this transformation benchmark transformation(It is also called Isometry).\nWith benchmark transformation, we define extract and filter operations which combine information bottleneck theory with spatial transformation:\nDefinition 2: If the norm of a vector is smaller/larger than the vector in benchmark transformation, we call this vector filtered/extracted. It is proven thatMoh [2020] M' with all vectors are unit vectors is a benchmark transformation. So, we compute filter/extract degree as the equations below:\n$deg = ||V_{trans} || - ||V_{origin}||\\= (v_{ori}^T M) - (v_{ori}^{T} M') (v_{ori}^T M')$ (20)\nWhen deg 0, it means this vector is extracted and otherwise filtered. Then we extend the definition to projection."}, {"title": "", "content": "Definition 3: If the project of a vector $v_1$ on $v_2$ is smallerlarger than the corresponding project on $\\frac{v_1^T v_2}{\\|v_2\\|}$, we call this vector is filtered/extracted at the direct $v_2$. We compute filter/extract degree as the equations below:\n$deg = v_1^T v_2 - \\frac{v_1^T v_2}{\\|v_2\\|}$ (21)\nNow we introduce the previous conclusion of activation functions into extract/filter definition. We can define the extract/filter operation of a neuron.\nWe first transform the weights of a neuron layer into benchmark transformation by dividing corresponding norms. The extract/filter operation of a neuron to a sample x can be written as(we regard bias as a feature with value 1):\n$deg = \\sigma (x^T w) - x^T \\frac{w}{\\|w\\|}$ (22)\n$x' = x + \\varepsilon \\cdot deg. w$ (23)\nThis means the vector w of which neuron is responsible is extracted/filtered. We add the degree to the original sample to reveal how the network transforms the sample. The & is the augmentation coefficient for downstream tasks such as generating more explicit figures in Fig.1. We adjust it according to the task requirements.\nIf there are n neurons in a layer, we calculate their mean:\n$x' = (\\sum x') /n$ (24)\nWe regard each layer's transformed x' as the outputs of the previous layer. So we can get the spatial trans-formation in the original space as shown in Fig.4 by performing the transformation from the output layer to the input layer. Fig.4(a) shows that similar concepts are transformed into nearby spaces such as \"water(blue)\", \"grass(green)\u201d and \u201cground(yellow)\". As shown in line 3, the ResNet-50 has learned how to extract the shapes like specialized segment networks. Prior workShotton et al. [2009], Li et al. [2021b] also demonstrated that shape is one of the most important decision factors. Fig.4(b) shows that no matter what shapes the original samples are, MLP will gradually transform them into a similar shape. For pooling layer or other special layer, see Supplementary for details."}, {"title": "Experiments", "content": "In this section, we conduct 3 experiments to demonstrate that our proposal satisfies the limitations and conduct one experiment to demonstrate that our method can be adopted in FL and can be aggregated into a global explanation. The implementation details are contained in Supplementary."}, {"title": "Functionality and Cost Comparison", "content": "In this part, we compared our method to current mainstream interpretability methods in terms of costs and functionalities to demonstrate that FreqX satisfies the limitation1 and limitation3. We set the batch size as 128 for all algorithms unless they exceed the capability of GPU.\nAs shown in Table.2. Current concept-based methods both have an extremely large time cost to explain the whole dataset. Additionally, they are based on the entire dataset to extract concept sets or auxiliary models/-training to separate concepts from the picture. Both of these two ways are costly and unsuitable for PFL. As for feature attribution methods, they can only find important areas. Some of them require even larger space than concept-based methods. And most of them need over 1000 seconds. Therefore, our method is more suitable for PFL because of its comprehensive functionalities and lower costs.(limitation1 and limitation3)"}, {"title": "Faithfulness Analysis", "content": "This experiment aims to demonstrate that our method can be transformed into a feature attribution method and to analyze the faithfulness of the method.\nMetrics: The metrics of faithfulness are still ongoing Yang et al. [2023a], Deng et al. [2024]. We adopt one of the earliest and most popular metrics which has been used until now. Deletion and insertion game measureSrinivas and Fleuret [2019]. This measure originates from game theory with the assumption that if we remove the most important part of the networks' decision-making basis, the output of the networks will change significantly. However, Srinivas and Fleuret [2019] argued that removing the most important pixels will create artifacts that puzzle networks. They suggested removing the least important pixels. To comprehensively evaluate the methods, we perform both versions: delete least important pixel and delete most important pixel. We evaluate the algorithms by removing the top/least K% important pixels. In the top K% part, the lower the curve is the better, while in the least K% part, the higher the better.\nAccording to the spatial transformation, we find out that the important and unimportant pixels have different transformation properties. Based on this we design our algorithm to extract important pixels. To show the faith-"}, {"title": "Concept and Parameter Analysis", "content": "To the best of our knowledge, NO CURRENT concept-based method is affordable for PFL. Therefore, this experiment focuses on demonstrating the advantages compared to the attribution-based methods(limitation2).\nWe design this experiment with the following assumption:\nThe current concept-based algorithms can extract concepts with a reasonable degree of accuracy. If our method can extract concepts too, we should have more similar results than those that cannot.\nBased on this, we choose CRAFTFel et al. [2023] as our benchmark concept-based method. CRAFT first separates the pictures into several pieces and regards them as pre-defined concepts. Then it clusters the concepts into groups and calculates their importance. For more details, turn to their paper. We separate the pictures in the same way. Details are displayed in the supplementary.\nThis experiment aims to demonstrate that spatial transformation contains concept information rather than strike for accuracy. We perform the K-means cluster algorithm directly in original sample space. We repeat the clustering 5 times and record their maximum and average values. After clustering the extracted concepts, we rank their importance by the algorithms we used in faithfulness analysis. Then we compare the results to CRAFT.\nWe set two methods $g_1$, $g_2$ for ablation analysis to demonstrate that spatial transformation does extract the concepts. g1 does not perform the spatial transformation algorithm but it calculates the importance scores. And $g_2$ randomly selects the top concepts.\nMetrics: All methods extract ten groups of concepts. We rank the groups according to their importance value. We select the top ten concepts of each group to compare their similarity. We design three measures. 1) The number N of overlapping concepts of the corresponding group. 2) The number M of overlapping concepts of all selected concepts. 3) The hit rate N/M. We repeat the experiment 15 times and each time we randomly"}, {"title": "Effectiveness", "content": "To demonstrate that our interpretability method can be aggregated into a global explanation and adopted into federated learning, we conduct a classic vertical federated learning scenario to calculate the contribution of"}]}