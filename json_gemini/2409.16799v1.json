{"title": "Large Language Model Predicts Above Normal All\nIndia Summer Monsoon Rainfall in 2024", "authors": ["Ujjawal Sharma", "Madhav Biyani", "Akhil Dev Suresh", "Debi Prasad Bhuyan", "Saroj Kanta Mishra", "Tanmoy Chakraborty"], "abstract": "Reliable prediction of the All India Summer Monsoon Rainfall (AISMR) is pivotal for\ninformed policymaking for the country, impacting the lives of billions of people. However,\naccurate simulation of AISMR has been a persistent challenge due to the complex interplay\nof various muti-scale factors and the inherent variability of the monsoon system. This\nresearch focuses on adapting and fine-tuning the latest LLM model, PatchTST, to accurately\npredict AISMR with a lead time of three months. The fine-tuned PatchTST model, trained\nwith historical AISMR data, the Ni\u00f1o3.4 index, and categorical Indian Ocean Dipole values,\noutperforms several popular neural network (NN) models and statistical models. This\nfine-tuned LLM model exhibits an exceptionally low RMSE percentage of 0.07% and a\nSpearman correlation of 0.976. This is particularly impressive, since it is nearly 80% more\naccurate than the best-performing NN models (here, CNN). The model predicts an\nabove-normal monsoon for the year 2024, with an accumulated rainfall of 921.6 mm in the\nmonth of June-September for the entire country.", "sections": [{"title": "Introduction", "content": "The South Asian Summer Monsoon (SASM) is considered a lifeline for the Indian\nsubcontinent\u00b9, contributing approximately 75% of India's annual rainfall and supporting the\nwater demands of billions of people. It plays a crucial role in multiple sectors, including\nagricultural planning, hydro-power generation, industrial expansion, etc. This makes accurate\nand unbiased seasonal monsoon rainfall prediction essential for effective planning across\nvarious sectors in India\u00b2. Although the contribution of agriculture to the country's GDP has\nshrunk from approximately 35% to 15% over the past three decades, a large chunk of\nagricultural activities are still strongly tied to the precipitation cycle during the summer\nmonsoon season\u00b3. Therefore, enhancing the precision of monsoon forecasts can significantly\nimprove resource management and bolster the region's economic stability.\nThe Indian Summer Monsoon Rainfall (ISMR) exhibits a large inter-annual variability of\n~10%, with large spatial variability in the precipitation distributions from year-to-year, due to\nits linkage with many local factors and remote coupled ocean-atmospheric processes,\ninfluencing the ISMR largely in a wide range of timescales, from intra-seasonal to decadal\u2074.\nApart from these natural variabilities in ISMR, a vast body of literature suggests that the\nwarming climate, anthropogenic emissions, and changes in land-use-land-cover contribute to\nincreased monsoon variability5,6. These changes make the ISMR highly non-linear and\nextremely challenging to understand and make accurate predictions. The resulting variability\nin ISMR impacts billions of people and their livelihoods, with profound implications for the\ncountry's GDP7.\nSubstantial advancement has been made in the long-range prediction of ISMR by the Indian\nMeteorological Department (IMD), starting from a simple statistical model with a small\nnumber of predictors to the involvement of both statistical models with many potential\npredictors of the summer monsoon and the ensemble mean of state-of-the-art (SOTA)\ncoupled models in recent days. Many studies have highlighted the introduction of coupled\nmodels for ISMR prediction, resulting in substantial improvement in prediction skills. This\napproach is also extensively used by the leading seasonal prediction centres 9,10. The\nprediction skill of these physical models largely depends on the initial conditions, model\nphysics, slowly varying boundary conditions, ocean-atmosphere coupling, and the loosely\nconstrained model parameters involved in the parameterization processes to represent the\nsub-grid scale phenomenon11,12. These processes are computationally resource-intensive, and"}, {"title": "", "content": "the differential equations involved in modeling the dynamics of earth systems are often\ncompletely unsolvable and are more than often approximated. As a result, these models are\nunable to serve the purpose of a reliable seasonal prediction, with predictions contradicting\nthe ground truth many a times, thereby limiting its effectiveness.\nIn recent decades, the advent of emerging technologies like artificial intelligence (AI) and\nmachine learning (ML) have emerged as effective tools in various fields such as energy,\nweather, transportation, healthcare, finance, agriculture, etc. One of the important\napplications in the field of atmospheric sciences is time-series forecasting using AI/ML13-15,\nas many existing methods use machine learning and deep learning techniques and have\nshown promising results. In many instances, these models outperform the conventional\nphysical models along with an advantage of minimal workforce and computational\nresources 13.\nThese machine learning models have demonstrated exemplary performance in capturing the\ncomplex non-linear relationships between the input features and the target output, thereby\nallowing them to capture complex real-world dynamic systems, e.g., weather forecasting,\nenergy distribution, traffic flow analysis, and financial market modeling. Among the\npresently available deep learning models, the pre-trained Large Language Models (LLMs)\nhave become increasingly popular in recent years because of their skill in time-series\nforecasting with limited training datasets as input owing to their large-scale pretraining over\nmountains of data16-18. Among the presently available LLMs, the channel-independent patch\ntime series Transformer (PatchTST) (see the methods for more details) has shown excellent\nperformance in long-term time series forecasting and outperforms many SOTA LLM-based\nmodels19.\nThe capabilities of LLMs in seasonal forecasting of key atmospheric variables remain largely\nunexplored and need detailed investigations. Our study examined the skill of many popular\nML models that are extensively utilized for timeseries forecasting, including three statistical\nmodels (Linear Regression, XG Boost, Support Vector Regression (SVR)), two neural\nnetwork (NN) models (Long Short-Term Memory (LSTM), Convolution Neural Network\n(CNN)), along with one LLM model (PatchTST) in forecasting the All India Summer\nMonsoon Rainfall (AISMR). It is noteworthy that our study represents one of the first\nattempts to explore the implications of LLMs in AISMR forecasting."}, {"title": "Results", "content": null}, {"title": "Model performance", "content": "The PatchTST model demonstrates exceptional performance, characterized by a very low\nRMSE percentage and a high Spearman correlation value (Fig 1). When compared with\nbaseline models, all versions of the PatchTST model exhibit significantly higher Spearman\ncorrelation values, indicating their proficiency in accurately capturing the rank of the\nobservations. This translates to the model's ability to capture the variability in rainfall\neffectively. Furthermore, these models exhibit a low RMSE percentage error, with the\nbest-performing model achieving an RMSE percentage as low as 0.07%.\nAmong the four versions of the PatchTST, the model fine-tuned with the AISMR+\nNi\u00f1o3.4+IOD (named as PatchTST_AISMR+Ni\u00f1o3.4+IOD) dataset emerges as the\nbest-performing model, boasting a Spearman correlation of 0.967 and an RMSE percentage\nerror of 0.07%. This is followed by the model fine-tuned on the AISMR+IOD\n(PatchTST_AISMR+IOD) dataset, with a Spearman correlation value of 0.889 and an RMSE\npercentage error of 0.25%. The model trained with the AISMR (PatchTST_AISMR) data has\nan RMSE percentage error of 0.27% and a Spearman correlation value of 0.816. Finally, the\nmodel trained with the AISMR+ Ni\u00f1o3.4 (PatchTST_AISMR+ Ni\u00f1o3.4) dataset has the\nlargest RMSE percentage error among all the different versions of the PatchTST model, with\nan error of 0.79% models and a lower Spearman correlation of 0.786. However, even this\nrelatively poorer-performing PatchTST model is comparable to the best-performing baseline\nmodels, underscoring the superior performance of the PatchTST approach. Moreover, the\nPatchTST model demonstrates the capability to accurately predict the AISMR while\nrequiring significantly less computational resources. This is a salient feature of the model,\nespecially since physical models require considerable computational resources and a skilled\nworkforce to make predictions. Comparing the forecasts from the best-performing PatchTST\nmodel, PatchTST_AISMR+Ni\u00f1o3.4+IOD, with observed cumulative rainfall, the model\npredicts the rainfall with high accuracy (Fig. 2) for the test period (2011-2023). The deviation\nof the model predictions with the observed rainfall is less than 1% during the test period and\nagrees well with the observed interannual variations of AISMR.\nThe PatchTST model's outstanding performance in predicting cumulative rainfall could be\nattributed to the model's unique feature of patching and channel segmentation, enabling the\nmodel to retain local semantic information essential for AISMR prediction. This underscores"}, {"title": "", "content": "the model's potential as a reliable tool for rainfall forecasting. Its superior performance\ncompared to baseline models, coupled with its resource efficiency make it a promising tool\nfor supplementing conventional physical models that require extensive computational\nresources."}, {"title": "Rainfall prediction for 2024", "content": "Our model predicts an \u201cabove normal\u201d (based on IMD tercile category\u00b9\u00b3) monsoon for the\nyear 2024 with a cumulative rainfall of 921.6 mm. The model can make these predictions\nwith a lead time of three months, which is early enough for adopting appropriate\npreparedness measures to tackle any unforeseen situations. The best-performing model\nutilizes the historical AISMR data along with the Ni\u00f1o3.4 index and IOD index and is able to\nunderstand the complex interaction among them, resulting in accurate prediction of AISMR.\nThis demonstrates the ability and applicability of powerful data-driven algorithms to exploit\nthe available data for a multitude of public-relevant applications. A similar approach could\nalso be employed for region-specific predictions of summer monsoon rainfall; as a matter of\nfact, any climatic prediction provided an appropriate dataset."}, {"title": "Impact of input features on model performance", "content": "The PatchTST_AISMR+Ni\u00f1o3.4 model exhibited the lowest performance from the rest of the\nPatchTST model versions (see Fig. 1), which was unexpected given the extensive literature\nemphasizing the potential teleconnection between AISMR and the Ni\u00f1o3.4 index20-22. Based\non these studies, we anticipated that the PatchTST_AISMR+Ni\u00f1o3.4 model would\noutperform the PatchTST_AISMR model. Interestingly, the model with the best performance\nutilized Ni\u00f1o3.4 values in conjunction with IOD and AISMR values for its predictions. This\nsuggests that while there exists a relationship between Ni\u00f1o3.4 and monsoon rainfall, this\nconnection alone is insufficient for accurately predicting AISMR. One possible explanation\nfor this finding is the evolving teleconnection between monsoon rainfall and the El\nNi\u00f1o-Southern Oscillation, showing a decreased teleconnection strength since the last four\ndecades21. The changing nature of this teleconnection could contribute to the reduced\neffectiveness of using AISMR and Ni\u00f1o3.4 data alone for model predictions.\nThis highlights the importance of carefully curating the input features for the model to utilize\nthe maximum potential of the models. The choice of input features directly influences the\nmodel's ability to capture complex relationships and patterns within the data, ultimately\ndetermining the accuracy and reliability of the model's predictions. Therefore, careful"}, {"title": "", "content": "consideration of the input features is paramount in ensuring that the model can make the most\nof the available information and deliver optimal performance."}, {"title": "Conclusion", "content": "The data-driven LLM, PatchTST model successfully simulated the seasonal forecast of\nAISMR for all years during the test period with minimal biases, showcasing a new and\nefficient approach to the resource-intensive AISMR forecast. This developed model could\ncomplement existing physical models to enhance the seasonal forecast skill for AISMR\nprediction. The PatchTST model predicts an \"above normal\" monsoon rainfall season for\n2024, achieving this prediction with a lead time of three months. This early prediction would\nsignificantly benefit agricultural activities and water resource management, thereby\nsupporting the country's overall economy. However, providing a point forecast for AISMR is\nnot sufficient to address the need for actionable seasonal forecasts. It necessitates\ngranular-scale seasonal forecasts across the entire Indian subcontinent. This requires adopting\ndata-driven models for individual grid scales to provide socially relevant seasonal monsoon\nrainfall information. Given that the IMD provides gridded data with a resolution of 0.25\ndegrees, it is practically feasible to perform regional forecasts at a granular scale by\nfine-tuning the model parameters for specific regions."}, {"title": "Methods", "content": "The models are trained on four different datasets. The first dataset consists of AISMR daily\ndata for the months of JJAS. The second dataset includes the AISMR daily data and monthly\nNi\u00f1o3.4 index values from the month of May of the previous year (T) till the month of May\nof the prediction year (T+1). The third dataset is composed of AISMR daily values for the\nmonth of JJAS combined with IOD index values of the prediction year (T+1). The final\ndataset incorporates the AISMR daily data, 13 months of Ni\u00f1o3.4 index values, and the IOD\nindex values. The IOD index is categorically classified as positive (+1), negative (-1), or\nneutral (0) in the input dataset."}, {"title": "Baseline models", "content": "The baseline models are based on the framework outlined by Narang et al. (2023). These\nbaseline models comprise of three statistical models and two NN models."}, {"title": "Statistical models", "content": "The statistical model includes Linear Regression, XGBoost, and Support Vector\nRegression (SVR)these models are trained on four different datasets with five varying\nlookback periods, resulting in a total of 20 different input datasets. The model's\nperformance is measured and compared against the PatchTST model."}, {"title": "Neural Network models", "content": "There are two NN-based models used as baselines in this study, namely LSTM and\nCNN-based models. Similar to the statistical models, these models are trained on the\nsame 20 different input datasets. The model's performance for each input dataset is\nmeasured separately and compared."}, {"title": "Transformers and PatchTST", "content": "A transformer is a deep learning architecture that employs a multiple-head attention\nmechanism. The input is divided into tokens in this architecture, and those tokens are\nsubsequently transformed into vectors by using a word embedding table. At each layer, these\ntokens are contextualized within a specified window, allowing them to interact with each\nother through a parallel multi-head attention process. This process emphasizes important\ntokens while less significant ones are downplayed. One key advantage of transformers is the\nabsence of recurrent units, which enables faster training compared to earlier recurrent neural\nnetworks (RNNs) like long-short term memory (LSTM). The transformer architecture has\nalso facilitated the development of pre-trained models like generative pre-trained\ntransformers (GPTs) and BERT (Bidirectional Encoder Representations from Transformers).\nThe PatchTST model architecture18 (the model discussed in this paper), leverages the power\nof Transformers to process univariate time series data. The model operates by transforming\nthe input data through a series of processing steps, including patching, projection, and\nTransformer-based encoding (Fig. 3). The model vectorizes individual time series into\npatches of a specified size and encodes these sequences using a Transformer. This process\nfacilitates the extraction of meaningful features and the generation of forecasts.\nThe model consists of two key components:"}, {"title": "", "content": "1. Segmentation into Patches: The time series is divided into patches, which are then used\nas input tokens for the Transformer. This approach helps retain local semantic\ninformation and reduces the computational complexity associated with processing long\nsequences."}, {"title": "", "content": "2. Channel Independence: Each channel in the model corresponds to a single univariate\ntime series. The model shares embedding and Transformer weights across all channels,\nmaking it a global univariate model. This design enhances the model's ability to\ngeneralize across different time series."}, {"title": "", "content": "The patching mechanism offers several advantages:\n1. Retention of Local Information: The model preserves important local patterns within the\ntime series by processing patches.\n2. Reduced Computational Load: The segmentation into patches reduces the size of the\ninput sequence, thereby decreasing the computational and memory requirements of the\nmodel.\n3. Enhanced Historical Context: The model can consider longer historical sequences by\nadjusting the patch and context lengths."}, {"title": "Model working and fine-tuning", "content": "The PatchTST model processes input data by segmenting time series into patches, each\nrepresenting a window of consecutive data points. The input features are first scaled using the\nStandard Scaler to normalize the data. The normalized data is then organized into sequences,\nwhere each sequence consists of a fixed number of consecutive data points defined by a\nwindow size (in this case, 30). These sequences are used as input tokens for the Transformer.\nThe model architecture includes an LSTM-based encoder with a specified number of layers\nand hidden dimensions. Each input patch is processed independently, with shared weights\nacross different time series channels, thus maintaining channel independence. This design\nallows the model to effectively capture local and global temporal dependencies.\nFine-tuning of the model involves adjusting several hyperparameters through grid search.\nKey parameters such as the number of LSTM layers, hidden dimensions, dropout rate, and\nlearning rate are optimized to improve model performance. For instance, the final model\nconfiguration includes 128 hidden units, 3 LSTM layers, and a learning rate of 0.001 with a\nbatch size of 64. These choices were made to balance the model's capacity and generalization\nability, ensuring that it captures complex patterns in the data without overfitting.\nTo enhance training stability and prevent gradient explosion, gradient clipping is applied.\nAdditionally, early stopping criteria are implemented to halt training if no significant\nimprovement in the loss is observed over multiple epochs, thereby preventing overfitting and\nensuring optimal model performance."}, {"title": "Model evaluation", "content": "The outputs of the trained models were evaluated using different metrics, these include \u2013\nRMSE Percentage: The root mean squared error (RMSE) percentage represents the difference\nbetween the actual and predicted values as root mean squared error percentages, facilitating\ndirect comparison between models. The better performing models generally has the lowest\nRMSE percentage values.\nRMSE(%) = $ \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (\\frac{Obs_i-Pred_i}{Obs_i})^2} $\nSpearman Correlation: Spearman Correlation: Spearman correlation represents the statistical\nmeasure of the strength of the relation between a ranked pair of variables. The Spearman\ncorrelation varies between \u20131 and +1, where positive correlation implies that if one variable\nincreases, the other also increases and vice-versa. Similarly, a negative correlation implies if\none variable decreases, the other variable increases and vice-versa. A correlation between the\nactual and predicted values closer to 1 signifies better model performance.\nSpearman correlation = 1 - $\\frac{6\\sum_{i=1}^{n} d_i^2}{n^3-n}$"}, {"title": "Data availability", "content": "The primary dataset used in this study includes daily rainfall data for the months of June to\nSeptember from the Indian Meteorological Department (IMD)23. The record spans 123 years,\nfrom 1901 to 2023. Additionally, monthly values of the Ni\u00f1o3.4 index (Ni\u00f1o3.4) and the\nIndian Ocean Dipole (IOD) index are used as predictors in this study due to their strong\nteleconnection with ISMR13. The Ni\u00f1o3.4 values are archived and maintained by the National\nOceanic\nand\nAtmospheric\nAdministration\n(https://psl.noaa.gov/gcos_wgsp/Timeseries/Data/nino34.long.anom.data), and the IOD index\nvalues\nare\nmaintained by the Japan Meteorological Agency\n(https://psl.noaa.gov/gcos_wgsp/Timeseries/Data/nino34.long.anom.data).\nFor the prediction of 2024 the multi-model mean forecasts of Ni\u00f1o3.4 index provided by the\nColumbia\nClimate\nSchool\n(https://iri.columbia.edu/our-expertise/climate/forecasts/enso/current/) and monthly IOD\nforecasts provided\nby\nthe Bureau\nof\nMeteorology Australia\n(http://www.bom.gov.au/climate/enso/#tabs=Indian-Ocean) have been used."}, {"title": "Code availability", "content": "All codes used in this study are available from the corresponding authors upon request."}]}