{"title": "BLENDRL: A FRAMEWORK FOR MERGING\nSYMBOLIC AND NEURAL POLICY LEARNING", "authors": ["Hikaru Shindo", "Quentin Delfosse", "Devendra Singh Dhami", "Kristian Kersting"], "abstract": "Humans can leverage both symbolic reasoning and intuitive reactions. In contrast, re-\ninforcement learning policies are typically encoded in either opaque systems like neural\nnetworks or symbolic systems that rely on predefined symbols and rules. This disjointed\napproach severely limits the agents' capabilities, as they often lack either the flexible low-\nlevel reaction characteristic of neural agents or the interpretable reasoning of symbolic\nagents. To overcome this challenge, we introduce BlendRL, a neuro-symbolic RL frame-\nwork that harmoniously integrates both paradigms within RL agents that use mixtures of\nboth logic and neural policies. We empirically demonstrate that BlendRL agents outper-\nform both neural and symbolic baselines in standard Atari environments, and showcase\ntheir robustness to environmental changes. Additionally, we analyze the interaction be-\ntween neural and symbolic policies, illustrating how their hybrid use helps agents over-\ncome each other's limitations.", "sections": [{"title": "INTRODUCTION", "content": "To solve complex problems, humans employ two fundamental\nmodes of thinking: (1) instinctive responses for immediate reac-\ntion and motor control and (2) abstract reasoning, using distinct\nidentifiable concepts. These two facets of human intelligence are\ncommonly referred to as System 1 and System 2 (Kahneman, 2011).\nWhile our reasoning system requires symbols to build interpretable\ndecision rules, instinctive reactions do not rely on such inductive\nbias, but lack transparency. Both systems are required in challeng-\ning RL environments, such as Kangaroo, where the agent's goal is\nto reach its joey (at the top), captured by monkeys that need to be\npunched out of the way, or in Seaquest, where the agent controls a\nsubmarine, and needs to collect swimming divers without running\nout of oxygen (cf. Figure 1). Developing agents that can effectively\nuse both information processing systems has proven to be a persis-\ntent challenge (Lake et al., 2017; Mao et al., 2019; Kautz, 2022).\nThe main difficulty not only lies in achieving high-level capabilities\nwith both systems, but also in seamlessly integrating these systems\nto interact synergistically, in order to maximize performances without sacrificing transparency."}, {"title": "BACKGROUND", "content": "Let us introduce the necessary background before formally introducing our BlendRL method.\nDeep Reinforcement Learning. In RL, the task is modelled as a Markov decision process, M =<\nS, A, P, R, \u03b3>, where, at every timestep t, an agent in a state $s_t \u2208 S$, takes action $a_t \u2208 A$, receives a reward\n$r_t = R(s_t, a_t)$ and a transition to the next state $s_{t+1}$, according to environment dynamics $P(s_{t+1}|s_t, a_t)$.\nDeep agents attempt to learn a parametric policy, $\u03c0_\u03b8(a_t|s_t)$, in order to maximize the return (i.e. \u03a3ytrt,\nwith \u03b3\u2208 [0, 1]). The desired input to output (i.e. state to action) distribution is not directly accessible, as RL\nagents only observe returns. The value $V^{\u03c0_\u03b8}(s_t)$ (resp. Q-value $Q^{\u03c0_\u03b8}(s_t, a_t)$) function provides the return of\nthe state (resp. state/action pair) following the policy $\u03c0_\u03b8$. We provide more details in App. \u0410.1.\nFirst-Order Logic (FOL). In FOL, a Language L is a tuple (P, D, F, V), where P is a set of predicates,\nDa set of constants, F a set of function symbols (functors), and V a set of variables. A term is either a\nconstant (e.g. obj1, agent), a variable (e.g. 01), or a term which consists of a function symbol. An atom is\na formula p(t1,..., tn), where p is a predicate symbol (e.g. closeby) and $t_1,..., t_n$ are terms. A ground\natom or simply a fact is an atom with no variables (e.g. closeby(obj1, obj2)). A literal is an atom (A)\nor its negation (\u00acA). A clause is a finite disjunction (V) of literals. A ground clause is a clause with no\nvariables. A definite clause is a clause with exactly one positive literal. If A, B1,..., Bn are atoms, then\nA\u2228 \u00acB\u2081 V... \u2228 \u00acBn is a definite clause. We write definite clauses in the form of A :- B1, ..., Bn. A is\nthe rule head, and the set {B1, . . ., Bn } is its body. We interchangeably use definite clauses and rules.\nDifferentiable Forward Reasoning is a data-driven approach to reasoning in First-Order Logic (FOL) (Rus-\nsell & Norvig, 2010). In forward reasoning, given a set of facts and rules, new facts are deduced by applying\nthe rules to the facts. Differentiable forward reasoning is a differentiable implementation of forward rea-\nsoning, utilizing tensor-based differentiable operations (Evans & Grefenstette, 2018; Shindo et al., 2023) or\ngraph-based approaches (Shindo et al., 2024b). This approach can be efficiently applied to reinforcement\nlearning tasks by encoding actions in the form of rules, where the head defines an action and the body speci-\nfies its conditions. To learn the importance or truth value of each rule, they can be associated with learnable\nrule weights. Consequently, hypotheses can be formulated in terms of rules and learned from data."}, {"title": "BLENDRL", "content": "BlendRL integrates both abstract reasoning and instinctive reactions by combining symbolic and neural poli-\ncies. As illustrated in Figure 2, the neural policy processes sub-symbolic (i.e. pixel-based) representations\nto compute a distribution over the actions, while the reasoning module employs differentiable reasoning on\nsymbolic states. These action distributions are then blended to obtain the final action distribution. We begin\nby describing the inner workings of each policy type, and of the blending module. Next, we discuss how we\nleverage the common sense encapsulated in pretrained large language models (LLMs) to obtain symbolic\nconcepts and their evaluation functions. Finally, we describe how we adapted the PPO actor-critic algorithm\nto perform end-to-end training of BlendRL modules. Let us first formally introduce the state representations."}, {"title": "HYBRID STATE REPRESENTATIONS", "content": "BlendRL agents employ two distinct state representations: (i) pixel-based representations and (ii) object-\ncentric representations, that can be extracted by object discovery models (Redmon et al., 2016; Lin et al.,\n2020; Delfosse et al., 2023b; Zhao et al., 2023). Pixel-based representations usually consist of a stack of\nraw images, provided by the environment, fed to a deep convolutional network, as introduced by Mnih et al.\n(2015). Our considered symbolic (object-centric) representations consist of a list of objects, with attributes\n(e.g. position, orientation, color, etc.), allowing for logical reasoning on structured representations (Zada-\nianchuk et al., 2021; Liu et al., 2021; Yoon et al., 2023; W\u00fcst et al., 2024; Stammer et al., 2024b)."}, {"title": "THE BLENDED NEURO-SYMBOLIC POLICIES", "content": "Using both state representations, BlendRL agents compute the action selection probabilities by aggregating\nthe probabilities of its neural and its logic policies.\nThe neural policy, $\u03c0^{neural} : R^{F\u00d7W\u00d7H\u00d7C} \u2192 [0, 1]^A$, consists of a neural network, parameterized by \u03b8, that\ncomputes action distributions based on pixel state representations (x). Convolutional neural networks based\npolicies (Mnih et al., 2015; Schulman et al., 2017; Hessel et al., 2018) are the most common deep policy\ntypes (Huang et al., 2022), but visual transformers can also be used (Chen et al., 2021; Parisotto et al., 2020).\nThe logic policy, $\u03c0^{logic}_\u03d5 : R^{n\u00d7m} \u2192 [0, 1]^A$, is a differentiable forward reasoner (Shindo et al., 2023),\nparameterized by 6, that reasons over object-centric states, such as the one depicted in Figure 2. Action\nrules are rules in first-order logic that encode conditions for an action to be selected (Reiter, 2001; Delfosse\net al., 2023a). An action rule defines an action as its head atom (action atom) and encodes its preconditions\nin its body atoms (state atoms). We provide exemplary action rules for Seaquest in Listing 1.\nThese rules are transparent, e.g. [R1] can be inter-\npreted as \"Select UP if the oxygen is empty\". The\nbody atom is_empty is a state predicate, whose\ntruth level can be computed from an object-centric\nstate. Each state predicate is associated with a\n(differentiable) function, known as a valuation function, to compute its truth value, or confidence."}, {"title": "LLM GENERATED LOGIC POLICIES", "content": "BlendRL employs Language Models (LLMs) to generate symbolic programs for precise reasoning, follow-\ning a chain of thought principle (Wei et al., 2022; Kojima et al., 2022):\n(i) it uses the task context and detectable objects description and implementation to create state predicates,\n(ii) it formulates action rules, with conjunctions of the generated predicates as body,\n(iii) it generates the predicates' valuations (i.e. their python implemented functions).\nFor steps (ii) and (iii), we used the few-shot prompting approach, providing the LLM with an example logic\nruleset obtained from NUDGE policies (Delfosse et al., 2023a). This circumvents the need for an expert to\nprovide the set of logic rules, the used logic predicates, and their implementations, thus allowing users to\neffectively introduce inductive biases in natural language (cf. Appendix A.3 and A.4 for additional details).\nA subset of the logic module of a BlendRL trained on Kangaroo is provided in Figure 4. The associated\nweights of the LLM-generated rules have been adjusted to maximize performance."}, {"title": "OPTIMIZATION", "content": "We use the PPO actor-critic to train BlendRL agents, and also employ hybrid value functions. We compose\nthe hybrid critics by computing values using both visual and object-centric states.\nMixed Value Function. As the value function approximates the expected return, we did not use logic to\nencode it. However, BlendRL incorporates a hybrid value function, that uses both the subsymbolic (x) and\nsymbolic (or object-centric z) state representations. Given state s = (x, z), the value is defined as:\n$V^{(\u03bc,\u03c9)}(s) = \u03b2 \u00b7 V^{CNN}_\u03bc(x) + (1 \u2212 \u03b2) \u00b7 V^{OC}_\u03c9(z)$.\nwith $V^{CNN}$ a CNN, that share its convolutional layers with the neural actor (Schulman et al., 2017) and\n$V^{OC}$ a small MLP on the object-centric state, and \u03b2 = $B_x(x, z)$, the blending weight.\nRegularization. To ensure the use of both neural and logic policies, we introduce a regularization term for\nBlendRL agents. To further enhance exploration, we penalize overly peaked blending distributions. This\napproach ensures that agents utilize both neural and logic policies without deactivating either entirely.\n$R = -\u03b2 log \u03b2 \u2013 (1 \u2013 \u03b2) log(1 \u2013 \u03b2)$\nThis helps agents avoid suboptimal policies typically caused by neural policies due to the sparse rewards."}, {"title": "EXPERIMENTS", "content": "We outline the benefits of BlendRL over purely neural or symbolic approaches, supported by additional in-\nvestigations into BlendRL's robustness to environmental changes. Furthermore, we examine the interactions\nbetween neural and symbolic components and demonstrate that BlendRL can generate faithful explanations.\nWe specifically aim to answer the following research questions:\n(Q1) Can BlendeRL agents overcome both symbolic and neural agents' shortcomings?\n(Q2) Can BlendRL can produce both neural and symbolic explanations for its action selection?\n(Q3) Are BlendRL agents robust to environmental changes?\n(Q4) How do the neural and symbolic modules interact to maximize BlendRL's agents overall performances?\nLet us now provide empirical evidence for BlendRL's ability to learn efficient and understandable policies,\neven without being provided with all the necessary priors to optimally solve the tasks."}, {"title": "EXPERIMENTAL SETUP", "content": "Environments. We evaluate BlendRL on Atari Learning Environments (Bellemare et al., 2013), the most\npopular benchmark for RL (particularly for relational reasoning tasks). For resource efficiency, we use the\nobject centric extraction module of (Delfosse et al., 2024b). Specifically, in Kangaroo, the agent needs to\nreach and climb ladders, leading to the captive joey, while punching incoming monkey antagonists, that try\nto stop it. In Seaquest, the agent has to rescue divers, while shooting sharks and enemy submarines. It also\nneeds to surface before the oxygen runs out. Finally, in Donkey Kong the agent has to reach the princess at the\ntop, while avoiding incoming barrels thrown by Donkey Kong. For additional details, cf. Appendix A.7. To\nfurther test BlendRL abilities to overcome the potential lack of concept necessary to solve task, we omitted\nsome part of the game in our prompt for the LLM that generates the policy rules. Specifically, we kept out\nthe facts that agents can punch the monkeys in Kangaroo, can shoot the enemies in Seaquest and can jump\nover the barrels in DonkeyKong. To test robustness and isolate different abilities of the agents, we employ\nHackAtari (Delfosse et al., 2024a), that allow to customize the environments (e.g. remove enemies). For\nthese ablation studies, we provide details about the use modification at the relevant point of the manuscript.\nBaselines. We compare BlendRL to purely neural PPO agents. Both agent types incorporate the classic\nCNN used for Atari environments. Additionally, we evaluate NUDGE, that uses a pretrained neural PPO\nagent for searching viable policy rules (Delfosse et al., 2023a), SOTA among logic agents on Atari tasks.\nWe train each agent types until all of them converge to a stable episodic return (i.e. for 15K episodes for\nKangaroo and DonkeyKong and 25K for Seaquest). For additional details, cf. Appendix A.6."}, {"title": "RESULTS AND ANALYSIS", "content": "Comparison to neural and neuro-symbolic agents (Q1). Figure 5 illustrates the episodic returns of the\nBlendRL, NUDGE, and NeuralPPO agents across various Atari environments. BlendRL surpasses the logic-\nbased state-of-the-art NUDGE baseline (Delfosse et al., 2023a) in all tested scenarios. In the Kangaroo\nenvironment, which requires fewer intuitive actions due to the relatively small number of enemies, NUDGE\nshows fair performance, even if its are less able to punch the monkeys and avoid their thrown coconuts.\nHowever, in the other environments, populated with more incoming threats, where neural policy becomes\ncritical for accurate controls, NUDGE lags behind. Additionally, the purely neural PPO agents often fall into\nsuboptimal policies. For instance, in Seaquest, surfacing without collecting any divers lead to negative re-.\nward. Neural PPO agents thus concentrate on shooting sharks to collect reward, but never refill their oxygen.\nIn contrast, BlendRL effectively selects its logic module to collect divers and surface when needed and its\nneural module to efficiently align itself with the enemies and shoot them. Overall, BlendRL significantly out-\nperforms both baselines across different environments, underscoring the efficacy of neuro-symbolic blending\npolicies in efficiently harnessing both neural and symbolic approaches to policy learning.\nBlendRL agents are interpretable and explainable (Q2). BlendRL's symbolic policies can easily be in-\nterpreted as they consist of a set of transparent symbolic weighted rules, as exemplified in Figure 4 for\nKangaroo. The interpretable blending module prioritizes neural agents in situations where finetuned control\nis needed (i.e. for dodging an incoming deadly coconut or accurately placing oneself next to the monkey\nand punching it). Conversely, the logic module is in use when no immediate danger is present, e.g. as a path\nfinding proxy. The logic rules for the other environments are provided in Appendix A.4.\nBlendRL also produces gradient-based explanations, as each component is differentiable. Logic-based ex-\nplanations are computed using action gradients (i.e. $d\u03c0^{logic}_\u03d5 (z)/dz$) on the state atoms, that underline the\nmost important object properties for the decision. Further, the integrated gradients method (Sundararajan\net al., 2017) on the neural module provides importance maps. We can visualize these two explanations,\nmerging them with the blending weight \u03b2, as shown in Figure 6. The most important logic rules for the\ndecision are also depicted. BlendRL provides interpretable rules for it reasoning part and importance maps.\nBlendRL is robust to environmental changes (Q3).\nDeep agents usually learn \u201cby heart\u201d spurious corre-\nlations during training, and thus fail to generalize to un-\nseen environments, even on simple Atari games (Fare-\nbrother et al., 2018; Delfosse et al., 2024a). We used\nHackAtari variations of the environment, to disable\nthe enemies in Kangaroo and Seaquest and to disable\nthe barrels in DonkeyKong. We additionally used a modified Kangaroo environment with relocated ladders."}, {"title": "Neural and symbolic policy interactions in BlendRL (Q4)", "content": "We here investigate how the symbolic and\nneural policies interoperate. One concern of such an hybrid systems is its potential reliance on only one\nmodule. The system could indeed learn to e.g. only rely on its neural component (i.e. \u1e9e \u2248 1.0). Figure 7\n(left and center) illustrates the outputs of BlendRL's neural and logic policies for 1k steps (i.e. 1-3 episodes).\nSpecifically, the maximum value of the action distributions ($max_a \u03c0^{logic}_\u03c6 (a|st)$ and $max_a \u03c0^{neural}_\u03b8 (a|st)$), at\neach time step t is depicted (at the top), underlined by the importance weighting of each module (blue if logic\nis mainly selected (\u1e9e \u2248 0), red if neural is (\u03b2 \u2248 1)), indicating how much each module is used at each step\non Kangaroo and Seaquest. Further, BlendRL's agents can adapt the amount to which they select each com-\nponent through training, as depicted on the right of this figure. As Seaquest is a progressive environment,\nin which the agent first faces few enemies (thus mainly relying on its logic module to collect the divers),\nbefore progressively accessing states in which many more enemies are spawning (cf. Figure 10), BlendRL\nagents first mainly relies on its logic module (blue), while progressively shifting this preference to its neural\none (red) for accurately shooting enemies. These results demonstrate the efficacy of BlendRL's policy rea-\nsoning and learning on neuro-symbolic hybrid representations, thereby enhancing overall performance. We\nprovide a further analysis comparing neural and logic blending modules in Appendix A.8, highlighting that\nthe logic-based blending module can utilize both policies effectively, resulting in better performance."}, {"title": "RELATED WORK", "content": "Relational Reinforcement Learning (Relational RL) (Dzeroski et al., 2001; Kersting et al., 2004; Kersting\n& Driessens, 2008; Lang et al., 2012; Hazra & Raedt, 2023) has been developed to address RL tasks in\nrelational domains by incorporating logical representations and probabilistic reasoning. BlendRL extends\nthis approach by integrating differentiable logic programming with deep neural policies. The Neural Logic\nReinforcement Learning (NLRL) framework (Jiang & Luo, 2019) was the first to incorporate Differentiable\nInductive Logic Programming (@ILP) (Evans & Grefenstette, 2018) into the RL domain. \u018fILP learns gen-\neralized logic rules from examples using gradient-based optimization. NUDGE (Delfosse et al., 2023a)\nextends this method by introducing neurally-guided symbolic abstraction, leveraging extensive work on"}, {"title": "CONCLUSION", "content": "In this study, we introduced BlendRL, a pioneering framework that integrates symbolic and neural policies\nfor reinforcement learning. BlendRL employs neural networks for reactive actions and differentiable logic\nreasoners for high-level reasoning, seamlessly merging them through a blending module that manages distri-\nbutions over both policy types. We also developed a learning algorithm for BlendRL agents that hybridizes\nthe state-value function on both pixel-based and object-centric states and includes a regularization approach\nto enhance the efficacy of both logic and neural policies.\nOur empirical evaluations demonstrate that BlendRL agents significantly outperform purely neural agents\nand state-of-the-art neuro-symbolic baselines in popular Atari environments. Furthermore, these agents ex-\nhibit robustness to environmental changes and are capable of generating clear, interpretable explanations\nacross various types of reasoning, effectively addressing the limitations of purely neural policies. Our com-\nprehensive analysis of the interactions between symbolic and neural policy representations highlights their\nsynergistic potential to enhance overall performance."}, {"title": "APPENDIX", "content": ""}, {"title": "DEITAILED BACKGROUND OF REINFORCEMENT LEARNING", "content": "We provide more detailed background for reinforcement learning. Policy-based methods directly optimize\n$\u03c0_\u03b8$ using the noisy return signal, leading to potentially unstable learning. Value-based methods learn to\napproximate the value functions $V_\u03c6$ or $Q_\u03c6$, and implicitly encode the policy, e.g. by selecting the actions\nwith the highest Q-value with a high probability (Mnih et al., 2015). To reduce the variance of the estimated\nQ-value function, one can learn the advantage function $\u00c2_\u03c6(s_t, a_t) = Q_\u03c6(s_t, a_t) \u2013 V_\u03c6(s_t)$. An estimate of\nthe advantage function can be computed as $\u0391_\u03c6(s_t, a_t) = \u03a3^{k-1}_{i=0} \u03b3^i r_{t + i} + \u03b3^k V_\u03c6(St+k) \u2013 V_\u03c6(st)$ (Mnih et al.,\n2016). The Advantage Actor-critic (A2C) methods both encode the policy $\u03c0_\u03b8$ (i.e. actor) and the advantage\nfunction A (i.e. critic), and use the critic to provide feedback to the actor, as in (Konda & Tsitsiklis,\n1999). To push $\u03c0_\u03b8$ to take actions that lead to higher returns, gradient ascent can be applied to $L^{PG}(\u03b8) =$\n$\u00ca[log \u03c0_\u03b8(a_t | s_t) \u00c2_\u03c6]$. Proximal Policy Optimization (PPO) algorithms ensure minor policy updates that avoid\ncatastrophic drops (Schulman et al., 2017), and can be applied to actor-critic methods. To do so, the main\nobjective constraints the policy ratio $r(\u03b8) = \\frac{\u03c0_\u03b8(a_t|s_t)}{\u03c0_{\u03b8^{old}}(a_t|s_t)}$, following $L^{PR}(\u03b8) = \u00ca[min(r(\u03b8)\u00c2_\u03c6, clip(r(\u03b8), 1\n\u2212 \u03f5, 1 + \u03f5)\u00c2_\u03c6)]$, where clip constrains the input within [1 \u2212 \u03f5,1 + \u03f5]. PPO actor-critic algorithm's global\nobjective is $L(\u03b8, \u03c6) = \u00ca[L^{PR}(\u03b8) \u2013 C_1 L^{VF}(\u03c6)]$, with $L^{VF}(\u03c6) = (V_\u03c6(s_t) \u2013 V(s_t))^2$ being the value function\nloss. An entropy term can also be added to this objective to encourage exploration."}, {"title": "DETAILS OF DIFFERENTIABLE FORWARD REASONING", "content": "We provide the details of differentiable forward reasoning.\nDefinition A.1 A Forward Reasoning Graph is a bipartite directed graph $(V_G, V_\u039b, E_{G\u2192\u039b}, E_{\u039b\u2192G})$, where\n$V_G$ is a set of nodes representing ground atoms (atom nodes), $V_\u039b$ is set of nodes representing conjunctions\n(conjunction nodes), $E_{G\u2192\u2227}$ is set of edges from atom to conjunction nodes and $E_{\u2227\u2192G}$ is a set of edges from\nconjunction to atom nodes.\nBlendRL performs forward-chaining reasoning by passing messages on the reasoning graph. Essentially,\nforward reasoning consists of two steps: (1) computing conjunctions of body atoms for each rule and (2)\ncomputing disjunctions for head atoms deduced by different rules. These two steps can be efficiently com-\nputed on bi-directional message-passing on the forward reasoning graph. We now describe each step in\ndetail.\n(Direction \u2192) From Atom to Conjunction. First, messages are passed to the conjunction nodes from atom\nnodes. For conjunction node $v_i \u2208 V_\u039b$, the node features are updated:\n$x^{(t+1)}_i = \u03c3 (\\frac{1}{\\left |N_G(v_i)\\right |} \\sum_{v_j \u2208 N_G(v_i)} x^{(t)}_j )$,\nwhere $N_G$ is a soft implementation of conjunction, and $\u2228$ is a soft implementation of disjunction. Intuitively,\nprobabilistic truth values for bodies of all ground rules are computed softly by Eq. 4.\n(Direction \u2190) From Conjunction to Atom. Following the first message passing, the atom nodes are then\nupdated using the messages from conjunction nodes. For atom node $v_i \u2208 V_G$, the node features are updated:\n$x^{(t+1)}_i = \u03c3 ( \u03a3_{v_j \u2208 N_\u039b(v_i)} w_{ji} x^{(t)}_{j} )$"}, {"title": "BODY PREDICATES AND THEIR VALUATIONS", "content": "We here provide examples of valuation functions for evaluating state predicates (e.g. closeby, left_of,\netc.) generated by LLMs (in Python)."}]}