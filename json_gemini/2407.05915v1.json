{"title": "Correction: Harnessing Federated Generative Learning for Green and Sustainable Internet of Things", "authors": ["Yuanhang Qi", "M. Shamim Hossain"], "abstract": "The rapid proliferation of devices in the Internet of Things (IoT) has ushered in a transformative era of data-driven connectivity across various domains. However, this exponential growth has raised pressing concerns about environmental sustainability and data privacy. In response to these challenges, this paper introduces One-shot Federated Learning (OSFL), an innovative paradigm that harmonizes sustainability and machine learning within IoT ecosystems. OSFL revolutionizes the traditional Federated Learning (FL) workflow by condensing multiple iterative communication rounds into a single operation, thus significantly reducing energy consumption, communication overhead, and latency. This breakthrough is coupled with the strategic integration of generative learning techniques, ensuring robust data privacy while promoting efficient knowledge sharing among IoT devices. By curtailing resource utilization, OSFL aligns seamlessly with the vision of green and sustainable IoT, effectively extending device lifespans and mitigating their environmental footprint. Our research underscores the transformative potential of OSFL, poised to reshape the landscape of IoT applications across domains such as energy-efficient smart cities and groundbreaking healthcare solutions. This contribution marks a pivotal step towards a more responsible, sustainable, and technologically advanced future.", "sections": [{"title": "1. Introduction", "content": "The advent of the Internet of Things (IoT) [46, 55, 4] has ushered in a new era of connectivity and data-driven decision-making across various domains [32, 33, 35, 14]. IoT devices have permeated our daily lives, enabling a wide range of applications, from smart homes and cities to industrial automation and healthcare. However, the proliferation of these devices has raised concerns about their environmental impact, especially with regard to energy consumption and data privacy. In this context, Federated Learning (FL) [36] has emerged as a promising paradigm to address these challenges by enabling collaborative machine learning on decentralized IoT devices, while also respecting user privacy [26, 49, 41].\nFederated learning is a decentralized machine learning approach that enables multiple devices to collaboratively train a global model while keeping their data localized and private [30]. The workflow begins with the initialization of a global model, followed by device participation, where decentralized clients perform local training on their data, compute model updates, and securely aggregate them on a central server [7, 19]. This iterative process refines the global model over multiple rounds, monitored by evaluation metrics until the desired performance is achieved. FL's privacy-preserving and distributed nature makes it suitable for applications like green and sustainable IoT, healthcare, and federated edge computing, addressing data privacy concerns while enabling machine learning in diverse and resource-constrained environments [60, 1, 20, 37, 40].\nTraditional FL approaches often require numerous communication rounds between the central server and participating devices, which can result in significant energy consumption, latency, and communication overhead [25, 24]. Inspired by previous work [56, 44, 21, 58], this paper introduces a novel approach known as \u201cOne-shot Federated Learning(OSFL),\u201d aimed at minimizing resource utilization while maximizing the sustainability of IoT networks. Leveraging the power of generative learning [58], this approach allows IoT devices to learn and share knowledge efficiently in a single communication round, reducing the need for constant connectivity and data transmission. This approach not only promotes green and sustainable IoT but also enhances the scalability and practicality of FL in resource-constrained environments.\nThe need for sustainable IoT solutions has become increasingly pressing as the world grapples with the challenges of climate change and resource depletion [3]. IoT devices are projected to proliferate exponentially in the coming years, making it imperative to design and adopt technologies that can mitigate their environmental impact [43]. By incorporating generative learning techniques into the FL framework, this paper presents a promising avenue for reducing energy consumption and prolonging the lifespan of IoT devices, thereby contributing to the broader goal of sustainable technology development [47].\nPrivacy concerns also loom large in the IoT landscape, as the data generated by these devices often contains sensitive information [1]. Traditional FL methods inherently preserve user privacy by keeping data localized, but they still involve multiple interactions that raise potential security risks [50, 31]. OSFL enhances privacy further by minimizing data exposure and limiting communication, making it an attractive choice for applications where privacy is paramount. This paper explores how this approach can strike a delicate balance between data utility, model performance, and user privacy in green and sustainable IoT environments. To this end, we summarize the challenges we face as follows:\n\u2022 (C1.) Difficult to Converge: Since OSFL only communicates once, this brings severe challenges to global model convergence. Unlike traditional FL, OSFL cannot iteratively update the model and perform multiple rounds of communication [58].\n\u2022 (C2.) New Privacy Concerns: Although OSFL only conducts one-shot communication to mitigate privacy risks to a certain extent, adversaries still have the opportunity to threaten OSFL's privacy. The main reason is that the knowledge uploaded by OSFL is not protected by privacy protection measures.\nTo tackle the aforementioned challenges, this paper utilizes the concept of OSFL as a novel and environmentally friendly approach to machine learning within the realm of the IoT. Specifically, we present an innovative framework termed Federated Generative Learning (FGL) which was proposed by Zhang et al. [58]. FGL harnesses the robust capabilities of generative models, including Stable Diffusion and GPT-4V, to generate high-fidelity surrogate training data directly on the server. This process is guided by prompts provided by individual clients. In a concrete sense, our method streamlines the workflow by having each client upload only their respective prompts, linked to their local training data. Once all prompts are gathered from the clients, the server orchestrates prompt aggregation and subsequently synthesizes a premium-quality surrogate training dataset. This dataset can then be employed to train a global model. By leveraging generative learning techniques and minimizing communication rounds, this approach aims to address the challenges of energy efficiency, scalability, and privacy in IoT networks. The subsequent sections of this paper will delve into the technical details, experiments, and practical implementations of OSFL, illustrating its potential to usher in a greener and more sustainable era for IoT applications. This paper's primary contributions are as follows:\n\u2022 We customize a new one-shot federated generative learning system for green and sustainable IoT, which can effectively alleviate data privacy and communication overhead issues.\n\u2022 We propose two new local prompt generation strategies, which aim to extract fine-grained feature information of local data to enhance the model's generalization ability.\n\u2022 We develop a one-shot FL method with generative learning as the core, which is organically combined with generation to improve the overall performance of FL. In particular, the one-shot FL method based on generative learning significantly improves the problem of expensive communication in IoT.\n\u2022 We conduct comprehensive validation and evaluation on three benchmark datasets, and the experimental results demonstrate the effectiveness of the proposed method.\nThe paper's structure is organized as follows: In Section 2, we provide a comprehensive overview of the background and concepts related to FL and the IoT. This section highlights the existing challenges and open problems within this context. Section 3 delves into the foundational aspects of federated learning and text-to-image models, offering a clear understanding of these crucial components. Furthermore, we explore the practical application scenarios of text-to-image models and delve into the core challenges currently faced in this field. In Section 4, we introduce our innovative One-shot FGL system, detailing its architecture and mechanisms. We then proceed to Section 5, where we present a series of case studies conducted on three real-world datasets. In these case studies, we thoroughly analyze the experimental results, providing valuable insights and observations. Lastly, in Section 6, we draw our conclusions, summarizing the key findings and contributions of our work in the context of OSFL for green and sustainable IoT. We summarize the notation descriptions used in this paper in Table 1."}, {"title": "2. Related Work", "content": "In the pursuit of advancing green and sustainable IoT through the lens of FL, this section provides an overview of existing research in four key subsections:"}, {"title": "2.1. Federated Learning in IoT", "content": "The integration of FL in the IoT has garnered substantial attention, primarily for its ability to harmonize the divergent requirements of data-driven intelligence and sustainability in resource-constrained environments. Prior research in this domain has delved into diverse FL strategies, encompassing federated optimization techniques, communication-efficient algorithms, and privacy-preserving methods. Notable contributions include the works of McMahan et al. [36] on federated averaging, which laid the foundation for distributed model training, and Bonawitz et al. [8] on secure aggregation, addressing privacy concerns in FL. Moreover, recent research by Wei et al. [50] introduced the concept of \"Federated Learning with Differential Privacy,\" further enhancing privacy guarantees in IoT applications. We build upon these foundations and extend them by introducing One-shot Federated Learning, which minimizes communication rounds and maximizes sustainability through generative learning.\nEfforts to minimize communication between IoT devices and the central server have been pivotal in the context of FL for green IoT [45]. Various federated optimization [30, 39, 48] techniques have been proposed to reduce the amount of information exchanged during each communication round. Notable works include the FedPAQ algorithm introduced by [42], which enables IoT devices to send only model updates to the central server rather than transmitting raw data. This approach significantly reduces communication bandwidth, making it an essential step towards greener and more sustainable IoT practices.\nQuantization and compression techniques have emerged as effective means to further enhance communication efficiency in FL for IoT [9, 36]. Researchers have explored methods to quantize model updates into compact representations, thereby reducing the data transfer requirements between clients and the central server. Chen et al. [9] proposed a quantization approach, Q-FFL, which compresses model updates to a fraction of their original size, alleviating the communication burden on IoT devices. Such techniques align with the green IoT agenda by conserving energy and reducing the carbon footprint associated with data transmission."}, {"title": "2.2. Sustainability in IoT", "content": "Promoting environmental sustainability in IoT has become a critical research area, given the ever-growing number of IoT devices and their potential ecological impact [5, 2]. Previous studies have examined energy-efficient communication protocols, low-power hardware design, and green data center technologies. Additionally, researchers have proposed solutions to optimize IoT device lifespans and reduce their carbon footprint. The work of Xue et al. [53] on energy-efficient IoT communication protocols and Zhang et al. [59] on sustainable IoT design are notable examples.\nAdditionally, advancements in green data center technologies and eco-friendly IoT infrastructure have contributed to reducing the overall carbon footprint of IoT deployments. These research endeavors underscore the importance of mitigating environmental impact, a key objective that aligns closely with the vision of our proposed One-shot Federated Learning approach, which seeks to enhance sustainability in IoT by optimizing machine learning while minimizing energy consumption and communication overhead. Our paper contributes to this body of knowledge by presenting One-shot Federated Learning as a novel approach that minimizes energy consumption during model updates and enhances the sustainability of IoT networks."}, {"title": "2.3. One-shot Learning in FL", "content": "The hallmark of OSFL [56, 44, 10, 58] is its ability to streamline the FL process into a single communication round, minimizing the energy expended on data transmission and device connectivity. Traditional FL workflows require multiple rounds of communication to exchange model updates and converge towards a global model [36]. In contrast, OSFL condenses this multi-round process into a one-shot operation, reducing the overall energy footprint of IoT devices. This reduction in energy consumption is of paramount importance in green and sustainable IoT, where power-efficient operations are essential to mitigate environmental impact and prolong the lifespan of battery-operated devices.\nThe introduction of OSFL underscores the importance of striking a balance between sustainability and model performance in IoT applications. While reducing energy consumption and communication overhead are critical sustainability objectives, it is equally crucial to ensure that model quality and learning performance are not compromised. OSFL addresses this challenge by optimizing generative learning processes to create high-quality synthetic updates that faithfully represent the knowledge of participating devices [23]. Through this balance, OSFL presents a compelling solution for green and sustainable IoT, enabling energy-efficient machine learning while maintaining or even improving the quality of learned models. Recent research has further pushed the boundaries of FL efficiency by exploring one-shot learning techniques. Li et al. [29] demonstrated the feasibility of one-shot learning, enabling models to generalize from limited data instances. While these advancements primarily targeted traditional FL, they underscored the potential for a more streamlined and efficient FL workflow. Our work bridges the gap between communication-efficient FL and one-shot learning, introducing OSFL as a novel approach that synergizes these concepts to maximize sustainability in IoT while minimizing the environmental footprint of machine learning operations."}, {"title": "2.4. Generative Learning in FL", "content": "Generative learning has emerged as a powerful technique in machine learning, enabling models to generate data samples that capture underlying data distributions [11]. In the context of FL, generative learning has been applied to enhance data privacy, as seen in the work of Nasr et al. [38] and Zhang et al. [58] on generative models, e.g., generated adversarial networks (GANs) and Stable Diffusion, for privacy-preserving data sharing. Furthermore, recent research by Zhang et al. [58] demonstrated the feasibility of generative models for one-shot learning tasks.\nGenerative Learning has also played a role in data augmentation for FL. Techniques such as federated data generation, as proposed by Xin et al. [52], leverage generative models to create synthetic data samples that help balance class distribution and improve model robustness in FL scenarios with imbalanced data. Furthermore, the application of generative models in one-shot learning tasks has demonstrated significant promise. Zhang et al. [57] explored the use of generative models for efficient adaptation to new tasks, enabling FL models to learn from a single example. This concept has implications for resource-efficient machine learning and can be integrated into the FL framework to optimize knowledge transfer with minimal communication.\nIn the context of our paper, we leverage the capabilities of generative learning within the FL workflow to introduce OSFL, a novel approach that combines generative learning and FL to maximize sustainability in IoT while minimizing communication overhead and energy consumption. This integration represents a pivotal advancement in the field, addressing the pressing need for eco-friendly and efficient machine learning operations in IoT environments. Building upon these foundations, our paper leverages generative learning to optimize the FL workflow, reducing communication overhead and fostering sustainable IoT by achieving learning objectives with minimal data transfer and device energy consumption."}, {"title": "3. Preliminaries", "content": "3.1. Federated Learning\nFL empowers decentralized participants to engage in collaborative machine learning model training without the need to share their private data with one another, as highlighted in [36]. In a conventional supervised FL framework, the structure typically comprises a server denoted as S and a set of K clients. Each client maintains a comprehensive labeled dataset represented as D, while the server itself does not possess any data. This architecture ensures data privacy and security during the collaborative training process. Specifically, a widely used FL training strategy (i.e. FedAvg [36]) is described as follows.\nAt the beginning of FL, the server initializes an ML model parameter \u03c9 and broadcasts it to a subset of clients (i.e., K0, and K0 \u2286 K) for local training. Afterward, for each communication round t, the clients in the selected subset Kt train the received global model \u03c9t on the local dataset. For instance, the minimization objective function for the k-th client training a local model \u03c9kt (i.e., \u03c9kt \u2190 \u03c9t) on dataset Dk = {(x1, y1), \u2026, (xnk, ynk)} can be formulated as follows.\n$L_{k} = \\frac{1}{n_{k}} \\sum_{i=1}^{n_{k}} l_{i}(y_{i}, p(y_{i}|x_{i}; w_{k}^{t})).$ (1)\nIn this context, we define the variables as follows: nk represents the number of training samples held by client k, li represents the loss function applied to the i-th training sample, yi denotes the ground-truth corresponding to xi, and p(yi|xi; wkt) signifies the probability vector predicted by model \u03c9kt for the i-th training sample. Concurrently, model \u03c9kt undergoes continuous updates until the completion of the local training process by client k.\n$w_{k}^{t+1} = w_{k}^{t} - \\eta \\nabla L_{k}(D_{k}; w_{k}^{t}).$ (2)\nIn this context, \u03b7 represents the learning rate, governing the magnitude of each model update step. Subsequently, within the subset Kt, each client uploads their locally trained model to the server for aggregation, with the aggregation formula outlined as follows.\n$w^{t+1} = \\sum_{k \\in K_{t}} p_{k} w_{k}^{t+1},$ (3)\nwhere $p_{k} = \\frac{n_{k}}{\\sum_{k \\in K_{t}} n_{k}}$ represents the contribution rate of the k-th client to the current global model. Again, the server selects a subset of clients for the next communication round to send down the global model \u03c9t+1 until the termination condition is reached.\n3.2. Text-to-Image Generative Models\nText-to-Image Generative Models (T2IGMs) [13, 32, 6] signify a groundbreaking advancement at the convergence of natural language processing and computer vision. These models are specifically engineered to produce visually coherent and contextually relevant images based on textual descriptions, effectively closing the traditional gap between linguistic and visual comprehension. In recent years, T2IGMs have witnessed notable advancements, emerging as a formidable tool with applications ranging from content creation to augmenting the capabilities of IoT systems [12, 16]. This section provides a comprehensive background and definition of T2IGMs, tracing their evolution and elucidating their profound impact across diverse domains.\nT2IGMs typically consist of two primary components: an encoder-decoder architecture and a generative model [13]. The encoder processes the textual input, converting it into a latent representation that captures the semantics of the text. The decoder, on the other hand, takes this latent representation and generates a corresponding image. Often, the generative model incorporates GAN-based techniques to ensure the generated images are both visually convincing and semantically faithful to the input text [18, 17]. The synergy between these components enables T2IGMs to understand and translate textual descriptions into high-quality images. The training of a T2IGM involves a GAN-like setup with a discriminator and generator [34]. The discriminator tries to distinguish between real images and images generated from text, while the generator aims to generate images that fool the discriminator. The loss function typically involves both a generator loss lgen and a discriminator loss ldisc:\n$L_{gen} = -log(D(\\text{ Generated Image }))$\n$L_{disc} = log(D(\\text{ Real Image })) \u2013 log(1 \u2013 D(\\text{ Generated Image })),$"}, {"title": "4. Methodology", "content": "4.1. Overview of Federated Generative Learning Framework\nFGL, as shown in Figure 1, is an innovative and emerging area of research that combines two powerful domains: FL and generative learning. FGL seeks to harness the potential of generative models to create synthetic data while preserving data privacy and decentralization principles in a federated environment. Here's an overview of the key components and concepts within Federated Generative Learning:\n\u2022 Generative Models for Privacy-Preserving Data Synthesis: FGL employs a generative model to create synthetic data samples on the server. This model uses generation prompts sent by each client to create synthetic data samples without centralizing them, preserving data privacy.\n\u2022 Edge Device Collaboration: In contrast to conventional federated learning approaches, which typically involve the transmission of features, parameters, or gradients, our method transmits prompts that correlate with the private data stored on clients to the central server. This novel approach enhances privacy preservation and communication efficiency.\n\u2022 Server-Side Aggregation: The central server aggregates the prompts from edge devices, enhancing the global generative model. This one-shot aggregation might involve techniques to balance the contributions from each device and maintain model quality.\n\u2022 Synthetic Data Generation: Once a robust global generative model is established, it can be employed to generate synthetic data samples that closely resemble the data distribution present on edge devices.\nFGL offers numerous advantages, including enhanced data privacy, reduced communication overhead, and the ability to create synthetic data for data augmentation. It has applications in various fields, especially IoT, to create synthetic IoT sensor data for model training while protecting device privacy. It is worth noting that our method does not require multiple communications and only requires one-shot communication to complete model training, which greatly promotes the realization of green and sustainable IoT.\n4.2. Local Prompt Generation\nIn this paper, we follow [58], and introduce Local prompt generation, as shown in Figure 2, which serves as a fundamental pillar in the groundbreaking paradigm of FGL. FGL seamlessly merges the principles of FL and generative learning, offering a privacy-preserving and communication-efficient approach to data-driven tasks. At the heart of this innovative approach lies the concept of Local prompt generation, which empowers individual edge devices within a federated network to contribute their private data without compromising data privacy or centralized control.\nIn this context, we utilize two distinct local prompt generation strategies [58], both of which are visually depicted in Figure 2. The first strategy is designed to characterize classes within the local data, while the second focuses on characterizing individual entities present in the local datasets. These two prompt generation strategies share a common objective: to extract essential features from the local training data, facilitating the server-side generation model's ability to effectively collaborate in data synthesis. Importantly, it is essential to emphasize that these strategies do not entail the transmission of model updates or gradients. This unique characteristic substantially mitigates communication overhead, aligning with the core principles of efficient and privacy-preserving FGL.\n4.3. Training Data Synthesis\nTraining data synthesis constitutes a pivotal phase within the innovative paradigm of FGL. FGL seamlessly integrates the principles of FL and generative learning, with the overarching goal of revolutionizing the execution of data-driven tasks while upholding data privacy and decentralized control. In the FGL framework, training data synthesis assumes a transformative role, enabling individual edge devices to actively contribute to the development of a global generative model without exposing sensitive data.\nTraining data synthesis in FGL commences with the generation of descriptive prompts by edge devices. These prompts encapsulate the essence of the local data they hold, serving as abstract yet informative representations. The prompts are designed to convey crucial insights about the data, such as class characteristics or entity attributes, without divulging raw data details. This synthesis of prompts aligns with the privacy-centric ethos of FL, as no actual data is shared during this process.\nUpon receiving all the prompts contributed by the edge devices, the central server embarks on the pivotal phase of data synthesis. This process involves the generation of every training sample, denoted as ti, by prompting the pre-trained generative models, e.g., Stable Diffusion model, with each prompt pi. We follow [58], the sequence of actions can be summarized as follows:\n$t_{i} = G (z_{i}, p_{i}) = \\sqrt{\\beta} \\sum_{t=1}^{T} \\sqrt{\\frac{1}{T}} \u22c5 G_{t}(z_{i}, p_{i}).$ (6)\nIn this process, zi represents a randomly generated noise vector, pi signifies the prompt generated by edge devices, and Gt(...) stands for the denoising network, which is parameterized with t at the specific time step t. The hyperparameter \u03b2 plays a pivotal role in balancing image quality against diversity, while T dictates the number of diffusion steps to be taken. This inference process follows an iterative pattern wherein the generated image undergoes denoising at each step before ultimately yielding the finalized synthetic training image. Subsequently, the central server orchestrates the generation of the synthetic training dataset, denoted as S, comprising pairs of training samples and corresponding labels, represented as $S = \\{(s_{i}, y_{i})\\}_{i=1}^{N}.$\nCrucially, training data synthesis in FGL operates without the exchange of model updates or gradients, a distinctive departure from traditional FL. Instead, it empowers edge devices to participate actively in the generative learning process without relinquishing control over their data. This approach not only enhances privacy preservation but also minimizes communication overhead, a critical consideration in resource-constrained environments like IoT.\n4.4. One-shot Learning\nOne-shot learning streamlines the knowledge transfer process within FGL. In traditional FL, multiple rounds of model updates are exchanged between edge devices and the central server, often leading to substantial communication overhead. In contrast, one-shot learning condenses this into a singular communication round, dramatically reducing the amount of data transferred. The core objective of one-shot updating is to enrich the global generative model with collective insights from edge devices. These insights are encapsulated in the prompts generated by edge devices, which offer descriptive information about their local datasets. The central server leverages this valuable input to enhance the capabilities of the generative model. Following the acquisition of the synthetic training set denoted as $S = \\{(s_{i}, y_{i})\\}_{i=1}^{N},$ our subsequent step involves the joint training of the model on the central server. Specifically, we employ the AlexNet architecture as the model of choice and utilize the cross-entropy loss function.\n$\\mathcal{L} = -\\frac{1}{N} \\sum_{i=1}^{N} y_{i} \\log (\\hat{y_{i}}) + (1 \u2212 y_{i}) \\log (1 \u2013 \\hat{y}}).$ (7)\nThe efficiency of one-shot updating contributes to the robustness and sustainability of FGL. It allows for the seamless integration of generative learning techniques into IoT and other data-sensitive domains, where resource-efficient machine learning is essential. Therefore, one-shot updating in FGL embodies the essence of efficient and privacy-conscious knowledge transfer. It empowers edge devices to contribute effectively to the generative learning process while preserving data privacy and minimizing communication overhead.\n4.5. Complexity Analysis\nHere, we focus on analyzing the algorithm complexity on the client side. It can be seen from the above algorithm that on the client side, the client needs to generate local prompts and upload the generated prompts. For the sake of simplicity, we let the complexity of generating a prompt be C (depending on the specific generation algorithm), then the algorithm complexity of clients performing local prompt generation operations is O(KCN). In addition, considering that the upload operation can be completed in one go, the complexity is O(N)."}, {"title": "5. Experomental Results", "content": "5.1. Experiment Setup\nThe experiments are conducted within a uniform computing environment, comprising Linux Ubuntu 10.04, an Intel i5-4210M CPU, 32GB RAM, and a 1024GB SSD. The implementation is facilitated through the utilization of the Pytorch and Foundation Model libraries.\nDatasets. We adopt three image datasets for evaluations, i.e., Fashion-MNIST [51], CIFAR-10 [27], and CIFAR-100 [27]. The datasets cover different attributes, dimensions, and a number of categories, allowing us to explore the poisoning effectiveness of FGL.\nFashion-MNIST. Fashion-MNIST is a popular dataset in the field of machine learning and computer vision. It serves as an alternative to the traditional MNIST dataset but offers more challenging and diverse tasks. F-MNIST consists of 28x28 grayscale images of fashion items, such as clothing and accessories, categorized into ten different classes. These classes include items like T-shirts, dresses, sneakers, handbags, and more. F-MNIST is often used for benchmarking and evaluating machine learning algorithms and models, especially for tasks related to image classification, feature extraction, and deep learning.\nCIFAR-10. The CIFAR-10 dataset is another well-known dataset in the field of computer vision. It contains 60,000 color images, divided into ten classes, with each class representing a different object category. Each image in CIFAR-10 has a resolution of 32x32 pixels and is labeled with one of the following categories: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, or truck. CIFAR-10 is widely used for research in image classification, object recognition, and deep learning due to its diversity and complexity.\nCIFAR-100. CIFAR-100 is an extension of the CIFAR-10 dataset and is designed for more fine-grained image classification tasks. It also consists of 60,000 images, but this time they are divided into 100 different classes. Each class represents a more specific object category, making CIFAR-100 suitable for tasks where distinguishing between closely related objects is required. The dataset includes objects like different types of birds, insects, and vehicles. CIFAR-100 is commonly used in research for tasks involving fine-grained object recognition, hierarchical classification, and model evaluation.\nData Partition. In FGL, data partitioning plays a pivotal role in shaping the training process. Setting up IID (Independent and Identically Distributed) data involves ensuring that each edge device's data is representative of the entire dataset and follows the same data distribution. To achieve this, the dataset is divided equally or approximately equally among the edge devices, ensuring that each device receives a balanced representation of data classes and characteristics. On the other hand, setting up non-IID (Non-Independent and Non-Identically Distributed) data in FGL involves recognizing the diversity of data among edge devices. Each device may possess a unique data distribution (we employ the Dirichlet distribution [28]), varying class proportions, or distinct data characteristics.\nModels. For our experimental setup, we employ a straightforward deep learning model, specifically a Convolutional Neural Network (CNN) comprising two convolutional layers, succeeded by a single fully connected layer. This architecture is utilized for classification tasks involving the F-MNIST datasets. In contrast, for classification tasks related to the CIFAR-10 and CIFAR-100 datasets, we utilize the AlexNet [22] model.\n5.2. Numerical Results\nPerformance Evaluation. In this section, we first evaluate the system performance of the proposed FGL under IID and non-IID data settings. As shown in Table 2 and Table 3, we can find that the system performance of FGL on the F-MNIST and CIFAR-10 data sets is better than the baseline schemes. Specifically, on the CIFAR-10 dataset, the performance of FGL is 5.67% higher than that of FedAvg. In addition, we can also find that the system performance of FGL is comparable to the baselines on the CIFAR-100 data set. After thorough research and reflection, we can get the following lessons:\n(Takeaway-1) 1) FGL's system performance is generally better than the baseline and does not require multiple rounds of communication, thanks to the powerful learning ability of the basic model. 2) Non-IID data setting does not significantly affect the system performance of FGL. 3) The OSFL method does not affect system performance.\nCommunication Efficiency Evaluation. Second, we aim to explore the communication overhead between FGL and baselines. Since FGL utilizes one shot learning and updating methods, we expect that the communication overhead of FGL is much smaller than that of baselines. As shown in Figures 3 and 4, we find that the communication overhead of FGL under IID and non-IID settings is approximately 1/5 of the FedAvg scheme. Note that since centralized training does not involve communication protocols it is not compared here. Furthermore, we note that the running time of FGL is approximately 1/2 of the baseline solutions, as shown in Figure 7 and Figure 6. This means that FGL has significant advantages in communication efficiency and operating efficiency, and is especially suitable for resource-constrained IoT environments. After thorough research and reflection, we can get the following lessons:\n(Takeaway-2) 1) FGL benefits from the one-shot learning and updating method, which greatly reduces its communication overhead. 2) Since FGL no longer requires local training, its running efficiency has also been accelerated. 3) In resource-constrained IoT environments, FGL can achieve a perfect trade-off between model utility and resource energy consumption.\nHyperparameter Evaluation under Number of Synthetic Data. Third, we explore the impact of different amounts of synthetic data on system performance. Specifically, we set the number of synthetic data to N = {2k, 5k, 10k}, and record the system performance of FGL in these three cases. Table 4 reports the performance results of FGL, and we can find that as the amount of synthetic data increases, the performance of FGL continues to improve. However, this phenomenon cannot continue indefinitely. When the performance limit is reached, the increase in the amount of synthetic data is no longer beneficial to model training. After thorough research and reflection, we can get the following lessons:\n(Takeaway-3) 1) The number of synthetic data has an impact on the performance of FGL, and within a certain range, the larger the number, the more beneficial it is for model training. 2) Increasing the amount of synthetic data is a simple and economical way to improve performance.\nVisual Analysis of FGL's Generalization Ability. As shown in Fig. 7, we visualized the loss landscape of FGL and naive FL and found that the FGL framework not only has a more balanced optimization curve, but also has a lower loss value and is easier to obtain the optimal value. This means that although FGL is one-shot communication, it still has good convergence and generalization capabilities.\nPerformance Evaluation under Different Prompt Methods. Finally, we explore the impact of different prompt methods on FGL system performance. As mentioned above, this article designs two local prompt strategies. We evaluate these strategies by comparing the system performance under different prompt strategies. Table 5 reports the results. We found that using a more precise entity-level prompt strategy leads to higher accuracy, for example, reaching 68.62% on CIFAR-10 compared with the accuracy of the class-level prompt strategy. This result clearly highlights the importance of considering more detailed entity-level information in prompt strategy design. However, the entity-level prompt policy involves specific real image information, which may cause potential privacy issues, and we leave it as future work.\n(Takeaway-4) 1) Both of the designed prompt strategies can effectively achieve the established goals. 2) The effect of entity-level prompt strategy is better than that of class-level prompt strategy. 3) Although the effect of the entity-level prompt policy is better than the class-level prompt policy, the entity-level prompt policy has certain privacy concerns."}, {"title": "5.3. Discussion", "content": "Privacy has been a paramount concern in IoT ecosystems, and our One-shot FGL approach offers a pragmatic solution. By allowing edge devices to share abstract prompts instead of raw data, we strike a balance between data utility and privacy preservation. This design mitigates the risk of data breaches and unauthorized access while enabling the collaborative generation of synthetic data. The localization of data aggregation further fortifies privacy, as prompts, not individual data samples, are aggregated. The privacy-centric nature of FGL aligns with the ethical use of data and user expectations for data confidentiality in the IoT era.\nSustainability is at the core of our approach. By reducing the need for extensive data transmission and centralization, FGL contributes to the ecological sustainability of IoT systems. The conservation of energy and bandwidth resources not only supports environmental goals but also enhances the operational longevity of IoT devices. FGL's ability to generate high-quality synthetic data from decentralized sources promotes sustainable practices by minimizing the environmental footprint of data transfer and storage. This integration of sustainability and privacy-consciousness positions FGL as a pioneering approach in building responsible and eco-friendly IoT ecosystems.\nWhile our approach shows promise, several challenges and future directions warrant consideration. One key challenge is optimizing the generative model's performance while working with decentralized and diverse data sources. Developing advanced generative techniques that can adapt to various data distributions across edge devices is essential. Additionally, research in federated learning orchestration and model aggregation techniques needs further exploration to maximize the utility of FGL.\nFinally, it is crucial to highlight that the proposed framework exhibits flexibility and can be readily extended to various tasks. The adaptability stems from the ability to employ different generation models for prompt generation corresponding to diverse task data. For instance, the VisualGLM model can be employed to generate prompts and subsequently facilitate learning from video data. The scalability of the framework across different tasks is confirmed, underscoring the importance of task scalability in a learning framework."}, {"title": "6. Conclusions", "content": "In this paper, we have introduced and delved into the concept of OSFL as an innovative approach aimed at addressing the dual imperatives of sustainability and privacy within the realm of the IoT. OSFL, characterized by its reduction of communication rounds and optimization for energy efficiency through generative learning, holds significant promise for reshaping the landscape of machine learning in IoT ecosystems. Our research has successfully demonstrated the feasibility and effectiveness of OSFL in achieving sustainable IoT operations. By condensing the traditional federated learning process into a single communication round, OSFL substantially mitigates energy consumption, communication overhead, and latency. Moreover, the integration of generative learning techniques ensures the preservation of data privacy, even as knowledge is efficiently shared among IoT devices. OSFL stands as a novel and"}]}