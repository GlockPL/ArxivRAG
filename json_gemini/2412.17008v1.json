{"title": "DATA VALUE ESTIMATION ON PRIVATE GRADIENTS", "authors": ["Zijian Zhou", "Xinyi Xu", "Daniela Rus", "Bryan Kian Hsiang Low"], "abstract": "For gradient-based machine learning (ML) methods commonly adopted in practice such as stochastic gradient descent, the de facto differential privacy (DP) technique is perturbing the gradients with random Gaussian noise. Data valuation attributes the ML performance to the training data and is widely used in privacy-aware applications that require enforcing DP such as data pricing, collaborative ML, and federated learning (FL). Can existing data valuation methods still be used when DP is enforced via gradient perturbations? We show that the answer is no with the default approach of injecting i.i.d. random noise to the gradients because the estimation uncertainty of the data value estimation paradoxically linearly scales with more estimation budget, producing estimates almost like random guesses. To address this issue, we propose to instead inject carefully correlated noise to provably remove the linear scaling of estimation uncertainty w.r.t. the budget. We also empirically demonstrate that our method gives better data value estimates on various ML tasks and is applicable to use cases including dataset valuation and FL.", "sections": [{"title": "INTRODUCTION", "content": "With growing data privacy regulations (Bukaty, 2019; Council of European Union, 2014) and machine learning (ML) model attacks (Shokri et al., 2017), privacy has become a primary concern in many scenarios such as collaborative ML (Sim et al., 2020; Xu et al., 2021) and federated learning (FL) (McMahan et al., 2017; Yang et al., 2019). Differential privacy (DP) (Dwork & Roth, 2014) is commonly adopted as the de facto framework to provide privacy protection for training data with theoretical guarantees. In deep learning, DP is typically achieved by perturbing the gradients w.r.t. the model's loss on the training data (Abadi et al., 2016).\nData valuation (Ghorbani & Zou, 2019) has received increased interest from providing attribution for parties in collaborative ML (Sim et al., 2022) and identifying data quality for data curation (Ghorbani & Zou, 2019) and data marketplace (Agarwal et al., 2019). Data values are often estimated with sampling-based methods such as Monte Carlo (Castro et al., 2009) on user statistics, e.g. gradients of user data (Ghorbani & Zou, 2019). However, the sensitive nature of the user statistics requires privacy during data valuation to protect the data of participants in collaboration (Sim et al., 2023) or to facilitate interaction between data buyers and sellers in a marketplace (Chen et al., 2023). Specifically, enforcing DP on the data is desirable in these scenarios for two reasons: 1) DP enjoys the post-processing immunity (Dwork & Roth, 2014), allowing further access to data without risking privacy leakage; 2) DP offers a natural trade-off between privacy protection and data quality: Data contributors can determine at their discretion the level of information protection at the expense of degraded data quality. While some previous works considered DP in data valuation (Sim et al., 2023; Wang et al., 2023; Watson et al., 2022), they either focused on a limited class of ML models or required a trusted central server, both of which are difficult to satisfy in real-world scenarios (Sim et al., 2022). A natural question arises: Can we circumvent these two challenges in data valuation while enforcing DP?\nOne might think of perturbing gradients on user data before using them for updating a gradient- trained parametric model to overcome the challenges. Unfortunately, we demonstrate that the naive"}, {"title": "RELATED WORK", "content": "Prior works (Sim et al., 2023; Wang et al., 2023; Watson et al., 2022; Usynin et al., 2024) that consider privacy-aware data valuation have limited applicability due to limited settings. (Sim et al., 2023) considered perturbing user statistics to ensure DP but is restricted on the class of Bayesian models whereas we consider a wider family of models trained with gradient-based methods including neural networks. (Wang et al., 2023) proposed a private variant of KNN-Shapley but did not generalize to other semivalues, whereas our method applies to all semivalues. Watson et al. (2022) directly perturbed the semivalue estimates. However, both (Wang et al., 2023; Watson et al., 2022) require a trusted server to centralize the original gradients which may not reflect real-world scenarios where untrusted central servers pose added privacy risks, whereas our approach does not require a trusted central server. (Usynin et al., 2024) assessed using variance of gradients and privacy loss-input susceptibility score to select useful data points for DP training. The authors further propose methods to compute the DP version of these scores. (Bani-Harouni et al., 2023) considered improving the performance of DP-SGD by utilizing cosine similarity between privatized per-sample gradient and original gradient to decide whether to include the gradient in averaged gradient.\nLi & Yu (2023); Wang & Jia (2023) identified that stochastic utility functions can lead to noisy data values which deteriorated the rank preservation and proposed to use (weighted) Banzhaf values as the"}, {"title": "PRELIMINARIES", "content": "We recall the definition of semivalue for data valuation (Ghorbani & Zou, 2019) and the necessary preliminaries on DP."}, {"title": "DATA VALUATION", "content": "Semivalues. Denote $[n] := \\{1, 2, ..., n\\}$. The semivalue of $i$ in a set $[n]$ of parties w.r.t. a utility function $V : 2^{[n]} \\rightarrow \\mathbb{R}$ and a weight function $w : [n] \\rightarrow \\mathbb{R}$ s.t. $\\sum_{r=1}^{\\binom{n}{n-1}} w(r) = n$ is (Dubey et al., 1981)\n$$\n\\Phi_{i} := \\sum_{r=1}^{\\binom{n}{n-1}} w(r) \\sum_{S \\subseteq N\\{i\\},\\S\\=r-1}[V(S\\cup \\{i\\}) - V(S)] .\n$$\nLeave-one-out (Cook, 1977), Shapley value (Shapley, 1953), and Banzhaf value (Wang & Jia, 2023) are examples of semivalues. In data valuation, a party can be represented by a data point, a dataset, or (the data of) an agent in FL setting. In ML, semivalues are often treated as a random variable and estimated using Monte Carlo methods (Castro et al., 2009; Maleki et al., 2014) since $[n]$ is usually large and $V$ is stochastic (more details in App. B.1). Denote $\\mathcal{P}_{r}$ as the set of predecessors of party $j$ in a permutation $\\pi$ uniformly randomly drawn from the set of all permutations $\\Pi$, and let $p_j(\\pi) := 2^{n-1}w(|P_j \\cup \\{j\\}|)$, then $\\Phi_{j} = \\mathbb{E}[\\psi_j]$ with $\\psi_j$ an average over $k$ random draws:\n$$\n\\psi_{j} = (1/k) \\sum_{t=1}^{k} p_{j}(\\pi^{t})[V(P_{\\pi^{t}} \\cup \\{j\\}) - V(P_{\\pi^{t}})] .\n$$"}, {"title": "DIFFERENTIALLY PRIVATE MACHINE LEARNING (DP ML)", "content": "Definition 3.1 (($\\epsilon$, $\\delta$)-Differential Privacy (Dwork & Roth, 2014, Def. 2.4)). A randomized algorithm $\\mathcal{M}$ with domain $\\mathcal{D}$ and range $\\mathcal{R}$ is said to be ($\\epsilon$, $\\delta$)-differentially private if for any two neighboring\u00b9 datasets $d, d' \\in \\mathcal{D}$, and for all event $S \\subseteq \\mathcal{R}$, $Pr(\\mathcal{M}(d) \\in S) \\leq exp(\\epsilon)Pr(\\mathcal{M}(d') \\in S) + \\delta$.\nImportantly, the DP guarantee of $\\mathcal{M}$ is immune against post-processing (Dwork & Roth, 2014, Proposition 2.1): The composition $f \\circ \\mathcal{M}$ with an arbitrary randomized mapping $f$ have the same DP guarantee as $\\mathcal{M}$. We adopt this definition of DP to show the linearly scaling effect of perturbation (Sec. 5.1). We elaborate in App. B.2 that our analysis can be extended to other DP frameworks."}, {"title": "SETTINGS AND PROBLEM STATEMENT", "content": "Settings. Our analysis is based on the G-Shapley framework (Ghorbani & Zou, 2019) for gradient- based ML methods where a parametric ML model learns from the data of each party via the perturbed gradients of the data against a deterministic loss function $L : [n] \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$ which maps the (data of a) party and model parameters $(\\in \\mathbb{R}^d)$ to a score $(\\in \\mathbb{R})$. The utility improvement reflects the data value after the model updates its parameters with the gradient. For an evaluation budget $k$, $k$ uniformly random permutations $\\pi^{1}, ..., \\pi^{k} \\in \\Pi$ are sampled, and for each sampled permutation $\\pi$ (superscript omitted), a model is randomly initialized with $\\theta_{0}$ and updated by the parties in sequence according to $\\pi$. Then, denote $\\theta_{\\pi, j}$ the model parameters immediately before party $j$ updates the model in permutation $\\pi$. For each subsequent party $j$ in $\\pi$, the Gaussian mechanism is adopted to obtain a perturbed gradient $\\hat{g}_{\\pi, j}$ to update the model $\\theta_{\\pi, j} := \\theta_{\\pi, i} - a\\hat{g}_{\\pi, j}$ where"}, {"title": "APPROACH AND THEORETICAL RESULTS", "content": "We summarize the results on estimation uncertainty for the naive approach (i.i.d. noise), our method, and a variant in Table 1. All proofs and derivations are deferred to App. C."}, {"title": "I.I.D. NOISE CAUSES SCALING ESTIMATION UNCERTAINTY", "content": "We first reveal that the propagation of noise from each $z_{t}$ to $V$ can be catastrophic to the estimator $\\psi$: a higher evaluation budget $k$, surprisingly, leads to a higher estimation uncertainty.\nThe estimator $\\psi$ in Eq. (2) aggregates various marginal contributions $m(\\pi^{t}) = V(\\theta_{\\pi^{t},j}) - V(\\theta_{\\pi^{t}, i})$ over different $\\pi \\in \\Pi$. Thus, if $\\psi$ requires $k$ evaluations of $m$, party $j$ needs to reveal its gradients $k$ times. The repeated release of gradients increases the privacy risk, requiring greater perturbation to maintain the same DP guarantee. In the Gaussian mechanism, the best known variance of the Gaussian noise grows in $\\Omega(k)$ (Abadi et al., 2016, Theorem 1) (more discussed in App. B.3), so each Gaussian noise is expressed as $z_{t} \\sim \\mathcal{N}(0, k(C\\sigma)^{2}I)$ where both $C$ and $\\sigma^{2}$ are constants satisfying a fixed $(\\epsilon, \\delta)$-DP guarantee hereafter.\nThe linear scaling of $Var[z_{t}]$ creates a vicious cycle: More marginal contributions are needed to obtain a more certain semivalue estimation, incurring a larger $k$, which in turn necessitates greater perturba- tions for DP and increasing the estimation uncertainty. Formally, we show that the perturbations used to guarantee DP are amplified by a strongly concave utility function $V$ (achievable with a strongly convex loss w.r.t. model parameters, such as K-Means, Lasso, and logistic regression with weight decay), and causes the estimation uncertainty of semivalues to grow in the order of $\\Omega(k)$, via two specific globally strongly convex loss functions:\nProposition 5.1. (I.I.D. Noise) $\\forall t \\in [k]$, denote $\\theta_{\\pi^{t}} := \\theta_{\\pi^{t}} - a\\hat{g}_{\\pi^{t}} = \\theta_{\\pi^{t}} - a(\\hat{g}_{\\pi^{t}} + z_t)$ where $\\forall t \\in [k]$, $z_t \\stackrel{iid.}{\\sim} \\mathcal{N}(0, k(C\\sigma)^{2}I)$ under the Gaussian mechanism. Denote a test dataset $D_{test} = \\{(x_1, y_1), ..., (x_l, y_l)\\}$ of $l$ data points. If $V$ is the negated mean-squared error loss on a linear regression model\n$$\nV(\\theta) := -l^{-1} \\sum_{i=1}^{l}(\\theta^{T}x_i - y_i)^{2},\n$$"}, {"title": "CORRELATED NOISE TO REDUCE ESTIMATION UNCERTAINTY", "content": "The key to mitigating this \u201clinearly scaling\" estimation uncertainty lies in reducing the variance of the noise $z_{t}$ in each iteration to render it less significant after amplification when propagated to the loss (equivalently $V$). Taking advantage of the way data values are estimated where private gradients are continuously released in each iteration, we can add a carefully correlated noise $z$ to the gradients $\\hat{g}_{\\pi^{t}}$ instead of independent noise $z_{t} \\sim \\mathcal{N}(0,k(C\\sigma)^{2}I)$ while achieving the same DP guarantee.\nConstructing $z$ exploits the post-processing property of DP: reusing previously released private statistics does not affect the DP guarantee level. At each iteration $t$, instead of directly injecting i.i.d. noise to $\\hat{g}_{\\pi^{t}}$ to become $\\bar{g}_{\\pi^{t}} := \\hat{g}_{\\pi^{t}} + z_{t}$, estimation uncertainty can be reduced by reusing the i.i.d.-perturbed gradients $\\hat{g}_{\\pi^{1}}, ..., \\hat{g}_{\\pi^{k}}$. To ease the understanding of the core idea, we begin by assuming that the original gradients at each iteration are identical, i.e. $\\hat{g}_{\\pi^{1}} = \\hat{g}_{\\pi^{2}} = ... = \\hat{g}_{\\pi^{k}}$, and relax it later. Instead of $\\bar{g}_{\\pi^{t}}$, a weighted sum (e.g., $\\bar{g}^{*}_{t} := t^{-1} \\sum_{l=1}^{t} \\hat{g}_{\\pi^{l}}$) of previously released private gradients can be utilized. This implicitly constructs the correlated noise $z^{*} := t^{-1} \\sum_{l=1}^{t} z_{l} \\sim \\mathcal{N}(0, (k/t)(C\\sigma)^{2}I)$, resulting a reduction in variance by a factor of $t$. To see how this variance reduction is propagated to $V$, compare the variance on the following strongly convex function (to mimic $V$): $Var[||g_t||^{2}] \\rightarrow t^{-2}Var[||\\bar{g}_{\\pi^{t}}||^{2}]$ as $k \\rightarrow \\infty$ (see Obs. C.1). Notice that the variance reduction is amplified from $t^{-1}$ to $t^{-2}$. Indeed, such variance reduction can lead to an $O(log^{2} k)$ bound for the estimation uncertainty (see Prop. C.9). However, two questions remain: (i) Is the \u201cidentical gradient\" assumption satisfied for semivalue estimation, and what if it is not? (ii) How to cleverly reuse previously privatized gradients to obtain a lower estimation uncertainty?\nFor question (i), unfortunately, the assumption is not satisfied: The gradients at each permutation $\\hat{g}_{\\pi^{t}}$, though obtained from the same underlying data, are not identical in general since $\\theta_{\\pi^{t},j}$ are different for different $\\pi^{t}$. Specifically, consider a party $j$ whose unperturbed gradients in $k$ evaluations are $g_{\\pi^{1}}, g_{\\pi^{2}},..., g_{\\pi^{k}}$. The unperturbed gradients are first injected with a Gaussian noise to produce the perturbed gradients $\\hat{g}_{\\pi^{t}} = \\hat{g}_{\\pi^{t}} + z_{t}$ for $t \\in [k]$. Then, party $j$ will release the gradient\n$$\n\\bar{g}^*_{t} := t^{-1} \\sum_{l=1}^{t} \\hat{g}_{\\pi^{l}} = t^{-1} \\sum_{l=1}^{t} \\hat{g}_{\\pi^{l}} + t^{-1} \\sum_{l=1}^{t} z_{l}\n$$\nwhich, however, is not an unbiased estimator for $\\hat{g}_{\\pi^{t}}$ since $E[\\bar{g}^*_{t}] = t^{-1} \\sum_{l=1}^{t} \\hat{g}_{\\pi^{l}} \\neq \\hat{g}_{\\pi^{t}}$ unless $g_{\\pi^{1}} = g_{\\pi^{2}} = ... = g_{\\pi^{t}}$ (i.e., identical gradients). Nevertheless, empirical observations suggest that as compared to $\\hat{g}_{\\pi^{t}}, \\bar{g}^{*}_{t}$ is much \"closer\" to $\\hat{g}_{t}$ in terms of the mean difference in cosine similarity\n$$\n\\Delta_{cos} := n^{-1} \\sum_{j\\in[n]} k^{-1} \\sum_{t=1}^{k} [cos(g^{*}_{\\pi^{t},j}, \\hat{g}_{\\pi^{t},j}) - cos(\\bar{g}_{\\pi^{t},j}, \\hat{g}_{\\pi^{t},j})]\n$$"}, {"title": "MORE VARIANCE REDUCTION WITH NON-SQUARE MATRIX", "content": "The bound in Prop. 5.3, though asymptotically better than $\\Omega(k)$, still grows in terms of $k$: estimates still become worse even with more samples. Inspired by the \u201cburn-in\u201d technique (Neiswanger et al., 2014) widely employed in MCMC, we show that it is possible to achieve a constant bound via a non-square matrix (i.e., relaxing P1). For an arbitrary square matrix X, consider its counterpart Y defined with a hyperparameter $q \\in (0,1), \\mathbb{Y}_{(k-kq)\\times k} := \\mathbb{X}_{(kq +1 : k; 1 : k)}$ where the bracket means taking a sub-matrix with the selected rows and columns. In other words, given a $\\mathbb{X}$, $\\mathbb{Y}$ is a"}, {"title": "EXPERIMENTS", "content": "We fix $C = 1.0$ and $(\\epsilon = 1, \\delta = 5 \\times 10^{-5})$-DP guarantee unless otherwise specified. All experiments are repeated over 5 independent trials. We focus on classification tasks as they are more susceptible to noise and defer regression task to App. D.3. We consider data selection and noisy label detection tasks as standard evaluations of the effectiveness of a data value estimate (Ghorbani & Zou, 2019; Kwon & Zou, 2022; Wang & Jia, 2023; Zhou et al., 2023). Exploiting Prop. 5.3, we set a large $k \\geq 200$ (except FL) and use $\\mathbb{X}^*$ and $\\mathbb{Y}^*$ for adding correlated noise. Additional experimental settings and results are in App. D.\nWhile our theoretical results have provided a $(\\epsilon, \\delta)$-DP guarantee level, in App. D.1, we verify the privacy protection of our method by constructing a membership inference attack (MIA) following the setting in (Wang et al., 2023), and demonstrate that our method can successfully defend against the constructed MIA. For experiments in the main text, we focus on how our method improves the data value estimation with privacy protection."}, {"title": "INCREASING k UNDER I.I.D. NOISE Does Not REDUCE THE ESTIMATION UNCERTAINTY", "content": "We empirically show the scaling estimation uncertainty as the evaluation budget $k$ increases and its implication. As a setup, we randomly choose 400 training examples from the diabetes dataset (Efron et al., 2004) with the remaining data points as the test dataset. We train a logistic regression using the negated cross-entropy loss on the test dataset as the utility function $V$. To evaluate the estimation uncertainty, we first examine the quality of data value estimates through mean- adjusted variance of $V_j$ (Zhou et al., 2023), which is the ratio between the empirical variance $\\delta_j^2 := k^{-1}(k - 1)^{-1} \\sum_{t=1}^{k}[m_j(\\pi^t) - \\mu_j]^2$ and the empirical mean $\\mu_j := k^{-1} \\sum_{t=1}^{k}m_j(\\pi^t)$. We also examine how test accuracy changes when removing training examples with the highest $\\Phi$'s. The leftmost figure of Fig. 2 shows that the mean-adjusted variance $\\sigma_{i.i.d.}^2/|\\mu_{i.i.d.}|$ with i.i.d. noise increases with $k$, indicating greater estimation uncertainty, whereas using correlated noise, $\\sigma_{corr.}^2/|\\mu_{corr.}|$ not only decreases but is also smaller by several magnitudes (in 10\u2075). Moreover, $\\mu_{i.i.d.}$.'s computed with i.i.d. noise are increasingly negative as $k$ increases, whereas $\\mu_{corr.}$.'s computed with correlated noise stay positive, suggesting that the estimated $\\Phi$'s are less affected by noise. The 3rd figure of Fig. 2 shows that $\\Phi$'s computed with greater $k$ produces higher test accuracy during removal (i.e., closer to random removal), verifying our identified paradox that $\\Phi$'s computed with more budget are poorer estimates of data values."}, {"title": "CORRELATED NOISE IMPROVES THE QUALITY OF THE ESTIMATES", "content": "Following the same setup as Sec. 6.1, we compute the $\\Phi$'s using correlated noise (with $q = 0.8$) and no injected noise due to DP respectively. The results are shown in the rightmost figure of Fig. 2. Removing data points with high $\\Phi$'s computed with no injected noise produces low test accuracy close to that with correlated noise. In contrast, the curve of $\\Phi$'s computed with i.i.d. noise lies far above (i.e., close to random removal), suggesting that $\\Phi$'s computed with correlated noise are much more reflective of the true worth of data as compared to $\\Phi$'s computed with i.i.d. noise.\nAblation study on the influence of $q$. We study how much $q$ affects the data value estimation in a noisy label detection setting on two datasets. We randomly perturb 30% of labels on a selection of 800 training examples from Covertype dataset (Blackard, 1998) (and MNIST dataset (LeCun et al., 1990) in App. D.4) respectively. The datasets are trained with logistic regression (LR) and a more complex convolutional network (CNN). Ideally, a good data value estimate should assign the lowest $\\Phi$'s to the perturbed training examples. To measure this, we plot the AUC-ROC curve (AUC) in Fig. 3 with $q = 0$ equivalent to $\\mathbb{X}^*$. With increased k, the AUC with our method increases especially in the large-$q$ region while the AUC with i.i.d. noise decreases. We also observe that AUC generally increases with $q$ when $q \\leq 0.9$.\nAdopting other utility functions. While our theoretical analysis is w.r.t. negated loss as the utility function $V$, we demonstrate similar results with test accuracy as $V$, shown in the right column of Fig. 3. Our method still outperforms i.i.d. noise, although the AUC is lower than when $V$ is negated loss. We think this is because accuracy is a less fine-grained metric. Hence subtle changes in the model performance (often due to perturbation) in this task cannot be well reflected. Moreover, we observe that as $q \\rightarrow 1$, AUC degrades, consistent with the result in Prop. 5.4.\nExperiments on other semivalues. We compare the effect of utilizing correlated noise with data Banzhaf (Wang & Jia, 2023) and Beta Shapley (Kwon & Zou, 2022) in Table 2. We consider the same setup as Sec. 6.2 with $k = 1000$. We compare the performance using $\\mathbb{X}^*$ and $\\mathbb{Y}^*$ with $q \\in \\{0.5, 0.9\\}$. For all variants, the AUC is higher than that with i.i.d. noise. Particularly, $q = 0.9$ works the best for all 4 semivalues tested. In contrast, the AUCs with i.i.d. noise using data Banzhaf are $\\approx 0.53$ on both datasets, close to randomness (0.5). The results show that our method generalizes to various data valuation metrics. We also note a similar improvement for LOO (Cook, 1977) in App. D.4 despite it not being a regular semivalue."}, {"title": "APPLICATION TO OTHER USE CASES", "content": "We verify the effectiveness of using correlated noise for dataset valuation (Wu et al., 2022) and collaborator attribution in federated learning (Wang et al., 2020) (refer to App. D.2 for setup). We consider MNIST dataset and CIFAR10 (Krizhevsky et al., 2012) dataset, trained on CNN and fine- tuned on pretrained ResNet18/34 (He et al., 2015). We tabulate the difference in AUC between using correlated noise with $q = 0.9$, denoted as $\\Delta_{corr.} := AUC_{no DP} - AUC_{corr.}$, and using i.i.d. noise, denoted as $\\Delta_{i.i.d.} := AUC_{no DP} - AUC_{i.i.d.}$. For ResNet34, we use $\\epsilon = 10$ as the model is more complex, causing both i.i.d. and our method to have degraded performance with strict privacy. Our methods outperform i.i.d. noise as shown in Table 3 top. For FL, we notice that the continually"}, {"title": "CONCLUSION, LIMITATIONS, AND FUTURE WORKS", "content": "In this work, we identify a problem in data valuation where DP is enforced via perturbing gradients with i.i.d. noise: The estimation uncertainty scales linearly (i.e., $\\Omega(k)$) with more budget $k$ and renders the data value estimates almost useless (i.e., close to random guesses in some investigated cases). As a solution, we propose to use correlated noise and theoretically show that using a weighted sum via matrix form using $\\mathbb{X}$ provably reduces the estimation uncertainty of semivalues from $\\Omega(k)$ to $O(1)$ and empirically demonstrate the implications of our method on various ML tasks and data valuation metrics. One limitation is the need to store the gradients. However, this limitation is alleviated when the number of parties is small (e.g. dataset valuation) or when the memory load can be distributed across parties (e.g. FL). Another limitation is that our theoretical result assumes an diagonal multivariate sub-Gaussian distribution of the gradients. Nevertheless, we empirically demonstrate that our method works for neural networks where the assumption is not explicitly satisfied. A future direction is to explore other possible $\\mathbb{X}$ to reduce estimation uncertainty further."}, {"title": "APPENDIX", "content": null}, {"title": "TABLE OF NOTATIONS", "content": null}, {"title": "ADDITIONAL DISCUSSIONS", "content": null}, {"title": "SEMIVALUE AS A RANDOM VARIABLE.", "content": "The exact computation of semivalues is often intractable in practice due to the need to compute an exponential number of marginal contributions $V (S\\cup \\{i\\}) - V(S)$. Moreover, in data valuation, the utility function $V$ is not deterministic w.r.t. $S$: $V$ is commonly defined as the test accuracy or test loss (Ghorbani & Zou, 2019), which is stochastic due to (stochastic) gradient descent and random model initialization. Such stochasticity becomes more pronounced when gradients are injected with artificial noise to ensure privacy such as in DP-SGD (Abadi et al., 2016) (recalled later). Hence, the semivalue estimator $V_i$ is treated as a random variable whose randomness comes from the marginal contributions."}, {"title": "DIFFERENTIAL PRIVACY FRAMEWORK.", "content": "We emphasize that our analysis adopts this definition as we rely on the Gaussian mechanism and leverage its composition property (Dwork & Roth, 2014). Our analysis holds for other DP frame- works capturing privacy guarantee with Gaussian mechanism and possessing composition and post-processing properties, such as R\u00e9nyi DP (Mironov, 2017) and z-CDP (Bun & Steinke, 2016). Typically, both R\u00e9nyi DP and z-CDP satisfy post-processing immunity and have composition proper- ties. Moreover, both provide a DP guarantee for the Gaussian mechanism. Notably, while both DP frameworks provide a modestly tighter privacy analysis than compared to (Dwork & Roth, 2014), to the best of our knowledge, they still require the variance of the Gaussian noise to linearly scale with k to satisfy a given level of DP guarantee. In fact, some existing works have identified that the moment accountant method (Abadi et al., 2016) which we adopt is an instantiation of R\u00e9nyi DP (Ouadrhiri & Abdelhadi, 2022) translated back to ($\\epsilon$, $\\delta$)-DP. Lastly, we note that our theoretical analysis is independent of the choice of DP framework except for requiring $z_t \\sim \\mathcal{N}(0, k\\sigma^2I)$.\nGaussian mechanism. We note that other mechanisms exist for achieving privacy with DP guaran- tees. We choose the Gaussian mechanism for the convenience of mathematical analysis compared to other mechanisms such as the Laplace mechanism as well as a manifold of existing works discussing the theoretical properties of the Gaussian mechanism."}, {"title": "COMPOSITION OF DP MECHANISMS.", "content": "While it is possible to find a theoretical privacy budget lower than the result in (Abadi et al., 2016, Theorem 1), the current asymptotic bound $\\sigma^2 = \\Omega(k)$ is the best we know. We also note that there have been efforts at improving the bound (Asoodeh et al., 2021). However, the results are not asymptotically better in terms of $k$, hence do not affect our theoretical results. Moreover, we emphasize that our theoretical analysis and proof strategy do not depend on the exact form of $\\sigma^2$ other than assuming it depends on $k$. If a lower bound is found in the future, our analysis can be readily adapted.\nNote on per evaluation DP guarantee and final DP guarantee. We are concerned about the final DP guarantee (i.e., ($\\epsilon$, $\\delta$)-DP guarantee after $k$ compositions) unless otherwise stated. Notably, by setting $z_t \\sim \\mathcal{N}(k\\sigma^2I)$, each per-evaluation DP-guarantee is stronger, or put differently, the privacy budget required at each evaluation to satisfy a given ($\\epsilon$, $\\delta$)-DP guarantee is lowered such that, after $k$ compositions, the desired final DP guarantees can be achieved."}, {"title": "DP GUARANTEE LEVEL AND DATA VALUE ESTIMATES", "content": "Noise introduced by DP causes a lower mean of data value estimates. Mathematically, DP imposes a limit on the leave-one-out property of the privatized mechanism $\\mathcal{M}$. In particular, if $V(\\cdot) \\in [0,1]$ (e.g., when test accuracy is used as $V$), then we have, for any party $i$ and subsets $S\\subseteq [n] \\backslash \\{i\\}$, the following (Dwork et al., 2015, Lemma 6):\n$$\n|V_i := |\\mathbb{E}_S[V(S\\cup \\{i\\}) - V(S)]|| < e^{\\epsilon} - 1 + \\delta\n$$\nwhere ($\\epsilon$, $\\delta$) are the parameters satisfying ($\\epsilon$, $\\delta$)-DP. Assuming a fixed $\\delta$, stronger DP implies a lower $\\epsilon$, hence decreased right-hand side value, i.e., the upper bound for the marginal contribution. This inequality suggests that stronger DP results in lower absolute value of the data value estimates. We find this a reasonable behavior as the decreased value reflects the erosion of information carried by the data. Instead, having a lower absolute data value does not forbid us to still preserve the relative order of the estimates by minimizing the estimation uncertainty."}, {"title": "SOCIETAL IMPACT", "content": "As discussed in the introduction in the main text, we believe our contribution has a huge potential societal impact in improving privacy, especially with the rising awareness of protecting personal data (Bukaty, 2019; Council of European Union, 2014). We do not find a direct path to any negative societal impact with our contribution."}, {"title": "PROOF OF THE EXAMPLE IN SEC. 5.2.", "content": "Observation C.1. For a particular party $j$", "k": "the norm-clipped gradients $\\hat{g"}, {"k": "bar{g"}, {"pi^{t},j}": "hat{g"}, {"bar{g}^*_{tj}": "t"}]}