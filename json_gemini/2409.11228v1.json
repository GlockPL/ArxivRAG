{"title": "Learning Source Disentanglement in Neural Audio Codec", "authors": ["Xiaoyu Bie", "Xubo Liu", "Ga\u00ebl Richard"], "abstract": "Neural audio codecs have significantly advanced audio compression by efficiently converting continuous audio signals into discrete tokens. These codecs preserve high-quality sound and enable sophisticated sound generation through generative models trained on these tokens. However, existing neural codec models are typically trained on large, undifferentiated audio datasets, neglecting the essential discrepancies between sound domains like speech, music, and environmental sound effects. This oversight complicates data modeling and poses additional challenges to the controllability of sound generation. To tackle these issues, we introduce the Source-Disentangled Neural Audio Codec (SD-Codec), a novel approach that combines audio coding and source separation. By jointly learning audio resynthesis and separation, SD-Codec explicitly assigns audio signals from different domains to distinct codebooks, sets of discrete representations. Experimental results indicate that SD-Codec not only maintains competitive resynthesis quality but also, supported by the separation results, demonstrates successful disentanglement of different sources in the latent space, thereby enhancing interpretability in audio codec and providing potential finer control over the audio generation process.", "sections": [{"title": "I. INTRODUCTION", "content": "Audio codecs have long played a crucial role in audio data storage and transmission. Traditional methods [1], [2] rely on handcraft audio features and signal coding tools. These approaches compress the raw waveform into codec representations and then decompress it back to the original signal, leading to high-quality reconstruction at an adequate bitrate. Training neural networks to transform signals in codecs is known as neural audio codec (NAC). A typical pipeline consists of an encoder and a decoder for signal transformation, as well as a quantizer between them for information coding. Recent advances in NAC by using residual vector quantization (RVQ) [3]-[5] have achieved higher fidelity and lower bitrates compared to traditional methods. Moreover, the ability to reconstruct high quality audio from discrete latent codes has opened the door for realistic audio generation when combined with language models [6]-[9] or diffusion models [10].\nDespite the impressive performance of current NAC models, they are typically trained on a mixture of different audio datasets, compressing audio signals from diverse domains into a single latent space. This unified model neglects the fundamental differences between audio from various source domains. For example, the harmonics in human voice differ significantly from those in musical instruments, and the melodies, which are widely present in musics, are rarely seen in ambient sounds. Although, this discrepancy has been acknowledged in the traditional audio coding field with some solutions such as the Unified Speech Audio Codec (USAC) [11], there is still a lack of investigation into this issue in deep learning based methods. We argue that this may limit the performance in diverse audio environments, especially the explainability of latent features. Addressing this is crucial for future advancements in audio compression and generation.\nIn this work, we propose the Source Disentangled Neural Audio Codec (SD-Codec), a novel framework that integrates latent source disentanglement into the neural audio codec framework. Given an audio input that may contain multiple sources, we design multiple domain-specific quantizers, each corresponding to audio sources including speech, music, and sound effects (SFX) [12]. SD-Codec learns to disentangle the latent features and distribute features of source domains to different quantizers. The decoder of SD-Codec can reconstruct either a single source track from a specific quantizer, or reconstruct the mixture using the sum of quantized latent features. Furthermore, inspired from [13], we explore the source disentanglement of SD-Codec by sharing the last several layers of the quantizers. Aiming at a generalized audio codec model, we follow the instructions from [12] and [14] to build mixed training audio clips across different datasets and train our models on a large scale with around 6,000 hours, as shown in Tab. I. We evaluate our approach on an unseen dataset Divide and Remaster (DnR) [12] and compare it with state-of-the-art methods.\nOur key contributions can be summarized as follows:\n\u2022 We introduce SD-Codec, a neural audio codec that can reconstruct audio and separate sources (e.g., speech, music, sound effects) from input audio clips.\n\u2022 We investigate using a shared RVQ in SD-Codec. The similar performance with and without a shared codebook suggests that RVQ's shallow layers encode semantic information, while its deeper layers capture local acoustic details.\n\u2022 We train our model on large scale dataset and demonstrate that SD-Codec achieves strong performance both on audio reconstruction and source separation."}, {"title": "II. RELATED WORK", "content": "Early approaches for end-to-end neural audio codecs can be traced back to 1990s [15] and have seen significant advancements with the introduction of deep convolutional neural networks [16]. Concurrently, VQ-VAE [17] introduced a learnable quantizer between the encoder and decoder, optimizing them within a probabilistic modeling framework. This approach has proven effective for neural audio codecs operating at low bit rates [18]-[20]. Subsequently, residual vector quantization and adversarial training were incorporated into VQ-VAE-based neural audio codecs, typical models like SoundStream [3] and EnCodec [4] have demonstrated impressive performance in reconstruction quality. More recently, DAC [5] introduced vector factorization and L2-normalization to enhance codebook usage and achieves state-of-the-art on audio reconstruction quality.\nIn parallel to improving the quality of reconstruction, some studies dive into learning disentangled latent features for neural audio codec. Multiple encoders were introduced in [21] to extract phonemes, pitch and speaker identity respectively with different training targets. Later in NaturalSpeech3 [22], a unified encoder was shown to be sufficient. In addition to disentangling semantic information, Gull [23] learn to disentangle different bandwidth, whereas [24], [25] learn to separate noisy or reverberated speech. Our work is most related to [25], but we consider a more general case where human speech, music and environment sound effects are all mixed up, and we do not force the latent feature to disentangle each audio source into certain channels, but let the quantizer to handle it.\nWhile supervised speech separation has been widely studied over the decade [26], currrent approaches usually compress the audio signals into a latent space and learn a specific separation network to separate different audio sources, such as ConvTasNet [27] and ResUNet [28]. Latterly, TDANet [29] achieved great advance by using the feature pyramid and repeating the separation module multiple times. Apart from learning a separation model on a single dataset, recent works, such as GASS [14] and AudioSep [30], scale up the size of training data, targeting a single model for universal source separation."}, {"title": "III. METHOD", "content": "Our model is based on DAC [5], designed for 16 kHz. We describe the details of SD-Codec below."}, {"title": "A. Model Architecture", "content": "The architecture of our model is shown in Fig. 1. The input is a single-channel audio waveform $x \\in \\mathbb{R}^T$, this audio signal can either be a single source from speech, music, sound effects, or any combination of them. The encoder (Enc) consists of an input 1D convolution, followed by several residual convolution blocks, and ends up by another 1D convolution. The latent features obtained from the encoder are $z = Enc(x) \\in \\mathbb{R}^{F \\times D}$, where $F$ is the number of frames and $D$ is the latent feature dimension. The decoder (Dec) is a mirror structure of the encoder. We use the same hyper-parameters as in DAC-16kHz [5] for the architecture design, the latent features thus have a dimension of 1,024 and have 50 latent frames per second. For quantization, We use the residual vector quantizer (RVQ). Each VQ layer consists of a projection layer to compress the latent features to lower dimensional space (d = 8). The compressed latent features will be assigned to a nearest codebook centroid based on Euclidean distance after L2-normalization, then the quantized features will be projected back to its orignal dimension D. Different from DAC, SD-Codec have three domain-specific RVQs including RVQspeech, RVQmusic and RVQsfx. Each RVQ has R residual VQ layers and learns domain-disentangled latent features, which can either be used to reconstruct the audio for a specific source domain, or can be added up to reconstruct the mixture. We further design a variant of SD-Codec where all the domain-specific RVQs share the last S VQ layers, i.e. a RVQshared is concatenated after domain-specific RVQs. Fig. 2 gives an example of R = 4 and S = 2. For example, if we assume the codebook size in each layer is 2K, then the bits allocated for each frame is $R \\times K$. In practice, we use R = 12 and K = 10 to have a bitrate of 6,000 bits per second (6 kbps), which is the same as DAC-16kHz for single source resynthesis."}, {"title": "B. Source Disentanglement and Resynthesis", "content": "We consider a mixture speech, music and sound effects, the latent features from the encoder can be written as:\n$x_{mix} = x_{speech} + x_{music} + x_{sfx}$ (1)\n$z = Enc(x_{mix})$ (2)\nThen, each source-aware RVQ learns the specific features related to its source, formally:\n$\\hat{z}^{(0)} = 0$, (3)\n$\\hat{z}^{(i+1)} = VQ^{(i+1)}_s (z - \\sum_{r=0}^{i} \\hat{z}^{(r)})$, (4)\nwhere $s \\in \\{speech, music, sfx\\}$ and $VQ^{(i+1)}_s$ represents the $(i+1)$th quantization layer for the source $s$.\nTo reconstruct a single source such as speech:\n$\\hat{z}_{speech} = \\sum_{i=1}^{R} \\hat{z}^{speech}_i$, (5)\n$\\hat{x}_{speech} = Dec(\\hat{z}_{speech})$. (6)\nTo reconstruct the mixture, we assume the linear additivity in the latent space:\n$\\hat{z}_{mix} = \\hat{z}_{speech} + \\hat{z}_{music} + \\hat{z}_{sfx}$, (7)\n$\\hat{x}_{mix} = Dec(\\hat{z}_{mix})$. (8)\nWhile this shows the case of three-source mixture, it is also valid for the mixture of two sources. Here we need three times more bitrate for 3-source mixture than single source. To be noted that, if we share all the codebooks, i.e. S = R, then the SD-Codec will be the same as the DAC model, whereas if there is no codebook shared, then SD-Codec will consist of three RVQ in parallel."}, {"title": "C. Training Objective", "content": "Since SD-Codec is able to reconstruct four audio (three single source and one mixture), the final loss is:\n$L_{tot} = \\sum_{s} L_{g}(s, x_s)$, (9)\nwhere $s \\in \\{mix, speech, music, sfx\\}$. The $L_g$ is the same generator loss used in DAC [5], which consists of the multi-scale mel-spectrograms loss, the feature matching loss, the codebook loss and the commitment loss. Same as DAC, we also use a multi-period waveform discriminator (MPD) and a complex STFT discriminator to compute the feature matching loss and the adversarial loss. Following [25], we randomly shuffle the latent feature combination in the batch to enhance the feature disentanglement. Moreover, to balance the model performance on resynthesis and separation, we randomly select the number of audio tracks used to make up the mix. In our experiments, the probabilities of using 1, 2, and 3 tracks are 0.6, 0.2, and 0.2, respectively."}, {"title": "IV. EXPERIMENTS", "content": "We trained our models on a large dataset across speech, music, and sound effects. As summarized in Tab. I, we used the clean speech corpus and noise corpus from DNS Challenge 5 [31], MTG-Jamendo [32], WHAM! [34] and MUSAN [33]. In general, the training dataset is compiled of 2,619 hours of speech data, 3,819 hours of music data, and 261 hours of sound effects data. To evaluate the generalized performance of our models, we used the validation set and test set from DnR [12]. Since the DnR dataset is composed of artificial mixtures using speech, music, and sound effects which are different from our training sources, the validation and test can be considered as zero-shot, or out-of-distribution evaluation. We trimmed the leading and trailing silence in the mixture audio, and clipped the remaining audio into segments with a length either of 5-second (validation), or of 10-second (test). We also removed those segments where the individual tracks in the mix were too short (less than 50% of the mixture). Finally, the evaluation dataset consisted of 2,853 mixtures (\u2248 4 h) from the validation and 1,840 mixtures (\u2248 5 h) from the test.\nFor training, we followed the pre-processing in DnR data composition \u00b9. We first normalized each source to a specific loudness units full-scale (LUFS) [35]. For speech, music and sound effects, the target LUFS values were -17, -24 and -21 dB respectively, with additional random perturbation uniformly sampled from \u00b12. Then we normalized the peak value to -0.5 dB if any of the audio sources had a peak value larger than this threshold. Finally, we mixed up all the individual audio sources and normalize the mixture to -27 LUFS with random perturbation."}, {"title": "B. Evaluation", "content": "For evaluating the performance on audio resynthesis and source separation tasks, we both use Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) [36] for signal-level audio quality evaluation and ViSQOL [37] for perceptual audio quality assessment. Since each source has been normalized to different LUFS, we also report SI-SDR improvement (SI-SDRi) in source separation."}, {"title": "C. Training", "content": "We train all models for 400,000 iterations with the Adam optimizer with a batch size of 64 examples of 2 seconds each, a learning rate of 1 * 10^{-4}, \u03b2\u2081 = 0.8, and B2 = 0.99. We linearly warm up"}, {"title": "D. Results", "content": "To evaluate the performance of SD-Codec on audio resynthesis and source separation, we compare it with DAC [5] and TDANet [29], the state-of-the-art models on audio codec model and source separation. For fair comparison, we use their official implementations 2 3 and re-train their models on the same training data corpus as SD-Codec. The results are shown in Tab. II. In general, we observe that all the models achieve decent performance on zero-shot evaluation with different data corpus and different data lengths, which confirms the benefit of generalization from scale-up training.\nIn audio resynthesis, since SD-Codec has triple bitrates than DAC on mixed audio, there is no surprise that SD-Codec achieves better reconstruction quality. When it comes to single source resynthesis, SD-Codec has the same bitrate as DAC. The reconstructed audio clips from SD-Codec have higher SI-SDR of around 1 dB than those from DAC, and achieve comparable VisQOL values.\nIn source separation experiments, we do not use the direct output of the decoder. Instead, we use a mask augmentation method where the decoder output is used to compute a magnitude mask. This mask is then applied to the magnitude of the mixture input, and each audio track is reconstructed using the noisy phase from the mixture. We observed that this operation will improve the SI-SDR, and has slight positive impact on VisQOL values. Compared to TDANet, although SD-Codec is not specifically designed for source separation and experiences quantization loss, it achieves comparable results to TDANet in terms of SI-SDR, SI-SDRi, and VisQOL.\nFrom the above results in Tab. II, we can conclude that our proposed SD-Codec, while explicitly assigning latent features from different sources to different codebooks, can be applied both on audio resynthesis and source separation, and achieve promising results.\nTo investigate whether SD-Codec learns a domain-specific quan-tizer, we present the results of reconstructing different audio sources with various RVQ configurations in Fig. 3. The results clearly demonstrate that only the corresponding RVQ module can reconstruct the audio signal with high quality."}, {"title": "E. Ablation Study", "content": "In ablation experiments shown in Tab. III, we mainly studied the changes brought about by different designs of SD-Codec:\n\u2022 Shared codebook: As shown in Fig. 2, we share the last layers for each source. We observe that by either sharing the last 4 layers or the last 8 layers, SD-Codec can maintain similar performance. This shows that in RVQ modules, the shallow layers encode the majority of source-aware information, while the deeper layers encode the local acoustic details.\n\u2022 Separation enhance: When training the original SD-Codec, we randomly used 1, 2, or 3 tracks as input with probabilities of 0.6, 0.2, and 0.2, respectively. We also experimented with different probabilities of 0.2, 0.2, and 0.6, which increased the likelihood of the model encountering mixtures of 3 tracks, thereby enhancing its ability to learn separation. As shown in the ablation results, this will bring higher reconstruction quality on the mixture and higher separation quality, with the expense of losing the quality of individual signal reconstruction.\n\u2022 Initization from DAC: We also studied the potential to leverage from pre-trained DAC on SD-Codec training. However, due to the huge difference in objective, i.e. learning a unified latent space for all audio sources and learning source disentangled latent space, SD-Codec suffers from the DAC pretraining."}, {"title": "V. CONCLUSION", "content": "In this work, we designed a source-aware audio codec, SD-Codec. By joint learning audio coding and source separation, SD-Codec obtain excellent results, comparable to the state-of-the-art both on audio reconstruction and separation, with a unified model. Meanwhile, SD-Codec explicitly assigns latent features from different audio domain to different codebook, which brings more explainability and interpretability on the latent features. Furthermore, due to the source disentanglement in the latent space, future audio generation models can have more fine-grained manipulations on the generated audio, paving the way to the research of controllable audio generation."}]}