{"title": "From Feature Importance to Natural Language Explanations Using LLMs with RAG", "authors": ["Sule Tekkesinoglu", "Lars Kunze"], "abstract": "As machine learning becomes increasingly integral to autonomous decision-making processes involving\nhuman interaction, the necessity of comprehending the model's outputs through conversational means\nincreases. Most recently, foundation models are being explored for their potential as post hoc explainers,\nproviding a pathway to elucidate the decision-making mechanisms of predictive models. In this work,\nwe introduce traceable question-answering, leveraging an external knowledge repository to inform\nthe responses of Large Language Models (LLMs) to user queries within a scene understanding task.\nThis knowledge repository comprises contextual details regarding the model's output, containing high-level features, feature importance, and alternative probabilities. We employ subtractive counterfactual\nreasoning to compute feature importance, a method that entails analysing output variations resulting from decomposing semantic features. Furthermore, to maintain a seamless conversational flow, we integrate\nfour key characteristics - social, causal, selective, and contrastive - drawn from social science research on human explanations into a single-shot prompt, guiding the response generation process. Our evaluation demonstrates that explanations generated by the LLMs encompassed these elements, indicating its\npotential to bridge the gap between complex model outputs and natural language expressions.", "sections": [{"title": "1. Introduction", "content": "As we become increasingly reliant on AI applications in our daily lives, it becomes imperative\nto interact with autonomous decision-making systems in human-understandable terms [1, 2].\nRegulatory frameworks and standards are also evolving to require transparency and account-\nability in AI-driven systems [3, 4]. The technical necessity for explanations to facilitate model\ndebugging and rectifying potential ethical and legal risks arising from biases and errors has\nbeen extensively discussed in the literature [5, 6, 7].\nThe increasing demand for transparent and interpretable AI models has prompted the ex-\nploration of various explanation methods, resulting in significant progress in both inherently\ninterpretable and post-hoc explainability methods [8, 9, 10]. While effective, these methods\nstruggle to provide explicit interpretability or intuitive explanations for non-technical users.\nConsequently, there has been a concerted effort to integrate interpretability approaches with\nNatural Language Processing (NLP) [11, 12]. Recently, Large Language Models (LLMs) have\ngained acclaim as post-hoc explainers, showcasing their potential to elucidate decisions made\nby other predictive models. Despite their success, a persistent challenge for these models is\nthe occurrence of object hallucinations-instances where plausible yet incorrect outputs are\ngenerated, such as featuring objects that do not exist in the images [13]. This raises questions\nabout the fidelity of explanations to the underlying model. To address this issue, we propose a\ntraceable question-answering, which informs LLM responses through an external knowledge\nsource that provides insights into the model's output. By utilizing this data, the LLM acts as\na reasoning engine to process the information rather than a source of information. Figure 1\nprovides a high-level depiction of this process.\nAdditionally, to cultivate a seamless conversational experience, we incorporated key elements\nderived from social science research on human explanations into the system prompt, directing\nthe response generation process. We present our vision for creating this integrated approach,\noutlining each component, and illustrating its viability through a real-world dataset. This\nintegration has the potential to bridge the gap between intricate model outputs and user\ncomprehension towards advancing the development of human-understandable explanations."}, {"title": "2. Background", "content": null}, {"title": "2.1. Recent advances in image explanations", "content": "The demand for transparent and interpretable AI models has led to exploring diverse explanation\nmethods for vision tasks [14]. Recent years have witnessed significant progress in both intrinsic\nand post-hoc explainability methods. Gradient-based techniques, which analyse the gradients\nof the model's output with respect to its input features, have been extensively studied [15].\nWhile effective, these methods may fall short in providing intuitive explanations for non-"}, {"title": "2.2. LLMs as post hoc explainers", "content": "Recent strides in Large Language Models (LLMs), exemplified by GPT-4 (OpenAI), Bard (Google),\nClaude-2 (Anthropic), and Llama-2 (Meta), mark a transformative era in Natural Language\nProcessing (NLP) research. These models have become pervasive across diverse applications,\nranging from machine translation and question-answering to text generation [17]. Notably,\nLLMs have recently gained attention as post hoc explainers, highlighting their potential to\nelucidate the decisions made by other predictive models [18]. Leveraging their in-context\nlearning capabilities by fine-tuning for specific tasks enhances their proficiency in generating\ncontextually relevant explanations [12, 19]. Despite their success, a persistent challenge for\nthese models is the occurrence of object hallucinations\u2014instances where plausible yet incorrect\noutputs are generated, featuring objects that do not exist in the images in vision-based tasks.\nThis issue raises questions about the faithfulness of explanations to the underlying model.\nOne way to address this is by recognizing the role of the semantic map layer in capturing\nthe meaning and context of the physical surroundings [13]. By effectively encoding valuable\nsemantic information into LLMs, we can truthfully represent and comprehend the intricate\ndetails of the model output within the language space."}, {"title": "2.3. Characteristics of explanations and XAI", "content": "In the context of XAI, the audience for explanations is predominantly human; thus, understand-\ning what makes an explanation human-friendly is paramount. While accuracy is important,\nthe clarity and presentation of explanations are equally crucial in ensuring comprehensibility.\nAccording to Miller [20], current work on interpretability in machine learning relies solely\non researchers' intuition regarding what constitutes an appropriate explanation for humans.\nMiller's survey highlights four major characteristics of explanations drawn from findings in\nsocial science research on human explanation: explanations are social, causal, selective, and\ncontrastive [20]. In the following section, we briefly discuss these characteristics in relation to\nXAI before demonstrating how we integrate them into our application."}, {"title": "Explanations are social", "content": "An explanation represents an interactive exchange between two roles: the explainer and the\nexplainee, and is governed by certain 'rules' [21]. Interactive explanations in a dialogue structure\nconsist of three fundamental components: opening statements, clarification questions, and\nclosing statements [22]. As such, it should follow the basic rules of conversation, which are\ncaptured by Grice's conversational maxims [23]. These include principles such as quality,\nquantity, relation, and manner, which can be understood as \u2018only say what you believe', 'only\nsay as much as is necessary', 'only say what is relevant', and 'say it nicely'. Manner further\nencompasses various sub-maxims, including avoiding unclear expression, eliminating ambiguity,\nmaintaining brevity to avoid unnecessary wordiness, and presenting information in an orderly\nfashion.\nMoreover, explanatory agents employing anthropomorphic traits like politeness markers\n(e.g., \u201cThank you!\u201d), warm and friendly tones, and empathetic expressions (e.g., \u201cI understand.\u201d)\nare pivotal for effective human interaction [24]. Theory of Mind also plays a crucial role in\nsocial explanations.\u00b9 As an integral part of a dialogue, an explanatory agent should keep track\nof what has already been explained, maintaining a simple mental model of others. This enables\nintelligent agents to tailor explanations based on past interactions and evolving contexts."}, {"title": "Explanations are causal", "content": "Causal explanations aim to elucidate the cause-and-effect relationship underlying a decision.\nWhen explaining a prediction made by a machine learning model, identifying the most influential\nfeatures can provide insight into the key causal connections to the prediction. This process,\nalso known as causal inference, starts with observations (e.g., what if a feature had a different\nvalue) and selecting some of those causes as the explanation. Miller divides casual inference\ninto two parts: counterfactuals and abductive reasoning [20].\nThe reasoner uses a model of hypothetical counterfactual cases to derive an explanation, such\nas perturbing inputs, to see how they causally impact the prediction (See Section 3.1). Abductive\nreasoning is inferring the causes from those observations to form the most probable explanation.\nOne can think of abductive reasoning as the following process: observing unexpected or\nsurprising events, generating one or more hypotheses about these events, judging the plausibility\nof the hypotheses, and selecting the 'best' hypothesis as the explanation [26]. It is important\nto note that new information (e.g., different feature perturbations) can alter the most likely\nexplanation."}, {"title": "Explanations are selected", "content": "Explanations are contextual by nature. While an event may have numerous causes, the recipient\nof the explanation typically cares about only a specific subset relevant to the context [27]. The\nexplainer selects a subset based on various criteria called casual selection. Hilton [28] argues\nthat explanation selection is used for cognitive reasons since causal chains are often too large to"}, {"title": "Explanations are contrastive", "content": "Studies have shown that people only request contrastive explanations, essentially 'why' ques-\ntions framed to imply the differences between two possible outcomes [30]. As a result, explana-\ntions are typically offered in relation to the cause of one event compared to another that did not\noccur.\nSimilarly, in the context of ML interpretability, people are not specifically interested in all\nthe factors that led to the prediction but instead in the factors that need to change so that the\nprediction would also change. Explanations that present some contrast between the instance to\nexplain and a reference point (i.e., a hypothetical instance) are preferable because the cognitive\nburden of complete explanations is too high. However, most existing work considers contrastive\nquestions but not contrastive explanations, providing two complete explanations individually.\nThis could be because the contrastiveness is application-dependent, given that a reference point\nmay not be apparent in all tasks [31]. For instance, in sentiment analysis or anomaly detection,\ncontrastive cases might not be as straightforward as other tasks, such as banking, e.g., loan\naccepted vs rejected cases."}, {"title": "3. Materials and Method", "content": "This section outlines the key elements of the traceable question-answering methodology. We\ndetail the proposed explainability approach for extracting feature importance and the prompting\ntechnique to generate accurate and human-friendly explanations. Additionally, we provide an\napplication example illustrating how we establish a connection between the model output, the\nexplainability technique, and language models to produce natural language explanations in an\ninteractive format."}, {"title": "3.1. Explanations through subtractive counterfactual reasoning", "content": "Subtractive counterfactual reasoning is the process of removing an event to understand its causal\ninfluence on an outcome [32]. Such reasoning is widely used as an XAI technique, exemplified by\nocclusion analysis and feature importance attribution, built on the same foundation - quantifying\nthe variation in model output through systematic modifications of input components [33, 14,\n34, 35]. There are distinctions in terms of how these methods remove features, elucidate model\nbehaviour, and summarize feature importance. Nonetheless, they provide valuable insights into\nthe relative significance of each feature influencing the decision-making process [36, 37, 38].\nThis study proposes a decomposition-based approach to examine the output variations by\ndecomposing input values. By measuring such variations, we determine the degree of impact\non an outcome and reason about the importance of each feature value. As a function, the model\nassigns a class label and prediction probability $f : x \\rightarrow f(x)$ given input y. To measure the effect\nof each input value, we observe the model's prediction p for y without the knowledge of event\n$T_i = T_k$, where $T_k$ is the value of feature $T_i$. To explicitly represent the absence of information,\nthe feature value $T_k$ is replaced with an undefined value, i.e., NaN (Not a Number). NaN values\nare treated as invalid or masked elements and are not considered in the computation. We attain\nan array containing probabilities for all perturbations resulting from decomposition. In order to\ndetermine the importance of an individual feature $T_i$, we begin by identifying the max and min\nvalues within this array, which establishes the decision boundaries for the given input. Next,\nwe compute the feature importance as the position of the probability- after decomposing the\nfeature- relative to the max and min probabilities. The computation is expressed as follows:\n$\\text{importance}(T_i) = \\frac{p(x|y\\backslash T_k)_{\\text{max}} - p(x|y \\backslash T_k)}{p(x|y \\backslash T_k)_{\\text{max}} - p(x|y \\backslash T_k)_{\\text{min}}}$\n(1)\nThis approximates an importance value between zero and one for each input feature. Consid-\nering that evaluating fractions has a higher cognitive load, the values are rounded to the nearest\ninteger to increase comprehensibility. Moreover, this analysis is carried out for the categories\nfollowing the main prediction simultaneously to generate explanations for the contrastive cases."}, {"title": "3.2. Prompting for traceable question-answering with RAG", "content": "Traceable question-answering, through a process known as Retrieval-Augmented Generation\n(RAG), integrates external knowledge sources to furnish responses to user inquiries. This\nmethod improves the performance of LLMs by referencing an external knowledge source prior\nto generating a response (See Figure 1). This approach mitigates the problem of 'hallucination'\nand ensures factual consistency, particularly in scenarios where facts may evolve over time [39].\nDepending on the specific implementation, RAG supports various file formats, including text,\nPDF files, word documents, structured data (tables), and images.\nIn our work, the knowledge repository contains tabular information about model output,\nfeatures, contrastive cases, and feature importance values obtained through the decomposition-\nbased explanation method described in Section 3.1. This information is saved as the output of\nthe explanation module as a .csv file. Then, the file, the system prompt (See Figure 3), and the\nuser input query are fed to the LLM model for response generation. The resulting responses\ncontextualise the feature importance grounded in the models' inner workings, thereby rendering\nthe explanations traceable to the underlying model.\nFurthermore, our aim is to imbue LLM with the characteristics of explanations discussed in\nSection 2.3 to foster a human-friendly interaction. Ideally, LLM-generated responses should\nadhere to conversational norms, encompassing opening, clarification, and closing statements\nwhile appropriately integrating social cues. The model is expected to deduce causal relationships"}, {"title": "3.3. Application example", "content": "To demonstrate our approach to traceable question-answering, we selected a visual scene\nunderstanding task. This involves scene classification, categorizing images based on their\ncontent and inferring high-level semantic scenes from low-level visual features. Many critical\napplications of scene classification include human-robot collaboration, autonomous driving,\nand other autonomous physical systems that rely on semantic segmentation to inform decision-\nmaking processes [40, 41, 42]. We experimented with the GoogleNet Places365 model designed\nspecifically for scene recognition and classification tasks [43]. The model is trained on the\nPlaces365 dataset, containing more than 365 scene categories. We focus on semantic urban\nscene understanding with the road segmentation dataset CamVid (Cambridge-driving Labelled\nVideo Database). The dataset helps to understand driving scenes, comprising images capturing\nstreet-level views from urban driving scenarios. It includes manual annotations across 32 classes,\nencompassing elements such as building, driveway, pavement, tree, traffic sign, car, pedestrian,\nand bicyclist. The Deeplab v3+ deep learning model is trained on the CamVid dataset for\nsemantic segmentation [44]. Figure 2 shows the segmentation result for the example presented\nin the next section. Since we utilized off-the-shelf models for illustration purposes, the emphasis\nis not on these models' performances.\nSemantic information plays a critical role in autonomous vehicle (AV) decision-making\nprocesses, as it captures the meaning and context of the physical surroundings observed in the\nroad scene. This includes understanding the appearance of various elements such as buildings"}, {"title": "4. Evaluation Results", "content": "In this section, we present our experimental findings. We analyzed nine distinct scene classes\nparking lot, street, residential neighbourhood, crosswalk, highway, industrial area, gas station,\nshopfront, and general store (outdoor)\u2014 across different scenarios (totalling 42), where the model\ndisplayed varying degrees of confidence in its output. We evaluated the responses in terms of\nsociability, causality, selectiveness, and contrastiveness generated by GPT-3.5 (gpt-3.5-turbo)\nand GPT-4 (gpt-4-1106-preview) models."}, {"title": "4.1. Sociability", "content": "In examining the sociability within the responses, our initial focus centres on the overall tone of\nthe responses. We employed the Valence Aware Dictionary and Sentiment Reasoner (VADER) to\ngauge the sentiment conveyed within the text, encompassing dimensions of positivity, negativity,\nand neutrality. Sentiment scores for each response are aggregated to delineate overall sentiment\ntrends. Figure 5 illustrates a noticeable positive trendline by both models, indicating a prevalent"}, {"title": "4.2. Expression of Causality", "content": "To evaluate the expression of causality, we initially extracted responses indicating cause and\neffect, primarily found within clarification statements. We then identified key terms related to\ncausality, such as 'because', 'if', 'then', 'albeit', 'due', 'contribute', 'influence', 'affect', 'impact',\nand 'effect', and tallied their occurrences within each answer to measure the presence of causal\nreasoning. In Figure 7, we compare the frequency of these terms used by two models in the\nresponse set. Notably, 'if' emerges as the most frequent term used by both models, often utilized\nfor expressing counterfactual reasoning (e.g., 'if it was not', 'if there is/were'), while 'influence'\ntypically indicates the cause of a decision. Overall, GPT-4 demonstrated a higher frequency of\ncausality terms and showcased greater diversity in expression compared to GPT-3.5.\nThrough the qualitative review, we examined existing patterns of dependency parsing signify-\ning causal relationships. Two primary grammatical structures emerged, suggesting cause-effect\nrelationships within the text. The first structure pertains to how the presence or absence of\ncertain features influences the likelihood of specific scenarios. The second structure involves\na cause-effect relationship between feature importance and probability, indicating that fea-\ntures with higher importance scores exert a greater impact on probability. It is important to\nacknowledge that while these grammatical structures and terms are widely used, their accuracy\nis not always consistent across models. In the subsequent section, we will further evaluate the\nselection and accuracy of inferred causal relationships."}, {"title": "4.3. Selectiveness", "content": "In this section, we evaluate whether the models effectively detect unusual values within the\nprovided knowledge repository, infer potential causes from those observations, and select\nthe most relevant ones to construct an explanation. Our evaluation encompasses analysing\nresponses across the number of causes given, graded selection, and sort order to assess whether\nexpected causes are addressed correctly. First, we specified the maximum number of uniquely\nsufficient causes for each case, which are feature importance values >5. The number of causes\nvaries across scenarios; some exhibit only a single highly important feature >5, while others\nmight involve multiple high-importance features with varying degrees of significance. Graded\nselection is crucial for discerning important values such that the language model must prioritize\nthe most relevant causes when encountered with multiple features of comparable importance"}, {"title": "4.4. Expression of Contrastiveness", "content": "Ideally, initial explanations should include a contrast with an alternative case to justify the\noutput. Then the user would investigate it further if desired. In our experiment, we observed\nthat in most cases, models, particularly with GPT3.5, failed to mention alternative scenarios\nunless explicitly prompted (e.g., inquiries such as \u201cWhat could it be otherwise if it wasn't\nX?\u201d or \u201cHow does X differ from other options?\u201d). Nevertheless, to measure the presence of\ncontrastiveness across models, we analysed the occurrence of contrastive terms and comparative\nphrases within the generated responses. We compiled a dictionary consisting of terms such\nas 'distinguish', 'different', 'contrast', 'compared to', 'in contrast', 'while', 'both', 'on the other\nhand', 'whereas', and 'conversely', which introduce contrasting explanations. Figure 8 shows\nthat 'distinguish' is the most frequently used term by GPT-3.5, whereas 'different' predominates\nin GPT-4's responses. Other frequently encountered terms include 'compared to', 'contrast',\nand 'differentiate. Overall, the results suggest that GPT-4 employs a wider array of contrastive\nterms than GPT-3.5 in its responses. Our qualitative observations further confirm that while\nGPT-3.5 tends to adhere to one type of contrastive explanation, GPT-4 explores diverse ways to\nexpress such contrasts."}, {"title": "5. Conclusion", "content": "This study contributes to advancing post-hoc explainability by proposing a traceable question-\nanswering approach. By integrating LLMs with the decomposition-based explainability tech-\nnique, we generate natural language explanations in an interactive format. Our approach\npromotes transparency and interpretability in scene-understanding tasks by fusing LLMs with\nsemantic feature importance. The proposed approach has the potential for application across\nvarious domains, offering comprehensible explanations for decision-making processes, includ-\ning critical areas such as medical applications. While this is a promising development, there\nare still areas for further exploration and improvement. One area that requires attention is the\noveruse of social cues by LLMs. While polite and helpful phrases can enhance user experience,\ntheir misuse can become vexing over time. It is important to evaluate such interactions through\nhuman subject studies to understand user perceptions of the explanations provided by the\nLLM. Future research could also explore dialogue management strategies to address issues\nrelated to interaction and personalization. On another note, the dynamic nature of the external\nknowledge source offers the potential for further expansion with additional observations. This\nnot only ensures that the system remains relevant and up to date but also allows the system to\nanswer different types of questions. Additionally, incorporating multimodal elements, such as\ncombining text with visual explanations, can enhance the effectiveness of the responses (e.g.,\nhighlighting the specific elements contributing to the prediction). Another consideration is the\nestimation of feature importance values, which are subject to perturbations and computational\nprocesses. It is important to assess and reconcile differences in feature importance across\nalgorithms to ensure stability and consistency."}]}