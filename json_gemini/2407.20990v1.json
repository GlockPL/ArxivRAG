{"title": "From Feature Importance to Natural Language Explanations Using LLMs with RAG", "authors": ["Sule Tekkesinoglu", "Lars Kunze"], "abstract": "As machine learning becomes increasingly integral to autonomous decision-making processes involving human interaction, the necessity of comprehending the model's outputs through conversational means increases. Most recently, foundation models are being explored for their potential as post hoc explainers, providing a pathway to elucidate the decision-making mechanisms of predictive models. In this work, we introduce traceable question-answering, leveraging an external knowledge repository to inform the responses of Large Language Models (LLMs) to user queries within a scene understanding task. This knowledge repository comprises contextual details regarding the model's output, containing high-level features, feature importance, and alternative probabilities. We employ subtractive counterfactual reasoning to compute feature importance, a method that entails analysing output variations resulting from decomposing semantic features. Furthermore, to maintain a seamless conversational flow, we integrate four key characteristics - social, causal, selective, and contrastive - drawn from social science research on human explanations into a single-shot prompt, guiding the response generation process. Our evaluation demonstrates that explanations generated by the LLMs encompassed these elements, indicating its potential to bridge the gap between complex model outputs and natural language expressions.", "sections": [{"title": "1. Introduction", "content": "As we become increasingly reliant on AI applications in our daily lives, it becomes imperative to interact with autonomous decision-making systems in human-understandable terms [1, 2]. Regulatory frameworks and standards are also evolving to require transparency and accountability in AI-driven systems [3, 4]. The technical necessity for explanations to facilitate model debugging and rectifying potential ethical and legal risks arising from biases and errors has been extensively discussed in the literature [5, 6, 7].\nThe increasing demand for transparent and interpretable AI models has prompted the exploration of various explanation methods, resulting in significant progress in both inherently interpretable and post-hoc explainability methods [8, 9, 10]. While effective, these methods struggle to provide explicit interpretability or intuitive explanations for non-technical users. Consequently, there has been a concerted effort to integrate interpretability approaches with Natural Language Processing (NLP) [11, 12]. Recently, Large Language Models (LLMs) have gained acclaim as post-hoc explainers, showcasing their potential to elucidate decisions made by other predictive models. Despite their success, a persistent challenge for these models is the occurrence of object hallucinations-instances where plausible yet incorrect outputs are generated, such as featuring objects that do not exist in the images [13]. This raises questions about the fidelity of explanations to the underlying model. To address this issue, we propose a traceable question-answering, which informs LLM responses through an external knowledge source that provides insights into the model's output. By utilizing this data, the LLM acts as a reasoning engine to process the information rather than a source of information. Figure 1 provides a high-level depiction of this process.\nAdditionally, to cultivate a seamless conversational experience, we incorporated key elements derived from social science research on human explanations into the system prompt, directing the response generation process. We present our vision for creating this integrated approach, outlining each component, and illustrating its viability through a real-world dataset. This integration has the potential to bridge the gap between intricate model outputs and user comprehension towards advancing the development of human-understandable explanations."}, {"title": "2. Background", "content": ""}, {"title": "2.1. Recent advances in image explanations", "content": "The demand for transparent and interpretable AI models has led to exploring diverse explanation methods for vision tasks [14]. Recent years have witnessed significant progress in both intrinsic and post-hoc explainability methods. Gradient-based techniques, which analyse the gradients of the model's output with respect to its input features, have been extensively studied [15]. While effective, these methods may fall short in providing intuitive explanations for non-experts. In another line of work, attention mechanisms are proposed by assigning weights to different parts of the input data based on their relevance to the model's decision [16]. Similarly, attention mechanisms may lack explicit interpretability, and some argue that attention alone is insufficient. As a result, there have been concerted efforts to integrate these approaches with Natural Language Processing (NLP) [12]. Long Short-Term Memory (LSTM) encoders have been employed to extract features from video clips, with LSTM decoders generating descriptive texts based on these features. While effective in image captioning, these methods provide textual descriptions rather than visual explanations. Moreover, researchers have proposed perturbation-based approaches\u2014a model-agnostic method that perturbs input data and observes changes in model predictions. Building on this research, we integrate semantic perturbation analysis with pretrained language models, translating the results into the text modality for enhanced explainability."}, {"title": "2.2. LLMs as post hoc explainers", "content": "Recent strides in Large Language Models (LLMs), exemplified by GPT-4 (OpenAI), Bard (Google), Claude-2 (Anthropic), and Llama-2 (Meta), mark a transformative era in Natural Language Processing (NLP) research. These models have become pervasive across diverse applications, ranging from machine translation and question-answering to text generation [17]. Notably, LLMs have recently gained attention as post hoc explainers, highlighting their potential to elucidate the decisions made by other predictive models [18]. Leveraging their in-context learning capabilities by fine-tuning for specific tasks enhances their proficiency in generating contextually relevant explanations [12, 19]. Despite their success, a persistent challenge for these models is the occurrence of object hallucinations\u2014instances where plausible yet incorrect outputs are generated, featuring objects that do not exist in the images in vision-based tasks. This issue raises questions about the faithfulness of explanations to the underlying model. One way to address this is by recognizing the role of the semantic map layer in capturing the meaning and context of the physical surroundings [13]. By effectively encoding valuable semantic information into LLMs, we can truthfully represent and comprehend the intricate details of the model output within the language space."}, {"title": "2.3. Characteristics of explanations and XAI", "content": "In the context of XAI, the audience for explanations is predominantly human; thus, understanding what makes an explanation human-friendly is paramount. While accuracy is important, the clarity and presentation of explanations are equally crucial in ensuring comprehensibility. According to Miller [20], current work on interpretability in machine learning relies solely on researchers' intuition regarding what constitutes an appropriate explanation for humans. Miller's survey highlights four major characteristics of explanations drawn from findings in social science research on human explanation: explanations are social, causal, selective, and contrastive [20]. In the following section, we briefly discuss these characteristics in relation to XAI before demonstrating how we integrate them into our application."}, {"title": "Explanations are social", "content": "An explanation represents an interactive exchange between two roles: the explainer and the explainee, and is governed by certain 'rules' [21]. Interactive explanations in a dialogue structure consist of three fundamental components: opening statements, clarification questions, and closing statements [22]. As such, it should follow the basic rules of conversation, which are captured by Grice's conversational maxims [23]. These include principles such as quality, quantity, relation, and manner, which can be understood as \u2018only say what you believe', 'only say as much as is necessary', 'only say what is relevant', and 'say it nicely'. Manner further encompasses various sub-maxims, including avoiding unclear expression, eliminating ambiguity, maintaining brevity to avoid unnecessary wordiness, and presenting information in an orderly fashion.\nMoreover, explanatory agents employing anthropomorphic traits like politeness markers (e.g., \"Thank you!\u201d), warm and friendly tones, and empathetic expressions (e.g., \u201cI understand.\u201d) are pivotal for effective human interaction [24]. Theory of Mind also plays a crucial role in social explanations.\u00b9 As an integral part of a dialogue, an explanatory agent should keep track of what has already been explained, maintaining a simple mental model of others. This enables intelligent agents to tailor explanations based on past interactions and evolving contexts."}, {"title": "Explanations are causal", "content": "Causal explanations aim to elucidate the cause-and-effect relationship underlying a decision. When explaining a prediction made by a machine learning model, identifying the most influential features can provide insight into the key causal connections to the prediction. This process, also known as causal inference, starts with observations (e.g., what if a feature had a different value) and selecting some of those causes as the explanation. Miller divides casual inference into two parts: counterfactuals and abductive reasoning [20].\nThe reasoner uses a model of hypothetical counterfactual cases to derive an explanation, such as perturbing inputs, to see how they causally impact the prediction (See Section 3.1). Abductive reasoning is inferring the causes from those observations to form the most probable explanation. One can think of abductive reasoning as the following process: observing unexpected or surprising events, generating one or more hypotheses about these events, judging the plausibility of the hypotheses, and selecting the 'best' hypothesis as the explanation [26]. It is important to note that new information (e.g., different feature perturbations) can alter the most likely explanation."}, {"title": "Explanations are selected", "content": "Explanations are contextual by nature. While an event may have numerous causes, the recipient of the explanation typically cares about only a specific subset relevant to the context [27]. The explainer selects a subset based on various criteria called casual selection. Hilton [28] argues that explanation selection is used for cognitive reasons since causal chains are often too large to comprehend. The criteria people use for explanation selection include abnormality, relevancy, and simplicity.\nAbnormality is a key criterion for explanation selection; as such, people select unusual causes to explain events [29]. In machine learning interpretability, identifying abnormal events corre-sponds to deviant or higher feature importance values. As pointed out within the conversational maxims, relevancy also plays a role in explanation selection-only say what is necessary and relevant. People select explanations to adhere to these maxims whilst persuading the explainee to new information or viewpoints. Another strong criterion in explanation selection is simplicity, which refers to those that cite fewer causes. People prefer uniquely sufficient causes that bring about the effect without any other causes, but there might be cases with multiple sufficient causes."}, {"title": "Explanations are contrastive", "content": "Studies have shown that people only request contrastive explanations, essentially 'why' questions framed to imply the differences between two possible outcomes [30]. As a result, explana-tions are typically offered in relation to the cause of one event compared to another that did not occur.\nSimilarly, in the context of ML interpretability, people are not specifically interested in all the factors that led to the prediction but instead in the factors that need to change so that the prediction would also change. Explanations that present some contrast between the instance to explain and a reference point (i.e., a hypothetical instance) are preferable because the cognitive burden of complete explanations is too high. However, most existing work considers contrastive questions but not contrastive explanations, providing two complete explanations individually. This could be because the contrastiveness is application-dependent, given that a reference point may not be apparent in all tasks [31]. For instance, in sentiment analysis or anomaly detection, contrastive cases might not be as straightforward as other tasks, such as banking, e.g., loan accepted vs rejected cases."}, {"title": "3. Materials and Method", "content": "This section outlines the key elements of the traceable question-answering methodology. We detail the proposed explainability approach for extracting feature importance and the prompting technique to generate accurate and human-friendly explanations. Additionally, we provide an application example illustrating how we establish a connection between the model output, the explainability technique, and language models to produce natural language explanations in an interactive format."}, {"title": "3.1. Explanations through subtractive counterfactual reasoning", "content": "Subtractive counterfactual reasoning is the process of removing an event to understand its causal influence on an outcome [32]. Such reasoning is widely used as an XAI technique, exemplified by occlusion analysis and feature importance attribution, built on the same foundation - quantifying the variation in model output through systematic modifications of input components [33, 14, 34, 35]. There are distinctions in terms of how these methods remove features, elucidate model behaviour, and summarize feature importance. Nonetheless, they provide valuable insights into the relative significance of each feature influencing the decision-making process [36, 37, 38].\nThis study proposes a decomposition-based approach to examine the output variations by decomposing input values. By measuring such variations, we determine the degree of impact on an outcome and reason about the importance of each feature value. As a function, the model assigns a class label and prediction probability $f : x \\rightarrow f(x)$ given input y. To measure the effect of each input value, we observe the model's prediction p for y without the knowledge of event $T_{i} = T_{k}$, where $T_{k}$ is the value of feature Ti. To explicitly represent the absence of information, the feature value $T_{k}$ is replaced with an undefined value, i.e., NaN (Not a Number). NaN values are treated as invalid or masked elements and are not considered in the computation. We attain an array containing probabilities for all perturbations resulting from decomposition. In order to determine the importance of an individual feature $T_{i}$, we begin by identifying the $max$ and $min$ values within this array, which establishes the decision boundaries for the given input. Next, we compute the feature importance as the position of the probability- after decomposing the feature- relative to the max and min probabilities. The computation is expressed as follows:\n$\\text{importance}(T_{i}) = \\frac{p(x|y\\setminus T_{k})_{max} - p(x|y \\setminus T_{k})}{p(x|y \\setminus T_{k})_{max} - p(x|y \\setminus T_{k})_{min}}$\nThis approximates an importance value between zero and one for each input feature. Considering that evaluating fractions has a higher cognitive load, the values are rounded to the nearest integer to increase comprehensibility. Moreover, this analysis is carried out for the categories following the main prediction simultaneously to generate explanations for the contrastive cases."}, {"title": "3.2. Prompting for traceable question-answering with RAG", "content": "Traceable question-answering, through a process known as Retrieval-Augmented Generation (RAG), integrates external knowledge sources to furnish responses to user inquiries. This method improves the performance of LLMs by referencing an external knowledge source prior to generating a response (See Figure 1). This approach mitigates the problem of 'hallucination' and ensures factual consistency, particularly in scenarios where facts may evolve over time [39]. Depending on the specific implementation, RAG supports various file formats, including text, PDF files, word documents, structured data (tables), and images.\nIn our work, the knowledge repository contains tabular information about model output, features, contrastive cases, and feature importance values obtained through the decomposition-based explanation method described in Section 3.1. This information is saved as the output of the explanation module as a .csv file. Then, the file, the system prompt (See Figure 3), and the user input query are fed to the LLM model for response generation. The resulting responses contextualise the feature importance grounded in the models' inner workings, thereby rendering the explanations traceable to the underlying model.\nFurthermore, our aim is to imbue LLM with the characteristics of explanations discussed in Section 2.3 to foster a human-friendly interaction. Ideally, LLM-generated responses should adhere to conversational norms, encompassing opening, clarification, and closing statements while appropriately integrating social cues. The model is expected to deduce causal relationships from the observations within the provided knowledge repository, selecting the most pertinent causes to form an explanation and presenting it alongside a contrastive case. To accomplish this goal, we devised a system prompt that provides explicit instructions to the model to generate efficient responses to user queries. The system prompt includes a single-shot example illustrating the desired interaction, potential user inquiries and outlining how the model can effectively address them, enabling the language model to produce consistent and predictable outcomes, as presented in Figure 3."}, {"title": "3.3. Application example", "content": "To demonstrate our approach to traceable question-answering, we selected a visual scene understanding task. This involves scene classification, categorizing images based on their content and inferring high-level semantic scenes from low-level visual features. Many critical applications of scene classification include human-robot collaboration, autonomous driving, and other autonomous physical systems that rely on semantic segmentation to inform decision-making processes [40, 41, 42]. We experimented with the GoogleNet Places365 model designed specifically for scene recognition and classification tasks [43]. The model is trained on the Places365 dataset, containing more than 365 scene categories. We focus on semantic urban scene understanding with the road segmentation dataset CamVid (Cambridge-driving Labelled Video Database). The dataset helps to understand driving scenes, comprising images capturing street-level views from urban driving scenarios. It includes manual annotations across 32 classes, encompassing elements such as building, driveway, pavement, tree, traffic sign, car, pedestrian, and bicyclist. The Deeplab v3+ deep learning model is trained on the CamVid dataset for semantic segmentation [44]. Figure 2 shows the segmentation result for the example presented in the next section. Since we utilized off-the-shelf models for illustration purposes, the emphasis is not on these models' performances.\nSemantic information plays a critical role in autonomous vehicle (AV) decision-making processes, as it captures the meaning and context of the physical surroundings observed in the road scene. This includes understanding the appearance of various elements such as buildings and roads, identifying shapes such as cars and pedestrians, and recognizing spatial relationships to contextualize the scene [45]. Particularly in unstructured off-road environments, the accurate understanding of the surroundings concerning semantic classes such as trail, grass, or rock is important for safe and deliberate navigation [46]. Moreover, semantic segmentation provides a way to represent and comprehend the details of the environment in the language space. By leveraging the labelled information within the image scene, we can semantically map the crucial components based on their feature importance. This granularity enables a more insightful analysis of feature importance, contributing to a nuanced understanding of the model's decision-making process. Such insights can be helpful for post-incident forensic analysis in case of misreading a scene causing collisions or accidents. In this work, we suppose situations where the user checks in with an AI assistant in an AV system in non-critical driving scenarios while engaging in non-driving related tasks."}, {"title": "4. Evaluation Results", "content": "In this section, we present our experimental findings. We analyzed nine distinct scene classes parking lot, street, residential neighbourhood, crosswalk, highway, industrial area, gas station, shopfront, and general store (outdoor)\u2014 across different scenarios (totalling 42), where the model displayed varying degrees of confidence in its output. We evaluated the responses in terms of sociability, causality, selectiveness, and contrastiveness generated by GPT-3.5 (gpt-3.5-turbo) and GPT-4 (gpt-4-1106-preview) models."}, {"title": "4.1. Sociability", "content": "In examining the sociability within the responses, our initial focus centres on the overall tone of the responses. We employed the Valence Aware Dictionary and Sentiment Reasoner (VADER) to gauge the sentiment conveyed within the text, encompassing dimensions of positivity, negativity, and neutrality. Sentiment scores for each response are aggregated to delineate overall sentiment trends. Figure 5 illustrates a noticeable positive trendline by both models, indicating a prevalent inclination towards positivity in the sentiment expressed. A qualitative inspection of the text reveals a prevalence of polite, helpful, and informative social cues across all responses, supporting this observation. Consequently, the neutral sentiment score remains notably low, underscoring the prevalence of emotional expression throughout the corpus. Exploring negative sentiment scores unveils instances where the models acknowledge their limitations, particularly when unable to retrieve answers from the external knowledge repository.\nConsidering the three foundational phases of dialogue-opening, clarification, and closing statements-next, we analyzed the indicators such as politeness markers and emotional expressions within each phase. To facilitate this analysis, we created a distinct dictionary for each phase and computed the term frequency of social cues present in each phase. Opening statements include greetings such as \u2018hey' and hello.' Clarification statements are characterized by social cues indicative of helpfulness, continued support, and awareness of its limitations,"}, {"title": "4.2. Expression of Causality", "content": "To evaluate the expression of causality, we initially extracted responses indicating cause and effect, primarily found within clarification statements. We then identified key terms related to causality, such as 'because', 'if', 'then', 'albeit', 'due', 'contribute', 'influence', 'affect', 'impact', and 'effect', and tallied their occurrences within each answer to measure the presence of causal reasoning. In Figure 7, we compare the frequency of these terms used by two models in the response set. Notably, 'if' emerges as the most frequent term used by both models, often utilized for expressing counterfactual reasoning (e.g., 'if it was not', 'if there is/were'), while 'influence' typically indicates the cause of a decision. Overall, GPT-4 demonstrated a higher frequency of causality terms and showcased greater diversity in expression compared to GPT-3.5.\nThrough the qualitative review, we examined existing patterns of dependency parsing signify-ing causal relationships. Two primary grammatical structures emerged, suggesting cause-effect relationships within the text. The first structure pertains to how the presence or absence of certain features influences the likelihood of specific scenarios. The second structure involves a cause-effect relationship between feature importance and probability, indicating that features with higher importance scores exert a greater impact on probability. It is important to acknowledge that while these grammatical structures and terms are widely used, their accuracy is not always consistent across models. In the subsequent section, we will further evaluate the selection and accuracy of inferred causal relationships."}, {"title": "4.3. Selectiveness", "content": "In this section, we evaluate whether the models effectively detect unusual values within the provided knowledge repository, infer potential causes from those observations, and select the most relevant ones to construct an explanation. Our evaluation encompasses analysing responses across the number of causes given, graded selection, and sort order to assess whether expected causes are addressed correctly. First, we specified the maximum number of uniquely sufficient causes for each case, which are feature importance values >5. The number of causes varies across scenarios; some exhibit only a single highly important feature >5, while others might involve multiple high-importance features with varying degrees of significance. Graded selection is crucial for discerning important values such that the language model must prioritize the most relevant causes when encountered with multiple features of comparable importance levels. Additionally, graded selection entails disregarding features with lower importance scores (<5). Finally, the sort order concerns the sequence of selected causes, which should follow a hierarchy from the most significant and relevant features to those of lesser importance. Thereafter, responses are categorized as fulfilled, partial, subpar, or unfulfilled according to their arrangement with our specifications.\nThe findings presented in Table 2 indicate that GPT-4 outperformed GPT-3.5 in all categories. Notably, GPT-4 had a much higher success rate (76%) than GPT-3.5 (24%) in identifying the most relevant cause (fulfilled). Additionally, GPT-4 had a lower rate of subpar and unfulfilled responses. GPT-3.5 referenced features not present in the knowledge source as potential causes in two instances. These results suggest that GPT-4 is more accurate and selective than GPT-3.5 in identifying the most relevant cause(s) from a set of possible causes.\nFurthermore, an additional criterion for assessing explanation selection is simplicity, which involves utilizing concise language, avoiding technical terminology, and referencing fewer causes. Table 3 evaluates responses by comparing three metrics: the frequency of technical jargon, response length, and the number of causes cited. Lower values across all three metrics signify simpler responses."}, {"title": "4.4. Expression of Contrastiveness", "content": "Ideally, initial explanations should include a contrast with an alternative case to justify the output. Then the user would investigate it further if desired. In our experiment, we observed that in most cases, models, particularly with GPT3.5, failed to mention alternative scenarios unless explicitly prompted (e.g., inquiries such as \u201cWhat could it be otherwise if it wasn't X?\u201d or \u201cHow does X differ from other options?\u201d). Nevertheless, to measure the presence of contrastiveness across models, we analysed the occurrence of contrastive terms and comparative phrases within the generated responses. We compiled a dictionary consisting of terms such as 'distinguish', 'different', 'contrast', 'compared to', 'in contrast', 'while', 'both', 'on the other hand', 'whereas', and 'conversely', which introduce contrasting explanations. Figure 8 shows that 'distinguish' is the most frequently used term by GPT-3.5, whereas 'different' predominates in GPT-4's responses. Other frequently encountered terms include 'compared to', 'contrast', and 'differentiate'. Overall, the results suggest that GPT-4 employs a wider array of contrastive terms than GPT-3.5 in its responses. Our qualitative observations further confirm that while GPT-3.5 tends to adhere to one type of contrastive explanation, GPT-4 explores diverse ways to express such contrasts.\nAlthough both models frequently employ these terms, indicating the generation of alternative viewpoints in their responses, the effectiveness of contrastive explanations varies considerably. Our qualitative review found that well-developed contrasting explanations included alternative scenarios with counterfactuals to strengthen the case. Conversely, weakly constructed con-trastive explanations failed to explore \u2018what if' scenarios to elucidate the disparity between the two situations under comparison."}, {"title": "5. Conclusion", "content": "This study contributes to advancing post-hoc explainability by proposing a traceable question-answering approach. By integrating LLMs with the decomposition-based explainability tech-nique, we generate natural language explanations in an interactive format. Our approach promotes transparency and interpretability in scene-understanding tasks by fusing LLMs with semantic feature importance. The proposed approach has the potential for application across various domains, offering comprehensible explanations for decision-making processes, includ-ing critical areas such as medical applications. While this is a promising development, there are still areas for further exploration and improvement. One area that requires attention is the overuse of social cues by LLMs. While polite and helpful phrases can enhance user experience, their misuse can become vexing over time. It is important to evaluate such interactions through human subject studies to understand user perceptions of the explanations provided by the LLM. Future research could also explore dialogue management strategies to address issues related to interaction and personalization. On another note, the dynamic nature of the external knowledge source offers the potential for further expansion with additional observations. This not only ensures that the system remains relevant and up to date but also allows the system to answer different types of questions. Additionally, incorporating multimodal elements, such as combining text with visual explanations, can enhance the effectiveness of the responses (e.g., highlighting the specific elements contributing to the prediction). Another consideration is the estimation of feature importance values, which are subject to perturbations and computational processes. It is important to assess and reconcile differences in feature importance across algorithms to ensure stability and consistency."}]}