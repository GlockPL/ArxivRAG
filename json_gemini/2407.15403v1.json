{"title": "Offline Imitation Learning Through Graph Search and Retrieval", "authors": ["Zhao-Heng Yin", "Pieter Abbeel"], "abstract": "Imitation learning is a powerful machine learning algorithm for a robot to acquire manipulation skills. Nevertheless, many real-world manipulation tasks involve precise and dexterous robot-object interactions, which make it difficult for humans to collect high-quality expert demonstrations. As a result, a robot has to learn skills from suboptimal demonstrations and unstructured interactions, which remains a key challenge. Existing works typically use offline deep reinforcement learning (RL) to solve this challenge, but in practice these algorithms are unstable and fragile due to the deadly triad issue. To overcome this problem, we propose GSR, a simple yet effective algorithm that learns from suboptimal demonstrations through Graph Search and Retrieval. We first use pretrained representation to organize the interaction experience into a graph and perform a graph search to calculate the values of different behaviors. Then, we apply a retrieval-based procedure to identify the best behavior (actions) on each state and use behavior cloning to learn that behavior. We evaluate our method in both simulation and real-world robotic manipulation tasks with complex visual inputs, covering various precise and dexterous manipulation skills with objects of different physical properties. GSR can achieve a 10% to 30% higher success rate and over 30% higher proficiency compared to baselines. Our project page is at https://zhaohengyin.github.io/gsr.", "sections": [{"title": "I. INTRODUCTION", "content": "Imitation learning [36, 22] is a powerful approach to learning robots that has achieved great success in robotic manipulation in recent years [38, 2, 3, 48, 8]. It involves training robots to mimic human demonstrations, allowing them to acquire manipulation skills in complex environments. Despite the remarkable strides made so far, we notice that most existing works usually assume expert-level task demonstrations, while many real-world robotic manipulation tasks involve precise, dexterous manipulation with complex objects which are hard for humans to provide high-quality demonstrations (Figure 1). For example, consider the task of making a robot manipulator grasp a tweezer to manipulate a tiny object, which is a common application scenario in a science laboratory. This task involves very fine-grained, precise control of gripping force and gripper movement. A human teleoperator can easily fail to provide successful demonstrations. Moreover, there usually exist suboptimal behaviors within a successful demonstration, such as retrying to grip the item if the first attempt fails. Naively using Behavior Cloning (BC) to imitate the collected suboptimal demonstration can result in incapable control policies that can reproduce failure behavior [28].\nWe need an algorithm that can learn from suboptimal human demonstrations to solve this problem. Typically, a suboptimal human demonstration is composed of multiple segments of different expertise levels, and the whole demonstration dataset contains a set of useful temporal segments that combined together can approximate the behavior of a good policy. The desired learning algorithm should be able to pick up those good segments and recover this desired policy, which is also known as trajectory stitching in reinforcement learning (RL) research [28]. In principle, it can be achieved by an offline deep-RL algorithm. However, in the application, previous research suggests that existing offline RL algorithms can usually struggle in realistic, long-horizon, pixel-based tasks [30, 40]. The simplest imitation learning algorithm like BC still performs the best among offline policy learning algorithms in many scenarios [30] and continues to be the standard practice in recent works.\nTo overcome the learning difficulties associated with typical offline RL, we propose GSR, a simple yet effective algorithm that learns from suboptimal demonstrations through Graph Search and Retrieval. Our key insight is to identify and stitch good behaviors directly through the use of graph representation and retrieval without boostrapping deep RL networks. Specifically, we leverage pretrained representations to build a graph representation for the collected interaction data. This graph serves as a nonparametric world model, with which we can reason about whether a temporal segment is proficient and can lead to desired future outcomes effectively through a graph search. Then, for each state in the dataset, we can retrieve the most proficient behavior on that state and use it for imitation learning (Figure 2). Our algorithm can be viewed as a filtering procedure to preprocess the dataset for imitation learning and can be used with any kind of deep imitation learning models (architectures). As a direct approach that uses graph search rather than deep RL, our method enjoys high time efficiency. The whole preprocessing procedure typically consumes 10 to 30 minutes (depend on the dataset) overhead compared to BC, making it practical for real-world applications.\nIn the experiments, we test our method in both simulation and real-world robotic manipulation tasks of various visual and physical complexities, involving high-resolution, multiview camera observation, and precise robot-object interactions. Despite numerous challenges in both perception and action, our method can consistently improve baselines' success rate by 10% to 30% and proficiency by over 30%. We also provide various quantitative and qualitative analyses to show that our method is capable of identifying good behaviors in the dataset."}, {"title": "II. RELATED WORK", "content": "Learning from Suboptimal and Unstructured Data How to extract and learn good behaviors from suboptimal datasets is a central challenge in policy learning. When reward labels are available (e.g. task completion reward in goal-conditioned setup in this paper), offline deep Reinforcement Learning [27, 28, 16, 7, 23, 24, 32, 42, 26, 35] is one prevailing approach to address this problem. Most of the existing offline deep RL methods take the form of value-based learning. They learn value functions to estimate the goodness of actions with recursive Bellman updates and train policy to select good actions that lead to high values. However, deep RL is highly unstable and fragile due to deadly triad [45, 40], and people usually find that such (offline) RL algorithms struggle in application scenarios involving high- dimensional visual inputs and complex physical dynamics, and fail to outperform simple imitation learning algorithms like BC [30]. Our method is also based on value-based learning, but different from the existing methods, we directly use graph search and retrieval instead of prevailing deep RL updates to evaluate the value functions and perform policy optimization. This direct approach sidesteps the difficulty of bootstrapping deep value functions in complex environments [40].\nWhen reward labels are not available, another line of work focuses on identifying proficient behaviors through other forms of feedback such as optimality labels, ranking, and preference [5, 4, 9, 43, 6, 46, 1, 31, 47, 19, 25]. This is an orthogonal research direction to the offline RL and our method. We believe our method can also benefit from these approaches by incorporating this additional information into the graph structure. This paper mainly considers a goal-conditioned setup and the proficiency can be defined through graph distances.\nGraph and Retrieval in Policy Learning Our method is built upon graph search and retrieval, and there are many policy learning works incorporating these techniques. Some works propose to incorporate a graph into the policy for goal- conditioned problems [37, 14, 13, 44, 49]. During inference, these methods involve locating the agent on the generated graph and running a graph search to decide the next subgoal. In other words, the graph can be considered as a high-level map- based planner to guide a low-level goal-conditioned policy. Our method differs from these works in the following ways. First, we only use the graph to estimate the goodness of behavior in the offline phase and do not use it as a high-level planner during inference which can fail to generalize and act robustly (pick up inappropriate next goal in deployment) [13]. Besides, we relate graphs to extracting better policies in an offline robotic dataset. Finally, we study the use of pretrained representations in building the graph in application scenarios for the first time, while existing works build the graph by training a specialized reachability network on the collected data [37].\nRecently, people have found retrieval a useful tool for policy learning, especially in the field of robotics. For example,"}, {"title": "III. PRELIMINARIES", "content": "In this paper, we study an offline policy learning setup.\nWe provide the robot agent with a human interaction dataset\nD containing several task-relevant interaction trajectories.\nEach trajectory is a sequence of observations $o_{0:T}$ and\ncorresponding actions $a_{0:T}$, i.e., $\\tau = (o_0, a_0, o_1, a_1, ..., o_T, a_T)$.\nFor each trajectory $\\tau \\in D$, the human teleoperator controls the\nrobot to solve the task and achieves success in the end, i.e.,\nthe last observation of the trajectory is a task success example.\nHowever, the demonstrated behavior can be highly suboptimal,\nincluding numerous errors and misses in the execution process.\nThe setup favors the data collection process: It allows the data\ngenerated through human practice to serve as a demonstration,\nprovided that the task is ultimately accomplished. We expect\nthe robot to learn the task-solving skill by learning from $D$\nwithout using any online interaction samples."}, {"title": "A. Problem Formulation", "content": "In this paper, we study an offline policy learning setup.\nWe provide the robot agent with a human interaction dataset\nD containing several task-relevant interaction trajectories.\nEach trajectory is a sequence of observations $o_{0:T}$ and\ncorresponding actions $a_{0:T}$, i.e., $\\tau = (o_0, a_0, o_1, a_1, ..., o_T, a_T)$.\nFor each trajectory $\\tau \\in D$, the human teleoperator controls the\nrobot to solve the task and achieves success in the end, i.e.,\nthe last observation of the trajectory is a task success example.\nHowever, the demonstrated behavior can be highly suboptimal,\nincluding numerous errors and misses in the execution process.\nThe setup favors the data collection process: It allows the data\ngenerated through human practice to serve as a demonstration,\nprovided that the task is ultimately accomplished. We expect\nthe robot to learn the task-solving skill by learning from $D$\nwithout using any online interaction samples."}, {"title": "B. Offline Policy Learning", "content": "Many offline policy learning algorithms usually involve\ntraining a policy network $\\pi_{\\theta}(a|o)$ to maximize the following\nweighted log-likelihood objective function:\n\n$L(\\theta) = E_{(o,a)\\sim D}[w(o, a) \\log \\pi_{\\theta}(a|o)]$,\n(1)\n\nwhere $w(o, a)$ is a predefined or learned weight function.\nFor example, if we define $w(o, a) = 1$ for all $(o,a) \\sim D$,\nthe objective corresponds to the behavior cloning. If we\ndefine $w(o, a) = exp(A(o,a))$ where $A$ is the advantage\nof taking action $a$ at observation $o$, this corresponds to\nthe policy extraction objective used in Advantage-Weighted\nRegression (AWR) [35] and Implicit Q-Learning (IQL) [24]."}, {"title": "IV. POLICY LEARNING\nBY GRAPH SEARCH AND RETRIEVAL", "content": "GSR is a simple method that computes each weighting\ncoefficient $w(o, a)$ using graph search and retrieval. Compared\nto the previous offline RL-based approach, it does not involve\nadvantage computation with an extra Q-learning procedure,\nwhich is known to be unstable for high-dimensional pixel-\nbased problems. For clarity, we first derive our idea step-by-\nstep from a tabular case example in section IV-A. We introduce\nthe implementation details in the remaining sections."}, {"title": "A. Overview", "content": "For simplicity, let us first consider the problem of defining\n$w(o, a)$ in a tabular case where we want the agent to reach\na certain goal observation $o_g$ efficiently at the fewest cost\nafter learning. Here, the cost function is the number of steps\ntaken to reach $o_g$. Obviously, for any observation $o$, the agent\nshould pick an action $a$ whose resulting next observation (more"}, {"title": "B. Graph Construction", "content": "The recent advance in large-scale representation pretraining\nprovides us with powerful tools to identify similarities between\ndifferent images. We can compare experiences from different\ntrajectories in the representation space, and connect similar\nstates to build a graph representation of the world. Concretely,\nwe construct a graph $G(V, E)$ to represent the whole dataset\nas follows."}, {"title": "C. Policy Improvement with Retrieval", "content": "With a constructed graph, we can calculate the value (good-\nness) of each dataset transition from $v_i$ to $v_{i+1}$ (i.e.\n$(o_{in:(i+1)n}, a_{in:(i+1)n}))$ using the distance on the graph. We\ndefine this as $Q(V_i) = Q(V_i, V_i \\to V_{i+1}) = -1 - d(v_{i+1},g)$.\nWith this function, we bias the action toward the better ones\nfor each $v$. To do this, we first retrieve $K$ nearest neighbors\nof $v$ in the feature space, denoted $N_K(v)$. Then, we reallocate\nthe default dataset weight (which is 1) assigned to $v$ (which is\n1) to $v$ and these retrieved $K$ nearest neighbors. Intuitively, we\nshould put high weight to a retrieved vertex $u$ only if (1) $u$ is\nsimilar to $v$ and (2) $u$ has a high value. More formally, inspired\nby the idea of locally-weighted regression [10], the weight\nallocated to each vertex $u \\in N(v) = {v} \\cup N_K(v)$ should\nbe proportional to $Similarity(u, v) \\times exp Q(u)$. Defining the\nfirst similarity term using an exponential form, we design the\nweight reallocation criterion at the vertex $v$ as\n\n$W_{alloc}(u) v = \\frac{exp [\\beta_1S(u, v) + \\beta_2Q(u)]}{\\Sigma_{u'\\in N(v)} exp [\\beta_1S(u', v) + \\beta_2Q(u')]} ,$\n(6)\n\nwhere\n\n$S(u, v) = \\begin{cases}\n\\frac{1}{2} (\\frac{Tol(u)}{\\left|Tol(u)\\right|} + \\frac{Tol(v)}{\\left|Tol(v)\\right|}) sim(f_u, f_v) \\leq 0\\\\\n1, & otherwise\n\\end{cases}$ (7)\n\nis a normalized feature similarity function (with respect to\n1-step neighborhood distance computed at $u$ and $v$) to evaluate\nsimilarity between $u$ and $v$. $\\beta_1$ and $\\beta_2$ are two temperature\nhyperparameters, controlling the strength of the similarity term\nand value term, respectively. In particular, when $\\beta_1 \\to \\infty$ and\n$\\beta_2 \\to 0$, the weight to distribute concentrates on $v$ and vanishes\nat the retrieved neighbors, and the policy learning reduces to\nthe case of BC. Therefore, our method smoothly interpolates\nbetween aggressive policy improvement and naive BC (no\npolicy improvement)."}, {"title": "D. Implementation and Time Complexity", "content": "We use R3M [33] as pretrained feature since it is pretrained\nwith a contrastive objective, which we find can represent\nfine-grained movement well. In the task that involves mul-\ntiple camera images, we calculate the feature embedding by\nconcatenating the embedding of each image. We can also\nfinetune the pretrained representation on the downstream task\ndemo dataset [29] to make the retrieval more robust. We\nperform finetuning with a time-contrastive learning objective in\nsimulated experiments and find that it improves the robustness\nto the selection of hyperparameters. We leave the discussion to"}, {"title": "V. EXPERIMENTS", "content": "In experiments, we evaluate our method in both simulation\nand real world to validate its effectiveness in various robotic\nmanipulation scenarios. We first study how much performance\ngain our method can achieve compared to the state-of-the-art\nimitation learning baseline. Then, through both quantitative\nand qualitative analysis, we show that our method identify and\nchain useful behaviors in the dataset to learn a robust policy.\nFinally, we present hyperparameter analysis to study design\nchoices of our method."}, {"title": "A. Experiment Setup", "content": "1) Simulation Experiments: We use the image-based\nRobomimic benchmark [30] as our testbed in simulation, which\nprovides several robotic manipulation tasks with FrankaPanda\nrobot. Specifically, we use three tasks that contain human\ndemonstrations of diverse qualities (Worse, Okay, Better):\n\u2022 Can Pick-and-Place In this task, the robot is required\nto pick up a can on the table and place it into a slot.\nWe use the following two dataset setups: Worse-Okay20"}, {"title": "VI. CONCLUSION", "content": "In this paper, we have presented GSR, a simple offline imi-\ntation learning method that can learn from suboptimal datasets\nthrough graph search and retrieval. We carried out experiments\nin both challenging simulation and real-world environments to\ndemonstrate its effectiveness. In the future, there are several\nopen problems to study. For example, how do we improve\nthe quality of the representation to make our algorithm more\npowerful? In this paper, we considered demonstrations in the\nsame context (i.e., the same workspace). However, it would\nbe more desirable to have a task-invariant representation that\ncan bridge experience from different contexts (backgrounds).\nBesides, in the context of large-scale pretraining, it would\nbe interesting to use pretrained forward prediction models to\ngenerate more long-horizon connections, rather than a 1-step\nconnection in this paper. This can leverage general knowledge\nfrom other relevant tasks for policy optimization. We hope\nthat our work can inspire more powerful policy optimization\nmethods."}, {"title": "APPENDIX", "content": "For both the simulated and real-world experiments, we use\nResNet-18 as the feature extractor. For the action denoiser,\nwe follow the setup used in the Diffusion Policy paper [8].\nSpecifically, we use a transformer-based action denoiser. We\nuse an embedding dimension of 256, and 8 layers with 4 heads.\nWe use AdamW optimizer to train our policy model. The\nlearning rate is set to 0.0001. For the simulated tasks, we train\nthe policy for 3000 epochs. For the real-world tasks, we train\nthe policy for 2000 epochs. For the DDIM noise scheduler, we\nuse 100 training steps with a squared cosine \u03b2 schedule. We use\n10 denoising steps during DDIM inference. We normalize both\ninput observation and action into [-1,1]. For the image input,\nwe apply a random crop data augmentation during training as in\nprevious works. We apply a fixed CenterCrop to the observed\nimage during inference. The setup of the hyperparameters is\nshown in Table IV."}, {"title": "A. Model and Training Details", "content": "For both the simulated and real-world experiments, we use\nResNet-18 as the feature extractor. For the action denoiser,\nwe follow the setup used in the Diffusion Policy paper [8].\nSpecifically, we use a transformer-based action denoiser. We\nuse an embedding dimension of 256, and 8 layers with 4 heads.\nWe use AdamW optimizer to train our policy model. The\nlearning rate is set to 0.0001. For the simulated tasks, we train\nthe policy for 3000 epochs. For the real-world tasks, we train\nthe policy for 2000 epochs. For the DDIM noise scheduler, we\nuse 100 training steps with a squared cosine \u03b2 schedule. We use\n10 denoising steps during DDIM inference. We normalize both\ninput observation and action into [-1,1]. For the image input,\nwe apply a random crop data augmentation during training as in\nprevious works. We apply a fixed CenterCrop to the observed\nimage during inference. The setup of the hyperparameters is\nshown in Table IV."}, {"title": "B. Representation Finetuning", "content": "Although pretrained representation can serve a broad range\nof tasks, it may still require finetuning for better retrieval\nperformance especially when the observation in the downstream\ntask is not covered by the pretraining dataset. We provide a\nsimple method for adapting the representation. We finetune\nthe representation on the downstream task trajectories with a\ntime contrastive loss used by R3M. Specifically, we freeze the\npretrained network $f$ and introduce a trainable MLP adapter\nhead $g$. Then, we train $g$ to maximize the following objective:\n\n$L =\\underset{\\underset{(x, {x_n},x_p)\\sim D }{}}{E}\\log\\frac{exp (S(z, z_p)/\\tau)}{\\sum_j exp (S(z, z_n)/\\tau) + exp (S(z, z_p)/\\tau)},$\nwhere $x$ is the anchor sample, ${x_n}$ is a set of negative samples,\n$x_p$ is the positive sample, and $z = g(f(x))$. $S(x, y) = -||x\n- y||^2$ is the $L2$ distance function. $\\tau = 0.1$ is a temperature\nhyperparameter. The selection of negative and positive samples\nis the same as that used in the R3M paper [33]. After training,\nwe use $g(f(x))$ as the finetuned representation. We apply\nrepresentation finetuning in the simulation experiments. $g$ is an\nMLP with hidden layer size [512, 256] which outputs a 128-d\nvector. We use a batch size of 128 and a learning rate of 0.0003.\nWe find that without representation finetuning, our approach can\nbe more sensitive to hyperparameters in simulation. Specifically,\nthe success rate and normalized proficiency can drop by 4%\nand 12% respectively, on average, across different $B_1$ used in\nour hyperparameter analysis part. However, the performance\nof the best hyperparameter setup without finetuning can still\nmatch that of the finetuned approach.\nSince the representation network $f$ is frozen in this procedure\nand the only trainable network is a shallow MLP $g$, the\nrepresentation finetuning procedure is also computationally\nefficient."}, {"title": "C. Human Demos", "content": "We use 100 demonstrations for the real world tasks. We\nrandomly initialize the object and the robot pose at the\nbeginning of each demo collection. We also slightly randomize\nthe camera pose in the workspace and the light during the\ndemo collection process. Some human demo examples can be\nfound in the trajectory visualization in section E."}, {"title": "D. Performance under Different Number of Demos", "content": "We provide the performance of different approaches when\nusing different numbers of demos in Can PickAndPlace and\nNut Assembly in Table III. Our method can achieve similar\nperformance gains in success rate and proficiency."}, {"title": "E. More Qualitative Results", "content": "We visualize more weight assignment plots in Figure 10,\nFigure 11, Figure 12 and Figure 13."}]}