{"title": "Offline Imitation Learning Through Graph Search and Retrieval", "authors": ["Zhao-Heng Yin", "Pieter Abbeel"], "abstract": "Imitation learning is a powerful machine learning algorithm for a robot to acquire manipulation skills. Nevertheless, many real-world manipulation tasks involve precise and dexterous robot-object interactions, which make it difficult for humans to collect high-quality expert demonstrations. As a result, a robot has to learn skills from suboptimal demonstrations and unstructured interactions, which remains a key challenge. Existing works typically use offline deep reinforcement learning (RL) to solve this challenge, but in practice these algorithms are unstable and fragile due to the deadly triad issue. To overcome this problem, we propose GSR, a simple yet effective algorithm that learns from suboptimal demonstrations through Graph Search and Retrieval. We first use pretrained representation to organize the interaction experience into a graph and perform a graph search to calculate the values of different behaviors. Then, we apply a retrieval-based procedure to identify the best behavior (actions) on each state and use behavior cloning to learn that behavior. We evaluate our method in both simulation and real-world robotic manipulation tasks with complex visual inputs, covering various precise and dexterous manipulation skills with objects of different physical properties. GSR can achieve a 10% to 30% higher success rate and over 30% higher proficiency compared to baselines. Our project page is at https://zhaohengyin.github.io/gsr.", "sections": [{"title": "I. INTRODUCTION", "content": "Imitation learning [36, 22] is a powerful approach to learning robots that has achieved great success in robotic manipulation in recent years [38, 2, 3, 48, 8]. It involves training robots to mimic human demonstrations, allowing them to acquire manipulation skills in complex environments. Despite the remarkable strides made so far, we notice that most existing works usually assume expert-level task demonstrations, while many real-world robotic manipulation tasks involve precise, dexterous manipulation with complex objects which are hard for humans to provide high-quality demonstrations (Figure 1). For example, consider the task of making a robot manipulator grasp a tweezer to manipulate a tiny object, which is a common application scenario in a science laboratory. This task involves very fine-grained, precise control of gripping force and gripper movement. A human teleoperator can easily fail to provide successful demonstrations. Moreover, there usually exist suboptimal behaviors within a successful demonstration, such as retrying to grip the item if the first attempt fails. Naively using Behavior Cloning (BC) to imitate the collected suboptimal demonstration can result in incapable control policies that can reproduce failure behavior [28].\nWe need an algorithm that can learn from suboptimal human demonstrations to solve this problem. Typically, a suboptimal human demonstration is composed of multiple segments of different expertise levels, and the whole demonstration dataset contains a set of useful temporal segments that combined together can approximate the behavior of a good policy. The desired learning algorithm should be able to pick up those good segments and recover this desired policy, which is also known as trajectory stitching in reinforcement learning (RL) research [28]. In principle, it can be achieved by an offline deep-RL algorithm. However, in the application, previous research suggests that existing offline RL algorithms can usually struggle in realistic, long-horizon, pixel-based tasks [30, 40]. The simplest imitation learning algorithm like BC still performs the best among offline policy learning algorithms in many scenarios [30] and continues to be the standard practice in recent works.\nTo overcome the learning difficulties associated with typical offline RL, we propose GSR, a simple yet effective algorithm that learns from suboptimal demonstrations through Graph Search and Retrieval. Our key insight is to identify and stitch good behaviors directly through the use of graph representation and retrieval without boostrapping deep RL networks. Specifically, we leverage pretrained representations to build a graph representation for the collected interaction data. This graph serves as a nonparametric world model, with which we can reason about whether a temporal segment is proficient and can lead to desired future outcomes effectively through a graph search. Then, for each state in the dataset, we can retrieve the most proficient behavior on that state and use it for imitation learning (Figure 2). Our algorithm can be viewed as a filtering procedure to preprocess the dataset for imitation learning and can be used with any kind of deep imitation learning models (architectures). As a direct approach that uses graph search rather than deep RL, our method enjoys high time efficiency. The whole preprocessing procedure typically consumes 10 to 30 minutes (depend on the dataset) overhead compared to BC, making it practical for real-world applications.\nIn the experiments, we test our method in both simulation and real-world robotic manipulation tasks of various visual and physical complexities, involving high-resolution, multiview camera observation, and precise robot-object interactions. Despite numerous challenges in both perception and action, our method can consistently improve baselines' success rate by 10% to 30% and proficiency by over 30%. We also provide various quantitative and qualitative analyses to show that our method is capable of identifying good behaviors in the dataset."}, {"title": "II. RELATED WORK", "content": "Learning from Suboptimal and Unstructured Data How to extract and learn good behaviors from suboptimal datasets is a central challenge in policy learning. When reward labels are available (e.g. task completion reward in goal-conditioned setup in this paper), offline deep Reinforcement Learning [27, 28, 16, 7, 23, 24, 32, 42, 26, 35] is one prevailing approach to address this problem. Most of the existing offline deep RL methods take the form of value-based learning. They learn value functions to estimate the goodness of actions with recursive Bellman updates and train policy to select good actions that lead to high values. However, deep RL is highly unstable and fragile due to deadly triad [45, 40], and people usually find that such (offline) RL algorithms struggle in application scenarios involving high- dimensional visual inputs and complex physical dynamics, and fail to outperform simple imitation learning algorithms like BC [30]. Our method is also based on value-based learning, but different from the existing methods, we directly use graph search and retrieval instead of prevailing deep RL updates to evaluate the value functions and perform policy optimization. This direct approach sidesteps the difficulty of bootstrapping deep value functions in complex environments [40].\nWhen reward labels are not available, another line of work focuses on identifying proficient behaviors through other forms of feedback such as optimality labels, ranking, and preference [5, 4, 9, 43, 6, 46, 1, 31, 47, 19, 25]. This is an orthogonal research direction to the offline RL and our method. We believe our method can also benefit from these approaches by incorporating this additional information into the graph structure. This paper mainly considers a goal-conditioned setup and the proficiency can be defined through graph distances.\nGraph and Retrieval in Policy Learning Our method is built upon graph search and retrieval, and there are many policy learning works incorporating these techniques. Some works propose to incorporate a graph into the policy for goal- conditioned problems [37, 14, 13, 44, 49]. During inference, these methods involve locating the agent on the generated graph and running a graph search to decide the next subgoal. In other words, the graph can be considered as a high-level map- based planner to guide a low-level goal-conditioned policy. Our method differs from these works in the following ways. First, we only use the graph to estimate the goodness of behavior in the offline phase and do not use it as a high-level planner during inference which can fail to generalize and act robustly (pick up inappropriate next goal in deployment) [13]. Besides, we relate graphs to extracting better policies in an offline robotic dataset. Finally, we study the use of pretrained representations in building the graph in application scenarios for the first time, while existing works build the graph by training a specialized reachability network on the collected data [37].\nRecently, people have found retrieval a useful tool for policy learning, especially in the field of robotics. For example,"}, {"title": "III. PRELIMINARIES", "content": "In this paper, we study an offline policy learning setup.\nWe provide the robot agent with a human interaction dataset\nD containing several task-relevant interaction trajectories.\nEach trajectory is a sequence of observations 00:T and\ncorresponding actions ao:T, i.\u0435., \u0442 = (00, 00, 01, 01, ..., \u043e\u0442, \u0430\u0442).\nFor each trajectory T\u2208 D, the human teleoperator controls the\nrobot to solve the task and achieves success in the end, i.e.,\nthe last observation of the trajectory is a task success example.\nHowever, the demonstrated behavior can be highly suboptimal,\nincluding numerous errors and misses in the execution process.\nThe setup favors the data collection process: It allows the data\ngenerated through human practice to serve as a demonstration,\nprovided that the task is ultimately accomplished. We expect\nthe robot to learn the task-solving skill by learning from D\nwithout using any online interaction samples."}, {"title": "A. Problem Formulation", "content": "In this paper, we study an offline policy learning setup.\nWe provide the robot agent with a human interaction dataset\nD containing several task-relevant interaction trajectories.\nEach trajectory is a sequence of observations 00:T and\ncorresponding actions ao:T, i.\u0435., \u0442 = (00, 00, 01, 01, ..., \u043e\u0442, \u0430\u0442).\nFor each trajectory T\u2208 D, the human teleoperator controls the\nrobot to solve the task and achieves success in the end, i.e.,\nthe last observation of the trajectory is a task success example.\nHowever, the demonstrated behavior can be highly suboptimal,\nincluding numerous errors and misses in the execution process.\nThe setup favors the data collection process: It allows the data\ngenerated through human practice to serve as a demonstration,\nprovided that the task is ultimately accomplished. We expect\nthe robot to learn the task-solving skill by learning from D\nwithout using any online interaction samples."}, {"title": "B. Offline Policy Learning", "content": "Many offline policy learning algorithms usually involve\ntraining a policy network \u03c0\u03bf(\u03b1|o) to maximize the following\nweighted log-likelihood objective function:\n$$L(\\theta) = E_{(o,a)\\sim D}[w(o, a) \\log \\pi_{\\theta}(a|o)],$$\nwhere w(o, a) is a predefined or learned weight function.\nFor example, if we define w(o, a) = 1 for all (o,a) ~ D,\nthe objective corresponds to the behavior cloning. If we\ndefine w(o, a) = exp(A(o,a)) where A is the advantage\nof taking action a at observation o, this corresponds to\nthe policy extraction objective used in Advantage-Weighted\nRegression (AWR) [35] and Implicit Q-Learning (IQL) [24]."}, {"title": "IV. POLICY LEARNING BY GRAPH SEARCH AND RETRIEVAL", "content": "GSR is a simple method that computes each weighting\ncoefficient w(o, a) using graph search and retrieval. Compared\nto the previous offline RL-based approach, it does not involve\nadvantage computation with an extra Q-learning procedure,\nwhich is known to be unstable for high-dimensional pixel-\nbased problems. For clarity, we first derive our idea step-by-\nstep from a tabular case example in section IV-A. We introduce\nthe implementation details in the remaining sections."}, {"title": "A. Overview", "content": "For simplicity, let us first consider the problem of defining\nw(o, a) in a tabular case where we want the agent to reach\na certain goal observation og efficiently at the fewest cost\nafter learning. Here, the cost function is the number of steps\ntaken to reach og. Obviously, for any observation o, the agent\nshould pick an action a whose resulting next observation (more"}, {"title": "B. Graph Construction", "content": "The recent advance in large-scale representation pretraining\nprovides us with powerful tools to identify similarities between\ndifferent images. We can compare experiences from different\ntrajectories in the representation space, and connect similar\nstates to build a graph representation of the world. Concretely,\nwe construct a graph G(V, E) to represent the whole dataset\nas follows."}, {"title": "C. Policy Improvement with Retrieval", "content": "With a constructed graph, we can calculate the value (good-\nness) of each dataset transition from vi to vi+1 (i.\u0435.\n(Oin:(i+1)n, ain:(i+1)n)) using the distance on the graph. We\ndefine this as Q(Vi) = Q(Vi, Vi \u2192 Vi+1) = \u22121 \u2013 d(vi+1,9).\nWith this function, we bias the action toward the better ones\nfor each v. To do this, we first retrieve K nearest neighbors\nof v in the feature space, denoted N\u03ba (v). Then, we reallocate\nthe default dataset weight (which is 1) assigned to v (which is\n1) to v and these retrieved K nearest neighbors. Intuitively, we\nshould put high weight to a retrieved vertex u only if (1) u is\nsimilar to v and (2) u has a high value. More formally, inspired\nby the idea of locally-weighted regression [10], the weight\nallocated to each vertex u \u2208 N(v) = {v} UN\u03ba(v) should\nbe proportional to Similarity(u, v) \u00d7 exp Q(u). Defining the\nfirst similarity term using an exponential form, we design the\nweight reallocation criterion at the vertex vas\n$$W_{alloc}(u) \\propto_v  \\frac{exp [\\beta_1S(u, v) + \\beta_2Q(u)]}{\\sum_{u' \\in N(v)} exp [\\beta_1S(u', v) + \\beta_2Q(u')]}$$\nwhere\n$$S(u, v) = \\frac{1}{2} \\cdot (\\frac{1}{Tol(u)} + \\frac{1}{Tol(v)}) \\cdot \\begin{cases}\nsim(f_u, f_v) & \\text{if } sim(f_u, f_v) \\leq 0\\\\\n1 & \\text{otherwise}\n\\end{cases}$$\nis a normalized feature similarity function (with respect to\n1-step neighborhood distance computed at u and v) to evaluate\nsimilarity between u and v. \u03b2\u2081 and B2 are two temperature\nhyperparameters, controlling the strength of the similarity term\nand value term, respectively. In particular, when \u1e9e1 \u2192 \u221e and\nB2 \u2192 0, the weight to distribute concentrates on v and vanishes\nat the retrieved neighbors, and the policy learning reduces to\nthe case of BC. Therefore, our method smoothly interpolates\nbetween aggressive policy improvement and naive BC (no\npolicy improvement)."}, {"title": "D. Implementation and Time Complexity", "content": "We use R3M [33] as pretrained feature since it is pretrained\nwith a contrastive objective, which we find can represent\nfine-grained movement well. In the task that involves mul-\ntiple camera images, we calculate the feature embedding by\nconcatenating the embedding of each image. We can also\nfinetune the pretrained representation on the downstream task\ndemo dataset [29] to make the retrieval more robust. We\nperform finetuning with a time-contrastive learning objective in\nsimulated experiments and find that it improves the robustness\nto the selection of hyperparameters. We leave the discussion to\nthe appendix. The pseudo-code of our method is summarized\nin Algorithm 1. The whole process is mainly composed of a\ngraph search and single-round retrieval procedure, followed\nby BC. In particular, when we do not finetune the pretrained\nrepresentation, the overhead computation compared to BC (Line\n2-9) in general takes less than 10 minutes in our experiments,\nmaking our method convenient to use. The time complexity\nof this procedure is O(N log N + |E|), where N is the size\nof the dataset, and E is the number of graph edges. Full\nimplementation is in the appendix."}, {"title": "V. EXPERIMENTS", "content": "In experiments, we evaluate our method in both simulation\nand real world to validate its effectiveness in various robotic\nmanipulation scenarios. We first study how much performance\ngain our method can achieve compared to the state-of-the-art\nimitation learning baseline. Then, through both quantitative\nand qualitative analysis, we show that our method identify and\nchain useful behaviors in the dataset to learn a robust policy.\nFinally, we present hyperparameter analysis to study design\nchoices of our method."}, {"title": "A. Experiment Setup", "content": "1) Simulation Experiments: We use the image-based\nRobomimic benchmark [30] as our testbed in simulation, which\nprovides several robotic manipulation tasks with FrankaPanda\nrobot. Specifically, we use three tasks that contain human\ndemonstrations of diverse qualities (Worse, Okay, Better):\n\u2022\nCan Pick-and-Place In this task, the robot is required\nto pick up a can on the table and place it into a slot.\nWe use the following two dataset setups: Worse-Okay20"}]}