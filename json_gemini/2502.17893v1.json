{"title": "Sample-efficient diffusion-based control of complex nonlinear systems", "authors": ["Hongyi Chen", "Jingtao Ding", "Jianhai Shu", "Xinchun Yu", "Xiaojun Liang", "Yong Li", "Xiao-Ping Zhang"], "abstract": "Complex nonlinear system control faces chal- lenges in achieving sample-efficient, reliable performance. While diffusion-based methods have demonstrated advantages over classical and reinforcement learning approaches in long- term control performance, they are limited by sample efficiency. This paper presents SEDC (Sample-Efficient Diffusion-based Control), a novel diffusion-based control framework address- ing three core challenges: high-dimensional state- action spaces, nonlinear system dynamics, and the gap between non-optimal training data and near-optimal control solutions. Through three innovations - Decoupled State Diffusion, Dual- Mode Decomposition, and Guided Self-finetuning - SEDC achieves 39.5%-49.4% better control ac- curacy than baselines while using only 10% of the training samples, as validated across three com- plex nonlinear dynamic systems. Our approach represents a significant advancement in sample- efficient control of complex nonlinear systems. The implementation of the code can be found at https://anonymous.4open.science/r/DIFOCON- C019.", "sections": [{"title": "1. Introduction", "content": "The control of complex systems plays a critical role across diverse domains, from industrial automation and biological networks to robotics. Given the challenges in deriv- ing governing equations for empirical systems, data-driven control methods which design control modules directly based on experimental data collected from the system, by- passing the need for explicit mathematical modeling-have gained prominence for their robust real-world applicabil- ity. Before data-driven control methods, traditional Proportional- Integral-Derivative (PID) controllers dom- inated complex system control through continuous error correction. However, these classical methods show limita- tions with complex nonlinear systems due to their linear control nature. Data-driven machine learning approaches have emerged to address these limitations by learning non- linear control policies from interaction data, falling into three categories: supervised learning, reinforcement learn- ing (RL), and diffusion-based methods. Supervised learning approaches like Behavior Cloning (BC) learn direct state-to-action mappings from expert demon- strations, while RL methods like Batch Proximal Policy Optimization (BPPO) learn control policies through value function approximation, showing better adaptability to high-dimensional states than classical methods. However, both approaches often exhibit myopic decision-making in long-horizon tasks due to their iterative view of control dynamics. In contrast, diffusion-based meth- ods reformulate control as sequence generation, enabling comprehensive optimization over entire system trajectories. This long-term perspective allows diffusion-based methods to overcome limitations of both classical and RL approaches, achieving superior long- term control performance.\nThe success of diffusion models in data-driven control stems from their exceptional ability to learn complex trajectory distributions from empirical data. In practice, these trajec- tories are typically collected from systems operated under empirical rules or random policies. Moreover, due to op- erational costs, the available data volume is often limited. Diffusion-based methods must therefore learn effective con- trol policies from such non-optimal and sparse trajectory data-a challenge that manifests in three key aspects. First, limited data volume impedes sample-efficient learning in high-dimensional systems. Existing diffusion-based controllers (e.g., DiffPhyCon) attempt to directly generate long-term (T steps) state-action tra- jectories by learning a $T \\times (P + M)$-dimensional distri-"}, {"title": "2. Related Works", "content": "Classic control methods. Data-driven control of com- plex systems has witnessed significant methodological de- velopments across multiple paradigms. Classical control methods, represented by Proportional-Integral-Derivative (PID) controllers, operate through contin- uous sensing-actuation cycles in a feedback-based manner. While these methods offer straightforward implementation, they face fundamental limitations when dealing with high- dimensional complex scenarios. More sophisticated analyt- ical approaches, such as those presented in , have attempted to determine optimal control inputs for complex networks without explicit dynamics knowledge. However, their foundation in linear systems theory inher- ently restricts their applicability to nonlinear systems.\nData-driven control methods. The emergence of super- vised learning and reinforcement learn- ing has in- troduced more adaptive approaches to complex control problems, demonstrating promising results in sequential decision-making tasks. However, these approaches often struggle with real-world deployment due to computational constraints and the challenge of making effective decisions over extended time horizons. More recently, denoising diffu- sion probabilistic models have emerged as a powerful framework for modeling high-dimensional dis- tributions, achieving remarkable success across various do- mains including image, audio, and video generation. This success has inspired their application to control problems, with several works demonstrating their potential in robotic control and trajec- tory generation. The diffusion framework has also shown promise in related tech- nical domains such as optimization and inverse problems. For diffusion-based control methods, works like demonstrate capa- bilities in generating long-term control trajectories for rein- forcement learning environments, but they employ generic architectures that struggle to capture highly nonlinear dy- namics, while our method incorporates specialized designs for effective nonlinear system learning. Other works like focus primarily on robotic control without specific considerations for complex system dynamics, while our approach is specifically de- signed to handle the challenges of high-dimensional state-"}, {"title": "3. Backgrounds", "content": "3.1. Problem Setting\nThe dynamics of a controlled complex system can be repre- sented by the differential equation $\\ddot{y}_t = \\phi(y_t, u_t)$, where $y_t \\in \\mathbb{R}^N$ represents the observed system state and $u_t \\in \\mathbb{R}^M$ denotes the control input. We assume the system satisfies the controllability condition without loss of generality: for any initial state $y_o$ and target state $y_f$, there exists a finite time $T$ and a corresponding control input $u$ that can drive the system from $y_o$ to $y_f$. This assumption ensures the technical feasibility of our control objectives. In practical applications, beyond achieving state transitions, we need to optimize the energy consumption during the control process. The energy cost can be quantified using the $L_2$-norm integral of the control input: $J(y, u) = \\int_0^T |u(t')|^2dt'$. For data- driven optimal control problems, we can only understand"}, {"title": "3.2. Diffusion Model", "content": "Diffusion models have become leading generative models, showing exceptional results across image synthesis, audio generation and other applications. These models,"}, {"title": "4. SEDC: the Proposed Method", "content": "In this section, we introduce our three key innovative designs of SEDC: Decoupled State Diffusion, Denoising Network Design of Dual Mode Decomposition, and Guided Self- finetuning."}, {"title": "4.1. Decoupled State Diffusion (DSD)", "content": "Decoupling Control Estimation using Inverse Dynamics. There are deep connections behind states, controls, and con- straints, considering both the relationship between control and state and learning the dynamics behind state evolution. Such relation becomes more complex as the dimension of the system states goes up. However, some diffusion-based methods (e.g., DiffPhyCon), which jointly diffuse over state and input, learn the relationship implicitly and may generate physically inconsistent state-control pairs that violate the underlying system dynamics. Additionally, control actions are less smooth than states, making their distribution more challenging to model. Therefore, rather than jointly sampling both control signals and intermediate states using the denoising network, we choose to decouple them and diffuse only states $y$, i.e.\n$$x := y^{[0:T]}$$\nThen we update the prediction of control $u$ sequence by inputting the generated state trajectory to an inverse dynamic model $f_{\\phi}$:\n$$u_{t, \\text{update}} = f_{\\phi}(y_t, y_{t+1}),$$\nwhere $\\hat{y}_t^0$ denotes the final output of the denoising timestep from the diffusion model. We parameterize it using an Autoregressive MLP and optimize it simultaneously with the denoiser via training data. Our final optimization loss function is:\n$$\\mathcal{L}(\\theta, \\phi) := \\mathbb{E}_{x, k, y_0, y_f, \\epsilon}[||x - x_\\theta(x_k, k, y_0, y_f, \\epsilon)||^2] + \\mathbb{E}_{y_t, u_t, y_{t+1}}[||u_t - f_{\\phi}(y_t, y_{t+1})||^2],$$\nwhere $y_t, u_t, y_{t+1}$ are sampled from the dataset. Note that the data used to train the diffusion model can also be utilized to train $f_{\\phi}$.\nCost Optimization via Gradient Guidance. After train- ing both models, we optimize the cost function $J$ through inference-time gradient guidance. During the denoising pro- cess, we modify the sampling procedure by incorporating cost gradients:\n$$\\mu_{\\theta}(x_k, k, y_0, y_f) = \\frac{\\sqrt{\\alpha_k}(1-\\bar{\\alpha}_{k-1})x_k + \\sqrt{\\bar{\\alpha}_{k-1}}\\beta_k x_{\\theta}}{\\sqrt{1-\\bar{\\alpha}_k}} - \\frac{\\beta_k \\sqrt{1-\\alpha_{k-1}}}{\\sqrt{1-\\bar{\\alpha}_k}} \\Sigma_k \\nabla_{x_k} J(x^0(x_k)),$$\nwhere $\\lambda$ controls guidance strength, $\\Sigma_k$ is the noise scale at step $k$, $\\alpha_k := 1 - \\beta_k$ and $\\bar{\\alpha}_k := \\prod_{i=1}^k \\alpha_i$. Since our diffusion model operates on states only, we recover control inputs using the inverse dynamics model $f_{\\phi}$ at each step. This approach enables optimization of arbitrary cost func- tions without model retraining, while maintaining trajectory feasibility through the learned diffusion process.\nTarget-conditioning as Inpainting. Modeling whether the generated trajectory accurately satisfies the initial state of $y_0$ and the desired target state of $y_f$ can also be regarded as a constraint satisfaction of equations, that is, the generated trajectory should contain the start and target. We adopt a more direct method to solve this: we not only input it as an additional condition to the diffusion denoising network but also treat it as an inpainting problem similar to image gener- ation. In brief, we substitute the corresponding location in"}, {"title": "4.2. Dual Mode Decomposition (DMD) for Denoising Module", "content": "In this section, we propose a design for the denoising net- work that decomposes the modeling of linear and nonlinear modes in the sampled trajectory by a dual-Unet architecture, as shown in Figure 1.\nOur design draws inspiration from control theory. For linear systems, Yan et al. (2012) demonstrated that optimal control signals have a linear relationship with a specific linear com- bination $y_c$ of initial and target states. Building upon this insight, we develop a framework where a bias-free linear layer first learns this crucial linear combination $y_c$ from the initial state $y_o$ and target state $y_f$. The first UNet then learns coefficients that map $y_c$ to control signals, establishing first- order terms, while the second UNet learns coefficients for quadratic terms. These quadratic terms, introduced through residual connections, refine the first-order approximation and enhance the network's capacity to model complex dy- namics. Note that the nonlinear terms essentially come from the nonlinearity of the dynamics.\nThe implementation includes several key components. The network accepts as input: (1) noisy trajectory $x_k$ with dimen- sion $(N)$ corresponding to the network's channel dimension; (2) initial state $y_o$ and target state $y_f$, which generate $y_c$ through a bias-less linear layer; and (3) diffusion timesteps $k$, encoded via sinusoidal embedding as $k_{\\text{emb}}$. The first UNet generates first-order coefficients to compute an initial prediction using $y_c$. Subsequently, the second UNet combines these features to generate quadratic coefficients, producing correction terms through quadratic operations with $y_c$. The final output $x^0$ combines these components to predict the denoised trajectory.\nThe architecture decomposes system dynamics into linear and nonlinear components, effectively handling complex features in nonlinear control systems while maintaining numerical stability during training. The first-order and quadratic terms based on $y_c$ incorporate fundamental con- trol principles into the network structure, providing effective constraints for the learning process. This structured induc- tive bias significantly improves the model's data efficiency, enabling reliable control strategy learning from limited train- ing samples.\nThe network performs sequential transformations on the input signals. Let $B$ denote batch size, $T$ sequence length, $C_1$ and $C_2$ feature dimensions, and $N$ the dimension of $y_c$. The input noisy trajectory $x_k \\in \\mathbb{R}^{B \\times T \\times N}$ and $y_c \\in \\mathbb{R}^{B \\times C_1}$"}, {"title": "4.3. Guided Self-finetuning (GSF)", "content": "Randomly generated training data cannot guarantee cover- age of optimal scenarios. To generate near-optimal controls that may deviate significantly from the training distribu- tion. To address this limitation, we propose leveraging the model's initially generated data (under the guidance of cost function), which naturally deviates from the training distribution toward optimality, for iterative retraining to sys- tematically expand the exploration space. This approach maintains physical consistency by ensuring generated sam- ples adhere to the underlying system dynamics.\nOur methodology involves extracting control sequences from the generated samples (i.e., the output of inverse dy- namics $u_{\\text{update}}^{0:T}$), and reintroduces it into the system to in- teract and generate corresponding state sequences $y_{\\text{update}}^{0:T}$. Together we add the renewed $[u_{\\text{update}}, y_{\\text{update}}]$ to the re-train data pool used for a new round of fine-tuning, no- tably without requiring explicit system parameter identifi- cation. We iterate this process over multiple rounds spec- ified by a hyperparameter, systematically expanding the model's exploration space to progressively approach op- timal control policy. Denote the sampling process under cost $J$'s guidance and the following interacting process as $[u_{\\text{update}}, y_{\\text{update}}] = S(x_k, y_0, y_f, J, \\Phi)$. The process can be formulated as:\n$$[u_{\\text{update}}, y_{\\text{update}}] = S(x_k, y_0, y_f, J, \\Phi) \\sim \\mathcal{D}(x_k, y_0, y_f, J, \\Phi),$$\n$$\\mathcal{D} = [\\mathcal{D}, [u_{\\text{update}}, y_{\\text{update}}]],$$\nwhere $\\mathcal{D}$ is the training set.\nWe provide the algorithm form of SEDC in Appendix 1."}, {"title": "5. Experiments", "content": "Experiment settings. We conducted experiments on three nonlinear systems, following the instructions in the pre- vious works for data synthesis. These systems include: the 1-D Burgers dynamics which serves as a fundamental model for studying nonlinear wave propagation and turbulent fluid flow; the Kuramoto dynamics which is essential for under- standing synchronization phenomena in complex networks and coupled oscillator systems; and the inverted pendulum dynamics, which represents a classical benchmark problem in nonlinear control theory and robotic systems. For each system, we generated control/state tra- jectory data using the finite difference method and selected 50 trajectories as the test set. Detailed descriptions of the system dynamics equations and data synthesis procedures are provided in the appendix.\nWe evaluate two metrics which is crucial in complex sys- tem control: Target Loss, the mean-squared-error (MSE) of $y_T$ and desired target $y_f$, i.e. $||y_T - y_f||^2$. (Note that $y_T$ is obtained by simulating the real system using the control inputs generated by each method, along with the given initial state conditions, rather than extracted from the sample trajectories of the diffusion-based methods); Energy $J = \\int_0^T |u(t')|^2dt'$, which measures the cumulative control effort required to achieve the target state. Lower values of both metrics indicate better performance."}, {"title": "Baselines.", "content": "We select the following state-of-the-art(SOTA) baseline methods for comparison. For traditional control approaches, we employ the classical PID (Proportional- Integral-Derivative) controller, which re- mains widely used in industrial applications. For super- vised learning, we employ Behavioral Cloning (BC), an established imitation learning approach. In terms of reinforcement learning methods, we incorporate BPPO, a state-of-the-art algorithm. For diffusion-based methods, we include several recent promi- nent approaches: DecisionDiffuser (DecisionDiff), which is a SOTA classifier-free diffusion- based planner; AdaptDiffuser, which enhances DecisionDiffuser with a self-evolving mechanism; RDM, which adaptively determines the timing of complete control sequence sampling; and DiffPhy- Con, which is specifically designed for controlling complex physical systems. Detailed descriptions of the baselines are included in Appendix D."}, {"title": "5.1. Overall Control Performance", "content": "Results. In Figure 2, we compare different methods' performance across three dynamical systems using two- dimensional coordinate plots, where proximity to the lower- left corner indicates better trade-offs between control accu- racy and energy efficiency. Since unstable control can lead to system failure regardless of energy efficiency, we prior- itize control accuracy and report metrics at each method's"}, {"title": "5.2. Sample Efficiency", "content": "Experiment settings. To evaluate the sample efficiency of diffusion-based methods, we conducted experiments on all the systems using varying proportions of the full training dataset. Specifically, we trained models using 1%, 5%, 10%, 20%, and 100% of the available data and assessed their performance using the Target Loss metric on a held-out test set.\nResults. Figure 3 demonstrates our method's superior per- formance in controlling Burgers and Kuramoto systems compared to state-of-the-art baselines. In all systems, our ap- proach achieves significantly lower target loss values across all training data percentages. Most notably, with only 10% of the training data, our method attains a target loss of 1.71e-4 for Burgers, 1.12e-5 for Kuramoto, and 6.35e-4 for Inverse Pendulum, matching(-5.5% in Burgers) or exceed- ing(+36.4% in Kuramoto abd +1.2% in Inverse Pendulum) the performance of best baseline methods trained on the complete dataset. This indicates our method can achieve state-of-the-art performance while requiring only 10% of the training samples.\nAmong baselines, DiffPhyCon performed poorest due to its dual diffusion model training requirement - a challenge amplified with limited data. AdaptDiffuser surpassed Deci- sionDiffuser through its retraining mechanism that enhances"}, {"title": "5.3. Ablation Study", "content": "Overall ablation study. We explore the main performance against each ablation of the original SEDC. Specifically, w/o DSD removes the inverse dynamics, unifying the diffusion of system state and control input, i.e. $x = [u, y]$. Therefore, the diffusion model is required to simultaneously capture the temporal information and implicit dynamics of the control and system trajectory. Note that the inpainting mechanism and gradient guidance are retained. w/o DMD removes the decomposition design, resulting in a single 1-D Unet struc- ture as the denoising network, following DecisionDiff. Finally, w/o GSF reports the performance with- out iterative self-finetuning, which means the model only uses the original dataset to train itself. To show the sample- efficiency performance, we also investigate the results under less amount of training sample(10%). For w/o DMD and w/o DSD, we adjust the number of trainable parameters at a comparable level against the original version.\nTable 1 shows the Target Loss performance of different abla- tions of SEDC across multiple datasets and different training sample ratios. As can be seen, removing any component leads to a certain decrease in performance, whether the train- ing data is limited or not, demonstrating the effectiveness of each design. The most significant performance drops are often observed in w/o DSD, highlighting the importance of explicit learning of dynamics in complex systems. w/o DMD exhibits the lowest decline across the three systems. This is because the single-Unet-structured denoising network can already capture the nonlinearity to some extent, but not as good as the proposed decomposition approach. with 10% of training data, removing individual components still led to noticeable performance degradation, and the patterns con- sistent with the full dataset results. This demonstrates that our designs remain effective in low-data scenarios."}, {"title": "Effectiveness of DSD.", "content": "To evaluate DSD's effectiveness against the curse of dimensionality, we compared the per- formance of original and w/o DSD models across Kuramoto systems with dimensions ranging from $N = 4$ to $N = 8$. Experimental results (Table 2) show that performance degra- dation (Dec.) from w/o DSD increases with system dimen- sionality, demonstrating DSD's enhanced effectiveness in higher-dimensional systems and validating its capability to address dimensionality challenges.\nTo investigate the effectiveness of dynamical learning, we compared the consistency between action sequences and diffusion-sampled state trajectories in models with and with- out DSD. While both approaches can sample state trajecto- ries from diffusion samples, they differ in action generation: SEDC uses inverse dynamics prediction, whereas w/o DSD obtains actions directly from diffusion samples by simul- taneously diffusing states and control inputs. We test both models using identical start-target conditions and visualize the state induced from the generated actions and the state sampled from the diffusion model, along with the differ- ence (error) between the above two states in Figure 4. We can observe that SEDC's action-induced state trajectories showed significantly higher consistency with sampled tra- jectories compared to w/o DSD, demonstrating that DSD using inverse dynamics achieves more accurate learning of control-state dynamical relationships."}, {"title": "Effectiveness of DMD.", "content": "To investigate the contribution of"}, {"title": "6. Conclusion", "content": "In this paper, we presented SEDC, a novel sample-efficient diffusion-based framework for complex nonlinear system control. By addressing fundamental challenges in data- driven control through three key innovations - Decoupled State Diffusion (DSD), Dual-Mode Decomposition (DMD), and Guided Self-finetuning (GSF) - SEDC achieves superior control performance while significantly reducing sample re- quirements. Our comprehensive experiments across three nonlinear systems demonstrate that SEDC outperforms ex- isting methods by 39.5%-49.4% in control accuracy while maintaining computational efficiency. Most notably, SEDC achieves state-of-the-art performance using only 10% of the training samples required by baseline methods, mark- ing a significant advancement in sample-efficient control of complex systems. These results validate our approach's"}, {"title": "A. Algorithm form of SEDC", "content": "Algorithm 1 SEDC: Training and finetuning\nInput: Initial dataset $\\mathcal{D}_0$, diffusion steps $K$, guidance strength $\\lambda$, self-finetuning rounds $R$, forward dynamics $f_{\\text{forward}}$\nOutput: Optimized trajectory $y_{0:T}^*$, controls $u_{0:T}^*$\nFunction Initial Training($\\mathcal{D}_0$)\nwhile not converged do\nSample batch ($y_{0:T}, u_{0:T}$) $\\sim \\mathcal{D}_0$\nSample $k \\sim \\mathcal{U}\\{1, ..., K\\}$, $\\epsilon \\sim \\mathcal{N}(0, I)$\nCorrupt states: $y_k = \\sqrt{\\bar{\\alpha}_k} y_0 + \\sqrt{1-\\bar{\\alpha}_k}\\epsilon$\nPredict clean states: $\\hat{y}^0 = G_{\\theta}(y_k, k, y_0, y_f, \\epsilon)$\nPredict controls: $\\hat{u}_t = f_{\\phi}(\\hat{y}_t, \\hat{y}_{t+1})$\nCompute losses: $\\mathcal{L}_{\\text{diff}} = ||y_0 - \\hat{y}^0||^2$\n$\\mathcal{L}_{\\text{inv}} = ||u_t - \\hat{u}_t ||^2$\nUpdate $\\theta, \\phi$ with $\\nabla (\\mathcal{L}_{\\text{diff}} + \\mathcal{L}_{\\text{inv}})$"}, {"title": "B. Detailed System and Dataset Description", "content": "B.1. Burgers Dynamics\nThe Burgers' equation is a governing law occurring in various physical systems. We consider the 1D Burgers' equation with the Dirichlet boundary condition and external control input $u(t, x)$:\n$$\\frac{\\partial y}{\\partial t} = -y \\frac{\\partial y}{\\partial x} + \\nu \\frac{\\partial^2 y}{\\partial x^2} + u(t, x) \\quad \\text{in } [0, T] \\times \\Omega$$\n$$y(t, x) = 0 \\quad \\text{on } [0, T] \\times \\partial \\Omega$$\n$$y(0, x) = y_0(x) \\quad \\text{in } \\{t = 0\\} \\times \\Omega$$\nHere $\\nu$ is the viscosity parameter, and $y_0(x)$ is the initial condition. Subject to these equations, given a target state $y_d(x)$, the objective of control is to minimize the control error $J_{\\text{actual}}$ between $y_T$ and $y_d$, while constraining the energy cost $J_{\\text{energy}}$ of the control sequence $u(t, x)$.\nWe follow instructions in (Wei et al., 2024) to generate a 1D Burgers' equation dataset. Specifically, for numerical simulation,"}, {"title": "B.2. Kuramoto Dynamics", "content": "The Kuramoto model is a paradigmatic system for studying synchronization phenomena. We considered a ring network of N = 8 Kuramoto oscillators. The dynamics of the phases (states) of oscillators are expressed by:\n$$\\dot{\\theta}_{i,t} = \\omega + \\gamma(\\sin(\\theta_{i-1,t-1} - \\theta_{i,t-1}) + \\sin(\\theta_{i+1,t-1} - \\theta_{i,t-1})) + u_{i,t-1}, \\quad i = 1, 2, ..., N.$$\nFor the Kuramoto model, we generated 20,000 samples for training and 50 samples for testing. The initial phases were sampled from a Gaussian distribution $\\mathcal{N}(0, I)$, and the random intervention control signals were sampled from $\\mathcal{N}(0, 2I)$. The system was simulated for T = 16 time steps with $\\omega = 0$, following Baggio et al. (2021). The resulting phase observations and control signals were used as the training and test datasets."}, {"title": "B.3. Inverted Pendulum Dynamics", "content": "The inverted pendulum is a classic nonlinear control system. The dynamics can be represented by:\n$$\\frac{d^2 \\theta}{dt^2} = \\frac{g}{L} \\sin(\\theta) - \\frac{\\mu}{L} \\frac{d \\theta}{dt} + \\frac{1}{mL^2} u$$\nwhere $\\theta$ is the angle from the upward position, and $u$ is the control input torque. The system parameters are set as: gravity $g = 9.81 \\text{ m/s}^2$, pendulum length $L = 1.0 \\text{ m}$, mass $m = 1.0 \\text{ kg}$, and friction coefficient $\\mu = 0.1$.\nTo generate the training dataset, we simulate 90,000 trajectories for training and 50 for testing with 128 time steps each, using a time step of 0.01s. For each trajectory, we randomly sample initial states near the unstable equilibrium point with $\\theta_0 \\sim \\mathcal{U}(-1, 1)$ and $\\dot{\\theta}_0 \\sim \\mathcal{U}(-1, 1)$, and generate control inputs from $u \\sim \\mathcal{U}(-0.5, 0.5)$. The resulting dataset contains the state trajectories and their corresponding control sequences."}, {"title": "C. Implementation Details", "content": "C.1. Implementation of SEDC\nIn this section, we describe various architectural and hyperparameter details:\n*   The temporal U-Net (1D-Unet) in the denoising network consists of a U-Net structure with 4 repeated residual blocks. Each block comprises two temporal convolutions, followed by group normalization, and a final Mish nonlinearity. The channel dimensions of the downsample layers are 1, 2, 4 * statedimension. Timestep embedding is produced by a Sinusoidal Positional Encoder, following a 2-layer MLP, and the dimension of this embedding is 32. The dimension of condition embedding is the same as the system state dimension.\n*   We represent the inverse dynamics $f_{\\phi}$ with an autoregressive model with 64 hidden units and ReLU activations. The model autoregressively generates control outputs along the control dimensions.\n*   We train $x_{\\theta}$ and $f_{\\phi}$ using the Adam optimizer with learning rates from \\{1e-3, 5e-3, 1e-4\\}. The exact choice varies by task. Moreover, we also use a learning rate scheduler with step factor=0.1. Training batch size is 32.\n*   We use $K = 128$ diffusion steps.\n*   We use a guidance scale $\\lambda \\in \\{0.01, 0.001, 0.1\\}$ but the exact choice varies by task."}, {"title": "C.2. Training and Inference Time Analysis", "content": "The diffusion-based methods are trained on single NVIDIA GeForce RTX 4090 GPU. We evaluate the training and inference time of all the diffusion-based methods evaluated in the experiment session. As shown in Table 4, we compare the training efficiency of different models across various datasets. DiffPhyCon consistently shows longer training times compared to other methods, because it requires training two models that learn the joint distribution and the prior distribution respectively, increasing its training time consumption. The training times of DecisionDiffuser, RDM, and AdaptDiffuser are generally comparable, while SEDC demonstrates relatively efficient training performance across most datasets. This may be because of the proposed designs that not only improve sample efficiency but also improve learning efficiency.\nThe inference time comparison in Table 5 reveals that DiffPhyCon requires longer execution time compared to other models, because it needs to sample from two learned distributions in the denoising process. RDM achieves relatively slower inference speeds than DecisionDiffuser, AdaptDiffuser, and SEDC, because RDM replans during inference, increasing planning time. Notably, all models exhibit shorter training and inference times on the IP dataset, suggesting the influence of system complexity on computational efficiency."}, {"title": "D. Baselines Description", "content": "D.1. PID\nPID (Proportional-Integral-Derivative) control is a classical feedback control methodology that has been widely adopted in industrial applications. The control signal is generated by computing the weighted sum of proportional, integral, and derivative terms of the error. The control law can be expressed as:\n$$u(t) = K_p e(t) + K_i \\int_0^t e(\\tau) d\\tau + K_d \\frac{d}{dt} e(t)$$\nWhile PID controllers exhibit robust performance and require minimal system modeling, their effectiveness may be compromised when dealing with highly nonlinear or time-varying systems, necessitating frequent parameter tuning.\nD.2. BC, BPPO\nBehavior Cloning (BC) represents a supervised imitation learning paradigm that aims to learn a direct mapping from states to actions by minimizing the deviation between predicted actions and expert demonstrations. Despite its implementation simplicity and sample efficiency, BC suffers from distributional shift, where performance degradation occurs when encountering states outside the training distribution. The objective function can be formulated as:"}, {"title": "D.3. Diffusion-based methods", "content": "*   DecisionDiffuser:\nA novel approach that reformulates sequential decision-making as a conditional generative modeling problem rather than a reinforcement learning task. The core methodology involves modeling policies as return-conditional diffusion models, enabling direct learning from offline data without dynamic programming. The model can be conditioned on various factors including constraints and skills during training.\n*   DiffPhyCon:\nA diffusion-based method for controlling physical systems that operates by jointly optimizing a learned generative energy function and predefined control objectives across entire trajectories. The approach incorporates a prior reweighting mechanism to enable exploration beyond the training distribution, allowing the discovery of diverse control sequences while respecting system dynamics.\n*   AdaptDiffuser:\nAn evolutionary planning framework that enhances diffusion models through self-evolution. The method generates synthetic expert data using reward gradient guidance for goal-conditioned tasks, and employs a discriminator-based selection mechanism to identify high-quality data for model fine-tuning. This approach enables adaptation to both seen and unseen tasks through continuous model improvement.\n*   RDM:\nA replanning framework for diffusion-based planning systems that determines replanning timing based on the diffusion model's likelihood estimates of existing plans. The method introduces a mechanism to replan existing trajectories while maintaining consistency with original goal states, enabling efficient bootstrapping from previously generated plans while adapting to dynamic environments."}, {"title": "E. Numeric Results of Figure 2", "content": "We leverage 2-D plots in the main paper to better illustrate the performance comparison of all the methods. Here we provide the provides the corresponding numerical results in detail in Table 6."}, {"title": "F. Results and Discussion of the ablation study on GSF", "content": "To validate GSF's effectiveness in guiding the model toward learning the optimal (energy- efficient) target distribution, we conduct ablation studies on fine-tuning rounds and test control signal energy on the test set. The result is provided in Table 7. Before finetuning, energy performance is the poorest because of the non-optimality of the initial training samples, and the first round of GSF greatly supplemented the training toward optimality, as the energy of the produced control inputs decreases at an average of 38.4%. The increases observed in the second round persist but are significantly lower than those in the first round. This may be because the first round of GSF has already captured the most significant deviations toward"}]}