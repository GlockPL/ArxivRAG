{"title": "Embodied Uncertainty-Aware Object Segmentation", "authors": ["Xiaolin Fang", "Leslie Pack Kaelbling", "Tom\u00e1s Lozano-P\u00e9rez"], "abstract": "We introduce uncertainty-aware object instance segmentation (UNCOS) and demonstrate its usefulness for embodied interactive segmentation. To deal with uncertainty in robot perception, we propose a method for generating a hypothesis distribution of object segmentation. We obtain a set of region-factored segmentation hypotheses together with confidence estimates by making multiple queries of large pre-trained models. This process can produce segmentation results that achieve state-of-the-art performance on unseen object segmentation problems. The output can also serve as input to a belief-driven process for selecting robot actions to perturb the scene to reduce ambiguity. We demonstrate the effectiveness of this method in real-robot experiments.", "sections": [{"title": "I. INTRODUCTION", "content": "Our goal is to build long-horizon manipulation systems that can operate in environments that contain previously unknown objects. A key step in such systems is segmenting images, either RGB or RGB-D, into candidate objects to be manipulated. This step is often called \"unknown object instance segmentation\u201d (UOIS) and a number of existing deep-learning models have been developed for this task [1], [2], [3]. However, the output from these models is inevitably imperfect, due to limitations in the model, for example, limited data or limited capacity, or to challenges in the images, for example, occlusion or lighting, or to fundamental ambiguity, for example, in a stack of toy blocks. In the \"embodied\" manipulation setting, where we have a robot available, we can interact with the scene so as to obtain additional information, such as pushing some of the objects and tracking how they move. Furthermore, with the advent of \"promptable\" segmentation models [4], we can also interact with the model to obtain additional information, such as obtaining multiple segmentations from different prompts. In this paper, we explore both of these methodologies for improving segmentation results: multiple prompting of the segmentation model and active robot interaction with the objects. In particular, we construct (by multiple prompting) a characterization of the uncertainty in the segmentation and use that representation to guide the physical interaction.\nImage segmentation, in its most general form, is fundamentally underconstrained. Is the bottle cap part of the bottle or a separate object? Is the shirt part of the person? In this paper, we limit ourselves to considering the segmentation of discrete rigid objects, where the answer to these questions is: if chunks of matter always move together rigidly, then they form a single object, and not otherwise. It will typically be impossible to find this ground-truth segmentation from an image of a cluttered scene and, in general, it may not be necessary to find it so as to achieve a particular robotic manipulation goal.\nWe define our task as that of uncertainty-aware object instance segmentation. Given an image, a solution to the problem partitions a scene into disjoint regions and provides a single interpretation for each region of the scene that has sufficiently low uncertainty and provides multiple interpretations for each region with high uncertainty. This is different from the classic instance segmentation task, where the objective is to deliver a single set of masks for the scene. By explicitly characterizing this region-factored uncertainty, we hope to enable improved performance on downstream tasks, for example, improving the choice of actions to gather additional information to disambiguate the segmentation.\nA crucial question in this approach is how to characterize the uncertainty in a proposed segmentation. We develop an uncertainty estimation and hypotheses generation method based on multiple queries to large pre-trained, \u201cpromptable\u201d models [4], [5]. Within a region of the image, we issue random point prompts and use consistency of the returned masks as an indication of uncertainty.\nHaving obtained object hypotheses, with multiple candidate segmentation of uncertain regions, we use the robot to do targeted exploration aimed at reducing the uncertainty. We use a maximal-uncertainty-reduction-driven action selection heuristic to lightly push a candidate object. We build a state estimator to track and update the object hypotheses. The most likely segmentation hypothesis can be computed from the resulting belief state and we show that the state estimation leads to better action choices which ultimately leads to better maximum-likelihood segmentation hypotheses."}, {"title": "II. RELATED WORK", "content": "This paper is related to previous work on unseen object instance segmentation (UOIS), the use of large pre-trained models in image segmentation, estimating uncertainty in segmentation, and on embodied image segmentation.\nUnseen object instance segmentation (UOIS) UOIS for robotics aims to find an instance segmentation of objects in the foreground, typically for a tabletop scene. Recent work leverages datasets generated in simulation using a large set of objects [1], [2], [3], [6]. A difference from common panoptic, semantic, and instance segmentation scenarios is that a depth image is assumed to be available. These methods make predictions based on both intensity cues and geometric cues. Although our goal is ultimately to obtain an object segmentation, the crucial difference is that our method estimates a factored distribution over segmentations, which is then improved by interacting with the scene, before committing to a particular segmentation hypothesis.\nSegment Anything Model (SAM) Recent large vision models have shown impressive results for various tasks. SAM [4] is an image segmentation model that has been pre-trained on a large dataset of 11 million images. It can produce segmentation masks through point queries or box queries. Due to the flexible prompt interface and strong performance, it has been used to improve different tasks such as 3D scene segmentation [7], [8] and tracking [9], [10]. It has also been combined with other large pre-trained models such as GroundingDINO [11] to segment objects with text-prompts [12]. In our work we exploit the prompting interface for uncertainty estimation.\nUncertainty Estimation in Segmentation Many approaches to uncertainty estimation in segmentation have produced a heat map of uncertainty over pixels [3], [13], [14]. However, the uncertainty we care about is object level uncertainty, rather than pixel-wise uncertainty. Some previous approaches have produced probability distributions over relatively small image patches [15], [16]. The common failure modes in modern UOIS are over- and under-segmentation of objects, therefore representing uncertainty via distributions over grouping of individual segmentation masks is more appropriate for our setting.\nEmbodied Segmentation Using robot actions to complement and enhance visual perception has a long history in robotics and is variously known as active perception, interactive perception or embodied perception; the survey by Bohg et al. [17] reviews this body of work, which includes work on interactive/embodied segmentation.\nA common strategy for interactive segmentation has been to take a bottom-up approach, starting from an over-segmentation of the scene, and identifying groupings by consistency in motion [15], [18], [19]. An action is chosen greedily to induce motion. There have been a number of strategies for choosing actions. In some cases, the explicit goal is to \"singulate\" (isolate) the objects [20]. Pajarinen et al. [16], on the other hand, formulate the action selection problem as a POMDP and try to pick actions that maximize long-term reward. Very recent work from Qian et al. [21] also seeks to improve segmentation based on a small number of robot interactions. The action is selected heuristically based on the pixel-wise uncertainty map from MSMFormer [3]. It differs from our focus on exploiting a representation of uncertainty obtained from prompting large pre-trained models. Another line of work aims to use robot interaction to gather data for self-supervised training of segmentation models [22], [23], [24]. This objective is in contrast with our objective of disambiguating only the current scene."}, {"title": "III. PROBLEM SETTING", "content": "Our ultimate objective is to obtain an accurate interpretation of potentially highly cluttered table-top scenes, in the form of a set of partial point clouds corresponding to individual objects in the scene. We assume that all objects in the scene are rigid and do not address the problem of revealing completely occluded objects.\nScene segmentation is a fundamentally ambiguous problem: it may be both difficult and unnecessary to obtain a single, exactly correct interpretation. For these reasons, we focus on constructing a distribution over segmentation hypotheses, and updating that distribution over time given new observations in which some objects have moved.\nThe robot embodiment consists of a camera that can observe the entire scene and capture registered RGB and depth images, and a robot arm that can reach the observed objects and make small perturbations by \"poking\" the objects. Our goal is to produce good interpretations of the scene with a minimal amount of disturbance to the objects.\nThe robot is assumed to be able to make precise, local contact with objects in the scene. The pushing action is determined by selecting an initial end-effector position, orientation, and motion distance. After executing each motion, the robot retracts to a position that leaves the scene unoccluded.\nAlthough our objective is to maintain a distributional estimate of the segmentation state, in order to compare most directly with existing segmentation methods, we will evaluate our segmentation results in terms of instance segmentation metrics on 2D image masks [6]. Given a hypothesized segmentation {$s_1,...,s_{N_s}$}, where $s_i$ is a set of pixels assigned to object i, and a ground-truth segmentation {$g_1,\u2026\u2026,g_{N_g}$}, we find an assignment $\\phi$ mapping each hypothesized segment into a ground-truth segment (or none) that maximizes the sum of the individual F-scores, and report an overall object-size normalized (OSN) precision, recall, and F-score,\n$P_n = \\frac{\\sum P_i}{N_s}$, $R_n = \\frac{\\sum R_i}{N_g}$, $F_n = \\frac{\\sum F_i}{max(N_s, N_g)}$\nwhere $F_i = 2P_iR_i/(P_i + R_i)$, $P_i = |s_i \\cap \\phi(s_i)|/|s_i|$, $R_i = |s_i \\cap \\phi(s_i)|/|\\phi(s_i)|$. The object-size normalized metric differs from the standard P/R/F measures [1] in that they explicitly average the scores over segments rather than pixels. This ensures that simply getting a few large objects correct does not overwhelm the scores of badly segmented smaller objects, which is important for manipulation problems. We additionally wish to achieve a good segmentation result with as little disturbance to the scene as possible. We do not explicitly measure the amount of motion among the objects, but do measure the improvement in segmentation quality as a function of the number of actions performed."}, {"title": "IV. EMBODIED UNCERTAINTY-AWARE SEGMENTATION", "content": "We propose embodied uncertainty-aware object segmentation (EOS), as illustrated in Fig. 1. EOS consists of three main components, including an uncertainty-aware object segmentation model (UNCOS), a belief state estimator, and an action planner, operating in closed-loop interaction with the scene. The initial RGB-D image is processed using UNCOS, which builds on a promptable image-segmentation model to construct a segmentation hypothesis set. This segmentation hypothesis set is used to initialize a belief state, representing a set of hypotheses about the structure of the 3D scene. Given a belief state, an action is selected and executed, and a new RGB-D observation is captured and used to update the belief. Finally, we generate a set of image masks corresponding to the most likely hypothesis."}, {"title": "A. Uncertainty-aware Object Segmentation Model", "content": "Our method, uncertainty-aware object segmentation model (UNCOS), provides a novel strategy for combining multiple queries to pre-trained 2D RGB image-segmentation methods with some operations on the 3D point-cloud generated from a depth image, to produce a set of possible segmentation hypotheses, together with confidence estimates.\nUNCOS approaches solving the problem from two aspects:\n\u2022\n\u2022\nA \"bottom-up\" method, that when queried, can return masks that cover a region of interest in an image. This ensures that every region in the image can be accounted for. It is essential that this method have high recall so that multiple queries to this method is likely to return most of the correct instance masks. We refer to this method as BUHIGHRECSEG. We assume it can take densely issued query points, to form an initial set of high recall masks of the whole image. We refer to this as BUSEED.\nA \"top-down\" method that returns a set of image masks with high precision. These masks are very likely to correspond to correct segments, but they may not contain all the correct segments. We refer to this method as TDHIGHPRECSEG.\nThe general strategy could use any method meeting these requirements. In our implementation, we use the Segment Anything Model (SAM) [4]. Given an image, it can be queried either with a pixel location or a bounding box.\nWe use the pixel-prompted segmentation as our BUHIGH-RECSEG module and its densely issued version (automatic mask generation) as our BUSEED module. Our experiments confirm that these two do indeed have very high recall.\nWe use GROUNDEDSAM [11], [12], which utilizes box-prompted segmentation with a natural language prompt, as our TDHIGHPRECSEG module. GROUNDEDSAM takes text as input, uses GROUNDINGDINO [11] to generate detection bounding boxes for the text, and then prompts SAM to generate a binary mask for each detection box. We query GROUNDEDSAM with a fixed prompt \"A rigid object.\". Our experiments confirm that this method does indeed have very high precision."}, {"title": "B. 3D Belief representation", "content": "Our embodied segmentation process starts with a belief state initialized with the results of UNCOS. This belief could be integrated into a general goal-directed manipulation planning process, which decides whether or not to invoke information-gathering actions, depending on its given task. The planner can select actions based on the residual uncertainty, picking actions that leads to plan success under any hypothesis (e.g., deciding to push something that might be a stack of objects from the bottom, rather than picking it up from the top, for the task of cleaning up the table).\nFor the purposes of testing the uncertain segmentation and belief-update process, we embed it in a loop in which the robot takes actions with the goal of reducing uncertainty in the segmentation. It selects an action based on the hypotheses in the initial belief, executes the action on the robot, and obtains a new RGB-D image of the scene after the interaction. We update the belief to both track the motion of the hypothesized objects and to get a new confidence score for each hypothesis. The process repeats for several steps. At any point in this process, we can retrieve the hypothesis with the highest confidence for evaluation against other strategies.\nOur 3D belief representation $B = (C^+,U^+)$ retains the factored structure of the 2D segmentation output, but is lifted to 3D and aggregated over time. The set $C^+$ now consists of a set of 3D objects $c_1,..., c_h$, represented as point clouds in a global frame. Each region $u^{(r)} \\in U^+$ consists of a set of region hypotheses: $u^{(r)} = (h^{(r)}_1,..., h^{(r)}_{n_r})$, each of which is an interpretation of the region. For simplicity of notation, we will drop the (r) from now on. It should be ranging from one to $|U^+|$. Each region hypothesis $h$ consists of a set of 3D objects ($o_{j1},..., o_{jn_j}$). Each object $o_{jk}$ consists of a point cloud $n_{jk}$ and a confidence score $s_{jk}$ indicating the likelihood that $o_{jk}$ is either a single object or part of a large whole, that is not under-segmented.\nAs we get additional observations, we will adjust the confidence values $s_{jk}$. We define a score for each region hypothesis h as\n$S(h) = \\frac{1}{|h|} \\sum_k s_{jk} - \\lambda |h| - min_m |h_m|$\nwhich combines the average \u201cwholeness\u201d confidence of the objects in the hypothesis with a penalty for having extra objects, thus preferring the simplest hypothesis that holds the rigidity assumption.\nSince the structure of the 3D belief is the same as the 2D output of UNCOS, we construct the initial belief by simply using the 2D masks to extract segments from the original point cloud P. We initialize all $s_{jk}$ to some fixed initial value $p_0$. Since the hypotheses in each region are independent of those in other ones, we take the most likely hypothesis for the whole scene to be the union of $C^+$ and the most likely hypothesis from each uncertain region."}, {"title": "C. Action selection", "content": "To demonstrate the utility of the belief representation, we use a robot to selectively poke objects in the scene using a simple greedy strategy that attempts to select a small perturbation that will maximize information gain. We take advantage of the factored uncertainty representation to select a region of the scene that has the highest degree of uncertainty and then select the action that, when applied to that region, induces an observation distribution that is maximally discriminating among its hypotheses.\nWe measure the uncertainty of a region in terms of the number of high-scoring hypotheses it has:\n$\\kappa(u^*) = |\\{h | S(h > \\delta)\\}|$\nAfter selecting the targeted region, we need to select an informative action. For example, if the two hypotheses for a region are about whether two horizontally aligned parts are rigidly attached, then pushing along the line connecting the part centers won't be as helpful as pushing perpendicular to that. We use the physical simulation result of motions with reconstructed world hypotheses as a heuristic for the potential information gain.\nTo evaluate the informativeness of an action, we construct simulated world models corresponding to all high-likelihood complete hypotheses. The worlds are constructed by taking the Cartesian product of the likely hypothesis sets for each region: $W = C^+ \\times (u^{(r)})_{U^{(r)}}$.\nEach world $w \\in W$ consists of a set of objects defined by partial point clouds. In order to carry out a simulation, we need to generate completions of these objects, represented as meshes. We follow the same object reconstruction pipeline as Curtis et al. [25]: we complete the partial point cloud using a shape completion network and vertical projection, filter out any inconsistency with the current depth image, and reconstruct a concave mesh.\nNext, we sample k actions, a, as follows: within the selected target region, we randomly sample an object among all hypotheses for the target region. We then randomly sample a pushing direction across the centroid of that hypothesized object. Next, we simulate the effect of each action in each world, obtaining new depth images $D_w^a$. We select the action that induces most differences between the hypotheses:\n$a^* = argmax_a \\sum_w |D_w^a - D^*, a|$\nwhere $D^*,a$ is the averaged depth for all w under a. Given $a^*$, we do motion planning and execute in the real world."}, {"title": "D. Belief update", "content": "After executing an action, we update the belief based on the robot's observation. We cannot take advantage of dense observations during the action execution because the object is typically occluded by the robot arm. Instead, we capture a new RGB-D image after the execution has terminated.\nTo track each hypothesis mask, we use XMem [26] as a multi-object tracker for two neighboring frames. Specifically, at each time step t, for each hypothesized object $o_{jk}$, we initialize XMem with $I_{t-1}$ and the 2D mask of $o_{jk}$ at time t-1. We query XMem with the new image $I_t$ and get the updated mask. Compared to optical-flow-based methods such as RAFT [27], XMem is more robust to occlusion and can handle larger movements.\nWith the tracked mask, we update the object point cloud and confidence based on our rigidity assumption. We register the point cloud $n_{jk}^{t-1}$ to that of tracked masked area $n_{jk}^{t}$ using RANSAC. This gives us a rigid transformation $T_k$. We use the percentage of inlier points in registered point cloud as a measure of how well the point cloud motion follows the rigid assumption. This is our current time step score $s_{jk}^{et}$. We assume that the point clouds are sufficiently well registered so that we can just take their union as an update: $n_{jk} \\leftarrow (T_k \\downarrow n_{jk}^{t-1}) \\cup n_{jk}^{t}$. The final confidence score $s_{jk}$ is the weighted average of $\\{s_{jk}^t\\}_{t=1,...,T}$ where the weights are determined by the displacement from $T^{t-1}$ at each step."}, {"title": "V. EVALUATION", "content": "We are interested in answering two main questions:\n\u2022\n\u2022\nDoes performing uncertainty-aware object segmentation model on a single input RGB-D image and generating its most likely hypothesis as output result in image segmentation results that are comparable to other SOTA methods?\nDoes the belief state initialized via uncertainty-aware object segmentation model and then updated via embodied uncertainty-aware object segmentation provide a good basis for selecting actions for interacting with the world?\nWe address these two questions in the following sections."}, {"title": "A. Segmentation from single images", "content": "We compare UNCOS with several methods. The first two are state-of-the-art UOIS methods that predict a single set of object segmentation masks directly from an RGB-D image: (1) UOIS-Net-3D [1] (2) UCN [2]. The next group of methods use SAM in some way, but do not carry out the repeated queries as in UNCOS.\n3) SAM: returns output of the automatic mask generation query to SAM without further processing.\n4) SAM-cluster: based on the observation that SAM tends to over-segment objects, we construct the connectivity graph as described in Alg. 2, and treat every connected cluster as a segmented object.\n5) SAM-per-pixel-ML: assigns the highest SAM-conf. mask to the pixel if multiple masks contain it [7]. SAM-conf. refers to the predicted confidence from the scoring head of SAM that it outputs with every predicted mask.\n6) GROUNDEDSAM: GROUNDEDSAM queried with a fixed prompt \"a rigid object\".\nWe consider our method, UNCOS, and several ablations:\n7) UNCOS - BootstrapScore: returns the hypothesis from UNCOS that has the highest average SAM-conf. value, instead of the bootstrap confidence score.\n8) UNCOS - TDHIGHPRECSEG: uses UNCOS without the TDHIGHPRECSEG masks from GROUNDEDSAM.\n9) UNCOS - TDHIGHPRECSEG - D: an ablation that further removes the depth filter for degenerate regions.\n10) UNCOS + UCN: add masks from UCN [2] as additional TDHIGHPRECSEG masks."}, {"title": "B. Improving segmentation through interaction", "content": "Once UNCOS has produced a distribution over possible segmentations, we use it to select physical interactions with the scene in order to reduce any remaining uncertainty. We evaluate our embodied uncertainty-aware object segmentation (EOS) system in the real world with a Franka Emika robot arm. To push the object precisely, the Franka grips a stick, as shown in Fig. 1. We use a bidirectional RRT for motion planning and check collisions between the arm and objects using the observed point cloud. The RGB and depth images are captured by a RealSense D435i camera mounted on the gripper. The two questions we want to answer through real-world experiments are: 1) Does UNCOS improve the efficiency of embodied segmentation; 2) Does building local memory and doing belief updates help with image segmentation.\nOur primary method, EOS, uses the action-selection method from Sec. IV-C based on a belief initialized from the UNCOS results, and updates using the methods from Sec. IV-D. For evaluation, at each time step, we compare the highest scoring hypothesis from the 3D belief state to human-labeled ground-truth masks. We compare EOS with two ablations:\n\u2022\n\u2022\nRANDOM: We retain the belief state initialization and update methods from EOS, but instead of selecting actions to disambiguate the most uncertain region, we randomly select a hypothesized object to interact with and randomly select a pushing direction. Differences in performance between this method and EOS can be attributed to the use of the uncertainty in the belief representation to focus action selection.\nFINALFRAME: We use random actions, as above, but rather than maintaining a belief state and updating it after each action, we simply take the single image of the object configuration after per interaction step, apply UNCOS to it, and return the most likely hypothesis in UNCOS result. Differences in performance between this method and RANDOM can be attributed to the aggregation of observation information over time in the belief-update mechanism. If this method reveals improved segmentation quality from the first to last frames, it can be attributed to the random motions causing physical separation between the objects, thus makes the segmentation problem easier.\nWe set up 20 scenes with a collection of 74 diverse objects, shown in Fig. 2. We ran both the EOS and RANDOM methods on each scene (the FINALFRAME method uses the same images as RANDOM, but a different method for generating a predicted segmentation). Although the replication of the scenes for the two runs was not perfect, we set them up as similarly as possible (comparing initial images as we did so). The robot carries out 3 actions in each scene."}, {"title": "VI. DISCUSSION", "content": "Limitations and Future Work. First, our method does not utilize multi-view images to reduce the uncertainty. We are looking to incorporate active perception strategies to reduce the uncertainty caused by occlusion. Second, the current setup seeks to reduce ambiguity in the whole scene. We plan to explore a task-specific information-gathering strategy where only task-relevant regions are explored."}, {"title": "Conclusion", "content": "We formulate an uncertainty-aware object instance segmentation problem as the basis for embodied segmentation. Our method UNCOS produces a distribution over possible segmentation hypotheses. The most likely hypothesis from UNCOS has achieved state-of-the-art performance on the UOIS task. Through real-world experiments, we have demonstrated that UNCOS can guide the embodied interaction for efficient targeted disambiguation."}]}