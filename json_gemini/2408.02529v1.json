{"title": "Counterfactual Shapley Values for Explaining Reinforcement Learning", "authors": ["Yiwei Shi", "Qi Zhang", "Kevin McAreavey", "Weiru Liu"], "abstract": "This paper introduces a novel approach Counterfactual Shapley Values (CSV), which enhances explainability in reinforcement learning (RL) by integrating counterfactual analysis with Shapley Values. The approach aims to quantify and compare the contributions of different state dimensions to various action choices. To more accurately analyze these impacts, we introduce new characteristic value functions, the \"Counterfactual Difference Characteristic Value\" and the \"Average Counterfactual Difference Characteristic Value.\" These functions help calculate the Shapley values to evaluate the differences in contributions between optimal and non-optimal actions. Experiments across several RL domains, such as GridWorld, FrozenLake, and Taxi, demonstrate the effectiveness of the CSV method. The results show that this method not only improves transparency in complex RL systems but also quantifies the differences across various decisions.", "sections": [{"title": "Introduction", "content": "Reinforcement learning (RL) has applications in autonomous navigation (Chen, Li, and Tomizuka 2021), healthcare (Liu, Ngiam, and Feng 2019), financial strategy optimization (Jiang, Xu, and Liang 2017), and smart city management (Kumar et al. 2021). However, the key to achieving widespread adoption of these applications lies in overcoming the challenge of explainability in RL models. the inability to adequately explain RL models could provoke ethical and regulatory issues and meet with resistance from potential users, collectively impeding the extensive application and further advancement of RL technology.\nResearch in explainable reinforcement learning is primarily focused on two approaches: intrinsic interpretability and post-hoc interpretability. Intrinsic interpretability strategies enhance model transparency by simplifying the model's structure, often at the expense of reducing the model's performance. For instance, decision tree models, which represent intrinsic interpretability, visually demonstrate decision pathways through their tree-like structure but may fail to capture more complex data patterns. In contrast, post-hoc interpretability methods treat the model as a black box and reveal the decision-making logic by analyzing the relationships between inputs and outputs, thus preserving the model's complexity. Counterfactuals and Shapley Values are both post-hoc explanation methods that provide insights without intervening in the internal structure of the model.\nShapley Value is a method for measuring the contribution of individuals to the total payoff in cooperative games. An application of Shapley values to Explainable Artificial Intelligence (XAI), each feature is regarded as a \"player\", while the model's predictive outcome is considered the \"total payoff\" (Lundberg and Lee 2017). The Shapley Value assesses the average contribution of each feature to the predictive outcome by considering all possible combinations of features.\nInvestigations employing Shapley Values are remarkably limited across both multi-agent reinforcement learning (MARL) and single-agent in RL. However, MARL research demonstrates a relatively richer engagement with Shapley value applications, which are principally concentrated across three aspects. Firstly, Value Decomposition, (Wang et al. 2022) integrates Shapley Value theory with multi-agent Q-learning (SHAQ) to address value decomposition in global reward games, where Shapley Values play a critical role in ensuring equitable distribution of rewards based on individual agent contributions. Secondly, Credit Assignment, (Wang et al. 2020) introduces the Shapley Q-value within MARL (SQDDPG) to reallocate global rewards among agents, leveraging Shapley Values to quantify and fairly distribute the rewards reflecting each agent's marginal contribution to the collective success. Lastly, Model Explanation, (Heuillet, Couthouis, and D\u00edaz-Rodr\u00edguez 2022) the use of Shapley Values to explain cooperative strategies and individual contributions in multi-agent reinforcement learning, approximating Shapley Values via Monte Carlo sampling to reduce computational costs. Shapley Values offer a way to interpret complex multi-agent interactions, making it clearer how each agent's decisions and actions contribute to the overall dynamics and results of the system.\nTo our knowledge, only the SVERL in (Beechey, Smith, and \u015eim\u015fek 2023), within the scope of RL research excluding MARL, utilizes Shapley Values to explain the decision-making process in reinforcement learning. Although SVERL provides a method for understanding and explaining the contribution of state features in the decision-making process of reinforcement learning agents, it has limitations in explaining specific action choices. The core of SVERL lies in analyzing the contribution of state features to the expected returns of an agent, rather than directly investigating why action a is chosen over action b in the same state, and it does not offer a mechanism to quantify the difference between these action choices. This means that while SVERL can help us indirectly understand how certain features influence an agent's decisions, it cannot directly answer why an agent prefers a specific action, nor can it compare the merits of different actions. This limitation is not unique to SVERL but is a challenge faced by all methods that rely on Shapley Values to explain the decision-making process in reinforcement learning.\nCounterfactuals can effectively address some of the limitations associated with explaining specific action choices in RL. By comparing the utility (Long-Term Expected Return) of the actual action with those of the counterfactual (alternative) action, counterfactuals can help clarify why an agent prefers one action over another and quantify the impact of different actions.\nIn reinforcement learning, two main generative explanation methods by counterfactuals   one based on deep generative models and the other on generating counterfactual states  share a common limitation: the difficulty in quantifying differences between counterfactual instances. While methods based on deep generative models (Olson et al. 2021) can create realistic \u201cwhat if\u201d scenarios, comparing these instances in a high-dimensional latent space is challenging, as conventional distance metrics may not apply. On the other hand, methods that focus on minor state adjustments to guide different decisions also face challenges in quantifying the actual impact of these subtle changes on agent behavior, since even small modifications can have widespread effects in complex RL environments. Therefore, although these methods provide valuable insights into the decision-making process in RL, their difficulty in quantifying differences between counterfactual instances limits their application scope and the depth of their explanations.\nTo address the challenges of quantifying differences in reinforcement learning explanations, we develop a novel approach called \"Counterfactual Shapley Value\". While previous research has explored the integration of counterfactual explanations with Shapley Values, such as addressing credit assignment problems in MARL in (Li et al. 2021) or providing explanations in supervised learning (Albini et al. 2022), the former develops a novel method to assign credits in multi-agent systems using counterfactual thinking and Shapley values, which quantifies each agent's contribution by considering what would happen if certain agents were absent, allowing for a fair and accurate assessment of each agent's impact on the collective outcome, the latter uses counterfactual scenarios to enhance Shapley value explanations, providing insights on how changes to input features could influence model predictions. It makes explanations more actionable by showing which features to adjust to achieve desired outcomes. Our work is fundamentally different. For the first time, we have applied the combination of counterfactual reasoning and Shapley Values to the domain of reinforcement learning, introducing the innovative CSV approach. This method not only retains the benefits of traditional Shapley Values in quantitative analysis but also enhances the understanding of decision-making by examining specific hypothetical scenarios, such as \u201cWhat would happen if a specific action were not taken?\". Our approach brings a more transparent and interpretable perspective to decision-making in reinforcement learning, significantly alleviating the limitations of previous explanatory methods.\nIn this paper, our research contributions are threefold: Firstly, we have introduced an innovative method called Counterfactual Shapley Value, which can precisely reveal the changes and differences in the contributions of state features to different actions within reinforcement learning. Secondly, we have designed two novel characteristic value functions - the Counterfactual Difference Characteristic Value and the Average Counterfactual Difference Characteristic Value, which incorporate counterfactual mechanisms for calculating Shapley Values, offering us new avenues for computation. Lastly, we have tested and analyzed this method across multiple RL environments, where the quantified results from CSV have allowed us to explain the behavior of agents, thereby demonstrating the effectiveness of our approach.\""}, {"title": "PRELIMINARIES", "content": ""}, {"title": "Markov Decision Process (MDP)", "content": "In the domain of reinforcement learning, an agent's interaction with the environment is conceptualized as a Markov Decision Process (MDP), denoted by $MDP = (S, A, P, R, \\gamma)$. Here, S is the set of all possible states, A is the set of actions, P is the state transition probabilities $P(s'|s, a): S\\times A \\times S\\rightarrow [0, 1]$, $R(s, a) : S\\times A \\rightarrow R$ represents the reward function $R(s, \\alpha)$, $\\gamma \\in [0,1]$ denotes the discount factor the cumulative reward $G_t$ represents the sum of discounted future rewards starting from time t. It is mathematically defined as $G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$, with $R_{t+k+1}$ signifying the immediate reward received at time t+k+1. The role of cumulative reward is fundamental, serving as a critical metric to assess and guide the agent's performance. The agent's objective is to discover a policy $\\pi : S \\times A \\rightarrow [0, 1]$ that assigns probabilities to actions in each state, with the aim of optimizing the cumulative expected reward over time. The state-value function $V^{\\pi}(s) = E_{\\pi} [G_t | S_t = s]$ computes the expected return from starting in state s under policy $\\pi$, while the action-value function $Q^{\\pi} (s, a) = E_{\\pi} [G_t | S_t = s, A_t = a]$ assesses the expected return after taking action a in state s under the same policy. The optimal policy, therefore, is defined by maximizing these functions, $V^* (s)$ and $Q^* (s, a)$, to achieve the highest possible expected returns from all states and actions."}, {"title": "Shapley Values in RL", "content": "In the domain of cooperative game theory, Shapley Values, originally proposed by (Shapley 1953), serve as a pivotal solution concept for the equitable allocation of collective gains or losses among participating entities, predicated on their individual contributions to the cumulative outcome. When transposed to the Reinforcement Learning (RL) framework, this concept facilitates an analytical assessment of the individual contributions of state features to the decision-making efficacy of an RL agent.\nIn RL, let F = {0,1,..., n \u2212 1} represent the set of indices corresponding to the comprehensive enumeration of state features within the state space S. The state space itself is structured into n discrete dimensions, expressed as $S = S^{(0)} \\times S^{(1)} \\times ... \\times S^{(n-1)}$, where each $S^{(i)}$ captures a distinct aspect of the environmental dynamics. Within this framework, each state is an ordered set, defined by $s = \\{s^{(i)}|s^{(i)} \\in S^{(i)}, i \\in F\\}$. Let $C \\subset F$ denote a subset of state features, specifically excluding the feature indexed by i. A partial observation of the state is then given by $s_C = \\{s^{(i)}|s^{(i)} \\in S^{(i)}, i \\in C\\}$, which offers a perspective of the state based on a subset of observable features. The Shapley value (Shapley 1953) for a state feature i, denoted as $\\phi_i$, quantifies the contribution of feature i and is mathematically expressed as follows:\n$\\Phi_{i} = \\sum_{C \\subset F \\setminus \\{i\\}} \\frac{|C|!(|F| - |C| - 1)!}{|F|!} \\delta(i, C)$ (1)\nwhere $\\delta(i, C) = v(C \\cup \\{i\\}) - v(C)$ quantifies the marginal contribution in the characteristic value function attributed to the incorporation of feature i into the subset C, where v(C) represents the characteristic value function assessed over the subset C excluding of feature i, and $v(C \\cup \\{i\\})$ represents the characteristic value function with the inclusion of feature i. This formulation enables the computation of the average marginal contribution of each state feature i across all conceivable coalitions of other features, thereby elucidating its significance in the decision-making processes of the RL agent."}, {"title": "Characteristic Value Function", "content": "In the field of machine learning, the integration of the characteristic value function (CVF) with Shapley Values provides a systematic method for quantifying the individual contributions of features towards predictive outcomes. This approach leverages principles from cooperative game theory, adapting them to clarify the significance of features within predictive models. Let $f_F : S \\rightarrow R$ denote the characteristic function defined over the complete feature set F in state space S, which estimates the expected value of model predictions as $v(F) := f_F (s)$. Correspondingly, for a subset $C \\subset F$, the characteristic function $f_C : S_C \\rightarrow R$ assesses the expected prediction value $v(C) := f_C(s_C)$, where $S_C$ denotes the subspace restricted to the features in C. When all features are unknown, that is, no features are input into the model, it can be assumed that the feature set is an empty set (\u00d8). Then, the function's value is typically set to 0, denoted as v(\u00d8) = 0, indicating that the contribution of features is zero."}, {"title": "COUNTERFACTUAL SHAPLEY VALUES", "content": "In the XRL, it is imperative to gain a comprehensive understanding of the fundamental motivations underlying the decision-making processes of agents. While Shapley Values do not clarify the policy significance of action choices directly, the incorporation of counterfactual analysis enhances the ability to assess the potential effects of various actions in hypothetical scenarios, thereby offering deep insights into the policy selections of agents.\nIt is noteworthy that the value functions Q(s, a) and V(s), while pivotal in guiding agents towards optimal decision-making, also serve as utility functions for computing Shapley Values, highlighting their significance in analyzing agent behavior. By employing these value functions as characteristic value function, a more precise quantitative analysis of the specific contributions of different dimensions in any given state can be achieved."}, {"title": "Counterfactual Differences", "content": "From the perspective of explaining agent behavior, effectively comparing two distinct actions requires not only evaluating the immediate reward following action execution and the expected long-term return but also exploring counterfactual scenarios -\"what would happen if a different action were chosen?\" To facilitate this, we introduce the notion of counterfactual difference (CD), including both action counterfactual differences (AQ) and state counterfactual differences (AV), to thoroughly analyze the impacts of various actions. The action counterfactual differences examines the difference in Q values between the actual action taken and a hypothetical alternative action in a given state, while the state counterfactual differences focuses on the variance in V values for the new states resulting from these actions.\nThe action counterfactual differences, denoted as $\\Delta Q^{\\pi} (s, a^*, a)$, is calculated by the comparison between the expected return of taking an optimal action $a^*$ under the policy $\\pi$ in state s, against the expected return of taking the other action a under the same policy $\\pi$ in the same state s. This comparison yields the differences value, revealing the potential utility deviation resulting from an alternative action choice. $\\Delta Q^{\\pi} (s, a^*, a)$ is calculated as follows:\n$\\Delta Q^{\\pi} (s, a^*, a) = Q^{\\pi} (s, a^*) \u2013 Q^{\\pi} (s, a)$ (2)\nwhere $Q^{\\pi} (s, a^*)$ represents the expected return of executing the optimal action $a^*$ in state s under the policy $\\pi$, while $Q(s, a)$ signifies the expected return of executing the counterfactual action a under the same policy $\\pi$ in the same state. Although the policy usually prioritizes executing what is assessed as the optimal action, the reward of non-optimal actions is also recorded during training to maintain a balance between exploration and exploitation. This forms the basis for a counterfactual analysis scenario, through which we can compare the expected return of non-optimal actions to optimal actions, thereby evaluating how different state features contribute to these two types of decisions.\nThe policy $\\pi$ can be either a \"fully learned policy\" or a \"partially learned policy\". In both types, the actions considered optimal are those with the highest Q values in each state. Typically, policies will choose these actions with the highest Q values for execution. A fully learned policy can always accurately execute these optimal choices. However, a partially learned policy, although theoretically aimed at maximizing rewards, may not always choose the best actions in practice. Since partially learned policies struggle to accurately calculate Q values to effectively predict long-term returns, it becomes difficult to assess how different state features contribute to decision-making. For these reasons, this study only considers policies that are fully learned and can execute accurately.\nThe state counterfactual differences, denoted as $\\Delta V^{\\pi}(s^*, s')$, quantifies the variation in expected return associated with transitioning from the current state s to an optimal state $s^*$ under the policy $\\pi$, in contrast to transitioning to a different state $s'$ under the same policy $\\pi$. This differences highlights the expected return change resulting from adopting the optimal action compared to an alternative action from the same starting point. $\\Delta V^{\\pi}(s^*, s')$ is calculated as follows:\n$\\Delta V^{\\pi} (s^*, s') = V^{\\pi} (s^*) \u2013 V^{\\pi} (s')$ (3)\nAlthough some states $s'$ might only be reached through suboptimal actions a with very low probabilities after learned policy, these states still have definitive value functions, which reflect the expected rewards that could be obtained starting from these states. Even if these states are not frequently visited under the optimal policy, we can still explore these states using counterfactual methods to analyze the choices of suboptimal actions. However, the analysis of state transitions typically relies on empirical data collected during the training process. This dependency can limit the model's ability to analyze complex environments, as the model may not have explored all possible states and action combinations. In contrast, methods based on Q-values do not depend on specific state transition details but instead directly evaluate the expected rewards for each state and action combination, effectively overcoming the dependency on empirical data and enhancing their applicability in complex environments.\nIn RL, the main difference between the Q-value and the V-value is that the Q-value involves a combination of state and action, allowing direct quantification of the expected return for each action taken in a given state through Q(s, a). This makes the Q-value particularly well-suited for direct use in the decision-making process, as it provides clear guidance on action selection for each state. In contrast, the V-value is only related to the state and does not involve actions, reflecting the maximum expected return achievable from a state when following the optimal action. This characteristic of not being directly linked to specific actions makes the V-value less straightforward in establishing connections between states from a policy perspective, typically requiring the use of the value functions of subsequent states V(s') or V(s*) to reflect the utility of actions."}, {"title": "Average Counterfactual Difference", "content": "To extend the analysis on agent behavior and the expected return (utility) impacts of varying policies, we investigate further into the counterfactual differences by introducing the concepts of Average Counterfactual differences (ACD) for Q(s, a) and V(s). These metrics offer a refined approach to evaluate and compare the efficacy of the optimal action against a spectrum of alternative actions.\nFor Q(s, a), the Average Action Counterfactual differences, denoted as $\\Delta \\overline{Q}^{\\pi} (s, a^*)$, provides a comprehensive measure by averaging the expected return differences between the optimal action $a^*$ and all other possible actions within the action space. This metric is particularly useful for evaluating the relative advantage of the optimal action by considering its expected return gain over the average expected return of all alternative actions. The formula for $\\Delta \\overline{Q}^{\\pi}$ is given by:\n$\\Delta \\overline{Q}^{\\pi} (s, a^*) = E_{a \\sim A} [\\Delta Q^{\\pi} (s, a^*, a)] = \\frac{1}{|A|} \\sum_{a \\in A} \\Delta Q^{\\pi} (s, a^*, a) = \\frac{1}{|A|} \\sum_{a \\in A} [Q^* (s, a^*) \u2013 Q^* (s, a)]$ (4)\nSimilarly, for V(s), the Average State Counterfactual Differences, $\\overline{\\Delta V}^{\\pi} (s)$, quantifies the average expected return change when transitioning from the current state s to an optimal state $s^*$, compared to transitions to all other potential states s' according to policy $\\pi$. This metric aids in understanding the broader implications of policy choices on the agent's position and subsequent expected return. The computation of $\\overline{\\Delta V}^{\\pi}$ is as follows:\n$\\overline{\\Delta V}^{\\pi} (s) = E_{s' \\sim \\pi} [\\Delta V^{\\pi} (s^*, s')] = \\frac{1}{|S'|} \\sum_{S'}{[\\Delta V^{\\pi} (s^*, s')]}= \\frac{1}{|S'|} \\sum_{S'}{[V^* (s^*) \u2013 V(s')]}$ (5)\nwhere $S' = \\{s' | P(s'|s,a) \\geq 0,\\forall a \\in A\\}$ which means that S' includes all states that may be reached by taking any action a in the action space A from the state s.\nThe comparison of $\\Delta \\overline{Q}$ with $\\Delta Q$, and $\\overline{\\Delta V}$ with $\\Delta V$, highlights their roles in providing analytical depth and breadth within reinforcement learning, crucial for both strategic planning and precise decision-making. More importantly, these models enhance the interpretability and transparency of decision-making processes. ACD offer a broad perspective on the impacts of different actions, aiding in policy clarity, while CD provide detailed insights into specific actions, enhancing operational transparency. Together, they ensure decisions are informed, accountable, and transparent, catering to both overarching polices and immediate actions."}, {"title": "Counterfactual Characteristic Value Function", "content": "In the Shapley Value calculations for reinforcement learning, selecting an appropriate characteristic value function is essential. Typically, these functions are derived from fundamental elements like the value function or the action-value function, which can be straightforwardly used to compute what are known as 'vanilla' characteristic values. Once the policy is learned, the agent's behavior becomes fixed. In any given state, it almost always chooses the best action, hardly considering other suboptimal options. In this way, choosing the optimal action versus not choosing it forms a counterfactual scenario, which represents theoretical possibilities that do not actually occur. However, to facilitate more detailed comparisons in counterfactuals, particularly between an optimal action and a suboptimal one within the same state, employing counterfactual characteristic value becomes essential. This method thoroughly quantifies the expected return difference when choosing the optimal action instead of a suboptimal alternative, this counterfactual analysis significantly contributes to the transparency and explainability of decisions within reinforcement learning models, as it offers clear insights into the reasons behind preferring certain actions over others, thus making the decision-making process more understandable and justifiable. The CD characteristic value functions are expressed as follows:\nCD Characteristic Value Function on AQ:\n$v_{Q^{a,a*,s}}(C) := \\Delta Q_C(s, a^*, a)$ (6)\nCD Characteristic Value Function on AV:\n$v_{V^{s*,s}}(C) := \\Delta V_C(s^*, s')$ (7)\nwhere $s^*_C \\sim P(\\cdot |s, a)$ and $s'_C \\sim P(\\cdot |s, a)$ are the states reached from state s by executing the optimal and suboptimal actions, respectively, and $\\Delta Q_C(s, a^*, a)$ and $\\Delta V_C(s^*, s')$ denote the Q-values and V-values obtained by training within a specific subset 'C' of the state space. These values, being derived from a limited set of dimensions, carry different interpretations and consequences than those calculated from the full state space.\nThe computation of the Shapley Value by CD approach is indispensable for highlighting the relative advantage and direct impact of the optimal action against a specific alternative, offering granular insights into the decision-making process. This method calculates the difference between two specific actions. Using the CD characteristic value function, we can obtain a value called the CD Shapley value, which directly reflects how each feature influences the difference in outcomes between these two actions.\nExpanding the scope, the computation of the Shapley Value by ACD approach averages the expected return differences between the optimal action and all suboptimal actions $A'$, providing a comprehensive evaluation of the action's efficacy across the entire action space. The ACD characteristic value functions are defined as:\nACD Characteristic Value Function on AQ:\n$v_{Q^{a,a*,s}}(C) := \\frac{1}{|A|} \\sum_{a \\in A} \\Delta Q_C^{*}(s, a^*, a)$ (8)\nACD Characteristic Value Function on AV:\n$v_{V^{s*,s}}(C) := \\frac{1}{|S'|} \\sum_{s'_{C} \\in S'_{C}} \\Delta V_C^{*}(s^*, s')$ (9)\nwhere $S'_{C} = \\{s'_{C} | P(s'_C|s,a) \\geq 0,\\forall a \\in A\\}$ which means that $S'_{C}$ includes all states that may be reached by taking any action a in the action space A from the state $s'_C$ of limited dimensions space C.\nThe ACD computation approach excels by demonstrating the overall dominance of the optimal action and providing a comprehensive view of the policy's depth and robustness. Unlike the CD method, the ACD method calculates the average difference between all possible actions and a specific action (usually the optimal action). The ACD Shapley value, obtained through the ACD characteristic value function, offers a evaluation on how a specific action generally compares to other available actions.\nTogether, the CD and ACD computation approach enrich the analysis of policy performance in reinforcement learning by offering both detailed and broad perspectives. While the CD method allows for focused evaluations of specific action dynamics, the ACD delivers a macroscopic understanding of the policy's general utility and resilience. This dual approach strengthens the analytical framework for policy interpretation, emphasizing policy insights over direct policy optimization."}, {"title": "EXPERIMENTS", "content": "We present experimental results from various domains in RL, applying Counterfactual Shapley Value analysis to understand the influence of optimal versus suboptimal actions on individual dimensions of the state space."}, {"title": "Explanation in Gridworld 1 and Gridworld 2", "content": "In GridWord-1 on Fig (1a)", "follows": "Vanilla-SPV(i) or Vani(i): The list results show the Shapley Values for each dimension (feature) of the state", "CD(ij)": "By examining the differences in the contribution of each dimension of a certain state to the selection of different actions i and j", "ACD": "By examining the average differences in the contribution of each dimension of a certain state to the selection of the optimal action versus all non-optimal actions, the list results provide the benchmark of contribution for comparing the average difference between executing the optimal and all non-optimal actions. This clarifies the average differential impact across various dimensions on the decision outcomes when optimal actions are executed as opposed to suboptimal actions, analyzing both the quality (positive or negative) and the degree of the impacts.\nThe Shapley values, computed as the difference between the optimal action and the average of all non-optimal actions, reflect the incremental value of the optimal action. Although these values may appear modest in comparison to the direct contributions of specific actions, the positive values still indicate the superiority of the optimal action over any other, given its positive contribution across all state dimensions. This explains why, in certain situations, selecting the optimal action as opposed to any other action is preferable.\nIn our analysis of the GridWorld-1 environment, several interesting insights were made. When three states s\u2081, s\u2082, and s\u2083 have the same V and Q values after policy learning, we intuitively expect their contributions to deciding the optimal action in each dimension to be consistent. However, when we use the Vanilla-SPV method to evaluate the specific contributions of each dimension on optimal action, we find that the contributions vary among similar states. For example, the contributions of state s\u2081in two dimensions (features) are 3.2 and 5.8, state s\u2082 contributes 4.3 and 4.8, while state s\u2083 contributes 5.5 and 3.5. These results suggest that even though the optimal actions for these states all involve moving to the right (which primarily affects the y-axis coordinate), the Vanilla-SPV method shows that in some cases the contribution of the first dimension (x-axis) appears to be higher than the second dimension (y-axis), which opposes our intuitive expectations. This indicates that although the Vanilla-SPV method can provide specific contribution data for each dimension, it may not accurately reflect which dimension plays a more critical role in decision-making.\nHowever, when we use the CD-SPV and ACD-SPV methods to evaluate the differences between states when performing optimal and suboptimal actions, we gain deeper insights. Taking state s\u2081 as an example, its optimal action is to move right (\u2192) while its suboptimal action is to move up (\u2191) (the other three actions have the same Q or V value, thus they are also considered suboptimal). In this scenario, the differences in dimensions for these two actions are [0, 2", "2": "and [-1.2, 3.2"}]}