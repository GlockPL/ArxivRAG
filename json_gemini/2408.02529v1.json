{"title": "Counterfactual Shapley Values for Explaining Reinforcement Learning", "authors": ["Yiwei Shi", "Qi Zhang", "Kevin McAreavey", "Weiru Liu"], "abstract": "This paper introduces a novel approach Counterfactual Shapley Values (CSV), which enhances explainability in reinforcement learning (RL) by integrating counterfactual analysis with Shapley Values. The approach aims to quantify and compare the contributions of different state dimensions to various action choices. To more accurately analyze these impacts, we introduce new characteristic value functions, the \"Counterfactual Difference Characteristic Value\" and the \"Average Counterfactual Difference Characteristic Value.\" These functions help calculate the Shapley values to evaluate the differences in contributions between optimal and non-optimal actions. Experiments across several RL domains, such as GridWorld, FrozenLake, and Taxi, demonstrate the effectiveness of the CSV method. The results show that this method not only improves transparency in complex RL systems but also quantifies the differences across various decisions.", "sections": [{"title": "Introduction", "content": "Reinforcement learning (RL) has applications in autonomous navigation (Chen, Li, and Tomizuka 2021), healthcare (Liu, Ngiam, and Feng 2019), financial strategy optimization (Jiang, Xu, and Liang 2017), and smart city management (Kumar et al. 2021). However, the key to achieving widespread adoption of these applications lies in overcoming the challenge of explainability in RL models. the inability to adequately explain RL models could provoke ethical and regulatory issues and meet with resistance from potential users, collectively impeding the extensive application and further advancement of RL technology.\nResearch in explainable reinforcement learning is primarily focused on two approaches: intrinsic interpretability and post-hoc interpretability. Intrinsic interpretability strategies enhance model transparency by simplifying the model's structure, often at the expense of reducing the model's performance. For instance, decision tree models, which represent intrinsic interpretability, visually demonstrate decision pathways through their tree-like structure but may fail to capture more complex data patterns. In contrast, post-hoc interpretability methods treat the model as a black box and reveal the decision-making logic by analyzing the relationships between inputs and outputs, thus preserving the model's complexity. Counterfactuals and Shapley Values are both post-hoc explanation methods that provide insights without intervening in the internal structure of the model.\nShapley Value is a method for measuring the contribution of individuals to the total payoff in cooperative games. An application of Shapley values to Explainable Artificial Intelligence (XAI), each feature is regarded as a \"player\", while the model's predictive outcome is considered the \"total payoff\" (Lundberg and Lee 2017). The Shapley Value assesses the average contribution of each feature to the predictive outcome by considering all possible combinations of features.\nInvestigations employing Shapley Values are remarkably limited across both multi-agent reinforcement learning (MARL) and single-agent in RL. However, MARL research demonstrates a relatively richer engagement with Shapley value applications, which are principally concentrated across three aspects. Firstly, Value Decomposition, (Wang et al. 2022) integrates Shapley Value theory with multi-agent Q-learning (SHAQ) to address value decomposition in global reward games, where Shapley Values play a critical role in ensuring equitable distribution of rewards based on individual agent contributions. Secondly, Credit Assignment, (Wang et al. 2020) introduces the Shapley Q-value within MARL (SQDDPG) to reallocate global rewards among agents, leveraging Shapley Values to quantify and fairly distribute the rewards reflecting each agent's marginal contribution to the collective success. Lastly, Model Explanation, (Heuillet, Couthouis, and D\u00edaz-Rodr\u00edguez 2022) the use of Shapley Values to explain cooperative strategies and individual contributions in multi-agent reinforcement learning, approximating Shapley Values via Monte Carlo sampling to reduce computational costs. Shapley Values offer a way to interpret complex multi-agent interactions, making it clearer how each agent's decisions and actions contribute to the overall dynamics and results of the system.\nTo our knowledge, only the SVERL in (Beechey, Smith, and \u015eim\u015fek 2023), within the scope of RL research excluding MARL, utilizes Shapley Values to explain the decision-making process in reinforcement learning. Although SVERL provides a method for understanding and explaining the contribution of state features in the decision-making process of reinforcement learning agents, it has limitations in explaining specific action choices. The core of SVERL lies in analyzing the contribution of state features to"}, {"title": null, "content": "the expected returns of an agent, rather than directly investigating why action a is chosen over action b in the same state, and it does not offer a mechanism to quantify the difference between these action choices. This means that while SVERL can help us indirectly understand how certain features influence an agent's decisions, it cannot directly answer why an agent prefers a specific action, nor can it compare the merits of different actions. This limitation is not unique to SVERL but is a challenge faced by all methods that rely on Shapley Values to explain the decision-making process in reinforcement learning.\nCounterfactuals can effectively address some of the limitations associated with explaining specific action choices in RL. By comparing the utility (Long-Term Expected Return) of the actual action with those of the counterfactual (alternative) action, counterfactuals can help clarify why an agent prefers one action over another and quantify the impact of different actions.\nIn reinforcement learning, two main generative explanation methods by counterfactuals one based on deep generative models and the other on generating counterfactual states - share a common limitation: the difficulty in quantifying differences between counterfactual instances. While methods based on deep generative models (Olson et al. 2021) can create realistic \u201cwhat if\u201d scenarios, comparing these instances in a high-dimensional latent space is challenging, as conventional distance metrics may not apply. On the other hand, methods that focus on minor state adjustments to guide different decisions also face challenges in quantifying the actual impact of these subtle changes on agent behavior, since even small modifications can have widespread effects in complex RL environments. Therefore, although these methods provide valuable insights into the decision-making process in RL, their difficulty in quantifying differences between counterfactual instances limits their application scope and the depth of their explanations.\nTo address the challenges of quantifying differences in reinforcement learning explanations, we develop a novel approach called \"Counterfactual Shapley Value\". While previous research has explored the integration of counterfactual explanations with Shapley Values, such as addressing credit assignment problems in MARL in (Li et al. 2021) or providing explanations in supervised learning (Albini et al. 2022), the former develops a novel method to assign credits in multi-agent systems using counterfactual thinking and Shapley values, which quantifies each agent's contribution by considering what would happen if certain agents were absent, allowing for a fair and accurate assessment of each agent's impact on the collective outcome, the latter uses counterfactual scenarios to enhance Shapley value explanations, providing insights on how changes to input features could influence model predictions. It makes explanations more actionable by showing which features to adjust to achieve desired outcomes. Our work is fundamentally different. For the first time, we have applied the combination of counterfactual reasoning and Shapley Values to the domain of reinforcement learning, introducing the innovative CSV approach. This method not only retains the benefits of traditional Shapley Values in quantitative analysis but also"}, {"title": null, "content": "enhances the understanding of decision-making by examining specific hypothetical scenarios, such as \u201cWhat would happen if a specific action were not taken?\". Our approach brings a more transparent and interpretable perspective to decision-making in reinforcement learning, significantly alleviating the limitations of previous explanatory methods.\nIn this paper, our research contributions are threefold: Firstly, we have introduced an innovative method called Counterfactual Shapley Value, which can precisely reveal the changes and differences in the contributions of state features to different actions within reinforcement learning. Secondly, we have designed two novel characteristic value functions - the Counterfactual Difference Characteristic Value and the Average Counterfactual Difference Characteristic Value, which incorporate counterfactual mechanisms for calculating Shapley Values, offering us new avenues for computation. Lastly, we have tested and analyzed this method across multiple RL environments, where the quantified results from CSV have allowed us to explain the behavior of agents, thereby demonstrating the effectiveness of our approach.\""}, {"title": "PRELIMINARIES", "content": ""}, {"title": "Markov Decision Process (MDP)", "content": "In the domain of reinforcement learning, an agent's interaction with the environment is conceptualized as a Markov Decision Process (MDP), denoted by $MDP = (S, A, P, R, \\gamma)$.\nHere, S is the set of all possible states, A is the set of actions, P is the state transition probabilities $P(s'|s, a): S \\times A \\times S \\rightarrow [0, 1]$, $R(s, a) : S \\times A \\rightarrow R$ represents the reward function $R(s, \\alpha)$, $\\gamma \\in [0,1]$ denotes the discount factor the cumulative reward $G_t$ represents the sum of discounted future rewards starting from time t. It is mathematically defined as $G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$, with $R_{t+k+1}$ signifying the immediate reward received at time t+k+1. The role of cumulative reward is fundamental, serving as a critical metric to assess and guide the agent's performance. The agent's objective is to discover a policy $\\pi : S \\times A \\rightarrow [0, 1]$ that assigns probabilities to actions in each state, with the aim of optimizing the cumulative expected reward over time. The state-value function $V^{\\pi}(s) = E_{\\pi} [G_t | S_t = s]$ computes the expected return from starting in state s under policy $\\pi$, while the action-value function $Q^{\\pi} (s, a) = E_{\\pi} [G_t | S_t = s, A_t = a]$ assesses the expected return after taking action a in state s under the same policy. The optimal policy, therefore, is defined by maximizing these functions, $V^* (s)$ and $Q^* (s, a)$, to achieve the highest possible expected returns from all states and actions."}, {"title": "Shapley Values in RL", "content": "In the domain of cooperative game theory, Shapley Values, originally proposed by (Shapley 1953), serve as a pivotal solution concept for the equitable allocation of collective gains or losses among participating entities, predicated on their individual contributions to the cumulative outcome. When transposed to the Reinforcement Learning (RL) framework, this concept facilitates an analytical assessment of the individual contributions of state features to the decision-making"}, {"title": null, "content": "efficacy of an RL agent.\nIn RL, let $F = \\{0,1,..., n - 1\\}$ represent the set of indices corresponding to the comprehensive enumeration of state features within the state space S. The state space itself is structured into n discrete dimensions, expressed as $S = S^{(0)} \\times S^{(1)} \\times ... \\times S^{(n-1)}$, where each $S^{(i)}$ captures a distinct aspect of the environmental dynamics. Within this framework, each state is an ordered set, defined by $s = \\{s^{(i)}|s^{(i)} \\in S^{(i)}, i \\in F\\}$. Let $C \\subset F$ denote a subset of state features, specifically excluding the feature indexed by i. A partial observation of the state is then given by $s_C = \\{s^{(i)}|s^{(i)} \\in S^{(i)}, i \\in C\\}$, which offers a perspective of the state based on a subset of observable features. The Shapley value (Shapley 1953) for a state feature i, denoted as $\\phi_i$, quantifies the contribution of feature i and is mathematically expressed as follows:\n$\\Phi_{i}=\\sum_{C \\subset F \\backslash\\{i\\}} \\frac{|C| !(|F|-|C|-1) !}{|F| !} \\delta(i, C) \tbinom{}{}(1)$\nwhere $\\delta(i, C)=v(C \\cup\\{i\\})-v(C)$ quantifies the marginal contribution in the characteristic value function attributed to the incorporation of feature i into the subset C, where v(C) represents the characteristic value function assessed over the subset C excluding of feature i, and $v(C \\cup\\{i\\})$ represents the characteristic value function with the inclusion of feature i. This formulation enables the computation of the average marginal contribution of each state feature i across all conceivable coalitions of other features, thereby elucidating its significance in the decision-making processes of the RL agent."}, {"title": "Characteristic Value Function", "content": "In the field of machine learning, the integration of the characteristic value function (CVF) with Shapley Values provides a systematic method for quantifying the individual contributions of features towards predictive outcomes. This approach leverages principles from cooperative game theory, adapting them to clarify the significance of features within predictive models. Let $f_F : S \\rightarrow \\mathbb{R}$ denote the characteristic function defined over the complete feature set F in state space S, which estimates the expected value of model predictions as $v(F) := f_F (s)$. Correspondingly, for a subset $C \\subset F$, the characteristic function $f_C : S^C \\rightarrow \\mathbb{R}$ assesses the expected prediction value $v(C) := f_C(s^C)$, where $S^C$ denotes the subspace restricted to the features in C. When all features are unknown, that is, no features are input into the model, it can be assumed that the feature set is an empty set ($\\O$). Then, the function's value is typically set to 0, denoted as $v(\\O) = 0$, indicating that the contribution of features is zero."}, {"title": "COUNTERFACTUAL SHAPLEY VALUES", "content": "In the XRL, it is imperative to gain a comprehensive understanding of the fundamental motivations underlying the decision-making processes of agents. While Shapley Values do not clarify the policy significance of action choices directly, the incorporation of counterfactual analysis enhances"}, {"title": null, "content": "the ability to assess the potential effects of various actions in hypothetical scenarios, thereby offering deep insights into the policy selections of agents.\nIt is noteworthy that the value functions Q(s, a) and V(s), while pivotal in guiding agents towards optimal decision-making, also serve as utility functions for computing Shapley Values, highlighting their significance in analyzing agent behavior. By employing these value functions as characteristic value function, a more precise quantitative analysis of the specific contributions of different dimensions in any given state can be achieved."}, {"title": "Counterfactual Differences", "content": "From the perspective of explaining agent behavior, effectively comparing two distinct actions requires not only evaluating the immediate reward following action execution and the expected long-term return but also exploring counterfactual scenarios -\"what would happen if a different action were chosen?\" To facilitate this, we introduce the notion of counterfactual difference (CD), including both action counterfactual differences (AQ) and state counterfactual differences (AV), to thoroughly analyze the impacts of various actions. The action counterfactual differences examines the difference in Q values between the actual action taken and a hypothetical alternative action in a given state, while the state counterfactual differences focuses on the variance in V values for the new states resulting from these actions.\nThe action counterfactual differences, denoted as $\\Delta Q^{\\pi} (s, a^*, a)$, is calculated by the comparison between the expected return of taking an optimal action $a^*$ under the policy $\\pi$ in state s, against the expected return of taking the other action a under the same policy $\\pi$ in the same state s. This comparison yields the differences value, revealing the potential utility deviation resulting from an alternative action choice. $\\Delta Q^{\\pi} (s, a^*, a)$ is calculated as follows:\n$\\Delta Q^{\\pi}(s, a^*, a) = Q^{\\pi}(s, a^*) - Q^{\\pi}(s, a) \tbinom{}{}(2)$\nwhere $Q^{\\pi} (s, a^*)$ represents the expected return of executing the optimal action $a^*$ in state s under the policy $\\pi$, while $Q(s, a)$ signifies the expected return of executing the counterfactual action a under the same policy $\\pi$ in the same state. Although the policy usually prioritizes executing what is assessed as the optimal action, the reward of non-optimal actions is also recorded during training to maintain a balance between exploration and exploitation. This forms the basis for a counterfactual analysis scenario, through which we can compare the expected return of non-optimal actions to optimal actions, thereby evaluating how different state features contribute to these two types of decisions.\nThe policy $\\pi$ can be either a \"fully learned policy\" or a \"partially learned policy\". In both types, the actions considered optimal are those with the highest Q values in each state. Typically, policies will choose these actions with the highest Q values for execution. A fully learned policy can always accurately execute these optimal choices. However, a partially learned policy, although theoretically aimed at maximizing rewards, may not always choose the best actions in practice. Since partially learned policies struggle to accurately calculate Q values to effectively predict long-term"}, {"title": null, "content": "returns, it becomes difficult to assess how different state features contribute to decision-making. For these reasons, this study only considers policies that are fully learned and can execute accurately.\nThe state counterfactual differences, denoted as $\\Delta V^{\\pi}(s^*, s')$, quantifies the variation in expected return associated with transitioning from the current state s to an optimal state $s^*$ under the policy $\\pi$, in contrast to transitioning to a different state s' under the same policy $\\pi$. This differences highlights the expected return change resulting from adopting the optimal action compared to an alternative action from the same starting point. $\\Delta V^{\\pi}(s^*, s')$ is calculated as follows:\n$\\Delta V^{\\pi}(s^*, s') = V^{\\pi}(s^*) - V^{\\pi}(s') \tbinom{}{}(3)$\nAlthough some states $s'$ might only be reached through suboptimal actions a with very low probabilities after learned policy, these states still have definitive value functions, which reflect the expected rewards that could be obtained starting from these states. Even if these states are not frequently visited under the optimal policy, we can still explore these states using counterfactual methods to analyze the choices of suboptimal actions. However, the analysis of state transitions typically relies on empirical data collected during the training process. This dependency can limit the model's ability to analyze complex environments, as the model may not have explored all possible states and action combinations. In contrast, methods based on Q-values do not depend on specific state transition details but instead directly evaluate the expected rewards for each state and action combination, effectively overcoming the dependency on empirical data and enhancing their applicability in complex environments.\nIn RL, the main difference between the Q-value and the V-value is that the Q-value involves a combination of state and action, allowing direct quantification of the expected return for each action taken in a given state through Q(s, a). This makes the Q-value particularly well-suited for direct use in the decision-making process, as it provides clear guidance on action selection for each state. In contrast, the V-value is only related to the state and does not involve actions, reflecting the maximum expected return achievable from a state when following the optimal action. This characteristic of not being directly linked to specific actions makes the V-value less straightforward in establishing connections between states from a policy perspective, typically requiring the use of the value functions of subsequent states $V(s')$ or V(s*) to reflect the utility of actions."}, {"title": "Average Counterfactual Difference", "content": "To extend the analysis on agent behavior and the expected return (utility) impacts of varying policies, we investigate further into the counterfactual differences by introducing the concepts of Average Counterfactual differences (ACD) for Q(s, a) and V(s). These metrics offer a refined approach to evaluate and compare the efficacy of the optimal action against a spectrum of alternative actions.\nFor Q(s, a), the Average Action Counterfactual differences, denoted as $\\Delta \\overline{Q}^{\\pi} (s, a^*)$, provides a comprehensive"}, {"title": null, "content": "measure by averaging the expected return differences between the optimal action $a^*$ and all other possible actions within the action space. This metric is particularly useful for evaluating the relative advantage of the optimal action by considering its expected return gain over the average expected return of all alternative actions. The formula for $\\Delta \\overline{Q}^{\\pi}$ is given by:\n$\\Delta \\overline{Q}^{\\pi}(s, a^{*})=E_{a \\sim A}[\\Delta Q^{\\pi}(s, a^{*}, a)] \\\\=\\frac{1}{|A|} \\sum_{a \\in A} \\Delta Q^{\\pi}(s, a^{*}, a) \\\\=\\frac{1}{|A|} \\sum_{a \\in A} [Q^{*}(s, a^{*})-Q^{*}(s, a)] \tbinom{}{}(4)$\nSimilarly, for V(s), the Average State Counterfactual Differences, $\\overline{AV}^{\\pi} (s)$, quantifies the average expected return change when transitioning from the current state s to an optimal state $s^*$, compared to transitions to all other potential states $s'$ according to policy $\\pi$. This metric aids in understanding the broader implications of policy choices on the agent's position and subsequent expected return. The computation of $\\overline{AV}^{\\pi}$ is as follows:\n$\\overline{AV}^{\\pi}(s)=E_{s' \\sim \\pi}[\\Delta V^{\\pi}(s^{*}, s')] \\\\=\\frac{1}{|S'|} \\sum_{s' \\in S'} \\Delta V^{\\pi}(s^{*}, s') \\\\=\\frac{1}{|S'|} \\sum_{s' \\in S'} [V^{*}(s^{*})- V^{*}(s')] \tbinom{}{}(5)$\nwhere $S' = \\{s' | P(s'|s,a) \\geq 0,\\forall a \\in A\\}$ which means that S' includes all states that may be reached by taking any action a in the action space A from the state s.\nThe comparison of $\\Delta \\overline{Q}$ with $\\Delta Q$, and $\\Delta \\overline{V}$ with $\\Delta V$, highlights their roles in providing analytical depth and breadth within reinforcement learning, crucial for both strategic planning and precise decision-making. More importantly, these models enhance the interpretability and transparency of decision-making processes. ACD offer a broad perspective on the impacts of different actions, aiding in policy clarity, while CD provide detailed insights into specific actions, enhancing operational transparency. Together, they ensure decisions are informed, accountable, and transparent, catering to both overarching polices and immediate actions."}, {"title": "Counterfactual Characteristic Value Function", "content": "In the Shapley Value calculations for reinforcement learning, selecting an appropriate characteristic value function is essential. Typically, these functions are derived from fundamental elements like the value function or the action-value function, which can be straightforwardly used to compute what are known as 'vanilla' characteristic values. Once the policy is learned, the agent's behavior becomes fixed. In any given state, it almost always chooses the best action, hardly considering other suboptimal options. In this way, choosing the optimal action versus not choosing it forms a counterfactual scenario, which represents theoretical possibilities that"}, {"title": null, "content": "do not actually occur. However, to facilitate more detailed comparisons in counterfactuals, particularly between an optimal action and a suboptimal one within the same state, employing counterfactual characteristic value becomes essential. This method thoroughly quantifies the expected return difference when choosing the optimal action instead of a suboptimal alternative, this counterfactual analysis significantly contributes to the transparency and explainability of decisions within reinforcement learning models, as it offers clear insights into the reasons behind preferring certain actions over others, thus making the decision-making process more understandable and justifiable. The CD characteristic value functions are expressed as follows:\nCD Characteristic Value Function on AQ:\n$v^{a,a,s}_Q (C) := \\Delta Q^C(s, a^*, a) \tbinom{}{}(6)$\nCD Characteristic Value Function on AV:\n$v^{s^*,s}_V (C) := \\Delta V^C(s^{C^*}, s^{C'}) \tbinom{}{}(7)$\nwhere $s^{C^*} \\sim P(\\cdot|s, a^*)$ and $s^{C'} \\sim P(\\cdot|s, a)$ are the states reached from state s by executing the optimal and suboptimal actions, respectively, and $\\Delta Q^C(s, a^*, a)$ and $\\Delta V^C(s^{C^*}, s^{C'})$ denote the Q-values and V-values obtained by training within a specific subset 'C' of the state space. These values, being derived from a limited set of dimensions, carry different interpretations and consequences than those calculated from the full state space.\nThe computation of the Shapley Value by CD approach is indispensable for highlighting the relative advantage and direct impact of the optimal action against a specific alternative, offering granular insights into the decision-making process. This method calculates the difference between two specific actions. Using the CD characteristic value function, we can obtain a value called the CD Shapley value, which directly reflects how each feature influences the difference in outcomes between these two actions.\nExpanding the scope, the computation of the Shapley Value by ACD approach averages the expected return differences between the optimal action and all suboptimal actions A', providing a comprehensive evaluation of the action's efficacy across the entire action space. The ACD characteristic value functions are defined as:\nACD Characteristic Value Function on AQ:\n$v^{a,a,s}_Q (C) := \\frac{1}{|A'|} \\sum_{a \\in A'} \\Delta Q^C(s, a^*, a) \tbinom{}{}(8)$\nACD Characteristic Value Function on AV:\n$v^{s^*,s}_V (C) := \\frac{1}{|S^{C'}|} \\sum_{S^{C'} \\in S^{C'}} \\Delta V^C(s^{C^*}, s^{C'}) \tbinom{}{}(9)$\nwhere $S^{C'} = \\{s^{C'} | P(s^{C'}|s,a) \\geq 0,\\forall a \\in A\\}$ which means that $S^{C'}$ includes all states that may be reached by taking any action a in the action space A from the state s of limited dimensions space C.\nThe ACD computation approach excels by demonstrating the overall dominance of the optimal action and providing"}, {"title": null, "content": "a comprehensive view of the policy's depth and robustness. Unlike the CD method, the ACD method calculates the average difference between all possible actions and a specific action (usually the optimal action). The ACD Shapley value, obtained through the ACD characteristic value function, offers a evaluation on how a specific action generally compares to other available actions.\nTogether, the CD and ACD computation approach enrich the analysis of policy performance in reinforcement learning by offering both detailed and broad perspectives. While the CD method allows for focused evaluations of specific action dynamics, the ACD delivers a macroscopic understanding of the policy's general utility and resilience. This dual approach strengthens the analytical framework for policy interpretation, emphasizing policy insights over direct policy optimization."}, {"title": "EXPERIMENTS", "content": "We present experimental results from various domains in RL, applying Counterfactual Shapley Value analysis to understand the influence of optimal versus suboptimal actions on individual dimensions of the state space."}, {"title": "Explanation in Gridworld 1 and Gridworld 2", "content": "In GridWord-1 on Fig (1a), it is easily discovered that the optimal action for each non-target state is to move right \u2192. However, relying solely on the value-table on Fig (1c) generated by state-value functions or action-value functions can only explain the choice of the current action, without explaining the specific impact of that action on the state features. In other words, due to the state possessing multiple dimensions, it is not feasible to directly compare the influence of any dimension on the selection of optimal actions through the analysis of value functions and related techniques.\nUsing the Counterfactual Difference and Average Counterfactual Difference Shapley Value methods (CD-SPV and ACD-SPV) for calculation of Shapley Value by state-value function (V-value), we can examine the differences between the optimal action and all non-optimal actions, as well as between the optimal action and specific non-optimal actions. This facilitates our comprehension of how these differences affect each dimension of the state. Throughout this process, we will utilize the Shapley Value method offered by the Vanilla characteristic value function (Vanilla-SPV) for thorough analysis.\nThe findings obtained from the analysis of the GridWord-1 scenario using the ACD-SPV,CD-SPV and Vanilla-SPV methods are presented in Table (1). The functionalities of these three methods are introduced as follows:\nVanilla-SPV(i) or Vani(i): The list results show the Shapley Values for each dimension (feature) of the state, which indicate the individual contributions of different dimensions (feature) to the choice of action i. In short, these Shapley Values allow us to see the importance and influence of each state dimension in the decision-making process. Lower index i corresponds to better actions, with action 0 corresponding to the optimal action, which has a Shapley Value for each feature denoted by Vani(0). The next best action"}, {"title": null, "content": "corresponds to Vani(1), and subsequent actions are similarly denoted by Vani(2), Vani(3), etc."}, {"title": null, "content": "CD-SPV(i, j) or CD(ij): By examining the differences in the contribution of each dimension of a certain state to the selection of different actions i and j, the list results show the relative importance of state dimensions (feature) by differences in Shapley Values under varying actions. Since we typically focus on the optimal action i = 0 compared to non-optimal action j, it can describe how the different dimensions of a state contribute differently to the optimal and non-optimal actions. This clarifies why, in certain scenarios, opting for action i over action j might be more advantageous. CD(0,1) compares the optimal action 0 with the suboptimal action 1, revealing the relative differences in each dimension when action 0 is executed instead of action 1. Conversely, CD(1,0) indicates the comparison when action 1 is executed instead of action 0, and CD(0,1) is the negative"}, {"title": null, "content": "of CD(1,0) in each dimension (feature), meaning CD(0,1) = -CD(1,0). The results show that a larger positive difference indicates that the dimension is more important for the decision-making; conversely, a larger negative difference indicates that the dimension has a negative impact on the decision; if the difference is close to zero, it means that this dimension contributes equally under both scenarios. Similarly, CD(0,2) and CD(0,3) compare the optimal action with the second and third suboptimal actions, respectively. We consider all suboptimal actions to be counterfactual actions because, once the policy is learned, the current policy will not execute these suboptimal actions. Of course, in the same state, there may be multiple optimal actions that are equally valid and executable. We choose one of these as the optimal action, and the rest are regarded as counterfactual actions, that is, suboptimal actions.\nACD-SPV or ACD: By examining the average differences in the contribution of each dimension of a certain state to the selection of the optimal action versus all non-optimal actions, the list results provide the benchmark of contribution for comparing the average difference between executing the optimal and all non-optimal actions. This clarifies the average differential impact across various dimensions on the decision outcomes when optimal actions are executed as opposed to suboptimal actions, analyzing both the quality (positive or negative) and the degree of the impacts.\nThe Shapley values, computed as the difference between the optimal action and the average of all non-optimal actions, reflect the incremental value of the optimal action. Although these values may appear modest in comparison to the direct contributions of specific actions, the positive values still indicate the superiority of the optimal action over any other, given its positive contribution across all state dimensions. This explains why, in certain situations, selecting the optimal action as opposed to any other action is preferable.\nIn our analysis of the GridWorld-1 environment, several interesting insights were made. When three states $s_1$, $s_2$, and $s_3$ have the same V and Q values after policy learning, we intuitively expect their contributions to deciding the optimal action in each dimension to be consistent. However, when we use the Vanilla-SPV method to evaluate the specific contributions of each dimension on optimal action, we find that the contributions vary among similar states. For example, the contributions of state $s_1$in two dimensions (features) are 3.2 and 5.8, state $s_2$ contributes 4.3 and 4.8, while state $s_3$ contributes 5.5 and 3.5. These results suggest that"}, {"title": null, "content": "even though the optimal actions for these states all involve moving to the right (which primarily affects the y-axis coordinate), the Vanilla-SPV method shows that in some cases the contribution of the first dimension (x-axis) appears to be higher than the second dimension (y-axis), which opposes our intuitive expectations. This indicates that although the Vanilla-SPV method can provide specific contribution data for each dimension, it may not accurately reflect which dimension plays a more critical role in decision-making.\nHowever, when we use the CD-SPV and ACD-SPV methods to evaluate the differences between states when performing optimal and suboptimal actions, we gain deeper insights. Taking state $s_1$ as an example, its optimal action is to move right (\u2192) while its suboptimal action is to move up (\u2191) (the other three actions have the same Q or V value, thus they are also considered suboptimal). In this scenario, the differences in dimensions for these two actions are [0, 2]. Similarly, for states $s_2$ and $s_3$, the differences in the same conditions are [0, 2] and [-1.2, 3.2] respectively. The reason these two values differ is that the state transition paths are different after executing optimal and suboptimal actions. For example, when executing the optimal action (\u2192), $s_2$ transitions to $s_5$ and $s_3$ transitions to $s_6$; whereas when executing the suboptimal action (\u2191), $s_2$ transitions to $s_1$ and $s_3$ transitions to $s_2$. The Shapley Value of each state's features contributing to the decision is different, which results in different Shapley Values for the difference between optimal and suboptimal actions. These results first reveal the differences in each dimension between optimal and suboptimal scenarios, indicating how much each dimension is improved or reduced, and also show that the second dimension contributes more significantly to decision-making. Despite the three suboptimal actions of state $s_1$ having the same value, their differences vary because performing the optimal action leads $s_1$ to state $s_4$, while performing the actions up (\u2191) and left (\u2192) keeps it in state $s_1$, but performing the action down (\u2193) moves it to state $s_2$, demonstrating that even if some actions have the same Q or V value, their utilities can be completely different. ACD-SPV provides a balanced view, showing the average expected return difference between the optimal action and all suboptimal actions. If the counterfactual difference between optimal and suboptimal actions is significantly greater than the average, it indicates a large expected return difference generated by these actions. This can be a useful metric for evaluating similar states in abstract state tasks.\nTherefore, from Table (1), it's clear that the contribution"}, {"title": null, "content": "of the second dimension significantly exceeds that of the first. The difference in contribution for the second dimension typically ranges between 2 and 3, while the first dimension has little to no positive contribution except in states $s_6$ and $s_9$. This situation occurs because, in these states, the Shapley Value of the first dimension exceeds that of the second, resulting in a slight positive contribution"}]}