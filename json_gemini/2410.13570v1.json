{"title": "RGB to Hyperspectral: Spectral Reconstruction for Enhanced Surgical Imaging", "authors": ["Tobias Czempiel", "Alfie Roddan", "Maria Leiloglou", "Zepeng Hu", "Kevin O'Neill", "Giulio Anichini", "Danail Stoyanov", "Daniel Elson"], "abstract": "This study investigates the reconstruction of hyperspectral signatures from RGB data to enhance surgical imaging, utilizing the publicly available HeiPorSPECTRAL dataset from porcine surgery and an in-house neurosurgery dataset. Various architectures based on convolutional neural networks (CNNs) and transformer models are evaluated using comprehensive metrics. Transformer models exhibit superior performance in terms of RMSE, SAM, PSNR and SSIM by effectively integrating spatial information to predict accurate spectral profiles, encompassing both visible and extended spectral ranges. Qualitative assessments demonstrate the capability to predict spectral profiles critical for informed surgical decision-making during procedures. Challenges associated with capturing both the visible and extended hyperspectral ranges are highlighted using the MAE, emphasizing the complexities involved. The findings open up the new research direction of hyperspectral reconstruction for surgical applications and clinical use cases in real-time surgical environments.", "sections": [{"title": "1. Introduction", "content": "Hyperspectral imaging (HSI) is increasingly recognized in surgical applications for its ability to reveal intricate details that conventional imaging methods often overlook [21, 2]. Unlike traditional cameras, which capture a broad range of wavelengths within the Red, Green, and Blue (RGB) parts of the electromagnetic spectrum, HSI extends this capability by detecting narrower wavelength bands over a larger range of the electromagnetic spectrum [24, 6]. The resultant hyperspectral image contains more than three channels, each representing one narrow spectral band of reflected light. Depending on the hardware C can vary with different mininmum and maximum wavelength. This capability allows HSI to unveil critical information, such as the presence of tumors and detailed functional insights like blood hemodynamics, without the need for contrast agents like indocyanine green (ICG) [8].\nDeep learning (DL) is often used as a tool to analyse this data and allow for predictions of multiple downstream tasks such as classification, localisation, regression or segmentation of biological tissue [9]. Despite its potential, the widespread adoption of HSI in surgery is hindered by significant challenges associated with specialized HSI cameras. These cameras are costly, bulky, and often compromise on factors such as spatial resolution, spectral resolution, frame rate, and overall size and cost [9]. To algorithmically alleviate some of these limitations, the vision community has proposed various concepts. One approach involves reconstructing spatial information from low-resolution RGB images to high-resolution RGB images. By capturing the semantic content of low-resolution images, it is possible to reconstruct high-resolution images with much better performance than simple upscaling could provide. Motivated by this success, researchers have explored the reconstruction of not only spatial information but also spectral information. RGB can be seen as a low-resolution, channel-wise representation, and the goal of the reconstruction task is to recover the much higher spectral resolution of multispectral (greater than three channels) and hyperspectral images (typically greater than 20 channels).\nAn emerging approach from the vision community has gained attention: reconstructing hyperspectral information from RGB images [31]. This approach, pioneered in challenges like the NTIRE challenge [3] utilizes DL models based on CNN or more recently transformer architectures, enabling existing RGB imaging systems to provide more comprehensive spectral data while circumventing the drawbacks of specialized HSI hardware. By exploring these contributions, this paper aims to advance the understanding and application of RGB to HSI reconstruction in surgical contexts.\nThis paper presents a pioneering analysis in the field of surgical HSI reconstruction, focusing on several key contributions. First, we introduce a comparative architectural framework for RGB to HSI reconstruction methods, thoroughly evaluating a range of architectures simple fully connected layers with non-linearities"}, {"title": "2. Related Work", "content": "Surgical HSI offers the ability to capture light information not only in greater detail within the visible spectrum but also beyond it. This detailed spectral information facilitates the separation of biological tissues, enhancing the predictive capabilities of statistical and DL models for functional information [21]. HSI has been used for many different clinical applications ranging from advanced dermatology [15] and wound care [23] to intraoperative tumour margin delineation [1], retinal vasculature segmentation [13] and determination of the transection margin during colorectal resection [17]. Additionally, HSI can be used to predict functional information, this includes specific, actionable insights such as tissue oxygenation levels or blood flow dynamics [16, 19] to be used by the surgeon. Specifically, surgeons may wish to visualize or estimate Tissue oxygenation (StO2) or other relevant functional informations such as Tissue Hemoglobin Index (THI) or Near Infrared Perfusion Index (NPI) [11] in order to aid decision making during procedures. While some work has attempted to derive this functional information from RGB images [18], the approach has not been widely explored and tends to lack ground truth labels, leading to non-robust models across different settings. Nevertheless, the success of these initial efforts serves as a strong motivation for our research.\nIn summary, the clinical applications of HSI are extensive, making the reconstruction of HSI information from RGB highly beneficial. Given the higher information density of HSI images compared to standard three-channel RGB images, this reconstruction is an ill-posed problem. In the vision domain, the reconstruction of HSI from RGB has been explored using various algorithm types. Statistical methods such as regression and sparse coding [29], as well as advanced CNN approaches [12, 25], have been investigated. Recently, a challenge (NTIRE [3]) compared different deep learning-based approaches, utilizing a variety of architecture types and loss functions. With the emergence of transformers, new methods for reconstructing hyperspectral information have also been developed [5], showcasing the potential of these advanced models to enhance spectral reconstruction.\nThe development of a RGB to HSI reconstruction model for surgical applications depends heavily on the availability of data. The dataset must not only contain HSI but also RGB information that have been spatially registered together or reconstructed. Spectralpaca [4], a recently introduced hyperspectral video dataset, is designed for human perfusion monitoring. It features spectral video recordings of ten healthy participants in different physiological states. While this dataset addresses medical applications, it does not specifically pertain to surgical contexts. The hyperspectral imaging benchmark (HIB) [20] dataset is comprised of hyperspectral and RGB images during neurosurgery, offering comprehensive insights into tissue composition and pathology. Additionally, the HeiPorSPECTRAL dataset [27]"}, {"title": "3. Method", "content": "The purpose of this study is to investigate whether RGB to HSI reconstruction is viable for surgical HSI. To achieve this, we compare various DL architectures from both the imaging domain and the spectral reconstruction domain."}, {"title": "3.1. Architecture", "content": "For the architecture choice we wanted to find a mix between advanced DL models based on CNNs and Transformers but also simpler models with fewer parameters and non-linearities to investigate the necessity and improvements between different architecture complexities."}, {"title": "3.1.1. PixelFeatureNet", "content": "The PixelFeatureNet employs a simple per-pixel design analysing the spectra across the entire range without considering any spacial neighborhood relationships. It consists of four fully-connected layers where the channel dimensionality per pixel increases progressively. The input comprises 3 channels (RGB), and the output dimensionality is C, representing the number of channels for the dataset. ReLU activation functions are used throughout."}, {"title": "3.1.2. LocalFeatureNet", "content": "The LocalFeatureNet replaces the fully-connected layers of PixelFeatureNet with convolutional layers, each utilizing a fixed kernel size of 3 across all four layers. This modification introduces local neighborhood information, aimed at enhancing spectral reconstruction capabilities while maintaining a lightweight model design."}, {"title": "3.1.3. UNET", "content": "Additionally, we incorporated a standard U-Net architecture [22], renowned for its effectiveness in medical segmentation and reconstruction tasks. The U-Net's design, featuring bottleneck layers and skip connections, facilitates capturing both global context and detailed local features, thereby aiding in precise spectral reconstruction. We use a ResNet50 [14] backbone pretrained on Imagenet [10]."}, {"title": "3.1.4. MST++", "content": "For our final model, we selected a transformer-based architecture [28] called MST++ [5]. This model has demonstrated significant potential in reconstructing real-world images and stands as the state-of-the-art on the NTIRE dataset. The transformer architecture of MST++ was designed to leverage global dependencies effectively, by utilising spectral-wise self-attention it specifically targets the HSI modality, potentially improving spectral reconstruction accuracy.\nOverall the selection of models allows us to explore different architectural paradigms and their impact on hyperspectral image reconstruction tasks."}, {"title": "3.2. Metrics", "content": "There are several metrics to evaluate the quality of the reconstruction of hyperspectral images. Hyperspectral data has the unique characteristic of being both an image and also a composition of spectra. For this reason we can not only evaluate image level values with metrics like mean absolute error (MAE), root mean squared error (RMSE), peak signal-to-noise ratio"}, {"title": "3.2.1. Mean Absolute Error (MAE)", "content": "The MAE is calculated as and is also known as the L1 Norm:\nMAE(y, \u0177) = $\\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|$"}, {"title": "3.2.2. Root Mean Square Error (RMSE)", "content": "RMSE is calculated as the square root of the average of the squared differences between the predicted and observed values. In hyperspectral reconstruction, RMSE quantifies the error between the reconstructed spectral data and the ground truth spectral data for each pixel.\nRMSE(y, y) = $\\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}$"}, {"title": "3.2.3. Peak Signal-to-Noise Ratio (PSNR)", "content": "Peak Signal-to-Noise Ratio (PSNR) is a measure of the quality of the reconstructed image compared to the original image. It is expressed in decibels (dB) and is inversely proportional to the mean squared error. Higher PSNR values indicate better reconstruction quality.\nPSNR = 10 log$_{10}$ $\\frac{MAX^2}{MSE}$\nwhere MAX is the maximum possible pixel value of the image, and MSE is the mean squared error."}, {"title": "3.2.4. Spectral Angle Mapper (SAM)", "content": "Spectral Angle Mapper (SAM) is a spectral classification method that measures the spectral similarity between two spectra by calculating the angle between them. It is used to compare the similarity of the spectral signatures of the reconstructed and ground truth images. Smaller angles indicate higher similarity.\nSAM(y, y) = cos$^{-1}$ $\\frac{\\sum_{i=1}^{N} y_i \\hat{y}_i}{\\sqrt{\\sum_{i=1}^{N} y_i^2} \\sqrt{\\sum_{i=1}^{N} \\hat{y}_i^2}}$"}, {"title": "3.2.5. Structural Similarity Index (SSIM)", "content": "Structural Similarity Index (SSIM) is a perceptual metric that measures the similarity between two images. It considers changes in structural information, luminance, and contrast. SSIM values range from -1 to 1, with 1 indicating perfect similarity.\nSSIM(y, y) = $\\frac{(2\\mu_y\\mu_{\\hat{y}} + C_1) (2\\sigma_{y\\hat{y}} + C_2)}{(\\mu_y^2 + \\mu_{\\hat{y}}^2 + C_1) (\\sigma_y^2 + \\sigma_{\\hat{y}}^2 + C_2)}$\nwhere $\\mu_y$ and $\\mu_{\\hat{y}}$ are the average pixel values of images y and prediction $\\hat{y}$, $\\sigma_y^2$ and $\\sigma_{\\hat{y}}^2$ are the variances, $\\sigma_{y\\hat{y}}$ is the covariance, and $C_1$ and $C_2$ are constants to stabilize the division.\nThese metrics collectively provide a comprehensive evaluation of the quality of hyperspectral reconstruction, assessing both pixel-wise accuracy and structural fidelity."}, {"title": "3.3. Analysis of Reconstruction in Different Wavelength Ranges", "content": "The reconstruction of hyperspectral information is a severely ill-posed problem [31]. However, the difficulty of reconstructing the channels C varies depending on the wavelengths $\\lambda$ within C. We hypothesise that hyperspectral wavelengths within the visible range are easier to reconstruct as RGB images contain a lot of macro information of these wavelengths. Conversely, wavelengths in the ultraviolet (UV) or infrared (IR) ranges are more challenging to reconstruct due to the lack of spectral information in these regions in the input RGB images. We consider the full hyperspectral range from the lowest wavelength in the hyperspectral dataset Amin to the highest wavelength Amax of a dataset as follows:\n$\\lambda \\in [A_{min}, A_{max}] \\quad (C_{full})$"}, {"title": "3.3.1. Visible Hyperspectral Range", "content": "The visible hyperspectral range is defined between\n400 nm $\\leq \\lambda \\leq$ 680 nm  $\\quad (C_{visible})$\nThis aligns with the common RGB sensor ranges [3]. We expect the reconstruction of these wavelength channels to be easier because it can be thought of as interpolation between the channels of RGB. While still ill-posed, it is not as severely ill-posed."}, {"title": "3.3.2. Extended Hyperspectral Range", "content": "As visualized in Fig. 1, wavelengths below the blue spectrum fall into the ultraviolet light range ($\\lambda <$ 460 nm), and those above the visible red spectrum fall into the infrared light range ($\\lambda >$ 680 nm). There is limited information about these ranges present in the RGB images used as an information source for reconstruction. Therefore, we define the extended hyperspectral range as follows:\n$\\lambda <$ 400 nm or $\\lambda >$ 680 nm $\\quad (C_{extended})$\nThis categorization highlights the challenges faced in reconstructing hyperspectral data in different wavelength ranges and sets the stage for further analysis of reconstruction methods."}, {"title": "4. Experimental Setup", "content": "For our study, we selected two distinct datasets representing different surgical interventions and data acquisition hardware. We aim to leverage diverse surgical scenarios and varied hardware setups to explore the robustness and applicability of our methods. The datasets chosen offer complementary insights into surgical imaging and enable comprehensive evaluations of our approach. We conducted an investigation into the results obtained by training models on the datasets individually without applying any transfer learning, as shown in table 1. Additionally, we explored the impact of pre-training on one dataset followed by fine-tuning on the other, which is detailed in table 3."}, {"title": "4.1. Datasets", "content": "For our study, we selected two distinct datasets representing different surgical interventions and data acquisition hardware. We aim to leverage diverse surgical scenarios and varied hardware setups to explore the robustness and applicability of our methods. The datasets chosen offer complementary insights into surgical imaging and enable comprehensive evaluations of our approach. We conducted an investigation into the results obtained by training models on the datasets individually without applying any transfer learning, as shown in table 1. Additionally, we explored the impact of pre-training on one dataset followed by fine-tuning on the other, which is detailed in table 3."}, {"title": "4.1.1. MSI Brain", "content": "The MSI Brain dataset is comprised of in-house neurosurgery data captured using a liquid crystal tunable filter system integrated with a Zeiss operating microscope, illuminated by a standard xenon surgical light source. The RGB data, with a resolution of 1920x1080 pixels, covers a comprehensive area of the surgical field. Following registration to hyperspectral imaging (HSI), the RGB data is masked to match the spatial dimensions of the HSI data. The HSI data itself spans a spectral range from 460 nm to 720 nm with a 10 nm sampling resolution, captured by a monochromatic camera with a spatial resolution of 576x768 pixels. Due to the multispectral acquisition lasting several seconds, motion artifacts were encountered, necessitating coregistration of the hyperspectral cubes. This preprocessing step involved initial SIFT registration followed by more precise RANSAC-Flow alignment to ensure high spatial registration accuracy. The dataset of 225"}, {"title": "4.1.2. HeiPorSPECTAL", "content": "The HeiPorSPECTAL dataset [27] comprises HSI data from 20 pigs, acquired at Heidelberg University Hospital. This dataset includes image annotations for 20 different organs, providing a valuable resource for medical imaging research. The acquisition system uses the Tivita\u00ae Tissue HSI camera system (Diaspective Vision GmbH, Am Salzhaff, Germany), capturing a spectral range from 500 nm to 1000 nm without overlaps. The resulting image resolution of each hypercube is 640x480x100 pixel. Corresponding RGB images are reconstructed from the HSI data by aggregating spectral channels that capture red, green, and blue light [16]. As the RGB is reconstructed from the hyperspectral data no registration is necessary between RGB and HSI as they are already aligned. The HSI cubes were further preprocessed with L1 normalization. L1 normalization scales the data so that the sum of the absolute reflectance values for each pixel equals 1, ensuring that each reflectance spectrum is proportionally scaled. We use the official split provided by the authors (Split2) with 827 images for training, 1260 for validation and 1366 for testing."}, {"title": "4.2. Training details", "content": "We trained all our models for 100 epochs with a learning rate of 1 \u00d7 10-4 and chose the best performing model based on the validation set and evaluate the model using the unseen test set. We utilized the Adam optimizer with betas set to (0.9, 0.999), and employed a cosine annealing learning rate scheduler initialized with total_iteration and min = 1 \u00d7 10-6 for training. To enhance data variability and mitigate overfitting to spatial cues, we employed standard geometric augmentation techniques, including resizing to 288 by 480 pixels in the spatial dimension, and applying random flipping, shifting, scaling, and rotation during training.\nFor the loss functions, we used the L1 loss (equivalent to Mean Absolute Error) and the Mean Relative Absolute Error (MRAE) frequently used for spectral reconstruction [5], defined as:\n$LOSSMRAE(Y,Y) = \\frac{1}{N} \\sum_{i=1}^{N} |\\frac{Yi-\\hat{Yi}}{Yi}|$\nwhere yi and \u0177i represent the ground truth and predicted values, respectively, for N samples. The division through the error with the label helps to normalize the error relative to the magnitude of the ground truth values, providing a measure of relative error that is independent of the scale of the data.\nDue to many instances in the MSI Brain dataset where yi equaled zero, the MRAE loss could not be used effectively. Therefore, we opted for the standard L1 loss function for the MSI Brain dataset."}, {"title": "5. Results", "content": "In this section, we present our findings on RGB to hyperspectral reconstruction, both quantitatively across three distinct hyperspectral ranges and qualitatively with examples from both datasets."}, {"title": "5.1. RGB to Hyperspectral Reconstruction Quantitative", "content": "As depicted in table 1, all models were successfully trained and fall within an acceptable range.\nNotably, the straightforward PixelFeatureNet, comprising only four linear layers, performs competitively against the LocalFeatureNet, which incorporates spatial information. On the HeiPorSPECTRAL dataset, the RMSE results show a marginal difference (delta = 0.0001) between them. Both PixelFeatureNet and LocalFeatureNet demonstrate superior performance over UNet on the HeiPorSPECTRAL dataset by 0.0003 and 0.0004, respectively. These findings suggest that elaborate models with numerous parameters do not significantly enhance spectral reconstruction. Simple models, despite their lack of spatial modeling, achieve satisfactory results.\nInterestingly, UNet exhibits the best overall performance on the HeiPorSPECTRAL dataset with an SAM of 0.1258, indicating that higher-level information and bottleneck features may aid in spectral consistency. Furthermore, the transformer-based MST++ model surpasses all others in terms of RMSE, PSNR, and SSIM, underscoring its efficacy in medical hyperspectral imaging. This suggests promising avenues for applying computer vision techniques from general domains like ImageNet to medical applications.\nConversely, on the MSI Brain dataset, PixelFeatureNet performs less favorably compared to other models, achieving the worst results across all metrics with an RMSE of 0.0415, compared to 0.0336 for the second-worst model. UNet, while ranking third, exhibits significantly lower SSIM (17 difference) compared to the top-performing models. LocalFeatureNet surpasses UNet across all metrics, indicating that model complexity can potentially lead to overfitting. Similar to the HeiPorSPECTRAL results, MST++ excels on the MSI Brain dataset, achieving top scores across all metrics. This reinforces the efficacy of transformer-based architectures like MST++ in medical hyperspectral imaging applications."}, {"title": "5.2. Model Size and Performance Comparison", "content": "As depicted in table 1, the number of parameters varies significantly among the different models (1.6K to 32.5M parameters). These differences in parameter counts across the datasets can be attributed to the higher dimensionality of the output layers. Interestingly, the largest model, UNet (32.5M parameters), does not yield the best performance in terms of metrics. Conversely, the smallest model, PixelFeatureNet (1.6K parameters), demonstrates adequate performance despite its limited parameter count. LocalFeatureNet (6.9K/9.3K) strikes a balanced trade-off between parameter size and performance. MST++ achieves the best results while maintaining significantly fewer parameters (416K/5.6M) than UNet, establishing it as the optimal choice overall."}, {"title": "5.3. Reconstruction for different hyperspectral ranges", "content": "As previously mentioned our hypothesis was that the extended hyperspectral range poses a larger challenge. Often these higher near infrared channels are used for the extraction of functional information such as blood oxygenation or perfusion parameters [16]. Therefore the reconstruction of the extended ranges should be high to ensure that any downstream tasks are performed accurately.\nIn table 2, we present a comparison of spectral reconstruction across different wavelength ranges using MAE as the evaluation metric. Our findings indicate that for the Full spectrum, results align closely with those from RMSE, except for the UNet model, which performs second best on the HeiPorSPECTRALDataset. RMSE, being more sensitive to large errors such as outliers, suggests that UNet generally exhibits larger errors despite having a lower MAE. Furthermore, our analysis reveals consistently lower MAE values for the visible range compared to the full range, underscoring the relative ease of reconstructing wavelengths within the visible spectrum. Conversely, reconstruction in the extended range proves more challenging, with MAE values nearly three times higher on the MSI Brain Dataset when compared between visible and extended ranges.\nAcross both datasets, models with lower overall MAE tend to exhibit lower MAE values for visible and extended ranges as well. However, the variability in MAE between visible and extended ranges underscores the importance of analyzing errors across different spectral bands or specific wavelengths, particularly"}, {"title": "5.4. Qualitative Evaluation", "content": "In Fig. 3, we present a qualitative assessment of spectral reconstruction using the MST++ model on the HeiPorSPECTRAL dataset. Positioned at the top left of"}, {"title": "5.5. Generalisation across Datasets", "content": "Pre-training on one dataset and fine-tuning on another using transfer learning can potentially enhance results and provide insights into the generalization of methods across different datasets [30]. This capability is particularly valuable given the limited availability of datasets and the frequent use of pre-training in the literature for RGB. This process is particularly challenging due to the disparity in output channels between the HeiPorSPECTRAL dataset, which has 100 channels, and the MSI Brain dataset, which has 27 channels."}, {"title": "6. Conclusion", "content": "In this study, we have explored various approaches for spectral reconstruction from RGB to hyperspectral images. Our findings underscore several key insights into the effectiveness of different model architectures and strategies for improving reconstruction accuracy.\nFirstly, we demonstrated that integrating spatial information significantly enhances reconstruction quality. Models like MST++ and LocalFeatureNet, which incorporate spatial context into their spectral predictions, consistently outperformed simpler models like PixelFeatureNet. This highlights the importance of leveraging spatial relationships to refine spectral predictions across diverse tissue types and imaging conditions. More parameters not necessarily translates to better reconstruciton as evident from UNet. However, the modeling power of transformers used in MST++ translates to best results validated on different surgical settings on two dataset.\nFurthermore, while our results demonstrate strong performance in reconstructing visible wavelength ranges, we recognize the challenges associated with extending this reconstruction to broader spectral ranges. Absorption spectra typically extend across a wide spectral region [7], therefore it is possible, in principle, to infer information from the IR or UV ranges from responses in the visible. However, the difficulties encountered in accurately capturing spectral information at longer wavelengths highlight the need for further investigation and refinement, especially for extracting functional insights critical to medical diagnostics.\nMoreover, our investigation revealed that spectral reconstruction offers a promising label-free approach for tissue classification in medical imaging. By reconstructing hyperspectral signatures from RGB data, we can exploit spectral fingerprints to classify tissues accurately. This capability not only simplifies data acquisition but also opens avenues for automating diagnostic processes and improving treatment decisions.\nOur results suggest that transfer learning has the potential to enhance spectral reconstruction by pre-training on one spectral dataset and then fine-tuning on a target dataset. We observe varying degrees of effectiveness across different architecture types, with transformers demonstrating the most significant improvements.\nIn conclusion, our study underscores the transformative potential of spectral reconstruction in medical imaging, driven by advancements in spatially informed models and insights into leveraging RGB data for hyperspectral analysis. Future research directions should focus on refining reconstruction techniques to extract comprehensive functional information and further validating these approaches across broader medical applications."}]}