{"title": "Predicting Steady-State Behavior in Complex Networks with Graph Neural Networks", "authors": ["Priodyuti Pradhan", "Amit Reza"], "abstract": "In complex systems, information propagation can be defined as diffused or delocalized, weakly localized, and strongly localized. This study investigates the application of graph neural network models to learn the behavior of a linear dynamical system on networks. A graph convolution and attention-based neural network framework has been developed to identify the steady-state behavior of the linear dynamical system. We reveal that our trained model distinguishes the different states with high accuracy. Furthermore, we have evaluated model performance with real-world data. In addition, to understand the explainability of our model, we provide an analytical derivation for the forward and backward propagation of our framework.", "sections": [{"title": "I. INTRODUCTION", "content": "Relations or interactions are ubiquitous, whether the interaction of power grid generators to provide proper functioning of the power supply over a country, or interactions of bio-molecules inside the cell to the proper functioning of cellular activity, or interactions of neurons inside brains to perform specific functions or interactions among satellites to provide accurate GPS services or interactions among quantum particle enabling quantum communications or the recent coronavirus spread [1-5]. All these systems share two fundamental characteristics: a network structure and information propagation among their components.\nIn complex networks, information propagation can occur in three distinct states diffused or delocalized, weakly localized, and strongly localized [6]. Localization refers to the tendency of information to condense in a single component (strong localization) or a few components (weak localization) of the network instead of information diffusing evenly (delocalization) throughout the network (Fig. 1). Localization or lack of it is particularly significant in solid-state physics and quantum chemistry [7], where the presence or absence of localization influences the properties of molecules and materials. For example, electrons are delocalized in metals, while in insulators, they are localized [7].\nInvestigation of (de) localization behavior of complex networks is important for gaining insight into fundamental network problems such as network centrality measure [8], spectral partitioning [9], development of approximation algorithms [10]. Additionally, it plays a vital role in understanding a wide range of diffusion processes, like criticality in brain networks, epidemic spread, and rumor propagation [11, 12]. These dynamic processes have an impact on how different complex systems evolve or behave [12]. For example, understanding epidemic spread can help in developing strategies to slow its initial transmission, allowing time for vaccine development and deployment [13-17]. The interactions within real-world complex systems are often nonlinear [18]. In some cases, nonlinear systems can be solved by transforming them into linear systems through changing variables. Furthermore, the behavior of nonlinear systems can frequently be approximated by their linear counterparts near fixed points. Therefore, understanding linear systems and their solutions is an essential first step toward comprehending more complex nonlinear dynamical systems [18].\nHere, we develop a Graph Neural Network (GNN) architecture to identify the behavior of linear dynamical states on complex networks. We create datasets where the training labels are derived from the inverse participation ratio (IPR) value of the principal eigenvector (PEV) of the network matrices. The GNN model takes the network structure as input and predicts IPR values, enabling the identification of graphs into their respective linear dynamical states. Our model performs well in identifying different states and is particularly effective across varying-sized networks. A key advantage of using GNN is its ability to train on smaller networks and generalize well to larger ones during testing. We also provide an analytical framework to understand the explainability of our model. Finally, we use real-world data sets in our model."}, {"title": "II. PROBLEM DEFINITION", "content": "We consider a linear dynamical process, D takes place on unknown network structures represented as G = {V, E} where V = {V1, V2, ..., Vn } is the set of vertices (nodes), E = {(Vi, vj)|Vi, vj \u2208 V} is the set of edges (connections). The degree of a node i in an unweighted graph is the number of nodes adjacent to it, which is given by $\\sum_{j=1} a_{ij}$ where $a_{ij}$ is the adjacency matrix element. The links in G represent dynamic interactions whose nature depends on context. For instance, in a social system, aij = 1 captures a potentially infectious interaction between individuals i\nand j [19], whereas, in a rumor-propagation network, it may reflect a human interaction for spreading information. To account for these dynamic distinctions, we denote the activity of each node as $x_i(t)$, which quantifies individual i's probability of infection or rumor propagation. We can track the linear dynamics of node i via\n$\\frac{dx_i(t)}{dt} = ax_i(t) + \\beta \\sum_{j=1}^n A_{ij} x_j(t)$ (1)\nwhere $x_i(t)$ is the self-dynamic term, the second term captures the neighboring interactions at time t, and a, \u03b2 are the model parameters of the linear dynamical system. In matrix notation, we can express Eq. (1) as\n$\\frac{dx(t)}{dt} = Mx(t)$ (2)\nwhere x(t) = $(x_1(t),x_2(t),...,x_n(t))^T$, M = \u03b1\u0399 + \u03b2A is the transition, A is the adjacency, and I is the identity matrices, respectively. If x(0) is the initial state of the system, the long-term (steady state) behavior (x*) of the linear dynamical system can be found as\n$x(t) = e^{Mt}x(0) \\sim u^M$ (3)\nwhere $u^M$ is the PEV of M (Appendix A). Further, if we multiply both side of M = I + BA by eigenvectors of A i.e., $u^A$, we get\n$Mu^A = [\\alpha + \\beta \\lambda ] u^A= \\lambda^M u^A$\nWe can observe that eigenvectors of M are the same as eigenvectors of A where $M=a + \\beta \\lambda A$ [11]. Thus,\n$x^* \\sim u^M = u^A$\nTherefore, understanding the long-term behavior of the information flow pattern for linear dynamical systems is enough to understand the behavior of PEV of the adjacency matrix. Further, the behavior of PEV for an adjacency matrix depends on the structure of the network (A = UAUT). Hence, we study the relationship between network structure and the behavior of PEV, leading to understanding the behavior of the steady state of linear dynamics.\nWe quantify the (de) localization behavior of the steady-state (x*) or the PEV (u = uA) using the inverse participation ratio ($\\gamma_{x^*}$), which is the sum of the fourth power of the state vector entries and calculate as [20]\n$\\gamma_{x^*} = \\frac{\\sum_{i=1}^n x_i^{*4}}{(\\sum_{i=1}^n x_i^{*2})^2}$ (4)\nwhere $x_i^*$ is the ith component of x* = $(x_1, x_2,...,x_n^*)^T$ and $\\sum_{i=1}^n x_i^{*2} = 1$ is the normalization term. A vector with component (c, c, . . ., c) is delocalized and has $\\gamma_{x^*} = \\frac{1}{n}$ for some positive constant c > 0, whereas the vector with components (1,0,...,0) yields $\\gamma_{x^*} = 1$ and referred as most localized. Furthermore, we consider the networks to be simple, connected and undirected. Hence, some information can easily propagate from one node to another and we never get a steady-state vector of the form x* = (1,0,..., 0) for a connected network, and thus the IPR value lies between $\\frac{1}{n} < \\gamma_{x^*} < 1$. Therefore, the localization-delocalization behavior of the linear dynamics in the network is quantified using a real value, i.e., each graph associates an IPR value $\\in$ [1/n, 1). Now, to identify the states (x*) belong to which category of dynamical behavior for linear dynamics, we formalize a"}, {"title": "III. METHODOLOGY AND RESULTS", "content": "threshold scheme for identifying IPR values (y = $\\gamma_{x^*}$) lies in the range [1/n, 1). We define two thresholds, T\u2081 and T2,\nsuch that 1/n < T1 < T2 < 1. An additional parameter e\n(\u20ac > 0) defines some flexibility around the thresholds.\nDelocalized region (r1). IPR values significantly below\nthe first threshold, including an e-width around 71:\nr\u2081 = {y \u2208 [1/n, 1) | y \u2264 T\u2081 \u2013 \u0454}\nWeakly localized region (r2). IPR values around and\nbetween the two thresholds, including e-width around 71\nand T2:\nr2 = {y \u2208 [1/n, 1) | T\u2081 - \u0454 < Y < T2 + \u0454}\nStrongly localized region (r3). IPR values significantly\nabove the second threshold, including an e-width around\nT2:\nr\u2083 = {y \u2208 [1/n, 1) | y \u2265 T2 + \u0454}\nThe regions can be defined using a piece-wise function:\nr(y, T\u2081, T\u2082, \u0454) = $\\begin{cases} 1 & \\text{if } y\u2264 T\u2081-\u0454 \\\\ 2 & \\text{if } T\u2081-\u0454 < Y < T\u2082 + \u0454 \\\\ 3 & \\text{if } y\u2265 T\u2082 + \u0454 \\end{cases}$ (5)\nFor instance, we consider a set of threshold values as\nT\u2081 = 0.05, T2 = 0.2, \u20ac = 1e - 6. Now, if we consider\na regular network (each node have the same degree) of\nn nodes, we have PEV, $u_r = (\\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}}...\\frac{1}{\\sqrt{n}})$ of A\n(Theorem 6 [22]) yielding, $\\gamma_{ur} = \\frac{1}{n}$, thus $\\gamma_{ur}$ \u2192 0\nas n\u2192\u221e. On the other hand, for a star graph\nhaving n nodes, $u_s = (\\frac{1}{n}, \\frac{2(\\sqrt{2} - 1)}{n}, ..., \\frac{2(\\sqrt{2} - 1)}{n})$\nand, $\\gamma_{us} = \\frac{4(n-1)(\\sqrt{2} - 1)^4 + 1}{n^2(\\frac{2(\\sqrt{2} - 1)}{n})^2 + \\frac{1}{n^2})}$. Hence, for n \u2192 \u221e, we get yus \u2248 0.25,\nand PEV is strongly localized for the star networks.\nFurther, it is also difficult to find a closed functional form of PEV for any network, and thereby, it is hard to find the IPR value analytically. For instance, in Erd\u00f6s-R\u00e9nyi (ER) random networks, we get a delocalized PEV due to each node having the same expected degree [23]. In contrast, the presence of power-law degree distribution for SF networks leads to some localization in the PEV. For SF networks, the IPR value, while being larger than the ER random networks, is much lesser than the star networks [24]. It may seem that when the network structure is close to regular, linear dynamics are delocalized, and increasing degree heterogeneity increases the localization. However, always looking at the degree heterogeneity not able to decide localization and analyzing structural and spectral properties is essential [11, 20]. A fundamental question at the core of the structural-dynamical relation is: Can we predict the steady state behavior of a linear dynamical process in complex networks?\nHere, we formulate the problem as a graph regression task to predict a target value, IPR, associated with each graph structure. For a given set of graphs {$G_i$}$_{i=1}^N$ where each Gi = (Vi, Ei) consists of a set of nodes Vi and a set of edges Ei such that ni = |Vi| and mi = |Ei|. We represent each $G_i$ using its adjacency matrix $A^{(i)}$. Further, each graph $G_i$ has an associated target value, i.e., IPR value, $y^{(i)} \\in \\mathbb{R}$. For each node v \u2208 Vi in $G_i$, there is an associated feature vector $h_v \\in \\mathbb{R}^d$ and $H^{(i)} \\in \\mathbb{R}^{|V_i|xd}$ be the node feature matrix where $h_j^i$ is the jth row. The objective is to learn a function f : G \u2192 R, such that f($G_i$) \u2248 $y^{(i)}$ for the given set of N graphs.\nThe function f can be parameterized by a model, in our case, Graph Convolutional Networks (GCN) and Graph"}, {"title": "A. GCN Architecture", "content": "The GCN architecture comprises three graph convolu- tional layers, each followed by a ReLU activation function. After that, a readout layer performs mean pooling to aggregate node features into a single graph representation. Finally, we use a fully connected layer that outputs the scalar IPR value for the regression task (Fig. 2). A brief description of the architecture is provided below.\nInput Layer: The input layer recieves a normalized adjacency (A) and initial node feature $H^{(0)}$ matrices.\nGraph Convolution Layers: We stack three graph convolution layers (Eq. 7) to capture local and higher- order neighborhood information of a node. After each graph convolution layer, we apply nonlinear activation functions (ReLU). Each layer uses the node representations from the previous layer to compute updated representations in the current layer. The first layer of GCN facilitates information flow between first- order neighbors (Fig. 3(a)), while the second layer aggregates information from the second-order neighbors, i.e., the neighbors of a node's neighbors (Fig. 3(b)), and this process continues for subsequent layers (Fig. 3(c)) and we get\n$H^{(l)} = \\sigma(A H^{(l-1)} W^{(l-1)}), l = {1,2,3}$ (7)\nwhere $H^{(0)} \\in \\mathbb{R}^{n \\times d}$ is the initial input feature matrix, and $W^{(0)} \\in \\mathbb{R}^{d \\times k_0}$, $W^{(1)} \\in \\mathbb{R}^{k_0 \\times k_1}$, $W^{(2)} \\in \\mathbb{R}^{k_1 \\times k_2}$ are the weight matrices for the first, second, and third layers, respectively. Hence, $H^{(1)} \\in \\mathbb{R}^{n\\times k_0}$, and $H^{(2)} \\in \\mathbb{R}^{n \\times k_1}$ are\nthe intermediate node feature matrices and after three layers of graph convolution, final output node features are represented as $H^{(3)} \\in \\mathbb{R}^{n \\times k_2}$.\nReadout Layer: For the scalar value regression task over a set of graphs, we incorporate a readout layer to aggregate all node features into a single graph-level representation (Fig. 3(d)). We use mean pooling as a readout function,\n$z = READOUT(H^{(3)}) = \\frac{1}{n} \\sum_{j=1}^{n} h_j^{(3)}$ (3)\nwhere $h_j^{(3)} \\in \\mathbb{R}^{1\\times k_2}$ is the jth row of $H^{(3)} \\in \\mathbb{R}^{n\\times k_2}$ and $z \\in \\mathbb{R}^{k_2}$. Finally, we pass it to a linear part of the basic neural network (Fig. 3(e)) to perform regression task over graphs and output the predicted IPR value as\n$\\hat{y} = z W^{(lin)} + b$\nwhere $W^{(lin)} \\in \\mathbb{R}^{k_2\\times 1}$ is the weight matrix and $b \\in \\mathbb{R}$ is the bias value. For our architecture,\n$\\theta = {W^{(0)}, W^{(1)}, W^{(2)}, W^{(lin)},b}$. After getting the predicted IPR value, we use the threshold scheme (Eq. 5) to identify the steady state behavior on complex networks."}, {"title": "B. Data Sets Preparation", "content": "For the regression task, we create the graph data sets by combining delocalized, weakly localized, and strongly localized network structures, which include star, wheel, path, cycle, and random graph models as Erd\u0151s-R\u00e9nyi (ER) and the Scale-free (SF) networks (Appendix D). As predictions of the GNN model are independent of network size, we vary the size of the networks during dataset creation and store them as edge lists ($A^{(i)}$). For each network, we calculate the IPR value from the principal eigenvector (PEV) and assign it as the target value ($y^{(i)}$) to $G_i$ in the datasets. Since we do not have predefined node features for the networks, and the GCN framework requires node features as input"}, {"title": "C. Training and Testing Strategy", "content": "During the training, the GCN model initializes the model parameters (\u03b8). We initialize $W^{(l-1)}$ at random using the initialization described in Glorot & Bengio (2010) [28]. During the forward pass, for each graph $G_i$, we compute the graph representation using the GCN layers, which involves message passing and aggregation of node features (Fig. (3)). Finally, the model predicts the target value $\\hat{y}^{(i)}$. Further, the model computes the loss L(\u03b8) using the predicted ($\\hat{y}^{(i)}$) and true target ($y^{(i)}$) values. During the backward pass, the model computes the gradients of the loss with respect to the model parameters. In the next step, the model updates the parameters using an optimization algorithm such as Adam with a learning rate of 0.01 and a weight decay of 5e-4. We repeat the forward propagation, loss computation, backward propagation, and parameter update steps until convergence. For our experiment, we choose d = 7 features for each node and weight matrix sizes as $k_0 = k_1 = k_2 = 64$ in different layers.\nWe start by creating a simple experimental setup where the input dataset contains only two different types of model networks. One type of network (cycle) is associated with delocalized steady-state behavior, and another (star) is in the strongly localized behavior. During the training phase, we send the edge list ($A^{(i)}$), node feature matrix ($H^{(0,i)}$), and IPR values ($y^{(i)}$) associated with the graphs as labels for the regression task. Once the model is trained, one can observe that the GCN model accurately predicts the IPR value for the two different types of networks (Fig. 4(a)). More importantly, we train the model with smaller-size networks ($n_i$ = 200 to 300) and test it with large-size networks ($n_i$ = 400 to 500) and training datasets contains $N_{train}$ = 1000 networks and testing datasets size as $N_{test}$ = 500. Thus, the training cost would be less, and it can easily handle large networks. Furthermore, for the expressivity of the model, we increase the datasets by incorporating two more different types of graphs (path and wheel graphs), where one is delocalized and the other is in strongly localized structures, and we trained the model. We repeat the process by sending the datasets for the regression task to our model and observing that the model provides good accuracy for the test data sets (Fig. 4(b)). Finally, we apply the threshold function (Eq. 5) on the predicted values and achieve very high accuracy in identifying the dynamic state during the testing (Fig. 4(c, d)). One can observe that the GCN model learns the IPR value well for the above network structures.\nWe move further and incorporate random graph structures (ER and scale-free random networks) in the data sets. Note that the ER random graph belongs to the delocalized state, and SF belongs to both the delocalized and weakly localized state. We train the model with only the SF networks, and during the testing time, one can observe accuracy is not good (Fig. 5(a)). To resolve this, we changed the model and the parameters. We choose the Graph Attention network [29], update the loss function by considering the log value, and choose AdamW optimizer instead of Adam. We also use a dropout rate of 0.6 and set learning rare to le - 5 in the model. The new setup leads to improvement in the results (Fig. 5(b)). Now, we consider both ER and SF networks and train the model, and during the testing time, we can observe good accuracy in predicting the IPR values (Fig. 5(c)).\nThe performance of GCN and GAT in identifying various dynamic states in model networks is highly accurate. GCN is particularly effective in distinguishing between strongly localized and delocalized states (Fig. 4), while GAT excels at differentiating weakly localized and delocalized states (Fig. 5(c)). Although trained models reliably predict states in model networks, applying them to real-world data presents challenges due to imbalanced state distribution and limited dataset size in the r\u2081 and r3 regions (Fig. 6). To assess real-world applicability, we trained the GAT model on real-world data sets and achieved reasonable accuracy on test datasets (Fig. 6(a- c))."}, {"title": "D. Insights of Training Process", "content": "To understand the explainability of our model, we provide mathematical insights into the training process via forward and backward propagation to predict the IPR value. Our derivation offers an understanding of the updation of weight matrices. We perform the analysis with a single GCN layer, a readout layer, and a linear layer for simplicity. However, our framework can easily be extended to more layers.\nForward Propagation:\nGCN Layer: $H^{(1,i)} = \\sigma(A^{(i)}H^{(0,i)}W)$\nReadout Layer: $z^{(i)} = \\frac{1}{n_i} \\sum_{j=1}^{N_i} h_j^{(1,i)}$\nLinear Layer: $\\hat{y}^{(i)} = z^{(i)}W^{(lin)} + b$\nLoss function: $L = \\sum_{i=1}^{N}(y^{(i)} \u2013 \\hat{y}^{(i)})^2$\nIn the above, $A^{(i)}$ is the normalized adjacency matrix,\n$H^{(0,i)}$ is the initial feature matrix, and W is the learnable weight matrix. Further, $h_j^{(1,i)} = \\sigma(\\sum_{k=1}^{N_i} A_{jk}^{(i)} h_k^{(0,i)} W)$\nis the feature vector of node j in graph i and jth row of updated feature matrix $H^{(1,i)}$ (Example 1). Further, $W^{(lin)}$ and b are the learnable weights of the linear layer. Finally, $y^{(i)}$ is the true scalar value for $G_i$ and $\\hat{y}^{(i)}$ is the predicted IPR value.\nBackward Propagation: To compute the gradients to update the weight matrices, we apply the chain rule to propagate the error from the output layer back through the network layers. We calculate the gradient of loss with respect to the output of the linear layer as\n$\\frac{\\partial L}{\\partial \\hat{y}^{(i)}} = \\frac{2}{N} (\\hat{y}^{(i)} \u2013 y^{(i)})$ (8)\nWe calculate the gradients for the linear layer. We know that each graph i contributes to the overall loss L. Therefore, we accumulate the gradient contributions"}, {"title": "IV. CONCLUSION", "content": "Using the graph neural network, we introduce a framework to predict the localized and delocalized states of complex unknown networks. We focus on leveraging the rich information embedded in network structures and extracting relevant features for graph regression. Specifically, a GCN model is employed to predict inverse participation ratio values for unknown networks and consequently identify localized or delocalized states. Our approach provides a graph neural network alternative to the traditional principal eigenvalue analysis [20] for understanding the behavior of linear dynamical processes in real-world systems at steady state. This method offers near real-time insight into the structural properties of underlying networks. A key advantage of the proposed framework is its ability to train on small networks and generalize to larger networks, achieving an accuracy of nearly ~ 100% with test unseen model networks to understand delocalized or strongly localized states. This makes the model scale-invariant, with the computational cost of state prediction remaining consistent regardless of network size, apart from the cost of reading the network data.\nOur trained GNN framework (e.g., GCN and GAT) effectively identifies three different states in unseen test model network data. Moreover, our model accurately identifies the states in the weakly localized regions for the real-world data. However, distinguishing between delocalized weakly localized and strongly localized states associated with real-world graphs poses a significant challenge. It might be due to the imbalance of data points in different states and limited dataset availability. Moving forward, we aim to address these challenges and improve identification accuracy."}, {"title": "A. Linear Dynamics", "content": "We can write Eq. (1) in matrix form as\n$\\frac{dx(t)}{dt} = Mx(t)$ (15)\nwhere M is a transition matrix given by M = \u03b1\u0399 + \u03b2\u0391, where I is the identity matrix. Note that M and A only differ by constant term. Hence,\nx(t) = $e^{Mt}x(0)$ (16)\nWe consider $M \\in \\mathbb{R}^{n \\times n}$ is diagonalizable, M = UAU-1 and UU-1 = I where columns of U are the"}, {"title": "B. Mathematical Insights of Graph Convolution Neural Network", "content": "Deep Learning models, for example, Convolutional Neural Networks (CNN), require an input of a specific size and cannot handle graphs and other irregularly structured data [31]. Graph Convolution Networks (GCN) are exclusively designed to handle graph-structured data and are preferred over Convolutional Neural Networks (CNN) when dealing with non-Euclidean data. The GCN architecture draws on the same way as CNN but redefines it for the graph domain. Graphs can be considered a generalization of images, with each node representing a pixel connected to eight (or four) other pixels on either side. For images, the graph convolution layer also aims to capture neighborhood information for graph nodes. GCN can handle graphs of various sizes and shapes, which increases its applicability in diverse research domains.\nThe simplest GNN operators prescribed by Kipf et al. are called GCN [32]. The convolutional layers are used to obtain the aggregate information from a node's neighbors to update its feature representation. We consider the feature vector as $h_i^{(l-1)}$ of node i at layer l - 1 and update the feature vector of node i at layer l, as\n$h_i^{(l)} = \\sigma( \\sum_{j\\in N(i) \\cup {i}} \\frac{1}{\\sqrt{d_i d_j}} h_j^{(l-1)} W^{(l-1)} )$, (20)\nwhere new feature vector $h_i^{(l)}$ for node i has been created as an aggregation of feature vector $h_i^{(l-1)}$ and the feature vectors of its neighbors $h_j^{(l-1)}$ of the previous layer, each weighted by the corresponding entry in the normalized adjacency matrix (\u00c2), and then transformed by the weight matrix $W^{(l-1)}$ and passed through the activation function \u03c3. We use the ReLU activation function for our work.\nThe sum $\\sum_{j\\in N(i) \\cup {i}}$ aggregates the feature informa- tion from the neighboring nodes and the node itself where N(i) is the set of neighbors of node i. The normaliza- tion factor $\\frac{1}{\\sqrt{d_i d_j}}$ ensures that the feature vectors from neighbors are appropriately scaled based on the node degrees, preventing issues related to scale differences in higher vs. lower degree nodes where di and dj being the normalized degrees of nodes i and j, respectively [33]. The weight matrix $W^{(l-1)}$ transforms the aggregated feature vectors, allowing the GCN to learn meaningful representa- tions. The activation function introduces non-linearity, enabling the model to capture complex patterns.\nSingle convolution layer representation: The operation on a single graph convolution layer can be defined using matrix notation as follows:\n$H^{(l)} = \\sigma(A H^{(l-1)} W^{(l-1)})$\n$= \\sigma( (D^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}) H^{(l-1)} W^{(l-1)} )$\nwhere H(l-1) is the matrix of node features at layer l - 1 where l = 1,2,3, with H(0) being the input feature matrix. Here, $\\tilde{A} = A + I$ is the self-looped adjacency matrix by adding the identity matrix I to the adjacency matrix A. After that we do symmetric normalization by inverse square degree matrix with A and denoted as $\\tilde{A} = D^{-\\frac{1}{2}} \\tilde{A} D^{-\\frac{1}{2}}$, where D \u2208 R^{n \\times n} is the diagonal degree matrix of A with $D_{ii} = \\sum_{j=1}^{n} A_{ij}$. Here, $W^{(l)} \\in \\mathbb{R}^{F_{in} \\times F_{out}}$ is a trainable weight matrix of layer l. A linear feature transformation is applied to the node feature matrix by HW, mapping the Fin feature channels to Fout channels in the next layer. The weights of W are shared among all vertices. We use the Glorot (Xavier) initialization that initializes the weights by drawing from a distribution with zero mean and a specific variance [28]. It helps maintain the variance of the activations and"}, {"title": "C. Mathematical Insights of Graph Attention Network", "content": "Graph Attention Networks (GATs) are an extension of Graph Convolutional Networks (GCNs) that introduce attention mechanisms to improve message passing in graph neural networks [29]. The key advantage of GAT is that it assigns different importance (attention) to different neighbors, making it more flexible and powerful than traditional GCNs, which use fixed aggregation weights. In a standard GCN, node embeddings are updated by aggregating information from neighboring nodes using fixed weights derived from the adjacency matrix. In a GAT, an attention mechanism is used to dynamically compute different weights for each neighbor, allowing the network to focus more on important neighbors. In GAT, each node feature vector (hi) is transformed into a higher-dimensional representation using a learnable weight matrix W as\n$h' = W h_i$\nwhere W \u2208 RF\u2032\u00d7F is a learnable weight matrix, and F\u2032 is the new feature dimension. Further, for each edge (i, j) \u2208 E, compute the attention score eij, which measures the importance of node j 's features to node i. The attention score is calculated as\n$e_{ij} = LeakyReLU(a^T [Wh_i || Wh_j ])$"}, {"title": "D. Complex Networks", "content": "We prepare the datasets for our experiments using several model networks (cycle, path, star, wheel, ER, and scale-free networks) and their associated IPR values. A few models (ER and scale-free networks) are randomly generated. The ER random network is denoted by GER(n, p) where n is the number of nodes and p is the edge probability [35]. The existence of each edge is statistically independent of all other edges. Starting with n number of nodes, connecting them with a probability p = (k)/n where (k) is the mean degree. The ER random network realization will have a Binomial degree distribution [35]. The SF networks (GSF) generated using the Barab\u00e1si-Albert model follows a power-law degree distribution [35]."}]}