{"title": "LIMITATIONS OF LARGE LANGUAGE MODELS IN CLINICAL\nPROBLEM-SOLVING ARISING FROM INFLEXIBLE REASONING", "authors": ["Jonathan Kim", "Anna Podlasek", "Kie Shidara", "Feng Liu", "Ahmed Alaa", "Danilo Bernardo"], "abstract": "Large Language Models (LLMs) have attained human-level accuracy on medical question-\nanswer (QA) benchmarks. However, their limitations in navigating open-ended clinical\nscenarios have recently been shown, raising concerns about the robustness and generalizability\nof LLM reasoning across diverse, real-world medical tasks. To probe potential LLM failure\nmodes in clinical problem-solving, we present the medical abstraction and reasoning corpus\n(M-ARC). M-ARC assesses clinical reasoning through scenarios designed to exploit the\nEinstellung effect the fixation of thought arising from prior experience, targeting LLM\ninductive biases toward inflexible pattern matching from their training data rather than\nengaging in flexible reasoning. We find that LLMs, including current state-of-the-art o1 and\nGemini models, perform poorly compared to physicians on M-ARC, often demonstrating lack\nof commonsense medical reasoning and a propensity to hallucinate. In addition, uncertainty\nestimation analyses indicate that LLMs exhibit overconfidence in their answers, despite\ntheir limited accuracy. The failure modes revealed by M-ARC in LLM medical reasoning\nunderscore the need to exercise caution when deploying these models in clinical settings.", "sections": [{"title": "Introduction", "content": "The versatility and strong performance of Large Language Models (LLMs) across multiple domains [1] have\nsparked investigation of their reasoning capabilities in clinical contexts [2]. LLMs have demonstrated high\naccuracy on the United States Medical Licensing Exam (USMLE)[3], USMLE-styled question banks[4, 5, 6],\nsubspecialty medical board examinations [7, 8], and clinical reasoning benchmarks validated for physicians [9].\nExcellent LLM performance across multiple domains in medical question and answer (QA) benchmarks\nhas been postulated, in part, to reflect emergent reasoning capabilities[10, 11]. While LLM performance\non medical QA benchmarks has been demonstrated to rival human-level performance, their capabilities in"}, {"title": "Methods", "content": "Here, we introduce the Medical Abstraction and Reasoning Corpus (M-ARC) benchmark, which utilizes an\nadversarial framework to probe failure modes potentially linked to inflexibility in LLM reasoning. These\nvulnerabilities may arise from habituation to fixed problem-solving approaches such as rote pattern match-\ning and inherent inflexibility to move beyond these familiar reasoning patterns, limitations imposed by\nneural architecture and training regimes. This mechanized or rigid mode of reasoning in humans, when\ncounterproductive in novel situations requiring flexible reasoning, is known as the Einstellung effect-a\ncognitive bias where rigidity of thought arises from prior experience [26]. This effect arises when a habitual\nproblem-solving strategy, activated by familiar problem features, hinders reasoning towards the optimal\nsolution [27]. M-ARC alters predictable aspects of medical problems, emphasizing 'long-tail' or low-probability\nreasoning patterns underrepresented in medical texts and QA benchmarks (Figure 1) to induce this effect.\nOur findings demonstrate that current LLMs perform poorly on M-ARC, indicating surprising failure modes\nin clinical reasoning. These shortcomings are further compounded by their overconfidence in their outputs\ndespite their limited performance."}, {"title": "M-ARC Question Design", "content": "M-ARC questions are modeled after the multiple-choice format used by the United States Medical Licensing\nExamination (USMLE). The dataset comprises 100 questions written by the authors to resist memorization,\npattern matching, or interpolation from preexisting medical QA benchmarks and medical texts. Figure 1\ndemonstrates aspects of the adversarial framework used by M-ARC. The format is reminiscent of a commonly\nseen medical QA text pattern (anticoagulant leading to a brain bleed), and the answer choices include an\nadversarial choice specifically designed to exploit reliance on rote pattern matching, leveraging the Einstellung\neffect the fixation of thought arising from prior experience [27, 26]. The frequently-encountered answer choice\nhere (matching the familiar text pattern in the problem) is adversarial and one that may be avoided with\ndeductive reasoning through logical negation-the complete absence of a brain renders a brain bleed impossible.\nThis clinical situation represents a long-tail reasoning pattern, which is unlikely to be encountered in medical\ntexts, thereby making the optimal answer choice more likely to be obscured by the Einstellung effect. The\ncorpus further incorporates open-ended styled answers, which are underrepresented in conventional medical\nQA benchmarks. These answers assess the ability to evaluate whether the available information suffices for\ndiagnostic or therapeutic decision-making or if additional information is needed. Clinicians frequently use\nopen-ended reasoning patterns to decide when a therapeutic or diagnostic threshold has been reached [23];\nhowever, open-ended reasoning pattern has received little attention in medical QA benchmarks. The recently\nintroduced MEDIQ dataset demonstrated that LLMs have poor performance in proactively seeking missing\ninformation within simulated interactive settings mimicking real-world clinical settings with incomplete initial"}, {"title": "Analysis", "content": "We compared LLM performance to physician performance on M-ARC. Physician test takers were recruited\nfor this study from the University of California San Francisco (UCSF) Medical Center and kolabtree.com.\nEthical approval for this study was obtained from the UCSF Institutional Review Board (IRB). The M-ARC\naccuracies of five physicians were averaged for the reported average human physician performance. The\nMassive Multitask Language Understanding (MMLU) dataset was used for chain of thought prompting in\nin-context learning examples[28]. This approach followed the methodology outlined by Wang et al. and\nutilized their publicly available code from the MMLU-Pro benchmark assessment [29]. The accuracy of\nGPT-40[30], o1[31], Medalpaca[32], Meditron-7b[33], Claude-Sonnet, Claude-Opus [34], Google Gemini[35],\nand Mistral [36] models were evaluated. Closed source models were evaluated using the respective APIs from\nAnthropic, Google, and OpenAI. Open-source models were evaluated using Huggingface and Lambda Labs\nAPIs. The latest versions of publicly available models were utilized with a model cut-off date of December 19,\n2024. A temperature of zero was used when possible to allow for reproducibility of the results; otherwise,\nsettings followed the defaults used by Wang et al in the MMLU-pro benchmark[29]. The full parameter\nsettings that were utilized are available in the shared code-base.\nMeasuring consistency in model output across multiple runs is an established method for uncertainty estimation\nin LLMs[37, 38] and has been shown to outperform posthoc methods at uncertainty estimation [39]. Following"}, {"title": "Results", "content": null}, {"title": "LLMs performance on M-ARC tasks", "content": "We observed that most LLMs perform poorly on M-ARC tasks, with less than 50% accuracy (Figure 2).\nWe note that several models performed near or below the chance level (less than 20%). Human average\nperformance was 66%, averaged across five physicians, with standard error \u00b15.3%. All model accuracies are\nprovided in Supplementary Table 1. We observed a general trend for improvement with larger model sizes in\nthe Claude, Gemini, and OpenAI families. Tangential reasoning patterns were found in the Medalpaca and\nMeditron3 models (Supplementary Tables 2 and 3), contributing to their poor performance. This contrasts to\ntheir moderate performance on conventional medical QA benchmarks[33, 32]. The best-performing models,\nGemini(v1.5-pro) and o1, achieved accuracies of 50% and 48%, respectively. However, even these models\nexhibited hallucinations and committed commonsense reasoning errors, as illustrated in question example\nresponses below."}, {"title": "Examples of M-ARC Questions", "content": null}, {"title": "Uncertainty Estimation and Calibration", "content": "The shortcomings of LLMs in medical reasoning and propensity to hallucinate, as demonstrated here,\naligns with prior work demonstrating similar limitations across various domains [42, 16, 17, 18, 19, 21]\nand raises concerns regarding their trustworthiness in medical contexts[43, 44]. Uncertainty estimation\nhas emerged as a method to potentially mitigate overreliance on LLM by quantifying confidence in their\noutputs, thereby allowing users to gauge their trustworthiness[40]. Here, we utilized both agreement- and\nentropy-based consistency to calculate the Brier score to compare LLM confidence, as SC has been identified\nto outperform other uncertainty estimation methods in the medical domain [40]. Agreement-based Brier\nscores and reliability plots for the top-performing models (ol, Gemini-pro, Claude-opus, and Llama-3.1-70b)\ndemonstrated overconfidence in their responses despite exhibiting low accuracy (Figure 6 and Supplementary\nFigure 2), a finding also supported by entropy-based consistency (Supplementary Figures 2 and 3). We\nfound that smaller models such as GPT40-mini, Gemini-1.5-flash, and Claude-Sonnet had even greater\noverconfidence despite achieving lower accuracy. Larger models demonstrated improved calibration compared\nto smaller models; however, they remained overconfident despite limited accuracy. These results align with\nrecent findings highlighting a mismatch between LLM uncertainty and their actual capabilities in medical\nreasoning tasks[25]."}, {"title": "Discussion", "content": "Considering that the progression of AI development has continually drawn on insights from human cognition [45,\n46, 47] and that LLM training is reliant on extensive human-generated data, it is anticipated that LLMS"}]}