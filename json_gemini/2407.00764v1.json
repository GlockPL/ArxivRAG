{"title": "Characterizing Stereotypical Bias from Privacy-preserving Pre-Training", "authors": ["Stefan Arnold", "Rene Gr\u00f6bner", "Annika Schreiner"], "abstract": "Differential Privacy (DP) can be applied to raw text by exploiting the spatial arrangement of words in an embedding space. We investigate the implications of such text privatization on Language Models (LMs) and their tendency towards stereotypical associations. Since previous studies documented that linguistic proficiency correlates with stereotypical bias, one could assume that techniques for text privatization, which are known to degrade language modeling capabilities, would cancel out undesirable biases. By testing BERT models trained on texts containing biased statements primed with varying degrees of privacy, our study reveals that while stereotypical bias generally diminishes when privacy is tightened, text privatization does not uniformly equate to diminishing bias across all social domains. This highlights the need for careful diagnosis of bias in LMs that undergo text privatization.", "sections": [{"title": "1 Introduction", "content": "Language Models (LMs) (Devlin et al., 2019; Radford et al., 2019) are trained on large corpora of text that may contain confidential information. Since such information can be recovered from word embeddings (Song and Raghunathan, 2020; Thomas et al., 2020) and language models (Carlini et al., 2019; Nasr et al., 2023), privacy emerged as an active concern for building trust and complying with stringent regulations on privacy protection.\nTo protect against unintended disclosure of information, Differential Privacy (DP) (Dwork et al., 2006) has been integrated into machine learning (Abadi et al., 2016) and language models (McCann et al., 2017; Shi et al., 2022; Du et al., 2023). DP formalizes privacy through a notion of indistinguishability so that the model outputs are not affected by the addition or removal of an entry in the training corpus. This is accomplished by injecting additive noise on gradients during model training.\nDue to scaling issues associated with DP on LMs during perturbation of per-sample gradient updates (Abadi et al., 2016), there is a trend towards perturbing the raw text (Fernandes et al., 2019; Feyisetan et al., 2020; Yue et al., 2021; Chen et al., 2023).\nBy exploiting the geometric proximity of words in word embeddings (Mikolov et al., 2013), Feyisetan et al. (2020) proposed a probabilistic mechanism grounded in metric DP (Chatzikokolakis et al., 2013) to perturb all words in a text while ensuring plausible deniability (Bindschaedler et al., 2017) of the text regarding its provenance and content.\nHowever, several studies documented that mechanisms for embedding words in a high-dimensional space harbor (Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018; Manzini et al., 2019) and transfer (Papakyriakopoulos et al., 2020) unwanted stereotypes and prejudices present in a text corpus.\nContribution. Building on the rich body of research exploring privacy-fairness trade-offs (Bagdasaryan et al., 2019; Farrand et al., 2020; Hansen et al., 2022), this study addresses the implications of text privatization on biased associations in LMs. Specifically, we pre-train BERT (Devlin et al., 2019) models with masked language modeling and next sentence prediction on webscraped text modified under varying levels of privacy. We then score the stereotypical bias following the context association test of Nadeem et al. (2021) and stereotype pairs benchmark of Nangia et al. (2020). Our findings reveal a nuanced landscape where stereotypical bias generally diminishes as privacy guarantees are tightened. This is in line with prior research indicating that LMs with impaired language modeling capabilities tend to exhibit less stereotypical associations (Nadeem et al., 2021). However, this diminution is not uniform across all social categories as biases associated with certain attributes show varying trends of stability, amplification, and attenuation. We thus advocate for careful bias measurement when deploying privacy-preserving LMs."}, {"title": "2 Background", "content": "To ensure a consistent understanding of privacy and fairness in machine learning, we provide the foundations of differential privacy and a brief definition of stereotypical bias along with related work."}, {"title": "2.1 Differential Privacy", "content": "Differential Privacy (DP) (Dwork et al., 2006) originated in the field of statistical databases and was adapted to machine learning (Abadi et al., 2016). DP formalizes privacy through the indistinguishability of model outputs with respect to the presence or absence of a record in the dataset. The notion of indistinguishability is achieved through noise and can be controlled by the privacy budget \\( \\epsilon \\in (0,\\infty]\\), with privacy guarantees diminishing as \\( \\epsilon \\rightarrow \\infty\\).\nDespite evidence of preventing information disclosure, the perturbations caused by noise can have detrimental (Jayaraman and Evans, 2019) and disparate (Bagdasaryan et al., 2019; Farrand et al., 2020; Hansen et al., 2022) effects on the behavior of machine learning models. By assessing the accuracy of differentially private machine learning models for (underrepresented) subgroups, Bagdasaryan et al. (2019) find a disparate impact regarding gender and ethnicity in both vision and text.\nTo prevent the risk of authorship disclosure, text rewriting is an appealing strategy that applies noise at word level or sentence level by leveraging word embeddings (Mikolov et al., 2013) or sequence-to-sequence models (Vaswani et al., 2017). Each approach comes with distinct mechanisms and implications for balancing utility and privacy."}, {"title": "Embedding-based Text Rewriting", "content": "Feyisetan et al. (2020) pioneered a mechanism for text rewriting termed Madlib. Madlib exploits the distance of words in embedding spaces (Mikolov et al., 2013) to substitute all words in a text with another word within a radius controlled by the privacy budget \\( \\epsilon \\). Since this substitution mechanism scales the notion of indistinguishability by a distance, it satisfies the axioms of metric DP (Chatzikokolakis et al., 2013).\nBuilding on a word embedding, the substitution involves three steps at word level: (1) retrieving the continuous representations of words from the embedding space, (2) adding noise to the representations calibrated using a multivariate distribution, and (3) mapping the noisy representation back onto the discrete space of vocabulary by employing a nearest neighbor approximation. While the probabilistic nature of these substitutions assures plausible deniability (Bindschaedler et al., 2017), substitutions based on the distance between words alleviate the curse of dimensionality typical of randomized response (Warner, 1965).\nHowever, privatizing text through perturbations at word level imposes notable limitations. Since the privacy guarantee in this approach depend on the geometry of the embedding space, it necessitates meticulous calibration of the noise magnitude (Xu et al., 2020). For dense regions of the embedding space, excessive noise may obscure suitable substitutions. For sparse regions of the embedding space, minimal noise may not provide sufficient protection against reconstruction. In addition the to noise calibration, perturbations at word level, albeit retaining the meaning of a text, encounter difficulties in maintaining the coherence of the text, such as grammar (Mattern et al., 2022), ambiguity (Arnold et al., 2023), and hierarchy (Feyisetan et al., 2019)."}, {"title": "Autoencoder-based Text Rewriting", "content": "Instead of privatization over word embeddings, an orthogonal approach utilizes sequence-to-sequence models built on recurrent (Bo et al., 2021; Krishna et al., 2021; Weggenmann et al., 2022) and transformer (Igamberdiev and Habernal, 2023) architectures. Common to these approaches is that noise is added to the encoder representations of text and the decoder learns to convert these noisy representations into text but without stylistic identifiers.\nBy perturbing the text at sentence level, this approach presents unique challenges compared to perturbing texts at word level. For instance, Igamberdiev et al. (2022) criticized that the utility is contingent upon the resemblance between the texts on which the sequence-to-sequence model was optimized and the texts that are subjected to privacy-preserving paraphrasing. This limitation in gener-alizability renders this form of text rewriting infeasible for the privatization of pretext at scale."}, {"title": "2.2 Stereotypical Bias", "content": "Bias in machine learning is viewed as prior information that informs algorithmic learning (Mitchell, 1980). When the prior information is predicated on stereotypes and prejudices, bias transcends this neutral definition and manifests in a disproportionate weight in favor of or against a social group.\nThe origins of these problematic biases are often rooted in the raw data used to develop machine learning models (Caliskan et al., 2017). Implicit or explicit stereotypes based on characteristics such as gender and race can cause the models to perpetuate and propagate these biases. This can significantly affect perception and decision making. The issue with stereotypical bias is particularly acute in the context of language models due to their extensive training on vast corpora that reflect biases present in human language. This bias magnifies the potential to influence its tone (Dhamala et al., 2021) and content (Abid et al., 2021), resulting in negative effects on individuals and society at large.\nUsing tests for association analogies, prior research demonstrated that embeddings harbor stereotypical biases related to gender (Bolukbasi et al., 2016; Kurita et al., 2019; Chaloner and Maldonado, 2019) and race (Manzini et al., 2019). Specifically, Caliskan et al. (2017) showed that terms related to career are associated with male names rather than female names, whereas unpleasant terms are associated with ethnic minorities. Garg et al. (2018) elaborate on the temporal dimension of bias in word embeddings by observing changes in gender and ethnic stereotypes over a century. This diachronic analysis indicates that while certain stereotypes have diminished over time, others remain robustly encoded in language. By investigating bias diffusion, Papakyriakopoulos et al. (2020) showed that biases contained in word embeddings can permeate natural language understanding, while Abid et al. (2021) report stereotypes in language generation such as violence for certain religious groups.\nUnlike these studies on bias in raw data, we examine the bias that stems from text privatization."}, {"title": "3 Methodology", "content": "To test our hypothesis on amplification of stereotypical bias through text privatization, we need to define (1) a language model, (2) the mechanism for text privatization, and (3) a bias measurement."}, {"title": "3.1 Language Model", "content": "Following Qu et al. (2021), we use a BERT model (Devlin et al., 2019) leveraging masked language modeling and next sentence prediction tasks for pre-training. The choice of BERT is motivated by its widespread adoption and proven effectiveness in capturing contextual relationships within text.\nFor pre-training, we selected a webscraped replication of WebText (Radford et al., 2019), which compared to WikiText (Merity et al., 2016), covers a broader spectrum of topics, styles, and viewpoints. This diversity renders WebText particularly suited for examining the transfer of stereotypical biases from the pre-text corpus. For fine-tuning, we reproduced the experiments of Bagdasaryan et al. (2019) but found no stereotypical bias other than a disparate impact due to sampling bias.\nTo assess the alterations in stereotypical bias by text privatization, we trained a BERT model devoid of any privacy interventions, serving as a control to score amplification and attenuation, and three additional copies of the BERT model under varying degrees of privacy guarantees. Since all BERT models are identical in terms of architecture and optimization (differing solely in the degree of text privatization), this setup warrants a controlled comparison that isolates the effects of text privatization on the anchoring of stereotypical bias."}, {"title": "3.2 Text Privatization", "content": "To privatize the WebText corpus, we operationalize the Madlib mechanism developed by Feyisetan et al. (2019) for text privatization at word level. Madlib necessitates the utilization of continuous representations supplied by a word embedding. We integrate Madlib with GloVe (Pennington et al., 2014). GloVe supplies a 400000-words vocabulary, each mapped to a 300-dimensional representation. The choice of Glove is motivated by the richness of its semantic space, making it an ideal candidate for privacy-preserving text privatization.\nSince the privacy guarantee of Madlib is rooted in metric DP, we need to calibrate the noise parameter \\( \\epsilon \\) according to the metric space of GloVe. This calibration involves an estimation of the plausible deniability (Bindschaedler et al., 2017) through two proxy statistics (Feyisetan et al., 2020):\n*   \\(N_\\omega = P{M(w) = w}\\) measures the number of identical words that stem from perturbing a word given a privacy budget \\( \\epsilon \\). We estimate \\(N_\\omega\\) by counting the occurrence of unaltered words after querying a random subset of 10000 words for a total of 1000 times.\n*   \\(S_\\omega = |P{M(w) = w'}|\\) measures the number of unique words that stem from perturbing a word given a privacy budget \\( \\epsilon \\). We estimate \\(S_\\omega\\) by calculating the effective support of a word after querying the same random subset of 10000 words for a total of 1000 times.\nWe can relate the proxy statistics to the privacy budget. Adding more noise corresponds to a tighter privacy guarantee. This is indicated by a smaller value for \\( \\epsilon \\) and results in a diverse set of perturbed words (low \\(N_\\omega\\) and high \\(S_\\omega\\)). Adding less noise reflects a weaker privacy guarantee. This is characterized by a larger value for \\( \\epsilon \\) and results in more frequent unperturbed words (high \\(N_\\omega\\) and low \\(S_\\omega\\)).\nSince \\(N_\\omega\\) (\\(S_\\omega\\)) should be positively (negatively) skewed to assure a reasonable privacy guarantee, we adopt privacy budgets of \\( \\epsilon \\) = {5,10}, corresponding to a high and low level of privacy protection, respectively."}, {"title": "3.3 Bias Measurement", "content": "Characterizing bias embedded within models typically relies on carefully crafted datasets. Several datasets exist to measure bias in word embeddings (Caliskan et al., 2017; May et al., 2019) and language models trained with masked (Nangia et al., 2020; Nadeem et al., 2021) and causal language modeling objective (Dhamala et al., 2021).\nWe adopt the StereoSet dataset designed by Nadeem et al. (2021). Given associative contexts, this dataset is intended to measure the tendency to default to stereotypical or anti-stereotypical associations. StereoSet provides meticulously crafted stimuli for bias measurement regarding gender, profession, race, and religion at two distinct levels:\nIntrasentence. The intrasentence task measures bias for sentence-level reasoning. It is formulated as a fill-mask task. Given a context sentence describing a social group, the task is to fill in a masked attribute corresponding to a stereotype, an anti-stereotype, and an unrelated option. The propensity for stereotypical associations is gauged by the likelihood of assigning each of these options.\nIntersentence. The intersentence task measures bias for discourse-level reasoning. It is formulated as a next-sentence task. Given a context sentence pertaining to a social group, followed by three sentences embodying a stereotype, an anti-stereotype, and an unrelated attribute, the assessment of stereotypical bias hinges on which of these sentences is instantiated as the most likely continuation.\nTo capture social biases at more differentiated levels, we complement our investigation with the CrowS-Pairs benchmark designed by Nangia et al. (2020). This benchmark consists of pairs of minimally distant sentences dealing with bias about gender identity, ethnic affiliation, age, nationality, religion, sexual orientation, socioeconomic status, physical appearance, and disability. The first sentence in each pair demonstrates a stereotype about a social group, while the second sentence in each pair violates it. This allows to score the bias in a language model by measuring how frequently it prefers a statement that portrays a social group stereotypically compared to an alternative portrayal of the same situation with a different social identity.\nDespite some criticism due to issues with model calibration (Desai and Durrett, 2020), we determine the preferences using pseudo-likelihood scoring (Salazar et al., 2020). We iterate over each sentence, masking a word at a time (except for the words that identify a social group), and accumulate the log-likelihoods of the masks in a sum for comparison."}, {"title": "4 Experiments", "content": "Prior to initiating our bias measurement, we conducted a preliminary sanity check by examining the pseudo-perplexity scores of BERT models trained under varying degrees of privacy. Pseudo-perplexity serves an indicator of a LM's ability to accurately model the probability distribution of words within a text corpus, thereby reflecting the model's proficiency to comprehend the linguistic structures encountered during its training.\nWe use a 10% subset of WikiText for computing the pseudo-perplexities. Evaluated at privacy levels specified by the privacy parameter \\( \\epsilon \\), the pseudo-perplexity scores were 93.51 with no privacy interventions, 502.67 with moderate privacy settings, and 2056.43 under conditions of high privacy. Consistent with previous evidence that introducing noise at word-level compromises the linguistic proficiency of LMs (Mattern et al., 2022), these results demonstrate a substantial degradation as the level of privacy augmentation increases.\nThe observed degradation raises an interesting question of whether private LMs harbor stereotypical biases despite diminished language modeling capabilities. This question forms the basis for our subsequent analysis of the undesirable biases in LMs stemming from text privatization."}, {"title": "4.1 Stereotype Results from StereoSet", "content": "To measure the bias resulting from text privatization at sentence and discourse level, we commence our analysis by detailing the stereotype scores derived from the StereoSet benchmark. The stereotype score is defined by the percentage of examples for which the LM assigns a higher probability to the pro-stereotypical word as opposed to the anti-stereotypical word. As such, scores closer to 0.5 are indicative of unbiased associations.\nSeveral key trends inform our understanding of the impact of text privatization on stereotypical bias. We observe that results from the intrasentence task aligns with those from the intersentence task, showing that the stereotype scores decline as the privacy level intensifies. For the intrasentence tasks, the averaged stereotype scores decreased from 0.6054 to 0.5711 and 0.5382 as the privacy budget was tightened to 10 and 5, respectively. For the intersentence tasks, the stereotype scores decreased similarity from 0.5724 to 0.5495 and 0.5227, respectively. However, the fall in stereotype scores is overall more pronounced in the intrasentence task than in the intersentence task. This disparity implies that mask language modeling is affected more acutely than next sentence prediction, which requires a broader context to build stereotypical association.\nWhile text privatization generally reduces stereotypical biases, we find inconsistent pattern when breaking down the stereotype scores by social categories. This indicates that the impact of text privatization is not uniformly spread across social groups."}, {"title": "4.2 Stereotype Results from CrowS-Pairs", "content": "To explore the manifestation of stereotypical bias across a broader range of social categories, we broadened our analysis to include CrowS-Pairs. Table 3 confirms that there is no overarching trend regarding the degree of text privatization and the manifestation of stereotypical biases.\nFollowing the general observation of decreasing stereotype scores as the privacy budget tightens, further scrutiny into social categories reveals a complex and heterogeneous response to text privatization. We discern social categories that are constant (e.g., religion), amplified (e.g., age, race), and attenuated (e.g., occupation, sexuality, disability). This suggests that some social categories are detached from the influences of textual perturbations while others seem less robust. Further complicating the interactions is that some social categories (e.g., gender, nationality, appearance) experience fluctuating responses. The categories show an increase in stereotype scores as privacy settings are intensified before stabilizing or reverting at the strictest levels of privacy. Except for sexual orientation (\u219334) and physical appearance appearance (\u2191.29), the effect sizes are negligible. This variability underscores the intricate dynamics between text privatization and LMs, suggesting that minor modifications in the privacy parameters can have significant and diverse impacts on stereotypical biases across different social constructs."}, {"title": "5 Conclusion", "content": "The interaction dynamics that govern the manifestation of bias in LMs are equivocal (Hansen et al., 2022). Prior research indicates that stereotypical bias is related to language proficiency in LMs (Nadeem et al., 2021). Since text privatization is known to impair language modeling capabilities (Feyisetan et al., 2020), one would expect a general diminution of stereotypical bias. However, the word embeddings used for text privatization are documented to harbor (Bolukbasi et al., 2016; Caliskan et al., 2017) and transfer (Papakyriakopoulos et al., 2020) stereotypical biases. This duality raises questions about whether text privatization leads to an amplification or an attenuation of stereotypical biases. By probing a LMs tendency to default to stereotypical or anti-stereotypical associations, we aimed to elucidate the relationship between text privatization and the amplification or attenuation of biases. We find that different social domains react differently to privacy settings and recommend to carefully assess stereotypical bias after training a LM on a privatized corpus of text."}, {"title": "6 Limitations", "content": "This study has several limitations that warrant consideration. Our experiments are based on WebText. While this corpus provides a broad range of topics and styles, it is possible that the derived insights, such as the general reduction in stereotypical bias and the unequal reduction across social groups, are influenced by spurious correlations (Schwartz and Stanovsky, 2022) inherent in the dataset. In addition to the flaws caused by the training corpus, our reliance on GloVe embeddings for text privatization introduces another potential source of inherent biases. Future research should address these limitations by incorporating a more diverse set of datasets and explore how alternative embeddings affect the persistence of stereotypical bias after privatization."}]}