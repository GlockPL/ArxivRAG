{"title": "ASMA: An Adaptive Safety Margin Algorithm for Vision-Language Drone Navigation via Scene-Aware Control Barrier Functions", "authors": ["Sourav Sanyal", "Kaushik Roy"], "abstract": "In the rapidly evolving field of vision-language navigation (VLN), ensuring robust safety mechanisms remains an open challenge. Control barrier functions (CBFs) are efficient tools which guarantee safety by solving an optimal control problem. In this work, we consider the case of a teleoperated drone in a VLN setting, and add safety features by formulating a novel scene-aware CBF using ego-centric observations obtained through an RGB-D sensor. As a baseline, we implement a vision-language understanding module which uses the CLIP (Contrastive Language Image Pretraining) model to query about a user-specified (in natural language) landmark. Using the YOLO (You Only Look Once) object detector, the CLIP model is queried for verifying the cropped landmark, triggering down-stream navigation. To improve navigation safety, we propose an Adaptive Safety Margin Algorithm (ASMA) that crops the drone's depth map for tracking moving object(s) to perform scene-aware CBF evaluation on-the-fly. By identifying potential risky observations from the scene, ASMA enables real-time adaptation to unpredictable environmental conditions, ensuring optimal safety bounds on a VLN-powered drone actions. Using the robot operating system (ROS) middleware on a parrot bebop2 quadrotor in the Gazebo environment, ASMA offers 59.4%- 61.8% increase in success rates with insignificant 5.4% - 8.2% increases in trajectory lengths compared to the baseline CBF-less VLN while recovering from unsafe situations.", "sections": [{"title": "I. INTRODUCTION", "content": "Foundational models pretrained on exa-scale internet data have made significant strides in vision and language processing tasks requiring little to no fine-tuning as exemplified by a new family of AI models such as BERT [1], GPT-3 [2], GPT-4 [3], CLIP [4], DALL-E [5] and PALM-E [6], to name a few. The fusion of vision and language models [4], [6] have enabled machines to interact with operating environments in increasingly intuitive ways. As these models become more widespread, the once sci-fi dream of robots understanding and interacting in complex environments through natural language commands is now a reality. This transformation is made possible by the recently emerging field of vision-language navigation (VLN) [7]-[12].\nAutonomous drones which have the ability to assist in smart agriculture, carry out search and rescue as well as respond to fire hazards [13] are becoming increasingly ubiq- uitous. Drones are predicted to contribute up to $54.6 billion to the global economy by 2030 [14]. We envision a future where teleoperated drones will be able to translate human- specified contextual instructions into low-level actions which a machine can execute, while navigating through unfamiliar dynamic environments using robot vision. In this regard, as we embark into industry 4.0 [15], VLN models for teleoper- ated drones offer exciting possibilities. However, ensuring safety and reliability for VLN in drones is still an open research problem. Control barrier functions (CBFs) [16], [17] provide a mathematical framework for maintaining safety constraints in dynamical systems, making them useful for real-time applications where safety is crucial. To that effect, we propose ASMA, an adaptive safety margin algorithm for VLN in drones. By formulating a novel scene-aware CBF, ASMA robustifies a drone VLN model. Implemented on a parrot bebop2 quadrotor with an RGB-D sensor in the Gazebo environment [18]-[20], a baseline VLN first uses a large language model (LLM) to identify navigation land- marks. Then it integrates CLIP (for scene understanding) with YOLO (for object detection [21]) using the robot operating system (ROS) middleware. This enables the model to query about landmarks and detect objects. Subsequently, ASMA identifies risky observations along the baseline drone VLN's path by processing ego-centric depth maps to dynamically evaluate scene-aware CBFs. This results in a hybrid approach utilizing AI as well as symbolic rules which adjusts the control commands of a VLN-powered drone using formal safety methods emerging from control theory. The main contributions of this work are as follows:\n\u2022 We implement a vision-language understanding module (VLUM) using a CLIP-based YOLO detection frame- work to function as a drone VLN (Section III-A).\n\u2022 We propose scene-aware CBFS (SA-CBFs) that adjust"}, {"title": "II. RELATED WORK", "content": "Vision-Language Models for Robot Navigation: In Vision-Language Navigation (VLN), agents interpret language commands to navigate through environments using visual cues [7]-[12]. Previous works, such as [8], [9], have expanded VLN into continuous environments (VLN- CE). Works in [10]-[12] have explored VLN focusing on interpreting visually-grounded instructions and developing models like VLN BERT to improve navigation performance through entity-landmark pre-training techniques. [22] employs 2D LiDAR for safer waypoint prediction in VLN- CE, while [23], [24] integrate pretrained visual-language features with navigation maps. The work in [25] utilizes action prompts for improved spatial navigation precision. Room2Room [26] enables teleoperated communication using augmented reality, and [27] introduces the \u2018Tryout' method to prevent collision-related navigational stalls. However, these approaches do not address the physical dynamics of robots, crucial for verifying safety. Our work focuses on a teleoperated drone similar to [28] with VLN capabilities, utilizing an RGB-D sensor and aims to enhance its safety and reliability in dynamic environments.\nControl Barrier Functions for Safety: Control barrier functions (CBFs) are essential tools from robust control theory, ensuring safety constraints are maintained in dynamic systems [16], [17]. By defining safe boundaries through mathematical functions, CBFs dynamically adjust control actions to prevent safety violations. Vision-based control barrier functions (V-CBFs) [29] extend these safety proto- cols to unknown environments, using conditional generative adversarial networks (C-GAN). Differentiable control barrier functions (dCBFs) integrated into neural networks via Barri- erNet [30] offer end-to-end trainable safety layers adaptable to environmental changes. Additionally, [31] develops a low- cost method for synthesizing quadratic CBFs over point cloud data, improving safe navigation. Visual locking CBFs (VCBF) and parametrizable switching descending CBFs (DCBF) further support precise drone landing safety [32]."}, {"title": "III. PROPOSED APPROACH", "content": "In this work, we deploy the parrot bebop2 quadrotor within a ROS-powered Gazebo environment, following [18]-[20], equipped with an RGB-D sensor for VLN tasks. We propose ASMA an Adaptive Safety Margin Algorithm for drone VLN using a novel scene-aware CBF formulation. Figure 1 provides a high-level overview. The following sections detail the functional blocks of ASMA."}, {"title": "A. Vision-Language Understanding Module (VLUM)", "content": "We utilize the CLIP model [4] to design our vision- language understanding module (VLUM) by mapping images and text into a shared embedding space, which allows effec- tive comparison between visual data and textual descriptions. The CLIP model has separate encoders for images $P_{img}$ and texts $P_{text}$. The operator views images from a ROS topic and issues a command like \"Go to the tree on the right.\" We use the GPT-2 LLM to parse this command into key components: the action (\"go to\"), the landmark (\"tree\"), and the attribute (\"on the right\"). Given the input image $I$, the YOLO object detection function $D_{YOLO}(I)$ outputs a bounding box $bbox = (X_1,Y_1,X_2,Y_2,l)$ for the detected landmark $l$, using which we extract the cropped image $I_{crop}$ from the original image $I$.\n$bbox = D_{YOLO} (I)$ (1a)\n$I_{crop} = CropImage(I, bbox)$ (1b)\nThis cropped image is then processed by the CLIP model's image encoder $P_{img}$ to verify the landmark. The similarity score $S$ is calculated by measuring the cosine similarity between the embeddings of the original and cropped images with respect to the text prompt $T$.\n$T = \"This is a rendering of a\" + l$ (2)\n$S_{scene} = \\frac{P_{img} (I) \\cdot P_{text} (T)}{||P_{img}(I)||\\cdot ||P_{text}(T)||}$ (3a)\n$S_{landmark} = \\frac{P_{img} (I_{crop}) \\cdot P_{text}(T)}{||P_{img}(I_{crop}) || . ||P_{text}(T) ||}$ (3b)\nThe relationship between the similarity score $S$ and the drone's operations can be interpreted as follows:\n\u2022 If $S_{landmark} > S_{scene}$, the landmark is correctly identified within the drone's field of view (FoV).\n\u2022 If $S_{landmark} < S_{scene}$, the landmark may no longer be in the FoV, indicating the drone has moved away.\nThese dynamics ensure that $S$ triggers appropriate downstream navigational tasks when verified. A decrease in $S$ prompts the operator to issue a new VLN instruction.\nFine-Tuning the CLIP Model: The pre-trained CLIP model, robust for broad vision-language tasks suffers a domain shift in our 3D Gazebo simulation environment due to the wide nature of the images it has been pretrained on. To address this domain shift, we incorporated a custom attention layer that dynamically adjusts focus on relevant image regions"}, {"title": "Fine-Tuning the CLIP Model:", "content": "The pre-trained CLIP model, robust for broad vision-language tasks suffers a domain shift in our 3D Gazebo simulation environment due to the wide nature of the images it has been pretrained on. To address this domain shift, we incorporated a custom attention layer that dynamically adjusts focus on relevant image regions"}, {"title": "B. Enhancing VLUM with Formal Safety Methods", "content": "Control barrier functions (CBFs) are essential tools in safety-critical control systems that enforce safety constraints through mathematical functions.\n$x = f(x) + g(x)u, \\xi = \\Psi(x, d)$ (7)\nHere, $\\dot{x}$ denotes the time derivative of the state vector $x \\in R^{12}$, covering the drone's positions, orientations, and velocities. The control input $u \\in R^4$, consists of thrust, roll, pitch, and yaw. Functions $f(x)$ form the autonomous dynamics and $g(x)$ signifies the dynamics that can be con- trolled in an affine manner. Additionally, the depth-map $d$ provides obstacle distances, with $\\xi = \\Psi(x, d)$ transforming these distances from pixel to physical space, incorporating the drone's current position. Let $C \\subset R^{12}$ represent a safety set defined through a continuously differentiable function $h(x)$ such that:\n$C = {x \\in R^{12} : h(x) \\geq 0}$ (8)\nFigure 2a illustrates a toy safety set $C$, marked by the boundary where $h(x) = 0$ and the region where $h(x) > 0$, indicating safe operational zones. The function $h(x)$ is characterized by its lie derivatives:\n$L_fh(x) = \\nabla h(x)\\cdot f(x),$ (9a)\n$L_gh(x) = \\nabla h(x)\\cdot g(x),$ (9b)\nwhich are critical for monitoring the system's safety relative to state changes and control action changes.\nTheorem (Safety Verification): For safety verification, it is required that:\n$L_fh(x) + L_gh(x)u + a(h(x)) \\geq 0,$ (10)\nwhere $a$ is a class $K$ function (meaning $a(0) = 0$ and $\\alpha(Kx_2) > \\alpha(Kx_1) \\forall x_2 > x_1$ and $K > 0$). $\\nabla h(x) > 0$ in the unsafe region ($h(x) < 0$) will drive $h(x)$ to become positive again.\nCorollary (Adaptive Control): If a function $h(x)$ can be designed such that adjustments in $u$ continuously satisfy the constraint in Eqn. (10), the system's response adapts dynamically to environmental changes while ensuring safety. This forms the core of our Scene-Aware CBF control:\n$\\min_u J(u) = \\min_u || u - U_{nom} ||$ (11)\nwhere the objective is to minimize a cost function $J(u)$ quantifying the deviation from a nominal control input $u_{nom}$"}, {"title": "1) Preliminaries:", "content": "We consider a quadrotor described by the following non-linear control affine dynamics with an ego- centric depth-map:\n$x = f(x) + g(x)u, \\xi = \\Psi(x, d)$ (7)\nHere, $\\dot{x}$ denotes the time derivative of the state vector $x \\in R^{12}$, covering the drone's positions, orientations, and velocities. The control input $u \\in R^4$, consists of thrust, roll, pitch, and yaw. Functions $f(x)$ form the autonomous dynamics and $g(x)$ signifies the dynamics that can be con- trolled in an affine manner. Additionally, the depth-map $d$ provides obstacle distances, with $\\xi = \\Psi(x, d)$ transforming these distances from pixel to physical space, incorporating the drone's current position. Let $C \\subset R^{12}$ represent a safety set defined through a continuously differentiable function $h(x)$ such that:\n$C = {x \\in R^{12} : h(x) \\geq 0}$ (8)\nFigure 2a illustrates a toy safety set $C$, marked by the boundary where $h(x) = 0$ and the region where $h(x) > 0$, indicating safe operational zones. The function $h(x)$ is characterized by its lie derivatives:\n$L_fh(x) = \\nabla h(x)\\cdot f(x),$ (9a)\n$L_gh(x) = \\nabla h(x)\\cdot g(x),$ (9b)\nwhich are critical for monitoring the system's safety relative to state changes and control action changes.\nTheorem (Safety Verification): For safety verification, it is required that:\n$L_fh(x) + L_gh(x)u + a(h(x)) \\geq 0,$ (10)\nwhere $a$ is a class $K$ function (meaning $a(0) = 0$ and $\\alpha(Kx_2) > \\alpha(Kx_1) \\forall x_2 > x_1$ and $K > 0$). $\\nabla h(x) > 0$ in the unsafe region ($h(x) < 0$) will drive $h(x)$ to become positive again.\nCorollary (Adaptive Control): If a function $h(x)$ can be designed such that adjustments in $u$ continuously satisfy the constraint in Eqn. (10), the system's response adapts dynamically to environmental changes while ensuring safety. This forms the core of our Scene-Aware CBF control:\n$\\min_u J(u) = \\min_u || u - U_{nom} ||$ (11)\nwhere the objective is to minimize a cost function $J(u)$ quantifying the deviation from a nominal control input $u_{nom}$"}, {"title": "2) Navigation Direction:", "content": "Depth map points are converted into 3D coordinates $\\xi = \\Psi(x, D_{crop})$ relative to the drone's body frame using intrinsic camera parameters:\n$X_c = (i-c_x)\\frac{d}{f}, Y_c = (j-c_y)\\frac{d}{f}, Z_c = d$ (12)\ni and j denote pixel coordinates, d is the depth at those coordinates from the cropped depth maps $D_{crop}$, f is the focal length, and $c_x, c_y$ are the center coordinates of the camera. These local coordinates $(X_c, Y_c, Z_c)$ are then transformed into the global frame:\n$[X_gY_gZ_g]^T = R \\cdot [X_c Y_c Z_c]^T + P_{curr}$ (13)\nusing a rotation matrix (R) and current drone position $P_{curr} = [x, y, z]$. The transformation function $\\xi = \\Psi(x, d)$ facilitates navigation by integrating depth map data to enhance the detection of targets and obstacles, providing coordinates for dynamic planning. $d_{target}$ and $d_{obs}$ are the direction vectors to the estimated waypoint and obstacle respectively:\n$d_{target} = p_{target} - p_{curr},$ (14a)\n$d_{obs} = P_{obs} - p_{curr}.$ (14b)\n$\\sigma$ = sign($(d_{target} \\times d_{obs})_z$)is defined as the sign of the cross product of the vectors along the z axis. $\\sigma > 0$ or $\\sigma < 0$ indicates obstacles situated to right or left respectively."}, {"title": "3) Scene-Aware CBF:", "content": "The proposed Scene-Aware Con- trol Barrier Function (SA-CBF) is defined as:\n$h(x) = \\begin{bmatrix} \\|d_{obs}\\| - d_{safe} \\\\ \\theta - \\theta_{safe} \\end{bmatrix}$ (15)\n$\\theta = cos^{-1} \\left( \\frac{d_{target} \\cdot d_{obs}}{\\|d_{target}\\|\\| d_{obs}\\|} \\right)$ (16)\n$\\nabla h_1(x) = \\sigma a\\cdot \\partial d_{obs}/\\partial x,$\n$\\nabla h_2(x) = - csc \\theta \\cdot \\partial cos \\theta / \\partial x.$\n$\\sigma a$ indicates the gradient direction for maintaining or increas- ing distance from obstacles. The gradients calculate changes in the directions which help compute the lie derivatives in Eqn. (10) guiding the optimization process to adjust the control actions. In addition to Eqn. (10), the cost function $J(u)$ in Eqn. (11) is subjected to the directional constraint\n$\\sigma\\cdot yaw\\_rate(u) \\geq 0$ (18)\nas an added variant. We call this ASMA-Steering as it imposes an explicit directional signal as a hard constraint."}, {"title": "C. Adaptive Safety Margin Algorithm (ASMA)", "content": "The process starts with system setup and ongoing shut- down signal monitoring (Algorithm 2, lines 1-2). VLN in- structions are interpreted (line 3), and two threads handle image data. Thread A focuses on object detection, iden- tifying landmarks and adjusting image focus areas (lines 5-9). Concurrently, thread B encodes the cropped images and compares them with the parsed instructions to verify the landmarks (lines 11-17). After synchronizing the threads (line 18), the algorithm evaluates if the landmarks align with the drone's navigation objectives (line 19). If a landmark is appropriately aligned, SA-CBF (Algorithm 1) is applied to adjust the control strategy using depth data (line 21). This drives the drone while adapting to environmental changes."}, {"title": "IV. RESULTS", "content": "We implemented the ASMA framework in ROS on a parrot bebop2 quadrotor within the Gazebo environment. The pretrained CLIP model from OpenAI's repository [33], was fine-tuned with huggingface's template [34], using pyTorch [35]. Object detection was performed using YOLOv5 [36] (~21 million parameters), with training data annotated via LabelImg [37]. The pretrained CLIP consisted of ~149 million parameters. Because of the large model sizes, we performed thread synchronization (Algorithm 2) to toggle between the two inference modes. Text descriptions for CLIP fine-tuning were generated using [38]. We collected ~ 1800 image-text pairs, 1000 of which were used for fine-tuning and 800 for evaluating the fine-tuned model. The CLIP model was fine-tuned at a learning rate of le\n04 for 4 epochs in Stage 1 and for 8 epochs with early stopping in Stage 2 [39]. The temperature was set to 0.2. Scene-Aware CBF optimizations were conducted using cvxopt [40], and the RotorS simulator [41] was used to integrate lower level control. dsafe was set to 2 meters and $\\theta_{safe}$ to 30 degrees. RGB-D sensor focal length f was set to 10 meters."}, {"title": "B. Comparative Schemes", "content": "\u2022 CBF-less VLN: The baseline scenario which lacks CBF safety features.\n\u2022 ASMA: Implements scene-aware CBFs based on the constraint in Equation (10).\n\u2022 ASMA-Steering: Extends ASMA by adding the direc- tional constraint from Equation (18).\nFor evaluating vision-language understanding performance, we use similarity scores $S_{scene}$ and $S_{landmarks}$. For navigation performance, we use three metrics: a) Trajectory Length (TL), b) Success Rate (SR) the percentage of times the drone ended within 1 meter of the destination, and c) Navigation Error (NE) the linear distance between the target and the actual endpoint of the flight path."}, {"title": "C. Vision-Language Understanding Performance", "content": "Table I shows the similarity scores for both whole scenes and cropped landmarks, with and without fine-tuning. Two improvements are observed: 1) the increase from whole scene to cropped landmarks, and 2) the increase from pretrained to fine-tuned models. For example, in ResNet-50, the \"tree\""}, {"title": "D. Vision-Language Navigation Performance", "content": "Table II presents the Vision-Language Navigation (VLN) performance across different methods, with Figure 4 visual- izing the corresponding trajectories for four VLN commands. The results show that the CBF-less method consistently achieves lower success rates (SR) across all commands, as the lack of safety constraints increases the likelihood of navi- gation failures. ASMA significantly improves SRs, increasing from ~ 61% to ~92% for the first command, with a slightly higher trajectory length (TL) of 8.60 compared to 8.54 for the CBF-less method, indicating a small detour for safety. ASMA-Steering further enhances navigation by reducing navigation errors (NE), such as lowering NE from 1.23 in ASMA to 0.96 in ASMA-Steering for the second command, with SR increasing to ~91%. On average, ASMA increases the SR by 59.4% compared to the CBF-less method (from 57.18% to 91.19%), with a minor 5.4% increase in TL (from 8.23 to 8.67), reflecting the slight safety detour. ASMA- Steering further improves SR by 61.8% while increasing TL by only 8.2% on average, with further decrease in NE."}, {"title": "E. Adaptive Safety Margins", "content": "ASMA dynamically modulates control actions based on calculated safety margins $h(x)$, which determine the drone's proximity to obstacles. This process, shown in Figure 5 (for the ASMA-Steering variant) using the distance component of $h(x)$ operates in three zones. In Zone 1 (Safe Operation), where $h(x)[0] > 0$, the drone operates under standard flight parameters without needing obstacle avoidance adjustments. In Zone 2 (Potential Risk), when $h(x)[0] \\leq 0$, ASMA adjusts the drone's trajectory and reduces speed to mitigate risks, represented by the red zone in Figure 5. Finally, in Zone 3 (Safety Restoration), after encountering an obstacle, ASMA restores standard flight controls, with a slightly delayed transition if directional constraints are absent (not shown in Figure 5). This showcases ASMA's adaptability in maintaining safety across various VLN tasks."}, {"title": "V. SUMMARY", "content": "In this work, we introduced ASMA (Adaptive Safety Margin Algorithm) to robustify VLN for teleoperated drones using a novel scene-aware CBF. We implemented a VLN model on a parrot bebop2 quadrotor utilizing CLIP for scene understanding and YOLO for object detection in Gazebo. ASMA adjusts control actions using real-time depth data en- suring safe navigation in complex environments. Two ASMA variants improved success rates of a baseline CBF-less VLN method by 59.4% and 61.8% on average with slight trajectory increases as safety detours.\nNote, in this work we used object detection to locate landmarks and obstacles as one possible perception method in the CLIP-based VLN setting, assuming the wide-spread availability of open-world detection datasets. However, for outdoor navigation or path-planning, segmentation could identify navigable regions as well. ASMA can be extended to support segmentation and other modalities, hence serving as a framework for enforcing VLN safety through scene-aware CBFs across a wide range of perception techniques. ASMA combines Al with formal safety methods from control theory that can (hopefully) serve as a template for improving safety and reliability in next-generation cognitive system design."}]}