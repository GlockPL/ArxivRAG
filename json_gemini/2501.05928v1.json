{"title": "Towards Backdoor Stealthiness in Model Parameter Space", "authors": ["Xiaoyun Xu", "Zhuoran Liu", "Stefanos Koffas", "Stjepan Picek"], "abstract": "Backdoor attacks maliciously inject covert functionality into machine learning models, which has been considered a security threat. The stealthiness of backdoor attacks is a critical research direction, focusing on adversaries' efforts to enhance the resistance of backdoor attacks against defense mechanisms. Recent research on backdoor stealthiness focuses mainly on indistinguishable triggers in input space and inseparable backdoor representations in feature space, aiming to circumvent backdoor defenses that examine these respective spaces. However, existing backdoor attacks are typically designed to resist a specific type of backdoor defense without considering the diverse range of defense mechanisms. Based on this observation, we pose a natural question: Are current backdoor attacks truly a real-world threat when facing diverse practical defenses?\nTo answer this question, we examine 12 common backdoor attacks that focus on input-space or feature-space stealthiness and 17 diverse representative defenses. Surprisingly, we reveal a critical blind spot that backdoor attacks designed to be stealthy in input and feature spaces can be mitigated by examining backdoored models in parameter space. To investigate the underlying causes behind this common vulnerability, we study the characteristics of backdoor attacks in the parameter space. Notably, we find that input- and feature-space attacks introduce prominent backdoor-related neurons in parameter space, which are not thoroughly considered by current backdoor attacks. Taking comprehensive stealthiness into account, we propose a novel supply-chain attack called Grond. Grond limits the parameter changes by a simple yet effective module, Adversarial Backdoor Injection (ABI), which adaptively increases the parameter-space stealthiness during the backdoor injection. Extensive experiments demonstrate that Grond outperforms all 12 backdoor attacks against state-of-the-art (including adaptive) defenses on CIFAR-10, GTSRB, and a subset of ImageNet. In addition, we show that ABI consistently improves the effectiveness of common backdoor attacks. Our code is publicly available.", "sections": [{"title": "1 Introduction", "content": "While deep neural networks (DNNs) have achieved excellent performance on various tasks, they are vulnerable to backdoor attacks. Backdoor attacks insert a secret functionality into a model, which is activated by malicious inputs during inference. Such inputs contain an attacker-chosen property that is called the trigger. Backdoored DNNs can be created by training with poisoned data [5, 12, 35, 50].\nMore powerful and stealthy backdoors can also be injected through the control of a training process [1, 6, 32, 38, 44, 67, 69], or by direct weights modification of the victim model [2, 16].\nIn early backdoor attacks [5, 12, 27], triggers could induce noticeable changes that human inspectors or anomaly detectors [3, 26, 52] could easily spot. To enhance the ability to remain undetected against such defenses (i.e., achieve input-space stealthiness), smaller or more semantic-aware triggers are designed [9, 35, 55]. Input-space stealthy backdoor attacks usually need to change labels of poisoned samples to the target class (i.e., dirty-label), which makes detection easier [3]. To this end, another line of backdoor attacks poisons the training data without changing the labels [50, 65] (i.e., clean-label), improving backdoor stealthiness.\nDespite the stealthiness concerning input images and labels, it has been widely observed that existing backdoor attacks introduce separable representations in the feature space, which can be exploited to develop backdoor defenses [31, 37, 53, 61, 70]. For example, featureRE [53] utilizes feature separability and designs a feature space constraint to reverse engineer the backdoor trigger. In response to feature-space defenses, state-of-the-art (SOTA) backdoor attacks focus on eliminating the separability in the feature space [32, 37, 44, 71] to increase the feature-space stealthiness, i.e., the undetectability against feature-space defenses. Considering a different threat model, supply-chain backdoor attacks assume control over the training or directly modify the model's weights [2, 6, 16], and the backdoored model is provided as a service or as the final product. For example, supply-chain attacks could introduce a penalty to the training loss that decreases the distance between the backdoor and benign features to increase feature-space stealthiness [8, 44, 67, 69].\nAn important observation is that most backdoor attacks are designed to be stealthy to resist a specific type of defense. For example, WaNet [35] and Bpp [55] design imperceptible triggers to bypass input-space defenses (such as NC [52]), but introduce significant separability in the feature space [53]. Adap-patch [37] avoids feature separability but uses patch-based triggers, which a human inspector can detect. More critically, current backdoor attacks are barely evaluated against parameter-space defenses [21, 24, 58, 62, 68, 70]. This oversight is significant because backdoor behaviors are ultimately embedded in and reflected by the parameters of the backdoored model, which is the final product of any backdoor attack. As such, there is a lack of systematic evaluation of backdoor attacks against the latest parameter-space defenses.\nTo this end, in this paper, we first systematically analyze 12 attacks against 17 backdoor defenses. All evaluated defenses and their characteristics, including detection and mitigation, are summarized in Table 1. Surprisingly, our experiments demonstrate that parameter-space defenses can easily mitigate SOTA stealthy backdoor attacks (including supply-chain attacks), indicating that existing stealthy backdoor attacks fail to provide parameter-space stealthiness and, as a result, still need substantial improvement to be stealthy in the model's parameter space. More importantly, our analysis reveals that even though some backdoor attacks can resist several defenses, bypassing all defense types is far from trivial."}, {"title": "2 Background & Related Work", "content": "In this section, we introduce the background and provide relevant backdoor literature.\n2.1 Preliminaries on Backdoor Training\nThis paper considers a C-class classification problem with an L-layer CNN network f = f\u2081 \u00b0\u00b7\u00b7\u00b7 f\u2081. Suppose that D = {(xi, yi)}~\u2081 is the original training data, containing N samples of xi \u2208 RdcxdpXdw and its label y \u2208 {1, 2, ..., C}. dc, dh, and dw are the number of input channels, the height, and the width of the image, respectively. The attacker chooses a target class t and creates a partially poisoned dataset Dp by poisoning generators Gx and Gy, i.e., Dp = DcUDb. De is the benign data from original dataset, D\u2081 = {(x', y')|x' = Gx(x), y' = Gy(y), (x, y) \u2208 D \u2013 Dc}. In the clean-label setting, Gy(y) = y. For the dirty-label attacks, Gy(y) = t. In the training stage, the backdoor is inserted into f by minimizing the loss on Dp:\n$\\min_{\\theta} L_D (\\theta) = E_{(x,y) \\in D_p} l(f(x; \\theta), y).$ (1)"}, {"title": "2.2 Backdoor Attacks", "content": "Backdoor attacks compromise the integrity of the victim model so that the model performs naturally on benign inputs but is misled to the target class by inputs containing the backdoor trigger. The trigger can be a visible pattern inserted into the model's input in the input space or a property that affects the feature representation of the model's input in the feature space. Eventually, however, the backdoored model's parameters in the parameter space will be altered regardless of the exact backdoor attack (see Figure 2). To insert a backdoor, the attacker is assumed to only control a small portion of the training data under the poison training scenario [5, 12, 66]. In the supply-chain setting (backdoor models provided to users), the attacker also control the training process [1, 34, 35, 44, 55]. Moreover, the backdoor can also be created by directly modifying the model's weights [2, 16, 28, 38].\nInput-space attacks. Traditional attacks typically use simple patterns as their triggers. For example, BadNets [12] uses a fixed patch, and Blend [5] mixes a Hello Kitty pattern into the images as the trigger. These non-stealthy triggers introduce abnormal data into training data and can be easily detected by human inspectors or defenses [3, 52]. To improve the stealthiness, various triggers are proposed to achieve invisibility in the input space. IAD [34] designed a dynamic solution in which the triggers vary among different inputs. WaNet [35] proposed the warping-based trigger, which is invisible to human inspection. Bpp [55] used image quantization and dithering as the trigger, which makes imperceptible changes to images. Although these methods successfully build invisible triggers and bypass traditional defenses [52], they still introduce separable features and can be detected by feature-space defenses [53, 61]. These input-invisible attacks can be even more noticeable than input-visible attacks (BadNet, Blend) in the feature space [62]. We conjecture this is because they have less modification on input pixels than input-visible attacks. Therefore, input-invisible attacks require more influential features to achieve a successful attack.\nFeature-space attacks. Knowing the vulnerability of input-space attacks against feature-space defenses, backdoor attacks are improved for feature-space stealthiness. A common threat model of this attack type is to assume additional control over the training process. For example, [44, 67, 69] directly designed loss functions to minimize the difference between the backdoor and benign features. [8] formulated the difference between backdoor and benign features by Wasserstein-2 distance and used the difference as a regularization constraint in backdoor training. Aside from design loss penalties, TACT [48] and SSDT [32] point out that source-specific (poison only the specified source classes) attack helps to obscure the difference in features between benign and backdoor samples. In addition, [37] proposed Adap-blend and Adap-patch, which obscures benign and backdoor features by 1) including poisoned samples with the correct label, 2) asymmetric triggers (using a stronger trigger at inference time), and 3) trigger diversification (using diverse variants of the trigger during training). Unfortunately, existing attacks lack systematic evaluation against the latest defenses. For example, Adap-blend can be thoroughly mitigated by recent works [61, 62, 70]. SSDT can be mitigated by traditional defenses, such as fine-pruning [25] and vanilla fine-tuning according to Tables 2 and 3. In summary, feature-space attacks usually introduce visible triggers and cannot defeat the latest defenses.\nSupply-chain attacks. Supply-chain attacks are getting more attention due to their potential in real-world applications where backdoored models are provided as the final product to users. In supply-chain attacks, adversaries could control both training data and the training process. Note that feature-space attacks [6, 8, 23, 32, 42, 44, 59, 67, 69] with the assumption of control over the training process are a subset of supply-chain attacks, as their output is the backdoor model. In addition to training control, another kind of supply-chain attack directly adjusts the model's weights in parameter space to introduce a backdoor, i.e., parameter-space attack. T-BFA [41], TBT [40], and ProFlip [4] explore modifying a"}, {"title": "2.3 Backdoor Defenses", "content": "Backdoor defenses can be classified into detection and mitigation. Detection refers to determining whether a model is backdoored (model detection) [26, 52, 54, 61, 67] or a given input is applied with a trigger (input detection) [11, 13, 32]. Mitigation refers to erasing the backdoor effect from the victim model by pruning the backdoor-related neurons (pruning-based defenses) [21, 25, 58, 68] or unlearning the backdoor trigger (fine-tuning-based defenses) [31, 61, 64, 70]. In addition, recent works [20, 39, 56] also consider the home-field advantage\u00b2 to design more powerful proactive defenses.\nBackdoor detection. Backdoor trigger reverse engineering (also known as trigger inversion) is considered one of the most practical defenses for backdoor detection as it can be applied to both poisoning training and supply-chain scenarios [53, 54, 61, 62], i.e., it is a post-training method. Specifically, trigger inversion works by searching for a potential backdoor trigger for a specific model. The model is determined as backdoored if a trigger is found, and the trigger can be used to unlearn the backdoor. The searching is implemented as an optimization process corresponding to the model and a local benign dataset. For example, NC [52] firstly proposes trigger inversion for detection by optimizing the mask and pattern in the input space that can mislead the victim to the target class. This optimization is repeated for all classes. The model is considered backdoored if an outlier significantly smaller than triggers for all other classes exists. Tabor [14] designs better optimization objects due to overly large (large size triggers but no overlaps with the true trigger) and overlaying (overlap the true trigger but with redundant noise) problems of NC. Although methods similar to NC perform well against fixed patch trigger attacks, such as BadNets [12] and Blend [5], they may not be effective against input-stealthy attacks like WaNet [35]. To address this problem, FeatureRE [53] moves trigger inversion from input space to feature space. Unicorn [54] further proposes a transformation function for attacks in other spaces, such as numerical space [55]. Recent works [61, 62] focus on exploring new optimization objectives addressing the inefficiency problem of previous trigger inversion methods due to optimization over all classes. BTI-DBF [61] trains a trigger generator by maximizing the backdoor feature difference between benign samples and their generated version (by the trigger generator) and minimizing the"}, {"title": "3 Comprehensive Backdoor Stealthiness", "content": "In this section, we first introduce the threat model (Section 3.1). Then, we analyze the behavior of neurons of backdoored models, suggesting that stealthy input- and feature-space backdoor attacks can be identified in parameter space (Section 3.2). Then, to achieve comprehensive stealthiness, we propose Grond that includes backdoor generation and Adversarial Backdoor Injection (Section 3.3).\n3.1 Threat Model\nAttacker's goal. The attacker provides pre-trained models to users. The aim is to inject backdoors into the pre-trained model so that the model performs well on clean inputs but predicts the attacker-chosen target label when receiving inputs with a backdoor trigger, i.e., an all-to-one attack.\nAttacker's knowledge. The attacker has white-box access to the training processes, the training data, and the model weights, i.e., the supply-chain threat model. During inference, the backdoor trigger is imperceptible to human inspectors.\nAttacker's capabilities. The attacker can train a well-performed surrogate model to generate UPGD, which is used to perturb the victim model's input. Additionally, the attacker can alter the model's weights during training. Table 12 in Appendix A.3 shows that the threat model of Grond is aligned with baseline attacks.\n3.2 Lack of Parameter-Space Stealthiness\nAs introduced in the related work, early backdoor attacks that introduce noticeable changes in either input [5, 12] or feature space [34, 35] have been empirically shown powerful, even with very low poisoning rates [12, 65]. Focusing on the backdoor-introduced noticeable changes, backdoor defenses are improved to distinguish backdoor patterns in either input or feature space [24, 53]. Meanwhile, backdoor attacks are optimized to increase stealthiness in input [35] or feature space [37]. However, regardless of the implementation of input- or feature-space attack logic, backdoor behaviors are eventually embedded in the backdoored model's parameters. For this reason, it is important to investigate whether backdoor attacks introduce visible changes in the parameter space of the attacked models that can be used by the parameter-space defenses. Taking this observation into consideration, we ran an initial experiment to understand the behavior of neurons of backdoored models. We use the TAC values [68] to quantify the relevance of a neuron to the backdoor behavior according to the difference when the network accepts benign and backdoor inputs. A higher TAC value indicates that the neuron is strongly relevant for backdoor behaviors. Specifically, TAC is defined as:\n$TAC^{(k)} (D_c) = \\frac{1}{|D_c|} \\sum_{x \\in D_c} ||f^{(k)} (x) - f^{(k)} (G_x(x))||_2,$ (2)"}, {"title": "3.3 Grond for Comprehensive Stealthiness", "content": "To address the vulnerabilities identified in the parameter space, We propose a stealthy backdoor attack, Grond, that considers comprehensive stealthiness, i.e., stealthiness in input, feature, and parameter space. Grond includes two key parts: UPGD trigger generation and Adversarial Backdoor Injection (ABI)."}, {"title": "Algorithm 1 UPGD Generation Algorithm", "content": "Input: Surrogate model fosur, training data D, perturbation budget\n\u03b5, the number of iteration I, the target class t.\nOutput: UPGD \u03b4\nS = B(\u03b4; \u03b5) = {\u03b4 \u2208 Rdc\u00d7dh\u00d7dw : ||\u03b4||\u221e \u2264 \u03b5}\n\u03b4 \u2190 random_initialization AS \u03b4\u2208 S\nfor i \u2208 (0, I \u2013 1) do\nx \u2190 sample_batch(D)\n$L_D(\\theta) = E_{(x,y) \\in D} l(f_{osur}(x + \u03b4; \\theta), t),$\n\u03b4 \u2190 min L1 (\u03b8)\n\u03b4\u2208S\nend for\nBackdoor trigger generation for input-space stealthiness. We use imperceptible adversarial perturbations to generate imperceptible backdoor triggers inspired by adversarial example studies [33, 66]. We modify the original PGD algorithm to generate a universal PGD (UPGD) perturbation as the backdoor trigger. UPGD contains non-robust but generalizable semantic information [49], which correlates with the benign functions of the victim model and shortens the distance between poisoned data and the target classification region [66]. Consequently, backdoor patterns tend to make fewer prominent changes to the victim network.\nSimilar to [65, 66], UPGD is generated on a well-trained surrogate model trained on the clean training set. The architecture and parameters of the surrogate model do not necessarily need to be the same as the victim model (see Table 15 in Appendix B.2). UPGD is optimized following the PGD [30] algorithm to decrease the surrogate model's cross-entropy loss that takes as inputs the adversarial examples (the poisoned samples in our case) and the target class label. This procedure is described formally in Algorithm 1. The \u03b4 is the generated UPGD that will be used as a backdoor trigger; thus, Gx(x) = x + \u03b4. S is the ball function with the radius \u03b5, and the small \u03b5 guarantees the imperceptibility of the backdoor trigger as it controls the perturbation's magnitude.\nThe backdoor is injected during training by poisoning some training data from the target class, i.e., applying the UPGD trigger to the training data. In the inference stage, our backdoor is activated by the same trigger. The motivation for our small-size trigger (\u03b5 = 8) is imperceptibility.\nAdversarial Backdoor Injection for parameter-space stealthiness. Backdoor neurons (i.e., trigger-related neurons) regularly show higher activation values for inputs that contain the trigger, which results in powerful performance [24, 26, 53]. To this end, backdoor training needs to substantially increase the magnitude of parameters of backdoor neurons [21, 58, 68], which harms the parameter-space stealthiness of backdoor attacks.\nOne way to find the sensitive neurons with higher activation values is to analyze the Lipschitz continuity of the network. Leveraging this fact, we introduce a novel backdoor training mechanism, Adversarial Backdoor Injection, to increase the parameter-space backdoor stealthiness. Specifically, each neuron's Upper bound of Channel Lipschitz Condition (UCLC [68]) is calculated, based on which the weights of these suspicious neurons are set to the mean of all neurons' weights in the corresponding layer after every training epoch. In our implementation, we use the weights before every"}, {"title": "", "content": "batch normalization as the neuron weights corresponding to the channel setting in UCLC. We prune neurons by substituting their weights with the mean ones because pruning to zeros makes the training unable to converge in our experiments. Formally, the kth parameter of the lth layer, \u03b8l(k), is updated as follows:\n$\\theta_l^{(k)} := \\begin{cases} mean(\\theta_l), & \\sigma(\\theta_l^{(k)}) > mean(\\sigma(\\theta_l)) + u \\times std(\\sigma(\\theta_l)) \\\\ \\theta_l^{(k)}, & otherwise, \\end{cases}$ (3)\nwhere u is a fixed threshold and \u03c3 is the UCLC value of the given weights. The measure for quantifying backdoor relevance can be changed from UCLC to others, such as the distance of neuron outputs when receiving benign and backdoor inputs, where a larger distance means the neuron is more relevant to backdoor behaviors and can be pruned. We use the modified UCLC for training efficiency as UCLC is data-free, which does not require calculation based on the outputs of neurons.\nIn adversarial training [30], adversarial examples are introduced during training to increase the model's robustness during inference. Similarly, during the Adversarial Backdoor Injection, we use backdoor defenses to increase the resistance of backdoor attacks to parameter-space defenses. At the end of each training epoch, Adversarial Backdoor Injection prunes the trained model to decrease the weights of backdoor neurons. Iteratively, backdoored neurons spread across the whole model instead of forming a few prominent backdoor neurons, as illustrated in Figure 1.\nFeature-space stealthiness. We hypothesize that feature-space stealthiness is a by-product of parameter-space and input-space stealthiness since the variation of feature maps is strongly correlated with model parameters and inputs. Figures 3 and 6 show that Grond can substantially increase the feature-space stealthiness. Detailed experimental analysis can be found in Section 4.2."}, {"title": "4 Experimental Evaluation", "content": "This section contains the main experimental results and analysis. Section 4.1 covers the datasets, baseline attacks, and defenses used in our experiments. Section 4.2 evaluates common backdoor attacks and Grond against pruning- and fine-tuning-based defenses. Section 4.3 provides an in-depth backdoor analysis, followed by Section 4.4, which explores how ABI can enhance common attacks. Section 4.5 covers backdoor model and input detection results, and Section 4.6 provides a comparison to supply-chain attacks. Finally, Section 4.7 covers the ablation study.\n4.1 Experimental Setup\nDatasets and Architectures. We follow the common settings in existing backdoor attacks and defenses and conduct experiments on CIFAR-10 [19], GTSRB [46], and a subset of ImageNet [7] with 200 classes and 1,300 images per class (ImageNet200). More details about the datasets can be found in Appendix A.1. The primary evaluation is performed using ResNet18 [15]. Moreover, we evaluate Grond using four additional architectures, VGG16 [45], DenseNet121 [18], EfficientNet-B0 [47], and one recent architecture InceptionNeXt [63] (see Table 15 in Appendix B.2).\nAttack Baselines. Grond is compared with 12 representative attacks: BadNets [12], Blend [5], WaNet [35], IAD [34], AdvDoor [66],"}, {"title": "4.2 Main Results on Backdoor Mitigation", "content": "All evaluated backdoor attacks are ineffective to at least one parameter-space backdoor defense on the CIFAR-10, as demonstrated in Tables 2 and 3. It suggests that common backdoor attacks designed to be stealthy in input and feature spaces are vulnerable to parameter-space defenses. Given that all backdoor behaviors are embedded in parameters of backdoored models, this finding suggests that future backdoor attacks should take parameter-space defenses into account as a standard step to evaluate comprehensive stealthiness.\nNot surprisingly, Grond's attack performance is better than all baseline attacks when considering evaluated backdoor defenses since Grond is designed to consider comprehensive stealthiness. On four pruning-based mitigations, Grond achieves 7.18% absolute higher ASR on average than the best backdoor attack, Narcissus. On five fine-tuning mitigations that show more powerful defense capability than pruning-based mitigations, Grond achieves 29.25% absolute higher ASR on average than Narcissus. In addition, Grond bypasses the five model detection and two input-space detections (see Section 4.5).\nPruning-based mitigation. We take a closer look at the details of pruning-based backdoor mitigation experiments in Table 2, presenting the results of all attacks against four pruning-based defenses. BadNets and Blend perform better on average than input-space stealthy attacks, e.g., WaNet and Bpp, because input-space stealthy attacks introduce significant separability in the feature space (see Figure 3). Across all pruning-based defenses, FP performs the worst, as expected, since it follows regular model pruning practice and is not a tailored backdoor pruning method.\nFine-tuning-based mitigations. Table 3 presents the backdoor performance against five fine-tuning-based defenses. In general, fine-tuning-based defenses are more effective than pruning-based defenses. For example, Narcissus and Adap-Blend can achieve ASRs higher than 60% against three out of four pruning-based defenses but are much less effective against most fine-tuning-based methods. FT-SAM is the most effective across all defenses, as shown in Tables 2 and 3, being able to compromise the effectiveness of all attack baselines. One important reason is that FT-SAM adopts Sharpness-Aware Minimization [10] to adjust the outlier of weight norm (large norms) to remove the potential backdoor. Larger weights of neurons are introduced by existing attacks to guarantee a high ASR [26], which also causes large differences when receiving benign and backdoor inputs (see Figure 4). Grond can bypass FT-SAM, as expected, since it deliberately decreases the weights of backdoor neurons, compromising the core working mechanism of FT-SAM.\nOn ImageNet200 and GTSRB. Real-world classification tasks may involve more categories, such as GTSRB (43 classes) and ImageNet200 (200 classes), and the percentage of each class in the dataset will commonly be much less than 10%. We target InceptionNext-Small on Imagenet200 and ResNet18 on GTSRB. The lo norm perturbation budget of UPGD is \u20ac = 16 for GTSRB and \u20ac = 8 for ImageNet200 to achieve imperceptible perturbations. Table 4 demonstrates that Grond is still effective on datasets with more classes and higher resolutions, especially against the most powerful parameter-space defense, FT-SAM.\n4.3 Backdoor Analysis\nAdaptive backdoor analysis by TAC pruning. Backdoor triggers are essential for calculating TAC values, making our TAC-based analysis highly adaptive to evaluating backdoor attacks. We directly utilize the trigger information to build a new pruning method based"}, {"title": "4.4 ABI Improves Common Backdoor Attacks", "content": "In this section, we show that our Adversarial Backdoor Injection (ABI) strategy generalizes to all evaluated common backdoor attacks. We combine the ABI module with baseline attacks to improve their resistance against parameter-space defenses. Figure 5 demonstrates that ABI is effective for all attacks when evaluating against the parameter-space defense ANP, where ASRs increase after adversarial injection, especially for BadNets, Blend, AdvDoor, Narcissus, and Adap-Blend. The improvement for feature space attacks (WaNet, IAD, and Bpp) is incremental. We speculate that feature space attacks rely too much on prominent features as their modification in the input space is minor. To activate the backdoor with such minor input modifications, the prominent features are required in the feature space. In addition, Figure 8 in Appendix B.3 shows the results of ABI without defense, demonstrating that it does not harm in general the BA and ASR when no defense is"}, {"title": "4.5 Backdoor Detection", "content": "Following previous works [61, 62], we choose five representative backdoor model detections for evaluation. The model detection refers to determining whether a given model is backdoored. We use 20 models for each poisoning rate with different random seeds. Then, we report the number of models detected as backdoor models out of the 20. Table 5 shows that all detections fall short when detecting Grond. In particular, NC [52], Tabor [52], and BTI-DBF [61] can detect a small part of backdoored models, while FeatureRE [53] and Unicorn cannot detect any of them. For featureRE [53], we conjecture it is over-dependent on the separability in the feature space, but Grond does not rely on prominent backdoor features according to Figure 3. For Unicorn [54], the false positive rate is high, and it tends to report every class as the backdoor target, even on models trained with benign data only. Except for model detection, Grond can also bypass input-space detections as demonstrated in Appendix B.1.\n4.6 Comparison with Supply-Chain Attacks\nSupply-chain backdoor attacks assume the adversary could directly manipulate models' parameters or control the backdoor training process for more powerful and stealthy backdoors. The backdoored model is provided as a service or final product to end users. Supply-chain backdoor attacks are attracting increasing industry and research community attention because of their stealthiness and significant real-world impact [16]."}, {"title": "4.7 Ablation Study", "content": "Trigger generation. To explore the influence of trigger patterns, we employ and evaluate three types of triggers: random noise, PGD perturbation, and UPGD perturbation, using ResNet18 on CIFAR-10. The random noise is sampled from a uniform distribution, and the PGD employs a projected gradient descent to generate sample-wise perturbations [30]. The generation of UPGD is described in Algorithm 1. All three triggers are limited to 8/255 (l\u221e norm) for imperceptibility and use the same training settings described in Table 11 in Appendix A.2.\nWe show in Table 7 that random noise is ineffective as a backdoor trigger, with an ASR around 1%, even if no defense is applied. The sample-wise PGD perturbation is more effective than random noise and shows (limited) robustness against CLP and FT-SAM. UPGD generates the most effective backdoor trigger with an ASR higher than 80% after CLP and FT-SAM, and we speculate that the reason is that UPGD exploits features from the target class, similar to Narcissus [65].\nAdversarial backdoor injection is critical. There are two components in Grond: the UPGD trigger generation and Adversarial"}, {"title": "A Additional Details about Experimental Settings", "content": "A.1 Datasets\nCIFAR-10. The CIFAR-10 [19] contains 50,000 training images and 10,000 testing images with the size of 3 \u00d7 32 \u00d7 32 in 10 classes.\nGTSRB. The GTSRB [46] contains 39,209 training images and 12,630 testing images in 43 classes. In our experiments, the images are resized to 3 \u00d7 32 \u00d7 32.\nImageNet200. ImageNet [7] contains over 1.2 million high-resolution images in 1,000 classes. In our experiments, we randomly select 200 classes from the ImageNet dataset as our ImageNet200 dataset. Each class has 1,300 training images and 50 testing images. The ImageNet images are resized to 3 \u00d7 224 \u00d7 224.\nA.2 Backdoor Attacks\nOur attack is compared with 12 well-known and representative attacks: BadNets [12], Blend [5], WaNet [35], IAD [34], AdvDoor [66], BppAttack [55], LC [50], Narcissus [65], Adap-Blend [37], SSDT [32], DFST [6], and DFBA [2]."}, {"title": "B Additional Experiments", "content": "B.1 Detection of backdoor input.\nBackdoor input detection is a defense technique that determines whether or not a given input includes a backdoor trigger. We show that Grond-generated backdoor samples can resist established backdoor detection methods. Table 14 shows the input-space detection results using Scale-up [13] and IBD-PSC [17]. We report the True Positive Rate (TPR), False Positive Rate (FPR), AUC, and F1 score in Table 14 for baseline attacks and Grond, where Scale-up and IBD-PSC are effective against three baseline attacks but cannot detect Grond-generated backdoor samples.\nB.2 Different Architectures with Different Surrogate Models\nWe evaluate Grond with four additional victim architectures in Table 15: VGG16, DenseNet121, EfficienNet-B0, and InceptionNeXt-Tiny. In addition, as Grond requires a surrogate model to generate UPGD as the backdoor trigger, we provide the results when UPGD is generated using different architectures for the surrogate model. For each architecture, UPGD is generated by either the victim architecture or ResNet18 to perform our attack. In Table 15, we use the three"}]}