{"title": "Topology-aware Preemptive Scheduling for Co-located LLM Workloads", "authors": ["Ping Zhang", "Lei Su", "Jinjie Yang", "Xin Chen"], "abstract": "Hosting diverse large language model workloads in a unified resource pool through co-location is cost-effective. For example, long-running chat services generally follow diurnal traffic patterns, which inspire co-location of batch jobs to fulfill resource valleys between successive peaks, and thus to saturate resource allocation in cluster-wide scope. These heterogeneous workloads often have different business priorities, and therefore preemption can be leveraged for resource elasticity. However, workloads often have distinct topology preferences as well. The resources released by lower-priority instances may fail to meet the requirements of high-priority online services which are usually latency-sensitive. The root cause behind such mis-match is a lack of topology awareness of resource scheduler, especially during preemption. To bridge this gap, we develop a fine-grained topology-aware method for preemptive scheduling of hybrid workloads. The method ensures that the resources freed by preempted tasks adhere to the topological affinity needs of high-priority preemptors in a guaranteed or best-effort manner. This dynamic alignment significantly increases the efficiency of preemption and improves overall scheduled performance for LLM workloads by 55%.", "sections": [{"title": "1 Introduction", "content": "The rapid deployment of services powered by large language models (LLMs) has gained significant attention due to their versatility across a wide range of applications [1]. With their surge in popularity, service providers that host self-managed LLMs are facing increasing pressure to optimize resource utilization, especially of GPU servers, which constitute a substantial portion of operational costs.\nUnlike the pre-training phase, which typically runs a single large-scale job on dedicated or exclusive clusters, where resource allocation is straightforward [2], the inference serving stage of LLMs benefits from co-locating multiple workloads in a unified resource pool. For example, the online chat services such as ChatGPT\u00b9 and Baixiaoying usually exhibit diurnal patterns in traffic [3], inspiring auto-scaling to ensure performance demands in peaks while to save cost in valleys. By co-locating offline workloads such as data processing jobs and/or offline inference tasks, the resource valleys between successive peaks of online services can be then padded, improving resource efficiency and reducing cost [4].\nWhile co-location improves resource utilization, it introduces scheduling complexities due to the heterogeneous nature of diverse LLM workloads, and complicates the maintenance of consistent performance, especially during preemption, in which lower-priority tasks are evicted in order to guarantee the resource demands of higher-priority services when resources are insufficient. Concretely, the scheduling cycle in a co-located cluster is often driven by auto-scaling of online services, responding to fluctuations in user request volume. Those online services are usually latency-sensitive, requiring a high performance placement on the GPU server according to its hardware topology affinity [5]. However, the resources freed by preempted instances often do not satisfy the topological affinity requirements of online services. This mismatch stems from the lack of topology awareness in preemptive scheduling, frequently resulting in deployment"}, {"title": "2 Background and Motivation", "content": ""}, {"title": "2.1 Distributed LLM Serving", "content": "Existing work has focused on the high cost optimization of LLM serving by improving the inference engine [7]. The popular open-sourced serving engines include FasterTransformer [8], vLLM [9], FlexGen [10], TGI [11], DeepSpeed-Inference [12], TensorRTLLM [13] et al. Readers are refereed to [14] for a comprehensive survey. This line of work targets on single-instance optimization, and thus is the basis of efficient multi-instance serving.\nTo manage multi-instance distributed services across a machine cluster, containerization and Kubernetes orchestration have become the de-facto standard [4]. Kubernetes offers robust scalability and flexible orchestration, enabling efficient management of containerized LLM services by automating deployment and scaling for diverse workloads. Influential providers such as OpenAI are hosting LLM chat services using Kubernetes.\nThis paper focuses on multi-instance LLM serving with diverse workload co-location. Figure 1 illustrates the co-location from a high-level deployment view. Each workload consists of multiple LLM instances spreading on GPU servers across the cluster. The instances within the same workload have identical resource allocations, including GPU, CPU, and DRAM (analogous to Pods within the same Deployment in Kubernetes). In contrast, instances from different"}, {"title": "2.2 Hardware Topology Preference", "content": "Scheduled Performance On a specific type of GPU server, the performance of LLM inference depends on both the communication patterns of the application task (i.e., regarding to the inference engine mentioned in Section 2.1) and the hardware topology of resources allocated to LLM instances by resource scheduler [5]. Modern multi-GPU systems have intricate hardware topology, including the connectivity between CPU and CPU, CPU and GPU, GPU and GPU. To ensure efficient execution and maximize resource utilization under co-location, workload schedulers must account for the underlying hardware topology and the communication needs of the workloads when allocating CPU and GPU resources. In what follows, we refer to such performance gain that is significantly affected by resource scheduling as scheduled performance."}, {"title": "2.3 Auto-scaling of Co-located Workloads", "content": "It is feasible to guarantee the topology affinity of performance-sensitive workloads during the initial deployment stage. For example, the topology manager [15] of Kubernetes, functioning as a per-node agent (i.e., the kubelet), can prevent an instance from starting if, despite sufficient overall resources on the machine, the resource distribution does not align with the required topology policy. However, this issue arises during the kubelet admission phase, meaning the scheduler has already made an incorrect scheduling decision by that point. This leads to either the instance failing to start or causing significant performance degradation for high-priority services in co-location scenarios, particularly during auto-scaling and priority-based preemption.\nAs an illustrative example, Figure 3 presents a snapshot of resource allocation in a cluster of 4090 Server for three co-located workloads. Workload A and Workload B are high-priority, latency-sensitive online inference services with the same priority. They both have strict topology affinity requirements including guaranteed NUMA alignment and best-effort socket-affinity. These workloads cannot be preempted. In contrast, Workload C is a low-priority offline inference job, utilizing cluster resources on a best-effort basis and subject to preemption at any time. Table 1 summarizes the detailed configuration. The instance specifications of Workload A, B and C are (32cores, 4GPUs), (16cores, 2GPUs), and (8cores, 1GPU), respectively.\nIn Figure 3, the snapshot displays one instance of Workload A, six of Workload B, and eight of Workload C scheduled across three machines, all of which are fully allocated, especially in terms of GPU resources. When the demand for high-priority Workload A (or B) spikes, the scheduler initiates preemption to scale up Workload A. At this point, the scheduler must choose preemption candidates from Machine 1 or Machine 2, both of which satisfy the resource requirements. However, selecting Machine 2 would violate the topology affinity of Workload A, leading to resource fragmentation or degraded performance, depending on whether a guaranteed or best-effort affinity policy is used. The core issue here is the scheduler's lack of topology awareness during preemption. Similarly, if Workload B scales up and"}, {"title": "3 Topology-aware Preemption", "content": "Topology-aware scheduling has been thoroughly studied in existing research. We provide an in-depth review and discussion in Section 6. However, the integration of real-time resource topology into preemption decisions between co-located workloads has been largely overlooked. In this section, we present the design of our proposed topology-aware preemption approach. We begin by outlining the overall architecture in Section 3.1, followed by the propose of a unified representation for resource topology of GPU servers in Section 3.2 and its maintenance in Section 3.3. Lastly, we introduce the proposed preemption algorithm in Section 3.4, offering a thorough analysis of its complexity and the optimizations we have applied."}, {"title": "3.1 Overview", "content": "To maximize the resource utilization, the scheduling strategy employs saturation allocation during the initial deploy phase, where all available cluster resources are fully allocated by co-locating multiple workloads. When certain tasks experience increased load and require scaling, lower-priority tasks are preempted to free up resources for the expansion. This approach ensures that the cluster operates at maximum capacity, while dynamically adjusting resources through preemption to support scaling demands. We advocate saturation allocation due to the expensive GPU resources and the unacceptable machine-level elasticity overhead, which in practice usually takes half-an-hour, or even much time, to add a new GPU server into the serving cluster when workload traffic spikes. Figure 4 shows the overview architecture of our topology-aware preemption for co-located LLM workloads.\nKubernetes is the standard open-source orchestration system for containerized applications, making it central to our management of distributed LLM serving in production. Accordingly, this paper focuses on resource scheduling within the Kubernetes ecosystem. As shown in Figure 4, the system architecture consists of three key components: a Custom Resource Definition (CRD) named FlexTopo, the FlexTopo agent responsible for its maintenance, and a topology-aware scheduler featuring Guaranteed Filtering and Best-effort Sorting.\nThe default Kubernetes scheduler lacks topology awareness, which can lead to Pod startup failures even when a node has sufficient total resources. This occurs because the scheduler may select a node where the resource distribution does not satisfy topology policies during the Kubelet admission phase. A more effective approach would involve informing the scheduler with topology information about available resources on each node, allowing it to choose nodes and victims that satisfy both resource requests and topology constraints of the preemptor. To address this, we propose FlexTopo, a unified representation of real-time resource topology for each server in the cluster. For example, FlexTopo provides the"}, {"title": "3.2 Unified Topology Representation", "content": "In terms of hardware topology, current research often relies on specific hierarchical models and lacks a standardized representation that can universally describe all types of mainstream GPU servers, including those using NVIDIA devices. This limitation makes topology-aware scheduling methods highly specific to certain setups. As a result, their generalizability is reduced, leading to increased deployment costs due to the need for adaptation in each use case.\nTo accommodate the diversity in hardware architectures, we propose FlexTopo (i.e., Flexible Topology), a unified resource topology representation that employs a flexible, graph-based model to accurately depict both static hardware topology and dynamic resource allocation states. In FlexTopo, hardware components, such as sockets, CPU cores, NUMA nodes, and GPUs, are represented as nodes, while edges depict the physical or logical topology between them. Each node and edge is annotated with attributes, as summarized in Table 2. Note that in FlexTopo we omit the details of PCIe switches, NVSwitches et al. aimed to only focusing on informed scheduling decisions.\nAs shown in Table 2, we group multiple CPU cores into a unit called a CPU CoreGroup. This approach is motivated by two main factors. First, modern GPU server used for LLM serving often come with a large number of CPU cores-such as SXM A100 servers, which may have hundreds of cores. Constructing a FlexTopo graph at the individual core level would result in a graph that is too complex to process efficiently. Second, in practice, GPU-based workloads,"}, {"title": "3.3 Maintenance of Topology Information", "content": "To maintain the FlexTopo for each server in the cluster efficiently but in a timely manner, practical considerations must be accounted for in the design of the FlexTopo agent. The motivation is to ensure that hardware topology and resource allocation information remain up-to-date, while also minimizing system overhead and avoiding unnecessary strain on the underlying infrastructure.\nThe hardware topology of a server may change under two primary conditions: (1) a complete server failure, and (2) the failure of one or more GPU devices. In the first scenario, the scheduler and Kubernetes itself handle this issue by detecting the absence of status updates from the Kubelet or by identifying a failed node through a health check mechanism. This ensures that the rest of the cluster is aware of the server failure and can redistribute workloads as needed. In the second scenario, topology changes due to the failure of GPU devices nearby specific NUMA nodes, which can affect preemption decision.\nHowever, hardware failures are relatively infrequent in modern GPU servers, and while such failures can be significant in large-scale clusters, the frequency of these events for individual servers tends to be low. Therefore, we adopt a periodic check of the hardware topology on each server. After each check, the results are compared against the internally maintained hardware state. If discrepancies are detected, FlexTopos are updated to reflect the newest hardware configuration.\nIn contrast, resource allocation states tend to fluctuate more frequently, particularly in large-scale clusters where many diverse workloads are continuously scheduled, running, and terminated. Timely updates to the FlexTopo CRD"}, {"title": "3.4 Preemptive Scheduling", "content": "Pipeline of Preemptive Scheduling Using FlexTopo, the scheduler can make topology-aware preemption decisions. Algorithm 1 outlines the primary pipeline in this preemptive scheduling process. Since preemptors often have specific topology requirements, as shown in Table 1, we distinguish two scenarios of topological QoS: Guaranteed and Best-Effort."}, {"title": "4 Implementation", "content": "We present the implementation of the proposed topology-aware preemption in three key components: the FlexTopo API, the FlexTopo Agent, and the Scheduler Plugin. The FlexTopo API and Agent code are available at https:// github.com/agiping/flextopo, and the Scheduler Plugin code can be found at https://github.com/agiping/ godel-scheduler.\nFlexTopo API. To decouple data collection from scheduling logic, we developed the FlexTopo API as an independent, shared module utilized by both the FlexTopo agent and the Scheduler. This API defines a lightweight FlexTopo CRD structure and offers a comprehensive client set for both the Agent and Scheduler, enabling interaction with the API server for reading and writing FlexTopo data. By isolating the API as a standalone component, we ensure code consistency between the Agent and Scheduler, aligning with best practices of Kubernetes community."}, {"title": "5 Evaluation", "content": "We evaluate the proposed topology-aware preemption through both simulations and near-production clusters. Due to the risks of extensive testing under high load in production\u2014potentially disrupting regular workloads\u2014we primarily rely on simulations to validate our approach. In production, we assess resource allocation distributions before and after deploying our topology-aware solution, while more detailed analyses use simulated clusters with varied LLM workloads.\nResource Allocation Distribution To illustrate the topology-aware improvements in scheduling, Figure 8 presents snapshots of the GPU allocation before and after deploying FlexTopo-based preemption. This cluster, a near-production environment, includes 41 GPU servers, each with 8 NVIDIA RTX 4090 GPUs. Each server has a 2-socket, 8-NUMA hardware configuration. Each GPU is aligned with one NUMA node, of which those indexed with 0 \u2013 3 are hosted by Socket 0 and 4 - 7 are hosted by Socket 1.\nIn Figure 8, the vertical axis represents ordered GPU device indices, and the horizontal axis denotes servers. Multiple co-located workloads use these GPUs, with allocations ranging from 1, 2, 4, to 8 cards per instance. Lines link GPUs"}, {"title": "Simulation Configuration of KWOK", "content": "To have an in-depth investigation, we simulate a 100-node GPU cluster with type of RTX 4090 using Kubernetes Without Kubelet (KWOK) [19]. KWOK provides a lightweight, fast solution for creating Kubernetes clusters, in which all nodes are simulated to mimic the behavior of actual cluster nodes."}, {"title": "Topology Affinity Hit Rate", "content": "To evaluate the improvements achieved with FlexTopo-based preemption, we analyze the topology affinity hit rate for Workloads B and C (shown in Table 3), where each instance requests 4 and 2 GPU devices, respectively. We compare the hit rate during preemption under G\u00f6del's standard approach and our topology-aware method. Notably, workloads requiring 8 GPU devices, such as Workload A, are excluded from this hit rate analysis, as their allocation inherently ensures either complete affinity or none at all; with 8 devices, a single server is fully occupied, rendering GPU allocation as the dominant factor. Similarly, Workload D, assigned the lowest priority, is also excluded from the hit rate analysis.\nTable 4 presents the results. For a robust comparison, we initiated 100 simulation cycles under conditions of saturated allocation, with each cycle triggering 50 instances to scale up. This setup equates to 50 independent preemptions per"}, {"title": "Overhead Analysis", "content": "Scheduling cost is a critical metric for evaluating performance in large-scale clusters. To assess the overhead introduced by topology-aware preemption and our optimization based on IMP 2, we measure the time cost under three conditions: G\u00f6del's standard preemption, FlexTopo-based preemption without IMP optimization, and FlexTopo-based preemption with IMP optimization. The analysis specifically focuses on the time cost incurred during the candidate sourcing phase within the preemption pipeline, as this is the primary contributor to time overhead in our observation. The scheduler is allocated a configuration of 1 CPU and 1 GiB of memory (1U-1Gi) and runs on a Linux server with AMD/64-bit CPU architecture.\nFirst, we examine the overhead differences in candidate sourcing across various workloads. Figure 10 displays the time cost of candidate sourcing over five preemptions for each of the four workload types from Table 3. Notably, Workload B incurs a significantly higher overhead in identifying suitable candidates compared to the other workloads. This is intuitive, as instances of Workload B require 4 GPUs, compelling the scheduler to evaluate numerous combinations of potential victims, especially those with only 1\u20132 GPU devices. Consequently, considerable time is spent assessing various combinations.\nConversely, Workload C, which requires only 2 GPUs per instance, has a notably lower time cost, validating this conclusion. Interestingly, Workload A, requiring 8 GPUs, incurs lower overhead than Workload B. This occurs because"}, {"title": "6 Related Work", "content": "Workload Co-location and GPU-Sharing. Server-level workload co-location and device-level GPU sharing have shown significant potential in improving GPU utilization [21, 22, 23]. For instance, Horus [22] introduces an interference-aware, prediction-based resource manager that refines workload co-location strategies by forecasting GPU utilization based on computation graph of deep learning model. While such approaches improve resource usage during workload co-location, they are not designed for preemptive scheduling, rendering them unsuitable for addressing preemption-based auto-scaling challenges under saturation allocation as discussed in Section 3.1. Device-level GPU sharing studies [23, 24, 25, 26, 27, 28] concentrate on optimal workload placement on a single GPU to maintain SLAs"}, {"title": "Topology-aware Scheduling", "content": "Amaral et al. [5] present a topology-aware workload placement strategy for scheduling deep learning jobs on multi-GPU systems. The authors propose a graph-based topology representation to describe the communication patterns of jobs and the GPU distribution of hardware. This topology graph is then leveraged for informed scheduling decisions. However, the comprehensive topology representation is highly specific to certain GPU servers, such as the IBM Power8 system [29] equipped with 4 NVIDIA Tesla P100 cards [30]. Additionally, the study does not explore adaptation and application in preemptive scheduling, where careful candidate and victim selection is crucial. Li et al. [31] propose a Microservice-Oriented Topology-Aware Scheduling Framework (MOTAS), which utilizes microservice and cluster topologies to optimize network overhead in microservice applications using a heuristic graph mapping algorithm. Compared to MOTAS, our approach focuses on minimizing communication costs within the same server and can thus be used alongside MOTAS, which primarily addresses network overhead optimization. G\u00f6del [4] also supports topology-aware scheduling in both the normal scheduling cycle and preemption. It leverages the Custom Node Resource (CNR) CRD from the Katalyst project [32], open-sourced by ByteDance, to represent the topology of each server in the cluster. However, G\u00f6del's NUMA-aware preemption mainly supports Exclusive Dedicated Cores, Non-Exclusive Dedicated Cores, and Shared Cores NUMA allocation, rather than offering flexible topology affinity. These three NUMA policies need to be tailored to the specific server hardware and workload requirements individually, posing challenges for application in large-scale clusters. Another line of work [33, 34, 35] explores the use of topology information in specialized application scenarios such as autonomous driving and virtual reality. In contrast, our research focuses on LLM serving within co-located cloud-native workloads."}, {"title": "LLM Inference Optimization", "content": "Many studies have aimed to enhance LLM inference efficiency. One focus is on memory optimization, using techniques like KV cache management and reuse [36, 9]. Another line of research targets latency reduction through scheduling improvements [37, 38], early exits [39], and kernel optimization [40]. Further approaches include sparsification [41] and quantization [42]. Unlike these methods that optimize inference engine itself, our work keeps the engine setup fixed and emphasizes on victim selection for optimal instance placement during preemptive auto-scaling. These line of work is complementary with ours."}, {"title": "7 Conclusion", "content": "In this study, we presented a topology-aware preemptive scheduling approach tailored to the demands of LLM workloads in co-located clusters. Traditional schedulers often lack topology awareness, leading to suboptimal preemption that fails to meet the high-performance needs of latency-sensitive, high-priority services. Our solution, FlexTopo-based IMP, provides a unified, flexible topology representation to guide preemption decisions, ensuring that resources freed by preempted tasks match the topological preferences of critical workloads, thereby improving efficiency and reducing resource fragmentation.\nEvaluations on simulated and near-production clusters demonstrate FlexTopo's effectiveness, achieving a 100% topology affinity hit rate versus 45% with standard methods, reducing scheduling failures and improving scheduled performance by 55%. This method not only enhances resource elasticity and efficiency but also offers a scalable framework for topology-aware scheduling in dynamic, large-scale environments."}]}