{"title": "Continuous Autoregressive Models with Noise Augmentation Avoid Error Accumulation", "authors": ["Marco Pasini", "Javier Nistal", "Stefan Lattner", "Gy\u00f6rgy Fazekas"], "abstract": "Autoregressive models are typically applied to sequences of discrete tokens, but recent research indicates that generating sequences of continuous embeddings in an autoregressive manner is also feasible. However, such Continuous Autoregressive Models (CAMs) can suffer from a decline in generation quality over extended sequences due to error accumulation during inference. We introduce a novel method to address this issue by injecting random noise into the input embeddings during training. This procedure makes the model robust against varying error levels at inference. We further reduce error accumulation through an inference procedure that introduces low-level noise. Experiments on musical audio generation show that CAM substantially outperforms existing autoregressive and non-autoregressive approaches while preserving audio quality over extended sequences. This work paves the way for generating continuous embeddings in a purely autoregressive setting, opening new possibilities for real-time and interactive generative applications.", "sections": [{"title": "1 Introduction", "content": "Autoregressive Models (AMs) have become ubiquitous in various domains, achieving remarkable success in natural language processing tasks [1, 2]. These models operate by predicting the next element in a sequence based on the preceding elements, a principle that lends itself naturally to inherently sequential data like text. However, their application to continuous data, such as images and audio waveforms, presents unique challenges.\nFirst, autoregressive models for image and audio generation have traditionally relied on discretizing data into a finite set of tokens using techniques like Vector Quantized Variational Autoencoders (VQ-VAEs) [3, 4]. This discretization allows models to operate within a discrete probability space, enabling the use of the cross-entropy loss, in analogy to their application in language models. However, quantization methods typically require additional losses (e.g., commitment and codebook losses) during VAE training and may introduce a hyperparameter overhead. Secondly, continuous embeddings can encode information more efficiently than discrete tokens (the same information can be encoded in shorter sequences),"}, {"title": "2 Related Work", "content": "Autoregressive models have achieved remarkable success in natural language processing, becoming the dominant approach for tasks like language modeling [8, 9, 1, 2]. Extending autoregressive models to image and audio generation has been an active area of research. Early attempts directly model the raw data, as exemplified by PixelRNN [10] and WaveNet [11], which operate on sequences of quantized pixels and audio samples, respectively. However, these approaches are computationally demanding, particularly for high-resolution images and long audio sequences. To address this challenge, recent works have shifted towards modeling compressed representations of images and audio, typically obtained using autoencoders. A popular approach involves discretizing these representations using Vector Quantized Variational Autoencoders (VQ-VAEs) [3], enabling autoregressive models to operate on a sequence of discrete tokens. This strategy has led to significant advances in both image [12, 13] and audio generation [14, 15].\nRecent approaches explore training AMs directly on continuous embeddings. GIVT [6] uses the AM's output to parameterise a Gaussian Mixture Model (GMM), enabling training with cross-entropy loss. At inference, continuous embeddings can be sampled directly from the GMM. Despite its success in high-fidelity image generation, GIVT requires additional techniques, such as variance scaling and normalizing flow adapters, that add complexity to the model and training procedure. Alternative approaches like Masked Autoregressive models (MAR) [5] learn the per-token probability distribution using a diffusion procedure. A shallow MLP is used to sample a continuous embedding conditioned on the output of an autoregressive transformer. However, the authors show that a sequential autoregressive model with causal attention (i.e., GPT-style [9]) performs poorly in this setting and requires bidirectional attention and random masking strategies during training. Our work tackles this inconvenience to make training of GPT-style models feasible, which we believe can unlock new avenues for real-time interactive applications, especially in the field of audio generation."}, {"title": "3 Background", "content": "3.1 Denoising Diffusion Models (DDMs) are a class of generative models that learn a given data distribution p(x) by gradually corrupting it with noise (diffusion) and then learning to reverse this process (denoising). Specifically, they model the score function of the noise-perturbed data distribution at various noise levels. Given a set of noise levels ${\\sigma_t}_{t=1}^T$, we can define a series of perturbed data distributions $p_{\\sigma_t}(x_t) = \\int p(x) \\mathcal{N}(x_t; x, \\sigma_t^2I) dx$. For each noise level $\\sigma_t$ with $t = 0, 1, ..., T$, DDMs learn a score"}, {"title": "3.2 Rectified Flow (RF)", "content": "RF [16] offers a conceptually simpler and more general alternative to DDMs and was shown to perform better than competing diffusion frameworks on latent embedding generation tasks [17]. RF directly connects two arbitrary distributions $\\pi_0$ and $\\pi_1$ by following straight line paths. In the basic framework, $\\pi_0$ is the data distribution, and $\\pi_1$ is the noise distribution, typically sampled from a standard Gaussian. Given a set of samples $(x_0 \\sim \\pi_0, x_1 \\sim \\pi_1)$, a rectified flow is defined by the ordinary differential equation (ODE) $\\frac{dz_t}{dt} = v(z_t, t)dt$, where $z_t$ represents the data point at time t, and $v(z_t, t)$ is the so-called drift force and it is parameterized by a neural network trained to minimize the loss:\n$\\mathcal{L} = E [|| (x_1 - x_0) - v(tx_1 + (1 - t)x_0, t)||_2^2]$\nThis objective encourages the flow to follow the straight line paths connecting $x_0$ and $x_1$, resulting in a more efficient deterministic mapping than other diffusion-based frameworks."}, {"title": "3.3 Autoregressive Models for Continuous Embeddings", "content": "as proposed in MAR [5], employ diffusion models to predict the next element $x_t$ in a sequence, based on the preceding elements $(x_0, x_1, ..., x_{t-1})$. This can be formulated as estimating the conditional probability $p(x_t|x_0, x_1, ..., x_{t-1})$. To predict $x_t$ MAR first transforms $(x_0, ..., x_{t-1})$ into a vector $z_t$ using a Backbone neural network, and then model $p(x_t|z_t)$ using a diffusion process. A second network, Sampler, predicts a noise estimate from $y_t$, which represents $x_t$ corrupted with noise $\\varepsilon \\sim \\mathcal{N}(0, I)$. The training objective is formulated as:\n$\\mathcal{L} = E_t [||\\epsilon - \\text{Sampler}(y_t|z_t)||_2^2] \\text{ where } z_t = \\text{Backbone} (x_0, ..., x_{t-1})$.\nThis objective encourages the model to learn to denoise the corrupted embedding $y_t$ and recover the original $x_t$ based on the information about previous timesteps contained in the condition $z_t$. At inference time, the model generates a new sequence by iteratively predicting conditioning vectors $z_t$ based on the previously generated elements and then using a reverse diffusion process to sample $x_t$ from the learned distribution $p(x_t|z_t)$. MAR, however, shows that naive training of GPT-style models-using causal modeling of ordered sequences fails to deliver compelling results. Instead, masked modeling and bidirectional attention mechanisms are necessary to achieve performance on par with non-autoregressive approaches. We argue that masked modeling, which involves predicting random timesteps, mitigates error accumulation by discouraging the model from relying exclusively on preceding time steps to generate the current one."}, {"title": "4 Proposed Method", "content": "Training As seen in Sec. 3.3, while MAR [5] enables training AMs on continuous embeddings, a significant challenge emerges when generating ordered sequences: error accumulation. At inference, prediction errors propagate throughout the generation process and compound at each subsequent predicted time step, leading to a divergence from the learned data distribution. To address this, we introduce a novel strategy that injects noise during training to simulate erroneous predictions, encouraging the model to be robust against it (see Fig. 1). Specifically, we assume that at inference, the Sampler (see Sec. 3.3) generates embeddings that can be expressed as a linear combination of the real data $x_t \\sim \\pi_0$ and an error $\\varepsilon \\sim \\mathcal{N}(0, I)$, weighted by an unknown error level $k_t$:\n$\\hat{x}_t = k_t * \\varepsilon + (1 - k_t) * x_t$.\nWe can then simulate inference conditions during training, aligning the distribution of embeddings with those generated during inference, which inherently exhibit error accumulation. This can help us"}, {"title": "Inference", "content": "At inference, CAM generates a new sequence of embeddings autoregressively, following the temporal order of the sequence. Given the initial conditioning vector $z_{\\text{sos}}$, the Sampler generates the first embedding $\\hat{x}_1$ by performing an iterative reverse diffusion process (see Sec. 3.1). Subsequent embeddings are generated by concatenating $\\hat{x}_{t-1}$ to the existing sequence of previously generated embeddings. The sequence is fed as input to the Backbone to produce the conditioning vector $z_t$, which is then used by the Sampler to generate $\\hat{x}_t$. This process is repeated iteratively until the desired sequence length is reached. Since the Sampler is parameterised by a shallow MLP, the computation required by the denoising process can be negligible compared to the forward pass of the Backbone. To further dampen the effects of error accumulation, we observe that adding a small constant amount of Gaussian noise $k_{\\text{inf}}$ to each generated embedding $\\hat{x}_t$ before feeding it back to the Backbone can yield higher quality when generating long sequences. We hypothesize that this noise helps to reduce the mismatch between the Gaussian distribution used for perturbation during training and the actual distribution of errors of the Sampler's predictions."}, {"title": "5 Experiments and Results", "content": "Datasets: For training and evaluation purposes, we use an internal dataset composed of ~ 20,000 single-instrument recordings covering various instruments and musical styles. Each audio file is stereo and has a 48 kHz sample rate. We preprocess the dataset by extracting continuous latent representations using an in-house stereo version of Music2Latent [18], a state-of-the-art audio autoencoder. This results in compressed latent embeddings with a sampling rate of ~ 12 Hz and a dimensionality of 64. During training, we randomly crop each embedding sequence to 128 frames, corresponding to approximately 10 seconds of stereo audio.\nImplementation Details: The Backbone in CAM is a transformer with a pre-LN configuration, 16 layers, dim = 768, mlp_mult = 4, num_heads = 4. We use absolute learned positional embeddings. The Sampler is an MLP with 8 layers, dim = 768, mlp_mult = 4. Both $z_t$ and $y_t$ are concatenated and fed as input to the MLP, while information about the noise level $\\sigma_t$ is introduced via AdaLN [19]. The total number of parameters for the entire model is 150 million. Regarding training, we use AdamW [20] with $\\beta_1$ = 0.9, $\\beta_2$ = 0.999, weight decay = 0.01, and a learning rate of le \u2013 4. All models are trained for 400k iterations with a batch size of 128.\nBaselines: We compare CAM against several autoregressive and non-autoregressive baselines: GIVT models [6] with 8 and 32 modes, the model proposed by [5] in its fully autoregressive and causal configuration (we denote this model as MAR), and a non-autoregressive diffusion model trained using the Rectified Flow [16] framework. We also provide the results of MAR trained using Rectified Flow instead of its original linear noise-prediction objective and of GIVT trained using our proposed noise augmentation technique. To ensure a fair comparison in model capacity, we use the same architecture for all models, and we increase the number of transformer layers to 21 in those models that do not use a Sampler to roughly match the total number of parameters. We provide audio samples at sonycslparis.github.io/cam-companion/.\nEvaluation Metrics: We use Frechet Audio Distance (FAD) [21] to evaluate the quality of generated samples. We use FAD calculated using CLAP features [22], which accepts 10-second high-sample rate samples as input and has been shown to exhibit a stronger correlation with perceived quality compared to VGGish features [23]. FAD is calculated using a reference set of 10,000 samples and"}, {"title": "Influence of Rectified Flow", "content": "In 2a, we first compare MAR trained using the original noise-prediction with linear schedule diffusion framework to the same model trained using a Rectified Flow formulation. For each model, we use the number of denoising steps in the range (10,100) that results in the lowest FAD. The model trained using Rectified Flow achieves a lower FAD."}, {"title": "Influence of Inference Noise", "content": "We evaluate FAD and FADacc when CAM uses different values of $k_{\\text{inf}}$ in the [0, 0.05] range. Fig. 2b shows the results obtained for each noise level. Remarkably, we note how with $k_{\\text{inf}}$ = 0.02, FADacc < FAD, pointing to an improvement in generation quality for longer generations. A possible explanation of this result is: since the Backbone receives a maximum context of ~ 10 seconds, it generates all embeddings after the 10 seconds mark using a full context, which may result in higher quality embeddings. We use $k_{\\text{inf}}$ = 0.02 for all subsequent experiments."}, {"title": "Comparison with Baselines", "content": "We evaluate CAM and the baselines concerning their ability to generate high-fidelity audio. The FADacc metric directly evaluates the resilience of the models to error accumulation. A model that does not suffer from error accumulation would achieve the same results on both the first and the second 10-second generated audio sequence. Since we are not interested in evaluating or minimizing inference speed, for each model relying on diffusion sampling we use the number of denoising steps in the range (10,100) that results in the lowest FAD. We also use variance scaling for GIVT to sample embeddings with a temperature of t = 0.9, which we empirically find to result in a lower FAD. A technique to simulate sampling with different temperatures has also been proposed for MAR [5]; however, we find that the best metrics are obtained with t = 1.\nAs we show in Tab. 2c, CAM outperforms all autoregressive and non-autoregressive baselines on FAD metrics. CAM also exhibits a decrease in FAD when autoregressively generating longer sequences. The same result can be noticed for GIVT when trained with our proposed noise augmentation, which also performs vastly better than the original GIVT models. This demonstrates that our proposed training approach can be successfully adapted to different categories of autoregressive models for continuous embeddings. In contrast, all other autoregressive baselines show a degradation in audio quality as the generated sequence length increases."}, {"title": "6 Conclusion", "content": "This paper introduced CAM, a novel method for training purely autoregressive models on continuous embeddings that directly addresses the challenge of error accumulation. By introducing random noise into the input embeddings during training, we force the model to learn robust representations resilient to error propagation. Additionally, a carefully calibrated noise injection technique employed during inference further mitigates error accumulation. Our experiments demonstrate that CAM substantially outperforms existing autoregressive and non-autoregressive models for audio generation, achieving the lowest FAD while maintaining consistent audio quality even when generating extended sequences. This work paves the way for new possibilities in real-time and interactive audio applications that benefit from the efficiency and sequential nature of autoregressive models."}]}