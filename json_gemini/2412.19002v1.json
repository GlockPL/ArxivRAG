{"title": "Tempus Core: Area-Power Efficient Temporal-Unary Convolution Core for Low-Precision Edge DLAs", "authors": ["Prabhu Vellaisamy", "Harideep Nair", "Thomas Kang", "Yichen Ni", "Haoyang Fan", "Bin Qi", "Jeff Chen", "Shawn Blanton", "John Paul Shen"], "abstract": "The increasing complexity of deep neural networks (DNNs) poses significant challenges for edge inference deployment due to resource and power constraints of edge devices. Recent works on unary-based matrix multiplication hardware aim to leverage data sparsity and low-precision values to enhance hardware efficiency. However, the adoption and integration of such unary hardware into commercial deep learning accelerators (DLA) remain limited due to processing element (PE) array dataflow differences. This work presents Tempus Core, a convolution core with highly scalable unary-based PE array comprising of tub (temporal-unary-binary) multipliers that seamlessly integrates with the NVDLA (NVIDIA's open-source DLA for accelerating CNNs) while maintaining dataflow compliance and boosting hardware efficiency. Analysis across various datapath granularities shows that for INT8 precision in 45nm CMOS, Tempus Core's PE cell unit (PCU) yields 59.3% and 15.3% reductions in area and power consumption, respectively, over NVDLA'S CMAC unit. Considering a 16x16 PE array in Tempus Core, area and power improves by 75% and 62%, respectively, while delivering 5x and 4x iso-area throughput improvements for INT8 and INT4 precisions. Post-place and route analysis of Tempus Core's PCU shows that the 16x4 PE array for INT4 precision in 45nm CMOS requires only 0.017mm\u00b2 die area and consumes only 6.2mW of total power. We demonstrate that area-power efficient unary-based hardware can be seamlessly integrated into conventional DLAs, paving the path for efficient unary hardware for edge AI inference.", "sections": [{"title": "I. INTRODUCTION", "content": "Since the introduction of AlexNet [1] in 2012, AI computational demands have grown rapidly [2], driving the adoption of hardware accelerators like GPUs and TPUs for deep learning (DL) training and inference. However, DL's computational needs have quickly outpaced the growth of available compute power, leading to what is now termed the \u201cAI compute gap\".\nThis gap has renewed interest in innovative computational techniques and architectures aimed at closing this compute performance and energy efficiency gap.\nIn recent years, quantization techniques have become an effective strategy for deploying AI models with reduced model complexity. While 32-bit floating point (FP32) was the standard for DL training, FP16 is now widely adopted, offering 4x-8x performance improvements [3]. Building on this trend, FP8 precision for both training and inference has been proposed [4] and implemented by Nvidia in their H100 GPUs, which support FP8 models through the Transformer Engine library [5]. In particular, FP8 training on various convolutional neural network (CNN) models has shown minimal accuracy degradation while increasing throughput by 2x-4x [3]. In addition to floating point formats, 8-bit integer (INT8) training has been demonstrated to reduce the training time of CNNs on Pascal GPUs by 22% [6]. Although INT8 has become the standard for DL inference, a shift toward INT4 precision has led to a 77% performance improvement over INT8 [7]. These quantization results offer tremendous promise particularly for edge inference deployment, where computational resources are limited.\nUnary computing has recently emerged as a promising paradigm for low-precision AI, enabling highly area- and power-efficient hardware by trading off latency for efficiency. In particular, temporal encoding techniques have been shown to significantly reduce computational overhead in matrix multiplication units, as demonstrated by recent works [9] [10] [11], which exploit the inherent sparsity of DL models. However, these existing implementations focus primarily on GEMM-level designs and are not scaled to inherently support convolution dataflows. This work addresses that gap by extending unary computing to convolution dataflows, while ensuring compatibility with widely-used deep learning accelerators"}, {"title": "II. BACKGROUND AND RELATED WORKS", "content": "A. General Matrix Multiplication\nGeneral matrix multiplication (GEMM) is a key DL operation involving activation and weight matrix multiplication, defined in the BLAS library [16], and can be expressed as:\n\n$\\\u039f := \u03b1\u0391\u00d7B + BC$\n\nwhere O is the output matrix of size M\u00d7P, and A, B, and C, are input matrices of sizes M\u00d7N, N\u00d7P, M\u00d7P, respectively. \u03b1 and \u1e9e are scaling factors.\nThe compute-intensive nature of fully connected and convolutional layers allows them to be efficiently mapped to matrix multiply units. A study indicates that up to 89% of CPU runtime and 95% of GPU runtime are spent processing fully connected and convolutional layers in AlexNet [17], underscoring the critical role of GEMM in DL computations.\nB. tubGEMM\ntubGEMM [10][11] is a novel temporal-binary hybrid INT-based GEMM architecture that inherently leverages the sparsity found in DL workloads, resulting in significant improvements in latency and energy efficiency. This architecture processes activation data as binary inputs while encoding"}, {"title": "C. NVDLA Accelerator", "content": "NVDLA [12] is an open-source, industry-grade inference engine developed by NVIDIA, integrated into Xavier systems on chip (SoCs) for self-driving applications. It offers an end-to-end software-hardware stack that provides a DL framework along with a runtime environment and spans all the way to RTL implementation. This study employs the nv_small configuration, which is tailored for embedded and cost-sensitive applications [12]. Unlike the nv_large variant, the nv_small design does not include a dedicated control processor; instead, it relies on the host processor for task scheduling, memory management, and NVDLA control. Although other open-source DLAs, such as Gemmini [18], exist, they are primarily academic designs focused on GEMM acceleration. Moreover, NVDLA was reported to be 3.77x faster than Gemmini when comparing equivalently sized configurations running ResNet-50 [19]. Given its performance and proven industry reliability, we select NVDLA as our experimental platform.\nFig. 3 provides a high-level overview of NVDLA's convolution pipeline. It comprises (i) the convolution buffer (CB), which stores input activations and filter weights, (ii) the convolution core (CC), which serves as the convolution datapath, and (iii) the post-processing unit, which includes the activation engine, the pooling engine, and other components essential for accelerating CNNs. The primary datapath performing MAC operations to calculate partial sums in CC is the convolution MAC (CMAC) unit. The CMAC unit consists of k processing element (PE) cells (termed MAC Cells in [12]), each with n multipliers, producing a partial result for each kernel. Data is broadcast from the CB via a convolution scheduler (CSC) to the k PE cells, with partial results accumulated in the adder trees of the convolution accumulation (CACC) unit. Each cell supports clock gating to reduce dynamic power consumption during idle or underutilized conditions when the kernel count is insufficient for full utilization. In addition, CMAC features intermediate registers that facilitate retiming and pipelining.\nTo summarize the NVDLA microarchitecture hierarchy:"}, {"title": "III. TEMPUS CORE MICROARCHITECTURE", "content": "The Tempus Core microarchitecture (Fig. 3) is implemented as a temporal-unary-binary (tub) hybrid convolution engine, designed as a drop-in replacement for the convolution core (CC) in NVDLA. Tempus Core adheres to the original dataflow in NVDLA and can directly replace its convolution core. Tempus Core consists of a modified CSC, PE cell unit (PCU), and the CACC. The PCU is analogous to the CMAC unit in NVDLA, containing k \u00d7 n tub-based PE array for efficient MAC operation. In this setup, direct convolution involves sharing the feature data array across k PE cells, with each cell caching a different weight data array. Each PE cell contributes a partial sum, producing k partial sums per feature and weight data array. These partial sums are subsequently accumulated in the CACC unit. While NVDLA's CMAC unit generates k partial sums per cycle, the Tempus Core'sPCU generates k partial sums over multiple cycles, with the cycle count equal to the largest weight magnitude in the kxn array. The modified CSC in Tempus Core feeds transposed feature data since W \u00d7 FT = accum(WF). Note each PE cell also incorporates n 2s-unary blocks into the temporal encoder, allowing optimal temporal-to-binary data conversion. The registers corresponding to the PE cells cache the partial sums, which are only forwarded to the CACC once all partial sums have been generated across the cells.\nHaving detailed the key components of Tempus Core, we now outline its dataflow structure and its seamless integration into existing DLAs.\n\n\u2022 The PCU comprises a k\u00d7 n PE array, with dedicated registers for caching outputs and handshaking logic to coordinate multi-cycle operations.\n\u2022 The input data and initial weight data cubes are divided into 1\u00d71\u00d7n element cubes. Each PE cell caches one weight cube from the weight kernel, and the single input data cube is shared between the k PE cells.\n\u2022 Each tub multiplier performs a multiplication between a single weight and input data element over multiple cycles, and the adder tree in the cell accumulates the results into a partial sum to be fed next to the CACC unit.\nFurther microarchitectural enhancements in Tempus Core include the integration of additional handshaking protocols with buffer blocks to accommodate multiple tub cycles per partial sum computation within each PE cell. This modification ensures efficient synchronization and data integrity. The number of compute cycles for k\u00d7 n array in Tempus Core is determined by the largest weight magnitude present in the array. We profile quantized CNNs to report application-specific latency per array in Section V."}, {"title": "IV. EVALUATION METHODOLOGY", "content": "This section details the evaluation setup, profiling methodology, and key EDA tools used to measure latency and energy values across designs. Comparative analysis between NVDLA's CC and Tempus Core is performed at three levels of granularity: (i) single PE cell, (ii) k \u00d7 n PE array, and (iii) entire CMAC unit versus PCU in Tempus Core. The analysis spans both INT8 and INT4 precisions, with varying k \u00d7 n configurations. The designs were synthesized using NanGate45 open-source cell library in Synopsys Design Compiler, operating at a fixed 250 MHz clock frequency to maintain consistent timing across evaluations. The binary PE cells are elaborated with DesignWare-optimized multipliers and Wallace adder tree during synthesis. Furthermore, place-and-route results are obtained using Cadence Innovus with the NanGate45 library to further extend the evaluation.\nWeight-value profiling is performed on two INT8-quantized models, MobileNetV2 and ResNeXt101, to determine application-specific latency and energy consumption. Using a 16\u00d716 (k=16, n=16) max pool across weights present in the model's convolution layers, the largest weight value within each 16x16 tile is determined and its frequency of occurrence as the largest value across is derived. This directly correlates to the compute cycles since the largest value across an array of 16 PE cells with 16 multipliers bottlenecks the tub model compute. Note the PCU takes a few extra cycles for caching in and out the values, hence analysis pertains to the array. Additionally, sparsity is analyzed in a similar fashion to estimate the average number of \u201csilent\u201d PEs per array, where tub multipliers remain inactive for zero-valued weights."}, {"title": "V. RESULTS", "content": "A. Area-Power Efficiency\nPost-synthesis results for a single PE cell, containing registers, multipliers, and an adder tree, are summarized in Table II. Single tub-based PE cells significantly outperform their binary counterparts in both area and power efficiency. For INT8, tub multipliers reduce area by 91.8% and power by 91.3% for n=1024 PEs. INT4 implementations achieve 88.3% area and 87.2% power reductions. PE cell with tub multipliers demonstrate superior scalability across n: area scales by 7.7x for INT4 and 8.5x for INT8, compared to 16.9x and 19x for binary PE cell. Power consumption scales by 2.9x for INT4 and 3.5x for INT8, versus 11.4x and 15x for binary PE cell.\nTempus Core 's scalability is maintained as we scale from a single PE cell to a 16 \u00d7 16 array. At INT8 precision, the binary-based implementation requires 0.09 \u00b5m\u00b2 of area and 3.8 mW of power, respectively. In comparison, Tempus Core's tub-based PE cells consume only 0.018 \u00b5m\u00b2 and 1.42 mW, achieving a 75% area reduction and 62% power savings. Similarly, for INT4, the reductions are 80% in area and 41% in power. These results are illustrated in Fig. 4.\nFinally, moving further up in the hierarchy, post-synthesis results of the entire CMAC unit and the PCU of Tempus Core are compared and illustrated in Fig. 5, for various widths (n) in INT8, INT4, and INT2 precisions. The PCU improves area and power consumption by 59.3% and 15.3%, respectively.\nKey Takeaway: PCU (and in turn, Tempus Core) demonstrates significantly superior area-power efficiency compared to the baseline binary-based CMAC unit across diverse integer-based precisions and granularities of compute datapath, paving the path for more silicon-optimized and scalable DLAs.\nB. Place-and-Route Results\nPlace-and-route analysis for INT4-based CMAC and PCU units with 16 \u00d7 4 array is conducted using 45nm CMOS, maintaining 70% floorplan utilization for a fair comparison. As shown in Table III, PCU reduces total area by 53% and power by 44% compared to binary multipliers. This confirms the potential of temporal-unary designs for low-precision edge-based DLAs. The area utilization for both designs is illustrated in Fig. 6, which depicts the decrease in area utilization of the PCU design compared to the CMAC unit.\nC. Energy Evaluation with Workload-Dependent Latency\nTempus Core's 2s-unary encoding [10] allows it to leverage dynamic value sparsity, improving energy efficiency by reducing compute latency. Higher sparsity in the unary encoded signal results in lower latency. We profiled two DNN workloads, MobileNetV2 and ResNeXt101, using max-pooling over 16 \u00d7 16 tiles of convolution layer weights. Fig. 7 shows"}, {"title": "D. Iso-Area Throughput Improvements", "content": "While the Tempus Core design incurs multiple cycles for convolution due to the incorporation of the tub PEs, throughput improvements can transcend the latency increase. In NVDLA's binary PE array, k partial sums are produced per cycle, whereas the tub-based array in Tempus Core requires Mkxn cycles, where mkxn represents half of the largest weight magnitude in the array (2s-unary encoding). INT8 analysis shows 16 binary PE cells consume 0.09 \u00b5m\u00b2 (from Fig. 4), whereas 80 tub PE cells fit in the same area (0.018 \u00b5m\u00b2 for 16 PE cells), yielding a 5x throughput improvement. Similarly, INT4 shows 4x iso-area throughput boost for the same array size. For a single PE cell, scaling across different n number of multipliers (reported in Table II), the iso-area throughput improvements are illustrated in Fig. 9 for both INT8 and INT4 precisions (assuming the same m cycles to generate one partial product). From the area scaling estimates, we can further project iso-area throughput improvements for a quadratic increase from n = 256 to n = 65536 multipliers. The throughput increases by as much as 26x and 18x for INT8 and INT4 precisions, respectively. Hence, based on these observations, we expect iso-area throughput gains to scale even further with increasing array sizes, compensating for the increased compute cycles thus further improving the practical feasibility of Tempus Core."}, {"title": "VI. CONCLUSIONS AND FUTURE WORK", "content": "In this paper, we introduced Tempus Core, a temporal-unary-binary hybrid convolution engine optimized for seamless integration into modern edge DLAs. By extending unary computing beyond GEMM-level optimizations to support full convolution operations, Tempus Core demonstrates substantial improvement in area and power efficiency. By demonstrating 5x and 4x iso-area throughput improvements for INT8 and INT4 precisions, respectively, for a 16 \u00d7 16 PE array, we have established a pathway to significantly enhance the performance-per-unit cost ratio of edge AI accelerators. In the future, we would like to extend the work towards unary-based compute architectures targeted towards ultra-low precision quantized large language models (LLMs). Additionally, we aim to explore custom dataflows and compiler optimizations that further reduce latency and energy consumption, enhancing the practicality of unary computing in AI hardware."}]}