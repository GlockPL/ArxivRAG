{"title": "Boosting Fine-Grained Visual Anomaly Detection with Coarse-Knowledge-Aware Adversarial Learning", "authors": ["Qingqing Fang", "Qinliang Su", "Wenxi Lv", "Wenchao Xu", "Jianxing Yu"], "abstract": "Many unsupervised visual anomaly detection methods train an auto-encoder to reconstruct normal samples and then leverage the reconstruction error map to detect and localize the anomalies. However, due to the powerful modeling and generalization ability of neural networks, some anomalies can also be well reconstructed, resulting in unsatisfactory detection and localization accuracy. In this paper, a small coarsely-labeled anomaly dataset is first collected. Then, a coarse-knowledge-aware adversarial learning method is developed to align the distribution of reconstructed features with that of normal features. The alignment can effectively suppress the auto-encoder's reconstruction ability on anomalies and thus improve the detection accuracy. Considering that anomalies often only occupy very small areas in anomalous images, a patch-level adversarial learning strategy is further developed. Although no patch-level anomalous information is available, we rigorously prove that by simply viewing any patch features from anomalous images as anomalies, the proposed knowledge-aware method can also align the distribution of re-constructed patch features with the normal ones. Experimental results on four medical datasets and two industrial datasets demonstrate the effectiveness of our method in improving the detection and localization performance.", "sections": [{"title": "Introduction", "content": "Visual anomaly detection (AD) aims to detect images that show significant deviation from normal ones. In practice, the deviation could happen at image-level or at the more fine-grained pixel-level, where anomalies only appear at certain local areas, while the remaining large region looks to be normal (Yang et al. 2022). Fine-grained anomalies happen a lot in real-world applications, such as the X-ray/CT images with nodules or other lesions in medical diagnosis (Fernando et al. 2021), images of products with defects like undesired scratches or small holes in industrial manufacturing etc (Liu et al. 2024). Because of the ubiquitousness of fine-grained anomalies and their intrinsic distinction from image-level anomalies, in this paper, we focus our attention on the fine-grained anomaly detection. In addition to detecting anomalies, some applications also require locating the anomalies, like automatically marking the defective areas (Roth et al. 2022a; Liu et al. 2023b). Thus, it is of great value to develop methods that can simultaneously detect and locate anomalies accurately.\nConsidering the extreme diverseness of anomalies, existing methods (Ruff et al. 2018; Akcay, Atapour-Abarghouei, and Breckon 2019; Tack et al. 2020; Roth et al. 2022b; Deng and Li 2022; Reiss and Hoshen 2023) mostly only assume the access to normal datasets. By using it to train a model that characterizes the normal patterns, anomalies can then be detected by assessing their conformity to it. The reconstruction-based method (Akcay, Atapour-Abarghouei, and Breckon 2019; Vasilev et al. 2020; Zavrtanik, Kristan, and Sko\u010daj 2021b; Guo et al. 2023), as one of the most widely-used fine-grained anomaly detection methods, is a concrete example of this idea, which first trains an auto-encoder on normal images and then uses the reconstruction error map to detect and localize anomalies. However, due to the strong modeling and generalization ability of neural networks, it is found that auto-encoders sometimes can also reconstruct the anomalous areas well due to the leakage of visual information from other parts of images, causing the reconstruction error map unreliable for anomaly detection and localization (You et al. 2022).\nThe reason why the reconstruction-based methods cannot lead to competitive performance can be largely attributed to the only usage of normal samples when training the auto-encoder, which makes the decision boundary between normal and anomalous samples blurred (Xia et al. 2022). To boost the detection and localization accuracy, we propose to introduce a small dataset comprised of anomalous images. We argue that it is feasible to collect a small number of anomalies in many real-world applications (Ruff et al. 2019; Pang et al. 2021a; Ding, Pang, and Shen 2022). As exemplified in medical diagnosis and industrial manufacturing, there often exist some X-Ray images that have been confirmed to be problematic by veteran doctors, or images that are taken for products with defects. But compared with datasets in classification tasks, the anomaly datasets possess two unique characteristics: 1) Coarseness: anomalies only occupy a small area of the image, but only the entire image is known to be anomalous; 2) Incompleteness: the dataset only covers a tiny subset of all possible anomaly types due to the diverseness of anomalies. Thus, the dataset can be viewed as a kind of coarse anomaly knowledge, and our goal is to make use of it for better anomaly detection and localization. We note that some recent works have also proposed to collect an additional anomaly dataset and then use it to help detect anomalies like the Deep SAD (Ruff et al. 2019), PANDA-OE (Reiss et al. 2021), DevNet (Pang et al. 2021a), DRA (Ding, Pang, and Shen 2022), AA-BiGAN (Tian, Su, and Yin 2022), PReNet (Pang et al. 2023), etc. However, these methods simply view an entire image in the anomaly dataset as anomalous, and rarely explicitly take the coarseness characteristics of anomalies into account. Thus, when they are applied to fine-grained anomaly detection problems, they may not make the best use of the knowledge hidden in the anomaly dataset, resulting in a performance that is not good enough. Moreover, all of these methods only consider how to improve anomaly detection accuracy, but rarely bear in mind the anomaly localization task that is also important in many scenarios.\nIn this work, we proposed to leverage the coarse anomaly knowledge to boost the anomaly detection and localization performance of feature-reconstruction-based methods. To this end, a Coarse-Knowledge-Aware adversarial learning Anomaly Detection (CKAAD) strategy is developed to suppress the auto-encoder's reconstruction ability on anomalies by seeking to align the distribution of reconstructed features with that of normal features, where the features are extracted from a pre-trained ResNet in advance. Specifically, we first utilize the coarse anomalous knowledge to develop an energy-based discriminator, which is trained to assign low energy for normal features, but high energy for reconstructed features and anomalous features. The discriminator can effectively incentivize the auto-encoder to output normal features, while explicitly avoiding outputting anomalous features, thereby achieving the alignment with the distribution of normal features. Considering that anomalies often only occur in small areas in the anomalous images, a patch-level adversarial learning strategy is further developed. Because there are no patches that are known to be anomalous surely, we show that we can simply view all patches from an anomalous image as anomalies and then apply a similar adversarial learning strategy at image-level to guide the auto-encoder's reconstruction. We rigorously prove that the proposed patch-level adversarial learning strategy can also lead to the alignment between the reconstructed features and normal features, even though the patch-based anomalous knowledge is not completely correct. Experimental results on real-world anomaly datasets including four medical datasets and two industrial datasets demonstrate that by leveraging a small number of coarsely labeled anomalies, our proposed coarse-knowledge-aware method can boost the detection and localization performance remarkably."}, {"title": "Related Work", "content": "Unsupervised methods Existing unsupervised methods only use normal samples. Among them, reconstruction-based methods commonly use auto-encoder (Zavrtanik, Kristan, and Sko\u010daj 2021b; Zhang, Wang, and Chen 2022), variational auto-encoder (Vasilev et al. 2020) or generative adversarial network (Schlegl et al. 2017; Akcay, Atapour-Abarghouei, and Breckon 2019) to reconstruct normal images, assuming that unseen anomalous images can not be reconstructed well, so the difference between the input and reconstructed images can be used to detect anomalies. Because these image-level methods hardly localize anomalous areas accurately, the feature reconstruction methods (Salehi et al. 2021; Deng and Li 2022; Tien et al. 2023; Guo et al. 2023) are further proposed. But whether reconstructing in the image or feature level, these reconstruction-based methods face the same problem that the anomalies are also restored well, decreasing the detection performance. Besides the reconstruction-based methods, one-class methods (Ruff et al. 2018; Yi and Yoon 2020; Reiss et al. 2021; Gui et al. 2022) map normal images or patches to a compact space, then determine anomalies based on the distance. Embedding-based methods (Tack et al. 2020; Defard et al. 2021; Cho, Seol, and Lee 2021; Reiss and Hoshen 2023) discriminate anomalies according to the learned embedding features. Other unsupervised methods (Reiss et al. 2021; Zavrtanik, Kristan, and Sko\u010daj 2021a; Li et al. 2021; Liu et al. 2023b) utilize synthesized pseudo anomalies for directly predicting anomaly labels or masks, can result in poor detection performance on real anomalies.\nWeakly-supervised methods In recent years, some weakly-supervised methods have been proposed to enhance anomaly detection performance by utilizing observed partial anomalies. Based on pre-trained models, PANDA-OE (Reiss et al. 2021) directly employs classification models, but directly using a discriminator to find a boundary can result in misclassifications of unseen anomalies. Instead of training a classifier, Deep SAD (Ruff et al. 2019) maps normal samples to a compact space, pushing anomalous samples away. DPLAN (Pang et al. 2021b) guides the model to find anomaly samples and assigns higher anomaly scores through reinforcement learning. GAN-based methods (Kimura et al. 2020; Tian, Su, and Yin 2022; Tian, Su, and Yu 2023; Lv et al. 2024) avoid generating anomalous samples and get the anomaly score based on the norm of latent feature or the reconstruction error. DevNet (Pang et al. 2021a) and DRA (Ding, Pang, and Shen 2022) use deviation loss or score heads to separate anomalous patches from normal ones. PReNet (Pang et al. 2023) detects anomalies based on pairwise anomaly scores learned by three kinds of instance pairs. However, these methods directly enlarging the distinction of anomaly scores between normal and anomalous samples, are prone to over-fit on these collected samples. Besides, most of them primarily use these incomplete anomalies for detection, rarely leveraging such weakly supervised information to achieve accurate localization performance."}, {"title": "Preliminaries on Feature Reconstruction-Based Anomaly Detection", "content": "Due to the diverseness of anomalies, current anomaly detection methods are generally established on the assumption of access to a normal dataset $X^+ = \\{x_1^+, x_2^+, ..., x_N^+\\}$. By seeking to characterize the normal patterns in normal samples as accurately as possible, anomalies can be identified by checking whether they conform to the normal patterns. Regarding methods following this approach, the most widely used one is the reconstruction-based method, which basically trains an auto-encoder (AE) on normal samples and then uses the reconstruction error to detect anomalies.\nFor image data, to yield better performance, it is often suggested to reconstruct feature maps ${\\{F^l\\}}_{l=1}^L$ that are extracted from pre-trained ResNet (He et al. 2016), instead of reconstructing original images, with $F^l$ and $L$ denoting the feature map from the l-th block of ResNet and the total number of blocks. Since the feature maps from different blocks are of different sizes, when implementing the encoder, we first down-sample maps to the same size as the smallest one by a sequence of 3 \u00d7 3 convolutional operators and then concatenate them along the channel dimension. The concatenated maps are then further fed into a ResNet block to produce the final latent representation z, as shown in Figure 1. The whole procedure can be described as $z = E({\\{F^s\\}}_{s \\in S})$, where $S \\subset \\{1,2,\\dots,\\dots, L\\}$ represents a subset of L ResNet blocks that are chosen to participate in reconstruction. Given the latent code z, residual deconvolution operations are then employed to up-sample it stage by stage to recover the feature maps of different sizes $F^s$ for $s \\in S$, that is, ${\\{F^s\\}}_{s\\in S} = D_e(z)$. By viewing the original feature maps ${\\{F^s\\}}_{s\\in S}$ as input and the recovered maps ${\\{F^s'\\}}_{s\\in S}$ as output, the encoder $E_\\theta(\\cdot)$ and decoder $D_\\theta(\\cdot)$ can be jointly written as\n$\\{F^s'\\}_{s\\in S} = G_\\theta({\\{F^s\\}}_{s\\in S}),$ (1)\nwhere $G_\\theta(\\cdot) = D_\\theta(E_\\theta(\\cdot))$.\nThe auto-encoder $G_\\theta(\\cdot)$ is trained to reduce the difference between $F^s'$ and $F^s$ for $s \\in S$ on all training normal samples by minimizing the loss\n$L_{rec} = \\frac{1}{\\vert X^+\\vert} \\sum_{x \\in X^+} \\sum_{s \\in S} (1 - cos( flatten (F^s), flatten(F^{s'})) )$ (2)\nwhere flatten() and $cos(\\cdot,\\cdot)$ mean flattening the tensor into one-dimensional vector and the cosine similarity. Since the loss is trained only on normal samples, the auto-encoder tends to reconstruct the input normal images well, but poorly for anomalous ones. That is why the error maps between $F^s$ and $F^{s'}$ can be employed to detect and localize the anomalies. However, due to the generalization ability of neural networks, it is found that some anomalous images can also be well reconstructed, making the error map not always reliable in detecting and localizing the anomalies."}, {"title": "Boosting Reconstruction-Based Anomaly Detection via Coarse-Knowledge-Aware Adversarial Learning", "content": "To improve the detection and localization accuracy, in addition to the normal dataset $X^+$, we further assume the availability of a small anomaly dataset\n$X^- = \\{x_1^-, x_2^-, ..., x_m^-\\}$. (3)\nAs elaborated in the Introduction, due to the diverseness of anomalies, the collected anomalies in $X^-$ may only cover a very tiny number of all possible anomaly types. Moreover, due to the annotating cost, we assume we only know the images in $X^-$ are anomalous, but have no knowledge of the exact location of the anomalies. This paper aims to leverage this coarse anomalous knowledge to better detect and locate anomalies.\nOne of the most direct ways to leverage the dataset $X^-$ is to minimize the following loss\n$L'_{rec} = L_{rec} + \\lambda L_{rec}^-,$ (4)\nwhere $L_{rec}^-$ denotes the reconstruction error computed over samples in $X^-$; and $ \\lambda \\in R^+$ is a positive weighting factor. However, the loss $L'_{rec}$ only drives the auto-encoder to output feature maps $F^s$ that look different from the input $F^s'$ for anomalous samples, but never restricting how the difference should look like. Because anomalies often only occupy a small area in anomalous images in many real-world applications, simply enlarging the reconstruction error of anomalous samples has the risk of turning normal areas into anomalies, deteriorating the detection and localization performance. To enable the auto-encoder to detect and localize the anomalies accurately, we argue that we should encourage it to possess the following two properties:\nP1) The auto-encoder should manage to transform an anomalous image to its normal counterpart, e.g., transforming an anomalous image with a small hole into one that has the hole removed.\nP2) The reconstruction quality of normal samples should not be compromised too much by the introduced anomalies.\nIf the two appealing properties hold, the reconstruction error map between input and output features could be used to detect and localize the anomalies more reliably."}, {"title": "Aligning the Distributions of Reconstructed Features with Normal Features", "content": "To achieve the appealing properties above, we propose to view the auto-encoder as a generator $G_\\theta(\\cdot)$. For the simplicity of presentation, we only present how to process one feature map $F^s \\in {\\{F^s\\}}_{s\\in S}$, and denote the chosen map $F^s$ as F for conciseness, while the remaining feature maps can be processed similarly. Since we now have two datasets available, $X^+$ and $X^-$, we can use them to construct a distribution regarding input feature maps as\n$P(F) = \\alpha P^+(F) + (1 - \\alpha) P^-(F),$ (5)\nwhere $P^+(F)$ and $P^-(F)$ represent feature map distributions of normal samples $x^+ \\in X^+$ and anomalous samples $x^- \\in X^-$, respectively; $ \\alpha \\in [0, 1]$ is a parameter used to control the mixture ratio of the two distributions. For any input feature F drawn from the distribution P(F), the generator (auto-encoder) will output a feature map $G_\\theta(F)$. The distribution of the generated feature map $G_\\theta(F)$ can be represented as\n$P_g(F) = \\alpha P_g^+(F) + (1 - \\alpha) P_g^-(F),$ (6)\nwhere $P_g^+(F)$ and $P_g^-(F)$ denote the distributions of generated (reconstructed) feature map $G_\\theta(F)$ when the input F is drawn from $P^+(F)$ and $P^-(F)$, respectively. If the distribution of $P_g(F)$ is forced to align with the distribution of normal features $P^+(F)$, it amounts to driving $P_g^-(F)$ towards $P^+(F)$, that is, having the reconstructed features of anomalous samples to look similar to normal ones. In this way, the auto-encoder transforms an anomalous sample into a normal one effectively, thereby partially achieving the appealing property P1.\nImage-Level Alignment To align the generative distribution $P_g(F)$ with normal feature distribution $P^+(F)$, a generative adversarial network (GAN) (Goodfellow et al. 2014) of the form below can be used to regularize the generator\n$\\min_G \\max_D E_{F \\sim P^+(F)} log[D(F)] + E_{F \\sim P_g(F)} log[1 - D(F)]$ (7)\nAfter convergence, the distribution of $G_\\theta(F)$ (i.e., $P_g(F)$) will be equal to $P^+(F)$. Although the vanilla GAN can align the generative distribution to the normal feature distribution, the valuable collected anomalies are only used to construct the input distribution P(F). Since the discriminator has not been informed of any anomaly information, the discriminator could mistakenly recognize anomalies as normal samples, compromising the model's ability of transforming anomaly images into normal ones. To address this issue, we propose to use the anomaly dataset $X^-$ to enhance the discriminator's ability to distinguish between normal and anomalous samples. To this end, we propose an energy-based discriminator $D(\\cdot) : F \\rightarrow [0, +\\infty)$, which is designed to assign low energy to normal features and high energy to generated and anomalous features. The discriminator is trained by minimizing the following loss\n$L_D(D,G_\\theta) = E_{F \\sim P^+(F)}D(F) + \\gamma E_{F \\sim P_g(F)} [0, a - D(F)]_+ + (1 - \\gamma) E_{F \\sim P^-(F)} [0, a - D(F)]_+,$ (8)\nwhere $[u, v]_+ = max(u,v)$, $a > 0$ is a threshold, and $ \\gamma \\in (0,1]$ is the balance coefficient. With the trained discriminator, the generator is encouraged to output features to get lower energy by minimizing\n$L_G(D,G_\\theta) = \\gamma E_{F \\sim P_g^+(F)}D(F) + (1 - \\gamma) E_{F \\sim P_g^-(F)} D(F) - E_{F \\sim P^+(F)} D(F).$ (9)\nWhen (8) and (9) are used to update the discriminator and generator, it can be proved that the output feature distribution $P_g(F)$ can align with the normal feature distribution $P^+(F)$. We give the following theorem.\nTheorem 1. Let $P^+(F)$, $P^-(F)$ and $P_g(F)$ be the distributions of normal, anomalous, and generated feature maps. Assuming the $P^+(F)$ and $P^-(F)$ are two disjoint distributions, and $ \\gamma \\in (0,1]$. When Discriminator $D(\\cdot) : F \\rightarrow [0, +\\infty)$ and generator $G(\\cdot)$ are updated according to (8) and (9), there exists a Nash equilibrium of the system $(D^*, G^*)$ such that $P_g(F) = P^+(F)$."}, {"title": "Patch-Level Alignment", "content": "Proof. Please see the proof in the Supplementary Material.\nThe discriminator trained with loss (8) has the ability to identify anomalous features from normal ones. If an anomalous image is generated, it will be recognized by the discriminator, which will then incentivize the auto-encoder to transform it into normal one. On the other hand, when input to the encoder is a normal sample, output from the generator will be similar to the input, thus the adversarial learning will almost not affect this sample's reconstruction. In this way, we achieve the two appealing properties P1 and P2.\nPatch-Level Alignment The proposed knowledge-aware discriminator in (8) distinguishes between normal and anomalous features at image-level. In practice, anomalies often only occupy small areas of an image. Hence, to achieve more accurate detection and localization, it is better to perform the alignment at the patch level. If we directly divide an image into numerous patches and then pass them through the pre-trained ResNet and the auto-encoder, it will be very expensive. Fortunately, the feature at position (h, w) (i.e., F(h, w)) corresponds to a patch in the origin image, thus it can be viewed as the feature of a patch. Similarly, the generated feature $F'(h, w)$ at position (h, w) can be viewed as the generated path feature. As shown in Figure 1, $a^+$ and $a^-$ represent a patch feature of a normal and abnormal image, respectively.\nDenote the distribution of features of normal patches as $P^+(p)$. Obviously, features of all patches from normal images follow the distribution $P^+(p)$. For the generated feature $F'(h, w)$ at position (h,w), we denote its distribution as $P_g^{(h,w)}(p)$, and further denote the generator that reconstructs the (h, w)-th patch feature as $G_\\theta^{(h,w)} (\\cdot)$. To align the distribution of generated patch features with that of normal patch features, unlike the alignment at the image-level, no path-level anomaly annotations are available. For an anomalous image, it contains both normal and anomalous patches. Thus, for images from $X^-$, we only know that its patch features follow a mixture distribution\n$P_m(p) = \\beta P^+(p) + (1 - \\beta) P^-(p),$ (10)\nwhere $ \\beta \\in (0,1)$ is the unknown mixed ratio of $P^+(p)$ and $P^-(p)$, with $P^-(p)$ denoting the distribution of anomalous patches. Although the available anomalous knowledge is noisy, the following theorem shows that we can still make use of it to enhance the auto-encoder's ability to recover normal patches from anomalous ones.\nTheorem 2. Suppose the $P^+(p)$ and $P^-(p)$ are two disjoint distributions and $ \\gamma \\in (0,1]$. When Discriminator $D^{(h,w)}(\\cdot) : P \\rightarrow [0, +\\infty)$ and generator $G_\\theta^{(h,w)}(\\cdot)$ are updated by minimizing the two losses\n$L_D(D^{(h,w)}, G_\\theta^{(h,w)}) = \\gamma E_{p \\sim P_g^{(h,w)}(p)} [0, a - D^{(h,w)}(p)]_+ + (1 - \\gamma) E_{p \\sim P_m(p)} [0, a - D^{(h,w)}(p)]_+ + E_{p \\sim P^+(p)} D^{(h,w)}(p),$ (11)\n$L_G(D^{(h,w)}, G_\\theta^{(h,w)}) = \\gamma E_{p \\sim P_g^{(h,w)}(p)} D^{(h,w)}(p) + (1 - \\gamma) E_{p \\sim P_m(p)} D^{(h,w)}(p) - E_{p \\sim P^+(p)} D^{(h,w)}(p),$ (12)\nthere exists a Nash equilibrium of the system $(D^{*(h,w)}, G_\\theta^{*(h,w)})$ such that $P_g^{(h,w)}(p) = P^+(p)$."}, {"title": "Training and Testing", "content": "Proof. Please see the proof in the Supplementary Material.\nAccording to (11), the discriminator is designed to assign low energy to normal patch features, while assigning high energy to patch features that are generated or from anomalous images. Although the patch discriminator seems contradictory in the sense of encouraging to output low and high energy to normal patch features simultaneously, due to the term $E_{p \\sim P^+(p)}D(p)$ in (11) has greater weight, its influence will override that of anomalous images $E_{p \\sim P_m(p)} [0, a - D^{(h,w)}(p)]_+$, thus leading to the overall low energy for normal patches. With the discriminator capable of outputting low and high energy for normal, anomalous and generated patches, by driving the generator to generate patches with low energy, we can align the distribution of generated patches with normal ones'. Thus, by comparing the input patch features with the reconstructed ones, anomalous areas can be localized accurately. In experiments, a common patch feature discriminator D(p) is used for all patch feature generators $G_\\theta^{(h,w)} (p)$.\nTraining and Testing For patch feature from layer s \u2208 S, the discriminator $D_s(p)$ can be used to distinguish patch features of all positions as (11). Combining discriminators of all layers that participate in the reconstruction, we can get a total discriminator loss and corresponding generator loss $L_D = \\sum_{s \\in S} \\sum_{h=1}^{H^s} \\sum_{w=1}^{W^s} L_D(D_s, G_\\theta^{(h,w)})$ and $L_G = \\sum_{s \\in S} \\sum_{h=1}^{H^s} \\sum_{w=1}^{W^s} L_G(D_s, G_\\theta^{(h,w)})$, where $H^s$ and $W^s$ are the height and width of the feature map $F^s$. As the structure shown in Figure 1, we use the knowledge-aware adversarial learning to regularize the output features of auto-encoder, and train the model by minimizing the following two losses alternatively\n$\\min_D L_D, \\min_\\theta L_{rec} + \\lambda L_G.$ (13)\nTesting For any test sample x, we first obtain its pre-trained feature maps $F'$s, and reconstructed feature maps $F'$s. For pixel-level anomaly detection, we compute the value\n$S_{map}(h, w) = 1 - cos (F_s(h, w), F'_s(h, w))$ (14)\nof position (h, w) in anomaly maps for s \u2208 S, and upsample them to the same size as the input image by bi-linear interpolation. Finally, we sum the upsampled error maps of all layers together and smooth the summed map with a Gaussian kernel, yielding the final anomaly score map $S_{map}$. For image-level anomaly detection, we obtain the average result of the top-k values in the anomaly maps to produce the final anomaly score $S_{img}$."}, {"title": "Experiments", "content": "Experimental Setups\nDatasets We mainly conduct experiments on real-world anomaly detection medical datasets and industrial datasets. Medical datasets: i) ISIC2018 (Tschandl, Rosendahl, and Kittler 2018; Codella et al. 2019): The ISIC2018 challenge dataset (task 3) contains 7 categories and we classify NV (nevus) as normal, the rest 6 categories as abnormal. ii) Chest X-rays (Kermany et al. 2018) contains normal and abnormal Chest X-rays scans. iii) Br35H(Hamada 2020; Zhou et al. 2024): Brain Tumor Detection dataset contains non-tumorous and various tumorous images. iv) OCT (Kermany et al. 2018): Retinal optical coherence tomography (OCT) contains normal OCT scans and three types of scans with diseases. Industrial dataset: i)MVTec AD (Bergmann et al. 2019) is a widely known industrial dataset comprising 15 classes with 5 textures and 10 objects. ii) Visa (Zou et al. 2022) is an industrial dataset containing 12 classes.\nImplementation All images are resized to 256. $ \\alpha$, $ \\gamma$ are set to 0.5. $ \\lambda$ is set to 0.02. Adam optimizer is utilized with $ \\beta = (0.5,0.999)$. The learning rate for the auto-encoder is set to le-03 for medical datasets, 5e-03 for industrial datasets, and le-04 for the discriminator. Resnet18 is chosen as the backbone and S = {2,3} for medical datasets. WideResnet50 and S = {1,2,3} are set for industrial datasets because the anomalies are more subtle.\nMetrics We report the AUC (Area Under receiver operating characteristic Curve), best F1, Accuracy (ACC) for medical datasets, and image-level AUC, pixel-level AUC and PRO (Per-Region Overlap) for industrial datasets.\nExperimental Scenarios For medical datasets, we evaluate the performance with consideration of two key parameters for the incomplete anomalous information: 1) the number of types of collected anomalies k; 2) the ratio of collected anomalies ri in the training data. Specifically, we first fix k = 1 but increase ri gradually. In this case, anomalies from any anomaly category can be used as the only labeled anomaly class during training, and performance averaged over all anomaly types is reported. Then, we fix ri but increase k gradually. The k anomaly types are randomly selected from all possible anomaly categories, so the performance is averaged over 5 random experiments. For industrial datasets, since there are no anomalies for training, elastic transformation (see details in Supplementary) is used to distort normal images to produce corresponding anomalies and the average performance of all classes is reported."}, {"title": "Detection Results on Medical Datasets", "content": "We compared our proposed methods with several unsupervised methods GANomaly(Akcay, Atapour-Abarghouei, and Breckon 2019), DifferNet(Rudolph, Wandt, and Rosenhahn 2021), Fastflow(Yu et al. 2021), AE-FLOW(Zhao, Ding, and Zhang 2023), ReContrast(Guo et al. 2023), and weakly-supervised methods Deep SAD(Ruff et al. 2019), DevNet(Pang et al. 2021a), AA-BiGAN(Tian, Su, and Yin 2022), DRA(Ding, Pang, and Shen 2022), PReNet (Pang et al. 2023). All methods are run under the same settings.\nPerformance under different ratios of labeled anomalies As shown in Table 1, with k = 1, the performance of unsupervised and weakly supervised methods is reported. Compared to the reconstruction baseline (Recon), our proposed method can enhance the reconstruction model's performance by utilizing labeled anomalies. Using only 1% anomalies, our method improves the AUC by 3.51% for ISIC2018, 7.63% for Chest X-ray, 1.52% for Br35H. While adding ri to 2% and 5%, the performance continuously increases. On the other hand, compared to other weakly-supervised methods, our method exhibits the best overall performance of three metrics on all medical datasets.\nPerformance under different types of labeled anomalies As Figure 2 shows, we evaluate the performance with different numbers of labeled anomaly types on ISIC2018 which"}, {"title": "Ablation Study", "content": "There are several strategies of utilizing anomalies, including the Recon (2), ReconSub (4), pure GAN (7), coarse-knowledge-aware image-level adversarial learning (CKAlmg)(8)(9) and patch-level adversarial learning (CKAPatch)(11)(12). To compare these strategies, we conduct experiments with $r_1 = 5\\%$, k = 1 for medical datasets and distorted anomalies for industrial datasets. As shown in Table 3, compared to Recon, ReconSub can decrease performance a lot on MVTec, indicating it is unsuitable for fine-grained"}, {"title": "Qualitative Visualization Results", "content": "We show the visualization results of using 5% of anomalies in the training set. As shown in Figure 6 and 7, the anomalous parts in the medical and industrial datasets are assigned higher anomaly scores. The qualitative results in these figures show that our methods can be used to localize anomalous areas using incomplete anomalous information."}, {"title": "Additional Experimental Results", "content": "We mainly evaluate the performance of our method on real-world anomaly detection datasets including medical and industrial datasets in previous experiments. Many anomaly detection methods conduct experiments on the commonly used dataset CIFAR10 for one-class classification. We also conduct experiments on CIFAR10. For each category in CIFAR10, we regard the category as normal while other categories as anomalous classes. Following the data setting on the medical dataset, for each class in CIFAR10, we conduct 5 random experiments by using 5% anomalies from randomly selected 3 anomalous classes in the training set. Totally, we conduct 50 experiments in the weakly-supervised scene. The results of performance are shown in Table 5. Compared to the reconstruction base, our method improves the average detection AUC on CIFAR10 from 88.32% to 92.14%, proving that our method is effective in such semantic anomaly detection datasets. Compared to other weakly-supervised methods, our method also gets the overall best detection performance for the one-class anomaly detection in CIFAR10."}]}