{"title": "Efficient Mixture-of-Expert for Video-based Driver State and Physiological Multi-task Estimation in Conditional Autonomous Driving", "authors": ["Jiyao Wang", "Xiao Yang", "Zhenyu Wang", "Ximeng Wei", "Ange Wang", "Dengbo He", "Kaishun Wu"], "abstract": "Road safety remains a critical challenge worldwide, with approximately 1.35 million fatalities annually attributed to traffic accidents, often due to human errors. As we advance towards higher levels of vehicle automation, challenges still exist, as driving with automation can cognitively over-demand drivers if they engage in non-driving-related tasks (NDRTs), or lead to drowsiness if driving was the sole task. This calls for the urgent need for an effective Driver Monitoring System (DMS) that can evaluate cognitive load and drowsiness in SAE Level-2/3 autonomous driving contexts. In this study, we propose a novel multi-task DMS, termed VDMoE, which leverages RGB video input to monitor driver states non-invasively. By utilizing key facial features to minimize computational load and integrating remote Photoplethysmography (rPPG) for physiological insights, our approach enhances detection accuracy while maintaining efficiency. Additionally, we optimize the Mixture-of-Experts (MoE) framework to accommodate multi-modal inputs and improve performance across different tasks. A novel prior-inclusive regularization method is introduced to align model outputs with statistical priors, thus accelerating convergence and mitigating overfitting risks. We validate our method with the creation of a new dataset (MCDD), which comprises RGB video and physiological indicators from 42 participants, and two public datasets. Our findings demonstrate the effectiveness of VDMOE in monitoring driver states, contributing to safer autonomous driving systems. The code and data will be released.", "sections": [{"title": "I. INTRODUCTION", "content": "ROAD safety is still an extreme challenge for societies [1], [2]. The World Health Organization (WHO) reported that approximately 1.35 million people die each year as a result of road traffic accidents, with human error being a significant contributing factor. While autonomous technologies promise to reduce human error-the leading cause of traffic accidents-there remains a critical period where human drivers must share control with automated systems. Particularly, at the Society of Automotive Engineers (SAE) Level-2 (L2) automation [3], the vehicle can control both steering and acceleration/deceleration but still requires the driver to remain actively engaged and ready to take over at any moment; while with SAE Level-3 (L3) automation, the vehicle can handle most driving tasks but still requires human drivers to step in when prompted by the take-over requests (TORs). Both have shifted the role of the driver from an operator to a supervisor. As a result, drivers are more likely to experience low cognitive load if driving is the sole task, or more likely to be cognitively over-demanding, if drivers are engaged in non-driving-related tasks (NDRTs), both can be detrimental to driving performance [4]. For example, the cognitive overload induced by multi-dimensional cognitive resource consumption can lead to a limited field of vision [5], and a reduced ability to foresee potential hazards [6]. Futher, the effect of long-time driving fatigue and the high cognitive load as a result of NDRTs may co-exist, leading to impaired driving performance [7], [8]. Thus, the Driver Monitoring System (DMS) that is capable of estimating drivers' cognitive load and drowsiness is still critical to enhancing the safety and reliability of SAE L2/L3 vehicles.\nDMS has been widely studied in recent years. Traditional DMS relied on various sensors, including physiological sen-sors (e.g., ECG sensors on the steering wheel) and vehicle-based sensors (e.g., steering angle, lane departure warnings) [9]. However, in SAE L2 or L3 vehicles, the driver's manual driving performance data is not available most of the time, as the vehicle is controlled by the driving automation. Physiological signals, though can be served as effective indicators of driver's state [10], [11], usually depend on invasive phys-iological sensors (e.g., electroencephalography (EEG) [12], [13], electrooculography (EOG), [14]) and thus are currently not practically applicable in vehicle cabins. Although there were some attempts to leverage various non-invasive physio-logical sensors [15], [16], drivers were still required to wear inconvenient and costly signal acquisition equipment, which is still far from large-scale commercialization. Thus, considering the feasibility and cost of real-world in-vehicle deployment, camera is still a compelling alternative to monitor driver states non-intrusively [17]. Especially, the fast development in deep computer vision enables efficient non-contact state detection, and thus, in recent years, several facial image-based driver drowsiness detection methods have been proposed [18], [19].\nHowever, most of the previous methods were based on single frame detection, which loses temporal variation in both surface facial expression and internal physiological reflection. Compared to image-based methods, video-based methods [20], [17], [21] can make effective use of continuous temporal features and have the potential to enhance detection accuracy. However, existing methods still have the following short-comings: (1) some video-based drowsiness detection methods [22] directly utilize continuous frames as the input, leading to significant computational resources and interference from redundant information; (2) some methods [17], [23] tried to replace video by key optimized facial features (e.g., landmarks, eye and mouse areas), while they cannot assess physiological features without supplementing signals from physiological sensors; (3) current video-based DMS focused mostly on single-task fatigue [20], [17], [21] or distraction [24] estima-tion. However, driver states do not appear independently; in-stead, the interaction effects among multiple states make driver state estimation complicated. For example, in the context of conditional autonomous driving (i.e., driving with SAE L2/L3 vehicles), the cognitive load can moderate both fatigues [25] and physiological reflection [26]. However, few non-contact solutions were proposed for cognitive load estimation, fatigue detection and physiological reflections. Given that training separate models for each task incurs high deployment costs and reduces iterative efficiency [27], it is urgent to develop a unified model and make full use of the association across drivers' multiple states.\nTo address the aforementioned issues, we propose an RGB video-based multi-task DMS (VDMoE). Specifically, based on previous studies [17], [34], we first utilized the key facial features to reduce redundant information and computational costs. At the same time, considering cardiac and respiration activities are crucial predictors of driver states [26], [25], and are also important driver health indicators, we integrated the remote Photoplethysmography (rPPG) technology [35] into our model. Since the facial landmark and local eye and mouth region process contain limited temporal color change of the facial skin pixel points, which can be converted to a change of the blood volume under the skin [27], we first selected key facial regions of interest based on facial landmarks. Then, by transforming the color space (from RGB to YUV) as well as by band-pass denoising, we obtained the alter-native multi-modal information input Spatial-temporal Map (STMap). Next, to fully leverage the dependencies between different tasks, the Mixture-of-Experts (MoE) [36] structure is introduced. Being different from the classic MoE structure, the heterogenous gating mechanism and spatio-temporal expert separation are designed for the multi-modal alternative inputs to improve the multi-task performance. Besides, recent video-based DMS used various types of neural networks (e.g., CNN [20], Transformer [17]) that are suitable for processing different types of data or extracting different levels of feature information [37]. However, given the demand for real-time assessment of DMS, networks with excessive parameters and computational complexity will pose a challenge to deployment hardware. Therefore, we instantiated the nonlinear feature learning component of the basic block of VDMoE as a simple two-layer multi-layered perception (MLP) network to replace the previous modules represented by residual convolution [38] or multi-head attention [39]. Lastly, for the optimization goal, to explicitly capture the dependency between different states, we strengthen the learning capacity of VDMoE through one regularization based on prior knowledge from human factors fields [25]. Given there is no proper dataset for multi-task video-based DMS development that considered both the cog-nitive load and drowsiness in the driving automation context (seeing Table II), a driving simulator experiment with 42 participants was conducted. Extensive experiments based on our dataset illustrate the effectiveness of our method. In all, the contributions of our work are summarized as follows:\n\u2022\tA multi-task RGB video-based driver state monitoring method (VDMOE) is proposed in this work. This system concentrates on the fatigue and physiological reflection moderated by the multi-dimensional cognitive load in the context of driving automation.\n\u2022\tTo achieve the balance between estimation accuracy and efficiency, we replaced the full video with key facial features (landmarks, eye, and mouse area) as input, and introduced the STMap representing color changes in facial regions to supplement alternative physiological features.\n\u2022\tInspired by the superior performance of MoE in multi-task learning, we further optimized the MoE structure to accommodate the multi-modal and spatio-temporal input through heterogeneous gating mechanism and spatio-temporal expert separation. At the same time, we re-"}, {"title": "II. RELATED WORKS", "content": "Driver fatigue [40] can negatively affect drivers' perfor-mance when driving with automation, especially in takeover events. Thus, driver state monitoring is necessary to ensure safe driving in the context of driving automation [11]. Ac-cording to [41], [42], in conditional autonomous vehicles, certain physiological metrics can be strongly correlated with driver state. Therefore, researchers proposed various physio-logical signal-based driver state monitoring methods. Com-monly used psychophysiological measures include electro-physiological signals and human physical movements [43]. For example, Cui et al. [44] designed an interpretable con-volutional neural network for drowsiness detection using EEG signals. In addition, due to the difficulty of obtaining EGG signals, methods based on Electrocardiography(ECG) signals [45] and Electrodermal Activity(EDA) signals [46] have also been widely proposed. Yang et al. [34] developed an attention-enabled recognition network with a decision-level fusion archi-tecture to estimate cognitive load based on physiological data. Another study used physiological signals and facial images to detect driver fatigue [20]. However, the above-mentioned studies used physiological data collected through intrusive sensors, i.e., the sensors had to contact with the driver's body, which makes them infeasible to be used in the actual driving environment.\nSince 2008, Verkruysse et al. [35] proposed the rPPG technique for detecting physiological data using only a single consumer-grade camera. Nowadays, the rPPG technique has become a mainstream method in remote physiological data monitoring because of its non-contact characteristics. For ex-ample, to improve the robustness of pulse rate measurements, Hann et al. [47] proposed an analytical approach to tackle the motion problem in rPPG measurement. Wang et al. [48] introduced a mathematical model to increase understanding of the rPPG and designed a new algorithm to measure the heart rate. Tarassenko et al. [49] used autoregressive modeling to implement respiratory rate monitoring. However, traditional rPPG methods require manual adjustment of inaccurate data and are mostly limited to a single task.\nRecently, the combination of deep learning algorithms and rPPG technology has effectively improved accuracy in complex environments [50], [51]. Many studies have been conducted in the direction of multi-task monitoring of physi-ological data. For example, Liu et al. [52] presented a multi-task network to enable respiration and heart rate measurements on a mobile platform. Narayanswamy [53] et al. proposed a model based on facial movement, heart rate and respiration measurement. Wang et al. [27] designed a PhysMLE model to simultaneously calculate HR, heart rate variability, respiration rate, and blood oxygen saturation."}, {"title": "III. METHODOLOGY", "content": "From previous studies in human factors [25], [26], we know that both drivers' drowsiness and cognitive load are not only highly correlated with each other, but are also significantly associated with facial features (e.g., the degree of opening and closing of the eyes and mouth) and physiological features (e.g., HR, RR). Therefore, integrating state and physiological estimation into one multi-task model can effectively exploit correlations between tasks and features while compressing the computational cost further.\nInspired by this, we introduced a multi-task learning ar-chitecture, a video-based multi-task DMS mixture of experts model (VDMoE). As shown in Figure 1, suppose we have a batch of B raw videos. Being different from previous methods taking 3D videos as input [56], [19], we extracted heterogeneous facial features from videos as input X. Firstly, following [17], we extracted facial landmarks, and subregion facial videos (i.e., eyes and mouse) from the video. Then, as only movement features from landmarks and above subregions might be insufficient for physiological measurement, we com-bined the knowledge from rPPG [60], [61] to supplement skin light changes into the input X. The specific preprocessing steps are in the following section. Finally, given X, we tried to obtain both physiological signals {hi, ri}=1 and state estimation {di, ci }1 by training a multi-task model f(X;0), where hi is the heart rate, ri is the respiratory rate, di is drowsiness levels and ci is the cognitive load levels. The trainable parameter @ is our optimization objective. Finally, the multi-task estimation target is Y = {h, r, d, c}\nIn the original dataset, each video consists of T frames. Since each input sample is required to be of the same size, we segmented and augmented the raw dataset with the sliding window to obtain more samples with the same frames, effec-tively alleviating overfitting. We set the number of frames of all video samples to a fixed value T, and obtain the samples from the original video through a sliding window with a step size of S.\nAt the beginning of preprocessing, we first applied the facial detection methods to each frame of the input video to generate 2D coordinates of Nf facial landmarks with Nf key points. Since the length of each video is T frames and each facial landmark is represented by horizontal and vertical coordinates, denoted as (xi, Yi), each video can be represented as a facial landmarks matrix of shape T \u00d7 Nf \u00d7 2. Then, by using the outer boundaries corresponding to the facial landmarks as bounding boxes, the regions of the left eye, right eye, and mouth Il, Ir, Im were obtained from each original facial video frame. To standardize the input, the subregion video of the left eye and right eye It, Ir are resized to TX Le \u00d7 We \u00d7 C, where Le, We is the length, width of subregion videos corresponding to eyes. Similarly, the subregion video of mouse Im is in the size of T\u00d7 Lm \u00d7 Wm \u00d7 C, where Lm, Wm is the length, and width of mouse video. Subsequently, the entire facial areas were cropped into Ns, regions of interest (ROIs) [62]. These ROIs were converted from RGB to YUV. Then, since we know that the skin light changes owing to the volume and oxygen saturation of the blood corresponds to the low amplitude of low-frequency component [63], we used the first-order difference to remove the large amplitude caused by head movements and change in light. After a Butterworth filter with parameter [0.4,10] Hz to retain the frequency band of interest [62], we processed the ROIs into a spatial-temporal map (STMap) Is \u2208 RT\u00d7N\u300f\u00d7C to measure physiological signals. Among them, the channel number C = 3 is constant. In all, the model's input X can be described as X = {I1, Ir, Im, If, Is}.\nDue to the different shapes of the heterogeneous facial features (i.e., STMap Is, facial landmarks matrix Is, and the regions of the left eye I\u2081, right eye Ir, and mouth Im), we designed feature extraction modules tailored to ensure that facial features from different types of inputs are encoded to the same size of high dimension space. The specific architecture and parameters of the network are listed in Table III. Specifically, a series of convolution and pooling operations for each frame of the facial subre-gion videos were built separately, simultaneously processing feature representations from different temporal frames and maintaining the temporal information. First, we performed a dimension-up operation and then a convolution to keep the temporal order of the matrix. Then, for the feature embedding of STMaps, due to the characteristics of STMaps that condense both temporal and spatial information, we designed a series of convolutions, which retained the dimensions of STMaps in the spatial dimension, and kept the correlation between the\ntemporal features of STMaps and other input features. At the same time, in the final feature embedding stage, we adopted adaptive pooling and reshaping steps to compress the spatial dimension information into one-dimensional vectors, aligning the output dimensions with those of other feature embeddings. The facial landmarks matrix was formed by combining the facial landmarks in chronological order per frame. In other words, we used a feature embedding similar to the facial region for each frame of facial landmarks. Finally, we compressed the features into one-dimensional vectors through a fully connected layer to obtain a fixed output shape.\nThrough feature embedding, each type of input facial feature was converted to a two-dimensional vector of shape T \u00d7 D. It is worth noting that, since all dimension transformations were conducted over the spatial dimension (i.e., dimensions exclude the T for each sample) and we fixed the temporal dimension T in the embedding phase, our feature embedding process actually distilled the spatial facial features of each frame into a vector of size D and preserved the temporal ordering. Finally, by stacking these vectors at the last dimension, we got the combined feature vector mt with the shape of T \u00d7 5D. For descriptive convenience, we generalize 5D to spatial dimension V.\nTo learn the underlying correlations between different tasks, based on the classic MoE block [36], we proposed the hierarchical VDMoE architecture to accom-modate the spatio-temporal characteristics of heterogenous facial features. There are L VDMoE blocks in total. Specif-ically, as previous feature embedding modules have already encoded heterogenous facial features to high-dimension space, to minimize the computational cost and avoid the decline in generalizability due to high complexity, we instantiated each expert in the VDMOE block with a two-layered MLP. Additionally, given our mt put the spatial information from heterogenous facial features at the last dimension, following the matrix multiplication in linear algebra, the weights of MLP experts can only be applied to the last dimension (i.\u0435., spatial information). However, the temporal information is also important [54], as both drivers' behavior and physiological reflection under specific states should not be instantaneous. Thus, we introduced the spatio-temporal expert mechanism in our VDMOE block. Specifically, given that only spatial information can be processed in mt, we first transposed the mt at the last two dimensions and obtained the mt \u2208 RV\u00d7T, where temporal information of each spatial channel can be learned. Then, in each VDMoE block, we constructed K experts, which can be further equally classified as spatial experts Es(*) and temporal experts Et(*) corresponding to m and mt respectively. Experts share no parameters and are initialized separately. The structure of each type of expert is"}, {"title": "IV. MATERIALS", "content": "To mitigate the gap in previous public datasets, we collected a new multi-modal cognitive load and drowsiness driving dataset (MCDD). Some example frames of drivers with dif-ferent states and the distribution of our extracted HR and RR are shown in Figure 2.\nPrevious study [69] showed that drivers' cognitive resources are multi-dimensional, and dif-ferent dimensions of non-driving-related tasks (NDRTs) can result in different levels of cognitive load. Given the moderat-ing effect of drivers' cognitive load on drowsiness and physio-logical responses [7], [26], this driving simulator study used a within-subject design to investigate the impact of varying types of cognitive NDRT on drivers' states. As detailed in Table V and Figure 3, three types of cognitive tasks, encompassing 6 specific tasks, were used in addition to a baseline condition without NDRTs, leading to 7 NDRT tasks. Each participant completed three trials per NDRT task, resulting in a total of 21 drives (7 NDRTs * 3 trials). A Latin square design, with 21 unique orders, was implemented to mitigate potential order effects.\nBefore conducting the experiment, a power analysis was performed using MorePower software [74] to determine the minimum sample size needed. The analysis indi-cated that a sample size of 24 participants would be sufficient to generate a statistical power of 80%, with confidence interval = 95% and effect size of (\u03c3\u00b2) of 0.06.\nThe study involved a total of 42 participants (25 male, 17 female) with a mean age of 35.28 years (SD = 9.10, range = 23-53 years). Participants were recruited across four age groups (20-60 years) to minimize the influence of age-related factors on physiological signals and enhance the model's generalizability. All participants were required to have a valid driver's license for at least one year and no prior experience with advanced driving systems (ADS). Participants received an hourly compensation of 70 RMB and were given the\nopportunity to earn a performance-based bonus of up to 30 RMB for completing cognitive tasks.\n apparatus: The study utilized a fixed-base driving sim-ulator equipped with three 42-inch screens, providing a 150\u00b0 horizontal and 47\u00b0 vertical field of view (Figure 4(a)). An external tablet with two touch buttons allowed participants to activate and deactivate the driving automation system. Driving scenarios were developed and vehicle operation data was collected at a frequency of 60 Hz using Silab 7.1 software by WIVW.\nParticipants' facial RGB videos were recorded using an Orbbec Gemini pro camera. The camera was installed in front of the driver, which was recorded at a resolution of 640x480 pixels and a frequency of 30 Hz. Physiological data, including electrocardiogram (ECG), and respiration (RESP), were collected at a frequency of 100 Hz using a 3-channel SMD electrocardiograph and respiratory belt from Ergoneers (Figure 4(b)), based on which the ground truth HR and RR were calculated. The distribution of HR and RR in the MCDD\ndataset is visualized in Figure 5.\nBesides, based on the KSS questionnaire collected after each trial, there are 75.86% samples labeled as 'awake' (KSS <5), and 24.14% labeled as 'drowsy' (KSS \u2265 5). Similarly, using the NASA-TLX questionnaire, there are 29.61% labeled as 'high cognitive load' (weighted score >10), and 60.39% samples labeled as 'normal cognitive load' (weighted score < 10).\nWe also evaluated our method in other two public datasets. However, as there is no dataset that satisfies the requirements of the multi-task remote detection (i.e., more than one driver state, and contains physiological signals), we selected one dataset for detecting drowsiness (FatigueView) [32] and one another dataset for multi-task vital signs (V4V) [75].\nSpecifically, The FatigueView dataset consists of videos captured by RGB and infrared (IR) cameras from five different locations. These videos depict real-life scenarios of drowsy driving and tagged visual cues of drowsiness, varying from subtle to obvious. As no physiological signals or cognitive load levels were collected in this dataset, we utilized the RGB-Front video only from this dataset. In addition, the V4V dataset was widely used in rPPG [60], [27]. It collected physiological signals and facial videos under ten tasks and the ground-truth HR and RR were provided in V4V; however, it was not driving-related and no driver states were assessed."}, {"title": "V. EXPERIMENT", "content": "We separated all subjects in each dataset into two parts: 80% for training and 20% for testing. Before training, to ensure a more accurate RR and HR measurement, we set the sliding\nwere initiated, which took the original embedding feature mt as input. Then, seeing Figure 3, after the average pooling and a two-layered MLP, the output of each gate Gi(*) was controlled by a Sigmoid function, which makes each element of the output vector in the range [0, 1]. We argue that, being different from Softmax-based weighted aggregation [36], [64], the element-wise Sigmoid-based gate can adaptively control the importance of low-level information relative to specific tasks, thus selecting proper subspaces {m}4 \u2208 RV from a uniform task-generic space m'. Lastly, we inputted these subspaces into four task-specific estimation heads as:\nY = P(G(m) \u00b7 Pool(m'))\n= Linear(Sigmoid(MLP(Pool(m))) \u00b7 Pool(m')).  (2)\nAs has been pointed out in previous studies [25], there was an overall negative correlation between drowsiness and\ncognitive load. In general, the prolonged low workload envi-ronments can lead to a 'dormant' state, increasing drowsiness; whereas complex tasks can be cognitively demanding and thus help keep the brain active and delay the onset of drowsiness [65]. Inspired by these, we designed a prior-driven soft regular-ization Lalign to keep a general negative relationship between the estimated distribution of drowsiness and cognitive load.\nSpecifically, given the estimated vector of drowsiness and cognitive load by corresponding heads, we applied the Soft-max function to each vector to convert them to the binary prob-ability vector pdrow and Pcog. Then, based on the Kullback-Leibler Divergence, we tried to maximize the distributional difference between Pdrow and Pcog with Eq. (3). Additionally, considering this correlation might not be robust in some extreme situations (e.g., cognitive load exceeding the cognitive capacity will also speed up drowsiness development [66]), we applied a temperature hyper-parameter T to scale the two probability distributions, which can control the sharpness of the output distribution. After trial and errors, we set as 0.5 to smooth the distributions:\n\u03a3 exp (log (Pdrow/T)) log\n(exp (log (Prow/T)))\n(3)\nTo facilitate the estimation of drowsiness, cognitive load, HR, and RR. We designed different optimization goals depend-ing on the property of each task. For the loss of drowsiness Ldrow and cognitive load Lcog, given there is strong subjective consciousness and individual differences in assessing drowsi-ness and cognitive load with the self-report questionnaires [67], participants will label their current status with varied standards. When training the model with data from different individuals, the mapping between facial features and state labels is not robust. Thus, we applied the generalizable cross-entropy loss (Truncated Loss) [68] to optimize the drowsiness and cognitive load estimation, and the smooth L1 loss for the estimation of HR Chr and RR Lrr following [60]. Besides, to suppress some meaningless effects of the regularization at the different early iterations, we applied the adaptation factors A as Eq. (5)\nItercurrent\nIfertotal\n\u03bb =1+ exp-10t  (4)\nThen, we combined the multi-task optimization goals and the regularization term into one loss formula Loverall to conduct the joint training with one trade-off parameter k1.\nLoverall = Ldrow + Lcog + Chr + Lrr + 1 * k1 * Lalign.  (5)\nwith K = 2 tend to lead to a higher association for HR, while K = 4 tends to perform better for RR.\nAdditionally, there is a general trend that increasing the number of experts and layers leads to a larger number of parameters, which is consistent with the expectation that more complex models have a higher capacity (and potentially greater computational demands). In general, there is no clear linear correlation between the number of parameters and the performance metrics (F1 scores and Pearson correlations). For instance, the highest number of parameters (15.39 M, see 6(e)) does not correspond to the highest performance across tasks, indicating that simply increasing model complexity does not guarantee improved performance."}, {"title": "VI. DISCUSSION", "content": "Firstly, the VDMoE model's outstanding performance, par-ticularly in comparison with single-task models and multi-task baselines, underscores the effectiveness of its architec-ture. Notably, the model's superior accuracy in drowsiness estimation, even when compared to specialized single-task models, suggests that a well-designed multi-task model does not necessarily trade off performance in individual tasks for versatility. Besides, the better performance of VDMOE compared to multi-task baselines without task-specific feature subspace construction (e.g., ResNet3D and ViViT) illustrated that shared low-level information extraction and separate task-related feature space can enhance performance across related tasks by leveraging common features. Moreover, the com-parative underperformance of models relying solely on facial video inputs versus those utilizing preprocessed facial features highlights the potential limitations of raw video data in cap-turing subtle physiological or behavioral cues. This finding suggests that preprocessing steps or feature engineering might still play a crucial role in optimizing model performance, even in an era where end-to-end deep learning models are prevalent. Particularly, considering both facial features at the\naction level (e.g. blinks and expressions) and changes in color (STMap) during pre-processing may help extract changes in physiological indicators associated with the driver states.\nIn addition, the inclusion of a prior-driven loss, denoted as Lalign plays a pivotal role in aligning the model's outputs with prior knowledge. This loss function helps guide the learning process, which ensures that the model not only optimizes estimation accuracy but also considers the underlying structure or relationships within the data that are known as priori. By incorporating Lalign, the model is encouraged to learn representations that are consistent with the findings in previous research (i.e., the correlation between drowsiness and cognitive load), potentially leading to more robust and generalizable performance. In the future, exploring more sophisticated forms of prior-driven losses that can be explicitly guided by subtle relationships among tasks could further improve the model's robustness and generalizability.\nLastly, the choice to structure the VDMoE as a multi-task MoE model using MLPs, avoiding more complex networks, is to ensure both the model's performance and its compu-tational efficiency. While 3DCNNs and Transformers have demonstrated remarkable capabilities in capturing spatial and temporal dependencies in data, they come with significantly higher computational costs and parameter counts. For real-time applications, such as driver state monitoring systems where low latency is crucial, the efficiency of the model is as important as its accuracy. MLPs, by contrast, offer a simpler and more computationally efficient alternative. When structured as part of an MoE framework, MLPs can be highly effective in capturing complex relationships within the data. Each \"expert\" in the MoE model can specialize in different aspects or features of the data, and the \"gating\" mechanism can learn to dynamically allocate computational resources by acti-vating relevant experts for a given task or input. This allows the VDMOE to maintain a balance between model complexity and computational efficiency, making it suitable for deployment in real-world settings where resources may be constrained. Future studies should investigate the extension of the MoE framework to incorporate other types of network architectures, such as lightweight CNNs or compact Transformer variants, which could offer a pathway to further decrease computational demands in the deployment."}, {"title": "VII. LIMITATIONS", "content": "While the VDMOE model demonstrates impressive per-formance in multi-task driver state and physiological mea-surement estimation in conditional autonomous driving, some limitations still exist in the current study. Firstly, although the VDMOE achieved a balance between performance and compu-tational efficiency through its architecture, the challenges for model interpretability are still unsolved. As a neural network with complex high-dimension mapping and nonlinear trans-form, understanding why the model makes specific decisions or how it differentiates between tasks at a granular level is important in building trustworthy AI systems [81].\nBesides, while this study proposed new multi-task datasets\nin the context of driving, the generalizability of the model"}, {"title": "VIII. CONCLUSION", "content": "In this study, we introduced the innovative VDMoE model, tailored for comprehensive driver state and physiological mea-surement estimation, leveraging the rich cues available in fa-cial videos. Our approach uniquely combines the robustness of a MoE framework with the simplicity and efficiency of MLP, and carefully avoids the computational burdens that are often associated with more complex networks like 3DCNNs and Transformers. The VDMoE model is meticulously designed to distill crucial information from facial features. It employs a prior-driven loss function, Lalign, to align model predic-tions with known physiological and behavioral patterns, and enhances both accuracy and generalizability. The incorporation of this loss function, alongside the model's architecture, facil-itates a nuanced understanding of both temporal and spatial dynamics in the data, enabling superior performance across tasks of drowsiness, cognitive load, HR, and RR estimation.\nMoreover, we presented the MCDD, a comprehensive, large-scale dataset for the monitoring of driver states, designed to explore and estimate the multiple states and the phys-iological indicators within conditional autonomous driving scenarios, utilizing RGB video data. This dataset was collected based on a driving simulator platform, with strict experi-ment methodologies employed to accurately reflect the co-occurrence of multi-dimensional cognitive load and drowsiness in real-world conditional autonomous driving environments. We hope that the release of MCDD will benefit future research in this area, particularly in addressing practical real-world con-cerns. In future work, we will continue to design and collect new multi-task datasets and validate our proposed VDMoE in a real-world conditioned autonomous driving environment."}]}