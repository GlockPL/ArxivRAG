{"title": "REPRESENTATIVE ARM IDENTIFICATION: A FIXED CONFIDENCE\nAPPROACH TO IDENTIFY CLUSTER REPRESENTATIVES", "authors": ["Sarvesh Gharat", "Aniket Yadav", "Nikhil Karamchandani", "Jayakrishnan Nair"], "abstract": "We study the representative arm identification (RAI) problem in the multi-armed bandits (MAB)\nframework, wherein we have a collection of arms, each associated with an unknown reward distribu-\ntion. An underlying instance is defined by a partitioning of the arms into clusters of predefined sizes,\nsuch that for any j > i, all arms in cluster i have a larger mean reward than those in cluster j. The\ngoal in RAI is to reliably identify a certain prespecified number of arms from each cluster, while using\nas few arm pulls as possible. The RAI problem covers as special cases several well-studied MAB\nproblems such as identifying the best arm or any M out of the top K, as well as both full and coarse\nranking. We start by providing an instance-dependent lower bound on the sample complexity of any\nfeasible algorithm for this setting. We then propose two algorithms, based on the idea of confidence\nintervals, and provide high probability upper bounds on their sample complexity, which orderwise\nmatch the lower bound. Finally, we do an empirical comparison of both algorithms along with\nan LUCB-type alternative on both synthetic and real-world datasets, and demonstrate the superior\nperformance of our proposed schemes in most cases.", "sections": [{"title": "1 Introduction", "content": "The stochastic multi-armed bandit (MAB) problem [1] is a widely studied online decision-making framework, which\nconsists of K arms each associated with an a priori unknown reward distribution. Each pull of an arm results in a\nrandom reward, generated i.i.d. from the associated distribution. At each round, the learner can decide which arm to\npull based on the entire history of pulls and rewards. The MAB framework has found application in a wide variety\nof domains ranging from A/B testing [2] and online advertising [3] to network routing [4], clinical testing [5], and\nhyperparameter optimization [6] in machine learning.\nThere are several objectives that a learner might be interested in while interacting with the MAB. For example, one\nwidely studied goal is to maximize the expected cumulative reward accrued by the learner over a certain time horizon,\nor equivalently to minimize the regret with respect to an oracle which knows the arm reward distributions beforehand.\nSeveral regret minimization algorithms have been proposed in the literature [7, 8], and they are typically based on the\nidea of balancing exploration (trying different arms to reduce uncertainty about their mean rewards) and exploitation\n(pulling the arms known to have high rewards).\nAnother popular learning objective is identifying the best arm in terms of the mean reward [9, 10]. The problem has\nbeen studied in both the fixed confidence setting [10], where the goal is to find the best arm using the minimum number\nof pulls while guaranteeing a certain pre-specified error probability d; and the fixed budget setting [11] where the total"}, {"title": "2 Problem Formulation", "content": "We consider a stochastic multi-armed bandit with a collection N of N = |N| arms, each associated with a \u03c3-\nsubGaussian reward distribution\u00b2, which is a priori unknown to the learner.\nTo define the representative arm identification (RAI) problem, we sort the arms in decreasing order of their mean\nrewards and then partition them into m clusters of predefined sizes given by c = (C1, C2,..., cm), such that for any\nj > i, all arms in cluster i have a larger mean reward than those in cluster j. Clearly, $\\sum_{i=1} C_i = N$.\nWe label the arms as follows: for each i \u2208 [m] := {1,2,...,m} and j \u2208 [ci] := {1,2,...,ci}, we have an\nassociated reward distribution $II_j^i$ with the j-th arm in cluster i, so that each pull of the arm results in an i.i.d.\nreward sample from $II_j^i$. The corresponding mean reward is denoted by $\u00b5_j^i$, and we have that $\u03bc_{i_1}^{j_1} \\geq \u03bc_{i_2}^{j_2}$ for any\n$i_1 \\neq i_2, j_1 \\neq j_2$ such that $\u03bc_{i_1}^{j_1} = \u03bc_{i_2}^{j_2}$."}, {"title": "3 Lower Bound", "content": "In this section, we derive a lower bound on the sample complexity of any 8-PC algorithm for the RAI problem. To state\nthe result, we first need a few definitions.\nDefinition 1. (Arm Gap): Consider an instance I = (c, r, II); recall that $\u00b5_j^i$ denotes the mean reward of the jth arm in\ncluster i. The arm gap $\u2206_j^i$ for that arm is defined as\n$\\Delta_j^i := \\min\\{\u00b5_j^i - \u00b5_{r_i}^{i-1}, \u00b5_{r_{i+1}}^{i+1} - \u00b5_j^i\\}, \\forall j \\in c_i, i \\in [m].$\nHere, for notational simplicity, we have assumed two dummy clusters, 0 and m + 1, having one arm each, such that\n$\u03bc_1^0 = \\infty, \u03bc_{r_{m+1}}^{m+1} = \u22128$.\nNext, we define the bottleneck gap associated with an instance.\nDefinition 2. (Bottleneck Gap): Consider an instance I = (c, r, I). For each cluster i \u2208 [m], let $\u2206_i$ denote the $r_i$-th\nlargest arm gap amongst its arms. By convention, $\u2206_i = \\infty$ if $r_i = 0$. The bottleneck gap $\u25b3_I$ associated with the\ninstance I is defined as\n$\u25b3_I := \\min\\{\u0394_1, \u0394_2,\u2026\u2026, \u0394_m\\}.$\nIntuitively, $\u2206_i$ captures the complexity associated with the sub-task of identifying $r_i$ arms from cluster i, while A1\ncaptures the complexity associated with the complete RAI task; the smaller these gaps, the harder the corresponding\ntask. This is formalized in the following theorem.\nTheorem 1. For a given error threshold d \u2208 (0, 1), in the space of \u03c3-Gaussian instances (i.e., each arm has a Gaussian\nreward distribution with standard deviation \u03c3), any 8-PC algorithm A for the RAI problem satisfies\n$\\displaystyle\\lim \\inf_{\\delta \\to 0} \\frac{E[T_I(A)]}{\\log(1/\\delta)} \\geq \\frac{1}{D(I)} = \\frac{1}{2(\u0394_I)^2}$\nWe conclude this section by specializing the lower bound in Theorem 1 to the task of identifying M out of the\ntop K arms, where 1 < M < K < N (this special case of the RAI problem was analysed in [16]). Without\nloss of generality, suppose that the arms are also labelled 1,2,..., N such that the corresponding mean rewards\nsatisfy \u03bc1 \u2265 \u03bc2 \u2265\u2026\u2026\u2265 \u03bc\u03bd. Then it is easy to show that the bottleneck gap for this task is given by $\u2206_I = \u03bc_M-\u03bc_{K+1}$.\nTo the best of our knowledge, such an explicit, interpretable, instance-dependent complexity characterization is not\navailable in the literature for this task."}, {"title": "4 Algorithms", "content": "This section describes the two algorithms we propose to solve the RAI problem and presents upper bounds on their\nsample complexity. In spirit, these algorithms are similar to the successive elimination style schemes which are\nwidely used for MAB problems [10]. Under both algorithms, active arms are pulled in a round robin fashion, and\nsuitable confidence intervals are maintained for the mean reward of each active arm. From time to time, arms whose\nmembership in a certain cluster can be inferred based on the computed confidence intervals are 'selected' to be part of\nthe algorithm output; these selected arms are then removed from the active set. The two algorithms differ with respect\nto the scheduling of the membership check-the Vanilla Round Robin Algorithm performs this check after each round\nrobin cycle, whereas the Butterscotch Round Robin Algorithm performs the membership check only on each halving of\nthe confidence interval widths.\nVanilla Round Robin Algorithm for RAI\nThe Vanilla Round Robin Algorithm is stated formally as Algorithm 1. This algorithm proceeds in rounds; each round\ninvolves the following steps:"}, {"title": "5 Numerical Case Studies", "content": "In this section, we conduct an empirical evaluation of Algorithms 1 and 2 using both synthetic and real-world datasets.\nIn addition, we also consider a suitably tailored version of LUCB-style sampling [12], which has been widely used\nin the multi-armed bandit literature and offers a sequential sampling strategy as opposed to the parallel nature of the\nsuccessive elimination style strategies. In each round, the LUCB algorithm considers every empirical cluster for which\nthe arms requirement hasn't yet been fulfilled and then based on the current confidence intervals of the mean estimates,\nselects from each cluster an arm (together with the 'boundary' arms of the neighboring clusters) whose membership is\nmost likely to be confirmed with the additional pull; further details along with a proof that the proposed variant is 8-PC\nare provided in Appendix B. For our simulations, we assume the reward distribution to be Bernoulli[0, 1], a member of\nthe 1/2 SubGaussian distribution family. Finally, we set the error probability 8 = .01 and present sample complexity\nresults which are averaged over 100 independent runs of the corresponding algorithms.\nWe first examine an instance with 10 arms divided into clusters of sizes 3,5, and 2, respectively. The true means\nfor this instance are given by: [0.9, 0.85, 0.7, 0.66, 0.65, 0.6, 0.4, 0.35, 0.2, 0.15]."}, {"title": "B LUCB", "content": "The LUCB version for the representative identification problem is inspired from [12] and is stated in Algorithm 3. In\nthis algorithm, we have an intelligent sampling rule following a pull strategy based on the LCB and UCB of the arms.\nWe start this section, by defining the upper and lower confidence bounds."}, {"title": "A Proof of Theorems", "content": "Before proceeding with the proofs, let us state the necessary concentration inequality used in proving Theorem 2 and\nTheorem 3\nLemma 1. Let X1, X2, \u2026\u2026\u2026, XR \u2208 \u03c3-SubGaussian be independent and identical random variables. Than for the\nempirical mean $\u03bc(R) = \\sum X_i /R$ we have\n$P(|\u03bc(R) \u2013 \u03bc| \u2265 \\epsilon_R) \u2264 2 exp(-2R\u03b5_R^2)$\nProof of Lemma 1. From Hoeffdings inequality for any \u03c32-SubGaussian random variable, we have\n$P(|\u03bc(R) \u2013 \u03bc| > \u03b5R) \u2264 2 exp\\left(-\\frac{R\u03b5_R^2}{2\u03c3^2}\\right)$\nOn substituting \u03c3 = \u00bd in Equation 1, for any \u00bd-SubGaussian random variable, we get\n$P(|\u03bc(R) \u2013 \u03bc| \u2265 \u03b5R) \u2264 2 exp(-2R\u03b5_R^2)$\n(1)\n(2)"}, {"title": "A.1 Proof of Theorem 1", "content": "The proof of the lower bound on expected sample complexity starts from recognizing that the RAI problem fits in the\n'multiple correct answers' framework of [17] and thus the general lower bound derived there applies to the RAI problem\nas well. However, that lower bound is in the form of a min min max optimization problem and the rest of the proof\ninvolves simplifying it to obtain an interpretable form.\nTo state the lower bound in [17], we have to introduce some additional notation. Consider an RAI problem instance\nI = (c, r, II), where the arm reward distributions are Gaussian with standard deviation 1/2 and the mean reward vector\nis given by \u03bc := {\u03bc}, where \u00b5 is the mean reward for arm j from cluster i under the current instance. Let i* [\u03bc]\ndenote the set of all correct answers when the reward distributions are specified by \u03bc. Note that each such correct\nanswer corresponds to a set of arms such that r\u2081 of them belong to cluster 1, r2 of them belong to cluster 2 and so on.\nNext, for any correct answer a \u2208 i* [\u03bc], we need the notion of an alternate mean reward vector \u03bb such that a is not a\ncorrect answer when the underlying arm reward distributions are Gaussian with standard deviation 1/2 and the mean\nreward vector is X = {x}. We will denote the collection of all such alternate mean reward vectors by \u00aca. Finally, let\nAK denote the K-dimensional simplex and d(a, b) denote the Kullback-Leibler (KL) divergence between two Gaussian\ndistributions with means a and b, and variance 1/2 each. Note that d(a, b) = 2(a \u2013 b)\u00b2.\nNext, from [17, Theorem 1], we have the following lower bound on the expected sample complexity of any 8-\u0420\u0421\nalgorithm for an RAI problem I = (c, r, II), where the arm reward distributions are Gaussian with variance 1/2 and\nthe mean reward vector is given by \u03bc := {\u03bc}:\n$\\displaystyle \\lim \\inf_{\\delta \\to 0} \\frac{E[T(A)]}{\\log(1/\\delta)} \\geq D(I)^{-1}$\n$\\text{where } D(I) =  \\min_{\\alpha \\in i^*[\\mu]}  \\max_{\\lambda \\in \\neg \\alpha}   \\frac{\\sum_{i=1}^m \\sum_{j=1}^{c_i} w_i d(\\mu_{ij}, \\lambda_{ij})\\}}{|| W_A ||\\} $\n(3)"}, {"title": "A.2 Proof of Theorem 2", "content": "The proof of this theorem is organized as follows. First, we define a good event 4. Based on this condition, we then\ndemonstrate the correctness of Claims 1 and 2.\n$\\displaystyle \\psi \\leqq  {\\forall R \\in Z, \\forall i \\in [m], \\forall j \\in  [c_i], |{\\hat\\mu} \\cdot \\frac{\\sqrt{\\ln (\\frac{\\pi^2 R^2 N}{3\\delta})}}{2R}} \\}$\nClaim 1. Under A = Alg1, let \u017f\u00ee\u00fb\u00ba(R) be the empirical mean of arm j from cluster i at round R. Let & be the event\ndefined in 4. Than the $P_*(\u03c8) \u2265 1 \u2212 \u03b4$\nProof of Claim 1. By the union bound, we have\n$\\begin{aligned} P(\\psi^c) &\\leq \\sum_{R=1}^{\\infty}  \\sum_{i=1}^{m} \\sum_{j=1}^{c_i} P\\left[|\\mu_{ij}(R) - \\mu_{ij}| > \\sqrt{\\frac{\\ln (\\frac{\\pi^2 R^2 N}{3\\delta})}{2R}}\\right] \\\\ & < \\sum_{R=1}^{\\infty}  \\sum_{i=1}^{m} \\sum_{j=1}^{c_i} \\frac{6 \\delta}{\\pi^2 R^2 N} \\\\ & = \\sum_{R=1}^{\\infty} \\frac{6 \\delta}{\\pi^2 R^2} \\\\ & < \u03b4\\end{aligned}$\n(5)\n(6)\n(7)\n(8)"}, {"title": "A.3 Proof of Theorem 3:", "content": "The proof of Theorem 3, follows a similar approach to that of Appendix A.2. We start with defining the good event,\nconditioned on which the rest of the proof follows.\n$\\displaystyle \\psi \\leqq  {\\forall R \\in Z, \\forall i \\in [m], \\forall j \\in [c_i], |{\\hat\\mu} \\cdot \\frac{15\\}$$2\\$$1"}]}