{"title": "BoundingDocs: a Unified Dataset for Document Question Answering with Spatial Annotations", "authors": ["Simone Giovannini", "Fabio Coppini", "Andrea Gemelli", "Simone Marinai"], "abstract": "We present a unified dataset for document Question-Answering (QA), which is obtained combining several public datasets related to Document AI and visually rich document understanding (VRDU). Our main contribution is twofold: on the one hand we reformulate existing Document AI tasks, such as Information Extraction (IE), into a Question-Answering task, making it a suitable resource for training and evaluating Large Language Models; on the other hand, we release the OCR of all the documents and include the exact position of the answer to be found in the document image as a bounding box. Using this dataset, we explore the impact of different prompting techniques (that might include bounding box information) on the performance of open-weight models, identifying the most effective approaches for document comprehension.", "sections": [{"title": "Introduction", "content": "The increasing number of documents produced in various fields, including scientific research, legal proceedings, healthcare, and business, has created an enormous demand for efficient information extraction (IE) methods.\nIn document processing research, Optical Character Recognition (OCR) has proven essential for transforming scanned documents and images into machine-readable text, facilitating further analysis. Initially, statistical methods [1] were used alongside OCR to extract information, followed by machine learning approaches.\nSubsequently, deep learning techniques [2], especially methods related to natural language processing (NLP), became crucial in advancing document understanding. Today, the focus has shifted towards Large Language Models (LLMs) [3], which, with their exceptional ability to model natural language in complex contexts, have further enhanced document comprehension and the automation of information extraction from extensive volumes of text.\nOCR tools and LLMs are now extensively used to perform several tasks in Document AI, including:\n\u2022 Document Image Classification: classifies document images into types such as invoices, scientific papers, and receipts [4].\n\u2022 Layout Analysis: examines a document's structure, identifying elements like text, images, and tables [5];\n\u2022 Visual Information Extraction: extracts entities and relationships from unstructured content, considering text, visual elements, and layout [6];\n\u2022 Visual Question Answering: answers natural language questions based on a document's content [7];\nThe two main motivations for building the BoundingDocs dataset\u00b9, that is focused on Information Extraction and Question Ansering, are:\n1. the lack of extensive and diverse QA datasets in the field of Document AI;\n2. the lack of precise spatial coordinates in the existing datasets.\nCurrent datasets do not effectively incorporate positional data, which is essential for reducing hallucinations and improving performance by enabling LLMs to understand document layout more precisely."}, {"title": "Contribution", "content": "In this work, we propose a unified approach to build a Question-Answering dataset. Such a dataset can be used for evaluating how good Document AI models are to extract relevant information when answering to natural language questions. In doing so, we aim to address the following research questions:\n\u2022 RQ1: How can existing datasets be unified into a common Question-Answering format?\n\u2022 RQ2: Can rephrased questions generated by LLMs enhance answer accuracy for document-based questions?\n\u2022 RQ3: Does including layout information in prompts (e.g. [8, 9]) improve the model's performance on document comprehension tasks?\nTo explore these questions, our study is organized into the following sections. Section 2 reviews the existing literature and benchmarks in the field"}, {"title": "State of the art", "content": "We provide an overview of the main models and techniques proposed for QA and Visual Question-Answering (VQA) [7]. We also discuss the features of the main datasets in the Document AI that we considered in our research."}, {"title": "Related Datasets", "content": "As summarized in Table 1, we selected datasets that best match our focus on comprehensive document understanding and advanced VQA, addressing challenges across both single-page and multi-page documents. For a more detailed review of datasets specific to Document Layout Analysis, please refer to our additional survey [10], which includes more datasets focused on layout-related tasks.\nAmong the foundational datasets, DocVQA [7, 15] stands as one of the earliest benchmarks dedicated to VQA on document images, focusing on understanding both textual and layout aspects of documents. Launched in 2020, DocVQA comprises multiple tasks designed to push the boundaries of document comprehension. The primary tasks include answering questions about individual document pages and analyzing multi-page documents a crucial capability for real-world applications. The Single Page [7] subset of this dataset includes 50,000 questions over 12,767 documents, while the Multi Page [15] subset contains 46,436 questions spanning 5,929 documents (covering 47,952 pages in total). These datasets require models to interpret the visual structure of documents and to derive insights that go beyond simple text extraction.\nDUDE [13] builds on this foundational work by extending VQA to multi-domain, multi-purpose documents. The dataset provides 5,000 annotated PDF files with 18,700 question-answer pairs across"}, {"title": "Related methods", "content": "In recent years, the QA task [7, 15] has been approached in many ways, leveraging different techniques and model architectures. These methods can be broadly categorized into NLP-based, LLMs, and multimodal architectures, each addressing different aspects of document understanding and question answering.\nNLP-based approaches build on general Question-Answering models, primarily focusing on text semantics without explicitly incorporating document layout or visual features. A prime example is BertQA [7], which utilizes a BERT architecture followed by a classification head to predict the start and end indices of an answer span. Modifications such as changes in hyperparameters and the introduction of new pre-training tasks have been explored in multiple works [24, 25], resulting in improved outcomes.\nLLM-based methods leverage large language models to perform document understanding tasks by encoding structural and layout information directly into the input. For instance, LMDX [26] incorporates layout information via bounding box coordinates in the prompt, enhancing retrieval precision and reducing hallucination. DocLLM [27], which builds on the LayoutLM family, includes a specialized pretraining phase focused on structured layout data to improve document layout understanding. In contrast, NuExtract [28] is designed for extracting structured JSON data from documents, using training data derived from the Colossal Clean Crawled Corpus [29].\nMultimodal architectures combine visual and textual features to enhance document comprehension across layout, content, and structure. Among OCR-free methods, mPLUG-DocOWL 1.5 [30] integrates a Vision Transformer (ViT) [31] with an LLM for comprehensive Document AI analysis, aligning layout and textual cues effectively without requiring separate OCR stages. Similarly, Donut [32] and Dessurt [33] operate without OCR preprocessing, directly integrating image and text data for robust document understanding.\nIn contrast, OCR-dependent models further refine document comprehension by incorporating OCR-based tokens. Hi-VT5 [15], for example, combines OCR tokens with visual features, optimizing its effectiveness for Question-Answering tasks that rely on precise textual information. Additionally, LayoutLMv3 [34] introduces visual patch embeddings in place of traditional CNNs to better align text, layout, and visual cues, resulting in improved performance on tasks requiring fine-grained structural interpretation."}, {"title": "Dataset construction", "content": "We base our new dataset, BoundingDocs, on the following datasets selected from Table 1: SP-DocVQA, MP-DocVQA, DUDE, Deepform, VRDU, FATURA, Kleister Charity, Kleister NDA, FUNSD, and XFUND. This collection encompasses a diverse range of document types, linguistic features, and question-answer formats, providing essential resources for training and evaluating advanced Document AI models.\nIn Figure 1 we show the implemented pipeline for dataset construction."}, {"title": "Dataset format definition", "content": "For each document, a JSON file contains the annotation (examples in Figure 2). Each word in the answer is linked to its corresponding bounding box. Following established practices in the literature (e.g., LayoutLM [34], BERT [2]), the bounding boxes are normalized integers ranging from 0 to 1000 relative to the actual page size. Each bounding box is defined by a list of four values: the width, the height, the X and Y coordinates of the top-left vertex of the rectangle."}, {"title": "Producing annotations", "content": "A significant challenge comes from integrating various types of annotations into a unified structure. Datasets like Deepform, Kleister, and FATURA provide annotations that only establish a relationship between a key and its corresponding value in the text, such as annotating Address = 48 Woodford, SandyFord. However, these datasets lack essential positional information, such as the text's location, frequency of occurrence, and page number. In contrast, datasets like VRDU and DocVQA"}, {"title": "Dataset preparation", "content": "Upon collecting and downloading the datasets, the following preliminary operations have been considered case by case. These additional steps are critical to standardize and prepare the datasets for the generation of annotations.\nAnnotation Conversion: When the annotations in a dataset have an undocumented or complex format, they are converted into a standardized, more straightforward format. This is particularly necessary for the VRDU dataset, where the original annotations require interpretation and conversion.\nFiltering Pages/Questions: Some datasets contain redundant or irrelevant content, as unnecessary pages or questions, that have been removed. For instance, in the DocVQA dataset, pages from the Multi Page set were excluded from the Single Page set to prevent duplication. Additionally, for both DUDE and DocVQA datasets, pure visual questions, i.e., lacking the answer as recognized by the OCR in the image, are filtered out.\nDownloading Original Documents: In datasets where only annotations are provided without the corresponding documents, the original documents are downloaded from external sources. This step was necessary for the Deepform dataset, where the PDFs were not included alongside the annotations.\nOCR Processing with Textract: To ensure consistency across all datasets, Amazon Textract has been applied to all documents, regardless of whether they already contained OCR data. Datasets were processed through Textract not only when OCR data was completely absent, but also when OCR was only provided for the annotated fields. This process has been applied to datasets such as VRDU, FATURA, Kleister, SP-DocVQA, Deepform, FUNSD, and XFUND, where OCR data is either insufficient or not provided.\nKey-Value Association Creation: For certain datasets, key-value pairs for information extraction were manually generated from the annotations. For instance, in FUNSD and XFUND datasets the key-value associations are automatically created from existing document annotations. This step involves linking elements labeled as questions to their corresponding answers to facilitate coherent information extraction."}, {"title": "Matching annotations and OCR", "content": "To match the answer to each question with the data extracted by Textract [35], a script has"}, {"title": "Rephrasing questions", "content": "After completing the matching between annotations and OCR, the questions for the new dataset are generated. Inspection of these questions, which followed a simple template-based structure, revealed that they are often grammatically incorrect, overly simplistic, and consistently adhered to the same pattern. This raised concerns that training an LLM on these questions could introduce bias, potentially leading to poor performance on questions written by humans, which may not follow the template.\nTo mitigate this issue, we employed the Mistral 7B model [36] to correct and rewrite the questions, aiming to fix errors and introduce linguistic diversity. Other Mistral models, such as Mistral Large [37] and Mixtral 8x7B [38], were also tested, but they produced overly complex, verbose, and unnatural questions.\nThe prompt for question rewriting included manually written examples to guide the model, with no information about the correct answer to avoid biasing the generation. For example, the question What is the Gross Amount? was rewritten by the LLM as What is the value of the Gross Amount?.\nThis procedure was applied to most questions in the dataset, adding a new attribute, rephrased_question. Questions from DUDE, MP-DocVQA, and SP-DocVQA were excluded as they were already human-written. Additionally, questions from XFUND were excluded due to concerns over the model's ability to generate questions in languages other than English."}, {"title": "Statistics & splits", "content": "The dataset is split into training, validation, and test sets, using an 80-10-10 split based on document count, where all questions related to a single document are contained within the same set. Table 2 gives an overview of the dataset's size and sources distribution. Detailed statistics can be found in the Supplementary Material.\nTo ensure that question types and document layouts are uniformly distributed across the three sets, documents from each source dataset are sampled separately. Specifically, documents from Deepform are split in an 80-10-10 ratio, followed by documents from FATURA, DUDE, and all the others. The union of these individual splits yields"}, {"title": "Dataset examples", "content": "In Fig. 3 (Deepform) and Fig. 4 (VRDU Registration Form) it is possible to observe two pages while Table 3 contains their QA pairs. In Fig. 3 the extracted fields are the advertiser's name and the gross amount for the various transmissions. In Fig. 4 the fields to be extracted are only two: the registrant name and the registration number.\nThese two examples illustrate how, despite the high number of documents in the collection, the potential amount of information present in the documents is underutilized, as the annotated fields are few compared to the entire body of the documents, indicating that the potential of this large document collection is not being properly exploited.\nAdditional examples that provide a full overview of the entire variety of the dataset can be found in the Supplementary Material."}, {"title": "Experimental Results", "content": "Table 4 presents our experimental results across the different datasets and model configurations. The finetuning and testing pipeline implemented is summarized and plotted in Figure 5."}, {"title": "Evaluation Metrics", "content": "We use the standard metric ANLS* [39], which supports a wider range of tasks including line-item extraction and document-processing tasks.\nFor each model-dataset pair in our results, we report two key measurements: the ANLS* value (rescaled between 0 and 100 for easier reading) and the percentage of non-JSON parsable responses relative to the total number of queries."}, {"title": "Prompt Construction", "content": "In this study, each question in the dataset may have answers located on multiple pages. The significant computational costs associated with multi-page document processing, as reported e.g. by Multi PageDocVQA [15], together with the context size limitation of smaller LLMs, make us opt for an atomic approach instead of encoding everything in a single prompt.\nFor questions requiring information from multiple pages, we generate independent prompts for each relevant page, appending the same question to each prompt. For instance, if a five-page document contains relevant information on pages 2 and 4, we generate two prompts\u2014one containing page 2's content and the other page 4's\u2014each coupled with the question.\nEach prompt includes the document text, the question, and a specification for the answer format (JSON), facilitating structured data extraction."}, {"title": "Baseline Models", "content": "We evaluated three popular open-weight models as baselines: Mistral 7B Instruct v0.3 [36], Llama 3 8B Instruct [40], and Phi 3.5 3.8B Instruct [41]. These models were chosen for their established performance and recognition in the"}, {"title": "Ablation Study: Question Formulation", "content": "For investigating the impact of question formulation, we selected the Mistral 7B v0.3 (base version) for fine-tuning. We evaluated two types of questions\u2014template-based (simple, consistent format) and rephrased (more varied, user-friendly language). Each model was tested with both question types, resulting in four experimental conditions:\n\u2022 Template-Template: Model trained and tested with template-based questions.\n\u2022 Template-Rephrased: Model trained with template-based questions, tested with rephrased questions.\n\u2022 Rephrased-Template: Model trained with rephrased questions, tested with template-based questions.\n\u2022 Rephrased-Rephrased: Model trained and tested with rephrased questions."}, {"title": "Incorporating Bounding Box Information", "content": "To assess the impact of spatial information, we incorporated bounding box coordinates into the prompts, denoted as Reph-Reph-bbox in Table 4. Each Textract-extracted text element in the prompt was annotated with bounding box coordinates, enabling the model to reference spatial context. In this configuration, the model was specifically fine-tuned to produce more complex JSON outputs that include not only the answer but also a comprehensive list of all locations where the extracted value appears in the document. While this approach provided richer spatial awareness, the requirement to generate more structured outputs introduced additional complexity that led to increased parsing errors.\nTo address these parsing challenges, we implemented the Reph-Reph-bbox w/regex configuration, which introduced a regex-based post-processing step. When the model's structured JSON output was not parsable due to format inconsistencies or generation errors, the regex extraction mechanism served as a fallback solution to retrieve the target value, effectively maintaining the benefits of spatial information while mitigating the impact of parsing failures."}, {"title": "Research Question answers", "content": "Our experimental findings provide clear answers to our research questions, defined in Section 1:\n\u2022 RQ1 Dataset Unification: By standardizing data from various sources (e.g., receipts, invoices, forms) into a consistent Question-Answering format, models are exposed to a wide range of document layouts and content types,"}, {"title": "Conclusions", "content": "The paper addresses the growing need for evaluating LLMs in Document AI tasks by proposing a unified dataset designed for document Question Answering taking into account the position of answers' text in the document. Baseline experiments using open-weight LLMs demonstrate the challenges of applying generic models to specialized Document AI tasks; the performance of instruct models reveal clear limitations in generating correct and well-structured answers.\nThe paper reveals that while off-the-shelf LLMs struggle with document-specific tasks, targeted fine-tuning can significantly improve their capabilities. Rephrasing questions using LLMs improves the models' understanding and response accuracy across different question formulations, suggesting that LLMs benefit from exposure to diverse linguistic variations during training. Incorporating layout and positional information into the prompt led to improved accuracy across most datasets, but at the cost of a higher percentage of non-parsable responses, reflecting the increased complexity of generating JSON outputs that include bounding box information.\nIn future work we aim to explore various methods for incorporating bounding boxes into prompts to better capture the spatial structure of documents and evaluate open-weight multimodal LLMs specifically designed to handle both textual and visual information. The constructed dataset and the experimental results provide a solid foundation for future research in Document QA. Fine-tuning models with enriched prompts has shown promising improvements."}, {"title": "Declarations", "content": "Competing interests The authors declare no competing interests."}]}