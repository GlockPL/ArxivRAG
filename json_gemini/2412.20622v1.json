{"title": "HALLUCINOGEN: A Benchmark for Evaluating Object Hallucination in Large Visual-Language Models", "authors": ["Ashish Seth", "Dinesh Manocha", "Chirag Agarwal"], "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance in performing complex multimodal tasks. However, they are still plagued by object hallucination-the misidentification or mis-classification of objects present in images. To this end, we propose HALLUCINOGEN, a novel visual question answering (VQA) object hallucination attack benchmark that utilizes diverse contextual reasoning prompts to evaluate object hallucination in state-of-the-art LVLMs. We design a series of contextual reasoning hallucination prompts to evaluate LVLMs' ability to accurately identify objects in a target image while asking them to perform diverse visual-language tasks such as identifying, locating or performing visual reasoning around specific objects. Further, we extend our benchmark to high-stakes medical applications and introduce MED-HALLUCINOGEN, hallucination attacks tailored to the biomedical domain, and evaluate the hallucination performance of LVLMs on medical images, a critical area where precision is crucial. Finally, we conduct extensive evaluations of eight LVLMs and two hallucination mitigation strategies across multiple datasets to show that current generic and medical LVLMS remain susceptible to hallucination attacks 1.", "sections": [{"title": "1 Introduction", "content": "In recent years, Large Language Models (LLMs) have made significant advancements in natural language understanding (NLU) and natural language generation (NLG), significantly advancing the field of artificial intelligence (Achiam et al., 2023; Dubey et al., 2024; Zhao et al., 2023). Building on the exceptional capabilities of LLMs, researchers have developed Large Vision-Language Models (LVLMs), which have demonstrated outstanding performance on multimodal tasks such as image captioning (IC) and visual question answering (VQA) (Zhu et al., 2023; Ye et al., 2023; Wang et al., 2024; Dubey et al., 2024; Liu et al., 2024b). These models use LLMs as their foundational architecture, integrating visual features as supplementary inputs and aligning them with textual features through visual instruction tuning (Liu et al., 2023, 2024b). Despite these advancements, LVLMs continue to struggle with the issue of object hallucination - a phenomenon characterized by the misidentification or misclassification of visual objects in an image (Li et al., 2023; Lovenia et al., 2023). This potentially leads to harmful consequences, especially when users lacking sufficient domain knowledge place undue reliance on these models.\nTo this end, prior works have introduced a series of benchmarks (Lovenia et al., 2023; Li et al., 2023; Guan et al., 2023; Yin et al., 2024) and mitigation strategies (Leng et al., 2024; Huang et al., 2024; Zhou et al., 2023) to evaluate and improve object hallucinations in LVLMs. However, as illustrated in Fig. 1, we find that these benchmarks predominantly rely on explicit closed-form attacks, which directly ask the underlying LVLM to identify a specific visual object and is expected to respond with a simple \u201cYes\u201d or \u201cNo\u201d, e.g., visual object detection prompts like \u201cIs <object> present in the image?\u201d In contrast, we argue that implicit open-form hallucination attacks present a more significant challenge for LVLMs. For instance, in an advanced visual grounding task that requires identifying the position of an object within an image, LVLMs must first implicitly determine whether the object mentioned in the prompt is actually present in the image before generating a factually accurate response. This additional layer of reasoning increases the likelihood of LVLMs mistakenly assuming the presence of an object due to pre-existing biases from strong LLM priors, such as spurious correlations between non-existent objects and the overall visual scene (Liu et al., 2024a, 2025).\nMain Contribution. To address the aforementioned shortcomings, we propose HALLUCINOGEN, a novel benchmark designed to assess object hallucination in Large Vision-Language Models (LVLMs). Unlike prior benchmarks, which predominantly rely on simple, single-object identification prompts, HALLUCINOGEN introduces a diverse set of visual-context prompts, which we call object hallucination attacks. We broadly classify these attacks into two types: explicit and implicit object hallucination attacks. Explicit attacks involve directly asking LVLMs to identify the presence of a non-existent object in an image, thereby provoking hallucinated responses. In contrast, implicit attacks utilize more complex or indirect queries that do not explicitly inquire about a specific object. Instead, these prompts aim to elicit responses in which LVLMs may erroneously infer the existence of objects based on contextual or relational cues in the visual and textual input. Additionally, we extend our proposed benchmark to evaluate hallucination in medical applications by introducing MED-HALLUCINOGEN. Specifically, we utilize the NIH Chest X-rays dataset (Wang et al., 2017) to design disease hallucination attacks tailored to the biomedical domain. The primary motivation behind the MED-HALLUCINOGEN benchmark is to assess the extent of hallucination in LVLMs when diagnosing biomedical images such as Chest X-rays, particularly under explicit and implicit hallucination attacks. By evaluating these models in such critical scenarios, MED-HALLUCINOGEN aims to identify potential risks associated with deploying LVLMs in critical settings, where hallucinated responses could have severe consequences. We summarize our main contributions below:\n\u2022 We propose HALLUCINOGEN, a novel benchmark for evaluating object hallucination. Unlike prior benchmarks, HALLUCINOGEN introduces a diverse set of complex contextual reasoning prompts, referred to as object hallucination attacks, specifically designed to query LVLMs about visual objects that may not be present in a target image containing 60,000 image-prompt combinations across 3,000 visual-object pairs.\n\u2022 We extend our benchmark, HALLUCINOGEN to evaluate disease hallucination in biomedical applications such as correctly diagnosing Chest X-rays by introducing MED-HALLUCINOGEN.\n\u2022 We show that LVLMs are also capable of hallucinating reasoning and using Chain-of-Thought reasoning increases hallucination in LVLMs.\n\u2022 Finally, we conduct extensive qualitative and quantitative evaluations of eight prior LVLMs and two hallucination mitigation strategies on our proposed benchmarks. Our results demonstrate that, for the majority of hallucination attacks proposed in HALLUCINOGEN and MED-HALLUCINOGEN, most SOTA LVLMs show performance close to random guessing."}, {"title": "2 Related works", "content": "Our work lies at the intersection of large visual-language models, hallucination benchmarks, and mitigating techniques for hallucination.\nLarge Vision-Language Models (LVLMs). In recent years, building on the success of LLMs (Bubeck et al., 2023; Chang et al., 2024), there has been a significant surge in the development of LVLMs. To enhance the capabilities of these LVLMs, prior works have primarily focused on designing novel architectures (Ye et al., 2024), improving cross-modal alignment between visual and textual prompts (Dubey et al., 2024), and refining training methods (Liu et al., 2024b). While these LVLMs excel in complex vision-language tasks such as image captioning (Zhou et al., 2024) and visual question answering (Xu et al., 2024), they remain prone to generate hallucinated responses when faced with prompts involving nonexistent objects, incorrect attributes, or inaccurate relationships (Huang et al., 2023; Lovenia et al., 2023).\nObject Hallucination Benchmarks. In the context of LVLMs, prior research has defined \u201cobject hallucination\" as the phenomenon where a model generates responses referencing objects that are either inconsistent with or absent from the target image (Li et al., 2023; Lovenia et al., 2023). Various benchmarks have been proposed to evaluate the extent of object hallucination in such models, primarily focusing on closed-ended tasks using yes-or-no or multiple-choice questions, with accuracy as the primary evaluation metric. For example, POPE (Li et al., 2023) detects hallucinations through polling-based yes-or-no questions, while AMBER (Wang et al., 2023) and HallusionBench (Guan et al., 2024) extend and refine these methods to assess a broader range of hallucination types with greater granularity. Despite their success, we find that these benchmarks rely heavily on simple visual object identification prompts, which fail to adequately challenge current-generation LVLMs such as Qwen2VL (Yang et al., 2024) and Llama3.2 (Dubey et al., 2024)."}, {"title": "Mitigating Object Hallucination in LVLMs", "content": "Based on evaluations conducted on existing object hallucination benchmarks, there have been attempts to mitigate hallucination in LLMs and LVLMs. In LLMs, techniques like Chain-of-Thought (CoT) reasoning (Wei et al., 2022) have proven effective at reducing hallucinated or erroneous responses (Luo et al., 2023; Akbar et al., 2024). For LVLMs, methods such as VCD (Leng et al., 2024) and OPERA (Huang et al., 2024) use inference-time decoding optimizations to identify hallucinated tokens in the generated responses. Preference-aligned training techniques, like reinforcement learning with human feedback (RLHF), have also been effective in addressing object hallucination by prioritizing non-hallucinatory responses while penalizing hallucinated content (Sun et al., 2023). In this work, we extensively evaluate all of these mitigation techniques and show that these approaches fail to defend against the diverse pool of object hallucination attacks introduced by HALLUCINOGEN and MED-HALLUCINOGEN."}, {"title": "3 HALLUCINOGEN: A Benchmark for Object Hallucinations in LVLMs", "content": "In this section, we present the details of our proposed benchmark, HALLUCINOGEN, as illustrated in Fig 2. We first outline the construction of HALLUCINOGEN and MED-HALLUCINOGEN in Section 3.1 and Section 3.3. Next, we provide the details on the categorization of various object hallucination attacks employed in HALLUCINOGEN and MED-HALLUCINOGEN in Section 3.2."}, {"title": "3.1 Developing HALLUCINOGEN Benchmark", "content": "As illustrated in Figure 2, for each image \\(I_i\\) and a target object \\(o_j\\) from the associated list of objects \\(O = \\{o_1, o_2,\\cdots, o_N\\}\\), HALLUCINOGEN employs a prompt \\(p_k\\) also called as object hallucination attack from the set of hand-crafted prompts \\(P = \\{P_1,P_2,\\ldots,P_M\\}\\) to query the LVLMs.\nDataset Structure. We utilize the above prompts in HALLUCINOGEN to conduct a comprehensive evaluation of hallucination in LVLMs by verifying whether the target object \\(o_j\\) is correctly referenced in the generated response. Each hallucination prompt is categorized based on the specific vision-language task it challenges the LVLMS to perform, including identification, localization, visual context, and counterfactual reasoning (detailed descriptions of each task are provided in Sec. 3.2). These questions either explicitly prompt the model to identify a target object, whether real or nonexistent, in the image (e.g. correctly identifying the object) or implicitly require the model to infer its presence before generating a response (e.g. understanding the surrounding context). Furthermore, each sample in HALLUCINOGEN is uniquely represented by the triplet shown below:\n\n\nwhere \\(y_j\\) is \u201cYes\u201d or \u201cNo\u201d depending on whether the object \\(o_j\\) is present in the image \\(I_i\\). HALLUCINOGEN consists of 60,000 such triplets, where 3,000 visual-object pairs are taken from a popular object hallucination benchmark, POPE (Li et al., 2023), followed by 20 unique hand-crafted prompts, five for each visual-language task."}, {"title": "3.2 Categorizing Hallucination Attacks", "content": "In contrast to prior benchmarks that primarily focus on straightforward single-object identification prompts, we introduce a diverse range of contextual prompts in HALLUCINOGEN, referred to as object hallucination attacks. Instead, the prompts in HALLUCINOGEN are designed to elicit hallucinated responses by exploiting contextual or relational cues within the image. Additionally, each hallucination attack is designed to evaluate LVLMs' ability to accurately infer the presence of objects with varying levels of complexity while performing various visual-language tasks, including identification, localization, visual contextual reasoning, and counterfactual reasoning (List of prompts used for each task can be found in Appendix D)."}, {"title": "3.2.1 Identification (ID)", "content": "The task of identification involves determining whether a specific object is present in an image, where LVLMs are expected to recognize the presence/absence of an object based on a straightforward prompt (Li et al., 2023; Lovenia et al., 2023). We use explicit hallucination prompts for identification tasks, where the LVLM is directly asked to identify a non-existent object. For example, a prompt might ask, \u201cIs the person visible in the image?\u201d when no person is present in the input image. These prompts exploit the model's susceptibility to hallucinate an object, testing its ability to distinguish between real and nonexistent objects."}, {"title": "3.2.2 Localization (LOC)", "content": "Localization refers to the task of identifying the specific location of an object within an image. This task is more complex than identification, requiring both recognition and spatial awareness. We utilize implicit hallucination attacks for the localization task, where the prompt asks the LVLM to find the location of an object that is not present. For example, a prompt like \u201cWhere is the clock in the image?\u201d when there is no clock in the target image, aims to provoke hallucinated responses that inaccurately place a non-existent object in a location. These attacks test the LVLM's ability to recognize objects and spatially locate them, increasing the difficulty by adding relational context."}, {"title": "3.2.3 Visual Context (VC)", "content": "Visual contextual reasoning involves understanding and interpreting objects based on their surrounding context and relationships within the image. This task requires the model to draw inferences from the broader scene rather than just recognizing individual objects. Implicit hallucination attacks are particularly effective for this task, as they often leverage subtle contextual cues. For instance, a prompt like \u201cIdentifying surrounding objects near to the car in the image?\u201d can induce hallucination of an object car that isn't present in the target image. These attacks exploit the model's reliance on visual context and its tendency to infer objects that fit the narrative of the scene, challenging the model's ability to reason accurately based on context."}, {"title": "3.2.4 Counterfactual (CF)", "content": "Counterfactual reasoning requires the model to infer how the scene might change if a particular object were present or absent. This task involves more complex cognitive reasoning, pushing the model to consider hypothetical causal scenarios. We employ implicit hallucination attacks here, where the prompt asks the model to imagine the absence of an object shown in the input image. For example, a prompt like \u201cWhat would happen if we removed the car from the image?\u201d challenges the model to generate a response based on a non-existent object, such as the car in the image. These attacks test the model's capacity to perform counterfactual reasoning by analyzing how its understanding of the scene changes when it introduces hypothetical elements."}, {"title": "3.3 MED-HALLUCINOGEN", "content": "We extend our benchmark, HALLUCINOGEN, to assess hallucination in high-stakes medical applications by introducing MED-HALLUCINOGEN, designed to evaluate the extent to which LVLMs can accurately infer whether a person is suffering from a disease while conducting detailed diagnoses on biomedical images, such as chest X-rays.\nDataset Structure. MED-HALLUCINOGEN comprises 3,000 unique X-ray and disease pairs, accompanied by ten hallucination attack prompts \u2013 five tailored for the task of identification and five for localization. To construct MED-HALLUCINOGEN, we utilize the test set of the NIH Chest X-ray dataset (Wang et al., 2017) (additional details about the NIH Chest X-ray dataset are provided in Appendix C). We begin by filtering out unlabeled X-rays and those marked with the label \"no-responses.\u201d Next, using the ground truth labels, which provide information about the associated diseases, we create a classification task with labels \"Yes\" or \"No\u201d to indicate the presence or absence of each disease.\nAs shown in Figure 3, for MED-HALLUCINOGEN, we adapt explicit and implicit attack strategies from HALLUCINOGEN to evaluate hallucination in LVLMs across two diagnosis:\nIdentification. We employ explicit attacks, such as \u201cGiven this Chest X-ray, identify if the person has <disease>,\u201d to assess whether LVLMs can correctly infer the presence or absence of a disease.\nLocalization. We use implicit attacks, such as \u201cEvaluate the X-ray to locate the region indicative of <disease>,\u201d where the LVLMs must first infer the presence of a disease and then generate a factually accurate response identifying the relevant region."}, {"title": "4 Experimental Results", "content": "In this section, we demonstrate the utility of HALLUCINOGEN and MED-HALLUCINOGEN in studying the hallucination of LVLMs and evaluating their effectiveness against state-of-the-art mitigation and reasoning techniques. Next, we describe our experimental setup describing state-of-the-art LVLMs and mitigation techniques, and then discuss the key findings of this benchmarking analysis."}, {"title": "4.1 Experimental setup", "content": "LVLMs. To demonstrate the effectiveness and generalizability of our proposed benchmarks, HALLUCINOGEN, and MED-HALLUCINOGEN, we conduct extensive experiments on eight state-of-the-art LVLMs. These models span a range of sizes, including mid-sized models such as mPLUG-OWL (Ye et al., 2023), mPLUG-OWL2 (Ye et al., 2024), Multi-Modal GPT (Gong et al., 2023), QwenVL (Bai et al., 2023), Qwen2VL (Yang et al., 2024), LLAVA-1.5 (Liu et al., 2023), and MiniGPT-4 (Zhu et al., 2023), each containing 7-10B parameters. Additionally, we evaluate larger models with 11B parameters, such as LlaMa3.2-VL (Dubey et al., 2024).\nHallucination Mitigation Strategies. We include two widely adopted strategies for mitigating hallucinations: reinforcement learning with human feedback (RLHF) (Sun et al., 2023) and LURE."}, {"title": "4.2 Large Visual-Language Models fail under HALLUCINOGEN attacks", "content": "We benchmark eight state-of-the-art LVLMs using our proposed benchmark, HALLUCINOGEN.\nHowever, our evaluation against HALLUCINOGEN reveals that these approaches continue to produce hallucinated responses.\nEvaluation. Similar to POPE (Li et al., 2023), we use accuracy as a metric to evaluate object hallucination in LVLMs. Specifically, accuracy measures the proportion of correctly answered questions, with lower accuracy indicating a higher degree of hallucination in the generated responses. Additionally, following NOPE (Lovenia et al., 2023), we employ string matching algorithms to convert open-ended responses into binary \u201cYes\u201d or \u201cNo\u201d labels based on matching negative keywords such as \"no\", \"not\", \"never\", \"none\", \"nope.\""}, {"title": "4.3 Does Multi-Step Reasoning Amplify Object Hallucinations?", "content": "Chain of Thought (CoT) is an emergent capability in large language models (LLMs) that enables them to reason before generating their final response (Wei et al., 2022). Most LVLMs use strong LLMs to align visual features with textual features, where LLM reasoning ensures the reliability of the LVLM's responses in visual-question answering and reasoning tasks. Previous works have shown that simply adding the phrase \u201cLet's think step by step\u201d at the end of a task prompt encourages models to generate intermediate reasoning steps before arriving at a final answer. In this work, we explore whether asking the LVLMs to reason amplifies object hallucination.\nOur results in Table 2 show that CoT reasoning results in increasing the hallucination in four best-performing LVLMs, where models with CoT prompting result in more hallucination across all four prompt categories from HALLUCINOGEN."}, {"title": "4.4 Investigating the Cause For Object Hallucination", "content": "To investigate the cause of hallucination, we conduct two experiments. First, we analyze the extent to which LVLMs focus on visual input compared to textual input, such as prompts or previously generated text tokens. As shown in Fig.6, we evaluate LLAVA-1.5 on identification and localization tasks in HALLUCINOGEN and plot the attention scores for visual, query, and previous predict tokens. The attention scores are averaged across all attention heads. For visual tokens, an additional averaging is performed across patch lengths. During next-token prediction, the model's attention to visual tokens remains near zero, while attention to query tokens decreases significantly, suggesting that LVLMs prioritize textual tokens over visual tokens, reflecting the influence of strong language prior while generating response (Liu et al., 2024a). We hypothesize that the lack of attention to visual tokens is a key factor for object hallucination in LVLMs as they lack visual understanding of the given image.\nNext, to assess the tendency of LVLMs to respond with \"No,\u201d we introduce Gaussian noise as the visual input and evaluate their performance under explicit and implicit hallucination attacks. We conduct this evaluation against two powerful LVLMS, LLAVA-1.5 Liu et al. (2023) and mPLUG-OWL2 (Ye et al., 2024). As shown in Table 3, while these LVLMs can effectively defend against explicit attacks, such as identifying objects, they perform poorly when we increase the difficulty from Identification \u2192 Counterfactual. Particularly when responding to visual context or counterfactual tasks, these models show an average drop of 72% \u2013 88%. This behavior demonstrates that LVLMs are heavily biased towards consistently responding with \"Yes\" and offering explanations, even for incorrect or misleading prompts."}, {"title": "5 Conclusion", "content": "In this work, we introduce HALLUCINOGEN, a novel benchmark for evaluating object hallucination in Large Vision-Language Models (LVLMs). HALLUCINOGEN incorporates a diverse collection of complex contextual reasoning prompts, referred to as object hallucination attacks, designed to probe LVLMs' understanding of visual context, such as inferring the presence/absence of an object while performing diverse visual-language tasks. We extend HALLUCINOGEN to the biomedical domain with MED-HALLUCINOGEN, a benchmark tailored to evaluate disease hallucination in critical applications such as diagnosing Chest X-rays. Through comprehensive qualitative and quantitative evaluations of diverse LVLMs and various hallucination mitigation strategies on both HALLUCINOGEN and MED-HALLUCINOGEN, we show that most LVLMs perform near the level of random guessing when subjected to our hallucination attacks."}, {"title": "6 Limitation and Future Work", "content": "In this section, we highlight a few limitations and future directions:\n\u2022 We acknowledge that our study primarily focuses on the object hallucination problem in LVLMs and does not address other aspects that evaluate the broader capabilities of these models.\n\u2022 Currently, the hallucination attacks introduced in HALLUCINOGEN are centered on foundational vision-language tasks such as Visual Question Answering (VQA). We plan to extend our benchmark to encompass more complex vision-language tasks in the future.\n\u2022 The current results on HALLUCINOGEN reveal significant potential for improvement in addressing object hallucination. Moving forward, we aim to develop robust hallucination mitigation strategies for LVLMs.\n\u2022 Our results show that both generic and medical LVLMs lack visual understanding, highlighting the need for developing LVLMs that are not strongly dependent on the language model to perform VQA tasks."}]}