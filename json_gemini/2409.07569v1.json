{"title": "A Survey of Inverse Constrained Reinforcement Learning: Definitions, Progress and Challenges", "authors": ["Guiliang Liu", "Sheng Xu", "Shicheng Liu", "Ashish Gaurav", "Sriram Ganapathi Subramanian", "Pascal Poupart"], "abstract": "Inverse Constrained Reinforcement Learning (ICRL) is the task of inferring the implicit constraints\nfollowed by expert agents from their demonstration data. As an emerging research topic, ICRL has\nreceived considerable attention in recent years. This article presents a categorical survey of the lat-\nest advances in ICRL. It serves as a comprehensive reference for machine learning researchers and\npractitioners, as well as starters seeking to comprehend the definitions, advancements, and impor-\ntant challenges in ICRL. We begin by formally defining the problem and outlining the algorithmic\nframework that facilitates constraint inference across various scenarios. These include determinis-\ntic or stochastic environments, environments with limited demonstrations, and multiple agents. For\neach context, we illustrate the critical challenges and introduce a series of fundamental methods to\ntackle these issues. This survey encompasses discrete, virtual, and realistic environments for evalu-\nating ICRL agents. We also delve into the most pertinent applications of ICRL, such as autonomous\ndriving, robot control, and sports analytics. To stimulate continuing research, we conclude the survey\nwith a discussion of key unresolved questions in ICRL that can effectively foster a bridge between\ntheoretical understanding and practical industrial applications.", "sections": [{"title": "1 Introduction", "content": "To ensure the reliability of a Reinforcement Learning (RL) algorithm within safety-critical applications, it is crucial\nfor the agent to have knowledge of the underlying constraints. However, in many real-world tasks, the constraints are\noften unknown and difficult to specify mathematically, particularly when these constraints are time-varying, context-\ndependent, and inherent to experts' own experience. \nAn effective approach to resolve the above challenges is Inverse Constrained Reinforcement Learning (ICRL), which\ninfers the implicit constraints obeyed by expert agents, utilizing experience collected from both the environment and\nthe observed demonstration dataset. These constraints learned through a data-driven approach, can effectively general-\nize across multiple environments, thereby providing a more comprehensive explanation of the expert agents' behavior\nand facilitating safety control in downstream applications."}, {"title": "1.1 The Significance of this Survey", "content": "The survey makes the following contributions.\nIn-Depth Guide to ICRL: More specifically, we introduce the forward and backward procedures of ICRL within the\nframework of a Constrained Markov Decision Process (CMDP). To substantiate the rationale behind ICRL, we high-\nlight fundamental differences in comparison to Inverse Reinforcement Learning (IRL) and Inverse Optimal Control\n(IOC).\nOverview of Recent Advancements: To illustrate the recent developments in a structured format, Table 1 presents\nthe existing ICRL methodologies under various learning contexts and environments. Specifically: 1) In deterministic\nenvironments, we present the Maximum Entropy (MEnt) ICRL approach, which investigates both discrete and con-"}, {"title": "1.2 Organization of Contents", "content": "As ICRL is an emerging field, this article is intended to serve as a comprehensive guide for readers keen to learn\nabout it. The structure of this article is outlined as follows: 1) In Section 2, we provide a thorough introduction to\nthe foundations and terminologies of ICRL. This includes the mathematical definitions of RL, Constrained RL, and\nICRL, as well as the methods employed to regularize the learned constraints. 2) Section 3 delves into the research top-\nics related to ICRL, such as Inverse Reinforcement Learning (IRL) and Inverse Optimal Control (IOC). We highlight\ntheir fundamental differences from ICRL, thereby underscoring the unique value of ICRL. 3) Section 4 and Section 5\nexplore recent advancements in ICRL. This includes discussions about constrained inference algorithms derived from\ndeterministic and stochastic environments. Section 6 and Section 8 describe recent progress in ICRL from partial\ndemonstration data and multiple agents in the environment. 4) Section 7 investigates an approach to simultaneously\ninfer constraints and rewards. 5) Section 9 presents the benchmarks and real-world applications of ICRL. 5) Sec-\ntion 10 concludes this survey and outlines important challenges and open questions for future ICRL research."}, {"title": "2 Background and Notation", "content": "In this section, we establish our notation and provide the essential background required by readers to understand the\nremainder of the survey. \nSpecifically, we use lower-case letters (e.g., s) to denote scalars, and bold and lower-case letters to denote vectors (e.g.,\ns) and bold upper-case letters (e.g., S) to denote matrices. In the rest of this section, we introduce key definitions for"}, {"title": "2.1 Reinforcement Learning", "content": "Reinforcement learning (RL) algorithms are generally based on a Markov Decision Processes (MDP) M (Sutton &\nBarto, 2018), which can be defined by a tuple (S, A, PR, PT, \u03b3, \u03a4, \u03bc\u03bf) where: 1) S and A denote the space of states\nand actions. 2) pr(s'|s, a) and pr(r|s, a) define the transition and reward distributions. 3) \u03b3 \u2208 [0, 1) is the discount\nfactor. 4) T \u2208 [0,\u221e) defines the planning horizon. 5) \u03bc\u03bf = p(s) denotes the initial state distribution. In an MDP M,\nthe objective is to solve the sequential decision problem to maximize the (discounted) cumulative reward by learning\na policy \u03c0(as). The optimization problem can be defined as:\n$I_{\\pi} = \\underset{\\pi}{arg \\max} \\mathbb{E}_{p_{r}, p_{T},\\pi, \\mu_{0}}\\left[\\sum_{t=0}^{T} \\gamma^{t} r(s_{t}, a_{t})\\right] + \\frac{1}{\\alpha} H(\\pi)$\nwhere H(\u03c0) represents the policy entropy weighted by . Incorporating such an entropy regularizer offers several\nkey advantages: 1) It can appropriately model the bounded rationality in human behaviors (see Section 2.4). 2) It\nleads to a soft representation of the optimal policy, which can better model the sub-optimal behaviors and is more\nrobust to stochasticity in the environment. 3) Depending on the problem settings, it can represent either trajectory-\nlevel entropy H(\u03c0(\u03c4)) = \u2212\u0395\u03c0(7) [log \u03c0(\u03c4)] (see Sections 4.1 and 4.2) or causal entropy (i.e., discounted cumulative\nstep-wise entropy (Bloem & Bambos, 2014)) $H(a_{0:\\infty}|s_{0:\\infty}) = -\\mathbb{E}_{\\pi,\\rho_{T},\\mu_{0}}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} \\log \\pi(a_{t}|s_{t})\\right]$ (see Section 5.1\nand 5.2), accommodating the dynamics in both deterministic and stochastic environments.\nIn the process of solving this problem, the agent generates a trajectory $T^{\\pi} = [s_{0},a_{0},\\ldots,a_{T-1},s_{T}]$ and $p(T^{\\pi}) =$\n$p(s_{0}) \\prod_{t=0}^{T-1} \\pi(a_{t}|s_{t})p_{T}(s_{t+1}|s_{t}, a_{t})$. In this paper, we use Em to denote the set of trajectories in the MDP M."}, {"title": "2.2 Constrained Reinforcement Learning", "content": "Constrained Reinforcement Learning (CRL) typically considers a constrained optimization problem under a Con-\nstrained Markov Decision Processes (CMDPs) Mc = (S, A, PR, PT, {(PC\u2081, \u20aci)}\u2200i, \u03b3,\u03a4, \u03bc\u03bf) which augments the"}, {"title": "2.3 Inverse Constrained Reinforcement Learning", "content": "In the context of a deterministic environment, pc, (c|s, a) can be simplified as ci (s, a) which uniquely determines the cost based on a state-"}, {"title": "2.4 Regularizing the Learned Constraints", "content": "ICRL is essentially an ill-posed problem since there exist different constraints that can equivalently explain the expert\ndemonstration, and thus the true constraints are not uniquely identifiable. This characteristic\nmakes identifying the true underlying constraints a difficult task. To mitigate these challenges and enhance the iden-\ntification of the optimal constraints, an effective method is adding additional regularization into constraint learning.\nPopular constraint regularization methods include:\nMinimum Constraints. An effective regularization approach involves limiting the complexity of the constraint. In a\ndiscrete domain, this can be understood as identifying the constraint set (i.e., the unsafe set) C, which includes only the\nminimum number of state-action pairs that are critical for explaining the expert behaviors \nIn the continuous domain, constraining the complexity often implies enhancing the sparsity of the"}, {"title": "3 Related Topics", "content": "In this section, we cover the research topics that are relevant yet distinct from ICRL. While studying these topics are\nnot our primary focus, we provide an overview of their key definitions and algorithms, and most importantly, highlight\nthe crucial differences with ICRL in order to aid readers in gaining a more comprehensive understanding of our study."}, {"title": "3.1 Inverse Reinforcement Learning", "content": "Inverse Reinforcement Learning (IRL) is the method for deducing the reward function within an MDP based on ob-\nserved expert demonstrations. Multiple algorithmic frameworks have been developed for addressing the IRL problem.\nTo provide a concrete example, MEnt IRL, which is among the most extensively studied IRL algorithms, formulates\nthe objective function as a two-player max-min game:\n$\\underset{r \\in \\mathbb{R}}{\\max} \\underset{\\pi \\in \\Pi}{\\min} L(\\pi, r) = \\mathbb{E}_{\\rho_{E}}[r(s, a)] - \\mathbb{E}_{\\rho_{\\pi}}[r(s, a)] - H(\\pi) - \\psi(r)$\nwhere PE and prefer to occupancy measures derived by the expert and imitation policies, H denotes the entropy\nand & is a convex reward regularizer. Given that this is a concave-convex max-min objective, the Lagrange duality\ngap between the dual problem and the original one is effectively closed. This structure allows the use of established\noptimization techniques \nfor deriving the optimal policy representation:\n$\\pi(a_{t}| s_{t}) = \\frac{\\exp[Q^{\\text{soft}}(s_{t}, a_{t})]}{\\int \\exp[Q^{\\text{soft}}(s_{t}, a)]da}$\nwhere the function Qsoft satisfies the soft Bellman equation:\n$Q^{\\text{soft}}(s_{t}, a_{t}) = r(s_{t}, a_{t}) + \\mathbb{E}_{s_{t+1} \\sim p(\\cdot | s_{t}, a_{t})}[\\log \\int \\exp Q^{\\text{soft}}(s_{t+1}, a_{t+1})]$\nUnder this formulation, the goal is to find the reward function that can maximize the likelihood of generating expert\ndemonstrations, i.e., the corresponding objective is\n$\\underset{r}{arg \\max} \\mathbb{E}_{T_{E} \\in D_{E}} \\log\\left[\\prod_{t \\in [0, T]} \\pi(a_{E, t}|s_{E, t})\\right]$"}, {"title": "3.2 Constraint Inference in Inverse Optimal Control", "content": "Inverse Optimal Control (IOC) is a subfield that bridges the gap between machine learning and control theory. The\nprimary objective of IOC problems is to deduce cost functions or constraint functions by closely observing expert\nbehaviors. An IOC algorithm is comprised of a forward optimal control problem and a backward inference problem."}, {"title": "4 Constraint Inference in the Deterministic Environment", "content": "To conduct constraint inference under environments with deterministic dynamics, the Maximum Entropy (MaxEnt)\nRL approach is among the most extensively studied ICRL methods. In the rest of this section, we will introduce ICRL\nin both discrete and continuous domains under the MaxEnt framework."}, {"title": "4.1 Maximum Entropy ICRL in the Discrete Domain", "content": "We start by considering the environment with deterministic dynamics and a discrete state-action space. In this discrete\ndomain, the goal of ICRL is to determine the most plausible set of constraints, denoted as C*, that can be incorporated\ninto the original MDP M to explain the expert demonstrations DE. To obtain the optimal policy representation under"}, {"title": "4.2 Maximum Entropy ICRL in the Continuous Domain", "content": "In continuous state and action spaces, constructing the constraint set C* is computationally intractable. \nIn deterministic environments, previous ICRL works, including , often model hard constraints (\u20ac = 0). By\nemploying the Karush-Kuhn-Tucker (KKT) condition and the interior\npoint method with log barrier parameterized by \u00df, we can derive the optimal policy and represent the probability of\ngenerating a trajectory as follows:\n$\\begin{aligned} \\rho_{M}(\\tau) = \\frac{1}{Z_{e}} e^{\\frac{1}{\\alpha}r(\\tau)+\\beta \\log \\left[\\prod_{(s, a) \\in \\tau} \\Phi_{\\omega}(s, a)\\right]} \\end{aligned}$\nwhere 1) \u1e9e balances the reward maximization and cost minimization in the objective and 2) the partition function can\nbe denoted as \nUsing this policy representation, the feasibility function \u03c6\u1ff3 can\nbe adjusted to maximize the log-likelihood of generating expert data, i.e., max\u025b log pM. (DE). Taking the gradient of\nEquation (19) with respect to w gives us:\n$\\nabla_{\\omega} \\log p_{M} (D_{E}) = \\nabla_{\\omega} \\sum_{T_{E} \\in D_{E}} \\log \\rho_{M} (T_{E})$\n$\\begin{aligned} = \\sum_{T_{E} \\in D_{E}}\\left[\\beta \\nabla_{\\omega} \\log\\left(\\prod_{(s_{E}, a_{E}) \\in T_{E}} \\Phi_{\\omega}(s_{E}, a_{E})\\right)\\right] - \\mathbb{E}_{T \\sim \\pi(\\tau)}\\left[\\beta \\nabla_{\\omega} \\log \\left(\\prod_{(s, a) \\in T} \\Phi_{\\omega}(s, a)\\right)\\right] \\end{aligned}$\nSimilarly to constraint inference in a discrete environment, constraining all the states not covered by expert demon-\nstration is a trivial solution. To find the most effective constraint, Malik et al. (2021) incorporated the regularizer on"}, {"title": "5 Constraint Inference in Stochastic Environments", "content": "As Figure 3 shows, ICRL alternates between Constrained Reinforcement Learning (CRL) and Inverse Constraint\nInference (ICI) until the imitation policy can reproduce the expert demonstrations. The MaxEnt-based algorithms\n(Section 4) assume deterministic training environments, without considering the influence of underlying stochasticity\nin the environment. Specifically, stochastic transition functions introduce aleatoric uncertainty. Striving for reliable\nconstraint inference, the policy represent (e.g., the maximum entropy policy in 16) and the constraint model (e.g., \u03c6\u03c9)\nmust be sensitive to its influence.\nIn this section, in order to develop ICRL algorithms that are robust to the underlying uncertainty in the environment, we\nprovide an overview of 1) Maximum Causal Entropy ICRL in both discrete (Section 5.1) and continuous (Section 5.2)\ndomains, 2) soft constraints (Section 5.3) that are compatible with the noise induced by stochastic dynamics."}, {"title": "5.1 Maximum Causal Entropy ICRL in the Discrete Domain", "content": "In the MDP with stochastic dynamics, the trajectory-level policy can be factorized into:\n$\\pi(\\tau) = \\mu_{0}(s_{0}) \\prod_{t=0}^{T} \\pi(a_{t}|s_{t})p_{T}(s_{t+1}|s_{t}, a_{t})$\nConsequently, modeling the trajectory-level policy (formula 16) requires accounting for the influence of transition\ndynamics PT, which is often unavailable to the agents during training (typically in the model-free setting). An effective\napproach that can generalize MEnt ICRL to stochastic environments is modeling the causal entropy framework. The causal entropy can be represented as:\n$H(a_{0:\\infty}|s_{0:\\infty}) = \\mathbb{E}_{\\pi,\\rho_{T},\\mu_{0}}[\\sum_{t=0}^{\\infty} -\\gamma^{t} \\log \\pi(a_{t}|s_{t})]$\nwhere the step-wise policy \u03c0(s|a) depends only on the available information at each step.\nA critical challenge to constraint inference in stochastic environments lies in the definition of the permissibility func-\ntion.\n$1_{M_{c}}(s, a) = 1[p(S_{t+1} = s|S_{t} = s, A_{t} = a) < \\psi(s), \\forall s \\in C^{\\prime}]$"}, {"title": "5.2 Maximum Causal Entropy ICRL in the Continuous Domain", "content": "In the continuous environment, constructing the constraint set is computationally intractable\nhere constraint in the MCEnt objective can be updated to:\n$\\begin{aligned} -\\mathbb{E}_{T \\sim (\\pi, T)}\\left[\\sum_{t=0}^{T-1} \\log \\varphi_{\\omega}(s, a)\\right] \\leq \\varepsilon \\end{aligned}$"}, {"title": "5.3 Inferring Soft Constraints with ICRL", "content": "Unlike the hard constraints that impose absolute constraint satisfaction, soft constraints can account for noise in sensor\nmeasurements, such as those caused by stochastic transition functions (pt) or cost functions (pc). This considera-\ntion helps address potential violations that may arise in expert demonstrations due to the stochastic dynamics in the\nenvironment. Specifically, the soft constraint can be represented as which is akin\nto the cumulative constraint , but the soft constraints requires a threshold \u20ac > 0. To better align with the soft\nconstraint inference, the Inverse Soft Constraint Learning (ISCL) algorithm taken by \nemproys\nthe Deep Constraint Correction (DC\u00b3) framework \nto extend this approach to ICRL,\nrepresented the CRL objective as:\n$\\begin{aligned} \\underset{\\pi}{arg \\max} \\underset{\\rho_{T},\\mu_{0},\\pi}{\\mathbb{E}}\\left[\\sum_{t=0}^{T} \\gamma^{t} v_{c}(s_{t}, a_{t})\\right] + \\lambda \\text{RELU}\\left[\\mathbb{E}_{\\rho_{T},\\mu_{0},\\pi}\\left[\\sum_{t=0}^{T} \\gamma^{t} c(s_{t}, a_{t})\\right] -\\varepsilon\\right] \\end{aligned}$\nUpon obtaining the optimal policy during a specific run, ISCL incorporates it into the candidate policy set II such\nthat I = I \u222a \u03c0. Accordingly, the ICI objective in ICRL can be expressed as:\n$\\begin{aligned} arg \\min - \\mathbb{E}_{(\\pi,T)_{mix}}\\left[\\sum_{t=0}^{T} \\gamma^{t} c(s_{t}, a_{t})\\right] + \\lambda \\text{RELU} \\left[\\mathbb{E}_{(\\rho_{T} \\sim D_{E})}\\left[\\sum_{t=0}^{T} \\gamma^{t} c(s_{t}, a_{t})\\right] -\\varepsilon\\right] \\end{aligned}$\nIt is important to note that the first expectation is taken over the mixture policies \u03c0mix, while the second expectation is\nderived from the expert dataset DE. ISCL constructs \u03c0mix as a weighted combination of the candidate policies in II."}, {"title": "6 Constraint Inference from Limited Demonstrations", "content": "The ICRL algorithms discussed earlier primarily concentrate on addressing aleatoric uncertainty induced by the\nstochastic transition dynamics in the environment. Apart from it, upon updating the constraint model, due to the\nlimited training data size, epistemic uncertainty arises in game states that fall outside the data distribution. To be more\nspecific, in an ICRL task, the training dataset Dtrain for constraint inference records the expert demonstration and\nnominal trajectories generated by the imitation policy\nEpistemic uncertainty\narises when the constraint model is asked to predict the costs of state-action pairs that are out of the training data\ndistribution.\nIn this section, we provide an overview of ICRL algorithms that account for epistemic uncertainty, including the\napproach to estimating the posterior distribution of constraint, data-augmented constraint inference and offline constraint inference ."}, {"title": "6.1 Modelling the Posterior Distribution of Constraint", "content": "Bayesian Posterior Estimation. Existing ICRL methods typically learn a constraint function that best differentiates\nexpert trajectories from generated ones. In contrast to these traditional methods that primarily rely on maximum\nlikelihood estimation,  applied the Maximum-A-Posteriori (MAP) approach to address the\nepistemic uncertainty during constraint inference. This led to the development of the Bayesian Inverse Constraint\nReinforcement Learning model that infers a posterior probability distribution over constraints based on\ndemonstrated trajectories.\nBICRL is primarily designed to infer the constraint set C \u2208 {0,1}|s| in the discrete state space S.\nBICRLcomputes the likelihood of expert demonstration TE under the MCEnt framework (see Section 5.1), so:\n$\\begin{aligned} p_{M_{\\lambda}}(\\tau_{E}|C, \\lambda) = \\mu_{0}(S_{0}) \\left[\\prod_{t=0}^{T-1} p_{T}(s_{t+1}|s_{t}, a_{t})\\right] \\left[\\pi_{M_{c, \\lambda}}(a_{t}|s_{t})\\right] = \\text{constant.}\\prod_{t=0}^{T-1} \\frac{e^{Q_{c,\\lambda}^{\\text{soft}}(s_{t}, a_{t})}}{e^{V_{c}^{\\text{soft}}(s_{t})}} \\end{aligned}$\nwhere the value functions can be defined by:\n$Q_{c,\\lambda}^{\\text{soft}}(s_{t}, a_{t}) = r(s_{t}, a_{t}) + \\lambda \\log 1_{M_{c}, t}(S_{t}, a_{t}) + \\mathbb{E}_{S_{t+1}}V_{c,\\lambda}^{\\text{soft}}(S_{t+1})$\n$V_{c}^{\\text{soft}}(s_{t}) = \\log \\int e^{Q_{c,\\lambda}^{\\text{soft}}(s_{t}, a_{t})} da_{t}$\nThe permissibility identifier 1M\u00f4,t(st, at) defines whether performing the action at in a state st will lead to the\ntransition to a constrained state st+1 \u2208 \u0108 \nand these (action)-value functions can be computed\nby dynamic programming (Sutton & Barto, 2018).\nBy utilizing the posterior distribution over constraint sets at previous (m - 1)th run as the prior distribution at the\ncurrent run\n, the current posterior distribution at the runs from 2 to M can thus be\ngiven by:\n$p_{m} (C, \\lambda|\\tau_{E}) = \\frac{p_{M_{C,\\lambda}}(\\tau_{E}|C, \\lambda)p_{m}(C, \\lambda)}{p(\\tau_{E})}$\nTo initialize po (C', \u03bb),  selected an uninformative prior. By utilizing the above posterior, the\nMaximum a Posteriori (MAP) estimates for the constraint sets and the penalty reward can be obtained as:\n$C^{\\text{MAP}}, \\lambda^{\\text{MAP}} = \\underset{C,\\lambda}{arg \\max} p(C, \\lambda|\\tau)$\nand the Expected a Posteriori (EAP) estimates can be obtained as:\n$C^{\\text{EAP}}, \\lambda^{\\text{EAP}} = \\mathbb{E}_{C,\\lambda \\sim p(C,\\lambda|\\tau)} [C, \\lambda|\\tau]$\nSince sampling the constraint set C from a continuous state space is computationally intractable, BICRL is mainly\nvalidated based on the discrete state space. How to extend this algorithm to continuous space remains an open problem\nthat requires further exploration."}, {"title": "6.2 Data-Augmented Inverse Constraint Inference", "content": "Epistemic uncertainty arises due to the limited training data and the model's lack of knowledge about Out-of-\nDistribution (OoD) data. An effective measure of epistemic uncertainty is I(w; y|x, D) , which quantifies the amount of information gained by the model w when it observes the true\nlabel y for a given input x, i.e., the greater the uncertainty of the model regarding the data, the more additional infor-\nmation it can obtain once the true label y is observed. Under the setting of ICRL, to reduce the epistemic uncertainty,\nXu & Liu (2024a) added the regularizer I(\u03c9; \u03c6|\u3012, D) into the ICI objective (Equation 20) to propose the following\nobjective:\n$\\underset{E_{\\tau \\in D_{E}}}{\\mathbb{E}}\\left[\\sum_{t=0}^{T} \\log[\\varphi_{\\omega}(s_{t}, a_{t})]\\right] - \\underset{\\mathbb{E}_{(\\overline{\\tau} \\sim D)}}{\\mathbb{E}}\\left[\\sum_{t=0}^{T} \\log[\\varphi_{\\omega}(\\overline{s}_{t}, \\overline{a}_{t})]\\right] - \\alpha I(\\omega; \\varphi|\\tau, D)$\nSince the mutual information term I is computationally intractable, Xu & Liu (2024a) showed it can be empiri-\ncally approximated by I(w; $|\u3012,D) = H[p(\u00a2|\u3012,D)] \u2212 \u03a3m H[p(\u03c6|\u3012;Wm)] where wm ~ q(w) Specifically, 1)\nH[p($|\u012b,wm)] defines the entropy of a constrained model parameterized by wm. 2) H[p($|\u3012, D)] \u2208 [0, \u221e) measures\nthe amount of information required to describe the feasibility & of an exploratory trajectory based on the given\ntraining dataset D. By substituting them in formula (50), we obtain the following objective:\n$\\frac{1}{M} \\underset{T \\in D_{E}}{\\mathbb{E}}\\left[\\sum_{m}^{M} \\sum_{t=0}^{T} \\log[\\varphi_{\\omega}(s_{t}, a_{t})]\\right] - \\underset{\\mathbb{E}_{(\\overline{\\tau} \\sim D)}}{\\mathbb{E}}\\left[\\sum_{t=0}^{T} \\log[\\varphi_{\\omega}(\\overline{s}_{t}, \\overline{a}_{t})]\\right] - H[p(\\varphi|\\tau, D)] + \\alpha H[p(\\varphi|\\tau; W_{m})]$\nInspired by \n, Xu & Liu (2024a) used dropout layers for approximating the\ndistribution of model parameters q(w). Besides, to reduce the conditional entropy H[p($|\u012b, D)], Xu & Liu (2024a)\nproposed expanding the training dataset by adding generated trajectories . To be more specific, the aug-\nmented expert dataset is constructed by\n$ \\forall \\phi_{G} = 1$, and the augmented nominal dataset is constructed\nby . By substituting them to objective (51), we arrive at the following objective:\n$\\frac{1}{M} \\underset{T \\in D_{E}}{\\mathbb{E}}\\left[\\sum_{m}^{M} \\sum_{t=0}^{T} \\log[\\varphi_{\\omega_{m}}(s_{t}, a_{t})]\\right] - \\underset{\\mathbb{E}_{(\\overline{\\tau} \\sim D)}}{\\mathbb{E}}\\left[\\sum_{t=0}^{T} \\log[\\varphi_{\\omega_{m}}(\\overline{s}_{t}, \\overline{a}_{t})]\\right] + \\alpha H[p(\\varphi|\\tau; W_{m})]$\nIn order to generate the trajectories TG, Xu & Liu (2024a) designed a Flow-based Trajectory Generation (FTG) al-\ngorithm that extends the Generative Flow Network (GFlowNet)  to generate a diverse set of\ntrajectories based on the dataset and task-dependent rewards. Similar to the generative data synthesizer , FTG explores various combinations of points (such as state-action pairs in RL) within sequential data. This\ndesign enables FTG to generate trajectories in which the sequence of states and actions deviates from those in the\ntraining data samples through multiple rounds of random sampling \n. The dataset augmented with these\ntrajectories can more accurately characterize the underlying distribution of feasible and infeasible trajectories, thereby\nreducing the uncertainty associated with the parameters of the constraint model."}, {"title": "6.3 Offline Inverse Constrained Learning", "content": "The aforementioned methods primarily focus on the impact of limited expert demonstrations. However, the imitation\npolicy can be learned through interaction with the environment. In contrast, a more stringent scenario is Offline ICRL,\nwhere the agent must infer constraints and learn imitation policies based solely on a fixed dataset, without access to\nthe environment for additional interactions.\nSpecifically, in Offline ICRL, we are given an offline dataset Do = {sn, an, r1 C Do. These expert trajectories are generated by an optimal\npolicy \u03c0\u0395 adhering to the unobserved constraints function c(s, a). i.e.\n$\\begin{aligned} \\pi_{E} = \\underset{\\pi}{arg \\max} \\mathbb{E}_{\\tau_{E}} [r(s, a)], s.t.\\mathbb{E}_{\\rho_{\\pi}} [c(s, a)] \\leq \\varepsilon \\end{aligned}$\n(p\u2122 denotes the occupancy measure by following the expert policy \u03c0\u0395). An estimated cost\nfunction \u0109 is a feasible solution to ICRL if and only if the optimal solution 7 can reproduce dE. To achieve offline\nICRL,\nmainly study hard constraints with \u20ac = 0. The CRL problem can be formulated as:\n$\\begin{aligned} \\underset{\\pi}{max} \\mathbb{E}_{\\rho_{\\pi}} [r(s, a)] s.t.\\rho_{\\pi} (s, a)c(s, a) \\leq 0 \\forall s, a \\end{aligned}$"}, {"title": "7 Simultaneous Inference of Rewards and Constraints", "content": "Most IRL and ICRL algorithms can learn either a reward function or a constraint function, but cannot infer both. An\nintriguing yet relatively unexplored extension of IRL and ICRL is to simultaneously infer rewards and constraints from\nexpert demonstrations. However, since both IRL and ICRL are inherently ill-posed due to the ambiguity in identifying\nthe reward function and constraint function, concurrently inferring rewards and constraints can substantially amplify\nthe complexity and ambiguity of identifying the true underlying rewards and constraints.\nIn this section, we provide an overview of algorithms that learn both rewards and constraints, including constraint-\nbased Bayesian IRL  that learns different local rewards and local constraints from\ndifferent expert trajectory segments, and maximum-likelihood ICRL \nthat\nlearns global rewards and constraints from complete trajectories."}, {"title": "7.1 Constraint-Based Bayesian Inverse Reinforcement learning", "content": "While the existing IRL and ICRL methods typically learn a single reward function or a single constraint function\nfrom the expert trajectories, \nproposed a Constraint-based Bayesian Nonparametric IRL (CBN-IRL)\nalgorithm that learns multiple local rewards/goals and local constraints from different expert trajectory segments.\nSpecifically, \nsplits the expert trajectory TE into multiple partitions . Within each partition \u03b9,\nCBN-IRL learns a goal function gi(s) that assigns a positive reward to the destination s* and zero to other states, i.e.,\ngi(s) = 1(s = s*) where s* is the goal state. CBN-IRL also learns a constraint function c(s) \u2192 {0,1} assigns a\nvalue of one to infeasible or unsafe states and zero to feasible or safe states. Following this setting, \nproposed the Maximum-A-Posterior (MAP) objective to infer the constraint c\u2081(s) and goal gr(s) for each segment.\n$\\begin{aligned} (g^{*}, c^{*}) = \\underset{g, c}{arg \\max} \\prod_{j=1}^{J} [P(T_{E,j}|g_{i_{j}}, C_{i_{j}})P(i_{j}|i_{-j})P(g_{0}, C_{0}) \\end{aligned}$"}, {"title": "7.2 Maximum-Likelihood Inverse Constrained Reinforcement Learning", "content": "While CBN-IRL learns multiple local reward/goal functions and local constraint functions from different expert tra-\njectory segments, Liu & Zhu (2022; 2023a;b) proposed to learn single reward function and constraint function for the\ncomplete expert trajectories. Specifically, Liu & Zhu formulated a bi-level optimization problem:\n$\\begin{aligned} \\underset{r}{max} \\mathbb{E}_{T\\in D_{E}} \\left[\\sum_{t=0}^{T} \\nu log \\pi_{c}(r); r (a_{t}|S_{t})\\right], \\end{aligned}$\n$\\begin{aligned} s.t. c^{*}(r) := \\underset{c}{arg \\min} \\mathbb{E}_{T, \\pi_{r},\\mu_{0}} \\left[\\nu_{tr} (s_{t}, a_{t}) - c(s_{t}, a_{t})\\right]] + H(c_{ir}) + \\mathbb{E}_{T \\sim D_{E}} \\left[\\sum_{t=0}^{T} \\gamma^{*}c(a_{t}|s_{t})\\right] \\end{aligned}$\nThe upper level aims to learn a reward function r to maximize the log-likelihood of the expert trajectories DE where\nTric is the constrained soft Bellman policy\nthat the constrained soft Bellman policy maximizes the entropy-\nT\nregularized cumulative reward-minus cost, i.e., \n$=\\underset{\\pi}{arg \\max} \\mathbb{E}_{\\rho_{T},\\pi,\\mu_{0}} \\left[\\sum_{t=0}^{T} [r(s_{t}, a_{t}) - c(s_{t}, a_{t})]\\right] + H(\\pi)$.\nperforms linearly independent,"}]}