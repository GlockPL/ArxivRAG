{"title": "Chat3GPP: An Open-Source Retrieval-Augmented Generation Framework for 3GPP Documents", "authors": ["Long Huang", "Ming Zhao", "Limin Xiao", "Xiujun Zhang", "Jungang Hu"], "abstract": "The 3rd Generation Partnership Project (3GPP) documents is key standards in global telecommunications, while posing significant challenges for engineers and researchers in the telecommunications field due to the large volume and complexity of their contents as well as the frequent updates. Large language models (LLMs) have shown promise in natural language processing tasks, but their general-purpose nature limits their effectiveness in specific domains like telecommunications. To address this, we propose Chat3GPP, an open-source retrieval-augmented generation (RAG) framework tailored for 3GPP specifications. By combining chunking strategies, hybrid retrieval and efficient indexing methods, Chat3GPP can efficiently retrieve relevant information and generate accurate responses to user queries without requiring domain-specific fine-tuning, which is both flexible and scalable, offering significant potential for adapting to other technical standards beyond 3GPP. We evaluate Chat3GPP on two telecom-specific datasets and demonstrate its superior performance compared to existing methods, showcasing its potential for downstream tasks like protocol generation and code automation.", "sections": [{"title": "I. INTRODUCTION", "content": "In global telecommunications networks, the 3rd Generation Partnership Project (3GPP) documents serve as a foundational standard, covering various aspects such as wireless communication, network architecture, core networks, access networks, and signaling and data transmission between devices. Since its establishment in 1998, the 3GPP standards have evolved through multiple generations of communication technologies, including 2G, 3G, 4G, 5G, and continue to play an important role in the development of 6G standards. The extensive scope and complexity of 3GPP documents, filled with specialized terminology and subject to frequent updates and revisions, present significant challenges. Even experienced engineers often need to spend substantial time reviewing and comprehending these documents to grasp key details. In this context, how to efficiently and accurately understand and apply these specification documents has become a critical challenge for technicians, engineers, and researchers in the telecommunications field.\nRecently, large language models (LLMs) have made significant developments and advancements in the field of natural language processing (NLP). Models such as LLaMA and Mistral have demonstrated outstanding performance in tasks such as text generation, summarization and question-answering, owing to their large-scale pre-training, which enables them to generate and reason with text more naturally and accurately. However, while these general-purpose LLMS perform well across many domains, they face limitations when applied to specialized fields like telecommunications. In order to bridge the gap between general-purpose LLMs and the specialized needs of the telecommunications, researchers have made substantial efforts. Bariah et al. adapted pre-trained generative models for identifying the 3GPP standard working groups. Maatouk et al. curated TeleQnA, a multiple-choice question (MCQ) benchmark dataset to evaluate the knowledge of LLMs in telecommunications. Furthermore, Zou et al. introduced to adapt general-purpose LLMs to telecom-specific tasks, while Maatouk et al. focued on open-sourcing a series of LLMs for telecommunications applications, along with two datasets for training and evaluation.\nHowever, training telecom-specific LLMs presents several challenges, including the lack of comprehensive telecommunications datasets and the diverse presentation formats such as figures and tables that complicate the training process. Additionally, the frequent updates in telecom knowledge make it difficult for LLMs to stay up-to-date, with high training costs as a further barrier. Therefore, a method, retrieval augmented generation (RAG), has emerged as an effective solution due to its cost effectiveness, adaptability, and scalability. RAG enhances the capability of LLMs by integrating an information retrieval system that supplies relevant data during the generation process. Recently, Telco-RAG provides generally applicable guidelines for overcoming common challenges in implementing an RAG pipeline in highly technical domains.\nIn this Paper, We introduce Chat3GPP, an RAG pipeline for 3GPP documents, which not only reduces model maintenance costs but also enhances the system's flexibility and scalability. By constructing a 3GPP document embedding database, it can swiftly retrieve relevant information in response to user queries and generate answers through a model. Importantly, Chat3GPP is not limited to 3GPP documents and its framework can be easily extended to other standard documents, offering significant potential for broader application. The key contributions of this paper are as follows:\n\u2022\tWe propose Chat3GPP, an open-source chatbot for 3GPP documents based on RAG framework without any domain-specific fine-tuning. This allows for easy"}, {"title": "II. RELATED WORK", "content": "This section first reviews the research progress on the application of LLMs in the telecommunications domain, and then introduces the background of RAG and summarizes the advancements in telecom-specific RAG research."}, {"title": "A. LLMs in Telecommunications", "content": "The application of large language models (LLMs) in the telecommunications domain has been a topic of increasing interest due to their potential to revolutionize various aspects of the industry. Maatouk et al. explored the potential impact of LLMs in the telecommunications industry and examined their application scenarios in the telecommunications field, while pointing out the key research directions that need to be addressed to fully exploit the potential of LLMs. Bariah et al. demonstrated the potential of LLMs for identifying 3GPP standard working groups by fine-tuning models such as BERT, DistilBERT, ROBERTa, and GPT-2. Nabeel et al. introduced a new framework to generate test scripts using a hybrid generative model, reducing lead time in telecom software development. Zou el al. proposed a framework for adapting general-purpose LLMs to telecom-specific LLMs by continuous pre-training, instruction tuning, and alignment tuning seperatly on pre-training datasets, instruction datasets, and preference datasets, demonstrating the effectiveness of domain-specific training and tuning. Maatouk et al. focused on developing a series of LLMs specifically tailored for the telecommunications domain, known as Tele-LLMs. They created the Tele-Data and Tele-Eval datasets and conducted extensive experiments with various training techniques to identify the optimal strategies for adapting LLMs to the telecommunications field."}, {"title": "B. RAG", "content": "RAG is a technique that combines information retrieval with language generation models to enhance the performance of LLMs in knowledge-intensive tasks. RAG was introduced to addresses the limitations of LLMs initially , such as knowledge cutoff and hallucination, by integrating external knowledge into the model. The core idea is to retrieve relevant information from a knowledge base and use it to augment the input to the language model, thereby improving the accuracy and relevance of the generated text.\nThe RAG system consists of two main components: the retriever and the generator. The retriever module uses a pre-trained text embedding model to convert queries and documents into vector representations, which are then searched in a embedding database to find the most relevant documents. The generator module combines the retrieved documents with the original query to form a richer context, which is used to generate the final response. This architecture has been applied in various domains, including question answering, text summarization, and content generation, where RAG has shown significant improvements in performance.\nRecent research has focused on optimizing and extending the RAG framework. For instance, RAFT adapts LLMS to specific domains by training the model to ignore irrelevant retrieved documents and focus on relevant information. Corrective RAG enhances the robustness and precision of LLMs by evaluating the quality and relevance of retrieved documents and using a confidence-based adaptive retrieval mechanism. Additionally, RA-ISF uses an iterative self-feedback process to enhance the problem-solving efficiency of LLMs, resulting in improved performance in factual reasoning and reduced hallucination.\nThe application of RAG in the telecommunications domain presents unique challenges due to the complex nature of telecom standard documents and the rapid evolution of the field. Telco-RAG , an open-source RAG framework, has been developed to handle the specific needs of telecommunications standards, particularly 3GPP documents. Telco-RAG addresses the critical challenges of implementing a RAG pipeline on highly technical content, paving the way for applying LLMs in telecommunications and offering guidelines for RAG im-plementation in other technical domains."}, {"title": "III. METHODOLOGY", "content": "In this section, we introduce the methodology of Chat3GPP, as shown in Fig. 1. The overall architecture involves several key phases, including data pre-processing, indexing, retrieval, and generation, ensuring both efficiency and accuracy in processing and retrieving information from the 3GPP technical documents."}, {"title": "A. Data Pre-processing", "content": "We first crawl the release 17 and release 18 technical specification documents from the 3GPP FTP site [15]. These documents provide detailed specifications for 3GPP technologies and serve as the primary knowledge source for Chat3GPP. We performed the following cleaning steps:\nFormat Conversion. We convert certain .doc format documents into .docx format in order to facilitate processing using the python-docx library.\nContent Filtering. We remov the Contents, References, and Annex sections as these sections irrelevant to what we are concerned about.\nText Extraction. We extract the text content, excluding images and tables, ensuring that only textual data was used for further processing."}, {"title": "B. Indexing Phase", "content": "In the Indexing phase, documents will be processed, segmented, and transformed into vectorial representations to be stored in a embedding database. We use the following strategies including text segmentation, embeddings, and indexing strategy.\n1) Chunking Strategy: Considering the context length limitations of the embedding model, we need to split them into smaller chunks. The quality of text chunks directly impacts the performance during the retrieval phase. Proper segmentation helps maintain both the document's structural organization and its semantic integrity, ensuring that the retrieved results are both accurate and contextually relevant. To achieve this, we adopt the following chunking strategies:\nHierarchical Chunking. We split the specification document according to subheadings, ensuring that each text segment includes the full title path of its corresponding section to preserve the contextual information from the document's structure. In the pre-ranking phase, we combine two retrieval methods: BM25-based retrieval and embedding retrieval .\nRecursive Chunking. The most common chunking method is to split the document into chunks on a fixed number of tokens. However, token-based splitting method leads to truncation within sentences and depends on the tokenizer. Consequently, we used a splitter from langchain named RecursiveCharacterTextSplitter to split long texts into chunks of approximately 1250 characters without overlap, ensuring that the chunks fit within the context length limitation of the embedding model while preserving semantic integrity.\n2) Embedding: In the RAG framework, embedding retrieval is performed by computing the similarity (e.g., cosine similarity) between the embeddings of the query and the text chunks. The effectiveness of this process heavily relies on the semantic representation power of the embedding model. For this purpose, we employed the open-source embedding model BGE-M3 to encode the document chunks into 1024-dimensional embedding vectors. These embeddings are then used for efficient retrieval in the subsequent stages.\n3) Indexing strategy: The text chunks are stored in database in the structure of \"filename, content, embedding\", where:\nfilename represents the specification document of the text block, serving as a reference for tracing the source of the chunk.\ncontent represents the natural language content of the text chunk, which is indexed as a text type in Elasticsearch . The text field is stored using an inverted index, which allows efficient retrieval of documents containing specific query terms. To further improve retrieval efficiency, we customized the analyzer in Elasticsearch and employed a stop-word filter to remove irrelevant words, which helps reduce storage space and enhances search speed.\nembedding represents the embedding vectors of the text blocks, stored in the dense_vector type and employing the Hierarchical Navigable Small World (HNSW) index. HNSW is an efficient approximate nearest neighbor (ANN) search algorithm that accelerates the search process through a graph-based approach."}, {"title": "C. Retrieval Phase", "content": "We employ a two-stage retrieval strategy, pre-ranking and ranking, to enhance both information retrieval efficiency and the quality of generated content as shown in Algorithm 1. In the pre-ranking phase, Chat3GPP first retrieves candidate chunks from the database that are relevant to the query using algorithms such as similarity-based retrieval. These candidate chunks are then scored for relevance, with the top-ranked chunks being selected for further processing, thereby filtering out irrelevant or low-quality candidates and improving retrieval efficiency. Considering both the input query and contextual requirements, the ranking stage further re-ranks candidate chunks to select the most helpful segments and provides high-quality input for the subsequent generation stage. Through the collaborative work of the pre-ranking and ranking stages, Chat3GPP can efficiently retrieve relevant information from large-scale databases and generate more accurate, relevant and high-quality answers."}, {"title": "D. Generation Phase", "content": "During the generation phase, we employ a prompt engineering to incorporate the Top-K2 retrieved text chunks along with the user's query as input to the model. The model is then tasked with generating a response based on these documents. Prompt Engineering refers to the process of designing and optimizing input prompts for LLMs to maximize the quality, relevance, and accuracy of the output. Therefore, we design distinct prompts specifically tailored for MCQs and open-ended questions to optimize performance across different query types."}, {"title": "IV. EXPERIMENTS", "content": "In this section, we use the settings for Chat3GPP as shown in Table I. Based on these settings, We conduct experiments on two evaluation datasets to evaluate Chat3GPP framework in enhancing the functionality of LLMs applied to the telecommunications domain."}, {"title": "A. Evaluation Datasets", "content": "Currently, there are only two open-source evaluation datasets available to test the telecommunications knowledge of domain-adapted models: TeleQnA and Tele-Eval . TeleQnA is a MCQ dataset derived from standard and research papers. While MCQ datasets simplify accuracy evaluation, LLMs are particularly strong at MCQ selection due to the inherent \"selection bias\" common across nearly all LLMs . To address limitation, proposed an alternative approach by creating Tele-Eval, an open-ended telecommunications question dataset.\nTo assess the telecommunications capabilities of Chat3GPP, we used two evaluation sets. The first consists of 734 related to Release 17 and 780 related to Release 18 from TeleQnA. The second includes 26,225 questions related to Release 17 and 32,402 related to Release 18 from Tele-Eval."}, {"title": "B. Evaluation Metrics and Benchmarks", "content": "For the first evaluation dataset, users typically do not provide options when querying an LLM, so we excluded these options during the retrieval process. Instead, we incorporated into the prompt during the generation phase. In the following results as Table II, we use accuracy as the evaluation metric, measuring the proportion of correct answers provided by Chat3GPP for queries in the dataset. We selected TelecomGPT , Llama-3-8B-Tele-it which is the best-performing model from Tele-LLMs, and TelcoRAG as benchmarks for comparison. Since TelecomGPT is a closed-source model, we directly adopted the benchmark results from the paper, which were evaluated on 3,500 questions, some of which derived from 3GPP documents, covering the Lexicon, Standards Overview, and Standards Specifications."}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce Chat3GPP, a RAG system specifically designed to enhance LLMs in understanding and applying 3GPP documents within the telecommunications domain. By leveraging the powerful combination of information retrieval and LLMs, Chat3GPP effectively addresses the challenges associated with the complexity, frequent updates, and specialized nature of 3GPP standards. This demonstrates the flexibility and scalability of the proposed framework, which can be easily adapted to other technical standards, such as those from ITU or IEEE. As a result, Chat3GPP presents a promising solution for improving the efficiency of engineers and technicians working with technical documentation. Additionally, it opens up new possibilities for tasks like document generation and code automation, further expanding its utility in the field.\nHowever, several limitations remain. Firstly, future work could explore deeper integration between fine-tuned models and the retrieval-augmented framework to better understand their interactions and optimize the combination of retrieval and generation components. This could further enhance the model's ability to respond to complex, domain-specific queries. Secondly, 3GPP documents contain not only textual data but also numerous tables and figures, which play a critical role in conveying information. Thus, integrating multi-modal data, such as tables and images, into the RAG framework could provide a richer, more contextually accurate foundation for text generation. In future work, we plan to explore further optimizations to better support a wider range of tasks within the telecommunications domain."}]}