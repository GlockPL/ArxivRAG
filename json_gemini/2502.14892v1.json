{"title": "EgoSpeak: Learning When to Speak for Egocentric Conversational Agents in the Wild", "authors": ["Junhyeok Kim", "Min Soo Kim", "Jiwan Chung", "Jungbin Cho", "Jisoo Kim", "Sungwoong Kim", "Gyeongbo Sim", "Youngjae Yu"], "abstract": "Predicting when to initiate speech in real-world environments remains a fundamental challenge for conversational agents. We introduce EgoSpeak, a novel framework for real-time speech initiation prediction in egocentric streaming video. By modeling the conversation from the speaker's first-person viewpoint, EgoSpeak is tailored for human-like interactions in which a conversational agent must continuously observe its environment and dynamically decide when to talk.\nOur approach bridges the gap between simplified experimental setups and complex natural conversations by integrating four key capabilities: (1) first-person perspective, (2) RGB processing, (3) online processing, and (4) untrimmed video processing. We also present YT-Conversation, a diverse collection of in-the-wild conversational videos from YouTube, as a resource for large-scale pretraining. Experiments on EasyCom and Ego4D demonstrate that EgoSpeak outperforms random and silence-based baselines in real time. Our results also highlight the importance of multimodal input and context length in effectively deciding when to speak. Code and data are available at website.", "sections": [{"title": "Introduction", "content": "Human-like conversational agents have long been a key objective in artificial intelligence. A critical aspect of human conversation is not only understanding what to say but also when to say it\u2014often framed as the study of turn-taking (Duncan, 1972). While most are designed under simplified assumptions where turn boundaries are well-defined or where only audio-based cues are available, real-world conversations can be highly fluid, with overlapping speech, unclear speaker roles, and frequent interruptions (Skantze, 2017, 2021).\nTo address these complexities, we introduce EgoSpeak, a framework that predicts when an agent should begin speaking based on egocentric streaming video. Concretely, EgoSpeak models speech initiation from the first-person perspective of the camera wearer, capturing exactly what the agent sees at each moment in real time. Unlike a third-person or fixed camera view, the egocentric perspective is especially relevant for real-world conversational agents such as social robots that must decide on the fly whether to speak or remain silent. By leveraging the camera wearer's immediate visual context (e.g., facing another person, noticing body language or gaze direction), EgoSpeak can more naturally detect subtle cues that signal an appropriate moment to start speaking. This is particularly crucial for a real-world agent that must not only process inputs in real time, but also respond autonomously in dynamic, multi-speaker environments to appear natural and engaging.\nEgoSpeak incorporates four key capabilities: (1) first-person perspective: aligns closely with real-world interactions for conversational agents, (2) RGB feature processing: handles scenarios where audio or non-verbal cues may be unreliable, (3) dynamic real-time turn-taking: enables more natural and fluid conversations, and (4) continuous untrimmed video stream processing: captures periods of silence and sporadic interactions."}, {"title": "Related Works", "content": "Turn-taking. Turn-taking research has evolved from simple audio-based models (Duncan, 1972; Khouzaimi et al., 2015) to sophisticated multimodal approaches (Maier et al., 2017; Lee et al., 2023; Mizuno et al., 2023; Kurata et al., 2023). Early offline methods, which process entire clips, often result in unnatural pauses. This prompted the development of continuous (online) methods (Skantze, 2017; Ekstedt and Skantze, 2022; Li et al., 2022), including recent multimodal models incorporating non-verbal cues (Onishi et al., 2023). However, these approaches typically rely on controlled dyadic conversations, limiting real-world applicability. EgoSpeak addresses these limitations by adopting a first-person perspective, processing both RGB and audio features, and handling untrimmed video streams, aiming to better align turn-taking models with the complexities of natural conversations."}, {"title": "EgoSpeak Framework", "content": "3.1 Framework Overview\nEgoSpeak is designed for in-the-wild conversational agents, building on the challenges discussed in Section 1, where \"in-the-wild\" refers to real-world conditions outside controlled environments with unpredictable variables and numerous influencing factors.\nEgoSpeak is grounded in the intuition that, in the egocentric video, the camera wearer's speaking moments naturally serve as cues for speech initiation. By predicting these moments from the agent's perspective, our framework learns natural turn-taking behavior, identifying when to speak even after long silences. Moreover, by anticipating these moments in advance, EgoSpeak effectively mirrors human turn-taking, deciding when to begin speaking as a real-world agent would. To achieve this, we train the model with a cross-entropy objective, akin to next-token prediction in language modeling, since it must anticipate speaking before the camera wearer actually speaks.\n3.2 Task Definition\nGuided by this intuition, we formulate the problem of predicting the target speaker's speech in an egocentric streaming video, where the camera wearer is naturally identified as the target speaker. Given the real-time nature of the stream, EgoSpeak only analyzes information available up to the current moment. This design allows our system to capture the continuously unfolding context and prepare a speech onset before a turn-shift occurs in complex, dynamic conversations.\nFormally, let $X_t = [x_1,...,x_t]$ be an online stream up to timestep t, where each $x_i$ can include multiple modalities $x_i^m$ including visual frames $x_i^v$\nor auditory signals $x_i^a$. We transform each $x_m$ into a representation $z_{im}$ via off-the-shelf feature extractors, concatenating them into $z_i$. Next, we define a temporal window $Z_{t-L+1,t} = [z_{t-L+1},...,z_t]$ of length L. Given an anticipation length a, the model performs a three-way classification (background / target speaker speaking / other speaking) for the future range $t+1$ to $t+a$. This anticipatory modeling gives the system extra time to prepare responses, rather than reacting only after a silence threshold. The final model output is a probability tensor of shape $[a, 3]$, where the dimensions correspond to the anticipated future timesteps and the three classes, respectively.\nPrediction vs Detection. A naive approach for determining when to speak is detection which occurs based on silence threshold. However, detection offers an inadequate response time of only 200ms for listeners. A psycholinguistic study (Levinson and Torreira, 2015) estimates that actual response time ranges from 600 to 1500ms, as humans begin preparing their responses while the other person is still speaking. Additionally, turn-shifts often occur as overlapping without any gaps (Skantze, 2021). The prediction will give conversational systems more time to generate reactions and enable human-like conversation.\nFrame-level Speech Labeling.  illustrates how transcript timestamps convert into per-frame, one-hot encoded labels. As our framework requires per-frame speech labels which are expensive to annotate, we developed a method to convert transcript annotations from egocentric videos into per-frame speech classification labels. At each timestep t, we label the datapoint $x_t$ as target speaker speaking if the camera wearer is speaking, other person speaking if others are speaking, and no speech otherwise."}, {"title": "YT-Conversation: Dataset for Multimodal Conversation Pretraining", "content": "Existing turn-taking resources often stem from controlled laboratory setups or video calls, which are expensive to annotate and capture only a fraction of the complexity found in real-world interactions, limiting scalability. To address this gap, we introduce YT-Conversation, a novel dataset derived from diverse YouTube content including interviews, podcasts, and casual dialogues.\nWhile YT-Conversation is not fully egocentric, it offers realistic face-to-face and multi-person interactions that can effectively transfer to first-person scenarios in egocentric video understanding (Zhang et al., 2022; Lin et al., 2022). By leveraging content from real-world YouTube videos through an automatic pipeline, YT-Conversation aims to provide a more scalable resource for turn-taking pretraining.\nCollecting Conversational Videos. We curated our dataset from four manually selected YouTube channels, covering diverse conversational formats including podcasts, interviews, and face-to-face dialogues. Videos were randomly sampled without further filtering, ensuring scalability. We preprocessed the videos by downsampling to 20 FPS for video and 16 kHz for audio, and trimmed opening segments. Our final dataset comprises 414 videos totaling 41 hours, with durations ranging from 1 to 60 minutes.\nPseudo Per-frame Annotation for Collected Videos Since manual annotation of each video frame is labor-intensive, we employ voice activity detection (VAD) from Pyannote (Plaquet and Bredin, 2023; Bredin, 2023) to generate pseudo-labels for speech activity. Specifically, we remove any speech segments under 200 ms (or non-speech gaps under 200 ms) to match our 200 ms resolution. This approach yields a speech vs. no-speech label per frame, effectively approximating the ground truth for large-scale pretraining.  shows a distribution of video durations and illustrates the diversity of conversation styles in YT-Conversation. For the validation of pseudo-annotation quality, see Appendix E."}, {"title": "Experimental Setup", "content": "4.1 Dataset\nWe propose to use publicly available egocentric conversational video datasets for evaluation: EasyCom (Donley et al., 2021) and Ego4D (Grauman et al., 2022).\nEasyCom. The EasyCom dataset contains egocentric videos of 3-5 participants conversing around a table in a room for about 30 minutes per session. It comprises 12 sessions totaling approximately 5 hours and 18 minutes. We use sessions 1-3 for testing and 4-12 for training. The dataset features human-annotated transcripts with precise timestamps and mono-channel audio.\nEgo4D. We use the Audio-Visual Diarization benchmark from Ego4D (Grauman et al., 2022), a large-scale, in-the-wild egocentric video dataset. This subset contains 5-minute clips from diverse scenarios, including both indoor and outdoor settings. However, the original test split is mostly limited to indoor settings. To ensure robust evaluation, we randomly split the combined original train and test sets into 346 training clips and 87 test clips.\nThe EasyCom and Ego4D datasets offer complementary scenarios for evaluating our framework. EasyCom provides a controlled setting with continuous conversations among fixed participants, while Ego4D presents diverse, real-world scenarios with varying numbers of speakers, environments, and intermittent speech patterns. This combination allows us to assess EgoSpeak's performance in both relatively structured and unstructured environments, testing its ability to predict utterance initiation across a range of conversational dynamics.\n4.2 Baselines & Models\nWe evaluate our framework using three trained models with different architectural backbones: RNN (An et al., 2023), Transformer (Xu et al., 2021), and State-Space-Model (Gu and Dao, 2023). Additionally, we implement two static baselines: a random baseline and a rule-based algorithm using silence as decision threshold (Bell et al., 2001). Detailed implementation details including architectural specifications, hyperparameters, training objective and feature extraction for the neural models are provided in Appendix A.\nRandom Baseline. This baseline randomly assigns one of the three possible labels (background, target speaker speaking, or other speaking) to each frame with uniform probability.\nSilence-based Algorithm. Simulating commercial spoken dialogue agents, this approach triggers speech only after a 600ms silence interval following other speakers. Our evaluation likely overestimates its real-world performance, since we use ground-truth labels to detect non-target speech and count the entire subsequent speech segment as correct once the start is identified (only a single timestep is penalized when incorrect).\nTransformer-based Model. We adopt Long Short-term TRansformer (LSTR) (Xu et al., 2021) for temporal modeling. LSTR uses long-term and short-term memory mechanisms to handle sequence data, with an encoder-decoder structure. The encoder leverages long context windows by compressing inputs, while the decoder processes shorter context windows, allowing for flexible temporal modeling.\nRNN-based Model. Inspired by An et al. (2023), we used a simple and effective RNN model containing one GRU layer. This model was chosen for its computational efficiency and strong performance.\nMamba-based Model. We implement a Mamba-based model (Gu and Dao, 2023) similar to the RNN architecture. Given the recent success of Mamba across various tasks, we include this model to explore its potential to predict speech initiation in egocentric videos while maintaining computational efficiency.\n4.3 Settings\nFeature Extraction. We process features at 5 FPS, predicting every 0.2 seconds to align with typical human response times (Skantze, 2021). For RGB features, we use a ResNet-50 (He et al., 2016) model pretrained on Kinetics-400 (Kay et al., 2017). Audio features are extracted using wav2vec2 (Baevski et al., 2020). These features are concatenated to create our multimodal input. Further details on feature extraction are provided in Appendix A.3.\nEvaluation Protocol. Most existing turn-taking evaluations rely on offline F1-scores after processing the entire clip (Lee et al., 2023; Kurata et al., 2023), or on sample-based F1-scores around turn-taking events using threshold-based detection (Ekstedt and Skantze, 2022; Onishi et al., 2023). However, both approaches fail to capture the continuous, overlapping nature of real-world conversations, where a decision must be made at every frame. As Heldner and Edlund (2010) suggested, overlaps occur frequently in human conversation. Consequently, we measure performance per frame to better reflect these natural conversational dynamics.\nTo address this need, we propose using per-frame mean average precision (mAP), inspired by prior work on online tasks (De Geest et al., 2016). This metric evaluates how well the model anticipates the target speaker's speech up to 10 timesteps (2 s) into the future. We compute mAP by 1) sorting all frame-level confidence scores in descending order, 2) iteratively using each score as a threshold, 3) calculating precision and recall at each threshold, and 4) averaging all precision values. This procedure is repeated for each class and timestep, then averaged to yield the final mAP."}, {"title": "Results and Analysis", "content": "5.1 Quantitative Results\n and  present comprehensive comparisons of our models across different modalities, datasets, and baselines. To ensure the robustness of our findings, we report performance over five random seeds with error bars in Appendix D.\nModel Performance Across Modalities As shown in , the multimodal (A+V) approach generally outperforms unimodal inputs on EasyCom, with the Transformer model achieving 58.7% mAP (compared to 56.9% for audio-only and 51.0% for visual-only). The GRU model performs best with A+V (60.6% mAP), while Mamba sees moderate improvements (57.4% mAP). On Ego4D, the Transformer with A+V attains 69.0% mAP, which is roughly on par with its audio-only counterpart. Interestingly, GRU and Mamba actually do better with audio alone, at 69.0% and 67.9% mAP respectively.\nPretraining Effects Our YT-Conversation pretraining results show that overall gains are modest. However, we do observe a small but consistent improvement in detecting other person speaking class, which is especially valuable in egocentric scenarios. In contrast, GRU and Mamba show little or no net gain, aligning with prior work that certain recurrent/state-space models often struggle with large-scale pretraining (Wang et al., 2023). We attribute these results to domain mismatch and the inherently noisier nature of real-world conversational videos. For a per-class breakdown and further discussion, refer to Appendix C.\nComparison with Baselines  compares our best Transformer (A+V) model with both baselines on EasyCom and Ego4D. Even though the silence-based approach benefits from an evaluation bias, our predictive model still achieves significantly higher AP (52.7% vs. 26.6% on EasyCom, and 66.8% vs. 27.7% on Ego4D). Moreover, the silence-based method performs similarly to random, indicating that requiring a fixed silence interval fails to accommodate the fluid, overlapping speech found in real-world conversations.\n5.2 Motion Inputs Contribute to Turn-Taking Prediction\nSince many non-verbal cues involve motion, we hypothesized that incorporating optical flow could improve utterance initiation prediction. To test this, we extracted optical flow using the Denseflow toolkit (Wang et al., 2020) with the TV-L1 algorithm (Zach et al., 2007), following a similar process to our RGB feature extraction. Optical flow is a computer vision technique that estimates object motion between consecutive video frames by calculating the apparent motion of brightness patterns. This is useful for tracking movement and analyzing dynamic scenes.\npresents the results of our experiment on EasyCom. Incorporating optical flow consistently improved performance across all model types and input combinations. These results suggest that motion information provides valuable cues for predicting utterance initiation, complementing static visual and audio features to enable more accurate predictions of speech onset.\n5.3 Models Do Not Exploit Short-Term Information Well\nOur framework uses online processing models that rely on context length to capture historical information. Because dialogue context is crucial in turn-taking (Skantze, 2021), the choice of context can strongly affect utterance initiation.  shows how the Transformer model's performance varies with different long-term and short-term window sizes. While extending the long-term window helps, increasing the short-term window unexpectedly degrades performance. This suggests that although a broader context provides valuable cues, an overly large short-term window may introduce noise or irrelevant data, reducing accuracy. These findings highlight the importance of balancing long-term and short-term context in untrimmed videos to optimize turn-taking predictions.\n5.4 Runtime Analysis\nWe evaluated the computational efficiency of our models by measuring their frames per second (FPS), parameter counts, and floating-point operations (GFLOPs) on a single RTX3090 GPU using the EasyCom dataset, as shown in ."}, {"title": "Qualitative Results", "content": " shows the qualitative results based on Transformer. Our observations indicate that the model using only RGB features struggles to effectively distinguish between speaking and non-speaking segments, leading to frequent misclassifications. In contrast, the model utilizing audio input shows notable improvement in predicting the target speaker's speech. However, the audio-only model often assigns high probabilities to the speech of other individuals, resulting in less accurate turn-taking. Notably, the model that integrates both audio and visual inputs demonstrates superior performance. This multimodal model effectively distinguishes the target speaker from others, accurately identifying speaking segments while minimizing false positives from other speakers."}, {"title": "Conclusion", "content": "We introduced EgoSpeak, a novel framework for real-time speech initiation prediction from an in-the-wild, first-person viewpoint. EgoSpeak integrates four key capabilities to better handle complex, dynamic real-world conversations. We also presented YT-Conversation, a large-scale dataset of in-the-wild YouTube videos for pretraining.\nEmpirical results on two egocentric conversational video datasets demonstrate that EgoSpeak significantly outperforms random and silence-based baselines in real time. Notably, our analysis revealed that incorporating optical flow significantly improves performance and highlighted a counterintuitive finding on context length. We encourage further exploration of large-scale conversational datasets, improved multimodal modeling techniques, and integration with large language models to generate responses, advancing turn-taking in real-world settings."}, {"title": "Limitations", "content": "Our proposed method relies on pre-encoded features, which can limit both performance and frames per second (FPS). A potential solution is to adopt an end-to-end framework that learns to extract relevant features directly from raw input. For example, E2E-LOAD (Cao et al., 2023) demonstrates improvements in both state-of-the-art performance and FPS in online action recognition task through such an approach.\nWhile our YT-Conversation dataset provides diverse pretraining data, it may not fully capture the nuances of first-person interactions. Future work could explore methods to augment the dataset with more egocentric conversational data, potentially improving model performance in first-person scenarios.\nLastly, our current approach does not explicitly model speaker-specific behaviors. Future research could incorporate individual speaking patterns and tendencies, by analyzing larger egocentric conversational datasets. By capturing these speaker-specific nuances, future models may better anticipate utterance initiation points, particularly in prolonged conversations with familiar participants."}, {"title": "Ethical Statement", "content": "Data Collection and Privacy Considerations\nAlthough the YT-Conversation, derived from publicly shared YouTube videos, provides natural conversations for training AI models, there is a possibility it may capture the facial features of the participants. However, the YT-Conversation dataset was collected under the principles of informed consent and data anonymization, adhering to the ACM Code of Ethics 1.6 (Respect privacy).\nInformed consent We selected videos that participants are likely aware of and have consented to be recorded and publicly shared, such as podcasts, interviews, and face-to-face conversations.\nData Anonymization We only released the YouTube IDs rather than the raw YouTube videos so that content creators can remove their videos from YouTube anytime, which will automatically exclude them from our dataset. Moreover, our transcripts only include the time ranges for the start and end of the speech, along with the corresponding video frames without personal information."}, {"title": "Use of AI Assistants", "content": "We used Claude 3.5 Sonnet to revise the paper and code, and GitHub Copilot to write the code."}, {"title": "Implementation Details", "content": "A.1 Architecture & Hyperparameters\nThis section provides detailed information on the architectures and hyperparameters used for each model in our experiments. We set the anticipation length to 10 timesteps for all models, predicting up to 2 seconds into the future. All experiments were done with a single RTX3090 GPU within one day.\nTransformer-based Model (LSTR) For the transformer model, we configured 16 attention heads and 1024-dimensional hidden units in the transformer blocks. The LSTR encoder processes long context windows up to 2048 frames, while the decoder handles shorter context windows up to 32 frames. We trained this model using the Adam optimizer (Kingma and Ba, 2014) with a weight decay of $5 \\times 10^{-5}$. The learning rate was scheduled to increase linearly from zero to $7 \\times 10^{-5}$ during the first 40% of training iterations, then decrease to zero following a cosine function. We trained the transformer model for 50 epochs with a batch size of 16.\nRNN-based Model For the RNN model, we used 2048-dimensional embeddings and 1024-dimensional hidden units. This model was trained for 30 epochs with a batch size of 64. We used the same optimizer and learning rate schedule as the transformer model.\nMamba-based Model The Mamba-based model builds upon the RNN architecture, replacing the GRU layer with a Mamba block. We set the SSM state factor to 16, local convolution width to 4, and block expansion factor to 2. The training settings were kept consistent with the RNN model.\nA.2 Training Objective\nFor training, we use cross-entropy loss between predicted confidence scores $s_t$ at time T and the ground-truth label $y_T \\in \\{0,1,..., K\\}$. $K$ is the number of classes and $s_t^k$ is the k-th element of the probability vector $s_t$. For Transformer-based model, $\\alpha_T$ is always 1. For RNN-based and Mamba-based models, $\\alpha_T$ is used to modulate the contribution of intermediate time steps during the computation of the loss. Specifically, $\\alpha_T$ takes the value 1 only at a designated step $t = L$ and 0 otherwise.\nWe also define a temporal window of length L, which determines the final step contributing to the objective function:\n$J(y_T, s_T; T) = \\sum_{k=0}^{K} \\alpha_T \\delta(k - y_T) \\log s_t^k$,\nA.3 Feature Extraction\nRGB Features As mentioned in Section 4.3, videos are downsampled to 20 FPS and processed in 4-frame chunks, resulting in a 5 FPS prediction rate. We use ResNet-50 (He et al., 2016) initialized with weights from a video action recognition model (Wang et al., 2016), implemented via MMAction2 (Contributors, 2020). The center frame of each chunk is sampled for feature extraction. For the EasyCom dataset, we cropped all clips in each session to remain only video frames and merged them to make one video per session.\nAudio Features We use wav2vec2's (Baevski et al., 2020) multi-layer convolutional feature encoder, as noted in Section 4.3. Every 10 encoded audio features are concatenated temporally to match the 5 FPS RGB features.\nB Importance of Recent Frames\n shows the distribution of attention weights across the encoder layers of a transformer model in the context of predicting utterance initiation in real-world conversations (Wang et al., 2021). The attention weights of the test set were averaged with respect to the layers, multi-heads, and batch and then normalized. These weights reveal the significance assigned to each frame in the sequence during prediction. Our analysis shows that the model focuses predominantly on the recent frames, with attention weights diminishing notably as the distance from the current frame increases. This pattern indicates that recent frames have a greater impact on the model's predictions for utterance initiation.\nC Error Analysis\nPretraining on YT-Conversation We observed that YT-Conversation pretraining yields modest overall gains, but a notable improvement for other person speaking class (+0.7% on EasyCom, +1.5% on Ego4D). lists the per-class average precision (AP) for the Transformer model with and without pretraining. Although this benefit can be crucial in egocentric scenarios\u2014where identifying others' speech fosters smoother turn-taking\u2014gains for Background and Target Speaker remain unchanged or slightly negative. We attribute this to domain mismatch (YouTube interviews vs. dynamic ego footage) and noisy data from real-world conversational videos such as visual effects or subtitles. Future work might address these limitations by bridging domain gaps\u2014e.g., with domain adaptation\u2014or introducing video filtering to obtain higher-quality conversational clips.\nBackchannels While our method aims to predict any utterance initiation point, there is a short and brief response that occurs when one participant is speaking and the listener reacts to signify the listener's attention, understanding, or emotion rather than take turns and speak. This behavior is referred to as \"backchannels\u201d (Yngve, 1970; Skantze, 2021). We observed that prediction scores usually do not increase before backchanneling.  illustrates this phenomenon, showing how the model's prediction scores do not significantly increase before a backchanneling event, in contrast to regular speaking turns.\nD Descriptive Statistics of Experimental Results\nWe evaluated each model with five random seeds \\{0, 10, 20, 29, 42\\} to measure performance variance.  shows the multi-seed mean average precision (mAP) on EasyCom and Ego4D, while Tables 8 and 9 provide per-timestep results (mean \u00b1 standard error). These tables complement the main text figures ( and 3) by offering a full breakdown of multi-seed performance at each time step, ensuring transparency and robustness in our results.\nE YT-Conversation Pseudo Annotation Quality Validation\nTo validate the quality of pseudo-annotations in our YT-Conversation dataset, we conducted a human evaluation study on 100 segments randomly sampled from 10 videos, excluding the first five segments of each (typically non-conversational teasers). Each segment received a label alignment score on a 5-point scale: (1) completely misaligned, with timestamps far off from actual speech; (2) poor alignment, missing large portions, or labeling silence as speech; (3) adequate but potentially off by 0.5-1 second; (4) good alignment, within about 0.5 second of true boundaries; and (5) excellent alignment, nearly matching human labels.\nAcross all evaluated segments, the average alignment score was 2.147. We want to note that as ASR models continue to advance (Zusag et al., 2024), the pseudo-label will be precise as well. We also use these pseudo-labels only for pretraining, ensuring the evaluations remain robust with human-annotated labels."}]}