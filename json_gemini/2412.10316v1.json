{"title": "BrushEdit: All-In-One Image Inpainting and Editing", "authors": ["Yaowei Li", "Yuxuan Bian", "Xuan Ju", "Zhaoyang Zhang", "Junhao Zhuang", "Ying Shan", "Qiang Xu"], "abstract": "BrushEdit is a cutting-edge interactive image editing framework that combines language models and inpainting techniques for seamless edits. Leveraging pre-trained multimodal language models and BrushNet's dual-branch architecture, users can achieve diverse edits such as adding objects, removing elements, or making structural changes with free-form masks.\n\nAbstract\u2014Image editing has advanced significantly with the development of diffusion models using both inversion-based and instruction-based methods. However, current inversion-based approaches struggle with big modifications (e.g., adding or removing objects) due to the structured nature of inversion noise, which hinders substantial changes. Meanwhile, instruction-based methods often constrain users to black-box operations, limiting direct interaction for specifying editing regions and intensity. To address these limitations, we propose BrushEdit, a novel inpainting-based instruction-guided image editing paradigm, which leverages multimodal large language models (MLLMs) and image inpainting models to enable autonomous, user-friendly, and interactive free-form instruction editing. Specifically, we devise a system enabling free-form instruction editing by integrating MLLMs and a dual-branch image inpainting model in an agent-cooperative framework to perform editing category classification, main object identification, mask acquisition, and editing area inpainting. Extensive experiments show that our framework effectively combines MLLMs and inpainting models, achieving superior performance across seven metrics including mask region preservation and editing effect coherence.", "sections": [{"title": "I. INTRODUCTION", "content": "The rapid advancement of diffusion models has significantly propelled text-guided image generation [1]\u2013[4], delivering exceptional quality [5], diversity [6], and alignment with textual guidance [7]. However, in image editing tasks where a target image is generated based on a source image and editing instructions\u2014such progress remains limited due to the difficulty of collecting large amounts of paired data.\n\nTo perform image editing based on diffusion generation models, previous methods primarily focus on two strategies: (1) Inversion-based Editing: This approach leverages the structural information of noised latent derived from inversion to preserve content in non-edited regions, while manipulating the latent in edited regions to achieve the desired modi-"}, {"title": "II. RELATED WORK", "content": "Image editing involves modifying object shapes, colors, poses, materials, and adding or removing objects [34]. Recent advancements in diffusion models [1], [2] have notably improved visual generation tasks, outperforming GAN-based models [35]\u2013[37] in image editing. To enable controlled and guided editing, various methods leverage modalities like text instructions [6], [13], [14], masks [15], [23], [38], layouts [8], [9], [39], segmentation maps [40], [41], and point-dragging interfaces [42], [43]. However, these methods often struggle with large structural edits due to noisy latent inversion's overwhelming structural information or rely on scarce high-quality \"source image-target image-editing instruction\" pairs. Additionally, they usually require users to operate in a black-box manner, demanding precise inputs like masks, text, or layouts, limiting their usability for content creators. These challenges impede the development of a free-form, interactive natural language editing system.\n\nMany Multi-modal Large Language Model (MLLM)-based methods leverage advanced vision and language understanding capabilities for image editing [15]\u2013[17], [25], [44]. MGIE refines instruction-based editing by generating more detailed and expressive prompts. SmartEdit enhances the comprehension and reasoning of complex instructions. FlexEdit integrates MLLMS to process image content, masks, and textual inputs. GenArtist employs an MLLM agent to decompose complex tasks, guide tool selection, and systematically execute image editing, generation, and self-correction with iterative verification. However, these methods often involve costly MLLM fine-tuning, are limited to single-turn black-box editing, or face both challenges.\n\nThe recent MagicQuill [17] enables fine-grained control over shape and color at the regional level using scribbles and colors, leveraging a fine-tuned MLLM to infer editing options from user input. While it provides precise interactive control, it requires labor-intensive strokes to define regions and incurs significant training costs to fine-tune MLLMs. In contrast, our method relies solely on natural language instructions (e.g., \"remove the rose from the dog's mouth\" or \"convert the dumplings on the plate to sushi\") and integrates MLLMs, detection models, and our dual-branch inpainting mode in a training-free, agent-cooperative framework. And our framework also supports multi-round refinement, users can iteratively adjust the generated editing mask and target image caption to achieve multi-round interaction. As summarized in Tab. I, our BrushEdit overcomes the limitations of current editing methods through an instruction-based, multi-turn interactive, and plug-and-play design, enabling flexible preservation of unmasked regions and establishing itself as a versatile editing solution."}, {"title": "B. Image Inpainting", "content": "Image inpainting remains a key challenge in computer vision, focusing on reconstructing masked regions with realistic and coherent content [45], [46]. Traditional methods [47], [48] and early Variational Auto-Encoder (VAE) [49], [50] or Generative Adversarial Network (GAN) [35]\u2013[37] approaches"}, {"title": "III. PRELIMINARIES AND MOTIVATION", "content": "In this section, we will first introduce diffusion models in Sec. III-A. Then, Sec. III-B would review previous in-painting techniques based on sampling strategy modification and special training. Finally, the motivation is outlined in Section III-D."}, {"title": "A. Diffusion Models", "content": "Diffusion models include a forward process that adds Gaussian noise e to convert clean sample zo to noise sample zy, and a backward process that iteratively performs denoising from zy to z0, where \u0454 ~ N (0,1), and T represents the total number of timesteps. The forward process can be formulated as follows:\n\n$z_t = \\sqrt{\\alpha_t}z_0 + \\sqrt{1 - \\alpha_t}\\epsilon$\n\nzt is the noised feature at step t with t ~ [1, T], and \u03b1 is a hyper-parameter.\n\nIn the backward process, given input noise z\u012b sampled from a random Gaussian distribution, learnable network e estimates noise at each step t conditioned on C. After T progressively refining iterations, zo is derived as the output sample:\n\n$z_{t-1} = \\frac{\\sqrt{\\alpha_{t-1}}}{\\sqrt{\\alpha_t}}z_t + \\sqrt{\\alpha_{t-1}}\\left(\\frac{1}{\\alpha_{t-1}}-1\\right)\\frac{1}{\\sqrt{1-\\alpha_t}} \\epsilon_{\\theta} (z_t, t, C)$      \nThe training of diffusion models revolves around optimizing the denoiser network e\u03b8 to conduct denoising with condition C, guided by the objective:\n\n$minE_{z_0,\\epsilon \\sim N(0,I),t \\sim U(1,T)} ||\\epsilon - \\epsilon_{\\theta} (z_t, t, C') ||$"}, {"title": "B. Image Inpainting Models", "content": "Previous image inpainting approaches can be broadly categorized into Sampling Strategy Modification and Dedicated Inpainting Models.\n\nSampling Strategy Modification. These methods perform inpainting by iteratively blending masked images with generated content. A representative example is Blended Latent Diffusion (BLD) [27], the default inpainting technique in popular diffusion-based libraries (e.g., Diffusers [58]). Given a binary mask m and a masked image xmasked, BLD extracts the latent representation zmasked using a VAE. The mask m is resized to mresized to match the latent dimensions. During inpainting, Gaussian noise is added to zmasked over T steps, producing zmasked, where t ~ [1,T]. The denoising starts from zmasked, with each sampling step (eq. 2) followed by:\n\n$z_{t-1} \\leftarrow z_{t-1} \\cdot (1 \u2013 m_{resized}) + z_{masked} \\cdot m_{resized}$      \nDespite its simplicity, Sampling Strategy Modification often struggles to preserve unmasked regions and align generated content. These shortcomings stem from: (1) inaccuracies introduced by resizing the mask, which hinder proper blending of noisy latents, and (2) the diffusion model's limited contextual understanding of mask boundaries and unmasked regions.\n\nDedicated Inpainting Models. To enhance performance, these methods fine-tune base models by adding the mask and masked image as additional UNet input channels, creating architectures specialized for inpainting. While they surpass BLD in generation quality, they face several challenges: (1) They merge noisy latents, masked image latents, and masks at the UNet's initial convolution layer, where text embeddings globally affect all features, making it difficult for deeper layers to focus on masked image details. (2) Simultaneously handling conditional inputs and generation tasks increases the UNet's computational load. (3) Extensive fine-tuning is required for different diffusion backbones, leading to high computational costs and limited adaptability to custom diffusion models."}, {"title": "C. Image Editing Models", "content": "Recent image editing methods can fall into two types:\na) Inversion Methods: These approaches [8]\u2013[10], [26], [62], [63] achieve editing by manipulating the latents obtained through inversion. First, they generate edit-friendly noisy latents using various inversion techniques, followed by three paradigms for preserving background regions while modifying target areas: (1) Attention Integration: They [8], [9], [42], [64]\u2013[67] fuse attention maps linking text and image between the source and editing diffusion branches. (2) Target Embedding: They [62], [63], [68]\u2013[74] manage to embed the editing information from the target branch and integrate it into the source diffusion branch. (3) Latent Integration: These methods [10], [26], [27], [42], [75]\u2013[77] try to directly inject editing instructions via noisy latent features from the target diffusion branch into the source diffusion branch. Although these methods are computationally efficient and achieve competitive zero-shot or few-shot performance, they are often limited in the diversity of supported edits (e.g., typically restricted to object interaction or attribute modification) due to simplistic generation controls. Additionally, the structural prominence in inversion latents often leads to failures when handling significant structural changes, such as object addition/removal or background replacement.\n\nb) End-to-end Methods: These methods [13], [78]\u2013[80] train end-to-end diffusion models for image editing, leveraging various ground-truth or pseudo-paired editing datasets. They support a broader range of edits and avoid the significant speed drawbacks of inversion methods, completing edits in a single forward pass. However, their performance is often constrained by the limited availability of ground-truth editing pairs, necessitating pseudo-pair generation via inversion methods, which hinders their upper-bound performance. Furthermore, these end-to-end models lack support for interactive, multi-round editing, preventing content creators from iterative refining or enhancing edits, thus reducing their practicality."}, {"title": "D. Motivation", "content": "Based on the analysis in Section III-B, a more effective inpainting architecture could incorporate an additional branch dedicated to processing masked images, enabling the backbone to recognize mask boundaries and the corresponding background without requiring modifications or retraining. Similarly, as discussed in Section III-C, there is an urgent need for a free-form, interactive natural language instruction editing model. Leveraging the exceptional multimodal understanding of MLLMs, such a model can efficiently identify the editing type, target objects, and regions to edit, as well as generate annotations for the desired output. With the support of image inpainting models, precise edits within the target masked regions can then be achieved. Moreover, this process can be iteratively refined, allowing users to create transparently and iteratively."}, {"title": "IV. METHOD", "content": "An overview of BrushEdit is shown in Fig. 2. Our framework integrates MLLMs with a dual-branch image inpainting model via agent collaboration, enabling free-form, multi-turn interactive instruction editing. Specifically, a pre-trained MLLM, acting as the Editing Instructor, interprets user instructions to identify editing types, locate target objects, retrieve detection results for the editing region, and generate textual descriptions of the edited image. Guided by this information, the inpainting model, serving as the Editing Conductor, fills the masked region based on the target text caption. This iterative process allows users to modify or refine intermediate control inputs at any stage, supporting flexible and interactive instruction-based editing."}, {"title": "A. Editing Instructor", "content": "In BrushEdit, we use an MLLM as an editing instructor to interpret users' free-form editing instructions, categorize them into predefined types (addition, removal, local edit, background edit), identify target objects, and utilize a pre-trained detection model to find the relevant editing mask. Finally,"}, {"title": "B. Editing Conductor", "content": "Our Editing Conductor, built on our previous BrushNet, employs a mixed fine-tuning strategy using both random and segmentation masks. This approach enables the inpainting model to handle diverse mask-based inpainting tasks without being restricted by mask types, achieving comparable or superior performance. Specifically, we inject masked image features into a pre-trained diffusion network (e.g., Stable Diffusion 1.5) through an additional control branch. These features include the noisy latent for enhancing semantic coherence by providing information on the current generation process, the masked image latent extracted via VAE to guide semantic consistency between the prompt foreground and the ground truth background, and the mask downsampled via cubic interpolation to explicitly indicate the position and boundaries of the foreground filling region.\n\nTo retain masked image features, BrushEdit employs a duplicate of the pre-trained diffusion model with all attention layers removed. The pre-trained convolutional weights serve as a robust prior for extracting masked image features, while excluding cross-attention layers ensures the branch focuses solely on pure background information. BrushEdit features are integrated into the frozen diffusion model layer-by-layer, enabling hierarchical, dense per-pixel control. Following ControlNet [33], zero convolution layers are used to link the frozen model with the trainable BrushEdit, mitigating noise during early training stages. The feature insertion operation is defined in Eq. 5:\n\n$\\epsilon_0 (z_t, t, C); = \\epsilon_{\\theta} (z_t, t, C)_i  + w \\cdot Z (\\epsilon_{BrushNet} ([z_t, z_{masked}, m_{resized}], t))$      \nwhere \u0454o (zt, t, C'); represents the feature of the i-th layer in the network e\u0473, where i \u2208 [1,n], and n denotes the total number of layers. The same notation is applied to eBrushNet. The network eBrushNet processes the concatenated noisy latent zt, masked image latent zmasked, and downsampled mask mresized, where concatenation is represented by [\u00b7]. Z refers to the zero convolution operation, and w is the preservation scale that adjusts the influence of BrushEdit on the pretrained diffusion model.\n\nPrevious studies have highlighted that downsampling during latent blending can introduce inaccuracies, and the VAE encoding-decoding process has inherent limitations that impair full image reconstruction. To ensure consistent reconstruction of unmasked regions, prior methods have explored various strategies. Some approaches [29], [31] rely on copy-and-paste techniques to directly transfer unmasked regions, but these often result in outputs lacking semantic coherence. Latent blending methods inspired by BLD [5], [27] also struggle to retain desired information in unmasked areas effectively. In this work, we propose a simple pixel-space approach that applies mask blurring before copy-and-paste using the blurred mask. Although this may slightly affect accuracy near the mask boundary, the error is nearly imperceptible and significantly improves boundary coherence.\n\nThe architecture of BrushEdit is inherently designed for seamless plug-and-play integration with various pretrained diffusion models, enabling flexible preservation control. Specifically, the flexible capabilities of BrushEdit include: (1) Plug-and-Play Integration: As BrushEdit does not modify the pretrained diffusion model's weights, it can be effortlessly integrated with any community fine-tuned models, facilitating easy adoption and experimentation. (2) Preservation Scale Adjustment: The preservation scale of the unmasked region can be controlled by incorporating BrushEdit features into the frozen diffusion model with a weight w, which adjusts the influence of BrushEdit on the level of preservation. (3) Blurring and Blending Customization: The preservation scale can be further refined by adjusting the blurring scale and applying blending operations as needed. These features provide fine-grained and flexible control over the editing process."}, {"title": "V. EXPERIMENTS", "content": "To comprehensively evaluate the performance of BrushEdit, we conducted experiments on both image editing and image inpainting benchmarks:"}, {"title": "A. Evaluation Benchmark and Metrics", "content": "Benchmark: To comprehensively evaluate the performance of BrushEdit, we conducted experiments on both image editing and image inpainting benchmarks:\n\n\u2022 Image Editing. We used PIE-Bench [11] (Prompt-based Image Editing Benchmark) to evaluate BrushEditand all baselines on image editing tasks. PIE-Bench consists of 700 images spanning 10 editing types, evenly distributed between natural and artificial scenes (e.g., paintings) across four categories: animal, human, indoor, and outdoor. Each image includes five annotations: source image prompt, target image prompt, editing instruction, main editing body, and editing mask.\n\u2022 Image Inpainting. Extending our prior conference work, we replaced traditional benchmarks [81]\u2013[86] with BrushBenchfor segmentation-based masks and EditBench for random brush masks. These benchmarks span real and generated images across human bodies, animals, and indoor and outdoor scenes. EditBench includes 240 images with an equal mix of natural and generated content, each annotated with a mask and caption. BrushBench, shown in Fig. 3, contains 600 images with human-annotated masks and captions, evenly distributed across natural and artificial scenes (e.g., paintings) and covering various categories such as humans, animals, and indoor/outdoor environments."}, {"title": "B. Implementation Details", "content": "We evaluate various inpainting methods under a consistent setting unless stated otherwise, i.e., using NVIDIA Tesla V100 GPUs and their open-source code with Stable Diffusion v1.5 as the base model, 50 steps, and a guidance scale of 7.5. Each method utilizes its recommended hyper-parameters across all images to ensure fairness. BrushEdit and all ablation models are trained for 430k steps on 8 NVIDIA Tesla V100 GPUs, requiring approximately 3 days. Notably, for all image editing (PnPBench) and image inpainting (BrushBench and EditBench) tasks, BrushEditachieves unified image editing and inpainting using a single model trained on Brush-Data-v2. In contrast, our previous BrushNet required separate training and testing for different mask types. Additional details are available in the provided code."}, {"title": "C. Quantitative Comparison (Image Editing)", "content": "Tab. II and Tab. III compare the quantitative image editing performance on PnPBench [11]. We evaluate the editing results of previous inversion-based methods, including four inversion techniques\u2014DDIM Inversion [2], Null-Text Inversion [95], Negative-Prompt Inversion [96], and StyleDiffusion [97]\u2014as"}, {"title": "D. Qualitative Comparison (Image Editing)", "content": "The qualitative comparison with previous image editing methods is shown in Fig. 4. We present results on both artificial and natural images across various editing tasks, including deleting objects (I), adding objects (II), modifying objects (III), and swapping objects (IV). BrushEdit consistently achieves superior coherence between the edited and unedited regions, excelling in adherence to editing instructions, smoothness at the editing mask boundaries, and overall content consistency. Notably, Fig. 4 I and II involve tasks such as deleting a flower or laptop, and adding a collar or earring.\n\nWhile previous methods failed to deliver satisfactory results due to persistent structural artifacts caused by inversion noise, BrushEdit successfully performs the intended operations and produces seamless edits that blend harmoniously with the background, owing to its dual-branch decoupled inpainting-based editing paradigm."}, {"title": "E. Quantitative Comparison (Image Inpainting)", "content": "Tab. IV and Tab. V present the quantitative comparison on BrushBench and EditBench [32]. We evaluate the inpainting results of the sampling strategy modification method Blended"}, {"title": "F. Qualitative Comparison (Image Inpainting)", "content": "The qualitative comparison with previous image inpainting methods is shown in Fig. 5. We evaluate results on both artificial and natural images across diverse inpainting tasks,"}, {"title": "G. Flexible Control Ability", "content": "Fig. 6 and Fig. 7 demonstrate the flexible control offered by BrushEdit in two key areas: base diffusion model selection and scale adjustment. This flexibility extends beyond inpainting to image editing, as it is achieved by altering the backbone network's generative prior and branch information injection strength. In Fig. 6, we show how BrushEdit can be combined with various community-finetuned diffusion models, enabling users to choose the model that best aligns with their specific editing or inpainting needs. This greatly enhances the practical value of BrushEdit. Fig. 7 illustrates the control over BrushEdit's scale parameter, which allows users to adjust the extent of unmasked region protection during editing or inpainting, offering fine-grained control for precise and customizable results."}, {"title": "H. Ablation Study", "content": "We conducted ablation studies to examine the impact of different model designs on image inpainting tasks. Since BrushEdit is based on an image inpainting model, the editing task is achieved through inference-only by chaining MLLMs, BrushEdit, and an image detection model as agents. The inpainting capability directly reflects our model's training outcome. Tab. VI compares the dual-branch and single-branch designs, while Tab. VII highlights the ablation study on the additional branch architecture.\n\nThe ablation studies, performed on BrushBench, average the performance for both inside-inpainting and outside-inpainting.\n\nThe results in Tab. VI show that the dual-branch design significantly outperforms the single-branch design. Moreover, fine-tuning the base diffusion model in the dual-branch setup yields superior results compared to freezing it. However, fine-tuning may limit flexibility and control over the model. Considering the trade-off between performance and flexibility, we chose to"}, {"title": "VI. DISCUSSION", "content": "This paper introduces a novel Inpainting-based Instruction-guided Image Editing paradigm (IIIE), which combines large language models (LLMs) and plug-and-play, all-in-one image inpainting models to enable autonomous, user-friendly, and interactive free-form instruction editing. Quantitative and qualitative results on PnPBench, our proposed benchmark, BrushBench, and EditBench demonstrate the superior performance of BrushEdit in terms of masked background preservation and image-text alignment in image editing and inpainting tasks.\n\nSome limitations: (1) The quality and content generated by our model heavily depend on the selected base model. (2) Even with BrushEdit, poor generation results still occur when the mask has an irregular shape or when the provided text does not align well with the masked image. In future work, we aim to address these challenges.\n\nNegative Social Impact.: Image inpainting models offer exciting opportunities for content creation but also present potential risks to individuals and society. Their reliance on internet-collected training data may amplify social biases, and there is a specific risk of generating misleading content by manipulating human images with offensive elements. To mitigate these concerns, responsible use and the establishment of ethical guidelines are essential, which will also be a focus in our future model releases."}]}