{"title": "EigenLoRAx: Recycling Adapters to Find Principal Subspaces for Resource-Efficient Adaptation and Inference", "authors": ["Prakhar Kaushik", "Ankit Vaidya", "Shravan Chaudhari", "Alan Yuille"], "abstract": "The rapid growth of large models has raised concerns about their environmental impact and equity in accessibility due to significant computational costs. Low-Rank Adapters (LoRA) offer a lightweight solution for finetuning large models, resulting in an abundance of publicly available adapters tailored to diverse domains. We ask: Can these pretrained adapters be leveraged to further streamline adaptation to new tasks while addressing these challenges? We introduce EigenLoRAx, a parameter-efficient finetuning method that recycles existing adapters to create a principal subspace aligned with their shared domain knowledge which can be further augmented with orthogonal basis vectors in low-resource scenarios. This enables rapid adaptation to new tasks by learning only lightweight coefficients on the principal components of the subspace - eliminating the need to finetune entire adapters. EigenLoRAx requires significantly fewer parameters and memory, improving efficiency for both training and inference. Our method demonstrates strong performance across diverse domains and tasks, offering a scalable for edge-based applications, personalization, and equitable deployment of large models in resource-constrained environments.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in natural language processing (Touvron et al., 2023) and computer vision (Rombach et al., 2021) have driven the rise of large-scale models with billions of parameters. However, the size and complexity of these models not only make it impractical for most researchers to train or fine-tune them on downstream tasks but also contribute significantly to their carbon footprint, raising concerns about environmental sustainability. To address these challenges, there has been growing interest in parameter-efficient finetuning (PEFT) methods, such as adapters (Houlsby et al., 2019; Chen et al., 2022; Luo et al., 2023), low rank adaptation (LoRA) methods (Hu et al., 2021; Kopiczko et al., 2023; Liu et al., 2024), prompt-based methods (Lester et al., 2021; Razdaibiedina et al., 2023; Fischer et al., 2024).\nLORA and its follow-up works (Meng et al., 2024; Liu et al., 2024) have gained significant attention for their simplicity. This has fueled the proliferation of thousands of low-rank adapters within the growing open-source community. Given that these adapters are underutilized, an important question arises: Can we recycle the information contained in them to improve the efficiency of subsequent tasks? Recent work has shown that weight updates in deep neural networks occur within low-dimensional invariant subspaces (Kwon et al., 2024), aligning with the universality hypothesis that neural network behavior and learned representations often reside in shared, structured subspaces (Chughtai et al., 2023; Guth & M\u00e9nard, 2024). This suggests that LoRA adapters may similarly share a principal subspace that can be reused, eliminating the need to rediscover it during the training of new adapters.\nWe introduce EigenLoRAx, a parameter-efficient fine-tuning (PEFT) method that leverages this insight by decomposing the weights of a set of trained adapters into principal components, identifying a compact, information-dense subspace. EigenLoRAx reduces the number of learnable parameters by up to 100\u00d7 compared to LoRA, accelerates optimization by up to 2\u00d7 for new adapters, and enables more memory-efficient inference with multiple task adapters, particularly benefiting edge devices (Liu et al., 2022). Additionally, in low-resource domains, we demonstrate that EigenLoRAx can be further enhanced by augmenting the principal subspace with random components, orthogonalized with respect to the existing subspace, preserving its efficiency while retaining performance.\nFurthermore, we provide an initial theoretical analysis of EigenLoRAx. Our experiments across a wide range of vision and language tasks demonstrate its versatility and effectiveness, reinforcing the potential of shared subspaces in neural network adaptation."}, {"title": "2. Related Works", "content": "Low-Rank Adaptation refers to modeling neural network weight updates as a function of low-rank matrices instead of training the entire weight matrix. This is a well-established line of research starting from Burer-Monteiro factorization (Burer & Monteiro, 2003), with a recent resurgence by Hu et al. (2021) (LoRA), who used it as a technique to finetune LLMs; and other related variants (Ma et al., 2024; Chi et al., 2019; Kwon et al., 2024). However, with rapid growth in the scale of models, Low-Rank Adaptation has also become relatively expensive; for example, LoRA with a rank of 16 on GPT-3 (Brown et al., 2020) requires 75.5 million parameters. Consequently, more efficient low-rank fine-tuning methods are being developed. Mixture of experts models (Huang et al., 2023; Wu et al., 2024; Diao et al., 2023; Zhong et al., 2024; Zhou et al., 2018) have been proposed as a method to adapt to new domains using a mixture of low-rank modules. But these approaches typically require a substantial number of high-quality adapters to work efficiently (Ku et al., 2024), which can significantly increase the model memory requirements (Zhou et al., 2022). Furthermore, complex gating or weighting mechanisms utilized with these models can exhibit training instability (Zoph et al., 2022).\nRecent methods have aimed to learn better subspaces for low-rank optimization, primarily by decomposing model weights into singular vectors for improved training. Meng et al. (2024) demonstrate that initializing LoRA with singular vectors is superior to random initialization, while Sharma et al. (2023) find that removing minor singular components enhances robustness. Using randomly initialized principal components (Kopiczko et al., 2023) or weight matrices (Koohpayegani et al., 2024) has also been explored to reduce the number of trainable parameters. However, as shown in Section 4, random initialized subspaces may not be very useful. This is intuitive as the random subspace may not have an overlap with domain-specific principal subspaces. On the other hand, EigenLoRAx uses trained adapters to extract a principal subspace suitable for a given domain of tasks resulting in a better subspace initialization than and parameter efficiency. Given our focus on resource and computation efficiency in this work, we focus primarily on LORA (Hu et al., 2021) as our main baseline, but EigenLoRAx can be used with any PEFT method like (Liu et al., 2024; Zhang et al., 2023) where task-specific weights can be analyzed together."}, {"title": "3. Method", "content": "In this section, we describe the theoretical motivation and the algorithm of our method, with a discussion on the hyperparameters and quantification of practical benefits. Note that we use the terms EigenLoRA, EigenLoRAx, ELORA and ELORAx interchangeably."}, {"title": "3.1. Theoretical Motivation", "content": "Let $W \\in \\mathbb{R}^{m\\times n}$ be a matrix representing a linear transformation between the vector spaces $\\mathbb{R}^m$ and $\\mathbb{R}^n$. When $W$ is of full rank, meaning $rank(W) = min(m, n)$, it is capable of expressing the entire space of possible linear mappings between these dimensions. However, as the rank of $W$ increases, modifying it to accommodate new tasks becomes computationally expensive and increasingly complex. To mitigate this challenge in fine-tuning large models, LoRA introduces a decomposition of $W$ into two low-rank matrices, $B\\in \\mathbb{R}^{m\\times r}$ and $A \\in \\mathbb{R}^{r\\times n}$, where $r < min(m, n)$. This factorization ensures that the product $BA$ retains the original dimensions of $W$ while having a significantly reduced rank. That is, $BA \\in \\mathbb{R}^{m\\times n}$, with $rank(BA) \\leq r$. As a result, although the transformation defined by $BA$ maps from $\\mathbb{R}^m$ to $\\mathbb{R}^n$, it does not span the full space of such mappings due to its constrained rank. By focusing on learning only the most essential updates within a low-dimensional subspace, LORA provides a parameter-efficient mechanism for model adaptation. The number of parameters in this adaptation framework is given by $mr + r \\cdot n$, which is substantially smaller than the full parameter count $m \\cdot n$, making it a computationally viable alternative for fine-tuning large-scale models.\nMany downstream adapters have been observed to converge toward a shared set of key directions in their parameter space (Meng et al., 2024; Liu et al., 2024), suggesting the existence of a principal subspace underlying diverse tasks. We hypothesize that LoRA adapters leverage such subspaces, enabling task-specific adaptations within a lower-dimensional structure rather than the full weight space. This not only reduces computational overhead but also reinforces the idea that task-relevant transformations reside within a compact, reusable subspace. To formalize this, we first define a space of tasks representable by linear transformation matrices, providing a foundation for analyzing the role of shared principal subspaces in model adaptation.\nDefinition 3.1 (Linear Transformation Tasks). We define a task domain $T_a$ as set of $d$ linear tasks, $T_a = \\{ t | X_t \\in \\mathbb{R}^m \\rightarrow Y_t \\in \\mathbb{R}^n \\}$ where for each task $t \\in T_a$, $W\\in \\mathbb{R}^{m \\times n}$ such that $Y = W*X + E_t \\forall X \\in X_t, Y \\in Y_t$ where $E_t$ denotes the noise in data distribution.\nFor a given set of pretrained weights (such as those from a foundation model) $W_0 \\in \\mathbb{R}^{m\\times n}$, LoRA weights BA at any layer modify the output as $W_0X + BAX + E_t$, allowing the model to adapt to the new task and converge toward the optimal solution $W^*$. The key here is that only B and A weights are updated during finetuning. Without loss of generality, assumer \u226an and let the true transformation matrix $W^* \\in \\mathbb{R}^{r\\times n}$ be interpreted as r n-dimensional vectors: $w_1,..., w^* \\in \\mathbb{R}^{n}$. Finding LoRA weights is equivalent to finding sets of these r vectors in $\\mathbb{R}^n$.\nDefinition 3.2 (Set of LoRA weights). We define the weights of a LoRA adapted for task $t$ as $B_tA_t$. Both $B_t$ and $A_t$ will have their own individual subspaces. For the purpose of the analysis we will consider a generic task specific weight matrix $W_t \\in \\mathbb{R}^{m\\times n}$ adapted to task $t$ such that $n < m$ and its rank $r < n$. The analysis, however, is valid for both $B_t$ and $A_t$. Now can define a set of LoRAs as stacked (along columns) weight matrices $W = \\{W_t\\}$ each adapted for a task $t \\in T_a$ and a training set $S_t = \\{\\{x_i, y_i\\} | x_i \\in X_t, y_i \\in V_t\\}_{i=1}^{s_t}$ where $s_t = |S_t|$ and $X_t \\times V_t$ are distributed according to some unknown Gaussian distribution with mean $X_t$ and $||X_t||_F \\leq M$ for some constant $M > 0$. Each weight matrix can have different ranks and the following method and analysis will still hold, however, for brevity we assume all weight matrices stacked in $W$ to have the same rank $r$.\nDefinition 3.3 (Subspace spanned by LoRAs from a task domain $T_a$). We define the subspace of weights $Z_a = \\{ CW | C\\in \\mathbb{R}^{m\\times m} \\}$ spanned within $\\mathbb{R}^{m\\times n}$.\nUsing Singular Value Decomposition (SVD) or Principal Component Analysis (PCA for a zero-centered $W$) , we can obtain $W = U\\Sigma DV^T$. We then represent top $K$ right singular vectors of $W$ (or top $K$ principal components if $W$ is zero-centered) as $V_K \\in \\mathbb{R}^{K\\times n} = \\{ v_k \\in \\mathbb{R}^{1\\times n} \\}_{k=1}^{K}$.\nDefinition 3.4 (Principal Subspace of LoRAs finetuned in domain $T_a$). We define the principal subspace of weights for a task domain $T_a$ as $Z_K = \\{ aV_K | a\\in \\mathbb{R}^{m\\times K} \\}$ spanned by top $K$ principal components of the LoRAs within $\\mathbb{R}^{m\\times n}$.\nNext, we introduce the idea of defining a new related task $t_{d+1}$\nDefinition 3.5 (New related task $t_{d+1}$). A new linear task $t_{d+1}$ with true solution $W^*$ is said to be related if it is spanned by the basis of $W$ i.e. $W^* \\subset W$ and it holds that $||W^* - a_{d+1}V_K||_F < ||W^* - \\alpha_{d+1}V_K||_F$ for all rank K linear transformation matrices $a_{d+1}$ and $||W^* - a_{d+1}V_K||_F < ||C|| \\sum_{i=K+1}^{nd} \\sigma_i^2$ where $\\sigma_i$'s are singular values of $W$. For such a task, we learn coefficients of K principal components $a_{d+1} \\in \\mathbb{R}^{m\\times K}$ resulting in EigenLoRAx weights $W_E = \\alpha_{d+1}V_K$.\nDefinition 3.5 establishes a bound over the related-ness of a new task with those in the known task domain $T_a$. If the true solution of the new task lies majorly in the principal subspace of $T_a$ i.e. has major principal components (PCs) within the top K principal components of $W$ with some finite bound on the misalignment along the PCs orthogonal to the top K PCs of $W$, then we can ideally quantify the relation between a new task and a task domain. Any task that has its true solution within a subspace defined by the PCs orthogonal to the top K PCs of $W$ is not as closely related as a task with its solution completely or majorly within the principal subspace. A task that has its solution completely orthogonal to all the PCs of $W$ is completely unrelated and is not the main focus of this study."}, {"title": "3.2. Algorithm", "content": "Assume we have N LORA adapters, each consisting of a set of A, B matrix pairs for every layer, trained on various tasks within a domain $T_a$ for a given base pretrained model. Algorithm 1 computes a list of top K principal components-referred to as EigenLoRAx PCs-that define an initial principal subspace for this domain. To construct this subspace, the algorithm aggregates LoRA matrices across tasks for each layer, separately for A and B matrices (though it can also be applied to the product BA). Each LoRA matrix, having rank r, is treated as a list of vectors, and a decomposition is performed on this stacked set of vectors. The most significant components extracted from this process serve as a basis for the principal subspace, providing an efficient representation that can be linearly combined to approximate the original LoRA weight matrices. We illustrate our algorithm using generic weight matrices $W_t$, where each $W_t$ represents a single A or B matrix from a single layer of the neural network. In practice, this procedure is applied to all relevant layers.\nSince real-world scenarios often involve low-resource domains with limited availability of LoRA adapters, we extend our subspace by introducing additional pseudo-PCs. Specifically, we sample random vectors of the same dimension as each PC and orthogonalize them with respect to all existing PCs. This process can be iterated to generate more pseudo-PCs, thereby augmenting the principal subspace. As empirically shown in Table 3, this augmentation strategy significantly outperforms naive random selection of PCs for subspace expansion.\nLearning new tasks Having extracted a set of PCs (including pseudo-PCs, if needed), $V_K \\in \\mathbb{R}^{K\\times n} = \\{V_k \\in \\mathbb{R}^{1\\times n}\\}_{k=1}^{K}$, we can approximate a given (LoRA) weight matrix by minimizing $||W - aV_KE||_F$ where $a$ are linear coefficients Section 3.1. In fact, we can analytically compute $a$ of the given LoRA matrices by calculating the linear coefficients which minimizes the above objective. For new tasks however, for which we do not have a LoRA matrix, we freeze the EigenLoRAx PCs and randomly initialize the $\\alpha$s. The forward pass in layer is calculated as\n$h = W_0x + a_B V_B V_A V_A(x)$  (1)\nHere, $W_0$ are the pre-trained base model weights and $V_B$, $V_A$ are EigenLoRAx PCs that remain fixed during training. The corresponding $\\alpha_B$ and $\\alpha_A$ are learned. This reduces the number of learnable parameters from O(rn) to O(rK), by a factor of $\\frac{K}{n}$ (assuming rank r to be fixed, which could also be changed).\nUsing the definitions 3.1, 3.2, 3.5, 3.4 and 3.3 we state the following theorem;"}, {"title": "Theorem 3.6.", "content": "For a task $t_{d+1}$, we assume a hypothesis h \u2208 $H_{W_{d+1}}$ expressed as h(W_{d+1}, X) = W_{d+1}X + W_0X + b where $W_{d+1}$ has rank m, b is some constant and $W_0$ represents weights of a pretrained foundation model that is frozen during finetuning and X, Y are sampled from $X_t, Y_t$ respectively. We have $h_E \u2208 H_{W_E}, h^* \u2208 H_{W}$ such that $h_E(W_E, X) = \\alpha_{d+1}V_KX + W_0X + b$ where $W_E$ has rank K and $h^*(W^*, X) = CWX + W_0X + b$ where $h^*(W_{d+1}, X) = Y$ is the true solution for task $t_{d+1}$. For a Lipschitz continuous loss $(l_t(h_w))$ that is strong convex within the principal subspace defined by principal components $V_K$ with Lipschitz constant (L), the risk can be written as $R_S(h_w) = E_{lS}[l_S(h_w)]$, and using Rademacher complexity bounds we can say with probability at least 1 \u2013 4\u03b4 for some \u03b4 > 0,\n$||W^* - W_{d+1}||_F < C_1 \\cdot (\\sqrt{\\frac{m}{St}} + C_2)$  (2)\n$||\\alpha_{d+1}V_K - W_E||_F < C_1 \\cdot (\\sqrt{\\frac{K}{St}} + \\frac{||C||}{\\sqrt{St}}\\sum_{i=K+1}^{nd} \\sigma_i^2 + C_2)$  (3)\nwhere \u03c3i are singular values of $W$, C is some constant such that $W^* = CW and C_1, C_2$ are some constants.\nProof. The derivation is straightforward, we can write the difference in risks for $h_E$ and $h^*$ as\n$R_{S_t}(h_E) - R_{S_t}(h^*) = E_{S_t} [l_{S_t}(h_E) \u2013 l_{S_t}(h^*)]$\nBy definition of strong convex loss function for some constant \u03bc \u2265 0,\n$E_{S_t}[l_{S_t}(h_E) - l_{S_t}(h^*)] = \\frac{\\mu}{2}||W_E - W^*||_F^2$\nWe also know from generalization error bounds using Rademacher Complexity from (Bartlett & Mendelson, 2003) that with probability at least 1 \u2013 2\u03b4,\n$R_{st}(H_{WE})\n|R_{S_t}(h) \u2013 R_{S_t}^F(h)| \\leq \\frac{R_{st}(H_{WE})}{\\sqrt{St}} + \\sqrt{\\frac{2\\ln(1/\u03b4)}{2St}}$\nWe can rewrite risk as\n$R_{S_t}(h^*) - R_{S_t} (h_E) = R_{S_t} (h^*) - R_{S_t}^S(h^*) \u2013 R_{S_t}(h_E)$\n$+ R_{S_t}^S(h_E) + R_{S_t}^S(h^*) - R_{S_t} (h_E)$\nSince we know by definition of h* that $R_{S_t}^S (h^*) \\leq R_{S_t}(h^*)$, we can say\n$R_{S_t}(h^*) - R_{S_t} (h_E) < R_{S_t} (h^*) - R_{S_t}^S(h^*) - R_{S_t}^S(h_E) + R_{S_t} (h_E)$\nThen we take a union bound to conclude that with probability at least 1 \u2013 4\u03b4,\n$R_{S_t}(h^*) - R_{S_t} (h_E) \u2264 R_{S_t}(H_{WE}) + \\sqrt{\\frac{2ln(1/\u03b4)}{St}}$\nHence, we can also say that with probability at least 1 \u2013 4\u03b4,\n$\\frac{\\mu}{2}||W^* \u2013 W_E||_F^2 < R_{st} (H_{WE}) + \\sqrt{\\frac{2ln(1/\u03b4)}{St}}$ (4)\nThe Rademacher complexity of a low-rank weight matrix class HWE with rank K can be directly bounded using results from (Bartlett & Mendelson, 2003) as\n$R_{S_t}(H_{WE}) = O(\\frac{\\sqrt{K}||W_E||_F}{\\sqrt{St}})$\n$= C_1 \\cdot (\\frac{\\sqrt{K}||W_E||_F}{\\sqrt{St}} + C_2)$\nFor a normalised $||W_E ||$ is usually bounded Hence,\n$||W^* \u2013 W_E||_F < C_1 \\cdot (\\sqrt{\\frac{K}{St}} + C_2)$ (5)\nSimilarly, we can also say for $W_{d+1}$ that\n$||W^* - W_{d+1}||_F < C_1 \\cdot (\\sqrt{\\frac{m}{St}} + C_2)$ (6)\nThis proves 2. Now to further prove 3, we use properties of Frobenius norm,\n||W_E \u2013 \\alpha_{d+1}V_K||_F=||W^* \u2013 \\alpha_{d+1}V_K||_F< ||W_E \u2013 W^*||_F^2\nThen following from the definition of $W^*$, we can say that,\n$||W_E \u2013 \\alpha_{d+1}V_K||_F-||C||\\sum_{i=K+1}^{nd} \\sigma_i^2 \u2264 ||W_E \u2013 W^*||$\nFinally, using the Rademacher complexity bound we provided earlier, we can say that with probability at least 1 \u2013 4\u03b4\n$||\\alpha_{d+1}V_K - W_E|| < ||W^* \u2013 W_E||$\n$< C_1 (\\sqrt{\\frac{K}{St}} + ||C||\\sum_{i=K+1}^{nd} \\sigma_i^2 + C_2)$\nWe can just rewrite $W_E = \\alpha_{d+1}V_K$ and get the same bound as above for $||\\{\u03b1}_{d+1} \u2013 \\alpha_{d+1}||$. We can similarly obtain the upper bound for 3\nThis concludes the proof."}, {"title": "How to choose optimal number of PCs K", "content": "The hyperparameter K, which determines the number of top PC, can be viewed as a function of task domain complexity-simpler domains require a smaller K, while more complex domains benefit from a larger K. In practice, we determine K based on empirical observations (Appendix B), evaluating performance across different values. Additionally, we can leverage established techniques from literature, such as explained variance and singular value thresholds (Gavish & Donoho, 2014). As illustrated in Figure 2, most of the relevant information is often concentrated in a few top EigenLoRAx PCs, providing a practical criterion for selecting K."}, {"title": "Memory Efficiency", "content": "Our method demonstrates significant memory efficiency across experiments. A single set of EigenLoRAx PCs, combined with lightweight task-specific coefficients, can effectively replace both past and future Lo-RAs within a task domain. This is particularly advantageous when serving a large number of adapters, where frequent loading and unloading in VRAM incurs high latency, or keeping all adapters in memory demands excessive VRAM. For N LoRAs, the memory footprint is O(Nrn). For EigenLoRAxs, it is O(Kn + NrK). As r, K \u226a n, EigenLoRAx becomes $\\frac{r}{K}$ times more memory efficient asymptotically. This becomes significantly useful for edge devices and large scale user serving AI systems."}, {"title": "4. Experiments and Analysis", "content": "In this section, we demonstrate the efficacy and versatility of EigenLoRAx across diverse tasks, modalities, and model architectures, highlighting its individual advantages. EigenLoRAx requires significantly fewer parameters to match or surpass LoRA's performance (Tables 1, 2) and achieves similar or faster loss convergence (Figure 3), making it a cost-effective alternative to random initialization and other methods (Meng et al., 2024). Additionally, we showcase its memory-efficient inference capabilities with a Stable Diffusion text-to-image generation model (Rombach et al., 2021) (Section 4.4). Notably, EigenLoRAx retains its efficiency even in low-resource scenarios where a large number of LORAs are unavailable.\nNote on Baselines Our focus is on recycling adapter knowledge and improving training and memory efficiency while maintaining performance, not solely on maximizing performance. We compare primarily with LoRA, as EigenLoRAx builds its principal subspace using pretrained LoRA adapters. Using better adapters and optimization could further enhance the subspace and performance.\nDue to lack of space, more experiments (3D Object pose estimation) and detailed ablation experiments are presented in Appendix A."}, {"title": "4.1. Image Classification", "content": "This simpler task involves related datasets where the LoRAs used to construct EigenLoRAx are well-aligned with the downstream tasks, highlighting its finetuning efficiency.\nSetup We evaluate EigenLoRAx using a pretrained Vision Transformer (ViT) (Dosovitskiy et al., 2021) across 3 datasets. Each dataset is partitioned into 5-6 non-overlapping sub-datasets, mimicking continual learning (Kaushik et al., 2021) and federated learning (Shenaj et al., 2023) setups. As the sub-datasets are derived from the same source, their tasks are more domain-aligned. For EigenLoRAx, we compute principal components (PCs) using all but one LoRA trained on individual sub-datasets (leave-one-out approach, Algorithm 1). The coefficient matrix a for the excluded task is then learned as described in Section 3.2. All methods are finetuned for 10 epochs, with additional details in Appendix A.1.\nParameter Efficiency Table 1 summarizes our experimental results. All models require training the last linear layer (approx. 15K parameters) due to the pre-trained ViT having a different number of categories. For the Base Model, no additional parameters are trained. EigenLoRAx adapts to new sub-datasets using only two principal components (96 additional parameters), enabling it to match or outperform LORA and VeRA, which use significantly more parameters. We also tested a zero-shot EigenLoRAx (weight initialized randomly within the principal subspace), training only the last layer. This model outperforms the base model with no additional parameters, demonstrating the effectiveness of principal subspace extraction. We also test a low resource scenario (ELORAxAUG), where only 2 LoRAs are available for extracting the PCs, which are then augmented using random, orthogonal PCs as described in Algorithm 1."}, {"title": "4.2. GLUE Benchmark", "content": "Next, we evaluate EigenLoRAx on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) datasets using the RoBERTabase model (Liu et al., 2019). We use 6 different tasks: MRPC, SST-2, COLA, QNLI, RTE and STS-B. Following the setup of VeRA, we omit time-intensive MNLI and QQP tasks, thus avoiding the use of MNLI initialization for MRPC, RTE, and STS-B tasks. In this setting, LoRAs are trained not on sub-datasets but on these different datasets representing a heterogeneous domain setting, where the domain difference may be larger relative to the more domain-aligned setting in Section 4.1. We follow the previous leave-one-out evaluation setup, where EigenLoRAx PCs are calculated using LoRAs of all but one task, and a is learnt for the left-out task. Refer to Appendix A.2 for all hyperparameters and implementation details.\nFaster Convergence Our results in Table 2 show that EigenLoRAx (K = 32) matches LoRA's performance with 100x fewer trainable parameters and outperforms VeRA. EigenLoRAx extracts a useful principal subspace across diverse domains, enabling robust adaptation to new tasks. We also evaluate EigenLoRAx(init) weight initialization speed-up. Unlike PiSSA (Meng et al., 2024), which initializes LoRA matrices with principal directions of pretrained weights, we randomly initialize weights within our extracted subspace. As shown in Figure 3, EigenLoRAx converges faster than PiSSA and VeRA, and slightly faster than LoRA, highlighting the effectiveness of the principal subspace. VeRA's poorer performance may stem from suboptimal random initialization that fails to align with task-critical components. ELORAx is also more efficient in terms of floating point operations for both forward and backward pass, as shown in Table 14.\nLow-Resource Scenario To demonstrate the effectiveness of our subspace augmentation strategy Algorithm 1, we conduct an experiment where EigenLoRAx is initialized with only 1-2 LoRAs. The results are presented in Table 3. We compare our method against augmenting EigenLoRAx with random components (EigenLoRAx+random) and using entirely random components (ELORAxrandom). As shown, our augmentation approach significantly outperforms random principal component selection. Interestingly, for MRPC, the base model's performance is retained. This suggests that the learned LoRA weights may not have influenced the base model, likely because they did not capture relevant information. While we do not provide theoretical guarantees for our principal component augmentation strategy-where randomly sampled vectors are iteratively orthogonalized to the existing EigenLoRAx principal vectors\u2014we hypothesize that this targeted guidance helps prevent redundancy within the subspace. Consequently, it increases the likelihood of capturing the necessary task-relevant components."}, {"title": "4.3. Lots of LoRAS", "content": "Finally, we also tested our method in settings where a large number of adapters may be trained on significantly diverse domains. Lots of LoRAs (Br\u00fcel-Gabrielsson et al., 2024) is a collection of over 500 adapters of the Mistral-7B-Instruct-v0.2 model (Jiang et al., 2023), trained on a variety of natural instruction tasks (Wang et al., 2022). It represents the realistic setting where we directly use publicly available trained adapters, which may present significant diversity in terms of quality and task domain. As all adapters are accompanied with their respective training datasets, Lots of LoRAs is particularly useful in evaluating EigenLoRAx. The task presents significant diversity and a higher K is necessary to represent this open domain.\nSetup We split adapters randomly into two sets (490,5). EigenLoRAX PCs were calculated using the larger \"training\" set and evaluations were done on the smaller \"test\" set. We evaluated EigenLoRAx in a zero-shot setting (calculated using the already available adapter weights, no finetuning) . The results are shown in Table 4 where we evaluate EigenLoRAx on the 5 tasks from the test set and also on 5 tasks from the training set to check for catastrophic forgetting or concept drift from scaling. The first 5 tasks are randomly sampled from the training set. EigenLoRAx nearly matches LoRA with 12 \u2013 95\u00d7 fewer parameters. EigenLoRAx recovers upto 88% of LoRA's performance even in a zero-shot setting at such a large scale. The performance of EigenLoRAx can be improved by fine-tuning the EigenLoRAx adapters. In this setting we use randomized SVD in order to speed up the calculation of the PCs. We believe this leads to some degradation in performance as there randomized methods are approximations of the actual calculations. Performance can be further improved if better implementations of SVD which do not sacrifice accuracy for speed are used in calculating the Principal Components."}, {"title": "4.4. Text-to-Image Image Generative Models", "content": "We showcase EigenLoRAx's versatility on complex multimodal tasks like text-to-image generation, where LoRAs are extensively used to adapt models like Stable Diffusion to various styles and datasets. Despite thousands of LORA adapters being available, most remain underutilized, occupying significant memory alongside their data. As adapter usage grows, a critical challenge is efficiently hosting multiple adapters for diverse tasks, especially on edge devices. Switching adapters during inference, often from CPU memory or disk, introduces latency that hinders real-time applications. EigenLoRAx tackles this by extracting a shared task-invariant subspace, significantly reducing in-memory parameters and enabling memory-efficient inference without compromising flexibility or performance. EigenLoRAx can effectively replace pretrained adapters, drastically reducing storage requirements. To demonstrate this, we extracted K = 14 principal components from N = 20 Stable Diffusion-XL (Podell et al., 2023) LoRA adapters (rank r = 32) from the HuggingFace diffusers library (von Platen et al., 2022). Using $\\alpha \\in \\mathbb{R}^{r\\times K}$, we analytically reconstructed the original LoRA weights within the extracted principal subspace. For image generation, we used 30 denoising steps with a fixed seed of 0. Results and comparisons are shown in Figure 4. This approach reduces storage requirements for all adapters from 4.6GB to just 261MB, achieving an 18\u00d7 reduction in low-rank"}]}