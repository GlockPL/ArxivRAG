{"title": "IMPLEMENTING AN ARTIFICIAL QUANTUM PERCEPTRON", "authors": ["Ashutosh Hathidara", "Lalit Pandey"], "abstract": "A Perceptron is a fundamental building block of a neural network. The flexibility and scalability of\nperceptron make it ubiquitous in building intelligent systems. Studies have shown the efficacy of a\nsingle neuron in making intelligent decisions. Here, we examined and compared two perceptrons\nwith distinct mechanisms, and developed a quantum version of one of those perceptrons. As a\npart of this modeling, we implemented the quantum circuit for an artificial perception, generated a\ndataset, and simulated the training. Through these experiments, we show that there is an exponential\ngrowth advantage and test different qubit versions. Our findings show that this quantum model of an\nindividual perceptron can be used as a pattern classifier. For the second type of model, we provide an\nunderstanding to design and simulate a spike-dependent quantum perceptron. Our code is available at\nhttps://github.com/ashutosh1919/quantum-perceptron", "sections": [{"title": "1 Introduction", "content": "A perceptron is an artificial unit of an intelligent system capable of making decisions. This artificial unit is inspired by\nthe biological neurons found in the human brain. The human brain has a network of billions of neurons connected to\neach other. This connectivity leads to the formation of a deep network. Thus, a perceptron is used as a fundamental\nbuilding block in deep learning systems. In classical computing, these perceptrons have two states, 0 and 1. When the\ninput of the perceptron is sufficient enough to generate an output over the threshold limit, the perceptron is said to be in\n'ON' or 1 state. On the other hand, if the output of the perceptron is less than its threshold value, then it is in 'OFF' or 0\nstate [1].\nDecades of research in the field of classical deep learning have given rise to state-of-the-art systems [2] [3] that\nmimic human-level intelligence. Drawing from recent research that suggests the role of quantum entanglement in\nconsciousness, there has been growing interest in exploring the potential of quantum computing to advance artificial\nintelligence. However, despite this progress, there remains a gap when implementing quantum algorithms in AI. In this\nstudy, we aim to bridge this gap by implementing a quantum model of a perceptron. Here, we review the available\nliterature [4] and implement the quantum circuit using Qiskit quantum simulator [5] to simulate the training of a single\nperception.\nAlmost every advanced deep learning system has artificial neurons as the fundamental building block. Inspired by\nthe success in the classical machine learning field, we attempt to implement a quantum version of a perceptron that\nmimics the properties of a classical perceptron but has the benefits of a quantum system and obeys the rules of quantum\nmechanics.\nPrevious works like [4] introduce a novel architecture and quantum algorithm to design a quantum version of a perceptron.\nWe examine the algorithm and simulate it to test the efficacy of the quantum algorithm. For the implementation, we\nuse QisKit quantum simulation tool and construct quantum gates as specified in the algorithm. We then develop an\nend-to-end pipeline to generate datasets, initialize weights, train the perceptron, and simulate the probability behavior\nas discussed in [4]. Following the training process, we conduct a comprehensive analysis to confirm the trained\nperceptron's ability to accurately classify patterns."}, {"title": "2 Related Work", "content": "The concept of a perceptron was first introduced in [8], which presented the classical mathematical framework for\nutilizing a perceptron as a supervised data classifier. Numerous successful examples have demonstrated the effective\napplication of this mathematical principle in real-world scenarios.\nIn 2013, Lloyed et. al. [9] introduced a theoretical notion of quantum perceptron for supervised and unsupervised\nlearning. Such perceptrons require generalized values and use qRAM [10] to store values. This study contributes to\nthe theoretical literature of quantum computing. In 2014, Schuld et. al. [11] introduced the concept of simulating\nperceptrons using tools. They used the same simulation tools used in [4] to implement the quantum circuit of a\nperceptron. The terminology and the approach are similar too. However, [11] utilizes QFT to create intermediate oracle\ncircuits to prepare the input and weight states which operates on an exponential number of gates. On the other hand, [4]\nmake use of hypergraph states to construct these oracles. This allows them to operate with a polynomial number of\nquantum gates. The most recent classical deep learning models, as described in [12], utilize bias vectors in addition to\nweight vectors for their perceptrons. As implementing perceptron algorithms in the quantum field is a relatively new\nconcept, we omit the bias vector and exclusively focus on training the weight vector."}, {"title": "3 Methods", "content": "Unlike a classical perceptron, a quantum perceptron has quantum gates that prepare the inputs and weights for the\nsystem to process. Unitary transformation functions are used to pre-process the input and weight vectors. A Unitary\ntransformation function, also known as an Oracle, houses quantum gates which act upon the input vectors to perform\noperations such as phase shift, imposing superposition, entanglement, etc. Akin to classical neurons, a quantum\nperceptron takes an input vector and a weight vector and outputs a probability of the outcome."}, {"title": "3.1 Architecture", "content": "Unlike a classical perceptron, a quantum perceptron has quantum gates that prepare the inputs and weights for the\nsystem to process. Unitary transformation functions are used to pre-process the input and weight vectors. A Unitary\ntransformation function, also known as an Oracle, houses quantum gates which act upon the input vectors to perform\noperations such as phase shift, imposing superposition, entanglement, etc. Akin to classical neurons, a quantum\nperceptron takes an input vector and a weight vector and outputs a probability of the outcome.\n$\\vert \\Psi_i \\rangle = \\frac{1}{\\sqrt{m}} \\sum_{j=0}^{m-1} i_j \\vert j \\rangle$\n$\\vert \\psi_w \\rangle = \\frac{1}{\\sqrt{m}} \\sum_{j=0}^{m-1} w_j \\vert j \\rangle$"}, {"title": "3.2 Dataset Generation", "content": "We used the same quantum perceptron to generate the dataset consisting of value-label pairs. Following Mcculloch et.\nal. [1], we replaced all the classical bits containing 1 with -1 and 0 bits with 1. For instance, if the input value is 12,\nthen the transition from classical to quantum vector will look as 12 \u2192 [1,1,0,0] \u2192 [\u22121, -1, 1, 1]."}, {"title": "3.3 Training", "content": "Classical deep learning systems need an enormous amount of training to achieve convergence. In contrast, quantum\ncomputing offers the advantage of rapidly converging models. The quantum perceptron training is described in the\nalgorithm 2."}, {"title": "4 Results", "content": "Pattern Classification: We trained a quantum perceptron and visualized its optimal weights after training. Figure 4\nshows the training steps and the transformation of randomly initialized weight into a complete pattern. Through our\nexperiments, we found that a single quantum perceptron can successfully classify simple patterns of horizontal and\nvertical lines. Here, we report one such pattern after training the perceptron.\nFaster Convergence: Compared to classical deep learning systems, a quantum perceptron can achieve optimal\nperformance faster and has the ability to terminate training once the optimal weight has been reached. We found that a\nfour-qubit system converged and reached the optimal weight before the training was completed.\nIdentical Input and Weight: Finally, we only get a probability of 1 when the input and the weight have the same\nvalue. The geometrical patterns in figure 5 denote the perceptron probability for all combinations of input and weight\nvalues."}, {"title": "5 Conclusion and Future Work", "content": "We implemented a quantum version of a perceptron and tested the algorithm's efficacy. Upon analysis, a single\nperceptron was able to classify patterns after training. The results suggest that a quantum perceptron converges faster\nthan a classical perceptron. This faster convergence highlights the parallel processing of the inputs present in the\nsuperposition states. One of the limitations of this work is the use of a single perceptron to design a classifier. Another\nlimitation is the absence of bias vectors in addition to the weight vectors in the training process. We also confine the\ninput values (only -1 and 1) when training the perceptron. Future work will focus on designing and implementing an\nadvanced network with more interconnected perceptrons. This will lead to the development of an advanced quantum\nnetwork for classification purposes."}, {"title": "6.1 Conflict of Interest", "content": "The authors declare that they have no conflict of interest."}, {"title": "6.2 Funding", "content": "The authors received no financial support for the research, authorship, and/or publication of this article."}]}