{"title": "OReole-FM: successes and challenges toward billion-parameter foundation models for high-resolution satellite imagery", "authors": ["Philipe Dias", "Aristeidis Tsaris", "Jordan Bowman", "Abhishek Potnis", "Jacob Arndt", "H. Lexie Yang", "Dalton Lunga"], "abstract": "While the pretraining of Foundation Models (FMs) for remote sensing (RS) imagery is on the rise, models remain restricted to a few hundred million parameters. Scaling models to billions of parameters has been shown to yield unprecedented benefits including emergent abilities, but requires data scaling and computing resources typically not available outside industry R&D labs. In this work, we pair high-performance computing resources including Frontier supercomputer, America's first exascale system, and high-resolution optical RS data to pretrain billion-scale FMs. Our study assesses performance of different pretrained variants of vision Transformers across image classification, semantic segmentation and object detection benchmarks, which highlight the importance of data scaling for effective model scaling. Moreover, we discuss construction of a novel TIU pretraining dataset, model initialization, with data and pretrained models intended for public release. By discussing technical challenges and details often lacking in the related literature, this work is intended to offer best practices to the geospatial community toward efficient training and benchmarking of larger FMs.", "sections": [{"title": "1 Introduction", "content": "Remote sensing (RS) imagery interpretation remains pivotal to understanding of the Earth surface, with Earth Observation (EO) applications including mapping of built environments [3], disaster management [29], and gravity mapping [54]. While deep neural networks (DNN) and computer vision have enable extraordinary progress in interpretation of imagery, most DNNs are task-specific, with limited generalization to out-of-distribution data, and rely on the onerous availability of large quantities of manually annotated data samples.\nRecently, Foundation Models (FMs) have emerged as a breakthrough with potential to address limitations of neural networks in terms of task-specificity, poor generalization to out-of-distribution data, and reliance on labeled data. FMs can be defined as large models (> 108 parameters) usually trained via self-supervised learning (SSL) on vast data volumes, and are characterized by extraordinary transfer learning capabilities on a wide range of downstream tasks.\nA growing body of self-supervised studies is emerging in the remote sensing community, and Vision Transformers(ViTs) [20] are by far dominating the large scale interpretation of remote sensing imagery. Compared to the 100B+ parameters powering modern LLMs, current FMs for RS are only up to few hundred million parameters. LLMs such as LLAMA (65B) [42], GPT-3 (175B) [7] and PaLM (540B) [13], while ViT-based structures have been extended for up to 20B+ parameters [17]. Importantly, studies in both NLP and computer vision have shown how scaling model capacity enables highly capable and generalizable FMs, including enabling the so-called emergent abilities: i.e., capabilities that are not present in smaller-scale models but are present in large-scale models, and cannot be predicted by simply extrapolating the performance improvements on smaller-scale models. Examples include capabilities that the model is not explicitly programmed or optimized for, such as NLP models trained for predicting the next word"}, {"title": "2 Related Works", "content": "Key ingredients for developing FMs. By leveraging surrogate tasks as sources of supervision, self-supervised learning (SSL) unlocks the potential of learning from unprecedentedly large unlabeled datasets for which manual labeling would be unfeasible, thus underpining the success of FMs. Masked modeling and contrastive learning (CL) have been the two prevailing approaches for SSL of computer vision models. Generative modeling schemes such as masked language modelling were key enablers for, e.g., GPT and BERT models [18, 36]. They have been analogously explored for computer vision in the form of masked image modeling (MIM) [24], where the model is tasked to reconstruct pixels of masked image patches based on the remaining visible image patches. Alternatively, CL schemes task models to maximize feature similarity for correlated image views constructed through augmentations [10].\nTo enable scaling models to billions of parameters, in addition to SSL three key ingredients have been needed: scalable architectures, large data volumes, and computational resources. Transformer-based architectures [44] have been widely used across domains, including Vision Transformers (ViT) [20] for scalable image analysis models [17, 57]. As discussed in [52], large models can easily overfit if data volumes are not scaled accordingly. Combined to the need for long pretraining, this implies the need for leveraging HPC resources for both fitting larger models in GPU memory, as well as enabling distributed data parallelism.\nFoundation models in RS. Transformer-based architectures have been the predominant strategy in works discussing FMs for RS. Masked Autoencoder (MAE) has been predominant for SSL, including SatMAE [15], Ringmo [40], Prithvi [27], [25], SATLASNet [5] and most recently the billion-scale foundation model (BFM) for remote sensing images [8]. Works including SeCo [34], GASSL [4], and Skysense [23] employ Contrastive Learning (CL) instead. In contrast to natural images, RS imagery allows leveraging image acquisitions covering the same location, but from different timestamps and sensors as augmentations for CL. Both GASSL and SeCo are designed to exploit spatially aligned images and temporal information to obtain seasonal positive pairs of images at different points in time for training seasonal contrasting learning objectives. In contrast, Skysense expoits features in a multi-granularity scheme to learn representations across different modal and spatial granularities.\nModel's architecture and training regime that exploit the characteristic features intrinsic in remote sensing imagery have been introduced, including image content [47], spectra [25], multi-scale features [37], temporal characteristics [15, 27]. As summarized in Table 1, with the exception of Skysense [23] all the aforementioned works consists of models whose architectural number of parameters is less than 300-million and are trained on datasets whose corpus is no where near the internet scale volumes encountered when training LLMs. Scaling the architectural sizes of FMs has been shown to bring forth many desirable benefits including task generalization and label efficiency. For these reasons, we contrast our work by introducing a systematic study profiling billion scale models pretrained on globally diversified high resolution satellite imagery."}, {"title": "3 Methods", "content": "All OREOLE-MR/-HR models are \"vanilla\u201d ViTs paired with MAE pretraining. We opt for such simpler configurations to focus on assessing model scaling effects. Future work will explore leveraging unique geospatial and temporal data characteristics, such as GSD-aware positional encodings [37], contrastive learning using multiple collections over same location, and other architectural strategies such as RVSA [47].\nModel Architecture Variants Table 2 summarizes the different ViT variants explored in this work. Width corresponds to embedding size, depth corresponds to number of encoding layers, while"}, {"title": "3.1 Pretraining datasets", "content": "Even though SSL removes the needs for data annotation, the acquisition of pretraining samples ought to be carefully designed. In the context of geospatial data, key characteristics to be considered during dataset curation include geographic diversity, richness, and scalability, to foment the learning of generic representations.\nExamples of high-resolution RS datasets attempting to capture these characteristics include the MillionAID and the functional map of the world (fMoW). The fMoW dataset [14] used by [15, 37] contains 500k+ optical patches collected from multiple sensors. In contrast, the MillionAID [32] used by [8, 47] contains 1M+ RS scenes from a variety sensors and ground sample distances (GSD=0.5-150m/px). RingMo instead [40] exploits a not publicly-available dataset containing 2M+ images for 419M iterations.\nGiven the compute intense demands for pretraining FMs and the lack of standardized datasets, early studies have sought to pretrain on a single benchmark dataset [4, 8, 37, 47], while other studies merge multiple data sources to increase the volume and diversity of samples[40]. A growing body of work[5, 23, 27, 34, 39] is curating multimodal datasets sampled from large archives of remote sensing imagery including national agricultural imagery program (NAIP), Sentinel-1/-2, Harmonized Landsat Sentinel-2 (HLS), or WorldView -2/-3. We introduce two additional datasets in this paper, including the novel TIU."}, {"title": "4 OREOLE-MR Experiments", "content": "We perform fine-tuning (FT) and linear probing (LP) experiments for comparison with existing baselines. Following the community trend, we consider image classification, object detection, and semantic segmentation downstream tasks. There is, however, a wide variability on the datasets used for performance benchmarking. We summarize in Table 4 the datasets commonly used to benchmark models pretrained on medium-to-high resolution imagery. Guided by this review, we selected the following datasets to compose the evaluation scenarios for OREOLE-MR variants: i) image classification: UCM [55], AID [50], and NWPU[11]; ii) object detection: DIOR [30] and DIOR-R [12]; iii) semantic segmentation: Potsdam and LoveDA [48]. Details on the composition of these datasets and finetuning configurations are available in the Appendix. Results are summarized in Table 5, with main takeaways discussed below."}, {"title": "5 OREOLE-HR experiments", "content": "We leverage the ORB dataset for OREOLE-HR experiments. A separate random subset of 4, 836 tiles is used for validation, and two FT configurations were considered: TR = 100%, with 55.71k labeled training tiles and FT for 100 epochs (5.57M iterations); TR = 10%, using 5, 571 (10%) randomly sampled tiles for FT (100 epochs = 557.1k iterations). Results are summarized in Table 6 and discussed below."}, {"title": "6 Conclusion", "content": "We introduced OREOLE, a family of FMs for RS image interpretation. Experiments with OREOLE-MR highlight model scaling benefits across downstream tasks, but with a pattern of diminished gains. Paired with takeaways from OREOLE-HR experiments, it highlights the importance of pairing model and data scaling, confirming for RS data a behavior observed in the broader computer vision [52].\nFurthermore, we share a rare discussion on challenges related to downstream task optimization and setting up evaluation protocols. Many works lack reproducibility details such as the number of training iterations as dataset sizes vary. Moreover, we show how hyperparameters co-opted from related works can be largely suboptimal, and how parameters are sensitive to model size and tasks. This is a challenge for benchmarking, as exhaustive hyperparameter tuning is computationally expensive and contradictory to the intended benefits of FMs. We argue in favor of benchmarks with fixed-budget hyperparameter search, as suggested in [28].\nBuilding on these conclusions and our work [43], we aim to scale models beyond 3B parameters with expanded pretraining datasets, conduct few-shot evaluations to explore potential emergent abilities and implement geospatial-aware methods and integrate multiple data modalities to improve using FMs for Earth understanding."}, {"title": "7 Appendix", "content": "7.1 Pretraining implementation details\nDevelopment of OREOLE-MR and OREOLE-HR models took place concomitantly. OREOLE-HR models were pretrained using the mmpretrain codebase [16], an open source pre-training toolbox based on PyTorch adapted using the gdal library [21] for ingestion of the BGR+NIR images in TIF-file format composing our TIU and ORB datasets. We opted for the mmpretrain for two main reasons. First, it contains implementations of a wide variety of model backbones and self-supervised learning strategies, offering flexibility for experimentation with other types of configurations in the long term. Moreover, it is part of the broader collection of repositories by OpenMMLab (e.g., mmsegmentation, mmdetection and mmrotate), thus potentially facilitating integration with configurations for downstream tasks.\nOREOLE-HR models were all trained with an effective batch size of 4096 samples, distributed across GPUs composing nodes of the Frontier Supercomputer [1]. Each Frontier node contains 4\u00d7 AMD Instinct MI250X GPU accelerators, with each MI250X comprising of 2\u00d7 Graphics Compute Dies (GCDs). Since the system identifies each GCD independently, from the application perspective it can be considered that each node has a total of 8 GPUs, each with 64 GB of high-bandwidth memory. For simplicity we use the term GPU when referring to a GCD. ViT-H model variants were trained using 16 Frontier nodes (128 GPUs), while ViT-B variants were trained with 8 nodes (64 GPUs).\nMeanwhile, OREOLE-MR models were pretrained using a codebase based on the original MAE work [24]. Specifically, we leverage the codebase and insights discussed in our recent work [43], which augments the original MAE codebase to enable the support for PyTorch's native Fully Sharded Data Parallel (FSDP) strategy [58]. Our studies detailed in [43] revealed a higher image throughput and hence time to solution for MAE pretraining when using FSDP NO_SHARD strategies as compared to PyTorch's Distributed Data Parallel (DDP). We leveraged this knowledge for pretrained each of our OREOLE-MR models using the FSDP NO_SHARD strategy to distribute an effective batch size of 2048 across 4 NVIDIA A100 (80GB) GPUs composing NVIDIA DGX clusters."}, {"title": "7.2 Image classification experiments details", "content": "Datasets UC Merced (UCM) is a image classification dataset containing 2100 remote sensing images from the USGS National Map, at a resolution of 1ft. Images are 256 \u00d7 256px large, distributed evenly across 21 categories. The AID dataset was compiled from multisensor data available in Google Earth, with spatial resolutions 0.5 - 8m/px for images 600 \u00d7 600px large. It contains 10k images distributed across 30 categories, with class distributions ranging from 220 to 400 images each. Finally, the NWPU-RESISC45 dataset contains 31.5k images evenly distributed across 45 categories and also collected from Google Earth for regions distributed across the world. Image are 256 \u00d7 256px large, with spatial resolutions ranging from 0.2 to 30m/px. We adopt the following splits for each dataset: UCM(TR = 50%) with 1050/1050 train/test images; AID (TR = 20%) with 2000/8000 images; NWPU (TR = 10%) with 3150/28350 images.\nImplementation details The MLP heads of pretrained models are replaced by a linear classifier for supervised training while keeping the weights for the rest of the model frozen. The setup in [47] reports experiments for 200 epochs of tuning with batch-size 256 for all three datasets while adapting usage of the LARS optimizer [56] with a base learning rate of 0.1 and no weight decay. As discussed in Section 4, we empirically observe that while keeping the same batch size, increasing the base learning up to 10 in this setup yields severe improvements in accuracy levels reached after linear probing. Thus, for linear probing evaluation with QUETZAL-MR model variants we adopt LARS with base learning rate 10, tuning the learnable heads for 100 epochs with a warmup period of 10 epochs."}, {"title": "7.3 Object detection experiments details", "content": "Datasets DIOR [30] contains 23,463 800 \u00d7 800px RGB images sourced from Google Earth, with GSD = 0.5 \u2013 30m/px. There are 20 object classes and a total of 192,472 object instances. DIOR is divided into training, validation, and testing sets with 5, 862, 5, 863, and 11, 738images, respectively. As in [46], we use the training and validation sets in a combined 11, 725-image \"trainval\" set for fine-tuning, and we validate using the testing set. The images, object classes, object instances, and data splits are identical to DIOR.\nImplementation details We augment our pretrained ViT backbones weights into VitDet [31], a ViT-based object detection architecture that demonstrates promising performance with minimal adaptation of backbones required. Faster-RCNN [22] was used as the detection head for the horizontal object detection task, and its variant Oriented Faster-RCNN was used for the oriented object detection task. The VitDet implementation was based on mmdetection [9] for DIOR data, and the same implementation was further adapted for DIOR-R data using the mmrotate [59] library. We report evaluation based on Mean Average Precision (mAP) calculated at Intersection over Union (IoU) threshold 50%.\nAs in [31], we adopt a patch size of 16 \u00d7 16px for all ViT backbones, interpolating the originally 14 \u00d7 14px patch embeddings pretrained for the ViT-H and ViT-1B backbones. With the pretrained ViT weights, we finetuned on DIOR and DIOR-R for 12 epochs, the same as the setting in [46]. Similar to [47], the base learning rate was set as 1e - 4 for ViT-B with a batch size of 4. Experimentation with [1, 3, 5, 10]e - 4 and same batch size for ViT-G has shown better convergence with 3e \u2013 4, while for ViT-B the original 1e - 4 revealed to be better. The remaining configurations follow [46] and are listed in the Table 8. Convergence issues are faced with ViT-H and ViT-e(3B) configurations, as curves in Figure 2a show. As discussed in Section 4, we conjecture the complexity of the object detection head and the need for different learning decay policies for different model sizes [31] are potential root causes."}, {"title": "7.4 Semantic segmentation experiments details", "content": "Datasets. The LoveDA [48] comprises of 5987 high resolution imagery (1024 \u00d7 1024px, GSD = 0.3m/px) from Google Earth, with per-pixel annotations of 7 land cover classes. Images are split into train/val/test sets comprising 2522/1669/1796 samples respectively. The evaluation metrics have been computed on the test set using the evaluation server. The ISPRS Potsdam dataset consists of 38 tiles of GSD = 0.5m/px"}, {"title": "7.5 OREOLE-HR experiments details", "content": "Similar to experiments reported in section 7.4, UperNet is paired to pretrained OREOLE-HR variants for semantic segmentation. Feature pyramids and rescaling factors are the same as reported in section 4, with the auxiliary FCN head however removed to reduce computing workload. We adopt distance-labels as per [53] to formulate the loss function, and experiments are performed with a effective batch size of 2048, AdamW with betas (0.9, 0.999), base learning rate of 1e \u2013 3, and a cosine annealing LR schedule with 10 epochs of warmup. Image size of 512 x 512 is adopted for Vit-B configurations and 504 \u00d7 504 for larger configurations trained with patches 14 \u00d7 14px large, to bypass mmsegmentation errors related to non-integer number of patches."}, {"title": "7.6 OREOLE-MR - additional results", "content": "Figure 2c additionally summarizes performances of our models when using 1, 10, and 50% of the total available training data for DIOR-R (object detection) and Potsdam (semantic segmentation) datasets. Here we highlight a lack of reproducibility details on related works, which report maintaining the same FT configuration when performing experiments with different data percentages. Such description is insufficient when training duration and schedules are reported in terms of number of epochs instead of iterations. For our experiments, we opted for maintaining the same number of training iterations for training data budgets considered, since the goal is to assess the sample efficiency of the models, not their sensitivity to training duration (iterations). Our results show consistent benefits of a larger model when > 10%(1170) training images of DIOR-R are used, and as low as 1%(35) training images for Potsdam. For DIOR-R, ViT-B values at lower sample budgets are significantly higher than the ones reported by [8], which we conjecture is due to our configuration maintaining the same number of iterations as when finetuning with full dataset."}, {"title": "7.7 OREOLE-HR additional results", "content": "Initialization using inflated OReole-MR pretrained weights In addition to improved F1 results discussed in Section 5, the loss curves in 4 show how initialization with pretrained OREOLE-MR weights inflated with a 4th band yield faster convergence (i.e., better starting point) for MAE pretraining on our 4-band datasets."}]}