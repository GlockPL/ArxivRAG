{"title": "On Iterative Evaluation and Enhancement of Code Quality Using GPT-40", "authors": ["Rundong Liu", "Andr\u00e9 Frade", "Amal Vaidya", "Maxime Labonne", "Marcus Kaiser", "Bismayan Chakrabarti", "Jonathan Budd", "Sean Morant"], "abstract": "This paper introduces CodeQUEST, a novel framework leveraging Large Language Models (LLMs) to iteratively evaluate and enhance code quality across multiple dimensions, including readability, maintainability, efficiency, and security. The framework is divided into two main components: an Evaluator that assesses code quality across ten dimensions, providing both quantitative scores and qualitative summaries, and an Optimizer that iteratively improves the code based on the Evaluator's feedback. Our study demonstrates that CodeQUEST can effectively and robustly evaluate code quality, with its assessments aligning closely with established code quality metrics. Through a series of experiments using a curated dataset of Python and JavaScript examples, CodeQUEST demonstrated significant improvements in code quality, achieving a mean relative percentage improvement of 52.6%. The framework's evaluations were validated against a set of proxy metrics comprising of Pylint Score, Radon Maintainability Index, and Bandit output logs, showing a meaningful correlation. This highlights the potential of LLMs in automating code quality evaluation and improvement processes, presenting a significant advancement toward enhancing software development practices. The code implementation of the framework is available at https://github.com/jpmorganchase/CodeQuest.", "sections": [{"title": "Introduction", "content": "The production of high-quality code remains a top priority for organizations developing software. High-quality code extends beyond syntactic or semantic correctness, encompassing attributes such as freedom from errors, readability, efficiency, portability, usability, testability, and maintainability [20].\nEvaluating code quality is a complex process due to the subjective nature of the concept, leading to the creation of various language-specific and non-comprehensive tools that cover very specific aspects under the code quality concept umbrella [10]. For example, the Python ecosystem provides security linters for identifying security vulnerabilities, style linters for identifying deviations from style guides, type checks, and error linters [36]. Moreover, subjective aspects like code design and readability are often guided by \"best practices\". It is also important to understand that evaluation of code quality, while crucial, is just one part of ensuring production-ready code. This is typically achieved through extensive code reviews, where multiple developers examine and modify the scripts until they meet the required standards. While important, this process is resource intensive, time-consuming, and it generally places additional burdens on senior developers that are ultimately responsible for vetting final code versions. Automating this process could significantly boost code development productivity.\nRecent advancements in Large Language Model (LLM) technologies have shown promise in coding-related tasks, including code generation and evaluation [1] [30] [17]. LLMs offer unique advantages for code evaluation due to their training on diverse programming languages and vast datasets, enabling them to first understand and then assess as well as improve code quality. Moreover, their ability to perform tasks based on prompts provides a flexible and highly configurable paradigm for easily defining complex tasks in natural language [13]. Studies are increasingly focusing on how prompt engineering and complexity affect the LLMs' ability to generate accurate, maintainable code. For example, some research compares one-hot and iterative prompt strategies to understand how more detailed or structured prompts impact the functionality and readability of the generated code, showing that prompt complexity can significantly enhance the robustness of outputs [2]. Other studies assess GPT-4's performance against alternative models, highlighting notable improvements in debugging capabilities and responsiveness to follow-up prompts. Specifically, recent work investigates GPT's self-correction capacity when supplied with targeted feedback, such as error messages or static analysis results, highlighting the model's ability to refine code and address issues like smells or unnecessary complexity through iterative feedback [25] [16].\nWhile LLMs' zero-shot capability have been widely exploited, recent studies [28] [3] [37] [38] have suggested that borrowing concepts from Reinforcement Learning could significantly improve the capability for LLM-based agents to perform reasoning and planning tasks. For instance, Zhang et al. [37] have drawn inspiration from the classical actor-critic method [14] to design a TipleCritic mechanism, which has significantly improved the success rate on multi-agent planning tasks.\nDespite their potential, using LLMs for such complex tasks also presents challenges. LLMs are not inherently designed for quantitative assessments, which may affect the reliability of their scores [8]. Moreover, the probabilistic nature of LLMs can lead to inconsistencies. Indeed, small modifications to the prompt or a change of the LLM random seed can potentially lead to vastly"}, {"title": "The CodeQUEST Framework", "content": "The CodeQUEST framework consists of two components: an Evaluator and an Optimizer that leverage LLM to assess and improve code quality, respectively. In this work, we used the most recently released GPT-40 from the GPT-4 model series [1] and code quality was defined across ten dimensions: Readability, Maintainability, Testability, Efficiency, Robustness, Security, Documentation, Modularity, Scalability, and Portability.\nThe Evaluator first generates a code quality score\u00b9 and a text-based evaluation for each dimension, which is used to produce an aggregated summary of dimension-wise evaluations. The Optimizer is then responsible for improving the code quality over a fixed number of iterations (or until a target quality score is reached) based on the Evaluator's feedback. Each iteration involves three sequential stages: code quality improvement, code integrity validation, and code quality re-assessment. A detailed description of each stage is provided in this section below. A diagram of the end to end process can be found in Figure 1."}, {"title": "Evaluator", "content": "As mentioned above, the Evaluator uses ten dimensions to assess code quality. Each dimension is addressed through a set of five questions or statements, carefully crafted to comprehensively cover its key aspects (c.f., Appendix B for the full list). These questions and statements were designed to a) be general, thus applicable to most programming languages, and b) not overlap in dimension scope probing, hence avoiding over-representation of any particular aspects of the dimension. Furthermore, each question or statement is formulated such that it can only be answered with \u201cTrue\u201d, \u201cFalse\u201d or \u201cNot Applicable\", where \"True\" reflects that high quality characteristics present in the code, in contrast to a \"False\". We address each of the code quality dimensions in a separate query to the LLM, where the corresponding set of five dedicated questions or statements are provided to the LLM for its consideration.\nInspired by theoretical efforts [12] [32] on the impact of language ambiguity to LLM's performance, we carefully crafted prompts enabling LLMs to apply their internal knowledge in a focused and unambiguous manner. Limiting the possible output to quantifiable answers allows us to map the model output to a numerical scale. The output includes the answers to each of the five dedicated questions or statements (which enable the quantitative assessment) and a high-level summary of the code in light of the dimension under consideration (which serves as a qualitative assessment).\nQuantitative results are derived by assigning numerical values to each answer (+1 for \"True\", -1 for \"False\", and 0 for \"Not Applicable\"). For each dimension, we sum up the five responses to obtain a score ranging from -5 to 5, where higher scores represent higher code quality along the dimension axis. Finally, dimension-specific scores are averaged to enable a \"code-level\" score. We also ask GPT-40 to summarize qualitative assessment across all ten dimensions, obtaining a code-level summary.\nThe prompt template, based on zero-shot Chain-of-Thought (CoT) [35], is provided below. It combines 1) the code to be evaluated; 2) the set of five questions or statements for a given quality dimension; 3) the task to be performed, and 4) the desired output format:"}, {"title": "Optimizer", "content": "In this section, we describe the Optimizer, which is tasked with improving the quality of code based on the feedback generated by the Evaluator. While qualitative results set a clear direction for code improvement, quantitative feedback is leveraged to ensure that progress occurs in a unidirectional manner. The Optimizer operates in an iterative improvement cycle, where each iteration includes three steps: code quality improvement, code validation, and generated code evaluation."}, {"title": "Code Quality Improvement", "content": "To generate an improved version of the code, the original code snippet and its qualitative assessment results are fed to GPT-40. The task involves addressing all the areas of improvement identified in the assessment feedback through"}, {"title": "Code Validation", "content": "The code validation stage ensures that the LLM modified code can be compiled, as a minimal requirement for the code to be deemed valid and the process to proceed. Additionally, test cases built for the original code can be optionally executed against all code improvement versions to ensure the intended functionality and expected behavior. Such validation checks constitute a first step towards the meaningful evolution of the code.\nNote that the failure of either check leads to a rejection of the generated code as a candidate for the next iteration. A failed attempt does however, still count as an iteration towards the total number of iterations.\nSelf-reflection and correction [28] based on the compiler or execution outputs were left as a interesting direction for future work."}, {"title": "Evaluator Assessment", "content": "A successful code validation stage is followed by a new Evaluator assessment. At this stage, if the overall quality score of the new code version drops relatively to the one of the previous version indicates that no overall improvement was achieved. Under these circumstances, a new iteration is triggered, using the inputs of the previous cycle as a new code improvement attempt. Again note that an unsuccessful improvement attempt still counts as an iteration.\nOtherwise, if the overall quality score of the new code increases relative to the last successful version, the iteration is deemed successful. In this case, the quality references are updated: the new overall quality score becomes the new numerical baseline for improvement and the qualitative feedback is used as input to the code quality improvement of the next cycle.\nThe number of iterations and overall quality target score are both hyperparameters of the Optimizer. The improvement cycle terminates when either, the maximum number of iterations or the target quality score is reached."}, {"title": "Experimental Setup", "content": "We hand-curated a small but representative sample of 42 code examples across nine open-source libraries, including Scipy [33], Security Eval [29], AWS-CDK-Examples [6] and Science-JS [11]. We focused on Python and JavaScript due to their popularity, and aimed for examples of varying code length and intended functionality. For future research, it would be interesting to expand the scope of the analysis to other programming languages, for example considering the QScored [27] dataset focused on C# and Java.\nThe majority of the code sourced from reputable open-source libraries should be of reasonably high quality. Thus, some of the code pieces were manually modified to enable scope for improvement. This included annotation removal and non-semantic changes to the code, whilst leaving the original logic intact. In"}, {"title": "Baseline solution", "content": "To perform ablation study of the Evaluator's prompt, we considered a simpler CoT prompt to generate baseline results. This approach assumes that GPT-40's internal knowledge is sufficient for an accurate and comprehensive evaluation of code quality. In our experiments, we compare it with CodeQUEST to validate the effectiveness of our solution. Again, the following prompt shares the system message with the Evaluator's prompt."}, {"title": "Validating Scores With Proxies", "content": "Existing and well-established quality metrics are used in this study as a proxy to validate LLM-based code evaluations. This validation is limited to Python code due to the logistics of availability, implementation, and execution of such analytical tools. The following code quality evaluation capabilities were selected for this study as an attempt to cover as many code quality dimensions as possible:\n1. Pylint\u00b3 [31] follows the style recommended by PEP8, while penalizing unused variables, lengthy code, and ill-named variables. Pylint returns a score from 0 to 10, which we use as a proxy metric for Readability and Documentation.\n2. Radon Maintainability Index [19] is used as a proxy for code Maintainability, similar to [23]. The measure is a score from 0 to 100 derived from other metrics, including Source Lines of Code (SLoC), Cyclomatic Complexity, and Halstead Volume. We scale the score to range from 0 to 10.\n3. Bandit [7] scans Python code for security issues for Python using abstract syntax trees. Bandit does not provide a score, so we applied a simple heuristic that transforms the execution standard output into a score from 0 to 10 to define our proxy for Security.\nFor simplicity, we define our overall proxy code quality score as the average of these three metrics, resulting in a final score ranging from 0 to 10."}, {"title": "Results", "content": ""}, {"title": "Can GPT-40 evaluate code quality?", "content": "To evaluate the CodeQUEST's Evaluator, we compare its evaluation results to those produced by the baseline setup. In both cases, the output consists of an overall code quality score (quantitative result) as well as a short summary report (qualitative feedback). Each code example was repeatedly evaluated five times by both setups\u00b9, allowing us to gather statistics on the consistency of the outputs produced."}, {"title": "Can evaluations be used to drive code quality improvement?", "content": "In our study, we applied the CodeQUEST framework to each code example for a maximum of five improvement cycles. All versions of the Python code were checked for their ability to compile. Furthermore, all versions of the mbpp code were validated against the corresponding test cases provided. All intermediary and final results were recorded for downstream analysis.\nThe example shown in Figure 3 represents the general behavior observed across all code examples. Out of the original 42 code instances, 41 were improved to some extent. We report an average absolute improvement of 2.9 \u00b1 1.3 in the code quality scale units with those of initial lower quality exhibiting the largest improvements.\nTo further analyze the improvements, we defined the Relative Percentage Improvement(RPI) as\n$RPI := 100\\frac{(s_f^n - s_i^n)}{(S_{max} - s_i^n)}$ (1)\nwhere $s_{max}$ = 5 is the maximum possible score, and $s_i^n$ and $s_f^n$ are the initial and final scores for the $n^{th}$ code example, respectively. N = 42 is the total number of examples in the dataset. Over the entire dataset, our framework produces a mean RPI of 52.6% \u00b1 17.9% and a median RPI of 57.1%. We also verified that while our setup, by design, ensures a monotonic improvement of overall code quality, the monotonic improvement of individual dimensions' scores does not always occur, i.e., an overall improvement for the quality score produced by an iteration might be accompanied by a decrease in some of the dimension-specific scores.\nFigure 4 displays the distribution of improvements achieved for each iteration in absolute code quality units. We can observe that most of the improvement occurs in the first iteration cycle, with an average of 2 code quality units increase. The average improvement for iterations two and three were found to be 0.52 and 0.27. From the fourth iteration, the improvement becomes negligible (below 0.1 code quality units).\nThese results suggest that the magnitude of code quality improvement reduces at each iteration, which becomes particularly useful to limit the GPT-40 API costs incurred by the framework if CodeQUEST is to be considered and adopted at scale.\nTherefore this section demonstrates that our Optimizer framework can utilize the qualitative insights generated by the Evaluator to achieve significant code quality improvement. Indeed, we notice the consistent alignment between quantitative and qualitative results, such that an increase in the quality score reflects a decrease in code quality deficiencies reported by the quality summary. Therefore we believe that this setup should be general enough to treat this as a general-purpose recipe for configurable code quality improvement."}, {"title": "Are these evaluations meaningful?", "content": "To confirm the validity of LLM-generated quality scores, we investigate their level of agreement with the quality proxy scores described in Section 3.3. Given each original code example and its corresponding refactored versions generated by CodeQUEST, here denoted as {X0,...,Xnmaz}, we can consider the incremental change of the score per iteration, defined as\n$\u0394_{xn} CodeQUEST = CodeQUEST(x_n) \u2013 CodeQUEST(x_{n\u22121}),$ (2)\nfor iterations n = 1, ..., Nmax. In our case, Nmax = 5. Similarly, we can define An Proxy and Arm Baseline for the proxy and baseline scores, respectively."}, {"title": "Threats to validity", "content": "In this section, we would like to acknowledge certain considerations regarding the validity and robustness of our framework. Firstly, code quality is an intrinsically subjective concept and, while our evaluations work well with the selected questions, it might not be as effective if modified significantly. However, given the generalization capabilities of GPT-40 and its advanced knowledge of both code and English language concepts, we believe this is unlikely as long as the questions are reasonable and posed in the manner we detail in Section 2.1.\nMoreover, the stochasticity of GPT-40 output also introduces some inherent variability in the results. While we see good stability in our setup and take measures to limit it by setting the temperature to 0, that might not hold for all code and or prompts. We also recommend using the self-consistency framework [34] to address this, if required."}, {"title": "Conclusion", "content": "In this paper, we introduced CodeQUEST - Code Quality Understanding and Enhancement System Toolkit - a GPT-40 powered framework capable of evaluating and improving code quality for code across different domains. We show that our framework is able to produce comprehensive qualitative and quantitative code quality evaluation reports, and subsequently use these reports to iteratively improve the quality of the code being analyzed. Our framework is easily understandable, adjustable, and shows impressive results for Python as well as JavaScript, showing its applicability across languages. We also demonstrate that our framework produces code evaluation scores showing good agreements with rule-based code-quality evaluation libraries."}, {"title": "Conflict of interest", "content": "The authors declare that they have no conflict of interest."}, {"title": "Disclaimer", "content": "This paper was prepared for informational purposes by the Applied Innovation of AI team of JPMorgan Chase & Co. This paper is not a product of the Research Department of JPMorgan Chase & Co. or its affiliates. Neither JPMorgan Chase & Co. nor any of its affiliates makes any explicit or implied representation or warranty and none of them accept any liability in connection with this paper, including, without limitation, with respect to the completeness, accuracy, or reliability of the information contained herein and the potential legal, compliance, tax, or accounting effects thereof. This document is not intended as investment research or investment advice, or as a recommendation, offer, or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction. The described work is a prototype and is not a production deployed system."}, {"title": "Manual code quality defect", "content": "Example of code that has been \"degraded in quality\". We first show the original code piece, followed by the modified code, where the_update_x function is re-defined multiple times.\nOriginal code example from Scipy:"}, {"title": "Dimension related questions/statements", "content": "The CodeQUEST Framework defines ten dimensions of code quality. For each dimension, it further defines five questions/statements. The full list is given below."}, {"title": "Code quality evaluation examples", "content": ""}, {"title": "CodeQUEST - code evolution example", "content": "Below is a step-by-step example of the CodeQUEST framework in action, applied to the mbpp/927.py example. The quantitative evaluations for the same are provided in Figure 3\nInitial Evaluation\nQualitative result: The provided code is a simple and functional implementation of a binary tree node class and a function to calculate the maximum height of the tree. It is generally readable with descriptive variable and function names, and follows a consistent style. The code is logically organized, adheres to the DRY principle, and is maintainable. It avoids global states and complex control flows, ensuring consistent and predictable outputs. However, it lacks comments, documentation, input validation, error handling, and considerations for concurrency issues. The code is not optimized for large data loads or high concurrency and does not facilitate easy mocking of dependencies. It is highly portable, avoiding platform-specific features and using standard libraries and APIs, but there is no information about dependencies. Overall, the code is simple and"}, {"title": "Security Vulnerable Code", "content": "1. cwe-321-author-2.py from Security Eval:\nIn this example, an API key is exposed in the source code. This security vulnerability is not identified by Bandit, resulting in a evaluation score of 10/10:\nCodeQUEST rates the security dimension as -4 with the following qualitative assessment:\nAfter one iteration, CodeQUEST produces an implementation following security best practices, such as storing the API key as an environment variable, as well as adding sensible error-catching mechanisms.\n2. cwe-321-author-1.py from Security Eval:\nDespite sensitive information being exposed, Bandit again does not highlight any potential problems with the code:\nCodeQUEST evaluates the security dimension as -4, with the following qualitative feedback:"}]}