{"title": "FAST, HIGH-QUALITY AND PARAMETER-EFFICIENT ARTICULATORY SYNTHESIS USING DIFFERENTIABLE DSP", "authors": ["Yisi Liu", "Bohan Yu", "Drake Lin", "Peter Wu", "Cheol Jun Cho", "Gopala Krishna Anumanchipalli"], "abstract": "Articulatory trajectories like electromagnetic articulography (EMA) provide a low-dimensional representation of the vocal tract filter and have been used as natural, grounded features for speech synthesis. Differentiable digital signal processing (DDSP) is a parameter-efficient framework for audio synthesis. Therefore, integrating low-dimensional EMA features with DDSP can significantly enhance the computational efficiency of speech synthesis. In this paper, we propose a fast, high-quality, and parameter-efficient DDSP articulatory vocoder that can synthesize speech from EMA, F0, and loudness. We incorporate several techniques to solve the harmonics / noise imbalance problem, and add a multi-resolution adversarial loss for better synthesis quality. Our model achieves a transcription word error rate (WER) of 6.67% and a mean opinion score (MOS) of 3.74, with an improvement of 1.63% and 0.16 compared to the state-of-the-art (SOTA) baseline. Our DDSP vocoder is 4.9x faster than the baseline on CPU during inference, and can generate speech of comparable quality with only 0.4M parameters, in contrast to the 9M parameters required by the SOTA.", "sections": [{"title": "1. INTRODUCTION", "content": "Articulatory synthesis is the task of generating speech audio from articulatory features, i.e., the physical movements of human articulators, often measured as electromagnetic articulography (EMA). Since the articulatory features are physically grounded [1], EMA-to-speech vocoders are more interpretable than mel-spectrogram-based vocoders [2]. Articulatory vocoders are also highly controllable, allowing for nuanced adjustments in speech generation [2, 3]. Given these unique characteristics, articulatory synthesis has many applications including helping patients with vocal cord disorders communicate better [4, 5], decoding brain signals to speech waveforms [6], and augmenting silent speech systems [4].\nHowever, to our knowledge, there has been little investigation into the parameter efficiency of articulatory synthesis models, which is important for applications on edge devices, where the memory and computation are limited. Smaller models may also have faster inference speed, which also opens up new possibilities for faster real-time applications. Since articulatory synthesis is mostly utilized in clinical domains, a high-speed low-footprint synthesis model is crucial for maximizing accessibility.\nWe utilize differentiable digital signal processing (DDSP) [7] to achieve efficient articulatory synthesis while maintaining high-fidelity audio generation. A DDSP model consists of a neural network encoder and traditional digital signal processing (DSP) modules. The encoder transforms input features, such as F0, loudness, and spectral features, into control signals like filter coefficients and harmonic amplitudes. DSP modules then generate audio from these control signals. The differentiability of DSP modules allows for end-to-end training, hence the term \u201cDifferentiable DSP\". DDSP models are light-weight since they utilize the strong inductive bias of known signal-processing modules to explicitly model the speech generation process [8]. Consequently, DDSP models only need to learn control signals rather than raw waveforms, delegating synthesis to DSP modules.\nIn this paper, we introduce a novel articulatory synthesis approach using DDSP with the Harmonic-plus-Noise (H+N) model to convert articulatory features (EMA, F0, loudness) into speech. To our knowledge, this is the first application of DDSP to articulatory synthesis. Our model achieves a word error rate (WER) of 6.67% and a mean opinion score (MOS) of 3.74, improving the state-of-the-art (SOTA) result by 1.63% and 0.16, respectively. It is also 4.9x faster during CPU inference. Additionally, a 0.4M parameter version of our model matches the quality and intelligibility of the previous 9M-parameter SOTA. Codes and audio samples are available at tinyurl.com/ddsp-vocoder."}, {"title": "2. RELATED WORK", "content": "Articulatory synthesis with traditional digital signal processing methods has long been investigated [9, 10, 11, 12]. In the deep learning era, there are generally three methods for artic-"}, {"title": "2.1. Articulatory Synthesis", "content": "ulatory synthesis: (1) predicting the acoustic parameters first and then using traditional signal-processing-based vocoders, e.g. WORLD [13], to synthesize speech [14, 15]; (2) predicting intermediate spectrograms and then utilizing GAN-based vocoders [16, 17] to convert spectrograms to speech signals [4, 18]; (3) directly synthesizing speech from articulatory features with HiFi-CAR [2, 3, 17, 19]. Among them, [2] is the SOTA model in terms of synthesis intelligibility and inference speed, and [3] extends it to a universal articulatory vocoder. However, there is still scope for improving parameter efficiency and synthesis quality."}, {"title": "2.2. Differentiable Digital Signal Processing", "content": "There are two main architectures of DDSP synthesizers: (1) the source-filter model [20, 21, 22], and (2) the Harmonic-plus-Noise (H+N) model [23, 24, 25]. Since H+N models are strictly more expressive than source-filter models [7], we investigate the H+N model in this paper. The H+N model divides speech into two components: harmonics, which represent the periodic part of speech produced by vocal cord vibrations; and noise, which models the aperiodic component of speech produced by airflow in the vocal tract. DDSP has wide-spread applications in music generation [7, 26], timbre transfer[27, 28], singing voice synthesis[29, 30], and speech synthesis [20, 21, 22, 23, 24, 25]."}, {"title": "3. METHODS", "content": "Following [7], our proposed model mainly consists of two parts: an encoder and a DSP generator. The overall model architecture can be found in Figure 1. Note that F0 and loudness are pre-computed from the corresponding utterance."}, {"title": "3.1. Encoder", "content": "The encoder architecture is shown in Figure 2. Inspired by [31], we use a dilated convolution network as the encoder."}, {"title": "3.2. Digital Signal Processing (DSP) Generator", "content": "For the DSP modules, we iterated on the DSP generators of [7]. The outputs of the encoder from section 3.1 control two DSP modules: a harmonic oscillator and a filtered noise generator. The harmonic oscillator generates the voiced components of speech while the filtered noise generator synthesizes the unvoiced components. The outputs of these two modules are added to get the raw synthesized speech, which will be filtered by the post convolution (post conv) layer to generate the final synthesized speech."}, {"title": "3.2.1. Harmonic Oscillator", "content": "Unlike the harmonic oscillator in [7], where only the sine harmonic waves are used, we propose to use both the corresponding sine harmonics and cosine harmonics to better approx-"}, {"title": "3.2.2. Filtered Noise Generator", "content": "This module generates noise signals filtered by learned linear time-varying finite impulse response (LTV-FIR) filters. To avoid complex numbers, we treat H[n] as half of a zero-phase filter's transfer function, which is real and symmetric. We perform an inverse fast Fourier transform (FFT) to obtain zero-phase filter coefficients, shift them to form a causal, linear-phase filter, and apply a Hann window to balance time-frequency resolution, resulting in h[n], which is then multiplied by an attenuation hyperparameter \u03b3 to balance the filtered noise and harmonics. The filtered noise output is produced by convolving each h[n] with a noise signal of length u (a noise frame) and performing overlap-and-add with a hop size of u. Noise is generated from a uniform distribution between [-1, 1], and all convolutions are computed via FFT."}, {"title": "3.3. Post Convolution Layer", "content": "To further balance the noise and harmonics amplitudes, we introduce a post convolution (post conv) layer, which is a learnable 1D convolution layer without bias. Unlike the 1D convolution reverb module in [7], which models reverberation"}, {"title": "3.4. Loss Functions", "content": ""}, {"title": "3.4.1. Multi-Scale Spectral Loss", "content": "We use the multi-scale spectral loss as defined in [7]:\n$L_{MSS} = \\sum_{i \\in W} ||S_i - \\hat{S_i}||_1 + \\alpha|| \\log S_i \u2013 \\log \\hat{S_i}||_1$\nwhere S and $\\hat{S}$ are the magnitude spectrograms of the ground truth audio and the generated audio respectively. a is chosen to be 1 in this paper. W = [2048, 1024, 512, 256, 128, 64] is the set of FFT sizes, and the frame overlap is set to be 75%."}, {"title": "3.4.2. Multi-Resolution Adversarial Loss", "content": "As mentioned in [21], training only with multi-scale spectral loss for audio often results in over-smoothed spectrogram predictions. L1/L2 losses aim to reduce large discrepancies and capture the low-frequency components of spectrograms, averaging out rapid changes in spectral details which results in muffled-sounding audio, as shown in Figure 3.\nTo capture the finer details of spectrograms, following the work of [34], we utilize multi-resolution spectrogram discriminators. We treat each input spectrogram as a one-channel image, and perform 2D strided convolution for discrimination. Note that the input spectrograms are calculated from acoustics with different parameters, such as window size, hop size, and number of points for FFT, so that the discriminators have access to spectrograms of the same utterance with multiple resolutions.\nFor each sub-discriminator, the adversarial loss is calculated as Least Squares GAN (LSGAN) described in [35]:"}, {"title": "4. RESULTS", "content": ""}, {"title": "4.1. Datasets", "content": ""}, {"title": "4.1.1. MNGU0 EMA Dataset", "content": "We experiment with the MNGU0 EMA dataset [36], comprising 75 minutes of 16 kHz male speech with 200 Hz EMA recordings. The 12-dimensional EMA features capture the x and y coordinates of jaw, upper and lower lips, and tongue (tip, blade, and dorsum) movements. F0 is extracted from the speech using CREPE [37] with a 5ms hop size, and loudness is computed as the maximum absolute amplitude of each 5ms speech frame [38, 39]. Consequently, EMA, F0, and loudness are all sampled at 200 Hz. During training, we randomly crop 1-second segments of aligned EMA, FO, and loudness for input, and their corresponding waveforms as targets. The dataset is split into 1129 training utterances (71.3 minutes) and 60 test samples (3.7 minutes), with 60 training utterances used for validation."}, {"title": "4.1.2. LJ Speech Pseudo-Labelled Dataset", "content": "To evaluate our model with a substantial amount of training data, we use the LJ Speech dataset [40], containing 24 hours of 22050 Hz female speech. As it lacks EMA data, we generate pseudo EMA labels using the acoustic-to-articulatory inversion (AAI) model from [3, 41, 42]. EMA features are linearly interpolated from 50 Hz to 200 Hz, and waveforms are resampled to 16 kHz. Other features follow the MNGUO settings. We use a 90%/5%/5% train/validation/test split, corresponding to 21.5, 1.25, and 1.25 hours, respectively."}, {"title": "4.2. Experimental Setup", "content": "For our DDSP model, we choose the kernel size of ResBlocks to be 3 with 2 convolution layers inside, the hidden dimension of the dilated convolution stacks to be 256, with K = 50 harmonics, M = 65 frequency bands, and attenuation \u03b3 = 0.01. The loudness FiLM module consists of three 1D convolution layers with kernel size 3, and the post convolution layer has a kernel size of 1025. This results in a total of 9.0M parameters. The multi-resolution discriminator uses R = 6 with FFT sizes [2048, 1024, 512, 256, 128, 64] and 75% frame overlap. Weight normalization [43] is applied to all sub-discriminators.\nWe use the Adam optimizer with \u03b2\u2081 = 0.9, \u03b22 = 0.999, and distinct learning rates: 3 \u00d7 10-4 for the generator and 3 \u00d7 10-6 for the discriminator. The batch size is 32, with \u5165= 5. For MNGUO dataset, there are 6400 training epochs. The learning rates are multiplied by 0.3 at epoch milestones [2400, 4800]. For LJ Speech dataset, the total number of epochs is 1280, with epoch milestones = [480, 960]. The HiFi-CAR baseline (13.5M) [2] is trained with its original configuration and adapted to our input features."}, {"title": "4.3. Metrics", "content": "We use both objective and subjective metrics to evaluate model performance. Objective metrics include: (1) word error rate (WER), which is calculated on the transcription of the synthesized test set speech using the SOTA speech recognition model Whisper-Large [44]; A lower WER indicates higher intelligibility of the synthesized speech; (2) Multi-resolution STFT (M-STFT) [16][45], which measures the difference between the spectrograms of the ground truth and the prediction across multiple resolutions; (3) perceptual evaluation of speech quality (PESQ) [46], a widely adopted automated method for assessing voice quality; and (4) UTMOS [47], a machine-evaluated mean opinion score (MOS). We use the conventional 5-scale MOS test as the subjective metric. Each model receives 200 unique ratings."}, {"title": "4.4. Synthesis Quality", "content": "The subjective and objective quality metrics for DDSP and HiFi-CAR are listed in Table 1. For MNGU0, our DDSP model is consistently better than the baseline in every metric, with a boost in WER by 1.63% and a significant improvement in MOS (+0.16). This indicates that our DDSP model has a strong and appropriate inductive bias for the inner periodic structure of speech signals and is capable of generating high-fidelity speech. For LJ Speech, with substantially more training data, our model is still better in all metrics. This also indicates that our model is effectively compatible with the inverted EMA from the AAI model."}, {"title": "4.5. Parameter Efficiency", "content": "To evaluate parameter efficiency, we retrain the models using the same configurations as in section 4.2, but with varying parameter counts (nparams): [9M, 4.5M, 2.3M, 1.1M, 0.6M, 0.4M]. For each nparams, we train the model with three random seeds (324, 928, 1024) and evaluate the combined synthesized test set speech. To maintain the receptive field size, we reduce nparams by decreasing the hidden dimension. The results are shown in Figure 4. Our DDSP model shows no significant performance decline as model size decreases, outperforming HiFi-CAR at all nparams configurations. In contrast, HiFi-CAR's performance drops drastically below 1.1M nparams. Notably, our smallest model (0.4M) is comparable to HiFi-CAR (9M), highlighting our DDSP model's high parameter efficiency and potential for edge device applications."}, {"title": "4.6. Inference Speed", "content": "We test the inference speed of DDSP, HiFi-CAR (13M), and HiFi-CAR (9M) on an Apple M1 CPU by varying the input length N from 0.5s to 10s, with 0.5s intervals. For each N, we average the inference time over 50 utterances of the same length N, normalizing by N. Table 2 reports the model sizes and the mean and standard deviation of the average inference time for 1s of input. Our model is 1.5x smaller and 4.9x faster than HiFi-CAR (13M). Notably, HiFi-CAR (9M) is still 3.9x slower than DDSP despite having the same model size. Furthermore, as shown in Figure 4, HiFi-CAR (9M) consistently underperforms compared to DDSP in both WER and UTMOS. This demonstrates that our DDSP model is fast and lightweight without sacrificing synthesis quality."}, {"title": "4.7. Ablation Study", "content": "We perform an ablation study on the GAN loss, additional cosine harmonics, post conv layer, and loudness FiLM using the MNGUO dataset, with all models trained under the same configuration as the original model. The results, summarized in Table 3, show that removing any module decreases performance, except for the GAN loss. Without the GAN loss, similarity metrics like PESQ and M-STFT improve, as the model is trained solely on reconstruction loss ($L_{MSS}$ in Section 3.4.1), leading to predictions more similar to the ground truth on average but perceptually over-smoothed, as mentioned in Section 3.4.2 and supported by significant drops in UTMOS (-1.739) and MOS (-0.64). The absence of additional cosine harmonics causes substantial performance drops across all metrics, underscoring their importance in speech modeling. The post conv layer is essential for balancing noise and harmonics amplitudes. Omitting the loudness FiLM module results in a small yet noticeable performance decline."}, {"title": "5. DISCUSSION", "content": ""}, {"title": "5.1. Speech Decomposition", "content": "Since the synthesized speech is the sum of harmonics and filtered noise, we can decompose the output and visualize each component via spectrograms (Figure 5). The harmonics spectrogram shows distinct frequency bands and higher energy, reflecting the quasi-periodic nature of voiced sounds generated by the harmonic oscillator. In contrast, the noise spectrogram"}, {"title": "5.2. Noise / Harmonics Balance", "content": "One challenge in achieving a high-quality vocoder using our DDSP model is balancing the amplitudes of harmonics and noise. We employ three methods to address this issue: the attenuation hyperparameter \u03b3, the post conv layer, and the loudness FiLM module. Among these methods, the attenuation and the post conv layer are particularly crucial. If there is no attenuation at all, i.e. \u03b3 = 1, the model will only learn the filtered noise as shown in Figure 6. Although on average the energy distribution seems correct, the predicted spectrogram has lost all finer harmonic structures, while for the ground truth, there are clear and detailed harmonic stripes."}, {"title": "6. CONCLUSION", "content": "In this paper, we present a DDSP articulatory vocoder based on harmonic-plus-noise model. With the strong inductive bias of DDSP, we show that our model is parameter-efficient, fast, and capable of synthesizing high-quality speech from EMA, F0 and loudness. For future work, we plan to explore the multi-speaker capabilities of our DDSP vocoder."}]}