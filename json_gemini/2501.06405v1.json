{"title": "FocusDD: Real-World Scene Infusion\nfor Robust Dataset Distillation", "authors": ["Youbing Hu", "Yun Cheng", "Olga Saukh", "Firat Ozdemir", "Anqi Lu", "Zhiqiang Cao", "Zhijun Li"], "abstract": "Dataset distillation has emerged as a strategy to compress real-world datasets for efficient training.\nHowever, it struggles with large-scale and high-resolution datasets, limiting its practicality. This paper\nintroduces a novel resolution-independent dataset distillation method Focused Dataset Distillation\n(FocusDD), which achieves diversity and realism in distilled data by identifying key information patches,\nthereby ensuring the generalization capability of the distilled dataset across different network architectures.\nSpecifically, FocusDD leverages a pre-trained Vision Transformer (ViT) to extract key image patches,\nwhich are then synthesized into a single distilled image. These distilled images, which capture multiple\ntargets, are suitable not only for classification tasks but also for dense tasks such as object detection. To\nfurther improve the generalization of the distilled dataset, each synthesized image is augmented with a\ndownsampled view of the original image. Experimental results on the ImageNet-1K dataset demonstrate\nthat, with 100 images per class (IPC), ResNet50 and MobileNet-v2 achieve validation accuracies of 71.0%\nand 62.6%, respectively, outperforming state-of-the-art methods by 2.8% and 4.7%. Notably, FocusDD is\nthe first method to use distilled datasets for object detection tasks. On the COCO2017 dataset, with an\nIPC of 50, YOLOv11n and YOLOv1ls achieve 24.4% and 32.1% mAP, respectively, further validating the\neffectiveness of our approach.", "sections": [{"title": "Introduction", "content": "Contemporary deep learning has achieved remarkable success largely due to the exponential growth in model\nsizes (Dosovitskiy et al., 2020; He et al., 2016; Radford et al., 2021; Szegedy et al., 2015) and data scales (Deng\net al., 2009; Kirillov et al., 2023; Ridnik et al., 2021). This growth has led to the development of advanced\nneural networks that achieve groundbreaking performance in tasks like image classification (Dosovitskiy et al.,\n2020), object detection (Carion et al., 2020), and natural language processing (Vaswani et al., 2017). However,\nthis progress is not without its challenges. The rapid expansion of model complexities and data volumes\nhas led to significantly increased computational costs and time expenses, in particular when training large\nneural networks on high-resolution and large-scale datasets (Jiang et al., 2021; Liu et al., 2021; Touvron et al.,\n2021). These challenges significantly hinder the practical deployment of deep learning models, especially in\nresource-limited environments (Ignatov et al., 2019)."}, {"title": "Related Work", "content": "Data distillation (Wang et al., 2018) aims to reduce the computational costs of training deep learning\nmodels by condensing large datasets into smaller, information-rich subsets. Most previous dataset distillation\nmethods (Cazenavette et al., 2022; Guo et al., 2023; Lee et al., 2022; Nguyen et al., 2021; Wang et al., 2022,\n2018; Zhao and Bilen, 2023; Zhao et al., 2020; Zhou et al., 2022) focus on small-scale and low-resolution\ndatasets (Chrabaszcz et al., 2017; Kim et al., 2022; Le and Yang, 2015) and can be classified into several\ncategories: Bi-level optimization methods treat dataset distillation as a meta-learning problem, where an outer\nloop optimizes the synthetic dataset while an inner loop focuses on model training using distilled data, methods\ninclude FRePo (Zhou et al., 2022), DD (Wang et al., 2018), RFAD (Nguyen et al., 2021), KIP (Nguyen\net al., 2021), and LinBa (Deng and Russakovsky, 2022). Trajectory-matching methods align model training\ntrajectories on the original and distilled datasets over multiple iterations, methods include MTT (Cazenavette\net al., 2022), TESLA (Cui et al., 2023), and DATM (Guo et al., 2023). Distribution-matching methods match\nthe distribution of the distilled dataset with that of the original in a single optimization step, with examples\nlike KFS (Lee et al., 2022), DM (Zhao and Bilen, 2023), CAFE (Wang et al., 2022), HaBa (Liu et al., 2022),\nand IT-GAN (Zhao and Bilen, 2022). Gradient-matching methods align gradients of the network trained on\noriginal and synthesized data, with examples including DSA (Zhao and Bilen, 2021), IDC (Kim et al., 2022),\nDC (Zhao et al., 2020), and DCC (Lee et al., 2022).\nBuilding on these foundations, recent approaches have extended dataset distillation to large-scale, high-\nresolution datasets. For example, SRe2L (Yin et al., 2024) decouples model updates and dataset synthesis\nthrough \"squeeze\", \"restore\". and \u201crelabel\" stages, pioneering the expansion of dataset distillation to ImageNet-\nscale resolutions. SCDD (Zhou et al., 2024) further improves on SRe2L by replacing batch-level statistics with\nglobal dataset statistics, achieving notable performance gains. D3S (Loo et al.) reframes dataset distillation\nas a domain shift problem, introducing a scalable algorithm, while RDED (Sun et al., 2024) generates distilled\nimages by randomly cropping and selecting high-realism image regions. Additionally, some dataset distillation\nmethods (Gu et al., 2024; Su et al., 2024) employ the concept of diffusion models for distilling datasets.\nAlthough previous methods excel with high-resolution images, they compress the original dataset into a\nspecific architecture (Sun et al., 2024; Yin et al., 2024; Zhou et al., 2024), limiting the generalization of the\ndistilled dataset. In contrast, FocusDD synthesizes datasets using the well-established Attention mechanism,\nwhich improves generalization, as shown in Table 1 and Table 5 across different ViT models. Furthermore, by\nsynthesizing images focused on target locations, FocusDD extends its use to dense tasks like object detection,\nmarking the first application of dataset distillation in this domain."}, {"title": "Approach", "content": "We first provide background knowledge on dataset distillation and ViT in Sec. 3.1. Next, we give a detailed\ndescription of our method FocusDD in Sec. 3.2, along with a theoretical analysis in Appendix D. Finally, we\ndiscuss how to train models using the distilled dataset in Sec. 3.3."}, {"title": "Preliminaries", "content": "Data Distillation/Condensation. Dataset distillation (Wang et al., 2018) aims to compress information\nfrom a large-scale original dataset to a new compact dataset while striving to preserve the utmost degree of\nthe original data informational essence. The resulting compressed dataset denoted as D', should enable a\nmodel trained on it to perform comparably to a model trained on the original, full dataset D. Considering\na large labeled dataset D = {(X1,Y1), ..., (X\\|D\\|,Y\\|D\\|)}, where |D| denotes the total number of samples,\nand each xi is an image with its corresponding label yi. The aim is to create a condensed dataset D' =\n{(X1,Y1),..., (X\\|D'\\|, Y\\|D'\\|)} that retains the key features of D, with |D'| < |D|, ensuring that this reduction\nin size does not compromise the dataset integrity. The learning objective is to minimize the performance\ndisparity between the model trained on D' and the one trained on D, as expressed by the following constraint:\n$\\sup{\\{|l(\\phi_{\\theta_{D'}}(x), y) \u2013 l(\\phi_{\\theta_{D}}(x), y) |\\}} _{(x,y)\\sim D} \\leq \\epsilon,$\nwhere e represents the allowable performance disparity between models trained on D' versus those trained on\nD. Here, OD parameterizes the neural network 6, optimized on D as follows:\n$\\theta_{D} = arg \\min_{\\theta} E_{(x,y)\\in D}[l(\\phi_{\\theta}(x), y)].$\nIn this formulation, l is the loss function, and Op is defined in a similar manner for the condensed dataset.\nThis framework ensures that D' maintains the essential characteristics of D, allowing effective training on a\nsmaller scale.\nVision Transformer. Vision Transformer (ViT) (Dosovitskiy et al., 2020) adapts the Transformer\narchitecture (Vaswani et al., 2017), originally developed for natural language processing, to the domain of image\nanalysis. They treat image patches as sequential inputs, allowing the model to capture global dependencies\nacross the image. Each image is segmented into patches, which are embedded and supplemented with\npositional encodings to maintain spatial information, denoted as: x = [xcls; E(p1); E(p2); . . . ; E(pk)] + Epos,\nwhere E is the embedding function, pi are the patches, cls is the class token, and Epos represents the\npositional encodings. The self-attention mechanism then calculates attention scores to determine the relevance\nof each patch relative to others:\n$A(Q, K) = Softmax(\\frac{QK^T}{\\sqrt{d}}) = [A^1; A^2; ...; A^K],$\n$Attention(Q, K, V) = A(Q, K)V,$"}, {"title": "Focused Dataset Distillation with Attention", "content": "This section introduces FocusDD, a dataset distillation method that reconstructs compiled images by focusing\non the target and representative background information of real images. Fig. 3 and Algorithm 1 in Appendix B\nprovide an overview. Further details are provided below.\nAttention-guided Information Extraction. We utilize an attention mechanism to identify and extract\nregions with the highest attention scores from multiple images, thereby compiling images with enhanced detail.\nThese regions are then combined to form a detailed composite image set, as illustrated in Fig. 3. The process\ninitiates by performing the following steps on each image xi \u2208 RH\u00d7W\u00d7Ch within each category-specific subset\nDe of the dataset D: each i is downsampled tox and segmented into non-overlapping patches of size\nP \u00d7 P. This downsampling produces K = \u00d7 patches per image, which are subsequently reorganized\ninto the structured form R\ubbd0\u00d7\u8a48\u00d7P2Ch, with each row and column representing a token. These tokens are\nembedded and fed into a pre-trained ViT model, yielding predictive distributions p and attention scores\nsi \u2208 RK. Likewise, we reorganize each attention score si into the format \u00d7 W. To determine the size of\nthe highest attention score region for each image x, we introduce an adjustable hyperparameter a, which\nspecifies the number of patches [a\u00d7\u300d. We then introduce a realism score seal to identify the key\npatch for each image. Specifically, our realism score combines the prediction distribution p of each image\nwith the highest attention region score sarea, defined as follows:\n$s^{real}_i = max(softmax(p^i)) + \\eta s^{area},$\nwhere \u03b7 is a balancing factor. Intuitively, steal indicates the need to select a representative image with a focus\non the target region within it. This implies that our selection process should prioritize images that represent\nthe overall scene accurately and emphasize the specific area of interest, ensuring that the target region is\nwell-captured and highlighted in the chosen image.\nAfter calculating the realism score seal, we associate each score with its corresponding image De and sort\nthe scores in descending order. Based on these scores, we select the top-M images from the sorted De and"}, {"title": "Information Reconstruction", "content": "The size of key patches is typically smaller than the target distilled\nimages. Directly using these key patches as distilled images can result in sparse information distribution in\nthe pixel space, thereby reducing the effectiveness of the learning model (Shen and Xing, 2022; Yin et al.,\n2024; Yun et al., 2021). As shown in Table 8, using distilled image sets composed solely of key patches leads\nto a decreased model performance. Therefore, we combine the set of images containing key information\npatches To with the set of low-resolution images T' to supplement the class category e information with the\ntypical context in which they appear. Specifically, we randomly select m patches from Te and n low-resolution\nimages from Te each time. The selected images are then concatenated to compile the final composite image\nXj:\n$\\tilde{x}_j = concat((\\{x^{k}\\}_{k=1}^{m} \\in T_c), (\\{x'^{k}\\}_{k=1}^{n} \\in T'_c)).$\nBy default, we set the combined total of patches and images to m + n = 4 (see Fig. 6 in the Appendix C.3),\nwhere m = 3 represents the selection of three patches from the key information patch collection Te, and n = 1\ncorresponds to selecting one low-resolution image from the background information collection T' (Table 8).\nFollowing the RDED (Sun et al., 2024) and SRe\u00b2L (Yin et al., 2024), we apply a soft label approach (Shen\nand Xing, 2022) to the compiled images. This method generates region-level soft labels \u1ef9 = l(\u00a2\u04e9, (x)),\nwhere\nis the k-th region in the distilled image, and \u1ef9 is its corresponding soft label.\nBy iterating over each category c in D, performing the information extraction and image reconstruction\nprocesses, and adding the generated images and labels {xj, yj} to the distilled dataset D', we ultimately\nobtain the complete distilled dataset D'."}, {"title": "Model Training on Distilled Datasets", "content": "After assembling the distillation dataset D', we initiate training of a student model des from random\ninitialization using this dataset, in line with strategies proposed by Yin et al. (2024) and Sun et al. (2024).\nFor classification tasks, the training employs a cross-entropy loss function defined as:\n$L=-\\sum_j \\sum_k y_{jk} log p_{\\theta_{D'}}(x^j_k).$\nTo optimize training efficiency for the detection task, we input the distilled images into YOLOv11x (Khanam\nand Hussain, 2024) to compute the classification and bounding box losses and supervise model updates using"}, {"title": "Experiments", "content": "Experimental Setup\nDatasets and Implementation Details. We conducted rigorous and extensive validation of FocusDD on\nthe large-scale ImageNet-1K dataset (Deng et al., 2009) to comprehensively evaluate its performance. The\nImageNet-1K dataset consists of approximately 1.2 million training images with a resolution of 224\u00d7224\npixels, spanning 1000 categories. For key patch extraction, we utilized the Deit-S model (Touvron et al.,\n2021), pre-trained by Hu et al. (2024). We maintain a constant side ratio a of 0.8 and 7 of 30. We set the\nvalue of N equal to IPC and M equal to 3xIPC, effectively limiting the size of the distillation dataset to the\ntotal number of pixels in the IPC image. We train target models including ResNet-{18, 50, 101} (He et al.,\n2016), MobileNet-v2 (Sandler et al., 2018), and EfficientNet-b0 (Tan and Le, 2019) to validate the distilled\ndatasets. All models are trained on the distilled dataset for 300 epochs with 224\u00d7224 image resolution. Our\nexperiments were conducted using an NVIDIA 4090 GPU. Additional experimental details and Tiny-ImageNet\n(Le and Yang, 2015) experiments are provided in Appendix A and Table 14 Appendix C.2, respectively.\nEvaluation and Baselines. We compare our approach with several SOTA methods for distilling large-\nscale, high-resolution datasets, including SRe2L (Yin et al., 2024), SCDD (Zhou et al., 2024), GVBSM (Shao\net al., 2023), D3S (Loo et al.) and RDED (Sun et al., 2024). In our evaluation process, we generate a unique\ndistillation dataset for each IPC level (1, 10, 50, 100) for FocusDD and reuse it across multiple network\narchitectures."}, {"title": "Performance Evaluation", "content": "ImageNet-1K Classification. Tables 2 and 3 present the experimental results of FocusDD on the\nImageNet-1K dataset, showing its significant advantages across various architectures (e.g., ResNet-18, ResNet-\n50, ResNet-101, MobileNet-V2, EfficientNet-B0) and IPC settings. FocusDD consistently outperforms other\nmethods, especially for low IPCs (1, 10, and 50), achieving higher accuracy, which is crucial for scenarios with\nlimited samples or resource constraints. For instance, on ResNet-18, FocusDD achieves accuracies of 8.8% and\n45.3% at IPCs of 1 and 10, respectively, significantly surpassing RDED and D3S. Even for higher IPCs (e.g.,\nIPC = 100), FocusDD maintains strong performance, often achieving or nearing the best results on ResNet-50\nand Efficient Net-B0. This demonstrates FocusDD's ability to excel under minimal and small-sample data\nconditions, adapting effectively across different models and IPC configurations.\nAdditionally, we compare our method with diffusion-based image generation models (Gu et al., 2024;\nPeebles and Xie, 2023) in Table 4. Appendix C.1 compares FocusDD with Coreset-based selection methods\n(Forgy, 1965; Welling, 2009) on ImageNet-1K, showing consistent superiority of FocusDD. Table 14 in\nAppendix C.2 shows FocusDD's strong performance on Tiny-ImageNet, even at low IPCs, aligning with\nresults on ImageNet-1K.\nCOCO Object Detection. In the object detection task, we use YOLOv1lx (Khanam and Hussain, 2024)\nas the teacher model to perform soft-supervised training on YOLOv11n and YOLOv1ls models from scratch\nfor a total of 100 epochs, with all experimental settings following the official YOLOv11 (Khanam and Hussain,\n2024) configuration. Fig. 1 shows the mAP performance of the FocusDD-distilled dataset on the COCO\nvalidation set under different IPC settings. The figure indicates that as IPC increases, model performance\nalso gradually improves. For example, when IPC is 50, YOLOv1ls achieves an mAP of 32.1%. FocusDD\nperforms effectively on object detection tasks because distilled images are composed of multiple patches\ncontaining targets, each of which may include objects of interest to the detection model."}, {"title": "Performance Analysis", "content": "Cross-Architecture Generalization. Table 5 evaluates the impact of different ViT models on FocusDD's\nperformance on ImageNet-1K, using ResNet-18 for validation. The results demonstrate that our method\nmaintains consistent performance across ViT architectures, corroborating the idea that the attention-based\nkey patch selection in FocusDD is similarly effective for also different transformer architectures. Table 1"}, {"title": "Conclusion", "content": "In this paper, we introduce FocusDD, a novel method that employs attention mechanisms to guide data\ndistillation effectively for large-scale and high-resolution datasets. FocusDD extracts key patches from\nimage target regions, ensuring critical information and realism, and combines them with low-resolution\ncontextual backgrounds to create distilled images for training. This diversifies the dataset and enhances model\ngeneralization. Additionally, FocusDD is invariant to the resolution of target images, making it a flexible and\nperformant choice for data distillation regardless of the underlying image resolution requirements. Extensive\nexperiments and ablation studies demonstrate FocusDD's effectiveness and offer insights into applying deep\nlearning to large-scale data and complex models for both classification and object detection tasks."}, {"title": "Appendix", "content": "In the appendix, we provide details omitted in the main text, including:\n\u2022 Section A: Implementation Details.\n\u2022 Section B: Focused Dataset Distillation Algorithm.\n\u2022 Section C: Further Experimental Results.\n\u2022 Section D: Theoretical Analysis.\n\u2022 Section E: Sample Visualizations of Synthetic Data."}, {"title": "Implementation Details", "content": "Pre-training ViT Models\nFor the ImageNet-1K dataset, we directly use the model pre-trained by LF-ViT (Hu et al., 2024), which is\nbased on the implementations of Deit-S (Hu et al., 2024) and LV-ViT-S (Jiang et al., 2021). This model\nperforms inference at both the standard resolution of 224\u00d7224 and a higher resolution of 288\u00d7288, efficiently\nextracting crucial information patches for dataset distillation. To further reduce inference time, we disable the\nFocus stage in the LF-ViT implementation. More details and features of LF-ViT can be found on the official\nwebsite. For the lower resolution and smaller scale Tiny-ImageNet dataset, we train a modified version of the\nDeit-S-based LF-ViT (Hu et al., 2024) from scratch to extract key information patches. Specifically, we reduce\nthe model's depth to 4 layers, set the patch size to 4\u00d74, adjust the embedding dimension to 192, and reduce\nthe number of heads to 3. This modified model is trained from scratch using the same hyperparameters as\nthose used for ImageNet-1K."}, {"title": "FocusDD Implementation Details", "content": "We maintain a fixed side ratio a = 0.8 and a balancing factor \u03b7 = 30 for both the ImageNet-1K and\nTiny-ImageNet datasets. To compile each image ; in the distilled dataset D', we set N and M to IPC and\n3xIPC, respectively. The compile process involves concatenating three key patches from the key information\ncollection Te and one low-resolution background image from T', resulting in the compiled image as described\nby Eq. 8. For instance, at an IPC of 100, we select 300 key information patches and 100 downsampled\nlow-resolution images with background information, ensuring the synthesis of a diverse and representative\nimage. This approach adapts to different IPC values to accurately reflect the dataset's variability. Aligned\nwith techniques from SRe\u00b2L (Yin et al., 2024) and RDED (Sun et al., 2024), we employ Fast Knowledge\nDistillation (Shen and Xing, 2022) to relabel distilled images. Each distilled image ; is randomly cropped\ninto several patches, with their coordinates recorded within j. Soft labels \u1ef9 are generated and stored for\neach k-th patch. These labels are aggregated to construct a comprehensive label \u1ef9; for each image, facilitating\nnuanced and accurate labeling reflective of the diverse visual features captured in the compiled images.\nTraining on Distilled Dataset. We use a model with the same architecture as the validation model,\npre-trained on the corresponding original and full datasets, to generate soft labels for the synthesized images.\nFor Tiny-ImageNet, our teacher model is pre-trained on the complete Tiny-ImageNet dataset, following the\nhyperparameters in (Yin et al., 2024). When training the validation model on the distilled Tiny-ImageNet\ndataset, we use the hyperparameters shown in Table 9. For ImageNet-1K, all teacher models use pre-trained\nmodels from the torchvision library. When training the validation model on the distilled ImageNet-1K\ndataset, we follow the parameters in Table 10. Both datasets are augmented by CutMix with a mix probability\np = 1.0 and a beta distribution \u03b2 = 1.0.\nFor the object detection task, we selected samples from the ImageNet-1K dataset corresponding to the cate-\ngories in COCO2017 (Lin et al., 2014) and generated a dataset based on the IPC settings. YOLOv11x (Khanam"}, {"title": "Focused Dataset Distillation Algorithm", "content": "Algorithm 1 Focused Dataset Distillation with Attention\nInput: Dataset D, pre-trained ViT model, \u03b1, \u03b7, M, N, m, n\nOutput: Distilled dataset D'\nfor each category-specific subset D C D do\nfor each image xi \u2208 De do\nDownsample xi to x and segment into non-overlapping patches of size P \u00d7 P\nEmbed patches and feed into ViT model\nObtain predictive distributions p and attention scores si \u2208 RK\nUse the predefined a to determine the size of the patch\nCalculate realism score steal by Eq. 5 and associate it with the corresponding image xi.\nend for\nSort image De by each image's realism score in descending order\nSelect the top-M images and obtain the center indices of the key patch regions by Eq. 6\nExtract key patches x by Eq. 7\nAdd key patches into set Te\nRandomly select N downsampled low-resolution images in De from non-top-M images\nAdd selected downsampled low-resolution images to set T\nfor me Te and x \u2208 T do\nRandomly select m key patches from Te and n downsampled images from T'\nConcatenate to compile composite image ; by Eq. 8\nApply soft label approach to xj\nAdd {xj, yj} to distilled dataset D'\nend for\nend for\nreturn Distilled dataset D'"}, {"title": "Dynamic Fine-Tuning", "content": "Following the training of model des on the distilled dataset D', we implement the Dynamic Fine-Tuning\n(DFT) process. The DFT process involves fine-tuning the model on subsets of the original dataset that are\ndynamically sampled at each epoch. To preserve consistency with the structural properties of the synthetic\ndataset, images are randomly selected at an IPC level from each category to form new datasets for fine-tuning.\nThis strategy is systematically applied throughout each epoch, introducing variability and generating a\nunique dataset for fine-tuning in every cycle. This approach significantly enhances the diversity of the data\nwithout additional training overhead, thereby boosting the model's generalization ability across diverse data\nrepresentations. Furthermore, the DFT methodology not only capitalizes on the attributes of synthetic data\nbut also closely aligns the model's performance with real-world data distributions, culminating in notable"}, {"title": "Coreset Selection", "content": "Comparison with Coreset Selection Baselines. In this evaluation, we use ResNet-18 as a validation\nmodel on the ImageNet-1K dataset with IPC set to 10, comparing it to a dataset extraction strategy based\non coreset selection. We evaluate the top-1 validation accuracy achieved by three distinct Coreset selection\nmethodologies: (1) Random selection; (2) Herding, as introduced by Welling (2009); and (3) K-Means"}, {"title": "Background and Definitions", "content": "To analyze how the dataset distillation with an attention-based region selection affects the generalization\nability of models on a testing dataset, we employ Rademacher Complexity Yin et al. (2019) as a theoretical\nframework. We first present the setup and the analysis of our proposed FocusDD method, followed by the\nempirical validation and the insights.\nOriginal Dataset D. The original dataset, denoted as D, consists of |D| samples, represented by\n{\\D|}\n$D\\{x_i\\}_{i=1}^{|D|}.$\nDistilled Dataset D'. The distilled dataset, D', is created by merging m samples from D based on\nkey regions identified by an attention mechanism such as a Vision Transformer (ViT) and n samples with\nbackground information. This results in D' samples, ${\\D'\\{x_i'\\}_{i=1}^{|D'|}, where |D'| < |D|.\nRademacher Complexity. Rademacher Complexity measures the capacity of a class of functions to fit\nrandom noise, providing a metric for the complexity and generalization capability of hypothesis classes:\n$\\mathcal{R}_D(H) = E_{\\sigma} sup_{h \\in H} \\frac{1}{|D|} \\sum_{i=1}^{|D|} \\sigma_i h(x_i),$ where \u03c3i are independent random variables taking values +1 or -1 with equal probability. We apply this\nmetric when evaluating the distilled datasets because it can provide insight into whether the distillation\nprocess preserves the richness of the hypothesis space or if it overly simplifies the dataset, potentially losing\nimportant variances needed for higher generalization."}, {"title": "Impact of Dataset Distillation of FocusDD", "content": "For the distilled dataset S', the Rademacher Complexity becomes:\n$\\mathcal{R}_{D'}(H) = E_{\\sigma} sup_{h \\in H} \\frac{1}{|D'|} \\sum_{i=1}^{|D'|} \\sigma_i h(x'_i).$\nEach distilled data instance \u017e\u2081 = concatenate({\\{x^{k}\\}_{k=1}^{m}, {x'^{k}\\}_{k=1}^{n}), where x* represents the key sub-region\ndata and x' means the down-scaled low resolution data with backgound information.\nNote that the term 1/|D'\u2758 determines the scaling of the sum of fits to random labels (noise) in the\nRademacher Complexity formula. When analyzing a dataset that has undergone distillation to produce D',\nwhere each sample i aggregates the informational content of multiple samples from the original dataset, the\nactual number of samples |D'| might not accurately reflect the dataset's complexity. Instead, the Efficient\nSample Size (ESS) (Elvira et al., 2022) is applied to represent the number of independent observations in\na dataset that would provide the same amount of information as the actual dataset, which can be noted\nas D'eff. If Deff represents a more accurate measure of the independent information content in D', the\ncomplexity measure can be adjusted to:\n$\\mathcal{R}_{D'}(H) = E_{\\sigma} sup_{h \\in H} \\frac{1}{|D'_{eff}|} \\sum_{i=1}^{|D'|} \\sigma_i h(x'_i).$\nThis adjustment recognizes that the effective diversity and informational independence in D' might be greater\nthan simply counting D', hence potentially leading to a more accurate estimation of how the hypothesis\nclass H will perform.\nThe complexity induced by each new sample i can reduce the variance among samples, as they inherently\nrepresent a more uniform distribution of the key features and contexts of the original dataset. The formula\nfor Rademacher Complexity has to consider the effective sample size Deff that accounts for this aggregation:\n$|D'_{eff}| = |D'| \\times (m * \\gamma + n * \\beta),$ where y and \u1e9e represent the degression parameters due to selecting only the key regions or using down-scaled\ndata, which range from 0 to 1. The setting y = \u03b2 = 1 means that we naively concatenate m + n original data\ninstances.\nSimilarly, we can determine i and Deff for two baseline methods as shown in Table. 18: Na\u00efve and\nRDED (Sun et al., 2024). A higher |Def indicates that each sample in D' contains more \"independent-like\"\ninformation than initially apparent, suggesting that D' may exhibit a lower Rademacher Complexity than"}, {"title": "Remarks", "content": "The proposed distillation method, FocusDD, is expected to enhance generalization by utilizing more informative\nand representative samples. The associated reduction in Rademacher Complexity indicates a diminished\ncapacity for fitting random noise, which typically suggests improved performance on unseen data.\nThe practical implementation may encounter challenges, such as increased computational overhead from\nprocessing larger values. Additionally, there is a risk of information redundancy if the parameters m and\nn are not optimally selected."}, {"title": "Sample Visualizations of Synthetic Data", "content": "Fig. 10 presents visualization examples of object detection training samples generated by FocusDD. Fig. 11\nfurther compares FocusDD-compiled images at different resolutions, showing that as resolution increases,\neach image patch transitions from capturing only parts of objects to representing entire objects. This trend is\nquantified in Fig. 5, which also highlights a corresponding improvement in accuracy. In Fig. 12, we compare\nthe Tiny-ImageNet samples compiled by SRe\u00b2L (Yin et al., 2024), SCDD (Zhou et al., 2024), GVBSM (Shao\net al., 2023), RDED (Sun et al., 2024), and FocusDD. To provide a more comprehensive perspective, Figs. 13\nand 14 present visualizations of compiled samples from ImageNet-1K. Our compiled data, cropped directly\nfrom real image target areas, demonstrates superior realism in texture, shape, and detail compared to SRe2L,\nSCDD, and GVBSM. Unlike RDED, our method incorporates a low-resolution background in the compiled\nimages, enriching them with additional semantic information. These results collectively demonstrate the\nhigher quality of our compiled data."}]}