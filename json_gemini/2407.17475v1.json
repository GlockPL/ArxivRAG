{"title": "An Approach to Detect Abnormal Submissions for CodeWorkout Dataset", "authors": ["Alex Hicks", "Yang Shi", "Arun-Balajiee Lekshmi-Narayanan", "Wei Yan", "Samiha Marwan"], "abstract": "Students' interactions while solving problems in learning en-vironments (i.e. log data) are often used to support students'\nlearning. For example, researchers use log data to develop\nsystems that can provide students with personalized prob-lem recommendations based on their knowledge level. How-ever, anomalies in the students' log data, such as cheatingto solve programming problems, could introduce a hiddenbias in the log data. As a result, these systems may provideinaccurate problem recommendations, and therefore, defeattheir purpose. Classical cheating detection methods, suchas MOSS, can be used to detect code plagiarism. However,these methods cannot detect other abnormal events such asa student gaming a system with multiple attempts of sim-ilar solutions to a particular programming problem. Thispaper presents a preliminary study to analyze log data withanomalies. The goal of our work is to overcome the abnormalinstances when modeling personalizable recommendations inprogramming learning environments.", "sections": [{"title": "1. INTRODUCTION", "content": "Students cheating to submit programming solutions is acommon occurrence. Cheating can be of any kind - copyingsolutions to the problem available online, by other studentslearning programming with the course or by other means ofplagiarism. Generally, researchers have explored methodsto curb cheating in the context of academic integrity [1].Some techniques that could work [6] include the detectionof collusion and continual feedback to students to encouragethem towards better academic integrity. There is a tendencyfor students to cheat when solving programming puzzles orpractice assignments. When online log data is collected us-ing the interaction logs of the interfaces for programmingassignments, there is a risk for some of these anomalies tobe recorded among regular student interaction logs. Thiscould potentially affect student modeling approaches thatuse the interaction logs to make recommendations for stu-dents [3].Student modeling in the context of solving programmingassignments like the Normalized Student Modeling for Pro-gramming [4] use Error Quotient and Watwin score thatmeasure changes help estimate student knowledge or under-standing [4, 7]. In other cases, student modeling facilitatesthe identification and prediction of students' learning pro-files in tutoring systems, which, in turn, enables such sys-tems to be adaptive and personalized to students' needs [10].This makes them sensitive to the quality of the data andanomalies created by students gaming the system or cheat-ing / plagiarizing solutions may cause the model to overesti-mate or underestimate student knowledge or understandingof the introductory programming concepts.For example, a study conducted by Hellas et al. found in-stances where students copied content to complete their as-signments [5]. This behavior can significantly compromisethe quality of student modeling approaches applied to thesedata. Moreover, these cheating instances may lead to erro-neous predictions, revealing a threat to the field of studentmodeling technology.Another example discussed by Sosnovsky and colleagues [9]discusses student modeling anamolies observable as suddenchanges in the learning rate of a student when learning withan adaptive educational system. This could be attributedto any form of assistance offered to the student by a moreexperienced or knowledgeable peer indicated Low-High-Lowor High-Low-High patterns in the student's learning rate.To address this challenge, researchers have developed toolsfor detecting plagiarism in students' code (e.g., [2]). Oneof the most popular approaches is \"The Measure Of Soft-ware Similarity (MOSS)\", an open-source tool designed toidentify similarities between students' programming assign-ments [2]. However, to our knowledge, there is no evidencethat researchers apply cheating detection methods on onlineshared data before applying log data analysis and studentmodeling.We present a work in progress, where we look into this aspectclosely in order to mitigate anamolies in student submis-sions: 1) using classical methods like Measure of SoftwareSimilarity (MOSS), 2) alternative approaches of analyzinglog data). We use the CodeWorkout (CWO) programmingdataset (as introduced in [8])\u00b9. While the use of generativeAI has been very popular now, this dataset was collectedbefore 2021 when Generative AI was not generally used tocheat when submitting programming solutions."}, {"title": "2. METHODS & ANALYSIS", "content": "In this work, we compare two ways to analyze abnormalsubmissions:Proposed method: Log Data Analysis. We used two mainidentifiers to explore anomalies such as suspected cheatingbehaviors from submission log data: the number of sub-mission attempts before completing the exercise, and theelapsed time between correct submissions. The choice ofthese variables correlates with the possibility that studentswho attempt and submit a correct solution on their first at-tempt could be cheating. We discuss more details on thisbelow.Baseline method: MOSS. MOSS is a tool used to detectcheating in programming submissions. The tool works bytaking into all the students' submissions and comparing thempairwise for similarities. We compared students' code sub-missions using MOSS to identify similarities in submissionsfor a selected set of problems from a collection of easy,medium, and hard assignments made available on CWO."}, {"title": "3. RESULTS & DISCUSSIONS", "content": "We further evaluated whether accessible and common cheat-ing detection tools such as MOSS can be applied to detectstudents' cheating in this dataset. However, we found thatrunning MOSS across CWO exercises led to high rates ofsimilarity on a majority of students' submissions. In addi-tion, we found no clear difference between students whom wepreviously identified and those whom we believe that haveengaged authentically with the CWO exercises. We hypoth-esize that this failure could be due to the size of the solutionsto several CWO exercises. Some solutions to these exercisescould be just 10 lines of source code as these problems arewell-constrained and target specific learning goals. Hence,these problems may not have possible alternative solutions(refer Figure 1). Students like those in the example may endwith 93% of their solutions matching despite no indicationsof anomalous behaviour. This indicates that identifying anacceptable threshold for MOSS detection on CWO exercisesis unreasonable and highlights the need for other options."}, {"title": "3.2 Log Data Analysis Detection", "content": "We calculate students' \"one shot\" percent, or the percent ofCWO exercises where a student correctly answers an exer-cise on their first attempt. In Table 1, this is represented asthe one_shot column and is calculated as a correlation withthe student's final course grade. Once this value was calcu-lated, we were able to compare the differences between thecorrelations on a student's first score on a given problem tohow often they were getting their first attempt fully correctand found a suspicious difference. Figures 2 shows the rela-tionship between the first scores of the students' submissionto the exercises and their final exam scores, and the figureon the right shows the distribution of the first scores of thestudents' submission. While many students perform well ontheir first submissions of exercises, showing their mastery ofprogramming skills, only a small subset match this perfor-mance in the course as a whole. Specifically, students whoperform well in the CWO exercises on their first attempt,often do not perform well for their final grade of the course.This preliminary data analysis did not make intuitive senseand led us to further investigate this phenomenon using moretraditional methods, including MOSS."}, {"title": "4. LIMITATIONS AND FUTURE WORK", "content": "This preliminary investigation focused only on CWO sub-missions, but we hope this data cleaning approach can begeneralized to other datasets that use the ProgSnap2 format.We also hope to continue investigating the metadata aboutsubmissions included in this format to find more accurateindicators of cheating behavior in the programming snap-shot data. While MOSS is generally used to compare finalstudents' submissions with other final students' submissions,in future work, we will consider the case for running MOSSwith sequential data where submissions made on platformslike CWO that allow multiple submissions. For example, wecould compare attempt 1 of a student 1 with attempt 2 ofstudent 2 and so on to see if a students copy each others'solutions from their first attempt onwards or after tryingmultiple attempts, failing and then cheat to proceed to thenext programming problem on the CWO platform."}]}