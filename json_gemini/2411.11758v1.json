{"title": "The Power of Many: Multi-Agent Multimodal Models for Cultural Image Captioning", "authors": ["Longju Bai", "Angana Borah", "Oana Ignat", "Rada Mihalcea"], "abstract": "Large Multimodal Models (LMMs) exhibit impressive performance across various multimodal tasks. However, their effectiveness in cross-cultural contexts remains limited due to the predominantly Western-centric nature of most data and models. Conversely, multi-agent models have shown significant capability in solving complex tasks. Our study evaluates the collective performance of LMMs in a multi-agent interaction setting for the novel task of cultural image captioning. Our contributions are as follows: (1) We introduce MoSAIC, a Multi-Agent framework to enhance cross-cultural Image Captioning using LMMs with distinct cultural personas; (2) We provide a dataset of culturally enriched image captions in English for images from China, India, and Romania across three datasets: GeoDE, GD-VCR, CVQA; (3) We propose a culture-adaptable metric for evaluating cultural information within image captions; and (4) We show that the multi-agent interaction outperforms single-agent models across different metrics, and offer valuable insights for future research. Our dataset and models can be accessed at https://github.com/MichiganNLP/MosAIC.", "sections": [{"title": "Introduction", "content": "Large Multimodal Models (LMMs) demonstrate remarkable performance across various multimodal tasks. Despite these achievements, their effectiveness in cross-cultural contexts remains limited due to the predominantly Western-centric nature of most data and models (Hershcovich et al., 2022; Bhatia et al., 2024). Conversely, multi-agent models have proven to be highly capable, often excelling in solving complex tasks (Guo et al., 2024). In this paper, we propose to evaluate and analyze the collective performance of LMMs as multi-agent models in the novel multimodal task of culturally enriched image captioning.\nCulture is a complex and elusive concept. As Adilazuarda et al. (2024) show, Culture is a multifaceted concept meaning different things to different people at different times. This complexity is apparent in various cultural expressions, such as proverbs, social norms, and other context-dependent elements. In our work, we adopt the definition provided by Nguyen et al. (2023) and focus on visual cultural elements such as food, drinks, clothing, traditions, rituals, and behaviors.\nCulture is strongly tied to our group-oriented human nature, which allows us to learn from one another over generations. Furthermore, as sociologists and anthropologists have demonstrated, our progress as a species is primarily due to our cooperative nature, rather than individual knowledge (Henrich, 2015). Inspired by the success of human collective intelligence, we conceptualize the culturally enriched image captioning task as a \"social task\". Specifically, we frame it as a di-"}, {"title": "Related Work", "content": "Large Multi-Agent Multimodal Models. The inspiring progress of Large Language Models (LLMs) has led to the proposal of LLM-based multi-agents that leverage the collective intelligence and specialized skills of multiple agents (Guo et al., 2024). In this context, multiple independent agents discuss and make decisions, mirroring the cooperative human nature. This approach has facilitated progress on various tasks such as software development (Hong et al., 2023), society simulation (Park et al., 2022), game simulation (Xu et al., 2023), debate simulation (Chan et al., 2023), and polarization (Ohagi, 2024).\nAt the same time, Large Multimodal Models (LMMs) have extended the capabilities of traditional language models by integrating several data modalities such as text, videos, and images. LMMs such as LLaVA (Liu et al., 2023), GPT-4 (OpenAI, 2023) or LENS (Berrios et al., 2023) have shown promising results in complex vision-language tasks due to their pretraining on terabytes of image and language data with billion-parameters (Bai et al., 2023; Zhang et al., 2023a) To the best of our knowledge, our study is the first to employ LMMs in a multi-agent setting for a cross-cultural multimodal understanding task.\nCross-Cultural Multimodal Understanding. Even though LLMs and LMMs are already instrumental in various real-life applications, such as recommender systems (Li et al., 2023b) and customer service (Pandya and Holia, 2023), these models often mirror Western-centric perspectives, leading to reinforcing stereotypes and algorithmic monoculture (Kleinberg and Raghavan, 2021; Hershcovich et al., 2022; AlKhamissi et al., 2024).\nSeveral efforts have been made in the AI community to enhance the diversity of data and models, both linguistically and visually. Specifically, recent language studies have developed cross-cultural benchmarks such as CultureBank (Shi et al., 2024) and NORMAD (Rao et al., 2024) to enhance LLMs' cultural awareness. The vision-language community also has started to focus on creating multilingual, geographically, income, and culturally diverse multimodal datasets such as Dollar Street (Rojas et al., 2022), GeoDE (Ramaswamy et al., 2023a), GD-VCR (Yin et al., 2021), CVQA (Romero et al., 2024), MaRVL (Liu et al., 2021), and WIT (Srinivasan et al., 2021a).\nDespite the increased availability of cultural benchmarks, the current evaluation metrics and methods are not suited to capture cultural information (Awal et al., 2023). Evaluation metrics such as Accuracy or F1 score do not focus on the cultural nuances in LMMs' generations and, therefore, cannot reflect their cultural awareness in practice. Generation-focused metrics such as ClipScore (Hessel et al., 2021), LongCLIP (Zhang et al., 2024), and Completeness score (Zhang et al., 2023b) also do not account for cross-cultural variations. However, more culture-focused metrics are emerging, such as Culture Noise Rate (CNR) (Yun and Kim, 2024), which measures the ratio of cultural words among all words generated in a caption. The cultural words are extracted from a cultural commonsense knowledge base (CCSK), which contains several cultural facets like food, drinks, clothing, traditions, rituals, and behaviors (Nguyen et al., 2023). Our work aligns with Yun and Kim (2024), as both studies address the task of culturally enriched image captioning. However, our approach diverges by focusing on multi-agent settings and evaluating the models based on three culturally diverse benchmarks."}, {"title": "MosAIC: A Framework for Cultural Image Captioning", "content": "We introduce MosAIC, a framework for Multi-Agent Interactions, as shown in Figure 2, to tackle cultural image captioning, a complex task that involves not only describing the visual content of the image but also capturing the cultural elements it represents. The framework consists of a multi-agent model, a cultural benchmark, and evaluation metrics, as described below."}, {"title": "Muti-Agent Interaction Model", "content": "We introduce a multi-agent setup (Figure 3) to emulate collaboration in a culturally diverse group. Our multi-agent model consists of five agents, each with specific roles: three Social agents, a Moderator agent, and a Summarizer agent.\nModerator. The Moderator agent has two primary tasks. First, it generates questions based on the image to which the Social agents respond. Second, it guides the Social agents to focus on aspects relevant to their cultures, promoting more comprehensive and culturally diverse image descriptions.\nSocial. Each of the Social agents assumes a persona from three cultures: China (C), India (I), and Romania (R). Furthermore, the agents are encouraged to embody a curious persona to facilitate more interaction in their conversation. In the first conversation round, each agent shares their initial description (d) of the given image and a question (q) about the image from the ones provided by the Moderator. In the next conversation rounds, the agents learn from one another, enriching the image description with more comprehensive and detailed content. Specifically, each agent answers the questions addressed by the other agents in the current and previous rounds and asks a new question. For example, in Figure 3 Round 2, agent R answers all the questions from the other agents posed in Round 1 and Round 2. Note that agent R answers more questions than the others as it responds last. To balance the number of questions each agent answers, we randomize the order of agents for each round and image. In the final round of conversation, Figure 3 Round 3, each Social agent summarizes (s) everything learned from the previous rounds, including all the initial image descriptions (d), the questions (q) and the corresponding answers (a) from all agents. The summaries distill the most important information gained from the interaction, helping to condense and focus the key insights.\nSummarizer. The Summarizer agent collects all the summaries from the Social agents and generates a summary representing the final image description.\nAgent Memory. Each agent has its own memory. The Moderator agent generates questions stored in a shared question memory that is accessible to the Social agents. Initially, the Social agents independently analyze the image without memory of/knowing their peers' responses, minimizing potential bias. In the conversation rounds, each Social agent can access the responses from all agents in previous rounds and those preceding them in the current round. Finally, each agent's memory is erased after the Summarizer agent completes the image caption. We also tested a longer-term memory across multiple images but found no performance improvement, likely due to the significant differences between the images."}, {"title": "Cultural Benchmark", "content": "We introduce a new dataset of cross-cultural captions for 2,832 images from three cultures: China, India, and Romania generated by MosAIC and other models. To achieve this, we use images from three geographically diverse datasets: GeoDE, GD-VCR, and CVQA. We provide image captions generated by MosAIC, our top-performing model, alongside LLaVA-13b captions to facilitate comparisons between single-agent and multi-agent approaches. Furthermore, for a subset of the images (25 images per dataset and culture), we provide human-generated captions as described in section 4.1.\nGeoDE. GeoDE (Ramaswamy et al., 2023b) is a geo-diverse dataset for object recognition with crowd-sourced 61,940 images from 40 classes and 6 world regions, namely West Asia, Africa, East Asia, South-East Asia, the Americas, and Europe.\nGD-VCR. GD-VCR (Yin et al., 2021) is a geo-diverse visual commonsense reasoning dataset with 328 cultural and geo-location-specific images from Western, East Asian, South Asian, and African countries.\nCVQA. CVQA (Romero et al., 2024) is a culturally diverse multilingual visual question-answering dataset with 4,560 images from 28 countries across Asia, Africa, South America, and Europe."}, {"title": "Evaluation Metrics", "content": "We employ both automated metrics (alignment, completeness, cultural information) and human evaluation (Turing test and caption correctness) to comprehensively assess the image captions.\nAlignment. We measure text-to-image alignment using the LongCLIP (Zhang et al., 2024). This metric builds on CLIPScore (Hessel et al., 2021), a popular reference-free evaluation metric for image captioning that outperforms existing reference-based metrics (Vedantam et al., 2015). LongCLIP uses a knowledge-preserved stretching of positional embedding to increase the maximum input length of CLIPScore from 77 to 248 tokens.\nCompleteness. We evaluate the completeness of the image captions by calculating the ratio of words mentioned in both the image and the caption to the total number of words (tags) in the image. To generate a comprehensive list of image tags, we use the Recognize Anything Model (RAM) (Zhang et al., 2023b) and expand it with their corresponding synonyms from WordNet (Miller, 1994).\nCultural Information. We propose a new metric to quantify the presence of cultural information in image captions. This approach is inspired by the Culture Noise Rate (CNR) (Yun and Kim, 2024), which measures the proportion of cultural words in image captions. However, given that the captions generated by our model tend to be longer than those from other models, a ratio-based metric like CNR may disproportionately affect performance. To address this, we instead compute the count of unique cultural words in a caption, a length-invariant metric, to better capture cultural specificity. Further, to improve the metric coverage, we generate and include 700 additional cultural words from 14 categories, such as Traditions and Festivals (50 words per category). Human validation (one native annotator per country) confirmed that all GPT-generated words aligned with the provided cultural categories. Our final cultural information metric integrates the filtered cultural terms from CNR with the additional GPT-generated words. This metric is straightforward to compute and adaptable for assessing cultural specificity across various countries."}, {"title": "Evaluation and Results", "content": "We assess the influence of multi-agent interaction on image captioning by comparing our multi-agent interaction model, MosAIC, with single-agent models (BLIP-2, LLaVA-13b) and a human baseline."}, {"title": "Baseline Models", "content": "BLIP-2. BLIP-2 (Li et al., 2023a) leverages frozen pre-trained image encoders ViT-L/14 from CLIP (Radford et al., 2021) and a FlanT5 LLM (Chung et al., 2024) by training a lightweight, 12-layer Transformer encoder in between them. It achieves an impressive state-of-the-art zero-shot performance on image captioning.\nLLaVA-13b. LLaVA-1.5 13b is an end-to-end trained large multimodal model that connects pre-trained CLIP ViT-L/14 visual encoder and the Vicuna LLM (Zheng et al., 2024), using a projection matrix for general multimodal understanding.\nHuman Baseline. To establish a human baseline, we recruited three native annotators from each of three different countries (nine annotators in total). To ensure consistency and facilitate fair comparisons, the annotation guidelines include cultural aspects, as in the model prompts and examples of human-generated captions, as shown in Figure 4. Each annotator creates 75 image captions evenly distributed across three datasets (25 images per dataset). We compute two metrics: the average"}, {"title": "Cross-cultural Interaction Results", "content": "Our results show that multi-agent cross-cultural interaction improves performance in the cultural image captioning task. As shown in Figure 5, MosAIC outperforms non-interaction models and humans in Completeness and Cultural Information, while matching other models in Alignment. These performance trends are consistent with results on human-annotated data (Appendix Figure 8).\nWe hypothesize that MosAIC's similar Alignment performance is due to its longer captions, which hurts the score. Additionally, Alignment penalizes content not directly visible in the image, such as cultural values (see A.3 for details).\nRegarding cultural information, LMMs tend to generate more culture-specific content than humans, driven by exposure to diverse data, lack of personal context, and statistical learning from cultural biases (Li et al., 2024; Mukherjee et al., 2024). However, the Expert-Human outperforms the non-interaction LLaVA-13b model in capturing cultural information (Cultural Info: 15.57 vs. 14.44). Finally, MosAIC, driven by its curious and collaborative cultural personas, outperforms the non-interaction LLaVA-13b model, generating more culturally specific (Cultural Info: 26.01 vs. 14.55) and complete captions (Completeness: 0.41 vs. 0.28)."}, {"title": "Ablation Studies", "content": "MosAIC Setting. Our model, MosAIC, employs CoT prompting and operates through three rounds of conversation (see Figure 3). It functions in a zero-shot learning setting without the need for fine-tuning. We also perform ablation studies to assess MosAIC's performance across various settings: the number of conversation rounds, prompting techniques, fine-tuning, and different cultures and datasets, as shown in Figure 6."}, {"title": "Number of Conversation Rounds", "content": "Figure 6 a) shows that increasing the number of agent conversations from three to four rounds improves Cultural Information (26.1 vs. 31.1) while keeping Alignment and Completeness stable. The slight decrease in Cultural Information from rounds two to three is attributed to the Summarizer's failure to synthesize key cultural aspects, instead concatenating the conversations."}, {"title": "Prompt Techniques", "content": "Given the challenges in achieving cross-cultural alignment between the agents (Ananthram et al., 2024), we experiment with different prompt techniques:\nSimple. This strategy offers straightforward instructions, such as asking a social agent to describe an image and its cultural significance.\nMultilingual. We prompt agents from specific cultures by translating the Simple prompt into the dominant languages of their countries, such as Mandarin Chinese, Hindi, and Romanian. The generated responses are in English for consistency.\nAnthropological. This prompting technique considers emic and etic perspectives, cultural context, socioeconomic background, individual values, personal experience, cultural relativism, spatial and temporal dimensions in a nuanced manner as introduced by AlKhamissi et al. (2024).\nChain of Thought (CoT). CoT prompting involves generating intermediate reasoning steps, mimicking human problem-solving to arrive at a final answer. (Wei et al., 2023). Inspired by multi-modal CoT (Zhang et al.), we guide agents in making detailed image observations.\nInsights. As shown in Figure 6 b), CoT prompting outperforms other strategies, while Anthropological prompting-designed to enhance cultural alignment in LLMs-performs similarly to or worse than Simple prompting. This suggests LMMs need further refinement for effective cross-cultural alignment. Additionally, Multilingual prompting ranks lowest in Cultural Information, likely due to confusion from inputs in three languages, highlight-"}, {"title": "Fine-tuning Impact", "content": "Current LLMs and LMMs struggle to align with diverse global cultures, often reflecting predominantly WEIRD (Henrich et al., 2010) cultural norms (Atari et al., 2023; Ke et al., 2024). To improve our model's cultural alignment, we apply fine-tuning, which has previously shown promise (Li et al., 2024). For fine-tuning data, we utilize the Wikipedia-based Image-Text (WIT) dataset from Srinivasan et al. (2021b). We implement two fine-tuning setups:\n1. We fine-tune a LLaVA-13b model on 9000 WIT images and captions across three cultures, creating two models: the non-interaction LLaVA-13b-ft-all, which only summarizes, and the interaction MosAIC-ft-all, where the fine-tuned agents collaborate.\n2. We fine-tune three LLaVA-13b models, one for each culture, using 3000 WIT images and their corresponding captions. The interactions among these agents yield the multi-agent model MosAIC-ft-specific.\nInsights. Fine-tuning generally enhances Cultural Information (Figure 6 c), with a modest 4-point improvement for the multi-agent model (MosAIC vs. MosAIC-ft-specific) and a more substantial 9-point gain for the single-agent model (LLaVA-13b vs. LLaVA-13b-ft-all), considering the compute-intensive nature of the process. Furthermore, MosAIC outperforms the non-interaction LLaVA-13b-ft-all model (Cultural Info: 26.01 vs. 23.18), underscoring the benefits of multi-agent interaction over fine-tuning. The fine-tuned models show lower performance in Alignment and Completeness, as the fine-tuning primarily focuses on enhancing cultural alignment. Among fine-tuned models, ft-specific setting performs the best as each agent in interaction has specific cultural knowledge about the country they represent."}, {"title": "Performance across Cultures", "content": "Figure 6 d) reveals similar trends across the three cultures: China, India, and Romania. Notably, MosAIC achieves the highest Cultural Information performance across all cultures, underscoring the significance of incorporating diverse cultural perspectives in generating image captions."}, {"title": "Performance across Datasets", "content": "Figure 6 e) shows that Cultural Information is highest for GD-VCR, followed by CVQA, and lowest for GeoDE, which aligns with expectations since GD-VCR and CVQA contain more cultural information than GeoDE. Although MosAIC scores lower than Humans on Alignment, it achieves higher scores for Completeness and Cultural Information across all datasets."}, {"title": "Human Evaluation and Error Analysis", "content": "We assess the human-likeness of generated captions using Turing Test accuracy (Section 3.3). MosAIC scores lower than LLaVA-13b (83.1 vs. 87.9), suggesting MosAIC's captions are more human-like. However, the high overall scores indicate LMMs still struggle to match human captioning, mainly due to stylistic differences, as humans tend to use a more casual, direct style, as shown in the qualitative results (Section 4.5).\nWe evaluate caption correctness (Section 3.3), finding 94.5% of human captions correct, compared to 60.2% for MosAIC and 64.56% for LLaVA-13b. At the dataset level, we observe that MosAIC performs equally or better than LLaVA-13b on GD-VCR (Human - Machine correctness difference (lower is better): 28.5 vs. 28.5) and CVQA (34.2 vs. 37.1). We hypothesize that MosAIC underperforms on GeoDE (40.0 vs. 25.0) because this dataset contains less culturally rich information. MosAIC's lower correctness, compared to LLaVA-13b, may also result from compound hallucinations caused by the interaction of multiple LMMs. Future work can address this issue by making each agent less susceptible to hallucinations, as detailed in the Limitations section. Common errors include incorrect country, object recognition, people counting, and overly general descriptions (examples in Appendix A.5)."}, {"title": "Qualitative Results", "content": "In Figure 7, we compare image captions generated by MosAIC, Humans, and LLaVA-13b for images from China, India, and Romania.\nCompared to LLaVA-13b, MosAIC shows closer alignment with Human captions, capturing more cultural elements. For example, in the Chinese image, MosAIC identifies the giant panda as a national treasure, similar to the Human caption. In the Indian image, both MosAIC and Human captions recognize the religious significance of bells, highlighting MosAIC's greater cultural sensitivity, while LLaVA-13b provides Western-centric descriptions."}, {"title": "Lessons Learned and Actionable Steps", "content": "Our findings reveal the performance of multi-agent LMMs in cultural image captioning, highlighting lessons learned and suggesting steps to enhance cultural richness in future models.\nPrioritize multi-agent models. While LMMs excel in tasks like generation and retrieval, they fall short in cross-cultural performance, even with culture-centric prompting strategies (AlKhamissi et al., 2024). Our findings show that even Simple and CoT prompts in multi-agent LLMs are helpful and outperform Anthropological and Multilingual prompts (Figure 6 b). Additionally, increasing the number of conversation rounds between agents enhances cultural information (Figure 6 a). To further improve cross-cultural understanding, future work should focus on developing equitable frameworks using multi-agent LMMs and cross-cultural benchmarks.\nDevelop efficient cross-cultural techniques. Current approaches to improving cross-cultural understanding in LMMs often rely on fine-tuning. However, we show that interaction-based models outperform fine-tuned, non-interaction models (Figure 6 c), highlighting both the effectiveness and efficiency of multi-agent LMMs. For instance, LLaVa-13-ft-all requires 54 hours on a single NVIDIA A100 GPU to generate captions (12h for fine-tuning on 9000 WIT images and 42h for inference). In contrast, MosAIC completes the task in 47 hours with only inference. Given these findings, future work should focus on multi-agent models to improve sustainability and accessibility.\nLLMs focus on culture; humans focus on correctness. LMMs tend to be more culturally specific in their responses, while humans provide more accurate answers (Section 4.4). The main sources of errors in LMMs stem from poor object recognition and hallucinations-instances where the model generates incorrect or fabricated information. Future work can integrate a state-of-the-art object recognition system to enhance caption accuracy."}, {"title": "Conclusion", "content": "In this paper, we leverage LMM agents interaction to enhance cross-cultural image captioning, introducing MosAIC. We presented a comprehensive analysis of three cultures across three datasets, using various prompting techniques. Additionally, we demonstrate the advantages of multi-agent LMM interactions, comparing their performance with compute-intensive methods like fine-tuning for improving cultural alignment. We also open-source our culturally enriched captions dataset generated by our proposed framework, MosAIC, alongside baseline models. Finally, we create a comprehensive and culture-adaptable metric for evaluating cultural information within image captions. Based on our findings, we share ideas for future work. Our dataset and models can be accessed at https://github.com/MichiganNLP/MosAIC."}, {"title": "Limitations and Ethical Considerations", "content": "The Complexity of Defining and Evaluating Cultural Information. Our approach utilizes multi-agent LLM interactions, where each LLM represents distinct cultural personas based on specific countries. While we explore various prompting strategies and fine-tuning techniques to align the models with different cultural contexts, it is important to acknowledge that culture is an inherently complex and multifaceted concept. Relying solely on countries as proxies for cultural identity oversimplifies the rich variation in cultural experiences and individual perspectives (Adilazuarda et al., 2024). Using country and language information represents only the tip of the iceberg when it comes to capturing cultural diversity. While these factors provide a basic framework for understanding cultural distinctions, they do not fully account for the deeper, more nuanced aspects that define human cultures. We encourage future work to delve into these deeper dimensions, extending beyond simple national or linguistic markers. Important areas to explore include values, attitudes, and biases, which shape individual and collective worldviews.\nMulti-Agent Setup affects Correctness. Multi-agent models are more prone to hallucinations compared to single-agent models due to the compound effect, where errors or hallucinations from one agent can influence and amplify those in other agents. This cumulative effect results in a lower Caption Correctness score. Future research could explore ways to mitigate this issue by making each agent less susceptible to hallucinations. This might involve improving the architecture of individual agents for better accuracy, using grounding techniques or external knowledge to verify information, and creating stronger communication protocols between agents to prevent errors from spreading. These improvements could enhance the overall correctness and reliability of the model's outputs."}, {"title": "Further Assessing the Impact of Conversation Rounds and Fine-Tuning on Cultural Metrics", "content": "While our ablation studies demonstrate that both conversation rounds and fine-tuning lead to enhanced performance on cultural metrics, additional analysis is necessary to evaluate the impact of various configurations (e.g., interaction vs. non-interaction settings). This deeper investigation will allow us to better evaluate how effective each setup is and reveal the key factors behind the observed improvements. Understanding these factors will be essential for refining our approach and enhancing the model's ability to adapt across different cultural contexts."}, {"title": "Limited Cultural Alignment in LLMs", "content": "Cultural alignment in LLMs and LMMs is a well-researched area. While it is not the central focus of our research, we recognize that the prompt engineering techniques and fine-tuning methods we employ may not achieve perfect cultural alignment. This could lead to inconsistencies in how each multi-cultural agent produces responses across different cultural contexts. Additionally, our study is limited to just three countries, each with relatively high representation in the training data, due to the lack of a broader, more diverse set of human evaluations. This limitation highlights the need for more comprehensive validation across a wider range of cultures to ensure better alignment and more reliable cross-cultural performance."}, {"title": "Appendix", "content": "Cultural Information Metric\nCNR's cultural words are derived from the CANDLE commonsense knowledge base (Nguyen et al., 2023), which covers various cultural facets like Food, Clothing, and Traditions. However, we identified generic terms, such as occupations (e.g., \"attorney\"), that lack cultural specificity. Additionally, CNR includes countries outside our focus-Romania, India, and China-and does not include Romania. To refine this, we filtered out occupation-related terms from CNR and utilized ChatGPT (OpenAI, 2024) to generate additional country-specific cultural words (50 words per category). We use the following prompt: Give a comprehensive list of 50 cultural words related to CATEGORY in COUNTRY. Make sure to include words that are related to both traditions and festivals"}, {"title": "Fine-tuning Details", "content": "For fine-tuning, we use the WIT dataset (Srinivasan et al., 2021b). WIT comprises a curated set of 37.5m entity-rich image-text examples with 11.5m unique images across 108 Wikipedia languages. For fine-tuning, we choose Romanian, Hindi, and Chinese languages for Romania, Hindi and Chinese respectively.\nFor fine-tuning LLaVA-13b, we utilize the Transformer Reinforcement Library with LoRA (Hu et al., 2021) enabled allowing for parameter-efficient fine-tuning. We use a 4-bit quantization, which reduces memory consumption, and a 'bf16' precision for training. This reduces memory footprint. We train the ft-specific models for 3 epochs and ft-all model for 5 epochs, with a batch size of 16 and a learning rate of 1.4e-5. Total compute time on NVIDIA A100 is 2.5 GPU hours for each ft-specific model and 6.5 GPU hours for ft-all model."}, {"title": "Results", "content": "On the caption length impact on Alignment score. LLaVA-based image captions can extend up to three sentences, frequently surpassing Long-CLIP's input limit of 248 tokens, negatively impacting Alignment performance. Moreover, this metric penalizes aspects not directly visible in the image, such as cultural context (e.g., traditions, social norms, and values). To mitigate this, we instruct the LLaVA models to focus on describing the image content in the initial sentence and to address cultural elements in subsequent sentences. Conversely, BLIP-2-generated captions are constrained to a single sentence and lack cultural information, leading to higher performance in Alignment. Consequently, Alignment scores should be evaluated alongside the other metrics to provide a comprehensive assessment."}, {"title": "Quantitative Results", "content": "See results on data annotated by humans (Figure 8). The trends are consistent with performance on all"}, {"title": "Qualitative Analysis", "content": "Across Different Datasets. For relatively simple datasets (Figure 9) with minimal cultural content or complex scenes, both MosAIC and LLAVA-13b exhibit performance comparable to that of human annotators, displaying fewer hallucinations and higher levels of agreement. However, when applied to more complex datasets like GD-VCR10, which consist of movies from diverse cultural backgrounds, MosAIC continues to effectively identify country-specific information and cultural elements, maintaining greater alignment with human annotators. In contrast, LLaVA-13b tends to deviate from human-like behavior, generating more hallucinations (e.g., referencing individuals not present in the image).\nAcross Different Conversation Rounds. With only two conversation rounds, MosAIC tends to merely compile the perspectives of three agents from different countries without offering substantial insights, often struggling to identify the correct country information accurately. As the number of conversation rounds increases to four, MosAIC provides more comprehensive cultural information,"}]}