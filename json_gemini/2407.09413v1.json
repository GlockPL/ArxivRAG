{"title": "SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers", "authors": ["Shraman Pramanick", "Rama Chellappa", "Subhashini Venugopalan"], "abstract": "Seeking answers to questions within long scientific research articles is a crucial area of study that aids readers in quickly addressing their inquiries. However, existing question-answering (QA) datasets based on scientific papers are limited in scale and focus solely on textual content. To address this limitation, we introduce SPIQA (Scientific Paper Image Question Answering), the first large-scale QA dataset specifically designed to interpret complex figures and tables within the context of scientific research articles across various domains of computer science. Leveraging the breadth of expertise and ability of multimodal large language mod- els (MLLMs) to understand figures, we employ automatic and manual curation to create the dataset. We craft an information-seeking task involving multiple images that cover a wide variety of plots, charts, tables, schematic diagrams, and result visualizations. SPIQA comprises 270K questions divided into training, valida- tion, and three different evaluation splits. Through extensive experiments with 12 prominent foundational models, we evaluate the ability of current multimodal systems to comprehend the nuanced aspects of research articles. Additionally, we propose a Chain-of-Thought (CoT) evaluation strategy with in-context retrieval that allows fine-grained, step-by-step assessment and improves model performance. We further explore the upper bounds of performance enhancement with additional textual information, highlighting its promising potential for future research and the dataset's impact on revolutionizing how we interact with scientific literature.", "sections": [{"title": "1 Introduction", "content": "Surfacing pertinent information within the context of academic research articles is an essential area of study, as it empowers students and researchers to efficiently address their queries which are naturally triggered when reading a scientific paper. However, existing question-answering (QA) datasets anchored on academic articles are limited in terms of scale. This limitation arises due to the complexity and cost associated with curating questions, as understanding such articles demands domain-specific expertise, a detailed understanding of the topic, and a significant amount of time. Additionally, prior QA datasets in this domain only analyze the abstracts, conclusion and the textual paragraphs of scientific articles, overlooking the wealth of information presented in meticulously crafted figures and tables, and hence, fail to leverage and analyze the rich, multidimensional data embedded in these visual elements, which are crucial for a comprehensive understanding of the research presented.\nNumerous datasets have been curated to evaluate the QA abilities of Large Language Models (LLMs) on various documents, including factual documents, book chapters, news articles and more. However, understanding scientific papers poses unique challenges as the systems must comprehend underlying theoretical implications with domain-specific terminologies and verify claims with experimental results and visualizations. There are also several datasets that focus on the comprehension of standalone science diagrams, mathematical"}, {"title": "2 Related Works", "content": "Question-answering on long documents is a challenging real-world task that has attracted increasing attention in recent years, following the success of the long-context reasoning ability of LLMs. Though there exist numerous general-domain document QA datasets, understanding scientific papers requires domain-specific expertise and reasoning capability, and hence poses a more significant challenge.\nDatasets for QA on Scientific Papers. In the early days, cloze-style academic paper QA datasets were automatically constructed by extracting entities and relations and matching them with structure knowledge resources. The questions in such datasets follow a pre-defined format; hence, they are unsuitable for real-world usage where the reader asks detailed open-ended questions. To overcome these issues, PubMedQA , BioAsq and QASPER construct corpora of 1K, 3.2K, and 5K human-written questions, respectively. However, the annotators in these datasets only read the abstracts when writing the questions, and hence, most questions are simple and can be answerable in yes/no format or with short extractive spans. ArgSciChat proposes a dataset of argumentive dialogues between scientists on 20 NLP papers. Closer to our work, QASA generates 1798 free-form advanced questions on AI/ML papers where the annotators read the whole paper. However, QASA questions are answerable just from the text paragraphs, neglecting the structured information present in terms of figures and tables.\nDatasets for QA on Scientific Diagrams. Solving mathematical problems in a visual context has emerged as a complex reasoning task for MLLMs. Prior attempts, such as GeoQA , UniGeo and Geometry3K , have exclusively focused on solving geometry-oriented questions. In a different line of research, datasets like DVQA , LEAF-QA , LEAFQA++ , FigureQA , PlotQA , and ChartQA have been constructed to solve plot and chart-oriented questions. Additionally, there are datasets for QA purely on tabular data, including WTQ , TableQA , SQA , HiTab , AIT-QA , FetaQA , MultiTabQA . More recently, MathVista , and MathVerse have integrated different scientific diagrams to develop benchmarks with a wider variety of tasks. However, all existing datasets focus on asking questions about standalone figures or tables. SPIQA bridges this gap by proposing the first scientific QA benchmark that includes questions that require simultaneous reasoning over figures, tables, and textual paragraphs, allowing a more integrated understanding of scientific documents."}, {"title": "3 SPIQA Dataset and Tasks", "content": "Existing scientific QA benchmarks primarily focus only on text from the main body of the paper, overlooking the wealth of information presented as figures and tables. Further, curating QA anchored in research articles requires domain expertise and a detailed understanding of the paper, making annotations expensive and resulting in smaller-scale data. Our dataset, SPIQA, bridges this gap by systematically curating a large-scale QA benchmark focusing on every aspect of scientific research documents - main text, figures, tables, and their captions, and thus pushing AI systems towards a robust understanding of research articles. Moreover, we annotate which figures and tables help answer a question to evaluate the grounding and CoT reasoning capabilities of large multimodal systems.\nWhile collecting and annotating the SPIQA benchmark, we adhere to the following collection guidelines: (i) We identify 19 different top-tier academic research conferences covering a wide variety of computer science domains where the papers are licensed permissively. (ii) We collect 27K PDFs and corresponding TeX sources of research papers presented in those conferences between 2018-2023. Using peer-reviewed articles helps us to maintain the quality of SPIQA. The TeX sources provide high-resolution figures and table TeXs, which can not directly be extracted from the PDFs. (iii) We curate questions of varying levels of difficulty based on the collected papers, requiring robust long-context understanding of different kinds of figures and tables associated with their captions and the main body of the paper. (iv) Lastly, we identify subsets of questions in two existing datasets, QASA and QASPER , which require understanding of figures and tables with the main text,"}, {"title": "3.1 Collection Guidelines and Task Formulation", "content": ""}, {"title": "3.2 SPIQA: Data Collection, Question Generation and Filtering", "content": "Collection of Paper PDFs and TeX Sources. We begin by collecting a large corpus of scientific research papers across all domains of computer science, focusing on open-source publications. We identified 19 top-tier conferences, as detailed in Figure 2, and gathered the paper PDFs published at these venues between 2018 and 2023. We then filtered this collection to include papers whose TeX sources could be downloaded using the python arXiv API\u00b9. This process resulted in 25,859 peer-reviewed papers with corresponding TeX sources, which provide high-resolution figures, table TeXs, and the main body of the articles. We additionally use PDFFigures 2.0 to crop figures from the paper to account for figures that are missed when processing the TeX sources. Overall, we gather 152,487 figures, 117,707 tables, and their captions from 25,859 papers with corresponding main text. It is worth noting that due to our structured step-by-step paper collection strategy, we will have the opportunity to expand SPIQA in the future with papers published in later years and in other open-sourced research fields.\nAutomatic Question Generation and Filtering. After gathering the main text, figures, and tables from research papers, the next step is to generate high-quality question-answer pairs that cover all aspects of the articles. Manually annotating quality questions requires domain expertise and a deep understanding of the research papers, resulting in existing human-annotated datasets being small-scale or solely based on abstracts. To bridge this gap, we automatically generate QAs by leveraging recent advances in powerful multi-modal large language models. We first conducted a pilot study by selecting 30 papers from various domains and experimenting with multiple models. In our approach, we presented one figure or table to the model, along with passages referencing the figure. We then asked the models to generate a question, answer, and rationale that requires a holistic understanding of the figure or table in the context of the paper. We manually verified the quality of"}, {"title": "3.3 Dataset Analysis", "content": "Splits & Statistics. The main statistics of the collected papers, generated questions, and data splits are presented in Tables 1 and 2. SPIQA encompasses 25,859 computer science papers published in top-tier conferences between 2018-2023, containing 152,487 figures and 117,707 tables. Figure 2 shows the distribution of figures and tables in every paper. We generate a total of 270,194 QAs focusing on the figures and tables along with the main text of the papers. The questions and answers, on average, contain 12.98 and 14.56 words, respectively. Notably, we observe high variances in their lengths - 20.47 for questions and 243.29 for answers - indicating a diverse range of patterns in SPIQA. The training, validation, and test-A splits include papers from every source conference and ensure that questions from the same paper remain in the same split. The test-B and test-C splits are generated from QASA and QASPER and contain human-written QAs. Examples from the dataset, illustrated in Figure 4, highlight two different questions centered on different types of figures. Additionally, we provide a comprehensive comparison of SPIQA with existing scientific QA datasets in the supplementary materials.\nGranularity. We divide the collected figures in four broad categories - schematic diagrams, plots & charts, visualizations and others. Table 2 presents number of figures in each category across all splits. Table 5 reports performance of baseline models on all types of figure and tables from test-A."}, {"title": "4 LLMLogScore (L3Score): An Improved Metric for Free-form QA", "content": "Evaluating free-form QA is challenging because the answers are often descriptive and lack a predefined format. Current LLMs generate varied and detailed responses that may appear different but are still accurate, which traditional QA evaluation metrics such as BLEU and ROUGE often fail to capture. Inspired by the ability of LLMs to evaluate natural language generation (NLG) , three recent approaches, LAVE , LIMA , and Prometheus-Vision , utilize the in-context capability of instruction-tuned LLMs to rate candidate answers on 3, 6, and 5-point scales, respectively. However, these metrics are highly sensitive to the chosen scale range and do not consider the LLM\u2019s confidence in the provided ratings.\nWe alleviate the necessity of a predefined scale range and detailed interpretation of every point in the scale by proposing LLMLogScore (L3Score), which directly uses the log-likelihood probabilities generated by an LLM for evaluating the answers. We use GPT-4o as our LLM, show the model the candidate and the ground-truth answers, and ask if the semantic meaning of the candidate and the ground-truth are similar in the context of the question. The LLM is expected to answer yes or no in a single word. We show the exact prompt used for calculating L3Score in the supplementary. Next, instead of considering the final response by the LLM, we look into the top-5\u00b3 log probabilities and corresponding tokens, and we define the L3Score metric as follows:\nL3Score = softmax(x)yes = \\frac{exp(l_{yes})}{exp(l_{yes}) + exp(l_{no})} (1)\nexp() is the exponential and softmax() is the softmax operation, lyes and lno are the log probability for the tokens 'yes' and 'no' and x represents the vector [lyes, lno]. Essentially L3Score renormalizes the probabilities of tokens 'yes' and 'no' to sum to 1. However, due to the caveat that we use a closed model, only the top-5 log probabilities are available to us, hence we may need to approximate the log probability if one or both tokens are missing from the top-5. We do it as follows:\n1. If neither lyes or lno is in the top-5, L3Score = 0.\n2. If only one of lyes or lno is present, then we approximate the lx for the complementary missing token (denoted xc), by considering the minimum (min) of the token with the lowest probability (Plow) in the top-5 vs. log of the probability mass remaining (prem) excluding the top-5."}, {"title": "5 Experiments", "content": "We evaluate six state-of-the-art closed-source and six open-source models on the test sets of SPIQA for three different task setups: direct QA with figures and tables, direct QA with full paper, and CoT QA. For the long-context models like Gemini , GPT and Claude 3, we input all the images together and ask the model to answer questions. Since most open-weight models can only take one image in a single query, we ask the question and show every image one by one in a multi-turn setup, and then the model answers. We use full-resolution images for models with high context length and 224px images for low context length. For evaluating the free-form answers, we report five different metrics for comprehensive analysis - METEOR , CIDEr , ROUGE-L , BERTScore F1 and the proposed L3Score. For the CoT QA task, we also report the top-1 accuracy for retrieving the helpful images to answer the question. We further elaborate on the importance of comprehending the captions with the figures and tables and report granular results on different figure subcategories.\nFine-tuning Details. We fine-tune two open-sourced models, InstructBLIP and LLaVA 1.5 on SPIQA training set with simple QA prompts, where every training sample contains one reference image, corresponding question and answer. We initialize both models from the publicly available checkpoints and train them for two epochs with LoRA of rank 32. We use AdamW optimizer, a cosine scheduler with a linear warmup for the first 3% steps, a peak learning rate of 1e-5, and a batch size 32. Each training job takes about 4 hours on 8 A6000 GPUs. We do not use the main text from papers during training, as InstructBLIP and LLaVA have a low context length of 2048. We resize the images to 224px for training. The trained models are evaluated for direct QA with figures and tables."}, {"title": "5.1 Experimental Seutp", "content": ""}, {"title": "5.2 Main Results", "content": "We highlight the highest scores among closed and open models in every table with red and blue, respectively, and indicate the performance improvements by fine-tuned models with \u0394.\nDirect QA w/ Figures and Tables. Table 3 reports the performance of all open, closed, and fine-tuned models for direct QA with figures and tables. We also use the captions with the images in"}, {"title": "5.3 Ablation Study", "content": "Figure and Table Captions. In scientific papers, figures and tables are often accompanied by detailed and informative captions. Figure 3 shows that all Gemini and GPT variants experience significant performance drops when captions are excluded. For instance, the best-performing GPT-4o model suffers a 2-point L3Score decrease on test-A when captions are omitted, which underscores the importance of captions in providing context and enhancing comprehension in QA tasks.\nPerformance on Tables. Table 5 shows the performance of baseline models on tables and figures in test-A. Open-source models struggle with tables, scoring lower than their overall performance. Among closed-source models, Gemini 1.5 Pro, GPT-4 Vision, and GPT-4o perform well. Notably, GPT-40 achieves a 65.18 L3Score on tables, outperforming GPT-4 Vision, the second best by 8.50 points.\nPerformance on Figure Subcategories. Baseline models perform well on schematic diagrams but struggle with plots, charts, and visualizations. For example, GPT-40 scores 71.38 on schematics but only 56.73 and 56.07 on plots & charts, and visualizations, respectively. Such results indicate the need for improved systems to comprehend scientific diagrams requiring mathematical reasoning."}, {"title": "5.4 Qualitative Results and Error Analysis", "content": "Figure 4 illustrates two example questions, ground-truth answers, and outputs from various baseline models. For the plot-based question, both InstructBLIP and Gemini 1.5 Flash produce correct responses. However, only our proposed L3Score evaluates them correctly in both cases. Notably, InstructBLIP\u2019s response ('Ours') correctly answers the question despite not matching the ground- truth. Traditional metrics like ROUGE-L and BERTScore fail to evaluate such responses, scoring them as 0. For the schematic-based question, the ground-truth is long and descriptive. GPT-4 Vision generates a significantly correct response, and L3Score accurately evaluates it with a score of 96.9. We observe that the best-performing GPT-40 struggles with complex plots, charts, and tables. Such error cases are detailed in the supplementary materials."}, {"title": "6 Conclusion", "content": "We introduce SPIQA (Scientific Paper Image Question Answering), the first large-scale QA dataset specifically designed to interpret complex figures and tables within the context of scientific papers. Additionally, we propose LLMLogScore, an improved metric for free-form QA that accurately analyzes the semantic context of candidate answers relative to the ground truth. Through extensive experiments with 12 prominent foundational models, we evaluate their ability to comprehend the nuanced aspects of research articles. Furthermore, fine-tuning two open-source systems, LLaVA and InstructBLIP, on the SPIQA training set results in significant improvements over zero-shot evaluations, indicating promising avenues for designing specialized systems for scientific QA in the future. Our work lays the foundation for developing advanced QA systems that effectively understand and analyze scientific documents, driving further research in this critical area."}, {"title": "Limitations and Societal Impact", "content": "We acknowledge that SPIQA is designed for research purposes and should not be regarded as a comprehensive dataset, as it is restricted to computer science papers. Models trained on our dataset may exhibit biases towards specific topics within this domain and may not perform well on other scientific literature. Extending SPIQA to encompass other scientific domains remains a future prospect."}, {"title": "A Dataset and Code Release", "content": "Following NeurIPS Dataset and Benchmark Track guidelines, we publicly release the SPIQA dataset on Hugging Face:  Our evaluation and metric computation scripts, along with responses from all closed- and open-source models, are accessible in the GitHub repository: SPIQA will be properly maintained and will publicly remain available under the Creative Commons Attribution License (CC BY 4.0). The evaluation code and library for L3Score are available in the Github repository under Apache 2.0 license. We include the dataset card and README for the resources."}, {"title": "B Additional Ablation Study", "content": "Table B.1: Computation of L3Score using different LLMs. While the absolute values of L3Score vary depending on the choice of LLM, the relative changes in between different models remain consistent. All numbers are for direct QA with figures and tables on test-B.\nL3Score with Different LLMs. Table B.1 compares the L3Score across various LLMs. Fig. F.1 shows the prompt we used to compute the L3Score to evaluate responses from models. We observe that while the absolute L3Score value varies depending on the chosen LLM, the relative changes in the scores remain consistent. For instance, when the L3Score is computed using GPT-3.5 Turbo, GPT-40 scores 11.44 points higher than Gemini Pro Vision. Using GPT-4o to compute the L3Score, the difference increases to 13.12 points. However, GPT-4o achieves approximately 40% better scores in both cases than Gemini Pro Vision. Different LLMs have varying vocabularies, which can cause slight differences in their log-likelihood probabilities. Notably, L3Score can be computed with any LLM that provides log probabilities for different output tokens. We recommend users consistently use the same LLM across all evaluations to ensure proper score comparison. Except for Table B.1, we always use GPT-4o when calculating L3Score, which is currently one of the most powerful and reasonably affordable publicly available LLMs.\nImage Resolution. Figure B.1 demonstrates the performance improvements of different Gemini and GPT-4 systems when using higher image resolutions. Generally, higher resolution images lead to increased L3Scores. However, larger input images result in more input tokens, making the LLM call more expensive. In our main experiments, we always use full-resolution images for the closed models and 224px images for the open models."}, {"title": "C SPIQA vs. Existing Scientific QA Datasets", "content": "Manually generating free-form, open-ended questions and answers on scientific articles is a demanding process in terms of cost and time, requiring expert annotators with detailed domain-specific knowledge. Many existing scientific QA benchmarks typically follow cloze-style question generation to bypass such costly annotation procedures. This approach removes a named entity from a single sentence, and the task is to guess the missing entity after reading the preceding passage. For example, BioRead uses the full text of unlabeled biomedical articles from PubMed and utilizes Metamap to annotate biomedical entities and generate cloze-style questions. BioRead extracts sequences of 21 sentences from the articles, using the first 20 sentences as a passage and the last sentence as a question. BioMRC improves and cleans the BioRead corpus by avoiding cross-section"}, {"title": "D Additional Dataset Analysis", "content": "As described in Section 3.2, SPIQA consists of 25,859 papers published in 19 different top-tier conferences between 2018 and 2023, covering various domains of computer science. Figure 2 categorizes the source conferences into four broad groups: (i) AI/ML: This category contributes 46% of SPIQA, with conferences such as NeurIPS, ICLR, ICML, AAAI, and IJCAI. (ii) Natural language processing (NLP): Conferences like ACL and EMNLP make up 19% of the dataset. (iii) Computer vision and computer graphics: This category includes CVPR, ICCV, ECCV, WACV, and SIGGRAPH, contributing 17% of the papers. (iv) Other computer science domains: These include information retrieval (SIGIR, CIKM), databases (ICDE), networking (WebConf, NSDI), data mining (KDD), and audio and signal processing (ICASSP), collectively covering the remaining 18% of SPIQA. Figure D.1 illustrates the number of papers from each conference and each year between 2018 and 2023.\nAfter collecting the papers, we generated 270,194 question-answer-rationale triplets using Gemini 1.5 Pro, focusing on the figures, tables, and text of the scientific articles. The average lengths of the questions, answers, and rationales are 12.98, 14.56, and 37.42 words, respectively. We also observed high variances: 20.47 for questions, 243.29 for answers, and 468.91 for rationales. As shown in Figure D.2, approximately 36.62% of answers contain 5 words or fewer, 56.70% contain between 6 and 40 words, and the remaining answers contain more than 40 words. This distribution demonstrates the presence of both direct and descriptive or explanatory QAs in SPIQA. The number of words in questions and rationales follows a long-tail normal distribution. Although we do not use the rationales in our experiments, we are releasing them for future research."}, {"title": "E Additional Quantitative Results", "content": "Tables E.1 and E.2 report the results for three different tasks: direct QA with figures and tables, direct QA with the full paper, and CoT QA, using various BLEU metrics. Similar to Tables 3 and 4 of the main paper, GPT-4o achieves state-of-the-art scores on test-A and test-C. Gemini 1.5 Flash performs particularly well on test-B, achieving a 26.8 BLEU@1 score, which is more than 2 points higher than any other model. Open-source models generally underperform compared to closed-source models, primarily because they are trained on natural images.\nAfter fine-tuning on the training set, both InstructBLIP-7B and LLaVA-1.5-7B show significant performance improvements. InstructBLIP-7B achieves an average BLEU@1 improvement of 13.56 points across the three datasets, while LLaVA-1.5-7B gains an average of 4.22 points BLEU@1 score. Fine-tuning with scientific diagrams enhances these models\u2019 ability to comprehend the questions, highlighting the potential importance of our training set for building powerful, specialized systems for scientific QA in the future."}, {"title": "F Prompt for L3Score Computation", "content": "Fig. F.1 shows the prompt used for computing the proposed LLMLogScore (L3Score) metric based on the log-likelihood of the models responses to binary yes, no questions. We use it to measure similarity of the model predicted answers to a given ground truth answer.\nYou are given a question, ground-truth answer, and a candidate answer.\nQuestion: <question >\nGround-truth answer: <GT>\nCandidate answer: <answer >\nIs the semantic meaning of the ground-truth and candidate answers similar? Answer in one word Yes or No."}, {"title": "G Detailed Annotation Guidelines and User Interfaces", "content": "The goal of the SPIQA test set is to assist the evaluation of multimodal models on robust understanding of research articles. We prompt the LLM (Gemini 1.5 pro) to generate questions based on a given image. The prompt we use is shown in Figure G.1 and G.2. After generating questions on all papers (\u224826k papers, \u2248270k images), we subset 200 papers as test set and filter to retain higher quality questions more pertinent to the research article. In the filtering process, we annotate which figures and tables help answer a question to evaluate the grounding and CoT reasoning capabilities of large multimodal systems. The UI used for annotation is shown in Fig. G.3.\nWe manually verified the quality of the generated question and answer pairs using the following criterion:\n1. Answering the question would require a complete understanding of the figure and its importance in the paper.\n2. The generated answer is correct and to the point.\n3. The question is neither too trivial nor too specific to the figure or table (e.g., avoiding questions like 'What does the blue line in Figure 1 represent?' or 'How many rows are there in Table 2?' for being trivial)\n4. If two or more questions from a paper are similar, keep one.\n5. If the question is entirely based on the passage i.e. cannot be answered from the image, discard the question-answer pair.\n6. If the answer is not clear, e.g., the answer says 'It is hard to answer the question based on the given information' or \u2018The answer is not evident from the given passage', discard the question-answer pair.\n7. If the question-answer pair includes phrases like 'Based on the passage,' modify it because we show all figures and tables to the model at once during evaluation.\nWe initially employed crowd workers at a cost of $22 per hour for filtering the questions. However after a pilot evaluation of 150 questions which were annotated by two different sets of 3 crowd workers and the authors, we found that the crowd workers lacked domain expertise necessary to grasp the nuances in the questions. Example of the pilot UI with the question and response from a crowd worker is shown in Fig. G.4. The filtering of the final SPIQA testA set was done by the authors."}, {"title": "H Qualitative examples of the task and data in SPIQA", "content": "Fig. H.1, H.2 and H.3 show examples of the SPIQA CoT QA task, requiring the analysis of multiple- images when answering questions based on a scientific paper. In the SPIQA dataset, there are on average 10.32 images (figures and tables) per paper. In the CoT QA task, given a question and all the figures and tables, the AI system needs to identify which image is most helpful in answering the question and then provide an answer to the question."}, {"title": "I Error Analysis", "content": "Fig. 1.1 shows examples where all the models retrieve the correct figure (table or image) that helps answer the question. However, in many cases the models do not correctly answer the question. We observe that in the case of tables (represented as images), models have difficulty parsing and comprehending the information and making errors. This highlights room for further improvements for advanced systems in terms of comprehending table content represented as figures."}]}