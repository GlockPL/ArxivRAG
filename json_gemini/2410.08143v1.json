{"title": "DELTA: An Online DOCUMENT-LEVEL TRANSLATION AGENT BASED ON MULTI-LEVEL MEMORY", "authors": ["Yutong Wang", "Jiali Zeng", "Xuebo Liu", "Derek F. Wong", "Fandong Meng", "Jie Zhou", "Min Zhang"], "abstract": "Large language models (LLMs) have achieved reasonable quality improvements\nin machine translation (MT). However, most current research on MT-LLMs\nstill faces significant challenges in maintaining translation consistency and ac-\ncuracy when processing entire documents. In this paper, we introduce DELTA,\na Document-levEL Translation Agent designed to overcome these limitations.\nDELTA features a multi-level memory structure that stores information across\nvarious granularities and spans, including Proper Noun Records, Bilingual Sum-\nmary, Long-Term Memory, and Short-Term Memory, which are continuously re-\ntrieved and updated by auxiliary LLM-based components. Experimental results\nindicate that DELTA significantly outperforms strong baselines in terms of trans-\nlation consistency and quality across four open/closed-source LLMs and two rep-\nresentative document translation datasets, achieving an increase in consistency\nscores by up to 4.58 percentage points and in COMET scores by up to 3.16\npoints on average. DELTA employs a sentence-by-sentence translation strategy,\nensuring no sentence omissions and offering a memory-efficient solution com-\npared to the mainstream method. Furthermore, DELTA improves pronoun trans-\nlation accuracy, and the summary component of the agent also shows promise\nas a tool for query-based summarization tasks. We release our code and data at\nhttps://github.com/YutongWang1216/DocMTAgent.", "sections": [{"title": "INTRODUCTION", "content": "Large language models (LLMs) such as GPT-4 (OpenAI, 2023) have recently demonstrated rea-\nsonable performance on the machine translation (MT) task within the natural language processing\ndomain (Garcia & Firat, 2022; Hendy et al., 2023; Zhang et al., 2023; Siu, 2023; Jiao et al., 2023).\nNumerous studies have been carried out to further unleash LLMs' potential for MT (Ghazvininejad\net al., 2023; Peng et al., 2023; Zeng et al., 2023; He et al., 2024; Wang et al., 2024c). However, the\nmajority of these researches mainly focus on sentence-level translation, operating under the strong\nassumption that source sentences are independent of one another. This isolated approach may fail to\nmodel the discourse structure and overlook the coherence in continuous document texts (Scarton &\nSpecia, 2015; Bawden et al., 2018).\nDocument-level machine translation (DocMT) systems have been receiving growing focus in recent\nyears, which involves the whole document or some part of it to capture more context information to\nguide the translation process (Kim et al., 2019; Maruf et al., 2021). Researchers find that modeling\ndiscourse phenomena (Bawden et al., 2018) while translating the whole document helps increase the\ncoherence and consistency in the generated translation (Maruf & Haffari, 2018; Wang et al., 2017;\nZhang et al., 2018; Tan et al., 2019). Recently, a few studies have proposed to introduce LLMs to\nthe DocMT task, utilizing their inherent context information modeling and long text processing ca-\npabilities (Wang et al., 2023b; Wu & Hu, 2023; Wu et al., 2024a). However, existing DocMT-LLMs"}, {"title": "RELATED WORK", "content": "Document-Level Machine Translation In recent years, studies on DocMT have achieved rich\nresults (Kim et al., 2019; Maruf et al., 2021; Fernandes et al., 2021). These studies can be separated\ninto two categories. Studies of the first group employ a document-to-sentence (Doc2Sent) approach,\nwhere the source-side context sentences are encoded to generate the current target sentence (Wang\net al., 2017; Tan et al., 2021; Lyu et al., 2021). However, these approaches suffer from limitations\ncaused by separated encoding modules of the current sentences and their context (Sun et al., 2022;\nBao et al., 2021), as well as the failure to utilize target-side context (Li et al., 2023b). Studies of\nthe second group employ a document-to-document (Doc2Doc) approach, where the translation unit\nis extended from a single sentence to multiple sentences (Zhang et al., 2020; Liu et al., 2020; Lupo\net al., 2022; Bao et al., 2021; Li et al., 2023b)."}, {"title": "MOTIVATION", "content": ""}, {"title": "MAIN CHALLENGES FOR DOCMT-LLMS", "content": "Due to the maximum context limitation inherent in LLMs, translating a lengthy document in a\nsingle pass becomes unfeasible. A conventional strategy involves segmenting the document into\nsmaller translation windows and translating them sequentially. In our study, we initially leverage\nthe GPT-3.5-turbo-0125 model to translate the IWSLT2017 En \u21d2 Zh test set, comprising 12\ndocuments sourced from TED talks. We employ a window of size $l$ to facilitate document translation,\nwhere $l$ source sentences are simultaneously processed to generate $l$ hypothesis sentences. Once all\nsource sentences are translated, they are concatenated to form the complete target document. The\nprimary challenges associated with DocMT-LLMs arise from the following two aspects."}, {"title": "Translation Inconsistency", "content": "Given a source document $D_s = (s_1,s_2,...,s_N)$ and its correspond-\ning target document $D_t = (t_1,t_2, ..., t_N)$, if there exists a proper noun $p \\in P$ ($P$ denotes the set\nof all proper nouns in $D_s$, including names of people, locations, and organizations), and $p$ appears\nmultiple times in $D_s$, we expect that all occurrences of its translation in $D_t$ should be consistent.\nLyu et al. (2021) propose the Lexical Translation Consistency Ratio (LTCR), a metric that quantifies\nthe proportion of consistent translation pairs among all proper noun translation pairs in the target\ndocument. However, we argue that the translations of the proper nouns are supposed to not only\nmaintain consistency throughout the document but also align their first appearance. This consid-\neration is particularly important for enhancing the reading experience of audiences. Therefore, we\nintroduce the LTCR-1 metric for the DocMT-LLMs, which calculates the proportion of proper noun\ntranslations that are consistent with the initial translation within the document:\n$LTCR-1(D_s, D_t) = \\frac{\\sum_{p\\in P} \\sum_{i=2}^{k_p} \\mathbb{1}(T_i(p) = T_1(p))}{\\sum_{p \\in P} (k_p - 1)}$\n$T_i(p)$ represents the i-th translation of $p$ in $D_t$, and $k_p$ denotes the number of occurrences of $p$ in\nthe document. The indicator function $\\mathbb{1}(T_i(p) = T_1(p))$ returns 1 if the translations $T_i(p)$ and $T_1(p)$\nare identical, and 0 otherwise. The numerator is the number of times the proper nouns appear again\nand their translation remains the same as their first appearance, and the denominator represents\nthe sum of all occurrences except the first one of all proper nouns. To compute this metric, we"}, {"title": null, "content": "initially annotate all proper nouns in the source document using spaCy\u00b9. Subsequently, we utilize\nthe token align tool awesome-align (Dou & Neubig, 2021)\u00b2 to determine the translations of these\nproper nouns in the target document. To mitigate the impact of errors from the alignment tool, we\nintroduce a fuzzy match version of this metric, where two proper noun translations are considered\nconsistent when one is a substring of the other:\n$LTCR-1f(D_s, D_t) = \\frac{\\sum_{p\\in P} \\sum_{i=2}^{k_p} \\mathbb{1}(T_i(p) \\subseteq T_1(p) \\lor T_1(p) \\subseteq T_i(p))}{\\sum_{p \\in P} (k_p - 1)}$\nAs shown in Table 1, translating every sentence separately (window size = 1) causes poor translation\nconsistency. An example is illustrated in Appendix A. Increasing the window size consistently leads\nto higher scores across all three consistency metrics. This suggests that when more sentences are\nprocessed within a single translation pass, the LLM is better able to model discourse phenomena\nand maintain consistent translation of proper nouns throughout the document. However, due to the\ninherent limitations in the context length of LLMs, resolving translation inconsistencies cannot be\nachieved solely by indefinitely expanding the window size."}, {"title": "Translation Inaccuracy", "content": "When employing a large window size for document translation, LLMs\ntend to process the input source sentences as cohesive documents rather than as individual sen-\ntences. As a result, the model prioritizes maintaining the general meaning of the text and loses\ntrack of the detailed information in each sentence. This can lead to undertranslation issues and a\ndecline in translation quality. We utilize two neural metrics to assess the quality of document trans-\nlation. The first is the sentence-level COMET (sCOMET) score\u00b3, for which we utilize the model\nUnbabel/wmt22-comet-da to obtain the scores. The second metric is the document-level COMET\n(dCOMET) score proposed by Vernikos et al. (2022), for which we use wmt21-comet-qe-mqm\u00b3 to\nderive reference-free scores. In calculating this document-level metric, the model encodes previous\nsentences as context rather than encoding only the hypothesis, making this approach more accurate\nfor evaluating document translations.\nAs illustrated in Table 1, an increase in window size correlates with a higher tendency for the LLM\nto omit sentences from the source document, resulting in missing translations in the target document.\nAn example of this undertranslation issue is presented in Appendix A. Additionally, quality metrics\nsuch as SCOMET and dCOMET do not demonstrate a consistent improvement with larger translation\nwindows. Therefore, we conclude that translating documents in batches of several sentences at\na time may introduce translation inaccuracy issues. These concerns are particularly significant in\ncontexts where precise translations are essential, such as in technical manuals or official documents."}, {"title": "WHY USING A DOC2SENT APPROACH?", "content": "Previous experiments indicate that translating a document by processing multiple sentences at once\nmay occasionally result in sentence omissions. Although human translators often translate entire\nparagraphs simultaneously, which can also lead to occasional omissions, their underlying transla-\ntion mechanism is fundamentally distinct from that of DocMT-LLMs. DocMT-LLMs are prone to\nomitting source sentences due to hallucination issues or limited capabilities in handling long texts\neffectively. Therefore, we argue that, at this moment, a Doc2Sent approach offers a more promis-\ning alternative for DocMT-LLMs to produce precise and high-quality document translations. In\nour study, we provide LLMs with contextual information from the document while asking them\nto translate each source sentence separately. Once all sentences are translated, we concatenate the\ntarget sentences to form the final target document."}, {"title": "DELTA: DocMT AGENT BASED ON MULTI-LEVEL MEMORY", "content": "Considering the multi-granularity and multi-scale of key information in the document during transla-\ntion, we introduce DELTA, an online DocMT agent. DELTA employs a multi-level memory stream\nthat captures and preserves critical information encountered throughout the translation process. This\nmemory stream accommodates a wide range of perspectives, spanning from recent to historical, con-\ncrete to abstract, and coarse-grained to fine-grained details. DELTA translate the source document in\na sentence-by-sentence manner while updating its memory in real-time. This approach addresses the\ncontext limitations of large language models and ensures the generation of a sentence-level aligned\ntarget document, thereby preserving both the quality and rigorousness of the translation. The main\nframework of DELTA is illustrated in Figure 1, the algorithm of DELTA is detailed in Algorithm 1,\nand the prompts used for each module are given in Appendix C."}, {"title": "Proper Noun Records", "content": "The first level of the agent's memory component we introduce is a dictio-\nnary called the Proper Noun Records R to store proper nouns $p$ in the document along with their\ntranslations upon first encounter $T_1(p)$ within the document: $R^{(i)} = {(p, T_1(p)) | p \\in s_j, 1 < j <\ni}$, where $R^{(i)}$ represents the state of R before translating the i-th sentence, and the same applies to\nother components. When translating the subsequent sentence $s_i$, the agent consults $R^{(i)}$ to obtain all\nrecorded proper nouns that are also contained in $s_i$: $R^{(i)} = {(p, T_1(p)) | p \\in s_i, (p, T_1(p)) \\in R^{(i)}}$.\nThe Proper Noun Records are continuously updated by an LLM-based component of the agent\nknown as the Proper Noun Extractor $L_{Extract}$. After each sentence is translated, the LLM is asked\nto extract newly encountered proper nouns from the source sentence and their corresponding trans-\nlations from the target sentence $L_{Extract}(s_i, t_i) = {(p, T_i(p)) | p\\in s_i, T_j(p) \\in t_i, \\forall (p', T_k(p)) \\in\nR^{(i)}, p\\neq p'}$ and add them to R."}, {"title": "Bilingual Summary", "content": "Unlike previous studies (Wang et al., 2023a; Lee et al., 2024), our research\nimplements a bilingual summary approach as the second level of the agent's memory component to\naddress the challenges of translating documents with extensive texts on both the source and target\nsides. We maintain a pair of summaries throughout the translation process to enhance accuracy and\nfluency. The Source-Side Summary $A_s$ encapsulates the main content, domain, style, and tone of\nthe previously translated sections of the document. This summary serves to preserve a coherent\nunderstanding of the text's overall context, thereby aiding the LLMs in producing more accurate\ntranslations. Conversely, the Target-Side Summary $A_t$ focuses solely on the main content of the\npreviously translated target text.\nThe pair of summaries are generated by two LLM-based components of the agent: the Source\nSummary Writer $L_{writes}$ and the Target Summary Writer $L_{WriteT}$. These summaries are updated"}, {"title": "Long-Term & Short-Term Memory", "content": "The last two levels of the agent's memory component are\nthe Long-Term Memory and the Short-Term Memory, respectively. These two components are\ndesigned to address the requisite coherence across document-level translations. The Short-Term\nMemory M retains the last k source sentences along with their corresponding translations, where\nk represents a relatively small number: $M^{(i)} = {(s_{i-k},t_{i-k}),..., (s_{i-1}, t_{i-1})}$. This component\nis specifically designed to capture immediate contextual information in adjacent sentences, which is\nthen seamlessly integrated into the translation prompt, serving as the context for the current sentence.\nConversely, the Long-Term Memory N component preserves a broader range of context by main-\ntaining a window of the last $l$ sentences from the source document, with $l$ being significantly greater\nthan k: $N^{(i)} = {(s_{i-1}, t_{i-1}),..., (s_{i-1}, t_{i-1})}$. This allows for the retention of extended coherent\ninformation throughout the document. Before translating a given source sentence, an LLM-based\ncomponent called the Memory Retriever $L_{Retrieve}$ is asked to choose n source sentences that are most\nrelevant to the current translation query: $\\tilde{N}^{(i)} = L_{Retrieve}(s_i, N^{(i)})$. These m sentences with their\ntranslations are subsequently incorporated into the translation prompt as demonstration exemplars."}, {"title": "Document Translator", "content": "We utilize an LLM-based component called Document Translator $L_{Translate}$\nto perform the final translation process. Information from the multi-level memory is integrated\ninto the prompt to support the translator in producing high-quality and consistent translations:\n$t_i = L_{Translate} (s_i, R^{(i)}, \\tilde{N}^{(i)}, A^{(i)}, A^{(i)}, M^{(i)})$. The sentence-by-sentence approach ensures that\nthe resulting target document is consistently aligned with the source document at the sentence level,\neffectively minimizing the risk of missing target sentences. Additionally, this method allows for\nstraightforward evaluation of translation quality using sentence-level metrics such as sCOMET."}, {"title": "EXPERIMENTS", "content": ""}, {"title": "SETTINGS", "content": ""}, {"title": "Datasets & Metrics", "content": "We conduct our experiments on the two test sets. The first is the tst2017\ntest sets from the IWSLT2017 translation task\u00f3 (Akiba et al., 2004), which consists of parallel doc-"}, {"title": "ANALYSIS", "content": ""}, {"title": "Ablation Study", "content": "Table 4 presents an ablation study in the En \u21d2 Zh direction using\nGPT-3.5-Turbo-0125 as the backbone model. When provided with context sentences (Model 2),\nthe model exhibits improved translation quality scores, but no significant enhancement in translation\nconsistency is observed. The introduction of long-term memory contributes to more consistent trans-\nlations (Model 3), as evidenced by an increase of 2.22 percentage points in LTCR-1. Incorporating\nbilingual summaries (Model 6) led to an increase in COMET scores as well as consistency metrics,\nindicating that this component not only enhances translation quality but also reinforces translation\nconsistency. When proper noun records are introduced (Model 7), a slight decrease in sCOMET and\ndCOMET scores is observed, likely due to the perturbation introduced by incorporating additional\ninformation. However, translation consistency improves significantly, with LTCR-1 increasing by\n3.95 points and LTCR-1f increasing by 1.65 points compared to Model 6. Moreover, it is evident that\nthe bilingual summary has a superior impact on both translation quality and consistency compared\nto using a summary on either the source side or the target side alone. Among Models 4, 5, and 6,\nBilingual Summary achieves the highest scores across all four metrics."}, {"title": "Consistency Distance", "content": "One significant challenge in\ndocument-level translation is maintaining long-term\nconsistency. To evaluate whether our approach ad-\ndresses this challenge, we divide the sentence-wise\ndistance between each proper noun's translation and\nits first occurrence into several intervals. We then re-\nport the proportion of consistent translations in each\ninterval in En\u21d2 Xx (upper) and Xx \u21d2 En (lower) in\nFigure 2 using Qwen2-72B-Instruction. We observe\nthat our approach almost outperforms the Sentence and\nContext methods in achieving proper noun translation\nconsistency across all distance intervals. Notably, our\napproach excels when the distances exceed 50 sen-\ntences, yielding a larger proportion of consistent trans-\nlations than the baseline methods. This demonstrates\nthe effectiveness of our approach in enhancing long-\ncontext translation consistency."}, {"title": "Pronoun Translation", "content": "We follow Miculicich et al.\n(2018); Tan et al. (2019); Lyu et al. (2021) to evalu-\nate the accuracy of pronoun translation (APT) of our\nsystem using the reference-based metric proposed by\nMiculicich Werlen & Popescu-Belis (2017). The eval-\nuation results of GPT-3.5-Turbo-0125 in the En \u21d2\nZh direction are demonstrated in Table 5. DELTA can\nimprove the performance of pronoun translation compared to the sentence-level and Context-based"}, {"title": "DELTA as a Summarize Writer", "content": "Our system employs\nan iterative approach to document summarization, where\na summary writer generates partial summaries every 20\nsentences, sequentially merging them with previous sum-\nmaries to produce an updated version. To assess the ef-\nfectiveness of this component, we conduct an experiment\nusing the QMSum (Zhong et al., 2021) test set. QMSum is\na benchmark for query-based multi-domain meeting sum-"}, {"title": "Memory Costs", "content": "Our agent system consumes less mem-\nory than LLM-based Doc2Doc methods, such as those de-\nscribed by Wang et al. (2023b). In their research, mul-\ntiple continuous sentences are translated in a single con-\nversational turn, and the whole document is translated\nwithin a single chat box. However, this approach suf-\nfers from significant memory costs. As shown in Fig-\nure 3, we compared the memory usage of the Doc2Doc\nmethod with our agent-based online approach by utiliz-\ning Qwen2-72B-Instruction to translate a document in\nEn \u21d2 Zh on a device with 2 NVIDIA A800 80GB GPUs."}, {"title": "CONCLUSION", "content": "In this paper, we start with analyzing two critical challenges for DocMT-LLMs, namely translation\ninconsistency and inaccuracy. To tackle these issues, we design an online document-level translation\nagent equipped with a multi-level memory component. This memory structure retrieves and stores\nkey information to assist the document translation process, significantly enhancing translation con-\nsistency and quality. Notably, the effectiveness in maintaining proper noun translation consistency\nis particularly pronounced in novel translation, and our approach is still able to maintain consistency\neven when there is a large distance between the occurrences of a proper noun pair. The sentence-by-\nsentence online translation method avoids sentence omissions and reduces GPU memory consump-\ntion, in contrast to mainstream Doc2Doc approaches. Further analysis indicates that our method\nimproves pronoun translation accuracy, and the built-in summarizer component in our agent is also\ncapable of the query-based summarization task."}, {"title": "LIMITATIONS", "content": "In this work, we present a framework for the DocMT agent, without prioritizing its inference ef-\nficiency. Given the complexity of DELTA's inference process, LLMs are frequently invoked dur-"}]}