{"title": "Object-Oriented Material Classification and 3D Clustering for Improved Semantic Perception and Mapping in Mobile Robots", "authors": ["Siva Krishna Ravipati", "Ehsan Latif", "Ramviyas Parasuraman", "Suchendra M. Bhandarkar"], "abstract": "Classification of different object surface material types can play a significant role in the decision-making algorithms for mobile robots and autonomous vehicles. RGB-based scene-level semantic segmentation has been well-addressed in the literature. However, improving material recognition using the depth modality and its integration with SLAM algorithms for 3D semantic mapping could unlock new potential benefits in the robotics perception pipeline. To this end, we propose a complementarity-aware deep learning approach for RGB-D-based material classification built on top of an object-oriented pipeline. The approach further integrates the ORB-SLAM2 method for 3D scene mapping with multiscale clustering of the detected material semantics in the point cloud map generated by the visual SLAM algorithm. Extensive experimental results with existing public datasets and newly contributed real-world robot datasets demonstrate a significant improvement in material classification and 3D clustering accuracy compared to state-of-the-art approaches for 3D semantic scene mapping.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advancements in mobile robotics have underscored the importance of autonomous navigation and manipulation in unknown environments. A pivotal challenge in this domain is the accurate identification and classification of surface materials of objects, a capability crucial for effective decision-making and interaction within these environments, whether for exploration, manipulation, or clearing tasks [1]. Specifically, the deployment of robots in exploration and mapping applications [2], [3] benefits from accurate perception and understanding of the environment, especially when integrated with SLAM (Simultaneous Localization and Mapping) algorithms [4].\nWhether in domestic settings, firefighting, or logistics, understanding the material composition of surroundings is crucial for preplanning operations and navigating effectively [5], [3]. For instance, distinguishing between concrete and black ice is essential for safely operating self-driving vehicles and service robots. Incorporating material recognition into conventional object recognition, scene understanding, and SLAM pipeline can significantly enhance robot performance, especially in use cases involving physical interactions and realistic renderings in virtual environments [6], [7].\nCurrent computer vision methods for material classification often focus on visual cues (shape and color) from RGB images. However, traditional RGB-based scene-level semantic (material) segmentation methods provide limited insights into the surface properties of the objects. While generally effective, they fall short in providing the nuanced material recognition necessary for robotic tasks, thus motivating the use of depth modality (i.e., RGB+D) for extracting rich features [8]. The challenge lies in effectively utilizing depth maps to predict material types from camera images, which becomes critical in complex and dynamic environments [9]. Therefore, creating a semantic material map using RGB-D images presents a valuable research avenue. Such maps enhance scene understanding and aid in robotic exploration and manipulation. Additionally, point cloud mapping, which creates a 3D representation of the environment, is invaluable for mapping and navigation, as well as in generating detailed models for mixed reality and architectural planning [4].\nMotivated by the need to enhance robotic perception [10], particularly in the context of material classification, we introduce a novel approach to material classification and semantic mapping for mobile robots. Our framework presents a unique result, as illustrated in Fig. 1, which showcases the point cloud output combined with object-oriented material identification and clustering. We integrate our approach into the well-recognized ORB-SLAM2 algorithm [4] leveraging the benefits of visual odometry and SLAM, providing a real-time, accurate global map essential for mobile robots.\nThe key contributions in this paper include:\n\u2022 A novel material classification network through complementarity-aware fusion of RGB and Depth-based convolutional neural networks, built on top of an RGB-based object detection pipeline for fast (real-time) and accurate material classification. The approach takes advantage of the extraction and fusion of distinctive and correlated features from RGB and depth modalities.\n\u2022 Integration of the material classification outcome with"}, {"title": "II. RELATED WORK", "content": "Recent advancements in vision-based SLAM and material recognition have leveraged deep learning to achieve notable progress. The complexity of indoor environments and the diverse material composition of objects therein make this an especially challenging and relevant problem [12], [9], [13]. Mur-Artal and Tard\u00f3s [4] introduced ORB-SLAM2, an efficient SLAM pipeline for monocular, stereo, and RGB-D cameras. It provides high accuracy but lacks semantic understanding. In the domain of material recognition, Qi et al. [14] proposed PointNet++, which directly processes point clouds for 3D classification and segmentation. Although they process point clouds efficiently, these models do not consider the material properties of objects. Our approach complements this by providing material-level semantic information.\nChen et al. [15] developed a progressively complementarity-aware fusion network for RGB-D salient object detection. Their method effectively fuses RGB and depth features but is not tailored for material classification. Our proposed CA fusion module extends this idea to material recognition, enhancing the accuracy of object-material association. Schwartz and Nishino [16] focused on material recognition from a global context, emphasizing the role of local features. Their work, while insightful, does not incorporate depth information, which is critical for distinguishing materials with similar textures. By incorporating depth data, our method provides a more robust solution for material classification in complex scenes.\nIn robot mapping, Zhao et al. [12] implemented a deep learning method for 3D reconstruction and material recognition, yet their approach does not effectively handle dynamic environments. Similarly, the study in [17] achieves simultaneous material segmentation and 3D reconstruction, primarily in static industrial settings, but it falls short in adapting to changing environments that are integral to mobile robots. Our method addresses these limitations with a voxel-based matching component, significantly enhancing the SLAM system's adaptability in dynamic scenarios. This is a critical improvement over existing methods, as it enables accurate real-time mapping and material recognition in environments where conditions and object placements are constantly evolving. Furthermore, works in [6], [18], [19] contributed to integrating object-level semantic information with SLAM to improve localization accuracy through graph-based pose corrections, but these methods do not differentiate different materials. While we do not focus on improving the localization, our approach fills the gap by adding material-level semantic maps with a vision to extend the capabilities of robotic systems in complex, variable settings by providing a detailed and dynamic semantic material map essential for interaction tasks such as grasping and manipulation.\nIn a recent work [20], the authors proposed an online 2D and 3D semantic, modular map representation and object detection framework using RGB-D data over-refinement and likelihood maintenance to avoid false detection. However, the object-based likelihood maintenance mechanism may miss out on semantically important objects that are occluded by other objects and have a low hit-to-miss ratio in likelihood calculation. In our approach, we use a voxel-based feature matching technique, which considers all the objects irrespective of their occurrence frequency.\nIn summary, our approach departs from the above SOTA methods by integrating RGB-D data fusion and clustering object-level results with point clouds for consistent semantic mapping in 3D environments. Leveraging a complementarity-aware (CA) fusion module, our system synergistically combines RGB and depth data, enabling more refined material classification than traditional models that treat these modalities in isolation. This method captures unique visual characteristics and incorporates depth information, crucial for discerning materials with similar appearances under varying lighting conditions. Unlike existing methods that might overlook material attributes in environmental mapping, our approach recognizes and classifies materials with improved accuracy. Additionally, using RGB-D images can improve the system's robustness to lighting changes and provide more accurate representations of the environment in low-light conditions.\nAnother key aspect of our method is incorporating voxel-based matching within the SLAM framework. This component ensures both dynamic and static objects in the point cloud map are correctly identified and associated with their material types. Such an advancement is vital in robotics, where accurate material recognition influences tasks like manipulation or navigation. Our approach extends the capabilities of existing systems like ORB-SLAM2 [4], PointNet [14], and SemanticFusion [6] by addressing their limitations in depth and semantic understanding and maintaining the consistency of semantic mapping in 3D, a feature not fully realized in prior works. Consequently, our approach offers a more robust, real-time semantic material map, greatly enhancing a robot's perception and interaction abilities in complex and dynamically changing environments. As a result, combining object detection and point cloud mapping with RGB-D images can provide a rich and detailed represen-"}, {"title": "III. PROPOSED APPROACH", "content": "The core problem we address is the semantic mapping of materials in a 3D environment captured by a mobile robot. Mathematically, this involves identifying and classifying various materials within the robot's field of view, represented by a 3D point cloud. Let P denote the point cloud, where each point \\(p_i \\in P\\) has associated \\(f_{RGB}\\) color and \\(f_{Depth}\\) depth feature map obtained through fusion mechanisms. The goal is to assign a material label \\(l_i \\in L\\) to each point \\(p_i\\) associated with fused map \\(f_{fuse}\\), forming clusters of points (map) with similar material properties.\nThe proposed solution involves three main pillars. It first detects objects in the environment using a fast object detection pipeline (Sec. III-A), followed by material classification of these detected objects (Sec. III-B). Using these classified features with a 3D point cloud map generated by a visual SLAM (VSLAM) algorithm (e.g., ORB-SLAM2), a feature matching technique described in Sec. III-D is applied to create a comprehensive semantic map of the environment. An architectural overview of the proposed approach is shown in Fig. 2. This integrated approach is designed to enhance the perception capabilities of mobile robots by enabling them to recognize and accurately classify materials in their environment. Algorithm 1 provides the pseudocode description of our semantic mapping of materials from the RGB-D data."}, {"title": "A. Object Detection (OBJ) Pipeline", "content": "Our approach uses an object-oriented pipeline, meaning that the object detections trigger the material classifications. Input RGB images (from an RGB-D camera) are used for object detection, and we build our architecture on top of the SOTA YOLO (You Only Look Once) model [23] because of its fast, robust, accurate, and versatile real-time object detection capabilities. We specifically used the YOLOv5 version, as it had proven to be robust in the ROS framework (for SLAM integrations) with excellent performance on several benchmarks, including the COCO object detection dataset [24]. Regardless, the research community has consistently upgraded the YOLO-based models. For instance, the newer YOLOv8 [25] can replace the YOLOv5 in our pipeline. In"}, {"title": "B. Material Classification Network (MCN)", "content": "Once the objects are detected, each object is cropped from the RGB and the aligned Depth images using the object's bounding boxes. Because of the different image generation mechanisms between RGB and depth images, fusing cross-modal features effectively is a key issue for RGB-D-based material classification. In our work, we exploit the concept of complementarity-aware (CA) fusion proposed in [15] as it effectively merges the distinct features from RGB and depth data, addressing the limitations of relying solely on the visual appearance in RGB images. By processing RGB and depth images independently, our CA fusion for material classification extracts diverse features, capturing various aspects of the material types and enhancing accuracy by fusing distinctive and correlative features from the two modalities (color and depth). While recent works such as [26] have explored the concept of late fusion of classification outcomes from multiple modalities, they might yield some specific patterns in multiple modalities instead of finding shared common modal patterns. In a late fusion, the classifications are obtained from two parallel networks adopted to learn saliency maps from the high-level features of RGB and depth images separately. These are then concatenated to obtain a final prediction map. In contrast, the CA fusion mechanism encourages the determination of complementary information from the different modalities at different abstraction levels.\nTo excavate the complementarity of two modalities and maintain the discriminability of cross-modal features, we use a Complementarity-aware Fusion Network (CAFN). We first model the distinctive features from two modalities, then select complementary information of two modality features in spatial dimension with two symmetry gates. Finally, an element-wise weighting mechanism is conducted to fuse them to capture more discriminative cross-modal features. The fused features retain not only information existing in both modalities but also modality-specific information. This reduces fusion ambiguity and increases fusion efficiency. In principle, CAFN can be extended to include other modalities as well (e.g., depth-aligned LIDAR data).\nIn our implementation, CAFN includes two symmetric backbones for RGB and depth feature extraction and five cascaded fusion modules. We use ResNet-101 as unimodal symmetric backbones similar to [27]. We remove the average pooling and the fully connected layers of the backbone. The last two stages are modified with dilated convolution to maintain feature resolution for more spatial information. Then we use hierarchical features from RGB and depth branches respectively, i.e., \\(\\{F_{RGB}^i | i=1,2,3,4,5\\}\\) and \\(\\{F_{Depth}^i | i=1,2,3,4,5\\}\\) with five cascade layers. Two unimodal features \\(F_{RGB} \\in \\mathbb{R}^{C \times H \times W_i}\\) and \\(F_{Depth} \\in \\mathbb{R}^{C \times H \times W_i}\\) extracting from corresponding backbones are sent to fusion modules, where \\(C_i\\), \\(H_i\\) and \\(W_i\\) refer to the channel, height, and width number of the \\(i\\)th layer respectively. As a result, CAFN can select complementary information from two modalities and then fuse enhanced unimodal features for accurate cross-modal features with the help of multiple cascaded layers.\nLet \\(I_{RGB}\\) and \\(I_{Depth}\\) be the RGB and Depth input images, respectively, and \\(f_{RGB}\\) and \\(f_{Depth}\\) be their corresponding CNN feature maps which can be obtained by element-wise multiplication with their unimodal features:\n\\(f_{RGB} = I_{RGB} \\odot F_{RGB}\\), \\(f_{Depth} = I_{Depth} \\odot F_{Depth}\\),\nherein, \\(\\odot\\) is element-wise multiplication. Further, at each level, the feature maps are fused as \\(f_{fuse} = f_{RGB} \\oplus f_{Depth}\\). Next, a CA attention mechanism is used to highlight the complementary regions of the two feature maps:\n\\(\\mathcal{A}_{RGB} = \\sigma (W_{RGB} * f_{RGB})\\), \\(\\mathcal{A}_{Depth} = \\sigma (W_{Depth} * f_{Depth})\\),\n\\(\\mathcal{A}_{fuse} = \\mathcal{Q}_{RGB} \\oplus \\mathcal{Q}_{Depth} (W_{fuse} * f_{fuse})\\),\n\\(\\alpha = \\frac{\\mathcal{A}_{fuse}}{\\mathcal{A}_{RGB} + \\mathcal{A}_{Depth} - \\mathcal{A}_{fuse}}\\),\nwhere * denotes the convolution operation, \\(W_{RGB}\\), \\(W_{Depth}\\) and \\(W_{fuse}\\) are learnable convolutional filters, \\(\\sigma\\) is the sigmoid activation function, and \\(\\mathcal{Q}_{RGB}\\), \\(\\mathcal{Q}_{Depth}\\), \\(\\mathcal{A}_{fuse}\\) are the attention maps for the RGB, Depth and fused feature maps, respectively, the attention map \\(\\alpha\\) is the normalized"}, {"title": "C. Visual SLAM (VSLAM) Pipeline", "content": "We build our 3D semantic mapping on top of a visual SLAM solution. We employ the ORB-SLAM2 [4] algorithm for VSLAM due to its real-time performance, scalability, and portability advantages. Furthermore, ORB-SLAM2 is designed to handle monocular, stereo, and RGB-D cameras, and it is based on the ORB (Oriented FAST and Rotated BRIEF) feature descriptors and uses a combination of point features and line features for robust and accurate localization and mapping, producing sparse 3D reconstruction. In our pipeline, the SLAM module uses the RGB-D data to obtain the point cloud map of the environment in real time."}, {"title": "D. Voxel Based Matching (VOXM) and 3D Clustering", "content": "The voxel-based point cloud matching component is crucial in our architecture. This module uses the depth information from the RGB-D sensor to estimate the 3D coordinates of the bounding boxes obtained through the OBJ module in the camera coordinate system. Note that both the object detections and the SLAM algorithm's outputs are in the same coordinate system as the RGB-D camera frame. Specifically, this component creates a voxel grid to divide a point cloud into smaller voxels during voxelization. It associates each point in the point cloud with the voxel to which it belongs. Then, it iterates over all the voxels, and for each voxel, it finds the closest bounding box. This step is essential as it enables the efficient processing of the point cloud by reducing the number of points that need to be considered for segmentation and material label propagation.\nLet \\(p_i \\in P\\) be the set of points in the point cloud (obtained from VSLAM, and \\(b_i \\in B\\) be the set of 3D bounding boxes obtained from OBJ, and \\(l_i \\in L\\) be the label of classified material obtained through MCN. The Voxel-Based Matching Component aims to find a mapping between \\(b_i\\), \\(l_i\\), and the voxels of \\(p_i\\). Let V be the voxel grid created by dividing \\(P_i\\) into a 3D grid of small cubes or voxels. Each voxel is assigned a 3D coordinate and contains a set of points from the point cloud that fall within its boundaries.\nThe mapping between \\(b_i\\) and V can be represented as a function \\(M: b_i \rightarrow V\\), which maps each bounding box in \\(b_i\\) to the voxel in V that contains the majority of its points. Once the voxel grid is created, point cloud segmentation is applied in such a way that a static color map \\(C \rightarrow C\\) is used to color the material associated with each classified material label in the output semantic map SM to separate it into different cluster points. To achieve this objective, we leverage the multi-scale connected components (MSCC) algorithm [28], which provides an efficient way to propagate material labels in the point cloud. The MSCC algorithm is applied on the point cloud after it has been matched with the 3D bounding boxes obtained from object detection using the voxel-based matching component. The algorithm is applied at multiple scales, starting from a large scale and gradually reducing it to capture more fine-grained details in the point cloud. At each scale, the algorithm applies connected component labeling to group points that are spatially close and similar. These groups of points are then assigned a unique label, and the scale is reduced until the minimum scale is reached.\nLet \\(S = s_1, s_2,..., s_n\\) be the set of scales. At each scale, \\(s_i\\), the MSCC algorithm performs the following steps:\n1) Divide V into a set of larger voxels at scale \\(s_i\\).\n2) Apply connected component labeling to group points that are spatially close and similar to one another within each larger voxel.\n3) Merge the resulting clusters across adjacent voxels, considering their spatial proximity and similarity.\n4) Assign a unique label to each resulting cluster.\nThe resulting segmentation at scale \\(s_i\\) can be represented as a function \\(Seg(s_i) : V \rightarrow L\\), which maps each voxel in V to its assigned label in the segmentation at scale \\(s_i\\). The final segmentation of V is obtained by merging the segmentations at all scales: \\(Seg = merge(Seg(s_1), Seg(s_2), ..., Seg(s_n))\\). Where merge is a function that merges the labels of overlapping voxels across scales.\nNext, the material labels \\(l_i\\) obtained from CAFN are propagated to each cluster obtained from the MSCC algorithm. This is done by finding the closest bounding box for each cluster and assigning the material label of that bounding box to the cluster. This step is important as it enables the creation of a point cloud map with material labels.\nThis process can be repeated for all segments in the point cloud, resulting in the accurate propagation of material labels to the corresponding segments in the point cloud. The output of the voxel-based matching component is a point cloud map with material labels, which can be used for various applications such as robot navigation, object recognition, and scene understanding. The final output is a semantic map SM represented as a 3D point cloud, where each point is associated with semantic (object type) and material labels. This approach provides a detailed and informative perception of the environment, which is crucial for efficient robot navigation and interaction with its physical surroundings."}, {"title": "IV. EXPERIMENTAL VALIDATION", "content": "We train and evaluate our approach using publicly available RGB and RGB-D datasets and demonstrate the accuracy of the semantic map through real-world mobile robot experiments in our lab setting. We compare various components of our material classification and clustering pipeline with relevant SOTA methods from the literature."}, {"title": "A. Datasets and Model Training", "content": "Given the fact that most object and material classification datasets available are RGB images, we used the benchmark MINC-2500 [29] (RGB) and Flickr Material Database (FMD) [30] (RGB). For RGB and depth network fusion, we used the Washington RGB-D [31] dataset. In our material classification experiments, we grouped the objects together based on their material type. We categorized them into ten material classes: {Cardboard, Ceramic, Cloth, Glass, Metal, Paper, Plastic, Rubber, Sponge, Wood}. The material type predictions with max probability less than 0.5 are considered as an additional \"other\" type. Furthermore, for the semantic mapping objective, we used the TUM RGB-D dataset [32], which is a comprehensive collection of 39 real-world indoor sequences categorized into Handheld SLAM, Robot SLAM, and Dynamic Objects. The YOLOv5 network for object detection has been pre-trained on the COCO dataset [24] and can distinguish between 80 different classes of objects. In addition, we created a custom YOLOv5 model built on top of the pre-trained model by adding new object classes, such as board, door, mat, robot, and trash bin, that are not commonly available in existing datasets (but useful in robotics and navigation context [19]). We collected 500 images from the public internet for each of these additional classes labeled using the LabelImg annotation tool. To verify the effectiveness of our object detection pipeline on these new classes, we compared the \"door\" and \"trash bin\" object detections with [19] in two of their datasets (sequence1-Kinect and sequence3-astra), where we found our pipeline providing superior detection accuracies (e.g., ours 85.9% compared to their's 53.3% for \"door\" object detections).\nWe performed our experiments on a PC with 3 NVIDIA GeForce GPUs. The learning rate, weight decay, and mini-batch size are set to le-5, 0.0005, and 4, respectively. The training procedure used 50 epochs. We evaluate our method on the material classification task using five cross-validation splits. Each split consists of roughly 12,440 training images and 3,920 images for testing. During the test, the task of the model is to assign the correct class label to a previously unseen object instance. During the inference step in our pipeline applied to real-time RGB-D data, the profiling of average per-keyframe processing times at various stages is: {OBJ: 56 ms, MCN: 3.27 ms, VSLAM: 6.34 ms, VOXM: 92 ms, Final label propagation: 8.26 ms, Total: 165.87 ms}. As expected, point cloud segmentation along with YOLOv5 takes significant time, both of which can be upgraded or improved for strict real-time applications."}, {"title": "B. Material Classification", "content": "First, we validate the performance of our MCN against GoogLeNet [33] and VGG_CNN_M [34] networks on the benchmark MINC-2500 [29] and FMD [30] datasets, based only on the RGB images. The comparison of the accuracy results for the common materials (available in the specific dataset) can be seen in Table I. Our approach demonstrates robust and competitive performance in material classification, excelling particularly in recognizing cloth, plastic, and wood across different RGB datasets.\nNext, we present the accuracy of our material classification network on the Washington RGB-D dataset [31], with comparisons to the late fusion scheme [26] as well as schemes that use single-modality (RGB or Depth) inputs. Results in Table II show that our multi-modal CA fusion network outperforms the late fusion scheme with up to 12% improvement in accuracy over all material classes. The late fusion scheme fails to utilize the depth data effectively and, in some cases, performs more poorly than the RGB-only models. In contrast, the CA fusion effectively fused diverse and contrastive features from both the depth and RGB images and thus provides superior accuracy in all classes. Also, we can observe the advantages of adding the depth modality, which provided consistent improvements in the classification accuracy by our CA fusion method on the RGB-D data compared to the RGB-only modality."}, {"title": "C. Point Cloud Segmentation and 3D Clustering", "content": "We used the TUM RGB-D dataset [32] to evaluate the performance of our 3D clustering of material labels with the SLAM integration. Multiple objects are successfully detected and represented as semantic labels on the map with their corresponding material type. We were able to estimate the size of objects accurately using their point clouds, which gives an idea of their general dimensions. As a preliminary evaluation, we qualitatively compare our work to the closest relevant work [18] that performs 3D object segmentation on the point clouds from the SLAM algorithm to refine the localization accuracy. The corresponding examples are shown in Fig. 4. As we see, the segmentation in [18] detects only a few objects and naively clusters them as 3D spheres without consistency checks. In contrast, our pipeline detects various objects and estimates their material type to achieve a smoother and finer clustering consistency across the voxels.\nTo formally evaluate, we compare our object-oriented clustering results against [20], where a similar semantic object-level 3D clustering is proposed (without material classification). To allow this comparison, we disregarded material classification and applied the 3D clustering based on the object semantics. Table III shows the performance metrics in terms of mean average precision (mAP) (for object/material detection accuracies), Intersection over Union (IoU) (for clustering accuracy), and the number of object detections (for object-oriented effectiveness) on two different TUM dataset sequences. Our model has outperformed [20] in all the metrics due to the robust multiscale clustering."}, {"title": "D. Real-world Mobile Robot Experiments", "content": "We used a mobile robot platform (4WD Rover Zero V3) equipped with a D435i Intel Realsense RGB-D camera, an RPLidar-v3, and an NVIDIA Jetson Nano. We conducted teleoperated mobile robot trajectories in multiple rooms (consisting of diverse scenes and different object compositions) in controlled indoor conditions and recorded them as ROSbag datasets. The experiments were run on an Intel Core i7 laptop with Nvidia GeForce GTX 1050 Ti running on Ubuntu 18.04 with ROS-Melodic. The goal was to obtain enhanced 3D maps with object material information that does not change over time, such as doors, desks, chairs, and other objects. We intentionally prepared the settings such that the datasets consist of multiple objects made of similar materials (e.g., cabinets, doors, desks made of wood) and the same object types made of different materials (e.g., wooden, plastic, and metal chairs). A manually labeled ground truth map specified the static objects' location. Each sequence included raw data from multiple sources, including two RGB-D cameras, LiDAR, and odometry. Fig. 5 shows the robot setup, experiment trajectory, and visuals for each room. Appendix A provides more details on the objects and material information of different rooms in the dataset.\nWe organized the results based on the material types. Fig. 6 showcases example outputs from multiple stages in our object-oriented pipeline, and Table IV shows the"}, {"title": "V. CONCLUSION", "content": "We proposed an object-oriented pipeline for 3D semantic mapping of surface material properties in a mobile robotics environment. The proposed approach significantly advances the SOTA in mapping and perception by integrating semantic objects and material identification into a cohesive and effective mapping system. The resulting semantic map associates each point in a 3D point cloud with semantic and material labels. Our extensive experiments, conducted with public and in-house datasets, have demonstrated the robustness and accuracy of our method in creating detailed semantic maps better than comparable approaches. These maps have been validated with qualitative results, showing successful object detection and material classification, which are crucial for robots operating in dynamic and unknown environments. This dual-label (object and material) mapping can prove to be a valuable asset for static landmark identification, facilitating more precise trajectory planning."}]}