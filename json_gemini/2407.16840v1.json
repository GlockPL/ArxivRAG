{"title": "Synth4Kws: Synthesized Speech for User Defined Keyword Spotting in Low\nResource Environments", "authors": ["Pai Zhu", "Dhruuv Agarwal", "Jacob W. Bartel", "Kurt Partridge", "Hyun Jin Park", "Quan Wang"], "abstract": "One of the challenges in developing a high quality custom\nkeyword spotting (KWS) model is the lengthy and expensive\nprocess of collecting training data covering a wide range of lan-\nguages, phrases and speaking styles. We introduce Synth4Kws\na framework to leverage Text to Speech (TTS) synthesized\ndata for custom KWS in different resource settings. With no\nreal data, we found increasing TTS phrase diversity and utter-\nance sampling monotonically improves model performance, as\nevaluated by EER and AUC metrics over 11k utterances of the\nspeech command dataset. In low resource settings, with 50k\nreal utterances as a baseline, we found using optimal amounts\nof TTS data can improve EER by 30.1% and AUC by 46.7%.\nFurthermore, we mix TTS data with varying amounts of real data\nand interpolate the real data needed to achieve various quality\ntargets. Our experiments are based on English and single word\nutterances but the findings generalize to i18n languages and other\nkeyword types.", "sections": [{"title": "1. Introduction", "content": "With the growing demand for voice control and personal devices\nfrom a variety of products such as health rings, smart watches,\nvoice control earbuds, and smart phones etc., a high performance\nkeyword spotting model, with low power consumption and mem-\nory footprint becomes increasingly important. Traditionally,\nkeyword spotting (KWS) has focused on predefined keywords\nsuch as \"Siri\", \"Hey Google\", etc. [1-3]. However, the surge of\npersonalization and customization in these smart devices has fu-\neled the need for a custom keyword spotting solution that allows\nusers to choose their preferred keyword to trigger a device or\nsoftware. It is also an integral part of a seamless user experience\nwhen interacting with AI agents.\nHigh accuracy ASR systems [4] are commonly used for key-\nword detection but their energy demand and memory footprint\nmake them less suitable for continuous usage on devices such as\nphones. Recent studies have employed speech classification mod-\nels to generate speech embeddings from their encoder hidden\nlayers. Embeddings of different utterances are then compared\nusing cosine similarity to determine the degree of match. As an\nexample, Lin et al. [5] used a five-layer convolution network as\na shared encoder, and trained 125 independent decoders, each\nclassifying over 40 different keywords. While this paper\nalso explores the usefulness of TTS data on speech fields, their\nexperiments and evaluations are optimized for utterance clas-\nsification. Rybakov et al. [6] further explored different model\narchitectures such as DNN, CNN, SVDF [2], CRNN, multi head\nself attention (MHSA), MHSA-RNNs, etc., and greatly improved\nclassification accuracy.\nSuch classification models are trained to correctly classify\nthe specific keyword in an utterance, but their utterance embed-\ndings fail to distinguish different phrases, as they are not trained\nto maximize the embedding distance between utterances with\ndifferent phrases, and vice versa. To directly optimize embed-\nding distances and improve keyword matching quality, other\nresearch [7, 8] has adapted triplet loss from its original appli-\ncation in FaceNet [9]. In a batch of examples, they sample an\nanchor utterance, and then sample a positive utterance that has\nthe same phrase as the anchor utterance, and a negative utter-\nance with a different phrase. The loss function is optimized to\nminimize the embedding distance of the positive utterance pair,\nand maximize the embedding distance of the negative pair. They\nused a GRU two-layer model and achieved reasonable accuracy\non the WSJ dataset [10].\nIn contrast to detecting a pre-defined keyword, in which\nutterances contain a small set of specific keywords, it is very\nexpensive to collect utterances for a wide range of phrases and\nhave a good distribution of different locales, speaker demograph-\nics, speaking styles, and prosodies. However, with the recent\nadvancement of Text-to-Speech(TTS) technologies including\nMAESTRO [11], and Virtuoso [12], speech utterances can be\ngenerated with over a distribution of the above speech character-\nistics for more than 100 languages [13]. Notably, TTS has been\nused in a range of applications such as automatic dubbing [14]\nand voice conversion [15], and proven to be a great data augmen-\ntation source in low resource environments. Studies have shown\nthat TTS can be leveraged to augment the training data of other\nspeech models, such as speech recognition [16] and speaker\nrecognition [17]. Researches [18, 19] have also applied TTS to\nkeyword spotting tasks. However those work have focused on\nusing TTS data to improve model quality for different product\ntypes, rather than comprehensively evaluate the synthetic data\nto real data ratio and its quality impact in different resource\nsettings.\nIn this paper, we show how applying different levels of TTS\ndata improve custom keyword spotting quality. Our experiments\nand analysis address three practical scenarios:\n\u2022 No available real data. We aim to understand how different\namounts and diversity of TTS data can improve model quality.\n\u2022 Low/limited real data. Similarly, we aim to understand how\nadditional TTS data helps model quality when training with\nlimited real data.\n\u2022 Need planning for real data. We aim to understand the\nquantity of real data need to be collected, on top of sufficient\nTTS data, to achieve various quality targets.\nOur paper is organized as follows. Section 2 introduces the"}, {"title": "2. Methods", "content": "2.1. Model Architecture\nLong Short Term Memory (LSTM) [20] is a commonly used\nRNN network that captures long and short term dependencies\nusing memory gates. In this paper we used a standard three\nLSTM layer model, with hidden layer dimension 384 and output\nlayer dimension 128. This results in a 2.8Mb model (247Kb\nquantized) that can be continuously run on-device wth reasonable\npower consumption. We have tried other LSTM model sizes and\ndifferent architectures including Conformer [21]. We choose the\n247Kb quantized LSTM model for our TTS experiments based\non its good accuracy and low memory footprint, which can fit\nmost device types.\n2.2. Optimized Triplet Loss\nOur triplet loss is optimized based on the traditional method\nused in Sacchi et al. [7]. Instead of sampling one positive utter-\nance pair and one negative utterance pair per batch, we use the\ngeneralized-end-to-end approach [22] to construct a batch with\nX phrases and Y sampled utterances per phrase. For each phrase,\nwe used half the sampled utterances as enrollment utterances\nand the other half as test utterances. Each test utterance em-\nbedding is compared to the centroid of the enrollment utterance\nembeddings. This reduces the variance of enrollment utterance\nsamples and improves training convergence stability. Thus, one\nbatch contains X * Y/2 positive examples and X(X \u2013 1) * Y/2\nnegative examples. Since this results in a polarized training\ndistribution with a majority of negative examples and posteriors\nthat can be adversely affected by a skewed training prior, we\ndownweight the negative examples by a factor \u03b3. In addition,\nwe calculate the loss from all sampled examples using matrix\noperations to significantly speed up training.\n2.3. TTS utterance sampling\nVirtuoso [12] is used for TTS data generation for its naturalness\nand generalization to unseen transcripts and languages. It is built\non the foundation of MAESTRO [11] and leverages different\ntraining schemes that combine supervised and unsupervised data\n(e.g. untranscribed speech and unspoken text data).\nAs shown in Fig. 1(a), we use the same phrase list containing\n38k unique words from MSWC [23]. For each word, utterances\nare generated by sampling from 726 speakers and five different\nprosodies. Virtuoso supports more than 139 locales [13]. Our\nexperiments focus on English but this approach could benefit\nother locales even more, especially less common languages with\na limited availability of real data."}, {"title": "3. Data and Experiment Setup", "content": "3.1. Training Resources and Setup\nOur real utterances are sampled from the MSWC [23] open\nsource dataset. We process the raw audio into 40 spectral energy\nfeatures for each 25ms frame. We construct the training batch by\nrandomly choosing eight phrases, and sample 10 utterances per\nphrase. For each phrase, we use five utterances to build the en-\nrollment centroid. The remaining utterances are used for testing\nby computing cosine similarity against the enrollment centroids.\nFor training framework, we use Tensorflow/Lingvo [24] for its\nadvantages of automatic streaming inference conversion.\n3.2. TTS Experiment Set Up\nAs mentioned in Section 1, we explore three practical scenarios\nto analyze TTS's impact on model training and quality. Specifi-\ncally:\n\u2022 No real data available. As shown in the top part of Fig. 1(b),\nwe conduct model training with TTS data alone. To under-\nstand the how phrase diversity affects model quality, we tested\n500, 1k, 10k and 38k unique phrases respectively with 100\nsamples per phrase. We also investigate the impact of the\nnumber of samples per phrase. With the full phrase diversity,\nwe tested our model with 10, 25, 50, 75, and 100 samples per\nphrase.\n\u2022 Limited real data available. As shown in the bottom part of\nFig. 1(b), we start with around 50k real utterances, randomly\nsampled from MSWC, to build a baseline model for bench-\nmark purposes. Our TTS phrases and utterances sampling\nmethods are the same as the above but sampled utterances are\nmixed with real data in training. All models are trained from\nscratch, with different data mixtures.\n\u2022 Relationship between quality and real data. As using TTS\ndata only may be inadequate to reach some quality targets,\nwe investigate how much real data is needed. We start with a\nbaseline of 38k unique phrases and 100 samples per phrase\nof TTS data alone, and incrementally increase the real data\namount and analyze the results. We interpolate the model\nquality vs. real utterance count curves to infer the amount of\nreal data needed for desired quality.\n3.3. Eval Dataset and Process\nReliable keyword matching evaluation and metrics are important\nfor understanding the effectiveness of TTS data. We use the test\nsplit of the commonly used Speech Command Dataset [25] for\nevaluation. This dataset contains more than 11k utterances from\n35 unique phrases. For each phrase, illustrated in Fig. 1(c), we\nrandomly choose 10 utterances as an enrollment set and use the\nrest for testing. The enrollment and test utterances are treated\nas a match (positive) if their similarity is above a threshold,\nand treated as a mismatch (negative) otherwise. By comparing\nthe model prediction and utterances true label (determined by\ntranscripts), we can compute different metrics to rank the models.\n3.4. Eval Metrics\nThe left part of Fig. 2 shows the cosine similarity histogram\nof the 247Kb LSTM model's evaluation result for the phrase\n\"up\". A perfect model would have a vertical line (threshold) to\nseparate the true positive (red) and true negative (blue) examples.\nThe overlapping parts are the errors, which, depending on the\nthreshold, can be divided into False Accepts (FA) and False\nRejects (FR). When sliding the threshold from 0 to 1 with a\nstride of 0.01, we plot the False Accept Rates (FARs) and False\nReject Rates (FRRs) corresponding to each threshold in the DET\ncurve on the right in Fig. 2. We use the area under the DET\ncurve (AUC) under the full FAR/FRR range to measure model\nperformance independently of threshold. To measure model\nperformance independently of enrollment phrases, we average\nthe AUC across phrases. Lower values are better, and this is the\ndefault metric used in the rest of the paper to rank models and\ncheckpoints."}, {"title": "4. Experiment Results", "content": "Section. 3.2 explained three scenarios for exploring TTS data\nimpact on improving custom KWS models. This section will\nshow the experiment results and analyses.\n4.1. TTS Model Experiments with No Real Data\nWe analyze the model improvements from TTS data by varying\nthe amount of unique TTS phrases and varying amount of utter-\nance samples per phrase. Table. 1 top rows shows that when the\nnumber of unique phrases increases from 500 to all phrases: 38k\n(with 100 samples per phrase), the Equal Error Rates (EERs) and\nAUCs improve monotonically. The DET curves with triangles\nin Fig. 3 show that the improvement is consistent over different\noperating points. When fixing the phrase diversity at 38k and\ngradually increasing sampled utterance from 10 to 100, as shown\nin bottom rows in Table. 1 and the dotted DET curves in Fig. 3,\nmodel quality improves monotonically. In the environment with-\nout any real data, both TTS phrase diversity and sample sizes\ncontribute proportionally to model performance.\n4.2. TTS Model Experiments with Low Real Data\nIn the scenario in which some real utterances are available but\nnot in sufficient numbers for training, we explore the impact of\nTTS data by varying the number of unique phrases and utterance\nsamples per phrase. In this experiment we use a model trained\nwith 50k real utterances as a benchmark baseline. Shown in Ta-\nble. 2 and the DET curves with triangles in Fig 4, as the number\nof unique TTS phrases increase, the EER improves from 11.73%\nto 8.7% (25.8% rel.) for a model trained with 38k different\nphrases, and AUC improves from 5.15% to 2.94% (42.9% rel.).\nMoreover, with a fixed amount of 38k unique phrases, as the\namount of utterance samples per phrase varies, the following\ntrend occurs: the model continuously improves as the number of\nsamples increases at the beginning. However, as shown in the\nbottom half of Table 2, and in the dotted curves in Fig. 4, the\nimprovement peaks when the number of utterances per phrase\nreaches 50 and declines after adding more examples. One expla-\nnation for this effect is that extra TTS data might overshadow the\nreal data contribution so the model gets less information from\nreal utterances. In the best case, when the model is trained with\n38k phrases and 50 utterances per phrase, the model get further\nimproved to 8.19% EER (30.1% rel. improvement) and AUC\n2.64 (48.7% rel. improvement).\n4.3. TTS Model Experiments with Varying Real Data\nWe explored the impact of TTS data for model improve-\nments in two scenarios: \"no real data\" and \"low real data\".\nCollecting real data is an expensive process but still necessary\nwhen the model is required to perform at high quality. To help\nresource planning and inform product decisions, we start with a\nmodel baseline that uses only TTS data (with 38k unique phrases\nand 100 utterances per phrase), and record the EER and AUC\nmetrics when increasing the amount of the real training utter-\nances to 0, 50k, 150k, 250k, 500k, 1M, 3M, and 5M. We plot the\nEER and AUC trends in Fig. 5. It is easy to see that the amount\nof the real utterances is positively correlated model performance.\nTo find amount of real utterances needed to achieve a quality\ntarget, we interpolate the data points in Fig. 5. For example, to\nachieve 5% or lower EER, the interpolation shows about 700k\nreal utterances need to be collected. Similarly, to achieve 2%\nor lower AUC, we can see about 200k real utterances should be\ncollected."}, {"title": "5. Conclusion", "content": "In this paper, we propose Synth4Kws a framework to lever-\nage synthetic speech data for developing customizable keyword\nspotting models. With this framework, we carried out a system-\natic study on the impact of TTS data on user-defined keyword\nspotting tasks in various resource environments. In \"no real\ndata\" experiments, we found that model quality monotonically\nimproves when increasing TTS phrase diversity or utterances per\nphrase. In \"low real data\" experiments, with a baseline model\nwith 50k real data, selecting the right amount of TTS phrases\nand utterance samples can improve EER by 30.1% and AUC by\n48.7%. We also noticed that extra TTS data could overshadow\nthe real data and produce worse results. Lastly, we interpolate\nthe curve composed of model quality v.s. real data quantity\nand infer the amount of real data needed on top of TTS data\nto achieve different quality targets, which could be informative\nfor resource planning. Finally although our experiment train-\ning and evaluations are based on the English language and one\nword utterances, we believe the results will generalize to other\nlanguages and multi-word keyword spotting tasks."}]}