{"title": "ContextModule: Improving Code Completion via Repository-level Contextual Information", "authors": ["Zhanming Guan", "Junlin Liu", "Jierui Liu", "Chao Peng", "Dexin Liu", "Ningyuan Sun", "Bo Jiang", "Wenchao Li", "Jie Liu", "Hang Zhu"], "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in code completion tasks, where they assist developers by predicting and generating new code in real-time. However, existing LLM-based code completion systems primarily rely on the immediate context of the file being edited, often missing valuable repository-level information, user behaviour and edit history that could improve suggestion accuracy. Additionally, challenges such as efficiently retrieving relevant code snippets from large repositories, incorporating user behavior, and balancing accuracy with low-latency requirements in production environments remain unresolved. In this paper, we propose ContextModule, a framework designed to enhance LLM-based code completion by retrieving and integrating three types of contextual information from the repository: user behavior-based code, similar code snippets, and critical symbol definitions. By capturing user interactions across files and leveraging repository-wide static analysis, ContextModule improves the relevance and precision of generated code. We implement performance optimizations, such as index caching, to ensure the system meets the latency constraints of real-world coding environments. Experimental results and industrial practise demonstrate that ContextModule significantly improves code completion accuracy and user acceptance rates.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have shown remarkable performance in code-related tasks in the industry [1]-[3], driven by the extensive knowledge of programming languages and frameworks acquired during pre-training. Among these code-related tasks, code completion has emerged as one of the most impactful applications, where LLMs assist developers by predicting and generating the code they need in real time.\nThis capability significantly boosts development efficiency by reducing the amount of manual coding required and has given rise to notable code assistants in the form of IDE (Integrated Development Environment) plugins, such as GitHub Copilot, Cursor and MarsCode, etc.\nWhile much of the current focus of code completion is on the immediate file that a developer is working on, leveraging repository-level contextual information provides additional op-portunities to improve code completion. This context includes core function definitions, configuration files, and code snippets from elsewhere in the repository that align with the developer's current logic. By incorporating this broader context, LLMs can generate more accurate and context-aware code, ultimately increasing the acceptance rate of suggestions and further enhancing coding efficiency.\nAlthough both the academia and the industry have ex-plored how to leverage repository-level information to help LLMs predict code, several practical challenges remain under-explored, particularly in real-world production environments. These challenges include:\n\u2022 Challenge 1. Limited Contextual Understanding: Cur-rent systems rely mainly on the file being edited, lacking broader repository-level context such as relevant function definitions and similar code snippets.\n\u2022 Challenge 2. User Behavior and Intent Recognition: Current approaches do not effectively incorporate devel-opers' cross-file browsing and editing behavior, missing valuable intent information.\n\u2022 Challenge 3. Context Retrieval in Large Repositories with Low Latency: Retrieving contextual information from large repositories and ensuring accurate suggestions while meeting strict latency requirements in real-world production environments is a significant challenge."}, {"title": "II. BACKGROUND", "content": "A. LLMs and Code Completion\nLLMs, such as GPT-4, DeepSeek and Doubao, have emerged as powerful tools for a wide range of natural language processing (NLP) tasks, including text generation, question answering, and text translation. These models are pre-trained on vast amounts of data, enabling them to learn complex language patterns, including those found in programming languages. By leveraging the same principles that allow them to understand and generate natural language, LLMs can also generate high-quality code, making them valuable assets in software development.\nOne of the most prominent applications of LLMs in the software engineering domain is code completion, where the model assists developers by predicting and generating code snippets in real-time as they write. Code completion tools, powered by LLMs, provide suggestions based on the devel-oper's current context, such as the surrounding code, function names, or documentation. This functionality significantly en-hances productivity by reducing the amount of manual coding, helping developers write faster and more efficiently.\nB. Challenges and Motivation\nDespite the remarkable advancements in LLM-based code completion, several challenges remain when deploying these models in real-world environments:\n1) Limited Contextual Understanding: While relying on the content of the file the user is working on can generate useful suggestions, it often lacks the broader repository-level context that can be crucial for more complex tasks. For instance, relevant function definitions, configuration files, or similar code snippets elsewhere in the repository may not be considered, limiting the model's ability to generate the most accurate or context-aware code.\n2) User Behavior and Intent Recognition: Developers fre-quently browse and edit code across multiple files while working on a particular task. This cross-file behavior con-tains implicit information about the developer's intent that can be valuable for code generation. However, current code completion systems do not effectively incorporate this user behavior, resulting in suggestions that may not fully align with the developer's immediate goals. Integrating user behavior into the completion process presents both a technical and design challenge."}, {"title": "III. APPROACH", "content": "In this section, we present the design and workflow of Con-textModule based on the observations of the coding process of our developers, as illustrated in Figure 1. During code completion, ContextModule retrieves three types of contextual information from the repository where the user is working on. These retrievals are concatenated with the current file content to form a prompt, which is then passed to the LLM for code generation. In the rest of this section, we describe in detail the retrieval strategy for each type of contextual information.\nA. User Behavior Code\n1) Observation: During development, users frequently en-gage in cross-file coding and browsing activities. These actions provide critical insights into the user's intent, even if the code they browse or edit in other files doesn't closely resemble the code at their current position. For example, users may:\n\u2022 Browse other files to reference the implementation of some logic;\n\u2022 Browse other files to reference certain function parameter definitions;\n\u2022 Implement helper methods elsewhere in the repository.\nThese interactions carry valuable information, enhancing the LLM's ability to generate code more aligned with the user's goals.\n2) Retrieval Strategy: Each time a user browses or edits code, their cursor activity (clicks, file path, and line number) is recorded. We analyze this cursor history to extract code snippets related to the user's recent behavior. Specifically, we retrieve the N most recently browsed files (default is set to 5) and divide them into snippets using a sliding window. We then count cursor clicks in each snippet and select the top K snippets with the highest interaction counts as the retrieval results.\nB. Similar Code\n1) Observation: During the coding process, users often refer to the existing code in the repository to achieve similar functionality. For example:\n\u2022 When adding a new API, there could be a lot of reusable or similar code for logging and database read/write op-erations;\n\u2022 When implementing model training logic, there might exist some reusable code for data loading and model training.\nSuch similar snippets, closely related to the user's current task, serve as valuable references for code completion.\n2) Retrieval Strategy: We retrieve similar code snippets based on text similarity. First, all files in the repository are divided into snippets using a sliding window. Text features are then extracted for each snippet and stored in an indexed knowledge base. Upon triggering code completion, we extract features from the code near the cursor and use it to search for similar snippets in the knowledge base. The retrieved snippets are sorted by similarity scores.\nAfter comparing token-based and embedding-based retrieval methods, we chose a token-based approach due to its su-perior performance and lower implementation cost. Tokens are extracted using regular expressions, with further splitting of snake and camel case, followed by stop-word removal. Snippets are ranked by Jaccard similarity scores.\n3) Caching Strategy: In production environments, it is common to have very large repositories containing tens of thousands to hundreds of thousands of files. Additionally, code completion has strict latency requirements, typically requiring a response time of a few hundred milliseconds. Therefore, it is not practical to index the entire repository and perform a full search and continuous update of the indexed knowledge base. To address this issue, we designed a caching scheme.\na) File Sorting Strategy: We designed a file sorting strategy to determine the priority of different files during the construction and update of the index cache. For different levels of directories, we use a BFS strategy (current directory subdirectory - parent directory). For files in the same directory level, we determine their priority by comparing the longest common prefix of the target file name and the current file name.\nb) Index Cache: For a given ordered file sequence, we build an index for each file and store it in a cache queue, with a set queue limit (default value is 3000). We maintain a set of inverted index systems for all snippets in the cache, which can retrieve all indexed snippets containing any given token.\nc) Index Update: When a user creates a new file, the index update mechanism is triggered. Specifically, this mech-anism builds an index for the sorted files and updates it in the cache. To avoid performance degradation, we set a limit"}, {"title": "IV. EVALUATION", "content": "In this section, we provide an empirical and practical eval-uation of ContextModule, analyzing its effectiveness across different types of retrieved context and their impact on code completion in the production environment.\nA. Experiment Setup\n1) Model Selection and Inference Parameters: We chose DeepSeek-Coder-6.7b-Base [5] as the experimental model for this study. Since our focus is on assessing the impact of different retrieval strategies within ContextModule, we did not compare our results against other models.\nDuring inference, we employed greedy search to reduce the effect of randomness, setting the maximum input length to 4000 tokens and the maximum generation length to 100 tokens. These settings are sufficient for the majority of the test cases.\n2) Prompt Structure: We used the fill-in-the-middle [6] format to construct the prompts, combining the retrieved context with the prefix and suffix from the current file. Fill-in-the-middle is a prompt format where the model is given both the beginning (prefix) and the end (suffix) of a code snippet, and the task is to generate the missing code in between. This format allows the model to leverage both preceding and fol-lowing context when predicting the missing code, improving the accuracy of its completions. The retrieved context is added in the form of comments to minimize interference with the model's understanding, ensuring that the additional informa-tion guides the model without altering the code structure.\n3) Metrics: We evaluated the model's performance using two metrics: edit similarity and soft exact match.\n\u2022 Edit Similarity: This metric calculates the ratio of the Levenshtein edit distance between two strings to their total length, providing a measure of how similar the generated code is to the ground truth.\n\u2022 Soft Exact Match: This is an improvement over tradi-tional exact match, which only checks if the generated output matches the label exactly. Soft exact match con-siders a result correct as long as the retrieved context helps the model generate the correct answer, even if the output doesn't stop precisely at the right point, reducing false negatives caused by unnecessary trailing content.\nB. Evaluation Dataset Construction\nWe constructed three datasets to evaluate different types of context: User Behavior Code, Similar Code, and CKG-based Symbol Definition. For user behavior code and similar code, evaluation datasets were built for Go, Python, and TypeScript with 1500 samples per language. The symbol definition dataset was built only for Go. We used tree-sitter \u00b9 for syntax parsing during the dataset construction."}, {"title": "V. EXPERIMENTAL RESULTS AND ONLINE PERFORMANCE", "content": "A. User Behavior Code as Context\nWe compared the code completion performance of the subject model with and without user behavior code, where we used the top 2 user behavior codes as context. As shown in the Table II, after adding user behavior code, the soft exact match (SEM) metric is improved by 1.65% to 2.55%, and the edit distance score is improved by 1.8% to 2.4%. This indicates that user behavior code contains information about the user's coding intent, which helps improve code completion effectiveness. Considering that this improvement comes from real user completion data collected online, it is consistent with online metrics and further validates the important role of user behavior code.\nB. Similar Code as Context\nThe results of similar code as context are shown in Ta-ble III. We compare and analyze the model performance from two aspects: different text feature extraction strategies and maximum input length."}, {"title": "1) Text Feature Extraction Strategy", "content": "We compare different text feature extraction methods, including two token extraction strategies and one embedding retrieval strategy:\n\u2022 Origin Token: Using regular expressions to extract to-kens such as variables, methods, and parameters from all code as text features.\n\u2022 Cut Token: For origin tokens named in camel and snake formats, we further split them based on underscores and capitalization. Then, we filter the entire token using stop words, including keywords from different code languages and natural language stop words provided by nltk [7].\n\u2022 Embedding: Using codet5p [8] model to embed the code and obtain a text vector containing semantic information.\nFor token-based retrieval, we use Jaccard [9] score to calculate similarity and set 0.1 as the filtering threshold. For embedding-based retrieval, we use cosine similarity and set 0.7 as the filtering threshold. We set the window size of code snippets to 30 lines. After retrieving from the repository, we sorte and filtered the results based on similarity scores and remove snippets with overlapping lines. Finally, we select the top 2 similar code snippets as the context.\nAs shown in the table, retrieving similar code from the repository can greatly improve code completion performance, validating the effectiveness of similar code. The cut strategy can further improve the performance by 1%-3% on top of the original strategy. This is mainly because there are many tokens in camel and snake formats in the code, and splitting them can obtain more fine-grained semantic information, helping to retrieve more similar code snippets. For example, the tokens getUserCreditInfo and getUserBasicInfo have high semantic similarity, which can be captured by the cut strategy but not the origin strategy. The performance of embedding-based retrieval is similar to that of the cut strategy, better in Python but worse in Go and TypeScript. This may be because we directly use sliding windows to slice the code when constructing code snippets, which destroy the semantic structure of the code and make it difficult for the embedding model to perform well. Considering the high implementation cost of the embedding model, we have not explored it further for now."}, {"title": "2) Model Max Length", "content": "We compare the impact of similar code snippets on the model's maximum length at 4k/8k. For a maximum length of 8k, we increase the window size to 60 lines and select the top 4 code snippets to concatenate into the prompt. The results show that increasing the maximum length to 8k further improves the model's performance by about 2%-6%. This is because a larger search range provides more opportunities to retrieve similar code that can help with code completion. This also further validates the effectiveness of similar code."}, {"title": "C. CKG-based Symbol Definition", "content": "We compare the effectiveness of adding symbol information in three scenarios: function definition, function call, and struct initialization. The results shown in Table IV indi-cate that adding symbol information significantly improves the performance in all three scenarios, fully validating the effectiveness of ckg-based symbol definition. In the function definition scenario, using all input parameters as context leads to a 1%-2% improvement in relevant metrics. This indicates that providing input parameter information to LLM can help it better understand the logic of the function to be implemented and generate content more accurately. Similarly, there is a significant improvement in the function call and struct initialization scenarios, with the improvement in struct initialization exceeding 10%. This is mainly because adding context solves the model's hallucination problem and helps it generate parameters accurately. Function call parameters are relatively simple and can obtain some information from in-file content, while struct initialization is more difficult. This is why there is a significant difference in the improvement between the two scenarios."}, {"title": "D. Context Fusion and Online Performance", "content": "We did not conduct offline experiments on context fusion (combing all context retrieval strategies), we report the per-formance gain in production environments in this section. Specifically, we concatenate the context in order of symbol, similar code, and user behavior code (each context having a maximum length), and combine it with the prefix and suffix of the current file to form a completion prompt. We adapt a rule-based fusion strategy instead of other ranking algorithms such as BM25 [10] and embedding similarity to sort retrieval fragments. This is mainly because the three types of context provide features for code completion from different dimensions, while most ranking algorithms can only compare semantic similarity and are difficult to provide truly effective sorting. For example, a code fragment that a user has browsed may have a very low similarity to the current file code, but it contains the key configuration that the user needs now. For the above reasons, we manually set priorities and length limits to complete the fusion of context.\nIn terms of online performance, we implemented three strategies in sequence of user behavior code, similar code and symbol definition, achieving relative accept rate improvements of 7.1%, 6.3%, and 4.9%, respectively. It should be noted that the three strategies were implemented in order, and there might be overlapping effects on the results, so the above indicators could not fully represent the effectiveness of each strategy. We did not conduct separate or overall ablation experiments on these strategies in the production environment, but according to our estimates, the overall improvement could achieve a relative increase of more than 15%."}, {"title": "E. Case Study", "content": "We select representative cases from real user completion to demonstrate the positive impact of different contextual information. Considering privacy and space issues, we blurred the code and only displayed the key parts.\na) CKG Case 1: As shown in Figure 5, the user needs to call LineAndColumn method in Mapper to obtain variable startLine when implementing isSingleLine method. By adding the struct definition and method signature of Mapper, the model accurately generates LineAndColumn method. Without this context, the model generates LineColumnForPosition method, which does not exist.\nb) CKG Case 2: In the completion shown in Figure 6, the user needs to call GetCommitsLog method to obtain commit information based on fromSHA and toSHA. By adding the signature information of GetCommitsLog method, the model knows that it needs to construct CommitOption based on fromSHA and toSHA as a method parameter. Without this context, the model directly passes fromSHA and toSHA to the method, which causes the method call failure.\nc) CKG Case 3: As illustrated in Figure 7, the user needs to initialize App struct. By adding the struct definition of App, the model correctly assigns values to several properties including Name, Description, and Mode. Without the context, the model incorrectly generates Icon and Config properties.\nd) User Behavior Case 1: In Figure 8, the user needs to call img_to_base64 method to convert the specified image to the base64 type for downstream tasks. By searching the user behavior code, we found that the user had just loaded and processed the image under examples/slide.jpg. The model used this information well to generate accurate path information for the user. Without this context, the model can only simulate a path for recommendation.\ne) User Behavior Case 2: As shown in Figure 9, the user needs to rely on Writer_pb2.WriteCell method to process variable content as the write_item parameter when calling writer_pb2.WriteResponse method. By searching the user be-havior code, we found that there is a similar WriteResponse method call under demo_pb2 that provides a good example for the model to generate the answer accurately. Without this context, the model does not know the existence of WriteCell method and generates the parameter incorrectly.\nf) Similar Code Case: In completion shown in Figure 10, the user needs to initialize the style and className parameters for x-text component in ProfileViewNotice page. By searching the repository for similar code, we found similar code logic in AtNotice page, which helps the model accurately generate the two parameters and values. Without the context, the model generates text, ellipsize-mode, and className parameters in-correctly."}, {"title": "VI. RELATED WORK", "content": "In recent years, the application of LLMs for code comple-tion has garnered significant attention in both academia and industry [11]-[15]. Code completion has traditionally been viewed as a token prediction task [16], but it has evolved to encompass line and block completion, where entire lines or blocks of code are predicted [1], [17], [18]. This evolution has been accompanied by the development of increasingly so-phisticated models that can utilize both left-context (preceding the cursor) and, in some cases, bi-directional context to predict missing code segments [19]."}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "In this paper, we presented ContextModule, a framework designed to enhance code completion by retrieving three types of contextual information: user behavior code, similar code snippets, and CKG-based symbol definitions. ContextModule significantly improves code prediction accuracy while meeting the low-latency demands of production environments through the implementation of an index caching mechanism and a code knowledge graph. Both offline experiments and real-world deployment within our internal code completion system demonstrated substantial improvements in key performance metrics and user acceptance rate.\nIn future work, we plan to extend our work in several directions. First, we aim to expand the Code Knowledge Graph (CKG) to support additional programming languages and leverage dataflow parsing techniques to retrieve variable types and related definitions, further enhancing context retrieval. Second, we intend to build a remote knowledge base to incorporate methods and components from open-source repos-itories, recognizing the growing reliance on external libraries in modern software development. Finally, we will explore more advanced methods for capturing user intent by analyzing the content of code edits in greater detail, complementing the existing use of cursor history to provide even more precise code completions."}]}