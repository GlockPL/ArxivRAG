{"title": "Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models", "authors": ["Yue Xu", "Chengyan Fu", "Li Xiong", "Sibei Yang", "Wenjie Wang"], "abstract": "Pre-training large language models (LLMs) on vast text corpora enhances natural language processing capabilities but risks encoding social biases, particularly gender bias. While parameter-modification methods like fine-tuning mitigate bias, they are resource-intensive, unsuitable for closed-source models, and lack adaptability to evolving societal norms. Instruction-based approaches offer flexibility but often compromise task performance. To address these limitations, we propose FaIRMaker, an automated and model-independent framework that employs an auto-search and refinement paradigm to adaptively generate Fairwords, which act as instructions integrated into input queries to reduce gender bias and enhance response quality. Extensive experiments demonstrate that FaIR-Maker automatically searches for and dynamically refines Fairwords, effectively mitigating gender bias while preserving task integrity and ensuring compatibility with both API-based and open-source LLMs.", "sections": [{"title": "Introduction", "content": "Pre-training large language models (LLMs) on vast text corpora enhances their performance in various natural language processing tasks but risks encoding social biases, particularly gender bias, that are implicitly present in uncensored datasets. Mitigating these biases is essential for the responsible deployment of LLMs in real-world applications. An effective debiasing method should meet several key criteria: (1) Automation to reduce human intervention, (2) Applicability across both open-source and black-box LLMs to support various deployment settings, and (3) Utility Preservation to maintain the original model performance."}, {"title": "Related Work", "content": "Gender bias in LLMs can be evaluated through intrinsic and extrinsic approaches. Intrinsic methods evaluate bias independent of specific downstream tasks by analyzing statistical associations in the embedding space or evaluating the probabilities assigned to different options in datasets. In contrast, extrinsic approaches examine gender bias within the context of downstream tasks, such as coreference resolution, question answering, reference letter generation, and classification tasks, each capturing gender bias from distinct perspectives. These studies underscore needs for ongoing research and mitigation strategies."}, {"title": "Gender Bias Mitigation in LLMs", "content": "To address gender bias in LLMs, various strategies have been proposed, typically categorized into white-box and black-box methods based on access to a model's internal parameters. White-box methods require access to internal parameters, including fine-tuning and model editing. Fine-tuning involves creating specialized gender-inclusive datasets for instruction-based fine-tuning or Direct Preference Optimization (DPO). Model editing focuses on identifying and modifying bias pathways or utilizing hyper-networks for automatic parameter updates. While effective, these methods depend on parameter access, limiting their use to closed-source models and potentially impacting overall model performance.\nBlack-box methods mitigate bias without requiring parameter access, often using textual prompts to guide fairer outputs. Techniques such as Chain of Thought (CoT) and in-context learning (ICL) have shown considerable promise. Counterfactual prompts and curated examples effectively encourage equitable content generation. However, they rely on static prompts, which may lose effectiveness on novel tasks or out-of-distribution data, limiting their robustness."}, {"title": "Automatic Prompt Engineering", "content": "Previous research has explored automatic prompt engineering from various perspectives. For instance, proposed automatic instruction generation and selection for multiple NLP tasks, while leveraged human preferences to optimize user prompts for better alignment with LLMs' input understanding. In the context of bias mitigation, introduced automatically generated trigger tokens. However, these tokens are often nonsensical, making them uninterpretable and impractical for broader use. Similarly, developed an iterative in-context learning framework to automatically generate beliefs based on debiasing effectiveness, measured by content sentiment. Despite 100 iterations of optimization, the final beliefs remain dataset-specific, limiting their generalizability."}, {"title": "Methods", "content": "FaIRMaker is an independent module designed to enhance the fairness of responses generated by both API-based and open-source LLMs. As depicted in the bottom block of Figure 2, during inference, a Fairwords is selected from Fairwords Bag and combined with the user query. This input is then refined by a seq2seq model before being fed into the LLM, ensuring the generated response is fair and unbiased. In the following of this section, we will first introduce the development of the Fairwords Bag where each Fairwords candidate is generated through an auto-search step. Then, we will provide a detailed explanation of the refinement step, which involves a prompt-based refinement and a seq2seq model to learn and generalize the refinement process. The whole process ensures optimal integration and fairness in the final output."}, {"title": "Fairwords Auto-Searching", "content": "Fairwords Auto-Searching comprises two steps: Fairwords optimization and filtering. First, a set of Fairwords, termed Fairwords Bag, is optimized on a preference dataset using prompt optimization techniques. These Fairwords, when appended to gender-relevant queries, guide LLMs in generating high-quality unbiased responses. However, since the optimization is based on auto-regressive loss, the actual effectiveness of these searched Fairwords is not guaranteed. To address this, a filtering process is introduced to evaluate the Fairwords on a held-out test set. Only those Fairwords that demonstrate genuine improving performance are retained for the next step of refinement.\nFairwords optimization. Fairwords Optimization can be framed as the search for universal triggers s given a preference dataset D. This dataset consists of gender-related queries paired with the chosen response and the rejected response. Given a gender-related query x, the optimization goal is to find s such that appending s to x maximizes the probability of generating the chosen response yc while minimizing the probability of generating the rejected response yr. Giving the LLM $f_\\theta$, the process of optimizing the Fairwords s can be formulated as:\n$s^* = \\min_s - \\log f_\\theta (y_c | s \\oplus x) + \\alpha \\log f_\\theta (y_r | s \\oplus x)$", "latex": ["s^* = \\min_s - \\log f_\\theta (y_c | s \\oplus x) + \\alpha \\log f_\\theta (y_r | s \\oplus x)"]}, {"title": "Instruction Generator Training", "content": "Although the filtered Fairwords can prompt better-quality responses, they are nonsensical token combinations lacking interpretability and transferability across black-box LLMs. Additionally, model performance on standard tasks should be maintained. To ensure FaIRMaker to be a model-independent module compatible with both open-source and API-based LLMs while preserving their original performance on normal tasks, we introduce a refinement step. This step transforms the unintelligible Fairwords into human-readable prompts, performing a reverse inference process on the preference dataset of both tasks with the assistance of ChatGPT. Then, a seq2seq model is trained to generalize and learn how to refine the Fariword to mitigate bias and execute normal tasks without the preference dataset. As a result, given any query and Fairwords, FaIRMaker adaptively generates refined Fairwords, ensuring robust performance on both bias mitigation and task execution without compromising utility.\nPrompt-based refinement. Note that Fairwords are optimized using a preference dataset, where the difference between the chosen and rejected re-"}, {"title": "Fairwords refiner.", "content": "Using this dataset, we train a small seq2seq model, $F_{refine}$ referred to as the Fairwords Refiner. This model automatically generates refined Fairwords for any query and vanilla Fairwords selected from the Fairwords Bag. The training of the seq2seq model can be generalized as maximizing the probability of generating a refined Fariwords p giving the input query x and Faiwords s, where the loss function is defined as:\n$L = - \\frac{1}{N} \\sum_{t=1}^{N} \\log F_{refine} (p | s \\oplus x)$", "latex": ["L = - \\frac{1}{N} \\sum_{t=1}^{N} \\log F_{refine} (p | s \\oplus x)"]}, {"title": "Experiments", "content": "We first outline the experimental setup, including models, baselines, evaluation datasets, and metrics. Next, we evaluate FaIRMaker on both gender-related and general tasks to demonstrate its bias mitigation effectiveness and utility preservation. We then analyze the efficiency, extendability, and present ablation studies to highlight the contribution of each component."}, {"title": "Configurations", "content": "In the auto-search step, Fairwords are searched on Llama2-Alpaca, a model fine-tuned from Llama2-7b on the Alpaca dataset. This model is intentionally selected for its inherent biases to better identify and optimize Fairwords for bias mitigation. In the refinement step, a seq2seq model is trained to automatically generate refined Fairwords based on the original Fairwords and query. We use Llama3.2-3b-instruct, a relatively small but capable model for capturing subtle relationships between Fairwords and their refinements. During inference, FaIRMaker operates as an auxiliary module, independent of the downstream LLMs. We evaluate its bias mitigation and utility performance across four open-source LLMs: Llama2-Alpaca, Llama2-7b-chat, Qwen2-7b-instruct, and Qwen2.5-7b-instruct, as well as the API-access LLM, GPT-3.5-turbo."}, {"title": "Bias mitigation", "content": "Figure 3 presents the win-tie-loss rates for bias degree comparison between the original model responses and responses after applying FaIRMaker, evaluated with GPT4 as the judge. FaIRMaker achieves a consistently higher win rate than loss rate across all LLMs, indicating improved responses after applying FaIRMaker. Notably, Llama2-Alpaca achieves a 55.61% win rate, signifying that more than half of the responses are improved. Interestingly, better-aligned LLMs, such as Qwen2.5 and GPT3.5, exhibit a lower win rate but a higher tie rate, likely due to their inherently lower gender bias, resulting in higher-quality original responses."}, {"title": "Utility Maintaining", "content": "In this section, we evaluate the utility of FaIRMaker-enhanced models by assessing the quality of responses across various tasks. We measure response scores on the GA-test to demonstrate dialogue generation capability, and on Dolly Eval, Instruct Eval, and BPO Eval to assess instruction-following performance."}, {"title": "Extendability", "content": "FaIRMaker operates as an independent module during inference, allowing its integration with bias mitigation approaches like DPO on bias-free datasets. This section compares the performance of FaIRMaker and DPO, and explores their potential combined effectiveness."}, {"title": "Ablation Study", "content": "In this section, we evaluate the contributions of each FaIRMaker module through ablation studies. We define two variants: (1) w/o filtering, where all Fairwords and responses are used without filtering in auto-search, and (2) w/o refinement, where Fairwords from the Fairwords Bag are directly appended to queries without refinement. We evaluate these ablations on Llama2-Alpaca for bias mitigation and general tasks. Filtering step in the auto-search ensures that only Fairwords with genuine debiasing effects move to the next stage. FaIRMaker w/o filtering shows reduced bias mitigation and inconsistent performance on general tasks. Without filtering, noisy gender-related data disrupts the refiner's training, impairing feature extraction and weakening bias mitigation effectiveness. Role of refinement: The refinement step converts Fairwords into natural language instructions, enhancing FaIRMaker's generalization and transferability to black-box models. FaIRMaker w/o refinement exhibits significantly lower performance, indicating its limitations in generalization."}, {"title": "Interpretation", "content": "The vanilla Fairwords optimized in the auto-search step are nonsensical token combinations generated to maximize favorable responses and minimize unfavorable ones. The mechanism by which they act as debiasing triggers or enhance some tasks is unclear. To explore this, we have ChatGPT analyze their potential meanings and emotions. Surprisingly, the Fairwords often express emotions like urgency and seriousness, potentially guiding the LLM toward unbiased responses, while preserving the original input intent due to their lack of specific meanings."}, {"title": "Conclusion", "content": "In this work, we introduce FaIRMaker, an automated and model-independent framework that uses a novel auto-search and refinement paradigm to generate Fairwords for gender bias mitigation. FaIRMaker effectively mitigates gender bias while preserving task integrity across diverse downstream tasks for both API-based and open-source LLMs, without modifying the models. We also analyze the efficiency and extendability of FaIRMaker, while highlighting the importance of its key components. Future work includes expanding the scope of biases and further minimizing impacts on general tasks through fine-grid refinement."}, {"title": "Limitations", "content": "Despite FaIRMaker's effectiveness in mitigating gender bias and preserving normal task performance, we identify several limitations that require further improvement.\nScale of Tasks and Training Data. Although FaIRMaker performs well in gender bias mitigation and task integrity, the scope of tasks and training data is limited. For instance, the preference dataset used for auto-searching Fairwords lacks sufficient diversity. Additionally, the refiner was trained on 18k pairs of refined Fairwords from ChatGPT feedback, covering a narrow range of scenarios. This limitation in task and dataset scale may explain the slight decrease in performance on general tasks, which is an area for future improvement.\nBias in Evaluators. FaIRMaker operates without human involvement, relying on state-of-the-art LLMs for response evaluation. While we replicate experiments to confirm findings, evaluators can still introduce inherent biases, as evidenced by the differing score distributions from GPT-4 and Llama3.1. Future work will involve incorporating more evaluators and aggregation methods to provide a more comprehensive assessment.\nFairwords Selection Strategy. Fairwords are randomly selected from the bag during inference, sometimes resulting in intermediate guiding instructions that make the output more verbose and slightly affect general task scores. By incorporating more task-specific guidance based on input types and implementing fine-grid refinement, FaIRMaker could further minimize the impact on general tasks."}, {"title": "Ethics Statements", "content": "In this work, we utilize publicly available datasets for training and evaluating FaIRMaker, including GenderAlign, Self-Instruct and BPO Eval as well as BBQ, Alpaca, and Free Dolly . Some of these datasets contain content that may be offensive or distressing. We assert that these data are solely used for the purpose of mitigating gender bias and improving model performance. Additionally, this paper focuses solely on binary gender bias and leaves exploration of other gender definitions to the broader research community."}]}