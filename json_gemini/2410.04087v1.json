{"title": "GLOBESUMM: A Challenging Benchmark Towards Unifying\nMulti-lingual, Cross-lingual and Multi-document News Summarization", "authors": ["Yangfan Ye", "Xiachong Feng", "Xiaocheng Feng", "Weitao Ma", "Libo Qin", "Dongliang Xu", "Qing Yang", "Hongtao Liu", "Bing Qin"], "abstract": "News summarization in today's global scene\ncan be daunting with its flood of multilingual\ncontent and varied viewpoints from different\nsources. However, current studies often ne-\nglect such real-world scenarios as they tend to\nfocus solely on either single-language or single-\ndocument tasks. To bridge this gap, we aim to\nunify Multi-lingual, Cross-lingual and Multi-\ndocument Summarization into a novel task, i.e.,\nMCMS, which encapsulates the real-world re-\nquirements all-in-one. Nevertheless, the lack\nof a benchmark inhibits researchers from ade-\nquately studying this invaluable problem. To\ntackle this, we have meticulously constructed\nthe GLOBESUMM dataset by first collecting a\nwealth of multilingual news reports and restruc-\nturing them into event-centric format. Addi-\ntionally, we introduce the method of protocol-\nguided prompting for high-quality and cost-\neffective silver summary annotation. In MCMS,\nwe also highlight the challenge of conflicts be-\ntween news reports, in addition to the issues of\nredundancies and omissions, further enhancing\nthe complexity of GLOBESUMM. Through ex-\ntensive experimental analysis, we validate the\nquality of our dataset and elucidate the inher-\nent challenges of the task. We firmly believe\nthat GLOBESUMM, given its challenging na-\nture, will greatly contribute to the multilingual\ncommunities and the evaluation of LLMs\u00b9.", "sections": [{"title": "1 Introduction", "content": "Summarization is a long-standing task in natural\nlanguage processing (NLP) research (Paice, 1990).\nIn recent years, significant advancements have been\nmade in the field thanks to the rapid development\nof large language models (LLMs) (Zhao et al.,\n2023; Liu et al., 2023; Dong et al., 2023; Wei et al.,\n2022a,b; Shanahan, 2022). While LLMs have ef-\nfectively addressed many traditional text summa-\nrization tasks (Adams et al., 2023; Goyal et al.,\n2022; Pu et al., 2023; Zhang et al., 2023), the rapid\nglobalization of information dissemination has cre-\nated new demands for summarization techniques\nthat can effectively summarize a large collection of\nevent-centric multilingual news articles worldwide.\nEvents involved with armed conflicts, interna-\ntional relations, and political elections have always\nfascinated people worldwide. However, relying\nsolely on news articles in a single language to gain\nan in-depth understanding of such events can be\nlimiting. This is because news reports from dif-\nferent countries are often influenced by their na-\ntional standpoints and cultural biases, resulting in\npotential distortions (Boykoff and Boykoff, 2004;\nBaum and Groeling, 2009; Baumeister and Hast-\nings, 2013). To obtain a more comprehensive\ninsight into these events, it is crucial to explore\nnews articles from various countries and languages,\nallowing us to consider diverse perspectives and\naccess more objective information. Surprisingly,\nwhile advancements in LLMs have shown promis-\ning results in many NLP tasks, little research has\nbeen conducted for such real-world scenarios.\nTo this end, we present the task of MCMS\nthat unifies Multi-lingual, Cross-lingual and Multi-\ndocument Summarization into a more general set-\nting, aiming to align better with the multifaceted\nrequirements in real-world scenarios. The goal of\nMCMS is to succinctly capture the key information\nfrom a collection of documents written in various\nlanguages and present a cohesive summary in the\ntarget language. Notably, the MCMS task has three\ndistinctive features: (1) the input consists of multi-\nple documents, (2) the multiple documents are in\ndifferent languages, and (3) the multiple documents\nrevolve around the same event. However, the ab-\nsence of a dataset that encompasses such features\ninhibits researchers from further study.\nTo close this gap, we meticulously construct\nthe GLOBESUMM dataset, which comprises the"}, {"title": "2 The GLOBESUMM Dataset", "content": "We gather news reports from countries and regions\nacross 26 languages worldwide. This is accom-\nplished by utilizing the news URLs provided in\nGoogle GDELT 2.0 project\u00b3. We exclusively uti-\nlize news data from May 2023 to October 2023 for\nour benchmark to avoid introducing prior knowl-\nedge from web-crawled articles used in pre-training\nlarge language models\u2074."}, {"title": "2.1 Data Collection", "content": "One setting in MCMS is noteworthy that the mul-\ntiple news reports within the same round input"}, {"title": "2.2 Source Data Construction", "content": "should be highly relevant to the same news event,\nrather than an open-domain task. To address this,\nwe employ a method involving event retrieval and\nmanual verification to restructure the news reports.\nEvent Retrieval To pinpoint news related to spe-\ncific events, we leverage Wikipedia's current events\nportal as a seed set. Each event in this set serves\nas a query input for our retrieval process. Our goal\nis to identify highly relevant news reports from\nthe multilingual corpus. Initially, we translate the\nquery event (originally in English) into multiple\nlanguages. Subsequently, we employ the BM25\nretriever in Lin et al. (2021) for retrieval in the re-\nspective language corpora, searching for the most\nquery-relevant news articles.\nManual Verification The retrieved news articles\nin different languages are supposed to be highly\nrelevant to the provided description, but high rel-\nevance does not necessarily imply that they all\npresent the same news event. Hence, we incor-\nporate a post-retrieval manual verification process\n(see Appendix A.1)."}, {"title": "2.3 Silver Summary Annotation Methodology", "content": "Next, we will elaborate on how we craft our\nsilver-quality summaries in GLOBESUMM (all the\nprompts can be found in Appendix A.2).\nChronological Recurrent Summarization (CRS)\nOur summary annotation approach is conducted\nunder the CRS schema, aiming to distill key infor-\nmation from news articles in chronological order.\nSpecifically, we begin by organizing these news\ndocuments in order of their respective timestamps.\nThen the summarization process is initiated by gen-\nerating a concise summary for the first two articles.\nThe obtained summary is then integrated with the\nsubsequent article, and iteratively throughout the\nwhole document set. CRS delivers a concise, timely\nsummary by capturing the dynamic narrative and\nproviding a comprehensive overview of the evolv-\ning information landscape in news articles.\nStep 1: Key Information Split (KIS) The large\ninput length of a whole document set, averaging\nnearly 12K tokens in GLOBESUMM, poses a great\nobstacle in MCMS. Therefore, we introduce the\nmethod of KIS to reduce the length of input by\norganizing key information from each document\ninto several finely-grained sentences before sum-\nmarizing the whole document set.\nStep 2: Cross-lingual Prompting (CLP)\nAchieving cross-lingual alignment poses another\nfundamental challenge in multi- and cross-lingual\ntasks. To effectively capture the alignment from\nvarious input languages to target language, we em-\nploy cross-lingual alignment prompting method,\nwhich was first introduced in Qin et al. (2023).\nStep 3: Protocol-guided Prompting (PGP) We\nfirst introduce the method of protocol-guided\nprompting (PGP) to achieve high-quality summary\nannotation. Based on our manual observation of"}, {"title": "2.4 Statistics", "content": "Following the methodology described in Sec-\ntion 2.3, our silver-quality summaries are generated\nwith GPT-4 model as the backbone.\nA total of 370 news events, consisting of 4687\nnews articles, have been finally retained in GLOBE-\nSUMM. The entire dataset spans 26 languages and\neach news event is associated with a minimum\nof 10 news reports in different languages, adding\nto the challenge of our dataset. Due to the recur-\nrent nature of CRS schema (Section 2.3), GLOBE-\nSUMM offers silver summaries for document sub-\nsets of any size within the whole collection of doc-\numents related to the same event, totaling 4317 in\nnumber. And GPT-4's responses to [where] and\n[strategies] are also available in GLOBESUMM.\nThe language distribution can be found in Table 7.\nAs shown in Table 1, GLOBESUMM stands out\nfor being multi-lingual, cross-lingual, and multi-\ndocument and focuses on addressing redundancies,\nomissions, and conflicts. These qualities make\nGLOBESUMM distinctive and practically valuable.\nWe split GLOBESUMM into train, validation and\ntest sets (Table 2). Subsequent experiments (Sec-\ntion 4) are carried out on the test set. Our expenses\ncan be found in Appendix D."}, {"title": "3 Annotation Quality Assessment", "content": "We next examine the superiority of our annotation\nmethod."}, {"title": "3.1 Compete with Human Annotation", "content": "In this section, we evaluate how well GPT-4 ad-\ndresses [where] and [strategies] under the\nguidance of our protocol by comparing its perfor-\nmance with human annotation."}, {"title": "Precision* =", "content": "$\\frac{TP+FPP}{TP+FP}$"}, {"title": "Recall* =", "content": "$\\frac{TP + FPP}{TP + FN}$"}, {"title": "F* =", "content": "$\\frac{2* Precision * Recall}{Precision + Recall}$"}, {"title": "3.2 Component-wise Analysis", "content": "Next, we will explore where the advantages of\nKIS and CLP are specifically manifested. We con-\nduct comparative experiments on XQuAD (Artetxe\net al., 2020; Dumitrescu et al., 2021), exploring KIS\nversus Summarize and CLP versus Direct Translate\n(see detailed implementation in Appendix B)."}, {"title": "3.3 LLM's Scale-Effect on PGP", "content": "The sensitivity observation (Section 4.3) prompts\nour study into the llama2 (Touvron et al., 2023)\nseries models with varying sizes (Appendix C.3).\nWe compare the performance of Direct Sum\u0442\u0430-\nrization and Protocol-guided Prompting (Table 9,\n10), the A results shown in Table 9 exhibit favor-\nable changes in both omission and conflict aspects\nas the model size increases (NIC\u2193: 13.96 \u2192\n5.01 \u2192 4.36; CRE\u2191:-4.96 \u2192 0.61 \u2192 3.32).\nThis indicates that with the growth of model scale,\nprotocol-guided prompting outperforms direct sum-\nmarization, but redundancy remains an issue."}, {"title": "4 Experiments", "content": "Our experiments utilize various baselines, each\ncomposed of a combination of \"schema + pipeline\".\nSchemas To validate the effectiveness of Chrono-\nlogical Recurrent Summarization (CRS), we inves-\ntigate the two schemas for comparison: (a) Single-\nturn Summarization summarizes a document set\nwithin a single-turn generation; (b) Chronological\nRecurrent Summarization iteratively summarizes\ntwo documents at a time in a time-ordered manner.\nPipelines To further validate the advantages of\nKIS and CLP in addressing lengthy inputs and\ncross-language understanding, we conduct com-\nparative tests with these commonly used methods:\n(a) Translate-then-Summarize; (b) Summarize-then-\nTranslate; (c) KIS-then-CLP.\nSimilarly, we conduct experiments using two\ndifferent approaches for summarization: (a) Direct\nSummarization; (b) Protocol-guided Prompting.\nDetailed introductions to these pipelines can be\nfound in Appendix C.1\nModels We select three representative LLMs that\nfeature long context capability, each of which sup-\nports at least a 16k context window.\n\u2022 GPT-3.5-turbo-16k is an advanced GPT-3.5 se-\nries model with a 16k context window.\n\u2022 Vicuna-7B-v1.5-16k (Zheng et al., 2023a) is\nan open-source language model fine-tuned from\nLlama2, and supports a 16k context window.\n\u2022 ChatGLM3-6B-32k (Du et al., 2022) is an open-\nsource language model based on General Lan-\nguage Model (GLM) framework, and supports a\n32K context window."}, {"title": "4.1 Baselines", "content": "Our experiments utilize various baselines, each\ncomposed of a combination of \"schema + pipeline\"."}, {"title": "4.2 Metrics", "content": "We evaluate the quality of the generated summaries\nusing following metrics (see Appendix C.2 for de-\ntailed definitions and formulas):"}, {"title": "4.3 Main Results", "content": "The main results are illustrated in Table 5 (see Ta-\nble 11 in Appendix for full ROUGE results). From\nthe results, we have the following observations:\n(1) Omissions and Conflicts mitigated, yet Re-\ndundancies persist. As shown in Table 5, unlike\nomissions and conflicts, which can be mitigated\nwith the introduction of our methodology (CRS,\nKIS, CLP and PGP), redundancies, on the contrary,\ntends to persist, even exacerbate. The results across\nall three models do not seem to reflect the effec-\ntiveness of our approach in addressing redundancy.\nThis divergence on different issues emphasizes the\nmultifaceted nature of McMs.\n(2) Preferential performance in CRS with\nProtocol-guided Prompting. From the results\non GPT-3.5-turbo-16k and Vicuna-7b-v1.5-16k as\nillustrated in Table 5, we find that protocol-guided\nprompting outperforms Direct only under the\nCRS schema, while its superiority is not evident un-\nder STS. This is within our expectations, as STS re-\nquires LLMs to simultaneously identify and coordi-\nnate redundancies, omissions, and conflicts across\nall news documents, while CRS simplifies summa-\nrization by focusing on two documents at a time.\n(3) LLM's Sensitivity to Protocol-guided\nPrompting. Protocol-guided prompting demon-\nstrates certain advantages on both GPT-3.5-turbo-\n16k and Vicuna-7b-v1.5-16k in Table 5. However,\nwith Chatglm3-6b-32k model, regardless of STS\nor CRS schema, protocol-guided prompting under-\nperforms direct summarization. This indicates that\nthe effectiveness of protocol-guided prompting de-\npends on the model's capabilities, which requires\nunderstanding relatively complex prompts."}, {"title": "5 Further Analysis", "content": "We conduct ablation studies to investigate the effect\nof the KIS-then-CLP stage, as shown in the rows of\nTable 6 (see Table 8 for full ROUGE results)."}, {"title": "4.1 Baselines", "content": "5.1 Ablation Study"}, {"title": "5.2 Error Analysis", "content": "We further present the average error rates of LLMs\nfor each type of conflict as a proportion of the total\nerrors in Figure 4.\nThe results illustrate that conflicts caused by di-\nverse perspectives account for the majority of errors"}, {"title": "5.3 LLM's Scale-Effect on PGP", "content": "The sensitivity observation (Section 4.3) prompts\nour study into the llama2 (Touvron et al., 2023)\nseries models with varying sizes (Appendix C.3).\nWe compare the performance of Direct Sum\u0442\u0430-\nrization and Protocol-guided Prompting (Table 9,\n10), the A results shown in Table 9 exhibit favor-\nable changes in both omission and conflict aspects\nas the model size increases (NIC\u2193: 13.96 \u2192\n5.01 \u2192 4.36; CRE\u2191:-4.96 \u2192 0.61 \u2192 3.32).\nThis indicates that with the growth of model scale,\nprotocol-guided prompting outperforms direct sum-\nmarization, but redundancy remains an issue."}, {"title": "5.4 Apathy towards Low-Resource Languages", "content": "Within McMs, we undertake several experiments\nto investigate LLM's prejudices across various lan-\nguages (details can be found in Appendix C.3).\nThe results on GPT-3.5-turbo-16k, Vicuna-7B-\nv1.5-16k in Figure 5 all indicate a tendency to pri-\noritize content from documents in high-resource\nlanguages like English and Spanish, with only a\nsmall part from documents in low-resource lan-\nguages, like Hindi, Greek, and Hebrew. This pref-\nerence poses a challenge for current LLMs to be\nfair summarizers across all languages."}, {"title": "6 Related Work", "content": "Multi-lingual summarization (MLS) aims to pro-\ncess documents in multiple languages and gener-\nate their summaries in the corresponding language.\nThe MultiLing-2015 dataset (Giannakopoulos et al.,\n2015) initiates interest in this task, leading to in-\ncreasing subsequent studies (Vanetik and Litvak,\n2015; Litvak et al., 2016; Cao et al., 2020b). Re-\ncently, with the availability of many large-scale\nMLS datasets (Varab and Schluter, 2021; Hasan\net al., 2021; Feng et al., 2022), notable progress\nis achieved one after another. Cross-lingual sum-\nmarization (CLS) summarizes given documents in\none language into summaries in another target lan-\nguage. The early work mainly focuses on pipeline\nmethods (Yao et al., 2015; Ouyang et al., 2019;\nWan et al., 2010), leading to error propagation. The\nrecent large-scale CLS datasets (Zhu et al., 2019;\nWang et al., 2022; Zheng et al., 2023b) are shifting\nthe research attention to end-to-end studies (Cao\net al., 2020a; Liang et al., 2022). Considering the"}, {"title": "6.1 Multi-lingual and Cross-lingual\nSummarization", "content": "close relation between MLS and CLS, Feng et al.\n(2022) evaluate the MLS models on CLS to show\ntheir zero-shot CLS ability, Wang et al. (2023) uni-\nfies MLS and CLS into a more general setting of\nmany-to-many. Unlike typical MLS and CLS tasks,\nMCMS involves multi-document summarization\nacross multiple languages in a single input round,\nposing a greater challenge."}, {"title": "6.2 Multi-document Summarization", "content": "Multi-document summarization (MDS) refers to\nthe task of summarizing the text in multiple doc-\numents into a concise summary. Previous stud-\nies have delved into various approaches, encom-\npassing extractive (Angelidis and Lapata, 2018;\nZheng et al., 2019; Mao et al., 2020) and abstrac-\ntive techniques (Gehrmann et al., 2018; Lebanoff\net al., 2018; Zhang et al., 2018). And researchers\nmainly focus on reducing the redundancy among\ndocuments (Peyrard et al., 2017; Xiao and Carenini,\n2020; Chen et al., 2021). Currently, there is a grow-\ning focus on MDS tasks in more diverse settings.\nZhou et al. (2023) highlights the challenge of open-\ndomain MDS, Amar et al. (2023) proposes aspect-\nbased summarization in MDS to better fit the needs\nin real-world scenarios. Our MCMS extends typi-\ncal MDS task by incorporating a multi-lingual us-\nage. Unlike prior MDS efforts that targeted redun-\ndancy reduction, MCMS also highlights the chal-\nlenges of addressing omission and conflict between\nmultipe documents, which is crucial for real-world\ninformation management across diverse sources."}, {"title": "7 Conclusion", "content": "To conclude, our study presents the task of MCMS\nthat unifies Multi-lingual, Cross-lingual and Multi-\ndocument Summarization to align better with\nthe diverse needs in real-world scenarios. Our\nbenchmark, GLOBESUMM, serves this demand as\nthe first dataset for such scenario, offering high-\nquality summaries generated through protocol-\nguided prompting. Through experiments and anal-\nysis, conducted on outperforming LLMs, we unveil\nthe shortcomings of LLMs in MCMS and highlight\nthe challenges of addressing redundancies, omis-\nsions and conflicts. Overall, we believe GLOBE-\nSUMM holds the potential to be used for evaluating\nthe performance of LLMs in handling multi-lingual\nand multi-document tasks and the way we utilize\nprotocol-guided prompting can serve as a practical\ncase for cost-effective annotation."}, {"title": "Ethics Statement", "content": "We utilize publicly available news data, which\nmay contain viewpoints from different perspectives.\nThe output results in the paper do not necessarily\nrepresent the views of the authors."}, {"title": "Limitations", "content": "While our dataset is constructed with GPT-4, bud-\nget constraints prevent us from exploring further\nexperimental results on the GPT-4 model.\nOur work primarily focuses on addressing re-\ndundancies, omissions, and conflicts among docu-\nments. However, in our attempts, we have found\nthat while omissions and conflicts can be alleviated\nto some extent through our method, redundancies\nhave not shown significant improvement.\nDue to the recurrent nature of CRS, our refer-\nence summaries can cover any truncation length\nwithin the document set, as opposed to only pro-\nviding a single final summary for each document\nset in many typical MDS datasets. However, in this\nwork, there has not been an extensive investigation\ninto this particular aspect, such as the impact of\ndocument quantity and language diversity on the\ndifficulty of McMs.\nGaining a profound understanding of a specific\nglobal news event involves more than the MCMS\ntask discussed in our work. Exploring how to group\nnews reports about the same event is also a worth-\nwhile research endeavour. However, in the data con-\nstruction phase of this study, the effectiveness of\nthis step is ensured through manual post-validation\nwithout delving into its methodology."}, {"title": "A GLOBESUMM Construction Details", "content": "Manual Verification \"high relevance does not\nnecessarily imply that they all present the same\nnews event\", here is a case for distinction:\nBoth the [News1] and [News2] are highly rel-\nevant in overlapping terms (e.g. Venezuela, US, ...)\nwith the given description. Obviously [News1]\nis the exact news event as described in the pro-\nvided description, but it's challenging for a BM25\nretriever to distinguish between them.\nTherefore, we incorporate a post-retrieval man-\nual verification. 5 annotators are invited to assess\nthe relevance of retrieved news reports based on\nthe specified event description. Only news meeting\nat least one of the following criteria is retained: (1)\nnews that describes the same event as the given\nquery, (2) news that involves the causes and con-\nsequences of the given query event and (3) news\nthat reflects diverse perspectives on the given query\nevent."}, {"title": "A.1 Source Data Construction", "content": "A.2 Reference Annotation Methodology\nKey Information Split (KIS) In order to prevent\ninformation from becoming overly fragmented af-\nter being splitted, thereby overlooking the contex-\ntual connections, our prompt explicitly instructs\nthe model to employ specific entity names instead\nof pronouns. The full request is formulated as fol-\nlows:"}, {"title": "B Quality Assessment Details", "content": "We have made slight modifications to the tradi-\ntional precision, recall, and F\u2081 metrics for evaluat-\ning the performance of protocol-guided prompting\nwith GPT-4. Here, we introduce the concept of\nFalse Positive Positive (FPP): predictions that do\nnot align with the standard golden set but are still\nclassified as correct answers after manual inspec-\ntion. The formulas are as follows:"}, {"title": "B.1 Variations of Precision, Recall, and F\nmetrics", "content": "B.2 Component-wise Analysis Details\nThe KIS step is introduced to shorten the cumu-\nlative input length of a document set, preventing\nexcessive length. Summarizing each document\nbefore generating an overall summary is also a\ncommonly used method for condensing. Thus, we\nconducted comparative experiments on XQuAD, a"}, {"title": "Cross-lingual Prompting (CLP)", "content": "The prompt is\ndesigned as:\nPlease act as an expert in cross-lingual understanding\nin {Source Language}\nRequest: {Given content X }\nLet's understand the content in {Target Language}\nsentence-by-sentence."}, {"title": "Protocol-guided Prompting (PGP)", "content": "The full\nprompt for [where], [strategies] and summa-\nrization process are provided in Table 12."}, {"title": "scorered", "content": "$\\frac{\u03a3_i max_{j:i\u2260j} Sim(x_j, x_i)}{|X|}$"}, {"title": "coverage", "content": "$\\frac{count(entailed)}{count(sents)}$"}, {"title": "NIC", "content": "$1 - \\frac{coverage}{log(|cand|)} * 10$"}, {"title": "4.4 Scale-Effect on PGP", "content": "LLM's Scale-Effect on PGP As we only inves-\ntigate the impact on protocol-guided prompting, in\norder to reduce time and computational cost, we\nutilize the results of the pre-steps (translate-then-\nsummarize, summarize-then-translate, and KIS-\nthen-CLP) from GPT-3.5-turbo-16k model. The\nllama2 series models are only employed in the fi-\nnal summarization step. Due to the The llama2\nmodels are running with the default settings in this\nproject."}, {"title": "5.5 The Apathy towards Low-Resource Languages", "content": "The Apathy towards Low-Resource Languages\nWe employ an NLI model t5_xxl_true_nli_mixture\nto discern whether the sen-\ntences in the generated summary are entailed within\nthe documents in the document set. If a sentence\nis entailed in a document, we consider it to be\nconcluded from that document. Due to the NLI\nmodel's lack of capabilities in handling lengthy\ntexts and multiple languages, we consider the out-\nput of the original document after undergoing KIS-\nthen-CLP as the premise. Each sentence from the\ngenerated summary is then treated as a hypothesis.\nThe entail score for each language is calculated as\nfollows:"}, {"title": "scorelang", "content": "$\\frac{\\sum \\frac{count(entailed)}{count(sents)}}{S}$"}, {"title": "norm_scorelang", "content": "$\\frac{score_{lang}}{\\sum score_{langs}}$"}, {"title": "C.3 Experimental implementations", "content": "norm_scorelang"}, {"title": "6.6 Conflict Resolution Effectiveness (CRE)", "content": "Conflict Resolution Effectiveness (CRE) met-\nric evaluates how well a candidate summary ad-\ndresses conflict. We use GPT-3.5-turbo as a ref-\neree to assess the conflict resolution strategies\npresented in the candidate summary. We employ\nconflicts identified by GPT-4 as the standard, as-\nsessing the effectiveness of the candidate sum-\nmary's handling of conflicts based on the prompts\nprovided in Table 13 and the result is present in\na three-class classification of 1, 0, -1. In order to\nminimize errors resulting from conflicts update,\nwe do not consider all conflicts identified in the\nsummarization process for evaluation. Instead,\nwe only select conflicts identified in the last 5\nrounds of the CRS iteration process. The Con\nscore is calculated as follows:"}, {"title": "penalty", "content": "$log(count(0) + e)$"}, {"title": "score.con", "content": "$\\frac{count(1)}{count(1/ -1) + a * penalty}$"}, {"title": "Acknowledge", "content": "Xiaocheng Feng is the corresponding author of\nthis work. We thank the anonymous review-\ners for their insightful comments. This work\nwas supported by the National Natural Science\nFoundation of China (NSFC) (grant 62276078,\nU22B2059), the Key R&D Program of Hei-\nlongjiang via grant 2022ZX01A32, the Interna-\ntional Cooperation Project of PCL, PCL2022D01\nand the Fundamental Research Funds for the Cen-\ntral Universities (Grant No.HIT.OCEF.2023018)."}]}