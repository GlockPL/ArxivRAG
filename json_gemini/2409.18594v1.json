{"title": "\u201cOh LLM, I'm Asking Thee, Please Give Me a Decision Tree\u201d: Zero-Shot Decision Tree Induction and Embedding with Large Language Models", "authors": ["Ricardo Knauer", "Mario Koddenbrock", "Raphael Wallsberger", "Nicholas M. Brisson", "Georg N. Duda", "Deborah Falla", "David W. Evans", "Erik Rodner"], "abstract": "Large language models (LLMs) provide powerful means to leverage prior knowledge for predictive modeling when data is limited. In this work, we demonstrate how LLMs can use their compressed world knowledge to generate intrinsically interpretable machine learning models, i.e., decision trees, without any training data. We find that these zero-shot decision trees can surpass data-driven trees on some small-sized tabular datasets and that embeddings derived from these trees perform on par with data-driven tree-based embeddings on average. Our knowledge-driven decision tree induction and embedding approaches therefore serve as strong new baselines for data-driven machine learning methods in the low-data regime.", "sections": [{"title": "Introduction", "content": "Machine learning algorithms are data-hungry (Banko and Brill 2001; Halevy, Norvig, and Pereira 2009; Knauer and Rodner 2024), including intrinsically interpretable models like decision trees (Van Der Ploeg, Austin, and Steyerberg 2014). In many domains, though, large- or medium-sized datasets are not easily available. In the healthcare sector, for instance, data is often limited for diagnostic or prognostic modeling when medical conditions are rare or dropout rates at follow-up assessments are high, respectively (Moons et al. 2015, 2019; Steyerberg 2019). In such a low-data regime, training predictive models from scratch is challenging and it may be necessary to make extensive use of prior knowledge for valid and reliable inferences (Harrell 2015; Heinze, Wallisch, and Dunkler 2018; Knauer and Rodner 2023). In recent years, advances in natural language processing and computing have granted practitioners and researchers access to the condensed world knowledge from much of the web in the form of pretrained deep learning models, i.e., large language models (LLMs) (Anthropic 2024a; Google 2024b; OpenAI 2024b). Although LLMs provide powerful means to leverage prior knowledge for predictive modeling when data is scarce, interpreting their decisions remains an open challenge (Longo et al. 2024). Additionally, state-of-the-art LLMs are proprietary (Chiang et al. 2024) and therefore cannot be readily used with sensitive data (Anthropic 2024b;Google 2024a; OpenAI 2024a). This limits their applicability for domains where privacy is critical such as the clinical setting.\nIn this work, our contributions are as follows:\n1. We show how state-of-the-art LLMs can be used for building intrinsically interpretable machine learning models, i.e., decision trees, without access to the model weights and without any training data (Sect. 3). The zero-shot setting naturally preserves the data privacy and thus broadens the applicability of LLMs for predictive modeling across industry verticals.\n2. We demonstrate how our zero-shot decision trees can also serve as feature representations for downstream models (Sect. 4).\n3. We offer a systematic comparison of our decision tree induction and embedding approaches with state-of-the-art machine learning methods on 13 public and 2 private tabular classification datasets in the low-data regime. We show that our knowledge-driven trees achieve a better performance than data-driven trees on 27% of the datasets and that our zero-shot representations are not statistically different from data-driven tree-based embeddings (Sect. 6). Therefore, we argue that our zero-shot induction and embedding approaches can serve as powerful new baselines that data-driven machine learning methods should surpass to show their efficacy."}, {"title": "Related Work", "content": "Transferring knowledge from one task with more data to another task with less data, i.e., transfer learning, has a long history (Pan and Yang 2009). However, the compressed world knowledge within LLMs has opened up new dimensions to augment data-driven machine learning methods with prior knowledge. Embedding approaches extract feature representations from LLMs and apply them as inputs to downstream models (Peters et al. 2018). Fine-tuning approaches update the LLM parameters for downstream tasks (Howard and Ruder 2018). In-context learning, on the other hand, is an emergent ability in LLMs where training examples can be presented to LLMs via prompts, without any additional parameter training or updates (Brown et al. 2020). This does not only allow for rapid prototyping on new tasks(Hollmann et al. 2023; Sun et al. 2022), but also for easily incorporating additional knowledge by changing the prompt (Kojima et al. 2022; Wei et al. 2022). Although the exact working mechanism of in-context learning is not yet fully studied (Fu et al. 2024; Shen, Mishra, and Khashabi 2024; Xie et al. 2022), it has been shown that deep learning architectures that power modern LLMs, i.e., transformers and state space models, can in-context learn (sparse) linear functions, two-layer neural networks, and decision trees (Garg et al. 2022; Grazzi et al. 2024). With \u2265 1 training examples, decision rules or trees can thus not only be extracted from machine learning models using explainable artificial intelligence (XAI) approaches (Molnar 2022; Ribeiro, Singh, and Guestrin 2016, 2018), but also from LLMs using in-context learning (Li et al. 2023; Wang et al. 2024).\nMost related to our work, Nam et al. (2024) recently leveraged LLMs to generate features for tabular data. First, they used an LLM to propose a new feature name and to find a rule for generating the new feature values using only a task description and the feature names, but without any training examples. Second, they augmented the data with the new feature, trained a decision tree on the new data, and obtained a validation score. Finally, they used the LLM to iteratively improve the feature generation rule given a task description and the feature names as well as previous rules, trained decision trees, and validation scores. However, the focus of our research is not on feature generation, which can be detrimental when data is scarce (Harrell 2015; Heinze, Wallisch, and Dunkler 2018; Knauer and Rodner 2023). Rather, we present the first approach to apply state-of-the-art LLMs for zero-shot model generation using in-context learning, i.e., we show how LLMs can build intrinsically interpretable trees without access to the model weights and without any training data (Sect. 3). Furthermore, we demonstrate how our zero-shot trees can also serve as effective feature representations, i.e., embeddings, for downstream models (Sect. 4)."}, {"title": "Zero-Shot Decision Tree Induction", "content": "In the following, we present how state-of-the-art LLMs can be used to generate decision trees without any training data. We therefore go beyond prior research that has employed LLMs to generate new features for machine learning models (Nam et al. 2024). Our approach does not only allow us to leverage the prior knowledge within LLMs for predictive modeling, but also to extract interpretable decision rules from LLMs and to naturally respect the data privacy. This broadens the applicability of LLMs for domains where data is frequently scarce, interpretability is desirable, or data protection is mandatory, such as in the healthcare sector (Longo et al. 2024; Moons et al. 2015, 2019; Steyerberg 2019).\nListing 1 shows our prompting template for building a zero-shot decision tree $T$. We start each prompt by presenting some background context, followed by the actual task description. The prediction target $p \\in P$ and the tree's maximum depth $d \\in N^+$ need to be set depending on the task. Most importantly, we employ in-context learning to provide the LLM with an output indicator, i.e., we encourage the LLM to output the decision tree in a textual format. Finally, we pass the $k \\in N^+$ feature names $f = [f_1, \u2026, f_k] \\in F$ to the LLM including, if applicable, the categorical values or measurement units in brackets. Since state-of-the-art LLMs have seen massive text corpora and datasets from the web during pretraining (Bordt, Nori, and Caruana 2024; Bordt et al. 2024), they can leverage their condensed world knowledge to transform feature names into decision trees, given that the feature names are meaningful (Appendix A). This allows the LLM to derive a mapping $\\phi(p, f_1, ..., f_k, d) \\rightarrow T$."}, {"title": "Zero-Shot Decision Tree Embedding", "content": "Zero-shot decision trees encode rich structural dependencies between the input features and prediction target. In this section, we show how our knowledge-driven trees can be used as feature representations, i.e., embeddings, for downstream models. While prior work has focused on generating tree-based embeddings in a data-driven way (Borisov et al. 2023; Moosmann, Triggs, and Jurie 2006), our approach allows us to extract task-specific embeddings from a pretrained LLM and augment data-driven machine learning models with the LLM's compressed knowledge about the world.\nKnowledge Distillation Decision tree ensembles often achieve state-of-the results on tabular data (McElfresh et al. 2024), so we distill the knowledge from an LLM into > 1 zero-shot decision trees for our embedding approach. Since employing similar trees within a single representation would not add much information, we leverage the LLM's inherent stochasticity (set by the temperature) and further increase its degrees of freedom by letting it determine the maximum tree"}, {"title": "Experimental Setup", "content": "In the following, we provide details on the experimental setup to assess our zero-shot decision tree induction (Sect. 3) and embedding (Sect. 4), including the datasets, preprocessing, machine learning baselines, and performance metrics."}, {"title": "Datasets", "content": "To evaluate our induction and embedding methods, we selected 13 small-sized tabular classification datasets from the public Penn Machine Learning Benchmarks (PMLB)."}, {"title": "Decision Tree Induction Setup", "content": "Methods For our decision tree induction experiments, we used 3 state-of-the-art LLMs as well as 5 machine learning baselines, i.e., 2 data-driven intrinsically interpretable predictive models, 2 automated machine learning (AutoML) frameworks, and 1 off-the-shelf pretrained deep neural network.\nOur zero-shot decision trees were generated with the top 3 LLMs on the LMSYS Chatbot Arena Leaderboard from July 1st, 2024 (Chiang et al. 2024): GPT-40-2024-05-13 (OpenAI 2024b), Claude 3.5 Sonnet (Anthropic 2024a), and Gemini-1.5-Pro-API-0514 (Google 2024b). For all LLMs, we set the temperature to 0 for more determinstic outputs and fixed the maximum depth $d$ to 2 for more interpretable trees. However, we did not observe significant performance differences for other values of these hyperparameters in a smaller pre-evaluation. Furthermore, missing values were imputed using an optimal 10-nearest-neighbors-based imputation (Bertsimas, Pawlowski, and Zhuo 2018) before feeding test data to our trees.\nAs data-driven interpretable baselines, we applied cardinality-constrained best subset selection for logistic regression (BSS) (Bertsimas, Pauphilet, and Van Parys 2021) and optimal classification trees (OCTs) (Bertsimas and Dunn 2017) using Interpretable AI 3.1.1 with Gurobi 11.0.1. For preprocessing, we again imputed missing values. We then passed nominal feature indicators to both methods and used a 3-fold cross-validation with an F1-score validation criterion to find the best hyperparameters for BSS (i.e., the cardinality from the grid [1, 2, 3, 4] (Evans et al. 2022) and the regularization strength from the grid [0.1, 0.02, 0.004]"}, {"title": "Decision Tree Embedding Setup", "content": "Methods For our decision tree embedding experiments, we used the same 3 state-of-the-art LLMs as in our induction setup. To introduce greater variability in our tree generation, though, we set the LLMs' temperature to their respective default values (0.7 for GPT-40, 1.0 for Claude 3.5 Sonnet, and 1.0 for Gemini 1.5 Pro), fostering more diverse and creative outputs (Sect. 4). We then generated 5 decision trees for each LLM to form our decision forest for the embedding. The embeddings were used as feature representations for a simple linear probe, i.e., a logistic regression classifier.\nOur zero-shot embeddings are inherently unsupervised, so our primary competitor was an unsupervised random trees embedding (Moosmann, Triggs, and Jurie 2006). We also fitted random forests (Breiman 2001), extremely randomized trees (Geurts, Ernst, and Wehenkel 2006), and XGBoost (Chen and Guestrin 2016) with 5 trees on our design"}, {"title": "Experimental Results", "content": "In this section, we present the experimental results for our zero-shot decision tree induction (Sect. 3) and embedding (Sect. 4). Overall, we found that Claude 3.5 Sonnet yielded deterministic responses (except once) and that Gemini 1.5 Pro frequently needed multiple API calls to derive our zero-shot trees. Furthermore, we noted that the LLMs often generated explanations in addition to the decision trees,\nwhich can provide valuable insights into their reasoning processes. In terms of predictive performance, we found that LLM-based zero-shot decision tree generations showed a better average overall performance than the data-driven OCTs on 27% of our evaluated datasets, and that our zero-shot decision tree embeddings were not statistically different from the best data-driven tree-based embeddings. We therefore argue that our knowledge-driven induction and embedding approaches serve as powerful baselines for data-driven machine learning methods in the low-data regime."}, {"title": "Decision Tree Induction Results", "content": "On the public data, our knowledge-driven trees achieved a competitive performance to data-driven trees on individual datasets - without having seen any of the training data. At 67%/33% train/test splits, for instance, Gemini 1.5 Pro achieved a better median test balanced accuracy to OCTs on labor (+0.05) and Claude 3.5 Sonnet even reached a better"}, {"title": "Decision Tree Embedding Results", "content": "On the public data, our LLM-based zero-shot embeddings narrowed the gap to the data-driven decision tree induction. The median test F1-score difference to OCTs decreased by +0.08 for Claude 3.5 Sonnet, by +0.23 for Gemini 1.5 Pro, and by +0.22 for GPT-40. Similarly, the median test balanced accuracy difference to OCTs decreased by +0.20 for Claude 3.5 Sonnet, by +0.24 for Gemini 1.5 Pro, and by +0.25 for GPT-40. Our zero-shot embeddings were on par with, i.e., not statistically different from, the best self-supervised and supervised embeddings, and surpassed unsupervised random trees embeddings by at least +0.06 and +0.06 in terms of median test F1-score and balanced accuracy. On individual datasets, the median test F1-score and balanced accuracy improved compared to no embedding by up to +0.33 and +0.30. On our ACL injury data, GPT-40 increased the median test F1-score and balanced accuracy compared to no embedding by +0.20 and +0.20 and was only outperformed by self-supervised extra trees embeddings with improvements of +0.28 and +0.30, respectively. Unsupervised random trees embeddings only showed much smaller improvements of +0.11 and +0.10. On our post-trauma pain data, the embeddings by Gemini 1.5 Pro and self-supervised XGBoost were the only ones to demonstrate a performance improvement compared to no embedding, with increases of +0.10 and +0.10 in median test F1-score and +0.11 and +0.07 in median test balanced"}, {"title": "Conclusion", "content": "LLMs provide powerful means to leverage prior knowledge in the low-data regime. In this work, we presented how we can use the condensed world knowledge within state-of-the-art LLMs to induce decision trees without any training data. We showed that these zero-shot trees can even surpass data-driven trees on some small-sized tabular datasets, while remaining intrinsically interpretable and privacy-preserving. Additionally, we demonstrated how we can generate knowledge-driven embeddings using our zero-shot trees and combine them with downstream models to infuse prior knowledge into data-driven approaches, with a predictive performance that matches data-driven tree-based embeddings on average. We therefore argue that our tree induction and embedding approaches can serve as strong new baselines for data-driven machine learning methods.\nNevertheless, we want to point out that our conclusions are so far based on small-sized tabular classification datasets and do not necessarily extend to other settings. We also focused on a simple prompting template that could be adapted for better performance (Dong et al. 2024; Nori et al. 2023; Schulhoff et al. 2024), e.g., by enriching the prompts with additional information from standards, guidelines, or research papers. We also expect further performance boosts with the rise of even more powerful LLMs (OpenAI 2024c), and by iteratively improving our zero-shot trees with training data (Chao et al. 2024; Eiben and Smith 2015; Nam et al. 2024; Ryan, O'Neill, and Collins 2018; Snell et al. 2024). Alternatively, our trees could be used to obtain probabilistic outputs, e.g., by counting the fraction of training examples in each leaf. Even different probabilistic classifiers like logistic regression could potentially be generated following our general template (Garg et al. 2022; Grazzi et al. 2024). Overall, we believe that leveraging LLMs as zero-shot model generators opens up a new toolbox for practitioners and researchers to tailor LLM-based machine learning models to their needs."}, {"title": "Reproducibility Checklist", "content": "Unless specified otherwise, please answer \"yes\" to each question if the relevant information is described either in the paper itself or in a technical appendix with an explicit reference from the main paper. If you wish to explain an answer further, please do so in a section titled \u201cReproducibility Checklist\" at the end of the technical appendix.\n1. This paper\n(a) Includes a conceptual outline and/or pseudocode description of AI methods introduced (yes/partial/no/NA) [yes] See Sect. 3 and Sect. 4.\n(b) Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (yes/no) [yes] See, e.g., Sect. 6 and 7.\n(c) Provides well marked pedagogical references for less-familiar readers to gain background necessary to replicate the paper (yes/no) [yes] See Sect. 2.\n2. Does this paper make theoretical contributions? (yes/no) [no]\n3. Does this paper rely on one or more datasets? (yes/no) [yes]\n(a) A motivation is given for why the experiments are conducted on the selected datasets (yes/partial/no/NA) [yes] See Appendix A.\n(b) All novel datasets introduced in this paper are included in a data appendix. (yes/partial/no/NA) [no] Our private ACL and post-trauma pain datasets contain sensitive patient information and therefore cannot be shared.\n(c) All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (yes/partial/no/NA) [no] See above.\n(d) All datasets drawn from the existing literature (potentially including authors' own previously published work) are accompanied by appropriate citations. (yes/no/NA) [yes] See Appendix A.\n(e) All datasets drawn from the existing literature (potentially including authors' own previously published work) are publicly available. (yes/partial/no/NA) [yes] See Appendix B.\n(f) All datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing. (yes/partial/no/NA) [yes] See Appendix B.\n4. Does this paper include computational experiments? (yes/no) [yes]\n(a) Any code required for pre-processing data is included in the appendix. (yes/partial/no) [yes] See Code Appendix.\n(b) All source code required for conducting and analyzing the experiments is included in a code appendix. (yes/partial/no) [yes] See Code Appendix.\n(c) All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes. (yes/partial/no) [yes]\n(d) All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from (yes/partial/no) [yes] See Code Appendix.\n(e) If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results. (yes/partial/no/NA) [yes] Where possible, random seeds were set explicitly in our source code. Note that random seeds could not be set for, e.g., Gemini 1.5 Pro and Claude 3.5 Sonnet.\n(f) This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks. (yes/partial/no) [yes] See Sect. 5.\n(g) This paper formally describes evaluation metrics used and explains the motivation for choosing these metrics. (yes/partial/no) [yes] See Sect. 5.\n(h) This paper states the number of algorithm runs used to compute each reported result. (yes/no) [yes] See Sect. 5.\n(i) Analysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information. (yes/no) [yes] See Sect. 6 and Appendix C.\n(j) The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank). (yes/partial/no) [yes] See Sect. 6 and Appendix C.\n(k) This paper lists all final (hyper-)parameters used for each model/algorithm in the paper's experiments. (yes/partial/no/NA) [yes] See Sect. 5.\n(l) This paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting. (yes/partial/no/NA) [yes] See Sect. 5."}]}