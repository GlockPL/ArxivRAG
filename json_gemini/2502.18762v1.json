{"title": "Online Prototypes and Class-Wise Hypergradients for Online Continual Learning with Pre-Trained Models", "authors": ["Nicolas Michel", "Maorong Wang", "Jiangpeng He", "Toshihiko Yamasaki"], "abstract": "Continual Learning (CL) addresses the problem of learning from a data sequence where the distribution changes over time. Recently, efficient solutions leveraging Pre-Trained Models (PTM) have been widely explored in the offline CL (offCL) scenario, where the data corresponding to each incremental task is known beforehand and can be seen multiple times. However, such solutions often rely on 1) prior knowledge regarding task changes and 2) hyper-parameter search, particularly regarding the learning rate. Both assumptions remain unavailable in online CL (onCL) scenarios, where incoming data distribution is unknown and the model can observe each datum only once. Therefore, existing offCL strategies fall largely behind performance-wise in onCL, with some proving difficult or impossible to adapt to the online scenario. In this paper, we tackle both problems by leveraging Online Prototypes (OP) and Class-Wise Hypergradients (CWH). OP leverages stable output representations of PTM by updating its value on the fly to act as replay samples without requiring task boundaries or storing past data. CWH learns class-dependent gradient coefficients during training to improve over sub-optimal learning rates. We show through experiments that both introduced strategies allow for a consistent gain in accuracy when integrated with existing approaches. We will make the code fully available upon acceptance.", "sections": [{"title": "1. Introduction", "content": "Continual Learning (CL) has gained significant popularity in the past decade (Kirkpatrick et al., 2017; Rao et al., 2019; Zhou et al., 2024a). The main idea relies on learning from a sequence of data rather than a fixed dataset. Consequently, the data distribution can change, and new classes can appear, which often leads to the well-known problem of Catastrophic Forgetting (French, 1999). In this paper, we focus on the Class Incremental Learning problem (Hsu et al., 2018).\nFundamentally, Continual Learning scenarios can be divided into two categories: offline Continual Learning (offCL) (Tiwari et al., 2022) and online Continual Learning (onCL) (Mai et al., 2022). The former and most popular scenario assumes that this sequence of data is clearly defined in various tasks and that the learning process on a given task is completely analogous to traditional learning. Namely, data in each task are i.i.d. and the model can be trained for several epochs on a given task before visiting the next one. On the contrary, onCL assumes that the incoming data is analogous to a stream and therefore cannot be seen more than once by the model to enforce fast adaptation. Moreover, in the objective of moving toward more realistic scenarios, various studies now consider unclear or blurry task boundaries (Moon et al., 2023; Koh et al., 2023; Bang et al., 2022) which completely disables task information usage. Such differences make methods designed for offCL seldom transferable to onCL as the vast majority relies on leveraging multiple epochs, as well as task boundary information. Recent representation-based methods such as RANPAC (McDonnell et al., 2024) or EASE (Zhou et al., 2024b) are good examples, which require clear task boundaries to compute the task-specific representations, making them incompatible with onCL. While the difficulty of not having access to task information in onCL has been addressed in various studies (Aljundi et al., 2019; Koh et al., 2023; Moon et al., 2023; Michel et al., 2024), it remains under-explored"}, {"title": "2. Related Work", "content": "2.1. PTM based Continual Learning\nIn recent years, pre-trained models (PTMs) have been widely utilized in offCL (McDonnell et al., 2024; Lin et al., 2023; Zhou et al., 2024b; Smith et al., 2023; Wang et al., 2022b). However, their application in onCL remains largely unexplored, partly because most existing methods heavily depend on task boundaries, i.e., explicit knowledge of when"}, {"title": "2.2. Online Continual Learning", "content": "In onCL, incoming data can be seen only once, analogous to a continuous data stream (He et al., 2020). Therefore, clear boundaries are unlikely to be available and several studies suggest working in boundary-free scenarios (Buzzega et al., 2020) where task change is unknown. However, if the change is clear, it can easily be inferred. In that sense, blurry boundary setting have been proposed (Moon et al., 2023; Koh et al., 2023; Bang et al., 2022; Michel et al., 2024) in previous work. In particular, we are interested in the Si-Blurry setting (Moon et al., 2023) where not only task change is blurry, but some classes can appear or disappear during multiple tasks, which brings the experimental setup one step closer to real-world scenarios while being more challenging. While numerous studies rely on prototypes for Continual Learning (McDonnell et al., 2024; Lin et al., 2023; Zhou et al., 2024b), such representation-based methods must generally be combined with task boundary knowledge as prototypes are updated at the end of each task. In onCL, prototypes are harder to capitalize on when training a model from scratch due to the shift of representations hindering prototype computation (Caccia et al., 2022). However, when working with PTM, such a shift is drastically reduced as representations are already of high quality, making the usage of prototypes more efficient."}, {"title": "2.3. Hypergradients and Gradient Re-Weighting", "content": "Hypergradient (Baydin et al., 2018; Almeida et al., 1999) addresses the problem of finding the optimal learning rate in conventional training scenarios. In that sense, the authors proposed to derive a gradient descent algorithm to learn the LR. Notably, they demonstrate that computing the dot product between gradients from previous steps $\\nabla L(\\theta_t) \\cdot \\nabla L(\\theta_{t-1})$ is sufficient to complete one step of the learning rate update rule, with t the index of the current step, $\\theta$ the parameters, and L the loss function. However, such techniques have been, to the best of our knowledge, developed solely for offline scenarios at a global level. In Continual Learning, gradient re-weighting strategies have been designed for replay-based CL methods. Notably, previous work proposed to re-weight the gradient at the loss level to mitigate its accumulation during training in CL context, also called gradient imbalance (Guo et al., 2023). Recently, to compensate for the class imbalance, class-wise manually defined weights in the last Fully Connected (FC) layer have been leveraged (He, 2024). Our work on Class-Wise Hypergradients lies at the cross-road between Hypergradients and Gradient Re-Weighting."}, {"title": "3. Learning Rate Selection in Online Continual Learning", "content": "3.1. Preliminary\nGenerally, the problem of CL is defined as training a model $f_{\\theta}(\\cdot)$ parameterized by $\\theta$ on a sequence of T tasks where each task of index $k \\in \\{1,\\cdots, K\\}$ is defined by its corresponding dataset $\\mathcal{D}_k$, each potentially being drawn from a different distribution. In the case Class Incremental Learning (Hsu et al., 2018), we have $\\mathcal{D}_k = (X_k, Y_k)$, the data-label pairs. In offCL, the model is trained sequentially on each task while other task data is unavailable. For onCL, while the model is similarly trained sequentially on each task, only the data of the current batch is available and can be seen only once during training. The final objective in offCL and onCL is the same: to maximize performance across all tasks. Equivalently, to minimize the average loss $\\mathcal{L}$ across all tasks:\n$\\underset{\\theta}{\\text{m}}\\{{\\text{in}}} \\frac{1}{T} \\sum_{k=1}^T \\mathcal{L}(\\theta, \\mathcal{D}_k).$"}, {"title": "3.2. Finding the Optimal Learning Rate", "content": "Traditional Learning. Finding the optimal LR is a common challenge inherent to training any deep neural network with gradient-descent-based optimization techniques. Mainstream methods heavily rely on LR schedulers (He et al., 2016; Vaswani, 2017), which would typically decrease the LR value over time after each epoch. Of course, the starting value as well as the speed of the learning rate decrease must still be found. To this end, grid search remains a popular and powerful technique in traditional learning scenarios.\nLR in offCL. Similarly, in offCL, state-of-the-art methods have adopted LR schedulers (Smith et al., 2023; Wang et al., 2022b; Roy et al., 2024), however their strategy regarding finding hyperparameters is often unclear. While grid search is feasible in offCL at the task level, clear limitations can be identified. Firstly, finding the best initial LR at a given task k on dataset $\\mathcal{D}_k$ does not give any guarantee regarding the LR to use on subsequent datasets $\\mathcal{D}_{k+1},..., \\mathcal{D}_{T}$. Secondly, a suboptimal learning rate with regard to $\\mathcal{D}_k$ can lead to overall higher performances when evaluating on $\\{\\mathcal{D}_0,......, \\mathcal{D}_{k-1}\\}$. Indeed, as discussed in the work of Mirzadeh et al. (Mirzadeh et al., 2020), large learning rates can disturb weights that are important for previous tasks. The loss minimized at specific timestep conflicts with the overall objective since the distribution of incoming data from $\\mathcal{D}_k$ and $\\cup_{k'} \\mathcal{D}_{k}$ differ."}, {"title": "3.3. LR and Learning Behavior in onCL", "content": "Impact on Stability-Plasticity. It is clear that selecting an appropriate learning rate is essential for optimal performance. In standard scenarios, the impact of its choice on loss minimization and convergence speed has been extensively studied (Ruder, 2016). For offCL, previous studies have considered to impact of the LR on forgetting (Mirzadeh et al., 2020). Notably, a higher LR would increase forgetting, and vice-versa. Intuitively, the learning rate gives a direct control on the plasticity-stability tradeoff (Wang et al., 2024). To confirm such behavior in onCL, we experiment with larger and smaller LR values. As it can be seen in Figure 1, when trained with a higher learning rate (5 \u00d7 10-2), the model tends to obtain higher performances on the latest tasks while exhibiting especially low performances on earlier tasks. When trained with a lower LR (5 \u00d7 10-5), the model tends to achieve better performance on earlier tasks compared to training with a higher LR. In other words, a high LR value induces more plasticity and less stability, and vice-versa.\nFC Layer Behavior. For PTM in offCL, previous work considers different LR values for different layers (McDonnell et al., 2024). Notably, leveraging a higher LR in the last FC layer seems beneficial to the training procedure. In onCL, we observe similar behavior. To do so, we experiment with CODA (Smith et al., 2023) where we multiply the gradient values with regard to the final layer by a coefficient $\\kappa \\in \\{1,\\ldots,1000\\}$, with $\\kappa = 1$ falling back to the regular CODA training in onCL. Such results are presented in Table 1 on various onCL datasets. It can be seen that increasing the LR of the final FC layer can often lead to an improvement in terms of average performances, even though in cases of lower LR such an effect is barely noticeable. While this naive approach can lead to improvements in certain scenarios, it induces more hyper-parameter tuning and considers the same LR strategy for each class."}, {"title": "4. Proposed Method", "content": "4.1. Motivations\nAs discussed in Section 3, LR is critical in onCL as it not only has a direct impact on the plasticity-stability trade-off, it is near impossible to use the optimal value. While previous studies highlight the need for different LR at the layer level, the case of different learning rates at the class level is still in its infancy. Following the analysis described in Section 3.3, we make the hypothesis that a class-wise LR strategy focusing on the last FC layer is crucial for onCL. The intuition is that different classes require different learning rates so that the model can adapt its stability-plasticity tradeoff not only over time, but also over classes. In that sense, we reckon hypergradients (Baydin et al., 2018) to be adequate for improving over non-optimal initial LR values. However, hypergradients are not designed for the onCL scenario and a naive implementation leads to a severe performance drop. Indeed, hypergradient computation is identical regardless of the network layer or the classes. Inspired by the work of He et al. (He, 2024), we propose learning how to rescale the gradient coefficient of the last FC layer, class-wise, using hypergradient learning theory.\nAdditionally, to adapt PTM-based methods to the onCL scenario further, we leverage the stability of the output representation of PTM by computing Online Prototypes."}, {"title": "4.2. Class-Wise Hypergradients", "content": "Let us consider a model $f_\\theta$ parameterized by $\\theta$ such that for an input $x \\in \\mathbb{R}^d$, with d the dimension of the input space, we have $f_\\theta(x) = h_w(x)^T \\cdot W$, with $W \\in \\mathbb{R}^{l \\times c}$, c the number of classes, l the dimension of the output of $h_w$ and $\\theta = \\{w, W\\}$. In this context, $h_w$ would typically be a PTM and W the weight of the final FC layer (including the bias). Looking at the final FC layer W, for a learning rate $\\eta$, we can write the gradient-based weight update:\n$W_{t+1} = W_t - \\eta \\nabla \\mathcal{L}(W_t),$ \nwith t the iteration index. Here, we omit the input data for simplicity. Then, for any class index $j \\in \\{1,...,c\\}$, we can write the update rule of the weights corresponding to j:\n$W^j_{t+1} = W^j_t - \\eta \\nabla \\mathcal{L}(W^j_t),$ \nwith $W^j$ the jth column of W. Building upon previous studies (He, 2024), we introduce step-dependent class-wise weighting coefficients, leading to the following update rule:\n$W^j_{t+1} = W^j_t - \\alpha^j_{t+1} \\eta \\nabla \\mathcal{L}(W^j_t),$ \nwith $\\alpha^j_t \\in \\mathbb{R}^{+*}$ class dependent gradient weighting coefficient at index t. While those coefficients were traditionally introduced to compensate for class interference, and computed with hand-crafted rules, we propose to learn them through an online adaptive rule based on hypergradients theory. In particular, we want to construct a higher level update for $\\{\\alpha^j_t\\}$; such that, in the case:\n$\\alpha^{j}_{t+1} = \\alpha^j_t - \\beta \\frac{\\partial \\mathcal{L}(W)}{\\partial \\alpha^j_t},$"}, {"title": "4.3. Online Prototypes", "content": "In order to reduce forgetting in the last layer, we compute Online Prototypes (OP) $\\mathcal{P} = \\{p_1,p_2,\\ldots, p_c\\}$ of each class during training. For a given class j, the class prototype $p^j_k$ computed over $k_j$ samples is updated when encountering a new sample $x^j_{k_j+1}$. For simplicity, we omit the j index in $k_j$ going forward. Therefore, the prototype update rule is:\n$p_{k+1} = \\frac{k \\cdot p_{k} + h_w(x_{k+1})}{k + 1},$ \nwith $x_{k+1}$ the $k + 1$th encountered sample of class j. For all classes, prototypes are initialized such that $p_0^j = 0$. Prototypes are then used to recalibrate the final FC layer, analogous to replaying the average of past data representation during training. In that sense, we define the prototype-based loss term as:\n$\\mathcal{L}_{OP} = - \\underset{j \\in \\mathcal{C}_{old}}{\\text{log}} \\frac{e^{f_{\\theta}(p^j) \\cdot W^j}}{\\sum_{j=1}^c e^{f_{\\theta}(p^j) \\cdot W^j}},$ \nwith $\\mathcal{C}_{old} = \\{j \\in \\{1,c\\} | p_k^j \\neq 0\\}$. $\\mathcal{L}_{OP}$ is the cross-entropy with regard to prototypes of encountered classes. OP acts as simple and low-budget memory data."}, {"title": "4.4. Overall Training Procedure", "content": "The approach relies on two components that are orthogonal to most existing methods, as long as the model optimizes a final FC layer for classification. In that sense, we propose to integrate it into various state-of-the-art baselines relying on PTM, notably prompt-based approaches. To do so, if we consider $\\mathcal{L}_{base}$ to be the loss of the baseline method, to which we attach both components OP and CWH, we have the overall loss to minimize:\n$\\mathcal{L} = \\mathcal{L}_{base} + \\mathcal{L}_{OP}.$"}, {"title": "5. Experiments", "content": "In the following sections experiment with combining our proposed approach with several flagship CL approaches.\n5.1. Metrics\nAverage Performances (AP). We follow previous work and define the Average Performance (AP) as the average of the accuracies computed after each task during training (Zhou et al., 2024a). Formally, when training on $\\{\\mathcal{D}_1,......,\\mathcal{D}_T\\}$, we define $A_k$, the Average Accuracy (AA) as:\n$A_k = \\frac{1}{k} \\sum_{l=1}^k \\alpha_{l,k}$ \nwith $\\alpha_{l,k}$ the accuracy on task l after training on $\\mathcal{D}_k$. Building on this, we define the Average Performance (AP) as:\n$\\mathcal{P} = \\frac{1}{T} \\sum_{k=1}^T A_k$\nPerformance Across LR. To show the improvement in the case of unknown optimal LR, we propose to experiment with various LR values and report individual and averaged performances across these values. Specifically, we experiment for LR values in $\\{5 \\times 10^{-5},5 \\times 10^{-4},5 \\times 10^{-3}\\}$. The main motivation is that we reckon that the optimal LR is likely to fall into that range, therefore we wanna take into account the effect of experimenting with a learning rate that is either above or below the optimal value. Such a metric should emphasize the validity of the approach when the optimal LR is unknown and result in a fairer comparison than using the same LR blindly for every approach."}, {"title": "5.2. Experimental Setting", "content": "Baselines and Datasets. In order to demonstrate the efficiency of our approach as presented in Algorithm 1, we integrate it with several state-of-the-art methods in offCL. Notably, L2P (Wang et al., 2022b), DualPrompt (Wang et al., 2022a), CODA (Smith et al., 2023), ConvPrompt (Roy et al., 2024). These methods are not naturally suited for the online case, so they had to be adapted. More details on the adaptation of such methods are in Appendix D. Additionally, we include one state-of-the-art onCL method that leverages PTM, MVP (Moon et al., 2023). We evaluate our method on CUB (Wah et al., 2011), ImageNet-R (Hendrycks et al., 2021) and CIFAR100 (Krizhevsky, 2012). More details in Appendix B.2.\nClear Boundaries. We experiment in clear boundaries settings, for continuity with previous work, despite its lack of realism for onCL. In that sense, we consider an initial count of 10 classes for the first task, with an increment of 10 classes per task. This results in 10 tasks with 10 classes per task for CIFAR100, as well as 20 tasks with 10 classes per task for CUB and ImageNet-R.\nBlurry Boundaries. To evaluate our method in more realistic scenarios, we reckon the Si-Blurry (Moon et al., 2023) setting to be the most relevant to our study case. Specifically, we use their implementation of Stochastic incremental Blurry boundaries (Si-Blurry). We use the same number of tasks as for the clear setting. In this case, some classes can appear or disappear during training and the transitions are not necessarily clear. More details on this setting can be found in Appendix E. To adequately adapt the proposed methods to the online problem, we use batch-wise masking."}, {"title": "5.3. Experimental Results", "content": "We combined our method with four offline approaches and one online state-of-the-art approach, all using PTM. As mentioned in Section 4.4, our method must be applied on the classification head, therefore prompt-based approaches are especially suited as they all leverage a final FC layer on top of the PTM representation for classification.\nAverage Performances. We experiment in both clear and blurry settings and present the results in terms of Average Performance in Table 2 and Table 3. On top of datasets and boundary scenarios, we propose a novel evaluation procedure specific to the problem at hand. Namely, we present results for LR values in $\\{5 \\times 10^{-5},5 \\times 10^{-4},5 \\times 10^{-3}\\}$, as well as the AP across all these values. The objective is to observe how would the method perform on average when the optimal learning rate is unknown and might be far from optimal, by being either too high or too low. Therefore, in all cases, combining both proposed components leads to a significant improvement in AP over the baselines when looking at the average across learning rates, with up to 30% improvement on CUB. Such improvements are observed in both blurry and clear scenarios, confirming the ability of our approach to perform in realistic and traditional contexts. Individual contributions of OP and CWH are detailed below.\nBlurry Boundaries. Even though performance improvement can be observed as well in the blurry scenario, all methods suffer from a significant drop when transitioning from one scenario to the other. Such behavior highlights the importance of focusing on more realistic setups in future research. In that sense, our method remains completely applicable regardless of the presence of boundaries.\nAblation Study. To make apparent the contribution of each component of our method, we included the performances of the original baselines, followed by the performance of such baselines combined with OP (+ OP) and eventually the performance of the same baseline combined with prototypes and CWH (+ CWH). Such results are included in Tables 2 and 3. While it is clear that the usage of prototypes is largely beneficial, in some situations the addition of CWH can lead to a slight drop in performances. However, in those rare scenarios, the performance drop remains minimal with the maximum drop value being 3.79% in the case of CODA on CUB with an LR of 5 \u00d7 10-3. In other cases, performance loss is around 1%. Despite this limitation, it is important to note that the gain of including CWH is especially important for longer task sequences such as CUB, which is a more realistic scenario. Moreover, when the initial LR value is particularly low (5 \u00d7 10-5), the gain of including CWH is generally more important. When fine-tuning PTM, it is common to start with low LR values. This property further confirms the ability of our approach to perform in realistic contexts."}, {"title": "6. Discussions", "content": "6.1. Stability of OP\nDespite its apparent simplicity, OP gives the largest performance gain. This is partially due to the fact that only the FC layer and the input prompts are trained, so the output prototypes are stable over time. In Table 4 we show that unfreezing intermediate weight not only drastically decreases overall performances but similarly negates the effect of OP significantly. Additionally, in Figure 2, we show the average Euclidean distance of Online Prototypes between two training step iterations. As we can see, the learned prototypes tend to stabilize rapidly over time, even more so when the intermediate weights remain frozen. This stability of the prototype indicates that even when computed online, they can be used as a reasonable proxy for class average representation."}, {"title": "6.2. Selecting $\\gamma$", "content": "The main drawback of leveraging CWH is the addition of an extra hyper-parameter $\\gamma$, whereas our objective is to reduce the dependency on hyper-parameters for onCL. However, we argue that selecting $\\gamma$ remains particularly simple. To demonstrate this, we experiment with $\\gamma \\in \\{0,1 \\times10^{-5},1\\times 10^{-3}, 1\\}$, for an initial LR $\\eta \\in \\{5 \\times 10^{-5},5 \\times 10^{-3}\\}$. In that case, $\\gamma = 0$ is equivalent to disabling CWH. Such results are presented in Table 5. When the initial LR is lower, including CWH leads to an improvement in performance, especially when $\\gamma$ is large. Experimentally, this can be seen as a consequence of $\\{\\alpha^j_t\\}$ values increasing during training in all cases, as discussed in Section 6.3. Therefore, it is natural that lower initial LR values would benefit more from such a strategy. When the initial LR is large, as discussed in Section 5.3, a marginal drop in performance can be observed in some cases. However, the final performances remain stable for all values of $\\gamma$, minimizing the need for hyperparameter search. Additionally, the lower the value of $\\gamma$, the closer it is to the original method.\nIn conclusion, for higher LR we recommend lower values of $\\gamma$, as their effect on the training is reduced. For lower LR, any value of $\\gamma$ leads to an improvement, however, higher values are recommended for optimal accuracy gain. Default values such as 0.001 should lead to an increase in performances in most cases or a slight decrease in worst cases. Overall, selecting $\\gamma$ remains a simpler and more realistic task than doing a grid search in onCL."}, {"title": "6.3. Values of learned $\\alpha$", "content": "Trend and Distribution Across Classes. The class-wise coefficients $\\alpha^j_t$ are learned in each training step as described in Eq. (8). We report coefficients' evolution in Figure 3. Firstly, learned coefficient values are superior to 1 and increase during training. Such a trend is aligned with findings from previous studies suggesting adopting a higher LR value for the final FC layer (McDonnell et al., 2024). Secondly, it can be seen that coefficient values tend to be larger for later tasks than for earlier tasks. Intuitively, following the analysis from Section 3, this corresponds to giving more plasticity to newly encountered classes than older classes.\nRelation with Learning Rate. We investigate the behavior of the learned coefficient with regard to the initial LR. As we can see in Figure 4, the smaller the initial LR, the larger the values of learned coefficients. Intuitively, a smaller initial LR requires a larger compensation in the FC layer to reach a similar learning regime."}, {"title": "7. Conclusion", "content": "In this paper, we studied the problem of leveraging Pre-Trained Models in the context of Online Continual Learning. In that sense, we focused on two central problems. Namely, the unavailability of task boundaries and the unfeasibility of hyper-parameter search. We tackled the former by leveraging Online Prototypes, a simple yet powerful strategy that takes advantage of the stable representation output of PTM to use class prototypes as replay samples. For the latter, we introduced Class-Wise Hypergradients which can help mitigate the drop in performances due to inadequate LR value. To reflect the efficiency of our approach, we evaluate every model with various initial LR values and show that on average both OP and CWH can be beneficial when combined with baseline models. Nonetheless, the problem of using optimal hyper-parameters in an online context remains unsolved and we hope that this work can shed light on its importance and pave the way to additional research in this direction."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}, {"title": "A. Algorithm of our Adam Implementation", "content": "As explained in Section 4.2, the actual implementation that we used for our experiments is based on an Adam update. For the sake of clarity, we presented our method with SGD. Similarly, we omitted the bias from the pseudo-code. Therefore, we give the full details of the procedure in Algorithm 2, in a pseudo-code Pytorch-like implementation."}, {"title": "B. Datasets and Baselines", "content": "B.1. Datasets\nPTMs are often pre-trained on ImageNet-21k, making it unfair to experiment on such datasets. To showcase the performances of our approach we experiment with the following:\n\u2022 CUB (Wah et al., 2011): The CUB dataset (Caltech-UCSD Birds-200) contains 200 bird species with 11,788 images, annotated with attributes and part locations for fine-grained classification.\n\u2022 ImageNet-R (Hendrycks et al., 2021): ImageNet-R is a set of images labeled with ImageNet label renditions. It contains 30,000 images spanning 200 classes, focusing on robustness with images in various artistic styles.\n\u2022 CIFAR100 (Krizhevsky, 2012): CIFAR-100 consists of 60,000 32x32 color images across 100 classes, with 500 images per class, split into 500 training and 100 test samples per class.\nB.2. Baselines\nPrompt learning-based methods (Zhou et al., 2024a) are particularly suited for being combined with our approach in onCL as they all capitalize on a final FC layer for classification. Therefore, we consider the following.\n\u2022 L2P (Wang et al., 2022b): Learning to Prompt (L2P) is the foundation of prompt learning methods in Continual Learning. The main idea is to learn how to append a fixed-sized prompt to the input of the ViT (Dosovitskiy et al., 2021). The ViT stays frozen, only the appended prompt as well as the FC layer are trained.\n\u2022 DualPrompt (Wang et al., 2022a): DualPrompt follows closely the work of L2P by addressing forgetting in the prompt level with task-specific prompts as well as higher lever long-term prompts.\n\u2022 CODA (Smith et al., 2023): CODA-prompt improves prompt learning by computing prompt on the fly leveraging a component pool and an attention mechanism. Therefore, CODA benefits from a single gradient flow and achieves state-of-the-art performances.\n\u2022 ConvPrompt (Roy et al., 2024): ConvPrompt leverages convolutional prompts and dynamic task-specific embeddings while incorporating text information from large language models.\n\u2022 MVP (Moon et al., 2023): MVP uses instance-wise logit masking, contrastive visual prompt tuning for Continual Learning in the Si-Blurry context."}, {"title": "C. Additionnal Evaluation Metrics", "content": "Here we report additional metrics in the clear and blurry contexts for all methods for additional insights into the performances\nC.1. Final Average Accuracy\nWe report the final Average Accuracy $A_T$ as per the definition of Section 5.1. Such results are presented in Table 8 and Table 9.\nC.2. Average Performances on Old Classes\nWe report the Average Accuracy on old classes only at the end of training, $A_{T-1}$ as per the definition of Section 5.1. Such results are presented in Table 8 and Table 9. A higher value of performance in old classes indicates a better ability to retain knowledge, also known as stability.\nC.3. Additional Ablation\nWe include additional experiments with CODA on CUB to evaluate the impact of combining CWH only. Such results are presented in Table 6. As we can see, CODA still benefits the most from OP, however, it reaches the best performances in average across learning rate when being combined with OP and CWH.\nC.4. Time Complexity\nExperiments were run on various machines including Quadro RTX 8000 GPU, Tesla V100 16Go GPU, A100 40Go. In this section, we report the times of execution of each method, as well as the overhead induced by leveraging our components. To do so we run all methods on a single V100-16Go. The results are presented in Table 7. As expected, the time consumption overhead of including OP and CWH is minimal.\nC.5. Spatial Complexity\nClass-Wise Hypergradients. The usage of CWH solely requires storing one float per class (with c classes total) as well as previous gradient values in the last FC layer in the case of SGD. This amounts to a total of $c \\times c \\times (l + 1)$ additional floats to store. We multiply by l + 1 to account for the bias. For CIFAR100 a Vit-base, we have c = 100 and l = 768. In the case of Adam update, Adam parameters must equally be included.\nOnline Prototypes. Storing OP only requires one vector of dimension l per class, with l = 768 in the case of ViT base. Additionally, an extra integer per class must be stored to keep track of the index of the update of each OP. If the index is stored as a float, the additional amount of floating points to store is $c \\times (l + 1)$, with c the number of classes, and l the output dimension of the PTM.\nD. Adaptation of Methods to our setup\nSince most methods compared here were originally designed for offCL, they had to be specifically adapted to the onCL scenario. In that sense, some parameters have been chosen arbitrarily, based on their offCL values, without additional hyper-parameter search. Such a situation is similar to one that would be observed in real-world cases where an offCL model tries to be adapted to an onCL problem. For all methods, we use a fixed initial learning rate, no scheduler, and Adam optimizer. Of course, we disabled an operation that was"}]}