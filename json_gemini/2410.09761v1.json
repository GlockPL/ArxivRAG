{"title": "ChartKG: A Knowledge-Graph-Based Representation for Chart Images", "authors": ["Zhiguang Zhou", "Haoxuan Wang", "Zhengqing Zhao", "Fengling Zheng", "Yongheng Wang", "Wei Chen", "Yong Wang"], "abstract": "Chart images, such as bar charts, pie charts, and line charts, are explosively produced due to the wide usage of data visualizations. Accordingly, knowledge mining from chart images is becoming increasingly important, which can benefit downstream tasks like chart retrieval and knowledge graph completion. However, existing methods for chart knowledge mining mainly focus on converting chart images into raw data and often ignore their visual encodings and semantic meanings, which can result in information loss for many downstream tasks. In this paper, we propose ChartKG, a novel knowledge graph (KG) based representation for chart images, which can model the visual elements in a chart image and semantic relations among them including visual encodings and visual insights in a unified manner. Further, we develop a general framework to convert chart images to the proposed KG-based representation. It integrates a series of image processing techniques to identify visual elements and relations, e.g., CNNs to classify charts, yolov5 and optical character recognition to parse charts, and rule-based methods to construct graphs. We present four cases to illustrate how our knowledge-graph-based representation can model the detailed visual elements and semantic relations in charts, and further demonstrate how our approach can benefit downstream applications such as semantic-aware chart retrieval and chart question answering. We also conduct quantitative evaluations to assess the two fundamental building blocks of our chart-to-KG framework, i.e., object recognition and optical character recognition. The results provide support for the usefulness and effectiveness of ChartKG.", "sections": [{"title": "I. INTRODUCTION", "content": "DATA visualizations are used almost everywhere and a large amount of chart images (e.g. bar chart, line chart, and pie chart) have been created and accumulated online, where most of them are stored as bitmap images. Accordingly, various benchmark datasets of chart images, like VizML [19], VisImages [12], and PlotQA [45], have been created. Also, it has become increasingly popular to analyze chart images and conduct knowledge mining from them for various tasks and applications, such as chart retrieval [32], chart redesign [66], visual reasoning [52] and chart question answering [41].\nDepending on how chart images are represented for subse-quent knowledge mining, the existing methods for knowledge mining from chart images can be categorized into two groups: end-to-end methods [39], [52], [66] and data-extraction based methods [28], [41], [45]. The end-to-end methods often leverage deep neural networks to directly represent the hidden knowledge in the original chart images for the corresponding tasks. For example, ScatterNet [39] leverages deep learning models to capture the subjective similarity of scatterplots. However, such deep learning-based methods are often designed for one specific task and cannot be generalized to other purposes. Also, these methods usually work like a blackbox and lack explainability [52]. The data-extraction-based methods often conduct reverse-engineering and convert the original chart images to the extracted raw data of charts, and then all the subsequent analyses will be built upon the extracted raw data. For instance, ReVision [50] utilizes image processing techniques to identify graphical marks and infer the underlying data, which is further used for the redesign of charts. But these methods focus on extracting the data encoded in charts, and do not explicitly provide a generic and easy-to-use representation to preserve the critical information of charts such as data, visual elements and the corresponding relationships among them (e.g., the height of a bar in a bar chart encodes the math score of a specific student), which, however, is critical for many downstream chart image analysis tasks like automated infographics design [9]. In summary, a unified, expressive, and explainable representation for chart images that can facilitate the knowledge mining of chart images is still missing.\nInspired by the recent research in knowledge graphs [33], we aim to explore the possibility of representing chart images as knowledge graphs to facilitate downstream knowledge-mining tasks of chart images. However, it is a non-trivial task and the challenges originate from two major aspects: unified representations and automated extraction of entity and relationship. First, there are various charts with different visual elements and visual encodings. It is difficult to represent all the necessary information of different chart images in a unified, expressive, and explainable manner. Second, chart images consist of different visual elements and encode data with different visual channels. It remains unclear on how to automatically extract visual elements as well as their relations with a good accuracy.\nTo tackle the above challenges, we develop ChartKG, a"}, {"title": "II. RELATED WORK", "content": "There have been a series of research studies on extracting various information, such as the underlying data and detailed visual encodings, from charts. Such extracted information will be further used for visualization re-designing or indexing. For example, Jung et al. [25] proposed an interactive chart data extraction tool ChartSense, which extracts the underlying data from chart images based on a deep learning-based clas-sifier and a semi-automatic interactive extraction algorithm. M\u00e9ndez et al. [44] presented a web-based tool iVoLVER, which requires a relatively large number of interactions to accurately extract data from chart images and reconstruct the representation of data. ReVision [50] is an automatic approach that classifies chart types and extracts data from chart images, which can extract data from bar charts and pie charts. Choi et al. [10] developed a Google Chrome extension to automatically extract data from charts in web pages and help people with visual impairment understand the web content. Harper and Agrawala [16] presented a method"}, {"title": "B. Knowledge Graph Construction for Images", "content": "Recent years have witnessed the resurgence of knowl-edge engineering featured by the fast growth of knowledge graphs [69]. A knowledge graph (KG) is essentially a semantic network that models the semantic relationships among differ-ent entities and has been applied to modeling different types of data including images. Specifically, modeling images as knowledge graphs has been used in a wide range of real-world applications, including image understanding [60], [64], visual retrieval [67] and visual question answering [65]. For example, various methods have been proposed to generate scene graphs (SG) for natural images, which serves as an abstraction of objects and their complex relationships within scene image [36]. It not only provides fine-grained visual cues for low-level recognition tasks, but has further proven their applications on numerous high-level visual reasoning tasks, such as visual question answering (VQA) [59], [68], image captioning [6], [61] and scene synthesis [22], [24]. Besides natural images, semantic knowledge graphs have also been constructed to model the semantic relationship and domain knowledge of remote sensing images [34], [58]. In summary, knowledge graphs have been used to represent the semantic information of various images. Inspired by the powerful capability of knowledge graphs in representing the semantic information of various images, we propose leveraging knowledge graphs to model the semantic information of chart images."}, {"title": "C. Chart Retrieval", "content": "Charts have emerged as valuable search targets, especially when they contain data not easily obtainable through alter-native sources. With a plethora of chart data at hand, the need to retrieve charts tailored to a user's specific require-ments becomes paramount. At the core of chart-based queries lies the primary data retrieval question, a concern that has been explored in earlier research [21]. This question can often be addressed by executing essential operations on the extracted chart data. Some studies have leveraged template-based questions, offering a structured way to specify data sources and variables [54]. Existing methods for visual chart retrieval primarily fall into two categories: task-oriented ap-proaches, primarily focused on matching queries to charts, and keyword-based techniques [7], [31]. The latter category involves matching queries against specific textual roles or chart-style properties [7], [18]. Additionally, certain methods prefer more elaborate queries, employing automated entity detection techniques within queries, and comparing them to comprehensive textual descriptions of the charts. These iden-tified entities are then matched with the content stored within the index, thereby enhancing matching accuracy [35]. To improve query completeness, some approaches consider query expansion by including synonyms of query keywords [7], [35]. However, these methods are often constrained to basic keyword searches, which may not adequately capture complex semantic aspects such as trends. Our method overcomes this limitation by effectively representing the fundamental elements in charts and their relationships, while also embedding visual insights akin to trends. Leveraging ChartKG, we enable more sophisticated queries based on entities and relationships, en-compassing aspects like visual encoding and visual insight."}, {"title": "D. Visual Question Answering", "content": "The Visual Question Answering (VQA) task is designed to produce informative responses to natural language questions about specific images. It represents a convergence of com-puter vision and natural language processing (NLP), holding significant promise for advancing interdisciplinary research. Currently, two prominent methodological models dominate the VQA landscape.\nThe first model is the classification-based visual QA model, which leverages encoders to represent both the query and the image. It employs attention mechanisms to seamlessly merge the distinctive features of the query and image before performing classification. Kafle et al. [26] have contributed substantially to this approach. They have introduced robust baselines, including an end-to-end neural network and a dy-namic local dictionary model. Their work not only automates the extraction of numerical and semantic information from bar graphs but also incorporates a chart QA algorithm that intelli-gently combines question and image features. This intelligent fusion facilitates the aggregation of learned embeddings to effectively answer posed questions [27]. Additionally, architec-tures focused on chart element localization and QA encoding for chart elements [3], [56] have offered valuable insights into improving QA methods. However, it's crucial to note that these approaches are often tailored to specific local problems. Another distinct approach is the table QA method. It either assumes the presence of a data table for the image [29], [40] or employs visual techniques to extract the data table directly"}, {"title": "III. KG-BASED REPRESENTATION FOR CHART IMAGES", "content": "Chart exists in various forms, such as bitmap images, SVG, and program specifications [5]. In this paper, we focus on chart images, i.e., charts in the format of bitmap images, since they are widely seen and charts in other formats like SVG and program specifications can be easily converted to bitmap images. Therefore, we create a knowledge graph to depict the relationships among the entities within the bitmap-image-based charts.\nAccording to the previous work [49], the line chart, bar chart, pie chart and scatter plot are the widely used and basic chart types. We select these four types of charts as representatives to achieve a unified expression of the knowl-edge graph, which is convenient for future extension to other chart types. Since entities and relationships form the core components of our knowledge graph, we will elaborate on our knowledge-graph-based representation by defining entities and relationships.\nThe initial step in constructing our knowledge graph is to define the entity type. Charts consist primarily of or a combination of the following elements [8]: title, x-axis title, y-axis title, legend title, x-axis label, y-axis label, legend label, and graphical mark. Based on these elements we constructed five types of entities.\nVisual elements (VE), are the key components of charts, such as graphical marks (bar, line, and so on) and axis.\nVisual element property values (VEPV), such as bar height or line color, are typically encoded with different data values and are therefore considered entities in the knowledge graph.\nData variables (DV), including x-axis-title, y-axis-title, and legend-title, which contain crucial semantic informa-tion pertaining to data values.\nData variable values (DVV), including x-axis-label, y-axis-label, and legend-label, which convey specific data values for both categorical and continuous data, are also essential entities for proper comprehension of the chart.\nVisual insights (VI). we consider the easily perceptible visual insights within charts as a distinct entity type. By grouping these elements into separate entities, we can more effectively model the relationships between them in our knowledge graph.\nRelationships. After introducing five classes of entities, we further define four classes of semantic relationships between different entities within a chart: visual property correspon-dences, data variable correspondences, visual encoding map-pings and visual insight correspondences. A comprehensive list of these relations can be found in Figure 2.\nVisual property correspondences. This type of relation-ship aims to represent the property values of the visual elements. It connects visual elements to visual element property values (VE \u2192 VEPV), for example, \"the color of a bar is blue\".\nData variable correspondences. It focuses on specific instances of data variables, for example, \"England\" and \"America\" can be the value of the \"Country\" variable. We connect data variable values to data variables (DVV \u2192 DV), representing the relationship between variables and values, such as \"2011 is a value of Year\".\nVisual encoding mappings. The visual encoding map-pings express the semantic information conveyed by the underlying data and model the mapping from visual element property values to data variables or data variable values (VEPV \u2192 DV), for example, \"the color of a bar is blue representing England.\".\nVisual insight correspondence. It is employed to represent complete visual insights, which typically describe the un-derlying characteristics between different data variables and data variable values. We establish relationships be-tween the existing visual insights and corresponding data variables/data variable values (DV\u2192VI and DVV\u2192VI) to completely represent the quick insights. QuickInsight [13] presents 12 data insights that are easily and rapidly per-ceptible across various visualization charts. Using these insights as a reference, we have embedded the 12 visual insights into our knowledge graph, aiding machines to quickly retrieve charts that match insights.\nOnce all entities and relations are defined, we extract triplets from existing chart images. These triplets serve as edges connecting individual entities and thereby constructing a knowledge graph representation for a chart. Our knowl-edge graph provides a comprehensible representation of the relations between chart elements and data information by as-signing them to different nodes. Our KG-based representation enables quick and effective chart downstream tasks such as chart retrieval and chart question answering."}, {"title": "IV. CHART-TO-KG CONVERSION FRAMEWORK", "content": "Chart-to-KG is a framework to generate knowledge graphs from chart images without any user interaction. Determining the chart type and subsequently extracting chart elements based on the specific chart type is a fundamental step. Dif-ferent types of charts contain distinct chart elements, making it essential to identify the chart type to accurately extract these elements. So the framework mainly consists of three modules: chart classification, chart parsing, and knowledge graph construction as shown in Figure 1. Firstly, Convolutional Neural Networks (CNNs) are applied to classify an input chart image to identify the chart type (Section IV-A). Then we introduce different object recognition methods for different chart types (Section IV-B), to ensure the scalability of the framework. Finally, we provide a series of rules to construct a knowledge graph of chart elements (Section IV-C)."}, {"title": "A. Chart Classification", "content": "We categorize the input chart images into four groups: bar chart, line chart, pie chart and scatter plot by ResNet50 [17]. The domain of chart classification has also witnessed the extensive utilization of deep learning techniques [20], [55], [62]. Among these options, ResNet50 stands out as a suitable choice for chart classification due to its relatively low number of parameter counts and high accuracy [63]. We choose ResNet50 and employ a pre-trained ResNet model on the Imagenet dataset [48]. We fine-tuned the model on our corpus using the Adam optimizer with a learning rate set to 0.0005. The input chart image will be divided into four types by the trained model. After testing, the model achieved an accuracy of 87.2% on our dataset."}, {"title": "B. Chart Parsing", "content": "To extract entities and relationships from a chart, it is essential to analyze the various elements used to convey information in the chart. In this section, we will outline the three fundamental steps of chart parsing: object recognition, optical character recognition, and graphics mark parsing.\nThe initial step to extract entities from charts is to extract all its elements. We summarized eight types of elements from common standard charts (Section III) since different types of charts contain various visual elements. Although their visual forms are different, most common standard charts consist of some or all of these elements. Many object detection models are proposed to detect the bounding boxes and recognize the object class in natural images. YOLOv5 [23] is a well-known object detection model with remarkable accuracies and scalabilities. Also, it can be easily extended to detect elements within charts. Therefore, we use YOLOv5 to detect the categories and bounding boxes of the chart elements. Due to the different types of elements and features in various charts, we adopt the approach of training an object detection model for each chart type to improve the scalability of our method. This approach allows us to add new chart types more efficiently. When adding a new chart type, we only need to further train the model on the data specific to these new types, without the need of retraining the model on the entire dataset containing all chart types. The corpus required for training the model includes annotations of the bounding boxes of chart elements, as well as their corresponding labels under that chart type. The corpus will be introduced in Section V-A. Once the training of the model is complete, we can use it to predict the labels and bounding boxes of elements in the new charts. The final YOLOv5 model returns one bounding box for each element within the charts, with possible labels and the bounding box. Furthermore, we observed that the accuracy of recognizing text and symbols within legends was relatively low. Therefore, we developed a specialized legend recognition model to enhance the accuracy of identifying color markers and text within legends. Once we obtain the chart type through the classification model, we can easily determine the element categories and bounding boxes using the corresponding element extraction and legend extraction models.\nTexts within a chart are crucial for conveying the meaning of the underlying data semantics. For example, the title, x-axis label, and y-axis label provide detailed context about the data being presented. It is imperative to accurately identify the textual content in order to extract meaningful semantic information from a chart. To extract specific textual information from chart images, optical character recognition (OCR) technology is used in our framework. Numerous techniques have been proposed in the literature for extracting texts from charts [1], [53], [57]. Our study employs Tesseract [57] an open-source OCR engine developed by Google, to enable the retrieval of textual information from identified bounding boxes of texts, which the object recognition model detects. Given the robust text recognition capabilities of the pre-trained Tesseract module, we directly utilize it to extract text content from text bounding boxes at this stage. First, we crop the images based on the bounding boxes and magnify them threefold to enhance character recognition accuracy. The resulting partial images are then input into the Tesseract module to obtain the final text content. By combining the text roles obtained from the Object Recognition phase, we acquire complete textual data with both their role and the context, along with the confidence score. In addition, our experiments with the pre-trained Tesseract module on our dataset demonstrate its strong performance (Section V-C).\nFor standard charts, graphic marks represent the main encoding method for data, and the major differences between different types of charts rely on the use of graphical marks. Hence, it is necessary to analyze the graphical marks specific to each type of chart. According to the prior study [2], position, color, and size are the most commonly used visual channels. Therefore, we will specifically introduce how to parse the graphics mark focus on size, color, and position in bar charts, line charts, pie charts and scatter plots.\nBar chart. The graphics marks in bar charts are bars. We will extract the size, color value, and position index of the bars. The object detection model is responsible for identifying bars within bar charts and determining the coordinates of their"}, {"title": "C. Knowledge Graph Construction", "content": "After completing the chart parsing process, we perform entity classification and relationship construction based on the extracted content, thus building the final graph.\nWe initially categorize the parsed chart elements into five distinct entity types. According to the entity definitions outlined in section III, the graphic marks ex-tracted from the charts are treated as multiple entities, denoted as VE, with each mark's property values such as color and size categorized under the entity type VEPV. As for DV and DVV, the title of chart components typically describes variables, while the labels of the chart components are generally used to represent specific variable values. Therefore, we classify the textual content in the chart into two categories, DV and DVV, based on their text roles. Finally, visual insights encap-sulate vital semantic information represented within charts, challenging to directly glean from raw data. We collate the property values of graphical marks into data tuples (e.g., (height, index, color)). Furthermore, we converted the color and index from the data tuples into rows and columns, and used the height as the data value. Then we transform them into a table, which is the input format required by QuickInsights. Then we extract the insights within a chart and treat each extracted visual insight as an individual entity building upon QuickInsights [13].\nAfter classifying entities, the relationships including visual property correspondences, data variable correspondences, visual encoding mappings, and visual insight correspondences will be constructed using rule-based methods as follows:\nVisual property correspondences. The visual property cor-respondences describe the properties of graphical marks. When we extract the visual elements, the property values of visual elements will be parsed, as shown in Sec-tion IV-B3. Then the link between the visual element and its property values will be constructed. For example, we will link a bar with the value of the bar height.\nData variable correspondences. The data variable corre-spondences focus on specific instances of data variables, for example, \"England\u201d and \u201cAmerica\u201d can be the value of the \"Country\" variable. After classifying the entities, we obtained DVs and DVVs associated with the chart el-ement labels. Based on the structure of the chart elements corresponding to the data variables and their values within the charts, we established rules, such as the label being an instance of the title. Typically, in charts with axes, the variable values represented by x-labels are instances of the variables represented by x-titles. For example, \"2011\" is an instance of \"Year\".\nVisual encoding mappings. The visual encoding map-pings represent the semantic information of the under-lying data, modeling the relationship between visual element properties and data variables or their values. To establish the visual encoding mapping relationship, we link the VEPV to the DVV and the VEPV to the DV, according to the similarities such as distance similarity, color similarity and so on. For example, for bar charts, we calculate the distance between each x-axis label and the bar by the left corner of their bounding boxes, and we designate the text of the x-axis label located closest to the bar as the position index of the bar. Furthermore, the relationship between the bar and legend text is established by determining the similarity between their respective colors.\nVisual insight correspondences. The visual insight corre-spondence relationships are utilized to represent complete visual insights, which typically describe the underlying characteristics between different data variables and their values. When we extract a visual insight and treat it as"}, {"title": "V. QUANTITATIVE EVALUATION OF CHART-TO-KG CONVERSION FRAMEWORK", "content": "To evaluate the effectiveness of our framework in construct-ing the KG-based representation for charts, we conducted quantitative assessments of its key components, object recogni-tion and optical character recognition. Section V-A describes the datasets used for these evaluations."}, {"title": "A. Corpus", "content": "Since our framework encompasses chart classification, ob-ject recognition, and optical character recognition, it is im-perative that we have a sufficient amount of training data to train our models and employ testing data to validate the effectiveness of our framework. Due to the limited com-putation resources, we obtained a subset corpus from the PlotQA [45] with rich chart elements annotation information, which includes three chart types: bar charts, line charts, and dot plots. Since the difference between a dot plot and a line plot primarily lies in whether the dots are connected or not, we replaced the dot plots with pie charts and scatter plots which are used more widely. Using the matplotlib package, we generated a batch of charts with corresponding annotation information automatically. Our final corpus consists of four types of charts, each with a quantity of up to ten thousand. To be consistent with the procedure used in PlotQA, we randomly selected 70% of each chart type for training, 15% of each chart type for validation, and 15% of each chart type for testing. Then the corpus will be used to evaluate the performance of the object recognition and OCR."}, {"title": "B. Object Recognition Evaluation", "content": "Metrics. We assessed the performance of object recognition using several key evaluation metrics, including Mean Average Precision (mAP) [14], Precision, and Recall. These metrics are fundamental in evaluating the effectiveness of object recogni-tion models. Mean Average Precision is a widely used metric that provides a comprehensive measure of object recognition performance. It considers the precision-recall curve and calcu-lates the average precision across different levels of confidence thresholds. Specifically, we calculated mAP at two different Intersections over Union (IOU) settings, where IOU measures the degree of overlap between predicted and ground truth bounding boxes. The first setting, mAP at IOU 0.5, focuses on evaluating the model's ability to accurately detect objects when there is at least a 50% overlap between the predicted and ground truth bounding boxes. The second setting, mAP from IOU 0.5 to 0.95, provides a broader perspective, considering the model's performance across a range of IOU thresholds. Precision evaluates recognition accuracy, representing the ratio of correctly predicted objects to the total number of predicted objects. In contrast, recall measures the model's capacity to identify all instances of an object class by calculating the ratio of correctly predicted objects to the total ground truth objects of that class. By employing these evaluation metrics, we were able to comprehensively analyze and report the performance of our object recognition method, providing valuable insights into its precision, recall, and across different IOU thresholds.\nResults. Table I shows the evaluation results, which indi-cate that our framework performs well in terms of the metrics. Across various object categories, the majority of them exhibit excellent recognition rates, with mAP50, Precision, and Recall scores consistently exceeding 0.9 and mAP50-95 mostly above 0.7. However, the recognition of line charts seems to be less effective, even though they achieve mAP scores above 0.7 or close to 0.7. We believe that the reason behind the relatively lower recognition scores for line charts and scatter plots might be due to their low number of pixels, making their features challenging to identify. Despite these challenges, it's worth noting that our framework achieves mAP scores above 0.7 for these categories. This demonstrates that, while there is room for improvement in recognizing line charts, our system still provides reasonably accurate and reliable results for these object types. The evaluation results show that our chart KG construction framework can achieve object recognition with high accuracy and reliability."}, {"title": "C. Optical Character Recognition Evaluation", "content": "The textual content within charts primarily originates from the titles and labels of elements(Section III). We evaluate the recognition accuracy of each text category by comparing the predicted text content with the annotated text content. The specific accuracy results are depicted in the Appendix. Most categories of text OCR accuracy are above 70%. Although errors in OCR do not lead to issues in the construction of the chart structure, they can result in inaccuracies of the extracted variable names, values, and other elements within the chart, which can affect the accuracy of downstream tasks. However, in the future, OCR accuracy can be further enhanced with the introduction of more comprehensive training data and models."}, {"title": "VI. CASE STUDY", "content": "To demonstrate the effectiveness of ChartKG in charts, four knowledge-graph-based representations generated for real charts and a generated chart are presented in Figure 3. We will introduce the content of the charts and explicate the information captured in the knowledge graphs from three aspects: data information, visual encoding information, and visual insights.\nFigure 3 depicts four charts along with their corresponding knowledge graphs. The bar chart illustrates the savings of different countries in 2010, with the bar representing the Arab region notably higher than the others, making it visually prominent. The line chart displays the variations in education costs for India and Ukraine over the years. Both countries show an upward trend in education spending from 2006 to 2008, indicating a positive correlation. The pie chart illustrates the Browser Market Share in North America in 2020, where it's evident that Chrome dominates the market with a share exceeding 50%. Lastly, the generated scatter plot exhibits two categories of data points, one of which includes noticeable outliers.\nIn the generated knowledge graphs, two types of entities, DV and DVV, express the main topic of the chart, which indicates the semantic content of the data described in the charts. As an example, Figure 3a illustrates the adjusted net savings of different countries in the year 2010. The information presented in the chart employs named entities, leading to their clear interpretation. Furthermore, the path VE \u2192 VEPV \u2192 DVV represents the data label bound by visual elements, while the path VE \u2192 VEPV \u2192 DV indicates the data size bound by visual elements. Specifically, the blue path in Figure 3c represents the visual element PieSlice0 with the \"Chrome\" data label, while the red path represents the data value bound by the same visual element, which is 0.55. By considering the meanings of these paths, we can infer that the proportion of Chrome is 0.55 based on the complete path meaning.\nThe chart knowledge graph showcases the utilization of visual encoding methods, which are related through VEPV \u2192 DV or VEPV \u2192 DVV. As illustrated in Figure 3a, the red path denotes the height of the bar, which represents the value of \"Savings\". Meanwhile, the position index indicates the arrangement order of elements, mapping to their respective values on the x-axis variable. In Figure 3b, the red paths represent the color encoding of lines for different countries, where blue corresponds to \"India\", and brown corresponds to \u201cUkraine.\"\nTo facilitate a comprehensive machine understanding of the underlying semantics conveyed by charts, we embed salient visual insights from the chart into the KG, as indicated by the gray shaded region in Figure 3. Specifically, as depicted in Figure 3a and c, where the \"The Ara\" bar and the \"Chrome\" pie slice are significantly higher or larger than the other elements, we represent this feature using associations between VI and DV/DVV entities. Similarly, in Figure 3b, it can be observed that there is a clear correlation between \u201cIndia\u201d and \u201cUkraine\u201d, with both showing a significant upward cost trend. In Figure 3d, the clustering characteristics of these"}, {"title": "VII. EXAMPLE APPLICATIONS", "content": "We further demonstrate the usefulness of chartKG via two example applications semantic-aware chart retrieval and visual question answering. we only need to conduct straightforward matching of graph nodes and edges for downstream chart-related tasks, which is interpretable. Firstly, we introduce a chart semantic retrieval method designed to cater to a wider range of user retrieval requirements. This highlights the flexi-bility and user-friendly nature of Chart KG in managing chart databases. Moreover, we accomplish the task of generating textual descriptions for charts using a predefined template approach. In comparison to existing model-based methods, our approach is lightweight and exhibits higher levels of accuracy and interpretability. Additionally, since our chartKG does not focus on precise data extraction, it cannot be directly used for tasks related to data understanding. Instead, further data extraction is needed to replace the visual element property values in the KG."}, {"title": "A. Semantic-aware Chart Retrieval", "content": "Chart retrieval that aligns with user preferences is a basic chart downstream task. Conventional chart retrieval methods are confined to keyword-based matches within textual ele-ments of charts [7]. However, with the escalation of data volume, relying solely on keywords for chart matching has become inadequate to fulfill users' retrieval needs, especially when delving into the deeper semantic expressions encapsu-lated by charts. For instance, the task of locating a line chart depicting a declining trend in educational expenditure from a massive pool of charts illustrates this challenge. To address this, we implement semantic-aware chart retrieval using the ChartKG. It effectively showcases the advantages of ChartKG.\nTo enhance retrieval efficiency and facilitate user interaction, we format the user's input and employ conditional filtering. Users are prompted to sequentially provide the chart type, DV/DVV, encoding relationship, and existing visual insights. The specific steps are as follows:\nStep 1: Input formatting. Users specify the chart type, key entities, and the relationships among entities in sequence based on their desired sentence.\nStep 2: Chart type matching. As our chart dataset is stored in groups based on chart types, it is facile to filter out a significant portion of the dataset using the specified chart type.\nStep 3: Entity matching. Leveraging our pre-extracted DV and DVV, we construct a variable dictionary to match user-input variables and variable names, further narrowing down the scope of target charts.\nStep 4: Relation matching. Traversing through chart knowl-edge graphs that satisfy the above conditions, we sequentially match encoding relations using our defined relationship types associated with visual encodings. The remaining outcomes represent charts that align with user requirements."}, {"title": "2) Evaluation:", "content": "To demonstrate the superiority of our KG in semantic-aware chart retrieval, we conducted ten retrieval experiments, comparing with the keyword-based method [7] and our chartKG-powered chart retrieval method. These ex-periments comprised five queries targeting basic variables and another five queries targeting insights within the charts. Subsequently, we enlisted the evaluations of three domain scholars in visualization to assess the retrieval results. Some example results are presented in Figure 4. The alignment between retrieval results and retrieval criteria is crucial for evaluating the effectiveness of retrieval methods. We presented the retrieval criteria and results separately to three users, without disclosing which retrieval method corresponded to the results. We then asked them to rate the retrieval results based on whether they met the retrieval criteria, using a scale ranging from one (least satisfactory) to five (most satisfactory). The average of the ratings from the three users was taken as the final score for the retrieval results. Additionally, retrieval time is often a critical concern for users, as slow retrieval is not desirable. We also calculated the average time taken by different methods during the retrieval process to demonstrate the efficiency of our approach in terms of time."}, {"title": "3) Result:", "content": "The user satisfaction with the retrieval results and the efficiency of retrieval for both methods are shown in the Appendix. In the retrieval of variables, our chartKG-powered chart retrieval method and the keyword-based method are closely matched in terms of time and user ratings. This indicates that our ChartKG is capable of performing the most basic semantic retrieval tasks efficiently. For semantic retrieval, our average rating exceeds 4.0, which is much higher than the keyword-based method. Moreover, our time consumption is only slightly increased, by just a few seconds. This clearly demonstrates that our chartKG-powered chart retrieval method can meet users' various retrieval requirements more accurately without significantly sacrificing time efficiency."}, {"title": "B. Visual Question Answering", "content": "Visual question answering (VQA) is a crucial task in the field of visual data analysis [41]. However, existing methods for VQA often require the joint training of both the charts and the associated questions, leading to limited scalability and interpretability of the resulting models due to the complex training process [27]. In contrast, utilizing a knowledge graph as a backbone can benefit VQA due to its ability to provide a structured and interpretable representation of relevant knowl-edge, which in turn enhances the interpretability of the VQA models and results. We demonstrate how ChartKG can en-hance visual question answering. We validated its effectiveness through experiments."}, {"title": "1) KG-based chart question-answering:", "content": "Through graph-based retrieval and inference, our knowledge graph enables tasks related to data comparison, visual encoding queries, and visual insight reasoning. Therefore, based on our Chart KG, we have designed three question templates, as detailed in the Appendix, to encompass aspects of data comparison, visual encoding, and visual insight. As the chart knowledge graph we proposed does not focus on the original data, specific data"}, {"title": ""}]}