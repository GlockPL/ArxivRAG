{"title": "Towards Transparent and Accurate Diabetes Prediction Using Machine Learning and Explainable Artificial Intelligence", "authors": ["Pir Bakhsh Khokhar", "Viviana Pentangelo", "Fabio Palomba", "Carmine Gravino"], "abstract": "Diabetes mellitus (DM) is a global health issue of significance that must be diagnosed as early as possible and managed well. This study presents a framework for diabetes prediction using Machine Learning (ML) models, complemented with explainable Artificial Intelligence (XAI) tools, to investigate both the predictive accuracy and interpretability of the predictions from ML models. Data Preprocessing is based on the Synthetic Minority Oversampling Technique (SMOTE) and feature scaling used on the Diabetes Binary Health Indicators dataset to deal with class imbalance and variability of clinical features. The ensemble model provided high accuracy, with a test accuracy of 92.50% and an ROC-AUC of 0.975. BMI, Age, General Health, Income, and Physical Activity were the most influential predictors obtained from the model explanations. The results of this study suggest that ML combined with XAI is a promising means of developing accurate and computationally transparent tools for use in healthcare systems.", "sections": [{"title": "I. INTRODUCTION", "content": "Diabetes is a global healthcare issue with major prevalence increasing and millions living with the disease [5]. In 2020, type-2 diabetes was among the top ten causes of death, as listed by the World Health Organization [30]. Diabetes mellitus also warrants particular comment due to its chronic nature and many associated complicating conditions, including cardiovascular disease, neuropathy, retinopathy and kidney failure [11], [15], [20]. As a result, prediction of early and accurate onset of diabetes is essential to improve patient outcome and quality of life [25]. Interventions started earlier can help prevent serious complications of the disease in high-risk individuals.\nThe use of Machine Learning (ML) models to predict and diagnose diabetes from true patient data has been exploited widely [3], [4], [25]. While such models have achieved good predictive performance in predicting diabetes with high accuracy [3], [4], the interpretability of these models has rarely been addressed. Predictions made using complex algorithms are frequently challenging for healthcare professionals to comprehend, limiting their practical applicability in clinical settings. Thus, research has begun to employ eXplainable Artificial Intelligence (XAI) techniques [19], [21] to enhance the transparency of ML predictions. Such increased transparency and interpretability enable healthcare providers, particularly those who are not experts in AI, to interpret, trust, and rely more confidently on the outputs of predictive models.\nHowever, despite significant advancements in XAI techniques and their growing application in interpreting ML model predictions for diabetes, these methods are often treated as isolated black-box tools, primarily focusing on evaluating the results of individual ML algorithms [17], [24], [27]. Indeed, we identified a research gap in the lack of comprehensive empirical studies evaluating both ML and XAI methods in the context of diabetes mellitus prediction. Most existing studies are limited in scope and typically rely on limited datasets [27] or focus on a single XAI algorithm [6], [15], which constrains the generalizability of their findings. Furthermore, no considerations are made for XAI algorithms, nor are any assessments provided on their performance and suitability for explaining health predictions. These limitations highlight the need for a more extensive evaluation of ML and X\u0391\u0399 techniques in this domain.\nThe present study aimed to address the identified research gap by conducting a large-scale empirical analysis on a diverse set of ML models, including both simple and ensemble approaches tailored for diabetes mellitus prediction. We evaluated ML models based on well-established performance metrics and applied multiple XAI techniques to interpret their predictions. Moreover, we performed a detailed comparative analysis not only on the models themselves but also on the XAI methods. By leveraging the comprehensive Diabetes Binary Health Indicators dataset, derived from the Behavioral Risk Factor Surveillance System (BRFSS) and available at Kaggle, our ultimate objective is to generate in-depth insights into the effectiveness and usability of both the models and their interpretability techniques, ultimately enhancing the general-izability and accessibility of results for medical practitioners who are not Al experts."}, {"title": "II. LITERATURE REVIEW", "content": "The prediction of diabetes using ML techniques has been a subject of considerable interest, particularly in recent years [3]. However, in the medical context, achieving accurate predic-tions and identifying the most influential factors to provide valuable explanations and facilitate a deeper understanding of the data and models [26] is crucial. In this regard, techniques aimed at enhancing the clarity and transparency of models, namely XAI techniques, are being investigated to explore their potential for diagnosing and elucidating diabetes.\nIn this section, we present an overview of the state of the art concerning the application of XAI techniques to diabetes prediction, the most frequently considered settings, and the primary insights obtained. Detailed information from related literature is presented in Table I.\nLocal Interpretable Model-agnostic Explanations (LIME) [21] and SHapley Additive explanations (SHAP) [19] are among some of the ways most often used in the literature. There have been two of these algorithms for producing ML model prediction explanations, and they have become so popular in this context that they serve as the base for many studies in the domain. LIME provides individual prediction explanations rather than the entire model. In particular, it has been used to explain predictions in ML models by detecting key features such as glucose levels [16], [28]. On the other hand, SHAP is already a popular XAI technique based on game theory: it produces both local and global interpretability by distributing feature contributions across all possible combinations. Different models have been utilized to improve clinical decision support systems further and highlight influencing predictors such as HbAlc and glucose levels, with high accuracy, across various models [6]\u2013[8]. SHAP has also been used to find the critical regions in ocular images and to enhance the prediction accuracy [23]; however, ensemble methods such as Eye-Net have shown promising performance in automated screening systems [18].\nMost importantly, LIME and SHAP are not mutually exclusive. These have often been combined or compared to realize greater insights into model predictions. Studies integrating both techniques have demonstrated their utility in identifying key features, such as glucose and BMI, enhancing the understanding of global and local model behavior [10], [17], [24]. Comparative analyses also suggested that LIME may offer greater stability in certain cases [1]. The most recent trend in the literature indicates that researchers have begun to explore more comprehensive models and XAI techniques to enhance performance insights [26], which analyzed XAI applications for diabetes prediction, identifying limitations such as poor generalizability from small datasets, and emphasizing the need for improved data quality, diverse real-world sources, and integration with IoT sensors for personalized diabetes man-agement. Vivek et al. [27] compared multiple XAI techniques, including SHAP, LIME, QLattice [2], ELI5 [9], and anchor [22], within a clinical decision support system.\nThe literature analysis revealed that related work generally treats XAI methods as isolated black-box tools, limiting their application to evaluating ML model outputs alone. Most com-prehensive studies either focus on a single XAI technique [6], [7], [16] or apply different techniques without providing insights into their performance [17], [24], [27], highlighting the gap of a systematic evaluation of XAI algorithms for explaining diabetes-related predictions."}, {"title": "A Identified Limitation", "content": "Most of the literature focuses on individual techniques or models as black boxes without considering interpretabil-ity challenges. This leaves questions unanswered, such as identifying effective methods to explain machine learning model outputs, understanding trade-offs between global and local XAI techniques, and evaluating their practical relevance in clinical contexts.\nThis study is motivated by significant limitations in prior studies on machine learning (ML) and explainable AI (XAI) for predicting diabetes. While fundamental models such as Logistic Regression [6] and SVM [16] have been frequently employed due to their interpretability, they demonstrate in-adequate capacity to capture the complex, nonlinear patterns present in structured healthcare information. Ensemble tech-niques, as investigated by Kibria et al. [17], enhanced accuracy but failed to resolve the interpretability issues crucial for clinical implementation. Additionally, research such as Tasin et al. [24], which combined various XAI approaches, lacked a comprehensive framework to assess the balance between global and local explainability or to incorporate these insights into practical medical workflows.\nThis investigation aims to fill existing gaps by building upon the state-of-the-art XAI tools, including SHAP, LIME, Explainable Boosting Machines (EBM), and counterfactual explanations, and anchor an ensemble framework. In contrast to prior studies that have targeted these techniques, this study rigorously evaluated these techniques in a healthcare context, highlighting that they can create actionable insights and facilitate improved clinical decision-making. This study seeks to achieve a balance between predictive accuracy and interpretability by avoiding the pitfalls of previous research that have either exclusively applied XAI methods [24], [27] or is not applicable in practical clinical settings. The integration of ensemble techniques and a comprehensive array of XAI tools ensures a dual focus on accuracy and transparency, establishing a new standard for applying AI in healthcare. This approach combines robust predictive ML models with explain-ability frameworks and allows clinicians to make decisions based on the data from patients with increased confidence."}, {"title": "III. MATERIALS AND METHODS", "content": "To address the limitations identified in the existing research and guide our investigation and methodology, we formulated a set of research questions. These questions were designed to tackle key areas of uncertainty regarding explainability techniques and their practical implementation in predicting diabetes mellitus."}, {"title": "? RQ1", "content": "Which ML models guarantee the highest predictive accuracy and simultaneously offer comprehensible results that are useful in diagnosing diabetes mellitus?"}, {"title": "? RQ2", "content": "Which types of explanation methods are the most useful in explaining the result of the chosen ML models in diabetes prediction?"}, {"title": "? RQ3", "content": "What are the advantages and drawbacks of combining two or more explainability approaches to improve both global and local interpretability of model results?"}, {"title": "? RQ4", "content": "How to evaluate the quality and reliability of explainability method in ML models and how those metrics affect the interpretability and trustworthiness of predictions?"}, {"title": "A. Data Source and Data Preprocessing", "content": "The Diabetes Binary Health Indicators dataset, obtained from the BRFSS, included 253,680 patients with 22 numeric attributes of clinical risk factors, including BMI, cholesterol, blood pressure, activity level, and smoking status. The se-lection variable Diabetes_binary categorizes the population into Diabetic and Non-diabetic, making binary classification possible. However, the database skewed heavily towards class 0, which was non-diabetic, with only 13.07% of the cases being diabetic class one. This imbalance is a challenge in the process of training the model, as it would allegedly be able to predict non-diabetic cases. To tackle this, SMOTE was used on the minority class to synthesize new samples and to train the model independently, i.e., decisional bias towards the majority class might otherwise affect the overall performance.\nData preprocessing makes it possible to feed the dataset into the model through activities such as handling missing values, scaling features, and handling class imbalance. Median imputation handles missing values because this method appropriately assesses healthcare datasets when outliers are frequently noted. When using the median, it retains this central tendency without skewing by large data points or what is referred to as 'outliers'.\nSubsequently, feature scaling was done using Standard-Scaler, which scales each feature to have zero mean and unit variance. This step was needed because health indicators in-clude BMI, blood pressure, and cholesterol, wherein the units will not be the same and there will be problems with scaling. Standardization also enabled every feature to be independent when contributing to the model convergence, reducing the time to convergence and enabling comparability of feature weights."}, {"title": "B. Data Splitting for Model Evaluation", "content": "For model training, tuning, and assessment, the entire dataset was split into training (70%), validation (15%), and test (15%). Such a split enabled the training set for a pattern, a cross-validation set to tune hyperparameters, and a test set to measure the model's performance, providing a real-world indication of overfitting without compromising generality.\nBy ensuring that the class distribution was maintained across all subsets of stratified sampling (86.07% non-diabetic and 13.93% diabetic), the real-life split was maintained to support metric evaluations."}, {"title": "C. Model Selection, Training and Hyperparameter Tuning", "content": "Several ML models were built using Random Forest (RF), XGBoost, LightGBM, SVM, Decision Tree (DT), Naive Bayes, and Logistic Regression and tested for the diagnosis of diabetes. These techniques were chosen because they can operate with binary data and yield good results when working with structured health data. All the built models were first trained on the balanced training set created from the original training set, and then the performance was assessed based on the validation set.\nFurthermore, we used 3-fold cross-validation and Randomized SearchCV to tune the hyperparameters necessary for each model. This approach allowed a stable tuning technique by avoiding overfitting of the models and guaranteeing that the frequency within all data folds was well-tuned."}, {"title": "D. Ensemble Model Creation", "content": "Ensemble models were built using Random Forest, XGBoost, and LightGBM to ensure robustness in structured health data analysis. The former provides stability and avoids overfitting through bagging [12], [13], whereas XGBoost han-dles nonlinear relationships efficiently with gradient boosting. LightGBM processes large datasets with a high accuracy in less time.\nSoft voting was performed to take advantage of the strengths of these models by averaging their prediction probabilities to allow balanced and robust outputs. Unlike hard voting, which works on the principle of majority decisions, soft voting con-siders model confidence; hence, it is better for handling class imbalances and weak patterns in diabetes datasets. Optimized precision, stability, and speed of handling such complexities in diabetes prediction."}, {"title": "E. Model Evaluation", "content": "The performance of both individual ML models and the ensemble model was rigorously evaluated using a comprehensive set of metrics: accuracy, ROC-AUC, precision, recall, and F1-score. These metrics offer a thorough evaluation and comparison of ML models.\nTo avoid overfitting, an evaluation was performed on both the validation and test sets. In addition, the process highlights the need for an ensemble model and for evaluating the performance of ensemble models versus individual models in terms of predictive accuracy and confidence."}, {"title": "F. Explainability of Models", "content": "Numerous explainability approaches have been employed as part of model processing for transparency and to inter-pret the findings. SHAP offers both global and local feature importance, and thus, it helps us understand which health indicators carry the most significant weight for prediction as well as individual interpretability. Furthermore, a model-specific approach based on feature importance was used to obtain a complete understanding of the impact of the variables on diabetes risk, and critical predictors were identified.\nPartial Dependence Plots (PDPs) were used to describe the interaction between features that affected the prediction, i.e., how adding a feature increased the prediction of the outcomes once specific values were chosen for the feature under consideration. For example, PDPs associated with BMI or hypertension have been shown to influence the likelihood of diabetes risk.\nFor instance-level interpretations, LIME was used in the study because it provides clear heatmaps of the features' contributions to the achieved prediction. The heatmap was a noteworthy feature because it allowed for comparison and explanation of predictions irrespective of the models used in the application. We also considered EBM, which attains high accuracy alongside interpretability by design, and Global Surrogate Models, which replace difficult-to-explicate models with easier-to-explicate ones."}, {"title": "G. Explainability Metrics", "content": "Several explanatory metrics were applied to assess the quality and reliability of the model explanations.\n\u2022 Fidelity is the degree to which an explanation aligns with the model's original predictions, accurately representing the model's behaviour.\n\u2022 Faithfulness evaluates if explanations accurately reflect the model's underlying mechanisms, providing real in-sights into the model's decision-making process.\n\u2022 Sparsity stresses minimizing features with retained rele-vance, exposing the most important variables in deciding on predictions.\n\u2022 Stability promotes similar inputs to result in similar explanations, generating more reliable model outputs.\n\u2022 Consistency assesses the degree of agreement in feature importance for different models or conditions as its main value for validating results in varied situations.\nSuch measures offer a strong foundation for assessing the quality of model explanations and aligning their interpretabil-ity and accuracy in the context of healthcare.\nIn our study, we developed an accurate diabetes prediction model using integral data preprocessing, multiple modelling and ensembles, and explainability techniques. The model is interpretable and can generate informative predictions for clinicians. The proposed ensemble scheme demonstrates the potential of solutions for healthcare decision-making."}, {"title": "IV. ANALYSIS OF THE RESULTS", "content": "In this section, we present and analyze the results of our study according to the four research questions related to model performance, explainability methods, and the advantages and disadvantages of combining explainability methods and their assessment of explainability metrics."}, {"title": "A. RQ1 - ML Models for Diabetes Prediction", "content": "The highest predictive accuracies were achieved by indi-vidual models, such as XGBoost, LightGBM, and RF, be-cause these can model nonlinear relationships and complex interactions in healthcare datasets. These models are robust to variations in data structures and are, therefore, suitable for predicting diabetes mellitus. Surprisingly, the best result among the three models was the ensemble model using soft voting, which achieved a test accuracy of 92.5% and a ROC-AUC of 0.975 (see Table II). Using the ensemble strategy, we used XGBoost for its precision, LightGBM for its speed with large datasets, and Random Forest to reduce overfitting, thus retaining interpretability simultaneously as a generalization. This balance between accuracy and practical utility makes such a combination highly relevant for clinical decision-making."}, {"title": "\u2713 Answer to RQ1", "content": "Ensemble model outperformed other ML models in terms of accuracy and ROC-AUC. This combination success-fully retains the non-linearity while still being inter-pretable and thus proved to be the most suitable for diagnosing diabetes mellitus."}, {"title": "B. RQ2 - Explanation Methods for ML Predictions", "content": "We used various explainability techniques to reveal global patterns and explain feature interactions, generate personalized insights to help healthcare professionals understand and trust machine learning predictions, and build a robust framework.\n1) Global Interpretability - Key Predictors Driving Dia-betes Risk: We used global interpretability tools to identify the most important predictors and their overall contribution to diabetes risk. SHAP was used to explain the importance of global and local features. Important drivers contributing to diabetes risk included BMI, GenHlth, Age, and income (see Fig. 2). The results also indicated that BMI level has a positive correlation, and general health and physical activity have a moderating impact, as they act as buffer variables. SHAP enhances the depiction of such associations to facilitate the identification of clinical intervention priorities."}, {"title": "\u2713 Answer to RQ2", "content": "SHAP and EBMs fared well in assigning high importance to predictors such as BMI and general health and LIME gave individual focus. Feature interactions were explained between PDPs and Anchors, while Counterfactual Explanations provided potential interventions to build a strong internal structure of diabetes prediction."}, {"title": "5) RQ3 - Combining Explainability Approaches", "content": "The achieved results prove the value of integrating explainability approaches in interpreting ML models for diabetes prediction. The study strikes a balance of understanding feature impor-tance while still offering personalized insights by combining global methods, such as SHAP and EBM, together with local methods, such as LIME. By highlighting the role of BMI and general health, SHAP elucidates how these factors affect indi-vidual predictions. In contrast, LIME helps them understand the specific drivers within each person's predictions to lead them to the right treatment path. Cross-validation and consis-tent SHAP and EBMs identified BMI, physical activity, and general health as critical predictors. By anchoring, we simplify decision-making while providing intuitive thresholds based on clinical practice. Integration adds complexity; different outputs from methods can overwhelm users or contain contradictory insights. Despite these drawbacks, the combination has the potential to offer a robust trade-off between transparency and actionability for diabetes prediction and management."}, {"title": "\u2713 Answer to RQ3", "content": "SHAP, LIME, PDPs, and Anchors enhance global and local interpretability of models by defining important predictors, explaining the prediction of an individual, and describing interactions and decision rules, this comes at the price of complexity and computation, and careful handling is needed for that."}, {"title": "6) RQ4 - Quality, and Reliability of Explainability Methods", "content": "The evaluation metrics used to assess the quality of explain-ability methods in diabetes prediction models are provided in Table IV. The fidelity value ensures that the explanations accurately represent the model predictions, and faithfulness of 0.709 confirms that they also depict the model's decision-making. Trust is built by uniformly explaining the model's behavior together. Sparsity makes interpretations easier to understand by following only the most important features, enabling more straightforward application results, such as diabetes prediction. Stability produces consistent explanations for similar inputs, a cornerstone characteristic for clinical applications that is critical for consistency. This validation indicates that key predictors go away inconsistently across models or scenarios, meaning making choices based only on consistency is more generalizable. Together, these metrics pro-"}, {"title": "\u2713 Answer to RQ4", "content": "Explanation accuracy, transparency, and reliability are guaranteed by explanation metrics so that predictions are more interpretable and trustworthy for diabetes diagnosis."}, {"title": "V. DISCUSSION", "content": "The results of this study demonstrate the accuracy, inter-pretability, importance of explainability metrics, and clinical usability of ML models for diabetes prediction. This section describes more of these insights and provides actionable outcomes for both researchers and practitioners.\nComparative Analysis with Existing Studies: In studies using logistic regression, decision trees, and random forests, accu-racies were in the range of 70% to 91% and were not robust across a variety of datasets [4], [6], [17]. Leveraging ensemble models and dealing with the imbalances in the data, this study was able to achieve an accuracy of 92.50% and ROC-AUC of 0.975, outperforming previous studies. Results show that ensemble techniques resulted in a dramatic improvement in precision and recall over previous work. Previous studies predominantly used isolated explainability methods, which restrain them from aggregated insights [6], [23]. They lack concrete recommendations or patient-specific insights, such as BMI, age, and glucose levels [10], [24] without considering more general lifestyle factors. This combination of SHAP and LIME revealed key predictors, such as Age, BMI, General Health, and physical activity, leading to actionable recommen-dations for increasing activity or reducing BMI.\nThese findings impact both research and practice. This study provides a framework for combining ensemble learning with explainability tools and establishes a new standard for Al healthcare research. It allows actionable insights, such as counterfactual explanations, to help clinicians make evidence-based decisions and propose lifestyle changes. Anchors set thresholds for high-risk classification [23], [27]. This study has added advantages in terms of accuracy, insights, and generalizability. It addresses the gaps in past research and offers a practical framework for enhancing diabetes prediction and care across various healthcare settings.\nPotency of ML Models for Diabetes Prediction: The en-semble model outperforms previous research due to strategic methodological choices like XGBoost, LightGBM, Random Forest, SMOTE for class balancing, and soft voting for ef-fective aggregation, enabling the model to identify complex feature interactions.\nThe framework effectively addresses false negatives in dia-betes diagnosis by incorporating clinically relevant predictors like BMI, age, and physical activity. This aligns with clinical priorities, enabling actionable recommendations and targeted interventions, bridging the gap between abstract predictions and practical applications. This methodology could be ex-tended to include multimodal data or to validate across other demographics as future research. Model performance is similar to that of human practitioners and can serve as a clinical decision-support tool. It provides transparent insights and greater diagnostic precision, helps reduce clinician burden and improves patient outcomes. Al integration through clinical workflows represents a defining moment for the use of AI in personalized care and equitable medical decision-making.\nEffectiveness of Explainability Methods: According to the study, integrating several explainability methods (SHAP and EBMs) also increases model transparency and clinical usage. It strengthens the trust between healthcare professionals by"}, {"title": "Evaluation of Explainability Metrics", "content": "The explainability met-rics showed that machine learning systems can be success-fully used in clinical environments. Fidelity and faithfulness measures of outputs from the model are consistent with the internal mechanisms of the model, providing assurance and aiding in making informed medical decisions. Due to the sparsity metric, the interpretation of critical predictors, includ-ing BMI and age, becomes more achievable for healthcare professionals. Stability means that the same inputs predict the same explanations, and consistency means that we can build reliable insights from different patient cohorts. Thus, these evaluation criteria indicate that the system has the potential as a reliable clinical decision-support tool. Thanks to the predictions linked to reasonable insights, the model suggests both high-risk patients early on and actionable interventions such as lifestyle changes. By emphasizing explainability and robustness, we set a standard for machine learning in health-care integration to improve diagnostic precision and patient outcomes while directing further research to develop XAI as a clinical application."}, {"title": "Advantages and Drawbacks of Combining Explainability Approaches", "content": "Global and local explainability techniques used in conjunction with machine learning enhance predictive capa-bilities by supplying comprehensive system insights and per-sonalized recommendations. Global methods, such as SHAP, identify critical predictors, such as BMI and exercise habits, while local methods, such as LIME and Anchors, provide patient-specific interpretations. This dual-level strategy en-hances clarity, credibility, and clinical relevance but may increase computational power. This study presents a repro-ducible methodology for incorporating explainability tools into medical settings, offering guidance on lifestyle modifications to reduce the risk of diabetes. It aligns with clinical objectives, enhancing diagnostic precision, health outcomes, and opera-tional efficiency. The model enables healthcare professionals to make informed decisions with confidence and enhances overall medical outcomes.\nClinical and Broader Implications: This study assists health-care providers in understanding predictions through the uti-lization of SHAP, EBM, and LIME, translating these into personalized recommendations such as lifestyle modifications or monitoring protocols. This methodology aligns with clinical reasoning, enhances confidence, and integrates with elec-tronic health records (EHRs) or real-time monitoring sys-tems, thereby optimizing clinicians' workflows. Our study demonstrates to researchers how the combination of ensemble models with explainability techniques can balance predictive accuracy and transparency, providing a replicable methodology for various medical fields. For practitioners, this framework reduces false negatives, minimizes delayed treatments, and offers accurate, interpretable results tailored to individual patient needs. It supports collaborative decision-making by providing patients with clear risk explanations and actionable measures, thereby fostering trust to medical advice.\nOn a broader scale, the system has significant public health implications, identifying high-risk populations and inform-ing preventive strategies. Economics, in this case, is about minimizing resource allocation and the financial burden of late-stage disease. The study solves the 'black box' issue for accountability and transparency in building trust between healthcare professionals and patients. It lays down the ground-work for how we can bring ethical AI implementation to light, enhancing diagnostic accuracy and improving patient outcomes while making life easier for healthcare providers. The system takes AI-driven personalized medicine to a new level, incentivizing further work and its immediate adaption into healthcare to give the best outcomes with transparency and assurance."}, {"title": "Limitations and Future Directions", "content": "Further research and practical application would require a few limitations of this study to be addressed. In reality, artificial data for balancing classes might not reveal real-world challenges, and other studies should widen its scope of data augmentation meth-ods. However, the ligand model cannot be applied techni-cally in the clinical setting due to its large computational requirements. Reducing computational costs and expanding the model to include multiple data types (e.g. imaging or continuous monitoring) will be needed to enhance its clinical value further. Moreover, we will include social factors such as income, education, and the availability of healthcare services that increase model relevance in other demographic groups. Other diabetes research should focus on adapting the model for various patient populations and healthcare settings alike, making it useful in managing diabetes in different settings more broadly."}, {"title": "VI. CONCLUSION", "content": "This study demonstrates how various machine learning algo-rithms and explainable methods can lead to diabetes prediction based on BMI, general health conditions, and physical activity level. These models exhibit high accuracy and reproducibil-ity using both global and local explainability methods, thus providing clear guidance for clinical decisions. The practical insights generated by these models may enhance targeted treatments and optimize clinical practices, rendering these models valuable for clinicians. From a scientific perspective, this investigation necessitates further validation using clinical information from real cases, as well as advancements in im-proving the interpretability of the models. Interoperability with electronic health records and continuous monitoring systems is essential. Consequently, it will facilitate the development of more accurate, sensitive, and individualized methods and approaches to care and expand the application of artificial intelligence in the field of medicine. In practical terms, the results emphasized how the establishment of trust through explainable artificial intelligence is critical when implementing models in clinical practice, as other healthcare professionals can comprehend the outputs that the model provides, as well as gain confidence in applying them for patient care. The results establish a foundation for the advancement of a preventive healthcare system, appropriate diagnostic tools, and improved patient care. In the future, there is a need to enhance the flexibility and generalizability of these models to play key roles in redesigning telemedicine and the future of efficient health services provision and treatment."}]}