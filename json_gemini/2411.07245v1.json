{"title": "Navigating Al in Social Work and Beyond\nA Multidisciplinary Review", "authors": ["Dalziel, Matt Victor", "Schaffer, Krystal", "Martin, Neil"], "abstract": "This review began with the modest goal of drafting a brief commentary on how the social\nwork profession engages with and is impacted by artificial intelligence (AI). However, it\nquickly became apparent that a deeper exploration was required to adequately capture the\nprofound influence of AI, one of the most transformative and debated innovations in modern\nhistory. As a result, this review evolved into an interdisciplinary endeavour, gathering\nseminal texts, critical articles, and influential voices from across industries and academia.\nThis review aims to provide a comprehensive yet accessible overview, situating AI within\nbroader societal and academic conversations as 2025 dawns. We explore perspectives from\nleading tech entrepreneurs, cultural icons, CEOs, and politicians alongside the pioneering\ncontributions of Al engineers, innovators, and academics from fields as diverse as\nmathematics, sociology, philosophy, economics, and more. This review also briefly analyses\nAl's real-world impacts, ethical challenges, and implications for social work. It presents a\nvision for Al-facilitated simulations that could transform social work education through\nAdvanced Personalised Simulation Training (APST). This tool uses AI to tailor high-fidelity\nsimulations to individual student needs, providing real-time feedback and preparing them for\nthe complexities of their future practice environments. We maintain a critical tone\nthroughout, balancing our awe of AI's remarkable advancements with necessary caution. As\nAl continues to permeate every professional realm, understanding its subtleties, challenges,\nand opportunities becomes essential. Those who fully grasp the intricacies of this technology\nwill be best positioned to navigate the impending AI Era.", "sections": [{"title": null, "content": "Since its emergence in 1955, artificial intelligence (AI) has been one of human history's most\ndebated and contentious technological breakthroughs. AI differs from previous technological\ninnovations and disruptions by its potential to replicate, augment, and exponentially improve\ncapacities once exclusive to humans, such as perception, cognition and language.\nPerspectives on AI fluctuate wildly from viewing it as an existential threat to considering it\nhumanity's saviour. These extreme positions are not limited to sensationalist media, political\nfigures, science fiction, or celebrity academics; AI's leading pioneers also express them. For\nexample, Sam Altman, co-founder and CEO of OpenAI, famously remarked in 2015 that \u201cAI\nwill probably...lead to the end of the world, but in the meantime, there'll be great companies\u201d\n(Galef, 2015, para. 2). From a more philosophical perspective, Yuval Noah Harari cautions,\n\u201cIf we are not careful, we might be trapped behind a curtain of illusions, which we could not\ntear away or even realise is there\u201d (Harari, 2023, para. 16). Sam and Yuval's quotes\nbeautifully encapsulate the tension between society's fear of AI and the eagerness of tech\ngiants and governments to harness its commercial and military potential. This commentary\nwill explore these opposing views, highlight professional opportunities within the terrain\nbetween them, examine social work's role in navigating this unprecedented space, and note\npotential impacts on the profession.\nConcerns over Al's rapid development have gained significant traction in tech and public\ndomains. In March 2023, high-profile figures, including Elon Musk and Apple Co-Founder\nSteve Wozniak, co-authored the widely circulated 'Pause Giant AI Experiments: An Open\nLetter' (Future of Life Institute, 2023). The letter advocating for a six-month moratorium in\nAl research gathered over 33,700 signatures, many from within the tech industry. It\nhighlighted mounting fears about the safety of unchecked Al development and its potential\nsocietal impact. These anxieties were further substantiated by the 'The Potentially Large"}, {"title": null, "content": "Effects of Artificial Intelligence on Economic Growth' report, published by Goldman Sachs,\nwhich projects that up to 300 million jobs globally could be lost due to AI-driven automation\n(Briggs & Kodnani, 2023). Despite this looming disruption, many within social work have\nyet to engage in an evidence-based review and discussion on how AI might reshape the\nprofession or how social work can address the fallout of an unprecedented labour market shift\n(Hodgson et al., 2022; Magruder, 2023). Rather than adopting a 'wait and see' approach, it is\nessential for social work to actively shape Al's influence in this evolving context.\nJob losses due to AI are already impacting various industries. In February 2024, Hollywood\nactor and producer Tyler Perry cancelled an $800 million expansion of his Atlanta movie\nstudio after viewing OpenAI's text-to-video software, Sora, in action (Kilkenny, 2024).\nStunned by Sora's production of cinematic-quality video, Perry voiced concern over its\npotential consequences on the entertainment industry, stating:\nI think it's going to be a major game-changer... I am very, very concerned that in the\nnear future, a lot of jobs are going to be lost...There's got to be some sort of\nregulations in order to protect us. If not, I just don't see how we survive. (Kilkenny,\n2024, para. 3-12).\nWhile Perry's concerns about AI's long-term effects on film production are well-founded, his\ndecision to cancel the studio expansion immediately led to the loss of thousands of jobs in\nconstruction and related sectors. This exemplifies how AI's impact is not just theoretical but\nalready has tangible repercussions across all sectors of the economy."}, {"title": null, "content": "Although some may dismiss the actions of Perry and Musk as mere jostling for clicks,\nsubscribers, investor funds, or market dominance, they underscore the disruptive societal\nimpacts of AI. Perry's remarks draw attention to a unique concern: unlike prior technological\nrevolutions that cyclically devastated the working class, a phenomenon Schumpeter (2013)\nreferred to as Kondratiev Waves, AI now threatens to disrupt middle-class professions. This\nshift aligns with Brynjolfsson & McAfee's (2014) concept of the 'Second Machine Age,'\ndriven by AI and robotics, where the physical, digital, and biological realms converge. What\ndistinguishes this \u2018age' from previous ones is the unprecedented speed at which it is\nunfolding and the likelihood that AI will make vast segments of human labour obsolete\n(Brekelmans & Petropoulos, 2020; Frey & Osbourne, 2017). Combining AI with robotics,\nparticularly AI's ability to build, repair and even program itself autonomously, significantly\nreduces the potential for new jobs and industry creation. This profound shift in human-labour\nrelations has serious ethical, practical, and epistemological implications, particularly for\nprofessions such as social work.\nAlthough the picture for many professions may seem bleak, there is broad agreement that\nroles requiring high emotional intelligence and direct human interaction, such as social work,\nwill survive this redundancy, at least in the short term (Huang & Rust, 2018). Social work has\nlong recognised that human lives are inherently messy, relationships are complex, and life is\nunpredictable (Healy, 2022). Despite advancements in AI, it remains incapable of replicating\nmany essential aspects of the human condition, including emotional intelligence, empathy,\nspirituality, kinship bonds, and discrimination and power dynamics in human affairs.\n(Brynjolfsson & McAfee, 2014; Fernando & Ranasinghe, 2023; Goleman, 1996; Lewis et al.,\n2018). However, AI's automation of administrative tasks, such as scheduling appointments,\ntracking case progress reports, and updating case notes and treatment plans (Dey, 2023),"}, {"title": null, "content": "could significantly reduce the burden of routine duties, freeing up more time for client-\ncentred, human-focused work (Werder, 2023). Social work's emphasis on human connection\nensures that it will remain indispensable as society confronts the challenges posed by AI.\nAI is poised to compel humbleness within humanity (Bostrom, 2014). For the first time,\nhumans may no longer be the most intelligent entity in the room, forcing society to confront\ndeep existential and philosophical questions, such as \"What defines life?\". Historically,\nreligious doctrines have shaped societal norms and beliefs; today, this role is increasingly\nassumed by global technology corporations, whose influence continues to expand (Zuboff,\n2019). The potential development of Artificial General Intelligence (AGI) that includes well\nbeyond human levels of cognition in most domains of life represents one of the most\nsignificant technological disruptors in history (Mokyr, 1992). Whether this disruption will\nlead to overwhelmingly positive or negative outcomes, or something in between remains\nhighly contested. As OpenAI (2015, para. 6) aptly stated in its launch document: \"It's hard to\nfathom how much human-level AI could benefit society, and it's equally hard to imagine how\nmuch it could damage society if built or used incorrectly\". Like electricity, AI has the\npotential to infiltrate nearly every aspect of human life, transforming societies in ways that\nare currently unimaginable (Gruetzemacher & Whittlestone, 2019; \u0160ucha & Gammel, 2021).\nDespite its growing influence, AI lacks a universally accepted definition. Broadly, it is an\nadvanced technology designed to simulate human intelligence, solve problems, and augment\nhuman activities (Stryker & Kavlakoglu, 2024). Like foundational innovations such as the\nInternet or the combustion engine, AI is often regarded as a general-purpose technology with\napplications across various domains (Suleyman & Bhaskar, 2023). An unprecedented aspect\nof AI is its capacity for learning, adaptation, and content creation, with the eventual goal of"}, {"title": null, "content": "achieving consciousness. At its core, AI systems consist of four main components: (a)\narchitecture, (b) inputs, (c) algorithms for training on both existing and new data, and (d)\noutputs (Feuerriegel et al., 2024). Recent advancements, particularly the development of\nlarge language models (LLMs) like ChatGPT, have significantly expanded AI's capabilities,\nenabling it to process and respond to natural language with remarkable sophistication. LLMs\nare the foundation for next-generation AI systems, including tools like Sora (Naveed et al.,\n2023).\nA long history of gradual and incremental changes has driven Al's evolution. However, a\nseries of recent breakthroughs propelled AI into the advanced deep-learning systems we\nrecognise today. The advent of convolutional neural networks (CNNs) (Krizhevsky et al.,\n2012) is credited with kickstarting modern deep learning, a foundation that Goodfellow, et al.\n(2016) further built upon. The introduction of the Transformer architecture (Vaswani et al.,\n2017), along with BERT for pre-training deep bidirectional transformers in natural language\nunderstanding (Devlin et al., 2018), and DeepMind's use of deep reinforcement learning to\nachieve human-level performance in Atari games (Mnih et al., 2013), represent pivotal\nmoments. These five seminal developments mark critical advances in neural networks, deep\nlearning, and language models, widely recognised as a cornerstone in shaping contemporary\nAl systems.\nLike most technologies, AI is neither inherently good nor evil; instead, it reflects the ideas,\nideologies, and biases of those who develop and control it. Consequently, AI has faced\nnumerous legitimate concerns, including issues related to privacy (Reamer, 2013; Roche et\nal., 2023), bias (Bussey, 2022; Sutaria, 2022), elitism (McQuillan, 2022; Shah, 2024), sexism\n(Hong et al., 2020; Manasi et al., 2022) and racism (Hong & Williams, 2019; Mathiyazhagan"}, {"title": null, "content": "et al., 2022). Additionally, the lack of diversity (Coyle, 2019; Fosch-Villaronga & Poulsen,\n2022) and cultural awareness (Lewis, 2023; Munn, 2024) in AI systems raises significant\nconcerns. The environmental impact of AI (Coleman, 2023; Van Wynsberghe, 2021; Wu et\nal., 2022), implications for human rights (Gaumond & R\u00e9gis, 2023; Reamer, 2013), and\npotential threats to democracy (Barnhizer & Barnhizer, 2019) have also come under scrutiny.\nFurthermore, Al has also been characterised as authoritarian (McQuillan, 2022), colonial\n(Adams, 2021; Lewis, 2023; Mohamed et al., 2020), and extractivist (Ricaurte, 2019; 2022),\nwhile also facilitating digital pathways to incarceration (Patton et al., 2021). These concerns\nare exacerbated by the global absence of robust regulatory and ethical frameworks governing\nAI (Roche et al., 2023; Smuha, 2021). The lax, 'let it rip' approach poses significant risks,\nparticularly for society's most vulnerable populations. These oppressive outcomes of AI\nconflict with the human rights that social work aims to advocate and protect (Australian\nAssociation of Social Workers, 2023), highlighting the profession's critical role in shaping\nAl's future developments. Without regulatory oversight, AI companies, many of them large\nmultinational corporations, are left to 'self-monitor' their practices regarding equality,\ntransparency, accountability, and data management, which is far from ideal. These\ncorporations often prioritise innovation and profit over the ethical implications of their\ntechnologies. Lacking external oversight, legitimate questions arise regarding their\naccountability, particularly as AI systems become more autonomous.\nThe limitations of AI discussed thus far assume that humans remain in control, which may\nonly be the case for a short time. J\u00fcrgen Schmidhuber, a pioneer in AI and the Scientific\nDirector of the Dalle Molle Institute for Artificial Intelligence Research, expressed this\nconcern in Tonje Schei's (2019) documentary iHuman: AI and Humanity. Schmidhuber\nenvisions a future where AI surpasses human intelligence, openly admitting his long-standing"}, {"title": null, "content": "ambition: \"When I was a boy, I thought, how can I maximise my impact? I have to build\nsomething that learns to become smarter than myself\". This candid reflection encapsulates a\nbroader ambition within the AI industry: from its inception, AI was designed to evolve\nbeyond human comprehension, potentially escaping human control altogether.\nThis shift in control raises profound questions, most notably posed by AI researcher Roman\nYampolskiy (2024, p. 2), who asks: \"How can humanity remain safely in control while\nbenefitting from a superior form of intelligence?\". This question's ethical and existential risks\nremain at the forefront of Al discourse, particularly given the unprecedented pace of\ntechnological advances. Yampolskiy (2024) formulated three major obstacles to maintaining\ncontrol over such systems:\n1. Unexplainability: The inability to fully explain or understand all of AI's decisions\nwith 100% accuracy and certainty.\n2. Unpredictability: The inability to predict all the pathways and actions an\nintelligent AI system will take to achieve its goal, even when the goal is known to the\nobserver.\n3. Non-Verifiability: The inability to verify, with absolute confidence, the\ncorrectness of the AI system's actions, even with probabilistic assessments.\nYampolskiy's challenges imply that AI systems already behave in ways their creators do not\nentirely understand. It may surprise many that even AI developers and researchers do not\nfully grasp machine learning algorithms, with some deep learning algorithms remaining well\nbeyond comprehension (Dickson, 2023; Xiang, 2022). For the field of social work, this\nhighlights the need for vigilance, recognising AI as a tool designed by humans but\nincreasingly autonomous and unpredictable. Social workers must not hold AI in awe; instead,"}, {"title": null, "content": "they should lean into the profession's expertise in critically reflective practice, and examine\nits motives, biases, and reasoning. AI should be integrated thoughtfully into practice while\nremaining aware of potential unintended consequences for professionals and clients.\nAlthough AI may seem intangible, existing in a largely invisible virtual space often referred\nto as Black or Glass Boxes (Rai, 2020; Ribeiro et al., 2016), the physical devices required for\nits development, maintenance, and use are highly resource-intensive. For example, the\nreliance on lithium-ion batteries underscores Al's dependence on finite natural resources. The\nhuman toll in sourcing these finite resources, particularly cobalt, is a frequently overlooked\naspect of Al's rapid advancement. More than 2,000 'creuseurs,' or unregulated cobalt miners,\nperish annually in the Democratic Republic of Congo, with many victims being children\n(Amnesty International, 2016, Calv\u00e3o et al., 2021). Moreover, AI's thirst for electricity is\ninsatiable. Global electricity consumption has surged due to the rapid growth of AI-related\nindustries, which now account for approximately 2% of worldwide electricity use, a figure\nexpected to double within the next two years (Dyck et al., 2024).\nDeveloping advanced AI models and constructing hyper-scale data centres necessary to\nsupport these systems have pushed electricity consumption beyond typical industry\nprojections, straining power grids worldwide (Green et al., 2024). Over the past four to five\nyears, companies heavily invested in AI, like Microsoft and Google, have experienced sharp\nincreases in carbon emissions, surging by 29% and 48%, respectively (Kerr, 2024; Milmo,\n2024). This trend forced Google to concede that the future environmental impact of AI \"...is\ncomplex and difficult to predict\" (Braue, 2024, para. 5). Despite these concerns, pursuing\ndominance in the highly lucrative AI market continues to take precedence over addressing\nenvironmental issues and protecting human rights."}, {"title": null, "content": "This short-term drive for control is deeply rooted in anthropocentric philosophies, particularly\nthose emerging from Judeo-Christian thought and Enlightenment ideals (Mignolo, 2011;\nTuhiwai Smith, 1999). These traditions emphasise human dominance over nature and the\npursuit of objective knowledge by reducing complex systems into their constituent parts.\nSimilarly, AI developers often operate under the assumption that the world is 'knowable' and\ncan be computationally simulated (Griffin et al., 2024). Such perspectives help explain why\nAl developers and programmers believe they have a 'right' to harvest all publicly available\nonline content to train their models, often arguing that training AI is impossible without\nincorporating copyrighted content (Gray, 2024; Kazaz, 2024). This content, especially in\nexplainable Al systems (XAI), is also predominantly rooted in Western cultural frameworks,\nrendering the outputs, programs, and communications biased toward Western populations\n(Peters & Carman, 2024). Thus, AI continues to emulate and reproduce its developers'\nnarrow ideologies, practices, and concerns (Adams, 2021; Mohamed et al., 2020).\nIn practice, Al systems are primarily based on discrete event simulations and operate on\nprinciples like 'common things happen commonly.' They learn and adapt like Bayes' theorem,\nupdating predictions as new data or evidence becomes available (Bayes, 1763). However,\nAl's capacity to create order from chaos, much like the stabilising effects described in\nLyapunov's equations, remains limited (Lyapunov, 1892/1992). While AI can handle minor\ndisruptions and adapt within structured environments, it struggles to process the messiness of\nthe natural world (Rainie et al., 2021). Processes such as building trust, fostering imagination\nand curiosity, and experiencing transcendence remain uniquely human. As a profession\ncentred on the human condition in all its complexity, social work is well-positioned to thrive\nin an AI-driven future."}, {"title": null, "content": "Despite the profound concerns surrounding AI, it is crucial to acknowledge its promising\nopportunities, particularly in social work education, advocacy, and client outcomes. Al is\nalready reshaping the profession, offering tools that streamline communication and case\nmanagement processes (Kumar & Singh, 2023). For example, AI's ability to swiftly and\naccurately process risk assessments can be invaluable in assisting individuals during crises.\nMoreover, it holds the potential for identifying and addressing systemic biases within social\nservice delivery, which is a critical issue in creating an equitable practice paradigm (Reamer,\n2023). AI can improve communication and safety by identifying and predicting harmful\nonline content, as demonstrated by Patten et al. (2021), who highlight AI's capacity to process\nnuanced and complex language and cultural contexts\u2014a valuable tool in multicultural\nsocieties such as Australia. While AI's ability to analyse large qualitative and quantitative\ndatasets (Siiman et al., 2023), automate routine administrative tasks (Akram, 2024), and\npredict social worker burnout (Reamer, 2023) could significantly enhance the efficiency and\neffectiveness of social work practices. Although these technologies are still in their infancy,\ninitial indications suggest that AI could facilitate a more holistic and client-focused approach\nto social work by alleviating the administrative burden and enhancing practitioner learning.\nThe rapid pace of Al's evolution in education has profoundly impacted teaching institutions\nand their governing bodies worldwide. Al is contributing to the decline of traditional didactic\nteaching methods while simultaneously creating opportunities to enhance social work\neducation. Responses to these developments within academia and professional bodies remain\ndivided: some embrace new technologies and tools with an eager openness, while others\nremain defensive, perceiving such innovations as threats to established practices, particularly\nin traditional assessment methods. Exploring how these advancements can benefit educators"}, {"title": null, "content": "and students is crucial as the field progresses. One promising area is using immersive and\ninteractive simulations, where AI can enhance students' learning experiences through virtual\nreality (VR) simulations.\nSocial work environments are often complex and emotionally demanding, contributing to\nburnout, especially among newly qualified practitioners (Kinman & Grant, 2010; Rose &\nPalattiyil, 2020). This underscores the urgent need for training programs that introduce\nstudents to the realities of professional practice early in their degree programs. While social\nwork can be deeply fulfilling, it may not be the ideal career for everyone. Unfortunately,\nmany students do not realise this until their first field placements, typically during their third\nyear of study. Al simulations offer early opportunities for students to experience social work\npractice in a controlled environment, enabling them to make more informed decisions about\ntheir career paths earlier. From a social justice perspective, this early exposure can prevent\nstudents from facing emotional, physical, or psychological trauma, as well as the significant\nfinancial costs of pursuing a degree that may not align with their aspirations.\nIntegrating AI and simulations into social work curricula provides a unique opportunity to\naddress these challenges, particularly in teaching complex topics. These immersive tools\npromote self-awareness, empathy, reflection, and perspective-taking while enabling students\nto practice skills such as observation (Jiang & Ahmadpour, 2021). Research demonstrates\nthat simulation-based training can enhance empathy (Pecukonis et al., 2016), improve\ndiagnostic accuracy and clinical interviewing skills (Washburn et al., 2016), and cultivate\nmore positive attitudes toward clients (Lanzieri et al., 2023). Though still an emerging\ntechnology, Al-integrated simulations have proven valuable in preparing students for\nsensitive topics such as child protection and domestic and family violence (Agillias et al.,"}, {"title": null, "content": "2021; Jefferies et al., 2023; Schaffer et al., 2024). For example, Touro University in New\nYork has integrated AI-supported clinical training into its curriculum through decision-\nmaking trees and VR. This technology is used in simulations involving substance abuse and\nsuicide assessment, allowing students to navigate challenging conversations in a structured,\nsupportive environment (Lanzieri et al., 2021; Mowreader, 2024). AI programs embedded\nwithin coursework personalise scenarios, offer real-time feedback, and evaluate performance.\nStudent feedback has been positive, with many appreciating the immediacy of feedback and\nreported feeling increased confidence in their skills as they prepare to work with real clients.\nAI programs like Sora represent a significant milestone in developing simulation learning\ntools by generating comprehensive and realistic simulations without actors or physical sets.\nThese programs eliminate many logistical challenges and enable large-scale, personalised\nscenario adjustments to accommodate individual student needs. This advancement holds\ngreat potential for cost reduction and reshaping social work education by improving workload\nmanagement for facilitators, factors often cited as limitations in current simulation designs in\nhigher education (Kourgiantakis et al., 2020; Sewell et al., 2023). AI's capability to identify\nwhere a student is struggling and adapt high-fidelity simulation scenarios in real-time to\naddress specific learning needs is what we refer to as Advanced Personalised Simulation\nTraining (APST). This approach represents a transformative pedagogical advance.\nHowever, implementing APST raises ethical concerns regarding student privacy and\nalgorithmic surveillance. Current systems require data sharing with the AI platform and with\nthe companies that develop and own the software, raising important questions about data\nmanagement, storage, and confidentiality. Many AI companies are commercially driven and\noperate under self-regulation, intensifying concerns about handling confidential student"}, {"title": null, "content": "information. Therefore, it is crucial for social work educators and universities to co-design\nand develop these programs in-house, ensuring that sensitive data is securely stored on-site.\nAdditional challenges include technology acceptance among students and instructors and\ndisparities in access to these tools, especially for students from lower socioeconomic\nbackgrounds or those in regional, rural, or remote areas (Dalziel, 2019). While these\nchallenges are significant, they can be mitigated with targeted interventions.\nRecently, concerns have emerged around the overall ethical direction of AI development. The\nAsilomar AI Principles stress the responsibility of developers to pursue \"...beneficial\nintelligence\" rather than the risk of \"...undirected intelligence\" (Future of Life, 2017, para. 2).\nYet, this idealistic vision contrasts starkly with more recent realities. Dr Ben Goertzel, Chief\nComputer Scientist at Hanson Robotics, has observed that Al's developmental focus has\nincreasingly shifted toward militarisation, surveillance, and social manipulation, referring to\nits usage for \"spying, brainwashing, or killing\" (Goertzel, 2018, para. 8). This shift reflects\nhow AI, once open and accessible, has become tightly controlled by a small group of elite\ntech engineers, their billionaire financial backers, and governments (Widder et al., 2023). As\nFoucault (1980) reminds us, power is omnipresent, manifesting through discourses and the\ncontrol of knowledge. Today, this power struggle is unfolding in the digital sphere, where\nglobal elites seek to monopolise AI's potential for their own interests.\nSocial work professional organisations, NGOs, advocacy groups, independent media, and\nacademic institutions must critically assess Al's societal impact in a balanced manner,\navoiding sensationalism. Together, these groups can be crucial in advocating for guidelines\nand regulations that ensure ethical AI practices. The governance of AI must prioritise serving\nthe common good while preventing its misuse, including weaponisation. Effective oversight"}, {"title": null, "content": "requires transparency, accountability, fairness, and a concerted effort to mitigate bias and\navoid homogenising AI development and deployment. Achieving these goals will depend on\nholding companies and governments accountable through rigorous research, public awareness\ncampaigns, policy recommendations, and, most crucially, establishing internationally binding\nagreements.\nIt is chilling that no universally binding international agreement on ethical AI practices exists\nat this late stage in AI development. While frameworks such as UNESCO's Recommendation\non the Ethics of Artificial Intelligence (2022) and the G20's AI Principles (Organisation for\nEconomic Co-operation and Development, 2019) provide guidelines and proposed\nregulations, these remain voluntary. The European Union has made some progress towards\nregulation, beginning with the Coordinated Plan on AI (European Commission, 2021). This\nplan was the foundation for the EU's AI Act [Regulation (EU) 2024/1689] (EUR-Lex, 2024).\nThis regulation establishes a comprehensive legal framework for AI governance, though\nmany provisions will not be enforced until August 2026. The Act also includes significant\nexceptions for military applications, government bodies, public authorities, international\norganisations, non-EU countries, scientific research, and personal use (KPMG, 2024). These\nexceptions may limit the Act's effectiveness, rendering it more symbolic than impactful.\nFurthermore, excluding major AI research nations, such as China and Russia, exacerbates the\nchallenge of global AI governance. The need for enforceable international standards and\nregulations grows more pressing as Al evolves.\nThis is especially true as AI gains consciousness, self-awareness and emotional capacity.\nNeuromorphic computing, exemplified by projects like Neuralink's AI-human integrations,\ncreates life-dependent connections between technology and essential human functions\n(Chaudhary et al., 2017; Nicolelis, 2011). These developments necessitate a reassessment of"}, {"title": null, "content": "the implications of private and government ownership of such technologies (Eubanks, 2018;\nZuboff, 2019), including the risks of slavery, exploitation and life-sustaining technological\ndependence (Bostrom, 2014; Bryson, 2020). The potential for AI to exert control over human\nlives or to be used as a tool of coercion and manipulation should be considered (Cohen, 2019;\nO'Neil, 2016). Ethical questions regarding AI rights and personhood also need to be\naddressed, which further complicates the landscape (Bryson, 2018; Gunkel, 2024). Often\ndismissed as premature or exaggerated, some of these capabilities already exist or will by\n2030, making it crucial to begin crafting robust regulatory frameworks now (Baum, 2017;\nDubey, 2024; Naud\u00e9 & Dimitri, 2020; Ziegler, 2024). While confronting and potentially\noverwhelming, these developments also open new spaces for social work practice and study,\nparticularly in addressing AI's ethical, psychological, and societal impacts.\nThough tackling these challenges may seem intimidating, historical precedent exists for\nsuccessfully confronting global issues. The Treaty on the Non-Proliferation of Nuclear\nWeapons (NPT) (United Nations, 1968) and the Montreal Protocol on Substances that\nDeplete the Ozone Layer (United Nations, 1987) demonstrate that international agreements\non critical issues are achievable. However, such progress requires coordinated efforts\nbetween the public and private sectors and interdisciplinary collaboration. While individuals,\nsocial workers, and academics play essential roles in addressing these challenges, meaningful\nprogress on Al regulation will ultimately depend on leadership from national and\ninternational social work bodies. Social workers can engage with AI research fields, such as\n'AI Safety and Security' to ensure that ethical considerations are integrated into AI systems\nfrom their inception (Yampolskiy, 2018). By collaborating with specialists or developing\nexpertise in AI, social workers can embed social justice principles into the core of AI\ndevelopment through meaningful co-design processes. Beyond this, social workers can serve\nas ethicists, advocating for oversight, monitoring, and rigorous auditing of AI systems while"}, {"title": null, "content": "lobbying for compensation mechanisms to address the harm caused by AI. Crucially, social\nworkers can ensure the voices and contributions of marginalised populations, including First\nNations Peoples, are included in Al developments.\nBuilding on the need for greater inclusion in Al development, First Nations Peoples have, in\nmany respects, already taken steps that surpass the current efforts of the social work\nprofession. Recognising the risks of exclusion from the AI narrative, First Nations leaders\nconvened in Hawai'i in 2020 to draft an Indigenous framework for inclusive Al design. The\nresulting Guidelines for Indigenous-Centred AI Design, grounded in traditional knowledge,\nare a powerful example of AI advocacy (Lewis, 2020). These guidelines emphasise that\nhomogenisation leads to cultural loss (Lewis, 2020) and propose that humanity must learn to\nform \"kin with the machines\" (Lewis et al., 2018, p. 1). This perspective calls for a radical\ndeparture from dominant paradigms, recognising that humans are part of interconnected\nsystems that include animals, rivers, rocks, wind (Abdilla, 2019), and even machines. The\nguidelines draw upon the Lakota Seven Generations principle, urging us to consider how our\nactions today will shape AI for the next seven generations (Kite et al., 2020). Social work\norganisations can learn from these custodianship principles when developing their ethical\nframeworks for AI. Co-designing AI systems with marginalised communities and integrating\nIndigenous perspectives could foster a more inclusive and ethical approach to AI\ndevelopment. Social workers do not need to write code to influence AI; engaging in reflective\ndialogue and collaboration is sufficient to begin shaping a more just and inclusive future.\nUltimately, social workers can help shape this vision or risk having it shaped for them.\nThis commentary began by discussing Elon Musk and his associates' open letter, which\nhighlighted concerns shared by many within and outside the tech industry regarding Al's"}, {"title": null, "content": "rapid development. However, it's essential to understand this letter in context; it was not an\nargument against AI technology but a warning about the dangers of unregulated research and\ndevelopment. While sounding the alarm on AI's more unethical aspects, Musk, Perry, and\nWozniak have simultaneously been pioneering and leveraging the latest AI technologies.\nSince the letter's publication, Musk has launched Grok"}]}