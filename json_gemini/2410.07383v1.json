{"title": "SparseGrad: A Selective Method for Efficient Fine-tuning of MLP Layers", "authors": ["Viktoriia Chekalina", "Anna Rudenko", "Gleb Mezentsev", "Alexander Mikhalev", "Alexander Panchenko", "Ivan Oseledets"], "abstract": "The performance of Transformer models has been enhanced by increasing the number of parameters and the length of the processed text. Consequently, fine-tuning the entire model becomes a memory-intensive process. High-performance methods for parameter-efficient fine-tuning (PEFT) typically work with Attention blocks and often overlook MLP blocks, which contain about half of the model parameters. We propose a new selective PEFT method, namely SparseGrad, that performs well on MLP blocks. We transfer layer gradients to a space where only about 1% of the layer's elements remain significant. By converting gradients into a sparse structure, we reduce the number of updated parameters. We apply SparseGrad to fine-tune BERT and RoBERTa for the NLU task and LLaMa-2 for the Question-Answering task. In these experiments, with identical memory requirements, our method outperforms LoRA and MeProp, robust popular state-of-the-art PEFT approaches.", "sections": [{"title": "1 Introduction", "content": "Due to the tendency to increase the size of transformer models with each new generation, we need efficient ways to fine-tune such models on downstream task data. The usual practice is fine-tuning a large pre-trained foundational model on a downstream task. The major problem that prevents efficient fine-tuning is a steady increase in the memory footprint. One of the best strategies is high-performance methods for parameter-efficient fine-tuning (PEFT). Typically, such methods as LORA (Hu et al., 2021) focus on attention blocks and do not consider dense MLP blocks. Since MLP blocks can take a significant fraction of the model parameters (see Table 1), we propose to focus instead on MLP blocks. We introduce a novel selective PEFT approach called SparseGrad. Our method is based on finding a special sparsification transformation that allows us to fine-tune about 1% of the dense MLP layer parameters and still show good performance in downstream tasks."}, {"title": "2 Related Work", "content": "In the last few years, many approaches to PEFT have appeared. Lialin et al. (2023) distinguishes three types of methods: additive, reparametrization-based, and selective. In additive PEFT, small neural networks called adapters are added to the main model to steer the outputs of its modules (Pfeiffer et al., 2020). Adapters are trainable, therefore, the main model remains unchanged. Houlsby et al. (2019) adapt this approach to NLP. In reparametrization-based approaches low-rank representations of trainable parameters are used. For example, LoRA (Hu et al., 2021) parameterizes the weight update by a trainable low-rank matrix decomposition. In the original paper, LoRA is applied to self-attention modules, but not to MLP ones. In the selective methods, parts of the model or sets of the parameters are chosen for fine-tuning using some heuristics."}, {"title": "3 Method", "content": "Our aim is to reduce the amount of trainable parameters at the fine-tuning stage. Taking into account that fine-tuning data is restricted to a limited scope, we assume there is a basis where the weight gradient matrix is very close to being sparse. To identify this basis, we applied a decomposition technique to the stacked weight gradient matrices. As a result, we introduce a new PyTorch layer class, SparseGradLinear, which transitions weights to this sparse gradient space, accumulates gradients in sparse form, and enables the reverse transition back to the original space."}, {"title": "3.1 Preliminary Phase: Finding Transition Matrices", "content": "To obtain transition matrices, an initial procedure is necessary. During this, we perform n_steps"}, {"title": "3.2 Signal Propagation in SparseGradLinear Layer", "content": "Given a Transformer Linear layer with a weight matrix $W^T$, input activation $X$, and output $Y = XW^T$, we define the gradients of the output, input, and weights as $\\frac{\\partial L}{\\partial Y}$, $\\frac{\\partial L}{\\partial X}$, and $\\frac{\\partial L}{\\partial W^T}$, respectively. To create the corresponding SparseGradLinear layer, we represent the weights in the $U, V^T$ basis, such that the new weights are $\\tilde{W^T} = UW^TV^T$. Since the modules following SparseGradLinear remain unchanged in both forward and backward passes, it is crucial to maintain consistency between outputs of the Original Linear Layer $Y$ and the SparseGradLinear layer $\\tilde{Y}$, as well as their input gradients $\\frac{\\partial L}{\\partial X}$ and $\\frac{\\partial L}{\\partial Y}$."}, {"title": "3.3 Sparse-by-Dense Matrix Multiplication", "content": "We provide the SparseGradLinear class with updated Forward and Backward procedures. However, the addition of multiplications by U, V into them increased the execution time and affected peak memory in the training loop.\nThe sparsity of the gradient tensor $\\frac{\\partial L}{\\partial Y} = \\frac{\\partial L}{\\partial W^T} X$ results in some of the multiplicators being sparse. We explore the structure of each component in this formula and figure out that $\\frac{\\partial L}{\\partial W}$ has a sparsity approximately equal to $\\frac{\\partial L}{\\partial Y}$. Histograms\nMore precisely, to multiply the sparse matrix $A \\in R^{b \\times c}$ by a dense matrix $B \\in R^{c \\times d}$ we select $rows$ and $cols$ - indices of rows and columns of A which contain nonzero elements and multiply as follows:\n$C = A(rows, :)(:, cols)B(cols, :).$\nWe employ C either for further multiplications, or convert it into COO format and send it to SparseAdam optimizer. Indexes in COO format are defined by restoring indexes of A:\n$C_{coo}(rows(k), cols(l)) = C(k,l).$"}, {"title": "4 Time and Memory Consumption per Training Iteration", "content": "We measure the peak memory allocated during training using the CUDA memory allocator statistics. Table 3 demonstrates this statistic on average for all GLUE datasets for the ROBERTabase model.\nThe increase in peak memory with SparseGrad is attributed to the maintenance of matrices U and V and their multiplication by the dense objects, such as Input X."}, {"title": "5 Experiments", "content": "We conducted experiments on three transformer-based encoder models, BERT and ROBERTabase"}, {"title": "5.1 Natural Language Understanding with BERT and ROBERTa", "content": "We explore the acceptable sparsity level of the gradient matrices in the \u201csparse\u201d space, $\\frac{\\partial L}{\\partial W}, \\frac{\\partial L}{\\partial Y}$. By varying the number of remaining parameters in the Linear Layer from 100. 103 to 18. 103, we fine-tuned the model on the GLUE benchmark and identified the point at which performance begins to degrade. This occurs when the number of trainable parameters reaches 22 \\times 103, corresponding to 1% of the total weights.\nWe fine-tune BERT, ROBERTabase and ROBERTalarge (Zhuang et al., 2021) using Regular FT, LORA, MeProp and SparseGrad schemes for 20 epochs with early stopping for each task in the GLUE. We varied the batch size and learning rate using the Optuna framework (Akiba et al., 2019). The learning rate ranged from 1e-6 to 1e-1, and the batch size is selected from the set {8, 16, 32}."}, {"title": "5.2 Conversations with LLaMa-2", "content": "We apply the SparseGrad method to fine-tune LLaMa-2 7B (Touvron et al., 2023) model on the OpenAssistant conversational dataset (K\u00f6pf et al., 2023). Fine-tuning was performed on a single GPU NVIDIA A40 during 1 epoch with learning rate 9e-4. For Regular FT, we unfroze up_proj and down_proj layers in the MLP modules with a block index divisible by 3 (0, 3, 6, . . . ). We apply LORA with rank 32 to the selected blocks, leaving the rest of the model untrainable. In the SparseG-"}, {"title": "6 Conclusion", "content": "We propose a new selective PEFT method called SparseGrad, which identifies a space where the gradients exhibit a sparse structure and updates only its significant part. SparseGrad is validated through experiments conducted on the BERT, ROBERTa and LLaMa-2 model models, demonstrating its superiority over the additive LoRA and selective MeProp methods."}, {"title": "7 Limitations", "content": "The main limitation of our method is the additional memory requirements during the Preliminary Phase. The extra memory is assessed as follows: we need to unfreeze the MLP layers, which hold approximately half of the training parameters in Transformers (see Table 1), store and decompose a large tensor. For instance, 30 steps in the preliminary phase result in a tensor of approximately 276 MB for BERT and ROBERTA models, and 5.2 GB for LLaMa-2.7 B models. The decomposition part can be the most memory-consuming, as it involves reshaping a 3-dimensional tensor into a matrix with a dimension size equal to the product of two dimension sizes of the tensor (Cichocki et al., 2016).\nHowever, this part is executed only once during the entire fine-tuning process and can be computed on the CPU in a short time. The Higher Order SVD decomposition of such objects takes approximately 78 seconds for BERT and ROBERTabase layers and about 668 seconds for LLaMa on an Intel Xeon Gold 6342 CPU processor."}, {"title": "8 Ethics Statement", "content": "Our proposed approach involves a novel method for fine-tuning large language models, which can be considered as cost-effective as we only update 0.1% of the weights. This type of fine-tuning is environmentally friendly as it reduces resource wastage. We utilized pre-trained models from the Hugging Face repository and implemented updates using the Pytorch library. We exclusively used open-source datasets to avoid any potential harm or ethical concerns. By prioritizing ethical standards and recognizing potential risks, we strive to promote responsible and sustainable research practices."}]}