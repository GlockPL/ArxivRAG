{"title": "Improving Skeleton-based Action Recognition with Interactive Object Information", "authors": ["Hao Wen", "Ziqian Lu", "Fengli Shen", "Zhe-Ming Lu", "Jialin Cui"], "abstract": "Human skeleton information is important in skeleton-based action recognition, which provides a simple and efficient way to describe human pose. However, existing skeleton-based methods focus more on the skeleton, ignoring the objects interacting with humans, resulting in poor performance in recognizing actions that involve object interactions. We propose a new action recognition framework introducing object nodes to supplement absent interactive object information. We also propose Spatial Temporal Variable Graph Convolutional Networks (ST-VGCN) to effectively model the Variable Graph (VG) containing object nodes. Specifically, in order to validate the role of interactive object information, by leveraging a simple self-training approach, we establish a new dataset, JXGC 24, and an extended dataset, NTU RGB+D+Object 60, including more than 2 million additional object nodes. At the same time, we designe the Variable Graph construction method to accommodate a variable number of nodes for graph structure. Additionally, we are the first to explore the overfitting issue introduced by incorporating additional object information, and we propose a VG-based data augmentation method to address this issue, called Random Node Attack. Finally, regarding the network structure, we introduce two fusion modules, CAF and WNPool, along with a novel Node Balance Loss, to enhance the comprehensive performance by effectively fusing and balancing skeleton and object node information. Our method surpasses the previous state-of-the-art on multiple skeleton-based action recognition benchmarks. The accuracy of our method on NTU RGB+D 60 cross-subject split is 96.7%, and on cross-view split, it is 99.2%.", "sections": [{"title": "1 Introduction", "content": "Human action recognition is an important task in computer vision, with numerous applications including human-computer interaction [1, 2], smart security [3], and video content moderation [4]. Especially in the production field, as factories become increasingly digitized with the continuous advancement of information technology and the ongoing pursuit of greater production efficiency and reliability, the role of human action recognition in enhancing automation and monitoring processes becomes even more significant. However, digitizing human behavior in the production environment presents challenges due to the complexity of human actions and ethical concerns related to privacy. Thanks to advancements in deep learning technology, skeleton-based action recognition methods [5\u20138] are receiving increasing attention. Skeleton data, which represents the posture or movement of the human body using a graph structure composed of joint points and their connections, offers a compact representation that leads to higher computational efficiency than image-based data. Additionally, skeleton data offers good privacy as it does not reveal information such as faces, and it is more robust against changes in lighting and background noise, thanks to the filtering of complex appearance information.\nHowever, previous skeleton-based action recognition methods mostly have certain limitations:\nFirst, the skeleton graph is an oversimplified information carrier compared to an image. Many researchers attempt to add additional information such as RGB[9\u201312], depth[13], and optical flow[14] to skeleton modalities. Only a few methods[15-18] have attempted to introduce object information that interacts with human. These indicates that previous research did not fully recognize the importance of incorporating object interactions into the analysis of human action. As shown in Figure 1, many actions are difficult to distinguish based solely on skeleton information, such as \"Reading\" and \"Writing\". Conventional skeleton graphs do not include information about the objects that interact with people during these actions, such as the \"Book\" and \"Pen\". Because the skeleton sequences between these two actions are very similar, it is difficult to accurately distinguish them. In this context, the positions and various class attributes (such as category, size, shape, and color) of objects interacted with people play a crucial role in action recognition. As shown in Figure 3, this is also evident in action recognition within a factory environment. With this insight, we expanded the content of the skeleton graph to include information about the objects people interact with. We annotated approximately 2 million object nodes in the NTU RGB+D 60 dataset."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Skeleton Based Action Recognition", "content": "Skeleton-based action recognition has attracted considerable attention due to its compact data representation. The primary challenge is how to effectively model spatial and temporal patterns of skeleton sequences. While methods based on other network architectures such as Zoom Transformer [23], PoseC3D [11], Ta-CNN [24], and Structured Keypoint Pooling exist [15], GCN-based methods have remained the mainstream in this field. Since ST-GCN[5], a network architecture incorporating alternating spatial and temporal modules has achieved outstanding results and served as the foundation for subsequent notable methods[22, 25]. Shi et al. [26] enhanced the performance by analyzing the significance of second-order information, such as bone length and orientation. [27] utilizes the correlation-driven joint-bone fusion graph convolutional network (CD JBF-GC) to explore motion transmission between joint stream and bone stream. However, ST-GCN has certain limitations. For instance, it employs a fixed skeleton graph topology that only accounts for the physical connections between adjacent joints. To address this, several approaches[6-8] have attempted to capture implicit relationships among non-adjacent joints by utilizing diverse skeleton graph topologies, which may contain crucial latent information for action recognition. 2s-AGCN [26] proposed a dual-stream adaptive graph convolutional network that leverages the backpropagation algorithm to learn the graph topology in an end-to-end manner. Inspired by these approaches, we recognize the importance of adaptive connections and aim to overcome the limitation of a fixed number of nodes. To achieve this, we adopt a destructured graph construction strategy, which enhances the flexibility and generality of graph construction."}, {"title": "2.2 Skeleton Based Multimodal Action Recognition", "content": "Different modalities of data contain unique information, and integrating features from multiple modalities can improve action recognition results. Therefore, many methods have been proposed to enhance the effect of skeleton-based action recognition by incorporating information from other modalities. These methods can be classified into three categories.\nThe first category involves the fusion of visual-related information. For instance, SGM-Net [28] enhances important RGB information related to actions by using skeleton features to guide RGB features. The second category entails using additional sensor features. Fusion-GCN [29] integrates data from wearable sensors, such as IMUs. The third category involves leveraging other semantic information. For example, LA-GCN [30] proposes a graph convolutional network assisted by a large-scale language model (LLM) knowledge, which considers incorporating prior knowledge to aid potential representation learning for improved performance. Moreover, SGN [31] introduces higher-level semantics of joints (joint types and frame indices) to enhance feature representation capability.\nDespite the methods above effectively compensating for the limitations of skeleton modality, there still exist challenges in recognizing interactive actions. In interactive actions, the positions and categories of objects involved in human interactions are crucial information for identifying such actions. The method [15] based on the point cloud deep-learning paradigm introduced object information through object contour keypoints and class indices. [16] proposed a joint learning framework based on skeleton data for \"interactive object localization\" and \"human action recognition\" to mutually enhance these two tasks. [17] proposed that the object area can be detected by subtracting the human area from the moving area. However, previous GCN-based methods were unable to handle variable input node quantities. Therefore, this paper introduces object nodes and constructs Variable Graph to address these limitation. Additionally, unlike [15], we adopt class attributes (Encoding vectors based on textual descriptions) with better scalability to characterize the categories of object nodes."}, {"title": "3 Spatial Temporal Variable-Graph Convolutional Networks", "content": "In action recognition tasks, a substantial portion of actions entails interactions between humans and objects. This phenomenon is particularly conspicuous in industrial settings, exemplified by tasks such as product sorting. However, conventional skeleton graph lack information about the objects involved in these interactions, which limits the accuracy of action recognition. To address this issue, we propose ST-VGCN that introduces Variable Graph of object nodes through effective modeling to achieve more powerful action recognition. Our framework is shown in Figure 2.\nIn the first section, we introduce the setting and generation of object nodes and the construction of JXGC 24 and the extended dataset NTU RGB+D+Object 60. In the second section, we describe the construction strategy of Variable Graph. In the final section, we introduce the design of ST-VGCN."}, {"title": "3.1 Introduce Interactions Information", "content": ""}, {"title": "3.1.1 Interactive Object Node", "content": "Conventional skeleton graphs only focus on capturing human joint motions while ignoring interactive objects that play essential roles in many actions. Integrating interactive object information into action recognition is crucial to address the limitations of skeleton-based methods. This enhancement enables action recognition models to extract more discriminative features from interacting objects' categories, positions, and motions.\nInspired by multimodal nodes [29, 32], We introduce interaction information by adding object nodes $N_o \\in \\mathbb{R}^D$ to the skeleton graph, where D is the attribute dimension of the object node. $N_o$ has three sections, which can be expressed as follows:\n$N_o = I_{Pos} \\oplus I_{Prob} \\oplus I_{CA},$                                                                                                                   (1)\nwhere $\\oplus$ is the tensor concatenate operation. The first part, $I_{Pos} \\in \\mathbb{R}^{C_{Pos}}$, is the position information of the node. The second part, $I_{prob} \\in \\mathbb{R}^{C_{Prob}}$, is the predicted probability of the node. The third part, $I_{ca} \\in \\mathbb{R}^{C_{CA}}$, is the class attribute of the object, which includes a textual description encompassing details such as the object's name, shape, size, color, and other information, and is encoded using CLIP's text encoder [19]. The object category name and the encoding of the textual description are stored as corresponding key-value pairs within the class attribute dictionary. During preprocessing, the corresponding class attribute vector is found in the class attribute dictionary through the node's class index (obtained through object detection), and then concatenated to the corresponding node."}, {"title": "3.1.2 Object-Enriched Dataset", "content": "In order to promote the development of action recognition tasks for human interaction, we built an extended dataset NTU RGB+D+Object 60 based on NTU RGB+D 60. We selected 13 types of objects participating in the action from NTU RGB+D 60 to generate object nodes. We employ a self-training approach to train an object detection model for predicting object nodes, due to the massive quantity of annotations required for object nodes. The training process is outlined as depicted in Algorithm 1. We added about 2 million objects to NTU RGB+D+Object 60 through this self-training strategy. This annotation information covers objects position and category information."}, {"title": "3.2 Variable Graph Construction", "content": "We adopt a highly adaptable graph construction strategy to effectively represent the relationships between nodes in space and accommodate differences in the number of nodes between different videos or frames. In terms of node construction, inspired by scene graph[42], we adopt the idea of \"one frame, one graph\" and put all nodes (including skeleton nodes and object nodes) in each image frame into the same graph. We automatically map skeleton nodes and object nodes into a Variable Graph through simple rules, which greatly enhances the flexibility and scalability of the graph. Our single-frame Variable Graph is represented by a directed graph VG = (Vset, Eset), where Vset represents the node set and Eset represents the edge set. Firstly, to facilitate subsequent processing and modeling, we arrange the order of these nodes according to specific rules. The node set Vset can be formulated as:\n$V_{set} = concat(P_1, P_2, ..., P_m, Obj_1, ..., Obj_n),$                                                                                                                                   (2)\nwhere $P_m$ represents the m-th person, and $Obj_n$ represents the n-th category object node. They can be expressed as:\n$P_m = \\{v_i|1 \\leq i \\leq J, i \\in \\mathbb{N}\\},$                                                                                                                                    (3)\n$Obj_n = \\{v_i|1 \\leq i \\leq K, i \\in \\mathbb{N}\\},$                                                                                                                                (4)\nwhere m is the number of human individuals in the current frame, and n is the number of object node categories in the current frame. $v_i$ represents the attributes of a single node. J is the number of individual human skeleton nodes under the current standard. K is the total number of a single category object nodes in the current frame. For each human individual, the internal node order is performed according to the norms of different datasets. Regarding edge construction, inspired by methods such as infoGCN[7] and ST-GCN++[6], we realized the limitations of building connections based only on the physical topological relationship of the human body. At the same time, to improve the adaptability of the graph construction strategy, we adopt a destructured edge construction strategy:\n$E_1 = \\{(u, v), (v, u)|\\forall u, v \\in P_1 \\cup . . . \\cup P_m, u \\neq v\\},$                                                                                           (5)\n$E_2 = \\{(0, v)|\\forall o \\in Obj_1 \\cup . . . \\cup Obj_n, \\forall v \\in P_1 \\cup ... \\cup P_m\\},$                                                                     (6)\nwhere E1 is the edge between skeleton nodes, E2 is the unidirectional edge from the object node to the skeleton node, then the edge set of variable graph is as follows:\n$E_{set}=E_1\\cup E_2.$                                                                                                                                                                      (7)\nAccording to this construction strategy, the total number $E_n$ of edges in Variable Graph generated in a single frame is:\n$E_n = |E_{set}| = (m \\times J) \\times (m \\times J - 1) + m \\times J \\times \\sum_{i=1}^{n}K_i.$                                                                                                                                            (8)\nWith this graph construction method, we can fully capture the relationship between different nodes in space and flexibly adapt to the differences between videos or frames. By combining bidirectional and unidirectional edges, we can reduce the impact of noise introduced during the training and learning process while incorporating object information, further improving the performance of the networks."}, {"title": "3.3 The Design of ST-VGCN", "content": "In ST-VGCN, we made only necessary modifications to ST-GCN [5] to highlight the significant role of the introduced object nodes. We leverage a combination of Graph Convolutional Networks (GCN) and Temporal Convolutional Networks (TCN) to learn spatial and temporal relationships. Additionally, to handle Variable Graph, we introduce Node Padding and Class Attribute Fusion (CAF) modules. Moreover, we propose Weighted Node Pooling (WNPool) to emphasize critical node features and design the Node Balance Loss to balance the network's attention to different types of nodes. Finally, we propose a data enhancement method, Random Node Attack, to eliminate the overfitting problem caused by introducing category information."}, {"title": "3.3.1 Node Padding", "content": "Variable Graph may have different numbers of nodes between frames. Node padding ensures the consistency of the number of nodes, which is important for subsequent processing and calculation. Node padding is divided into two parts: inter-frame padding and intra-batch padding. Inter-frame padding ensures the consistency of the number of nodes within the skeleton sequence for each instance. First, we determining the maximum number of people and object nodes in all graphs in the skeleton sequence. Then, we padding the graph of each frame with empty nodes at the corresponding positions to match the maximum number of individual and object nodes. Intra-batch padding ensures the consistency of the number of nodes in each graph sequence within the same batch. The padding process is the same as inter-frame padding."}, {"title": "3.3.2 Class Attribute Fusion", "content": "At the channel level, the network may focus excessively on the class attribute channels, neglecting important information such as position and probability (original attributes in Figure 2). At the same time, these additional channels do not provide meaningful information for action recognition for many action classes that lack object nodes. Instead, they increase the dimensionality and complexity of the input data. We propose a novel and efficient fusion module called Class Attribute Fusion (CAF) to address these challenges. As shown in Figure 2, we fuse class attributes and spatial information in a cross-modal residual manner. With the help of CAF, the new features retain differentiated information and provide more compact and efficient representation."}, {"title": "3.3.3 Weighted Node Pooling", "content": "In action recognition, different actions involve the participation of different body parts, resulting in varying importance of different nodes across actions. However, previous methods [5, 6] often rely on simple average pooling to aggregate features from all nodes, which may obscure features on critical nodes. To address this issue, we propose Weighted Node Pooling (WNPool). Each node is assigned a learnable parameter before performing average pooling. By multiplying the features of each node by its corresponding parameter, we achieve weighted pooling of the node features. By introducing learnable parameters, our approach can adaptively adjust the weights of node features based on the dataset, highlighting the importance of critical nodes in action recognition. The specific formula of WNPool is as follows:\n$Y = WNPool(X) = XW/V.$                                                                                                                                                                         (9)\n$X \\in \\mathbb{R}^{N\\times C \\times V}$ and $Y \\in \\mathbb{R}^{N\\times C}$ are the input and output matrices, respectively. $W \\in \\mathbb{R}^{V}$ is the weight matrix. N is the batch size, C is the number of channels, and V is the total number of nodes."}, {"title": "3.3.4 Node Balance Loss", "content": "At the node level, we introduce the Node Balance Loss $L_{NB}$ to balance the network's attention between skeleton and object nodes and prevent overfitting issues. This loss function regulates the network's attention on these nodes by constraining the average channel size ratio between skeleton and object nodes. The average channel values Ssn and Son for skeleton and object nodes, respectively, are calculated using the function $f_{CA}$. The calculation formulas are as follows:\n$S_{sn} = f_{CA}(P_1 \\cup ... \\cup P_m) = \\frac{\\sum_{v\\in P_1\\cup...\\cup P_m} \\sum_{t=1}^{T}\\sum_{c=1}^{C} x_{tcv}}{P_1 \\cup ... \\cup P_m},$\n(10)\n$S_{on} = f_{CA}(Obj_1 \\cup ... \\cup Obj_n) =  \\frac{\\sum_{v \\in Obj_1\\cup...\\cup Obj_n} \\sum_{t=1}^{T}\\sum_{c=1}^{C} x_{tcv}}{Obj_1 \\cup ... \\cup Obj_n},$                                                                                                                                                                                                                                                                          (11)\nwhere $x_{tcv}$ is the value of the corresponding channel. Then, the $L_{NB}$ can be expressed as the following formula:\n$L_{NB} =  \\begin{cases}0, & \\text{Ssn} = 0 \\text{ or } \\text{Son} = 0 \\\\\\ - log(\\frac{\\sqrt[]{S_{sn}}}{\\sqrt[]{S_{on}}}), & \\text{Ssn} \\neq 0 \\text{ and } \\text{Son} \\neq 0.\\end{cases}$                                                                                                                                                                                                                                                                 (12)\n$L_{CE} = - \\sum_{i}y_i log(\\hat{y_i}),$                                                                                                                                                                                                     (13)"}, {"title": "3.3.5 Random Node Attack", "content": "In our experiments, we observed that incorporating object category information led to significant biases in the network. For instance, when a book was detected in unrelated videos, the model often misclassified the action as \"Reading\", disregarding the actual motion information. This behavior indicated severe overfitting. To address this issue, we propose a VG-based data augmentation method. During training, we augment the Variable Graph by adding a random number of object nodes, each with randomly assigned positions, a random confidence score, and a random category. This approach effectively addresses the network's tendency to overfit to additional object nodes."}, {"title": "4 Experiments", "content": "In this section, we evaluate the performance of ST-VGCN in the skeleton-based action recognition experiments. To validate the effectiveness of ST-VGCN, we conducted experiments on three datasets. We compared our method with the state-of-the-art approaches and performed ablation experiments to assess the individual contributions of the main component in our method. We performed experiments using the 2D skeleton estimated by HRNet , provided by pyskl[6], for all datasets. The extraction of keypoints for all objects is performed by the YOLOv5l model. Most experiments were implemented using the PyTorch deep learning framework on an NVIDIA RTX 4090 GPU. The model was trained for 150 epochs using the SGD optimizer. The learning rate was initially set to 0.00625, and the Cosine AnnealingLR strategy was used to adjust the learning rate. The momentum was set to 0.9, the weight decay was set to 0.0005, and the batch size was set to 8."}, {"title": "4.1 Datasets", "content": ""}, {"title": "4.1.1 NTU RGB+D", "content": "The NTU RGB+D 60 dataset is a large-scale dataset for human action recognition. The dataset contains 56,880 samples of 60 action classes collected from 40 subjects. Many action classes involve the interaction between humans and objects. The dataset evaluation includes two benchmarks: cross-subject (X-Sub) and cross-view (X-View), divided by actor and camera view for training and evaluation, respectively."}, {"title": "4.1.2 NTU RGB+D 120", "content": "NTU RGB+D 120 is an extension of NTU RGB+D, encompassing an additional 60 classes and 57,600 extra video samples. In total, NTU RGB+D 120 comprises 120 classes and 114,480 samples. The dataset evaluation includes two benchmarks: Cross-Subject (X-Sub) and Cross-Setup (X-Set)."}, {"title": "4.1.3 JXGC 24", "content": "JXGC 24 is a small dataset established based on human-object interactions in a specific environment, reflecting the actions of furniture factory workers during the wooden workpiece polishing process, consisting of 1770 video samples. The dataset encompasses two classification dimensions: action categories and interaction object categories. The dataset evaluation includes three benchmarks: \"Action\", \"Workpiece\", and \"Both\"."}, {"title": "4.2 Experimental Results", "content": "First, The results of the last row and the third last row of Table 2 indicate that adding object nodes can significantly improve the results of all benchmark tests. These results validate the efficacy of our proposed method in skeleton-based action recognition. To further demonstrate the superiority of our method, we compare it with the previous state-of-the-art methods on four benchmarks in Table 2. Our approach achieves new state-of-the-art performance on two of the benchmarks. Notably, on two benchmarks of NTU RGB+D 60, our method surpasses all previous methods in performance on a single modality. For Four-Stream results, our method significantly outperforms the previous state-of-the-art (X-Sub: 96.7% vs. 94.1%; X-View: 99.2% vs. 97.8%). When the accuracy is already relatively high, such improvements of 2.6% and 1.4% are rare in previous methods. We believe this illustrates the importance of introducing object nodes. On JXGC 24, our method significantly outperforms other methods, proving its generalization and effectiveness in specific industrial scenarios."}, {"title": "4.3 Ablation Studies", "content": "We examined the model's classification accuracy under different configurations to analyze the effects of individual components in our approach. All ablation experiments were conducted solely using the joint modality for classification on the JXGC 24 and the NTU RGB+D 60 dataset(X-Sub)."}, {"title": "4.3.1 Interactive object information", "content": "At the node level, we compared classification accuracy before and after incorporating Object Nodes, and the results presented in Table 2 demonstrate a significant enhancement in performance across all benchmark tests. Moreover, Figure 4 reveals that actions involving human-object interactions, such as 'Reading' and 'Writing,' exhibit improved classification accuracy after introducing object nodes. This underscores the importance of object information in actions that entail human-object interactions. At the channel level, our ablation experiments, as detailed in Table 4, further investigated the roles of position information and class attributes within object nodes. Position information alone boosted performance by 1.3%, class attributes alone by 2.7%, and combining both yielded a 3.5% improvement. These findings indicate that the position information and class attributes within object nodes contribute significantly to action recognition, and these two types of information complement each other. The experiments in Table 3 also demonstrate to us the mechanism of how our method works in actual scenarios. First, our method only achieves slightly better performance than the original version ST-GCN in \"Action\" split, which is reasonable because the type of action in the \"Action\" split has nothing to do with the type of interactive object. The information about the interactive object cannot help classify the action. On the contrary, our method significantly outperformers other methods in \"Workpiece\" split, where action types are strongly related to interactive objects. This also proves the importance of interactive objects for recognizing such actions. Finally, the results of the \"Both\" split show that our method can best distinguish which operation the worker performs on which workpiece, proving our method's superiority in the action recognition scenario of human-object interaction.\nAs shown in Table 5, the introduction of object nodes results in a slight increase in parameter count, computational complexity, and inference time. Such an overhead is often acceptable, especially in scenarios where higher model performance is required."}, {"title": "4.3.2 Unidirectional Edge", "content": "We conducted ablation experiments to assess the impact of the unidirectional edge from the object node to the skeleton node. As shown in Table 6, the experimental results using unidirectional edges are better than those using bidirectional edges. Our analysis believes that when there are bidirectional edges, the empty nodes formed by node padding will also produce non-zero values through graph convolution, which may constitute a kind of noise, so unidirectional edges are used to fix the values of object nodes, preventing the generation of noise nodes."}, {"title": "4.3.3 Class Attribute Fusion module", "content": "When the CAF module is removed, we retain the original network structure and directly concatenate the original attributes with the class attributes. This concatenated feature was then fused through a fully connected layer to replace the original attributes. As shown in Table 6, experimental results can always be improved when using the CAF module. This shows that the CAF module can effectively fuse two information modalities at the channel level."}, {"title": "4.3.4 Weighted Node Pooling", "content": "To validate the impact of WNPool on the model, we replaced the original average pooling with our proposed WNPool in both the baseline and after introducing object nodes. As shown in Table 6, in both scenarios, the utilization of WNPool led to a performance improvement of 0.3%. This indicates that WNPool effectively captures information from crucial nodes at the dataset level, enhancing the model's recognition capability."}, {"title": "4.3.5 Node Balance Loss", "content": "In Table 6 the introduction of node balance loss results in an improved recognition accuracy (95.7% vs. 96.1%). Simultaneously, Figure 5 indicates that node balance loss is sensitive to the setting of A. The network achieves optimal classification performance 93.0% when the parameter A is set to 0.1. Setting the parameter A to either high or low values reduces performance. Particularly, when A is excessively large, the accuracy falls below that of not using the node balance loss, possibly due to an overemphasis on node balancing, leading to overfitting."}, {"title": "4.3.6 Random Node Attack", "content": "The results in Table 7 show that the model trained without Random Node Attack has significant performance degradation when faced with interference from the 'Book' node or random nodes, which indicates that the network is overfitting the object nodes. After adding Aandom Node Attack during training, the performance was very stable during testing when faced with directional or random interference. This fully reflects the role of the data enhancement method we proposed."}, {"title": "5 Conclusion", "content": "This paper presents a novel approach for effectively recognizing actions involving human-object interactions. By introducing object nodes, we successfully integrate positional and class information of interacting objects into the skeleton graph. In addition, we established a new dataset JXGC 24, and an extended data set NTU RGB+D+Object 60, which is very meaningful for studying skeleton-based human-object interaction actions. Our proposed ST-VGCN can learn Variable Graph and improve recognition accuracy through introduced modules, such as CAF, WNPool, and node balance loss. Moreover, it also solves the overfitting problem caused by the introduction of category information through the random node attack strategy. The experimental results demonstrate that our framework performs state-of-the-art on three benchmark tests. Our work offers novel insights for future endeavors, such as incorporating relationships between humans and backgrounds and objects and objects into action recognition. This presents a meaningful direction for further exploration."}]}