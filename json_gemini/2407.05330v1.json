{"title": "Fast Proxy Experiment Design for Causal Effect Identification", "authors": ["Sepehr Elahi", "Sina Akbari", "Jalal Etesami", "Negar Kiyavash", "Patrick Thiran"], "abstract": "Identifying causal effects is a key problem of interest across many disciplines. The two long- standing approaches to estimate causal effects are observational and experimental (randomized) studies. Observational studies can suffer from unmeasured confounding, which may render the causal effects unidentifiable. On the other hand, direct experiments on the target variable may be too costly or even infeasible to conduct. A middle ground between these two approaches is to estimate the causal effect of interest through proxy experiments, which are conducted on variables with a lower cost to intervene on compared to the main target. Akbari et al. [2022] studied this setting and demonstrated that the problem of designing the optimal (minimum-cost) experiment for causal effect identification is NP-complete and provided a naive algorithm that may require solving exponentially many NP-hard problems as a sub-routine in the worst case. In this work, we provide a few reformulations of the problem that allow for designing significantly more efficient algorithms to solve it as witnessed by our extensive simulations. Additionally, we study the closely-related problem of designing experiments that enable us to identify a given effect through valid adjustments sets.", "sections": [{"title": "1 Introduction", "content": "Identifying causal effects is a central problem of inter- est across many fields, ranging from epidemiology all the way to economics and social sciences. Conducting randomized (controlled) trials provides a framework to analyze and estimate the causal effects of interest, but such experiments are not always feasible. Even when they are, gathering sufficient data to draw sta- tistically significant conclusions is often challenging because of the limited number of experiments often (but not solely) due to the high costs.\nObservational data, which is usually more abun- dant and accessible, offers an alternative avenue. How- ever, observational studies bring upon a new challenge: the causal effect may not be identifiable due to un- measured confounding, making it impossible to draw inferences based on the observed data [Pearl, 2009, Hern\u00e1n and Robins, 2006].\nA middle ground between the two extremes of observational and experimental approaches was introduced by Akbari et al. [2022], where the authors suggested conducting proxy experiments to identify a causal effect that is not identifiable based on solely observational data. To illustrate the need for proxy experiments, consider the following drug-drug interaction example, based on the example in Lee et al. [2020a].\nExample 1. (Complex Drug Interactions and Cardiovascular Risk) Consider a simplified example of the interaction between antihypertensives (X1), anti-diabetics (X2), renal function modulators (X3),"}, {"title": "2 Problem formulation", "content": "We begin by reviewing relevant graphical definitions. An acyclic directed mixed graph (ADMG) is a graph with directed (\u2192) and bidirected (\u2194) edges such that the directed edges form no cycles [Richardson, 2003]. We denote an ADMG G by a tuple G = (V, (V, E, E), where V, E, and E represent the set of vertices, directed edges, and bidirected edges, respectively. Note that E is a set of ordered pairs of vertices in V, whereas E is a set of unordered pairs of vertices.\nVertices of G represent variables of the system under consideration, while the edges represent causal relations between them. We use the terms \u2018variable' and \u2018vertex' interchangeably. When (y,x) \u2208 \u1eba, we say y is a parent of x and x is a child of y. The set of parents of X C V denoted by Pa(X) = {y : (y, x) \u2208 E for some x \u2208 X} \\X. We denote by G[W], the induced subgraph of G over vertices W C V. A subset W of V is said to form a district in G if any pair of vertices x, y \u2208 W are connected through a bidirected path x \u2194\u2192y in G[W]. In other words, G[W] is a connected component through its bidirected edges. We say x \u2208 V is an ancestor of SC V if there is a directed path x \u2192\u2192s for some s\u2208 S. We denote the set of ancestors of S in the subgraph G[W] by Ancw (S). Note that SC Ancw(S). When W = V, we drop the subscript for ease of notation.\nLet X, Y CV be two disjoint sets of variables. The probability distribution of Y under a (possibly hypothetical) intervention on X setting its value to x is often represented as either P(Y(x)), using Rubin's potential outcomes model [Rubin, 1974], or P(Y | do(X = x)) using Pearl's do operator [Pearl, 2009]. We will adopt the shorthand Px(Y) to denote this interventional distribution2.\nDefinition 1 (Identifiability). An interventional distribution Px(Y) is identifiable given an ADMG G and the intervention set family I = {11, . . .,It}, with Ii \u2286 V, over the variables corresponding to G, if Px(Y) is uniquely computable as a functional of the members of {P1(\u00b7) : I \u2208 I}.\nRemark 1. It is common in the literature to define identifiability with respect to observational data only (i.e., when I = {Z\u2081 = 0}). Our definition above follows what is known as the \u2018general identifiability' from Lee et al. [2020a], Kivva et al. [2022].\nWe will now define the important notion of a hedge, which, as we will see shortly after, is central to deciding the identifiability of an interventional distribution given the data at hand.\nDefinition 2 (Hedge). Let S \u2286 V be a district in G. We say W\u2287 S forms a hedge for S if (i) Wisa district in G, and (ii) every vertex w \u2208 W is an ancestor of S in G[W] (i.e., W = Ancw(S)). We denote by Hg(S) the set of hedges formed for S in G.\nFor example, in Fig. 2(b), S has two hedges given by H\u00e7(S) = {{S, X3, X1}, {S, X3, X1, X2}}.\nRemark 2. Definition 2 is different from the original definition of Shpitser and Pearl [2006]. The original definition was found to not correspond one-to-one with non-identifiability, as pointed out by Shpitser [2023]. However, our modified definition above is a sound and complete characterization of non-identifiability, as it coincides with the criterion put forward by Huang and Valtorta [2006] as well as the 'reachable closure' of Shpitser [2023].\nDefinition 3 (Hedge hull Akbari et al., 2022). Let S be a district in ADMG G. Also let Hg(S) be the set of all hedges formed for S in G. The union of all hedges in H\u00e7(S), denoted by H\u00e7(S) = UW\u2208HG(S) W, is said to be the hedge hull of S in G.\nFor instance, in Fig. 2(d), the hedge hull of S\u2081 is HG(S1) = {S1, S2, X1, X2, X3, X4, X5} and the hedge hull of S2 is HG(S2) = {S2, X3}. When a set S consists of more than one district, we simply define the hedge hull of S as the union of the hedge hulls of each district of S. The hedge hull of a set can be found through a series of at most |V| depth-first-searches. For the sake of completeness, we have included the algorithm for finding a hedge hull in Appendix B.1.\nThe following proposition from Lee et al. [2020a] and Kivva et al. [2022] establishes the graphical criterion for deciding the identifiability of a causal effect given a set family of interventions.\nProposition 1. Let G be an ADMG over the vertices V. Also let X,Y CV be disjoint sets of variables. Define S = Ancv\\x(Y), and let S = {S1,...,Sr} be the (unique) set of maximal districts in G[S]. The"}, {"title": "3 Reformulations of the min-cost intervention problem", "content": "In the previous section, we delineated the MCID problem as a discrete optimization problem. This problem, cast as Eq. (1), necessitates search within a doubly exponential space, which is computationally intractable. Algorithm 2 of [Akbari et al., 2022] is an algorithm that conducts this search and eventually finds the optimal solution. However, even when S comprises a single district, this algorithm requires, in the worst case, exponentially many calls to a subroutine which solves the NP-complete minimum hitting set problem on exponentially many input sets, hence resulting in a doubly exponential complexity. More specifically, their algorithm attempts to find a set of minimal hedges, where minimal indicates a hedge that contains no other hedges, and solves the minimum hitting set problem on them. However, there can be exponentially many minimal hedges, as shown for example in Fig. 2(c). Letting m = n/2, then any set that contains one vertex from each level (i.e., directed distance from S) is a minimal hedge, of which there are O(2n/2).\nFurthermore, the computational complexity of Algorithm 2 of Akbari et al. [2022] grows super- exponentially in the number of districts of S. This is due to the necessity of exhaustively enumerating every possible partitioning of these districts and executing their algorithm once for each partitioning."}, {"title": "3.1 Min-cost intervention as a WPMAX-SAT problem", "content": "We begin with constructing a 3-SAT formula F that is satisfiable if and only if the given query Px (Y) is identifiable. To this end, we define m+2 variables {xi,j}m for each vertex vi \u2208 V, where m = |Hg(S)\\S| is the cardinality of the hedge hull of S, excluding S. Intuitively, xi,j is going to indicate whether or not vertex vi is reachable from S after j iterations of alternating depth-first-searches on directed and bidirected edges. This is in line with the workings of Algorithm 2 for finding the hedge hull of S. In particular, if a vertex vi is reachable after m + 1 iterations, that is, Xi,m+1 = 1, then vi is a member of the hedge hull of S. The query of interest is identifiable if and only if Hg(S) = S, that is, the hedge hull of S contains no other vertices. Therefore, we ensure that the formula F is satisfiable if and only if Xi,m+1 = 0 for every vi \u2209 S. The formal procedure for constructing this formula is as follows.\nSAT Construction Procedure. Suppose a causal ADMG G = (V,E,E) and a set SCV are given, where S is a district in G. Suppose H\u00e7(S) = {v1,..., Un} is the hedge hull of S in G, where without loss of generality, S = {Um+1,... Un}, and {V1,... vm}\u2229S = 0. We will construct a corresponding boolean expression in conjunctive normal form (CNF) using variables {xi,j} for i \u2208 {1,...,m} and j\u2208 {0,..., m + 1}. For ease of presentation, we also define xi,j = 1 for all i \u2208 {m + 1,..., n}, j\u2208 {0,..., m + 1}. The construction is carried out in m + 2 steps, where in each step, we conjoin new clauses to the previous formula using 'and'. The procedure is as follows:\n\u2022 For odd j\u2208 {1, ...,m+1}, for each directed edge (vi, ve) \u2208 \u00c8, add (\u00abXi,j\u22121 \u2228 Xi,jVxl,j) to F.\n\u2022 For even j \u2208 {1, ..., m + 1}, for each bidirected edge {vi, ve} \u2208 E, add both clauses (\u00acXi,j\u22121 V Xi,j V \u00acxl,j) and (\u00abxl,j\u22121 V Xl,j V \u00abXi,j) to F.\n\u2022 Finally, at step m + 2, add clauses \u00acXi,m+1 to the expression F for every i \u2208 {1, ..., m}.\nTheorem 1. The 3-SAT formula F constructed by the procedure above given G and S has a satisfying solution {x;} where x1,0 = 0 for i \u2208 Z \u2286 {1, . . .,m} and x1,0 = 1 for i \u2208 {1, ...,m}\\I if and only if I intersects every hedge formed for S in G; i.e., I is a feasible solution to the optimization in Eq. (2).\nThe proofs of all our results appear in Appendix C. The first corollary of Theorem 1 is that the SAT formula is always satisfiable, for instance by setting 2,0 = 0 for every i \u2208 {1, ..., m}. The second (and more important) corollary is that the optimal solution to Eq. (2) corresponds to the satisfying assignment for the SAT formula F that minimizes\n$\\sum_{i=1}^{m}(1 \u2013 x_{i,0})C(v_{i}).$\nThis suggests that the problem in Eq. (2) can be reformulated as a weighted partial MAX-SAT (WPMAX- SAT) problem. WPMAX-SAT is a generalization of the MAX-SAT problem, where the clauses are partitioned into hard and soft clauses, and each soft clause is assigned a weight. The goal is to maximize the aggregate weight of the satisfied soft clauses while satisfying all of the hard ones.\nTo construct the WPMAX-SAT instance, we simply define all clauses in F as hard constraints, and add a soft clause xi,o with weight C(vi) for every i \u2208 {1, ..., m}. The former ensures that the assignment corresponds to a feasible solution of Eq. (2), while the latter ensures that the objective in Eq. (3) is minimized - which, consequently, minimizes the cost of the corresponding intervention."}, {"title": "4 Minimum-cost intervention design for adjustment criterion", "content": "A special case of identifying interventional distributions is identification through adjusting for confounders. A set ZC V is a valid adjustment set for Px (Y) if Px (Y) is identified as\n$\\displaystyle P_{X}(Y)=\\mathbb{E}_{Z}[P(Y \\mid X, Z)].$\nwhere the expectation w.r.t. P(Z). Adjustment sets have received extensive attention in the literature because of the straightforward form of the identification formula (Eq. 4) and the intuitive interpretation: Z is the set of confounders that we need to adjust for to identify the effect of interest. The simple form of Eq. (4) has the added desirable property that its sample efficiency and asymptotic behavior are easy to analyze [Witte et al., 2020, Rotnitzky and Smucler, 2020, Henckel et al., 2022]. A complete graphical criterion for adjustment sets was given by Shpitser et al. [2010]. As an example, when all parents of X (i.e., Pa(X)) are observable, they form a valid adjustment set. However, in the presence of unmeasured confounding, no valid adjustment sets may exist. Below, we generalize the notion of adjustment sets to the interventional setting."}, {"title": "5 Experiments", "content": "In this section, we present numerical experiments that showcase the empirical performance and time efficiency of our proposed exact and heuristic algorithms. A comprehensive set of numerical experiments analyzing the impact of various problem parameters on the performance of these algorithms, along with the complete implementation details, is provided in Appendix A. We first compare the time efficiency of our exact algorithms: WPMAX-SAT and ILP, with the exact algorithm of Akbari et al. [2022]. Then, we present results pertaining to performance of our heuristic algorithm. All experiments, coded in Python, were conducted on a machine equipped two Intel Xeon E5-2680 v3 CPUs, 256GB of RAM, and running Ubuntu 20.04.3 LTS.\nResults on exact algorithms. We compare the performance of the WPMAX-SAT formulation, the ILP formulation, and Algorithm 2 of Akbari et al. [2022], called Minimal Hedge Solver (MHS) from hereon. We used the RC2 algorithm [Ignatiev et al., 2019], and the Gurobi solver [Gurobi Optimization, LLC, 2023], to solve the WPMAX-SAT problem, and the ILP, respectively. We ran each algorithm for solving the MCID problem on 100 randomly generated Erdos-Renyi [Erdos and Renyi, 1960] ADMG"}, {"title": "6 Conclusion", "content": "We presented novel formulations and efficient algorithms for the MCID problem, offering substantial improvements over existing methods. Our work on designing minimum-cost experiments for obtaining valid adjustment sets demonstrates both practical and theoretical advancements. We highlighted the superior performance of our proposed methods through extensive numerical experiments. We envision designing efficient approximation algorithms for MCID as future work."}, {"title": "A.1 Implementation details", "content": "Our codebase is implemented fully in Python. We use the PySAT library for formulating and solving the WPMAX-SAT problem, and the PuLP library for formulating and solving the ILP problem.\nSolving the WPMAX-SAT problem. There are several algorithms to solve the WPMAX-SAT instance to optimality. These algorithms include RC2 [Ignatiev et al., 2019] and OLL [Morgado et al., 2014], both of which are core-based algorithms that utilize unsatisfiable cores to iteratively refine the solution. In this context, a \"core\" refers to an unsatisfiable subset of clauses within the CNF formula that cannot be satisfied simultaneously under any assignment. These algorithms relax the unsatisfiable soft clauses in the core by adding relaxation variables and enforce cardinality constraints on these variables. By strategically increasing the bounds on these cardinality constraints or modifying the weights of soft clauses based on the cores identified, the algorithms efficiently reduce the search space and converge on the maximum weighted set of satisfiable clauses, thereby solving the WPMAX-SAT problem optimally.\nSolving the ILP problem. Similarly, with the ILP formulation of the MCID problem presented in Section 3, we can utilize exact algorithms designed for solving ILP problems to find an optimal solution. ILP solvers work by formulating the problem with linear inequalities as constraints and integer variables that need to be optimized. Popular ILP solvers include CPLEX [IBM Corporation, 2023], Gurobi [Gurobi Optimization, LLC, 2023], and the open-source solver CBC [Forrest and Lougee-Heimer, 2023]. The latter is a branch-and-cut-based solver, and cutting plane methods to explore feasible integer solutions systematically while pruning the search space based on bounds calculated during the solving process. We use the Gurobi solver in our experiments."}, {"title": "A.2 Extended WPMAX-SAT simulations", "content": "We extended the simulations in Section 5 for up to n = 500 vertices, and the results are presented in Fig. 5. We observe that even at n = 500, WPMAX-SAT takes around the same time as Algorithm 2 of Akbari et al. [2022] does to solve n = 40 (230 s for both). Moreover, we can clearly see the exponential growth in time complexity, as expected, especially for n > 400."}, {"title": "A.3 Investigating the effects of directed and bidirected edge probabilities on the performance of exact algorithms", "content": "We run experiments on varying the probabilities of directed and bidirected edges in the graph. We fix the number of vertices at n = 20 and vary the probabilities of directed and bidirected edges from 0.001 to 1.00 in increments of 0.001. The results are presented in Fig. 6."}, {"title": "A.4 Investigating the effect of cost on the performance of the algorithms", "content": "We run experiments with n = 20 and costs sampled from a Poisson distributions with mean parameter ranging from 1 to 100. The results are presented in Fig. 7. Interestingly, there appears to be no clear trend in the time complexity of the algorithms with respect to the mean parameter of the Poisson distribution."}, {"title": "A.5 Investigating the effects of directed and bidirected edge probabilities on the performance of the heuristic algorithms", "content": "We run experiments on varying the probabilities of directed and bidirected edges in the graph. We vary n from n = 10 to n = 200 and the probabilities of directed and bidirected edges in {0.1, 0.5}. The results are presented in Fig. 8. We see that our proposed heuristic algorithm consistently outperforms the heuristic algorithms of Akbari et al. [2022] for all graph sizes and edge probabilities."}, {"title": "B.1 Pruning algorithm for finding the hedge hull", "content": "We include the algorithm for finding the hedge hull for the sake of completeness. This algorithm is adopted from Akbari et al. [2022]."}, {"title": "B.2 SAT construction procedure for multiple districts", "content": "The procedure for constructing the SAT formula when S comprises multiple districts was postponed to this section due to space limitations. This procedure is detailed below."}, {"title": "C Missing Proofs", "content": ""}, {"title": "C.1 Results of Section 3", "content": "Theorem 1. The 3-SAT formula F constructed by the procedure above given G and S has a satisfying solution {x,j} where x1,0 = 0 for i\u2208I\u2286 {1,...,m} and x0 = 1 for i \u2208 {1,...,m}\\I if and only if I intersects every hedge formed for S in G; i.e., I is a feasible solution to the optimization in Eq. (2).\nProof. Proof of 'if:' Suppose I hits every hedge formed for S. We construct a satisfying solution for the SAT formula as follows. We begin with xi,o:\n*$\\displaystyle x_{i,0}=\\begin{cases}0;\\quad\\text{ if }i \\in I\\\\1; \\quad\\text{o.w.}\\end{cases}.$\nFor every j\u2208 {1, ..., m + 1}, define Hj = {i : Xi,j\u22121 = 1}. Then x\u00b8\u00a1 for j \u2208 {1, ..., m + 1} is chosen recursively as below.\n\u2022 Odd j: x,j = 1 if i \u2208 Hj and vi has a directed path to S in G[Hj], and x,j = 0 otherwise.\n\u2022 Even j: x = 1 if i \u2208 Hj and vi has a bidirected path to S in G[Hj], and x,j = 0 otherwise.\nNext, we prove that {x} as defined above satisfies F. We consider the three types of clauses in F separately:\n\u2022 For odd j \u2208 {1, ..., m + 1}, the clause (Xi,j\u22121 \u2228 Xi,j \u2228 \u00acxl,j) corresponds to the directed edge (vi,ve) \u2208 E: if either 2-1 0 or x,j = 0, then this clause is trivially satisfied. So suppose xj-1 = 1, and xx,j = 1, which implies by construction that x,j\u22121 = 1. Therefore, i, l \u2208 Hj. Further, since x = 1, ve has a directed path to S in G[Hj]. Then vi has a directed path to S in G[Hj] because of the edge (vi,ve) \u2208 \u1eba. By the construction above, = 1, which satisfies the clause.\n\u2022 For even j \u2208 {1, ..., m + 1}, the clause (\u00abXi,j\u22121 \u2228 Xi,j \u2228 \u00acxl,j) corresponds to the bidirected edge {Vi, ve} \u2208 E: E: if either x-1 = 0 or x = 0, then this clause is trivially satisfied. So suppose"}, {"title": "C.2 Results of Section 4", "content": "Lemma 2. Let X, Y be two disjoint sets of vertices in G such that X is minimal as defined above. Set Z \u2286 V is a generalized adjustment set for Px(Y) under intervention I if (i) ZC Anc(S), and (ii) Z is a vertex cut between S and Pa(S) in (GTPa(S))m, where S = Ancv\\x(Y), and GTPa(s) is the ADMG resulting from omitting all edges incoming to I and all edges outgoing of Pa(S).\nProof. Define S = Ancv\\x(Y). First, we show that Pa(S) \u2286 X. Assume the contrary, i.e., there is a vertex w \u2208 Pa(S) \\ X. Clearly w has a directed path to S (a direct edge) that does not go through X. This implies that w \u2208 Ancy\\x(S), and since by definition, S = Ancv\\x(Y), w \u2208 Ancv\\x(Y) = S. However, the latter contradicts with w \u2208 Pa(S). Second, we note that from the third rule of do calculus [Pearl, 2009], Pw (S) = PPa(s) (S) for any W \u2283 Pa(S). Combining the two arguments, we have the following:\n$\\begin{equation}\\label{eq:1}P_{W}(S)=P_{X}(S),\\,\\forall W\\supseteq Pa(S). \\end{equation}$\nTo proceed, we will use the following proposition.\nProposition 3 (Lauritzen et al., 1990). Let S,R, and Z be disjoint subsets of vertices in a directed acyclic graph G. Then Z d-separates S from R if and only if Z is a vertex cut between S and R in (G[Anc(SURUZ)])m.\nChoose R = Pa(S) in the proposition above. Since Z\u2286 Anc(S), we have that Anc(SURUZ) = Anc(S). From condition (ii) in the lemma, Z is a vertex cut between S and R in (G\u012bpa(S))m, which implies it is also a vertex cut in (G\u012bpa(S)[Anc(S)])m, as every path in the latter graph exists in (GTPa(s))m. Using the proposition above, Z d-separates S and Pa(S) in G\u012bpa(s). This is to say, Z blocks all non-causal paths from Pa(S) to S in G7, and it clearly has no elements that are descendants of Pa(S). Therefore, Z satisfies the adjustment criterion of Shpitser et al. [2010] w.r.t. PPa(s) (S) in G\u012b. That is, the following holds:\n$\\begin{equation}P_{Z\\cup Pa(S)}(S)=\\mathbb{E}_{Z}[P_{1}(S \\mid Pa(S), Z)], \\end{equation}$\nwhere the expectation is w.r.t. P\u2081(Z). Choosing W = IUPa(S) in Eq. (6), we get\n$\\begin{equation}P_{X}(Y)=\\mathbb{E}_{P_{1}} [P_{1}(Y \\mid Pa(S), Z)].\\end{equation}$\nMarginalizing S \\ Y out in both sides of the equation above, we have\n$\\begin{equation}P_{X}(Y)=\\mathbb{E}_{P_{1}} [P_{1}(Y \\mid Pa(S), Z)].\\end{equation}$"}, {"title": "D Alternative Formulations", "content": ""}, {"title": "D.1 Min-cost intervention as a submodular function maximization problem", "content": "In this Section, we reformulate the minimum-cost intervention design as a submodular optimization problem. Submodular functions exhibit a property akin to diminishing returns: the incremental gain from adding an element to a set decreases as the set grows [Nemhauser et al., 1978].\nDefinition 5. A function f : 2V \u2192 R is submodular if for all A C B \u2286 V and v \u2208 V \\ B, we have that f(A\u222a {v}) \u2212 f(A) \u2265 f(BU {v}) \u2212 f(B).\nGiven an ADMG G = (V, \u00c8, E) a district S in G, and an arbitrary set I \u2286 V \\ S, we define fs(I) as the negative count of hedges formed for S in G[V \\ I].\nLemma 3. For any district S \u2286 V, the function fs : 2V\\S \u2192 Z\u22640 is submodular.\nNote that gs : 2V\\S \u2192 R, where gs(I) := fs(I) + \u03b1 $\\sum_{v\\in V}C(v)$, and a is an arbitrary constant, is also submodular as the second component is a modular function (similar definition as in 5 only with equality instead of inequality.).\nProposition 4. The combinatorial optimization of Eq. (2) is equivalent to the following unconstrained submodular optimization problem.\n$\\begin{equation} I^{*}= \\displaystyle argmax_{I \\subset V\\setminus S} \\dfrac{f_{S}(I)}{1+\\sum_{v \\in V\\setminus S}C(v)}. \\end{equation}$"}, {"title": "D.2 Min-cost intervention as an RL problem", "content": "We model the MCID problem given a graph G = (V, E) and S C V, as a Markov decision process (MDP), where a vertex is removed in each step t until there are no hedges left. The goal is to minimize the cost of the removed vertices (i.e., intervention set). Naturally, the action space is the set of vertices, V and the state space is the set of all subsets of V. More precisely, let st and at denote the state and the action of the MDP at iteration t, respectively. Then, st is the hedge hull for S from the remaining vertices at time t, and action at is the vertex that will be removed from Ve in that iteration. Consequently, the state transition due to action at is st+1 = Hhull(Vt \\ {at}). The immediate reward of selecting action at at state st will be the negative of the cost of removing (i.e., intervening on) at, given by\nr(st, at) = -C(at).\nThe MDP terminates when there are no hedges left and the hedge hull of the remaining vertices is empty (i.e., st = (\u00d8). The goal is to find a policy \u03c0 that maximizes sum of the rewards until the termination of the MDP. Formally, the goal is to solve\n$\\begin{equation} \\label{policy}argmmax_{\\pi} \\sum_{t=1}^{T} r(s_t,a_t), \\end{equation}$\nwhere s1 V and T is the time step at which the MDP terminates (i.e., sr = 0)."}]}