{"title": "Path Planning for Masked Diffusion Model Sampling", "authors": ["Fred Zhangzhi Peng", "Zachary Bezemek", "Sawan Patel", "Jarrid Rector-Brooks", "Sherwood Yao", "Alexander Tong", "Pranam Chatterjee"], "abstract": "In this paper, we explore how token unmasking order influences generative quality in masked diffusion models (MDMs). We derive an expanded evidence lower bound (ELBO) that introduces a planner to select which tokens to unmask at each step. Our analysis reveals that alternative unmasking strategies can enhance generation performance. Building on this, we propose Path Planning (P2), a sampling framework that uses a pre-trained BERT model or the denoiser itself to guide unmasking decisions. P2 generalizes all known MDM sampling strategies and significantly improves performance across diverse domains, including language generation (in-context learning, code generation, story infilling, mathematical reasoning, reverse curse correction) and biological sequence generation (protein and RNA sequences).", "sections": [{"title": "1. Introduction", "content": "Inspired by the success of diffusion models in continuous space and the desire for bidirectional reasoning, much work has sought to design performant training algorithms for discrete diffusion models. While there are many possible discrete noising processes, most successful discrete diffusion approaches have converged to absorbing state diffusion (Austin et al., 2021; Lou et al., 2023) with new, simplified training objectives resulting in scalable masked diffusion models (MDMs) (Sahoo et al., 2024; Shi et al., 2024; Gat et al., 2024).\nWhile most recent work has focused on improving MDM training, considerably less attention has been given to the impact of inference techniques on overall generative performance. This raises a question: Can we design new inference strategies to improve generative quality? In this paper, we answer affirmatively by investigating how the order in which tokens are unmasked during MDM inference affects generative quality. While the MDM reverse process requires that each token is uniformly likely to be unmasked at a given step, this correctly reconstructs the true data distribution only under a perfect denoiser (for further discussion of this perspective, see Appendix D.1). However, since any trained MDM is inherently imperfect and does not yield a tight ELBO, it has been empirically observed that a uniformly random unmasking order is suboptimal in many settings (Ou et al., 2024; Shih et al., 2022; Li et al., 2021).\nWe begin our study by reexamining the typical MDM ELBO and show that, for a fixed denoiser, we can expand the ELBO to include two additional terms, both involving a \u201cplanner\u201d whose role is to select which tokens should be unmasked at a given inference step as well as optionally choosing already unmasked tokens to be remasked. Our ELBO shows that while the optimal planner for the optimal denoiser is indeed uniform unmasking, the strategy prescribed by the reverse process, one can obtain better generative quality for an imperfect denoiser through the use of a well tuned, non-uniform planner. Of particular note is that the ELBO's planner terms are effectively a reweighting of the typical MLM objective with additional small differences due to an added dependence on the denoiser.\nThese observations lead to our proposed method, Path Planning (P2), which makes use of the expanded ELBO to introduce a family of planners for use at inference time. Crucially, by noting the similarity between the planner ELBO terms and the typical MLM objective we show that in practice we can obtain effective planners fully training-free by employing either pre-trained BERT-type models or simply using the already trained denoiser. Moreover, P2 is shown the generalize all known existing sampling strategies in the MDM literature (see Table 1). We validate our training-free planning framework across a diverse set of experimental settings, showing that by using P2 a 1B parameter MDM model can outperform a 7B Llama model in math reasoning while far outpacing state-of-the-art ARMs for code generation."}, {"title": "2. Background", "content": "Notation We will denote by $S = {1,...,N}$ a finite dictionary of tokens, by $\\hat{S} = S\\cup \\{M\\}$ the extension of this dictionary via the addition of some masked state M. For a metric space X, we define by P(X) the space of Borel probability measures on X. When X is finite we endow X with the discrete metric and let |X| denote the cardinality of X. With some abuse of notation we freely identify $\\mu \\in P(X)$ with a column vector in $[0, 1]^{|X|}$ corresponding to the associated probability mass function. We denote by $\\delta_a \\in P(X)$ the probability measure such that $\\delta_x(y) := 1$ if x = y and 0 otherwise and by Unif(X) $\\in P(X)$ the uniform probability measure on X. We suppose that we are interested in generating sequences of length L comprised of elements of S from some data distribution $P_{data} \\in P(S^L)$.\nWe use $x_i$ to denote the i'th coordinate of an elements x $\\in S^L$, and $x_{-i}$ to denote the element in $S^{L-1}$ which is the same as x but with the i'th token removed. For x $\\in S^L$ and y $\\in S$, we denote by $x_{-i,y}$ the element in $\\hat{S}^{L}$ which is the same as x but with the i'th token replaced by y. We denote by $M^L := (M, ..., M) \\in \\hat{S}^L$."}, {"title": "2.1. Masked Diffusion Models", "content": "In a masked diffusion model, one starts with a a collection of probability mass functions given by, for $y \\in \\hat{S}^L$ and t\u2208 [0,1]:\n$P_t(y; P_{data}) := \\alpha(t)p_{data}(y) + (1 - \\alpha(t))\\pi_{ML}(y)$   (1)\nfor a monotone-decreasing, continuously differentiable noise scheduler \u03b1 : [0,1] \u2192 [0,1] with \u03b1(0) = 1 and \u03b1(1) = 0, and finds continuous time Markov chain $X_t$ such that $P(X_t = x) = P_{1-t}(x; P_{data})$.\nA rate matrix generating $X_t$ is given for $x \\neq y \\in \\hat{S}^L$, by:\n$Q(y, x) = \\frac{\\dot{\\alpha}(1-t)}{1 - \\alpha(1-t)} \\sum_{i=1}^{L} \\delta_M(x_i) P_{data}(y_i | x_{/i} \\neq M) \\delta_{y_{/i}}(x_{/i})$,\nand $Q(x, x) = \\frac{\\dot{\\alpha}(1-t)}{1 - \\alpha(1-t)} \\sum_{i=1}^{L} \\delta_M(x_i)$ (see e.g. (Ou et al., 2024) Theorem 1). Here for $z \\in \\hat{S}^L$, $z \\neq M$ denotes the coordinates of z which are not equal to M, and for i \u2208 {1,..., L}, and j\u2208 S:\n$P_{data}(j|z_{\\neq M}) := P_{data}(\\{x : X_i = j\\}|z_{\\neq M})$.\nOne then attempts to approximate $X_t$ with mask with transition matrix given for x \u2260 y by:\n$Q_{mask}^{t}(y, x) := \\frac{\\dot{\\alpha}(1-t)}{1 - \\alpha(1-t)} \\sum_{i=1}^{L} \\delta_M(x_i) D_{y_i}^{i}(x) \\delta_{y_{/i}}(x_{/i})$.   (2)\nHere we are using the \"mean parametrization\" of the approximate backwards matrix. That is, we have a neural network parameterized by \u03b8 which gives a \u201cdenoising function\u201d $D^\\theta : \\hat{S}^L \\rightarrow P(S)^L$, with the hope that\n$D^\\theta_i(x) \\approx P_{data}(x_{/i} \\neq M) \\in P(S)$.  (3)\nIn particular, one enforces during inference that $D^\\theta_{y_i}^i(x) = \\delta_{x_i}(y_i)$ if $x_i \\neq M$.\nApproximate samples from the data distribution are then obtained via simulating the Markov chain $X^{0,mask}_{t}$ with $X^{0,mask}_{0} = M^L$ to time 1."}, {"title": "3. Path Planning", "content": "ation on same same-sized models. At the same time, for biological sequence design we show that the combination of P2 and DPLM (Wang et al., 2024a) leads to state-of-the-art generation quality for proteins, while for RNA design we outperform competitive models and observe that our sequences lead to higher structural plausibility than even true, naturally occurring sequences."}, {"title": "3.1. Mathematical Formulation", "content": "In order to formulate P2 we begin by modifying the jump matrix for the approximate backwards process (Eq. 2), introducing a new function $G^\\theta : S^L \\times \\hat{S}^L \\rightarrow [0, 1]$, which we refer to as the planner. $G^\\theta(y, x)$ approximates the likelihood that the j'th token in a partially denoised sequence x \u2208 $\\hat{S}^L$ should be (re)sampled given the conditional information about the rest of the sequence x and of the clean data y as predicted by $D^\\theta$. In Section 3.3, we discuss potential choices of planners and how previous works fall into this general framework.\nWe next define $F^\\theta : \\hat{S}^L \\times \\hat{S}^L \\rightarrow [0, 1]$ by\n$F^\\theta(y,x) := \\delta_M(x_j)E_{Y \\sim D^\\theta(x)} [G^\\theta(Y_{-j},Y_i,x)] + (1 - \\delta_M(x_j))E_{Y \\sim D^\\theta(x)} [G^\\theta(Y_{-j},x_j,x)]$ \nwhere here we use the shorthand $Y \\sim D^\\theta(x)$ to mean $Y_j \\sim D^\\theta_j(x)$.\nVia our interpretation of the role of $G^\\theta$, $F^\\theta(y, x)$ gives the likelihood that the j'th position of x should be (re)sampled given the information about the rest of the sequence x and the data's j'th token via averaging out the information provided about the rest of the data's tokens from $D^\\theta$.\nFinally, we define\n$D_{y_i}^i(x) = D_{y_i}^\\theta(x) \\delta_M(x_i) + \\frac{D_{y_i}^\\theta(x_{/i},M)}{1- D_{x_i}^\\theta(x_{/i},M)} (1 - \\delta_M(x_i))$.\nThat is, when $x_i$ is masked $D_{y_i}^i(x)$ approximates the probability that the i'th token of x should be unmasked to $y_i$ given the conditional information about the unmasked tokens in x, and when $x_i$ is not masked, $D_{y_i}^i(x)$ approximates the probability that i'th token of x should be resampled to a value other than $x_i$, given the conditional information about the unmasked tokens in x other than $x_i$.\nWe now seek to modify $Q^{mask}_{t}$ from Eq. 2 in a way so that $F^\\theta$ - by way of the planner $G^\\theta$ - plays the role of selecting which position should be unmasked/resampled and $D^\\theta$ plays the role of choosing what it should be (re)sampled to.\nFor x \u2260 y \u2208 $\\hat{S}^L$, we thus set:\n$Q^{t,+}(y, x) := \\frac{\\dot{\\alpha}(1-t)}{1 - \\alpha(1-t)} \\sum_{i=1}^{L} F^\\theta_i(y, x)D_{y_i}^i(x) \\delta_{y_{/i}}(x_{/i})$.  (4)\nFor reference, we provide a computationally viable Gillespie sampling method (Gillespie, 1977; 1976) which approximates samples from $X^\\theta$ with jump matrix $Q^\\theta$ and provides intuition for the role of the Planner is given by Algorithm 4 in Appendix D.3.\nObserving Algorithm 4, we see that P2 allows for the planner $G^\\theta$ to guide the denoising process towards a more optimal path of denoising orders using the information from both the partially noised sequence $x_t$ and the predicted clean sequence y from the denoiser, and further introduces the ability to resample previously masked tokens using information from both the partially generated sequence and the output of the denoiser.\nThe interpretation of the Planner as a mechanism for guiding the denoising process toward an optimal path is furthered by the following:\nProposition 1. Define $P^\\theta \\in P(S^L)$ by $P^\\theta_f(x) = P(X^\\theta_t = x)$, where $X^\\theta$ is the CTMC with rate matrix given in Equation Eq. 4. Then we have an \u201cEvidence Based Lower Bound\u201d $E(x) \\leq log(P^\\theta_f(x^0))$ for each fixed $x^0 \\in S^L$ given by $E(x) = EMP(x^0) + EUP(x^0) + ED(x^0)$, where:\n$EMP(x^0) = - \\int_0^1 \\frac{\\alpha(t)}{1-\\alpha(t)} E_{x_t \\sim P_t(;\\delta_{x^0})} \\times E_{Y \\sim D^\\theta(x_t)} [log(G^\\theta(Y_{-i}, Y_i, x_t))] \\delta_M ([x_t]_i) dt$\n$EUP(x^0) = - \\int_0^1 \\frac{\\alpha(t)}{1-\\alpha(t)} E_{x_t \\sim P_t(;\\delta_{x^0})} \\times E_{Y \\sim D^\\theta(x_t)} [log(1 - G^\\theta(Y_{-i}, Y_i, x_t))] (1 - \\delta_M ([x_t]_i))dt$\n$ED(x^0) = \\int_0^1 \\frac{\\alpha(t)}{1-\\alpha(t)} E_{x_t \\sim P_t(;\\delta_{x^0})} \\times log(D^\\theta_{Y_i}^i (x_t)) dt$.\nHere $P_t$ is defined per Eq. 1.\nThis ELBO offers a simple interpretation, recalling we seek to maximize the expected value of each term with respect to $X^0 \\sim P_{data}$. EMP(x) optimizes the role of the Planner as it pertains to masked tokens in a partially denoised sequence. That is, as a mechanism for selecting the a viable masked position to insert a \u201cclean\u201d token as suggested by $D^\\theta$. If $D^\\theta$ suggests to unmask the coordinate i to a value which is representative of the data distribution, then $G^\\theta$ should be large so that the i'th position is selected. Eup(x) optimizes the role of the Planner as it pertains to unmasked tokens in a partially denoised sequence. That is, as a mechanism for selecting the an unmasked token to resample via remasking and inserting back into $D^\\theta$. If the i'th token already contains"}, {"title": "3.2. A Family of Planners: The P2 Sampling Strategy", "content": "Here we introduce the P2 sampling strategy, which allows for controllability over the role of the planner, exploitation of the information provided about all tokens in the sequence from $G^\\theta$ and $D^\\theta$, and guaranteed convergence of the sampling procedure to a fully unmasked sequence.\nWe decompose the planner into two components:\n$G(y, x) = \\delta_M(x_j)G^{y,M}_j(y, x) + (1 - \\delta_M(x_j)) (1 - G^{U,y}_j(y, x))$.\nThat is, the \u201cmasked token planner\u201d $G^{y,M}_j(y, x)$ predicts the liklihood that a masked token at the j'th position should be unmasked, and the \u201cunmasked token planner\u201d $G^{U,y}_j(y, x)$ predicts the likelihood that an unmasked token at the j'th position should be kept.\nWe then employ a modified \u201ctop k\u201d sampling strategy, which introduces the possibility of changing multiple tokens per iteration and better exploits the information provided by the scheduler. We define \u03ba : {1,...,L} \u2192 {1,..., L} to be any monotone non-decreasing function with \u03ba(L) = L, which will serve as an \u201cunmasking scheduler\u201d for how many tokens should be denoised at a given time step. In particular, at the t'th iteration, \u03ba(t) tokens are guaranteed to be unmasked in the partially generated sequence.\nWe further introduce a stochasticity strength parameter \u03b7, and define the family of probability measures:\n$\\hat{G}(x, y) \\propto \\eta \\delta_M(x_j)G^{1}_j(y, x) + (1 - \\delta_M(x_j))G^{2}_j(y, x)$  (5)\nfor \u03b7 \u2265 0. Note that while the Planner $G^\\theta$ determines if the j'th token is a valid candidate to change (a masked token to"}, {"title": "3.3. Plug-and-Play Path Planning Sampler", "content": "Jy\n3.3.1. SELF-PLANNING WITH DENOISER-PREDICTED\nPROBABILITIES\nWe propose a self-planning mechanism by leveraging denoiser-predicted probabilities to guide unmasking and remasking decisions. Within the P2 framework, the un-mask planner and mask planner are unified by setting $G^{y,U}_j(y,x) = G^{y,M}_j(y, x) = D_{j,y_j}(x)$, that is, the denoiser itself serves as the planner. For mask positions, the denoiser is trained to predict tokens given the surrounding context, and the predicted probabilities serve as confidence estimates for the correctness of token predictions. This methodology aligns with established practices in the literature (Gong et al., 2024; Chang et al., 2022; Zheng et al., 2023; Wang et al., 2024a;b). However, a concern arises for unmasked positions, as these tokens act as context during training and are not directly supervised. This raises the question: Are the predicted probabilities for unmask positions meaningful? Our empirical evaluation demonstrates that, despite"}, {"title": "3.3.2. BERT-PLANNING", "content": "In BERT-planning, we introduce a class of special planner BERT (Devlin et al., 2019), a bidirectional language model trained to predict the correct tokens given the corrupted sequences (15% of tokens masked and 1.5% of tokens uniformly flipped to other tokens). Despite such a simple training objective, BERT learns to estimates the naturalness of a token with the predicted probabilities which demonstrates wide application in zero-shot mutation prediction (Hie et al., 2022). Compared to training a dedicated planner that is equal-size to denoiser as in DDPD (Liu et al., 2024), BERT is more versatile, flexible in sizes and often available in common tasks such as text (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2019), protein (Lin et al., 2023; Hayes et al., 2025; Wang et al., 2024a;b) and RNA (Peni\u0107 et al., 2024).\nLet $B^\\theta : S^L \\rightarrow P(S)^L$ be a pretrained BERT model, so that $B^\\theta_{Y_j}(y)$ is assigning the probability that the jth token in the sequence y is clean. In BERT planning we set unmask planner to be the BERT $G^{y,U}_j(y,x) = B^\\theta_{y_j}(y)$ and mask planner to be the denoiser $G^{y,M}_j(y, x) = D^i_{j,y_i}(x)$."}, {"title": "3.4. P2 Generalizes Existing Sampling Methods", "content": "In Table 1, we show the existing sampling methods fit into our P2 framework with specific parameters. Ancestral sampling disables the remasking by setting the Unmasked Planner ($G^{y,U}_j(y, x)$) to always output 1, i.e., the likelihood that an unmask token should be kept is always 1, and the mask planner $G^{y,M}_j(y, x)$ functions as a uniform sampler as it randomly selects mask positions. Greedy ancestral sampling improves open this by using the denoiser $D^i_{j,y_j}(x)$ as the mask planner $G^{y,M}_j(y, x)$. DFM sampling randomly selects positions, and enables remasking by introducing a tunable stochasticity strength \u03b7. RDM functions identically to our self-planning by using the denoiser for both mask and unmask planning but it omits the stochasticity control with the default stochas-"}, {"title": "4. Experiments", "content": "Jy"}, {"title": "4.1. Protein Sequence Generation", "content": "Setup and Evaluation. We benchmark our method against state-of-the-art protein sequence generation models, including discrete diffusion models (DPLM (Wang et al., 2024a), EvoDiff (Alamdari et al., 2024), and ESM3 (Hayes et al., 2025)), an autoregressive model (ProGen2 (Nijkamp et al., 2022)), and masked language models (ESM2 (Lin et al., 2023)). Each model generates 100 sequences across lengths in [200, 300,..., 800], following their respective sampling strategies, with modifications ensuring fair evaluation. Protein sequence quality is assessed using ESMFold (Lin et al., 2023), measuring foldability through pLDDT, pTM, and PAE scores. We define foldability as the percentage of sequences satisfying pLDDT > 80, pTM > 0.7, and pAE < 10. Additionally, we analyze token entropy and sequence diversity to detect mode collapse. Further details on experimental settings and evaluation metrics are provided in the Appendix F.2.\nResults. As summarized in Table 2, our P2 algorithm applied to DPLM (150M and 650M) consistently improves all folding metrics\u2014pLDDT, pTM, and pAE-outperforming the default RDM sampling strategy (Zheng et al., 2023)."}, {"title": "4.2. The Design Space of Path Planning", "content": "Our Path Planning (P2) framework generalizes existing sampling strategies, including vanilla ancestral sampling, greedy ancestral sampling, RDM sampling, and DFM sampling, by incorporating specific parameterizations. In Figure 3, we instantiate these sampling algorithms and evaluate their performance on protein sequence generation, focusing on foldability (additional metric results are provided in Figure S2).\nVanilla and greedy ancestral sampling employ a stochasticity strength of 0, effectively disabling remasking, which results in poor performance. DFM sampling introduces tunable stochasticity, leading to improved performance over ancestral sampling; however, it lacks trajectory planning, which limits its effectiveness. RDM sampling, by contrast, enables remasking with a default stochasticity strength of 1 and utilizes the denoiser's confidence for self-planning, yielding better sampling quality.\nP2 combines the advantages of these existing algorithms, offering both controllable stochasticity strength and planning guidance. By tuning stochasticity strength, P2 can"}, {"title": "4.3. Ablation of Path Planning", "content": "In this section, we utilize the protein sequence generation task as an ablation benchmark to analyze the implications of our Path Planning (P2) design choices. We experiment with the ESM2 (Lin et al., 2023) family of protein language models, including versions with 8M, 35M, 150M, 650M, and 3B parameters, for variants incorporating a BERT planner. For the denoiser, we train a 150M MDM from scratch, using the same architecture as ESM2-150M and DPLM-150M, for 500k steps with approximately 320k tokens per step. Training details are provided in Appendix G.0.1.\nResults. Table 3 demonstrates that our P2 approach consistently outperforms existing sampling strategies across all folding metrics, while maintaining strong token entropy and sequence diversity. Notably, results are further enhanced when an external BERT planner is utilized. To provide a comparative perspective, we perform an apple-to-orange evaluation against a planner-based sampling algorithm, DDPD, equipped with the same BERT planner. DDPD is prone to generating low-entropy, repetitive sequences with poor foldability, as it relies exclusively on"}, {"title": "4.4. Sampling Efficiency", "content": "Increasing the number of sampling steps generally enhances generative quality, albeit with increased computational time. To evaluate the scaling efficiency, we benchmark three sampling algorithms\u2014ancestral sampling, P2 (self-planning), and P2 augmented with an 8M BERT planner\u2014on the task of protein sequence generation. We measure the foldability across increasing sampling steps in terms of elapsed time (benchmarked on NVIDIA A100 GPUs). In Figure 5 top, P2 achieves superior foldability compared to ancestral sampling, while the inclusion of the external BERT planner demonstrates exceptional scalability, particularly at higher sampling steps. In Figure 5 bottom, we further analyze inference efficiency by examining elapsed time and speed (tokens per second) as a function of sequence length. P2 with self-planning maintains the same inference cost as ancestral sampling, as it does not rely on an external model. Conversely, P2 with the BERT planner doubles the number of sampling steps due to one additional BERT evaluation. However, since the planner is a lightweight 8M model compared to the 150M MDM, the overhead is negligible. This is evident in the figure, where the performance gap between P2 (self-planning) and P2 with the 8M BERT planner becomes indistinguishable at higher sampling scales."}, {"title": "4.5. Language Generation", "content": "It has been widely pointed out that the existing evaluation such as toy datasets and NLL in text generation can be easily gamed to achieve low perplexity (Zheng et al., 2024a). In our evaluation, we follow the language benchmarking from SMDM (Gong et al., 2024) and DiffuLLama (Nie et al., 2024), and investigate the capabilities of MDMs in real-world evaluation language generation tasks that have been largely overlooked in prior works (Austin et al., 2021; Lou et al., 2023; Sahoo et al., 2024; Shi et al., 2024). We additionally provide the experiments of breaking the reverse curse in the Appendix H.1.1.\nBenchmarks. We consider TriviaQA (Joshi et al., 2017) to test the reading comprehension of models and the last word completion task Lambada (Paperno et al., 2016) to test how models capture long-range dependencies in text. These two tasks are measured by exact match accuracy, i.e., given a prompt, we use MDMs to generate responses and calculate matching accuracy against the ground truth. Additionally, we employ complex tasks such as GSM8K (Cobbe et al., 2021), grade school math problem, to assess the math reasoning and story-infilling task using ROCStories (Mostafazadeh et al., 2016) and evaluate using ROUGE score (Lin, 2004). To test the code infilling, we also adopted Humaneval (Bavarian et al., 2022) single line infilling task, which is evaluated by pass@1 rate. We employ Language Model Evaluation Harness framework (Biderman et al., 2024) for performance assessment.\nBaselines. We adopt the baselines and their results from previous works (Nie et al., 2024; Gong et al., 2024), including continuous diffusion model Plaid1B (1.3B) (Gulrajani & Hashimoto, 2023), discrete diffusion model SEDD-S (170M), SEDD-M (424M) (Lou et al., 2023), MDM (1B) (Gong et al., 2024), DiffuLLama(7B) (Nie et al., 2024), DiffuGPT-S (127M), DiffuGPT-M (355M) (Nie et al.,"}, {"title": "4.6. RNA Sequence Generation", "content": "Experimental Setup. We train a 150M Masked Diffusion Model (MDM) trained on 27M RNA sequences from RNA-Central (Petrov, 2021) over 100K steps with a batch size of 320K tokens.\nWe adopted the protein sequence evaluation protocols, using an external folding model (Shen et al., 2024) to estimate structural quality via pLDDT. We additionally calculate the Minimum Free Energy (MFE), GC Content (%), and sequence entropy. We generate 100 RNA sequences of 100 base pairs (bp) each. Visualizations are described in Appendix H.2.6.\nBaselines. Two RNA language models, RiNALMo-150M and RiNALM0-650M (Peni\u0107 et al., 2024), served as primary language model baselines. Additionally, a reference set of 100 native 100-bp RNA sequences was included for comparative purposes. We apply the existing sampling strategies"}, {"title": "5. Conclusion", "content": "We demonstrated that unmasking order significantly impacts the generative performance of masked diffusion models (MDMs). By expanding the ELBO formulation, we introduced a planner that optimizes token selection during inference. We proposed Path Planning (P2), a sampling framework that generalizes all existing MDM sampling strategies. P2 delivers state-of-the-art improvements across diverse tasks, including language generation and biological sequence design, enabling MDMs to outperform larger autoregressive models. Our findings highlight the importance of inference strategies in discrete diffusion models, paving the way for more efficient and effective sequence generation."}, {"title": "A. Reproducibility Statement", "content": "We provide the PyTorch implementation in Appendix Section E. For the experiments, we integrate our approach into the SMDM (Gong et al., 2024) GitHub codebase\u00b2 to obtain the results for \"MDM (1.1B) + P2\" reported in Table 6. Similarly, the results for \"DiffuLLaMA (7B) + P2\" in Table 6 are derived using the DiffuLLaMA (Nie et al., 2024) GitHub codebase\u00b3. For the protein sequence generation experiments, we utilize the DPLM (Wang et al., 2024a) open-source codebase. The RNA sequence generation results are obtained by adapting the DPLM codebase for MDM training, combined with the RINALMO (Peni\u0107 et al., 2024) language model architecture."}, {"title": "B. Related Works", "content": "Masked Diffusion Models (MDMs) represent a promising alternative to autoregressive models for discrete data generation, particularly in language modeling. Recent advancements have focused on simplifying and generalizing the MDM framework to improve performance and training efficiency (Shi et al., 2024; Sahoo et al., 2024). These studies introduced a continuous-time variational objective for MDMs, expressed as a weighted integral of cross-entropy losses, facilitating the training of models with state-dependent masking schedules. At the GPT-2 scale, these MDMs outperformed prior diffusion-based language models and demonstrated superior capabilities in zero-shot language modeling tasks (Nie et al., 2024; Gong et al., 2024).\nMDMs generate sequences starting from a fully masked input and progressively unmasking positions until a clean sequence is reached. Once a token is unmasked, it will stay unchanged. However, there is not guarantee that the state is correct, considering the approximation errors arise from the imperfect fit to real-world data distributions. Additionally, time discretization (Zhao et al., 2024) and numerical errors (Zheng et al., 2024b) may further the error incurred during sampling processes.\nTo address these challenges, several solutions have been proposed. These include methods allowing models to revise prior predictions and guiding sampling trajectories using internal or external knowledge. Examples include informed correctors (Zhao et al., 2024), greedy ancestral methods (Gong et al., 2024), and RDM sampling techniques (Zheng et al., 2023; Wang et al., 2024a), which leverage model scores to replace random masking with targeted corrections. None of these works, however, allow for the use of an external planner, and (Zheng et al., 2023; Wang et al., 2024a) are simply using a top-k sampling strategy without any concern for the theoretical underpinnings of the sampling strategies viability.\nIn terms of theoretically-backed methods for selecting the denoising order during a generative model's sampling process, the current literature is quite sparse. (Shih et al., 2022; Li et al., 2021) discuss this task from the perspective of Any-Order Autoregressive models, with (Li et al., 2021) requiring a specially-trained external planner model using a specially designed architecture and Shih et al. (2022) taking the perspective that a fixed family of possible generation orders should be chosen a priori to eliminate redundancy.\nThe most closely related work to ours is likely the recent DDPD (Liu et al., 2024) introduced a generative process divided into a planner, which identifies corrupted positions, and a denoiser, which refines these positions. Though they discuss the ability to employ a MDM denoiser within their framework, their analysis and sampling is through the lens of uniform discrete diffusion models. In particular, as with (Li et al., 2021), the success of their strategy is contingent upon training a large specialized planner model of comparable size to the denoiser itself. Moreover, in their framework, since they are based on uniform diffusion models, the partially de-noised sequence never contains any masked states, and there is"}]}