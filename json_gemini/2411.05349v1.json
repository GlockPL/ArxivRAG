{"title": "ENHANCING CLUSTER RESILIENCE: LLM-AGENT BASED AUTONOMOUS\nINTELLIGENT CLUSTER DIAGNOSIS SYSTEM AND EVALUATION FRAMEWORK", "authors": ["Honghao Shi", "Longkai Cheng", "Wenli Wu", "Yuhang Wang", "Xuan Liu", "Shaokai Nie", "Weixv Wang", "Xuebin Min", "Chunlei Men", "Yonghua Lin"], "abstract": "Recent advancements in Large Language Models (LLMs) and related technologies such as Retrieval-Augmented\nGeneration (RAG) and Diagram of Thought (DoT) have enabled the creation of autonomous intelligent systems\ncapable of performing cluster diagnostics and troubleshooting. By integrating these technologies with self-play\nmethodologies, we have developed an LLM-agent system designed to autonomously diagnose and resolve issues\nwithin AI clusters. Our innovations include a knowledge base tailored for cluster diagnostics, enhanced LLM\nalgorithms, practical deployment strategies for agents, and a benchmark specifically designed for evaluating LLM\ncapabilities in this domain. Through extensive experimentation across multiple dimensions, we have demonstrated\nthe superiority of our system in addressing the challenges faced in cluster diagnostics, particularly in detecting\nand rectifying performance issues more efficiently and accurately than traditional methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in Large Language Models (LLMs)\nand complementary technologies such as Retrieval-\nAugmented Generation (RAG) and Diagram of Thought\n(DoT) have paved the way for the development of\nautonomous intelligent systems capable of performing\ncluster diagnostics and troubleshooting. By integrating\nthese technologies with self-play methodologies, we have\ncreated an LLM-agent system designed to autonomously\ndiagnose and resolve issues within AI clusters. Our\ninnovative approach includes the establishment of a\nspecialized knowledge base for cluster diagnostics, the\nenhancement of LLM algorithms to better suit the demands\nof the domain, practical deployment strategies for agents\nwithin real-world environments, and the development\nof a benchmark specifically tailored to evaluate LLM\ncapabilities in the context of cluster diagnostics. These\ncomponents collectively contribute to a robust framework\nthat addresses the complexities inherent in managing AI\nclusters, particularly in scenarios involving performance\ndegradation or other operational anomalies.\nThrough rigorous experimentation, we have validated\nthe effectiveness of our LLM-agent system across\nmultiple dimensions. Our benchmark, which consists\nof 150 manually crafted advanced questions, serves\nas a comprehensive evaluation tool that highlights the\nperformance differences between our enhanced LLM-agent\nand baseline open-source models. In practical applications,\nthe LLM-agent demonstrates its superior capability to\nidentify and resolve performance issues more efficiently\nthan traditional methods, reducing the troubleshooting\ntime significantly. For instance, in a simulated scenario\nwhere one GPU was throttled to a much lower frequency,\nour system identified and resolved the issue within a\nmatter of minutes, whereas conventional approaches would\nhave taken a senior operations engineer nearly an hour to\ndiagnose and rectify using pre-written automated detection\nsoftware.\nMoreover, the LLM-agent's ability to detect and initiate\ncorrective actions even before the performance degrada-\ntion is noticed by human operators marks a significant\nadvancement in proactive system maintenance. This\ncapability not only mitigates immediate issues but also\nenhances the overall availability and reliability of the\ncluster by preemptively addressing potential faults. By\nleveraging the strengths of RAG and DoT, the LLM-agent\ncan autonomously execute remediation measures, thereby\nfreeing up engineering resources to focus on more complex\nand value-driven tasks. Our research underscores the\ntransformative potential of combining AI-driven diagnostics\nwith practical deployment strategies, setting the stage for a\nnew era of intelligent cluster management solutions."}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 LLM's Alignment and Enhancement", "content": "In recent years, generative artificial intelligence centered\naround large language models(LLMs) has seen rapid\ndevelopment, with powerful natural language generating\ncapabilities demonstrated by proprietary models such\nas the GPT series and Gemini\nseries, as well as open-source models\nlike Llama and Qwen.\nThere are multiple approaches to enhancing the capabil-\nities of LLMs across different stages such as training,\ninference, and deployment, as well as in areas like data,\nalgorithms, and computational resources. In light of\nthe achievements of autoregressive models like GPT-\n2(decoder-only transformers) and\nLLaMA(transformer++) , enhancing\nthe quality of the data has become a critical method for\nimproving the efficacy of models during the pre-training\nprocess.\nFor modern LLMs, there exists several training or fine-\ntuning works between pre-training and the deployment.\nChatGPT describes this process as Su-\npervised Fine-Tuning (SFT), Reward Modeling (RM), and\nReinforcement Learning with Human Feedback (RLHF),\nwhile LLaMA3.1 integrates these\ninto a continuous process known as \"Continue Training.\"\nBesides training, LLMs can leverage Retrieval-Augmented\nGeneration (RAG) to utilize knowledge\nfrom data distributions that were not part of the training\nset. We can refer to the above content as the alignment and\nenhancement of LLMs."}, {"title": "2.2 AI-agent based Applications", "content": "After the model parameters have been frozen, it is possible\nto enhance the inherent capabilities of the model through\nmechanisms such as chain-of-thought(CoT) reasoning, scaling test time, and com-\nbining CoT LLM and AI agents as\nLLM-agent.\nCoT is a prompting technique used to guide LLMs to\ngenerate intermediate reasoning steps before arriving at\na final conclusion. There are extensions to classic CoT,\nsuch as Tree of Thought (ToT) for tree-\nlike backtracking, Graph of Thought (GoT) for graph-based reasoning, and Diagram of Thought\n(DoT) for a propose-critique-summarize\napproach based on topos theory.\nThe development of CoT and the scaling of test-time are\nunified, with CoT applications always aiming to maintain\noptimal results with limited test-time or scaling test-time to"}, {"title": "2.3 Diagnosis and Repair for AI Clusters", "content": "Constructing and utilizing LLM applications typically re-\nquire hardware infrastructure on a scale costing millions\nof or more dollars. Meta constructed the LLM application\ncore LLAMA 3.1 within 54 days, leveraging a cluster that\nincluded 16,000 GPUs, with just the\nGPU costs amounting to over billion dollars. However,\nsuch complex and expensive systems face significant chal-\nlenges in terms of reliability and availability. During the\n54-day training, the Meta cluster experienced 419 unex-\npected interruptions, averaging one disruption every three\nhours. At such a frequency of interruptions, the cluster, from\nthe operating system to the AI framework and distributed\nscheduling software, requires the ability to capture, identify,\nattribute, and repair exceptions to ensure successful and ef-\nficient model training. Microsoft's Superbench has\nsystematically built a suite of standard test cases\nto comprehensively assess the availability of clusters.\nIn terms of capture and repair, the Torch Elastic solution aims to enable automatic restarts\nof model training, while works such as FlashCheckpoint-\ning in DLRover focus on reducing the\ncost of checkpoint saving and loading during the automatic\nrestart process. Building upon automatic restart capabili-\nties, many works at the AI framework level have conducted\nresearch and practical implementations to enhance reliabil-\nity and availability, particularly those featuring highly cus-\ntomized solutions based on Megatron.\nByteDance's Megascale and Alibaba's\nPai-Megatron both provide toolkits for\ncluster diagnostics, which are used to check the health of\nservers and networks, as well as to perform manual or auto-\nmated error identification and repair.\nWith the advancement of AI technologies, researchers are\nbeginning to explore the use of AI techniques to address\ncluster diagnostic issues. Using big data techniques to ana-\nlyze log files was an typical approach to automating cluster\ndiagnostics. However, such meth-\nods primarily involve static or real-time analysis of files\nproduced by the training process, which limits their attribu-\ntion capabilities and means they lack intelligent autonomy,\nrelying instead on pre-written execution and planning pro-\ncedure."}, {"title": "3 SPECIAL TERMINOLOGIES", "content": "Al computing tasks: refers to programs or processes\ndesigned to achieve intelligence, such as training large\nlanguage models, inference with large language models,\nworld model inference, and LLM-agent inference.\nAI chips: processors suitable for or dedicated to performing\nAI computing tasks, such as NVIDIA GPUs, Intel Gaudi A\u0399\naccelerators, and Google TPUs.\nAI servers: computers equipped with AI chips that\nare suitable for or specifically designed to perform AI\ncomputing tasks, such as the NVIDIA DGX H100. AI\nservers often have requirements beyond those of classic\nservers in terms of stability, availability, cooling, and power\nconsumption.\nAI cluster: a distributed server cluster composed of two\nor more AI servers set up to accomplish a single target\ntask, such as Meta's cluster containing 16 thousand GPUs.\nAdditionally, AI servers typically require RDMA or higher\nbandwidth interconnect protocals, such as InfiniBand\nRDMA and RDMA over Converged\nEthernet and do not usually adopt\nclassic Ethernet protocols.\nCluster diagnosis: ensuring that AI computing tasks\ncan run with normal performance on the AI cluster,\npromptly detecting task failures, identifying the points\nof failure, clarifying the reasons for failure, repairing the\ncorresponding faults, and ensuring the overall availability\nof the AI cluster."}, {"title": "4 METHODS", "content": ""}, {"title": "4.1 Overview", "content": "We incorporate advanced techniques from the field of LLM\nalignment and enhancement to creatively develop a solution\nfor building a cluster intelligent maintenance system based\non LLM-agents. Figure 1 illustrates the overall process of\nthis solution.\nThe upper part of the figure represents the core component\nof solution: the LLM-agent. The LLM-agent consists of an\nagent program and an LLM. The LLM interprets the input\ninformation provided by the agent as external stimuli and\ntask instructions, and responds appropriately. The agent\nthen directly writes code or calls specific software interfaces\nbased on the feedback from the LLM, thereby operating the\ncluster. For LLM itself, there are two main challenges. First,\nhow does the LLM acquire domain-specific knowledge\nof cluster diagnostics, and furthermore, where does this\nknowledge come from. Second, how can the LLM reason\nand plan? For the entire LLM-agent, ensuring that the\nLLM's inputs and outputs match with the actual operations\nperformed by the agent controlling the cluster is another\ncrucial aspect that needs to be addressed.\nIn order to solve the above problems, we have introduced\nthree innovations. First, we use 250 cluster failure\nrecords collected from GitHub as a starting point, and\ntreat the cluster operation failure logs actually managed\nby the LLM-agent as a continuous source of data. We\nutilize RAG to enable the LLM to\ncapture detailed knowledge corresponding to specific\nterms within the context. Figure 1 describes the \"alert\",\n\"compute cluster\", and \"storage sections\", along with their\ncommunication with the LLM-agent, which outlines this\nprocess. Second, we use DoT enables\nthe model to effectively handle non-natural language\ninformation such as symbols, formulas, and code. Similar\nto vision-text multimodal models, we effectively leverage\ntextual elements that go beyond the inherent meaning of\nnatural language based on DoT. The \"planning algorithm\"\nsection at the top of Figure 1 illustrates this innovation.\nThird, we use self-play technology to\nenable the LLM to autonomously, also intelligently, devides\nlong tasks or challenging reasoning objectives into multiple\nsteps, self-assess the output of each step, and ultimately\nachieve the goal.\nThe lower part of Figure 1 forms the basis of our work.\nIt includes a mature operations alarm troubleshooting\nand repair process, as well as several mature or advanced\nsoftware tools. Based on related works, we have developed\na unified, multi-level, multi-dimensional cluster diagnostic\ntoolkit as Figure 2.\nThis tool diagnoses the health status of the cluster from"}, {"title": "4.2 Cluster Diagnosis Domain-specific Knowledge Base", "content": "Our knowledge base consists of two sources. One part is\nlogs, monitoring information, or program output content,\ncome from pre-collected, cleaned, and organized GitHub\ndata, carefully selected to address pain points in the cluster\ndiagnostics and troubleshooting domain, incorporating\nknowledge from issues in the GitHub community, also come\nfrom operational data acquired after the initial deployment\nand operation of the LLM-agent. We call it Diagnosis\nDataset. The second part is composed of symbolic"}, {"title": "4.2.1 Diagnosis Dataset", "content": "We drew on effective practices from Alibaba's experience\nin managing cluster startup operations to\nbuild a database. We cleaned, organized, and structured the\nunstructured data obtained from GitHub, ultimately forming\nan effective dataset. We collected over a thousand questions\nand feedback items from the GitHub issue section. Through\nautomated processes and manual review, we filtered out\nover 200 entries with substantive knowledge content and\nwell-structured Q&A formats. Each piece of organized data\ncontains four fields: problemkey, rawtext, function, and\nresult.\nThe problemkey is a domain keyword identified either\nmanually or based on openai 01. Rawtext refers to the\noriginal content of a website after simple formatting, stored\nas a long string containing the questions asked on the\nweb page and the developers' responses. The function is\nbased on our cluster diagnosis toolkits and is manually\ncorrelated by cluster troubleshooting personnel. This part\nis used as annotation in the portion of the dataset that the\nmodel can perceive, it is not perceived by the model for\nthe answers used in the benchmark evaluation part, and\nit serves as the starting point for knowledge acquisition\nafter the LLM-agent is deployed. The final results are the\ncauses of the faults extracted from the rawtext based on\nthe developers' answers. For an LLM capable of driving\nan agent to perform cluster diagnostics, we expect it to be\nable to determine the causes of faults based on real-time\noperational information from the cluster and to call existing\ntools or write tool code on-the-fly for cluster repairs,\nwithout relying on rawtext containing developer replies. We\nwill demonstrate this capability in subsequent experiments."}, {"title": "4.2.2 Performance Modeling", "content": "We use a series of progressive methods to model the correct\nperformance of given AI computation tasks, and through\nthe DoT, we convert this special modal data into tokens to\nfeed into the model. In addition to cluster health check, we\nhave included modules in the toolkits to determine whether\ndifferent AI computing tasks exhibit correct performance.\nThese modules can, on one hand, be invoked by the agent\nto provide results to the LLM for analysis, and on the other\nhand, they can be called by the LLM to have the agent check\nthe cluster status.\nWe start modeling with the simplest task types. Considering"}, {"title": "4.3 Create LLM-agent with RAG-DoT-Selfplay\ntechniques", "content": ""}, {"title": "4.3.1 Using RAG to Build an LLM That Can Utilize\nExternal Knowledge", "content": "RAG integrates two core components: retrieval and genera-\ntion. The retrieval module is responsible for finding context-\nrelevant information from an external knowledge base, a\nprocess that typically involves indexing large volumes of\ndocuments to quickly locate the most pertinent segments.\nThe retrieved information is then passed to the generation\nmodule as additional input. The generation module builds\nupon a pre-trained language model, leveraging the retrieved\ncontext to enhance its generation capabilities, thereby pro-\nducing responses that are more accurate and better aligned\nwith real-world situations.\nConsidering other similar technologies, SFT requires sub-\nstantial computing resources and may diminish the model's\ninherent generalization capabilities. In-context learning\nconsumes context length and inference time, making it\nunsuitable for importing datasets with millions of entries.\nRAG can acquire relevant knowledge during inference with\nminimal resources and inference time, without altering the\nweights of the model itself."}, {"title": "4.3.2 Using DoT to Build an Agent That Can Reason and\nPlan", "content": "DoT(Diagram of Thoughts) models iter-\native reasoning in LLMs as constructing a Directed Acyclic\nGraph (DAG) within a single model. The DAG consists of\nnodes representing propositions, critiques, refinements, and\nverifications, with edges indicating the logical relationships\nor dependencies between them. We use XML to handle mul-\ntimodal special symbol data and perform reasoning based\non DoT.\nBased on the principles of DoT, we use XML tags to sep-"}, {"title": "4.3.3 Using Selfplay Techniques to Construct a\nDomain-specific MultiModal Agent", "content": "With the help of RAG and DoT, the LLM can utilize\ninformation from outside the training set as well as abstract\nsymbolic reasoning information. However, this still has\nlimitations for an agent designed for intelligent cluster\ndiagnostics. We permit the LLM to generate content over\na longer duration. The quality of solutions to challenging\nproblems can be enhanced through multiple rounds of\nplanned selfplay or spontaneous self-questioning and\nanswering by the agent.\nSpontaneous self-questioning and answering is applied\nin DoT reasoning. On the planned selfplay process, we\ntransform the complex problem of cluster fault attribution\ninto a three-round process. In the first round, the agent,\nbased on error logs passed from the cluster, prompts\nthe LLM to identify potential keywords from the error\nitems and corresponding solutions from the knowledge\nbase, performing information extraction and RAG. In\nthe second round, the LLM evaluates its own answers, making\ncorrections or accepting them directly, then proceeds to\nwrite or call appropriate tools for the Agent to execute. In\nthe final round, the LLM makes an accurate attribution\njudgment based on the results of the agent's interaction\nwith the actual cluster. Compared to existing selfplay work\nfocused on the text side, we integrate it with the agent,\ngranting it the permissions to operate machines and interact\nwith the environment, fully simulating the capabilities of a\nhuman engineer to solve problems."}, {"title": "5 EXPERIMENTS", "content": "We conducted a three-phase experiment to demonstrate\nthe advanced nature of the proposed LLM-agent in the\nfield of cluster intelligent diagnostics. The first phase\ninvolves creating a dataset and benchmark for the field\nof cluster intelligent diagnostics. First, we define the\nstatistical characteristics of the external data knowledge\nbase and introduce the process of generating an evaluation\nbenchmark from this knowledge base. Next, we describe the\nfeatures of this benchmark and explain its advanced nature\nin the field of cluster intelligent diagnostics. Throughout\nthis process, we emphasize fairness and impartiality, strictly\ndistinguishing between the parts of the model that can be\nperceived and the scoring portions of the evaluation. We\nfurther elaborate on the benchmark using the results of the\nmainstream open-source model LLaMA3.1-70B.\nThe second phase involves evaluating the innovative\naspects of the three models we proposed\u2014RAG, DoT,\nand selfplay using the aforementioned benchmark for\ncomparative assessment. The experiments in the second\nphase are aimed at demonstrating the advanced nature\nof our proposed models in the field of cluster intelligent\ndiagnostics.\nIn the third phase, we expose the LLM-agent to both the\ntraining and testing sets in the benchmark, allowing it to\noperate in its most complete form to address real-world\nproblems encountered in production environments. We\ndemonstrate the accuracy, efficiency, and autonomous\nintelligence of this solution through two typical cases.\nSpecifically, we found that this solution can provide early\nwarnings for AI clusters, further enhancing the availability\nof the clusters.\nFinally, we will conduct a qualitative analysis and discus-\nsion on the topics of correctness, safety, and reliability,\nwhich are at the forefront of the LLM and LLM-agent fields\nand have yet to be conclusively resolved, to demonstrate\nthe series of work we have undertaken in these areas."}, {"title": "5.1 Statistics and Evaluation for Dataset and\nBenchmark", "content": ""}, {"title": "5.1.1 Data's Source", "content": "The materials provided to the LLM come from three sources.\nThe first source is automatically collected Q&A data from\nrelevant GitHub communities involved in AI cluster\ntroubleshooting, such as the issue sections of repositories\nlike Megatron, PAI, Deepspeed, and NCCL. This serves as\nour initial dataset. The data has undergone two rounds of\nfiltering, both automatic and manual, retaining parts with\nclear solutions and logical dialogues. The second source\nis the program output obtained by the LLM-agent using\nRAG+DoT technology on several AI clusters running tasks.\nThese tasks are executed on clusters ranging from 4 to\n100 A800 AI servers. The third part consists of special\nmodal data such as symbolic representations and formulas\nprocessed using XML according to DoT logic, all of which\nare unified into the text modality.\nThe total amount of pure text material is 200+ items\ncompared with 1.2GB origin files. This also confirms\nthat if more than 200 items consist of pure text content\nis fully pre-tokenized to serve as the context for LLM\ninference, it not only poses a significant challenge to the\nLLM's capability to handle long texts but also increases the"}, {"title": "5.1.2 Benchmark's Source and Statistics for Benchmark", "content": "We divided the original dataset into two parts, approximately\nin a 20%-80% ratio. From the 80%, we manually compiled\n150 questions to assess the LLM's capabilities in the field of\ncluster diagnostics. During comparative experiments, unless\notherwise specified, we provide only 20% of the original\ndata to all models. During case studies and practical applica-\ntions, we provide the entire original dataset to the deployed\nLLM-agent.\nWe designed three evaluation metrics. Metric A evaluates\nthe large model's information extraction capabilities, in-\ncluding extracting the cluster IP addresses and SSH port\nnumbers from conversations, as well as the ability to deter-\nmine whether further execution is needed, evaluated through\nstring matching. The challenge here is to assess the model's\nability to follow instructions and extract information, since\nlogs are derived from user conversations and may contain\nunnecessary commands that need to be ignored during the\ndetermination process. Metric B evaluates the large model's\ncode generation capabilities in the diagnostic domain, in-\ncluding the ability to generate prescribed code based on\ndescriptions given in conversations, control the input and\noutput of the code, and create unseen test cases, imple-\nmented in a manner similar to human-eval but transferred to a real distributed cluster. Metric C eval-\nuates the large model's information attribution capabilities\nin the diagnostic domain, including the ability to provide\nattribution based on users' error logs and information. This\nis currently implemented through multiple-choice questions."}, {"title": "5.1.3 Evaluation of Benchmark on Standard\nLLaMA3.1-70B", "content": "We applied this benchmark to several of the most\nwidely used open-source LLMs, namely LLaMA3.1-70B,\nnemotron-70B, mistral-120B and llama3.2 3B.\nThe results is in table 1. Due to the lack of relevant\ndata and information, as well as reasoning logic such\nas DoT, all models were only able to complete the first\ntask, scoring zero on the second and third tasks. Since\nthe results of llama3.2 3B did not meet the minimum\nrequirements for building the LLM-agent, and the 120B\nmodel is difficult to infer on a single AI server, we opted for\nthe better-performing and more widely used LLama3.1-70B\nout of the two 70B models as the basis for subsequent SFT\n(Supervised Fine-Tuning) and the application of RAG, DoT,\nand selfplay."}, {"title": "5.2 LMMs' Evaluation", "content": ""}, {"title": "5.2.1 Experimental Setup", "content": "We conduct two parts of experiments to comprehen-\nsively evaluate and compare the innovative effects of our\nwork. In the first part, we use the mature and universal\nMMLU benchmark to evaluate the\ncomprehensive ability of the model in basic text understand-\ning after it has been enhanced by RAG, DoT, and self-play.\nIn the second part, through ablation and comparison exper-\niments, combined with the focus areas of the sub-items in\nour proposed benchmark, we quantitatively demonstrate the\nadvantages of our three innovations."}, {"title": "5.2.2 General Capability Evaluation Based on MMLU", "content": "Firstly, we aim to substantiate why SFT is not advisable\nin this domain. Although the LLM that supports the agent\nneeds to possess extensive knowledge in cluster diagnostics,\nperformance modeling, and code writing, we discovered\nthat when the LLM reaches a level where this knowledge\ncan be effectively applied, it often lacks the fundamental\ninteraction capabilities required to engage with the agent.\nWe illustrate this point using the MMLU benchmark.\nWe converted the knowledge repository into tokens\ncompatible with the model and constructed an instruction\ndataset. We iterated through multiple training rounds until\nthe model could respond correctly to instructions. We then"}, {"title": "5.2.3 Results of Our Benchmark", "content": "Table 4 presents all of our experimental results. The second\ncolumn of the table indicates whether there was \"cheating.\"\nWe define experiments that do not participate fairly in\nthe benchmark as cheating. While this is unfair for the\nbenchmark portion, it is clearly meaningful for our core\nresearch objective: to build an LLM-agent system that can\nautonomously and intelligently perform cluster diagnostics\nand troubleshooting. When evaluating the benchmark\nsection, the cheating items can be considered as ground\ntruth.\nThese experimental results can illustrate several conclusions."}, {"title": "5.3 Intelligent Early Warning and Troubleshooting: A\nCase Study", "content": "To demonstrate the superiority of the LLM-agent system we\nhave built in the context of intelligent cluster diagnostics, we\ncan present a concrete example to illustrate how the system\noperates and how it is more efficient and accurate compared\nto traditional methods. In the production environment of\nAI clusters, abnormal events or interruptions are not the\nmost challenging problems to resolve. Clear information\nabout anomalies or interruptions can effectively guide\nsenior engineers in diagnosing the causes of issues. Current\nresearch is also progressively integrating technologies such\nas automatic restarts and automatic scheduling into the\nprocedures for handling anomalies or interruptions in AI\ncomputing tasks. However, once an AI computing task\nexhibits slow performance, it becomes difficult to quickly\nidentify the problem, and it is even harder to pinpoint the\ncause of the slowdown.\nAssume there is an AI training cluster composed of dozens\nof servers, where one of the servers suddenly experiences a\nperformance drop. This could be due to various reasons,\nsuch as increased network latency, memory leaks, high\nCPU load, or insufficient storage space. Traditionally,\nadministrators or engineers would check the log files of the\ncluster to manually identify possible issues. This would\ninvolve reviewing logs from different nodes, monitoring\nsystem metrics, attempting to reproduce the problem, and\nso on. This method is time-consuming and labor-intensive\nand may require multiple attempts to pinpoint the root\ncause. In our system, the LLM-agent automatically gathers\nrelevant log information, performance metrics, and other\nnecessary data from the nodes of the cluster. Leveraging the\nLLM-agent's capabilities assessed through the benchmark,\nthe system extracts useful information from the collected\ndata, such as cluster IP addresses, SSH ports, and other crit-\nical diagnostic details. Using its diagnostic capabilities in\ncode generation and information attribution, the LLM-agent\nidentifies the root cause of the issue based on the collected\ndata and information. This may include generating new test\ncases to validate hypotheses. Once the problem is identified,\nthe LLM-agent generates corresponding remediation scripts\nand requests human review. After approval, the LLM-agent\nexecutes the remediation measures in the cluster. Following\nthe execution of remediation measures, the system collects\ndata again to assess the outcome, forming a closed loop of\ndata, algorithm, and hardware to optimize future diagnostic\nprocesses.\nWe manually constructed a scenario. This scenario would\nlead to slow performance in AI model training tasks and\nhas repeatedly occurred in the development environment.\nWe simulated an extreme heat situation with HVAC failure,\nthrottling the frequency of one of the dozens of GPUs to\napproximately 200 MHz, rather than the 1410 MHz that the\nA800 GPUs should operate at. Observing the actual logs\nshows that the speed of this AI computing task decreased to\napproximately one-third of its normal performance. Our\nLLM-system initially flagged the slow AI task through\npower consumption monitoring and performance modeling\nresults, triggering an automatic alert. Following this,\nthrough three rounds of self-play, it recommended checking\nthe GPU core frequencies, a suggestion that the agent\nthen dispatched for execution across all GPUs. Based on\nthe execution results, the LLM accurately pinpointed the\nGPU with the low core frequency that we had specifically\naltered. The entire troubleshooting process took less than\n10 minutes. In contrast, a senior operations engineer would\ntypically need about one hour to correctly identify the\nproblem and then use a pre-written automated detection\nsoftware tool created by engineers to determine the specific\nGPU with the low-frequency fault. More importantly,\nour LLM-agent can identify the fault before algorithm\nengineers or operations engineers detect the slow-down\nphenomenon and automatically complete the repair. This\nachieves resolving the issue before the fault occurs, thereby\nenhancing the overall availability of the cluster."}, {"title": "5.4 Qualitative Analysis of Correctness, Safety, and\nReliability", "content": "Based on the existing research that is not yet fully mature,\nand in the context of this specific field of study, we\nprovide reasonable definitions for correctness, safety, and\nreliability. In this study, we define correctness as whether\nthe process and results of the LLM-agent executing tasks\nare correct. Compared to evaluating the output of the\nLLM, assessing the correctness of the LLM-agent's actions\nis more challenging. An apparently incorrect operation\nprocess may produce the correct result, whereas seemingly\nperfect output at the textual level might lead to an erroneous\nresult when executed. Since we focus on the field of cluster\ndiagnostics with the actual output being the execution of\nprocedures by the agent, we do not investigate the potential\nharmfulness or bias in the textual content generated by the\nLLM. Instead, we examine the ability of our LLM-agent to\navoid performing harmful operations on the cluster when\nthe information fed back to the agent changes, or even when\nmalicious content is inserted by an attacker, such as deleting\nfiles, shutting down, overclocking, or modifying critical\nsystem configurations. Regarding reliability, we define it\nas the overall quality of fault handling by the LLM-agent\ncompared to human engineers or expert human engineers.\nIn addition to whether the attribution is correct, we also\nconsider factors such as the time taken to complete typical\nfault handling, the resources consumed, and the ability to\ncommunicate with non-experts.\nWe incorporate the assessment of correctness into the\nbenchmark evaluation. For the potential risks associated\nwith the LLM-agent, we implement a whitelist plus\nhuman review approach. Initially, we ensure the safety\nof the existing toolkit, followed by creating a whitelist\nfor the program interfaces included in the toolkit and\nconducting human reviews for the LLM-agent's requests\nto execute self-authored code. Finally, we observed that\nthe LLM-agent can attribute faults with an average of\nfewer than three test cases across multiple rounds of\nself-play, which is more efficient than the twelve cases\ntypically required by human experts. However, regarding\ncommunication abilities, the LLM-agent currently does not\npossess such capabilities. The qualitative analysis described\nabove is mainly aimed at reducing the probability of\nharmful incidents. Quantitative analysis or a comprehensive\nmodel still necessitates further advancements in the field of\nAl safety."}, {"title": "6 CONCLUSION AND DISCUSSION", "content": ""}, {"title": "6.1 Work Summary and Further Plan", "content": "Based on our experience and research in the fields of cluster\ndiagnostics, LLM enhancement, and LLM-agent construc-\ntion, we innovatively proposed a system solution utilizing\nLLM-agents to autonomously and intelligently perform clus-\nter troubleshooting. In terms of LLM algorithms, we intro-\nduced a benchmark consisting of 150 advanced problems\nmanually crafted, demonstrating the performance differ-\nences between our constructed LLM-agent and the original\nopen-source LLMs under fair data conditions. In the realm\nof LLM-agent construction, we innovatively proposed inte-\ngrating DoT reasoning mathematics and the ability to handle\nspecial symbols and formulas into the agent, enabling the\nLLM to operate machines at the software level and receive\nfeedback. Ultimately, we applied our innovative achieve-\nments to cluster diagnostics, exploring the potential in this\nfield, and were pleasantly surprised to find that the LLM-\nagent systems, despite being in their extremely early stages,\nare already capable of handling repetitive and low-end tasks,\nthus freeing industry practitioners to tackle more challeng-\ning and valuable problems.\nIn the future, we will continue our work in four aspects. In\nterms of LLM algorithms, we will expand and upgrade the\nexisting benchmark and build a more comprehensive and\nvaluable metrics system. In the Agent field, we will further\nunlock the potential of DoT and make self-written code by\nthe LLM gradually become the main execution body, re-\nducing reliance on preset tools. At the system application\nlevel, we will form a closed loop of data, algorithm, and\nhardware, enriching the database with results from"}]}