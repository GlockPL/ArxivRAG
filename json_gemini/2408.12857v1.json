{"title": "Memory-Efficient LLM Training with Online Subspace Descent", "authors": ["Kaizhao Liang", "Bo Liu", "Lizhang Chen", "Qiang Liu"], "abstract": "Recently, a wide range of memory-efficient LLM training algorithms have gained substantial popularity. These methods leverage the low-rank structure of gradients to project optimizer states into a subspace using projection matrix found by singular value decomposition (SVD). However, convergence of these algorithms is highly dependent on the update rules of their projection matrix. In this work, we provide the first convergence guarantee for arbitrary update rules of projection matrix. This guarantee is generally applicable to optimizers that can be analyzed with Hamiltonian Descent, including most common ones, such as LION, Adam. Inspired by our theoretical understanding, we propose Online Subspace Descent, a new family of subspace descent optimizer without SVD. Instead of updating the projection matrix with eigenvectors, Online Subspace Descent updates the projection matrix with online PCA. Online Subspace Descent is flexible and introduces only minimum overhead to training. We show that for the task of pretraining LLaMA models ranging from 60M to 7B parameters on the C4 dataset, Online Subspace Descent achieves lower perplexity and better downstream tasks performance than state-of-the-art low-rank training methods across different settings and narrows the gap with full-rank baselines.", "sections": [{"title": "1 Introduction", "content": "The continual advancement in training large language models (LLMs) presents a compelling challenge in balancing computational efficiency with model performance. As the scope and complexity of these models grow, so does the necessity for innovative strategies that optimize memory usage without compromising the learning capabilities of the model. Recent approaches in low-rank adaptation strategies, including Stochastic Subspace Descent [13], LoRA [11], ReLoRA [15], Gradient Low-Rank Projection (GaLore) [25] and Sketchy [9], have paved the way for memory-efficient training by utilizing a periodically updated low-rank projection matrix to manage parameter updates. In particular, GaLore and Sketchy both utilize expensive singular value decomposition to determine the projection matrix, whereas stochastic subspace descent suggests using random matrices as projection matrices and provides convergence analysis on convex functions and objectives. However, to the best of our knowledge, no one has offered any guarantee of convergence for this class of methods on non-convex functions and objectives.\nIn this work, we provide the first convergence guarantee for arbitrary update rules of the projection matrix. This guarantee is significant because it is broadly applicable to a wide range of optimizers that can be analyzed within the Hamiltonian descent framework [18]. By establishing this convergence guarantee, we demonstrate that our approach is not limited to specific or narrowly defined update rules, but can be extended to include many commonly used optimizers in the field. In particular, this"}, {"title": "2 Optimization Background", "content": "The training of deep learning models reduces to an optimization problem\n$\\min_W L(W)$,\nwhere $W$ is the set of weight matrices of the model. For simplicity, we assume $W \\in \\mathbb{R}^{n \\times m}$ is a single matrix of size $(n, m)$ without loss of generality. For notation, we write $\\langle A, B \\rangle = \\text{tr}(A^T B)$ for inner products of matrices, and $||A||^2 = \\text{tr}(A^T A)$ the Frobenius norm. We use $A \\odot B$ to denote the elementwise product, and $A^{\\odot 2} = A \\odot A$.\nExample 2.1. Update rules of common optimizers:\nGradient Descent : $W_{t+1} = W_t - \\epsilon_t \\nabla L(W_t)$,\nMomentum : $W_{t+1} = W_t - \\epsilon_t M_t$, $M_t = (1 - \\beta) \\nabla L(W_t) + \\beta M_{t-1}$,\nLion-K [4] : $W_{t+1} = W_t - \\epsilon_t \\nabla K(N_t)$, $N_t = (1 - \\beta_1) \\nabla L(W_t) + \\beta_1 M_t$, $M_t = (1 - \\beta_2) \\nabla L(W_t) + \\beta_2 M_{t-1}$,\nAdam [12]: $W_{t+1} = W_t - \\epsilon_t \\frac{M_t}{\\sqrt{V_t + \\epsilon}}$, $M_t = (1 - \\beta_{1t}) \\nabla L(W_t) + \\beta_{1t} M_{t-1}$, $V_t = (1 - \\beta_{2t}) \\nabla L(W_t)^{\\odot 2} + \\beta_{2t} V_{t-1}$,\nwhere $\\epsilon_t$ are step sizes, and $M_t, V_t$ are the first and second order momentum, and $\\beta, \\beta_1, \\beta_2$ are momentum coefficients in $(0, 1)$, with $\\beta_{it} = \\frac{\\beta_i - \\beta^t}{1 - \\beta^{t+1}}, i = 1, 2$ for $\\beta_1, \\beta_2 \\in (0, 1)$ in Adam, and $K$ is any convex function with $\\nabla K(0) = 0$ for Lion-K [4], and Lion [5] uses $K(X) = ||X||_{1, 1}$ and $\\nabla K(X) = \\text{sign}(X)$."}, {"title": "3 Memory-Efficient Optimizers via Online Subspace Descent", "content": "We introduce the idea of constructing memory efficient optimzers by descending in the subspaces that dynamically changes across iterations as motivated by GaLore [25] and Sketchy [9]. We first derive static subspace descent by restricting the whole optimization on a subspace (Section 3.1), and then propose to dynamically change the subspace across iterations as a heuristic to attain the optimization in the full space while only using subspace descent (Section 3.2). In particular, we propose to update the subspaces via continuous online PCA like updates to avoids the need of exact SVD like in GaLore and Sketchy (Section 3.2). Finally, we remark in Section 3.3 the heuristic nature of the derivation of the method and highlight the difficulty in theoretical understanding, which motivates our analysis based on Hamiltonian dynamics in Section 4."}, {"title": "3.1 Static Subspace Descent", "content": "One popular approach to improving memory efficiency is to confine the optimization to a low-dimensional space. To do this, we impose a low rank structure of $W = P \\hat{W}$, where $P \\in \\mathbb{R}^{n \\times k}$ is a projection matrix to be determined later and $\\hat{W} \\in \\mathbb{R}^{k \\times m}$ is a dimension-reduced parameter. When $k \\ll n, P$ and $\\hat{W}$ are much smaller in size compared to $W$. Now consider\n$\\min_{\\hat{W}} L(P \\hat{W}).$\nApplying the optimizer from (1) to update $W$ along with an optimizer state $\\hat{S}$, and mapping the update rule $W_{t+1} = \\hat{W}_t + \\phi_t(\\hat{S}_t)$ to that of $W = P \\hat{W}_t$, we get\n$W_{t+1} = W_t + P \\phi_t(\\hat{S}_t)$, $\\hat{S}_t = \\psi_t(\\hat{S}_{t-1}, P^\\top \\nabla L(W_t))$,\nwhere we used the fact that $\\nabla_{\\hat{W}} L(P \\hat{W}) = P^\\top \\nabla_W L(W)$. This yields a more memory-efficient optimizer, as the size of $\\hat{S}_t$ is proportional to that of $\\hat{W}_t$, much smaller than $S_t$ in (1) when $k \\ll n$."}, {"title": "3.2 Online Subspace Descent", "content": "With a static $P$, regardless of its values, the parameter $W$ is restricted to have a low rank structure. Although low rank assumption is proved to be useful for fine-tuning with LoRA-like methods [11], it is often too limited for pre-training or when the desirable model weights are not inherently low-rank.\nTo address this problem, Zhao et al. [25] suggested to keep the projected updated in (4), but use different $P$ across the iterations:\n$W_{t+1} = W_t + P_t \\phi_t(\\hat{S}_t)$, $\\hat{S}_t = \\psi_t(\\hat{S}_{t-1}, P_t^\\top \\nabla L(W_t))$, $P_{t+1} = \\chi_t(P_t, W_t, \\hat{S}_t)$,\nwhere $\\chi_t$ is a update rule of $P_t$ that will be determined in the sequel. The intuition is to open up different projection directions at different iterations, so that optimization can be conducted in different subspaces across different iterations. This is similar to the update of coordinate descent, except in a continuous fashion. Note that the update of $P_t$ can be done in parallel with that of $(W_t, \\hat{S}_t)$, and incurs no slowdown once it is fast enough to not cause a speed bottleneck.\nExample 3.1. Examples of common optimizers equipped with online subspace descent:\nGradient Descent: $W_{t+1} = W_t - \\epsilon_t P_t P_t^\\top G_t$, $G_t = \\nabla L(W_t)$,\nMomentum : $W_{t+1} = W_t - \\epsilon_t P_t M_t$, $M_t = (1 - \\beta) P_t^\\top G_t + \\beta M_{t-1}$,\nLion-K : $W_{t+1} = W_t - \\epsilon_t P_t \\nabla K(\\tilde{N}_t)$, $\\tilde{N}_t = (1 - \\beta_1) \\tilde{G}_t + \\beta_1 M_t$, $\\tilde{G}_t = P_t^\\top G_t$, $M_t = (1 - \\beta_2) \\tilde{G}_t + \\beta_2 M_{t-1}$,\nAdam : $W_{t+1} = W_t - \\epsilon_t P_t \\frac{M_t}{\\sqrt{V_t + \\epsilon}}$, $\\tilde{G}_t = P_t^\\top G_t$, $M_t = (1 - \\beta_{1t}) \\tilde{G}_t + \\beta_{1t} M_{t-1}$, $V_t = (1 - \\beta_{2t}) \\tilde{G}_t^{\\odot 2} + \\beta_{2t} V_{t-1}$.\nHow Should $P_t$ be Updated? It is useful to draw intuition from the projected gradient descent rule\n$W_{t+1} = W_t - \\epsilon_t P_t P_t^\\top G_t$, $G_t = \\nabla L(W_t)$,"}, {"title": "4 Hamiltonian Descent Meets Subspace Descent: A Lyapunov Analysis", "content": "In this section, we show a surprising result that the complication outlined above in Section 3.3 is not a problem for optimizers that yields the Hamiltonian+descent structure in (2). Our result is two-fold:\n\u2022 Section 4.1: When applying Online Subspace Descent on systems in (2), the Hamiltonian+descent structure is preserved once the update rule of $P_t$ has a smooth continuous-time limit. Hence, under very mild conditions, Online Subspace Descent equipped with common optimizers like Adam and Lion automatically yield a Lyapunov function and hence benign continuous-time convergence. Moreover, $P_t$ can, in fact, be generalized to an arbitrary linear operator as shown in Section 4.3.\n\u2022 Section 4.2: For any smooth $P_t$ update rules that eliminates the degenerate case of $P_t^\\top G_t = 0$ while $G_t \\neq 0$ at convergence, the online subspace optimizer guarantees to converge in continuous time to a stationary point of the loss $L(W)$. This mild condition is satisfied, for example, when $P_t$ is updated by a typical optimizer on the online PCA objective $\\mathcal{L}_{G_t}(P)$."}, {"title": "4.1 Online Subspace Descent Preserves the Hamiltonian+Descent Structure", "content": "Applying dynamic projection to Hamiltonian descent in (2), we obtain the following systems:\n$\\frac{d}{dt} W = P_t \\partial_S H(W_t, \\hat{S}_t) - \\Phi(\\partial_W H(W_t, \\hat{S}_t))$\n$\\frac{d}{dt} \\hat{S} = -P_t^\\top \\partial_W H(W_t, \\hat{S}_t) - \\Psi(\\partial_{\\hat{S}} H(W_t, \\hat{S}_t))$\n$\\frac{d}{dt} P_t = \\Gamma(P_t, \\nabla L(W_t))$,\nwhere $\\Gamma$ specifies the update rule of $P_t$. Following essentially the same derivation as (3), one can show that $H(W, S)$ remains a Lyapunov function of (7), regardless of the choice of $\\Gamma$:\n$\\frac{d}{dt} H(W_t, \\hat{S}_t) = \\langle \\partial_W H_t, \\frac{d}{dt} W_t \\rangle + \\langle \\partial_{\\hat{S}} H_t, \\frac{d}{dt} \\hat{S}_t \\rangle = \\langle \\partial_W H_t, P_t \\partial_S H_t - \\Phi(\\partial_W H_t) \\rangle + \\langle \\partial_{\\hat{S}} H_t, -P_t^\\top \\partial_W H_t - \\Psi(\\partial_{\\hat{S}} H_t) \\rangle = - ||\\partial_W H_t||_{\\Phi}^2 - ||\\partial_{\\hat{S}} H_t||_{\\Psi}^2 \\leq 0,$\nwhere the key is to use the adjoint property of $P_t$ and $P_t^\\top$ that $\\langle P_t^\\top X, Y \\rangle = \\langle X, P_t Y \\rangle$, which cancels the crossing terms, independent of the values of $P_t$. There is no requirement on $\\Gamma$ here, besides that the derivative in (8) should exist. As shown in Section 4.3, we can generalize (8) by replacing $P_t$ and $P_t P_t^\\top$ with a general linear operator $\\mathcal{P}_t$ and its adjoint $\\mathcal{P}_t^*$.\nThe following are examples of continuous-time Momentum, Lion-K and Adam with subspace descent and their Hamiltonian functions.\nExample 4.1. Momentum + Online Subspace Descent is\n$\\frac{d}{dt} W = -P_t \\hat{M}_t$, $\\hat{G}_t = P_t^\\top \\nabla L(W_t)$, $\\frac{d}{dt} \\hat{M}_t = a(\\hat{G}_t - \\hat{M}_t)$,\nwith Lyapunov function $H(W, M) = L(W) + \\frac{||M||^2}{2 a}$, for which\n$\\frac{d}{dt} H(W, M) = -\\langle \\nabla L(W), P_t \\hat{M}_t \\rangle + \\langle \\hat{M}, a(P_t^\\top \\nabla L(W_t) - \\hat{M}_t) \\rangle = - ||\\hat{M}||^2 \\leq 0$.\nExample 4.2. Adam + Online Subspace Descent is\n$\\frac{d}{dt} W = P_t \\frac{\\hat{M}_t}{\\sqrt{\\hat{V}_t + \\epsilon}}$, $\\hat{G}_t = P_t^\\top \\nabla L(W_t)$, $\\frac{d}{dt} \\hat{M}_t = a(\\hat{G}_t - \\hat{M})$, $\\frac{d}{dt} \\hat{V}_t = b(\\hat{G}_t^2 - \\hat{V}_t)$,\nwith Lyapunov function $H(W, M, V) = L(W) + \\frac{1}{2 a} \\langle \\frac{M}{\\sqrt{V + \\epsilon}}, \\frac{M}{\\sqrt{V + \\epsilon}} \\rangle$, for which"}, {"title": "4.2 Convergence to Local Optima", "content": "In addition to the Lyapunov structure, we need an additional mild condition on the update rule of $P_t$ to ensure the system converges to the local optimum of the loss $L(W)$. The main idea is to prevent the system from stopping prematurely before reaching zero gradient $G_t = 0$ by excluding the degenerate case of $P_t^\\top G_t = 0$ while $G_t \\neq 0$ in the invariant set of the system.\nAssumption 4.4. Assume the functions in system (7) are continuously differentiable and\ni) $\\partial_S H(W_t, \\hat{S}_t) = 0$ implies $\\hat{G}_t = P_t^\\top \\nabla L(W_t) = 0$ and $\\frac{d}{dt} W_t = 0$.\nii) When $G_t = G \\neq 0$, the set $\\{P: \\frac{d}{dt} P^\\top G = 0\\}$ is not a positive invariant set of $\\frac{d}{dt} P_t = \\Gamma(P_t, G_t)$.\nThis is a mild condition. Assumption i) says that the optimizer should stop at a point with $\\hat{G}_t = 0$, which is easy to verify for the common optimizers like momentum, Adam, Lion-K. Assumption ii) ensures $\\hat{G}_t = 0$ would imply $G_t = 0$ in invariance sets, which is satisfied when for example, $P_t$ is updated by a reasonable optimizer of the online PCA loss that converges to a stable local minimum.\nTheorem 4.5. Assume Assumption 4.4 holds. Let $(W_t, \\hat{S}_t, P_t)_t$ be a bounded solution of (7), then all the accumulation points $\\{W_t\\}$ as $t \\to +\\infty$ are stationary points of $L(W)$.\nProof. By LaSalle's invariance principle, the positive limit set of $(W_t, \\hat{S}_t, P_t)_t$ must be contained in $\\mathcal{I}$, where $\\mathcal{I} = \\{\\text{the union of complete trajectories satisfying } \\frac{d}{dt} H(W_t, \\hat{S}_t) = 0, \\forall t \\}$."}, {"title": "4.3 Online Subspace Descent with General Linear Projection Operators", "content": "We can generalize the online subspace descent with general linear operators:\n$\\frac{d}{dt} W = \\mathcal{P}_t (\\partial_S H(W_t, \\hat{S}_t)) - \\Phi(\\partial_W H(W_t, \\hat{S}_t))$\n$\\frac{d}{dt} \\hat{S} = -\\mathcal{P}_t^* (\\partial_W H(W_t, \\hat{S}_t)) - \\Psi(\\partial_{\\hat{S}} H(W_t, \\hat{S}_t))$\n$\\frac{d}{dt} \\mathcal{P}_t = \\Gamma(\\mathcal{P}_t, \\nabla L(W_t))$,\nwhere we generalize $P_t$ to be any linear operator $\\mathcal{P}_t$ with an adjoint operator $\\mathcal{P}_t^*$, satisfying\n$\\langle X, \\mathcal{P}_t (Y) \\rangle = \\langle \\mathcal{P}_t^*(X), Y \\rangle, \\forall X, Y$.\nThe derivation of Lyapunov follows a similar way:\n$\\frac{d}{dt} H(W_t, \\hat{S}_t) = - ||\\partial_W H_t||_{\\Phi}^2 - ||\\partial_{\\hat{S}} H_t||_{\\Psi}^2 + \\langle \\partial_W H_t, \\mathcal{P}_t (\\partial_S H_t) \\rangle - \\langle \\partial_{\\hat{S}} H_t, \\mathcal{P}_t^* (\\partial_W H_t) \\rangle = - ||\\partial_W H_t||_{\\Phi}^2 - ||\\partial_{\\hat{S}} H_t||_{\\Psi}^2 \\leq 0,$\nwhere the crossing terms are again canceled due to the adjoint property.\nAs an example of the general framework, consider $\\mathcal{P}_t(X) = P_t X Q_t$, where $Q_t$ is another projection matrix applied on the different dimension of $X$ (see also [25]). The adjoint operator of $\\mathcal{P}_t$ is $\\mathcal{P}_t^*(X) = P_t^\\top X Q_t^\\top$. This can be verified by\n$\\langle P_t X Q_t, Y \\rangle = \\text{tr}(P_t X Q_t Y^\\top) = \\text{tr}(X Q_t Y^\\top P_t^\\top) = \\text{tr}(X (P_t^\\top Y Q_t^\\top)^\\top) = \\langle X, P_t^\\top Y Q_t^\\top \\rangle$.\nThe subspace descent system of this operator is\n$\\frac{d}{dt} W = P_t (\\partial_S H(W_t, \\hat{S}_t)) Q_t - \\Phi(\\partial_W H(W_t, \\hat{S}_t))$\n$\\frac{d}{dt} \\hat{S} = -P_t^\\top (\\partial_W H(W_t, \\hat{S}_t)) Q_t^\\top - \\Psi(\\partial_{\\hat{S}} H(W_t, \\hat{S}_t))$\n$\\frac{d}{dt} P_t = \\Gamma_P(P_t, Q_t, \\nabla L(W_t))$\n$\\frac{d}{dt} Q_t = \\Gamma_Q(P_t, Q_t, \\nabla L(W_t))$,\nwhere $P_t, Q_t$ can be updated jointly via an online SVD on $G_t$.\nAnother linear operator that involves two matrices is $\\mathcal{P}_t(X) = P_t X + X Q_t$, which yields $\\mathcal{P}_t^*(X) = P_t^\\top X + X Q_t^\\top$."}, {"title": "5 Experiment", "content": "We answer a number of key questions with pretraining experiments of LLaMA [22] on the C4 dataset [20]. All experiments except for large 7B experiments are conducted on a single NVIDIA A100 GPU."}, {"title": "5.1 Why do we Need Online Subspace Descent?", "content": "Overall, Online Subspace Descent offers two major advantages over previous methods that rely on SVD, better convergence and lower overhead. In this section, we discuss both in detail."}, {"title": "5.2 What Rank Should we Pick for Online Subspace Descent?", "content": "We conduct an ablation study on the rank of Online Subspace Descent. Figure 3 shows that the final perplexity is inversely correlated with rank: higher ranks result in lower convergent perplexity. However, the rate of reduction of perplexity decreases as the rank increases, eventually reaching a saturation point. We propose an intuitive explanation for this phenomenon. In language modeling, high-frequency tokens can be effectively learned with low-rank training. However, learning lower-frequency tokens requires higher ranks. Once these lower-frequency tokens are adequately learned, further increasing the rank does not significantly decrease perplexity. In conclusion, given sufficient time and resources, higher ranks yield better performance for Online Subspace Descent. It is recommended that the highest rank be selected until the perplexity reduction saturates."}, {"title": "5.3 What are the Best Hyperparameters?", "content": "$\\alpha$ and $\\lambda$: The parameter $\\alpha$ controls the update speed of $P_t$, while $\\lambda$ determines the regularization strength on the optimization objective of $P_t$. Empirically, we find that the result is not sensitive to $\\lambda$ for small models (60M). and set $\\lambda$ = 0.1 for all subsequent experiments. We find that $\\alpha$ must be kept small to avoid instability (Figure 3), and we set $\\alpha$ = 5 for all experiments.\nLearning rate: For the small model (60M), learning rate choices are more flexible, producing similar results. However, for larger models (350M, 1B), we recommend using a learning rate that is 10 times smaller, specifically 0.001. Larger learning rates cause unrecoverable spikes and instability, a general characteristic observed across all methods. See additional hyperparameter choices in Appendix A."}, {"title": "5.4 Can Online Subspace Descent be Applied to Different Optimizers?", "content": "One straightforward extension of Online Subspace Descent is to apply it to other base optimizers beyond AdamW8bit. We conduct ablation studies on LION [6] and Adafactor [21], finding that Online Subspace Descent behaves similarly to how it does with AdamW8bit. Despite the initial observation that updating $P_t$ with AdamW8bit consistently yields better results, we discover that updating $P_t$ with simple SGD can achieve similar performance."}, {"title": "5.5 Can Online Subspace Descent Scale to Larger Model?", "content": "We pretrain from scratch a 7B LLaMA model on the C4 dataset for 10K steps, where the $P_t$ matrix is updated by SGD. The perplexity is the lower the better. The final perplexity and training wall-clock time are provided in Table 3. We further provide the downstream evaluation of the pretrained checkpoints using Galore and our method on the GLUE benchmark in Table 4. Our method consistently outperforms Galore when the model size scales up."}, {"title": "6 Related Works", "content": "We discuss related works on memory-efficient optimization and low-rank adaptation techniques."}, {"title": "7 Conclusion", "content": "In conclusion, we provide the first convergence guarantee for arbitrary update rules of projection matrix, applicable to a range of optimizers that can be analyzed using Hamiltonian Descent, including common ones like LION, AdamW, and Adafactor. Inspired by this theoretical foundation, we introduce Dynamic Subspace Descent, a novel family of subspace descent optimizers that eschews SVD in favor of online PCA for updating projection matrix. Dynamic Subspace Descent is both flexible and minimally intrusive, and our experiments show that it achieves lower perplexity in pretraining LLaMA models (ranging from 60M to 1B parameters) on the C4 dataset compared to state-of-the-art low-rank training methods, while also closing the perplexity gap with full-rank baselines.\nFor future research, we propose several open and intriguing questions: (1) Are there alternative methods for updating projection matrix that could accelerate convergence? (2) What is the impact of weight decay on convergence in Dynamic Subspace Descent? (3) Can low-rank gradients and updates be combined with dynamic low-rank weights (e.g., Mixture of Experts) to further enhance training efficiency? (4) Can this method be applied to problems beyond language modeling? We hope that our work provides a strong foundation for exploring these questions."}]}