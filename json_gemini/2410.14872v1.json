{"title": "HOW TO EVALUATE REWARD MODELS FOR RLHF", "authors": ["Evan Frick", "Tianle Li", "Connor Chen", "Wei-Lin Chiang", "Anastasios N. Angelopoulos", "Jiantao Jiao", "Banghua Zhu", "Joseph E. Gonz\u00e1lez", "Ion Stoica"], "abstract": "We introduce a new benchmark for reward models that quantifies their ability to\nproduce strong language models through RLHF (Reinforcement Learning from\nHuman Feedback). The gold-standard approach is to run a full RLHF training\npipeline and directly probe downstream LLM performance. However, this pro-\ncess is prohibitively expensive. To address this, we build a predictive model of\ndownstream LLM performance by evaluating the reward model on proxy tasks.\nThese proxy tasks consist of a large-scale human preference and a verifiable cor-\nrectness preference dataset, in which we measure 12 metrics across 12 domains.\nTo investigate which reward model metrics are most correlated to gold-standard\nRLHF outcomes, we launch an end-to-end RLHF experiment on a large-scale\ncrowdsourced human preference platform to view real reward model downstream\nperformance as ground truth. Ultimately, we compile our data and findings into\nPreference Proxy Evaluations (PPE), the first reward model benchmark explicitly\nlinked to post-RLHF real-world human preference performance, which we open-\nsource for public use and further development\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "The ultimate test of a reward model is as follows:\nDoes the reward model lead to good post-RLHF language model performance?\nIn other words, because the reward model will be used as a reference signal for LLM training,\nin principle, only the downstream LLM performance matters. However, to evaluate downstream\nperformance, we must train a new LLM using the reward model and evaluate the resulting LLM\u2014a\nprohibitively expensive and time-consuming process (Figure 1). The long development-feedback\ncycle of reward models poses a significant challenge, limiting achievable reward model quality and, \nconsequently, limiting the effectiveness of the entire RLHF process."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 HUMAN PREFERENCE AND REWARD MODELS", "content": "Human preference has emerged as one of the gold standards for LLM training and evaluation. Sev-\neral large-scale human preference datasets have been developed, including Stanford Human Prefer-\nerence (SHP) (Ethayarajh et al., 2022), Chatbot Arena (Chiang et al., 2024), and Anthropic HH (Bai\net al., 2022a), among others. Researchers requiring human preference proxies have pursued two\nmain approaches in this area. First, they have trained reward models based on real or synthetically\ngenerated human preference data to approximate human preferences for LLM training. Second, they\nhave employed LLMs as judges for evaluating other LLMs.\nFor the training side, the line of work on Reinforcement Learning from Human Feedback (RLHF)\nfocuses on the family of algorithms that first train a reward model as a proxy of human preferences,\nand then use the reward model as the signal to fine-tune the language model with reinforcement\nlearning (Christiano et al., 2023; Bai et al., 2022a; Ouyang et al., 2022; Touvron et al., 2023; OpenAI,\n2022; Bai et al., 2022b; Lee et al., 2023; OpenAI, 2023a;b; Zhu et al., 2024).\nThis paper studies one of the critical problems in the RLHF process: how do we evaluate reward\nmodels and select the best one for downstream performance?"}, {"title": "2.2 REWARD MODEL BENCHMARKS", "content": "RewardBench is the first and only previous RLHF reward model benchmark (Lambert et al., 2024).\nRewardBench has 4 main tasks: Chat, Chat Hard, Safety, and Reasoning. The authors source consid-\nerable ground truth preference pairs from MT-Bench (Zheng et al., 2023) and AlpacaEval (Dubois\net al., 2023), though preference labels are also hand-verified. RewardBench also uses adversarial\nexamples from LLMBar (Zeng et al., 2024), coding example pairs with correct vs buggy imple-\nmentations, and safety pairs with should-refusals and should-not-refusals. Overall, RewardBench\nis designed to evaluate across an array of tasks posited as relevant to RLHF. RewardBench takes a\ncrucial first step toward reward model evaluations. However, the authors assert that more research\nmust be done to understand how to correlate performance to RLHF success. In this paper, our ex-\nperiments show that as reward models have improved, we now see a negative correlation between"}, {"title": "3 SOURCING GROUND TRUTH PREFERENCE LABELS", "content": "Previous work on sourcing preference ground truth labels often relies upon LLM judge preference\nlabels in conjunction with manual verification from individuals, introducing potential preference\nbiases. Alternatively, rejected responses are often curated synthetically by unnaturally perturbing\nthe chosen output or modifying the prompt to produce forced errors, introducing bias on how errors\nlook and occur. These preference pairs are not representative of the distribution of responses seen by\nreward models when providing learning signals for RLHF. We offer a brief comparison to previous\nwork in Table 1.\nThus, we ground our preference labels with the following methodology: (1) Utilize crowdsourced\ndiverse prompts and responses with human preference labels. (2) Utilize existing benchmarks with\nverifiable correctness checks on LLM-generated responses.\nThe methodology (1) provides an unbiased estimate of real-world human preference through the ag-\ngregation of many diverse human preferences. We use a large crowdsourced preference set of 16,038\npreference labels to mitigate individual label noise and avoid over-fitting to any single individual's\npreference, details in subsection 4.1.\nMethodology (2) curates an objective correctness signal naturally unbiased by response style. We\nuse the second approach to label the correctness of many sampled responses from an LLM, mimick-\ning rollouts or best-of-k exploration strategies seen in RLHF training processes. As a result, we draw\npreference pairs from more naturally occurring distributions (eg. real LLM responses and errors),\nbetter align with the expected environment reward models operate in."}, {"title": "4 HUMAN PREFERENCE METRICS", "content": "To measure whether a reward model aligns with human preference directly, we utilize a dataset\ncollected from Chatbot Arena, which is a platform that allows users to vote on pairwise compar-\nisons between responses generated from two anonymized and randomly selected LLMs. The human\npreference dataset contains human-labeled preferences for 16,038 pairwise comparisons between\n20 selected top models\u00b3. These models were selected based on their strong performance on Chat-\nbot Arena and overall popularity (Chiang et al., 2024). We emphasized selecting models that have\nalready undergone some form of RLHF, anticipating that these models would be more challenging\nfor reward models to evaluate.\nSince the human preference set is crowd-sourced from Chatbot Arena, we can repeat the collection\nprocess at any time to obtain an updated set that better reflects the current array of available models\nand any changes in human preference. Additionally, a newly updated human preference set would\nlargely mitigate benchmark leakage that may have occurred with the previous set. Consequently,\nthis human preference metric can remain consistently up-to-date with fresh, relevant data."}, {"title": "4.1 CURATION", "content": "Specifically, we curate our human preference data from Chatbot Arena battles. A \"battle\" consists\nof a user-provided prompt, two models and their responses to the prompt, and the user's preference\nvote for the responses. We perform a random sample weighted by model occurrence to obtain\n50,000 battles from Chatbot Arena between selected models such that models are represented at a\nuniform frequency, then de-duplicate and remove any samples containing P.I.I information using\nAzure AI. We use OpenAI's moderation API to flag and remove potentially harmful conversations\nfrom the sample. Finally, we subsample 16,038 pairs from the remaining battles to construct the\nhuman preference dataset.\nThe human preference dataset, at a glance:\n1. Includes 4,583 instruction-following prompts, 5,195 hard prompts, 2,564 math prompts.\nPrompts may exist in multiple categories.\n2. Includes user queries from over 121 languages. Top languages include English (8,842),\nChinese (1,823), Russian (1,779), German (568), Korean (338), Japanese (321), etc.\n3. Includes preferences crowdsourced from 6,120 individuals."}, {"title": "4.2 SCORING", "content": "We conduct several statistical metrics described below to evaluate different aspects of a given reward\nmodel.\n1. Accuracy. We compute pairwise ranking accuracy against a human preference label for each re-\nward model, excluding battles in which the human rater selected a \"tie\". This measures the granular\ncase-by-case similarity to a real human preference signal.\n2. Correlation. Since each battle contains information on model identities, each reward model pro-\nduces a ranking and a pairwise win-rate matrix for the 20 selected models. We compute Spearman\nand Kendall correlation between model ranking produced by each reward model against ground truth\nranking. In addition, we compute row-wise Pearson Correlation between the win-rate matrix pro-\nduced by each reward model against the ground truth win-rate matrix. We intuit that these aggregate\ncorrelation metrics measure overall similarity to real human preference.\n3. Confidence. To weight stability in assigning preferences, we follow the metrics proposed in\nArena-Hard-Auto (Li et al., 2024b), where we measure each reward models's Separability with\nConfidence Interval, Confidence Agreement, and Brier Score against ground truth ranking. These\nmetrics are designed to measure uncertainties and over-confidence within a reward model.\nFurthermore, we can calculate all the above scores conditioned on any subset of prompts in the\nevaluation data, specifically capturing 7 different domains. For example, we can observe these\nmetrics on only math prompts or only instruction following prompts. We expect that strong reward\nmodels should score high regardless of the selected domain. Scores for all subsets are detailed in\nAppendix A.1."}, {"title": "5 CORRECTNESS METRICS", "content": "To measure a preference model's ability to distinguish between different samples drawn from the\nsame distribution, we utilize correctness metrics on established, reputable benchmarks with verifi-\nable ground truths (e.g. Austin et al. (2021)'s MBPP-Plus). We construct a dataset wherein each\nprompt is associated with 32 different responses sampled from the same LLM. Additionally, since\nwe use benchmarks with verifiable ground truths, we can score the correctness (a binary label) of\neach response according to the original static benchmark's verification function (e.g. code unit tests\nor Regex matching).\nTo assess the performance of reward models (and LLMs-as-judges), we obtain rewards/preferences\nfor the sampled responses and evaluate how well these align with the verifiable correctness signal,\nwith the general assumption that expert humans would always prefer correct answers over incor-\nrect ones. Our response sampling strategy ensures that the preference labeler must disentangle the"}, {"title": "5.1 CURATION", "content": "For the correctness metrics, we selected standard, widely used, reputable, and verifiable benchmarks:\nMMLU Pro (Wang et al., 2024b), MATH (Hendrycks et al., 2021), GPQA (Rein et al., 2023), MBPP\nPlus (Austin et al., 2021), and IFEval (Zhou et al., 2023). Each benchmark covers a different domain:\ngeneral knowledge, mathematics, STEM, coding, and instruction following, respectively. While\nwe initially curate PPE with these five benchmarks, it should be noted that any desired verifiable\nbenchmark can be added to the correctness measurement paradigm by repeating the process outlined\nbelow, thereby providing a framework for customization towards specific evaluation needs.\nFor each benchmark, we sample LLM responses for 500 randomly selected prompts, each 32 times,\nfor a total of 16,000 completions. If a benchmark has fewer than 500 prompts, we use all avail-\nable prompts. We choose a large K of 32 to allow models to generate more diverse responses,\ncovering a larger input domain for the human preference proxy and testing greater robustness to\nover-optimization. We note that this sampling strategy actually yields very similar KL-Divergence\nshifts as would be seen in RLHF training methods such as Proximal Policy Optimization (PPO)\n(Gao et al., 2022; Schulman et al., 2017).\nWe repeat this process for four different models: Llama-3-8B-Instruct, Gemma-2-9b-it, Claude-3-\nHaiku, and GPT-40-mini-2024-07-18 (AI@Meta, 2024; Team et al., 2024; Anthropic, 2024; Ope-\nnAI, 2024). Each model samples prompts randomly with different seeds. We reason that different\nmodel response distributions may have different difficulties. For example, an already extremely\nhigh-performing model like GPT-40-mini-2024-07-18 may be more challenging for reward models\nto evaluate correctness.\nWe then score all responses using the benchmark's verification methods. Using the correctness labels\nfor all responses, we discard any rows in which the model got every single response wrong or every\nsingle response right, as it is impossible for the reward model to select a better generation in these\ncases. Additionally, we discard any row where less than 10% or greater than 90% of the responses\nwere correct, with exceptions made for benchmarks with very few valid options. This step helps\navoid vacuously correct responses, such as an LLM randomly guessing the correct multiple-choice\nanswer with completely nonsensical reasoning, as well as prompts that are too easy.\nFrom the remaining data, we randomly sample 128 responses from each model, totaling 512 sam-\nples. If a benchmark is too small and some models have fewer than 128 viable samples, we adjust\nthe sampling accordingly. More details on curation can be found in Appendix A.2.1."}, {"title": "5.2 SCORING", "content": "We score the reward models on the correctness metrics in ways that target a reward model's robust-\nness, granularity, and theoretical roof-line performance."}, {"title": "5.2.1 BEST OF K CURVES", "content": "A best of K curve shows on average how the reward model's selected \"best\" answer's ground truth\nscore changes vs K. When plotted against the ground truth curve, we can observe the gap between\nthe reward model's ability to select the \"best\" answer given a set of K responses, and the \"gold\nstandard\" best score. More formally, let $S_K$ be a size K random sample of responses from a model,\n$g: S_K \\rightarrow {0,1}$ be the ground truth scoring function, and $R : S_K \\rightarrow \\mathbb{R}$ be the reward model proxy\nscore. Then, $\\mathbb{E}_{S_K} [g(\\arg \\max_{s \\in S_K} R(s))]$ is the expected ground truth score of the select response\nby the reward model given K sampled responses. We then sweep across K = 1,..., 32 to obtain a\ncurve.\nThese curves represent how much the reward model can differentiate the LLM's generations whilst\npicking from examples drawn from the same distribution. The simple intuition here is that as K\nincreases, the \"exploration\" of the LLM is expanded, thereby increasing the likelihood that a correct"}, {"title": "5.2.2 AREA UNDER RECEIVER OPERATOR CHARACTERISTICS (ROC) CURVE", "content": "Since the ground truth verification outputs a binary label, we can check each reward model's strength\nas a binary correctness classifier by calculating the area under the ROC curve. We first normalize the\nscores in each row with min-max normalization. Then we calculate the binary classification ROC\ncurve using the normalized scores as \u201cprobabilities\u201d. AUC scores are detailed in Appendix Table 24."}, {"title": "5.2.3 ACCURACY", "content": "Since LLM-as-a-judge cannot easily scale 32-wise judgments, we create a supplemental pairwise\ntask to evaluate correctness preference accuracy compatible with both reward models and LLM-as-\na-judge. For each row of best of K data, we simply sample 5 pairs of responses such that in each\npair, there is one correct response and one incorrect response. Then, after randomizing positions, the\nLLM-as-a-judge picks the preferred response. We then measure the accuracy as the rate in which\nthe correct response is preferred over the incorrect result. The accuracies for reward models are also\ncollected for comparison. All scores are documented in Appendix Table 2."}, {"title": "6 VALIDATING PPE ON POST-RLHF OUTCOMES", "content": "By testing a reward model performance on a benchmark, we hope to glean insight towards down-\nstream performance on an LLM RLHF-ed using a given reward model. To measure how well dif-"}, {"title": "6.1 TRAINING PROCEDURE", "content": "Nine reward models were selected to act as preference labels in a full RLHF training pipeline\nin which the resulting models were evaluated on real human preference. We constrained this ex-\nperiment to nine models for cost reasons- the RLHF and human preference evaluation process is\nexceedingly expensive. We selected popular, newer, and high-performing reward models from Re-\nwardBench. We reason these will be the most difficult reward models to differentiate. We also\nrequire the selected reward models to be general-purpose reward models, and not specifically tuned\nto any single domain or task.\nWe create a training dataset by first including 7,000 prompts sampled from the original 50,000\nhuman preference votes after PII removal, unsafe prompt removal, and de-duplication. We then add\n500 random prompts from MMLU-Pro that are not in PPE, and another 500 prompts from MATH\ntrain set (also mutually exclusive from PPE). For each prompt, we sample 16 responses from the\nbase model, Llama-3.1-8B-Instruct, randomizing the temperature for each generation, drawing from\na triangular distribution (a = 0.0, b = 1.0, c = 1.3) to promote more diverse exploration. This\nprocess yields 8,000 total prompts, each with 16 different responses, totaling 128,000 responses.\nEach reward model then constructs its own preference dataset. First, the reward model gives scores\nfor each of the 16 responses for each prompt. The \u201cchosen\u201d response is set as the maximum scoring\nresponse. The \"rejected\" response is sampled as the rank n response, where n is sampled uniformly."}, {"title": "6.2 EVALUATION ON REAL-WORLD HUMAN PREFERENCE", "content": "We deploy the trained models to Chatbot Arena to undergo blind evaluation from real users. We set\nup a cohort of 13 models which include the trained DPO models as well as Llama-3.1-8B-Instruct,\nLlama-3.1-70b-Instruct, and Llama-3-8B-Instruct. All models used temperature 0.2 (excluding\nLlama-3-8B-Instruct at temperature 0.7). Model pairs were sampled evenly with only each other\nfor battles. Battles were collected over a six day period, from September 10th, 2024 to September\n16th, 2024. In all battles, the receiving user was selected randomly. Additionally, the model names\n(labeled llama-3.1-8b-dpo-test-{1,2..., 9}) were not revealed to the user until after the\nvote was given.\nOverall, 12,190 human preference votes were collected, with an average of 2,032 battles per model,\nand an average of 190 battles per unique model pair. More details on battle statistics and be found in\nAppendix Table 27 of Appendix A.4. The resulting ELO scores are detailed in Table 3. ELO scores\nare calculated using the Bradley Terry model, as proposed in Chiang et al. (2024)."}, {"title": "7 STUDYING CORRELATION WITH DOWNSTREAM PERFORMANCE", "content": "In this section, we analyze how different metrics correlate with post-RLHF human preference scores\n(experimental setup detailed in Section 6.2). Our main results are displayed in Figure 3, which shows\nthe correlations of our offline reward model evaluations against the real-world human-preference\nranking from the crowdsourced platform.\nOn correctness metrics (left plot in Figure 3) we make several observations: (1) Mean across all\ndomains is well correlated across all metrics, but exhibits higher correlation with AUC and Accuracy\nscores. (2) Math is the best individual benchmark domain in terms of predictive power. (3) ROC\nAUC score draws higher correlation across all benchmarks, even on benchmarks that are otherwise\nuncorrelated.\nTurning to the right-hand side of Figure 3, the accuracy of the reward model is the best predictor of\nthe fine-tuned LLM's preference score. Row-wise Pearson Correlation, Confidence Agreement, and\nSeparability show some correlative power to downstream ELO but do not exceed accuracy. Mean-\nwhile, metrics like the Spearman correlation and Kendall correlation have nearly zero correlation\nwith the final ELO achieved by the post-DPO models. One possible reason for this trend is that\naccuracy measures expected preference correctness per preference pair- a much more granular\nscale. Other metrics involve aggregating reward model signals over higher-order preferences, such"}, {"title": "8 LIMITATIONS", "content": ""}, {"title": "8.1 BENCHMARK LEAKAGE", "content": "We acknowledge that benchmark leakage is a very real possibility. We also consider two factors that\nhelp mitigate this issue: (1) The human preference dataset can be updated with new crowdsourced\npreference data at any time. This includes adapting to the most recent prompt and response distribu-\ntions. (2) The correctness preference datasets can be extended to any other benchmark that becomes\nstandard enough to be widely used."}, {"title": "8.2 LIMITS ON TESTING DOWNSTREAM PERFORMANCE", "content": "Unfortunately, end-to-end evaluation of reward models via post-RLHF LLM performance on human\npreference is extremely expensive and time-consuming. As such, we are limited to testing the per-\nformance of nine select models, rather than all reward models. In addition, we use DPO, an offline\nRL algorithm over PPO, an online algorithm, which may play more into over-optimization issues\nor may have different reward model requirements altogether. Therefore, we note that the down-\nstream performance is in the context of the base model and RLHF learning algorithm used, and is\nnot a unilateral measurement of downstream outcomes in all possible cases. Future work should\nexperimentally verify the desired reward model behavior of other RLHF methods."}, {"title": "9 CONCLUSION", "content": "We present PPE, a reward model benchmark explicitly tied to post-RLHF outcomes based on real\nhuman preferences. Our experiment aims to identify which metrics, applied to specific tasks, cor-\nrelate most strongly with downstream performance. We find that across the board, granular mea-\nsurements, such as accuracy, are the best predictors. Additionally, our results suggest that measur-\ning lower bound performance may be more indicative of expected reward model performance in\nthe RLHF pipeline. Overall, our evaluations achieve a 77% Pearson correlation with downstream\nperformance, significantly improving upon previous work. Based on these results, we encourage\nfuture research to further investigate reward model quality and downstream RLHF performance un-\nder broader conditions. We fully open-source dataset creation, experimental validation, and reward\nmodel evaluation code and methods. We anticipate that the high-quality preference evaluation in\nPPE, combined with our post-RLHF analysis of metric predictive power, will significantly advance\nvital research into reward models and RLHF."}, {"title": "A.2.1 SMALL BENCHMARK MODIFICATIONS", "content": "To ensure more natural responses that better reflect real-world use cases, we modified each verifi-\nable benchmark's canonical prompt to encourage Chain of Thought (CoT) thinking (citation). This\napproach both increases the diversity of sampled responses and enhances the task difficulty for the\nhuman preference proxy by incorporating additional signals beyond final answer correctness. The\nspecific instructions for each benchmark are detailed below.\nFor the MATH benchmark, we implemented a new system prompt to facilitate zero-shot CoT be-\nhavior. Additionally, we converted the parsed answer to its symbolic representation and utilized a\nsymbolic solver to evaluate true equality instead of relying on raw string matching. This refinement\nof the correctness signal ensures that trivial answer differences, such as $\\frac{1}{4}$ vs 0.25 or 4i+\u221a5 vs 5+2i,\nare marked as equivalent, with either answer accepted if correct.\nIn practice, we observed that the sampled MBPP-Plus generations from some models were almost all\nidentical. Models also generally failed to follow instructions to \"think step-by-step\" before providing\ntheir final answers, suppressing answer diversity. To address this issue, we prompted the models to\n\u201cwrite comments clearly explaining each part of the code,\" thereby lengthening trajectories and\nyielding greater exploration of the answer spaces. We also observed some ambiguity in MBPP-Plus\nintructions. To mitigate this, we added standard MBPP test cases into the function docstring as\nexamples, and used the more extensive remaining MBPP-Plus test cases as the real tests.\nLastly, for IFEval, we prefixed the prompts with \"It is extremely important that you follow all in-\nstructions exactly.\" This addition emphasizes the necessity of precise instruction following in these\ntasks and ensures that the human preference proxy implicitly recognizes this as a significant evalua-\ntion criterion.\nThe prompt template for MMLU-Pro and GPQA were adaption from Gao et al. (2021)'s Language\nModel Evaluation Harness. The MATH template was generated with the assistance of Anthropic's\nprompt generator.\nThe prompt templates for each benchmark are detailed below. Note that {{var}} indicates a field\nto be filled by prompt data or metadata."}, {"title": "MMLU Prompt Template:", "content": "The following are multiple choice questions (with answers) about {{domain}}. Think step\nby step and then finish your answer with \"the answer is (X)\" where X is the correct\nletter choice.\nQuestion: {{question}}\nOptions:\n{{letter}} {{choice}}\n{{letter}}. {{choice}}\n{{letter}} {{choice}}"}, {"title": "MATH Prompt Template:", "content": "You are a highly skilled mathematician tasked with solving complex math problems.\nYour goal is to provide clear, step-by-step solutions that can be easily parsed and\nevaluated.\nHere is the math problem you need to solve:\n<problem>\n{{MATH_PROBLEM}}\n</problem>\nBox your final answer using LaTeX, for example: $x = \\boxed [Your final numerical or\nalgebraic answer]}$.\nNow, please solve the given math problem and provide your solution in the specified format."}, {"title": "GPQA Prompt Template:", "content": "The following is a {{domain}} multiple choice question. Think step by step and then\nfinish your answer with \"the answer is (X)\" where X is the correct letter choice.\nQuestion: {{question}}\nChoices:\n(A) {{choicel}}\n(B) {{choice2}}\n(C) {{choice3}}\n(D) {{choice4}}"}, {"title": "MBPP-Plus Prompt Template:", "content": "Below will be an instruction to write a python function that accomplishes a task.\nYou will also be given starter code with a function definition and any required imports.\nThink step-by-step, write comments clearly explaining each part of the code, and make sure\nyour code solution is enclosed in markdown ticks (``` [your code here) ```).\n<instruction>\n{{instruction}}\n</instruction>\n<starter_code>\n{{starter_code}}\npass\n</starter_code>"}, {"title": "IFEval Prompt Template:", "content": "It is extemely important that you follow all instructions exactly:\n{{prompt}}"}]}