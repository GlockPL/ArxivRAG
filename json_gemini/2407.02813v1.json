{"title": "Data Overfitting for On-Device Super-Resolution with Dynamic Algorithm and Compiler Co-Design", "authors": ["Gen Li", "Zhihao Shu", "Jie Ji", "Minghai Qin", "Fatemeh Afghah", "Wei Niu", "Xiaolong Ma"], "abstract": "Deep neural networks (DNNs) are frequently employed in a variety of computer vision applications. Nowadays, an emerging trend in the current video distribution system is to take advantage of DNN's overfitting properties to perform video resolution upscaling. By splitting videos into chunks and applying a super-resolution (SR) model to overfit each chunk, this scheme of SR models plus video chunks is able to replace traditional video transmission to enhance video quality and transmission efficiency. However, many models and chunks are needed to guarantee high performance, which leads to tremendous overhead on model switching and memory footprints at the user end. To resolve such problems, we propose a Dynamic Deep neural network assisted by a Content-Aware data processing pipeline to reduce the model number down to one (Dy-DCA), which helps promote performance while conserving computational resources. Additionally, to achieve real acceleration on the user end, we designed a framework that optimizes dynamic features (e.g., dynamic shapes, sizes, and control flow) in Dy-DCA to enable a series of compilation optimizations, including fused code generation, static execution planning, etc. By employing such techniques, our method achieves better PSNR and real-time performance (33 FPS) on an off-the-shelf mobile phone. Meanwhile, assisted by our compilation optimization, we achieve a 1.7x speedup while saving up to 1.61\u00d7 memory consumption.", "sections": [{"title": "1 Introduction", "content": "With the rapid advancement of artificial intelligence, Deep Neural Networks (DNNs) have emerged as a cornerstone technology in various computer vision tasks, revolutionizing the field of image processing [15,22,24,34,61]. Among this, Video Super-Resolution (VSR) [4,10,30,48,51] has garnered increasing attention in recent years. Among the various approaches explored in the realm of VSR, a rising trend is focused on utilizing Super Resolution (SR) models to upscale the resolution of low-resolution (LR) videos instead of directly transmitting high-resolution (HR) videos [38,60]. This emerging representative aims to address the challenge of high bandwidth consumption between servers and clients, which often occurs when directly transmitting HR videos.\nIn the context of video transmission using neural networks, one prevalent ap-proach is the utilization of conventional VSR models [55,56], which are designed to cater to all types of videos. However, to achieve optimal performance, these models typically demand larger parameter sizes, rendering their deployment on mobile devices impractical [23,54,65]. Additionally, there is no assurance that a single model can consistently yield optimal results for all videos. To tackle these challenges, researchers have turned their attention to leveraging the overfitting property of DNNs. Instead of pursuing a one-size-fits-all approach, a novel strat-egy has emerged to utilize a dedicated model to overfit the whole video [59,60]. To enhance the quality of super-resolved video and reduce model size, raw videos are often split into segments based on time or spatial information [33,35,38], al-lowing each model to focus on a smaller segment. However, during the HR video recovery process, the frequent loading and unloading of numerous models can lead to significant overhead at the user end [65]. As shown in Figure 1, the over-head of switching model takes a large portion of total video length and brings more than 50% additional energy consumption, which is impossible to achieve real-time performance and system efficiency.\nTherefore, it is essential to find a video transmission framework capable of meeting the requirements for SR video quality and the resource demands of prevalent edge devices. Several key points are not fully discussed in previ-ous works. (i) The number of models and corresponding model size should be minimized under certain SR video quality requirements. (ii) A corresponding algorithm-compiler-hardware optimization framework that can ensure a real-time system and reasonable on-device resource usage. To fulfill the above expec-tations, this paper proposes Dy-DCA, as shown in figure 2, which consists of a dynamic deep neural network and a fine-grained data preprocessing methodology to achieve better performance while minimizing the model switching overhead on the hardware side. Meanwhile, a compiler-level optimization framework for accelerating dynamic DNNs is designed to achieve real-time inference and save memory consumption.\n(i) To minimize models while maintaining high PSNR. The prior art [33] splits video frames into evenly small patches and regroups them into different chunks according to texture complexity. Although this helps reduce the required model number and increases PSNR, these chunks & model pairs may still lead to I/O and model switching overhead at the user end [6,65]. Thus, in Dy-DCA, we propose a fine-grained data processing method that splits the frames into patches with uneven sizes (e.g., use large patches for monotonous backgrounds and smaller patches for detailed foregrounds), then overfits these data with a designed dynamic neural network, bringing the total transmitted model down to one. The uneven splitting minimizes the total number of patches, thereby reducing both server training effort and user-end I/O overhead. The dynamic neural network has a dynamic routing node and itself follows a tree structure to handle patches of different texture complexity.\n(ii) To ensure real-time performance on device. Although dynamic DNN resolves the system overhead caused by model switching, the dynamic in-put shapes, and control flow in the model poses many challenges for the compiler-hardware-level optimizations (e.g., loop fusion [71], execution order planning [2], etc.). Due to very conservative assumptions and/or expensive analyses at run-time, current approaches [1,26] face difficulties in achieving practical on-device efficiency. In this paper, we finish the last piece of our design for the system by proposing a nuanced approach that optimizes DNN dynamic features, which allows us to close the loop (algorithm, software, hardware) and achieve on-device intelligence. The foundation of our approach is an in-depth study of operators that form the basis for modern DNNs. These operators are classified into sev-eral groups on the basis of how the output shapes relate to the input shapes and values. Under this classification, we introduce a data-flow analysis frame-work dedicated to inferring the shapes and dimensions of intermediate tensors. Subsequently, the outcomes of the analysis are used for enabling a number of compilation optimizations, which include operator fusion and fused code gener-ation, static execution planning, and runtime memory allocation.\nWe summarize our contributions as follows:\nWith the proposed context-aware data pipeline paired with dynamic DNN, patches with appropriate content and dimensions are directed to the suit-able processing path, thus enhancing PSNR while reducing model shifting overhead.\nGiven the existence of dynamic features, we introduce a data-flow analysis framework based on the classified DNN operators, which enables us to infer the shapes and dimensions of intermediate tensors. This design helps reduce the inference latency at the edge while reducing memory consumption.\nBased on the data-flow analysis framework, we implement a series of compiler-level optimizations (e.g. fused code generation, static execution planning,"}, {"title": "2 Algorithm and hardware co-design", "content": "The main promotions of previous arts concentrated on elevating the PSNR and expediting the training process [33,35,38,60]. An important aspect that hasn't received adequate attention is whether these improvements can actually be im-plemented on the devices used by end-users while maintaining the required qual-ity. These methods often require multiple models for high performance and are sensitive to model number [35,38,60]. Also, based on the result in Figure 1, nu-merous models will bring tremendous overhead, which may have a severe impact on the user end. Thus, our goal is to minimize the total number of models while maintaining good performance.\nWe investigate the patterns of these methods and find that there are two video segmentation methods used in their framework. [60] first operates on the temporal axis (e.g., every 5 seconds of content as a segment). As there may be consistent and repeated content in the whole video, different models may learn the same content at different segments, which is not effective. Also, a single frame covers information with large differences in texture complexity (e.g., foreground and background) [3,16,31,53,64], potentially making overfitting more difficult.\n[33] splits video frames into evenly small patches (around 50\u00d750) and re-groups them into different chunks according to texture complexity. Although this method helps reduce the number of models, the small patches may carry similar information, which introduces meaningless computation and I/O over-heads when training or inferring them. In order to reduce the need for multiple models, a better segmentation pattern and model structure need to be proposed."}, {"title": "2.1 Motivations"}, {"title": "2.2 Algorithm level optimization for hardware friendliness", "content": "To address the model switching overhead mentioned above, in this section, we propose a scalable dynamic deep neural network paired with a fine-grained data processing method that reduces the number of models down to one while main-taining good performance and a reasonable model size.\nAs shown in Figure 2, for an input video, we first split those frames into different shape patches. These patches contain various levels of texture complex-ity. (e.g., the minimum informative patches are concentrated in the background section, while the maximum ones are on the foreground object.) To achieve this, we propose a coarse- to fine-grained data processing pipeline to dynamically produce patches of different texture complexity. In our framework, frames will first be split into large patches and evaluated by a general SR model to get the corresponding PSNR value for each patch. Guided by the PSNR values, we can roughly determine the texture complexity of each patch [16,33,35,53,64]. Specif-ically, for the patches where the PSNR value is greater than a certain threshold, as they contain less information, we will not further split those patches. For those lower ones, we will split them into smaller patches and follow the previous evaluation & split step. This forms an iterative processing pipeline to provide patches with different shapes and texture complexity. In this manner, patches with similar complexity features will almost all have the same shape. In Dy-DCA, each execution path is able to learn a similar distribution, which boosts the super-resolution performance. Also, the total number of patches in a video drops a lot compared to the method [33], thus reducing the I/O pressure on the user end.\nTo overfit the above patches with different shapes, we propose a dynamic deep neural network with different paths and a routing node. Specifically, with multiple paths for various resolutions and a learnable routing node to distribute input patches to corresponding paths, our proposed Dy-DCA resolves the sys-tem overhead caused by model switching. Meanwhile, taking advantage of mod-ular SR neural network design [37,63,67], we delicately design the structure of each path to maintain better performance and a reasonable model size for the resource-constrained devices."}, {"title": "2.3 Compiler level optimization to better support algorithm", "content": "In Section 2.2, we utilize a dynamic neural network to resolve the system over-head caused by model switching. However, our approach introduces dynamic in-put in the form of differently shaped patches, which are subsequently distributed across various paths, a process known as routing. This introduces uncertainty into model execution, impeding the compiler's ability to achieve high efficiency.\nThus, a compiler-level optimization method should be proposed to resolve the challenges brought by dynamic input and routing."}, {"title": "DNN Operator Classification."}, {"title": "DNN Operator Classification.", "content": "In dynamic DNNs, each tensor can be cat-egorized into an input tensor (I) and an output tensor (O) at operator (Tl), where l is the operator index. We denote the shape of an input tensor as I(S) and its corresponding value as I(V). Similarly, for an output tensor, we have O(S) and O(V) to represent shape and value, respectively.\nOur key observation is that dynamism (routing, input) brings uncertainty to optimization. For instance, loop fusion [71] relies on knowledge of the iden-tical index space between two loops, typically corresponding to the dimensions of respective input tensors. Likewise, planning execution order [2] to minimize memory usage or organizing memory allocation [44] is hindered in the absence of static knowledge regarding tensor sizes.\nThus, in order to help predict intermediate tensor shape and value, we first group DNN operators into four types: Input Shape Determined Output (I(S) \u2192 O(S,V)), Input Shape Determined Output Shape (I(S) \u2192 O(S)), Input Shape & Value Determined Output Shape (I(S, V) \u2192 O(S)), and Execution Determined Output (Exec \u2192 O(S, V)).\n(i) Input Shape Determined Output: The output tensor shapes are determined by the input tensor shape, but the input values do not impact the output. The representative operators include Shape and Eyelike.\n(ii) Input Shape Determined Output Shape: The output shapes are dependent on the input shapes, much like in the preceding category. The output values, however, depend on all the input values. Examples include Conv, Add, and Pooling. The significance of this category, as compared to the next set of categories, is that if the input shape of this operator is known, compiler opti-mizations (e.g., operator fusion, execution/memory optimizations) are enabled.\n(iii) Input Shape & Value Determined Output Shape: The output values are dependent on the input shapes and all the input values, just like the previous category. The distinction is that only a portion of the input data is used by the output shapes. Examples include Extend and Range.\n(iv) Execution Determined Output: Similar to the previous two cat-egories, the output values rely on the input shapes and all the input values. Examples include Nonzero and If."}, {"title": "Data-flow analysis.", "content": "In this section, we present a novel data-flow analysis methodology to infer the intermediate result tensor shape based on the DNN operator classification discussed above. Such an analysis enables a number of optimizations, including dynamic DNN operator fusion, execution path plan-ning, etc.\nOur main finding is that, for many operators and operator combinations (such as an input shape determined output operator and an input shape determined output shape operator), it is possible to infer the shape of the intermediate result tensor to some extent even without knowing the input tensor shape."}, {"title": "3 Experimental results", "content": "In this section, we evaluate Dy-DCA on multiple videos to show its superiority. Also, to demonstrate the effectiveness of our proposed algorithm and compiler co-design approach, we deploy Dy-DCA on an off-the-shelf mobile phone. The detailed information of dataset and implementation can be found in Section 3.1. We compare our method with current multimodel methods [33,35,60], and the main results are shown in Section 3.2. In Section 3.3, we deploy our model on edge and achieve real-time inference speed, which shows the advantages of our proposed compiler optimization. In Section 3.4, we compare our method with SRVC [27], both of which employ a single model to accommodate all videos."}, {"title": "3.1 Experiment settings"}, {"title": "3.1 Experiment settings", "content": "To evaluate the overfitting performance of our framework, we we adopt the UVG [40] and VSD4K [38] datasets. UVG dataset contains 16 test video sequences and each video has multiple resolutions and bitrates to choose from. In the VSD4K video dataset, there are 6 video categories, and each of the categories contains various video lengths. We set the resolution for HR videos to 1080p and the bitrate at 10bit, and LR videos are generated by bi-cubic interpolation to match different scaling factors.\nWe utilize EDSR [37], and WDSR [63] as our backbone. Both of them use a modular design, which provides flexibility in our design of dynamic neural networks. During training, we split frames into different sizes of patches. The PSNR thresholds we set for splitting are 40 and 30. Specifically, patches with a PSNR greater than 40 will not be split. The next round larger than 30 will not be split. [41] Regarding the hyperparameter configuration of training the SR models, we follow the setting of [37,38,63]. We adopt the Adam optimizer with $\u03b2_1 = 0.9$, $\u03b2_2 = 0.009$, $\u03f5 = 10^{-8}$ and we use L1 loss as a loss function."}, {"title": "3.2 Evaluation on VSD4K and UVG datasets", "content": "In this part, we sample three video categories from VSD4K and UVG and tested them on 45-second videos (VSD4K) and 20-second videos (UVG), respectively. Our results are shown in Table 2. The visual results are shown in Figure 4. We compare with the state-of-the-art DNN-based SR video overfitting methods, such as NAS [60] that splits a video into multiple chunks in advance and overfit each of each chunk with independent SR model, EMT [35] utilizes same training pattern while taking advantage of meta learning to accelerate training, and STDO [33] that uses texture information to a divided video chunk and overfits with multiple models. As we can see, we achieve a higher PSNR across these different videos."}, {"title": "3.3 Deployment on Mobile Devices", "content": "The on-device evaluations are conducted using a OnePlus 11 mobile phone, equipped with a Snapdragon 8 Gen 2 processor [46]. This processor is built on one Cortex-X3 based prime core, four Cortex A-715 and A-710 based perfor-mance core, three Cortex A-510 based efficient core, and paired with a Qualcomm Adreno 740 GPU, delivering superior performance while maintaining efficient power consumption. We tested our model on Alibaba MNN [25] with standard configuration, which utilizes 4 threads and employs FP16 precision to optimize computational efficiency. The inference process is executed 20 times, with the first 5 iterations serving as a warm-up phase to ensure system stability and to negate the effects of start-up transients on the experimental outcomes. The results are then averaged to account for any variations, and measured the perfor-mance under the designated conditions. The main points of our assessment are to methodically analyze model shift overhead, run-time memory usage, power consumption, and image loading overhead.\nMemory and latency result. In Table 3, the memory consumption and end-to-end execution latency are tested by MNN. As Dy-DCA has different in-put patches shape and these patches will be sent to different paths, we randomly chose 50 frames to inference then averaged them. Since NAS, EMT and STDO utilize static DNN, they do not support dynamic input shapes and routing, thus we do not show the results of these methods on our proposed optimiza-tion framework. Overall, with our designed compiler optimization framework, we achieve 1.7\u00d7 speedup and 1.61\u00d7 memory reduction compared with MNN. The average inference speed on mobile GPU is 30ms, which achieves real-time requirement [66].\nOverhead analysis. In Table 4, As our proposed Dy-DCA reduces the model down to one and provides a fine-grained video frame splitting module, we analyze the overhead in terms of model switching and I/O. As STDO also splits video frames into patches, compared with it, we achieve 4\u00d7 saving on model switching overhead and 7\u00d7 saving on I/O."}, {"title": "Overhead analysis."}, {"title": "3.4 More discussion on one-size-fits-all method", "content": "The method named SRVC proposed by [27] can adaptively fit different content with incremental model changes in a single model, which mitigates the model switching overhead. However, when the video content becomes complicated, the super-resolution quality drops dramatically. In the following Table 5, the inter-45s and game-45s have richer information. Compared with our method, the per-formance of SRVC drops a lot when using WDSR as the backbone with a scaling factor of \u00d72. Especially for gmae-45s, a 31.03 dB is not acceptable for a high-quality video requirement. Furthermore, our method is based on algorithm and compiler co-design, which is more favorable to practical usage. The operators and data flow can be fully supported by mobile devices."}, {"title": "4 Ablation Study", "content": "We evaluate different number of paths in the network structure of Dy-DCA. We compare the PSNR and the latency tested on mobile GPU for different dynamic paths of Dy-DCA. As shown in the following Table 6, more paths bring higher PSNR by introducing fine-grained data patch groups with more trainable parameters, which sacrifices latency at the user end."}, {"title": "Long video.", "content": "We also test different video lengths to show our design is capa-ble of handling longer and more complex video contents. We select a 2-minute game video in VSD4K [38] and combine the game-45s video and the vlog-45s video together into a 90s-long video (combine-90s). We conduct experiments us-ing WDSR as a backbone with a scaling factor of 4. These two videos are longer and more complex when compared with the videos we show in Table 2 in our paper. As we can see in the following Table 7, the 2-min game video is still maintaining an acceptable PSNR, and the PSNR of combine-90s is close to the average value of overfitting two videos (game-45s and vlog-45s). The results show that our proposed framework can work well even in longer videos with complex contents."}, {"title": "5 Related works", "content": "For Single Image Super-Resolution (SISR) tasks, SRCNN [13] is the pioneer of applying DNN to image super-resolution. Then, followed by FSRCNN [14] and ESPCN [49], both of them make progress in efficiency and performance. After this, with the development of deep neural networks, more and more network backbones are used for SISR tasks. For example, VDSR [29] uses the VGG [50] network as the backbone and adds residual learning to further improve the ef-fectiveness. Similarly, [32,37,63] proposed a SR network using ResNet [22] as a backbone. With the emergence of channel attention mechanisms networks rep-resented by SENet [45], various applications of attention mechanisms poured into the area of image super resolution [11,42,67,68]. Observing the remarkable performance of the transformer architecture in computer vision, as exemplified by [15] in their work on image processing, an increasing number of researchers are now employing various vision transformer models for image super-resolution tasks. [7,36,39]. The Video Super-Resolution (VSR) methods mainly drive from SISR. Some of the works described above that were primarily created for SISR, including EDSR [37] and WDSR [63], all have results on VSR. Several of the recent VSR works perform alignment to calculate optical flow by DNNs in order to estimate the motions between images [4,30,48,51]. However, accurate optical flow may not be easy to compute for videos with occlusion and large motions. Another method to perform alignment is called deformable convolution meth-ods, which is first proposed by [10]. The Deformable convolution (DConv) [10] was first used to deal with geometric transformation in vision tasks because the sampling operation of CNN is fixed. TDAN [52] applies DConv to align the input frames at the feature level, which avoids the two-stage process used in previous optical flow based methods. Other works like DNLN [55] and D3Dnet [62] also apply Dconv in their model to achieve a better alignment performance. However, these models incorporate with DConv may suffer from high computation com-plexity and difficulty for convergence. To increase the robustness of alignment and account for the visual informativeness of each frame, EDVR [56] uses their proposed Pyramid, Cascading and Deformable convolutions (PCD) alignment module and the temporal-spatial attention (TSA) fusion module."}, {"title": "5.1 DNN-based image and video super-resolution"}, {"title": "5.2 Development of Dynamic DNN", "content": "The development of dynamic DNNs is extensive, including recurrent neural net-works (RNNs) and their derivatives, along with instance-specific dynamic mod-els, spatial-oriented dynamic networks, and time-oriented dynamic models [21]. Our motivation for utilizing DNNs in super-resolution stems from their adapt-ability and the comprehensive range of dynamic capabilities they offer, allowing for enhanced performance in reconstructing high-resolution details from low-resolution inputs. This survey [21] highlights numerous studies pertinent to dynamic inference, illustrating the establishment of model aggregations through either cascaded or concurrent configurations and the selective activation of mod-els based on input conditions. Additionally, this advancement underscores the significance of deploying Spiking Neural Networks (SNNs), which conduct data-driven inference through the propagation of pulse signals [18]. This review also underscores several pivotal publications in these domains, such as Dynamic Net-work Surgery [20], Spatially Adaptive Computation Time (SACT) [17], Dynamic Conditional Networks (DCNs) [69], Dynamic Filter Networks [70], Dynamic Convolutional Neural Networks (DCNNs) [9], Dynamic Routing Between Cap-sules [47], Dynamic Skip Connections (DSCs) [19], and Dynamic Time Warping Networks (DTWNs) [5]. The discussion concludes by reflecting on the unresolved challenges in this field and suggesting intriguing paths for future research. These include the development of theories for dynamic networks, the crafting of efficient decision-making, and diversified application exploration in various disciplines."}, {"title": "5.3 Content-Aware DNN", "content": "It is not possible to develop a DNN model that can efficiently handle all web video. To ensure reliability and performance, [60] suggests that the video delivery system take into account employing DNN models to overfit each video chunk. Several livestreaming and video streaming applications [8,12,28,57,58] make use of overfitting property to guarantee great client performance. [28] proposes a live video ingest framework, which adds an online learning module to the original NAS [60] framework to further ensure quality. NEMO [58] selects key frames to apply super-resolution. This greatly reduces the amount of computation on the client sides. CaFM [38] splits a long video into several time-based chunks and design a handcrafted layer along with a joint training technique to reduce the number of SR models and improve performance. EMT [35] proposes to leverage meta-tuning and challenge patches sampling technique to further reduce model size and computation cost. STDO [33] takes spatial information as well as tem-poral information into account to further enhance model performance."}, {"title": "6 Conclusion", "content": "In this paper, we introduce a content-aware dynamic DNN to overfit videos. This design reduces the required model number down to one, thus reducing the model switching overhead at the user end. In order to resolve the challenges brought By using dynamic input patches and routing in dynamic DNN, we propose a data-flow analysis framework to predict the shape and value of intermediate tensor. Subsequently, the outcomes of the analysis are used to enable a number of compilation optimizations, which achieve real-time performance on the edge."}]}