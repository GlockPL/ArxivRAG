{"title": "Al-Compass: A Comprehensive and Effective\nMulti-module Testing Tool for Al Systems", "authors": ["Zhiyu Zhu", "Zhibo Jin", "Hongsheng Hu", "Minhui Xue", "Ruoxi Sun", "Seyit Camtepe", "Praveen Gauravaram", "Huaming Chen"], "abstract": "Al systems, in particular with deep learning techniques, have demonstrated superior performance for various real-world\napplications. Given the need for tailored optimization in specific scenarios, as well as the concerns related to the exploits of subsurface\nvulnerabilities, a more comprehensive and in-depth testing Al system becomes a pivotal topic. We have seen the emergence of testing\ntools in real-world applications that aim to expand testing capabilities. However, they often concentrate on ad-hoc tasks, rendering\nthem unsuitable for simultaneously testing multiple aspects or components. Furthermore, trustworthiness issues arising from\nadversarial attacks and the challenge of interpreting deep learning models pose new challenges for developing more comprehensive\nand in-depth Al system testing tools. In this study, we design and implement a testing tool, AI-COMPASS, to comprehensively and\neffectively evaluate Al systems. The tool extensively assesses multiple measurements towards adversarial robustness, model\ninterpretability, and performs neuron analysis. The feasibility of the proposed testing tool is thoroughly validated across various\nmodalities, including image classification, object detection, and text classification. Extensive experiments demonstrate that\nAI-COMPASS is the state-of-the-art tool for a comprehensive assessment of the robustness and trustworthiness of Al systems. Our\nresearch sheds light on a general solution for Al systems testing landscape.", "sections": [{"title": "INTRODUCTION", "content": "In recent years, the remarkable improvement of deep learn-\ning models have revolutionized the landscape of various\nindustry sectors and application domains, showcasing their\nunparalleled potential in solving complex problems and\ndriving innovation [1]\u2013[8]. The dynamic interplay between\ndata-driven insights and sophisticated model architectures\nhas propelled deep learning to the forefront of modern\ntechnology, enabling groundbreaking advancements across\na myriad of novel applications [9]\u2013[11]. From enhancing\nmedical diagnostics through image analysis to enabling\nautonomous vehicles to navigate and make informed de-\ncisions, the transformative capabilities of deep learning\nmodels have left an indelible mark on society [12]\u2013[15]. To\ncontextualize this transformative power, consider the case\nof natural language processing where models like GPT-3\nhave demonstrated human-level proficiency in generating\ncoherent and contextually relevant text, ushering in a new\nera of interactive and responsive AI systems [16], [17]. Such\nremarkable feats underscore the urgent need for a compre-\nhensive assessment framework that can holistically evalu-\nate the multifaceted dimensions of deep learning models,\ndelving into the intricate interplay of vast datasets, intricate\nmodel architectures, and immense computational resources\nthat underpin their unprecedented success [18]-[20].\nThe comprehensive assessment of deployed machine\nlearning (ML) models, particularly deep learning models,\nis of paramount importance [21]. Such assessments serve\nas a crucial precautionary measure to uncover potential\npitfalls and unanticipated consequences that could arise\nfrom utilizing inadequately evaluated models [22], [23].\nConducting a thorough performance and security evalua-\ntion ensures a nuanced understanding of the models' ca-\npabilities, limitations, and potential biases, empowering in-\nformed decision-making and responsible deployment [24].\nNeglecting a comprehensive assessment when deploying\ndeep learning models can lead to detrimental outcomes.\nBiased predictions, unreliable results, and unexpected be-\nhaviors may emerge, eroding user trust, triggering legal\nand ethical challenges, and compromising the models' real-\nworld performance [25]. Real-world examples vividly illus-\ntrate these perils. In healthcare, deploying a poorly assessed\nAl diagnostic system could endanger patients through mis-\ndiagnoses [26]. For instance, malicious actors may mislead\nthe ML tumor detection system into erroneously classifying\nbenign tumors as malignant ones by introducing imper-\nceptible perturbations to the original medical images. This\nhas the potential to misguide the physician's judgment,\nsubsequently leading to irreversible harm to the patient's\nhealth [27]. While inadequately evaluated autonomous ve-\nhicles might make flawed decisions, resulting in acci-\ndents [28]. For example, attackers can launch attacks on\nautonomous driving systems by introducing imperceptible\nperturbations to traffic signs. By applying imperceptible\nperturbations to stop signs as perceived by human eyes,\nthe ML system may misclassify them as yield signs. This"}, {"title": "", "content": "could lead to severe traffic accidents, posing a threat to user\nsafety [29]. The financial sector is also at risk, as untested\ndeep learning algorithms in market predictions could yield\nsevere economic repercussions [30]. These instances under-\nscore the urgency of a robust assessment framework, which\nis essential for mitigating risks and ensuring the safe and\neffective deployment of deep learning models [31].\nTo achieve a comprehensive assessment of deep models,\nit is imperative to delve into three pivotal dimensions: ad-\nversarial robustness, model explainability, and neuron anal-\nsis [32]\u2013[34]. Adversarial robustness stands as a bulwark,\nensuring consistent performance even amid uncertainties\nand adversarial scenarios, thus fortifying its real-world ap-\nplicability [35]. Meanwhile, model explainability serves as a\nbeacon of transparency, demystifying the decision-making\nprocess and fostering trust, especially in contexts where ac-\ncountability is paramount [36]. Simultaneously, the intricate\nrealm of neuron analysis grants us a profound understand-\ning of the model's inner workings, at the level of individual\nneurons, elucidating the pathways of feature extraction and\nrepresentation learning [37]. The convergence of these facets\nnot only empowers a comprehensive evaluation but also\nequips stakeholders with the insights needed to navigate\nthe nuanced landscape of deep learning models, promoting\ninformed deployment and harnessing their transformative\npotential across diverse domains [38].\nIn order to enhance the capability for testing Deep\nLearning Systems (DLS) in real-world applications, numeric\ntesting tools have been developed. Taking medical image\nanalysis as an example, DLTK [39], as an open-source DL\ntoolkit, provides a range of tools for testing and validating\nthe quality of DLS, including model evaluation, model\ninterpretation, and model visualization. DLTK provides a\ndetailed diagnostic report for medical images, reducing\nthe risk of misjudgment by explaining the model's be-\nhavior. DeepXplore [40] is an automated white-box testing\nframework for DLS that employs optimization techniques\nsuch as gradient ascent to detect potential failures in the\nsystem. As an effective approach for automated testing\nof deep neural network (DNN)-driven autonomous cars,\nDeepTest [41] designs a test generation framework that\ncombines mutation operators, metamorphic relations, and\nreal-world driving scenarios to generate test cases with\nhigher neuron coverage. However, even though the DLS\ntesting tools are constantly being upgraded, resembling an\narms race in multiple fields as described above, the inherent\nad-hoc, task-oriented nature of existing tools persists as an\nunavoidable limitation, often making them unsuitable for\nfulfilling multi-task testing requirements. For example, the\ntesting objective of DeepXplore is monotonous, and it is\nexclusively applicable to white-box testing, which makes it\nunsuitable as a general-purpose testing tool in a black-box\nenvironment. Furthermore, DeepXplore does not provide an\nexplanation for how a model's defects are detected. Both\nDLTK and DeepTest have limited testing capabilities in ap-\nplication scenarios unrelated to medical image analysis and\nDNN-driven autonomous driving, lacking sufficient tests of\nadversarial robustness or model interpretability. As far as\nwe know, existing testing tools can only conduct individual\nmodule tests on a model's adversarial robustness, inter-\npretability, or neuron analysis, rather than explaining the"}, {"title": "", "content": "relationships among these three aspects. In order to enable\nmultidimensional evaluation and selection of models, we\nare dedicated to integrating these modules for multi-task\ntesting and constructing a comprehensive testing tool. In\naddition, pruning has been proven to facilitate the interpre-\ntation of model decisions and reduce the occurrence of over-\nfitting during adversarial sample training [42], [43]. For the\nfirst time, we introduce an approach to neuron analysis with\npruning techniques, thereby exploring potential connections\namong the modules.\nIn this paper, we propose AI-COMPASS, a comprehen-\nsive and effective multi-module testing tool for DLS. Specif-\nically, combined with the basic utility module including\nindicator evaluation and mutation operations [44], [45],\nfor the first time, we design modules for adversarial ro-\nbustness, model interpretability and neuron analysis, to\nextensively evaluate the performance of DLS [46]. Through\na thorough validation involving 6 deep learning models\nacross 3 datasets, we demonstrate that AI-COMPASS is ca-\npable of testing image classification, object detection, and\ntext classification tasks in DLS. Compared to existing DLS\ntesting tools, AI-COMPASS not only conducts fundamental\nDLS testing but also delivers precise evaluations of model\nrobustness against adversarial attacks. Furthermore, it pro-\nvides trustworthy model interpretability reports, including\na quantified assessment of the tested model's interpretabil-\nity, along with attributional result charts for illustration.\nThe main contributions of this paper are as follows:\n\u2022 We present a comprehensive and effective frame-\nwork, AI-COMPASS, for automatically testing the\nquality of DLS. Specifically, combined with the basic\nutility module, for the first time, we design mod-\nules for adversarial robustness, model interpretabil-\nity, and neuron analysis, making a significant step\ntowards building robust and trustworthy DLS.\n\u2022 Inspired by the pruning algorithm, we conduct an\nin-depth analysis of neural network redundancy. We\ncomprehensively investigate the changes in adver-\nsarial robustness and model interpretability resulting\nfrom neuron pruning approach, thus providing valu-\nable insights for model architecture optimization.\n\u2022 We demonstrate that our AI-COMPASS can be effec-\ntively applied for multi-modal scenarios. The testing\nresults in image classification, text classification, and\nobject detection tasks verify the high scalability of\nour AI-COMPASS and solve the ad-hoc problem in\nexisting testing tools.\n\u2022 We have conducted extensive experiments and gen-\nerated detailed test reports to demonstrate the supe-\nriority of our AI-COMPASS in testing DLS.\n\u2022 The code is released for future research and enhance-\nments by scholars and industry professionals.\nThis study extends our previous conference paper [46].\nIn Section 2, we provide an overview of related work on\ntesting frameworks to afford readers a more comprehen-\nsive understanding of the field. Section 3 introduces the\npreparatory background, furnishing foundational knowl-\nedge regarding adversarial attacks, model interpretability,\nand pruning algorithms. In Section 4, we expand the con-\nceptual diagram of ML-compass [46] to assist readers in"}, {"title": "RELATED WORK", "content": "In this section, we introduce the existing testing tools for\nassessing deep learning models. Based on their functional-\nities, existing DLS testing tools could be categorized into\nthree groups, focusing on adversarial robustness, model\ninterpretability, and neuron analysis, separately.\nTesting tools for adversarial robustness. Adversarial at-\ntacks refer to malicious attempts by adversaries to introduce\nsubtle yet meaningful perturbations to input data, with the\naim of inducing misclassification or erroneous predictions\nfrom the model. Currently, mainstream adversarial attacks\ncan be categorized into white-box attacks and black-box\nattacks. In a white-box setting, relevant information such\nas the structure and parameters of the target model are\ntransparent. Leveraging this characteristic, white-box attack\nalgorithms can generate high-quality adversarial examples"}, {"title": "", "content": "to assess the target model's resilience against various types\nof attacks. Therefore, white-box attacks serve as an ideal\napproach to evaluate the robustness of models against ad-\nversarial attacks. By continuously challenging and attacking\nmodels, researchers and software developers can uncover\nand address hidden flaws, thereby contributing to the con-\nstruction of more secure and reliable DLS that safeguard\nthe security of users and data. The Adversarial Robustness\nToolbox (ART) [50] is an open-source testing tool dedicated\nto evaluating and enhancing the robustness of deep learning\nmodels. It offers a range of white-box attack algorithms and\ndefense mechanisms tailored for deep learning models, such\nas FGSM [51], DeepFool [52], and C&W [53], which are of\nsignificant relevance for assessing the robustness of systems.\nTextAttack [54], as a testing tool focused on adversarial ex-\nample generation and model robustness testing for natural\nlanguage processing tasks, is suitable for white-box attack\nalgorithms like HotFlip [55] and TextFooler [56], providing\nsupport for security testing in text-based applications.\nCompared to the white-box conditions, model informa-\ntion in the black-box setting is difficult to obtain. Further-\nmore, black-box attack algorithms can be used to simulate\nreal-world security threats and exploit scenarios, which is\ncrucial for enhancing model robustness. Foolbox [57] is\na Python library for generating adversarial samples and\nevaluating models, which supports various black-box attack\nalgorithms and demonstrates excellent performance across\nmultiple deep learning frameworks such as PyTorch, Keras,\nand TensorFlow. TextBugger [58] is a black-box adversarial\nsample generation framework specifically designed for text\nclassification tasks. It can be employed to assess the robust-\nness of deep learning models in text-related tasks. However,\nthe aforementioned DLS testing tools are limited in their\napplicability as they are designed for specific environments\n(white-box or black-box) for adversarial robustness evalua-\ntion. They do not constitute a universal testing tool and are\nincapable of effectively assessing model interpretability.\nTesting tools for Model Interpretability. Performing in-\nterpretability analysis on models is an effective approach\nto understanding the process by which models generate\npredictions for different inputs. Moreover, interpretability\nanalysis of models serves as a tool for elucidating the rea-\nsons behind errors encountered during DLS testing, thereby\nenhancing the trustworthiness of models and constituting a\nvital component of Explainable AI (XAI) research. Presently,\nseveral DL testing tools have been developed to elucidate"}, {"title": "", "content": "the internal workings and decision processes of DLS, aiming\nto ensure system quality and reliability. InterpretDL [49]\nprovides a range of functionalities, including feature im-\nportance analysis, sample explanations, and model visual-\nization, enabling users to analyze model predictions and\ngain insights into the underlying patterns and informa-\ntion embedded within the model. NeuronFair [47]addresses\nreliability and fairness concerns that may arise when ap-\nplying DNNs in sensitive domains, which serves as a\nfairness testing framework for building fairer and more\ntrustworthy DLS. However, InterpretDL and NeuronFair\noften rely on specific DL frameworks, which can be limiting\nfor researchers and practitioners using customized or less\ncommon frameworks. In addition, the interpretability of DL\nmodels is a complex issue. InterpretDL and NeuronFair do\nnot provide a complete explanation for every decision made\nby the model, as the effectiveness of model interpretation\nmay be constrained by model complexity. Similarly, the\naforementioned methods solely assess the interpretability of\nDLS, lacking consideration for adversarial robustness.\nTesting tools for neuron analysis. A testing tool providing\nneural analysis utilizes the properties of neurons, including\nthe parameter of each neuron or the performance of each\nneuron's output under the specific testing input. Many test-\ning tools primarily focus on this aspect. DeepXplore [40] is\nthe first testing tool to propose neural coverage and use the\njoint optimization method with gradient ascent to generate\ntesting examples. DeepGauge [48] uses neuron states to\nsupervise the purpose of multi-granularity testing coverage.\nDeepTest [41] uses transformation operations to get higher\nneural coverage under the specific field of DNN-driven\nautonomous cars. Although neural coverage is a widely\nutilized criterion among testing tools, several studies [59],\n[60] have revealed that relying solely on neural coverage\ncan lead to the generation of misleading testing examples.\nOveremphasizing neural coverage may result in a limited\nnumber of test inputs, potentially overlooking defects in\nDLS. Based on the premise, Jin et al. [61] use shapley\nvalue to define excitable neural, which can be regarded as\nother types of neural analysis. In our research endeavor,\nwe harness the pruning property inherent in DLS, which\nentails the removal of extraneous neurons from the neural\nnetworks through neural analysis. By employing this ap-\nproach, we aim to discern the intricate relationship between\nthe network's robustness and the presence of indispensable\nneurons."}, {"title": "PRELIMINARIES", "content": "In this section, we introduce preliminaries of adversarial\nattacks, interpretability methods, and neuron pruning meth-\nods for evaluating adversarial robustness, model explain-\nability, and model's neuron analysis."}, {"title": "Adversarial attacks", "content": "During the development of ML and DL, DNNs have been\nproved to have state-of-the-art results in massive fields\nsuch as image classification [62], speech recognition [63],\nnatural language processing [64], and recommendation sys-\ntems [65]. As a multiple-layer unsupervised neural network,"}, {"title": "", "content": "the output of DNN is available by layer-to-layer mapping,\nwhich can effectively extract the hidden features from the\ninput space and achieve outstanding performance beyond\nhuman. Besides, several optimal training methods such as\ndropout regularization [66] and mini-batching [67] serve to\nreduce the computation cost of DNNs, allowing the model\nto have a high prediction accuracy and a fast convergence\nspeed. However, the complex decision boundary of DNNs\nraises a threat in software quality [68]. Adversarial samples\nwith human-add perturbations as well as noises existed\nin real-world enable an issue of incorrect model predic-\ntions [69]. For example, a DL software system is easily\nfooled in an image classification task due to its vulnerability\ntowards adversarial samples in the pixel space [70]. The\ninstability of DNNs in the face of adversarial attacks will\ncause serious security problems, especially in some practical\napplications that require low false-positive rates (e.g., au-\ntonomous driving [71] and cancer detection [72]). It is thus\nnecessary and urgent to explore a deep learning testing tool\nthat can measure the adversarial robustness of DLS.\nNowadays adversarial algorithms are a major approach\nto test the robustness of models under attack because\nof the ability to generate promising adversarial samples.\nGenerally, according to the model information that can be\naccessed, adversarial algorithms can be divided into two\ncategories: white-box attacks [51], [53], [73]\u2013[77] and black--\nbox attacks [78]\u2013[86]. In the white-box environment, the\nmodel information(e.g., parameters and structure) can be\nvisited by the attacker. On the contrary, in the black-box\nenvironment it is difficult to obtain model details.\nWhite-box adversarial attacks. Gradient-based white-box\nattacks aim to apply advanced gradient operations to in-\ncrease the success rate of attack. The FGSM [51] algo-\nrithm and some of its derivatives such as I-FGSM [73],\nMI-FGSM [75], TI-FGSM [76], SINI-FGSM [80], etc. have\nbeen proved to have an excellent success rate in the white\nbox model. PGD [74] and C&W [53] algorithms focus on\nrestricting perturbations or using special mathematical con-\nstraints to improve the robustness of adversarial examples.\nIn addition, AdvGAN [77] generates adversarial samples by\nlearning a generator network instead of perturbing input\nsamples directly.\nBlack-box adversarial attacks. As query-based algorithms\nin black-box attacks, QEBA [78] and ZOO [79] rely on small\nbatches of queries to obtain model information to train\nadversarial samples. While SSA [84], DIM [81], PIM [82],\nRAP [87] and NAA [83] are trained on a surrogate model\nto test the effectiveness of adversarial samples when trans-\nferred into the target model."}, {"title": "Model interpretability", "content": "Recently, DNNs remain to be difficult to be interpreted due\nto the complex hidden layer parameters and incomprehen-\nsible nonlinear structure. It is currently unclear how deep\nmodels interpret the relationship between their inputs and\noutputs. Exploring the ambiguous decision-making process\nof DNNs is an important task in Explainable Artificial\nIntelligence (XAI) research. For DL software quality testing,\na trustworthy system not only needs high accuracy, but also\nrequires to have easy-to-interpret properties in the process"}, {"title": "", "content": "of obtaining the results [88]. Therefore, the verification of\nmodel interpretability is an important factor for in-depth\nexploration of the eligibility of DL systems.\nTo get the corresponding information for the model\nfeatures and predictions, local approximation methods and\ngradient based methods are two common directions to-\nwards interpreting DNNs. The former attempts to obtain\nan approximate explanation of the complex target model\nthrough a relatively simple and interpretable model, while\nthe latter aims to use the gradient information of the model\nto obtain the specific relationship between the input features\nand the outputs.\nLocal approximation methods. Linear models and decision\ntree models are widely used in local approximation methods\ndue to the high interpretability of these models [89]-[91].\nOther approximation methods add perturbations to training\ndata to obtain the most sensitive part of the inputs with\nrespect to the model outputs [92]-[94].\nGradient based methods. As two early gradient based\nmethods, Grad-CAM [95] and Score-CAM [96] are both class\nactivation mapping (CAM) based methods which aim to\nexplain the relationship between gradient information and\nintermediate layers of DNN feature maps. Saliency Map [97]\n(SM) applies gradient directly to obtain the visualisation of\nthe particular features with respect to the model outputs.\nGuided Backpropagation [98] uses non-negative gradients\nof the model to get the desired explanation. However,\nGuided Backpropagation is poorly interpretable for features\nin negative gradient directions. In addition, only using\ngradient information is limited for current deep models\nwith increasingly complex structures and diverse appli-\ncation scenarios. For example, SM suffers from gradient\nsaturation and interpretation distortions caused by some\nnoise or changes in external conditions.\nTo solve the misinterpretation of gradients in specific\nregions existing in earlier gradient analysis methods, the\nIG [99] attribution algorithm first extends the original\nsimple gradient calculation into a linear gradient integral\nfrom baseline features to input features, improving the\ninterpretability of the model. Introducing prior knowledge\nas a prior probability distribution for feature attribution,\nEG [100] has obtained further interpretability improvement\non the basis of IG. BIG [101] is firstly proposed to use\nadversarial attack to determine suitable decision boundaries\nand apply the attribution method based on IG to find\nthe exact information leading to these decision boundaries.\nAGI [102] noted that the IG method must seek a specific\nreference point in the attribution path as a starting point\nfor iteration. In different models, the selection of reference\npoints is complex and unique, which is not conducive to\nthe generalization of IG. Therefore, AGI uses the gradient\ninformation of the adversarial sample to integrate along the\npath with the steepest gradient, so that the contribution\nof all input features can be calculated without selecting a\nreference point."}, {"title": "Algorithm pruning", "content": "The initial purpose of the pruning algorithm was to reduce\nthe computational cost of the DL system. The pruning\nalgorithm means preserving valuable parameters in the"}, {"title": "", "content": "model while removing redundant parameters [103]. Some\nwork [43], [104] proves that connections between model\npruning and robustness exist. From another perspective,\nthe pruning algorithm naturally determines which neuron\nundertakes the work of model decision-making. As many\ntesting tools have claimed, neural coverage can help in-\ncrease the quality of testing examples. In order to gain a\ncomprehensive understanding of DLS, it is recommended\nto employ a pruning algorithm that imposes a stringent\nconstraint on the neurons prior to conducting efficient\nrobustness testing. By implementing pruning techniques,\nwe can alleviate concerns regarding testing methodology\nbias and focus our efforts on identifying and eliminating\nredundant parameters.\nIt is important to note that pruning algorithms can be\ncategorized into two types: those with fine-tuning [105] and\nthose without fine-tuning [106]. Our research specifically\nconcentrates on the design of pruning algorithms without\nfine-tuning. This choice is motivated by the fact that fine-\ntuning alters the original parameters, even if it improves\nperformance. Our objective in pruning is to selectively re-\nmove certain parameters from DLS while preserving others\nentirely.\nIn earlier studies, Hu et al. [107] proposed a pruning\nalgorithm to eliminate neurons with zero activation. Sub-\nsequently, similar pruning algorithms have placed greater\nemphasis on the dynamic performance of DLS, such as\nOBD [108]. OBD employs a second-order performance estima-\ntion to assess the importance of each neuron. Additionally,\nTaylor [109] utilizes Taylor expansion to estimate the con-\ntribution of individual neurons in decision-making. Greg-\n2 [110] applies regularization techniques to constrain the\nestimation of neuron importance, utilizing a clever method\nto obtain relative importance differences instead of directly\ncalculating the Hessian matrix. OBD, Taylor, Greg-2, and\nASL [111] are capable of being executed without the need for\nfine-tuning, and our work will integrate these approaches."}, {"title": "AI-COMPASS STRUCTURE OVERVIEW", "content": "In this section, we provide a general overview of our work,\nan all-in-one comprehensive and effective multi-modal test-\ning tool for DLS. The main components of our framework\nare shown in Figure 1. We aim to develop a framework that\nexhibits excellent testing performance in both image and\ntext input data, catering to the needs of testing across differ-\nent modalities. Specifically, we have designed five modules,\nnamely Basic Metrics, Basic Mutants, Robustness Analysis,\nInterpretability, and Neuron Analysis, to comprehensively\ntest DLS. Within each module, we employ appropriate\nevaluation metrics to obtain the most reasonable assessment\nresults for the corresponding module. Moreover, we intro-\nduce pruning techniques to analyze the redundancy levels\nof model neurons and investigate the potential alterations\nin individual modules. This serves as a crucial foundation\nfor optimizing model structure.\nIt is noteworthy that our framework is an all-in-one\nsolution, serving as a comprehensive, multifunctional, and\ncustomizable DLS testing tool. During the preparation stage,\ndiverse DLS undergo two initial collection layers to collect"}, {"title": "EXPERIMENT SETUP", "content": "The present tool is developed on PyTorch 1.11. All exper-\niments conducted in this study are performed on a server\nrunning Ubuntu 20.04.4, equipped with AMD EPYC 7642\n48-Core Processor, NVIDIA RTX3090 GPU, and 80GB RAM."}, {"title": "Datasets", "content": "In this study, we employed several well-known datasets\nfrom the domains of image classification, object detection,\nand text classification. Specifically, for the image classifi-\ncation task, the CIFAR-100 dataset [114] was utilized. The\nCOCO dataset [115] was employed for the object detection\ndomain. As for the text classification task, we utilized the\nSTT-2 dataset [116]."}, {"title": "Models", "content": "In this experiment, in order to demonstrate the compre-\nhensiveness and effectiveness of our testing framework, we\nconducted three different categories of tasks: image classifi-\ncation, object detection, and text classification, within each\nmodule. Additionally, for each task, we tested two different\nmodels to examine the performance differences between\nthem. We use ResNet-50 [1] and VGG-16 [2] for image clas-\nsification tasks, TextCNN [117] and AB-LSTM [118] for text\nclassification tasks, Faster R-CNN [119] and RetinaNet [120]\nfor object detection tasks."}, {"title": "Metrics", "content": "In our evaluation part, in addition to the basic and com-\nmonly used metrics mentioned in Module 1, such as accu-\nracy, recall, precision, etc., we have also included additional\nmetrics in the extended modules to provide a more compre-\nhensive assessment of DLS performance.\nRegarding the evaluation of model robustness, we uti-\nlized the Attack Success Rate (ASR), which represents the\nproportion of successful adversarial samples in the total\nnumber of attack samples. Thus, it can be used to evaluate\nthe performance of an attack method on a specific model.\nIn terms of evaluating model interpretability, we intro-\nduced the concept of interpretability analysis for individual\ndata samples and multiple data samples. For the former,\nanalysis is conducted by examining the heatmaps returned\nby the evaluation framework, which utilize different colors\nto assess the accuracy of the DLS system in capturing salient\nfeatures. In the case of multiple samples, we employed\nthe Insertion score and Deletion score [113] for evaluation.\nThe Insertion score involves starting with an empty image\nand progressively adding pixels based on their attribution\nscores, beginning with the highest score and moving to-\nwards the lowest. Similarly, the Deletion score is obtained\nby iteratively removing pixels from the original image in\ndescending order of their attribution scores.\nIn the neural analysis of DLS, we employed the pruning\nrate as our evaluation metric, which represents the propor-\ntion of parameters pruned from the model out of the total\nmodel parameters."}, {"title": "Parameter Setting", "content": "In this experiment, apart from the pruning rate, all other\nparameter settings followed the default parameters spec-\nified in the original method. Users have the flexibility to\ncustomize these parameters for their subsequent usage. As\nfor the pruning rate parameter, we set it to 0.35, 0.4, 0.45,\nand 0.5, respectively."}, {"title": "Research questions", "content": "In our experimental study, we aim to investigate and ad-\ndress the following research questions:\n\u2022 RQ1: Does AI-COMPASS effectively integrate each\nmodule so as to provide a comprehensive assessment\nof the model's performance?\n\u2022 RQ2: In addition to image classification tasks, does\nAI-COMPASS meet the test requirements under other\nmodal tasks such as text classification and object\ndetection? Does it overcome the shortcoming of ad-\nhoc in existing testing tools?\n\u2022 RQ3: Combining the Adversarial Robustness and\nModel Interpretability modules, can AI-COMPASS\nuse the pruning method to evaluate model depth\nperformance and give optimization recommenda-\ntions?"}, {"title": "EXPERIMENTAL RESULTS", "content": "In this section, we performed basic utility evaluation, ro-\nbustness evaluation, interpretability analysis and neuron\nanalysis with pruning to verify which model in each module\nshows superior performance."}, {"title": "Basic utility evaluation", "content": "As shown in Table 3, considering all the metrics collectively,\nit can be observed that under various data processing meth-\nods such as the original dataset, label errors, missing data,\nand shuffled data, ResNet-50 slightly outperforms VGG-16\nwith higher accuracy and lower loss values in these scenar-\nios, indicating its superior performance and generalization\ncapability in handling such data.\nHowever, under the data processing method involving\nnoise perturbation, VGG-16 exhibits a slight advantage over"}, {"title": "", "content": "ResNet-50. Although VGG-16 achieves a slightly higher\naccuracy compared to ResNet-50, the difference between\nthe two models is not statistically significant.\nTherefore, taking into account the performance across\ndifferent data processing methods, it can be concluded\nthat ResNet-50 and VGG-16 perform comparably overall,\nbut in most cases, ResNet-50 demonstrates slightly better\nperformance than VGG-16."}, {"title": "Robustness evaluation", "content": "In the experiment of this module, we employed two differ-\nent types of attack methods: white-box attacks and black--\nbox attacks. As shown in Table 4, for white-box attacks,\nVGG-16 demonstrates a lower ASR compared to ResNet-\n50. Therefore, on the dataset used in this experiment, VGG-\n16 exhibits better robustness than ResNet-50. Considering\nthe practical application scenarios of DLS, we incorporated\ntransfer-based black-box attacks to simulate real-world test-\ning conditions. In the black-box attacks, VGG-16 consis-\ntently achieves a lower ASR than ResNet-50, indicating it\nhas better robustness than ResNet-50 in this particular task."}, {"title": "Interpretability analysis", "content": "In the experiments conducted in this module, we initially\nperformed a global assessment of model interpretability\nusing the Insertion Score and Deletion Score. As shown\nin Table 5, we observed that ResNet-50 exhibits relatively\nhigher Insertion Score and lower Deletion Score compared\nto VGG-16, indicating that ResNet-50 possesses better inter-\npretability. Furthermore, based on Table 5, we found that\nthe AGI, BIG, and Saliency Map methods demonstrate rela-\ntively good attribution performance on both ResNet-50 and\nVGG-16. Therefore, we analyzed the heatmaps generated\nby these three methods for further analysis. As shown in\nFigure 2, the white regions in the heatmaps represent the\nfeatures that the model focuses on. From the top-left corner,\nwhich displays the original image, it can be observed that\nResNet-50 exhibits a more concentrated focus on specific\nfeatures compared to VGG-16, thus demonstrating better\ninterpretability."}, {}]}