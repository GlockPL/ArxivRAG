{"title": "Rethinking Diffusion-Based Image Generators for Fundus Fluorescein Angiography Synthesis on Limited Data", "authors": ["Chengzhou Yu", "Huihui Fang", "Hongqiu Wang", "Ting Deng", "Qing Du", "Yanwu Xu", "Weihua Yang"], "abstract": "Fundus imaging is a critical tool in ophthalmology, with different imaging modalities offering unique advantages. For instance, fundus fluorescein angiography (FFA) can accurately identify eye diseases. However, traditional invasive FFA involves the injection of sodium fluorescein, which can cause discomfort and risks. Generating corresponding FFA images from non-invasive fundus images holds significant practical value but also presents challenges. First, limited datasets constrain the performance and effectiveness of models. Second, previous studies have primarily focused on generating FFA for single diseases or single modalities, often resulting in poor performance for patients with various ophthalmic conditions. To address these issues, we propose a novel latent diffusion model-based framework, Diffusion, which introduces a fine-tuning protocol to overcome the challenge of limited medical data and unleash the generative capabilities of diffusion models. Furthermore, we designed a new approach to tackle the challenges of generating across different modalities and disease types. On limited datasets, our framework achieves state-of-the-art results compared to existing methods, offering significant potential to enhance ophthalmic diagnostics and patient care. Our code will be released soon to support further research in this field.", "sections": [{"title": "1 Introduction", "content": "Retinal imaging technology plays a crucial foundational role in the diagnosis, evaluation, and treatment of various retinal diseases [12,19]. Numerous methods are available for the early detection of retinal diseases, such as fluorescein angiography (FA) and fundus photography (FP), which are complementary imaging techniques widely used for detecting and diagnosing retinal conditions. FA provides clear images of the retina and the complex vascular structures of the eye, but it is an invasive procedure requiring the injection of fluorescein dye into the patient's body through an intravenous line or hand. However, this invasive method can lead to a range of adverse reactions, including potential nausea, allergic shock, and even death.\nThe invasiveness of FA has led to the development of non-invasive FP-to-FA synthesis methods. In recent years, Generative Adversarial Networks (GANs) [5] and their variants have been extensively used in this field. For example, Kamran et al [10] proposed a semi-supervised conditional GAN, VTGAN, which combines multi-scale coarse and fine generators to capture the features of color fundus photography (CFP) images across scales and generate realistic FFA images. Additionally, a transformer was introduced in the discriminator to differentiate between healthy and pathological FFA. Fang et al [3] introduced UWAT-GAN, which was the first study to synthesize Ultra-Wide-Field FA (UWF-FA) from Ultra-Wide-Field Scanning Laser Ophthalmoscopy (UWF-SLO), overcoming the limitations of UWF-FA imaging. Meanwhile, Wang et al [21] proposed DDG-GAN, which incorporates the concept of Diffusion into GANs and introduced the Multi-disease Paired Ocular Synthesis (MPOS) dataset for CFP-to-FFA synthesis. Although GAN-based methods are powerful for generation tasks, they face challenges such as training instability and mode collapse. In addition, due to the high resolution of the retinal images, GAN-generated images often contain artifacts, which can limit their effectiveness in clinical diagnosis.\nRecently, diffusion models have emerged as one of the most promising generative models. They consist of two basic processes: a forward process that adds noise and a reverse process that progressively removes noise from the image. In the field of medical imaging, Wu et al [25] first introduced a diffusion-based medical image segmentation model, MedSegDiff, bringing diffusion models into the medical field. Jiang et al [9]. proposed Cola-Diff, which was the first to apply diffusion to MRI synthesis, addressing the problem of generating multi-modal MRI images. Despite these advances, challenges remain. The stochastic nature of the diffusion model generation process contrasts sharply with the determinism required for cross-modal image synthesis. Additionally, the performance of diffusion models decreases as the amount of data decreases, making it challenging to apply diffusion-based models to FA image generation.\nIn this paper, we propose a new framework based on conditional diffusion models, which builds on Stable Diffusion[15], a latent diffusion model that performs the diffusion process in a latent space by encoding images through a Variational Autoencoder (VAE).\nThe main contributions of this paper can be summarized as follows:\nTo the best of our knowledge, our work is the first to introduce diffusion models to the study of FFA synthesis in ophthalmology.\nWe fine-tune a pre-trained Latent Diffusion Model (LDM), which is a simple and resource-efficient approach that overcomes the limitations of small ophthalmic multi-modal imaging data."}, {"title": "2 Methodology", "content": "Our proposed method is illustrated in the Fig.1. We use a Stable Diffusion-based generative model for cross-modal image generation. Specifically, the proposed method is divided into two stages: the first stage involves diffusion training, and the second stage involves fine-tuning the decoder."}, {"title": "2.1 Preliminaries", "content": "Latent Diffusion Model. Latent diffusion models [15] represent a class of diffusion models that operate within the latent space encoded by an autoencoder, formalized as $D(E(\u00b7))$. A prominent example of a latent diffusion model is Stable Diffusion[15], which leverages the autoencoder from Variational Autoencoders (VAE) [11] and a conditional U-Net for noise estimation. Additionally, Stable Diffusion utilizes the CLIP VIT-L/14 [13] text encoder to convert input text prompts into corresponding text embeddings, which serve as conditions for the diffusion process. Similarly, Stable Diffusion V1.5 (SDV1.5) is built upon a latent diffusion model comprising three main components: a VAE [11], a U-Net-based denoising model, and a conditioning module. In text-to-image applications, latent images are initially generated from random initialization and are then processed by the diffusion model in collaboration with the conditioning module to produce new images. In this study, text features are excluded from the conditions of Stable Diffusion, as images are used as the primary driver.\nDuring the training phase, given an image $I \\in R^{H_1 \\times W_1 \\times 3}$ and its associated text condition $C_{embed} \\in R^{D_c}$, the latent representation is\n$z_0 = E(I) \\in R^{H_2 \\times W_2 \\times D_z}$  (1)\nThe latent representation undergoes a diffusion process over T time steps. This process is modeled as a deterministic Gaussian process, eventually reaching $z_T \\sim N(0, I)$. The training objective for Stable Diffusion is encapsulated by the following loss function:\n$L = E_{E(I), C_{embed},\\epsilon \\sim N(0,1), t} [\\| \\epsilon - e_\\theta(z_t, t, C_{embed}) \\|_2^2]$   (2)\nHere, t uniformly sampled from {1, . . ., T }, $e_\\theta$ represents the trainable components of the model, which include a denoising U-Net with Residual Blocks [6] and Transformer Blocks [20] that facilitate self-attention and cross-attention. These components are tasked with processing the noisy latent variable $z_t$ and the embedded conditional embeddings $C_{embed}$. After training, the original latent $z_t$ is reconstructed using a deterministic sampling method, such as the Denoising Diffusion Implicit Models (DDIM) [18]. The decoder D then decodes the latent $z_t$ to generate the final image output. This approach not only preserves the fidelity of the generated images but also ensures that they are contextually consistent with the input auxiliary conditions, demonstrating the effectiveness of integrating conditional processes into generative models."}, {"title": "2.2 Fine-Tuning Protocol", "content": "In the field of text-based image generation, Rombach et al [15] trained a diffusion model on the large-scale image and text dataset LAION-5B [17] and demonstrated an unprecedented quality of image synthesis. Their approach is based on a Latent Diffusion Model (LDM), where the denoising process operates in an efficient latent space, significantly reducing the complexity of learning the mapping. However, in the field of medical imaging, the limited availability of medical datasets has been a major challenge for medical image generation. The performance of diffusion models is directly related to the scale of the dataset, making it a persistent area of exploration for researchers to apply diffusion models effectively to medical image generation and fully realize their generative potential.\nTo address this issue, we fine-tune the pre-trained SDV1.5 model, leveraging it as prior knowledge for the task. This allows the feature representations learned during pre-training to generalize well to medical images through transfer learning, even when medical images differ in content and distribution. The model can adapt more quickly. Compared to training from scratch, using a pre-trained SDV1.5 model as a foundation significantly reduces the computational resources and training time required. Fine-tuning the model on limited medical image data can achieve effective results without the need for large datasets and high computational costs."}, {"title": "2.3 Diffusion Training", "content": "In the first stage, we freeze the VAE and only train the denoising U-Net. To better inject the features of the non-invasive image $X_{source}$, we use a VAE encoder to encode the non-invasive image $X_{source}$ and its corresponding invasive image $Y_{target}$ into the same latent space for training the conditional denoiser $E_{\\theta}(z(X_{source}), z(Y_{target}), t)$. Additionally, we concatenate the non-invasive latent tensors $z(X_{source})$ and its corresponding invasive latent tensors $z(Y_{target})$ along the feature dimension and input them into the denoising U-Net. The input channels of the first layer of the U-Net are then doubled to accommodate the expanded input. To prevent the activation values of the first layer from inflating and to keep the pre-trained structure as stable as possible, we duplicate the weights tensor of the pre-trained first input layer and divide its values by 2."}, {"title": "2.4 Fine Tune Decoder", "content": "Although diffusion models demonstrate strong generative capabilities in many computer vision tasks, they are prone to losing high-frequency information due to the iterative noise added during the forward diffusion process. To address this issue, we fine-tuned the Decoder to enhance the representation of high-frequency information. In the second stage, we freeze the denoising U-Net trained in the first stage while training only the Decoder part of the VAE. The reason for fine-tuning only the Decoder and not the Encoder is that we need to ensure that the paired images are mapped to the same latent space, where the diffusion process takes place.\nDuring the fine-tuning process, we applied multiple loss functions to constrain the Decoder. First, we introduced a reconstruction loss to make the generated $Y_{pred}$ closer to the target $Y_{target}$. In addition, we introduced perceptual loss and adversarial loss to emphasize the details in the image during optimization, ensuring the recovery of high-frequency information and generating finer images. Therefore, the objective function for the second stage is:\n$L = L_{Recon} + \\lambda_{perceptual}.L_{Perceptual} + \\lambda_{adversarial}.L_{Adversarial}$  (3)"}, {"title": "2.5 Image Embedding and Offset Noise", "content": "Image Embedding.Traditional Stable Diffusion designs are tailored for Text2Image tasks. Unlike previous methods, to better inject the features of the Source image into the denoising U-Net, we replaced the text with an image. Specifically, source image features $C_{source} \\in R^{Df}$ are extracted using a pre-trained CLIP visual encoder and input into the cross-attention mechanism of each U-Net block, denoted as CrossAttn(z(s), $C_{source}$). This approach not only ensures the generalization of Source image feature extraction but also accurately retains characteristics such as blood vessels and lesions.\nOffset Noise Strategy. To better guide the model in generating images, we introduced a strategy of offset noise, explicitly injecting the perceptual characteristics of the target modality into the latent space generation process. This helps the generated images to more closely resemble the statistical properties and style of the target modality. Specifically, we extracted the mean and standard deviation of the target invasive images in the Normal Case, and used these statistical features to generate the offset noise, as shown below.\n$Offset_{noise} = \\mu_\\tau + \\sigma_\\tau \\cdot Z$  (4)\nWhere:$\\mu_\\tau$ is the mean of the target image. $\\sigma_\\tau$ is the standard deviation of the target image. Z is a random variable sampled from the standard normal distribution N(0, 1). Therefore, in the end, the noise added at each step of the forward diffusion is\n$Noise = Z + \\lambda \\cdot Offset_{noise}$ (5)\nWhere $\\lambda$ is the scaling factor that controls the offset noise."}, {"title": "3 Experiments and Results", "content": "Data. In our experiment, we used two datasets. The first consists of UWF images collected from a local hospital, including paired UWF-SLO and UWF-FA images, with a resolution of 3900\u00d73072 pixels. The second dataset is the MPOS dataset [21], a publicly available paired CFP-FFA dataset with a resolution of 1920\u00d7991 pixels. Specifically, the composition of the two datasets is shown in Table 1. Additionally, we enhanced vascular clarity through automatic color balancing to improve image quality. We also increased the number of training image pairs by randomly cropping 40 image patches, each 768\u00d7768 pixels, from each original training image. In the experiments, for the SLO2FFA dataset, 70% of each category was randomly selected as the training set, and 30% as the test set. For the MPOS dataset, we followed the original paper's settings for dividing the training and test sets, as detailed in the Table 2.\nImplementation Details. In this study, all algorithms were developed and trained within a PyTorch environment, using four NVIDIA A6000 GPUs for training and testing. For the denoising network in Stage One, the U-Net module was trained for approximately 200,000 steps. In Stage Two, the VAE Decoder was trained for 200 epochs with a batch size of 6. Both models were optimized using the Adam algorithm, with a learning rate of 1e-4.\nEvaluation Metrics. Based on previous research, we employed six evaluation metrics: FID(\u2193) [7], KID(\u2193) [1], LPIPS(\u2193) [27], PSNR(\u2191) [16], SSIM(\u2191) [23], and MS-SSIM(\u2191) [24]. Together, these metrics provide a framework for assessing the overall quality of generated images. FID and KID are key metrics for evaluating the fidelity and realism of generated images. LPIPS reflects the perceptual similarity between real and generated images, with smaller values indicating lower perceptual differences, making the images visually more similar. PSNR quantifies distortion by calculating the mean squared error between images, directly reflecting the quality loss or degradation of generated images compared to real ones. On the other hand, SSIM and MS-SSIM provide a detailed assessment of structural similarity between images from single-scale and multi-scale perspectives, including dimensions such as luminance, contrast, and structural information. All of these metrics are essential for evaluating the effectiveness of image generation models."}, {"title": "3.2 Comparison with SOTA", "content": "We benchmarked our model against several state-of-the-art image generation models, selecting methods from both GAN and diffusion networks for comparison. Due to modality differences between the two datasets, we selected different comparative methods for each. For Dataset1 (SLO2FFA Dataset, based on GAN methods, we chose Pix2pix [8], Pix2pixHD [22], UWAT-GAN [3], and UWAFA-GAN [4]. Pix2pixHD combines a multi-scale generator design with perceptual loss to generate high-resolution, high-quality images. UWAT-GAN is the first method to generate UWF-FA from UWF-SLO using GAN, and UWAFA-GAN is an improved version of it. For diffusion-based methods, we selected LDM [15], ControlNet [26], and Instructpix2pix [2]. For the MPOS dataset, in the GAN-based methods, we chose Pix2pixHD [22], VTGAN [10], Reg-GAN [14], and DDG-GAN [21]. VTGAN is the first GAN-based method to generate FFA from CFP, and DDG-GAN is currently the best method on the MPOS dataset. In the diffusion-based methods, we selected LDM, ControlNet, and Instructpix2pix.\nFor SLO2FFA Dataset, as shown in Tabel 3, our method achieves outstanding performance across most metrics for all categories. Compared to the best GAN-based method, UWAFA-GAN, in the Normal Case, our method reduces FID, KID, and LPIPS by 25.65%, 38.86%, and 28.21%, respectively, while improving PSNR, SSIM, and MS-SSIM by 1.41 dB, 0.0962, and 0.0866, respectively. In the BRVO Case, FID, KID, and LPIPS decrease by 42.18%, 86.63%, and 64.38%, while PSNR, SSIM, and MS-SSIM increase by 2.05 dB, 0.0861, and 0.066, respectively. For the DR Case, FID, KID, and LPIPS decrease by 40.83%, 77.19%, and 62.27%, while PSNR, SSIM, and MS-SSIM improve by 1.7 dB, 0.081, and 0.0802, respectively.\nCompared to the best diffusion-based method, Instructpix2pix, in the Normal Case, our method reduces FID, KID, and LPIPS by 22.16%, 35.82%, and 42.26%, respectively, and increases PSNR, SSIM, and MS-SSIM by 3.16 dB, 0.3269, and 0.1436, respectively. For the BRVO Case, FID, KID, and LPIPS decrease by 42.18%, 86.63%, and 64.38%, while PSNR, SSIM, and MS-SSIM increase by 2.05 dB, 0.0861, and 0.066, respectively. In the DR Case, FID, KID, and LPIPS decrease by 40.83%, 77.19%, and 62.27%, while PSNR, SSIM, and MS-SSIM improve by 1.7 dB, 0.081, and 0.0802, respectively.\nSimilarly, for the MPOS dataset, our method demonstrates superior performance across all five categories. Particularly for lesion generation, as shown in Fig3, our method accurately generates lesion areas and related pathological regions.\nThese results demonstrate that our method performs well not only in Normal Cases but also in cases with diseases, generating images that closely resemble real ones. As shown in the figure, we present pairs of non-invasive and invasive images across different methods and categories. Clearly, our proposed method generates sharper images with more accurate vascular structures and finer details."}, {"title": "3.3 Ablation Study", "content": "To evaluate the effectiveness of different components in our diffusion model, we designed the following ablation experiments (as shown in the Table6), involving four experiments:\n1) Baseline: We chose InstructPix2Pix as our base network due to its inherent advantages.\n2) M1: This incorporates an Image Encoder into the baseline to enhance the feature injection capability for annotated images.\n3) M2: Built upon M1, Offset Noise is introduced to improve the contrast of the generated images.\n4) Ours: Finally, based on M2, the VAE Decoder is fine-tuned to enhance high-frequency information.\nAs shown in the Table5 and Table 6, our proposed method outperforms the baseline in all metrics, demonstrating the feasibility and effectiveness of the approach."}, {"title": "3.4 Clinical Validation of Generated Images", "content": "To evaluate the clinical applicability of our method, we designed a simple disease classification network based on dual-modal images, aiming to use the generated invasive images to assist non-invasive images in diagnosing retinal diseases. The dual-modal image classification network employs ResNet50 [6] as the backbone for feature extraction. Features are extracted separately from invasive and non-invasive images, concatenated, and then fed into a fully connected layer for classification prediction. For the dual-modal image classification network, all images were resized to 512x512 as input, and the network was trained using cross-entropy loss and the Adam optimizer. For the evaluation metrics, we adopted Accuracy (ACC) and Area Under the Curve (AUC). ACC represents the percentage of correctly predicted samples out of the total number of samples. AUC represents the area under the Receiver Operating Characteristic curve (ROC), indicating the classifier's performance across different thresholds.\nSpecifically, we conducted the following experiments. For SLO2FFA Dataset:\n1) Perform disease classification (DR, BRVO) using only UWF-SLO images, which serves as our baseline.\n2) Perform disease classification using UWF-SLO images along with their corresponding real UWF-FA images.\n3) Perform disease classification using UWF-SLO images and their corresponding generated UWF-FA images.\nSimilarly, for the MPOS dataset, we adopted the same setup, except it involved a five-class classification problem (Normal, DR, RVO, AMD, CSC).\nAs shown in the Table7 and Table8, the invasive images generated by our proposed method significantly improve the disease diagnostic performance based on single-modal non-invasive images and achieve performance levels close to those obtained using real invasive images in the dual-modal setting. In contrast, invasive images generated by other methods reduced the diagnostic performance of single-modal non-invasive images. This indicates that the images generated by our model provide additional value by enhancing pathological information, enabling the disease diagnosis algorithm to make more accurate assessments. In addition, the confusion matrix shown in the Fig6indicates that the synthesized invasive image information not only improves the model's accuracy in recognizing negative samples but also enhances the accuracy in identifying positive samples. The results demonstrate that the proposed method achieves superior performance in generating invasive images."}, {"title": "4 Conclusion", "content": "In this paper, we propose an advanced latent diffusion model-based framework, Diffusion, aimed at generating accurate invasive fundus images from non-invasive fundus images. This framework addresses the challenge of leveraging the generative capabilities of diffusion models with limited medical data by designing a fine-tuning protocol. Additionally, a novel diffusion model architecture is introduced to address the uncertainty in diffusion model generation and improve controllability. Extensive experiments demonstrate the effectiveness of the proposed framework, achieving state-of-the-art performance across multiple datasets and providing a new approach for non-invasive-to-invasive image generation."}]}