{"title": "Is Functional Correctness Enough to Evaluate Code Language Models?\nExploring Diversity of Generated Codes", "authors": ["Heejae Chon", "Seonghyeon Lee", "Jinyoung Yeo", "Dongha Lee"], "abstract": "Language models (LMs) have exhibited impressive abilities in generating codes from natural language requirements. In this work, we highlight the diversity of code generated by LMs as a critical criterion for evaluating their code generation capabilities, in addition to functional correctness. Despite its practical implications, there is a lack of studies focused on assessing the diversity of generated code, which overlooks its importance in the development of code LMs. We propose a systematic approach to evaluate the diversity of generated code, utilizing various metrics for inter-code similarity as well as functional correctness. Specifically, we introduce a pairwise code similarity measure that leverages large LMs' capabilities in code understanding and reasoning, demonstrating the highest correlation with human judgment. We extensively investigate the impact of various factors on the quality of generated code, including model sizes, temperatures, training approaches, prompting strategies, and the difficulty of input problems. Our consistent observation of a positive correlation between the test pass score and the inter-code similarity score indicates that current LMs tend to produce functionally correct code with limited diversity.", "sections": [{"title": "1 Introduction", "content": "Language models (LMs) pre-trained on massive-scale code corpora show remarkable performance in generating codes from natural language requirements. Extensive research has explored various methods to enhance the functional correctness of code generated by these models (Austin et al., 2021; Wang et al., 2022; Chen et al., 2021). As a result, recent benchmarks assessing the functional correctness of generated code (Ben Allal et al., 2022) show that 4 out of 5 generated codes pass all test cases, confirming their accuracy (Guo et al., 2024). This success highlights the potential of LMs to generate code from high-level specifications.\nBeyond the significant success of LMs in generating functionally correct codes, the diversity of generated code represents another promising criterion for evaluating the model's code generation abilities, for the following reasons: First of all, the ability to generate diverse, correct solutions using different approaches for a given problem indicates that the model truly understands the essential requirements and can implement the codes in various ways to fulfill them. For instance, incorporating diverse reasoning paths during training significantly enhances the code generation capabilities of LMs (Wang et al., 2024). This aligns with recent studies that emphasize the importance of diverse reasoning paths in enhancing LMs' reasoning capabilities (Zhou et al., 2022; Ho et al., 2022), as illustrated in Figure 1.\nMoreover, code LMs are rapidly finding diverse applications these days, and the diversity of the generated code, a fundamental quality metric, is directly influencing software ecosystems. A report identifies that with the emergence of AI coding assistants, i.e., Copilot,\u00b9 there has been an increasing trend of repetitive code uploads on GitHub (Harding and Kloster, 2024); this trend incurs the degradation of software ecosystems by violating a principle of software development, DRY (Don't Repeat Yourself)."}, {"title": "2 Related Work", "content": "2.1 Code Generation with Language Models\nRecently, there have been notable improvements in code generation tasks due to the great advances in language models (LMs). Austin et al. (2021) and Chen et al. (2021) initially introduced LMs for synthesizing a program and demonstrated their capability by evaluating the functional correctness of generated codes through the unit test. With this evaluation protocol, researchers have developed the open-source models pre-trained on massive code corpora (Roziere et al., 2023; Li et al., 2023; Guo et al., 2024) or designed prompt engineering techniques to elicit their code generation capabilities (Kojima et al., 2022; Chen et al., 2022; Ridnik et al., 2024), verifying their effectiveness on code generation tasks."}, {"title": "2.2 Similarity Measure between Codes", "content": "Predicting the semantic relevance between two codes has been a long-standing research topic in code clone detection tasks (Feng et al., 2020; Lu et al., 2021; Zakeri-Nasrabadi et al., 2023). Especially, Roy and Cordy (2007) designed criteria for code similarity measure with varying levels from matching exact codes to consider the functional equivalence between two syntactically different codes. With these levels, a variety of techniques have been developed to measure code similarity, e.g., token-based (Sajnani et al., 2016), learning-based (Zakeri-Nasrabadi and Parsa, 2022), or hybrid techniques (Perez and Chiba, 2019). Another approach relies on LMs' code understanding capability by prompting them to detect clones between two generated programs (Dou et al., 2023). Building on this line of research, we integrate these similarity measures into a unified framework (Figure 2) and develop an enhanced similarity measure with large LMs while focusing on the criteria from Roy and Cordy (2007) to estimate the diversity of generated codes and inspect them in a multifaceted view."}, {"title": "3 Methods", "content": "In this section, we present a systematic approach to evaluate the code generation ability of LMs in terms of code diversity. First, we introduce several pairwise code similarity measures that focus on different aspects and various levels of code-related concepts within a given code pair. Using these pairwise similarity measures, we then design a metric that efficiently captures the similarity of K generated codes. Finally, we devise a novel metric that jointly considers both code diversity and functional correctness, enabling an investigation into the trade-off between these two aspects."}, {"title": "3.1 Pairwise Code Similarity Measurement", "content": "We present three different approaches for modeling the proximity between two pieces of codes, capturing various levels of code characteristics, such as lexical, syntactical, semantical, and logical similarities."}, {"title": "3.1.1 Token-based similarity", "content": "The first approach to measuring pairwise code similarity is to use off-the-shelf clone code detectors that mainly rely on lexical similarity between codes. In this work, we employ a popular open-source detector, SourcererCC (Sajnani et al., 2016), which determines if two pieces of code are the same by primarily considering their token-based proximity. Notably, to effectively handle the presence of comments, which frequently appear within code but do not affect its functionality, SourcererCC automatically detects the comments and removes them before comparing the two codes. We use its binary predictions (i.e., 0 for non-copy code, 1 for copy code) with a pre-specified threshold as the measure of token-based similarity."}, {"title": "3.1.2 Embedding-based similarity", "content": "As a pairwise code similarity measure, another approach that captures the semantics within code using pre-trained code encoders is considered. Specifically, we compute the contextualized embeddings of codes by employing the bi-directional encoder (Reimers and Gurevych, 2019), using their cosine similarity as the measure. To acquire embeddings familiar with code, we use an encoder further trained on code search datasets (Husain et al., 2019). Similar to token-based similarity, comments are removed manually before evaluating code similarity."}, {"title": "3.1.3 Reasoning-based similarity", "content": "To focus on the core architecture of codes (e.g., algorithm and logic) instead of superficial code texts and their semantics, we propose a novel similarity measure that comprehends and contrasts codes with the help of the reasoning ability of large language models (LLMs). Motivated by the pioneering work that commands powerful language models to evaluate the quality of generated texts (Liu et al., 2023b),"}, {"title": "3.2 Similarity Metric for K Sampled Codes", "content": "Based on the pairwise code similarity measures, we define a quantitative metric for evaluating LM's code generation ability in terms of similarity. Similar to conventional metrics for functional correctness (e.g., Pass@K) that focus on the codes gen-"}, {"title": "3.3 Joint Metric for K Sampled Codes: Similarity and Functional Correctness", "content": "To thoroughly evaluate LMs' code generation ability while jointly considering both similarity and functional correctness, we introduce novel metrics to evaluate the K generated codes based on (1) inter-code similarity conditioned on their correctness (denoted by CSim@K) and (2) diversity-weighted accuracy (denoted by DPass@K).\nWe first construct the set CK by examining the functional correctness of each code in C\u043a, \u0456.\u0435.,\n$C'_K = \\{c | I[c \\rightarrow Pass], c \\in C_K \\}$. Then, similar to Sim@K, we define CSim@K by the inter-code similarity for the code set Ck as follows:\n$CSim@K = \\mathbb{E}_{C_k, C_{k'}\\sim C'_K} sim(C_k, C_{k'})$.\nNote that this score is designed to measure the diversity of the codes conditioned on functional correctness. Moreover, to examine how well code LMs can generate correct implementations in diverse ways, we define the diversity-weighted correctness metric by\nDPass@K = Pass@1 \u00b7 (1 \u2013 CSim@K).\nParticularly, the DPass@K metric assigns the weight to the correct codes, proportional to their diversity (i.e., 1 \u2013 CSim@K). We remark that all these metrics above are calculated for a given problem and then averaged over the dataset."}, {"title": "4 Experiments", "content": "In this section, we extensively analyze the implementations generated by the code LMs with the following research questions about code diversity.\n\u2022 RQ1: Can recent code LMs generate sufficiently diverse solutions to specific problems?\n\u2022 RQ2: Is there a correlation between the diversity and correctness of the generated codes?\n\u2022 RQ3: Do advanced code generation strategies enhance both code diversity and correctness?"}, {"title": "4.1 Experimental Setup", "content": "Datasets We utilize two representative datasets that cover diverse coding problems for code generation tasks: APPS (Hendrycks et al., 2021) and HumanEval (Chen et al., 2021). Both datasets are designed to ask the models to implement Python programs based on the requirements written by natural languages. In particular, APPS categorize their problems into three difficulty levels, i.e., Introductory, Interview, and Competition, and we report the score separately to identify whether the diversity of generated codes varies depending on their difficulty. HumanEval consists of basic Python coding problems with the function format where the docstring contains the requirements. Due to the limited computational resources, we randomly sampled 50 problems for each dataset.\nModels We include several recent competitive LM families in our experimental setup to differentiate their behavior in terms of code diversity.\n\u2022 CodeLlama (Rozi\u00e8re et al., 2024) is one of the most prominent code LM families. Especially, we employ a variant, i.e., CodeLlama-P, that is specifically finetuned for Python. To observe the effect on model size, both 7B and 13B models are adopted in our experiments.\n\u2022 StarCoder2 (Lozhkov et al., 2024) is another open-sourced code LM powered by the big-code organizations. They initially show remarkable performance in code generation in terms of functional correctness.\n\u2022 Deepseek-Coder (Guo et al., 2024) is a competitive code LM family that outperforms other code LMs in recent code generation benchmark (Ben Allal et al., 2022).\n\u2022 GPT-3.5-turbo (Brown et al., 2020) is a well-known proprietary LM powered by OpenAI 5."}, {"title": "4.2 Diversity of Generated Codes (RQ1)", "content": "Table 2 reports the Sim@10 scores of various code LMs using four different datasets. We highlight the key observations from the table as follows:\nComparison across different datasets We clearly observe that the similarity of generated code varies significantly depending on the dataset. In particular, in APPS dataset, whole models exhibit similar trends based on the difficulty level of the problems. As the problems become more difficult, the similarity among the generated codes decreases, indicating that most LMs tend to produce more diverse code as the coding problems become hard. As challenging problems lead to uncertain token distributions, LMs produce varied reasoning paths and implementations. For HumanEval dataset, they show higher similarity score compared to APPS dataset. We speculate the reason for this experimental results as HumanEval dataset consists of basic coding problems and code LMs are likely to encounter similar form of coding examples during their pre-training stage.\nAnalysis on similarity measures Our results also demonstrate that the three code similarity measures capture diverse aspects between two codes. For example, CodeLlama-P 13B produces more similar codes than StarCoder2 15B on APPS-Competition with SCC and CodeBERT measures, but we obtain different results with LLM-Eval measure. This implies that LLM-Eval properly captures the latent proximity between the generated codes through reasoning, which is supported by the re-"}, {"title": "4.3 Relationship between Diversity and Functional Correctness (RQ2)", "content": "To further examine the characteristics of generated codes, we study the relationship between the diversity and functional correctness of the generated codes. To this end, we utilize the Pass@K score, which is widely used to measure functional correctness. Pass@K score represents the proportion of solved problems where each problem is regarded as solved if, among K candidate implementations, at least one implementation passes the whole test case.\nWe plot Pass@1, Pass@10, and Sim@10 scores of various code LMs on HumanEval dataset (Figure 4). Overall, Pass@1 scores show a noticeable increase as the model fine-tuned to follow instructions. However, Pass@10 scores for the instruction-tuned models do not increase as Pass@1 scores do. These results imply that while instruction tuning increases the probability of generating correct codes compared to their base models, it also has the unintended consequence of narrowing the distribution of the implementation space, thereby hindering the model's ability to generate creative or diverse solutions for the given problems.\nTo further consider the diversity among correct implementations, we examine the generated implementations with our proposed metrics in Figure 5. For all the models, CSim@10 scores are generally higher than Sim@10 scores, indicating that the correct codes roughly share similar mechanisms. On the other hand, DPass@10 scores are significantly lower, due to the challenge of generating implementations that operate using different correct mechanisms. Interestingly, increasing the temperature to promote diverse implementations allows StarCoder2-I to achieve competitive DPass@10 scores. This indicates that while StarCoder2-I may be less likely to produce correct code on its own, it can generate a wider range of accurate solutions, suggesting its potential as a generator of diverse and accurate code."}, {"title": "4.4 Effect of Temperature and Prompting Strategies (RQ3)", "content": "We evaluate how the decoding and prompting strategies affect the diversity of generated codes. Figure 6 represents Sim@10 and CSim@10 scores with various temperatures. The scores for HumanEval dataset show high CSim@10 scores as the problems are easy to solve. In contrast, the APPS dataset contains more challenging problems, resulting in the generation of more diverse code. When considering functional correctness, many similar codes are filtered out, causing a tendency for the CSim score to be lower. However, all datasets"}, {"title": "4.5 Correlation of Human and Automatic Evaluation on Pairwise Code Similarity", "content": "We recruit 15 graduate students majoring in computer science and ask them to evaluate the similarity between given code pairs by using their domain knowledge. No detailed instructions or guidelines are provided, so the scoring criteria rely solely on the judgment of the human evaluators. Table 4 summarizes the Pearson correlation between pairwise code similarity measures and human evaluation, based on 100 randomly sampled code pairs spanning a diverse range. Our findings indicate that reasoning-based similarity (i.e., LLM-Eval) exhibits the strongest correlation with human evaluators' assessments, with its correlation coefficient up to 25% higher than others. In contrast, embedding-based similarity (i.e., CodeBERT) exhibits no significant correlation with human evaluation (approximately 50%). These results strongly suggest that the proposed detailed instructions for LLM prompting can serve as effective guidelines for evaluating pairwise code similarity with a deep understanding of the coding domain."}, {"title": "5 Conclusion", "content": "In this paper, we explore the code generation ability of LMs by simultaneously evaluating both inter-code similarity (i.e., diversity) and functional correctness. To achieve this, we propose an evaluation framework that integrates diverse pairwise code similarity measures and comprehensive metrics to assess the quality of K sampled codes. Specifically, drawing from software engineering principles, we"}, {"title": "Limitations", "content": "Despite our solid evaluation framework and consistent findings, we identify several limitations in our work and list promising research directions for facilitating research about code diversity. First, our analysis does not cover large-scale code LMs due to insufficient computational resources. As evaluating extremely large models such as CodeLlama 70B requires expensive computational resources, devising an efficient evaluation framework could be a promising future work to enable investigating the behavior of large-scale code LMs and provide valuable insights to the model developers. Additionally, we randomly sample fifty problems from APPS dataset due to the expensive cost of GPT-4. Although our human experiment shows that LLM-Eval is highly correlated with humans even with small samples, scaling up this evaluation into whole examples could be useful to some extent. Lastly, we inevitably adopt a handful of code similarity measures in our experiments even though we extensively search the existing literature about code similarity measures. Therefore, developing a code similarity measure that captures different aspects between codes could be useful and synergize with our proposed framework."}, {"title": "Ethical Consideration", "content": "In our research, we have exclusively utilized open-source language models (LMs) and datasets. This ensures transparency and allows the broader research community to replicate and build upon our work without legal or proprietary constraints. Also, we recruited graduate students in computer science to evaluate code similarity, ensuring their involvement was voluntary and informed. We do not intend harm or overlook significant ethical issues."}, {"title": "A Prompt Details", "content": "A.1 LLM-Eval\nFor evaluating the similarity score by LLM-Eval using GPT-4, we adapt the G-Eval framework (Liu et al., 2023b). The descriptions for each score are defined according to the hierarchy we established. Full details in Table5"}, {"title": "A.2 Planning strategy", "content": "To display prompt strategy's effect on Sim@10, we utilize a part of AlphaCodium 's (Ridnik et al., 2024) framework, self-reflecting, generating possible solutions, and choosing the best solutions. In self-reflecting (Table 6) LLM understands the given description and generates the commentary and explanations in bullet points. In Table 7 based on the self-reflection and explanation of the previous stage, LLM generates possible solutions to the problem. In the final stage(Table 8, the LLM selects the best solution from the possible solutions by considering the problem and self-reflection. This final solution is the role of guidelines to generate appropriate code for the problem. To ensure consistency, the decoding temperature is set to 0.2 at every step."}]}