{"title": "Is Functional Correctness Enough to Evaluate Code Language Models? Exploring Diversity of Generated Codes", "authors": ["Heejae Chon", "Seonghyeon Lee", "Jinyoung Yeo", "Dongha Lee"], "abstract": "Language models (LMs) have exhibited impressive abilities in generating codes from natural language requirements. In this work, we highlight the diversity of code generated by LMs as a critical criterion for evaluating their code generation capabilities, in addition to functional correctness. Despite its practical implications, there is a lack of studies focused on assessing the diversity of generated code, which overlooks its importance in the development of code LMs. We propose a systematic approach to evaluate the diversity of generated code, utilizing various metrics for inter-code similarity as well as functional correctness. Specifically, we introduce a pairwise code similarity measure that leverages large LMs' capabilities in code understanding and reasoning, demonstrating the highest correlation with human judgment. We extensively investigate the impact of various factors on the quality of generated code, including model sizes, temperatures, training approaches, prompting strategies, and the difficulty of input problems. Our consistent observation of a positive correlation between the test pass score and the inter-code similarity score indicates that current LMs tend to produce functionally correct code with limited diversity.", "sections": [{"title": "1 Introduction", "content": "Language models (LMs) pre-trained on massive-scale code corpora show remarkable performance in generating codes from natural language requirements. Extensive research has explored various methods to enhance the functional correctness of code generated by these models (Austin et al., 2021; Wang et al., 2022; Chen et al., 2021). As a result, recent benchmarks assessing the functional correctness of generated code (Ben Allal et al., 2022) show that 4 out of 5 generated codes pass all test cases, confirming their accuracy (Guo et al., 2024). This success highlights the potential of LMs to generate code from high-level specifications."}, {"title": "2 Related Work", "content": "2.1 Code Generation with Language Models\nRecently, there have been notable improvements in code generation tasks due to the great advances in language models (LMs). Austin et al. (2021) and Chen et al. (2021) initially introduced LMs for synthesizing a program and demonstrated their ca-pability by evaluating the functional correctness of generated codes through the unit test. With this evaluation protocol, researchers have devel-oped the open-source models pre-trained on mas-sive code corpora (Roziere et al., 2023; Li et al., 2023; Guo et al., 2024) or designed prompt engi-neering techniques to elicit their code generation capabilities (Kojima et al., 2022; Chen et al., 2022; Ridnik et al., 2024), verifying their effectiveness on code generation tasks."}, {"title": "2.2 Similarity Measure between Codes", "content": "Predicting the semantic relevance between two codes has been a long-standing research topic in code clone detection tasks (Feng et al., 2020; Lu et al., 2021; Zakeri-Nasrabadi et al., 2023). Especially, Roy and Cordy (2007) designed criteria for code similarity measure with varying levels from matching exact codes to consider the functional equivalence between two syntactically different codes. With these levels, a variety of techniques have been developed to measure code similarity, e.g., token-based (Sajnani et al., 2016), learning-based (Zakeri-Nasrabadi and Parsa, 2022), or hy-brid techniques (Perez and Chiba, 2019). Another approach relies on LMs' code understanding capa-bility by prompting them to detect clones between two generated programs (Dou et al., 2023). Build-ing on this line of research, we integrate these simi-larity measures into a unified framework (Figure 2) and develop an enhanced similarity measure with large LMs while focusing on the criteria from Roy and Cordy (2007) to estimate the diversity of gen-erated codes and inspect them in a multifaceted view."}, {"title": "3 Methods", "content": "In this section, we present a systematic approach to evaluate the code generation ability of LMs in terms of code diversity. First, we introduce several pairwise code similarity measures that focus on different aspects and various levels of code-related concepts within a given code pair. Using these pair-wise similarity measures, we then design a metric that efficiently captures the similarity of K generated codes. Finally, we devise a novel metric that jointly considers both code diversity and func-"}, {"title": "3.1 Pairwise Code Similarity Measurement", "content": "We present three different approaches for modeling the proximity between two pieces of codes, capturing various levels of code characteristics, such as lexical, syntactical, semantical, and logical similar-ities.\n3.1.1 Token-based similarity\nThe first approach to measuring pairwise code sim-ilarity is to use off-the-shelf clone code detectors that mainly rely on lexical similarity between codes. In this work, we employ a popular open-source de-tector, SourcererCC (Sajnani et al., 2016), which determines if two pieces of code are the same by primarily considering their token-based proximity. Notably, to effectively handle the presence of com-ments, which frequently appear within code but do not affect its functionality, SourcererCC auto-matically detects the comments and removes them before comparing the two codes. We use its binary predictions (i.e., 0 for non-copy code, 1 for copy code) with a pre-specified threshold as the measure of token-based similarity.\n3.1.2 Embedding-based similarity\nAs a pairwise code similarity measure, another ap-proach that captures the semantics within code us-ing pre-trained code encoders is considered. Specif-ically, we compute the contextualized embeddings of codes by employing the bi-directional encoder (Reimers and Gurevych, 2019), using their co-sine similarity as the measure. To acquire em-beddings familiar with code, we use an encoder further trained on code search datasets (Husain et al., 2019). Similar to token-based similarity, comments are removed manually before evaluating code similarity.\n3.1.3 Reasoning-based similarity\nTo focus on the core architecture of codes (e.g., al-gorithm and logic) instead of superficial code texts and their semantics, we propose a novel similarity measure that comprehends and contrasts codes with the help of the reasoning ability of large language models (LLMs). Motivated by the pioneering work that commands powerful language models to evalu-ate the quality of generated texts (Liu et al., 2023b),"}, {"title": "3.2 Similarity Metric for K Sampled Codes", "content": "Based on the pairwise code similarity measures, we define a quantitative metric for evaluating LM's code generation ability in terms of similarity. Similar to conventional metrics for functional correct-ness (e.g., Pass@K) that focus on the codes gen-"}, {"title": "3.3 Joint Metric for K Sampled Codes: Similarity and Functional Correctness", "content": "To thoroughly evaluate LMs' code generation abil-ity while jointly considering both similarity and functional correctness, we introduce novel met-rics to evaluate the K generated codes based on (1) inter-code similarity conditioned on their cor-rectness (denoted by CSim@K) and (2) diversity-weighted accuracy (denoted by DPass@K).\nWe first construct the set CK by examining the functional correctness of each code in C\u043a, \u0456.\u0435., CK = {c|I[c \u2192 Pass],c \u2208 CK}. Then, similar to Sim@K, we define CSim@K by the inter-code similarity for the code set Ck as follows:\nCSim@K = E sim(Ck, Ck').\nCkCk/~CK\n(2)\nNote that this score is designed to measure the diversity of the codes conditioned on functional correctness. Moreover, to examine how well code LMs can generate correct implementations in di-verse ways, we define the diversity-weighted cor-rectness metric by\nDPass@K = Pass@1 \u00b7 (1 \u2013 CSim@K).\n(3)\nParticularly, the DPass@K metric assigns the weight to the correct codes, proportional to their diversity (i.e., 1 \u2013 CSim@K). We remark that all these metrics above are calculated for a given prob-lem and then averaged over the dataset."}, {"title": "4 Experiments", "content": "In this section, we extensively analyze the imple-mentations generated by the code LMs with the following research questions about code diversity.\n\u2022 RQ1: Can recent code LMs generate sufficiently diverse solutions to specific problems?\n\u2022 RQ2: Is there a correlation between the diversity and correctness of the generated codes?\n\u2022 RQ3: Do advanced code generation strategies enhance both code diversity and correctness?\n4.1 Experimental Setup\nDatasets We utilize two representative datasets that cover diverse coding problems for code gen-eration tasks: APPS (Hendrycks et al., 2021) and HumanEval (Chen et al., 2021). Both datasets are designed to ask the models to implement Python programs based on the requirements written by nat-ural languages. In particular, APPS categorize their problems into three difficulty levels, i.e., Introduc-tory, Interview, and Competition, and we report the score separately to identify whether the diversity of generated codes varies depending on their diffi-culty. HumanEval consists of basic Python coding problems with the function format where the doc-string contains the requirements. Due to the limited computational resources, we randomly sampled 50 problems for each dataset.\nModels We include several recent competitive LM families in our experimental setup to differen-tiate their behavior in terms of code diversity.\n\u2022 CodeLlama (Rozi\u00e8re et al., 2024) is one of the most prominent code LM families. Especially, we employ a variant, i.e., CodeLlama-P, that is specifically finetuned for Python. To observe the effect on model size, both 7B and 13B models are adopted in our experiments.\n\u2022 StarCoder2 (Lozhkov et al., 2024) is another open-sourced code LM powered by the big-code organizations. They initially show remarkable performance in code generation in terms of func-tional correctness.\n\u2022 Deepseek-Coder (Guo et al., 2024) is a com-petitive code LM family that outperforms other code LMs in recent code generation bench-mark (Ben Allal et al., 2022).\n\u2022 GPT-3.5-turbo (Brown et al., 2020) is a well-known proprietary LM powered by OpenAI."}, {"title": "4.2 Diversity of Generated Codes (RQ1)", "content": "Table 2 reports the Sim@10 scores of various code LMs using four different datasets. We highlight the key observations from the table as follows:\nComparison across different datasets We clearly observe that the similarity of generated code varies significantly depending on the dataset. In particular, in APPS dataset, whole models exhibit similar trends based on the difficulty level of the problems. As the problems become more difficult,"}, {"title": "4.3 Relationship between Diversity and Functional Correctness (RQ2)", "content": "To further examine the characteristics of generated codes, we study the relationship between the di-versity and functional correctness of the generated codes. To this end, we utilize the Pass@K score, which is widely used to measure functional correct-ness. Pass@K score represents the proportion of solved problems where each problem is regarded as solved if, among K candidate implementations, at least one implementation passes the whole test case.\nWe plot Pass@1, Pass@10, and Sim@10 scores of various code LMs on HumanEval dataset (Fig-ure 4). Overall, Pass@1 scores show a noticeable increase as the model fine-tuned to follow instruc-tions. However, Pass@10 scores for the instruction-tuned models do not increase as Pass@1 scores do. These results imply that while instruction tuning in-creases the probability of generating correct codes compared to their base models, it also has the unin-"}, {"title": "4.4 Effect of Temperature and Prompting Strategies (RQ3)", "content": "We evaluate how the decoding and prompting strategies affect the diversity of generated codes. Figure 6 represents Sim@10 and CSim@10 scores with various temperatures. The scores for Hu-manEval dataset show high CSim@10 scores as the problems are easy to solve. In contrast, the APPS dataset contains more challenging problems, result-ing in the generation of more diverse code. When considering functional correctness, many similar codes are filtered out, causing a tendency for the CSim score to be lower. However, all datasets"}, {"title": "4.5 Correlation of Human and Automatic Evaluation on Pairwise Code Similarity", "content": "We recruit 15 graduate students majoring in com-puter science and ask them to evaluate the similar-ity between given code pairs by using their domain knowledge. No detailed instructions or guidelines"}, {"title": "5 Conclusion", "content": "In this paper, we explore the code generation ability of LMs by simultaneously evaluating both inter-code similarity (i.e., diversity) and functional cor-rectness. To achieve this, we propose an evaluation framework that integrates diverse pairwise code similarity measures and comprehensive metrics to assess the quality of K sampled codes. Specifically, drawing from software engineering principles, we"}, {"title": "Limitations", "content": "Despite our solid evaluation framework and con-sistent findings, we identify several limitations in our work and list promising research directions for facilitating research about code diversity. First, our analysis does not cover large-scale code LMs due to insufficient computational resources. As evalu-ating extremely large models such as CodeLlama 70B requires expensive computational resources, devising an efficient evaluation framework could be a promising future work to enable investigating the behavior of large-scale code LMs and provide valuable insights to the model developers. Addi-tionally, we randomly sample fifty problems from APPS dataset due to the expensive cost of GPT-4. Although our human experiment shows that LLM-Eval is highly correlated with humans even with small samples, scaling up this evaluation into whole examples could be useful to some extent. Lastly, we inevitably adopt a handful of code similarity measures in our experiments even though we ex-tensively search the existing literature about code similarity measures. Therefore, developing a code similarity measure that captures different aspects between codes could be useful and synergize with our proposed framework."}, {"title": "Ethical Consideration", "content": "In our research, we have exclusively utilized open-source language models (LMs) and datasets. This ensures transparency and allows the broader re-search community to replicate and build upon our work without legal or proprietary constraints. Also, we recruited graduate students in computer science to evaluate code similarity, ensuring their involve-ment was voluntary and informed. We do not in-tend harm or overlook significant ethical issues."}, {"title": "A Prompt Details", "content": "A.1 LLM-Eval\nFor evaluating the similarity score by LLM-Eval using GPT-4, we adapt the G-Eval framework (Liu et al., 2023b). The descriptions for each score are defined according to the hierarchy we established. Full details in Table5\nA.2 Planning strategy\nTo display prompt strategy's effect on Sim@10, we utilize a part of AlphaCodium 's (Ridnik et al., 2024) framework, self-reflecting, generating pos-sible solutions, and choosing the best solutions. In self-reflecting (Table 6) LLM understands the given description and generates the commentary and explanations in bullet points. In Table 7 based on the self-reflection and explanation of the previ-ous stage, LLM generates possible solutions to the problem. In the final stage(Table 8, the LLM selects the best solution from the possible solutions by con-sidering the problem and self-reflection. This final solution is the role of guidelines to generate appro-priate code for the problem. To ensure consistency, the decoding temperature is set to 0.2 at every step."}]}