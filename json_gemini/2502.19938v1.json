{"title": "FLEXIBLE BIVARIATE \u0392\u0395\u03a4\u0391 \u039c\u0399XTURE MODEL: A PROBABILISTIC APPROACH FOR CLUSTERING COMPLEX DATA STRUCTURES", "authors": ["Yung-Peng Hsu", "Hung-Hsuan Chen"], "abstract": "Clustering is essential in data analysis and machine learning, but traditional algorithms like k-means and Gaussian Mixture Models (GMM) often fail with nonconvex clusters. To address the challenge, we introduce the Flexible Bivariate Beta Mixture Model (FBBMM), which utilizes the flexibility of the bivariate beta distribution to handle diverse and irregular cluster shapes. Using the Expectation Maximization (EM) algorithm and Sequential Least Squares Programming (SLSQP) optimizer for parameter estimation, we validate FBBMM on synthetic and real-world datasets, demonstrating its superior performance in clustering complex data structures, offering a robust solution for big data analytics across various domains. We release the experimental code at https://github.com/\nyung-peng/MBMM-and-FBBMM.", "sections": [{"title": "Introduction", "content": "Clustering is a fundamental task in data analysis and machine learning that aims to group data points into clusters such\nthat the points in the same cluster are more similar than those in other clusters. This unsupervised learning method is\nwidely used in various applications, including image analysis, information retrieval, text analysis, bioinformatics, and\nmany more [1, 2, 3, 4]. Clustering helps uncover the underlying structure of the data, facilitates data summarization,\nand sometimes serves as a preprocessing step for other algorithms [2].\nDespite its widespread use, one of the primary challenges many traditional clustering algorithms face is that they often\nassume that the data points form clusters with convex shapes. For example, centroid-based algorithms like k-means\nand distribution-based models like Gaussian Mixture Models (GMM) typically produce clusters that are hyperspherical\nor ellipsoidal [5]. Although this assumption simplifies the clustering process, it restricts the flexibility of these models\nto handle complex data distributions that do not conform to convex shapes.\nThis convexity constraint can lead to suboptimal clustering results, especially when the data inherently possesses\nnonconvex structures. Examples include data points that form concentric circles, crescent shapes, or other intricate\npatterns. Traditional methods may fail to correctly group these points, leading to less optimal clustering results and\nloss of valuable structural information.\nWe propose the Flexible Bivariate Beta Mixture Model (FBBMM) to address these limitations. Unlike conventional\nmodels, FBBMM leverages the flexibility of the bivariate beta distribution, which can accommodate a wide range of\nshapes, including convex, concave, and other irregular forms. This adaptability is crucial for accurately capturing the\nproper structure of complex datasets.\nThe FBBMM offers several advantages over traditional clustering algorithms. First, versatile cluster shapes: FBBMM\ncan model clusters with various shapes using the bivariate beta distribution, providing a better fit for nonconvex data\nstructures. Second, soft clustering: like GMM, FBBMM assigns a probability to each data point to belong to different"}, {"title": "Related Work", "content": "Clustering algorithms can be categorized into four types: centroid-based, density-based, hierarchical, and distribution-\nbased methods. Each has its strengths and limitations, as discussed below, followed by a comparison with our proposed\nFlexible Bivariate Beta Mixture Model (FBBMM).\nCentroid-based methods like k-means [6] are computationally efficient but assume convex clusters, making them un-\nsuitable for nonconvex data. Density-based methods like DBSCAN [7] identify clusters of arbitrary shapes and are\nrobust to noise but depend heavily on hyperparameter tuning. Hierarchical methods, such as agglomerative cluster-\ning [8], build a tree-like structure and do not require pre-specifying cluster numbers but are computationally expensive\nand struggle with large datasets. Distribution-based models like Gaussian Mixture Models (GMM) [9] handle soft\nclustering but are limited to elliptical cluster shapes. MBMM [5] addresses this by assuming multivariate beta distri-\nbutions, allowing nonconvex clusters but restricting correlations to be positive.\nFBBMM overcomes these limitations by employing the flexible bivariate beta distribution, enabling it to model both\nconvex and nonconvex clusters and handle positive and negative correlations. It supports soft clustering and is gen-\nerative, capable of producing new data points for tasks like data augmentation. Although FBBMM handles bivariate\ndata, this limitation can be mitigated using dimension reduction techniques such as PCA or autoencoders.\nAs shown in Table 1, FBBMM's flexibility in cluster shapes and ability to handle positive and negative correlations\nmake it a more versatile and effective clustering method compared to traditional approaches."}, {"title": "Flexible Bivariate Beta Mixture Model", "content": "The Flexible Bivariate Beta Mixture Model (FBBMM) leverages the flexibility of the bivariate beta distribution to\nmodel clusters with a variety of shapes, addressing the limitations of traditional clustering methods, which often\nassume convex cluster shapes. In this section, we describe the FBBMM in detail, including the PDF of the flexible\nbivariate beta distribution, the FBBMM density function, and the parameter learning process."}, {"title": "Bivariate Beta Distribution", "content": "The definition of the beta distribution is unique. However, the beta distribution is only defined on a univariate variable\nwithin the interval [0, 1] or (0, 1). When the number of variates is greater than one, the definition of the multivariate\nbeta distribution is ambiguous [10, 5]. Eventually, we use the flexible bivariate beta distribution based on the definition"}, {"title": "Generative Process and Probability Density Function of FBBMM", "content": "We introduce the FBBMM from the perspective of a generative process. Figure 2 gives the plate notations of the\nobserved and latent variables of the FBBMM, with the notations listed in Table 2. An observed random variable \u00e6n is\nassumed to be sampled by the following process. First, we sample a latent variable zn from a multinomial distribution\nwith parameters \u03c0 = [\u03c01,...,\u03c0\u03b1]. The latent variable zn represents the cluster ID of the data point \u00e6n. Next, an is\nsampled from the flexible bivariate beta distribution with parameters a\u02dc\u00a8, a\u017e\u00a8, a, an, the four parameters defining\nthe bivariate beta distribution for the cluster zn."}, {"title": "Parameter Learning for FBBMM", "content": "In practice, we only observe $x_1, ..., x_N$, but the other variables $a_{1:C}, a^1_{1:C}, a^2_{1:C}, a^3_{1:C}$, and $\\pi_{1}, ..., \\pi_{c}$ are unknown.\nTo learn the parameters of the FBBMM, we use the Expectation Maximization (EM) algorithm. Our objective is to\nfind the parameters $\\theta$ that maximize the likelihood function:\n\n$L(\\theta) = p(X|\\theta) = \\prod_{n=1}^{N} p(x_{n}|\\theta) = \\prod_{n=1}^{N} \\sum_{c=1}^{C} \\pi_{c}BBe(x_{n}|\\theta_{c}).$\n\nDue to the numerical instability of multiplications when N is large, we take the logarithm of the likelihood function\nby convention to form the log-likelihood."}, {"title": "Experimental Setup", "content": "We preprocess the data such that the value of each feature is normalized: let $x_n = [x_{n,1},...,x_{n,m}]$, each $x_{n,j}$ is\nnormalized below.\n\n$x_{n,j} = 0.01 + \\frac{(x_{n,j} - min(x_{*,j}))(0.99 - 0.01))}{max(x_{*,j}) - min(x_{*,j})}.$\n\nwhere $X_{*,j} = [X_{1,j}, X_{2,j}, ..., X_{N,j}]$, i.e., the jth feature of all instances."}, {"title": "Experiments on the Synthetic Datasets", "content": "The synthetic datasets were generated using scikit-learn to test the characteristics of different clustering algorithms.\nThese datasets consist of five different shapes. Each dataset includes 500 two-dimensional data points.\nFigure 3 compares the clustering results of k-means, MeanShift, DBSCAN, Agglomerative Clustering, GMM,\nMBMM, and FBBMM on five synthetic datasets.\nThe first dataset includes concentric circles. If a point from the outer circle is selected, the most distant data point is\npositioned on the opposite side of the same circle. This characteristic makes the synthetic dataset highly challenging\nfor centroid-based and distribution-based methods to group the entire outer circle into a single cluster. As shown in\nthe first row of Figure 3, density-based algorithms (DBSCAN) and Hierarchical clustering method (Agglomerative\nClustering) and two beta distribution-based models (MBMM and FBBMM) successfully separate the two circles.\nThe second dataset contains two distant 2D Gaussian distributions with small variances in each dimension, and the\nthird distribution has a large variance, located in the middle. Thus, several data points sampled from the third dis-\ntribution are mixed with the first two distributions. Since the middle cluster has a wider spread, the centroid is far\nfrom some points within the same cluster, making certain clustering algorithms, k-means, MeanShift, and DBSCAN,\nmisidentify some data points in the middle cluster as other clusters; details are in the second row of Figure 3.\nThe third and fourth datasets each comprise three 2D Gaussian distributions with isolated means. However, the two\ncovariates are highly correlated: the covariates are negatively correlated for the third dataset and positively correlated\nfor the fourth. As a result, data points are sometimes closer to those generated from other distributions. Thus, k-\nmeans, MeanShift, DBSCAN, and Agglomerative Clustering make errors on some data points. MBMM only handles\ndata points whose covariates are positively correlated [5]. FBBMM and GMM are the only models that handle the two\ndatasets well, as presented in the third and fourth rows of Figure 3.\nFinally, the last dataset includes data points from three 2D Gaussian distributions with distant means and small vari-\nances in each dimension. Therefore, a data point is close to other data points within the same Gaussian distribution\nbut far from others. All clustering algorithms perform well in this ideal case.\nOverall, our proposed FBBMM performs well on all synthetic datasets."}, {"title": "Experimented the Open Datasets", "content": "The open datasets include the wine dataset [12] and the MNIST dataset [13]. The wine dataset contains chemical\nanalysis results of wines grown in the same region of Italy but derived from three different cultivars. There are 178\ninstances with 13 features. The second dataset, the MNIST dataset, comprises 70,000 grayscale images of handwrit-\nten digits (0-9), with each image having 28x28 pixels. The two datasets represent structural data and image data,\nrespectively."}, {"title": "Evaluation Metrics for Open Datasets", "content": "We evaluate clustering results using three metrics: Clustering Accuracy (CA), Adjusted Rand Index (ARI), and Ad-\njusted Mutual Information (AMI).\nClustering Accuracy is calculated as the number of correctly clustered data points divided by the total number of\ndata points. Since clustering results and actual labels may not directly correspond, a mapping is performed before\ncomputing accuracy. For example, assume that we have a dataset with four data points whose labels are [a, a, b, b],\nand a clustering algorithm produces the output with cluster IDs [b, b, a, a]. Despite the mismatch between the cluster\nIDs and the actual labels, the clustering is perfect because all points with the actual label a are grouped in cluster b\nand vise versa. We call a set of lists the identical lists if one list can be transformed into another list by permuting\nthe labels. Thus, clustering accuracy is defined as the maximum accuracy among all identical lists of predicted cluster\nIDs [14].\n\n$CA (y, \\hat{y}) := max_{p(\\hat{y})\\in P(\\hat{y})} \\frac{1}{N} \\sum_{i} I(y_i = \\hat{y_i}).$"}, {"title": "Results on the Open Datasets", "content": "Table 3 compares the clustering performance of FBBMM with six baseline clustering methods on the wine dataset.\nSince each of the six baseline methods can handle datasets with any number of features, we use the entire 13 features\nprovided in the wine dataset. However, because FBBMM only handles datasets with bivariate variables, we use an\nautoencoder to reduce the original feature to two dimensions. As shown, FBBMM outperforms all baseline clustering\nmethods.\nWe also project the dataset from the original 13-dimensional to 2-dimensional dataset using an autoencoder and apply\nbaseline clustering algorithms on the 2-dimensional dataset. In doing so, we ensure a consistent evaluation envi-\nronment that isolates the effects of the dimensionality reduction, enabling us to accurately assess the strengths and\nweaknesses of each method in this specific context.\nTable 4 compares clustering performance on the wine dataset after dimension reduction. Probably because of autoen-\ncoder's ability in extracting key feature combinations, all baseline methods improved. FBBMM still performs the best\nin all three evaluation metrics, demonstrating its superiority in clustering.\nThe MNIST dataset consists of 70,000 grayscale image. Due to the high dimensionality and a convolutional neural\nnetwork (CNN)'s ability to handle images, we use a CNN as the feature extractor, followed by applying an autoencoder\nfor dimension reduction. Since we are dealing with a clustering algorithm, we need to prevent CNN from learning\ninformation from the labels in the MNIST dataset. Thus, we use fashion-MNIST [13] to train a CNN. After training,\nwe remove the last fully connected layer. Then, we pass MNIST to this trained CNN to convert an image into a 1 \u00d7 512\ndimensional vector. Subsequently, we feed this vector into an autoencoder to reduce the features to 2 dimensional.\nTable 5 shows the clustering performance in clustering digits 1 and 7 after the feature reduction. FBBMM, again,\nachieves the best performance in all metrics, demonstrating its effectiveness in handling the digit recognition task."}, {"title": "Discussion and Future Work", "content": "This paper introduces the Flexible Bivariate Beta Mixture Model (FBBMM), a novel probabilistic clustering model\nleveraging the flexibility of the bivariate beta distribution. Experimental results show that FBBMM outperforms popu-\nlar clustering algorithms such as k-means, MeanShift, DBSCAN, Gaussian Mixture Models, and MBMM, particularly\non nonconvex clusters. Its ability to handle a wide range of cluster shapes and correlations makes it highly effective.\nFBBMM offers several advantages. Its use of the beta distribution allows for flexible cluster shapes, capturing complex\nstructures more accurately than traditional models. It supports soft clustering, assigning probabilities to data points\nfor belonging to clusters, which is versatile for overlapping clusters. Additionally, FBBMM is generative, capable of\nproducing new data resembling the original dataset, useful for tasks like data augmentation and simulation.\nHowever, FBBMM has limitations, including higher computational complexity due to iterative parameter estimation.\nFuture work could focus on improving efficiency through parallelization or better optimization strategies, extending\nFBBMM to multivariate data, and enhancing robustness to noise and outliers. Applying FBBMM in diverse domains\nsuch as bioinformatics and image analysis could further validate its versatility and impact."}]}