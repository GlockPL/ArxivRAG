{"title": "FLEXIBLE BIVARIATE \u0392\u0395\u03a4\u0391 \u039c\u0399XTURE MODEL: A PROBABILISTIC APPROACH FOR CLUSTERING COMPLEX DATA STRUCTURES", "authors": ["Yung-Peng Hsu", "Hung-Hsuan Chen"], "abstract": "Clustering is essential in data analysis and machine learning, but traditional algorithms like k-means and Gaussian Mixture Models (GMM) often fail with nonconvex clusters. To address the challenge, we introduce the Flexible Bivariate Beta Mixture Model (FBBMM), which utilizes the flexibility of the bivariate beta distribution to handle diverse and irregular cluster shapes. Using the Expectation Maximization (EM) algorithm and Sequential Least Squares Programming (SLSQP) optimizer for parameter estimation, we validate FBBMM on synthetic and real-world datasets, demonstrating its superior performance in clustering complex data structures, offering a robust solution for big data analytics across various domains. We release the experimental code at https://github.com/\nyung-peng/MBMM-and-FBBMM.", "sections": [{"title": "Introduction", "content": "Clustering is a fundamental task in data analysis and machine learning that aims to group data points into clusters such that the points in the same cluster are more similar than those in other clusters. This unsupervised learning method is widely used in various applications, including image analysis, information retrieval, text analysis, bioinformatics, and many more [1, 2, 3, 4]. Clustering helps uncover the underlying structure of the data, facilitates data summarization, and sometimes serves as a preprocessing step for other algorithms [2].\nDespite its widespread use, one of the primary challenges many traditional clustering algorithms face is that they often assume that the data points form clusters with convex shapes. For example, centroid-based algorithms like k-means and distribution-based models like Gaussian Mixture Models (GMM) typically produce clusters that are hyperspherical or ellipsoidal [5]. Although this assumption simplifies the clustering process, it restricts the flexibility of these models to handle complex data distributions that do not conform to convex shapes.\nThis convexity constraint can lead to suboptimal clustering results, especially when the data inherently possesses nonconvex structures. Examples include data points that form concentric circles, crescent shapes, or other intricate patterns. Traditional methods may fail to correctly group these points, leading to less optimal clustering results and loss of valuable structural information.\nWe propose the Flexible Bivariate Beta Mixture Model (FBBMM) to address these limitations. Unlike conventional models, FBBMM leverages the flexibility of the bivariate beta distribution, which can accommodate a wide range of shapes, including convex, concave, and other irregular forms. This adaptability is crucial for accurately capturing the proper structure of complex datasets.\nThe FBBMM offers several advantages over traditional clustering algorithms. First, versatile cluster shapes: FBBMM can model clusters with various shapes using the bivariate beta distribution, providing a better fit for nonconvex data structures. Second, soft clustering: like GMM, FBBMM assigns a probability to each data point to belong to different"}, {"title": "Related Work", "content": "Clustering algorithms can be categorized into four types: centroid-based, density-based, hierarchical, and distribution-based methods. Each has its strengths and limitations, as discussed below, followed by a comparison with our proposed Flexible Bivariate Beta Mixture Model (FBBMM).\nCentroid-based methods like k-means [6] are computationally efficient but assume convex clusters, making them un-suitable for nonconvex data. Density-based methods like DBSCAN [7] identify clusters of arbitrary shapes and are robust to noise but depend heavily on hyperparameter tuning. Hierarchical methods, such as agglomerative clustering [8], build a tree-like structure and do not require pre-specifying cluster numbers but are computationally expensive and struggle with large datasets. Distribution-based models like Gaussian Mixture Models (GMM) [9] handle soft clustering but are limited to elliptical cluster shapes. MBMM [5] addresses this by assuming multivariate beta distri-butions, allowing nonconvex clusters but restricting correlations to be positive.\nFBBMM overcomes these limitations by employing the flexible bivariate beta distribution, enabling it to model both convex and nonconvex clusters and handle positive and negative correlations. It supports soft clustering and is gen-erative, capable of producing new data points for tasks like data augmentation. Although FBBMM handles bivariate data, this limitation can be mitigated using dimension reduction techniques such as PCA or autoencoders."}, {"title": "Flexible Bivariate Beta Mixture Model", "content": "The Flexible Bivariate Beta Mixture Model (FBBMM) leverages the flexibility of the bivariate beta distribution to model clusters with a variety of shapes, addressing the limitations of traditional clustering methods, which often assume convex cluster shapes. In this section, we describe the FBBMM in detail, including the PDF of the flexible bivariate beta distribution, the FBBMM density function, and the parameter learning process."}, {"title": "Bivariate Beta Distribution", "content": "The definition of the beta distribution is unique. However, the beta distribution is only defined on a univariate variable within the interval [0, 1] or (0, 1). When the number of variates is greater than one, the definition of the multivariate beta distribution is ambiguous [10, 5]. Eventually, we use the flexible bivariate beta distribution based on the definition"}, {"title": "Generative Process and Probability Density Function of FBBMM", "content": "We introduce the FBBMM from the perspective of a generative process. An observed random variable $x_n$ is assumed to be sampled by the following process. First, we sample a latent variable $z_n$ from a multinomial distribution with parameters $\u03c0 = [\u03c0_1,...,\u03c0_C]$. The latent variable $z_n$ represents the cluster ID of the data point $x_n$. Next, $x_n$ is sampled from the flexible bivariate beta distribution with parameters $\u03b1_1^{z_n}, \u03b1_2^{z_n}, \u03b1_3^{z_n}, \u03b1_4^{z_n}$, the four parameters defining the bivariate beta distribution for the cluster $z_n$."}, {"title": "Parameter Learning for FBBMM", "content": "In practice, we only observe $x_1, ..., x_N$, but the other variables $\u03b1_1^{1:C}, \u03b1_2^{1:C}, \u03b1_3^{1:C}, \u03b1_4^{1:C}$, and $\u03c0_{1}, ..., \u03c0_{C}$ are unknown.\nTo learn the parameters of the FBBMM, we use the Expectation Maximization (EM) algorithm. Our objective is to find the parameters $\u03b8$ that maximize the likelihood function:\n\n$L(\u03b8) = p(X|\u03b8) = \\prod_{n=1}^{N} p(x_n|\u03b8) = \\prod_{n=1}^{N} \\sum_{c=1}^{C} \u03c0_cBBe(x_n|\u03b8_c).$\n\nDue to the numerical instability of multiplications when N is large, we take the logarithm of the likelihood function by convention to form the log-likelihood."}, {"title": "Experiments", "content": "This section presents the results of experiments that compare the performance of FBBMM with baseline clustering algorithms on different datasets. The compared methods include k-means, MeanShift, DBSCAN, Agglomerative Clustering, GMM, and MBMM. The experiments were carried out on synthetic and real-world datasets, including a structural dataset and an image dataset."}, {"title": "Experimental Setup", "content": "We preprocess the data such that the value of each feature is normalized: let $x_n = [x_{n,1},...,x_{n,m}]$, each $x_{n,j}$ is normalized below.\n\n$x_{n,j} = 0.01 + \\frac{(x_{n,j} - min(x_{*,j}))(0.99 \u2013 0.01)}{max(x_{*,j}) - min(x_{*,j})}$\n\nwhere $X_{*,j} = [X_{1,j}, X_{2,j}, \u2022 \u2022 \u2022, X_{N,j}]$, i.e., the jth feature of all instances."}, {"title": "Experiments on the Synthetic Datasets", "content": "The synthetic datasets were generated using scikit-learn to test the characteristics of different clustering algorithms.\nThese datasets consist of five different shapes. Each dataset includes 500 two-dimensional data points.\nThe first dataset includes concentric circles. If a point from the outer circle is selected, the most distant data point is positioned on the opposite side of the same circle. This characteristic makes the synthetic dataset highly challenging for centroid-based and distribution-based methods to group the entire outer circle into a single cluster. As shown in the first row of Figure 3, density-based algorithms (DBSCAN) and Hierarchical clustering method (Agglomerative Clustering) and two beta distribution-based models (MBMM and FBBMM) successfully separate the two circles.\nThe second dataset contains two distant 2D Gaussian distributions with small variances in each dimension, and the third distribution has a large variance, located in the middle. Thus, several data points sampled from the third dis-tribution are mixed with the first two distributions. Since the middle cluster has a wider spread, the centroid is far from some points within the same cluster, making certain clustering algorithms, k-means, MeanShift, and DBSCAN, misidentify some data points in the middle cluster as other clusters; details are in the second row of Figure 3.\nThe third and fourth datasets each comprise three 2D Gaussian distributions with isolated means. However, the two covariates are highly correlated: the covariates are negatively correlated for the third dataset and positively correlated for the fourth. As a result, data points are sometimes closer to those generated from other distributions. Thus, k-means, MeanShift, DBSCAN, and Agglomerative Clustering make errors on some data points. MBMM only handles data points whose covariates are positively correlated [5]. FBBMM and GMM are the only models that handle the two datasets well, as presented in the third and fourth rows of Figure 3.\nFinally, the last dataset includes data points from three 2D Gaussian distributions with distant means and small vari-ances in each dimension. Therefore, a data point is close to other data points within the same Gaussian distribution but far from others. All clustering algorithms perform well in this ideal case.\nOverall, our proposed FBBMM performs well on all synthetic datasets."}, {"title": "Experimented the Open Datasets", "content": "The open datasets include the wine dataset [12] and the MNIST dataset [13]. The wine dataset contains chemical analysis results of wines grown in the same region of Italy but derived from three different cultivars. There are 178 instances with 13 features. The second dataset, the MNIST dataset, comprises 70,000 grayscale images of handwrit-ten digits (0-9), with each image having 28x28 pixels. The two datasets represent structural data and image data, respectively."}, {"title": "Evaluation Metrics for Open Datasets", "content": "We evaluate clustering results using three metrics: Clustering Accuracy (CA), Adjusted Rand Index (ARI), and Ad-justed Mutual Information (AMI).\nClustering Accuracy is calculated as the number of correctly clustered data points divided by the total number of data points. Since clustering results and actual labels may not directly correspond, a mapping is performed before computing accuracy. For example, assume that we have a dataset with four data points whose labels are [a, a, b, b], and a clustering algorithm produces the output with cluster IDs [b, b, a, a]. Despite the mismatch between the cluster IDs and the actual labels, the clustering is perfect because all points with the actual label a are grouped in cluster b and vise versa. We call a set of lists the identical lists if one list can be transformed into another list by permuting the labels. Thus, clustering accuracy is defined as the maximum accuracy among all identical lists of predicted cluster IDs [14].\n\n$CA (y, \\hat{y}) :=  \\max_{\\tilde{y} \u2208 P(\\hat{y})} \\frac{1}{N} \\sum_{i=1}^{N} I(y_i = \\tilde{y}_i),$\n\nwhere $y = [y_1,..., y_N]$ is the list of ground-truth labels, $\\hat{y} = [\\hat{y}_1,..., \\hat{y}_N]$ is a list of predicted cluster IDs, $P(\\hat{y})$ returns a set of all identical lists for $\\hat{y}$, $I()$ is an indicator function, and $\\tilde{y} = [\\tilde{y}_1, . . ., \\tilde{y}_N]$ is an identical list of $\\hat{y}$.\nWe also use the Adjusted Rand Index (ARI) and Adjusted Mutual Information (AMI) for evaluation. ARI and AMI are biased toward different types of clustering results: ARI prefers balanced partitions (clusters with similar sizes), and AMI prefers unbalanced partitions [15, 14]."}, {"title": "Results on the Open Datasets", "content": "Table 3 compares the clustering performance of FBBMM with six baseline clustering methods on the wine dataset. Since each of the six baseline methods can handle datasets with any number of features, we use the entire 13 features provided in the wine dataset. However, because FBBMM only handles datasets with bivariate variables, we use an autoencoder to reduce the original feature to two dimensions. As shown, FBBMM outperforms all baseline clustering methods.\nWe also project the dataset from the original 13-dimensional to 2-dimensional dataset using an autoencoder and apply baseline clustering algorithms on the 2-dimensional dataset. In doing so, we ensure a consistent evaluation envi-ronment that isolates the effects of the dimensionality reduction, enabling us to accurately assess the strengths and weaknesses of each method in this specific context.\nTable 4 compares clustering performance on the wine dataset after dimension reduction. Probably because of autoen-coder's ability in extracting key feature combinations, all baseline methods improved. FBBMM still performs the best in all three evaluation metrics, demonstrating its superiority in clustering.\nThe MNIST dataset consists of 70,000 grayscale image. Due to the high dimensionality and a convolutional neural network (CNN)'s ability to handle images, we use a CNN as the feature extractor, followed by applying an autoencoder for dimension reduction. Since we are dealing with a clustering algorithm, we need to prevent CNN from learning information from the labels in the MNIST dataset. Thus, we use fashion-MNIST [13] to train a CNN. After training, we remove the last fully connected layer. Then, we pass MNIST to this trained CNN to convert an image into a 1 \u00d7 512 dimensional vector. Subsequently, we feed this vector into an autoencoder to reduce the features to 2 dimensional.\nTable 5 shows the clustering performance in clustering digits 1 and 7 after the feature reduction. FBBMM, again, achieves the best performance in all metrics, demonstrating its effectiveness in handling the digit recognition task."}, {"title": "Discussion and Future Work", "content": "This paper introduces the Flexible Bivariate Beta Mixture Model (FBBMM), a novel probabilistic clustering model leveraging the flexibility of the bivariate beta distribution. Experimental results show that FBBMM outperforms popu-lar clustering algorithms such as k-means, MeanShift, DBSCAN, Gaussian Mixture Models, and MBMM, particularly on nonconvex clusters. Its ability to handle a wide range of cluster shapes and correlations makes it highly effective.\nFBBMM offers several advantages. Its use of the beta distribution allows for flexible cluster shapes, capturing complex structures more accurately than traditional models. It supports soft clustering, assigning probabilities to data points for belonging to clusters, which is versatile for overlapping clusters. Additionally, FBBMM is generative, capable of producing new data resembling the original dataset, useful for tasks like data augmentation and simulation.\nHowever, FBBMM has limitations, including higher computational complexity due to iterative parameter estimation. Future work could focus on improving efficiency through parallelization or better optimization strategies, extending FBBMM to multivariate data, and enhancing robustness to noise and outliers. Applying FBBMM in diverse domains such as bioinformatics and image analysis could further validate its versatility and impact."}]}