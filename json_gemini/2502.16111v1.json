{"title": "PlanGEN: A Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving", "authors": ["Mihir Parmar", "Xin Liu", "Palash Goyal", "Yanfei Chen", "Long Le", "Swaroop Mishra", "Hossein Mobahi", "Jindong Gu", "Zifeng Wang", "Hootan Nakhost", "Chitta Baral", "Chen-Yu Lee", "Tomas Pfister", "Hamid Palangi"], "abstract": "Recent agent frameworks and inference-time algorithms often struggle with complex planning problems due to limitations in verifying generated plans or reasoning and varying complexity of instances within a single task. Many existing methods for these tasks either perform task-level verification without considering constraints or apply inference-time algorithms without adapting to instance-level complexity. To address these limitations, we propose PlanGEN, a model-agnostic and easily scalable agent framework with three key components: constraint, verification, and selection agents. Specifically, our approach proposes constraint-guided iterative verification to enhance performance of inference-time algorithms\u2013Best of N, Tree-of-Thought, and REBASE. In PlanGEN framework, the selection agent optimizes algorithm choice based on instance complexity, ensuring better adaptability to complex planning problems. Experimental results demonstrate significant improvements over the strongest baseline across multiple benchmarks, achieving state-of-the-art results on NATURAL PLAN (~8%\u2191), OlympiadBench (~4%\u2191), DocFinQA (~7%\u2191), and GPQA (~1%\u2191). Our key finding highlights that constraint-guided iterative verification improves inference-time algorithms, and adaptive selection further boosts performance on complex planning and reasoning problems.", "sections": [{"title": "1. Introduction", "content": "Effective planning is a crucial component for systems designed to solve complex real-world problems (Hao et al., 2023; Jiao et al., 2024; Wang et al., 2025, 2024c; Zhao et al., 2023). Traditional planning approaches, which rely heavily on template-based methods (Guan et al., 2023; Valmeekam et al., 2024; Wang et al., 2024b), often lack generalizability and fail to capture the nuances of real-world tasks. In contrast, natural planning with LLMs offers a more promising direction, aligning better with real-world planning scenarios such as a trip or meeting planning (Zheng et al., 2024). Furthermore, Wang et al. (2025) shows that planning in natural language helps solve practical problems such as code generation. Thus, we aim to enhance LLMs' ability to generate effective natural plans and demonstrate their usefulness in solving downstream reasoning tasks within the scientific and financial domains. For the scope of this study, \u201cplanning\" refers to the ability to decompose tasks and reason strategically to achieve solutions.\nIn recent years, LLM agents have shown impressive abilities to solve complex reasoning problems (Wang et al., 2024a; Xiao et al., 2024; Yao et al., 2023). Orthogonal to this exploration, scaling a search space during inference-time (i.e., test-time scaling) (Snell et al., 2025; Welleck et al., 2024) has gained popularity in tackling difficult problems such as mathematical reasoning (Zhang et al., 2024) and code generation (Wang et al., 2025). Despite the success of these frameworks, we hypothesize that they often struggle with complex planning problems due to the lack of better verification module, and a failure to account for instance-level complexity across single-task. Furthermore, although some initial explorations exist (Bohnet et al., 2024; Lee et al., 2025), effectiveness of these frameworks fornatural planning is under-explored (extended related work is presented in App. A). Motivated by these, we proposed PlanGEN, a model-agnostic, easily scalable, multi-agent framework for effective natural plan generation.\nPlanGEN consists of three specialized agents: constraint agent, verification agent, and selection agent. The constraint agent extracts instance-specific constraints (e.g., budget, concepts, rules, etc.); the verification agent evaluates plan quality and provides a reward score considering the constraints; and the selection agent dynamically chooses the best inference algorithm using an improved Upper Confidence Bound (UCB) formula (Han et al., 2024) for instance of different complexity. We explore popular and widely used three inference algorithms within PlanGEN: Best of N (Brown et al., 2024), Tree-of-Thought (ToT) (Yao et al., 2024), and REward-BAlanced SEarch (REBASE) (Wu et al., 2024a). We combine our agents with these algorithms, yielding four frameworks: (1) PlanGEN (Best of N), (2) PlanGEN (ToT), (3) PlanGEN (REBASE), and (4) PlanGEN (Mixture of Algorithms). In PlanGEN, we use \"Multi-Agent\" approach which signifies using the constraint and verification agents for the first three approaches, and all three agents for the \u201cMixture of Algorithms\u201d (Figure 1). Figure 1 shows example from NATURAL PLAN (Calendar scheduling), and App. F provides more examples.\nTo evaluate PlanGEN, we perform all experiments using Gemini-1.5-Pro (Team et al., 2024) as underlying model. We further present case-study on Gemini-2.0-Flash, and GPT-40 (Hurst et al., 2024) to show the model-agnostic nature. We evaluate natural language planning ability on NATURAL PLAN (Zheng et al., 2024), scientific/mathematical reasoning on GPQA (Rein et al., 2024) and OlympiadBench (He et al., 2024), and financial reasoning on DocFinQA (Reddy et al., 2024). Performance is compared against Zero-shot Chain-of-Thought (CoT) and a vanilla multi-agent baselines. We achieve"}, {"title": "2. PlanGEN", "content": "PlanGEN comprises three specialized LLM agents: a constraint agent, a verification agent, and a selection agent. Each agent utilizes an off-the-shelf LLM (e.g., Gemini, GPT) which is equipped with task-specific prompts for efficient performance."}, {"title": "2.1. Proposed LLM Agents", "content": null}, {"title": "2.1.1. Constraint Agent", "content": "We define \"constraints\u201d as the criteria necessary for verifying solutions to planning problems. These criteria are inherently instance-specific. For instance, in the calendar scheduling from NATURAL PLAN, relevant constraints include \u2018individual schedules', \u2018availabilities', and \u2018preferences'. In a scientific reasoning problems from GPQA, constraints might be the \u2018concepts used', 'calculation correctness', and 'formula selection'. We argue that careful extraction of instance-specific constraints is critical for successful verification. The constraint agent serves as a preprocessing component in the framework, designed to extract instance-specific constraints from the problem description. By analyzing the input problem, this agent identifies all possible critical constraints that are required for generated plan verification. The extracted constraints provide a foundation for verifying plans to improve the overall relevance and quality of the planning process. The prompt used by the constraint agent enables it to systematically identify constraints by asking the underlying LLM to focus on specific aspects of the problem description. This ensures that no critical information is overlooked and that the resulting constraints are comprehensive. Prompts used by the constraint agent and examples of generated constraints are provided in App. B and App. F, respectively."}, {"title": "2.1.2. Verification Agent", "content": "The verification agent plays a critical role in the framework by assessing the quality of generated plans based on constraints generated by the constraint agent. This agent ensures that plans are aligned with task objectives, adhere to constraints, and progress logically toward a correct and complete solution. The verification agent has two key components: (i) feedback generation, and (ii) numerical reward score generation based on feedback. Verification prompts and examples of verification are provided in App. B and App. F, respectively.\nFeedback Generation While verifying each generated plan against different constraints, the verification agent generates detailed natural language reasoning regarding plan quality. We consider this explanation as \u201cfeedback\u201d, offering interpretability and actionable next step towards improvement."}, {"title": "2.1.3. Selection Agent", "content": "The selection agent dynamically determines the most suitable inference algorithm for solving a given problem instance based on its complexity. It leverages a combination of historical performance; diversity, and recovery scores; and guidance from a LLM to adaptively select the best algorithm for the current instance. To create the selection agent, we utilize a modified Upper Confidence Bound (UCB) policy. The policy combines multiple factors, including normalized rewards, exploration bonuses, diversity adjustments, and recovery scores. Additionally, the agent incorporates LLM-guided priors, which provide algorithm suitability scores based on the problem statement, task requirements, and previous plan (if available). These priors enable the agent to align its selections with the input instance complexity and corresponding constraints, improving the relevance of the chosen algorithm.\nModified UCB Policy equation combines several terms to balance exploitation, and exploration when selecting the best algorithm for given task instance. To modify UCB, we first conducted a preliminary ablation study, presented in App. B.\n$UCB(a) = \\frac{R(a)}{N(a)} + \\sqrt{\\frac{2log(T + 1)}{N(\\alpha)}} + \\lambda_{prior} \\cdot Prior(a) + \\frac{\\lambda_{diversity}}{N(a) + 1} + \\lambda_{recovery} \\cdot S_{recovery}(a)$ \nAll terms in equation given above are calculated across one evaluation run. Here, the cost of calculation is negligible since it only utilizes reward values from previous runs, but only one LLM call require to get score for Prior(a). The first term, $\\frac{R(a)}{N(a)}$, represents the average reward for algorithm a, where $R(a)$ is the total reward accumulated by the algorithm, and $N(a)$ is the number of times the algorithm has been selected. This term ensures that algorithms with higher historical performance are prioritized. The second term, $\\sqrt{\\frac{2log(T+1)}{N(a)}}$, serves as the exploration component, encouraging the selection of algorithms with fewer trials, denoted as T. This term ensures that under-explored options are adequately evaluated. Furthermore, $\\lambda_{prior} \\cdot Prior(a)$, which leverages LLM-guided priors to align algorithm selection with the instance-specific complexity. Here, $\\lambda_{prior}$ is a dynamically decaying weight defined as $\\frac{A}{prior}$, where T represents the total number of trials. This decay gradually shifts the focus from initial priors to historical performance as trials progress. The diversity bonus, $\\frac{\\lambda_{diversity}}{N(a)+1}$, penalizes overused algorithms, ensuring balanced exploration across all options. Finally, the recovery term, $\\lambda_{recovery} \\cdot S_{recovery}(a)$, rewards algorithms that recover effectively from failures, with $S_{recovery}(a)$ representing the recovery score for algorithm a.\nSelection Process This process begins by initializing algorithm-specific variables, such as accumulated rewards, selection counts, and failure counts. Further details on the algorithm can be found in Algorithm 1 (App. B). The agent then incorporates LLM-guided priors to generate suitability scores for the algorithms based on the problem statement and any provided feedback. These priors are derived from a LLM (prompt for this given in App. B), and serve as initial estimates to adjust the UCB (Han et al., 2024) values."}, {"title": "2.2. Proposed Frameworks", "content": "Within PlanGEN, we propose four different frameworks: (1) PlanGEN (Best of N) (Figure 2), (2) PlanGEN (ToT) (Figure 3), and (3) PlanGEN (REBASE) (Figure 4), and (4) PlanGEN (Mixture of Algorithms) (Figure 1)."}, {"title": "2.2.1. PlanGEN (Best of N)", "content": "Motivated by Brown et al. (2024), we adapted the Best of N algorithm and modified it using our constraint and verification agents as illustrated in Figure 2. The framework generates N candidate plans (Plan 1, Plan 2, ..., Plan n), and each plan is assessed by a verification agent based on a set of constraints. Then, a corresponding reward (Reward 1, Reward 2,..., Reward n) gets assigned by the verification agent. Finally, the plan with the maximum reward is chosen, guaranteeing an optimal solution that best satisfies the problem constraints."}, {"title": "2.2.2. PlanGEN (ToT)", "content": "ToT algorithm has been studied in detail for solving many complex problems (Yao et al., 2024). As shown in Figure 3, we modify the ToT algorithm with our constraint and verification agents. The method begins by initializing a root node that represents the problem and generating multiple potential next steps, creating a tree-like structure. The generated steps are verified using a verification agent which assigns reward scores based on a set of constraints. The iterative process involves evaluating all possible steps at a given depth, selecting the most promising path based on reward scores, and expanding it further by generating new steps. This process continues until a valid solution is identified or a pre-defined limit on iterations is reached. Further details on various prompts for the ToT are presented in App. C."}, {"title": "2.2.3. PlanGEN (REBASE)", "content": "The REBASE tree search method inherits the exploitation and pruning properties of tree search and is well-studied for mathematical reasoning (Wu et al., 2024a). As shown in Figure 4, the framework incorporates a dynamic selection and expansion strategy to iteratively refine solutions. At each depth of the tree, candidate nodes are ranked based on their assigned reward scores (obtained using a verification agent), ensuring that the most promising candidates are explored first. Even steps with lower rewards are considered but with a reducing number of children, meaning that their exploration depth is limited. This hierarchical pruning helps maintain efficiency, thereby reducing unnecessary exploration of weaker nodes. This process continues until either a valid, complete solution is found or a predefined depth or width limit is reached. Also, there is a completion check similar to ToT which identifies nodes that represent complete solutions, enabling REBASE to terminate early once a satisfactory outcome is identified. App. C provides further details on prompts for the REBASE."}, {"title": "2.2.4. PlanGEN (Mixture of Algorithms)", "content": "The Mixture of Algorithms framework (Figure 1) introduces a selection agent (\u00a72.1.3) which dynamically selects the best possible inference-time algorithms proposed in the above sections based on instance-level complexity. The framework operates in a modular and iterative manner, ensuring adaptability in addressing planning and reasoning problems with different complexity effectively.\nOrchestration The process begins with generating an initial plan using LLM based on the task description and problem statement. Along with this, the constraint agent (\u00a72.1.1) is employed to generate an instance-specific set of constraints. Based on the constraints, the verification agent (\u00a72.1.2) evaluates the quality of the initial plan and provides a reward score (indicated as 'Score' in Figure 1). If the initial plan meets the required threshold (denoted Th), it is acceptable as the \u201cFinal Plan\u201d. Otherwise, the iterative refinement process begins.\nIterative Refinement The refinement loop is driven by a suite of inference algorithms as shown in Figure 1. During this iterative refinement, the selection agent (\u00a72.1.3) determines the most suitable algorithm based on the instance-specific complexity and historical UCB values. The selected algorithm produces an updated plan, which is then re-evaluated by the verification agent. To ensure continual improvement, the framework incorporates feedback generated by a verification agent that provides guidance, and this feedback loop enables the system to refine the plan incrementally."}, {"title": "3. Experiments and Results", "content": null}, {"title": "3.1. Experimental Setup", "content": "Datasets To demonstrate improvement in natural planning abilities, we utilize the NATURAL PLAN (Zheng et al., 2024). After improving the planning, we show that this significantly enhances the reasoning capabilities of LLMs on two benchmarks: GPQA (Rein et al., 2024) and OlympiadBench (text-only) (He et al., 2024). Additionally, we show that PlanGEN improves performance on a domain-specific dataset, DocFinQA (Reddy et al., 2024). Further details are presented in App. D.\nBaselines and Our Frameworks We develop two baselines for comparison with our frameworks: (i) Zero-shot CoT (Kojima et al., 2024) and (ii) a Vanilla Multi-Agent Baseline. In the Zero-shot CoT, we provide an input prompt to the model, which generates outputs in the form of <CoT reasoning, Answer>. For the \u201cMulti-Agent Baseline\", the same model is called iteratively across multiple iterations. The system repeatedly refines its outputs through feedback loops, where the feedback is generated based on a self-reflective prompt (App. D) designed to improve reasoning. We evaluate all proposed frameworks (\u00a72.2) on all benchmarks. For reasoning tasks, we use a two-stage approach: (1) generating an optimized plan using our frameworks, and (2) executing the plan to produce the final answer (Figure 1). App. D presents further details on model selection, metrics, and experiment hyper-parameters including the hyper-parameter choices for inference-time algorithms."}, {"title": "3.2. Main Results", "content": "Figure 5 compares performance of multi-agent frameworks across various single-agent and multi-agent baselines (varies across benchmarks some single-agent baselines for GPQA are obtained from https://klu.ai/glossary/gpqa-eval). From the results, it is evident that the multi-agent frameworks are consistently outperforming the baselines.\nPerformance on NATURAL PLAN From Figure 5a, PlanGEN (Best of N) achieves the highest EM scores across all tasks: 60.70 (Calendar), 43.80 (Meeting), and 41.63 (Trip). In calendar scheduling, all four frameworks surpass the strongest baseline (Multi-Agent Baseline) by ~ 10%. For meeting and trip planning, all except ToT outperform the best baseline (Gemini-1.5-Pro) by ~ 6% and ~ 7%, respectively. PlanGEN (Mixture of Algo.) achieves the second-highest performance in meeting and trip planning while remains competitive in calendar scheduling. These results demonstrate the effectiveness of our frameworks in handling diverse natural language planning tasks and establishing SOTA for all three categories of NATURAL PLAN.\nPerformance on OlympiadBench From Figure 5b, PlanGEN (Mixture of Algo.) achieves the highest accuracy in the MATH category (55.94%), outperforming the strongest Multi-Agent Baseline (50.68%) by ~ 5%. Notably, the superior performance of the PlanGEN (Mixture of Algo.) in MATH highlights its effectiveness in complex mathematical reasoning, setting a SOTA for the MATH. In the PHY category, all multi-agent frameworks surpass Gemini-1.5-Flash (strongest baseline), with PlanGEN (Best of N) achieving the highest accuracy (31.78%), setting a SOTA for the PHY.\nPerformance on GPQA From Figure 5c, the PlanGEN (Mixture of Algo.) achieves the highest accuracy (59.6%). The individual inference-time algorithms achieve a lower performance, indicating the usefulness of selection. All proposed frameworks outperform Gemini-1.5-Pro (46.2%), GPT models (~ 48%), and Claude-3-Opus (50.4%) by a large margin. While Claude-3.5-Sonnet, and Multi-Agent Baseline perform competitively (~ 59%) compared to PlanGEN (Mixture of Algo.).\nPerformance on DocFinQA From Figure 5d, our frameworks significantly improve performance on DocFinQA, with PlanGEN (Best of N) achieving the highest accuracy (31.16%) and F1-Score (29.45%), setting SOTA for the task. All our frameworks outperform the Gemini-1.5-Pro (strongest baseline) by a large margin (~ 7%). These results highlight the effectiveness of multi-agent frameworks in financial document understanding, and performing reasoning over them.\nPerformance of our frameworks w.r.t. different complexity As shown in Figure 6, we conduct a case study on calendar scheduling task from NATURAL PLAN to analyze the impact of varying complexity levels on the performance of different frameworks. For the calendar scheduling, we observe that PlanGEN (ToT) performs best for simple problems, while PlanGEN (Best of N) is more effective for intermediate problems. As complexity increases, a PlanGEN (Mixture of Algo.) proves to be the most effective approach. We further conduct a similar analysis for meeting and trip planning from NATURAL PLAN presented in App. E.\nMain Findings Compared to single-agent systems, multi-agent frameworks consistently outperform in generating optimized planning trajectories (Figure 5). Furthermore, Multi-Agent (Baseline) is not always the strongest benchmark, as self-correction can introduce challenges as shown in Huang et al. (2024). Thus, different agents within the system require distinct handling strategies similar to our PlanGEN. Additionally, even in multi-agent frameworks for PlanGEN, relying on a single inference-time algorithm proves insufficient for more complex problems (Figure 6). A PlanGEN (Mixture of Algo.) approach offers substantial advantages for solving complex planning problems, highlighting the importance of algorithm selection based on instance-specific complexity (Figure 1). Given that our frameworks are multi-agent, we provide further discussion on # of LLM calls vs. their performance in subsequent section."}, {"title": "4. Analysis and Discussion", "content": "Here, we discuss detailed analysis over importance of our agents and model-agnostic nature of our frameworks. Additionally, we also present more analysis on results in App. E.\nImportance of Verification Agent Figure 7 demonstrates the verification agent's crucial role in PlanGEN by showing a strong correlation between assigned reward values and prediction correctness (1 for correct, 0 for incorrect). The plotted points represent the average correctness rate for data buckets of varying reward values, each bucket containing hundreds of samples. A logistic regression model trained on DocFinQA and GPQA data (~ 1100 total samples) reveals a sigmoidal trend: higher rewards correlate with increased success probability, highlighting the agent's effectiveness. This reinforces the importance of constraint-guided verification for improving inference-time algorithms (see App. E for further details).\nImportance of Selection Agent Figure 8 illustrates the importance of the selection agent by comparing the performance on the NATURAL PLAN. Here, Multi-Agent (Ver.) includes only the verification agent, while Multi-Agent (Ver. + Selection) further includes a selection agent. The results highlight the progressive impact of these components.\nFor example, in calendar scheduling, Multi-Agent (Ver.) improves performance to 56.1 EM compared to Multi-Agent (Baseline). However, Multi-Agent (Ver. + Selection) achieves 59.3 EM, demonstrating the additional benefit of algorithm selection. A similar trend is observed in trip planning where Multi-Agent (Ver. + Selection) outperforms Multi-Agent (Ver.) (41.17 EM vs. 35.44 EM) and the Multi-Agent (Baseline). For meeting planning, Multi-Agent (Ver.) achieves 43.1 EM compared to 36.8 EM of Multi-Agent (Baseline), whereas, Multi-Agent (Ver. + Selection) achieves competitive performance. Together, verification and selection agents drive significant improvements over single-agent and multi-agent baselines.\nModel-Agnostic Nature The results from Table 1 demonstrate the model-agnostic nature of our proposed multi-agent frameworks. While the primary experiments were conducted using"}, {"title": "5. Conclusions", "content": "In this work, we proposed PlanGEN, an easily scalable multi-agent approach incorporating three key components: constraint, verification, and selection agents. We leveraged these agents to improve the verification process of existing inference algorithms and proposed three frameworks: Multi-Agent Best of N, ToT, and REBASE. Further, we introduced a Mixture of Algorithms, an iterative framework that integrates the selection agent (Figure 1) to dynamically choose the best algorithm. We evaluated our frameworks on NATURAL PLAN, OlympiadBench, GPQA, and DocFinQA. Experimental results demonstrate that PlanGEN outperforms strong baselines, achieving SOTA results across datasets. Furthermore, our findings suggest that the proposed frameworks are scalable and generalizable to different LLMs, improving their natural language planning ability."}, {"title": "Limitations", "content": "Despite the strong performance of our frameworks, an area of improvement is the reliance on predefined heuristics for selecting inference-time algorithms, which may not always generalize optimally across all tasks and domains. Additionally, while our frameworks demonstrate strong performance, their computational overhead could be further optimized for efficiency in real-world applications. We believe that our frameworks can be useful in further boosting the planning and reasoning capabilities of existing models such as o1 and Gemini-thinking. In addition, the use of reinforcement learning or meta-learning techniques to dynamically adapt agent strategies based on task complexity could be an interesting area to explore. Moreover, broadening the scope to multi-modal and multi-lingual reasoning would significantly expand the applicability of our approach, and exploring the use of generated planning trajectories for model training offers valuable direction."}, {"title": "Ethics Statement", "content": "The use of proprietary LLMs such as GPT-4, Gemini, and Claude-3 in this study adheres to their policies of usage. We have used AI assistants (Grammarly and Gemini) to address the grammatical errors and rephrase the sentences."}, {"title": "A. Related Works", "content": "LLM Agents for Planning Agent-based frameworks for planning have gained interest, focusing on enhancing how LLMs decompose tasks and refine their outputs. The Sibyl framework (Wang et al., 2024c) effectively decomposes tasks into smaller subtasks, assigning each to specialized agents that iteratively collaborate until a solution is reached. OS-Copilot (Wu et al., 2024b) introduces a generalist computer agent that employs self-improvement through modularization and feedback loops. Another approach is KnowAgent (Zhu et al., 2024), which integrates knowledge-augmented planning to enhance the decision-making capabilities of LLM agents. Similarly, Tool-Planner (Liu et al., 2024) proposed grouping tools based on similar functionalities into toolkits, allowing LLMs to select the best tool for a given task. Many agent-based works focusing on planning have been developed (Chen et al., 2024; Wang et al., 2024b; Xie and Zou, 2024). Despite the progress, these methods generally (i) focus on domain-specific tasks or limited benchmarks, reducing generalizability, and (ii) lack or under-explore mechanisms for verifying and refining plans iteratively. While some works explore natural language planning (Bohnet et al., 2024; Lee et al., 2025), they either single-agent frameworks or evaluate proposed framework on domain-specific benchmarks.\nInference-time Algorithms Inference-time algorithms have recently shown a significant improvement in LLMs performance during inference. For instance, Best of N sampling (Brown et al., 2024) selects the most promising output from multiple generations performed using temperature sampling, while Tree-of-Thought (ToT) (Yao et al., 2024) models reasoning as an iterative tree search. REBASE (Wu et al., 2024a) optimizes search-space pruning using reward balancing. One very popular approach is Monte Carlo Tree Search (MCTS) (Zhang et al., 2024) which iteratively explores solution paths during inference. Applied to models such as LLaMa-3-8B, it enables self-refinement by revisiting and improving initial solutions. Test-time optimization (Snell et al., 2025), focuses on dynamically adjusting computational resources during inference (Wu et al., 2024a). Furthermore, Wang et al. (2025) uses the inference time algorithms to improve LLMs planning capabilities to solve code synthesis problems. In inference-time algorithms, verification is the key component. In contrast to these past works, here, we enhance performance of inference-time algorithms utilizing constraint-guided verification, and multi-agent collaboration for natural language planning, and its applications in downstream complex reasoning tasks."}, {"title": "B. Further Details on LLM Agents", "content": "In this section, we provide additional details about each specialized agent in PlanGEN. We present the prompts used for each agent, highlighting their roles in the framework. The prompt for the constraint agent includes task-specific parameters that can be adjusted to extract relevant constraints for different tasks. In contrast, the prompts for the verification agent and selection agent are entirely task-agnostic, ensuring generalizability and adaptability across various problem domains."}, {"title": "Prompts for Constraint Agent", "content": "The constraint agent is responsible for extracting problem-specific constraints that guide the planning process. To enable systematic extraction of constraints, we design a task-specific prompt for the constraint agent:\nPrompt\nYou are an expert in understanding an input problem and generating set of constraints. Analyze the input problem and extract all relevant instance-specific constraints and contextual detailsnecessary for accurate and feasible planning.\n(Optional) These constraints may include:\n<You may provide any specific type of constraints>\n<You may provide any formatting instruction>\nInput Problem: "}, {"title": "Prompts for Verification Agent", "content": "The prompt for the verification agent is designed to be task-agnostic, meaning it can be applied across different problem domains without modification. By enforcing strict evaluation criteria, this agent enhances the reliability of PlanGEN, making it robust for various planning and reasoning tasks. In this prompt, list of constraints are generated using constraint agent. Notably, the list of constraints used in the verification prompt is dynamically generated by the constraint agent. This ensures that the verification process is based on instance-specific constraints rather than relying on predefined, static rules.\nPrompt\nProvide a reward score between -100 and 100 for the quality of the provided plan steps, using strict evaluation standards. Ensure the reward reflects how effectively the plan contributes to progressing toward the correct solution.\nProblem Statement:\n{problem}\nPlan:\n{plan}\nConsider the following constraints while evaluating:\n[Constraint 1]\n[Constraint 2]\n[Constraint 3]\nProvide feedback in the following format:\n[Step-by-step reasoning for the reward score]\nScore: [Strictly provide an integer reward score between -100 and 100]"}, {"title": "Prompts for Selection Agent", "content": "The prompt for the Selection Agent is task-agnostic, allowing it to be applied across various domains without modification. It processes feedback from the verification agent and contextual information from the problem statement to assign suitability scores to different inference-time algorithms.\nPrompt\nAnalyze the following planning problem and explain your reasoning for assigning priority scores to the algorithms based on their suitability. Scores should be between 0 and 1, where 1 represents the most suitable algorithm for the given problem.\nProblem Statement: \nRequirements: \nContext:  \nStart by providing a brief reasoning for each algorithm's suitability based on problem complexity. Then, ONLY output your response strictly as a list with the exact format below:\nReasoning:\n\u2022 Best of N: [Explain why this algorithm is or isn't suitable]\n\u2022 Rebase: [Explain why this algorithm is or isn't suitable]\n\u2022 ToT: [Explain why this algorithm is or isn't suitable]\nScores:\n[(\"Best of N\", float),\n(\"Rebase\", float),\n(\"ToT\", float)]"}, {"title": "Algorithm for Selection using UCB", "content": "The algorithm (Algorithm 1) presented is a modified UCB selection strategy that incorporates additional factors for exploration, diversity, and recovery. It initializes each algorithm with basic statistics like reward (R(a)), count of trials (C(a)), and recovery score (Rec(a)). The algorithm computes a normalized reward $Rnorm(a)$ for each option, balancing the reward with exploration (E(a)), which encourages trying less-used algorithms. A diversity bonus D(a) penalizes overused algorithms, while a recovery bonus RecB(a) rewards algorithms that perform well after prior failures. LLM-guided priors (LLM_prior) are used to influence the selection process based on prior knowledge. The final selection is made by maximizing the UCB score, which combines these factors to balance exploitation and exploration.\nAblation Study on UCB Modifications To design our selection agent, we conducted an ablation study evaluating modifications to the UCB formula, shown in Figure 10. Initially, we replaced the selection agent with a simple sequential strategy, termed \u201cMulti-Agent (Sequential)\u201d, where algorithms execute in sequence, and the verification agent selects the highest-scoring plan. Next, we implemented a UCB selection agent, but excluded the \u2018diversity bonus' and \u2018recovery term' introduced in our proposed formulation in the main paper, denoted as \u201cMulti-Agent (UCB w/o div. and rec.)\". Finally, we implemented the complete selection agent incorporating our proposed UCB, labeled \u201cMulti-Agent (UCB)\". As shown in Figure 10, the inclusion of the diversity bonus and recovery termsin the UCB formula (\u201cMulti-Agent (UCB)\u201d) resulted in ~ 3.5% performance gain compared to the UCB variant without these terms, further enhancing overall results. Note that the LLM-guided priors are still the part of Multi-Agent (UCB w/o div. and rec.) and Multi-Agent (UCB)."}, {"title": "C. Prompts for Proposed Frameworks", "content": "We provide further details in this section regarding the prompts used for PlanGEN (ToT) and PlanGEN (REBASE), as well as the specific algorithms used to execute these inference-time methods.\nPrompts used for ToT and REBASE PlanGEN (ToT) and PlanGEN (REBASE) employ three prompt types: (1) step prompt, (2) step reward prompt, and (3) completion prompt. Step prompt guide the model to generate subsequent steps based on the problem statement and previously generated steps. Step reward prompt evaluate each intermediate step against the problem statement and constraints, similar to the prompts used by a verification agent. Completion prompt check for a complete solution after each step. If a solution is found, exploration terminates; otherwise, the process continues until a solution is reached.\nStep Prompt\nYou are an expert assistant for generating step-by-step plan to solve a given question using specified tools. Given the problem and any intermediate steps, output only the next step in the plan. Ensure that the next action helps in moving toward the correct plan to solve the given question. Do not provide the full plan. Keep responses concise, focusing solely on the immediate next step that is most effective in progressing toward the correct plan.\n{Add a problem statement here}\n\n{Append previously generated steps}\n\nCompletion Prompt\nYou are an assistant tasked with verifying if the final, complete plan to solve the given question has been achieved within the intermediate steps. Output only '1' if the intermediate steps contain the full solution needed to solve the question. If the full plan has not yet been reached, output only '0'. Provide no additional commentary\u2014return exclusively \u20181' or '0'.\n{Add a problem statement here}\n\n{Append previously generated steps}"}, {"title": "D. Details on Benchmarks and Experiments", "content": "Statistics of Benchmarks For evaluation, we utilize evaluation sets of all four benchmarks. For NATURAL PLAN, we employed the provided evaluation sets, consisting of 1000 instances each for Calendar Scheduling and Meeting Planning, and 1600 instances for Trip Planning. The GPQA evaluation was conducted using the Diamond set, which comprises 198 highly challenging instances. From OlympiadBench, we selected the text-only problems, excluding those requiring a theorem prover, resulting in 674 instances for the MATH category and 236 for the PHY category. We also used 922 instances from the DocFinQA evaluation set.\nModels Our primary evaluations use Gemini-1.5-Pro for"}]}