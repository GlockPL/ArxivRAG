{"title": "Zero-Shot Text-to-Speech as Golden Speech Generator: A Systematic Framework and its Applicability in Automatic Pronunciation Assessment", "authors": ["Tien-Hong Lo", "Meng-Ting Tsai", "Berlin Chen"], "abstract": "Second language (L2) learners can improve their pronunciation by imitating golden speech, especially when the speech that aligns with their respective speech characteristics. This study explores the hypothesis that learner-specific golden speech generated with zero-shot text-to-speech (ZS-TTS) techniques can be harnessed as an effective metric for measuring the pronunciation proficiency of L2 learners. Building on this exploration, the contributions of this study are at least two-fold: 1) design and development of a systematic framework for assessing the ability of a synthesis model to generate golden speech, and 2) in-depth investigations of the effectiveness of using golden speech in automatic pronunciation assessment (APA). Comprehensive experiments conducted on the L2-ARCTIC and Speechocean762 benchmark datasets suggest that our proposed modeling can yield significant performance improvements with respect to various assessment metrics in relation to some prior arts. To our knowledge, this study is the first to explore the role of golden speech in both ZS-TTS and APA, offering a promising regime for computer-assisted pronunciation training (CAPT).", "sections": [{"title": "Introduction", "content": "With the unprecedented advancements in computer technology and the growing number of second-language (L2) learners worldwide, computer-assisted pronunciation training (CAPT) has emerged as a handy tool for L2 learners. In addition, CAPT can also facilitate instructional quality meanwhile reducing teacher workload. Typically, a CAPT system is deployed in a \u201cread-aloud\" scenario where a text prompt is presented, and an L2 learner is asked to say it out loud while mimicking the manner of speech of native speakers. Previous research indicated that L2 learners can significantly improve their pronunciation by imitating a \"golden speech\" which has a correct pronunciation and speech traits similar to their own (Watson and Kewley-Port, 1989), as opposed to imitating those of other native speakers with dissimilar characteristics (Ding et al., 2019; Jilka and M\u00f6hler, 1998; Probst et al., 2002; Nagano and Ozawa, 1990), as schematically depicted in Figure 1. In addition, the blending of an L2 speaker's voicing characteristics with the L1 (native) accent has also been shown to help enhance the learner's speaking proficiency in a foreign language (Nagano and Ozawa, 1990), indicating that such re-synthesized speech can more effectively support oral language learning than direct imitation of L1 speech. On top of these observations, some prior studies incorporated \"behavioral shaping\" through synthesized speech, thereby spurring further investigations (Silpachai et al., 2021; Felps et al., 2009; Aryal et al., 2013; Watson and Kewley-Port, 1989). Starting with the benefit of golden speech, we aim to generate learner-specific golden speech using modern synthesis techniques and in turn utilize it for other purposes, such as CAPT-related tasks.\nOn a separate front, the recent trend of speech synthesis has shifted the focus to drawing on deep neural networks due to their significant technological breakthroughs (Wang et al., 2017; Ren et al., 2019; Kim et al., 2021; Ren et al., 2021; Casanova et al., 2022; Tan et al., 2024; Saeki et al., 2023; Gong et al., 2023). The prevailing models, including but not limited to Tacotron (Wang et al., 2017), FastSpeech (Ren et al., 2019), VITS (Kim et al., 2021), and YourTTS (Casanova et al., 2022), have demonstrated marked performance, paving the way for neural network-based speech synthesis. These approaches have revealed several advantages, including high speech quality in terms of intelligibility and naturalness, alongside reduced reliance on handcrafted engineering. In addition, considerable research has been concentrated on few-shot and zero-shot text-to-speech (ZS-TTS) to address the challenge of both insufficient training data and versatile user scenarios (2019; Kim et al., 2021; Ren et al., 2021; Casanova et al., 2022; Tan et al., 2024; Saeki et al., 2023; Gong et al., 2023). For example, YourTTS (Casanova et al., 2022) has demonstrated compelling capabilities under the zero-shot scenario. Given its proven efficacy in some related synthesis applications, we base our approach on YourTTS to explore the capability of synthesis models to generate the golden speech.\nOn the other hand, in CAPT systems, automatic pronunciation assessment (APA) is the most indispensable component. APA aims to evaluate the pronunciation proficiency of L2 learners by considering both segmental and suprasegmental features (Ferrer et al., 2015; Li et al., 2017; Do et al., 2023; Chao et al., 2022). Earlier studies on the task of APA only modeled fluency, lexical stress, and intonation independently (Ferrer et al., 2015; Li et al., 2017), while recent research activities explore multi-aspect and multi-granularity modeling (Do et al., 2023; Chao et al., 2022) in order to offer in-depth feedback on the pronunciation quality of an L2 Learner. While the prior arts (Ferrer et al., 2015; Li et al., 2017; Do et al., 2023; Chao et al., 2022) have shown promising performance in various aspects of prediction, the potential of using the golden speech to enhance the APA models has so far been largely under-explored in the literature.\nTo address the questions raised above, we hypothesize that golden speech can be generated by zero-shot text-to-speech (ZS-TTS) synthesis and can serve as an effective metric for gauging the pronunciation proficiency of L2 learners. To validate the above hypothesis, the research goals of this study included 1) the design and development of a systematic framework for assessing the ability of a synthesis model to generate golden speech and 2) the in-depth investigations of the effectiveness of using golden speech in APA. To this end, we first utilized word error rate reduction (WERR) from an off-the-shelf ASR model (Radford et al., 2022) to measure intelligibility improvements. Additionally, we employ speaker embedding cosine similarity (SECS) (Wan et al., 2018) and apply the mean opinion score (MOS) (Saeki et al., 2022) to verify the quality of synthesized speech. We further adopt dynamic time warping (DTW) (Miodonska et al., 2016) to analyze the discrepancies between the original speech and the synthesized speech of L2 learners, offering an alternative measure for pronunciation proficiency. Finally, we validate the effectiveness of golden speech features on a cutting-edge APA model (e.g., 3M (Chao et al., 2022)). All variants of our approaches are evaluated on the L2-ARCTIC (Zhao et al., 2018) and the Speechocean762 (Zhang et al., 2021) benchmark datasets, which underscore the potential of TTS in generating golden speech for use in personalized and efficient CAPT. In summary, the main contributions of this study are as follows:\n1). We proposed a systematic framework for assessing the ability of a synthesis model to imitate a golden speaker.\n2). We validated the effectiveness of golden speech features on the APA task and the potential of such fusion to drive further innovative progress in APA.\n3). In particular, our work is first to explore the role of golden speech in both ZS-TTS and APA, offering a promising regime for CAPT."}, {"title": "Proposed Framework", "content": "In this section, we delved into the design and implementation of our proposed framework, detailed in Table 1, which unfolded in two parts: 1) the ability of synthesis models to mimic golden speech (e.g., intelligibility, speaker similarity, and naturalness) and 2) assessed the effectiveness of golden speech on APA."}, {"title": "Synthesizing Golden Speech of L2 Learners", "content": "Starting from exploring the benefit of learner-specific golden speech, we first seek to generate it automatically. For the idea to work, we utilize YourTTS (Casanova et al., 2022) to synthesize the golden speech for learners, which offers a novel approach to CAPT. This approach enables learners to practice pronunciation modeled by a golden speaker tailored to their specific language learning needs. In more detail, YourTTS, a pioneering zero-shot multi-speaker TTS model, was developed on top of the fully end-to-end VITS (Kim et al., 2021) architecture, which achieved excellent performance in speech synthesis. YourTTS employed a pre-trained H/ASP model (Heo et al., 2020) as its speaker encoder, leveraging transfer learning to enhance training further."}, {"title": "Ability of Synthesis Models to Mimic Golden Speech", "content": "The desired golden speech should possess excellent pronunciation quality. Additionally, it should closely match the voicing characteristics of the L2 learner. Importantly, the speech must not sound unnatural. Therefore, we designed three evaluation criteria intelligibility, speaker similarity, and naturalness-to assess whether synthesized speech can be considered the golden speech."}, {"title": "Intelligibility", "content": "Computer-assisted pronunciation training (CAPT) is typically categorized into two types: read-aloud and free-speaking assessments. The former primarily relies on the word error rate (WER) for intelligibility measurement, while the latter also considers grammar and language use. In this study, we focus exclusively on read-aloud assessments and leverage an off-the-shelf automatic speech recognition (ASR) system for our purpose. More specifically, we first utilize the Whisper (Radford et al., 2022) model to transcribe both synthesized and original speech into word sequences. We then calculate the word error rate (WER) and the word error rate reduction (WERR) between these sequences; WERR latter is formulated as:\n$WERR = \\frac{WER_{org}-WER_{syn}}{WER_{org}}$\nwhere WERorg signifies the WER between the reference text and the ASR transcript of the original speech produced by Whisper, while WERsyn denotes the WER between the reference text and the ASR transcript of the synthesized speech produced by Whisper."}, {"title": "Speaker Similarity", "content": "Drawing on prior studies (Casanova et al., 2022; Gong et al., 2023), we utilize the Resemblyzer \u00b9 package to calculate the pairwise cosine similarity between original and synthesized speech. After obtaining cosine similarity values, we design two metrics: SECSutt which requires identical linguistic content in the speech, and SECSspk, which imposes no such constraint:\n$SECS_{utt} = \\frac{1}{U_s} \\sum_{i=1}^{U_s} cos(e_i^o, \\hat{e_i^s})$\n$SECS_{spk} = \\frac{1}{S} \\frac{1}{U_s} \\sum_{i=1}^{S} \\sum_{j=1}^{U_s} cos(e_i^o, e_j^s)$\nIn Eq. (2) and Eq. (3), Us represents the total number of utterances for speakers, $e_i^o$ is the speaker embedding of the i-th original utterance for speakers, S signifies the total number of speakers. In Eq. (2), $\\hat{e_i^s}$ corresponds to the speaker embedding of the i-th synthesized utterance, reflecting a paired comparison between the original speech and the synthesized speech. In contrast, Eq. (3) allows for comparisons across different utterances for the same speaker to evaluate the speaker similarity value. The only distinction between the two equations lies in the comparison for the same (i = j in Eq. (2)) or across different utterances (i \u2260 j in Eq. (3))."}, {"title": "Naturalness", "content": "The mean opinion score (MOS) is a widely-used metric for the evaluation of speech naturalness,"}, {"title": "Effectiveness of Golden Speech on APA", "content": ""}, {"title": "DTW-based Correlation", "content": "We assume that the smaller the difference between the original speech and the synthesized (golden) speech of an L2 learner, the better the pronunciation proficiency of the learner. To validate this assumption, we first employ dynamic time warping (DTW) to calculate the differences (i.e., DTW cost) between the original speech and the synthesized speech based on their respective feature vector sequences (e.g., wav2vec 2.0 (Peng et al., 2021)). We then assess the correlation between the DTW costs and the pronunciation proficiency scores (e.g., TOEFL and total score) of L2 learners. A correlation value between the DTW costs and the proficiency score of the L2 learner is anticipated to indicate the effectiveness of using learner-specific golden speech as an important ingredient for APA."}, {"title": "Golden Speech-based APA Model", "content": "This work selects the 3M model (Chao et al., 2022), a cutting-edge APA model, as the foundation model to facilitate the validation of our hypothesis. 3M extracts multi-view inputs of speech of learners and delivers feedback across multiple aspects (e.g., accuracy, total, etc.) and granularities (e.g., phoneme, word, and utterance) simultaneously. Building upon the 3M model, we further enhance its performance by utilizing the synthesized (golden) speech of the L2 learner. Our proposed architecture, as depicted in Figure 2, includes a synthesized speech transformer encoder that mirrors the original 3M transformer (i.e., the left side of Figure 2) and incorporates various fusion mechanisms such as addition (ADD), attention (ATT), gating (GATE), and concatenation followed by projection (CAT):\nAddition (ADD): The addition mechanism merges the hidden vector sequences of the original speech, Horg, and the synthesized speech, Hsyn, by summing them together.\nH' = Horg + Hsyn\nAttention (ATT): In the attention mechanism, we designate Horg as the query, with Hsyn serving both as the key and the value. H' is derived using multi-head attention MHA and subsequently added to Horg in a residual manner:\nH' = Horg + MHA(Horg, Hsyn, Hsyn)\nGATE: In the gate mechanism, the weight matrices Worg and Wsyn are introduced to specify the importance of the original speech Horg and the synthesized speech Wsyn , respectively:\nWorg = GELU(Linearorg(concat(Horg, Hsyn)))\nWsyn = GELU(Linearsyn(concat(Horg, Hsyn)))\nH' = Worg * Horg + Wsyn * Hsyn\nConcatenation (CAT): In the CAT mechanism, Horg and Hsyn are concatenated and followed by MLP layer Linear to obtain the H':\nH' = Linear(concat(Horg, Hsyn))\nAfter the hidden vector sequences H' are obtained, H' is then fed into the prediction head Linearp which serves as multi-aspect and multi-granularity regressors to produce the final predictions.\nSp = Linearp (H')\nL = $\\frac{1}{Np} \\sum_{p=1}^{Np} Lp$\nwhere Sp, Lp, Np denote the predicted score value, loss value, and total number of the aspect p (e.g., accuracy), respectively. We adopt mean squared error (MSE) loss to optimize whole model parameters, while we evaluated the APA model by Pearson correlation coefficient (PCC)."}, {"title": "Dataset", "content": ""}, {"title": "L2-ARCTIC", "content": "L2-ARCTIC is an English speech benchmark dataset designed and curated for research in voice conversion, accent conversion, and mispronunciation detection and diagnosis, among others. This corpus contains about 3,600 utterances of 24 non-native speakers (12 males and 12 females, equipped with manual transcripts) across different nationalities. The dataset features about one hour of recorded speech per speaker, including challenging sentences for the L1 of non-native speakers. Furthermore, it provides manual phoneme-level annotations for mispronunciations in 150 sentences. In this study, L2-ARCTIC serves as an evaluation set for quantitative analysis of the synthesized golden speech for L2 learners."}, {"title": "Speechocean762", "content": "Speechocean762 is an open-source dataset designed for pronunciation assessment, containing 5,000 English utterances from 250 non-native speakers, ranging from adults (over 15 years old) to children (15 years old or younger). The Speechocean762 (SO762) dataset includes scores for pronunciation proficiency across multiple dimensions and at various levels of linguistic granularity, equipped with phoneme-level, word-level, and utterance-level annotation labels (i.e., pronunciation proficiency scores) with respect to different aspects, such as accuracy, fluency, completeness, prosody, and a total score. We selected the accuracy and total scores at phoneme, word, and utterance levels for evaluation to facilitate the validation of our hypothesis."}, {"title": "Experiments", "content": ""}, {"title": "Implementation Details", "content": "In our experiments, we validated the proposed approach along two experimental directions. The first direction focused on the ability of synthesis models to mimic golden speech (cf. Section 2.1). This involved utilizing the YourTTS (Casanova et al., 2022) synthesis model to generate synthesized speech, so as to validate the intelligibility of the golden speech using Whisper (cf. Section 2.2.1). Additionally, we assessed the speaker embedding cosine similarity (SECS) value using Resemblyzer (cf. Section 2.2.2), as well as naturalness through Mean Opinion Score (MOS) (cf. Section 2.2.3). As for YourTTS, we employed the multilingual model from the open project Coqui-ai/TTS\u00b3. Regarding Whisper, we made use of WhisperX\u2074 (Bain et al., 2023) to recognize both the original speech and the synthesized speech, with model sizes ranging from base to medium, meanwhile including both monolingual and multilingual models."}, {"title": "Intelligibility", "content": "At the outset, we evaluate here the capacity of YourTTS to generate golden speech using speech datasets of L2 speakers from Speechocean762 (SO762), comprising both adults (referred to as SO762-A) and children (referred to as SO762-C), as well as from L2-ARCTIC (referred to as L2-Arc). To ensure the reliability of our experiment, we report on the results using Whisper of various sizes, as well as the multilingual and monolingual versions. Table 2 shows the corresponding results of the original speech (L2-Arc, SO762-A, SO762-C), the native speech (L1, a seen speaker in the training set of the YourTTS model), and the golden speech (GLD). As can be seen, there is a significant and consistent improvement pertaining to two metrics, i.e., WER and WERR, for synthesized speech (L1 and GLD) across both L2-ARCTIC and SO762 datasets. For Whisper, the multilingual model outperforms the monolingual model, and models of larger sizes also yield better results.\nFor L2-ARCTIC, the WER for the L1 speech is improved from 7.90% to 4.63% with the medium and multilingual Whisper model, leading to a WERR of 34.70%. The golden speech also benefits from the same configuration change of Whisper, with a WERR of 31.59%. As for SO762-C, a similar trend to L2-ARCTIC is observed, with WERRs of 31.32% for the L1 speech and 34.03% for the golden speech.\nThese results indicate that while both the L1 speech and the golden speech have better intelligibility, the performance of the latter (GLD) on WERR was not as good as the former (L1). This discrepancy is probably due to the fact that L2 speakers were excluded from the training data, suggesting that including L2 speakers in the training dataset could enhance the performance of ZS-TTS. Nevertheless, the WERR of the golden speech, though slightly lower than that of the L1 speech, is within an acceptable range. We will continue to validate the effectiveness of golden speech (GLD) as a robust feature in subsequent experiments."}, {"title": "Speaker Similarity and Naturalness", "content": "Table 3 reports the SECS values (speaker similarity) and UTMOS (naturalness). Higher values in both metrics are desirable. We include the original speech here as an upper bound, shown in the first row of each block (L2-Arc, SO762-A, SO762-C).\nFrom Table 3, we can observe the following comparisons between L1 and GLD: For L2Arc, the SECS values for GLD are 0.75 (utt) and 0.70 (spk), compared to 0.51 (utt) and 0.50 (spk) for L1, respectively, showing a higher speaker similarity for GLD. The UTMOS for GLD is 3.67, higher than 3.50 for L1. This trend is consistent across SO762-A and SO762-C, in comparison to lower SECS values for L1 in both cases. In SO762, GLD achieves better SECS in the adult group (SO762-A) compared to the children group (SO762-C), due likely to a closer match between the speaker characteristics of adults and the training data. While UTMOS does not show significant correlations, satisfactory values are still reached for L2Arc, SO762-A, and SO762-C, respectively. These scores are comparable to those of the original and L1 speech, highlighting that the golden speech retains naturalness while improving intelligibility. Table 3 also reveals that GLD has higher speaker similarity compared to L1, retaining more of the original voicing characteristics of L2 speakers. Next, we will turn to explore the effectiveness of golden speech (GLD) in APA."}, {"title": "Correlation between speaking proficiency and DTW cost", "content": "To prove the effectiveness of golden speech in APA, we examine the correlation between DTW costs, calculated using original speech and the golden speech with the wav2vec 2.0 embeddings, as well as the pronunciation proficiency, indicated by TOEFL scores for L2-ARCTIC and total scores for SO762. As shown in Figure 3, the Pearson correlation coefficient (PCC) between DTW cost and TOEFL scores is -0.77 for L2-ARCTIC and - 0.88 for SO762. This demonstrates a clear negative trend, reinforcing the strong correlation between the DTW cost and the proficiency scores. The larger number of speakers in SO762 also makes it more representative. These findings confirm that DTW cost is an effective metric for assessing pronunciation proficiency, with lower DTW costs correlating with higher proficiency scores."}, {"title": "Performance on the APA Models", "content": "In this subsection, we report on the performance of two strong baselines, which are multi-granularity multi-aspect APA models, i.e., HiPAMA (Do et al., 2023) and 3M (Chao et al., 2022). Table 4 shows that our proposed approaches can yield small yet consistent improvements. These results underscore the vital role of golden speech traits in accurately evaluating multi-aspect pronunciation proficiency. The observed limitations for further improvement may stem from using grapheme-to-phoneme (G2P) conversion to generate phoneme sequences for the input of YourTTS, rather than canonical phoneme sequences from human annotations. This discrepancy could have constrained the effectiveness of the golden speech for the phoneme-level APA."}, {"title": "Model Prediction Across Different Target Ranges", "content": "Figure 4 illustrates the prediction accuracy of five models, namely the baseline 3M model and four proposed variations (3M-ADD, 3M-ATT, 3M-GATE, and 3M-CAT). The evaluation covers three aspects: phone accuracy, word total, and utterance total, with target score groups divided into 0-2, 3-4, 5-6, 7-8, and 9-10. Predictions are analyzed using box plots, revealing that accuracy is increased with wider target ranges. These improved models consistently outperform the baseline 3M model, particularly in the middle and higher target ranges, showing their superior accuracy and reduced variability. Overall, the findings suggest that all models will perform better as the target range (proficiency score) increases, with the improved models exhibiting significant advantages. Future research will focus on prompting the performance in the lower target ranges to achieve more consistent accuracy."}, {"title": "Conclusion", "content": "In conclusion, we have presented a novel and systematic framework for evaluating the potential of ZS-TTS as a golden speech generator and incorporating the golden speech as an indicative feature into automatic pronunciation assessment (APA). Experiments conducted on the L2-ARCTIC and Speechocean762 datasets have shown notable improvements with respect to WER and other evaluation metrics, confirming that the golden speech generated by ZS-TTS plays an important role in APA. Future studies will explore more sophisticated ZS-TTS architectures (Gong et al., 2023), as well as joint training and fine-tuning strategies to further unravel the potential role of the golden speech in APA and related applications."}, {"title": "Limitations", "content": "Zero-shot text-to-speech (ZS-TTS) models exhibit several limitations that affect their performance and applicability. First, ZS-TTS models heavily depend on the quantity of training data. When datasets are insufficient or biased, the resulting synthesis quality can be suboptimal, limiting the effectiveness of the models in real-world applications. Second, ZS-TTS models may struggle with accurately synthesizing speech for languages or accents underrepresented in the training data, leading to less natural or intelligible outputs for these cases. Third, while the concept of \"golden speech\" aims to improve pronunciation, it might not fully capture the nuances of different learners' pronunciation, especially those with unique speech patterns or non-standard accents. This limitation can reduce the effectiveness of using golden speech as a universal metric for pronunciation assessment. Fourth, current evaluation metrics like word error rate (WER), speaker embeddings cosine similarity (SECS), and mean opinion score (MOS) may not fully capture the qualitative aspects of pronunciation proficiency and naturalness, leading to potential gaps in assessment accuracy. Lastly, our proposed model focuses on APA using golden speech but exhibits key limitations. Its effectiveness is primarily tied to the Speechocean762 benchmark dataset, which might not capture the vast diversity of global English learners, potentially limiting the model's generalizability."}, {"title": "Ethical Considerations", "content": "The development and deployment of ZS-TTS and Automatic Pronunciation Assessment (APA) technologies also raise several ethical considerations. A major concern is the risk of inherent biases in the training data, which can lead to unequal performance across different demographic groups. Ensuring diverse and representative datasets is crucial to avoid perpetuating or exacerbating existing inequalities. Another critical consideration is accessibility and inclusivity. Finally, there is potential for misuse of synthesized speech technologies, such as creating deepfakes or misleading audio content. Implementing strict ethical guidelines and monitoring usage can help mitigate these risks."}]}