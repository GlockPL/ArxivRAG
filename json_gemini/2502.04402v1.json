{"title": "Beyond Interpolation: Extrapolative Reasoning with Reinforcement Learning and Graph Neural Networks", "authors": ["Niccol\u00f2 Grillo", "Andrea Toccaceli", "Jo\u00ebl Mathys", "Benjamin Estermann", "Stefania Fresca", "Roger Wattenhofer"], "abstract": "Despite incredible progress, many neural architectures fail to properly generalize beyond their training distribution. As such, learning to reason in a correct and generalizable way is one of the current fundamental challenges in machine learning. In this respect, logic puzzles provide a great testbed, as we can fully understand and control the learning environment. Thus, they allow to evaluate performance on previously unseen, larger and more difficult puzzles that follow the same underlying rules. Since traditional approaches often struggle to represent such scalable logical structures, we propose to model these puzzles using a graph-based approach. Then, we investigate the key factors enabling the proposed models to learn generalizable solutions in a reinforcement learning setting. Our study focuses on the impact of the inductive bias of the architecture, different reward systems and the role of recurrent modeling in enabling sequential reasoning. Through extensive experiments, we demonstrate how these elements contribute to successful extrapolation on increasingly complex puzzles. These insights and frameworks offer a systematic way to design learning-based systems capable of generalizable reasoning beyond interpolation.", "sections": [{"title": "Introduction", "content": "Neural architectures have made significant strides in various domains, yet a fundamental challenge persists: the ability to generalize effectively beyond their training distribution. This limitation is particularly evident in tasks requiring logical reasoning, where the structured nature of the problem amplifies the consequences of poor generalization. Consider a neural network trained to solve 3x3 Sudoku puzzles. While it may excel within this confined space, it often fails when presented with 4x4 or 9x9 grids, despite the underlying logical principles remaining the same. This toy example underscores a critical issue: many neural models achieve good performance during training, but fail to extract the fundamental logical relationships and dynamics governing the problem. The ultimate goal of these neural architectures is not to just interpolate between seen examples, but to genuinely understand, extract, and correctly reapply the underlying reasoning and knowledge. The key lies in developing systems that can comprehend and reason with core logical structures, enabling them to apply this knowledge to novel, more complex scenarios. This level of generalization - moving beyond interpolation to true logical understanding - is essential for creating future machine learning systems capable of robust reasoning across diverse contexts, including those that go beyond their training experiences.\nLogic puzzles provide an ideal testbed for addressing the generalization challenge in neural architectures. These puzzles offer a unique environment with clear rules and scalability. This allows us to systematically investigate a model's ability to reason beyond its training distribution by applying it to larger puzzle sizes. The inherent advantage of evaluating generalization through extrapolation is that larger puzzle configurations clearly lie outside the training distribution while the underlying rules stay the same. Therefore, we desire models that cannot only learn the correct principles from simpler scenarios, but also be applied to entirely new contexts. By controlling the difficulty and structure of puzzles, we can create such scenarios by testing whether a model has truly grasped the underlying logical principles or merely memorized patterns. Moreover, the unambiguous nature of puzzle solutions enables objective and exact measurement of performance.\nWe choose to study this problem using Reinforcement Learning (RL), as it offers distinct advantages, particu-"}, {"title": "Related Work", "content": "Reasoning and Generalization Over the years the study of reasoning with learning based approaches has often focused on the domain of games such as chess, shogi and Go (Lai 2015; Silver et al. 2016, 2017, 2018), Poker (Dahl 2001; Heinrich and Silver 2016; Steinberger 2019; Zhao et al. 2022) or board games (Ghory et al. 2004; Szita 2012; Xenou, Chalkiadakis, and Afantenos 2019; Perolat et al. 2022). While these mainly focus on correct play or in-distribution performance, the CLRS Algorithmic Reasoning Benchmark introduced by Velickovic et al. (2022) puts emphasis on generalizable reasoning. It consists of a diverse collection of well known algorithms collected from the textbook \"Introduction to Algorithms\" by Cormen et al. (2022) providing a resource to assess algorithmic reasoning for learning based approaches. Moreover, there exists a CLRS-Text (Markeeva et al. 2024) to better assess the reasoning capabilities from a language perspective as well. Abbe et al. (2024) provide a more theoretically supported view on the generalization performance of some classes of neural networks, trained with SGD. They specifically focus on Boolean functions, learned in a supervised training regime.\nGraph Neural Networks First introduced by the works of Scarselli et al. (2008), Graph Neural Networks have seen a recent emergence through a variety of new architecture types and applications (Kipf and Welling 2017a; Xu et al. 2019; Veli\u010dkovi\u0107 et al. 2018). The graph-based representation underlying these models is particularly powerful as it provides a natural framework for capturing relational information and structural dependencies between entities. This has made GNNs especially interesting for tackling combinatorial optimization problems (Dai et al. 2018; Cappart et al. 2022; T\u00f6nshoff et al. 2022) and reasoning tasks that require understanding relationships between multiple elements (Battaglia et al. 2018). A key advantage of graph-based approaches is their capability to handle problems of varying sizes and complexities. One specific research direction focuses how these models generalize across different problem instances and sizes (Xu et al. 2021; Schwarzschild et al. 2021), with benchmarks like CLRS (Velickovic et al. 2022) providing a more systematic evaluation frameworks for assessing algorithmic reasoning capabilities with a focus on size generalization. This in turn has sparked more investigations into developing appropriate tools and architectures for such reasoning (Ibarz et al. 2022; Numeroso, Bacciu, and"}, {"title": "Preliminaries", "content": "PUZZLES Benchmark\nThe PUZZLES benchmark, introduced by Estermann et al. (2024) is a comprehensive testing environment that aims to evaluate and enhance the algorithmic reasoning capabilities of reinforcement learning (RL) agents. It is centered around Simon Tatham's Portable Puzzle Collection (Tatham 2004) and encompasses 40 diverse logic puzzles that range in complexity and configuration, providing a rich ground for assessing RL algorithms' performance in logical and algorithmic reasoning tasks. In the PUZZLES benchmark, RL agents can interact with the puzzles using either a visual representation (e.g., images of the game) or a discretized representation in the form of tabular data describing the current state of the puzzle. The puzzle instances are configurable in terms of difficulty and size, allowing researchers to test different RL algorithms' scalability and generalization abilities. Additionally, PUZZLES is integrated into the widely-used Gymnasium framework.\nPUZZLES allows for the use of different observation spaces, including pixel-based inputs similar to those used in classic RL benchmarks like Atari (Bellemare et al. 2013), as well as discrete state representations that can be more suitable for logic-intensive tasks. Even though, the PUZZLES framework already allows to test generalization across different puzzles sizes in principle, preliminary results show that the proposed RL baselines struggle a lot in that domain. One current limitation is that the puzzle interface used for these test do not allow for an easy or natural way of adjusting to larger puzzle instances. In our work, we propose to overcome this issue by using a graph-based representation interface.\nGraph Neural Networks\nGraph Neural Networks (GNNs) are a class of neural networks specifically designed to operate on graph-structured data. Traditional neural architectures, such as Convolutional Neural Networks or Recurrent Neural Networks are tailored for processing image or sequential data. In contrast, GNNs are specifically designed to handle relational data by incorporating important symmetries of the data within its architecture."}, {"title": null, "content": "A graph G = (V, E) consists of V, the set of nodes and E, the set of edges. Each node v \u2208 V and each edge e = (u,v) \u2208 E may have associated feature vectors $h_v$ and $h_e$, respectively. Most common GNNs operate on the principle of message-passing which involves iterative neighborhood aggregation and node updates. Importantly, the mechanism is shared across all nodes in the graph, which allows GNNs to be applied to graphs of varying sizes, regardless of the number of nodes or edges. We follow the notion of Xu et al. (2019) to express the t-th layer as follows:\n$a_v^t = AGGREGATE(\\left\\{h_u^t | u \\in N(v)\\right\\})$\n$h_v^{t+1} = COMBINE(h_v^t, a_v^t)$.\nThe original input features of the nodes are defined as $h_v^0$ and messages from the neighborhood N(u) are aggregated and then combined with the previous state. In practice, we rely on parameterized functions $\u03a6_1$ and $\u03a6_2$ and use a permutation invariant aggregator such as sum.\n$h_v^{t+1} = \u03a6_1 \\Big(h_v^t, \\bigoplus_{u\\in N(v)} \u03a6_2 (h_v^t, h_u^t, h_{(v,u)})\\Big)$\nWith each added layer or round of message passing, the receptive field of the nodes is increased. After k rounds, a node's representation has been influenced by its k-hop neighborhood. This relationship between the number of rounds and the receptive field size is crucial for capturing local and global graph structures effectively. Moreover, we usually distinguish between node and graph level prediction tasks:\n$y_v = \u03a8_v(h_v^k)$\n$y_G = \u03a8_G(\\bigoplus_v \u03a8_v (h_v^k))$.\nFor node prediction $y_v$, the output of each node is typically a transformation of the last embedding after k rounds of message passing. Whereas for graph level prediction $y_G$, we usually apply a graph-pooling operation to aggregate all final node embeddings into a graph representation."}, {"title": "Reinforcement Learning", "content": "Reinforcement learning (RL) focuses on training agents to make sequential decisions in an environment to maximize cumulative rewards. An RL agent learns through trial and error by interacting with the environment: observing the current state, taking an action, and receiving a reward. This process is repeated, with the agent aiming to learn a policy \u03c0(as) that maximizes cumulative rewards over time. A common way to formalize an RL problem is as a Markov Decision Process (MDP), defined by states S, actions A, transition probabilities P, rewards R, and a discount factor \u03b3. The agent's goal is to find a policy \u03c0 that maximizes the expected cumulative discounted reward:\n$E_{\\pi} [\\sum_{t=0}^\\infty \u03b3^t R(s_t, a_t, s_{t+1})]$"}, {"title": "Methodology", "content": "Modeling Puzzles as Graphs\nThe PUZZLES benchmark (Estermann et al. 2024) provides a starting point for the selection of appropriate logic puzzles. While this benchmark already provides access to varying difficulties and puzzle sizes, there are a few details that make it challenging to study size generalization directly. The interface only provides the pixel observations or a discretized tabular view of the puzzle. This makes the development of models which can incorporate such a representation well when the size is varying challenging. Another aspect is that not all puzzles are equally suitable for the study of extrapolation. Indeed, some have interactive elements which make them more complex by design, while others rely on novel elements when increased in size. This adds another challenge of value generalization, even if the rules were learned correctly. For example Sudoku introduces new actions and elements in the form of additional numbers in larger instances. We aim to select a subset of puzzles which is large and diverse enough, but at the same time tries to decouple unnecessary complexities for the goal of evaluating extrapolation. Additionally, PUZZLES proposes an action space that involves moving a cursor around the playing field to then change the game state at its position. While this helps in providing a consistent action space for all puzzles, it adds an additional layer of complexity to the learning process. We therefore propose to focus on the following criteria:\n1. Action Space\nThe ability to describe the game as a collection of atomic cells that together represent the full state of the game.\n2. Independence\nEach cell can take an action independently of the actions of other cells. The resulting configuration does not necessarily have to be valid.\n3. Solvability\nThe games have no point of no return (e.g., chess). At every time step, from the current state of the game, it is always possible to solve the game.\n4. Complete Information\nThere are no interactive or stochastic elements in the game. Given just the initial state of the game, it is possible to derive its solution.\n5. Value Generalization\nThe game limits it's exposure to new concepts (new elements, increased value ranges) as it increases in size. The number of actions remains constant.\nTo create a strong and diverse benchmark, we select six puzzles that fit the above listed criteria: Light Up, Loopy, Mosaic, Net, Tents, and Unruly also shown in Figure 3. For an in-depth description of the puzzles and the rules we refer to the Appendix."}, {"title": "Training", "content": "We follow the training process outlined in PUZZLES (Estermann et al. 2024), using PPO as a relatively simple, yet solid training algorithm. Corresponding to the graph observation, each decision node in the graph represents a possible action in the game. For each puzzle, we have a distinct set of actions associated with every decision node. The fi-"}, {"title": "Extrapolation Evaluation", "content": "The most important aspect about our benchmark is to test the capability of generalizing outside the training distribution to larger puzzles. While other metrics such as training behaviour or in-distribution-performance are of interest, we"}, {"title": "Architectures", "content": "We distinguish between two different approaches to solve the puzzle, a recurrent and a state-less mode. In the recurrent mode, at each step t of the puzzle solving, the model gets as input the current state of the puzzle $x_t$ as well as the previous hidden state of the model $h_t$. Then, these embeddings are given to a processor module which can be either a GNN module or a transformer based architecture. Finally, the output of the model is $o_t$, the action for each decision-node of the puzzle and the next hidden state which is derived using a GRU unit helping to stabilize the computation over longer sequences. Whereas in the state-less mode, no hidden states $h_t$ are computed and only $x_t$, the current state of the puzzle, is provided as the sole input:\n$z_t = \u03a6_1(x_t||h_t)$\n$p_t = z_t + Processor(z_t)$\n$o_t = \u03a6_2(p_t)$\n$h_{t+1} = GRU(h_t, p_t)$.\nWe use two different processors, a graph and a transformer baseline. The graph baseline uses a GCN (Kipf and Welling"}, {"title": "Reward", "content": "Initial experiments were done using only a sparse reward: 0 reward to the agent if the game has not been solved and R reward if the game is solved. As this reward provides very little feedback during training, agents struggle to learn effective strategies leading to slow learning and poor performance, especially in complex puzzles. Instead, we work with two types of reward systems: iterative and partial.\nGiven a graph G with n decision cells (not considering meta-nodes) can be encoded with a unique sequence $\u03a6(G) = (g_i)_{i=1}^n$, with each $g_i \u2208 \\{1, 2, ..., m\\}$ denoting the i-th value on the grid and m representing the number of states a cell can take (e.g., m = 4 for Tents: empty, grass, tent, tree). The sequence corresponding to the puzzle's unique solution is denoted as \u011d = \u03a6(G), where G represents the solution graph. We measure the quality Q(G) of a graph as its similarity to the solution:\n$Q(G) = \\sum_{i=1}^n \u03b4(g_i, \u011d_i)$\nwhere \u03b4(., .) is the indicator function. To encourage policies to make iterative progress we follow the technique used by T\u00f6nshoff et al. (2022) which defines the reward $r_t$ based on the improvement in the graph's quality compared to the best quality $q_t^* = max_{t' < t} Q(G_{t'})$ observed so far. The reward is then defined as the incremental improvement of the current state relative to the highest quality achieved in prior iterations:\n$r_{iterative} = max \\Big(0, Q(G_{t+1}) \u2013 q^*\\Big)$\nThis way, the agent earns a positive reward if the actions result in a state that is closer to the solution than any previous state. To avoid penalizing the policy for departing from a local maximum in search of superior solutions, we assign a reward of zero to states that are not an improvement.\nThe puzzles often encode violations of the puzzle rules explicitly, i.e. if two neighboring cells or all cells in a row violate a constraint. In the partial reward scheme, this information is given to the actors encoded in both the decision-nodes and the meta-nodes of the puzzle state. In the partial reward system, we adjust the calculation to only include nodes that are not part of a violation. For each node we have a indicator C: \u03a6(G) \u2192 {0,1} if it is part of such a violation. Then the quality of a graph and the corresponding reward is defined as:\n$Q(G) = \\sum_{i=1}^n \u03b4(g_i, \u011d_i) \\cdot C(g_i)$\n$r_{partial} = max (0, Q(G_{t+1}) \u2013 q^\u2217)$"}, {"title": "Empirical Evaluation", "content": "Comparison to Non-Graph Baselines\nFirst, We compare our results to the baselines provided in the PUZZLES benchmark. Since the action space is different, we can only compare success rate but not episode length. The results are presented in Table 1. For the puzzles Tents, Mosaic, Loopy and Unruly, the GNN architecture was clearly able to surpass the performance of the baseline, even on much larger sizes. For Lightup and Net, the GNN architecture achieves similar performance to the baseline but on a substantially larger puzzle size. Note that all of these evaluations are done on the same size as the training distribution. For the following evaluations, we report the interquartile mean performance of all puzzles, including 95% confidence intervals based on stratified bootstrapping, following (Agarwal et al. 2021).\nGNN and Transformer Baselines\nNext, we compare the GNN architecture against a transformer baseline. The transformer baseline uses exactly the same nodes as the GNN. Because it misses the graph structure, the nodes are given a positional encoding. Our results in Figure 7 show that the GNN model performs much better than the Transformer baseline. We hypothesize that the GNN"}, {"title": "Recurrent vs State-less", "content": "Note that the puzzles are stateless, meaning that at every given moment during the solving, there is sufficient information to determine the solution with just the current puzzle state, i.e. what is visible on the board. However, it might still be beneficial to have a recurrent model for solving such a puzzle. Different steps towards computing the solution might require more computation or reasoning steps. As a consequence, for a GNN model it might be that the next correct action cannot always be determined with the information present in its 3 hop neighborhood. Therefore, it could be beneficial to allow these models access to a recurrent state, passing and storing information in between the different actions without affecting the game state. We compare a recurrent version of the graph architecture against a state-less variant in Figure 8. It seems that for modest extrapolation the recurrent version is more successful, whereas for larger sizes, the state-less architecture can solve more puzzles, even solving some instances that are 16 times larger."}, {"title": "Reward Systems", "content": "Finally, we want to evaluate the choice of reward system and its impact on size generalization. For this experiment, we use the GNN architecture with recurrent states. As depicted in Figure 6 by solely relying on sparse reward signals, it is very hard to learn to solve the puzzles, even during training. Both the iterative and partial variant show very comparable performance during training and modest extrapolation sizes. However, it seems that for the larger extrapolation setting the models trained using the partial reward generalize better."}, {"title": "Limitations", "content": "Our study focuses on how to learn generalizable reasoning, which naturally imposes certain limitations. We deliberately concentrate on model-free reinforcement learning methods, excluding model-based approaches and pre-trained models including large language models. While these methods have their own advantages, our study wants to put the focus"}, {"title": "Conclusion", "content": "The challenge of developing neural architectures capable of generalizable reasoning remains at the forefront of artificial intelligence research. Our study focuses on providing a controllable testbed of logic puzzles that can easily be scaled to test out-of-distribution generalization. Furthermore, through the unified graph representation, we demonstrate the potential of our graph-based multi-agent reinforcement learning approache in extrapolating their reasoning to larger, unseen instances.\nIn our empirical evaluation, we find that the graph-based modeling approach of the puzzles seems to be more fitting, resulting in both overall improved in-distribution and out-of-distribution performance compared to previous methods. Furthermore, we evaluate the inductive bias of a GNN architecture against a transformer baseline and find that the explicit graph structure aids generalization. Finally, we compare recurrent and state-less modeling for sequential decision making as well as different reward systems in the context of extrapolation. Our results underscore the challenges of achieving correct generalization without explicit guidance during training. This further highlights the importance of studying generalization in controlled environments. We aim to provide an stepping stone towards machine learning systems that can truly grasp the underlying reasoning and apply logical principles across diverse and increasingly complex domains."}, {"title": "Determining Puzzle Dimensions", "content": "Initial experiments have shown that if the puzzle size is chosen too small, the models tend towards overfitting on the smaller-sized games and not generalize at all. This is likely linked to the fact that on small puzzle sizes the number of different puzzle configurations is quite limited. Therefore, it is possible to more easily overfit to the training data. On the other hand, we want to avoid very large puzzle sizes during training as they often require more steps in order to be solved which is both a burden for the models to learn and computationally more expensive. Therefore, we decide to determine for each puzzle the smallest size which allows for at least 40'000 unique puzzle configurations. The final training sizes for each puzzle are highlighted in Table 3."}, {"title": "Empirical Evaluation", "content": "As highlighted previously, our objective is to train a reinforcement learning (RL) agent capable of solving a subset"}, {"title": "Training Parameters and Baseline Establishment", "content": "In addition to selecting the optimal puzzle dimensions, a crucial aspect of training the reinforcement learning (RL) algorithms involved fine-tuning several key parameters. This process ensured that the models were robust, generalized well across different puzzle sizes, and performed efficiently.\nTunable Parameters The parameters that were subject to tuning played a significant role in the performance and generalization capabilities of the RL models. These parameters were carefully selected based on initial experiments, and their values were adjusted iteratively to identify the best-performing configurations. Below is a list of the tunable parameters, along with the values that were considered during the training phase:\n1. reward_mode \u2192 [sparse, iterative, partial]. The two different reward schemes that were explained in the reward section.\n2. glbh\u2192 [recurrent, state-less]. The recurrent and stateless variants of the architecture.\nnet_arch \u2192 [gcn, transformer]. The gen flag represents the GENConv architecture of the GNN while transformer represents the transformer architecture.\nThe parameters in bold represent the baseline configuration that was used as a starting point for all experiments. This baseline provided a reference model against which other configurations were compared.\nSequential Tuning Process The training procedure followed a sequential tuning approach to manage the complexity of simultaneously testing multiple parameters, which would otherwise lead to an exponential increase in combinations. For each game and each GNN architecture, we started with the baseline configuration and then systematically tuned one parameter at a time.\nFixed Parameters While certain parameters were adjustable, others were kept constant throughout the training process to ensure consistency and comparability across experiments. Some notable ones are:\ntimesteps: Number of timesteps to reach for the algorithm to stop learning. For the GNN, set to 2,000,000 for each game, with exceptions for 'loopy' (2,400,000) and 'tents' (1,000,000). While for Transformer set to 2,000,000 for each game, with exceptions for 'loopy' (2,400,000).\nbatch_size: Fixed at 320 for GNN while is equal to 3200 for Transformer, with a minibatch size of 32 for both."}]}