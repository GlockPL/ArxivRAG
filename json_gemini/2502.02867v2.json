{"title": "Domain-Invariant Per-Frame Feature Extraction for Cross-Domain Imitation Learning with Visual Observations", "authors": ["Minung Kim", "Kawon Lee", "Sungho Choi", "Jungmo Kim", "Seungyul Han"], "abstract": "Imitation learning (IL) enables agents to mimic expert behavior without reward signals but faces challenges in cross-domain scenarios with high-dimensional, noisy, and incomplete visual observations. To address this, we propose Domain-Invariant Per-Frame Feature Extraction for Imitation Learning (DIFF-IL), a novel IL method that extracts domain-invariant features from individual frames and adapts them into sequences to isolate and replicate expert behaviors. We also introduce a frame-wise time labeling technique to segment expert behaviors by timesteps and assign rewards aligned with temporal contexts, enhancing task performance. Experiments across diverse visual environments demonstrate the effectiveness of DIFF-IL in addressing complex visual tasks.", "sections": [{"title": "1. Introduction", "content": "Imitation learning (IL) allows agents to learn complex behaviors by observing and replicating expert demonstrations without requiring explicit reward signals. This approach is widely applied in robotics, autonomous driving, and healthcare. The simplest IL technique, behavior cloning (BC) (Bain & Sammut, 1995; Pomerleau, 1991; Ross et al., 2011; Torabi et al., 2018a), directly mimics expert datasets but struggles with generalization when the agent deviates from training trajectories. Inverse reinforcement learning (IRL) addresses this limitation by inferring reward functions from expert behavior, enabling more robust learning (Ng & Russell, 2000; Abbeel & Ng, 2004; Ziebart et al., 2008). Adversarial imitation learning (AIL) builds on IRL by aligning state-action distributions between learners and experts using adversarial frameworks (Finn et al., 2016; Fu et al., 2018; Ho & Ermon, 2016; Torabi et al., 2018b; Zhang et al., 2020), often utilizing generative models like Generative Adversarial Networks (GANs) (Goodfellow et al., 2014). While effective in same-domain scenarios, these methods face challenges in cross-domain settings due to domain shifts that complicate policy transfer (Ben-David et al., 2006).\nIn cross-domain scenarios, mismatches arise from differences in viewpoints, dynamics, embodiments, and state spaces, creating significant hurdles for IL applications. For instance, autonomous driving may require learning from simulations while operating in real-world environments, or robots may rely on visual data to control physical joints. These shifts exacerbate learning difficulties, particularly with high-dimensional and noisy visual data, where even minor variations can disrupt alignment and stability. To address these issues, cross-domain IL techniques extract domain-invariant features from visual datasets to align source and target domains while retaining task-relevant information (Li et al., 2018; Liu et al., 2018; Cetin & Celiktutan, 2021; Shang & Ryoo, 2021; Choi et al., 2024). By focusing on features independent of domain-specific factors, these methods enable learners to mimic expert behavior using visual demonstrations, improving IL's effectiveness across diverse real-world scenarios (Sermanet et al., 2018).\nExisting IL methods often rely on image sequences spanning multiple timesteps to identify domain-invariant features for IRL and reward design, as single images cannot fully capture an agent's evolving behavior. However, these approaches frequently struggle with the complexity of sequence spaces, leading to misaligned features, poorly designed rewards, and suboptimal imitation of expert policies. To address these challenges, we propose Domain-Invariant Per-Frame Feature Extraction for Imitation Learning (DIFF-IL). DIFF-IL introduces two key contributions: (1) per-frame domain-invariant feature extraction to robustly isolate domain-independent task-relevant behaviors, and (2) frame-wise time labeling to segment expert behaviors by timesteps and assign rewards based on temporal alignment. Together, these innovations enable precise domain alignment and effective imitation, even in scenarios with limited overlap between source domain data and expert actions."}, {"title": "2. Related Works", "content": "Imitation Learning: IL trains agents to mimic expert behaviors. Behavior cloning uses supervised learning for replication (Kelly et al., 2019; Sasaki & Yamashina, 2020; Reddy et al.; Florence et al., 2022; Shafiullah et al., 2022; Hoque et al., 2023; Li et al., 2024; Mehta et al., 2025), while Inverse RL derives reward functions from expert demonstrations (Abbeel & Ng, 2004; Ziebart et al., 2008; Dadashi et al., 2020; Wang et al., 2022). Building on IRL, adversarial methods distinguish between expert and learner behaviors to provide reward signals (Ho & Ermon, 2016; Fu et al., 2017; Li et al., 2017; Peng et al., 2018; Lee et al., 2019; Ghasemipour et al., 2020). There are also approaches that aim to integrate the strengths of BC and IRL (Watson et al., 2024). Offline IL methods enable robust training without environment interaction (Kim et al., 2022; Xu et al., 2022b; Ma et al., 2022; Xu et al., 2022a; Hong et al., 2023; Yan et al., 2023; Li et al., 2023; Zhang et al., 2023; Sun et al., 2023), and strategies addressing dynamic shifts through diverse data have also been proposed (Chae et al., 2022).\nCross-Domain Imitation Learning (CDIL): CDIL transfers expert behaviors across domains with differences in perspectives, dynamics, or morphologies. Approaches include using the Gromov-Wasserstein metric for cross-domain similarity rewards (Fickinger et al., 2022), timestep alignment (Sermanet et al., 2018; Kim et al., 2020; Liu et al., 2018; Raychaudhuri et al., 2021), and temporal cycle consistency to address alignment issues (Zakka et al., 2022). Techniques also involve removing domain-specific information via mutual information (Cetin & Celiktutan, 2021), maximizing transition similarity (Franzmeyer et al., 2022), or combining cycle consistency with mutual information (Yin et al., 2022). Adversarial networks and disentanglement strategies further enhance domain invariance (Stadie et al., 2017; Sharma et al., 2019; Shang & Ryoo, 2021; Choi et al., 2024).\nImitation from Observation (IfO): IfO focuses on learning behaviors without access to action information. Approaches can be divided into those leveraging vectorized observations provided by the environment (Torabi et al., 2018b; Zhu et al., 2020; Desai et al., 2020; Gangwani et al., 2022; Chang et al., 2022; Liu et al., 2023; Freund et al., 2023) and those utilizing images to model behaviors (Li et al., 2018; Liang et al., 2018; Das et al., 2021; Karnan et al., 2022b;a; Belkhale et al., 2023; Zhang et al., 2024; Xie et al., 2024; Ishida et al., 2024; Aoki et al., 2024). Image-based methods, in particular, have gained attention for enabling robots to learn from human behavior captured in images, facilitating tasks like mimicking human actions (Sheng et al., 2014; Yu et al., 2018; Zhang et al., 2022; Mandlekar et al., 2023)."}, {"title": "3. Background", "content": "In this paper, all environments are modeled as a Markov Decision Process (MDP) defined by the tuple $\\mathcal{M} = (\\mathcal{S}, \\mathcal{A}, P, R, \\gamma, \\rho_0)$, where $\\mathcal{S}$ is the state space, $\\mathcal{A}$ the action space, $P: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R}^+$ the state transition probability, $R: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ the reward function, $\\gamma \\in (0, 1)$ the discount factor, and $\\rho_0$ the initial state distribution. At each timestep t, the agent selects an action $a_t \\sim \\pi$ from a stochastic policy $\\pi: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}^+$. The environment provides a reward $r_t = R(s_t, a_t)$ and the next state $s_{t+1} \\sim P(\\cdot|s_t, a_t)$. The goal in reinforcement learning (RL) is to optimize the policy $\\pi$ to maximize the discounted cumulative reward $\\sum_{t=0}^{\\infty} \\gamma^t r_t$.\nIL trains a learner policy $\\pi^L$ to mimic an expert policy $\\pi^E$ using an offline dataset $B^E$ of expert trajectories $\\tau^E$, where each trajectory $\\tau^E := (s_0, a_0, s_1, a_1,\\dots,s_H)$ consists of state-action pairs, with $a_t \\sim \\pi(\\cdot|s_t)$ for t = 0,\u2026\u2026, H \u2212 1, and H is the episode length. To improve IL performance, Generative Adversarial IL (GAIL) (Ho & Ermon, 2016) applies GAN (Goodfellow et al., 2014) principles to IL by using a label discriminator F to distinguish between learner trajectories $\\tau^L$ (label 0) and expert trajectories $\\tau^E$ (label 1). Rewards are designed in an IRL framework such that F assigns higher rewards to actions that are more ambiguous to classify. Here, the learner $\\pi^L$ acts as a generator, aiming to confuse F by performing online RL to maximize these rewards, engaging in adversarial training to align the trajectory distributions of $\\pi^L$ and $\\pi^E$. Building on this framework, Adversarial IRL (AIRL) (Fu et al., 2017) introduces a reward structure designed to enhance the learner's ability to perform IL more effectively, as follows:\n$R_F(s_t, a_t, s_{t+1}) = log \\frac{F(s_t, a_t, s_{t+1})}{1-F(s_t, a_t, s_{t+1})}$.\nTo enable practical IL in cross-domain scenarios, the expert's environment is modeled as an MDP $\\mathcal{M}_S$ in the source domain S, and the learner's as an MDP $\\mathcal{M}_T$ in the target domain T. The goal is to train the learner for cross-domain IL by minimizing domain differences and mimicking expert behavior through distribution matching techniques (Torabi et al., 2018b; Gangwani et al., 2022; Liu et al., 2023). In real-world applications, image observations in offline datasets are often used (Stadie et al., 2017; Kim et al., 2020; Zakka et al., 2022). The observation space $\\mathcal{O}_d$ is part of $\\mathcal{M}_d$ for each domain d \u2208 {S,T}, where each image frame $o_t^d$ captures a snapshot at time t. Since single frames cannot capture dynamics, IL relies on sequences of L frames, $O_{seq,t} = (o_{t-L+1},\\dots,o_t)$, with L = 4 fixed in this work.\nRecent cross-domain IL methods leverage random policies"}, {"title": "4. Methodology", "content": "In this section, we propose a domain-invariant per-frame feature extraction (DIFF) method to eliminate domain-specific information while preserving expertise-related details, enabling effective domain adaptation prior to utilizing image sequences for expertise assessment. Specifically, we define a shared encoder p and domain-specific decoders $q_d$, where d\u2208 {S, T}. The encoder p encodes image data into latent features $z_t \\sim p(\\cdot|o_t)$, while each decoder $q_d$ reconstructs the original image as $\\hat{o_t} = q_d(z_t)$. This ensures that the feature $z_t$ captures essential image characteristics. However, $z_t$ may still contain irrelevant domain-specific details (e.g., background, camera angles) in addition to expertise-related information (e.g., agent position, joint angles), which can hinder the learner's ability to interpret expertise.\nTo address residual domain-specific details in latent features $z_t$, we employ a Wasserstein GAN (WGAN) (Gulrajani et al., 2017b), where the encoder p acts as the generator and a frame discriminator $D_f$ distinguishes whether $z_t$ originates from the source or target domain. The frame discriminator $D_f$ is trained to assign higher scores to source features $z_t^S$ and lower scores to target features $z_t^T$, maximizing its ability to classify domains. Conversely, the encoder p learns in the opposite direction, aiming to confuse $D_f$. This adversarial process aligns the distributions of $z_t$ across domains, effectively removing domain-specific information in $z_t$. Simultaneously, the encoder-decoder structure preserves task-relevant details by penalizing reconstruction errors, ensuring $z_t$ retains expertise-critical information while achieving domain invariance. To further enhance alignment, we incorporate a consistency loss inspired by (Zhu et al., 2017), ensuring $z_t$ retains expertise-related information even when transferred between domains (Choi et al., 2024). Specifically, when $z_t$ passes through the opposite domain's decoder $q_{d'}$ and is re-encoded by p, the resulting latent $z_t' \\sim p(\\cdot|q_{d'}(z_t))$ remains consistent. This process removes domain-specific artifacts while preserving task-relevant features. The training involves three components: the frame discriminator loss $\\mathcal{L}_{disc, f} (D_f)$, the frame generator loss $\\mathcal{L}_{gen, f}(p)$, and the encoder-decoder loss $\\mathcal{L}_{enc-dec} (p, q)$, defined as:"}, {"title": "4.1. Domain-Invariant Per-Frame Feature Extraction", "content": "$\\mathcal{L}_{disc, f} := \\mathbb{E}_{z_t \\sim p(\\cdot|o_t), z_t' \\sim p(\\cdot|o_t')} [-D_f(z_t) + D_f(z_t')] + \\lambda_{gp,f} \\cdot GP,$\n$\\mathcal{L}_{gen, f} := \\mathbb{E}_{z_t \\sim p(\\cdot|o_t), z_t' \\sim p(\\cdot|o_t')} [D_f(z_t) - D_f(z_t')],$\n$\\mathcal{L}_{enc-dec} := \\sum_{d=S,T} \\mathbb{E}_{z_t \\sim p(\\cdot|o_t^d)} [\\lambda_{recon} ||o_t^d - \\hat{o_t^d}||_2 + \\lambda_{fcon} ||z_t - z_t'||_2],$\nwhere $\\hat{o_t} = q_d(z_t)$, $z_t' \\sim p(\\cdot|q_{d'}(z_t))$, and $\\bar{\\nabla}$ represents stopping gradient flow for x and GP represents the Gradient Penalty term to guarantee stable learning. Samples are drawn from domain-specific buffers $\\mathcal{B}_S := \\mathcal{B}_{SR} \\cup \\mathcal{B}_{SE}$ and $\\mathcal{B}_T := \\mathcal{B}_{TR} \\cup \\mathcal{B}_{TL}$, where $\\mathcal{B}_{TL}$ stores trajectories from $\\pi^{TL}$ during training. This approach ensures domain-invariant features while preserving task-relevant information, enabling robust cross-domain expertise alignment."}, {"title": "4.2. Sequential Matching with Expertise Labeling", "content": "The proposed per-frame feature extraction removes domain-specific information from individual frames. Building on this, we utilize sequences of these features, as in existing AIL methods, for expertise assessment. At each time step t, the feature sequence is defined as $z_{seq,t} := z_{t-L+1},..., z_t$, with L as the fixed sequence length. To classify expertise, we introduce a sequence label discriminator $F_{label,s}(z_{seq,t}) \u2208 [0, 1]$, trained to label feature sequences $z_{seq,t}^d$ from $B^{SE}$ as expert (label 1) and others as non-expert (label 0). Although per-frame domain-specific information is removed, domain-specific sequence differences (e.g., speeds, step sizes) may persist. To address this, we extend WGAN to feature sequences using a sequence discriminator $D_s$, ensuring residual sequence-related domain-specific information is further eliminated. In summary, the training for feature sequences also includes three components: sequence discriminator loss $\\mathcal{L}_{disc,s}(D_s)$, sequence generator loss $\\mathcal{L}_{gen,s}(p)$, and sequence label loss $\\mathcal{L}_{label,s} (F_{label, s}, p)$, are defiend as:\n$\\mathcal{L}_{disc,s} := \\mathbb{E}_{z_{seq, t} \\sim p(\\cdot|o_{seq, t}^S), z_{seq, t}' \\sim p(\\cdot|o_{seq, t}^T)} [-D_s (z_{seq,t}) + D_s (z_{seq,t}')] + \\lambda_{gp,s} \\cdot GP,$\n$\\mathcal{L}_{gen, s} := \\mathbb{E}_{z_{seq, t} \\sim p(\\cdot|o_{seq, t}^S), z_{seq, t}' \\sim p(\\cdot|o_{seq, t}^T)} [D_s (z_{seq,t}) - D_s(z_{seq,t}')],$\n$\\mathcal{L}_{label,s} := \\sum_{d=S,T} \\mathbb{E}_{z_{seq,t} \\sim p(\\cdot|o_{seq,t}^d)} [BCE(F_{label, s} (z_{seq,t}), 1_{o_{seq,t}^d \\sim B^{SE}})],$\nwhere BCE is the Binary Cross Entropy and $1_x$ is the indicator function, which is 1 if the condition x is true and 0 otherwise. The proposed loss ensures that the feature sequence $z_{seq,t}$ is free from domain-specific information, enabling pure expertise assessment. For WGAN, to balance per-frame and sequence mappings, the unified WGAN loss is redefined as: $\\mathcal{L}_{WGAN} = \\lambda_{disc} \\mathcal{L}_{disc} + \\lambda_{gen} \\mathcal{L}_{disc}$, where $\\lambda_{disc}$ and $\\lambda_{gen}$ are the scaling coefficients for discriminator and generator losses, respectively. The losses are defined as:\n$\\mathcal{L}_{disc} := \\alpha\\mathcal{L}_{disc, f} + (1 - \\alpha)\\mathcal{L}_{disc,s}$\n$\\mathcal{L}_{gen} := \\alpha\\mathcal{L}_{gen, f} + (1 - \\alpha)\\mathcal{L}_{gen,s}.$\nwhere \u03b1 \u2208 (0, 1) is the WGAN control parameter, adjusting the balance between per-frame WGAN ($\\mathcal{L}_{disc, f}, \\mathcal{L}_{gen, f}$) and sequence WGAN ($\\mathcal{L}_{disc,s}, \\mathcal{L}_{gen,s}$). Due to the large number of losses, most scales are fixed, while parameter search is conducted for the key WGAN hyperparameters \u03b1, $\\lambda_{disc}$, and $\\lambda_{gen}$, which are most relevant to the proposed DIFF method. Details on other loss scales are provided in Appendix A."}, {"title": "4.3. Frame-wise Time Labeling and Reward Design", "content": "The trained $F_{label,s}$ evaluates the expertise of feature sequences, with labels influenced by the overlap between the source domain's expert data $B^{SE}$ and random data $B^{SR}$ during sequence label loss training. When expert and random sequences overlap significantly, expert labels are distributed between 0 and 1, helping the target learner distinguish and mimic critical behaviors effectively. However, as shown"}, {"title": "5. Experiments", "content": "We evaluate the proposed DIFF-IL against various cross-domain IL methods on DeepMind Control Suite (DMC) (Tassa et al., 2018) and MuJoCo (Todorov et al., 2012), pairing similar tasks as source and target domains. The evaluation shows how well the target learner mimics the source expert and analyze the effectiveness of key components."}, {"title": "5.1. Experimental Setup", "content": "For comparison, we evaluate cross-domain IL methods using images: TPIL (Stadie et al., 2017), which extracts domain-invariant features from image sequences; DeGAIL (Cetin & Celiktutan, 2021), which enhances domain information removal with mutual information; D3IL (Choi et al., 2024), which isolates expertise-related behavior using dual consistency loss; and DIFF-IL (Ours). Additionally, GWIL (Fickinger et al., 2022), a state-based approach leveraging Gromov-Wasserstein distance, serves as a baseline. For DIFF-IL, we primarily tuned WGAN hyperparameters (\u03b1, $\\lambda_{disc}$, $\\lambda_{gen}$), fixing other loss scales. \u03b1 = 0.5 delivered consistently strong performance across environments, while $\\lambda_{disc}$ and $\\lambda_{gen}$ were optimized per environment. Further experimental details are provided in Appendix B."}, {"title": "5.2. Environmental Setup", "content": "We compare the baselines in environments with significant domain differences, focusing on adaptation across tasks with varying agent properties like joints, action spaces, and dynamics, rather than simple changes in viewpoint or color."}, {"title": "5.3. Performance Comparison", "content": "For various domain adaptation scenarios, Table 1 presents the mean final return results averaged over the last 10 episodes for all evaluated environments, categorized by method. From the results, it is evident that the proposed algorithm significantly outperforms other cross-domain IL methods in most environments. Notably, Fig. 7 illustrates the learning curves over time steps in MuJoCo environments. While other algorithms fail to closely replicate expert performance and often struggle to learn effectively, the proposed method successfully mimics expert behavior, enabling the agent to move efficiently and achieve higher scores. However, when Hopper is the source domain, the target performance plateaus below expert levels due to the physical limitations of the Hopper agent, which restrict its maximum achievable speed. As the Hopper expert itself cannot achieve higher speeds, the target domain inherits this limitation, resulting in capped performance. In addition, we also provide detailed learning curves for Pendulum tasks in Appendix C. These results demonstrate not only superior final performance across most tasks but also significantly faster convergence compared to other cross-domain IL methods, even within the same time steps. This improvement is attributed to the better-designed rewards in the proposed approach. Overall, these comparisons highlight the algorithm's ability to achieve superior domain adaptation and more effectively mimic the source domain's expert behavior compared to existing methods."}, {"title": "5.4. Image Mapping and Reward Analysis", "content": "In IL, understanding how expert behavior is mimicked is as crucial as performance. To analyze how the proposed DIFF-IL effectively mimics a source expert across domains, Fig. 8 focuses on the Walker-to-Cheetah environment, where DIFF-IL significantly outperforms other methods. The figure examines the target learner's (TL) progression toward the goal over time, presenting its image frames at initial, middle, and final timesteps mapped to those of the source expert (SE). Frame and sequence label predictions obtained by the label discriminators $F_{label, f}$ and $F_{label,s}$ along with the rewards R, are also visualized. For image mapping, each TL frame passes through the encoder p, producing a feature $z_t^L$, which is then matched to the SE image in the dataset with the closest feature $z_t^S$, as Fig. 2. The accompanying t-SNE graph, showing latent features of SE, SR, learned TL, and TR (identical to Fig. 1), illustrates the feature locations and confirms a one-to-one mapping between TL and SE frames across all timesteps. This demonstrates that the learned frame features effectively capture task-relevant positions while excluding domain-specific details.\nThe t-SNE visualization also reveals minimal overlap between SE and SR data distributions. Consequently, relying solely on sequence labels often misclassifies behaviors slightly deviating from random policies as expert, leading to suboptimal mimicry. In contrast, frame labels finely segment expert behavior over time, assigning higher rewards to frames closer to the goal. This segmentation ensures the agent progresses effectively and aligns its behavior with task objectives. These findings underscore the advantages of the proposed DIFF with frame-wise time labeling, enabling effective goal-oriented learning. Additional analyses,"}, {"title": "5.5. Ablation Studies", "content": "To evaluate the components of proposed DIFF-IL, we compare 4 configurations: 'W/O WGAN (Seq.)', excluding sequence-based WGAN losses $\\mathcal{L}_{disc,s}$ and $\\mathcal{L}_{gen,s}$; \u2018W/O $F_{label, f}$\u2019, omitting frame-wise time labeling while retaining per-frame feature extraction; 'Seq. Mapping Only', using only sequence-based mapping and labeling; and 'DIFF-IL', the full method. Fig. 9 compares performance in Walker-to-Cheetah and Pend-to-CS, where DIFF-IL shows the superior performance. Result shows that 'Seq. Mapping Only' fails to adapt effectively, while \u2018W/O WGAN (Seq.)\u2019 and \u2018W/O $F_{label, f}$\u2019 show moderate improvements. DIFF-IL achieves the highest performance compared to other setups by combining per-frame domain-invariant feature extraction and frame-wise time labeling to prioritize later-stage frames, highlighting their impact on domain adaptation and task success.\nTo investigate the impact of hyperparameters in DIFF-IL, we conducted an ablation study on WGAN-related hyperparameters. Here, we examine the WGAN control factor \u03b1, which balances per-frame and sequence-level mapping in DIFF-IL. Fig. 10 compares performance in Walker-to-Cheetah and Pend-to-CS for \u03b1 = 0.1, 0.5, and 0.9. The results indicate that \u03b1 = 0.5 achieves the best performance, validating it as the default setting. Lower or higher values reduce performance, highlighting the need for balanced per-frame and sequence-level domain adaptation for effective feature extraction. Additional analyses on \u03b1 and other hyperparameters across more environments are detailed in Appendix E."}, {"title": "6. Conclusion", "content": "In this paper, we propose a novel cross-domain IL approach, addressing challenges in image-based observations. Combining per-frame feature extraction with frame-wise time labeling, DIFF-IL successfully removes only domain-specific information. This enables superior alignment and performance, even under significant domain shifts, as demonstrated by experiments showcasing enhanced domain-invariant image mapping and accurate behavior imitation."}, {"title": "A.1. Redefined Loss Functions for DIFF-IL", "content": "In this section, we redefine the losses in DIFF-IL, explicitly including their associated parameters, as described in Section 4.The encoder is parameterized as \u03d5, the domain-specific decoders as \u03c8S (source) and \u03c8T (target), the frame and sequence discriminators as Df and Ds, and the frame and sequence label discriminators as Xf and Xs, respectively.\nThe unified WGAN losses for the discriminator and generator are redefined as:\n$\\mathcal{L}_{disc}(D_f, D_s) := \\lambda_{disc}\\cdot [\\alpha\\cdot \\mathbb{E}_{o_t^S, o_t^T}(-D_f(z_t^S) + D_f(z_t^T)) + (1 - \\alpha)(-D_s(z_{seq,t}^S) + D_s(z_{seq,t}^T))] + \\lambda_{gp}\\ GP \\qquad(A.1)$\n$\\mathcal{L}_{gen}(\\phi) := \\lambda_{gen}\\cdot [\\alpha\\cdot \\mathbb{E}_{o_t^S, o_t^T}(D_f(z_t^S) - D_f(z_t^T)) + (1 - \\alpha)(D_s(z_{seq,t}^S) - D_s(z_{seq,t}^T))] \\qquad(A.2)$\nwhere GP is gradient penalty term, $z_t \\sim p_{\\phi}(o_t)$ and $z_{seq,t} \\sim p_{\\phi}(\\cdot | o_{seq,t})$ for d \u2208 {S, T}. The coefficients $\\lambda_{disc}$, $\\lambda_{gen}$, and $\\lambda_{gp}$ control the contributions of the losses, while \u03b1 balances frame- and sequence-based mappings.\nThe encoder-decoder loss, incorporating generator, reconstruction, and feature consistency losses, is redefined as:\n$\\mathcal{L}_{enc-dec}(\\phi, \\psi^S, \\psi^T) := \\sum_{d=S,T} \\mathbb{E}_{o_t^d \\sim p_{\\phi}(o_t^d)} [\\lambda_{recon} ||o_t^d - \\hat{o_t^d}||_2 + \\lambda_{fcon} ||z_t - z_t'||_2] \\qquad(A.3)$\nwhere d' is the opposite domain of d, $\\hat{o_t^d} = \\psi^d(z_t)$ and $z_t' \\sim p_{\\phi}(\\cdot | \\psi^{d'}(z_t))$, with coefficients $\\lambda_{recon}$ and $\\lambda_{fcon}$ controlling reconstruction and feature consistency losses. The sequence label loss and the frame-wise time labeling loss are redefined as:\n$\\mathcal{L}_{label,s}(\\phi, \\chi_s) := \\sum_{d=S,T} \\lambda_{label,s}\\cdot \\mathbb{E}_{o_{seq,t}^d \\sim p_{\\phi}(o_{seq,t}^d)} [BCE(F_{X_s}(z_{seq,t}), 1_{o_{seq,t}^d \\sim B^{SE}})] \\qquad(A.4)$\n$\\mathcal{L}_{label,f}(\\chi_f) := \\lambda_{label,f}\\cdot \\mathbb{E}_{z_t \\sim p_{\\phi}(\\cdot | o_t)} [BCE(y_t, F_{X_f}(z_t))] \\qquad(A.5)$\nwhere $\\lambda_{label,s}$ is the sequence label loss coefficient and yt is the time label for frame $o_t^S$. Finally, the reward is redefined as:\n$R_t = -log(1 - F_{X_s}(z_{seq,t+1})\\cdot F_{X_f}(z_t) + \\epsilon) \\qquad(A.6)$\nwhere \\epsilon < 1 \u00d7 10-12 prevents numerical issues when the product of the sequence and frame labels approaches 1. Details of the loss scale coefficients for all losses are summarized in Appendix B.3."}, {"title": "A.2. Implementation of GP", "content": "To ensure stable training of the adversarial network, the WGAN framework (Gulrajani et al., 2017a) incorporates a gradient penalty (GP) to enforce 1-Lipschitz continuity for the discriminator. In the redefined discriminator loss in Eq. (A.1), the GP term can be defined as follows:\n$Gradient\\ Penalty = (||\\nabla_{\\hat{o_t} \\sim D_f}D_{\\phi}(S_{label, f}) + (1 - \\alpha)\\cdot \\nabla_{\\hat{o_{seq,t}} \\sim D_s}D_{\\phi}(S_{label, s}) ||_2 - 1)^2 \\qquad(A.7)$"}, {"title": "A.3. RL Implementation", "content": "To train the target learning policy $\\pi^{TL}$, we parameterize both the policy $\\pi^{TL}$ and the state-action value function Q using parameter \u03b8. Utilizing Soft Actor-Critic (SAC) (Haarnoja et al., 2018), the critic and actor losses are defined as follows:\n$L_Q(\\theta) = \\mathbb{E}_{(s_t, a_t, s_{t+1}, o_{seq,t}) \\sim B^{TL}} [\\frac{1}{2}(Q_{\\theta}(s_t, a_t) - (R_t + \\gamma \\mathbb{E}_{a_{t+1} \\sim \\pi_{\\phi}(s_{t+1})} [Q_{\\theta'}(s_{t+1}, a_{t+1}) - \\lambda_{ent}\\cdot log(\\pi_{\\theta}(a_t | s_t))]))^2] \\qquad(A.10)$\n$\\mathcal{L}_{\\pi}(\\theta) = \\mathbb{E}_{s_t \\sim B^{TL}}[D_{KL}(\\pi_{\\phi}(\\cdot|s_{t+1}) \\| \\frac{exp(Q_{\\theta}(s_t, a_t)/\\lambda_{ent})}{Z_{\\theta}(s_t)} )] \\qquad(A.11)$\nwhere DKL represents the Kullback-Leibler (KL) divergence, $Q_{\\theta}(s, a)$ denotes the parameterized state-action value function, \u03b8' is the parameter of the target network updated via the exponential moving average (EMA) method, $\\pi_{\\theta}(a|s)$ represents the target learner policy parameterized by \u03b8, and Rt is computed as in Eq. A.6, capturing the estimated effect of actions. The critic loss minimizes the difference between the predicted value Q\u03b8 and the target value derived from the Soft Bellman equation, ensuring accurate value estimation. The actor loss minimizes the divergence between the policy \u03c0\u03b8 and the Softmax distribution induced by Q, encouraging the policy to prioritize actions that maximize long-term rewards. To enhance training stability, SAC incorporates double Q-learning and automatic adjustment of the entropy coefficient \\lambda_{ent}."}, {"title": "A.4. Network Architecture and Configurations", "content": "This subsection outlines the architecture of the networks used in DIFF-IL", "follows": "n- Encoder (p\u03c6): A convolutional neural network that extracts features from input data. It comprises convolutional layers with 16, 32, and 6"}]}