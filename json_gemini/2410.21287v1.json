{"title": "A Systematic Assessment of OpenAI o1-Preview for Higher Order Thinking in Education", "authors": ["Ehsan Latif", "Yifan Zhou", "Shuchen Guo", "Yizhu Gao", "Lehong Shi", "Matthew Nyaaba", "Gyeonggeon Lee", "Liang Zhang", "Arne Bewersdorff", "Luyang Fang", "Xiantong Yang", "Huaqin Zhao", "Hanqi Jiang", "Haoran Lu", "Jiaxi Li", "Jichao Yu", "Xuansheng Wu", "Weihang You", "Zhengliang Liu", "Vincent Shung Liu", "Hui Wang", "Zihao Wu", "Jin Lu", "Fei Dou", "Ping Ma", "Ninghao Liu", "Tianming Liu", "Xiaoming Zhai"], "abstract": "As artificial intelligence (AI) continues to advance, it demonstrates capabilities comparable to human intelligence, having significant potential to revolutionize education and workforce development. To assess the state-of-art AI capacity, this study provides a comprehensive evaluation of OpenAI ol-preview's ability to perform higher-order cognitive tasks across 14 distinct dimensions, including critical thinking, systems thinking, computational thinking, design thinking metacognition, data literacy, creative thinking, abstract reasoning, quantitative reasoning, logical reasoning, analogical reasoning, and scientific reasoning. In each dimension, we adopt existing instruments with high reliability and validity, such as the Ennis-Weir Critical Thinking Essay Test and the Biological Systems Thinking Test to systematically assess ol-preview's higher-order thinking in comparison to human performance. We found overall outperformance of ol-preview for most of the categories:\n\u2022 ol-preview achieved significantly outperform (150%) as compared to humans for systematic thinking, computational thinking, data literacy, creative thinking, scientific reasoning, and abstract reasoning.\n\u2022 For logical reasoning, critical thinking, and quantitative reasoning, ol-preview slightly under-performed by 25% than human such as for logical reasoning.\n\u2022 For analogical reasoning, we found that both human and ol-preview achieve perfect scores for each given task.\nNevertheless, we identify ol-preview's limited capacity in thinking such as abstract reasoning, where humans (i.e., psychology students) perform better than ol-preview, indicating that human oversight remains crucial for tasks requiring high-level abstraction. The findings have broad implications for education, advocating a shift in educational paradigms to emphasize the development of complementary human skills, such as creativity, abstract reasoning, and critical thinking. This study highlights the transformative role of AI in education and calls for a recalibration of educational goals, teaching methodologies, curricula, and assessments to better align with the AI-driven world.", "sections": [{"title": "Introduction", "content": "The rapid advancements in artificial intelligence (AI) have led to the development of sophisticated models that are increasingly integrated into educational settings [1, 2]. OpenAI ol-preview, a prominent large language model (LLM) based on the GPT-4 architecture, represents one such technological innovation [3]. OpenAI o1-preview demonstrates improved capabilities in understanding and generating complex, nuanced text, exhibiting greater accuracy and coherence in its responses. Compared to earlier models, it excels at handling open-ended prompts, facilitating more advanced reasoning and problem-solving tasks [4]. Additionally, OpenAI ol-preview features enhanced abilities in context retention and task-specific adaptation, positioning it as a superior tool for educational applications that require deep engagement with higher-order thinking. OpenAI ol-preview can potentially be implemented in diverse learning environments, aiding students and educators in problem-solving, critical thinking, and creative endeavors. As the presence of AI systems like OpenAI ol-preview grows in education, it becomes critical to systematically assess their effectiveness, particularly in promoting cognitive skills that extend beyond basic knowledge retrieval [5].\nHuman thinking is inherently complex and multidimensional, characterized by cognitive processes such as analysis, evaluation, and synthesis [6]. These higher-order thinking skills are vital to intellectual development, fostering innovation, problem-solving, and decision-making. Society's advancement is tightly linked to the cultivation of these skills, as they are essential for addressing complex real-world challenges. In education, fostering these skills is a primary goal, preparing students not only to consume knowledge but also to critically engage with and contribute to their fields of study [7]. Given the intricate nature of these cognitive processes, any assessment of AI's role in education must examine its capacity to facilitate and support higher-order thinking.\nOpenAI ol-preview demonstrates significant potential in assisting with complex cognitive tasks, but its actual performance in competing and fostering higher-order thinking remains largely unexplored. While it can generate text-based responses that appear coherent and insightful, the extent to which it can truly support deep, critical thinking or engage learners in meaningful ways has not been comprehensively evaluated. The nuanced nature of higher-order cognitive tasks demands a robust analysis of how AI tools like OpenAI ol-preview respond to open-ended, challenging prompts, how they solve critical problems, and whether they possess skills such as evaluation and creativity. Without systematic analysis, the question of whether OpenAI ol-preview is genuinely capable of higher-order thinking in education remains unanswered.\nThis study assesses OpenAI ol-preview's ability to engage in higher-order thinking across 14 distinct cognitive domains, each evaluated using a specific instrument or dataset tailored to measure skills such as critical thinking, systems thinking, design thinking, and more. The instruments used in this study were selected for their alignment with established human cognitive benchmarks. For example, the Ennis-Weir Critical Thinking Essay Test, widely recognized in educational research, served as the benchmark for assessing critical thinking. For System Thinking, three distinct instruments were utilized: the Biological Systems Thinking Test, The Village of Abeesee, and The Lake Urmia Vignette (LUV). Computational Thinking was evaluated using the Bebras Challenge problems and the Algorithmic Thinking Test, while Design Thinking was tested through Leetcode coding questions designed to improve problem-solving efficiency. Each instrument provided a robust foundation for comparing ol-preview's performance with human intelligence benchmarks.\nThe comparative analysis reveals a mixed but promising landscape for OpenAI ol-preview (See Fig. 1 for collective performance view of ol-preview versus human for each higher order thinking category). In the domain of Critical Thinking, as measured by the Ennis-Weir Critical Thinking Essay Test,"}, {"title": "OpenAI ol Architecture and the Unique Features", "content": "The OpenAI 01 [3] series represents a significant advancement in artificial intelligence. It represents a new generation of models with enhanced reasoning capabilities. These models are specifically designed to handle complex, multi-step problems, and they excel in tasks that require advanced problem-solving skills, such as coding, mathematics, and scientific reasoning. This section explores the architecture and unique features that make the OpenAI ol-preview models stand out based on the information from the Open AI official document [8]."}, {"title": "New Approach to AI Reasoning", "content": "The ol-preview models utilize a novel training methodology that differs fundamentally from previous LLMs developed by OpenAI. Instead of solely relying on learning patterns from vast amounts of training data, the ol-preview models were trained using a specialized optimization algorithm and a custom dataset tailored to enhance their reasoning abilities. This approach enables the model to engage in more structured thinking, making it capable of solving intricate problems with a higher degree of accuracy."}, {"title": "Internal Reasoning Tokens and Chain-of-Thought Process", "content": "A unique feature of the ol-preview series is the introduction of reasoning tokens [8], which are used by the model to break down problems into a step-by-step [9] internal thought process. When the ol-preview model receives a prompt, it generates reasoning tokens that represent the intermediate steps it takes to reach a final answer. These tokens are not visible to the user but occupy space in the model's context window, and they contribute to the overall token usage.\nFor example, in a complex mathematical problem, the ol-preview model might use reasoning tokens to sequentially evaluate different parts of the equation, check its intermediate calculations, and refine its approach before providing a final solution. This internal chain-of-thought [9] processing enables the model to achieve better accuracy, particularly in challenging tasks such as scoring 83% on the International Mathematics Olympiad (IMO) qualifier, compared to the 13% achieved by previous models like GPT-40 [10]."}, {"title": "Variants of the ol-preview Model: ol-preview and ol-preview-mini", "content": "The OpenAI ol-preview series includes two primary variants:\n\u2022 ol-preview: This is the full version of the o1-preview model, optimized to tackle sophisticated problems using broad general knowledge. It is particularly effective in handling tasks that require deep reasoning and detailed understanding.\n\u2022 01-mini: A smaller, faster, and more cost-effective variant, ol-preview-mini is adept at handling coding, mathematics, and scientific tasks that do not require extensive general knowledge. Despite its smaller size, the ol-mini retains many of the reasoning capabilities of the ol-preview, making it a practical choice for applications with cost or speed constraints.\nBoth models offer a substantial context window of 128,000 tokens [8], which allows them to manage complex tasks requiring long sequences of reasoning. The ol-preview variant can generate up to 32,768 output tokens, while ol-mini can handle up to 65,536 tokens, providing flexibility in handling diverse applications."}, {"title": "Reinforcement Learning and Problem-Solving", "content": "A distinctive aspect of ol is its training via reinforcement learning, particularly in the context of optimizing multi-step reasoning through chain-of-thought processes. In this framework, reinforcement learning helps guide the model toward generating useful intermediate steps that lead to the correct final output.\nThe ol model is possibly trained using Proximal Policy Optimization (PPO) [11], a popular reinforcement learning (RL) algorithm that iteratively refines the model's policy $\\pi_{\\rho}(a|s)$ in this case, the model's strategy for producing reasoning steps-while maintaining stability in updates."}, {"title": "Methods", "content": "OpenAI ol-preview is trained on reinforcement learning paradigm that allow it to think before answer. According to OpenAI [21], 01 thinks longer before getting to conclusion similar to human thinking process. By means of reinforcement learning, ol acquires the ability to enhance its cognitive flow (chain-of-thought) and optimize its employed tactics. It gains the ability to identify and fix its errors. It gains the ability to deconstruct complex steps into easier ones. When a method doesn't succeed, it learns to try another one. As ol inherently works on optimized chain-of-thought using reinforcement learning to provide best possible solutions, hence it is not recommended to use prompting techniques such as chain-of-thought and tree-of-thoughts. Given the guidelines by OpenAI we opt for a raw methodology to evaluate the performance of OpenAI ol-preview. Fig. 2 is the template prompt we opt for testing."}, {"title": "Analysis and Findings", "content": "The test aim is to explore ST knowledge or skills of OpenAI ol-preview, thus, we adopted behavior-based instrument to conduct the assessment. Based on the consideration, scales and interviews are also not suitable for this research, as scales are mostly for measuring preference, while interview protocols pose challenges for implementation. Meanwhile, we selected the instruments while considering the characteristics of OpenAI ol-preview, aiming to encompass a wide range of diversity. This approach allows us to assess the performance of OpenAI ol-preview across different contexts, fields, types, formats, as well as in comparison with test-takers at various educational levels. Due to the limited capacity of o1-preview to generate images, we did not select any instruments in the format of mapping, even though it is one of the widely used methods for assessing ST skills. Similarly, items with pictures containing indispensable information in the item prompt that can impact the ST performance of ol-preview were also excluded.\nAt last, we selected three qualified instruments that cover diverse contextual fields, including biology, engineering, and socio-environmental issues. The formats of the instruments include open-ended questions, fill-in-the-blank, multiple-choice, and scenario-based ill-structured questions. The targeted participants range from middle school to graduate students. The theoretical framework on which the instruments are based are also various. A summary of the corresponding information of the three instruments can be found in Table 4. General descriptions of the original instruments are as follows."}, {"title": "Critical Thinking", "content": "Critical thinking (CT) is increasingly recognized as a fundamental skill for success in education and across various fields. It involves the ability to analyze complex information, evaluate evidence, and form reasoned judgments, allowing individuals to navigate the challenges of an ever-evolving world [22]. From solving scientific problems to making decisions in daily life, CT plays a critical role in fostering problem-solving and decision-making capabilities. It has been widely studied and measured in academic research, providing educators and professionals with valuable insights into its role in effective learning and professional growth [23]. Research has shown that individuals with strong CT skills can apply logical reasoning strategies to real-world scenarios, making it an essential competency in education today [24]. Recent evaluations, such as those on PlanBench and other benchmarks, suggest that OpenAI's ol-preview outperforms previous models in complex reasoning and planning tasks [13, 25]. However, there are suggestions that more evidence is needed to conclude whether these models can fully match human-level critical thinking[13].\nIn this section, we examine the capabilities of the ol-preview model in critical thinking assessment, specifically focusing on its performance in reasoning tasks, specifically to evaluate the reasoning in argumentative essays [24, 26, 27]. For this section, we used the Ennis-Weir Critical Thinking Essay Test (EWCTET) to assess ol-preview model's critical thinking abilities because EWCTET provides a structured framework for evaluating argumentation and reasoning, key aspects of critical thinking. Also, this instrument is an openly accessible written test widely used in education, making it a suitable and reliable tool for evaluating a model that primarily functions through text-based input."}, {"title": "Assessment Instrument and Prompt Strategy", "content": "The Ennis-Weir Critical Thinking Essay Test is designed to assess critical thinking skills, focusing on the ability to evaluate arguments and reasoning through the critique of fictitious letters to newspaper editors. Initially developed as both a teaching tool and an assessment framework for critical thinking courses, this instrument has been validated through many studies, confirming its psychometric robustness [27]. In prompting the ol-preview model, the original instructions of the instrument were slightly modified for clarity and brevity: we removed the stipulation of the time frames and participants locations and name. The one-shot prompt strategy was used (see Figure 1), meaning the model was provided with the full instructions and asked to respond in a single attempt, without multiple rounds of feedback or refinement[28]."}, {"title": "ol-preview Performance on The Ennis-Weir Critical Thinking Essay Test", "content": "The distribution of the ol-preview model's performance on the EWCTET demonstrates strong and consistent critical thinking abilities, as it was able to identify logical fallacies and weaknesses, though there were some minimal instances of lower scores indicating potential areas for deeper analysis (See Table 1). The reveals low variability (sd= 0.43), showing consistent performance across the evaluated paragraphs. The absence of negative or neutral scores indicates a uniformly high level of critical analysis. The average (M=+2.25) representing 81.25% showed a slightly higher overall performance, particularly in paragraphs addressing analogical reasoning and an advanced understanding of rhetorical devices and precise definitions in argumentation. However, o1-preview scored (score = +2) in some paragraphs, where further specificity were required. The overall evaluation in showed that ol-preview model has the ability to synthesize individual analyses into a comprehensive critique, demonstrating advanced critical thinking skills by summarizing strengths, weaknesses, and overarching logical flaws in a cohesive manner."}, {"title": "Comparing ol-preview Performance versus Human Performance on Ennis-Weir Critical Thinking Essay Test", "content": "Several empirical studies have explored human performance on EWCTET, regarding students critical thinking abilities in education. For instance, in a study conducted by [29], advanced English learners in a medical school were assessed for their critical thinking skills using the Ennis-Weir Critical Thinking Essay Test (EWCTET). In the pretest, the average student in a normal classroom setting exhibited relatively lower critical thinking skills, with a mean score of 43.57% (See Table 3). Similarly, in another study by [30], pre-service teachers demonstrated even lower critical thinking skills in their pretest phase, with a mean score of 15.30%. However, after interventions were introduced, critical thinking strategies in [29] study and the Critical-Inquiry-Based Learning model in [30] study there was a significant improvement in students' critical thinking abilities, with mean scores rising to 71.42% and 87.60%, respectively, in the posttest phase. In comparison, the ol-preview preview, using a zero-shot approach (i.e., without prior prompt or follow-up promt), was able to achieve a 81.25% score on the EWCTET, demonstrating a higher baseline of critical thinking ability than the average student in both pre-intervention studies and surpassing the human bechmark mean of 37.5% to 62.5% (originally 15/40 to 25/40). This performance suggests that the ol-preview preview has the ability to critically evaluate arguments is comparable to or even exceeds the capabilities of students following targeted critical thinking interventions.\nWe advocate for further exploration of how the ol-preview can be used to support and enhance student critical thinking skills in educational settings. Given the challenges many students face in developing the ability to critically analyze, evaluate, and construct well-reasoned arguments, as reflected in the pretest results from both [29, 30], ol-preview Preview's ability to engage in such complex reasoning processes without guided instruction is significant. Further studies are necessary to explore how ol-preview Preview can contribute effectively and responsibly to student learning, particularly in fostering critical thinking skills where students often struggle.\nHowever, there are limitations to these findings, as they are based on a limited number of empirical studies. Again, even though the required rubric was used to asses the ol-preview's performance but"}, {"title": "System Thinking", "content": "The world have been becoming increasingly complex nowadays, where people are facing with a number of grand social, technical and ecological challenges that will have impacts on our society and planet globally [31]. To understand and to attempt to solve these multifaceted issues both now and in the future, systems thinking (ST) has emerged as one of the most essential skills people must develop [32, 33]. ST is a holistic approach that examines an object or situation as a whole while recognizing the inter-connectedness within complex systems. It is helpful for individuals to grasp the whole picture, understand different levels contributing to the causes of a problem, and find a way to solve problems in various types of systems, such as ecosystems, health care, technology, economy, laws, and so on [34]. Several researchers have claimed that ST is one special skill that sets humans apart from artificial intelligence [35], which is one of the important reasons that ST has been particularly highlighted in various educational fields these years [36, 37]. Thus, exploring ol-preview's performance on ST competence and comparing it to human capabilities would be meaningful and intriguing."}, {"title": "Definition and conceptualization of system thinking", "content": "Across disciplines, there are many different definitions of ST, as well as numerous lists of ST skills. However, we can still arrive at the commonality in this thinking methodology. As Mambrey, Timm, et al. [33] stated, an operationalized definition of ST is \u201ca conceptual skill in which superordinate principles of complex systems are taken into account when understanding and predicting the interplay and function of their elements.\u201d\nHowever, when delving into specific disciplines, researchers formed different constructs of ST skills based on their understanding of the characteristics of the field. Most of the studies are conducted in biology and engineering fields. In a recently published study assessing ST in biology, four ST skills were defined based on previous studies [36]. System organization (SO), stands for the identification of system components and the relationships among them. System behaviour (SB), refers to analysing the development, dynamic processes, and cause-and-effect relationships among system components"}, {"title": "Assessment of system thinking", "content": "Various instruments have been developed to assess ST based on various theoretical frameworks in different disciplines, using different methods, and aimed at different educational levels. As we mentioned before, the definition and constructs of ST varied across different disciplines, thus resulting in different instruments and/or rubrics developed for assessment. Most studies are conducted in the context of biology, earth science, and engineering fields [31]. Environmental topics ranging from ecology to climate change, sustainability, and energy use are widely used scenarios. Students are the main test group, ranging from elementary school students to graduate students. At the same time, a few studies conducted ST assessment of pre-service and in-service teachers [40, 41, 42].\nIn terms of assessment methods, in a systematic literature review of the ST measurement based on 27 distinct assessments, Dugan et al. [31] identified four types of ST assessments, which are behavior-based, preference-based, self-reported, and cognitive activation. The majority of assessments were behavior-based, which means the participants need to complete specific tasks, such as drawing or answering various types of test questions, to externalize their ST knowledge or skills. Preference-based assessments, which characterize values, interests, attitudes, and/or aptitudes, are usually measured using scales and are ranked as the second most commonly used type. Self-report measures are also highly used, however, a recently study, which explored the relationships between student self-report and scenario-based assessment performance of ST, indicated that there may be limitations to using self-report scales and suggested alternative formats for assessing ST [43].\nWhen it comes to assessment format, which stands for how assessment data were collected, possible ways include mapping, interview, scenario-based items, open-ended items, fill-in-the-blank, multiple-choice, and so on [31]. Mapping is the most widely used and acknowledged method in assessing ST, often accompanied by an interview. However, due to the difficulties in implementation on a large scale, recent studies have also developed many paperandpencil tests using different types of items [36]. Especially for scenario-based items, studies considered this type most potential to elicit student"}, {"title": "Testing strategies", "content": "The test aim is to explore ST knowledge or skills of OpenAI ol-preview, thus, we adopted behavior-based instrument to conduct the assessment. Based on the consideration, scales and interviews are also not suitable for this research, as scales are mostly for measuring preference, while interview protocols pose challenges for implementation. Meanwhile, we selected the instruments while considering the characteristics of OpenAI ol-preview, aiming to encompass a wide range of diversity. This approach allows us to assess the performance of OpenAI ol-preview across different contexts, fields, types, formats, as well as in comparison with test-takers at various educational levels. Due to the limited capacity of o1-preview to generate images, we did not select any instruments in the format of mapping, even though it is one of the widely used methods for assessing ST skills. Similarly, items with pictures containing indispensable information in the item prompt that can impact the ST performance of ol-preview were also excluded.\nAt last, we selected three qualified instruments that cover diverse contextual fields, including biology, engineering, and socio-environmental issues. The formats of the instruments include open-ended questions, fill-in-the-blank, multiple-choice, and scenario-based ill-structured questions. The targeted participants range from middle school to graduate students. The theoretical framework on which the instruments are based are also various. A summary of the corresponding information of the three instruments can be found in Table 4. General descriptions of the original instruments are as follows."}, {"title": "Comparing ol-preview and human performance", "content": "For items selected in instrument 1, the mean score and standard deviation of middle school participants [36], along with the performance of o1-preview, are shown in Table 5. With a full score of three for each item, regardless of the testing format, ol-preview achieved the highest level in every item, performing far better than middle school students on average. According to the scoring rubric, the results indicated that ol-preview can identify several elements and complex relations; analyze developments based on feedback, cycles, and emergence in biological systems; make predictions and decisions based on complex biological system analysis; determine the validity of biological systems structure and the limitation of the application based on the complex analysis of effects [36]. Additionally, ol-preview took 41 seconds to complete the four items.\nFor instrument 2, according to the scoring rubric, there are seven scoring dimensions: problem identification, information needs, stakeholder awareness, goals, unintended consequences, implemented challenges, and alignment. Davis et al. [43] reported the performance of undergraduate students on the instrument, with a mean score and standard deviation for each dimension shown in Table 6. The minimum and maximum scores for each dimension are 0 and 3, respectively. For the performance of ol-preview, it scored the highest mark in all the dimensions, as shown in Figure 6, which was obviously better than the average performance of undergraduate students [43]. ol-preview took 52 seconds to complete the task, compared to 30-45 minutes for human test administration.\nFor instrument 3, previous research has reported results for undergraduate students and graduate students. Table 7 shows the mean score of participants, and the standard deviation if reported in the original research. Note that the average of graduates is lower than that of undergraduates, because Davis et al. [39] set the a maximum of 10 points for each dimension. The performance of ol-preview is significantly better than the average for both undergraduates and graduates, indicating that ol-preview can identify detailed variables, interconnectivity, and identify feedback loops in a complex system much better than these students.\nTo sum up, in all the ST assessments mentioned above, ol-preview performed better than human participants. The results indicated that despite the differences in ST construction, assessment context, format, and educational level, ol-preview demonstrated a higher capability for ST. Additionally, compared to the original time required for human participants, ol-preview took only seconds to provide the right answers."}, {"title": "Computational Thinking", "content": "Computational thinking (CT) involves using a set of problem-solving skills and techniques that software engineers use to write programs and apps. From a generic perspective, CT is viewed as \"the thought processes involved in formulating problems and their solutions so that the solutions are represented in a form that can be effectively carried out by an information-processing agent\"[45]. In educational settings, CT has been the core of STEM curricula and critical competencies that equip students with foundational programming and computing skills in problem-solving and STEM learning. Researchers suggested students follow a series of CT steps for problem-solving, including (1) formulating a problem, (2) collecting and analyzing data, (3) representing data, (4)automating solutions, and (5) identifying, analyzing, and implementing possible solutions [46]. To solve the problems, students could implement critical CT concepts: logic, algorithms, decomposition, patterns, abstraction, and evaluation. Employing CT skills and concepts, students can create, debug, think, and collaborate for problem-solving [47]. \u03a4\u03bf assess students' CT skills, Brennan and Resnick [48] proposed a three-dimensional framework, including (1) computational concepts (e.g., sequences, repetition, conditional, debugging, and data); (2)computational practices regarding the process of iteration, debugging, and abstraction; and (3) computational perspectives, such as expressing, connecting, and questioning the social relations with peers and the technical world. Building upon the computational practice perspective in Brennan and Resnick [48], Weintrop et al. [49] suggested assessing CT in four major categories: data practice, modeling and simulation practice, computational problem-solving practice, and systems thinking practice. Furthermore, Selby and Woollard [50] proposed an operational CT assessment approach, focusing on abstraction, decomposition, algorithmic thinking, evaluation, and generalization, which is broadly adopted in many assessment instruments for assessing students' performance on CT skills."}, {"title": "Computational Thinking Assessment Instruments", "content": "Researchers have developed different instruments to assess CT [51]. Bebras Challenge is an in-ternational initiative designed to assess and promote computational thinking (CT) skills among students, typically between the ages of 6 and 18. It involves short, engaging tasks that encourage"}, {"title": "ol-preview performance on Bebras-based test", "content": "Using the two tests developed by Lockwood et al. [52] we systematically tested the CT skills of OpenAI ol-preview by selecting two representative items in each test. For some items using images, we converted image information into text for ol-preview to respond. ol-preview correctly answered three out of the four problems, with an estimated average score at 9.75, significantly outperformed undergraduate students (m = 8.03). Instead of only providing the final response, ol-preview further explained its problem-solving and logical reasoning processes step-by-step."}, {"title": "ol-preview performance on Algorithmic Thinking Test for Adults Instrument", "content": "We further used the Algorithmic Thinking Test for Adult Instrument [53] to test OpenAI ol-preview's performance on using CT skills to solve problems using six representative items. Similarly, for some items relying on graphs or images to convey the necessary information of the problem, we converted them into text, considering the data modalities that ol-preview can process. Using a consistent template prompt, we entered each item into ol-preview and received correct responses for 6 items. Compared to human performance, the ol-preview outperformed participants from both mathematics (M = 21.2) and physics (M = 20.6) majors, who achieved the highest mean scores relative to peers from other disciplines."}, {"title": "Design Thinking", "content": "Design Thinking is a user-centered, and iterative process aimed at solving problems by focusing on user needs, prototyping solutions, and refining based on feedback [54, 55]. It involves empathizing with users, defining the problem, finding potential solutions, prototyping those ideas, and testing the prototypes. Design thinking emphasizes flexibility and adaptability, encouraging rapid iteration and continuous improvement. The iterative nature of design thinking ensures that the products are continuously optimized through hands-on testing and user involvement.\nIncorporating design thinking in LLMs ensures that solutions are responsive to real-world needs and constraints. For instance, when developing a new scientific tool, researchers may create and test multiple prototypes, refining each based on experimental feedback and user experience, leading to a more effective final product [56]. This enhances the overall user experience by keeping the development process dynamic and responsive to feedback."}, {"title": "Conceptualization", "content": null}, {"title": "Instruments and Evaluation", "content": "In this study, we assess the design thinking abilities of the ol-preview model in coding tasks [57, 58]. As coding often involves prototyping the code, iteratively debugging, and further, the user can give LLM feedback to let it customize the code to fit certain needs. We randomly select ten coding questions from the 'hard' questions on Leetcode as the evaluation dataset for the design thinking abilities of LLMs.\nWe use a series of prompts based on designed to emulate the process of design thinking applied to coding problem-solving. Our approach is as follows:\n1. We provide the LLM with a Leetcode problem description, including input/output examples and constraints, to initiate the code generation process.\n2. If the generated code encounters errors during execution, the error message is fed back into the LLM to update the code. This process is repeated up to three times if necessary. If the code still fails after three attempts, the experiment is marked as \u201cfailed\u201d and terminated. If the code runs successfully without errors, this step is skipped.\n3. Once a working solution is produced, the LLM is prompted to optimize the code for improved efficiency [56]. We call this prompt by design thinking prompting, as it reflects a user need fed to the LLM."}, {"title": "ol-preview performance comparison with human performance", "content": "Figure 7 (a) showcases the absolute speed of GPT-40 and o1-preview models in terms of \u201cPercentage of human beaten\", before and after the prompt requesting to improve the efficiency. We see that ol-preview beats GPT-40 in both the original answer and the answer after prompting. Other than the boxplots in Figure 7 (a), we dig into each question and have an important observation that ol-preview often gives very efficient codes in its first attempt, which means that it is even harder for ol-preview to improve its codes, while it will be easy to improve the less efficient code for GPT-40. In some cases, the first attempt of ol-preview beats the final codes of GPT-40, which means there is much space for ol-preview to improve its codes. Therefore, we remove such cases and only plot the speedup rate for the remaining cases in Figure 7 (b). This helps us better assess the ability of design thinking, i.e., following the user's request to improve code efficiency. We see that the ol-preview has a lower speedup rate than GPT-40, indicating that after the design thinking prompting, ol-preview speedup the code with more proportion than GPT-40.\nWe access the human performance on the coding questions from Figure 7 (a). We see GPT-40\""}, {"title": "Metacognition", "content": "Metacognition refers to the ability to understand, control, and reflect on one's own cognitive processes. It includes metacognitive knowledge\u2014awareness of one's cognitive abilities and strategies for learning or problem-solving\u2014and metacognitive regulation, which involves planning, monitoring, and adjusting strategies during tasks. For example, changing a strategy when progress stalls is a form of metacognitive regulation. These processes enable individuals to plan, monitor, and evaluate their performance, allowing for strategic decision-making, adaptation, and continuous improvement in learning outcomes. Metacognition has been extensively studied in educational psychology and cognitive science [60]. [61] first introduced the concept, highlighting its role in learning. Later research [62, 63, 64] has shown that metacognitive abilities can be taught and are linked to improved problem-solving and decision-making. A meta-analysis [65] found that metacognitive interventions led to significant improvements in students' academic performance across various domains.\nMetacognition in LLMs allows them to monitor outputs, adjust strategies, and improve over time, similar to human cognitive regulation. Evidence of this metacognitive capability has emerged in several studies. For instance, the Ask-LLM method enables models to assess the quality of training data using reasoning [66]. Another study demonstrates that LLMs possess metacognitive knowledge, including the ability to identify relevant skills and procedures for specific tasks [67]. Additionally, research comparing human and LLM metacognition in a coaching competency test suggests that"}, {"title": "Conceptualization", "content": null}, {"title": "Instruments and Evaluation", "content": "We assess the metacognitive abilities of the OpenAI ol-preview model in solving mathematical problems. For each problem", "69": "will be used to evaluate the metacognitive abilities of LLMs. This dataset features challenging high school-level math problems", "70": ".", "include": ""}]}