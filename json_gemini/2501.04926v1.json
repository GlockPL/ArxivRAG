{"title": "FLowHigh: Towards Efficient and High-Quality Audio Super-Resolution with Single-Step Flow Matching", "authors": ["Jun-Hak Yun", "Seung-Bin Kim", "Seong-Whan Lee"], "abstract": "Audio super-resolution is challenging owing to its ill-posed nature. Recently, the application of diffusion models in audio super-resolution has shown promising results in alleviating this challenge. However, diffusion-based models have limitations, primarily the necessity for numerous sampling steps, which causes significantly increased latency when synthesizing high-quality audio samples. In this paper, we propose FLowHigh, a novel approach that integrates flow matching, a highly efficient generative model, into audio super-resolution. We also explore probability paths specially tailored for audio super-resolution, which effectively capture high-resolution audio distributions, thereby enhancing reconstruction quality. The proposed method generates high-fidelity, high-resolution audio through a single-step sampling process across various input sampling rates. The experimental results on the VCTK benchmark dataset demonstrate that FLowHigh achieves state-of-the-art performance in audio super-resolution, as evaluated by log-spectral distance and ViSQOL while maintaining computational efficiency with only a single-step sampling process.", "sections": [{"title": "I. INTRODUCTION", "content": "Audio super-resolution (SR), also known as bandwidth extension, is a task with the purpose of reconstructing a high-resolution (HR) audio signal from a low-resolution (LR) input audio by estimating the missing high-frequency information, thereby enhancing perceptual quality. Deep learning-based audio SR research [1]-[4] has greatly outperformed traditional approaches, building on advancements in deep learning and signal processing [5]-[7]. Nevertheless, audio SR remains a challenging task due to its ill-posed nature, as an LR signal can correspond to numerous HR signals.\nRecent works [8]-[10] have employed generative models to address the one-to-many problem. Several studies have leveraged generative adversarial networks (GANs), such as AERO [11], mdctGAN [12], and MS-BWE [13], which focus on spectrum estimation for inverse transformation. Fre-painter [14] has demonstrated state-of-the-art performance by integrating a masked autoencoder and GANS with effective masking strategies. While these studies have yielded promising results, GANs are prone to training instability and mode collapse, which can impede model convergence.\nRecently, diffusion models [15]\u2013[17] have demonstrated impressive performance in audio SR [18]-[20] as well as various audio generation tasks [21]-[25]. Nu-wave2 [18] explores flexible input sampling rates and UDM+ [19] introduces a conditional sampling algorithm to enhance low-frequency fidelity during the reverse process. AudioSR [20] handles diverse audio types using a latent diffusion model. They also verify that leveraging prior knowledge from pre-trained vocoders can improve the audio SR performance [20], [26]. However, previous diffusion-based studies are often constrained by the need for a larger number of function evaluations (NFEs), leading to prolonged processing times to achieve high perceptual quality. In real-world scenarios [27], [28], such limitations significantly impair efficiency and increase the latency for delay-sensitive operations.\nFlow matching [29], [30] is a simulation-free method for training continuous normalizing flows (CNFs) [31] with a simple vector field regression objective. It learns the transformation between a simple prior and a complex data distribution via a straight trajectory, allowing for faster generation with fewer sampling iterations. Recently, it has emerged as a promising approach, demonstrating exceptional performance and efficiency across diverse audio generation tasks [32]-[39]. However, it has not yet been explored for audio SR.\nBuilding on these insights, we propose FLowHigh, a novel audio SR method based on Flow matching, developed to transform Low-resolution audio into High-resolution audio. This study aims to address the shortcomings of diffusion-based models, concurrently elevating audio SR efficacy. By leveraging flow matching, FLowHigh models the HR audio distribution conditioned on LR input using a simple vector field regression objective without adversarial training. FLowHigh incorporates a transformer-based vector field estimator that regresses the target vector field at the mel-spectrogram level. Additionally, it utilizes a pre-trained neural vocoder for synthesizing the waveform [26]. We further analyze the probability paths required for conditional flow matching (CFM) to devise a more efficient flow matching-based audio SR framework. We found that well-defined path derived from input data are tailored for HR audio reconstruction. Our approach notably outperforms existing methods and reconstructs the high-fidelity HR audio using a single-step sampling with the Euler method. Consequently, it offers a substantial speed advantage over the computationally expensive sampling processes of diffusion models.\nOur contributions are summarized as follows:\n\u2022 We propose FLowHigh, an efficient audio SR method based on flow matching. To the best of our knowledge, this is the first study to integrate flow matching into audio SR successfully.\n\u2022 We analyze various probability paths essential for CFM, exploring different source distributions along the path, to refine and improve audio SR performance.\n\u2022 The experimental results demonstrate that FLowHigh outperforms existing models on objective metrics, achieving superior performance with only a single-step sampling across various audio sampling rates. Code implementations and audio samples are provided at https://jjunak-yun.github.io/FLowHigh."}, {"title": "II. BACKGROUND: FLOW MATCHING", "content": "Let $x \\in \\mathbb{R}^d$ represent a data point in the data space $\\mathbb{R}^d$, sampled from unknown distribution $q(x)$. CNFs [31] are learned to transform a simple prior $p_0$ to a $p_1 \\approx q$, where $q$ is a complex target distribution. CNFs define the time-dependent probability density function, called probability density path $p_t : [0, 1] \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}_{>0}$, where $\\int p_t(x)dx = 1$ and $t \\in [0,1]$. The flow, time-dependent diffeomorphic mapping $\\Phi_t : [0, 1] \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, which induces the path $p_t$, is driven by time-dependent vector field $v_t : [0, 1] \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}^d$, defined via the following ordinary differential equation (ODE):\n$\\frac{d}{dt} \\Phi_t(x) = v_t(\\Phi_t(x)), \\qquad \\Phi_0(x) = x.$\nFlow matching [29] has been proposed as a promising approach for simulation-free training CNFs. The training objective of flow matching is simple vector field regression defined as follows:\n$\\mathcal{L}_{FM}(\\theta) = \\mathbb{E}_{t \\sim U[0,1], p_t(x)} [|| u_t(x) - v_t(x; \\theta)||^2]$,\nwhere $v_t(x; \\theta)$ is the vector field parameterized by $\\theta$ and $u_t(x)$ is a target vector field that produces the corresponding path $p_t$. Since (2) is intractable owing to the ignorance of the prior knowledge of $(u_t, p_t)$, [29] introduced tractable objective for CFM, and [40] further generalized it as:\n$\\mathcal{L}_{CFM}(\\theta) = \\mathbb{E}_{t \\sim U[0,1], q(z), p_t(x|z)} [|| u_t(x|z) - v_t(x; \\theta)||^2]$,\nwhere $p_t$ and $u_t$ are conditioned on $z$, which is sampled from some distribution $q$. They also proved that $\\mathcal{L}_{FM}(\\theta)$ and $\\mathcal{L}_{CFM}(\\theta)$ have identical gradients for $\\theta$ in the context of model optimization. Assuming the probability path to be a time-dependent Gaussian conditional path $p_t(x|z) = \\mathcal{N}(x|\\mu_t(z), \\sigma_t(z)^2 I)$, the target vector field that generates the flow $\\Phi_t(x) = \\sigma_t(z)x + \\mu_t(z)$ conditioned on $z$ is as follows:\n$u_t(x|z) = \\frac{\\dot{\\sigma_t(z)}}{\\sigma_t(z)} (x - \\mu_t(z)) + \\dot{\\mu_t(z)},$\nwhere $\\dot{\\sigma_t}$ and $\\dot{\\mu_t}$ denote the derivatives of $\\sigma_t$ and $\\mu_t$, respectively."}, {"title": "III. METHODS", "content": "We propose FLowHigh, a novel audio SR system utilizing flow matching at the mel-spectrogram level, as illustrated in Fig. 1. FLowHigh leverages the capacity of flow matching to competently model the complex distribution of HR audio representation data, enabling time-efficient and high-quality audio SR. More details are presented in the following subsections.\nFollowing [26], given input LR signal $x_l = [x_{s1}, x_{s2}, ..., x_{s\\cdot l}]$, with a sampling rate of $l$, we adjust the temporal resolution of the signal to generate $x_h = [x_{s1}, x_{s2}, ..., x_{s\\cdot h}]$, where $S$ is the length of the audio signal in seconds and $h$ is the target sampling rate with $h > l$. To operate in the frequency domain, the complex spectrogram $X \\in \\mathbb{C}^{N \\times (nfft/2 + 1)}$ is computed from $x_h$ using a short-time Fourier transform (STFT) with N frames, where $nfft$ denotes the FFT size. $X_h$ is converted into a mel-spectrogram $X_h \\in \\mathbb{R}^{N \\times F}$ by a mel-filter with F bins, which serves as the input to our model. We denote that $Y_h = [y_{s1}, y_{s2}, ..., y_{s \\cdot h}]$ is the target HR signal. Based on the Nyquist theorem, the highest bandwidths of the audio signals $x_l$, $x_h$, and $y_h$ are $l/2$, $l/2$, and $h/2$ Hz, respectively. Thus, we aim to reconstruct the missing high-frequency components of $x_h$ in the range $(l/2, h/2]$ Hz, approximating the target spectral representation of $y_h$.\nWhen employing flow matching in audio SR, it is important to determine the appropriate conditional probability path $p_t(x|z)$, as this selection is influenced by the mean $\\mu_t(z)$ and standard deviation $\\sigma_t(z)$. In this subsection, we introduce a probability path specifically designed to enhance the effectiveness of audio SR. Extending the flow matching [29], the studies in [40], [41] explored the generalized flow matching, which maps from an arbitrary source distribution rather than a standard Gaussian distribution to mitigate the independence of noise and the target data. We align the source distribution with the LR audio data distribution to enhance the convergence ability to learn the flow. This is achieved by strategically utilizing the mel-spectrogram obtained from the LR input audio, which helps improve the model's capacity to effectively map between distributions."}, {"title": "B. Conditional Flow Matching for Audio Super-Resolution", "content": "Following simplified CFM of [40], FLowHigh samples a pair of random variables $(x_0, x_1)$ as condition $z$ from $q(z) = q(x_0)q(x_1)$, where $x_0$ and $x_1$ denote the source point and target point. We let the conditional path with $\\mu_t(z) = tx_1 + (1-t)x_0$, which is a linear interpolation between $x_0$ and $x_1$ over time. Considering the absence of high-frequency components of data in the prior, we set $\\sigma_t(z) = 1-(1-\\sigma)t$, where $\\sigma$ is a sufficiently small value. Hence, FLowHigh is designed to model the straight path that transitions from a normal distribution centered at $x_0$ to $p_1(x|z) = \\mathcal{N}(x|x_1, \\sigma^2 I)$. Within our framework, the condition $(x_0, x_1)$ represents the mel-spectrogram samples of the input and the ground truth HR waveform, respectively. According to (4), we can obtain the conditional target vector field formula for the FLowHigh training objective as follows:\n$u_t^{FLH}(x|z) = \\frac{(x_1 - x_0) - (1 - (1 - \\sigma)t) (x - x_0)}{1 - (1 - \\sigma)t}.$\nInspired by [32], we adopt a transformer architecture [42] as a vector field estimator to parameterize conditional vector field $v_t(x|X_h; \\theta)$ for audio SR. As depicted in Fig. 1 (a), the model takes the mel-spectrogram $X_h$ and flow sample $\\Phi_t(x) \\in \\mathbb{R}^{N \\times F}$ as an additional input, which is placed at the probability path at the flow step $t$. Before being fed into the transformer, these inputs are concatenated and projected via a linear layer, resulting in an input of size $\\mathbb{R}^{N \\times d}$, where $d$ denotes the input dimension of the transformer block. The transformer, conditioned on the flow step $t$, outputs the hidden representation, which is then passed through a final linear projection to predict the vector field $v_t$, which has the same dimension as the flow.\nDuring training, we first obtain a sample $\\Phi_t(x)$ with randomly sampled flow step $t \\sim U[0, 1]$. Conditioned on $X_h$ from LR input, FLowHigh learns the flow that maps the sample $\\Phi_t(x)$ into the distribution of the target mel-spectrogram from HR audio by minimizing the vector field regression objective as follows:\n$\\mathcal{L}(\\theta) = \\mathbb{E}_{t, q(z), p_t(x|z)} [||u_t^{FLH}(\\Phi_t(x)|z) - v_t(\\Phi_t(x)|X_h; \\theta)||^2].$\nOnce the estimator completes training, the source point $x_0$ is first drawn from $p_0$ to sample the high-fidelity mel-spectrogram $Y_h$ of HR audio, which is rich in higher frequency information based on the $x_l$. Subsequently, we use the estimator and the Euler ODE solver to numerically compute $x_1 = Y_h$, which is sampled from the learned distribution $p_1$, as illustrated in Fig. 1 (b). The Euler method is as follows:\n$X_{t+\\tau} = X_t + \\tau v(x_t|X_h; \\theta),$"}, {"title": "C. Vector Field Estimator and Training Objective", "content": "where $\\tau$ is the step size. In this study, we utilize only single-step sampling to obtain the $Y_h$. A pre-trained neural vocoder processes the $Y_h$ to synthesize the waveform $\\hat{y}_h$. To retain the original lower frequency information of $x_l$ as completely as possible, we apply post-processing [14], [26], replacing lower frequency components of the $Y_h$ with those from the $x_l$ using STFT and inverse STFT (ISTFT), and obtain final output $\\hat{y}_h \\approx y_h$ as depicted in Fig. 1 (c)."}, {"title": "IV. EXPERIMENTS", "content": "We use the VCTK dataset [43], which contains 44 hours of speech at a sampling rate of 48 kHz uttered by 108 English speakers, for both training and evaluation. Following [14], we divide the 108 speakers into a training set of 100 speakers and an evaluation set of 8 speakers. Training data is generated by applying a Chebyshev Type I low-pass filter with random orders and ripples to the target audio, followed by downsampling to a lower sample rate $l$ randomly selected from 4 kHz to 32 kHz. For evaluation, the order 8 Chebyshev Type I low-pass filter with a ripple of 0.05 dB is applied to the target data. The audio SR experiments are conducted at various input sampling rates of 8, 12, 16, and 24 kHz, which correspond to cut-off frequencies of 4, 6, 8, and 12 kHz, respectively, for a target sampling rate of 48 kHz.\nThe vector field estimator consists of 2 layers of transformer blocks with a 16-head self-attention layer, 1024 embedding dimensions, and 4096 dimensions for feed-forward networks, leading to 35.4M parameters. We set the window size of 2048, hop length of 480, and FFT size of 2048 for STFT. The waveform is synthesized using BigVGAN [44], a pre-trained neural vocoder that operates at 48 kHz with 256 mel bins, based on the official implementation and trained for 850k steps. FLowHigh is trained with a batch size of 128 for 400k training steps on a single NVIDIA RTX A6000 GPU using the Adam optimizer [45] with $\\beta_1 = 0.9$, $\\beta_2 = 0.99$, and $\\epsilon = 10^{-8}$ and set the initial learning rate at $3 \\times 10^{-4}$. For high-quality reconstruction, we set $\\sigma$ to $10^{-4}$.\nWe employ the log-spectral distance (LSD) [18], [26], where lower values indicate better performance. Following [14], we further calculate the low-frequency LSD (LSD-LF) and high-frequency LSD (LSD-HF) to assess the reconstruction performance across different frequency ranges. We also utilize the virtual speech quality objective listener (ViSQOL) [46] to evaluate the perceptual quality in speech objectively, with higher scores indicating better perceived quality. We assess computational efficiency using real-time factor (RTF), calculated as the ratio of processing time to input audio's duration."}, {"title": "A. Dataset", "content": "C. Implementation Details"}, {"title": "V. RESULTS", "content": "To evaluate the audio SR performance of FLowHigh, we compared it against several baselines, including Nu-wave2\u00b9 [18], mdctGAN [12], UDM+ [19], and Fre-painter [14], all of which have exhibited notable performance in audio SR. NVSR [26] and AudioSR [20] were included as additional baselines. These two models employ a two-stage process, comprising mel-spectrogram generation and waveform synthesis via a vocoder, which is similar to our procedure. For all baseline models, we used their official implementations and pre-trained checkpoints\u00b2 to ensure a fair comparison. GT reconstruction represents the synthesized output from the ground truth mel-spectrogram using the same vocoder as in our model.\nAs shown in Table I, FLowHigh consistently demonstrated superior performance over the baseline methods, achieving the lowest LSD values across all evaluated input sampling rates. Note that the GT reconstruction with post-processing serves as the upper bound for our model. Given 24 kHz audio as input, the output of our model was close to this upper bound, with an LSD difference of only 0.04. Remarkably, these results were obtained using only single-step sampling, highlighting the inference efficiency of our model. While UDM+ [19] exhibited better restoration performance in the low-frequency range, our model was highly effective at reconstructing high-frequency components, as evidenced by the lowest LSD-HF values. The results of the ViSQOL evaluation also showed that our model successfully generated high perceptual-quality audio across various source sampling rates. Additionally, without post-processing, our model attained competitive LSD values and sub-optimal ViSQOL scores, leveraging the power of flow matching.\nA significant drawback of diffusion models is that they require considerable NFEs to generate a high-quality sample. We conducted"}, {"title": "A. Evaluation of Audio Super-Resolution", "content": "B. Comparison by the Number of Sampling Steps"}, {"title": "C. Analysis on Conditional Probability Path", "content": "This analysis aims to advance the capabilities of a CFM-based audio SR framework by exploring suitable conditional probability paths. We performed audio SR using various probability paths to validate the proposed path, with the evaluation results presented in Table III. First, we defined a path where $\\mu_0(z) = 0$ and $\\mu_1(z) = x_1$, using the same $\\sigma_t$ as the proposed path. While starting from a standard Gaussian distribution yielded satisfactory reconstruction performance, we observed that a setting with a data-dependent prior resulted in comparatively higher perceptual quality. We also conducted the experiments by defining the path with $\\sigma_t(z) = \\sigma$, where $\\mu_t(z)$ is the same as in the proposed path. This path had the advantage of allowing the target vector field $u_t$ to be expressed in the simplified formula $u_t(x|z) = x_1 - x_0$. However, it resulted in a prior distribution with nearly empty high-frequency components, and this sparsity acted as an obstacle to learning the flow effectively in audio SR."}, {"title": "VI. CONCLUSION", "content": "In this work, we proposed FLowHigh, a novel audio SR model based on flow matching. We leveraged the strength of CFM to effectively model the target data distribution for efficient audio reconstruction. We also introduced suitable conditional paths tailored for audio SR, utilizing prior distributions corresponding to the input audio. As a result, FLowHigh generated high-quality audio and outperformed existing audio SR methods across various input sampling rates in objective evaluations for a target sampling rate of 48 kHz. Comparative experiments confirmed the inference efficiency of our model, as it produced high-fidelity audio with only single-step sampling, significantly accelerating the inference process compared to diffusion-based models. However, there is room for improvement in audio quality through phase information modeling. In future work, we plan to incorporate phase information modeling into advanced generative models to enhance the fidelity."}]}