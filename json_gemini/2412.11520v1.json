{"title": "EditSplat: Multi-View Fusion and Attention-Guided Optimization for View-Consistent 3D Scene Editing with 3D Gaussian Splatting", "authors": ["Dong In Lee", "Hyeongcheol Park", "Jiyoung Seo", "Eunbyung Park", "Hyunje Park", "Ha Dam Baek", "Shin Sangheon", "Sangmin kim", "Sangpil Kim"], "abstract": "Recent advancements in 3D editing have highlighted the potential of text-driven methods in real-time, user-friendly AR/VR applications. However, current methods rely on 2D diffusion models without adequately considering multi-view information, resulting in multi-view inconsistency. While 3D Gaussian Splatting (3DGS) significantly improves rendering quality and speed, its 3D editing process encounters difficulties with inefficient optimization, as pre-trained Gaussians retain excessive source information, hindering optimization. To address these limitations, we propose EditSplat, a novel 3D editing framework that integrates Multi-view Fusion Guidance (MFG) and Attention-Guided Trimming (AGT). Our MFG ensures multi-view consistency by incorporating essential multi-view information into the diffusion process, leveraging classifier-free guidance from the text-to-image diffusion model and the geometric properties of 3DGS. Additionally, our AGT leverages the explicit representation of 3DGS to selectively prune and optimize 3D Gaussians, enhancing optimization efficiency and enabling precise, semantically rich local edits. Through extensive qualitative and quantitative evaluations, EditSplat achieves superior multi-view consistency and editing quality over existing methods, significantly enhancing overall efficiency.", "sections": [{"title": "1. Introduction", "content": "Text-driven 3D scene editing, which enables manipulation of 3D scenes using only text instructions, is gaining research momentum. The goal is to edit 3D representations accurately and efficiently through text prompts, facilitating real-time, user-friendly 3D editing for film and game development, digital content creation, or AR/VR applications. InstructNeRF2NeRF (IN2N) [12] introduced the pipeline for text-driven 3D editing by leveraging 2D diffusion model [40], such as InstructPix2Pix [4]. Subsequent methods [9, 34, 50, 51], inspired by IN2N, iteratively edit rendered images while updating the underlying 3D representation. However, these approaches independently edit single views with 2D diffusion models, neglecting multi-view consistency, as illustrated in Fig. 2-(a). More recent methods [7, 8, 11] incorporate multi-view information, but they lack a comprehensive strategy for consistent view alignment. For example, [8, 11] use only a limited set of key views, requiring additional diffusion passes or extra layers to integrate multi-view information. Similarly, [57] trains a diffusion model to achieve multi-view consistency, incurring high costs. Consequently, existing methods still exhibit multi-view inconsistency, resulting in noisy gradients that hinder optimization and produce suboptimal outputs, such as minimal edits or blur outputs.\n3D Gaussian Splatting (3DGS) [21] has emerged as a foundational model in 3D representation, surpassing Neural Radiance Field (NeRF) [33] in rendering quality and speed. Unlike NeRF's implicit representations, 3DGS employs explicit anisotropic ellipsoids, enabling faster training and high-quality reconstruction. However, editing a pre-trained 3DGS model introduces inefficiency, as pre-trained Gaussians tend to retain source visual and geometric details excessively, impeding efficient convergence as illustrated in Fig. 2-(b). This underscores the need to manage redundant Gaussians for more effective optimization.\nTo tackle the limitations in both the multi-view inconsistency and the optimization inefficiency of pre-trained Gaussians in editing, we propose a novel 3D scene editing framework, EditSplat. To ensure multi-view consistency between edited images, we propose a Multi-view Fusion Guidance (MFG) method that ensures multi-view consistent editing utilizing a 2D diffusion model and the geometric properties of 3DGS. Inspired by [7, 11], our method projects initially edited multi-view images onto a target view using 3DGS depth maps, enabling smooth blending based on depth values across views to integrate multi-view information. Leveraging classifier-free guidance in the text-image diffusion process [4, 16], we carefully edit source images based on text prompt by incorporating multi-view fused images into the diffusion process, ensuring robust alignment of edited images with multi-view details. To further preserve source fidelity, we utilize the source images as auxiliary guidance. By balancing guidance scores between multi-view fused images, source images, and text prompts, our method enables the diffusion model to edit source images to the multi-view-consistent outputs, aligning with essential multi-view information. Unlike iterative editing approaches that require expensive optimization processes during rendered image editing, our proposed EditSplat edits source images directly, allowing for immediate 3DGS model optimization.\nAdditionally, we propose an Attention-Guided Trimming (AGT) method to improve the optimization efficiency of pre-trained Gaussians and enable semantically rich local editing. Utilizing the explicit representation of 3DGS, we assign attention weight to each Gaussian based on the attention map from the diffusion model. High attention weights highlight semantically meaningful regions, indicating substantial visual and geometric changes required for effective edits. Thus, pre-trained Gaussians with high attention weight generally need significant changes with the optimization process and become redundant during the densification process as the number of Gaussians retaining excessive source information increases. To address this problem, we prune a suitable proportion of pre-trained Gaussians with high attention weight before editing, allowing the remaining Gaussians to achieve efficient densification and optimization. In addition, we selectively optimize Gaussians with high attention weight, enabling semantic local editing. Overall, the proposed AGT simplifies optimization and improves semantic precision, enabling faster and more efficient 3DGS editing.\nWe conduct extensive qualitative and quantitative experiments to validate the high quality and effectiveness of EditSplat. Our key contributions are as follows:\n\u2022 We propose a Multi-view Fusion Guidance (MFG) method that integrates multi-view information into the diffusion process to achieve robust alignment with multi-view details, ensuring consistent multi-view editing.\n\u2022 We introduce an Attention-Guided Trimming (AGT) technique that prunes 3D Gaussians with high attention weight and selectively optimizes them, enhancing optimization efficiency and enabling semantic local editing.\n\u2022 Through extensive qualitative and quantitative evaluations, we demonstrate that EditSplat outperforms existing methods in both performance and effectiveness."}, {"title": "2. Related Work", "content": "2D Image Editing. Despite advancements in 3D generative models [2, 6, 10, 20, 29, 32, 46, 54], their appli-"}, {"title": "3. Method", "content": "We propose a novel 3D editing framework, EditSplat, carefully designed to achieve three key objectives: (i) multi-view consistent editing, (ii) efficient optimization, and (iii) semantically rich local editing. To meet these goals, we introduce Multi-view Fusion Guidance (MFG) and Attention-Guided Trimming (AGT). MFG incorporates essential multi-view information into the diffusion model to ensure consistency. In AGT, attention weights from the diffusion model are explicitly assigned to each Gaussian in 3D Gaussian Splatting (3DGS). This enables efficient convergence by pruning Gaussians with high attention weight and selectively optimizing them for semantic local editing. The overall pipeline is shown in Fig. 3. In the following sections, we provide a review of 3DGS and classifier-free guid-"}, {"title": "3.1. Preliminaries", "content": "3D Gaussian Splatting. 3D Gaussian Splatting [22] is an explicit 3D scene representation using anisotropic Gaussians to model complex structures. A set of 3D Gaussians G represents the scene, where each 3D Gaussian \\(g \\in G\\) is defined by a center \\(\\mu \\in \\mathbb{R}^3\\), a covariance matrix \\(\\Sigma \\in \\mathbb{R}^{3 \\times 3}\\), spherical harmonic coefficients \\(f \\in \\mathbb{R}^k\\) (with k indicating the degrees of freedom), and opacity \\(\\sigma \\in \\mathbb{R}\\). The covariance matrix \\(\\Sigma\\) is composed of a rotation matrix R and a scaling matrix S as \\(\\Sigma = RSS^T R^T\\). For rendering, a 3D Gaussian function is defined as\n\\[g(x; \\mu, \\Sigma) = e^{-(x-\\mu)^T 2^{-1}(x-\\mu)}.\\]\nEach 3D Gaussian g is projected into a 2D image space using a world-to-image projective transform, with its Jacobian evaluated at \u03bc. The final color of each pixel is calculated by blending contributions from Gaussians along a ray:\n\\[C = \\sum_{i \\in N} C_i \\alpha_i \\prod_{j=1}^{i-1} (1 - \\alpha_j),\\]\nwhere blending weight \\(\\alpha_i \\in \\mathbb{R}\\) is defined as the density value of g multiplied by o, and c\\( \\in \\mathbb{R}^3\\) is color of the Gaussian. This tile-based approach supports real-time rendering, with Gaussian parameters \u03bc, \u03a3, f, and o optimized to represent scenes accurately through photometric loss.\nClassifier-free Guidance. Classifier-free diffusion guidance [16] improves the alignment between generated outputs and conditioning prompts in tasks, such as image- or"}, {"title": "3.2. Multi-View Fusion Guidance (MFG)", "content": "Recent studies [8, 9, 11, 12, 51] struggle with consistent editing across views due to limited consideration of multi-view information, resulting in multi-view inconsistency that causes minimal edits or blurriness. To overcome this challenge, we focus on integrating essential multi-view details into the 2D diffusion model, enabling consistent editing.\nMulti-View Fusion. We first edit the whole source images using the diffusion model. Inspired by [7, 11], we incorporate multi-view information by projecting and blending the initially edited multi-view images onto each target view using depth maps from 3DGS. To enhance output quality, we rank these images using ImageReward [52], which scores fidelity and text alignment based on human feedback. Since some initial edits are generally misaligned, we exclude the bottom 15% of low-scoring images from projection.\nFor the multi-view projection process, we select the top 5 adjacent views, prioritizing them based on their proximity to the target view, determined by camera position and orientation in world coordinates. To manage overlapping pixels across multiple views, we implement an iterative alpha blending strategy guided by depth values. Starting from the farthest pixel (i.e., highest depth value), pixel pairs are blended iteratively in decreasing depth order. The final pixel color is determined by aligning and unprojecting the multi-view images into 3D space using depth maps and camera parameters, followed by reprojecting onto the target view to resolve overlapping pixel contributions. This approach ensures that each target view captures comprehensive details shared across multiple views. To handle sparse background regions and achieve seamless integration, we refine these areas using SAM [25] to fill them with source content. This sequential multi-view fusion strategy enriches each target view with consistent and detailed information, reinforcing alignment and coherence across all views. The resulting multi-view fusion image hm for each target view serves as an essential input for subsequent editing processes, integrating comprehensive visual and geometric details shared across the views. Note that further details are provided in the supplementary material (Sec. B.1).\nAlignment with Multi-View Information. To effectively integrate multi-view fusion information into the diffusion process and edit source image without additional training [7], extra layers [8], or repeated diffusion passes [11], we leverage classifier-free guidance. This allows the direct incorporation of multi-view fusion details into the diffusion process. According to Liu et al. [31], a conditional diffusion model can combine score estimates from multiple conditioning values. Building on this, we adapt classifier-free guidance, facilitating the implicit classifier pe to assign high probabilities to multi-view fused visual features, color consistency, and structural properties. This enables the edited images to maintain a strong alignment with multi-view fused images throughout the editing process.\nSpecifically, we provide the multi-view fusion image hm as a guidance conditioning during source image editing. This enhances alignment with multi-view details, ensuring multi-view consistent editing. To maintain source fidelity, we include the original source image hs as auxiliary guidance, preserving its content. Our guided score prediction \\(\\hat{\\epsilon}_\\theta\\) is defined as follows:\n\\[\\hat{\\epsilon}_\\theta (z_t, h_s, h_t, h_M) =\\]\n\\[\\epsilon_\\theta (z_t, \\emptyset, \\emptyset)\\]\n\\[+ s_T (\\epsilon_\\theta (z_t, h_M, h_T) - \\epsilon_\\theta (z_t, h_M, \\emptyset))\\]\n\\[+ s_M (\\epsilon_\\theta (z_t, h_M, \\emptyset) - \\epsilon_\\theta (z_t, \\emptyset, \\emptyset))\\]\n\\[+s_S (\\epsilon_\\theta (z_t, h_s, \\emptyset) - \\epsilon_\\theta (z_t, \\emptyset, \\emptyset)),\\]\nwhere hm, hs, and hy correspond to the multi-view fusion image, source image, and text prompt, respectively. Each guidance input is modulated by its respective scale factor: sM for Multi-view Fusion Guidance, ss for the source image, and st for the text prompt.\nOur method efficiently directs the diffusion process toward generating outputs that achieve seamless multi-view consistency, including multi-view fusion images' texture, color, geometric, and source images. By balancing guidance scores between sM, source images ss, and text prompts st, our approach enables the diffusion model to align edits with essential multi-view information and generate consistent outputs. Unlike iterative editing approaches [9, 12, 50, 51] that costly edit rendered images and require optimization of the 3D model, our approach directly edits the source image, ensuring precise alignment with multi-view information and the text prompt."}, {"title": "3.3. Attention-Guided Trimming (AGT)", "content": "When editing a 3DGS, optimization inefficiency occurs as pre-trained Gaussians tend to excessively retain source attributes, obstructing effective optimization. Additionally, previous methods [8, 9, 11, 51] using 2D binary masks from SAM [25] for local editing often overlook semantic regions, as these masks are extracted solely from the source image rather than the edited output. This local editing issue can lead to suboptimal results when the target object is not clearly defined. These underlines the need to manage Gaussians in editing. To improve efficiency and semantic local editing, we trim Gaussians by pruning redundant Gaussians and selectively optimizing those relevant, guided by attention maps from the diffusion model.\nAttention Map. Attention maps from the text-to-image diffusion U-Net's cross-attention layers highlight regions that require intensified focus for generation [14, 30], revealing the correlation between individual words and generated image regions, computed as \\(Softmax (\\frac{Q K^T}{\\sqrt{q}})\\), where Q is the query matrix projected from the spatial features of the noisy image with dimension q, and K is the key matrix projected from the textual embedding. In image editing diffusion (IP2P), high attention weights similarly indicate areas where edits should be concentrated, typically aligning with regions of significant visual change.\nAttention Weighting 3D Gaussians. To assign attention weights to each pre-trained Gaussian, we first unproject multi-view-consistent cross-attention maps onto the 3D Gaussians through inverse rendering [9]. These maps, resized to rendering size using bilinear interpolation, correspond to specific word tokens for the editing target in the MFG editing process. Given the j-th Gaussian in the 3DGS, we compute its cumulative attention weight wj by aggregating the attention map from each view v of all views V as:\n\\[w_j = \\frac{1}{\\sum_{v \\in V} |S_{v,j}|} \\sum_{v\\in V} \\sum_{s \\in S_{v,j}} Softmax(\\frac{Q_v K_v^T}{\\sqrt{q}}),\\]\nwhere Sv,j denotes a set of indices of all attention weights affected by the j-th Gaussian from the attention map for the v-th view. \\(Softmax(\\frac{Q_v K_v^T}{\\sqrt{q}}) \\in [0,1]\\) denotes an individual attention weight at s from an attention map for the v-th view. By aggregating attention weight across V, this weighting mechanism ensures that each Gaussian reflects the overall importance of multi-view attention maps, emphasizing regions that require pronounced geometrical or visual changes during editing. Details on preparing the attention map are in the supplementary material (Sec. C.1).\nTrimming 3D Gaussians. After weighting attention map to each Gaussian, we apply thresholding to obtain w'j = wj1{wj > wthres} to exclude Gaussians with low attention from the optimization by setting those gradients to zero, directing updates solely toward Gaussians with high attention. This selective optimization enhances precision by concentrating on semantically rich regions and aligning edits with meaningful word-image correlations.\nPre-trained Gaussians with high attention weights hinder optimization for editing, as these regions indicate areas requiring substantial visual and geometric modifications. This inefficiency is intensified as the number of Gaussians retaining excessive source information increases during the densification process.\nTo improve densification and optimization efficiency, we prune an optimal proportion of Gaussians with high attention before training, focusing on essential areas for more effective editing.\n\\[G_{pruning} = \\{g_j | g_j \\in G, w'_j \\geq Top\\text{-}k\\%(G)\\},\\\\]\nwhere Top-k%(G) returns the threshold weight for top k% Gaussian and then Gpruning consists of Gaussians selected for pruning, the top k% of Gaussians in G based on attention weight w'. By removing Gpruning, this pruning step eliminates redundant pre-trained Gaussians with high attention weight, allowing efficient densification and optimization to concentrate on the remaining necessary Gaussians and thereby enhancing overall efficiency.\nOptimization for 3D Editing. Given the source Gaussians trimmed by AGT, Gagt = G \\ Gpruning, and its rendering function \u00ce, as well as the multi-view consistent edited image Imfg by MFG editing process, the final optimized Gaussians Gedit is obtained by minimizing the editing loss across all views V:\n\\[G_{edit} = arg \\min_{G_{agt}} \\sum_{v \\in V} L_{edit}(\\hat{I}_v(G_{agt}), I_{mfg}),\\]\nwhere Ledit measures the discrepancy between rendered image \\(\\hat{I}_v(G_{agt})\\) and \u00cemfg for each view v. We use combination of LPIPS [57] and L\u2081 loss, weighted at a 1:1 ratio, as commonly empolyed in 3D scene editing task [12]."}, {"title": "4. Experiments", "content": "4.1. Implementation Details\nOur EditSplat is built upon vanilla 3DGS [21] using PyTorch. We use InstructPix2Pix [4] (IP2P) as a 2D editor with our proposed MFG, as described in 3.2. To validate the 3D scene editing performance of EditSplat, we collect 4 scenes from IN2N [12], one scene from BlendedMVS [55], and 3 scenes from Mip-NeRF360 [3], covering datasets utilized in previous studies, including complex real-world large 360\u00b0 scenes. All experiments are conducted on a single RTX A6000 GPU. The complete editing process takes approximately 6 minutes for the \"Face\" scene in IN2N."}]}