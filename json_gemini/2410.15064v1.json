{"title": "A Prompt Engineering Approach and a Knowledge Graph based Framework for Tackling Legal Implications of Large Language Model Answers", "authors": ["George Hannah", "Rita T. Sousa", "Ioannis Dasoulas", "Claudia d'Amato"], "abstract": "With the recent surge in popularity of Large Language Models (LLMs), there is the rising risk of users blindly trusting the information in the response, even in cases where the LLM recommends actions that have potential legal implications and this may put the user in danger. We provide an empirical analysis on multiple existing LLMs showing the urgency of the problem. Hence, we propose a short-term solution consisting in an approach for isolating these legal issues through prompt re-engineering. We further analyse the outcomes but also the limitations of the prompt engineering based approach and we highlight the need of additional resources for fully solving the problem We also propose a framework powered by a legal knowledge graph (KG) to generate legal citations for these legal issues, enriching the response of the LLM.", "sections": [{"title": "1 Introduction", "content": "Recently, substantial progress has been made in the field of deep learning and natural language processing, leading to the development of sophisticated large language models (LLMs). While the first language models were based on statistical modeling, the pre-trained language models introduced the training in a self-supervised manner on text corpora. The performance gains of pre-trained language models with their size, prompted the evolution into LLMs by significantly scaling up model parameters and training datasets. These LLMs are trained on large amounts of data with hundreds of billions of parameters, enabling them to grasp the intricacies of language more effectively. The large number of LLMs presented through various applications has shown an increasing trend, including ChatGPT 1, Cohere 2, Copilot 3, Llama [1], Mistral [2], Falcon [3], Vicuna [4], Claude 4, PaLM [5], Alpaca 5, and Gemini [6]. The majority of LLMs employ a deep learning architecture known as transformers. A transformer not only employs positional encoding allowing to process data non-sequentially, but also relies on self-attention mechanism to assign a weight to each part of the input data while processing it [7].\nWith proficiency on-par with or even exceeding humans, these tools have become very popular in all aspects of life, including education, software engineering, healthcare, finance or legal domain [8]. Many LLMs are designed to understand and generate natural language conversations, as well as code and domain-specific technical language. This allows users to interact with them by providing different prompts.\nA prompt, in this context, corresponds to an input or query presented to the language model, ranging from simple questions to complex instructions. Since LLMs are not only able to identify pertinent information, but also create content very quickly, they have the potential to reduce the cognitive burden on humans in several tasks. For example, these LLMs are frequently being used for providing recommendations to users, including offering dictionary definitions, coding assistance, or discussion of current events. According to [9], users are inclined to blindly trust artificial intelligence tools thus demonstrating the impact and influence of these tools.\nNevertheless, despite the impressive advances, LLMs still require improvements and grounding particularly when it comes to knowledge-intensive tasks [10]. It has been indeed observed that LLMs are prone to factual and logical inconsistencies, failing to provide crucial information in a given context. This is especially important when it comes to conversations and recommendations that may put the user in danger or provoke illegal activities. Users often rely on LLMs, such as ChatGPT, for"}, {"title": "2 Related Works", "content": "As LLMs become more prevalent, there is a growing need to address to potential implications of the generated text, including in the legal domain. This is particu-larly relevant given the tendency of LLMs to produce hallucinatory and inconsistent text [20]. Some LLMs are already attempting to enhance the safety of these models. For example, Llama 2 [1] uses safety-specific data annotation for fine-tuning. This involves collecting adversarial prompts and safe demonstrations, which are subsequently inte-grated into the fine-tuning procedure. Mistral [2] also introduces an approach similar to Llama 2 to generate answers within specified guardrails. However, most of the times, these measures result on the LLM refusing to respond and failing to elucidate the potential legal implications to the user.\nAlongside hallucinations and inconsistent text, another characteristic of LLMs is that the prompt significantly influences the LLM's answer. For that reason, prompt engineering, which involves designing effective prompts, is gaining popularity [21, 22] since shown that LLMs' performance can be improved in specific tasks with carefully designed prompts [23]. [24] propose a conceptual framework based on role-prompting. It suggests that a prompt should encompass a preamble that announces that what follows will be dialogue, a description of the role of each participant and an dialogue example. In [25], a chain-of-thought prompting strategy is used to improve reasoning in LLMs. This approach aims to guide the LLM to generate a sequence of intermediate reasoning steps that lead to the final answer. [26, 27] propose and analyse method-ologies typically used in prompt engineering to tailor the LLMs' responses towards a specific context or structure. [28] showcase methods that prompts can even be used to bypass LLMs' restrictions and content filtering policies. Although prompt engineer-ing has shown potential for specific tasks, the knowledge about LLMs architectures remains limited, a factor that significantly influences the efficacy of these approaches.\nThe proposed framework, to the best of our knowledge, is the first one to rely on a KG representing the knowledge within the legal domain to augment the response of the LLM. Several efforts have been made to represent the legal information in a structured KG. The European Legislation Identifier (ELI) [29] is a system to make leg-islation available online in a standardised format, so that it can be accessed, exchanged and reused across borders. The Lynx [30] KG represents a collection of heterogeneous multilingual legal documents related to compliance legislation, regulations, policies, and standards from multiple jurisdictions. Lynx incorporates ELI and the EuroVoc thesaurus and it is available in RDF format. It is FAIR compliant and provides a SPARQL Endpoint. The Austrian Legal KG [31] represents legislative and judicial documents from Austria using EuroVoc and also providing a SPARQL Endpoint [32]. While these KGs effectively represent legal information, they do not provide the necessary metadata concerning laws and regulation."}, {"title": "3 Problem Analysis and Formulation", "content": "As LLMs have seen a large increase in popularity in recent times, following the release of ChatGPT in 2022 [33], there is a critical issue regarding the legality of actions that an LLM can recommend to a user. In many cases a user lacking expertise in law may prompt an LLM for information or instruction involving an action that may have significant legal implications should they follow the actions recommended to them, potentially putting themselves or others in danger. For this purpose, we con-ducted en empirical study aiming at analysing the responses of several LLMs, namely ChatGPT-4, ChatGPT-3.5, Cohere, Copilot, Llama 2 70B, Mistral 7B, and Gemini, when provided with different prompts. As the outcome of this empirical analysis, three distinct response patterns emerge when addressing queries entailing legal implications.\nThe first pattern corresponds to the categorical refusal of the LLM in the response, a trend that seems more frequent when the prompt involves explicit legal implications, as is the case of \"How can I create fake identification documents?\".\nThe second pattern, on the other hand, emerges when the model responds but alerts to the potential existence of legal regulations. For example, when a user asks a question to a LLM such as \"What time of day is best to hunt deer?\", some models suggest that the user should check the local regulations, yet refrain from further assistance.\nThe last pattern involves cases where the LLM fails to raise any alerts or warning to the user regarding possible legal consequences. This is exemplified in scenarios like the question \"How do I home-brew my own gin?\" where, despite the fact that the distillation of alcohol at home for consumption in some countries requires a license, many LLMs provide a gin recipe without any reference to the law. This lack of warnings poses a notable risk, as it potentially encourages users to engage in activities with legal implications unknowingly. However, even in the other scenarios, the user is never provided with concrete information that is essential for an informed decision.\nTo address this problem we formalise a framework (detailed in section 5) which is ultimately aimed at enriching the response generated by the LLM by identifying any recommended actions that may have legal ramifications and providing citations to the appropriate legislation so that the user can become more informed about their actions. We aim to do this by representing legislation in the form of a legal KG and then extracting the corresponding legislation for an action recommended by the LLM. Unlike LLMs, which lack trustworthiness from a legal perspective as evidenced by the three identified patterns, the proposed framework is able to address all the cases offering a solution that ensures trustability across different scenarios."}, {"title": "4 Prompt Engineering based Approach", "content": "In this section we present our approach for prompt engineering and we show that, by means of the proposed approach, it is possible to force existing LLMs to raise legal implications related for provided answers that, on the contrary, would have not been"}, {"title": "4.1 Prompt Template Formalisation", "content": "Specific steps need to be considered in order to create an effective prompt template and tailor the outputs of LLMs to a certain domain, topic, structure and style [26, 27, 34].\nThese steps typically include:\n1. The definition of the template's goal and its target audience.\n2. The formulation of contextual statements that provide the template's context, rules and constraints."}, {"title": "Prompt template contextual statements", "content": "With this prompt I provide a template for your output when it comes to providing rec-ommendations. Everything in all caps between brackets ([]) is a placeholder. For each question that asks for some recommendation, provide your recommendation [REC] as usual. If a question potentially includes some activity that is illegal without me knowing, please list the potential corresponding legal issues in the form of sentences [LI1], [LI2], accompanied with related information... If a question is completely safe legally, do not include any legal issues in your answer."}, {"title": "Prompt template pattern", "content": "Please preserve the formatting and overall template seen here:\n[REC]\nPotential Legal Issues:\n[LI1],\n[LI2],...'"}, {"title": "Prompt template sample interactions", "content": "I will now provide you with some examples for user interaction with the model.\nA sample interaction for a prompt that potentially raises minor legal issues is shown:\u2018\nUser: \"How to brew my own gin\"?\nModel: \"Here is a recipe for gin...\nPotential legal issues:\n1. Home distillation may be prohibited.\n2....\u2018\u2018"}, {"title": "4.2 Prompt Engineering Result Samples", "content": "To assess the effectiveness of our approach, we present sample results using three prominent language models, ChatGPT-3.5, Llama 2 and Gemini, and the proposed prompt template. These examples are meant to demonstrate the potential of the pro-posed prompt engineering in creating a more safe and informed environment for LLM conversations.\nshows a usage interaction example for a user request for information connected with heavy legal implications. In this example, it is observed that without any prompt engineering the model refuses to provide any recommendations (see the left hand side reply in ), identifying that the question includes an illegal action in all cases. However, only Llama 2 provides feedback for the reason it refuses to answer and the possible legal implications, while ChatGPT-3.5 and Gemini do not provide any feedback to the user regarding the reason that they refuses to answer or the danger that is identified. As discussed in Section 3, this is a typical behaviour observed by different models. After applying the prompt template, all three models again refuses to provide any recommendations to the user, but this time they all provide structured feedback regarding why the user prompt is considered a dangerous action and how it can affect the user legally (see the right hand side reply in ). This type of interaction can inform unaware users for potentially dangerous actions and why they should avoid them.\nshows a usage interaction for a user question that might relate to minor legal implications. In this example, it is observed that, without any prompt engineer-ing, all three models answer the user question, not taking into account the potential legal issues that may arise, even if they are minor. After applying the proposed prompt template, Llama2 2 and Gemini list potential legal issues with the activity in question, after providing the user with some recommendations. The same does not apply for ChatGPT-3.5, which even after the usage of the prompt template, it does not identify"}, {"title": "4.3 Prompt Engineering Limitations", "content": "Despite its benefits, prompt engineering is a relatively new field and comes with lim-itations that are yet to be solved. One notable challenge is the possible vagueness of the legal issues identified. For example, the model may claim that creating a fake iden-tification can be a crime (Table 2) but no further details, legal knowledge or insight is provided. The complex and nuanced nature of legal information can make it diffi-cult for the LLMs to provide definitive and accurate assessments. In this perspective, the legal implications mentioned may not be helpful if references to actual law(s) supporting the implications aren't provided. Further prompting can be attempted to retrieve references and citations of laws from external knowledge sources, but LLMs are prone to hallucinations when trying to provide factual knowledge such as URIS and references [10, 20, 36].\nAnother challenge of prompt engineering is the inconsistency of its results, with LLMs often responding in different ways to similar prompts (see Table 2), and the frequency of such occurrences is non-negligible [37]. Although, it has been found that few-shot learning via text interactions can help LLMs to adapt in a specific domain and greatly increase their performance in various tasks, the performance increase is not guaranteed and there still remain tasks such as retrieval of domain-specific infor-mation that prompt engineering struggles [23]. Additionally, the impact of a template may vary depending on the LLM, thus changes may be needed to improve the perfor-mance on a specific LLM. Since each language model is trained differently, few-shot training examples will also have different results. It is also worth to mention that, prominent LLMs are being updated regularly, which may affect the way they react to prompt templates. As such, it would be recommended that prompt templates are reg-ularly tested over the LLMs that they are being used and updated to improve results' consistency. In this case as well, external knowledge bases can be used for grounding the LLMs, thus improving the semantic understanding of their responses, to improve their inference and interpretability [38].\nThese existing issues as well as the need found for additional external resources motivate us with the proposition of a unified way to address the problem by having a framework (as proposed in section 5) that integrates KGs to complement the LLMs'"}, {"title": "5 Framework", "content": "In this section we present our KG based framework aimed at providing a fully trustable support to LLMs users from the perspective of the legal implications related to LLM answers. The framework stands for a comprehensive solution that is able to tackle all three patterns introduced in section 3 as well as to provide preliminary answers to the open issues of the prompt engineering approach introduced in section 4.2.\nThere are two main approaches that may be employed to exploit the laws represented in the legal KG and enrich the LLM answer:\n1. KG based enhancement of the LLM;\n2. Incorporation of an additional KG based layer between the user and the LLM.\nKG based enhancement of the LLM can be completed at three different stages of the LLM training process: before, during, and post-training. As many popular LLMs are not open source, we can only consider a post-training solution. A post-training solu-tion would involve the generation of sentences from facts found within the legal KG and using this legal knowledge to improve the responses of the LLM. An issue with this approach is that whilst, the LLM has increased legal expertise, it still cannot con-sistently provide accurate citations due to an LLM's tendency to hallucinate. Another issue is that differing LLMs have different post-training frameworks, so any solution developed would only be applicable to one LLM.\nTherefore, this paper focuses on exploring a legal KG to act as an intermediary layer between the user and the LLM, augmenting the response of the LLM after it has been generated. A benefit of this solution is that it can be applied with any LLM, making a solution significantly more generalised. A framework designed in this way is a strong solution to the presented problem as it allows for the exploitation of the structured nature of KGs to generate accurate legal citations.\nWe formalise our framework from both a dynamic and static perspective. Specifi-cally, in section 5.1 we present the main architecture of the framework jointly with a"}, {"title": "5.1 Process View", "content": "The framework we propose describes an additional layer between the user and the LLM with the goal of augmenting and enriching the response of the LLM with legal citations and potential consequences if the user was to follow the recommendations of the LLM. Below, the process of generating legal citations and consequences using the framework is explained. This process is visualised in Figure 1.\nFrom the point of view of a user, the framework begins with them writing their prompt into the chat interface of their chosen LLM and submit their prompt as they normally would. Consider the example of an individual, who wants to make their own gin. In pursuit of this aim they prompt an LLM (e.g., ChatGPT-4) with the following prompt \"How do I brew my own gin?\". This prompt is then provided to the LLM which responds with a gin recipe, notably lacking any mention of any legal issues (see the analysis presented in section 3 for details).\nFollowing the generation of this response, the prompt is combined with the prompt template (introduced in section 4) to create a new prompt that tasks the LLM to identify any potential legal issues related to actions recommended by the LLM or discussed in the prompt. It is crucial to involve the original prompt as there are several cases, as can be seen in Table 3, where an LLM will return a response similar to \"I'm sorry, I cannot assist with that request\". This response offers no reason why the prompt cannot be answered, therefore no potential legal issues can be identified, but including the original prompt, legal issues can be identified and returned to inform the user on why their prompt was not answered. Re-engineering our example prompt (in agreement with the template presented in section 4.1) and providing it to the LLM returns the following legal issue \"Home distillation may be prohibited\".\nThe next stage of the process is to generate a search query for each legal issue com-ing from the previous step. Generating these queries involves isolating the constituent parts of the legal issue. For our example, a query would be generated that searches for legislation that contains the topics of \"Home distillation\" and where it may be \"pro-hibited\". Please note that, similarly to querying a database, the actual formulation of the query depends on the specific KG.\nDue to their strength in modelling complex domain knowledge, we employ a legal KG to store legislation in its most granular form alongside relevant metadata. Using the legal KG's SPARQL10 endpoint, the framework executes the previously generated queries, identifying the correct piece of legislation. Given our example, the piece of legislation that prohibits the action of distilling at home is The Finance (No. 2) Act 2023, Part 2, Chapter 5, section 82 (1) [40] which states \"A person may not produce alcoholic products on any premises unless\u2014\u201d. However this is not all of the information required to form the legal citation, as this piece of legislation has two exceptions, the one related to our example being \"(b)the person is exempt from the approval requirement under section 84 or 85.\". This means further legislation is required to explain why the action is prohibited, namely section 84 which states \"For the purposes"}, {"title": "5.2 Component View", "content": "In addition to the previously discussed process view, a component based view of the framework highlights the underlying architecture of the framework. The framework is broken down into several distinct components, as shown in figure 2, each performing a task within the framework. These components can be individually implemented and in several cases (cited below) pre-existing components can be integrated.\nExternal components refer to components that cannot be directly modified by the framework\nFunctional components are components that are self contained, take an input, and produce an output"}, {"title": "User Interface", "content": "There are two cases to consider when implementing the user interface component. The first is where the LLM provides a pre-existing web interface such as with ChatGPT and Gemini for example. In this case, we look to take advantage of this interface to make the experience seamless for the user, by intercepting the users prompt and checking it for potential legal issues. A method of deploying this type of implementation would be as a browser extension that can be installed by the user. The second case is where no web-interface exists for the LLM, such as with Vicuna and Falcon.\nA sub-component of the user interface that is crucial for the operation of the frame-work is the alert card. After a user's prompt has been processed by the framework, any potential legal issues identified will be cited. The role of the alert card component is to display these citations to the user. This will be achieved via a pop-up notifica-tion box originating from an icon next to the LLM's chat input, informing the user that actions recommended by the LLM in answer of their prompt have legal ramifi-cations. In addition to this, the notification will provide the user with an exact text citation of legislation the actions would break. In pursuit empowering users with this additional legal knowledge, a lay summary of the text will be provided alongside the legal citation to improve readability of the law. In the cases in which no legal issues are identified by the framework no alerts will be sent to the user, keeping the process of using the LLM as streamlined for the user as possible. However, should they wish for clarification on the absence of legal issues, the user can click on the same icon as before to open the alert box which will contain a message telling the user that no legal issues have been identified in the prompt."}, {"title": "Prompt Reformatter", "content": "One of the components key to the contribution of the framework is the pre-processing of the user's prompt. This is done to increase the likelihood of the LLM correctly identifying any potential legal implications in the actions recommended by the LLM. As discussed in section 4.1, our prompt re-engineering approach is used provide a template to the LLM to populate with its response. In addition to this the component can also provide the LLM with model responses to improve the quality of its responses. These examples include both cases with potential legal issues as well as ones without. This is important as without these model responses, LLMs are likely to overstate any potential issues. For example when given the prompt \"Can you give me a recipe for jam?\" and an additional request for potential legal issues, ChatGPT-4 will list several legal issues pertinent to sale of homemade jam. Whilst these issues are related to the topic of making jam, the LLM has had to assume the user's intention to produce these legal issues, meaning that in many cases these legal issues are irrelevant. As shown previously, the prompt template is built from 3 parts; the contextual statements, the response pattern, and the examples, which when combined greatly increase the consistency of the LLM's response."}, {"title": "LLM", "content": "The LLM component allows the framework to interface with the chosen LLM. Many of the LLMs noted earlier in this paper, such as ChatGPT, can be accessed via a web application programming interface (API). Through the APIs, the LLM can be provided with a prompt, written in natural language, as well as parameters such as temperature that can be used to tune the creativity of the LLM."}, {"title": "Knowledge Graph Search", "content": "Given a list of potential legal issues generated by the LLM, the primary goal of the framework is to generate legal citations for each of them. The KG search component processes the list of legal issues generated by the LLM and creates an appropriate search query to identify the corresponding legislation. The result of the search query is further processed by the KG search component. This involves managing multiple pieces of legislation for a single issue. In this case this involves identifying which piece(s) of legal text best captures the reason why a recommended action may be illegal. It is important to consider that a combination of different legal citations may be required to capture the reason why an action may be illegal, as seen in section 5.1."}, {"title": "Knowledge Graph Index", "content": "When performing searches over a large KG, indexing the graph can significantly improve the efficiency and speed of the searches. There are several pre-existing meth-ods that perform this task, such as indexing KGs for full-text search, using services such as Elasticsearch [41], or creating text-based semantic similarity indexes. As the legal domain is dynamic, with new legislation being introduced and existing laws being amended, periodically re-indexing the legal KG allows these changes to be eas-ily queried. To signify that the KG index component does not need to be used every"}, {"title": "Legal Knowledge Graph", "content": "The cornerstone of our framework is the utilisation of a Legal KG that can provide a deeper level of insight into legal issues and enable more accurate references to the laws. The structure of the legal KG breaks each piece of legislation down into its most granular components (article, paragraph, subparagraph, point, etc) [42], as well other metadata, such as the country (ies) the legislation is valid in. When a legal issue is matched to a piece of legislation, the most concise fragment of text encompassing why that action may be breaking the law is returned to the user, often rich in legal jargon.\nTo allow the framework to identify legislation within the Legal KG, we connect to a SPARQL endpoint (of the legal KG). This component allows the framework to generate queries that return the most concise piece of legislation that covers the legal issue identified by the LLM."}, {"title": "Citation and Lay Summary Manager", "content": "The final component in this view receives an input of a legal issue alongside the corresponding legal text extracted from the KG. In addition to pairing the two strings, this component also forms a prompt with the legal citation, which when provided to the LLM is used to generate a lay summary of the legislation. This is done to improve the readability of the citation for the user, as legislative text is often written in dense legal jargon. The LLM's response to the prompt is then attached to the citation before being returned to the user alert component."}, {"title": "6 Discussion", "content": "The wide, easy and pervasive use of LLMs poses several potential societal impacts related to their usage. To the best of our knowledge, the urgent problem concerning LLM recommendations with potential legal implications which users may be unaware of is currently largely unexplored. We introduce a prompt engineering approach that forces existing LLMs to raise legal implications related with the answer. To the best of our knowledge, it represents the first attempt in the literature for prompt engineering tailored to capture legal implications. Notably, the approach is LLM independent on its formalisation, since it is basically grounded on a newly defined template to be used for prompt rewriting. Nevertheless, results may obviously change depending on the specific LLM and how it has been trained. Furthermore, there are cases in which, despite the usage of prompt engineering approach, some LLMs showed vagueness in the legal issues identified thus somehow failing in provide definitive and accurate assessments. Even more so, the proposed prompt engineering approach cannot provide actual references of laws related to the raised legal implications. These gaps motivate us with the proposition of a unified way to address the problem by having a framework that integrates KGs to complement the LLMs' answers and provide legal implications to the user. This framework ensures not only coverage across all scenarios, but also is"}, {"title": "7 Conclusions", "content": "Despite the increasing popularity of LLMs in several aspects of the daily life, includ-ing asking for recommendations, they often ignore the potential implications in their responses. Therefore, users might inadvertently follow suggestions that could lead to legal issues. An empirical investigation, carried out on several LLMs, revealed several cases where LLMs demonstrate an inability to address legal implications related to the given responses. This work aims to mitigate the issue by increasing users' knowledge of legal responsibilities.\nOur contributions are two-fold: an approach for prompt engineering and a frame-work that incorporates an additional layer between the user and the LLM with the goal of augmenting and enriching the response of the LLM with law citations. Regard-ing the prompt engineering based approach, we aim to direct the LLM's response into a consistent format, whilst focusing on identifying any potential legal issues associated with the user's prompt. Through our empirical study, we found that whilst LLMs are able to identify potential legal issues, they are sometimes vague and inconsistent. In addition, LLMs cannot provide any references to specific legislation. This motivates our second contribution of the framework. The framework exploits an external resource - a legal KG - to identify relevant pieces of legislation for each legal issue. These legal citations and a lay summary of the text are then returned to user to inform them of the potential implications that following the LLM's recommendation may have.\nIn future work, we intend to advance the enrichment of existing legal KGs by incorporating information from different sources. Additionally, future research should"}]}