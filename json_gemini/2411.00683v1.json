{"title": "TaxaBind: A Unified Embedding Space for Ecological Applications", "authors": ["Srikumar Sastry", "Subash Khanal", "Aayush Dhakal", "Adeel Ahmad", "Nathan Jacobs"], "abstract": "We present TaxaBind, a unified embedding space for characterizing any species of interest. TaxaBind is a multimodal embedding space across six modalities: ground-level images of species, geographic location, satellite image, text, audio, and environmental features, useful for solving ecological problems. To learn this joint embedding space, we leverage ground-level images of species as a binding modality. We propose multimodal patching, a technique for effectively distilling the knowledge from various modalities into the binding modality. We construct two large datasets for pretraining: iSatNat with species images and satellite images, and iSoundNat with species images and audio. Additionally, we introduce TaxaBench-8k, a diverse multimodal dataset with six paired modalities for evaluating deep learning models on ecological tasks. Experiments with TaxaBind demonstrate its strong zero-shot and emergent capabilities on a range of tasks including species classification, cross-model retrieval, and audio classification. The datasets and models are made available at https://github.com/mvrl/TaxaBind.", "sections": [{"title": "1. Introduction", "content": "Fine-grained species classification is a challenging task in computer vision, which is often necessary for ecologists to automatically label images of rare species. A related and arguably a more important task is species distribution mapping which aims to map the presence of a given species of interest. Until now, both tasks were addressed using separate frameworks and methodologies, often requiring different datasets. In this work, we propose learning a unified embedding space over six modalities that is useful for several downstream ecological tasks including but not limited to species distribution mapping, fine-grained classification, and audio classification.\n\nThe presence of a particular species at a given geographic location can reveal several important characteristics of that species. Previous studies attempted to implicitly learn the relationship between geographic location and the presence of species by considering either environmental features [1] or satellite images [2-4] describing the location. This leads to learning an effective representation of any geographic location which is useful for species distribution mapping. However, this type of modeling often overlooks important species attributes, such as their taxonomic hierarchy or audio signatures.\n\nRecent works such as BioCLIP [5] and ArborCLIP [6] have demonstrated impressive zero-shot species classification capabilities. However, these frameworks are restricted to image and text modalities, ignoring crucial geographic, audio, and habitat characteristics of species. Multimodal embedding frameworks like ImageBind [7] and GRAFT [8] have shown that it is possible to learn a joint representation space by aligning all available modalities to the ground-level image modality. This allows for training modality-specific encoders using only image-paired datasets. One"}, {"title": "2. Related Work", "content": "potential downside of such methods is that they perform locked tuning with the ground-level image modality. This means that the ground-level image encoder is kept frozen, while the other modalities are trained to project to the existing learned space of the ground-level image modality. This can lead to sub-optimal performance since task-specific unique information of each modality is lost [9].\n\nTo this end, we propose multimodal patching, building upon patching [10], a framework to distill knowledge from various modalities while still preserving the original embedding space of the binding modality. We show that multimodal patching can improve zero-shot classification performance of the binding modality. We create a joint embedding space containing six modalities (Figure 1). To facilitate future research and evaluation of ecological models, we present TaxaBench-8k, a truly multimodal dataset containing six paired modalities. The contributions of our work are fivefold:\n\n1.  Multimodal Patching. We propose a simple yet effective patching technique that improves over the ImageBind framework.\n\n2.  Multimodal Models for Ecological Applications. We propose modality-specific encoders that can handle various ecological tasks over six modalities: ground-level image, geographic location, satellite image, text, audio, and environmental features.\n\n3.  Multimodal datasets. We compiled two large-scale novel cross-view datasets: i) iSoundNat: ground-level images of species with their corresponding audio; and ii) iSatNat: ground-level images of species with their corresponding satellite imagery.\n\n4.  TaxaBench-8k. We present TaxaBench-8k, a benchmarking dataset containing six paired modalities for evaluating multimodal ecological models.\n\n5.  We demonstrate our models' effectiveness and emergent properties on several benchmarking and zero-shot tasks."}, {"title": "2.1. Multimodal Self-Supervised Learning", "content": "Multimodal self-supervised methods using contrastive learning (CL) objectives have shown impressive results across diverse tasks. These methods encompass advancements in CL frameworks, integration of multiple modalities into unified embedding spaces, and innovations in training strategies to further enhance model performance. Out of many notable works in this area, CLIP [11] utilizes symmetric InfoNCE loss [12]; SupCon [13] utilizes label information in its contrastive objective; SigLIP [14] employs binary classification loss. Frameworks such as ImageBind [7], Sat2Cap [15], GRAFT [8], GeoCLAP [16], and GeoBind [17] demonstrate the integration of additional modalities such as audio, satellite imagery, or metadata into"}, {"title": "2.2. Multimodal Learning for Ecology", "content": "CL-trained multimodal embedding spaces. The core idea of these strategies involves utilizing a pretrained image-text embedding space and learning to project all other modalities into this space. However, this simple yet effective strategy can suffer from information collapse [9].\n\nRecent innovations in training strategies include LiT [18], which enhances zero-shot performance by freezing the vision encoder while training the text encoder; OmniVec [19] and OmniVec2 [20], which improve performance through modality-specific encoders and shared backbones for multimodal multitask learning; factorized contrastive learning [9] which focuses on preserving unique modality-specific information; and Patching [10], which boosts performance by interpolating weights between pretrained and fine-tuned models. In this work, we generalize the concept of patching to more than two modalities. We call this multimodal patching which improves over the training strategy of ImageBind.\n\nMultimodal learning for ecological applications, such as species distribution modeling (SDM) and fine-grained visual classification (FGVC) of species, has recently advanced significantly. These advancements are driven by the availability of large-scale multimodal datasets [5, 6, 21, 22] and novel multimodal learning frameworks [2, 4, 23-25]. Methods like BioCLIP [5] and ArborCLIP [6] have demonstrated the utility of combining images of species with their corresponding taxonomic text descriptions. BioCLIP is a foundational model for the tree of life, trained with a CLIP-like contrastive loss between image representations of different species and taxonomic descriptions. ArborCLIP is a recent model contrastively trained on a large multimodal dataset containing over 134 million images of diverse species paired with their detailed taxonomic descriptions. Although such CLIP-based models have shown state-of-the-art performance in zero-shot FGVC, they are limited by the number of modalities they can consume and the tasks they can solve.\n\nOn the other hand, the multimodal fusion of remote sensing data has resulted in unprecedented performance in SDM under presence-only [1, 4, 24, 26] and presence-absence [3, 27] settings. Most of these methods are general-purpose ecological predictors that effectively learn multimodal features that can vary across space. In these frameworks, geolocations are represented either by learning an implicit neural function [1, 23, 24] or by representations derived from satellite imagery [3, 4, 27]. Other CL-based methods such as SatClip [28] and GeoCLIP [29] aim to learn general-purpose location representations which can then eventually be utilized for downstream ecological tasks. In our work, we combine these parallel lines of work into a single general-purpose framework, TaxaBind, that can"}, {"title": "3. Dataset", "content": "consume multiple modalities and solve numerous ecology-related tasks.\n\nIn this work, we train and evaluate our models using various large-scale multimodal datasets. We built three datasets to advance multimodal learning in the field. Here we provide the details about the datasets.\n\nTraining datasets. When it comes to applying deep learning in ecology, there is a lack of high-quality multimodal datasets, especially the ones with paired ground-level images of species. To bridge the gap, we constructed two large-scale datasets: iSatNat and iSoundNat (Table 1). iSatNat consists of pairs of ground-level and satellite imagery while iSoundNat contains pairs of ground-level images and audio recordings of species. We begin with the iNat-2021 dataset [21] which contains 2.7M images of species alongwith metadata including geolocation information. We built iSatNat by collecting 2.7M Sentinet-2 level 2A imagery corresponding to each ground-level image of species in the iNat-2021 dataset. iSoundNat was built by collecting ground-level images of species from the iNaturalist platform which contained audio recordings. We specifically downloaded research grade observations from the platform. This resulted in a total of 88,130 pairs of images and audio. For more details about the datasets, please refer to the appendix. We leverage BioCLIP's TreeofLife-10M [5] dataset for image-text pretraining. To simplify our experimentation, we use pretrained BioCLIP vision and text encoders. For training the location encoder, we use the geolocation information present in the iNat-2021 dataset corresponding to each image. We use the environmental variables from WorldClim-2.1 for training the environmental encoder. To do this, we extract the bioclimatic variables corresponding to the geolocations present in the iNat-2021 dataset.\n\nTaxaBench-8k. For evaluation and further research, we constructed a truly multimodal dataset containing six paired modalities: ground-level image, geographic location, satellite image, text, audio, and environmental features. We begin with the test split of our iSoundNat dataset which contains 8,813 image and audio pairs. For each sample, we downloaded Sentinel-2 level 2A imagery corresponding to the geolocation information present with that sample. Similarly, we extracted the environmental features from WorldClim-2.1 corresponding to the geolocations of the samples. Our TaxaBench-8k dataset is multifaceted and can be used to evaluate ecological models for various tasks such as cross-modal retrieval, species classification, and audio classification.\n\nEvaluation datasets. We evaluate our models on a range of downstream ecological tasks. We assess the effectiveness of each encoder through these downstream tasks. The de-"}, {"title": "4. Method", "content": "We aim to learn a unified embedding space that can uniquely characterize a given taxon. This is done by aligning all available modalities to the ground-level images of species. For this, we utilize the BioCLIP [5] embedding space as a teacher and learn to project the embeddings from all the modalities to this space. We propose multimodal patching to further distill task-specific unique knowledge into the modalitiy-specific encoders. This enables embedding arithmetic which is useful for zero-shot classification and cross-modal retrieval tasks. Further, the modalities exhibit emergent properties with each other when only trained with corresponding ground-level image pairs. Below we discuss the contrastive learning approach used for training, multimodal patching, and the implementation details of our framework."}, {"title": "4.1. Contrastive Learning", "content": "InfoNCE [12] is a widely used loss function for learning an embedding space with similar and dissimilar examples within a single mini-batch. This loss function works by treating paired examples as positives while considering all other pairs as negatives. These pairs can be constructed using augmentation techniques or can originate from different modalities, such as images and text. However, for our problem, we note that a single mini-batch may contain multiple instances of the same species category. Considering examples from the same species category as negative may lead to sub-optimal performance. Hence, we utilize the concept of CLIP loss [11] and combine it with SupCon loss [13] as the species category information is available for each ground-level image. Consider the pair of modalities (G, M), where G denotes the ground-level image modality while M denotes some other modality (e.g. satellite imagery). Consider a dataset with aligned observations of the two modalities and a mini-batch of examples during training {gi, mi}i=1,...N. Using deep networks f and h, one can"}, {"title": "4.2. Multimodal Patching", "content": "obtain normalized embeddings of the respective modalities as zi = f(gi) and Yi = h(mi). Let N(i) denote the set of all plausible indices in a mini-batch and P(i) denote the set of indices of examples with the same species category as that of the ith example. Then, the deep networks are optimized using the following combined loss:\n\n$$L_{G\\rightarrow M} = \\sum_{i\\in N(i)} -\\frac{1}{|P(i)|}\\sum_{j\\in P(i)} log\\frac{exp(z_i \\cdot Y_j/\\tau)}{\\sum_{n\\in N(i)} exp(z_i \\cdot Y_n/\\tau)}$$\n\n$$L_{M\\rightarrow G} = \\sum_{i\\in N(i)} -\\frac{1}{|P(i)|}\\sum_{j\\in P(i)} log\\frac{exp(Y_i \\cdot z_j/\\tau)}{\\sum_{n\\in N(i)} exp(Y_i \\cdot z_n/\\tau)}$$\n\n$$L_{G,M} = \\frac{L_{G\\rightarrow M} + L_{M\\rightarrow G}}{2}$$\n\nwhere \\$\\tau\\$ is a scalar temperature parameter that controls the sensitivity of the predicted softmax distribution. The loss function seeks to cluster positive examples from the modalities in the embedding space while pushing negative examples farther away.\n\nWe describe multimodal patching as shown in Figure 2. The overall framework consists of three steps. The pseudo-code for multimodal patching is presented in Algorithm 1. Let f be a pretrained binding modality encoder and h be an encoder of a different modality. First, we perform locked tuning of h to obtain h*. In this step, f is utilized as a teacher, and embeddings obtained from h are learned to project to the existing embedding space of f. In the second step, we perform full finetuning of f and h to obtain f* and h\u00b9 respectively. The encoders f* and h\u207a no longer preserve the original embedding space of f. Finally, in the third step, we perform patching [10] of h by linearly interpolating the weights between h* and h\u207a. The interpolation weights are determined by analyzing the performance of the patched models on a patching task P. We utilize zero-shot classification with text as the patching task. This patching task helps to preserve the original embedding space of f and enables emergent capabilities. These steps are repeated"}, {"title": "4.3. Implementation Details", "content": "for all available modalities. Since f is shared across the modalities, we perform sequential patching of f across all the modalities as described in Algorithm 1. Our strategy modifies the weights of f while preserving the original embedding space, unlike ImageBind, where f is frozen.\n\nWe aim to learn an embedding space consisting of six modalities. To do this, we employ modality-specific encoders to encode each of the modalities into a joint embedding space. This setup proves to be computationally feasible and allows for precomputing embeddings. We use the transformer architecture for encoding ground-level images, text, satellite images, and audio. We use pretrained BioCLIP [5] (ViT-B/16) vision and text encoders for the ground-level images and text respectively. We use pretrained CLIP [11] (ViT-B/16) vision encoder for the satellite images and a pretrained CLAP [30] encoder for audio. We use the architecture described in GeoCLIP [29] to encode the geographic location. It consists of an Equal Earth Projection (EEP) operation, a random Fourier feature transform, and an MLP network. We use a ResNet-style MLP [1] to encode the environment features for a given geographic location.\n\nWe use a native embedding size of 512 for all the encoders as used in BioCLIP. All the encoders are trained independently using the BioCLIP vision encoder. Images are normalized and resized to (224, 224) pixels before feeding them to their respective encoders. For training the location encoder, we additionally sample pseudo-negative locations as a way to train on locations absent in the training dataset [1]. For zero-shot classification using text, we consider the entire taxonomic description of a species. For each audio sample, we average across the channel dimension to get single-channel audio. Then we convert each"}, {"title": "5. Experiments", "content": "single channel audio sample into mel-spectrogram features using the default CLAP settings: {feature_size=64, sampling_rate=48000, hop_length=480, max_length_s=10, fft_window_size=1024} in the HuggingFace-wrapper: ClapProcessor for the pre-trained CLAP model clap-htsat-fused. For each training run, we use 2 NVIDIA H100 GPUs, a batch size of 256, and a gradient accumulation of 8.\n\nWe conduct several empirical evaluations of our modality-specific encoders against state-of-the-art methods across a range of ecology-related tasks. All details about each dataset used for evaluation are mentioned in appendix. Additionally, to evaluate the effectiveness of our proposed multimodal patching strategy, we compare it against the training recipe of ImageBind [7], where the binding modality encoder is kept frozen during the training. The ImageBind training recipe is equivalent to restricting our multimodal patching strategy to its first step. In the experiments, we show that adding the embeddings from multiple modalities"}, {"title": "Multimodal Patching.", "content": "leads to improved performance compared to using embeddings of a single modality. Below we discuss various experimental results.\n\nWe first show that patching improves zero-shot image classification of the binding modality encoder. In Figure 3, we present the zero-shot image classification performance of our model when patched with varying values of a. It is seen that for certain values of a, our model exhibits improved zero-shot performance than the base pretrained model of ImageBind. This behavior is observed across all the modalities independently. For all the modalities, the optimal value of a is close to 0.1, indicating that models patched using higher values of a (close to 1) move significantly away from the embedding space of BioCLIP. We further show in Table 7 that multimodal sequential patching has an additive effect on our model. These experiments empirically demonstrate that ImageBind's training method fails to effectively distill knowledge from various modalities and that it is possible to improve the performance of pretrained binding modality encoder."}, {"title": "Zero-shot image classification.", "content": "We evaluate Tax-"}, {"title": "5.1. Ablation Studies", "content": "aBind's image encoder on zero-shot image classification task using the taxonomic description of species. We use BioCLIP [5] and ArborCLIP [6] as image-only baseline models. For multimodal evaluation, we compare against our ImageBind-trained baseline. In this setting, the embeddings obtained from the image encoder are added to those obtained from a different modality encoder and then used for zero-shot classification. For image-only experiments, we use Birds525 [31], CUB-200-2011 [32] and BioCLIP-Rare [5]. For multimodal experiments, we use iNat-2021 [21] and TaxaBench-8k. The results presented in Table 2 show that our model outperforms the baseline models in both settings across 4 out of 5 datasets. It is worth noting that our model always outperforms BioCLIP, which indicates the benefit of distilling multimodal information into the ground-level image encoder. Finally, the addition of ground-level and satellite image embedding leads to the best zero-shot classification performance.\n\nTo demonstrate the emergent capabilities of our models, we evaluate on the task of cross-modal retrieval. We use TaxaBench-8k, which has a gallery size of 8,813 data points. In each retrieval setting, we selected the pair of modalities which were not explicitly trained together. The results, shown in Table 3, highlight the superior performance of TaxaBind compared to both a random baseline and the ImageBind framework. We further present six qualitative examples of species image to satellite image retrieval results in Figure 4. Here, we use a gallery of 100k satellite images and retrieve the top 4 most similar satellite images given a ground-level species image. The retrieved images show habitat characteristics correlated with those of the query species. This suggests that our models have learned to capture fine-grained habitat and climate-related information about the species. We present additional quantitative retrieval results in the appendix."}, {"title": "Cross-modal retrieval.", "content": "The results reported previously demonstrated that our proposed multimodal patching strategy outperforms the ImageBind training strategy. We now study the performance of using different patching strategies below.\n\nSingle-modality patching. In Figure 3, we presented the results achieved by the binding modality encoder when patched by training with each modality independently. Now, we investigate whether our sequentially patched model can outperform these single-modality-specific patched models. In Table 7, it is noticed that our model outperforms each of the other patched models. The sequential patching method has an additive effect and can distill knowledge from various modalities.\n\nMultimodal patching type. In addition to the sequential patching technique, the parallel patching technique can be used to simultaneously patch the binding modality encoder with all the modalities. This involves averaging the weights of all the single-modality patched models and then determining the best interpolation weight using the patching task. Table 7 demonstrates that parallel patching yields inferior results compared to the sequential patching method, and it even performs worse than models patched using geographic location or satellite images. This indicates that averaging the weights of the individually patched models is not optimal. It also suggests that modalities such as geographic location and satellite imagery are more crucial for the downstream zero-shot classification task."}, {"title": "6. Conclusions", "content": "In this work, we presented TaxaBind, a unified embedding space for ecological applications covering six modalities: ground-level images, geographic location, satellite images, text, audio, and environmental features. We introduced multimodal patching, a technique to distill knowledge from multiple modalities into a binding modality, improving upon existing methods like ImageBind. We curated two datasets, iSatNat and iSoundNat, to train our models, and introduced TaxaBench-8k, a multimodal dataset for evaluating ecological models. Our extensive experiments demonstrated TaxaBind's effectiveness on various tasks such as zero-shot species classification, cross-modal retrieval, and audio classification, outperforming state-of-the-art methods. Through our multimodal framework, we showed the effectiveness of combining multiple modalities for addressing downstream ecological tasks. We emphasize that our models are general purpose and could potentially be used for other ecology and climate-related applications such as deforestation mapping, tree canopy height mapping, etc. In the future, we plan to explore different techniques for integrating multimodal data in ecology."}, {"title": "7. Acknowledgements", "content": "We gratefully acknowledge the Taylor Geospatial Institute and the University of Illinois for providing access to TGI RAILS, a high-performance"}, {"title": "A. Retrieval Results", "content": "Here we provide additional cross-modal retrieval results of TaxaBind on our TaxaBench-8k dataset (Table 8). It is observed that the retrieval performance improves when embeddings from different modalities are added together. We also present six examples of ground-level image-to-satellite image retrieval (Figure 6). For each example, we present the top-4 most similar satellite images retrieved by our model. We also present range map predicted by our model for Abies balsamea in Figure 7."}, {"title": "B. Datasets", "content": "Here we provide additional details about the datasets used for training and evaluating our models."}, {"title": "B.1. Training Datasets", "content": "iSatNat. This dataset was built by collecting Sentinel-2 Level 2A imagery corresponding to each geolocation in the iNaturalist-2021 dataset. We used the training and validation splits of the dataset and dropped the testing split since it lacked the ground-truth labels. We used the validation split as the unseen testing set. We created a 90:10 split of the training dataset to create the final training and validation sets. Lastly, we applied a minimal filter to remove all the samples lacking geolocation entry.\n\niSoundNat. Using the iNaturalist platform, we filtered all the observations with audio recordings and ground-level images to date. We then removed all corrupted audio recordings and converted them to a common format (m4a). This"}, {"title": "C. Evaluation Datasets", "content": "resulted in a total of 88,130 observations. We then used a stratified sampling technique to split the dataset into 85:5:10 (train, validation, test) ratio. The spatial distribution of the dataset is shown in Figure 8.\n\nWorldClim 2.1. This dataset consists of 19 bioclimatic variables and an additional elevation map. All the channels are scaled to a resolution of 5 arc minutes.\n\nTaxaBench-8k. We extended the testing split of iSoundNat by downloading Sentinel-2 Level 2A imagery corresponding to each location in the split. This dataset is used for evaluating our models on zero-shot image classification and cross-modal retrieval. The spatial distribution of the dataset is shown in Figure 8c.\n\nBirds525 [31]. This dataset consists of images of bird species across 525 categories. Each image features a single bird species. To evaluate our models, we use the testing split of the dataset which consists of 2,625 samples.\n\nCUB-200-2011 [32]. This dataset consists of images of 200 bird species. We use the testing split of the dataset which contains 5,794 images.\n\nBioCLIP-Rare [5]. This dataset was used for the evaluation of BioCLIP. It contains 400 species categories not present in the TreeOfLife-10M dataset.\n\nBirdCLEF-2022 [39]. This dataset contains audio recordings of rare bird species in Hawaii. We use the training split of the dataset which contains annotations. In total, there are 14,852 audio recordings across 141 categories. We use stratified sampling to split the dataset into 85:5:10 (train,"}, {"title": "Ecoregions.", "content": "validation, test) ratios.\n\nBirdCLEF-2023 [40]. This dataset contains audio recordings of bird species in Kenya. We use the training split of the dataset which contains annotations. In total, there are 16,941 audio recordings across 264 categories. We use stratified sampling to split the dataset into 85:5:10 (train, validation, test) ratios.\n\nBirdCLEF-2024 [41]. This dataset contains audio recordings of bird species in Western Ghats, India. We use the training split of the dataset which contains annotations. In total, there are 24,459 audio recordings across 182 categories. We use stratified sampling to split the dataset into 85:5:10 (train, validation, test) ratios.\n\nWe use the ecoregion map from [38]. The map consists of 846 distinct categories of ecoregions. We randomly sample 100k points around the globe. We split the points into 85:5:10 (train, validation, test) ratio. Then we perform linear probing on our model and report top-1 classification accuracy on the testing split."}, {"title": "D. Ethics and Limitations", "content": "Biome. We use the biome map from [38]. The map consists of 14 distinct categories of biomes. We randomly sample 100k points around the globe. We split the points into 85:5:10 (train, validation, test) ratio. Then we perform linear probing on our model and report top-1 classification accuracy on the testing split.\n\nGeoPlant [27]. This dataset consists of the presence-absence of plant species across different countries in Europe. The dataset additionally contains presence-only observations from GBIF. We use only the presence-absence split which contains 88,783 unique locations. We use stratified sampling using country labels to split the set into 85:5:10 (train, validation, test) ratios. For each location, the dataset contains the presence of multiple species. We use SatBird's [3] training procedure and predict the presence-absence of species at each of the locations.\n\nSatBird [3]. This dataset contains the presence-absence checklist of bird species in two regions: the USA and Kenya. The dataset for the USA is further divided into two seasons: summer and winter. Each location in the dataset is associated with a multispectral Sentinel-2 satellite image.\n\nThe models built are a proof-of-concept for demonstrating the benefits of combining multiple modalities for solving ecological problems. Care must be taken when utilizing our models for real-life applications. Additional validation may be necessary before deploying our models. We recognize that the datasets used for training and evaluation may have some spatial bias. However, the goal of this work is not to specifically tackle the issue of spatial bias, but rather to utilize and understand patterns in multiple modalities. We observe that incorporating additional modalities into the framework helps to implicitly account for this bias in the data."}]}