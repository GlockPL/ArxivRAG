{"title": "The structure of the token space for large language models", "authors": ["Michael Robinson", "Sourya Dey", "Shauna Sweet"], "abstract": "Large language models encode the correlational structure present in natural language by fitting segments of utterances (tokens) into a high dimensional ambient latent space upon which the models then operate. We assert that in order to develop a foundational, first-principles understanding of the behavior and limitations of large language models, it is crucial to understand the topological and geometric structure of this token subspace. In this article, we present estimators for the dimension and Ricci scalar curvature of the token subspace, and apply it to three open source large language models of moderate size: GPT2, LLEMMA7B, and MISTRAL7B. In all three models, using these measurements, we find that the token subspace is not a manifold, but is instead a stratified manifold, where on each of the individual strata, the Ricci curvature is significantly negative. We additionally find that the dimension and curvature correlate with generative fluency of the models, which suggest that these findings have implications for model behavior.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) encode the correlational structure present in nat-ural language by fitting segments of utterances (tokens) into a high dimensional ambient latent space upon which the models then operate. Not every point in this latent space is linguistically meaningful; only a subspace of it corresponds to tokens that are extant in the learned vocabulary of the target language. We assert that in order to develop a foundational, first-principles understanding of the behavior and limitations of large language models, it is crucial to understand the topological and geometric structure of this token subspace.\nSince the tokens are determined prior to model training, one of the tasks of training from scratch is to determine numerical coordinates for each token. Operationally, the token subspace is therefore defined via the image of a learned embedding function (or simply an embedding), which defines the values of the numerical coordinates at each token. In this paper, we take this embedding function as our starting point.\nThe mathematical definition of an embedding requires that both the ambient latent space and the space of tokens each have a well-defined topology. Since the latent space is usually ascribed the usual Euclidean metric, this induces a topology on both the latent space and the token subspace. It is therefore most reasonable to assert that the space of tokens, abstractly, also has a topology induced by the embedding function, which satisfies the mathematical definition of an embedding. From these assertions, we can then show that it is possible to reliably estimate two properties of the token subspace: dimension (a topological property) and Ricci scalar curvature (a geometric property). We aim to show that with these estimates of dimension and Ricci scalar curvature, it is possible to reliably anticipate the inferential quality of model behavior within and across regions of the token subspace.\nIn this article, we present the dimension and Ricci scalar curvature for three open source large language models of moderate size: GPT2 [1], LLEMMA7B [2], and MISTRAL7B [3]. In all three models, using these measurements, we find that the token subspace is not a manifold, but is instead a stratified mani-fold, where on each of the individual strata, the Ricci curvature is significantly negative. These findings have implications for model behavior, its boundedness and predictability at inference, and its stability under retraining."}, {"title": "1.1 Contributions", "content": "It is a natural question whether the token subspace has the structure of a man-ifold, a topological space for which every point has a neighborhood that is"}, {"title": "1.2 Related research", "content": "Metrics for language have been considered before large language models became popular [6]. The problem of finding the token embedding itself is a special case of matrix factorization [7, 8], in which the subspace of tokens is assumed to span a low-dimensional linear subspace, which is a manifold [9]. Aside from [10, 11], in which it is argued that subspace of neuron activations or tokens might be a manifold with singularities, we are unaware of any work that attempts to interrogate the assumption that the relevant subspace of the latent space is a manifold, or attempts to discover the topological or geometric structure of the token subspace itself. This is surprising since a hypothesis test for manifolds is known [12], and the dimension of word embeddings has been estimated before (for instance [13, 14], among others). This work both attempts to detect and"}, {"title": "1.3 Implications", "content": "If the token subspace is not a manifold, this has important implications because the behavior of the transformer blocks\u00b9, which are piecewise smooth (hence continuous) transformations of the latent space [18], must therefore preserve the dimensions we observe. As a result, queries that cross stratification boundaries will yield responses that exhibit dramatic changes in behavior. This instability will likely preclude strong guarantees about the model's generative performance without intimate knowledge of how the token subspace is embedded within the ambient latent space."}, {"title": "2 Methods", "content": "In this section, we describe a novel Monte-Carlo-based method for estimating both local dimension and Ricci scalar curvature of the token subspace. We also briefly summarize approaches to interpret estimates using both visual and parametric methods.\nFirst, we introduce the relationship between volume and radius as a func-tion of dimension and curvature, our specific quantities of interest. In Euclidean space of dimension d, each point corresponds to a vector in Rd, in which dis-tances between two points x, y \u2208 Rd is given by the familiar formula\n$\\displaystyle dist(x, y) = \\sqrt{\\sum_{i=1}^d |x_i - y_i|^2}.$"}, {"title": "2.1 Estimating volume", "content": "Equation (3) gives a powerful tool for extracting topological and geometric fea-tures from a log-log plot of the volume versus radius of a disk centered at a particular token within the token subspace. The token embedding produces a discrete subset of Euclidean space (a so called point cloud), which has the appearance of being a set of samples drawn from a larger subspace that \"inter-polates them\" in the latent space. If we assert that the tokens are approximately uniformly distributed on the token subspace\u00b3, then a Monte Carlo estimation of volume is reasonable. The volume of a ball of radius r centered at a token j is proportional to the number of points within a distancer to j, that is\nv(r; j) \u2248 M#{i : ||i \u2013 j|| \u2264 r},\nwhere the vertical bars indicate a distance calculation, and M is the volume contribution of each token to the Monte-Carlo estimate. Except in the case of subspaces of known volume, M is usually not known and must be treated as a nuisance constant.\nSuppose that there are p tokens in total and that the tokens are located in general position in the latent space. Then for a given token j, we can obtain a sequence\nr1,j < 12,j <\u2026\u2026\u2026 <rp,j\nof distances to the other tokens such that\nv(rk,j;j) \u2248 Mk.\nWe can arrange the set of ri,j values in a matrix, in which we interpret i as specifying rows and jas specifying columns. Notice that each column of the ri,j matrix is sorted in ascending order. Since the sequence of volumes corresponding each token (column) is the same, namely the sequence of integers from 1 to p, we can rewrite the log-linear portion of Equation (3) as a matrix equation for token j,\n$\\begin{pmatrix}\n0 \\\\\n\\log 2 \\\\\n\\vdots \\\\\n\\log p\n\\end{pmatrix} + \\log M \\approx  \\begin{pmatrix}\n1 & \\log r_{1,j} \\\\\n1 & \\log r_{2,j} \\\\\n\\vdots & \\vdots \\\\\n1 & \\log r_{p,j}\n\\end{pmatrix} \\begin{pmatrix}\nn_j \\\\\n(\\log K_j)\n\\end{pmatrix} + O(r^2)$\nThis is readily solved via least squares regression to obtain estimates for the dimension ng and scaling coefficient K; at each token j.\nIf M is unknown, Equation (5) makes it clear that the estimates of K; are determined up to a multiplicative constant, and moreover n; is not impacted. The estimate of Ric in Equation (4) is also not impacted because it only depends upon the difference log Kj \u2013 logu, in which the contributions of M cancel."}, {"title": "2.2 Interpreting volume-versus-radius curves", "content": "The log-log plot of volume versus radius at each token provides considerable information. Briefly,\n1. The slope of the curve corresponds to local dimension,\n2. A \"knee\" in the curve, where the slope changes from one non-negative value to another is a clear sign that the space is not a manifold,\n3. A horizontal gap in the curve (a zero slope portion) may mean that the neighborhood of the token contains multiple connected components, and\n4. The concavity of the curve determines the local curvature, where con-cave up corresponds to negative Ricci scalar curvature, and concave down corresponds to positive Ricci scalar curvature.\nUnpacking each of these in turn, it is immediately clear that the slope (derivative) of such a curve for small r (near the bottom left of the plot), gives the dimension of the neighborhood of the token. Since our estimates are ob-tained by Monte-Carlo estimation, it is wise to exclude a small neighborhood of the token from analysis, since this tends to have large sampling error.\nIf the slope changes, this means that the dimension of the neighorhood of the token has changed as the radius increases. It is possible for the slope to increase or decrease, depending on whether the neighborhood expands to con-tain a greater or lesser dimension region. It is worth noting that estimates of dimension far from the original center token may be biased, but the fact of an abrupt change is compelling evidence that the token subspace is not a manifold.\nOn the other hand, a gap in the token subspace will mean that there are no additional tokens within a certain band of radii. This means that the volume estimate will not change over these radii. The result is a curve with horizontal (zero) slope.\nFinally, according to Equation (3), nonzero Ricci curvature adds a quadratic term to the volume. This makes the volume-versus-radius curve either concave up or down according to the sign of the curvature."}, {"title": "3 Results", "content": "We examined the token subspaces of three LLMs: GPT2, LLEMMA7B, and MISTRAL7B. The basic features of these subspaces are shown in Table 1. A"}, {"title": "3.1 GPT2", "content": "Figure 1 shows volume-versus-radius curves for several tokens in GPT2. Be-cause of Monte-Carlo estimation error, there is some variability for small vol-umes, so we begin our estimation with the volume corresponding to 10 tokens to avoid undue error.\nKnees between mostly straight segments are prominently visible in the curves for all tokens shown, which provides compelling evidence that the token subspace is a stratified manifold. It is also interesting that the dimension of the tokens, and the structure of their stratifications, is different for different tokens. For instance, the difference between the US dollar symbol $ versus other currency symbols may be due to its use in the syntax of programming languages.\nFigure 2 shows the distribution of local dimensions estimated by our method for GPT2's token subspace. The distribution of numeric tokens is unimodal"}, {"title": "3.2 LLEMMA7B", "content": "Figure 6 shows the volume-versus-radius curves for several tokens in LLEMMA7B. The first thing to notice is that the nearest 100 or so tokens are all equidistant from the two tokens shown. This can be inferred by the vertical slopes on the left edge of both curves. Since this is likely an artifact of how the embedding was designed, possibly due to rounding of the coordinates, it is appropriate to exclude volumes below 100 from our estimates below. While clear stratifications are not visible as knees in the curves, the token subspace has negative curvature near the two tokens shown since the curves are visibly concave up.\nFigure 7 shows the distribution of local dimensions for the token subspace of LLEMMA7B. Again, the numeric tokens have a statistically significantly different distribution of local dimensions from the non-numeric tokens. Indeed, many of the numeric tokens have a local dimension of 0, which means that they are isolated points. As such, many of the numeric tokens are not near each other.\nUnlike the case of GPT2, where the presence of knees in volume-versus-radius curves in Figure 6 establish that the token subspace is not a manifold, the stratifications in LLEMMA7B are a bit more subtle. Figure 8 shows the resulting tSNE visualization, again colored by dimension. The numeric tokens are located in two distinct regions of the plot as indicated. There are several connected components visible in Figure 8. The largest connected component shows a marked transition in local dimension, which is strong evidence of a stratification."}, {"title": "3.3 MISTRAL7B", "content": "Figure 11 shows the volume-versus-radius for a typical token in MISTRAL. As in the case of LLEMMA7B, there are about 150 tokens that are equidistant from this token. Since this appears to be the typical situation, we will use volumes with at least 150 for estimation purposes below. A knee in the curve corresponding to a stratification is visible, along with strong negative curvature.\nFigure 12 shows the distribution of dimensions for all tokens in MISTRAL7B. It is interesting to compare this with Figure 7, because the same set of tokens is"}, {"title": "4 Discussion", "content": "We produced strong evidence that, for the three large language models under consideration in this article, the local dimension of the token subspace in large language models varies along connected components of that token subspace. This evidence implies that the token subspaces of these models cannot be man-ifolds. Moreover, because the local dimension appears to change abruptly at multiple places along a slice through the token subspace, it seems likely that the token subspaces of these models consist of manifolds of different dimensions attached to each other. In other words, the token subspace of each of these three models are stratified manifolds.\nOur work suggests that the presence and nature of the stratification of the to-ken subspace impacts in a predictable way the inferential quality and generative fluency of large language models. The examination of numeric and non-numeric tokens in this article, across models, provides an illustration of this point. While the token subspace of GPT2 is not a manifold, the numeric token subspace is likely a manifold. Since all of the numeric tokens are nearby each other and are confined to a constant dimension submanifold, this limits the expressivity of any continuous dynamic map (e.g. a transformer) that operates upon them. Because the dynamic map induced by a transformer is continuous (which follows"}]}