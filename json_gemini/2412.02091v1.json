{"title": "The Problem of Social Cost in Multi-Agent General Reinforcement Learning: Survey and Synthesis", "authors": ["Kee Siong Ng", "Samuel Yang-Zhao", "Timothy Cadogan-Cowper"], "abstract": "The paper discusses the problem of social harms that can result from actions taken by learning and utility-maximising agents in a multi-agent environment. It proposes market-based mechanisms to quantify and control the cost of such social harms, particularly in the context of artificial general intelligence (AGI) agents.", "sections": [{"title": "Introduction", "content": "The AI safety literature is full of examples of powerful AI agents that, in blindly pursuing a specific and usually narrow objective, ends up with unacceptable collateral damage to others, including destroying humankind and the world in extreme cases; see, for example, [111, 18]. Many of these examples are effectively variations of the tragedy of the commons phenomenon, which has been studied extensively in economics [59, 100, 35]. Tragedy of the commons typically occur because of an externality that arises when a utility-maximising economic agent does not pay an appropriate cost when making a decision to take an action that provides private benefit but incurs social harm to others, in particular those that are not a party to the decision-making process. Pollution, overfishing, traffic are all classic examples of multi-agent economic systems that exhibit externalities.\nIn this paper, we consider the problem of social harms that can result from actions taken by learning and utility-maximising agents in a multi-agent environment. The problem of measuring social harms or impacts in such multi-agent settings, especially when the agents are artificial generally intelligent (AGI) agents, was listed as an open problem in [44]. We provide a partial answer to that open problem in the form of market-based mechanisms to quantify and control the cost of such social harms in \u00a7 4. The key proposal is the protocol described in \u00a7 4.1 and the agent valuation functions stated in equations (14)-(15).\nThe protocol captures many existing and well-studied special cases which we list in \u00a7 4.2. Our proposed setup is more general than existing formulations of multi-agent reinforcement learning with mechanism design in two ways: (i) the underlying environment is a history-based general reinforcement learning environment like in AIXI [69]; (ii) the reinforcement-learning agents participating in the environment can have different horizons. To demonstrate the practicality of the proposed setup, we survey some possible learning algorithms in \u00a7 5 and present a few applications in \u00a7 6.\nAs the background literature is rich, both in breadth and depth, we have taken the liberty to take an expository approach in writing the paper and will introduce key topics and concepts as they are required in the development of the paper, starting with General Reinforcement Learning in \u00a7 2 and Mechanism Design in \u00a7 3. Readers who are familiar with these topics should be able to skip those sections without issue."}, {"title": "General Reinforcement Learning", "content": ""}, {"title": "Single Agent Setting", "content": "We consider finite action, observation and reward spaces denoted by A, O, R respectively. The agent interacts with the environment in cycles: at any time, the agent chooses an action from A and the environment returns an observation and reward from O and R. Frequently we will be considering observations and rewards together, and will denote x \u2208 Ox Ras a percept x from the percept"}, {"title": "Multi-Agent Setting", "content": "In the multi-agent setup, we assume there are k > 1 agents, each with its own action and observation spaces A\u017c and Oi, i \u2208 [1...k]. At time t, the k agents take a joint action\n$at = (at,1,..., at,k) \u2208 A\u2081 \u00d7 \uff65\uff65\uff65 \u00d7 Ak = A$\nand receives a joint percept\n$ort = (ort,1,..., ort,k) \u2208 (01 \u00d7 R) \u00d7 \uff65\uff65\uff65 \u00d7 (Ok \u00d7 R) = O \u00d7 R$.\nThe joint history up to time t is denoted ht = aor1:t = a1or1a20r2... at ort. We assume for now that every agent can see the entire joint history; this assumption will be reexamined in \u00a7 5.2.\nDefinition 5. A multi-agent environment o is a sequence of probability distributions {00, 01, 02,...}, where on : (A)n \u2192 D(O \u00d7 R)", "satisfies\n$Vai": "n Vor<n en-1(or"}, {"title": "Mechanism Design", "content": ""}, {"title": "Tragedy of the Commons", "content": "The key to solving tragedy of the commons issues is to work out a way to 'internalise' the externality in the design of the multi-agent economic system of interest. There are two primary approaches: price regulation through a central authority, and a market-based cap-and-trade system. The former is sometimes referred to as Pigouvian tax after [106, 105], and it requires a central authority to (i) have enough information to quite accurately determine the unit price of the externality or social harm; and (ii) enforce its payment by agents that cause the externality, thereby internalising it. In contrast, the cap-and-trade system is motivated by the idea of Coasean bargaining [31, 30], whereby the maximum amount of the externality or social harm allowed is capped through the issuance of a fixed number of permits, each of which allows an agent to produce a unit of externality, and the agents are allowed to determine for themselves whether to use their permits to produce externality, or trade the permits among themselves for profit. The idea is that the cap-and-trade system will allow the agents that are most efficient in generating private benefit while minimising social harm to win because they can afford to pay a higher price for the permits. Indeed, the Coase 'Theorem' says that as long as the permits are completely allocated and there is no transaction cost involved in trading, then the agents will collectively end up with a Pareto efficient solution. So a market made up of utility-maximising agents, under the right conditions, is capable of determining the right price for the externality; there is no need for an informative and powerful central authority to set the price.\nIn the following sections, we will look at some concrete protocols from the field of Mechanism Design for implementing Coasean bargaining and trading in multi-agent environments. In keeping with the intended spirit of [32], we will largely avoid the term externality from here onwards and favour, instead, the term 'social harm'."}, {"title": "The VCG Mechanism", "content": "In a multi-agent environment, the different agents participating in it can be given different goals and preferences, either competing or collective, and the algorithms behind those agents can exhibit different behaviour, including differing abilities in learning and planning for the long term. Mechanism design [94] is the study of protocols that can take the usually dispersed and private information and preferences of multiple agents and aggregating them into an appropriate social choice, usually a decision among alternatives, that maximises the welfare of all involved.\nLet A be a set of alternatives for a set of k agents. The preference of agent i is given by a valuation function vi: A \u2192 R, where vi (a) denotes the value that agent i assigns to alternative a being chosen. Here, vi \u2208 Vi, where Vi C RA is the set of possible valuation functions for agent i. We will use the notation V_i = V\u2081 \u00d7 \u00d7 Vi\u22121 \u00d7 Vi+1 \u00d7 Vk in the following.\nDefinition 6. A mechanism is a tuple (f, p1, ..., pk) made up of a social choice function f: V\u2081 \u00d7 \uff65\uff65\uff65 \u00d7 Vk \u2192 A and payment functions P1,...,pk, where pi: V\u2081 \u00d7 \u00d7 V \u2192 R is the amount that agent i pays to the mechanism.\nGiven a mechanism (f, p1,...,pk) and k agents with value functions v1,..., Uk, the utility of agent i from participating in the mechanism is given by\n$U\u017c (V1, ..., vk) := vi(f(v1, ..., Uk)) \u2014 Pi (V1, ...,Uk).$\nDefinition 7. A mechanism (f, P1,..., pk) is called incentive compatible if for every agent i with valuation function vi \u2208 Vi, for every vi \u2208 Vi, and every V-i \u2208 V-i, we have\n$Vi(f(Vi, V-i)) - Pi(Vi, V_i) \u2265 Vi(f(vi, v\u2212i)) - Pi (Vi, V-i).$\nThus, in an incentive compatible mechanism, each agent i would maximise its utility by being truthful in revealing its valuation function vi to the mechanism, rather than needing to worry about obtaining an advantage by presenting a possibly false / misleading v.\nDefinition 8. A mechanism (f,P1,...,Pk) is individually rational if for every agent i with valuation function vi \u2208 Vi and every vi \u2208 V-i, we have\n$Vi(f(vi, v-i)) - Pi(Vi, V\u2212i) \u2265 0.$\nIn other words, the utility of each agent is always non-negative, assuming the agent reports truthfully.\nDefinitions 6, 7 and 8 can be generalised to allow the social choice function f and the payment functions pi's to be randomised functions, in which case we will work with the expectation version of (7), (8) and (9)."}, {"title": "The Exponential VCG Mechanism", "content": "We have shown in \u00a7 3.2 that VCG mechanisms are incentive compatible and individually rational, which means agents are incentivised to participate and be truthful. It turns out that VCG mechanisms can be made privacy-preserving too. The exponential mechanism [90], a key technique in differential privacy [39], has been shown in [64] to be a generalisation of the VCG mechanism that is differentially private, incentive compatible and nearly optimal for maximising social welfare. We now briefly describe this key result and furnish the proofs, which are rather instructive.\nDefinition 11. A randomized algorithm M : V\u2081 \u00d7 \u00d7 Vk \u2192 A is (\u20ac, \u03b4)- differentially private if for any v \u2208 V\u2081 \u00d7 \u00d7 Vk and for any subset CA\n$P(M(v) \u2208 \u03a9) \u2264 eP(M(v') \u2208 \u03a9) + \u03b4,$\nfor all v' such that |v - v'|1 \u2264 1 (i.e. there exists at most one i \u2208 [n] such that Vi \u2260 vi).\nDefinition 12. Given a quality function q : V\u2081 \u00d7\u00b7\u00b7 X VXA\u2192 R and a v \u2208 V\u2081 \u00d7 \u06f0\u06f0\u06f0 \u00d7 Vk, the Exponential DP Mechanism M\u2084(v) samples and outputs an element r \u2208 A with probability proportional to exp(2q(v, r)), where\n$max r\u2208A  V1,V2:||V1-02||1\u22641 |q(v1,r) - q(v2, r)|.$\nTheorem 4. The Exponential DP Mechanism is (\u20ac, 0)-differentially private."}, {"title": "The Social Cost of Actions", "content": ""}, {"title": "General Case", "content": "Let u be a multi-agent environment and assume there are k Bayesian reinforcement learning agents operating within it. These agents are concrete realisations of the concept of perfectly rational utility-maximising agents commonly assumed in economics. We have seen that, in the absence of some control mechanism, such multi-agent environments can exhibit bad equilibrium. To avoid tragedy of the commons-type issues, we need to impose a cost on each agent's actions, commensurate with the social harm they are causing other agents with that action, and we will see in this section how augmenting a multi-agent environment with, for example, VCG mechanisms can address such issues."}, {"title": "Special Cases and Related Settings", "content": "Here are some special cases of a mechanism controlled environment M\u25b7, all of which have a rich literature behind them."}, {"title": "Learning in the Presence of Social Cost", "content": ""}, {"title": "Measures of Success", "content": "In [115], when it comes to multi-agent reinforcement learning, the authors ask \"If learning is the answer, what is the question?\". The answer is not obvious, not only because the underlying environment is non-stationary - which can already appear in the single-agent reinforcement learning setting - but also because the agents can adapt to each other's behaviour so each agent can play the dual roles of learner and teacher at the same time. For example, the storied Tit-for-Tat strategy [8, 97, 96] is an agent policy that both learn from and 'teach' the other agents in iterated Prisoner's Dilemma-type problems.\nSeveral possible and non-unique theories of successful learning, both descriptive and prescriptive, are provided in [115, \u00a77]. In the context of multi-agent reinforcement learning under mechanism design, we are concerned primarily with designing learning algorithms that satisfy some or all of the following properties, noting that the agents do not learn policy functions to but only valuation functions that are fed into a VCG mechanism for joint action selection.\nConvergence Starting from no or only partial knowledge of the environment and the other agents, each agent's learned valuation function at any one time should converge to (15) and its realised utility should converge over time to (16).\nNo Regret An agent's learning algorithm minimises regret if, against any set of other agents, it learns a sequence of valuation functions that achieves long-term utility almost as well as what the agent can achieve by picking, with hindsight, the best fixed valuation function for every time step.\nRational An agent's learning algorithm is rational if, whenenever the other agents have settled on a stationary set of valuation functions, it settles on a best response to that stationary set.\nIncentive Compatibility In the case when the parameters of the VCG mechanism is learned through data, we require that the mechanism exhibits approximate incentive compatibility with high probability.\nSome of these concepts will be made more precise in the coming subsections."}, {"title": "Bayesian Reinforcement Learning Agents", "content": "In practice, an agent i operating in a given controlled multi-agent environment M\u25b7 will not actually have enough information to construct vt,i as defined in (14). First of all, it does not know what is. A good solution is to use a Bayesian mixture \u00c9p, for a suitable model class P, to learn 4. So at time t with history ht-1, agent i approximates the expression (ort ht-1at) by\n$\nort | ht-1at).$\nThe quantity Pt = (P1,..., Pk) in (15) can also be estimated directly using, say, another Bayesian mixture \u03beo via\n$\n\u2211wop(pt | aportat).$\nIn general terms, we can think of (17) as learning the dynamics of the underlying environment 6, and (18) as learning the preferences and strategies of the other agents in the environment."}, {"title": "Bandit VCG Mechanisms", "content": "Note that, in practice, the agents' valuation functions are not fully known but estimated from the actual percepts they get from the environment, which are in turn dependent on the joint actions chosen by the VCG mechanism. This means formula (13) in the mechanism-controlled environment protocol needs to be handled carefully; in particular there is an exploration-exploitation trade-off here, where we need to do enough explorations for the agents to arrive at good estimates of their valuation functions before we can reliably compute the arg maxa\u2208A over them. This problem can be solved using Bandit algorithms, and the techniques described in [74] that uses upper-confidence bounds [6, 83] are directly applicable in our setting. A key finding in [74] is that (asymptotic) truthfulness is harder to achieve with agents that learn their valuation functions; this may also be an issue in our setting."}, {"title": "Markov VCG Mechanisms in Unknown Environments", "content": "\u00a7 5.2 describes agents that learn in a mechanism-controlled environment. In this section, we take the perspective of the mechanism designer and look at reinforcement learning algorithms that can adjust the parameters of the VCG mechanism based on interactions with agents that are themselves capable of learning and adjusting.\nWe will first summarise the setup and algorithmic framework described in [107] and then describe some possible extensions. The environment with a controller and k agents is defined by an episodic Markov Decision Process = (S, A, H, P, {ri}=0), where S and A are the state and action spaces, H is the length of each episode, P = {Pt : S \u00d7 A \u2192 D(S)}#1 is the state transition function, and ri = {ri,t : S \u00d7 A \u2192 [0,1]}{}\u00a3\u2081 are the reward functions, with ro denoting the reward function for the controller and ri, 1 \u2264 i \u2264 k, denoting the reward function for agent i. Except for ro, the environment is unknown to the controller. The controller interacts with the k agents in multiple episodes, with each episode lasting H time steps. An initial state x1 is set at the start of each episode. For each time step t in the episode, the controller observes the current state xt \u2208 S, picks an action at \u2208 A, and receives a reward ro,t(xt, at). Each agent i receives their own reward ri,t(xt, at) and report a value ri,t(xt, at) to the controller. At the end of the episode, the controller charges each agent i a price pi. Given the controller's policy function \u03c0 = {\u03c0t : S \u2192 A}#1 and the prices {pi}=1, the controller's utility for the episode is defined by\n$k\u03bf = V\u00ed (x1; ro) + \u03a3\u03a1\u03af,$\nand each agent i's utility for the episode is defined by u\u2081 = V\u2081\"(x1;ri) - Pi,\nwhere\n$V(x;r) = \u03a3\u0395\u03c0,p[rt(xt, Tt(xt)) | Xh = x].$\""}, {"title": "Mechanism-level RL vs Agent-level RL", "content": "Note that (19) is a special case of (14)-(15), in that it is the policy (executed by the mechanism) that maximises \u03a3\u2081 41,i(\u20ac) (see (16)) when the underlying environment & is an episodic MDP, and all the agents have the same horizon m. The key advantage of performing reinforcement learning at the mechanism level is that, in the case when the mechanism is (approximately) incentive compatible, the RL algorithm has access to and can learn from all available data from interactions between the agents and the environment and mechanism, including all the individual rewards and payments. The key disadvantage is that it appears necessary to assume that all the agents have the exact same horizon. In comparison, the situation is reversed when RL is done at the level of individual agents: each agent's RL algorithm usually only has access to the subset of the interaction data in which it has a role in generating, but each agent can use different RL algorithms with different parameters like horizon / discount factor and different function approximation model classes."}, {"title": "Applications", "content": ""}, {"title": "Paperclips and All That", "content": "The paperclip maximiser [19] is a thought experiment in the AI safety folklore that illustrates a potential risk from developing advanced AI systems with misaligned goals or values. Here is the idea: Imagine an AI system is tasked with the goal of maximising the production of paperclips. (This could be an explicit high-level goal tasked by a human, or a subgoal inferred as needed by the AI system for fulfiling a higher-level goal.) As the AI system becomes more and more intelligent, it may pursue this paperclip-production goal with relentless efficiency, converting all available resources and matter into paperclips, potentially leading to disastrous consequences for humanity and the environment. A much older (and less trivial) variation of the problem was articulated by AI pioneer Marvin Minsky, who suggested that an AI system designed to solve the Riemann hypothesis might decide to take over all of Earth's resources to build supercomputers to help achieve its goal.\nWhile these are obviously extreme examples, these thought experiments help focus our minds on the following key issues in the development of increasingly powerful AI systems:\nEven seemingly harmless or trivial goals, if pursued by a superintelligent AI system without proper constraints, could lead to catastrophic outcomes as the AI single-mindedly optimises for that goal.\nIn particular, in single-mindedly pursuing a goal, a superintelligent AI may exhibit so-called convergent instrumental behavior, such as acquiring power and resources and conducting self-improvement as subgoals, to help it achieve its top-level goals at all costs, even if those actions conflict with human values or well-being.\nThese thought experiments underscore the importance of instilling the right goals, values, and constraints into AI systems from the outset, as it may be extremely difficult or impossible to retrofit them into a superintelligent system once created.\nThere is a rich literature [50, 44, 28] on aligning the design of AI systems with human values, and there is a lot of useful analyses in the single-agent general reinforcement learning (GRL) setting (\u00a7 2.1) on how to make sure, for example, that\nthe agent infers the reward function from a human by observing the person's actions through cooperative inverse reinforcement learning [58], leading to a provable solution for the off-switch problem [57] in some cases; and\npreventing an AI agent from performing adverse 'self-improvement' by tampering with its own reward function through robust learning techniques that incorporates causality [42] and/or domain knowledge [43].\nWe argue here that, in the multi-agent GRL setting, additional controls in the form of imposition of social cost on agent actions can help prevent paperclip maximiser-style AI catastrophes. In particular, in the multi-agent GRL setting where there are multiple superintelligent AI systems acting in the same environment, an AI system cannot unilaterally perform actions that destroy humanity and the environment, or engage in instrumental convergent behaviour like acquiring all available power and resources, without encountering significant frictions and obstacles because most of such actions, and their prevention by other agents (human or AI), are mutually exclusionary and therefore can be subjected to control through economic mechanisms like that described in \u00a7 4, imposed either explicitly through rules and regulations or implicitly through laws of nature (like physics and biology [24, 109]). This form of social control works in concert and in a complementary way with the controls at the single-agent GRL level. Some of the controls are likely necessary but none in isolation are sufficient; together they may be.\nIn the paperclip maximiser example, there are two forces that will stop paperclip production from spiraling out of control. Firstly, the environment will provide diminishing (external) rewards for the agent to produce more and more paperclips, assuming there are controls in place to prevent wireheading issues [92, 137] where the agent actively manipulates the environment to give it false rewards or changes its own perception of input from the environment. Secondly, at the same time that the utility of the paperclip maximiser agent is decreasing, the utility of other agents in the same environment in taking competing actions to prevent paperclip production will increase significantly as unsustainable paperclip production threatens the environment and the other agents' welfare. Thus, at some stage, the utility of the paperclip maximiser agent in producing more paperclips will become lower than the collective utility of other agents' proposed actions to stop further paperclip production, and a VCG-style market mechanism will choose the latter over the former and paperclip production stops. The argument above does rely on an assumption that the agents are operating on a more-or-less level playing field, where they need to take others' welfare into consideration when acting. In a completely lopsided environment where there is only one superintelligent agent and its welfare dominates that of all others, which could come about by design, accident, or over time through phenomenon like the Matthew effect (aka rich-get-richer or winner-take-all), social controls will not be able to stop unsustainable paperclip production, and it will only stop when the one superintelligent agent wants it to stop. Both conditions that uphold the Matthew effect and the circumstances that cause it to fail are studied in the literature [110, 13, 9] and those additional controls will likely be needed in addition to what we propose in \u00a7 4.\nThe argument presented in this section has similar motivations to those presented in [34]. We expect to see a lot more progress in this research topic in the coming months and years."}, {"title": "Cap-and-Trade to Control Pollution", "content": "Consider a set of oil refineries {R1, R2, ..., Rk}, where k > 1. Each refinery Ri:\nproduces unleaded fuel that can be distributed and sold in the retail market for $2 per litre. (We assume the output of the refineries is not big enough to change the market demand, and therefore the price of fuel.);\nrequires $1.80 in input cost (crude oil, chemicals, electricity, labour, distribution) to produce and distribute 1 litre of fuel (for details, see [46]);\ncan produce up to 100 million litres of fuel per day; and\nproduces greenhouse gases, costed at $190 per cubic ton. (See [2] for more detailed modelling.)\nThe refineries are not, however, equally efficient. Refinery Ri emits\n$Si(y) = mi(\\frac{y^3}{5} - 12y^2 + 200y + 888)$\ncubic tons of greenhouse gases per day, which is a function of y the amount of fuel (in millions of litres) it produces per day and mi, an inefficiency factor."}, {"title": "Automated Penetration Testing", "content": "There is growing interest among cyber security researchers in investigating the use of reinforcement learning (RL) algorithms, both model-based and model-free, to perform automated penetration testing of networks and cyber-physical systems [52, 114, 63, 113, 123, 140]. The key technical challenge in these problems is usually in controlling the large and discrete observation and action spaces. The former is typically addressed using vulnerability scanners like nmap in conjunction with knowledge bases like the Common Vulnerabilities and Exposures (CVE) database, and the latter is typically tackled by building the RL algorithms on top of mature penetration testing toolsuites like Metasploit [76] and logic-based planning methods like MulVAL for attack-tree generation [102].\nIn this section, we look at how we can formulate the automated penetration testing problem, in the case where there are multiple RL agents cooperating with the goal of finding, within a given time, as many vulnerabilities as possible on a system being tested, under the constraint that the agents' actions must not be too disruptive to each other and to the system being tested.\nFor each of the agents, the observation space is whatever can be returned from their scanning software. We will assume here that it is a JSON object, possibly with embedded graphics in standard formats, that can be formally represented and reasoned with in higher-order logic [87, 88], which provides us with tools for\ndefining and systematically enumerating possible feature functions, and\nperforming logical and probabilistic inference, including Bayesian filtering [25], with respect to a knowledge base (like CVE).\nThe action space is made up of two types of (parametrisable) action templates:\nrun a scanning software with certain parameter values;\nrun an existing exploit script in the Metasploit library with certain parameter values.\nInstantiating the parameters in the action templates can be done through search algorithms, possibly within a formal logic setup if we can define commonly applicable rules and use unification techniques to bind the free variables in a proof procedure [102, 54].\nHow about the reward function? We assume each action a has a computational cost C(a), which quantifies the amount of compute resources possibly a weighted combination of CPU, memory and disk usage required to execute the action on the system being tested. In practice, the agents have an estimate \u0108(a) obtained from historial data. This is motivated by the large literature on cost models for database queries and software tests [71, 136, 116]. Unsuccessful exploits are given reward 0, as long as they do not cause severe damage to the system being tested. Successful exploits should be given positive rewards, but obviously the severity of the system compromise should be taken into account here, with compromises like root shell access and remote code execution given higher rewards than simpler compromises like denial of service. For practical reasons, we will not be considering the full range of cyber harms as described in [3], but will instead look at a few proxy measures of the impact of the penetration-testing agent's actions on the system being tested. There are a few mature scoring systems related to security standards, including\nthe Common Vulnerability Scoring System (CVSS) used in the Common Vulnerabilities and Exposures (CVE) program. A CVSS score for a vulnerability is computed in the range 0.0 - 10.0, using three groups of metrics: base, temporal, and environmental. Base metrics reflect a vulnerability's exploitability, scope, and potential impacts of exploitation. Temporal metrics reflect the current state of exploit techniques or code availability, and the existence of any patches or workarounds. Environmental metrics enable the CVSS score to be customised.\nThe Common Weakness Scoring System (CWSS) is used in the Common Weakness Enumeration (CWE) program. A CWSS score for a software weakness is computed in the range 0.0 - 100.0. In CWSS, three groups of metrics are defined: base finding, attack surface, and environmental. Base finding metrics are intended to capture the inherent risk of the weakness, confidence in the accuracy of the finding, and strength of controls. Attack Surface metrics assess the barriers that an attacker must overcome in order to exploit the weakness. Environmental metrics reflect characteristics of the weakness that are specific to a particular environment or operational context.\nA suitable reward for a successful exploit a can thus be defined as a (weighted) average of the CVSS and CWSS scores associated with the exploit. In addition, given the goal is to find all vulnerabilities (to the extent possible), it would also make sense to introduce exploration bonus rewards [120, 11, 101, 134].\nThe following is the coordination protocol for the pen-testing agents, motivated by the pollution permits idea from \u00a7 6.2. At every time step t, a certain number Ut of compute permits is auctioned. (The compute permits can only be used during the time step and cannot be carried forward to future time steps.) Each pen-testing agent i has a current valuation function Vi,t and submits the bid\n$max Vi,t(a) subject to \u0108(a) \u2264 Ut$\nto the VCG mechanism. The winning agent i* then executes its action a* and receive a percept that can be used to update its valuation function. The actual compute resources consumed C(a*) is used to update \u0108(\u00b7). If Ut \u2013 C(a*) > 0, then another round of VCG auction is run to pick the next agent to perform its action, and this is repeated until Ut is exhausted.\nExperimental results for an implementation of a VCG-coordinated system of automated penetration testing agents based on Metasploit will be reported in a separate paper."}, {"title": "Discussion and Conclusion", "content": "In the spirit of [104], we considered in this paper the problem of social harms that can result from the interactions between a set of (powerful) general reinforcement learning agents in arbitrary unknown environments and proposed the use of (dynamic) VCG mechanisms to coordinate and control their collective behaviour. Our proposed setup is more general than existing formulations of multi-agent reinforcement learning with mechanism design in two ways:\nthe underlying environment is a history-based general reinforcement learning environment like in AIXI [69];\nthe reinforcement-learning agents participating in the environment can have different time horizons and adopt different strategies and algorithms, and learning can happen both at the mechanism level as well as the level of individual agents.\nThe generality of the setup opens up a wide range of algorithms and applications, including multi-agent problems with both cooperative and competitive dynamics, some of which we explore in \u00a7 5 and \u00a7 6. The setup closest to ours in generality in the literature is [70], with interesting applications like [143].\nA key limitation of our proposed approach, especially when it comes to regulating powerful AGI agents, is that there is no fundamental way to enforce the VCG mechanism on such agents outside of purposedly designed platform economies [77, 33, 41]. In particular, the proposed approach would not work on AGI agents operating \"in the wild\". Nevertheless, the study of the ideal market mechanism to regulate AGI agents is a useful step for understanding and benchmarking the design of more practical mechanisms like [75] that recommend but do not enforce social choices, and more indirect payment approaches like [141] and [80] to steer a set of agents to desired good behaviour. It would also be interesting, as future work, to understand how natural biological mechanisms like [24, 109] relate to ideal market mechanisms."}, {"title": "Notes on the Agent Valuation Function", "content": "We will start by unrolling (14)-(15) for m = 3 to see the general pattern. Given the empty history ho = 6", "U2,i(a\u2081(\u20ac)or1)": "Pi(\u20ac)$\n$=  \u03a3(or1|a1(c)) [r1", "pi(or1)": "Pi(\u20ac)$\n$=  \u03a3(or1|a1(c))[-pi(or1) \u2013 pi(e) + r1", "a\u00bd(or1))": "n$= \u03a3(or1|a1(\u20ac)) -Pi(or1) - Pi(e) + r1", "or1)or2)": "n$= \u03a3(or1a1(\u20ac)) \u03a3(or2|a1 (6) or 1a (or1))$\n[-pi(or\u2081) - Pi(e) + r1", "or\u2081a\u00bd(or1)or2)": "n$=  \u03a3 (or1:2 a1 (\u20ac)", "a1(6)or1a2(or1)or2)": "n$=  \u03a3 (or1:2 a1 (6)", "or\u2081))[-pi(or1": 2, "or1": 2, "\u03a3(or1": 28}, {"or1": 2}, {"or1": 2, "\u03a3(or1": 21, "pi(or1": 2, "Pi(\u20ac)": "n$=  \u03a3 (or1:31 (6)"}, {"or1": 2, "13,i": "n\u03a3(or1:2 a1 (\u20ac)", "pi(or1": 2, "pi(\u20ac)": "nwhere we denote vt(ht\u22121) = (vt,1 (ht\u22121, \u00b7), . . ., Ut,k(ht\u2212"}]}