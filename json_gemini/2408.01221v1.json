{"title": "Rubric-based Learner Modelling via Noisy Gates Bayesian Networks for Computational Thinking Skills Assessment", "authors": ["Giorgia Adorni", "Francesca Mangili", "Alberto Piatti", "Claudio Bonesana", "Alessandro Antonucci"], "abstract": "In modern and personalised education, there is a growing interest in developing learners' competencies and accurately assessing them. In a previous work, we proposed a procedure for deriving a learner model for automatic skill assessment from a task-specific competence rubric, thus simplifying the implementation of automated assessment tools. The previous approach, however, suffered two main limitations: (i) the ordering between competencies defined by the assessment rubric was only indirectly modelled; (ii) supplementary skills, not under assessment but necessary for accomplishing the task, were not included in the model. In this work, we address issue (i) by introducing dummy observed nodes, strictly enforcing the skills ordering without changing the network's structure. In contrast, for point (ii), we design a network with two layers of gates, one performing disjunctive operations by noisy-OR gates and the other conjunctive operations through logical ANDs. Such changes improve the model outcomes' coherence and the modelling tool's flexibility without compromising the model's compact parametrisation, interpretability and simple experts' elicitation. We used this approach to develop a learner model for Computational Thinking (CT) skills assessment. The CT-cube skills assessment framework and the Cross Array Task (CAT) are used to exemplify it and demonstrate its feasibility.", "sections": [{"title": "I. INTRODUCTION", "content": "INTELLIGENT Tutoring Systems (ITSs) are technological devices that support learning without the mediation of a teacher. They interact directly with the user, providing hints and suggestions that can only be effective if calibrated to the actual user's competence level. ITSs collect data on a learner's performance while accomplishing a task and use that data to develop a competence profile based on a predefined model of the learner's knowledge and behaviour. This profile helps determine the most appropriate intervention. The new knowledge collected along with the learning activity continuously updates the competence profile, making the interventions more focused. Therefore, the learner model is one of the main factors that contribute to the success of an AI-based educational tool.\nA learner model describes mathematically the learner's competencies, represented by a set of hidden variables, and their relations with the observable actions performed while solving the task. Such competencies combine knowledge, skills, and attitudes expressed in a specific context. Teachers can evaluate student competencies in realistic scenarios explicitly designed for this purpose and then compare their performance with a model of competence specified through an assessment rubric [1]. A rubric for assessing a student's performance consists of a list of competence components to be evaluated, a qualitative description of possible observable behaviours corresponding to different levels of such components, and a set of criteria for assessing the level of each component. A rubric, therefore, describes the relationship between competencies and observable behaviours of the learner that need to be formally codified.\nSeveral sources of uncertainty and variability may affect the relationship between the non-observable competencies and the corresponding observable actions. Therefore, a deterministic relationship is not capable of accurately modelling it. Instead, a more appropriate approach would be to use probabilistic reasoning to translate qualitative assessment rubrics into a quantitative, standardised, coherent measure of student proficiency. In the literature, Bayesian Knowledge Tracing (BKT) [2], Item Response Theory (IRT) [3], and Bayesian Networks (BNs) [4] are all popular probabilistic approaches to learner knowledge modelling. BNs are a robust framework for modelling dependencies between skills and students' behaviours in complex tasks. In addition, the graphical nature of the models makes them easily understandable by domain experts. Therefore, experts can easily use them in eliciting the student model [5]. Desmarais [6] reviewed all the most successful ITS experiences since Bloom's keynote paper and recognised and presented BNs as the most general approach to modelling learner skills. [7] conducted a systematic review of 53 papers about ITS applications from 2007 to 2017. The review explored the characteristics, applications, and evaluation methods of ITSs and found that a significant proportion of the reviewed papers used BN techniques. More recent works using BNs to model the learner knowledge in the context of ITSs include [8], who developed an ITS to help students acquire problem-solving skills in computer programming, and [9], who developed an automatic assessment method for students' engineering design performance using a BN model. Other recent works, such as [10], [11], support the construction ITSs based on BNs and further highlight the ever-present interest in BN techniques for ITSs. Building on these results, we focused our method on BN-based learner modelling.\nNot all BNs are easy to design, and a deep understanding of BN theory is required. Although BN arcs can be interpreted as a causal model, their definition by experts is not always trivial because of the complexity of the causal relationships involved and the presence of hidden causes. In addition, a significant effort may be necessary to obtain the network structure and parameters through expert knowledge or the availability of an extensive dataset to learn them directly.\nDecomposing domain knowledge into individual basic component skills is the choice typically adopted to simplify the model elicitation process. However, as pointed out in [12], \"complex skill mastery requires not only the acquisition of individual basic component skills but also practice in integrating such component skills with one another\". Further complications can also arise even when the structure of the learning model can be precisely defined. In some cases, eliciting and learning BN parameters can quickly make the computation of inferences unmanageable. The number of parameters and the problem complexity can rapidly increase with the network's number of arcs.\nThese issues can discourage ITS practitioners from using BN-based learner modelling in their applications when many skills are involved in the learner's actions. A solution to reduce the number of parameters in a BN-based learner model was proposed in our previous paper [13]. We exploit noisy-OR gates [14] to reduce the number of parameters to elicit from exponential in the number of parent skills for each observable action to linear. Similar advantages also concern the inference. In [15], we adopted a solution to set up a general approach for translating assessment rubrics into interpretable BN-based learner models with a complexity compatible with real-time assessment.\nLearner models based on assessment rubrics are more accessible to teachers, who are typically more familiar with them than probabilistic graphical models. Moreover, rubrics focus on the learners' complex behaviours in specific contexts. Therefore, although we do not explicitly model skill interactions, they can be captured in the hierarchy of complex behaviours identified by the rubric. However, in our previous work [15], this hierarchy of competencies was only indirectly modelled. This produced assessments assigning larger probabilities to higher-level competence, in contrast with the assumption that when a competence of a certain level is possessed, all lower-level competencies are also owned.\nIn the model presented here, this counter-intuitive behaviour has been eliminated by imposing the constraints codified by the rubric through observed auxiliary nodes that do not modify the network structure and retain the previous model's relative simplicity. Moreover, considering that assessment rubrics are usually limited to the competence components under assessment, hereafter also called target skills, the modelling approach in [15] may have led to oversimplified learner models, unable to grasp the actual causes of a failure which is not always due to the absence of the target skills. It may, for instance, follow from deficiencies in other skills necessary for the specific task, hereafter referred to as supplementary skills. Therefore, this work extends our previous model [15] by adding the possibility of modelling a set of supplementary skills necessary, in conjunction with the skills under examination, to succeed in the assigned tasks. To this goal, it is essential to extend the nosy-OR approach in [13] to combine disjunctive and conjunctive relations between skills since. At the same time, the behaviours in the assessment rubric are mutually exclusive (OR), and supplementary and target skills must be expressed jointly (AND).\nTo illustrate this approach, we focus on the activity proposed in [16] for the standardised assessment of algorithmic skills along the entire K-12 school path. We compare four learner models based on different assumptions and sets of expert-elicited parameters and apply them to the dataset collected in [16]. Overall, we obtain a general and compact approach to implementing a learner model given a set of competencies of interest and the corresponding assessment rubric. The resulting model has a simple structure and interpretable parameters, allowing for fast inferences with a reasonable effort for their elicitation by experts.\nThis article is organised as follows:\n\u2022 Section II-A provides some background about learner modelling based on Bayesian Networks, and noisy gates.\n\u2022 Section II-B introduces task-specific assessment rubrics.\n\u2022 Section II-C illustrates how to model a generic assessment rubric by Bayesian Networks.\n\u2022 Section III presents the case study and, in particular, the specific assessment rubric developed for the Cross Array Task (CAT) from [16] as well as the procedure for translating it into a learner model.\n\u2022 Section IV discusses the model outcomes based on the dataset of 109 pupils collected in [16].\n\u2022 Section V summarises the work's findings and contribution."}, {"title": "II. METHOD", "content": "The structure of a Bayesian Network (BN) over a set of variables is described by a directed acyclic graph G whose nodes are in one-to-one correspondence with the variables in the set. We call parents of a variable X, according to G, all the variables connected directly with X with an arc pointing to it. Learner models usually include a set of n latent (i.e., hidden) variables $X := (X_1, ..., X_n)$, henceforward referred to as skill nodes, describing the competence profile of the learner and some m manifest variables $Y := (Y_1,..., Y_m)$, hereafter called answer nodes, describing the observable actions implemented by the learner to answer each specific task. Arcs go from skill nodes to answer nodes modelling how the presence or absence of a specific competence directly affects the learner's behaviour in a task requiring such competence. Here, we consider only binary skill nodes, where value one, or true state, indicates that the pupil possesses the skill, and binary answers nodes, denoting a correct answer or behaviour shown in solving the task.\nThe relations of a BN-based learner model can be graphically depicted as in the example of Fig. 1. The answer nodes describe whether the learner has been able or not to program, for example, a maze game (Y\u2081) or a statistical model (Y\u2082). The skill nodes represent the ability to build this program using a block-based programming language such as Scratch (X\u2081) or a text-based programming language such as Python (X\u2082). The second skill can be applied to answer both questions, and therefore X\u2082 is a parent node for both answer nodes Y\u2081 and Y\u2082. Instead, the first skill can be used to answer just the first question, and therefore there is no direct arc from X\u2081 to Y\u2082.\nOnce the graph G structuring the BN is established, the definition of the BN over the n + m variables of the network $V := (V_1, V_2, ..., V_{n+m})$, including both skills (X) and answers (Y), consists in a collection of Conditional Probability Tables (CPTs) giving the probabilities P(Y\u2081 = 1|Pa(Y;)) that Y takes value one given all possible joint states of its parent nodes Pa(Y). Let V take values in \u03a9v, the independence relations imposed from G by the Markov condition, i.e., the fact that each node is assumed to be independent of its non-descendants non-parents given its parents, induce a joint probability mass function over the BN variables that factorises as follows [4]:\n$P(V = v) = \\prod_\\nu \\epsilon \\Omega_v P(v | pa(V)),$ (1)\nwhere $v = (v_1, v_2, ..., v_{n+m})$ represents a given joint state of the variables in V. BN inference consists of the computation of queries based on Eq. (1). In particular, we are interested in updating tasks consisting in the computation of the marginal posterior probability mass function for a single skill node Xq \u2208 X given the observed state ye of the answer nodes YE CY:\n$P(X_q|Y_E) = \\frac{\\sum_{v \\epsilon \\Omega_V \\backslash (X_q \\bigcup Y_E)} \\prod_{v \\epsilon V} P(v | pa(V))}{\\sum_{v \\epsilon \\Omega_{Y_E}} \\sum_{v \\epsilon \\Omega_V \\backslash (X_q \\bigcup Y_E)} \\prod_{v \\epsilon V} P(v | pa(V))},$ (2)\nwhere $\\Omega_{VV'} := {v : v_i = v'_i \\forall v_i \\epsilon V'}$.\nAccording to the above model, multiple parent skills may be relevant to the same answer. The paper in [13] discusses how this can lead to a critical complexity in elicitation and inference and demonstrates how using noisy gates can avoid these issues. In this work, we exploit the disjunctive noisy-OR gates, which shape interchangeable skills and are suitable for modelling assessment rubrics.\nThis section briefly introduces noisy-OR gates and their use in learner modelling. We refer to [13] and [15] for a more detailed discussion. A typical representation of the noisy-OR network structure, introducing n auxiliary variables (also called inhibitor nodes), is shown in Fig. 2."}, {"title": "A. BN-based Learner Models", "content": "To reduce the number of parameters, the noisy-OR defines the state of Y; as the logical disjunction (OR) of the auxiliary parent nodes, removing the need to specify the CPT of the answer node given the state of its parent nodes. Furthermore, the noisy-OR structure sets the input variable Xi as the unique parent of $X_i,j$ and constraints X to be zero with probability one when X\u2081 = 0. The relationship between skill and answers would be purely logical-deterministic were it not for the noise introduced by the so-called inhibition parameters $A_{i,j} = P(X_{i,j} = 0|X_i = 1)$, representing the probability of not expressing skill i in task j. Auxiliary variables can be interpreted as inhibitors of the corresponding skills, as they can be in the false state, e.g., $X_{i,j} = 0$ (with probability $A_{i,j}$) even when the corresponding skill node X\u2081 is in the true state, indicating that, although possessed by the learner, the skill could not be expressed in task Yj. By defining the probability of a failure in expressing a possessed skill in the specific task j, the inhibition parameter $A_{i,j}$ provides a measure of the task difficulty. If a pair skill-answer has a large inhibition, the state of the answer node tells, in general, little about the state of the skill node, the extreme case of $A_{i,j} = 1$ corresponding to a missing arc in the BN graph between skill i and answer j.\nThe noisy-OR network induces the following CPT between the n parent skill nodes X = (X\u2081, ..., Xn) and the observable answer node Y; [14]:\n$P(Y_j = 0|X = (x_1,...,x_n)) = \\prod_{i=1}^n (I_{x_i=0} + \\lambda_{i}I_{x_i=1}),$ (3)\nwhere I A is the indicator function returning one if A is true and zero otherwise. The second term $A_i x_i = 1$ represents the noise as it introduces the possibility that a skill X\u2081 that the student possesses is not expressed in task Y; (this phenomenon is also called slip elsewhere in this work). The value of A\u2081 implying the biggest uncertainty associated with the task-skill pair (Yj, Xi) is 0.5, whereas the value $A_i = 0$ models the certainty that skill Xi, whenever present, will be expressed in solving task Y\u2081 and, vice versa, $A_i = 1$ model the fact that X cannot be expressed in task Yj.\nIn [15], a leak node was added to the model to represent the possibility of a random guess, i.e., a correct answer or a behaviour given without mastering any required competencies. The leak is a Boolean variable playing the role of an additional skill node, Xleak, which is set in the observed state Xleak = 1 and added to the parents of all answer nodes for which random guessing is possible. The chances of guessing answer Yj at random are given by parameter 1 \u2212 $A_{leak,j}$.\nTo apply the above model, the domain expert (e.g., the teacher) should first list the parentless skill nodes (including, eventually, the leak) X\u2081,..., Xn, the childless answer nodes Y\u2081,..., Ym and connect by an arc the skills to all answer nodes in which they can be used. Then, the instructor should quantify for each pair of skill-answer nodes, Xi and Yj, connected by an arc, the value of the inhibition Ai,j. This results in a total of at most nm parameters to be elicited. Finally, the expert should state each skill's prior probabilities \u03c0i.\nWhile the BKT, in its standard implementation, traces the evolution of a single skill over time, our approach focuses on fine-grained skills modelling at the specific moment the assessment is performed. However, a parallel can be drawn between the two. BKT models student knowledge at time t as the (binary) latent variable X(t) of a hidden Markov model [2]. Learning is modelled as the transition of X(t) from state zero (lack of knowledge) to state one (knowledge acquired). The model defines four parameters: (i) the initial probability, i.e., the probability that the knowledge has been already acquired at the beginning of the activity; (ii) the learning probability, that is, the probability of acquiring the probability between t and t + 1; (iii) the slip probability of making a mistake when the knowledge is acquired; (iv) the guess probability of doing right in the lack of knowledge.\nIn our model, the probability of the slip may vary depending on the pair skill i and task j and is represented by the inhibition Aij. The guess probability depends on the task and is equal to 1 \u2212 Aleak, j. The initial probability of a skill X\u00bf is defined by its prior probability \u03c0\u00bf. Notice, however, that since our approach, differently from BKT, does not model the learning process, the concept of initial probability here is meant to describe our initial knowledge of the learner competence profile rather than the probability that the skill is initially acquired. For the same reason, no learning probability is defined in our model."}, {"title": "B. Assessment Rubrics", "content": "There are several possible approaches to identifying the knowledge components to be included in a learner model. In this article, we follow the one introduced in [15] and start from a rubric defined for assessing a given competence through a specific task or family of similar tasks [17], [18].\nA task-specific assessment rubric consists of a two-entry table where each row corresponds to a component of the given competence, described in the light of the given task. In contrast, each column corresponds to a competence level in ascending order of proficiency. For each combination of component and level, the rubric provides a qualitative description of the behaviour expected from a person with the given level in the given component. Identifying a person's competence level consists in matching the learner's behaviours while solving a given task with those described in the assessment rubric.\nFor instance, Table I shows the task-specific assessment rubric for an example focused on assessing the student's ability to use iterative instructions in algorithms. This competence has two levels depending on the tools used by the learner: a visual programming language (X1) or a textual programming language (X2). By checking how the learner produced the algorithm, the teacher can see whether he applied any of the methods in the rubrics and assign him the corresponding competence level.\nGenerally speaking, assessment rubrics define an ordering between competence levels and sometimes between competence components, as for the case study about computational thinking introduced in [15] and discussed in Section III. Here, a competence level or component is considered higher than another if the former implies the latter, meaning that a learner with the higher competence can also perform all the tasks that require the lower. In practice, the competence level matching the student's behaviours for a given component does not always correspond to the actual learner's state of knowledge. It is also possible that the person possesses a higher level but is underperforming.\nIn the case of a task composed of similar sub-tasks, i.e., tasks sharing the same assessment rubric, it is, therefore, possible to observe behaviours matching different competence levels in the various sub-tasks. In the following subsection, we illustrate how this uncertainty can be considered and how an overall assessment based on a full battery of tasks can be produced by modelling the learner competence profile with the BN-based approach described in Section II-A."}, {"title": "C. Modelling Assessment Rubrics by Bayesian Networks", "content": "Considering a task-specific assessment rubric, as defined in Section II-B, it is possible to derive a learner model, as presented in Section II-A, hereafter referred to as baseline model. For each cell (c,r) of an assessment rubric with R rows and C columns, we introduce a latent binary competence variable Xrc taking value one for a learner mastering the corresponding competence level and zero otherwise. Moreover, for each task t, in a battery of T similar tasks, and each competence variable Xrc, we define an observable (manifest) binary variable Yrc t taking value one if the behaviour described in the assessment rubric's cell (r, c) was applied successfully by the learner in solving task t and zero if he failed using it.\nIn this work, we improve the baseline model in two ways. Firstly, we explicitly impose the ordering of competence levels encoded by the rubric. Secondly, we include in the model task-specific supplementary skills which can be combined with each other and with the competencies of the rubric through arbitrary logic functions."}, {"title": "1) Ordering of competences", "content": "In the baseline model of Section II-A, it was indirectly accounted for the partial ordering between variables by setting as parents of answer node Yrc t the skill node Xrc and all skill nodes corresponding to higher competence levels. The network was quantified through noisy-OR relations, as described in Section II-A1. This structure assumes that an observed behaviour can be explained as the student mastering the corresponding competence level or a higher one if he is underperforming, thus not exploiting his full potential, but cannot be achieved through a lower level.\nAs mentioned above, we interpret the (partial) ordering between competencies defined by the assessment rubric as implication constraints, meaning that possessing a particular skill Xi implies that the learner posses also his inferior competencies. While exploited to design the network structure, this hierarchy of competencies is not strictly imposed by the above baseline model, giving rise to posterior inferences that are usually inconsistent.\nTo solve this issue, we enrich the model by adding an auxiliary variable Dik for each relation Xi \u21d2 Xk defined by the rubric. A constraint node Dik is always in the observed state one and has Xi and Xk as parent nodes. The desired implication constraint is then implemented by choosing a CPT for Dik such that P(Dik = 1|X\u2081 = 1, Xk = 0) = 0. The addition to the network of each observed node Dik changes the prior probabilities of Xi and Xk, initially set to \u03c0\u2081 and \u03c0\u03ba. Let\n$P_{00} = P(D_{ik} = 1|X_i = 0, X_k = 0)$\n$P_{01} = P(D_{ik} = 1|X_i = 0, X_k = 1)$\n$P_{11} = P(D_{ik} = 1|X_i = 1, X_k = 1),$ (4)\nbe the non-null parameters in the CPT of Dik. After updating with the evidence Dik = 1, one has\n$P(X_i = 1|D_{ik} = 1) = \\frac{\\pi_i \\pi_j p^*}{\\kappa},$\n$P(X_k = 1|D_{ik} = 1) = \\frac{\\pi_j \\pi_k p^*}{\\kappa},$ (5)\nwith $K = P_{11} \u03c0j\u03c0\u03ba + P_{01}(1 \u2212 \u03c0j)\u03c0\u03ba + Poo(1 \u2212 \u03c0j)(1 \u2212 \u03c0\u03ba). In this work, we simply assume poo = P01 = P11 and adopt uniform prior probabilities \u03c0\u2081 = \u03c0\u03ba = 0.5. Applying them to Eq.(5) give P(Xk = 1) = 1/3 and P(Xk = 1) = 2/3. This result follows from the fact that skill X\u2081 can only be possessed jointly with Xk, whereas Xk can also be owned when X\u2081 = 0. Under the assumption Poo = P01 = P11 = p*, the prior over the superior skill X\u2081 can be interpreted as the conditional probability of having it given that the learner possesses the inferior skill Xe since\n$P(X_i = 1|X_k = 1, D_{ik} = 1) = \\frac{\\pi_i \\pi_j p^*}{\\pi_i \\pi_j p^* + (1-\\pi_i) \\pi_j p^*} = \\pi_i$ (6)"}, {"title": "2) Supplementary skills", "content": "While the assessment rubric details the components of the competence of interest and their interactions with the specific task and available tools, it does not necessarily include all the skills required to solve the task successfully.\nFor instance, considering the assessment rubric proposed in Table I, to develop an iterative algorithm with a text-based programming language successfully, the learner might also need knowledge about the different types of statements, e.g., while, repeat, for, do until and so on. Ignoring such supplementary skills might be misleading in an automatic assessment system, as failures due to the lack of one of them would not be recognised as such and, eventually, be attributed to the absence of the competence components under assessment. Therefore, if not adequately modelled, the lack of unmodelled supplementary skills would translate into an unfairly negative evaluation of the competencies of interest.\nTo produce fairer assessments, we extend the model by an additional layer of auxiliary nodes combined with a logic function to allow for the inclusion of a suitable set of supplementary skills.\nFig. 3 shows an example of the structure of the extended network. Supplementary skills are described by additional skill nodes S1,..., Sm, which are grouped into sets of interchangeable skills (in the case of the example we have just one set). Each of these groups is connected through a noisy-OR to a node in the layer of auxiliary latent nodes, hereafter referred to as group nodes G1, ..., G\u0131, representing the success or failure in applying the type of competence described by each group to the specific task Y. Finally, the group nodes are connected to the answer node through a logic AND or any other logic function suitable for the particular task.\nWhen supplementary skills can be directly assessed through observing specific learner behaviours or by purposed questions, additional answer nodes, children only of the relevant supplementary skills, can be added to the network."}, {"title": "III. A CASE STUDY ON K-12 COMPUTATIONAL THINKING SKILLS", "content": "In this section, we use the case study about Computational Thinking (CT) skill assessment already introduced in [15] to illustrate the proposed approach. However, our methodology can be applied analogously to each task for which an assessment rubric can be defined.\nCT assessment is an important field of research [19] due to its relevance in evaluating the effectiveness of CT teaching and learning activities on an individual or class level and, on a larger scale, the impact of curricular and educational system policies on the development of CT skills of a given population."}, {"title": "A. The Cross Array Task", "content": "The Cross Array Task (CAT), proposed in [16], is an unplugged activity designed to assess the development of algorithmic skills, i.e., the ability to describe a complex procedure through a set of simpler instructions, in pupils aged from 3 to 16 years.\nThe authors of [16] carried out an experimental study from March to April 2021, collecting data from 109 students (51 girls and 58 boys) in eight classes from three public schools in Southern Switzerland."}, {"title": "B. Modelling the CAT assessment rubric", "content": "The assessment rubric for this case study, shown in Table II, is the same as presented in our previous work [15].\nThe instruction sequences built by the pupils, called algorithms, are ranked into three categories corresponding to the assessment rubric's competence components (rows). Each row represents the ability of the pupil to solve a CAT schema using, respectively, zero-dimensional (OD) algorithms \u2013 where the dots of the schema are described point by point -, one-dimensional (1D) algorithms where structures, such as rows, diagonal, squares etc. are also used to illustrate the coloured pattern \u2013, and two-dimensional (2D) algorithms where repetitions and loops on dots or structures are also used to describe the schema.\nThe degree of autonomy of the pupil and the tools used to accomplish the task have been hierarchically ordered and determine the competence levels in the columns of the rubric. Specifically, from the highest (right) to lowest (left), such levels correspond to the ability of the pupil to solve a CAT schema using: voice (V) \u2013 the pupil can communicate all the necessary instructions using only the voice \u2013, voice & schema (VS) \u2013 the pupil, in combination with the voice, uses an empty cross array schema to illustrate his instructions through gestures, e.g., by pointing with fingers the dots to be coloured \u2013, voice, schema & feedback (VSF) \u2013 the student also asks to remove the screen to have visual feedback of how the teacher is colouring the schema.\nAs introduced in Section II-C, the columns of an assessment rubric provide the competence levels in increasing order from left to right. Sometimes, as in this case study, this is true also for the rows, where competence components are ordered from the lower (OD at the top) to the highest (2D at the bottom). This follows from the assumption that mastering algorithms of higher complexity imply also mastering simpler ones. The same is valid for communication tools.\nSumming up, we can conclude that a competence level Xrc is higher than Xrd whenever c > c' and r > r', or c = c' and r > r'. When, instead, c > c' but r < r', neither skill can be said to dominate the other.\nFrom the CAT assessment rubric in Table II we define nine target skills to be assessed, representing the ability to develop an algorithm using elementary operations communicated either by a symbolic language (X13 \u2013 0D V), an embodied language (X12 \u2013 0D-VS) or supported by visual feedback (X11 \u2013 0D-VSF); elementary operations and structures communicated either by a symbolic language (X23 \u2013 1D-V), an embodied language (X23 - 1D-V) or supported by visual feedback (X21 1D-VSF); elementary operations, structures and repetitions communicated either by a symbolic language (X33 \u2013 2D-V), an embodied language (X32 - 2D-VS), or supported by visual feedback (X31 \u2013 2D-VSF).\nAccordingly, with the method described in Section II-C, a latent skill node Xre is included in the BN learner model for each of the nine target skills of the rubric. The hierarchy of competencies is then modelled by nine latent binary variables Drc,r'c', as described in Section II-B, encoding the implication Xrc Xrd for each pair of consecutive skills in the hierarchy, i.e., such that (r = r' + 1) ^ (c = c') or (r = r') ^ (c = c + 1).\nAlso, the BN network includes an observable answer node Yt for each skill in the rubric and each task t = 1, . . ., 12 in the sequence of 12 similar tasks administered during the CAT experiments. Observing Yrte = 1 means that the pupil has solved the t-th CAT schema using an algorithm of complexity corresponding to the c-th row of the rubric, and requesting the help in the r-th column. By way of example, a student solving the t-th schema using a 0-dimensional algorithm and with voice, empty schema and feedback (0D-VSF) result in the observed node Y\u2081\u2081 = 1.\nIn principle, all answer nodes should be explicitly observed through specific interactions with the pupil. However, this was not possible for the case of the dataset collected by [16] since the pupils were left free to choose their preferred approach. Therefore, to make such a dataset compatible with our model, in [15] we encode the collected answers as follows: a task t solved at level c* by an algorithm with complexity r* was translated into Y = 1 for all competence levels re lower than or equal to r*c*, thus assuming that, if requested, the pupil would have been able to implement solutions requiring a lower competence level than the one used. Similarly, we set all answer nodes Yt = 0 for all higher levels, leaving those not directly comparable unobserved.\nThe choice made in our previous work [15] also contributed to stress the skills ordering. Since the ordering is modelled by explicit constraints imposed through the auxiliary variables Drc,r'c', such a choice would be unnecessary and detrimental, as it would artificially multiply the number of observations.\nTherefore, in the constrained model, a task t solved at level c* by an algorithm with complexity r* would be better translated into the single observation Yrt* c* = 1. However, in the specific experimental setting adopted in [16], since pupils were always allowed to try solving the task with the lowest competence level (0D-VSF), a failure could only be observed for that level, with the consequence that only answer nodes Y\u2081\u2081 can be directly observed in the false state Y\u2081\u2081 = 0. To work around this problem, we set the answer nodes just above the one observed in the true state, i.e., Yrt* (c*+1) and Y*+1)* to the false state, leaving all other nodes unobserved. As an example, Table III (ii) shows how the answer encoding changes in the case of a 1D-VS solution to task t for the constrained model.\nFinally, having observed that, depending on the specific CAT schemes, other skills than those in the assessment rubric may be necessary, especially for the algorithm complexities 1D and 2D, by analysing the CAT schemes structures and characteristics, we identify ten supplementary skills, divided into three groups, to be added to the skill nodes in the network: (S1) paint dot \u2013 group 1; (S2) fill empty dots \u2013 group 2; (S3) paint monochromatic rows or columns \u2013 group 2; (S4) paint monochromatic squares \u2013 group 2; (S5) paint monochromatic diagonals \u2013 group 2; (S6) paint monochromatic 1-shaped patterns - group 2; (S7) paint monochromatic zigzags - group 2; (S8) paint polychromatic rows or columns \u2013 group 3; (S9) paint polychromatic diagonals or zigzags \u2013 group 3; (S10) repetition of a pattern \u2013 group 3.\nFrom the annotations collected during the experimental study in [16] it was possible to extract direct observations about using each supplementary skill in each task. Consequently, answer nodes Ys were added to the network for each task t = 1,...,12 and each supplementary skill Si with i = 1,...,10. Each schema can be solved using one or more supplementary skills, but using all of them is not always possible. Answer nodes Y take the value one if the pupil has used the i-th supplementary skill in the solution of CAT schema t, and zero otherwise.\nAs described in Section II-C, a noisy-OR combines the variables in the same group into the group auxiliary nodes Gi, with i = 1,...,4, where G4 combines the target skills Xrc. In contrast, the relation between the group nodes and the target skills is conveyed through the logical AND."}, {"title": "C. Summary score metric", "content": "To evaluate pupils' competence level in a specific CAT schema, we use the CAT score, a metric provided by [16], taking values from 0 to 4, as shown in Table IV. Unsolved CAT schemes, which were not considered in [16], are here assigned a score of -1.\nWhen using the BN-based learner model, a summary metric of the inferences at the end of the 12 tasks, hereafter referred to as the BN-based CAT score, was obtained as the sum of the marginal posterior probabilities of all target skill nodes estimated by the model and can be interpreted as the expected number of competence levels mastered by the student."}, {}]}