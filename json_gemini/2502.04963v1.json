{"title": "FAST ADAPTIVE ANTI-JAMMING CHANNEL ACCESS VIA DEEP Q LEARNING AND COARSE-GRAINED SPECTRUM PREDICTION", "authors": ["Jianshu Zhang", "Xiaofu Wu", "Junquan Hu"], "abstract": "This paper investigates the anti-jamming channel access problem in complex and unknown jamming environments, where the jammer could dynamically adjust its strategies to target different channels. Traditional channel hopping anti-jamming approaches using fixed patterns are ineffective against such dynamic jamming attacks. Although the emerging deep reinforcement learning (DRL) based dynamic channel access approach could achieve the Nash equilibrium under fast-changing jamming attacks, it requires extensive training episodes. To address this issue, we propose a fast adaptive anti-jamming channel access approach guided by the intuition of \u201clearning faster than the jammer\", where a synchronously updated coarse-grained spectrum prediction serves as an auxiliary task for the deep Q learning (DQN) based anti-jamming model. This helps the model identify a superior Q-function compared to standard DRL while significantly reducing the number of training episodes. Numerical results indicate that the proposed approach significantly accelerates the rate of convergence in model training, reducing the required training episodes by up to 70% compared to standard DRL. Additionally, it also achieves a 10% improvement in throughput over NE strategies, owing to the effective use of coarse-grained spectrum prediction.", "sections": [{"title": "1 Introduction", "content": "Wireless communications have found extensive applications in both civilian and military scenarios, where ensuring robust anti-jamming capabilities is paramount for secure transmissions, given the vulnerability of wireless links Wang et al. [2020], Tang et al. [2019]. The dynamic characteristics of wireless channels present significant challenges for conventional model-based anti-jamming methods Xiao et al. [2018], Lv et al. [2023], Jia et al. [2019]. It has been shown that the model-free approaches, such as reinforcement learning (RL) Watkins and Dayan [1992], Mnih et al. [2015], could help to empower the legitimate user with enhanced anti-jamming capability in wireless communication environments that are dynamic and unknown Liu et al. [2018], Xiao et al. [2020]. However, jammers have also evolved intelligently with advancements in Universal Software Radio Peripheral (USRP) Zhu et al. [2016] and artificial intelligence (AI) Amuru et al. [2016] in recent years. Significant breakthroughs have recently been made in the field of smart jamming and intelligent jamming Wang et al. [2019], Feng et al. [2022], Qi et al. [2021], Bout et al. [2023], Wang et al. [2022].\nConsiderable efforts have been devoted to combating intelligent jamming attacks. In Xiao et al. [2021], the authors treated the jammer as an integral component of the environment and introduced a DRL-based algorithm for combating an RL-based jammer. Considering the non-stationarity characteristic of the environment where the jammer could also be regarded as an RL agent Pirayesh and Zeng [2022], several works have formulated the interaction between the legitimate user and the intelligent jammer within a game theory framework. Xiao Xiao et al. [2018] proposed a Two-Dimensional anti-jamming communication scheme using a hotbooting deep Q-network to enhance mobile device utility and signal quality under cooperative jamming attacks. In Zhang and Wu [2024], Zhang explored the interactions between the legitimate user with faking-slot transmission and the intelligent reactive jammer within a bi-matrix game framework and derived the equilibrium for the game using the quadratic programming method. Additionally, a Neural Fictitious Self-Play (NFSP) method was proposed in Li et al. [2022] to identify the approximate Nash equilibrium (NE) solution for the dynamic game of radar anti-jamming with imperfect information by employing Minimax Q network Zhu and Zhao [2022], and a deceptively adversarial attack approach was proposed in Li et al. [2021] to tackle the challenge presented by smart jamming.\nOn the other hand, modeling the interaction between players using multi-agent reinforcement learning and leveraging the behaviors of other agents in multi-agent systems (MAS) has gained considerable attention in recent years He et al. [2016], Lowe et al. [2017], Hong et al. [2018], Lanctot et al. [2017], Lu et al. [2022], Yu et al. [2022]. The use of deep reinforcement opponent network (DRON) He et al. [2016] could achieve superior performance over that of DQN and its variants in multi-agent environments. Moreover, the model-based opponent modeling (MBOM) Yu et al. [2022] could simulate the iterative reasoning process within the environment model and generate a range of opponent policies for achieving a more effective adaptation in a variety of tasks. To address the anti-jamming problems in wireless communications, Li Li et al. [2023a] proposed an opponent modeling based anti-intelligent jamming (OMAIJ) algorithm that analyzes the jammer's policy and targets its vulnerabilities. Yuan Yuan et al. [2024] introduced an opponent awareness-based anti-jamming algorithm that considers the jammer's learning to effectively counter intelligent jamming attacks.\nAlthough the aforementioned works achieve additional performance gains by opponent modeling, their direct application in practical anti-jamming scenarios may encounter significant challenges. This is because these methods are based on the assumption that the user has complete knowledge about the jammer's action space and even its action at each step, which is not always reasonable in real-world scenarios. Additionally, the widely used e-greedy method for exploring Ye et al. [2020], Pourranjbar et al. [2021] and the inefficient practice of testing a single action per step Li et al. [2020] may result in the slow convergence in RL-based methods. If the RL-based anti-jamming approach fails to converge before changes occur in the jammer's strategy, its effectiveness may be significantly diminished Li et al. [2023b], Zhou et al. [2023], Yang et al. [2021].\nIn this paper, a novel fast adaptive channel access approach, which combines DQN and coarse-grained spectrum prediction, is proposed for anti-jamming. The proposed approach demonstrates a faster convergence rate than both the DRL-based approach and the opponent modeling approach in model training, outperforming the Nash equilibrium in scenarios involving DRL-based jammers. The contributions of this paper are outlined below.\n\u2022 Firstly, we describe the adversarial scenario in which a fixed-mode jammer and a DRL-based jammer operate simultaneously. The interaction between the legitimate user and the DRL-based jammer could be formulated as a Markov Game, where the user and the jammer have completely opposing objectives.\n\u2022 Furthermore, we introduce a novel fast adaptive anti-jamming channel access approach, with coarse-grained spectrum prediction serving as an auxiliary task for the DQN-based anti-jamming model, to identify the dynamic best response to the jammer with time-varying strategies."}, {"title": "2 Preliminary", "content": ""}, {"title": "2.1 System Model", "content": "As shown in Fig. 1, a legitimate transmitter communicates with its receiver in the presence of a fixed-mode jammer and an intelligent jammer. The fixed-mode jammer could launch traditional jamming attacks with fixed jamming pattern, e.g., sweeping jamming Pirayesh and Zeng [2022], comb jamming Gummadi et al. [2007], partial-band jamming Pelechrinis et al. [2011] etc. Motivated by Liu et al. [2018], we consider a DRL-based intelligent jammer that could adaptively adjust its jamming channel to disturb legitimate transmissions."}, {"title": "", "content": "Assuming that the communication band $[f_L, f_u]$, with total bandwidth $B = f_u - f_L$, could be divided into $M \\in Z^+$ non-overlapping channels. The available channel set is denoted as $F = \\{f_1, f_2, ..., f_M \\}$, where the bandwidth of each channel is $b = B/M$. We consider a synchronous time-slotted system, in which the basic time slots of the legitimate user and the DRL-based intelligent jammer are perfectly aligned. The duration of each basic time slot is $\\Delta t$, which is the smallest unit of time resolution. For convenience, the term \"basic time slot\" is abbreviated as \"time slot\" in what follows. During the k-th time slot (i.e., from $(k - 1)\\Delta t$ to $k\\Delta t$), the legitimate user selects an available channel $f_k \\in F$ for transmission, while the intelligent jammer targets $N_I$ consecutive channels in $I_k = \\{f_i\\}_{i=1,2,...,N_I}$ and the fixed-mode jammer releases jamming signals on channel $f$. Similar to Wu et al. [2013], the block fading channel model is assumed in the proposed anti-jamming model. Specifically, the channel gain from the transmitter to the receiver during the k-th time slot is defined as\n$h_{u,r}^k(f_k) = (d_{u,r})^{-\\alpha d} \\zeta_{f_k}^k$,                                                              (1)\nwhere $d_{u,r}, \\alpha_d$, and $\\zeta_{f_k}^k$ denote the distance, the path-loss exponent, and the instantaneous fading coefficient between the legitimate transmitter and the receiver. Similarly, during the k-th time slot, the channel gains from the DRL-based jammer to the legitimate receiver is defined as\n$h_{i,r}^k(f_i) = (d_{i,r})^{-\\alpha d} \\epsilon_{f_i}^k$,                                                                  (2)\nand the channel gains from the fixed-mode jammer to the legitimate receiver is defined as\n$h_{s,r}^k(f) = (d_{s,r})^{-\\alpha d} \\xi_{f_s}^k$.                                                                  (3)\nThen, the power spectral density (PSD) function at the receiver during the k-th time slot could be expressed as\n$S_k(f) = h_{u,r}^k(f_k)U(f - f_u) + h_{i,r}^k(f_i)J_i(f - f_i) + \\sum_{i=1}^{N_I}h_{i,r}^k(f_i) J_i(f - f_i) + N_k(f),$                                     (4)"}, {"title": "", "content": "where $U(f)$ is the PSD function of the legitimate user's baseband signal, $J_i(f)$ is the PSD function of the DRL-based intelligent jammer's baseband signal, $J_s(f)$ is the PSD function of the fixed-mode jammer's baseband signal, and $N_k(f)$ is the PSD function of noise. The legitimate receiver is capable of sensing the entire communication band, and the spectrum vector during the time interval of $[(k - 1)\\Delta t, k\\Delta t]$ (i.e., the k-th time slot) could be represented as\n$\\vec{s_k} = [s_1^k, s_2^k, ..., s_{N_F}^k]$,                                                                                                       (5)\nwith\n$s_l^k = 10 \\log \\int_{(l-1)\\Delta f}^{l\\Delta f} S_k(f + f_L) df \\quad l = 1,2,..., N_F,$,                                                  (6)\nwhere the PSD function $S_k(f + f_L)$ could be estimated by P-Welch algorithm Welch [1967] with the time-domain signals sampled from the k-th time slot, and $\\Delta f = B/N_F$ is the resolution of spectrum analysis.\nAdditionally, we assume that each hop has a time duration of $T_h = N_h\\Delta t$. To reflect the state of each available channel in a hop, we define the coarse-grained spectrum during the n-th hop (i.e., from $(n - 1)T_h$ to $nT_h$) as\n$\\vec{c_n} = (c_1^n, c_2^n, ..., c_M^n)$.                                                                                                            (7)\nThe m-th element of the coarse-grained spectrum, i.e., $c_m^n$, represents the discrete spectrum sample value on the m-th channel during the n-th hop, and could be calculated by\n$c_m^n = \\int_{(m-1)\\Delta f'}^{m\\Delta f'} S_n(f + f_L) df, \\quad m = 1,2,..., M,$,                                                     (8)\nwhere $S_n(f + f_L)$ denotes the PSD function estimated over the samples during the n-th hop, and $\\Delta f' = B/M = b$ is the resolution of coarse-grained spectrum analysis.\nThe illustration of the spectrum vectors and the coarse-grained spectrums in different hops are shown in Fig. 2 and Fig. 3, respectively. The small rectangles in Fig. 2 represent the samples of spectrum vectors, and the rectangles in Fig. 3 represent the samples of coarse-grained spectrums. The time resolution and the frequency resolution of spectrum analysis are $\\Delta t = T_h/N_h$ and $\\Delta f = B/N_F$, respectively, and the spectrum defined in (5) could be employed"}, {"title": "", "content": "to learn the behavior of both the legitimate user and the jammers during each time slot. While the time resolution and the frequency resolution of coarse-grained spectrum analysis are $T_h$ and $\\Delta f' = B/M = b$, respectively, and the coarse-grained spectrum defined in (7) is used to express the channel state during each hop with M samples. It is noted that, when representing the spectrum within the same time duration and frequency range, the number of samples in the spectrum matrix $[\\vec{s_{(n-1)N_h+1}},...,\\vec{s_{nN_h-1}},\\vec{s_{nN_h}}]$ is $\\frac{(N_h \\times N_F)}{M}$ times greater than the coarse-grained spectrum $c_n$.\nThe Signal-to-Interference-plus-Noise Ratio (SINR) at the legitimate receiver defined in (9) is often utilized to evaluate the quality of the received signal. If the SINR exceeds the given demodulation threshold $\\beta_{th}$, the user successfully mitigates jamming attacks. Otherwise, the legitimate transmission fails. The legitimate user aims to find an appropriate channel at the beginning of each hop for achieving a receiving SINR that exceeds $\\beta_{th}$ during each time slot of the hop.\n$\\beta(k, f) = \\frac{\\int_{f}^{f+b/2} h_{u,r}^k(f) |U(f)|^2 df}{\\sum_{i=1}^{N_I} \\int_{f}^{f+b/2} h_{i,r}^k(f_i) |J_i(f-f_i)|^2 + N_k(f) df}$                                                                    (9)"}, {"title": "2.2 Markov Game Model", "content": "In the proposed anti-jamming scenario, the adaptive channel access decision-making process of the legitimate user is sequential, and the nonstationarity evolution of the state happens when the intelligent jammer adopts a time-varying jamming policy. Under these circumstances, the evolution of the environment is influenced by the actions of both the legitimate transmitter and the intelligent jammer, rather than solely by the action of the legitimate transmitter. This non-stationary characteristic prompts us to model the interaction between the legitimate user and the intelligent jammer as a Markov Game (MG), where the user and the jammer could make decisions simultaneously, each pursuing completely opposing objectives. The anti-jamming MG could be described by a seven-tuple, namely,\n$\\mathcal{G} = \\{S, A_u, A^j, P, R_u, R_j, \\gamma\\},$                                                              (10)\nwhere S represents the set of environment states, $A_u$ and $A^j$ represent the action set of the legitimate user and the intelligent jammer, respectively, $P$ is the transition function, $R_u$ denotes the legitimate user's reward function, $R_j$ denotes the intelligent jammer's reward function, and $\\gamma$ is the discount factor.\nFor the considered anti-jamming MG, we assume that the actions of both the legitimate user and the intelligent jammer are allowed to change among hops. An illustrative diagram of the time slot structure for the proposed anti-jamming MG is illustrated in Fig. 2. Since the spectrum waterfall defined in Chen and Wen [2016] contains time, frequency and power domain information, it could be used to represent the complex spectrum state and provide enough information for either anti-jamming or jamming decision-making. Therefore, the environment state of the n-th hop could be represented as the observed spectrum waterfall at the beginning of the n-th hop, i.e.,\n$\\vec{S_n} = \\begin{bmatrix} \\vec{s_{(n-1)N_h-N_T+1}} \\\\ : \\\\ \\vec{s_{(n-1)N_h-1}} \\\\ \\vec{s_{(n-1)N_h}} \\end{bmatrix},$                                                                                    (11)\nwhere $s_i$ denotes the spectrum vector during the i-th time slot, $N_T$ denotes the length of historical data. The user takes $S_n$ as the input for anti-jamming decision-making. Similarly, the DRL-based intelligent jammer also takes the observed spectrum waterfall $S_j$ as the input for its jamming decisions. For convenience, $\\vec{S_{in}} = S_n$ is used in what follows.\nLet $\\pi(\\cdot)$ and $\\mu(\\cdot)$ denote the policies of the legitimate user and intelligent jammer, respectively. During the n-th hop, the legitimate user executes an anti-jamming action $a_u \\in F$ based on the policy $\\pi(S_n)$, while the intelligent jammer performs a jamming action $a_j \\subseteq F$ according to the policy $\\mu(S_n)$. Then, the state $S_n$ transits to the next state $S_{n+1}$ with probability $P(S_{n+1} | S_n, a_u, a_j)$, and the environment provides immediate rewards $r_u = R_u(S_n, a_u, a_j)$ and $r_j = R_j(S_n, a_u, a_j)$ to the legitimate user and the intelligent jammer, respectively.\nAt the end of each hop, the receiver responds to the transmitter with a feedback (i.e., ACK/NACK) through the control link. Specifically, if $\\beta(k, f_u) \\geq \\beta_{th}, \\forall k = (n - 1)N_h + 1,...,nN_h - 1, nN_h$ holds during the n-th hop, the receiver transmits an ACK signal to the transmitter. Otherwise, an NACK signal is transmitted. Motivated by Yuan et al. [2024], we assume that the legitimate user's agent is located at the receiver. The agent could determine which channel to access and transmit the channel decision message $a_u$ to the transmitter at the beginning of the n-th hop through the control link. Then it takes the minimum SINR during the n-th hop as the immediate reward for action $a_u$, namely,\n$r_u = \\min_{k} \\{\\beta(k, f_u)\\}, \\quad k=(n-1)N_h+1,...,nN_h-1,nN_h$                                                        (12)"}, {"title": "", "content": "Whereas the intelligent jammer has a completely contrasting objective. It is assumed that the worst jammer could accurately assess the effectiveness of jamming attacks, and the reward for the intelligent jammer's action $a_j^n$ is determined by the intercepted ACK/NACK signal from the control link during the n-th hop, namely,\n$r_j^n = \\begin{cases} 1, & \\text{if the NACK feedback is detected}, \\\\ -1, & \\text{if the ACK feedback is detected}. \\end{cases}$                                                                     (13)\nIn the process of the anti-jamming game, the legitimate user aims to maximize its cumulative reward $R_u(S_1)$, i.e.,\n$R_u(S_1) = \\sum_{n=1}^{\\infty} \\gamma^{n-1} r_u^n$                                                                                                (14)\nMeanwhile, the intelligent jammer aims to maximize its cumulative reward $R_j(S_1)$, i.e.,\n$R_j(S_1) = \\sum_{n=1}^{\\infty} \\gamma^{n-1} r_j^n$                                                                                                (15)\nIn Markov Games, no player's policy is inherently optimal, as its return is influenced by the actions of other players Zhu and Zhao [2022]. The best response (BR) and the Nash equilibrium (NE) are commonly used to evaluate the performance of one player against others in MGs. For the aforementioned anti-jamming MG with a legitimate user and an intelligent jammer, the BR and the NE are defined as follows.\nDefinition 1. (BR in the anti-jamming MG) Given the policy of the intelligent jammer $\\mu$, the policy $\\pi_b$ of the legitimate user is defined as the BR policy if there exists no alternative policy that could yield a higher cumulative reward, formally expressed as\n$R_u(S_1; \\pi_b, \\mu) \\geq R_u(S_1; \\pi, \\mu), \\quad \\forall \\pi.$                                                                 (16)\nConversely, when the legitimate user employs the policy $\\pi$, the BR policy of the intelligent jammer, denoted as $\\mu'$, must satisfy\n$R_j(S_1; \\pi, \\mu') > R_j(S_1; \\pi, \\mu), \\quad \\forall \\mu.$                                                                  (17)\nDefinition 2. (NE in the anti-jamming MG) The NE is defined as a pair of policies $(\\pi^*, \\mu^*)$, where both $\\pi^*$ and $\\mu^*$ represent the best responses to each other. Formally, this could be expressed as\n$R_u(S_1; \\pi, \\mu^*) \\leq R_u(S_1; \\pi^*, \\mu^*) < R_u(S_1; \\pi^*, \\mu), \\quad \\forall \\pi, \\mu;$\n$R_j(S_1; \\pi^*, \\mu) < R_j(S_1; \\pi^*, \\mu^*) < R_j(S_1; \\pi, \\mu^*), \\quad \\forall \\pi, \\mu.$                                                   (18)\nAccording to Nash [1951], the NE of the proposed anti-jamming MG always exists and is equivalent to the minimax solution of the game, i.e.,\n$R_u(S_1; \\pi^*, \\mu^*) = \\max_{\\pi} \\min_{\\mu} R_u(S_1; \\pi, \\mu) = \\min_{\\mu} \\max_{\\pi} R_u(S_1; \\pi, \\mu);$\n$R_j(S_1; \\pi^*, \\mu^*) = \\max_{\\mu} \\min_{\\pi} R_j(S_1; \\pi, \\mu) = \\min_{\\pi} \\max_{\\mu} R_j(S_1; \\pi, \\mu).$                                                   (19)\nThe NE delineates the maximum return that a legitimate user could achieve when facing a formidable opponent. It is particularly meaningful when the jammer is capable of adapting its policy in response to the user's actions. When the legitimate user adopts the policy $\\pi^*$, its return is guaranteed to be at least equal to the Nash equilibrium. While, if the legitimate user switches to an alternative anti-jamming policy, its return might fall below $R_u(S_1; \\pi^*, \\mu^*)$.\nMinimax Q learning Wang et al. [2011] could be employed to determine the NE for MGs without any prior knowledge of the environment dynamics. Additionally, the legitimate user could achieve a beyond NE performance by employing an opponent modeling based DQN Li et al. [2023a]. However, these methods assume that the legitimate user could access the actions selected by the intelligent jammer at the beginning of each hop, which is generally impractical. Meanwhile, these DRL-based methods necessitate extensive training episodes in non-stationary environments, where the strategies of both the legitimate user and the intelligent jammer could continuously evolve over time. To address these issues, we propose a fast adaptive anti-jamming channel access approach to find a policy beyond NE with fewer training episodes in the following sections."}, {"title": "3 Coarse-Grained Spectrum Prediction", "content": "Since the legitimate user could not directly observe the jammer's actions in practical anti-jamming scenarios, inferring the intelligent jammer's policy using existing opponent modeling approaches becomes challenging. Although the actions of the intelligent jammer are not accessible in practice, the spectrums observed during the current hop could still reflect the behavior of both the legitimate user and the jammers. Therefore, it is possible to employ a convolutional neural network (CNN) model for predicting the spectrum state of the current hop, thereby enhancing the learning capability of the legitimate user's agent. Since the resolution of the spectrum vector defined in (5) is too fine and thus inefficient for spectrum prediction, we employ the coarse-grained (CG) spectrum with only M samples for each hop.\nIntuitively, we formulate the coarse-grained spectrum prediction as a regression problem that could be addressed using a supervised learning model $F(\\cdot; \\psi)$, i.e.,\n$\\vec{\\hat{c}_n} = F(\\vec{S_n}; \\psi).$                                                                                                             (20)\nwhere the input $S_n$ is the observed spectrum waterfall at the beginning of the current hop, and the output $\\vec{\\hat{c}_n}$ is the predicted coarse-grained spectrum during the current hop. The supervised learning model $F(\\cdot; \\psi)$ could be implemented by a CNN, where $\\psi$ denotes the collection of parameters for the CNN model. As illustrated in Fig. 4, the proposed CNN model is composed of two convolutional (Conv) layers and three fully connected (FC) layers. The Conv layers process the input spectrum waterfall, while the FC layers integrate the processed information. It is worth noting that the architecture of the network depicted is just an example and could be tailored to suit the specific scenario."}, {"title": "", "content": "During the training procedure, the legitimate user collects the spectrum waterfall samples and the ground-truth labels (i.e., the corresponding coarse-grained spectrums) from the real-time interaction between the legitimate user and the wireless adversarial environment. The collected sample-label pair $(S_n, c_n)$ is then stored in the memory $D_c$. However, since the user's anti-jamming policy and the jammer's policy are continuously updated through their respective agents during the interactions, the regression model trained on a fixed training dataset may not perform well in a dynamically changing environment. This is because legitimate users with different anti-jamming policies may take entirely different actions when observing the same spectrum waterfall, and the same holds for the intelligent jammer. To address this, we design a dynamically refreshed memory that follows the first-in-first-out (FIFO) principle, storing the latest $|D_c|$ sample-label pairs for training the regression model $F(\\cdot; \\psi)$.\nIn each training epoch, a minibatch $B_c$ randomly selected from $D_c$ is fed to the regression model $F(\\cdot; \\psi)$ for training. The parameters $\\psi$ of the regression model could be updated by stochastic gradient descent (SGD), i.e.,\n$\\psi_{n+1} = \\psi_n - a_c \\nabla_{\\psi_n} \\mathcal{L}_C(\\psi),$                                                                                   (21)\nwhere $a_c$ is the learning rate and $\\mathcal{L}_C(\\psi)$ is the loss function. To minimize the regression errors, the standard root mean squared error (RMSE) loss is adopted as\n$\\mathcal{L}_C(\\psi) = E_{B_C \\subseteq D_C} [||\\vec{\\hat{c}_i} - \\vec{c_i}||^2] = E_{B_c \\subseteq D_C} [\\sum_{m=1}^M (\\hat{c}_m^i - c_m^i)^2],$                                          (22)\nwhere\n$\\vec{\\hat{c}_i} = F(\\vec{S_i}; \\psi) = [\\hat{c}_1^i, \\hat{c}_2^i, ..., \\hat{c}_M^i],$                                                                                                            (23)"}, {"title": "", "content": "denotes the output of the coarse-grained spectrum prediction model, and\n$\\vec{c_i} = [c_1^i, c_2^i, ..., c_M^i],$                                                                                                            (24)\ndenotes the ground-truth label of the sample $S_i$. It is noted that $(S_i, c_i) \\in B_c$ is a sample randomly selected from $D_c$, where $S_i$ is the spectrum waterfall composed of several spectrum vectors from previous hops, and the label $c_i$ is the coarse-grained spectrum of the current hop. This enables the network to predict the future coarse-grained spectrum based on the currently observed spectrum waterfall."}, {"title": "4 Fast Adaptive Anti-jamming Channel Access Approach", "content": "As previously discussed, the environmental dynamics are influenced by multiple agents. To identify the dynamic BR to the time-varying jamming policy, the legitimate user's agent needs to extract knowledge from the environment and exploit the opponent's behavior as soon as possible in a multi-agent system. To this end, we propose a fast adaptive anti-jamming channel access approach with joint DQN and coarse-grained spectrum prediction, where the coarse-grained spectrum prediction is adopted as an auxiliary task for learning the jammer's policy feature and environment dynamics.\nAs shown in Fig. 5, we construct a Q-function estimation network and a coarse-grained (CG) spectrum prediction network for determining the legitimate transmitter's channel access action at the beginning of each hop. Specifically, the Q-function estimation network consists of a feature extraction module and an inference module. Meanwhile, the coarse-grained spectrum prediction network is composed of a feature extraction module and a regression forecasting module. It is noted that both the Q-function estimation network and the CG spectrum prediction network take the spectrum waterfall $S_n$ as the input, then output the estimated Q-value and the predicted CG spectrum, respectively. The legitimate user's anti-jamming action is determined based on the Q-value $Q(S_n, a_u), \\forall a_u \\in F$ and the predicted CG spectrum $\\vec{\\hat{c}_n}$, jointly. The architecture and the training procedure of the proposed fast adaptive anti-jamming channel access approach are discussed in what follows."}, {"title": "4.1 Q-function Estimation", "content": "Since the jammer's actions are unknown to the legitimate user in practical scenarios, we simplify the proposed anti-jamming MG $\\mathcal{G} = \\{S, A_u, A^j, P, R_u, R_j, \\gamma\\}$ to an anti-jamming Markov decision process (MDP), which could be addressed using RL-based methods. The anti-jamming MDP could be formulated by a five-tuple $\\{S, A_u, P, R_u, \\gamma\\}$,"}, {"title": "", "content": "where S is the set of environment state, $A_u$ is the legitimate user's action set, P denotes the transition function, $R_u$ denotes the legitimate user's reward function, $\\gamma \\in (0, 1", "a_u": 25, "1998": "i.e.,\n$Q^*(S, a_u) = \\sum_{S'} P(S'|S, a_u) [r + \\gamma \\max_{a'_u} Q^*(S', a'_u)", "2015": "is employed for estimating the Q-function over the high-dimensional and complex state space. Here, we utilize a CNN-based model as the Q network to approximate the Q-function for each state-action pair $(S_n, a_u)$, i.e.,\n$Q(S_n, a_u; \\theta) \\approx E [r_n + \\gamma \\max_{a_{n+1}^u} Q(S_{n+1}, a_{n+1}^u; \\theta^{-}) | S_n, a_n; \\theta", "theta))^2": 29}]}