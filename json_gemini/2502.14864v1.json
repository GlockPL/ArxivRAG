{"title": "Benchmarking Multimodal RAG through a Chart-based Document\nQuestion-Answering Generation Framework", "authors": ["Yuming Yang", "Jiang Zhong", "Li Jin", "Jingwang Huang", "Jingpeng Gao", "Qing Liu", "Yang Bai", "Jingyuan Zhang", "Rui Jiang", "Kaiwen Wei"], "abstract": "Multimodal Retrieval-Augmented Generation\n(MRAG) enhances reasoning capabilities by\nintegrating external knowledge. However, ex-\nisting benchmarks primarily focus on simple\nimage-text interactions, overlooking complex\nvisual formats like charts that are prevalent in\nreal-world applications. In this work, we in-\ntroduce a novel task, Chart-based MRAG, to\naddress this limitation. To semi-automatically\ngenerate high-quality evaluation samples, we\npropose CHARt-based document question-\nanswering GEneration (CHARGE), a frame-\nwork that produces evaluation data through\nstructured keypoint extraction, crossmodal ver-\nification, and keypoint-based generation. By\ncombining CHARGE with expert validation,\nwe construct Chart-MRAG Bench, a com-\nprehensive benchmark for chart-based MRAG\nevaluation, featuring 4,738 question-answering\npairs across 8 domains from real-world doc-\numents. Our evaluation reveals three critical\nlimitations in current approaches: (1) unified\nmultimodal embedding retrieval methods strug-\ngles in chart-based scenarios, (2) even with\nground-truth retrieval, state-of-the-art MLLMs\nachieve only 58.19% Correctness and 73.87%\nCoverage scores, and (3) MLLMs demonstrate\nconsistent text-over-visual modality bias during\nChart-based MRAG reasoning. The CHARGE\nand Chart-MRAG Bench are released at https:\n//github.com/Nomothings/CHARGE.git.", "sections": [{"title": "1 Introduction", "content": "Multimodal retrieval-augmented generation\n(MRAG) (Zhao et al., 2023) enhances multimodal\nreasoning by retrieving relevant external knowl-\nedge, and leveraging multimodal large language\nmodels (MLLMs) for informed response genera-\ntion (OpenAI, 2023; Zhang et al., 2024a). This\napproach substantially mitigates hallucinations and\nimproves factual grounding (Gao et al., 2023).\nEffectively evaluating MRAG systems requires\nhigh-quality benchmarks that assess both retrieval\nand generation. Existing benchmarks such as\nMRAG-Bench (Hu et al., 2024) and Dyn-VQA\n(Li et al., 2024b) have made strides in assess-\ning MRAG capabilities through manually curated\nquestion-answering (QA) pairs. However, as illus-\ntrated in Fig.1(a) and (b), these benchmarks primar-\nily focus on scenarios involving images or simple\ncombinations of images and text. Such settings\nfail to capture the complex interactions between\nvisual details and corresponding text, particularly\nwhen dealing dense and structured information like\ncharts, which are widely used in real-world appli-\ncations (Masry et al., 2022). This leaves a critical\ngap in MRAG evaluation.\nTo bridge this gap, we propose a new task:\nChart-based MRAG. For a given text query, this\ntask involves three RAG sub-tasks: (1) Text-Chart\nMRAG, as illustrated in Fig. 1(c), both textual and\nchart data must be jointly retrieved to generate cor-\nrect answers. In addition, to allow for the sepa-\nrate evaluation of each modality's contributions, it\nalso provides (2) Text-only RAG, where answers\ncan only be found in textual information; and (3)\nChart-only MRAG, where answers depend exclu-\nsively on chart data. To comprehensively evalu-\nate these tasks, a major challenge is how to semi-\nautomatically generate high-quality QA pairs that\naccurately capture text-chart interactions.\nTo overcome this challenge, we propose CHARt-\nbased document question-answering GEneration\n(CHARGE), a framework for automatically gen-\nerating QA pairs from real-world chart-document\ndata. CHARGE follows a three-stage pipeline com-\nprising structured keypoint extraction from text\nand chart data, crossmodal verification for accu-\nracy, and keypoint-based generation to model com-\nplex multimodal interactions. Moreover, to further\nchallenge the chart-based MRAG task, MLLMs\nare employed to generate QA pairs that require\nmulti-hop reasoning based on intra-document or\ninter-document retrieval.\nBuilding on CHARGE, we introduce Chart-\nMRAG Bench, a high-quality, human-checked\nbenchmark tailored for Chart-based MRAG. With\nCHARGE, 5,866 qualified QA pairs were initially\ngenerated, after that, 4,738 (nearly 80%) were\nmeticulously selected through expert evaluation\nbased on clarity, accuracy, multimodal coherence,\nand ethical considerations. As shown in Table 1,\nChart-MRAG Bench comprises 267 documents\nspanning 8 domains, 8 types of questions, 1,283\nparagraphs, and 627 charts, capturing complex\ncrossmodal interactions in realistic scenarios.\nWe conducted a systematic evaluation of main-\nstream retrieval methods and MLLMs on Chart-\nMRAG Bench. In our evaluation, keypoint-based\nCorrectness and Coverage metrics were introduced\nto rigorously assess accuracy and comprehensive-\nness. The results reveal that unified multimodal em-\nbedding retrieval methods, which rely on a single\nvector store, perform poorly in high-density chart\nscenarios. Furthermore, even with ground-truth\nretrieval, the best-performing Claude-3.5 Sonnet\n(Team et al., 2024) only achieved 58.19 Correctness\nand 73.87 Coverage metrics, highlighting persis-\ntent challenges in text-chart multimodal reasoning.\nIn summary, the contributions of this paper are:\n1) We present Chart-based MRAG, the first ex-\ntension of MRAG to chart scenarios that introduces"}, {"title": "2 Related Work", "content": "Multimodal RAG Methods. Recent advances in\nRetrieval-Augmented Generation (RAG) (Izacard\net al., 2022; Zhang et al., 2024b) have success-\nfully extended to multimodal domains (Chen et al.,\n2022; Zhao et al., 2023, 2024), enabling cross-\nmodal tasks through MLLMs (Yao et al., 2024;\nTeam, 2024). While researchers have proposed var-\nious approaches (Ma et al., 2024a; Faysse et al.,\n2024; Yu et al., 2024; Methani et al., 2020; Mathew\net al., 2021) for crossmodal retrieval, current evalu-\nation methodologies predominantly rely on Visual\nQuestion Answering (VQA) datasets (Marino et al.,\n2019; Talmor et al., 2021; Schwenk et al., 2022;\nMasry et al., 2022). These evaluations fall short in\naddressing retrieval-specific challenges.\nMultimodal RAG Benchmarks. The effective-\nness of MRAG systems necessitates comprehen-\nsive evaluation benchmarks. While several bench-\nmarks (Hu et al., 2024; Li et al., 2024b; Zhou\net al., 2024) explore vision-based retrieval for ques-\ntion answering through manual annotation, they\nneglect the critical dimension of crossmodal col-\nlaborative generation. Some studies (Dong et al.,\n2025; Ma et al., 2024b; Ding et al., 2024) consider"}, {"title": "3 CHARGE Framework", "content": "We present CHARGE, a framework for generat-\ning multimodal multi-hop QA pairs from text-chart\ndocuments. CHARGE operates in three stages: (1)\nextracting self-contained keypoints from both tex-\ntual and visual content, (2) verifying the modality\nauthenticity of extracted keypoints through cross-\nmodal verification, and (3) generating diverse QA\npairs by combining related keypoints across docu-\nments and modalities."}, {"title": "3.1 Extract Keypoints", "content": "As illustrated in Fig 2, given multimodal docu-\nments D = {d1, ..., dn }, CHARGE process its tex-\ntual content into coherent chunks T = {t1, ..., tm}\nand charts as discrete units C = {C1, ..., Ck}. We\ndefine keypoints as self-contained factual state-\nments that capture core information from these\nsource materials. These atomic units are extracted\nfrom both textual and visual content (e.g., \"33% of\nU.S. adults say they use TikTok\") through:\n$K = \\begin{cases}\n\\Phi_t(T) & \\text{for text} \\\\\n$\\Phi_c(C, \\psi(C)) & \\text{for chart}\n\\end{cases}$   (1)\nwhere K = {k1, ..., kr} consists of structured in-\nformation units capturing factual statements, logi-\ncal inferences, or conclusive summaries. For tex-\ntual content, we utilize GPT-40 through function\n$\\Phi_t$. For visual content, we first extract numerical\nvalues using function $\\psi$ (implemented with Char-\ntOCR (Luo et al., 2021)), then employ GPT-40\nthrough function $\\Phi_c$ to jointly process the charts C\nand the extracted values, ensuring both contextual\ncomprehension and numerical precision. Detailed\nworkflow is presented in Appendix A."}, {"title": "3.2 Crossmodal Verification", "content": "To ensure the reliability of extracted keypoints, we\ndevelop a crossmodal verification mechanism that\nvalidates whether information truly belongs to its\nclaimed modality. Our key insight is: Authentic\nmodality-specific keypoints should be retrievable\nfrom its source modality but not from the other.\nWe first categorize keypoints into two funda-\nmental types: (1) Text-based keypoints (KT): in-\nformation exclusively present in textual form; (2)\nChart-only keypoints (KC): information uniquely\nextractable from chart visualization. While GPT-40\nperforms initial classification, crossmodal Verifica-\ntion is crucial for complex reasoning tasks.\nThe verification process employs crossmodal\nquerying with GPT-40 serving as a judge to de-\ntermine whether the queried information exists in\neach modality's response. Taking text-based key-\npoint verification as an example, for a given key-\npoint k \u2208 KT, we query both its source text chunk\nti and the paired chart ci (with OCR information\nvi). Let $\\hat{k}$ and $\\check{k}$ denote the model's responses\nfrom text and chart modalities respectively. The\nverification criterion is formalized as:\n$Status(k) = \\begin{cases}\nRetain & \\text{if } \\hat{k} = k \\text{and } \\hat{k} \\neq \\check{k}\\\\\nDrop & \\text{otherwise}.\n\\end{cases}$  (2)\nThis automated process retains keypoints only\nwhen correctly retrieved from their source modal-\nity and absent in others. Detailed algorithms are\nprovided in Appendix B."}, {"title": "3.3 Question-Answer Pair Generation", "content": "Since each keypoint represents a specific conclu-\nsion or data point, it can generate a correspond-\ning question-answer pair (commonly referred to as\nSingle-Point QA). In our CHARGE framework, we\nsupport this basic form of QA generation. Addition-\nally, recognizing that most real-world queries re-\nquire the integration of multiple knowledge points\nto be answered fully (known as Multi-hop QA), we\nfurther designed a multi-hop question answering\napproach: by combining semantically related key-\npoints to form a single question-answer pair. These\ntypes of questions cannot be completely answered\nusing just one keypoint; they require the retrieval of\nall information sources (text chunks or charts) con-\ntaining the constituent keypoints to be answered\ncorrectly. As illustrated in Fig 3, CHARGE gener-\nates Multi-hop QA that requires retrieving multiple"}, {"title": "4 Chart-MRAG Bench", "content": "By utilizing the CHARGE framework, we gener-\nated an initial pool of question-answer pairs. These\npairs underwent rigorous expert evaluation to en-\nsure high quality, culminating in the Chart-MRAG\nBench. This process was guided by 4 principles:\nAuthenticity and Diversity. The benchmark is\nbased on real-world data collected from the of-\nficial website\u00b9, a trusted source of high-quality\nsocial research. We collected data from Septem-\nber 2023 to September 2024, encompassing 267\ndocuments containing 1,283 text passages and 627\ncharts. As illustrated in Table 2 and Fig 4, Chart-\nMRAG Bench encompasses 8 distinct domains,\nintegrating over 10 chart types and 8 QA types.\nAnnotation Reliability. We engaged 12 expert an-\nnotators with Master's degrees. All annotators were\nproficient in English, with an average TOEFL score\nof 92 or equivalent language proficiency. The an-\nnotation process took 34 working days to complete.\nOur annotation protocol involved three indepen-\ndent reviewers evaluating each sample, achieving\na Fleiss's kappa (Fleiss and Cohen, 1973) of 0.82,\nindicating substantial inter-annotator agreement.\nRigorous Quality Control. Through meticulous\nmanual review, we refined the dataset from 9,600\ninitial candidates to 5,866 validated pairs by sys-\ntematically eliminating 2,631 samples with OCR\nerrors and 1,103 redundant samples. A consensus-\nbased sampling strategy required validation from at\nleast two reviewers, resulting in 4,738 high-quality\nsamples (nearly 80% of the validated pairs).\nHigh Information Complexity. Statistical anal-\nsis reveals the benchmark's sophistication: ap-\nproximately 70% of charts contain more than 8"}, {"title": "5 Experiments", "content": "We conduct comprehensive evaluations using 3\ndistinct retrieval methods and 8 diverse MLLMs.\nIncluding Multimodal Retrievers: CLIP (Rad-\nford et al., 2021), JINA (Koukounas et al., 2024),\nSigLIP (Zhai et al., 2023), BGE-M3-base/large\n(Chen et al., 2024) and E5-base/large (Wang et al.,\n2022). And Backbone MLLMs: GPT-40 (ver-\nsion 2024-11-20) (Radford et al., 2021), GPT-\n4-Vision (Radford et al., 2021), Gemini-1.5-Pro\n(Team et al., 2024), Claude-3.5-Sonnet (version\n2024-10-22) (Awadalla et al., 2023), SAIL-VL-2B\n(Team, 2024), Qwen2-VL-7B-instruct (Wang et al.,\n2024), MiniCPM-V-2.6 (8B) (Yao et al., 2024), and\nLlama-3.2-90B-Vision (Dubey et al., 2024).\nFollowing Wu et al. (2024), we evaluate multi-\nmodal retrieval models using Recall@5 (R@5) and\nRecall@10 (R@10). Please refer to Appendix E\nfor details of the retrieval setup and metrics. More-\nover, since chart-based MRAG is a newly proposed\ntask, existing evaluation metrics are inadequate.\nTherefore, we introduce Correctness and Coverage\nmetrics to assess the quality of responses.\nCorrectness. It measures the exact match be-\ntween response and ground truth keypoints. Given\na question-answer pair {Q, A, Kgt} with ground\ntruth keypoints $K^{gt} = \\{ k_1^{gt}, ..., k_n^{gt} \\}$, we extract"}, {"title": "5.1 Baselines and Evaluation Metrics", "content": "response using an LLM. The score is defined as:\n$Correctness(K^{m}, K^{gt}) = \\mathbb{1}[K^{m} = K^{gt}]$, (3)\nwhere $K^{m} = K^{gt}$ implies complete keypoint\nmatching and equal cardinality. This binary metric\nrequires perfect accuracy, with zero tolerance for\nmissing information or errors.\nCoverage. It quantifies the proportion of correctly\ncaptured ground truth keypoints:\n$Coverage(K^{m}, K^{gt}) = \\frac{|K^{m}|}{|K^{gt}|}$, (4)\nwhere $K^{m}$ represents matched ground truth key-\npoints. This continuous metric in [0,1] enables\ngranular evaluation."}, {"title": "5.2 Retrieval Performance Comparison", "content": "Table 3 reveals significant challenges in multi-\nmodal retrieval. While existing retrievers ex-\nhibit strong single-modal performance (JINA-CLIP\nachieves 77.78% Recall@5 in text-only questions\nand SigLIP + E5 reaches 84.18% Recall@5 in\nchart-only tasks), Inter-Document Text-Chart ques-\ntions yielded only 22.32% retrieval accuracy. The\nkey findings demonstrate that storing and retrieving\ncharts and text separately in the database substan-\ntially improves performance, achieving recall rates\nof 42.53% and 61.10% at k=5 and k=10.\nUnified multimodal embeddings fail in\nknowledge-intensive scenarios. While Method 1\noutperforms all other approaches in pure text-only\nQA, it achieves zero recall (0.00%) in chart-only\nQA and Text-Chart QA tasks. This phenomenon\nreveals a critical limitation: current unified mul-\ntimodal embedding models excel at representing\nknowledge-sparse content (e.g., identifying a dog\nin an image) but struggle with knowledge-intensive\nscenarios (e.g., retrieving specific numerical values\nfrom charts in a multimodal repository).\nChart captioning enables simple yet effective\nmultimodal retrieval. Methods 2 and 3 achieve\ncomparable performance (Recall@5: 41.53% vs\n42.53%), with differences primarily in chart re-\ntrieval due to the inherent limitations of text-based\nchart representations. However, considering the\nmaintenance overhead of separate modal stores,\ncaption-based retrieval provides a practical ap-\nproach that preserves effectiveness while signif-\nicantly reducing system complexity."}, {"title": "5.3 Generative Performance Comparison", "content": "Table 4 presents the comprehensive experimen-\ntal results of mainstream MLLMs, with retrieval\nmethod 3 consistently applied across all evalua-\ntions to ensure controlled comparison. The results\nreveal that state-of-the-art MLLMs achieve only\nmodest performance metrics (Correctness = 3.06\nand Coverage = 9.59) without multimodal RAG\nknowledge, highlighting Chart-MRAG Bench's ex-\nceptional challenging nature that surpasses existing\nbenchmarks in knowledge leakage control.\nClaude-3.5-Sonnet demonstrates superior over-\nall performance. The experimental results validate\nour keypoint-based evaluation methodology. With\nground truth retrieval, Claude-3.5-Sonnet achieves\nCorrectness of 58.19% and Coverage of 73.87%,"}, {"title": "5.4 Further Analysis", "content": "In this study", "shows": "nModel performance in multimodal retrieval sig-\nnificantly correlates with parameter scale. Em-\npirical analysis reveals a strong correlation between\nmodel scale and multimodal retrieval performance.\nWe evaluated eight models of varying parameter\nsizes under different retrieval settings (k = 2, 5,\n10, 15, 20), where retrieved items were balanced\nbetween images and text (split equally for even k,\nwith text receiving one additional item for odd k).\nFor each model, we selected 40 question-answer\npairs per category, totaling 320 pairs for compre-\nhensive evaluation. The results\ndemonstrate that larger models consistently achieve\nsuperior performance across all retrieval settings.\nIn contrast, smaller models show no significant im-\nprovement (even exhibit declining) in performance\nas the number of retrieved items increases.\nLarger retrieval windows lead to a non-trivial\ntrade-off between retrieval coverage and answer\nquality. To systematically investigate the impact\nof Top_k on response generation, we conducted\nextended experiments as visualized in With\nk=5, the system achieves a 42.53 Recall and 56.17\ncorrectness. When increasing k=10, although the\n61.10 Recall, the answer get 49.13 correctness. No-\ntably, while this adjustment results in an increase"}]}