{"title": "Generalizability of Memorization Neural Networks", "authors": ["Lijia Yu", "Xiao-Shan Gao", "Lijun Zhang", "Yibo Miao"], "abstract": "The neural network memorization problem is to study the expressive power of neural networks to interpolate a finite dataset. Although memorization is widely believed to have a close relationship with the strong generalizability of deep learning when using over-parameterized models, to the best of our knowledge, there exists no theoretical study on the generalizability of memorization neural networks. In this paper, we give the first theoretical analysis of this topic. Since using i.i.d. training data is a necessary condition for a learning algorithm to be generalizable, memorization and its generalization theory for i.i.d. datasets are developed under mild conditions on the data distribution. First, algorithms are given to construct memorization networks for an i.i.d. dataset, which have the smallest number of parameters and even a constant number of parameters. Second, we show that, in order for the memorization networks to be generalizable, the width of the network must be at least equal to the dimension of the data, which implies that the existing memorization networks with an optimal number of parameters are not generalizable. Third, a lower bound for the sample complexity of general memorization algorithms and the exact sample complexity for memorization algorithms with constant number of parameters are given. It is also shown that there exist data distributions such that, to be generalizable for them, the memorization network must have an exponential number of parameters in the data dimension. Finally, an efficient and generalizable memorization algorithm is given when the number of training samples is greater than the efficient memorization sample complexity of the data distribution.", "sections": [{"title": "1 Introduction", "content": "Memorization is to study the expressive power of neural networks to interpolate a finite dataset [9]. The main focus of the existing work is to study how many parameters are needed to memorize. For any dataset Dtr of size N and neural networks of the form F : Rn \u2192 R, memorization networks with O(N) parameters have been given with various model structures and activation functions [31, 50, 30, 29, 26, 47, 56, 11, 65]. On the other hand, it is shown that in order to memorize an arbitrary dataset of size N [64, 56], the network must have at least \u03a9(N) parameters, so the above algorithms are approximately optimal. Under certain assumptions, it is shown that sublinear O(N2/3) parameters are sufficient to memorize Dtr [49]. Furthermore, Vardi et al. [55] give a memorization network with optimal number of parameters: O(VN).\nRecently, it is shown that memorization is closely related to one of the most surprising properties of deep learning, that is, over-parameterized neural networks are trained to nearly memorize noisy data"}, {"title": "2 Related work", "content": "Memorization. The problem of memorization has a long history. In [9], it is shown that networks with depth 2 and O(N) parameters can memorize a binary dataset of size N. In subsequent work, it is shown that networks with O(N) parameters can be a memorization for any dataset"}, {"title": "3 Notation", "content": "In this paper, we use O(A) to mean a value not greater than cA for some constant c, and O to mean that small quantities, such as logarithm, are omitted. We use \u03a9(A) to mean a value not less than cA for some constant c, and \u03a9 to mean that small quantities, such as logarithm, are omitted."}, {"title": "3.1 Neural network", "content": "In this paper, we consider feedforward neural networks of the form F : Rn \u2192 R and the l-th hidden layer of F(x) can be written as\n\u03a7\u03b9 = \u03c3(W1X1\u22121 + bi) \u2208 Rn\u0131,\nwhere \u03c3 = Relu is the activation function, Xo = x and No = n. The last layer of F is F(x) =\nWL+1XL+bL+1 \u2208 R, where L is the number of hidden layers in F. The depth of F is depth(F) =\nL + 1, the width of F is width(F) = max=1{n\u0131}, the number of parameters of F is para(F) =\n\u03a3i=0ni(ni+1 + 1). Denote H(n) to be the set of all neural networks in the above form.\nL"}, {"title": "3.2 Data distribution", "content": "In this paper, we consider binary classification problems and use D to denote a joint distribution on\nD(n) = [0, 1]n \u00d7 {\u22121, 1}. To avoid extreme cases, we focus mainly on a special kind of distribution\nto be defined in the following.\nDefinition 3.1. For n \u2208 Z+ and c\u2208 R+, D(n, c) is the set of distributions D on D(n), which has a\npositive separation bound: inf(x,1), (z,-1)~D ||x-z||2 \u2265 C.\nThe accuracy of a network F on a distribution D is defined as\nAD(F) = P(x,y)~D(Sgn(F(x)) = y)."}, {"title": "3.3 Memorization neural network", "content": "Definition 3.4. A neural network F \u2208 H(n) is a memorization of a dataset Dtr over D(n), if\nSgn(F(x)) = y for any (x, y) \u2208 Dtr.\nRemark 3.5. Memorization networks can also be defined more strictly as F(x) = y for any (x, y) \u2208\nDtr. In Proposition 4.10 of [62], it is shown that these two types of memorization networks need\nessentially the same number of parameters.\nTo be more precise, we treat memorization as a learning algorithm in this paper, as defined below.\nDefinition 3.6. L: Un\u2208Z+2D(n) \u2192 Unez, H(n) is called a memorization algorithm if for any n\nand Dtr \u2208 D(n), L(Dtr) is a memorization network of Dtr.\nFurthermore, a memorization algorithm L is called an efficient memorization algorithm if there\nexists a polynomial poly : R \u2192 R such that L(Dtr) can be computed in time poly(size(Dtr)),\nwhere size(Dtr) is the bit-size of Dtr.\nRemark 3.7. It is clear that if L is an efficient memorization algorithm, then para(L(Dtr)) is also\npolynomial in size(Dtr).\nThere exist many methods which can construct memorization networks in polynomial times, and all\nthese memorization methods are efficient memorization algorithms, which are summarized in the\nfollowing proposition.\nProposition 3.8. The methods given in [9, 62] are efficient memorization algorithms. The methods\ngiven in [55, 49] are probabilistic efficient memorization algorithms, which can be proved similar\nto that of Theorem 4.1. More precisely, they are Monte Carlo polynomial-time algorithms."}, {"title": "4 Optimal memorization network for dataset under distribution", "content": "By the term \"dataset under distribution\", we mean datasets that are sampled i.i.d. from a data distribution, and is denoted as Dtr ~ DN. In this section, we show how to construct the memorization network with the optimal number of parameters for dataset under distribution."}, {"title": "4.1 Memorization network with optimal number of parameters", "content": "To memorize N samples, \u2229(\u221aN) parameters are necessary [6]. In [55], a memorization network is given which has O(VN) parameters under certain conditions, where O means that some logarithm factors in N and polynomial factors of other values are omitted. Therefore, O(VN) is the optimal number of parameters for a network to memorize certain dataset. In the following theorem, we show that such a result can be extended to dataset under distribution.\nTheorem 4.1. Let D \u2208 D(n, c) and Dtr ~ DN. Then there exists a memorization algorithm L such that L(Dtr) has width 6 and depth (equivalently, the number of parameters) O(\u221aNln(Nn/c)). Furthermore, for any \u0454 \u2208 (0,1), L(Dtr) can be computed in time poly(size(Dtr), ln(1/\u20ac)) with probability \u2265 1 \u2013 \u0454.\nProof Idea. This theorem can be proven using the idea from [55]. Let Dtr = {(xi, Yi)}N1. The mainly different is that in [55], it requires ||xi - Xxj|| \u2265 c for all i \u2260 j, which is no longer valid when Dtr is sampled i.i.d. from distribution D. Since D has separation bound c > 0, we have"}, {"title": "4.2 Memorization network with constant number of parameters", "content": "In this section, we prove an interesting fact of memorization for dataset under distribution. We show that for a distribution D\u2208 D(n, c), there exists a constant ND \u2208 Z+ such that for all datasets sampled i.i.d. from D, there exists a memorization network with ND parameters.\nTheorem 4.3. There exists a memorization algorithm L such that for any D \u2208 D(n, c), there is an N'\u266d \u2208 Z+ satisfying that for any N > 0, with probability 1 of Dtr ~ DN, we have para(L(Dtr)) < ND. The smallest N\u266d of the distribution D is called the memorization parameter complexity of D, written as ND.\nProof Idea. It suffices to show that we can find a memorization network of Dtr with a constant number of parameters, which depends on D only. The main idea is to take a subset D'tr of Dtr such that Dtr is contained in the neighborhood of Dr. It can be proven that the number of elements in this subset is limited. Then construct a robust memorization network of D't with certain budget [62], we obtain a memorization network of Dtr, which has a constant number of parameters. The proof is given in Appendix C.\nCombining Theorems 4.1 and 4.3, we can give a memorization network with the optimal number of parameters.\nRemark 4.4. What we have proven in Theorem 4.3 is that a memorization algorithm with a constant number of parameters can be found, but in most of times, we have N' > ND. Furthermore, if N is large for the memorization algorithm, the algorithm can be efficient. Otherwise, if N is closed to ND, the algorithm is usually not efficient.\nRemark 4.5. It is obvious that the memorization parameter compelxity ND is the minimum number of parameters required to memorize any dataset sampled i.i.d. from D. ND is mainly determined by the characteristic of D \u2208 D(n, c), so ND may be related to n and c. It is an interesting problem to estimate ND."}, {"title": "5 Condition on the network structure for generalizable memorization", "content": "In the preceding section, we show that for the dataset under distribution, there exists a memorization algorithm to generate memorization networks with the optimal number of parameters. In this section, we give some conditions for the generalizable memorization networks in terms of width and number of parameters of the network. As a consequence, we show that the commonly used memorization networks with fixed width is not generalizable.\nFirst, we show that networks with fixed width do not have generazability in some situations. Reducing the width and increasing depth is a common way for parameter reduction, but it inevitably limits the network's power, making it unable to achieve good generalization for specific distributions, as shown in the following theorem.\nTheorem 5.1. Let w \u2208 Z+ and L be a memorization algorithm such that L(Dtr) has width not more than w for all Dtr. Then, there exist an integer n > w, c \u2208 R+, and a distribution D \u2208 D(n, c) such that, for any Dtr ~ DN, it holds Ap(L(Dtr)) \u2264 0.51.\nProof Idea. As shown in [40, 48], networks with small width are not dense in the space of measurable functions, but this is not enough to estimate the upper bound of the generalization. In order to further measure the upper bound of generalization, we define a special class of distributions. Then, we calculate the upper bound of the generalization of networks with fixed width on this class of"}, {"title": "6 Sample complexity for memorization algorithm", "content": "As said in the preceding section, generalization of memorization inevitably requires certain conditions. In this section, we give the necessary and sufficient condition for generalization for the memorization algorithm in Section 4 in terms of sample complexity.\nWe first give a lower bound for the sample complexity for general memorization algorithms and then an upper bound for memorization algorithms which output networks with an optimal number of parameters. The lower and upper bounds are approximately the same, thus giving the exact sample complexity in this case."}, {"title": "6.1 Lower bound for sample complexity of memorization algorithm", "content": "Roughly speaking, the sample complexity of a learning algorithm is the number of samples required to achieve generalizability [44]. The following theorem gives a lower bound for the sample complexity of memorization algorithms based on ND, which has been defined in Theorem 4.3."}, {"title": "6.2 Exact sample complexity of memorization algorithm with ND parameters", "content": "In Theorem 6.1, it is shown that \u2229(NZ) samples are necessary for generalizability of memorization. The following theorem shows that there exists a memorization algorithm that can reach generalization with O(NB) samples.\nTheorem 6.5. For all memorization algorithms L satisfies that L(Dtr) has at most ND parameters, with probability 1 for Dtr ~ DN, we have\n(1) For any c \u2208 R, \u0454, \u03b4 \u2208 (0, 1), n \u2208 Z+, if D \u2208 D(n, c) and N > v (N2 In(ND/(826))), then\nPDtr~DN (A(L(Dtr)) \u2265 1 \u2013 \u0454) \u2265 1 \u2013 \u03b4,\nwhere v is an absolute constant which does not depend on N, \u043f, \u0441, \u0454, \u0431.\n(2) If P\u2260NP, then all such algorithms are not efficient.\nProof Idea. For the proof of (1), we need to use the ND to calculate the VC-dimension [6], and take such a dimension in the generalization bound theorem [44] to obtain the result. For the proof of (2), we show that, if such algorithm is efficient, then we can solve the following reversible 6-SAT [43] problem, which is defined below and is an NPC problem. The proof of the theorem is given in Appendix G.\nDefinition 6.6. Leto be a Boolean formula and the formula obtained from by negating each variable. The Boolean formula is called reversible if either both and are satisfiable or both are not satisfiable. The reversible satisfiability problem is to recognize the satisfiability of reversible formulae in conjunctive normal form (CNF). By the reversible 6-SAT, we mean the reversible satisfiability problem for CNF formulae with six variables per clause. In [43], it is shown that the reversible 6-SAT is NPC."}, {"title": "7 Efficient memorization algorithm with guaranteed generalization", "content": "In the preceding section, we show that there exist memorization algorithms that are generalizable when N = O(NZ), but such an algorithm is not efficient. In this section, we give an efficient memorization algorithm with guaranteed generalization.\nFirst, we define the efficient memorization sample complexity of D.\nDefinition 7.1. For (x, y) ~ D, let L(x,y) = min(z,\u2212y)~D ||x - z||2 and B((x, y)) =\nB2(x, L(x,y)/3.1) = {z \u2208 Rn : ||z-x||2 \u2264 L(x,y)/3.1}. The nearby set S of D is a subset of sample\n(x, y) which is in distribution D and satisfies: (1) for any (x, y) ~ D, x \u2208 U(z,w)\u2208sB((z, w)); (2)\n|S| is minimum.\nEvidently, for any D \u2208 D(n, c), its nearby set is finite, as shown by Proposition 7.7. SD = |S| is\ncalled the efficient memorization sample complexity of D, the meaning of which is given in Theorem\n7.3.\nRemark 7.2. In the above definition, we use L(x,y)/3.1 to be the radius of B((x, y)). In fact, when\n3.1 is replaced by any real number greater than 3, the following theorem is still valid.\nTheorem 7.3. There exists an efficient memorization algorithm L such that for any c \u2208 R, \u20ac, \u03b4\u2208\n(0, 1), n \u2208 Z+, and D \u2208 D(n, c), if N > SD ln(SD/8), then\nPDtr~DN (A(L(Dtr)) \u2265 1 \u2013 \u0454) \u2265 1 \u2013 \u0431.\nMoreover, for any Dtr ~ DN, L(Dtr) has at most O(N2n) parameters.\nProof Idea. For a given dataset Dtr C [0, 1]n \u00d7 {\u22121, 1}, we use the following two steps to construct\na memorization network.\nStep 1. Find suitable convex sets {Ci} in [0,1]n such that each sample in Dtr is in at least one of\nthese convex sets. Furthermore, if x, z \u2208 Ci and (x, yx), (z, yz) \u2208 Dtr, then yx = yz, and define\ny(Ci) = Yx.\nStep 2. Construct a network F such that for any x \u2208 Ci, Sgn(F(x)) = y(Ci). This network must\nbe a memorization of Dtr, because each sample in Dtr is in at least one of {Ci}. Hence, if x \u2208 Ci\nand (x, yx) \u2208 Dtr, then Sgn(F(x)) = y(Ci) = yx. The proof of the theorem is given in Appendix\nH.\nRemark 7.4. Theorem 7.3 shows that there exists an efficient and generalizable memorization algo-\nrithm when N = O(SD). Thus, Sp is an intrinsic complexity measure of D on whether it is easy to\nlearn and generalize. By Theorem 6.1, SD > NZ for some D, but for some \u201cnice\u201d D, SD could be\nsmall. It is an interesting problem to estimate SD.\nRemark 7.5. Theorem 7.3 uses O(N2n) parameters, highlight the importance of over-\nparameterization [45, 7, 4]. Interestingly, Remark 6.8 shows that if the network has O(VN) pa-\nrameters, even if it is generalizable, it cannot be computed efficiently."}, {"title": "8 Conclusion", "content": "Memorization originally focuses on theoretical study of the expressive power of neural networks. Recently, memorization is believed to be a key reason why over-parameterized deep learning models have excellent generalizability and thus the more practical interpolation learning approach has been extensively studied. But the generalizability theory of memorization algorithms is not yet given, and this paper fills this theoretical gap in several aspects.\nWe first show how to construct memorization networks for dataset sampled i.i.d from a data distribution, which have the optimal number of parameters, and then show that some commonly used memorization networks do not have generalizability even if the dataset is drawn i.i.d. from a data distribution and contains a sufficiently large number of samples. Furthermore, we establish the sample complexity of memorization algorithm in several situations, including a lower bound for the memorization sample complexity and an upper bound for the efficient memorization sample complexity.\nLimitation and future work Two numerical complexities ND and SD for a data distribution D are introduced in this paper, which are used to describe the size of the memorization networks and the efficient memorization sample complexity for any i.i.d. dataset of D. ND is also a lower bound for the sample complexity of memorization algorithms. However, we do not know how to compute ND and SD, which is an interesting future work. Conjecture 6.7 tries to give a lower bound for the efficient memorization sample complexity. More generally, can we write ND and SD as functions of the probability density function p(x, y) of D?\nCorollary 6.4 indicates that even for the \"nice\" data distributions D(n, c), to achieve generalization for some data distribution requires an exponential number of parameters. This indicates that there exists \"data curse of dimensionality\u201d, that is, to achieve generalizability for certain data distribution, neural networks with exponential number of parameters are needed. Considering the practical success of deep learning and the double descent phenomenon [45], the data distributions used in practice should have better properties than D(n, c), and finding data distributions with polynomial size efficient memorization sample complexity ED is an important problem.\nFinally, finding a memorization algorithm that can achieve SOTA results in solving practical image classification problems is also a challenge problem."}, {"title": "A Proof of Proposition 3.3", "content": "Using the following steps, we construct a distribution D in [0, 1] \u00d7 {\u22121,1}. We use (x, y) ~ D to mean that\n(1) Randomly select a number in {-1, 1} as the label y.\n(2) If we get 1 as the label, then randomly select an irrational number in [0, 1] as samples x; if we get-1 as the label, then randomly select a rational number in [0, 1] as samples x.\nThen Proposition 3.3 follows from the following lemma.\nLemma A.1. For any neural network F, we have AD(F) \u2264 0.5.\nProof. Let F be a network. Firstly, we show that F can be written as\n\\begin{equation}\\label{}\nM\nF = \\sum L_i(x)I(x \\in A_i),\n\\end{equation}\nwhere Li are linear functions, I(x) = 1 if x is true or I(x) = 0. In addition, A\u2081 is an interval and\nAj \u2229 Ai = \u00d8 when j \u2260 i, and Li(x)I(x \u2208 A\u2081) is a non-negative or non-positive function for any\ni\u2208 [M].\nIt is obvious that the network is a locally linear function with a finite number of linear regions, so\nwe can write\n\\begin{equation}\\label{}\nM\nF = \\sum L'_i(x)I(x \\in A'_i),\n\\end{equation}\nwhere L' are linear functions, A is an interval and A'; \u2229 A = \u00d8 when j \u2260 i.\nConsider that L'(x)I(x \u2208 A) = L\u00a6\u00bf(x)I(x \u2208 A, L(x) > 0) + L(x)I(x \u2208 A, L(x) < 0),\nand L'(x)I(x \u2208 A, L(x) > 0) is a non-negative function, {x \u2208 A, L(x) > 0} is an interval\nwhich is disjoint with {x \u2208 A, L\u00a6(x) < 0}. Similarly as L'(x)I(x \u2208 A, L'(x) < 0), so we use\nL(x)I(x \u2208 A) in (2) instead of L(x)I(x \u2208 A, L\u00a6\u00bf(x) > 0) + L(x)I(x \u2208 A, L\u00a6(x) < 0). Then\nwe get the equation (1).\nBy equation (2), we have that\nP(x,y)~D(Sgn(F(x)) = y)\n=\n\\mathbb{P}\\left(\\text{Sgn}\\left(\\sum_{i=1}^M L_i(x)I(x \\in A_i)\\right) = y\\right)\n=\n\\sum_{i=1}^MP_{(x,y)~\\mathbb{D}}\\left(\\text{Sgn}(L_i(x)I(x \\in A_i)) = y , x \\in A_i\\right)\n(3)\n=\n\\sum_{i=1}^MP_{(x,y)~\\mathbb{D}}\\left(\\text{Sgn}(L_i(x)I(x \\in A_i)) = y\\vert x \\in A_i\\right)P_{(x,y)~\\mathbb{D}}(x \\in A_i).\nThe second equation uses Ai \u2229 Aj = 0.\nFor convenience, we use x \u2208 Rr to mean that x is an irrational number and x \u2209 Rr to mean that x is a rational number. Then, if L\u2081(x)I(x \u2208 A\u00bf) is a non-negative function, then we have\nP(x,y)~D(Sgn(Li(x)I(x \u2208 Ai)) = y|x \u2208 Ai) < P(x,y)~D(x \u2208 Rr|x \u2208 A\u2081). Moreover, we have\nthat\n\\frac{P_{(x,y)~\\mathbb{D}}(x \\in R_r\\vert x \\in A_i)}{P_{(x,y)~\\mathbb{D}}(x \\in A_i)} = \\frac{P_{(x,y)~\\mathbb{D}}(x \\in R_r,x \\in A_i)}{P_{(x,y)~\\mathbb{D}}(x \\in A_i)}\n=\n\\frac{0.5P_{(x,y)~\\mathbb{D}}(x \\in A_i\\vert x \\in R_r)}{P_{(x,y)~\\mathbb{D}}(x \\in A_i)}\n=\n\\frac{0.5P_{(x,y)~\\mathbb{D}}(x \\in A_i\\vert x \\in R_r)}{P_{(x,y)~\\mathbb{D}}(x \\in R_r)P_{(x,y)~\\mathbb{D}}(x \\in A_i\\vert x \\in R_r)+P_{(x,y)~\\mathbb{D}}(x \\notin R_r)P_{(x,y)~\\mathbb{D}}(x \\in A_i\\vert x \\notin R_r)}\n=\n\\frac{P_{(x,y)~\\mathbb{D}}(x \\in A_i\\vert x \\in R_r)}{P_{(x,y)~\\mathbb{D}}(x \\in A_i\\vert x \\in R_r)+P_{(x,y)~\\mathbb{D}}(x \\in A_i\\vert x \\notin R_r)}.\nBy (2) in the definition of D, we have P(x,y)~D(x \u2208 Aix \u2208 Rr) = P(x,y)~D(x \u2208 Aix \u2209 Rr).\nSubstituting this in equation (3), we have that P(x,y)~D(Sgn(Li(x)I(x \u2208 Ai)) = y|x \u2208 Ai) \u2264\nP(x,y)~D(x \u2208 Rr|x \u2208 A\u00bf) =\n\\frac{P_{(x,y)~\\mathbb{D}}(x \\in A_i\\vert x \\in R_r)}{P_{(x,y)~\\mathbb{D}}(x \\in A_i\\vert x \\in R_r)+P_{(x,y)~\\mathbb{D}}(x \\in A_i\\vert x \\notin R_r)} = 0.5.\nProof is similar\nwhen Li(x)I(x \u2208 A\u2081) is a non-positive function."}, {"title": "B Proof of Theorem 4.1", "content": "For the proof of this theorem, we mainly follow the constructive approach of the memorization\nnetwork in [55]. Our proof is divided into four parts."}, {"title": "B.1 Data Compression", "content": "The general method of constructing memorization networks will compress the data into a low di-\nmensional space at first, and we follow this approach. We are trying to compress the data into a\n1-dimensional space, and we require the compressed data to meet some conditions, as shown in the\nfollowing lemma.\nLemma B.1. Let D be a distribution in [0, 1]n \u00d7 {\u22121,1} with separation bound c and Dtr ~ DN.\nThen, there exist w \u2208 R and b \u2208 R such that\n(1): O(nN\u00b2/c) > wx + b > 1 for all x \u2208 [0,1]n;\n(2): wx - wz| \u2265 4 for all (x, 1), (z, \u22121) \u2208 Dtr.\nTo prove this lemma, we need the following lemma.\nLemma B.2. For any v \u2208 R and T > 1, let u \u2208 Rn be uniformly randomly sampled from the\nhypersphere Sn\u22121. Then we have P(|(u, v)|<\\frac{\\frac{8}{\\eta \\pi}}{T})<\\frac{8}{\\eta T}.\nThis is Lemma 13 in [49]. Now, we prove the lemma B.1.\nProof. Let co = min(x,\u22121),(2,1)\u2208Dtr ||x - 2||2. Then, we prove the following result:\nResult R1: Let u \u2208 Rn be uniformly randomly sampled from the hypersphere Sn-1, then there are\nP(\\vert <u, (x \u2013 z)>\\vert > \\sqrt{\\frac{c_o}{4 \\mathbb{N}^2} \\frac{8}{\\eta \\pi}}, \\forall (x,-1), (z,1) \\in D_{tr}) > 0.5.\n8\n\u03b7\u03c0\nBy lemma B.2, and take T = 4N2, for any x, z which satisfies (x, \u22121), (z, 1) \u2208 Dtr, we have that:\nlet u \u2208 Rn be uniformly randomly sampled from the hypersphere Sn\u22121, then there are P(|(u, (x\n\u2212 z)>| < \\sqrt{\\frac{c_o}{4 \\mathbb{N}^2} \\frac{8}{\\eta \\pi}} ) < \\frac{8}{\\eta \\frac{4 \\mathbb{N}^2}{8}}, using \\Vert x-z\\Vert_2\\geq c_o here. So, it holds\n\u03b7\u03c0\n8\n1\u2013\\sum_{(x,-1),(z,1)\\in \\mathbb{D}_{tr}} \\mathbb{P}(\\vert <u,(x-z)>\\vert < \\sqrt{\\frac{c_o}{4 \\mathbb{N}^2} \\frac{8}{\\eta \\pi}}) \\\\\n> \\mathbb{P}(\\vert <u,(x-z)>\\vert \\geq \\sqrt{\\frac{c_o}{4 \\mathbb{N}^2} \\frac{8}{\\eta \\pi}}, \\forall (x,-1), (z,1) \\in D_{tr})> \\mathbb{P}(\\vert <u,(x-z)>\\vert \\geq \\sqrt{\\frac{c_o}{4 \\mathbb{N}^2} \\frac{8}{\\eta \\pi}}, \\forall (x,-1), (z,1) \\in D_{tr})> 1-\\sum_{(x,-1),(z,1)\\in \\mathbb{D}_{tr}} \\mathbb{P}(\\vert <u,(x-z)>\\vert < \\sqrt{\\frac{c_o}{4 \\mathbb{N}^2} \\frac{8}{\\eta \\pi}})\n1-\\frac{2 \\mathbb{N}^2}{4 \\mathbb{N}^2} =0.5\nWe proved Result R1.\n8\n\u03b7\u03c0\nIn practice, to find such a vector, we can randomly select a vector u in hypersphere Sn-1, and verify\nthat if it satisfies |<u, (x - z)) | \u2265 \\sqrt{\\frac{c_o}{4 \\mathbb{N}^2} \\frac{8}{\\eta \\pi}}\\sqrt{}, (x,-1), (z, 1)\u2208 Dtr. Verifying such a fact needs\npoly(B(Dtr)) times. If such a u is not what we want, randomly select a vector u and verify it again.\nIn each selection, with probability 0.5, we can get a vector we need, so with ln 1/\u20ac times the selec-\ntions, we can get a vector we need with probability 1 \u2212 \u20ac.\nConstruct w, b and verify their rationality\nBy the above result, we have that: there exists a u\u2208 Rn such that ||u||2 = 1 and |<u, (x \u2013 z))| \u2265\nC\n-\u221a(x, -1), (2,1) \u2208 Dtr, and we can find such a u in poly(B(Dtr), ln(1/6)) times.\n\u03b7\u03c0\n8\n4N2"}, {"title": "B.2 Data Projection", "content": "The purpose of this part is to map the compressed data into appropriate values.\nLet w \u2208 Rn and b \u2208 R be given and Dtr = {(xi", "v[": "where {v[", "N[\u221aN": "R+ are given values. This network has O(\u221aN)\nparameters and width 4", "N": ".", "argmaxj\u2208[v": {"j/\u221aN": "i"}, "F": "nThe 2i + 1 hidden layer has width 4", "is": "n(\\mathcal{F}^{2i+1})_1(x) = Relu((\\mathcal{F}^{2i})_2(x) - (x_{t(i)}+1) +\\frac{2q_{t(i)}}{3});\n(\\mathcal{F}^{2i+1})_2(x) = Relu((\\mathcal{F}^{2i})_2(x) - (x_{t(i)}+1) +\\frac{q_{t(i)}}{3});\n(\\mathcal{F}^{2i+1})_3(x) = Relu((\\mathcal{F}^{2i})_1(x));\n(\\mathcal{F}^{2i+1})_4(x) = Relu((\\mathcal{F}^{2i})_2(x)).\nFor the case i = 0, let (F\u00ba)2(x"}]}