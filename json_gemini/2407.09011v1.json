{"title": "One Stone, Four Birds: A Comprehensive Solution for QA System Using Supervised Contrastive Learning", "authors": ["BO WANG", "TSUNENORI MINE"], "abstract": "This paper presents a novel and comprehensive solution to enhance both the robustness and efficiency of question answering (QA) systems through supervised contrastive learning (SCL). Training a high-performance QA system has become straightforward with pre-trained language models, requiring only a small amount of data and simple fine-tuning. However, despite recent advances, existing QA systems still exhibit significant deficiencies in functionality and training efficiency. We address the functionality issue by defining four key tasks: user input intent classification, out-of-domain input detection, new intent discovery, and continual learning. We then leverage a unified SCL-based representation learning method to efficiently build an intra-class compact and inter-class scattered feature space, facilitating both known intent classification and unknown intent detection and discovery. Consequently, with minimal additional tuning on downstream tasks, our approach significantly improves model efficiency and achieves new state-of-the-art performance across all tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "UESTION answering (QA) system is a well-established and important application widely used in areas such as healthcare [1], finance [2] and e-commerce [3]. Despite the world gradually moving into the era of large language models (LLMs), classic QA systems still maintain a strong presence due to their low cost, ease of deployment, and high accuracy in answering questions.\n\nA typical QA system mainly comprises two components: user intent classification and answer retrieval. The efficiency of the former is crucial, as a correct response often depends on accurately determining the user's intent. To achieve robust classification, it is essential to efficiently extract features from user input, providing intent-specific and discriminative text representations for classification. Initially, text features were extracted manually using techniques like term frequency-inverse document frequency (TF-IDF [4]) or bag-of-words (BoW [5]). With the advent of neural networks, feature ex-traction became an automated process [6], [7]. Today, the power of pre-trained Transformers like BERT [8] has made it much easier to extract features and build classification models, requiring only a small amount of data with Cross-Entropy (CE) loss fine-tuning [9], [10].\n\nIt seems that the current QA system building is simple and mature, with nearly nothing to be changed. However, from the perspective of building a comprehensive system, current QA systems still face unresolved issues at both internal and external levels, which have not been fully addressed yet:\n\n\u2022 Externally, or at the system level: Current QA systems typically adopt a \"training-as-complete\" policy, meaning that once the model has finished training, its knowledge is not updated. This approach leaves the system vulnerable to unseen, unknown inputs (as out-of-domain, OOD inputs [11]). To maintain the robustness and security of the model, there should be mechanisms to handle these inputs. Furthermore, even when new data is collected to upgrade the system, the common practice is to re-train a new model from scratch, leading to a waste of labor and resources.\n\n\u2022 Internally, or in training efficiency: Increasing studies indicate that the BERT+CE training combination still has efficiency problems. For BERT, the raw text feature distribution is anisotropic [12]. Additionally, when using CE as the fine-tuning loss, text features of different intents often remain mixed with unclear boundaries in the embedding space [13]. Both issues hinder training efficiency and model performance.\n\nIn this paper, aiming to build a more robust QA system, we propose a novel and comprehensive solution to address both issues. To tackle the systematic issue, including the basic intent classification function, we define four key tasks and design a workflow between them (as shown in Figure 1), enabling the system to be self-consistent and recurrently updatable. Specifically, the tasks are:\n\nT-1 User input intent classification: The fundamental task for a QA system.\n\nT-2 OOD input detection: Detect abnormal inputs to prevent misleading output. The detected OOD texts are saved as source data for Task 3.\n\nT-3 New intent discovery: Utilize detected OOD texts to discover new possible intents once they accumulate to a sufficient quantity.\n\nT-4 Continual learning: Use newly discovered and pseudo-labeled data to incrementally train the initial classifica-tion model, enhancing the system's capability to handle the latest inquiries.\n\nTo address the training efficiency problem, we leverage supervised contrastive learning (SCL, [14]) to replace CE fine-tuning as the unified text representation optimization method, which requires minimal tuning to achieve all four task objectives. We choose SCL because, in our previous research on OOD detection [15], it demonstrated excellent capability in upstream feature optimization, allowing an ac-curate OOD text detection using only a simple distance-based algorithm in downstream.\n\nMore specifically, the advantage of SCL over CE fine-tuning lies in its ability to enhance both learning efficiency and feature separation. As shown in Figure 2-b), SCL em-ploys a simple but clear optimization target: pulling same-label text features together and pushing different ones fur-ther apart, resulting in an intra-class compact and inter-class"}, {"title": "II. RELATED WORKS", "content": "A. REPRESENTATION LEARNING IN INTENT CLASSIFICATION MODEL BUILDING\n\nEarly history of representation learning\n\nRepresentation learning (RepL), or feature learning, is a cru-cial process in building machine learning models. Good RepL methods capture the intrinsic patterns and structures of the data, thus providing a solid foundation for downstream tasks.\n\nInitially, RepL research focused on manually extracting meaningful features from raw data, with preliminary meth-ods including TF-IDF [4] and BoW [5]. Subsequently, the development of word embeddings extracted via neural net-"}, {"title": "III. PROBLEM AND TASK DEFINITION", "content": "As mentioned in Section I, we introduce four basic tasks to build a comprehensive and adaptive QA system (Figure 1). Here, before presenting the specific solutions, we formally define the problems and targets of each task.\n\nA. PRELIMINARY\n\nData: Suppose we initially have a set of known data \\(S_{known} = \\{x_i, y_i | y_i \\in C_{kn}\\}, where \\(x_i\\) is the i-th user inquiry text, \\(y_i\\) is the corresponding label, and \\(C_{kn}\\) is the label set of known intents. This data serves as the pre-training data, which is also referred as the In-Domain, IND examples, i.e., \\{x_i, y_i | y_i \\in C_{kn}\\} \\in D_{IND}\n\nWe also have a set of label-unknown data \\(S_{unlabeled} = \\{x_j, y_j | y_j \\in \\{C_{kn} + C_{uk}\\}\\), which refers to the real, daily user inputs. Note that besides the known intents, there are also"}, {"title": "B. T-1: USER INTENT CLASSIFICATION", "content": "The target of this task is to find an accurate mapping:\n\\[f_{cls}: ENC(x_i) \\rightarrow \\hat{y_i}   \\hat{y_i} \\in C_{kn}\\]\nFor traditional methods like CE-fine-tuning, this is achieved by further adding a fully-connected layer to the end of ENC and training them together using CE loss. In the proposed method, the encoder is pre-trained using SCL (refer to Section IV-A for details) and the parameters are fixed in the following tasks T-1/2/3. \\(f_{cls}\\) is realized by a nearest neighbor algorithm (Section IV-B1)."}, {"title": "C. T-2: OUT-OF-DOMAIN DETECTION", "content": "The target of this task is to determine whether an example \\(x_j\\) from \\(S_{unlabeled}\\) belongs to \\(D_{IND}\\) or \\(D_{OOD}\\). This is usually realized by comparing the detection score \\(sco_j\\) given by a detection algorithm \\(f_{det}\\) with an empirically-set threshold \\(\\lambda\\):\n\\[sco_j = f_{det}(ENC(x_j))\\]\n\\[x_j \\in \\begin{cases} D_{OOD}, \\text{ if } sco_j \\geq \\lambda \\\\ D_{IND}, \\text{ if } sco_j < \\lambda \\end{cases}\\]\nHere, the detected \\(x_{ood}^{j}\\) are saved for T-3 (new intent discov-ery), while the detected \\(x_{ind}\\) will be classified normally in T-1 and also saved for the final task, T-4 (continual learning)."}, {"title": "D. T-3: NEW INTENT DISCOVERY", "content": "Once the collected OOD texts \\(x_{ood}^{j}\\) reach a sufficient quantity, an algorithm \\(f_{nid}\\) is used to cluster and assign pseudo labels \\(\u0177_j' \\) to them, as the \u201cnew IND intents\u201d in \\(C_{kn'}\\):\n\\[f_{nid}: ENC(x_{ood}^{j}) \\rightarrow \\hat{y}_{j'}   \\hat{y}_{j'} \\in C_{kn'}\\]"}, {"title": "E. T-4: CONTINUAL LEARNING", "content": "Finally, using both the known IND data \\(\\{x_{j'}, \\hat{y}_{j'} | \\hat{y}_{j'} \\in C_{kn}\\}\\) and newly discovered \u201cIND\u201d data \\(\\{x_{j'}, \\hat{y}_{j'} | \\hat{y}_{j'} \\in C_{kn'}\\}\\), ENC is re-trained to obtain the ability of classifying both old and new intent inquires.\n\\[f_{cls-new}: ENC_{new}(x_k) \\rightarrow \\hat{y}_k    \\hat{y}_k \\in \\{C_{kn} + C_{kn'}\\}\\]"}, {"title": "IV. METHODOLOGY", "content": "In this section, we introduce the methodology of SCL as the RepL method in the pre-training stage, followed by the downstream solutions to each task.\n\nA. STAGE 1: SUPERVISED CONTRASTIVE LEARNING -BASED PRE-TRAINING\n\n1) Principle and optimizing effect of SCL\n\nAs discussed in Section II, BERT+CE fine-tuning exhibits issues with local loose borders and a global anisotropic dis-tribution in RepL, while SCL has the potential to solve both of these problems.\n\nSpecifically, as illustrated in Figure 2, for the local as-pect, compared with CE fine-tuning, SCL applies a more straightforward yet effective target: pulling the representa-tions of similar examples (positive examples) together and pushing dissimilar ones (negative examples) further apart. To tackle the global distribution problem, SCL further generates \u201cview\u201d augmentations as additional positive examples be-fore the contrasting process, optimizing features in a boarder scale. Together, these strategies provide a better supervision signal to arrange text representations to form compact and distinct clusters, enhancing both the alignment and uniformity of feature space, and thus sharpening the distinctions between texts from various intents.\n\nThis well-organized embedding space (Figure 2-b) pro-vides a strong foundation that benefits all downstream tasks:\n\nT-1: The clear separations between each intent cluster facili-tate easy and accurate classification.\n\nT-2: The compact and distinct IND clusters highlight the differences between them and OOD texts, which do not belong to any known intents.\n\nT-3: Although not trained directly with OOD data, the two optimization directions of SCL still drive the aggrega-tion of semantically similar features. This creates an environment conducive to intent discovery algorithms, making it easier to identify the clustered gathered data.\n\nT-4: The accurately discovered new intent data, pre-organized feature space, and SCL-based re-training, together ensure the high-performance of the updated system.\n\n2) Generation of text \"views\"\n\nAs discussed in the last subsection, the effective optimiza-tion of SCL is attributed to both the supervised positive-negative movement and the unsupervised \"view\" augmenta-tion. This augmentation also serves as a precaution against the"}, {"title": "3) Formal learning procedure definition", "content": "Here we give the formal definition of an SCL procedure:\n\nStep 1: Text \"View\" generation\n\nBefore the learning process, two text embedding views are generated for each example in the known dataset \\(S_{known} = \\{x_i, y_i | y_i \\in C_{kn}\\}\\). This is achieved by enabling the dropout option in network training settings and inputting the same sentence twice:\n\\[h_{2i'} = ENC_{dropout1}(x_i), h_{2i'+1} = ENC_{dropout2}(x_i)\\]\nHere, since the two view embeddings are derived from the same text, we assign them the same label as \\(x_i\\), that is: we get two views \\(\\{(h_{2i'}, y_{2i'}), (h_{2i'+1}, y_{2i'+1})\\}\\), where \\(y_{2i'} = y_{2i'+1} = y_i', i' \\in \\{1, 2, ..., N\\}\\).\n\nStep 2: Supervised contrastive learning\n\nThe SCL training loss is defined as follows:\n\\[L_{SCL} = \\sum_{i'=1}^{2N} \\frac{-1}{|Pos(i')|} \\sum_{p \\in Pos(i')} log \\frac{exp(\\frac{h_i' h_p}{t})}{\\sum_{q \\in Neg(i')} exp(\\frac{h_i' h_q}{t})}\\]\nwhere \\(i' \\in \\{1, 2, ..., 2N\\}\\) is the index of current sample (the anchor text). \\(Pos(i')\\) is the index set of all positive examples to the anchor text in the mini-batch, i.e., \\(y_p = y_{i'}\\), and \\(Neg(i')\\) is the index set of all negative examples, i.e., \\(y_q \\neq y_{i'}\\). t is the temperature factor to amplify the loss output. To calculate text similarity directly using dot product, we apply L2 normaliza-tion to all representations before training."}, {"title": "B. STAGE 2: SOLUTIONS FOR DOWNSTREAM TASKS", "content": "With a well-organized embedding space achieved through SCL, we can complete all the tasks with the least efforts.\n\n1) Solution for T-1 intent classification\n\nWe apply a Mahalanobis distance (MDist) - nearest neigh-bor (NN)-based method for IND intent classification, which does not require any additional training. We use MDist be-cause, compared to Euclidean distance, MDist considers the distribution of each feature component, providing a more ac-curate measurement of text similarity. Additionally, to further"}, {"title": "a) Class centroid calculation", "content": "The class centroid is the average data point of all text embeddings belonging to this intent:\n\\[\\bar{h_c} = \\frac{1}{num(c)} \\sum_{i=1}^{num(c)} h_{ci}\\]"}, {"title": "b) MDist calculation", "content": "We calculate the covariance matrix S over all data (eq. 10) and then calculate MDist (eq. 11).\n\\[S = \\frac{1}{|C|} \\sum_{i \\in C} (H - \\bar{H})^T (H - \\bar{H})\\]\n\\[MDist(h_{test}, h_c) = \\sqrt{(h_{test} - h_c) S^{-1} (h_{test} - h_c)^T}\\]\nwhere \\(H_c = [h_{c,1}, h_{c,2}, ..., h_{c,num(c)}]^T\\) is the sample matrix of category c, \\(H = [1, 1, .., 1] \\bar{h_c}\\).\n\nc) Label determination\n\nFinally, the label is determined by the nearest centroid:\n\\[\\hat{y}_{test} = arg \\min_{c \\in C} MDist(h_{test}, h_c)\\]"}, {"title": "2) Solution for T-2 out-of-domain detection", "content": "The solution for T-2 is very simple: we still calculate MDist of \\(x_{test}\\) to the nearest intent centroid as T-1, but the distance is used directly as the detection sco:\n\\[SCO(x_{test}) = min_{c \\in C} MDist(h_{test}, h_c)\\]\nThe distance itself can serve as the sco and be an effective detection indicator, since a larger value means that even tak-ing the shortest distance, current example is still far from / has less similarity to any known, IND category centroids. As shown in Figure 2-T-2, this suggests that the current \\(x_{test}\\) is more likely to be an unseen, i.e., OOD text.\n\nIn practical usage, as introduced in Section III-C, sco is fur-ther compared with a threshold A, and texts with an exceeding distance will be judged as OOD examples."}, {"title": "3) Solution for T-3 new intent discovery", "content": "To conduct intent discovery, the first step is to obtain the OOD examples to be clustered. Here, we first find a threshold A with a policy that when the model in T-2 gets a true positive rate, TPR=90% in T-2 on validation data (i.e., 90% OOD examples are correctly recognized), then we filter out OOD texts by comparing sco with this \\(\\lambda\\).\n\nNext, since SCL has provided a well-structured feature space, we apply KMeans, one of the simplest unsupervised clustering algorithms, to solve T-3. Taking the detected OOD samples \\(x_{ood}^{j}\\), KMeans clusters and assigns pseudo-labels \\(\\hat{y}_{j'}\\)"}, {"title": "4) Solution for T-4 continual learning", "content": "In the final task, the initial ENC will be re-trained with both detected IND examples \\(\\{x_{ind}^{j'}, \\hat{y}_{j'} | \\hat{y}_{j'} \\in C_{kn}\\}\\) (labeled by the ini-tial model) and detected OOD examples \\(\\{x_{ood}^{j'}, \\hat{y}_{j'} | \\hat{y}_{j'} \\in C_{kn'}\\}\\) (pseudo-labeled in T-3) to prevent forgetting. The training method here is still SCL (same as in Stage-1, Section IV-A3) but with an expanded label set \\(\\{C_{kn} + C_{kn'}\\}\\) without further modifications. After the training, the model gains the ability to handle inquiries from new intents."}, {"title": "V. EXPERIMENT", "content": "A. BASELINES\n\nWe apply the following latest and competitive methods as baselines for evaluation.\n\nFine-tuning, FT (All tasks): Despite being the most common training method, CE-based fine-tuning remains one of the most powerful methods actively used among all tasks [16], [31], [41]. Thus this time we applied it as a baseline of RepL, and tested the effects throughout all four tasks. The downstream detection and clustering method are all the same as SCL.\n\nPTO-Sup [38] (T-2): This work proposed a Prefix Tuning-based OOD detection method, training a generative GPT-2 model on IND data and then using the likelihood as sco. It assumes that the unseen OOD text should let model output a lower likelihood. This method originally supports unsu-pervised learning, but for a fair comparison, we applied its supervised mode with IND labels available.\n\nLayer-LOF [27] (T-2): This paper addressed that pre-trained language models like BERT may over-rationalize the input. That is, even it is an OOD input, the last-layer embedding may tend to be more like IND. Based on this observation, it proposed to consider a decision of multiple layers after fine-tuning, and finally used local outlier factor (LOF) with a voting mechanism for final decision. In this paper, we simply averaged its best-four layer results as the final result.\n\nMTP-KNNCL [44] (T-3): The state-of-the-art NID method that first pre-trains the model with a Masked Language Mod-eling (MLM) target to obtain prior semantic knowledge, then performs KNN contrastive learning for better cluster aggre-gation. The downstream clustering method is KMeans.\n\nDPN [45] (T-3): This Decoupled Prototypical Network first initializes some prototypes for known and unknown intents and aligns them as a bipartite matching problem. Unaligned prototypes represent new intents, and all embeddings are fur-ther optimized using semantic-aware prototypical learning. The downstream method is also KMeans.\n\nB. DATASETS\n\nWe evaluate the methods using three popular intent clas-sification datasets. To measure the performance of T-3 and T-4 quantitatively, we randomly select approximately 75% of"}, {"title": "V. CONCLUSION", "content": "In this paper, we addressed the functionality and efficiency problems of current QA system, and proposed our solution to solve them and build a comprehensive system. We first defined four key tasks to complement the system's function-ality shortcomings, and then applied SCL as the unified repre-sentation learning method with minimal additional tuning to achieve the task targets. The experiment results demonstrated that although being simple and straightforward, the proposed method still achieved leading results in each task.\n\nAs this complete solution is first proposed and is still a prototype, there are still many situations needed to be con-sidered for real deployment in the future. For example, there may also be some noisy inputs appearing, which are hard to be categorized into any intents. In such cases, developing a filtering algorithm before the intent discovery process might be necessary. In addition, to show efficiency, this time the downstream methods were all chosen to be as simple as pos-sible, while more advanced methods could potentially yield even better results when combined with SCL."}, {"title": "APPENDIX A KMEANS DETAILS", "content": "The KMeans learning process is shown in Algorithm 1."}, {"title": "Algorithm 1 KMeans clustering", "content": "Nood\nInput: Text embeddings \\(h_{j'}\\)_{j'=1} of detected \\(\\{x_{j'}\\}\\) in T-2, number of clusters K\n\nOutput: Label assignments \\(\\{\\hat{y}_{j'}\\}\\)\n1: Initialize K cluster centroids \\(\\{\\mu_{\\kappa}\\}_{\\kappa=1}\\) randomly.\n2: repeat\n3:  for each data point \\(h_j\\), do\n4:   Assign \\(h_j\\) to the nearest cluster centroid:\n\\[\\hat{y}_{j'} = arg \\min_{\\kappa \\in \\{1,2,...,K\\}} ||h_{j'} - \\mu_{\\kappa} ||^2\\]\n5:  end for\n6:  for each cluster k do\n7:   Update \\(\\mu_{\\kappa}\\) as the mean of all points assigned to cluster k:\n\\[\\mu_{\\kappa} = \\frac{1}{|num(cate \\kappa)|} \\sum_{j \\in num(cate \\kappa)} h_{j\\kappa}\\]\n8:  end for\n9: until the cluster assignments \\(\\{\\hat{y}_{j'}\\}\\) do not change or the centroids \\(\\{\\mu_{\\kappa}\\}\\) converge\nNood\n10: return Label assignments \\(\\{\\hat{y}_{j'}\\}_{j'=1}\\) Nood"}, {"title": "APPENDIX B METRIC CALCULATION DETAILS", "content": "Here we show the detailed calculation process of the used metrics.\n\nA. FOR TASK 1 AND 4: F1 VALUES\n\nThe Micro and Macro F1 scores are calculated as follows:\n\nMicro F1 score: is calculated by summarizing the classifi-cation situations over all classes. It is the harmonic mean of micro-averaged precision (P) and recall (R):\n\\[Micro F1 = \\frac{2 \\cdot PR}{P+R}\\]\nMacro F1 score: calculates the F1 score independently for each class and then takes the average, which treats all classes equally:\n\\[Macro F1 = \\frac{1}{C} \\sum_{c=1}^{C} F1-score\\]\nwhere C is the number of categories.\n\nB. FOR TASK 3: NMI, ARI, ACC\n\nThe NMI, ARI, ACC are calculated as follows:\n\nNormalized Mutual Information (NMI): is a measure of the similarity between two sets of clusters:\n\\[NMI(U, V) = \\frac{2 \\cdot I(U; V)}{H(U) + H(V)}\\]\nwhere I (U; V) is the mutual information between clusters U and V, and H(U) and H(V) are the entropies of U and V respectively. It ranges from 0 (no mutual information) to 1 (perfect correlation)."}, {"title": "Adjusted Rand Index (ARI):", "content": "Adjusted Rand Index (ARI): measures the similarity be-tween two data clusterings by considering all pairs of sam-ples, and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings:\n\\[ARI = \\frac{\\sum_{ij} \\binom{n_{ij}}{2} - [\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}] / \\binom{n}{2}}{[\\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2}] - [\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}] / \\binom{n}{2}}\\]\nwhere \\(n_{ij}\\) is the number of elements in the intersection of clusters i and j, \\(a_i\\) is the sum over row i, and \\(b_j\\) is the sum over column j.\n\nClustering Accuracy (ACC): evaluates the clustering result after finding the best one-to-one mapping between cluster labels and true labels:\n\\[ACC = \\frac{\\sum_{i=1}^n \\delta(y_i, map(\\hat{y}_i))}{n}\\]\nwhere \\(y_i\\) is the true label, \\(\\hat{y}_i\\) is the cluster label, map() is the mapping function where we use the Hungary algorithm this time, and \\(\\delta\\) is the Kronecker delta function."}, {"title": "APPENDIX C ABBREVIATIONS", "content": "Table 11 shows the abbreviations that have appeared in this paper and their complete names."}, {"title": "APPENDIX D LEARNING AND EVALUATION PROCEDURE (ALGORITHM FORM)", "content": "The learning and evaluation procedures of pre-training and each task are shown in Algorithm 2."}, {"title": "Algorithm 2 Overall process", "content": "Input: Train data, Val data, Test I data, Test II data\n\nOutput: Solutions for T-1 to T-4\n\n-Pre-training Stage-\n\n2: Epoch \u2190 0, AUROCbest-val \u2190 0\n\n3: while Epoch < 20 do\n\n4:  Train ENC on Xtrain using SCL loss object (eq. 8)\n\n5:  Calculate OOD detection AUROCval on this epoch\n\n6:  if AUROCval > AUROCbest-val then\n\n7:   Save model of this epoch, and record the thresh-old A when TPR=90%\n\n8:  end if\n\n9: end while\n\n-Task 1-\n\n11: Input Xtest I (IND part) to ENC to obtain the representa-tions:\n\n12:  htest I = ENC (xtest-1)\n\n13: The label is determined as the label of the nearest IND cluster centroid:\n\n14:  \\(\\hat{y}_{test I} = arg \\min_{c \\in c} MDist (h_{test I}, h_c)\\)\n\n15: Calculate Micro, Macro F1 values\n\n-Task 2-\n\n17: Input Xtest-1 (IND, OOD part) to ENC to obtain the repre-sentations and calculate the detection scores:\n\n18:  \\[SCO(x_{test 1}) = min MDist (h_{test 1}, h_c)\\]\n\n19: Calculate FPR, TPR results on various thresholds A, and finally AUROC, AUPR, FPR90 values\n\n-Task 3 preparation-\n\n21: Determine \\(\\lambda_{filter}\\) when TPR=90% on xval\n\n22: Divide Xtest-1 into Xtest-1, IND and Xtest-1, OOD two parts by comparing detection sco and \\(\\lambda_{filter}\\)\n\n-Task 3-\n\n24: Apply KMeans on Xtest-1, OOD and give new pseudo labels to xtest-1, OOD as \\(\\hat{y}_{test-1, OOD}\\) \u2208 Ckn'\n\n-Task 4-\n\n25: Calculate NMI, ARI, ACC metrics\n\n26:  -Task 4-\n\n27: Give pseudo labels to xtest-1, IND using the initial ENC as  \\(\\hat{y}_{test-1, IND}\\) \u2208 Ckn\n\n28: Train ENC on \\(\\{x_{test-1}, \\hat{y}_{test-1}\\}, \\hat{y}_{test-1}\u2208\\{C_{kn}+C_{kn'}\\}\\) using SCL loss (eq. 8)\n\n29: Test on xtest II and calculate Micro, Macro F1 values\n\n30: END"}]}