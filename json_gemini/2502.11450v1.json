{"title": "Fishing For Cheap And Efficient Pruners At Initialization", "authors": ["Ivo Gollini Navarrete", "Nicolas Mauricio Cuadrado", "Jose Renato Restom", "Martin Tak\u00e1\u010d", "Samuel Horv\u00e1th"], "abstract": "Pruning offers a promising solution to mitigate\nthe associated costs and environmental impact of\ndeploying large deep neural networks (DNNs).\nTraditional approaches rely on computationally\nexpensive trained models or time-consuming itera-\ntive prune-retrain cycles, undermining their utility\nin resource-constrained settings. To address this\nissue, we build upon the established principles\nof saliency (LeCun et al., 1989) and connection\nsensitivity (Lee et al., 2018) to tackle the challeng-\ning problem of one-shot pruning neural networks\n(NNs) before training (PBT) at initialization. We\nintroduce Fisher-Taylor Sensitivity (FTS), a com-\nputationally cheap and efficient pruning criterion\nbased on the empirical Fisher Information Matrix\n(FIM) diagonal, offering a viable alternative for\nintegrating first- and second-order information to\nidentify a model's structurally important parame-\nters. Although the FIM-Hessian equivalency only\nholds for convergent models that maximize the\nlikelihood, recent studies (Karakida et al., 2019)\nsuggest that, even at initialization, the FIM cap-\ntures essential geometric information of param-\neters in overparameterized NNs, providing the\nbasis for our method. Finally, we demonstrate\nempirically that layer collapse, a critical limita-\ntion of data-dependent pruning methodologies, is\neasily overcome by pruning within a single train-\ning epoch after initialization. We perform experi-\nments on ResNet18 and VGG19 with CIFAR-10\nand CIFAR-100, widely used benchmarks in prun-\ning research. Our method achieves competitive\nperformance against state-of-the-art techniques\nfor one-shot PBT, even under extreme sparsity\nconditions. Our code is made available to the\npublic\u00b9.", "sections": [{"title": "1. Introduction", "content": "We hear of breakthroughs daily thanks to Artificial Intelli-\ngence (AI) systems. The exponential increase in comput-\ning power and data availability has allowed success stories\nin robotics (Soori et al., 2023), computer vision (Khan &\nAl-Habsi, 2020), natural language processing (Torfi et al.,\n2020), healthcare (Habehh & Gohel, 2021), and many other\nfields. Naturally, new challenges have emerged alongside\nthis progress, particularly the growing size of AI systems\nand the associated high training and inference costs (Han\net al., 2015). The \"bigger is better\" approach goes against\nreal-life applications that require high accuracy and efficient\nresource usage. Furthermore, climate change awareness\nhighlights the environmental impact of AI, with inference\nbeing a significant contributor to model-associated carbon\nemissions (Wu et al., 2022; Chien et al., 2023). In addition,\ncomputational resources, energy, and bandwidth limit the\ndeployment of large systems on edge devices (Cheng et al.,\n2024).\nTo overcome these challenges, researchers have developed\ntechniques to address the constantly growing model sizes\nand associated costs of Deep Neural Networks (DNNs).\nThe most prominent are quantization (Dettmers et al., 2023),\nlow-rank factorization (Denton et al., 2014), knowledge dis-\ntillation (Xu et al., 2024), neural architecture search (Zhang\net al., 2021), and neural network pruning (Cheng et al.,\n2024). The latest stands out for its ability to dramatically\ndecrease the model size, reducing storage memory for mod-\nels and computation workload during training or inference\nwhile maintaining performance compared to the original\nnetwork. Cheng et al. (2024) proposed a comprehensive tax-\nonomy highlighting different aspects to consider in existing\npruning techniques based on three questions: (1) Does the\nmethod achieve universal or specific acceleration through\nneural network pruning? (2) When does the pruning hap-\npen in the training pipeline? and (3) Is there a predefined\npruning criterion, or is it learnable?\nRegardless of the kind of acceleration or criterion, prun-\ning after training (PAT) methods are investigated more fre-\nquently given the advantage of working with converged\nmodels, which implies parameters whose values are close to\noptimal and provide more information than randomly initial-\nized ones (Kumar et al., 2024). A simple but very effective"}, {"title": "2. Background", "content": "N\nProblem Setting. This research focuses on unstructured\npruning techniques in a supervised learning setting. Thus,\nwe assume access to the training set D = {(xn, Yn)}n=1,\ncomposed of tuples of input xn \u2208 X and output Yn \u2208 V.\nThe goal is to learn a model parameterized by w \u2208 Rd that\nmaps f : X \u2192 Y by minimizing an objective function:\n\nL(w) = 1/N \\sum_{n=1}^{N} l(yn, f(xn; w)).\n\nRegardless of the kind of acceleration or criterion, prun-\ning after training (PAT) methods are investigated more fre-\nquently given the advantage of working with converged\nmodels, which implies parameters whose values are close to\noptimal and provide more information than randomly initial-\nized ones (Kumar et al., 2024). A simple but very effective"}, {"title": "3. Methodology", "content": "FIM at initialization. The equivalence between FIM and\nHessian described in (7) only holds when the parameter\nvector w is well specified. In other words, w maximizes\nthe likelihood E[Vlog p(y | x, w)] = 0. However, with\nrandomly initialized parameters, the FIM-Hessian equiva-\nlence does not hold. Despite this, we argue that the FIM\nremains a valuable approximation for second-order informa-\ntion at initialization. Our claim is based on the findings of\nKarakida et al. (2019) for FIM in overparameterized DNNs\nwith randomly initialized weights and large width limits.\nThey demonstrate that even at initialization, the FIM cap-\ntures essential geometric properties of the parameter space.\nTheir study revealed that while most FIM eigenvalues are\nclose to zero and indicate local flatness, a few are signifi-\ncantly large and induce strong distortions in certain direc-\ntions. This suggests that even at initialization, the FIM could\noffer a notion of the parameters that significantly perturb\nthe objective function, in other words, the sensitivity. This\nis an important attribute as we claim that in the PBT setting,\nwe would rather preserve connections with the potential of\nimpacting the likelihood of the model.\nFisher-Taylor Sensitivity. Considering the effectiveness of\nthe saliency metric in identifying important parameters for\npreservation during pruning operation, the natural extension\nis to adjust the measurement for pruning at different training\npoints. We follow OBD (LeCun et al., 1989) and approxi-\nmate the objective function L using a Taylor series, where a\nperturbation \u03b4w of the parameter vector will change L by\n\ndL = L(w) \u2013 L(w + \u03b4\u03c9),\n= \u03b4\u03c9\u2207L(w) + 1/2 \u03b4\u03c9 \u0397\u03b4\u03c9 + \u039f(||\u03b4\u03c9||3).\n\nSince our goal is to prune at the initialization, neglecting\nthe first-order term is not viable as the model parameters\nare not yet optimized. Still, we operate assuming that the\nlocal error is quadratic to discard higher-order components.\nHowever, the perturbation model in (13) still faces the high\ncomputational cost of computing the Hessian. Additionally,\nwe cannot ensure that it is positive semidefinite (PSD) for\nrandomly initialized weights. This leads to misleading eval-\nuations of parameter importance, as negative eigenvalues in\nthe Hessian may suggest the presence of saddle points or\nnon-convex regions.\nTo address the computational cost of computing the Hessian,\nwe replace it with the FIM, a cheap and efficient approxi-\nmation of the second-order information. The FIM has the\ndesirable property of being PSD by construction, ensuring\na stable representation of the importance of the parameters.\nWe further simplify the approximation using the FIM di-\nagonal. This approach assumes that the sum of SL if the\nparameters are deleted individually equals the change in L\nif the parameters are removed simultaneously. Substituting\nthe FIM diagonal into (13), the Taylor series approximation\nbecomes:\n\u03b4\u00a3 = \\sum_{qEQ} \u03b4\u03c9\u03b1 \\frac{\\partial L(w)}{\\partial wq} + 1/2 \\sum_{qEQ}\u03b4\u03c9 Fqq .\n\nOur interest is to discover important parameters in the model\nfor one-shot pruning at initialization. We consider the term\nsensitivity more suitable than saliency for our setting be-\ncause induced perturbations will cause L to increase, de-\ncrease, or stay the same, unlike pruning a converged model\nwhere perturbation only increases or preserves L. Therefore,\nwe take the magnitude of (14) as our sensitivity criterion.\nNote that a high magnitude means that the parameter wa\nsignificantly changes the objective function (either positive\nor negative), and it must be preserved for the pruned model\nto learn. Based on this, we define Fisher-Taylor Sensitivity\n(FTS) as our score of parameter importance:\nSq = |\u03b4\u00a3| =|\u03b4\u03c9\\frac{\\partial L(w)}{\\partial wq} + 1/2 \u03b4\u03c9 Fqq|\n\nBatch-wise FIM Estimation. In practice, we take advan-\ntage of the additive properties of the empirical FIM and\napproximate it by aggregating the squared gradients of indi-\nvidual data points in the training set D, as seen in Equation\n(8). However, aggregation of single-sample gradients, es-\npecially at initialization, introduces high variance in the\napproximation. To mitigate this, we evaluated splitting D\ninto B batches, with B \u2208 B and B = {B}=1, to compute\na more stable approximation with the averaging gradients\nwhile reducing computational overhead. This leads to the\nfollowing batch-wise approximation of the FIM:\n\nF(w) \u2248  1/B \\sum_{k=1}^{B} diag (\u2207LBk (W)\u2207LBk (W)).\n\nPruning Mask. Given a partition of the data set, we com-\npute and accumulate the gradients in vector \u011f and the diag-\nonal entries of the FIM in vector df. Then, we calculate\nthe FTS score vector s, which contains the sensitivity score\nsq for each parameter wq in the model (see Algorithm 1).\nTo create the pruning mask m, we define a percentile p to\nnarrow the subset containing the parameter index to retain\nas:\n\nR = {q | sq is in the top (1 \u2013 p) of scores}.\n\nUsing this subset R, the elements of the binary mask m are\ndefined using the following rule:\n\nmq= 1, if q \u2208 R,\n0, otherwise."}, {"title": "4. Results and Discussion", "content": "To evaluate the effectiveness of the different pruning criteria\npresented, we performed experiments on architectures and\ndatasets commonly used in the pruning literature, specif-\nically on the ResNet18 and VGG19 architectures using\nCIFAR-10 and CIFAR-100 datasets, training details avail-\nable in the Appendix C. For comparison, we evaluate our\nproposed criteria against the following: random, parameter\nmagnitude, gradient norm (GN), SNIP (Lee et al., 2018),\nand GraSP (Wang et al., 2020).\nPerformance Analysis of Pruning Methods. Table 1 sum-\nmarizes the performance of the different criteria evaluated\nin the ResNet18 architecture with the CIFAR-10 test set. We\nobserve that the architecture is very robust to pruning, given\nthat all criteria perform on par with the baseline up to a spar-\nsity ratio of 0.70 (Appendix D.1). Therefore, our analysis\nfocuses on high sparsities (0.80 < sparsity < 0.95) and\nextreme sparsities (sparsity > 0.95), where the differences\namong the pruning criteria are more pronounced. Magni-\ntude pruning is a viable criterion up to a sparsity ratio of\n0.80, where it achieves the highest accuracy (91.10\u00b10.12%)\namongst all methods, with the alternative method FP match-\ning its performance (91.08 \u00b1 0.06).\nNevertheless, we observe a rapid decay as we increase the\nsparsity ratio, where magnitude pruning yields the worst\nperformance (71.99 \u00b1 0.28) at the highest sparsity, even\nlower than random (78.28 \u00b1 0.45). This further strengthens\nthe argument that a better-principled criterion is required\nfor efficient training of pruned models (LeCun et al., 1989).\nWith respect to the proposed FTS sensitivity criterion, our\nmethod constantly outperforms or matches the top performer\nfor the rest of the sparsity ratios. FTS achieves the high-\nest performance in the sparsities 0.90 and 0.99, with the"}, {"title": "5. Conclusion", "content": "This work introduces FTS, a computationally cheap and effi-\ncient pruning criterion that leverages first- and second-order\ninformation to perform one-shot pruning at initialization.\nOur approach builds on the empirical FIM diagonal, demon-\nstrating its effectiveness in finding important parameters,\neven with randomly initialized networks."}, {"title": "Appendix", "content": "A. Optimal Brain Surgeon Derivation\nIn the original setup in OBS, we have a local quadratic model for the loss L given by:\n\\nSL = L(w + \u03b4\u03c9) \u2248 L(w) + \u221awL7 \u03b4\u03c9 + 1/2 \u03b4\u03c9 \u0397\u03b4\u03c9\\n\nSince OBS is a pruning-after-training approach, they discarded the 1-st order component. Reducing the expression for\nsaliency as:\n\\n\u03b4L = 1/2 \u03b4\u03c9 \u0397\u03b4\u03c9\\n\nTo remove a single parameter, the authors of OBS introduced the constraint er dw + wq = 0, with eq being the qth canonical\nbasis vector. The pruning is defined as a constrained optimization problem of the form:\n\\nmin \u03b4\u03c9 \u0397\u03b4\u03c9 s.t \u03b5\u03b4\u03c9 + Wq = 0.\\n\u03b4\u03c9ER q\n\nAnd the choice of which parameter to remove becomes:\n\\nmin min (1/2 \u03b4\u03c9 \u0397\u03b4\u03c9), s.t \u03b5\u03b4\u03c9 + \u03c9\u03b1 = 0}\\nq\u03b5\u03a9 {\u03b4\u03c9\u03b5R\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 }\n\nTo solve the internal problem, we use a Lagrange multiplier A to write the problem as an unconstrained optimization case as\nfollows:\n\\nL(\u03b4\u03c9, \u03bb) = 1/2 \u03b4\u03c9 \u0397\u03b4\u03c9 + \u03bb\u03b5\u03b4\u03c9 + wq).\\n\nThen, to find the stationary conditions, we compute the partial derivatives with respect to \u03b4\u03c9 and A, and equate them to 0,\nobtaining:\n\\n\u22078wL = Hdw + deq = 0 \u2192 \u03b4\u03c9 = \u2212XH-1eq\\n\u2207xL = edw e + wq = 0 \u2192 \u03b5\u03b4\u03c9 edw = -Wq\\n\nWith some replacements, we get:\n\\neq \u03a4\u03b4\u03c9 = -\u03c9\u03b1 -Wq\\n\u2192e(-1H-1eq) = \u2212Wq \u2192 \u2212Xe H\u00af\u00b9eq = \u2212Wq \u2192 X = Wq= Wq= eH-1eq= [H-1]qq\\n\\n\u03b4\u03c9 = wqH-1eq/ [H-1]qq\\n\nReplacing the expression for dw in the saliency expression, we have:\n\\nSL = 1/2 \u03b4\u03c9 \u0397\u03b4\u03c9 = 1/2 (WqH-1eq) H (WqH-1eq)/ [H-1]2qq\\n\\n = [Wq/ [H-1]99] 12 (Heq)T H/ [H-1]gg(H-eq)\\n12 [H-1]99/2[H-1]299/2[H-1]gg = w12 [H-1]99 = 2[H-1]99\\n", ",": ","}, {",": ","}, {",": "\n],"}, {"title": "B. Fisher Brain Surgeon Sensitivity Derivation", "content": "As we considered a PBT setting, it is not possible to ignore the first-order term in the local quadratic approximation of the\nerror as it could still be informative. In this case, our model for sensitivity is given by:\n\\nTS\u03b4L = VwLT \u03b4\u03c9 +1/2 \u03b4\u03c9 \u0397\u03b4\u03c9\\n\nThe process to remove a single parameter remains similar; the constraint e edw + wq = 0, with eq is still valid, redefining\nthe optimization problem as:\n\\nmin \u03b4\u03c9 1VwLT \u03b4\u03c9 +1/2 \u03b4\u03c9 H\u03b4\u03c9 s.t \u03b5\u03b4\u03c9 \u03b4\u03c9 + wq = 0.\\n\u03b4\u03c9ER qq\n\nAnd the choice of which parameter to remove becomes:\n\\nminmin{\u25bdwL T dw + 1/2 dw Hdw dw + a = 0}\\n\\nUsing a Lagrange multiplier A as in the reference case, we solve the following unconstrained optimization problem:\n\\nL(\u03b4\u03c9, \u03bb) = wLT \u03b4\u03c9 +1/2 \u03b4\u03c9 \u0397\u03b4\u03c9 + \u03bb(\u03b5\u03b4\u03c9 + \u03c9\u03b1).\\n\nWith the following stationary conditions:\n\\nwL = \u2207wL + \u0397\u03b4\u03c9 + Xeq = 0 \u2192 \u03b4\u03c9 = \u2212(>H\u00af\u00b9eq + H\u00ae\u00b9\u2207wL)\\n\u2207xL = edw + wq = 0 \u2192 edw = -wq\n\nThe expression for A is redefined as follows:\n\\nT1 =  e(-(H-1eq + H-1\u2207wL)) = \u2212Wq\\nH-1 + H-\u00ae\u00b9\u2207wL = Wq\\\n[H-1]qq = wq - eH-1wL/ H-1wL = Y\\nw = Wq - eH-1wL/ [H-1]qq\\n\nReplacing the expression for \u03b4w in our sensitivity expression, we have:\n\\nTS \u03b4L = VwLT \u03b4\u03c9 +1/2 \u03b4\u03c9 \u0397\u03b4\u03c9 = VwLT [\u2212(H\u00af\u00b9eq + H\u2212\u00b9\u2207wL)]\\\n+1/2 [\u2212(H\u00af\u00b9eq + H\u2212\u00b9\u2207wL)] H [\u2212(H\u00af\u00b9eq + H-1\u2207wL)] = -XVwLTH-1eq \u2013 \u2207wLTH-1\u2207wL\\\n+[(AH-1eq) + (H-\u00b9\u2030L)T] [>HH\u00af\u00b9eq + HH\u00af\u00b9\u2207wL)]\n= \u2212>VwLTH\u00ae\u00b9eq \u2013 \u2207wLTH\u00ae\u00b9\u2207wL\\n+[(AH-eq) + (H-1wL)T] [deg + VwL]\n\\n-1", ",": ","}, {",": ","}, {",": ""}, {",": ""}, {",": ","}, {",": ","}, {",": ","}, {",": ","}, {",": ","}, {",": ","}, {",": ""}, {",": ""}, {",": ""}, {",": ",\n\""}, {",": "},"}, {",": ","}, {",": ","}, {",": ""}, {",": ",,\"\n\",\n\"", ",,,": "", ",,,,,": ",,"}, {",": "{\n      \"title\": \"C. Training and Testing Details\"", "content": "We perform an 80:20 stratified split, with a constant seed, on the CIFAR10/100 training dataset to obtain a validation\nset with the same class distribution. For both datasets, we have a training set with 40,000 samples, a validation set with\n10,000 samples, and a testing set of 10,000 samples. Validation is performed after each training step, and the weights of the\nbest-performing validation step (based on top-1 accuracy) are utilized for the final evaluation on the testing set. Table A1\nsummarizes the training parameters."}, {"title": "D.1. ResNet18", "content": "Table A2. Performance of different sensitivity methods for pruning evaluated using ResNet18 on the CIFAR-10 testset. The right side\nof the table presents our proposed criteria. The mean accuracy and standard deviation are reported across three initialization seeds for\nvarious sparsity levels. Baseline, no pruning: 91.78\u00b10.09."}, {"title": "D.2. VGG19", "content": "As discussed earlier, introducing a warm-up phase effectively mitigates layer collapse in data-dependent pruning methods.\nHere, we evaluate the impact of different warm-up durations by comparing no warm-up, a single warm-up epoch, and five\nwarm-up epochs. Table A3 demonstrates how performance drastically degrades with increasing sparsity, ultimately leading\nto layer collapse at 0.90 sparsity. However, as shown in the results, a single warm-up epoch is sufficient to prevent collapse\nand stabilize pruning performance. Moreover, as seen in Table A5, increasing the warm-up period to five epochs provides no\nsubstantial additional improvement. This indicates that prolonged warm-up training is not necessary; a single training step is\nenough to achieve gradient stabilization and overcome layer collapse."}, {"title": "E.1. ResNet18", "content": "CIFAR-100 results exhibit a similar trend to those observed on CIFAR-10, further reinforcing the robustness of our proposed\nFisher-Taylor Sensitivity (FTS) criterion. Across all evaluated sparsity levels, FTS consistently maintains strong performance,\nfrequently ranking among the top-performing methods. This trend is particularly evident at extreme sparsities, where many\npruning approaches suffer significant performance degradation. The stability of FTS across both datasets highlights its\neffectiveness in preserving network expressivity despite aggressive pruning."}, {"title": "E.2. VGG19", "content": "The results on VGG19 with CIFAR-100 exhibit a similar trend to those observed on CIFAR-10, reinforcing the effectiveness\nof our proposed approach. Once again, we identify the occurrence of layer collapse at extreme sparsities when no warm-up\nis applied, leading to a significant drop in accuracy. Introducing a single warm-up epoch effectively resolves this issue,\nrestoring pruning performance across all evaluated criteria. However, increasing the warm-up phase to five epochs does not\nyield any additional advantage, indicating that a brief warm-up period is sufficient to stabilize gradient-based importance\nscores and prevent collapse."}, {"title": "F. Mask Batch Size for Other Sparsities", "content": "The Effect of batch size on pruning performance across different sparsities. As sparsity increases, the effect of batch size\non pruning performance becomes more pronounced. At lower sparsities (0.90, 0.95), the differences across batch sizes\nare less evident, suggesting that even smaller batches provide a reasonable estimation of parameter importance. However,\nat extreme sparsities (0.98, 0.99), we observe a clear trend where larger batch sizes consistently lead to better parameter\nselection, ultimately improving accuracy. This aligns with our hypothesis that larger batches help reduce variance in gradient\nestimation, leading to more stable and effective pruning decisions."}, {"title": "G. Comparison of our criteria with magnitude-based pruning", "content": "Figure A2 illustrates the relationship between parameter magnitude and different sensitivity-based pruning metrics. Each\npoint represents a model parameter, with red points indicating the top-ranked parameters selected for retention by each\ncriterion. The green dashed line marks the 99th percentile of parameter magnitudes.\nA key observation is that the most effective pruning criteria, such as Fisher-Taylor Sensitivity, tend to retain parameters with\na broad range of magnitudes, including many that are relatively small (left of the green line). This shows that the estimated\nimportance does not always prioritize parameters based on their magnitude."}]}