[{"title": "Adaptive Foundation Models for Online Decisions: HyperAgent with Fast Incremental Uncertainty Estimation", "authors": ["Yingru Li", "Jiawei Xu", "Zhi-Quan Luo"], "abstract": "Foundation models often struggle with uncertainty when faced with new situations in online decision-making, necessitating scalable and efficient exploration to resolve this uncertainty. We introduce GPT-HyperAgent, an augmentation of GPT with HyperAgent for uncertainty-aware, scalable exploration in contextual bandits, a fundamental online decision problem involving natural language input. We prove that HyperAgent achieves fast incremental uncertainty estimation with \u00d5(log T) per-step computational complexity over T periods under the linear realizable assumption. Our analysis demonstrates that HyperAgent's regret order matches that of exact Thompson sampling in linear contextual bandits, closing a significant theoretical gap in scalable exploration. Empirical results in real-world contextual bandit tasks, such as automated content moderation with human feedback, validate the practical effectiveness of GPT-HyperAgent for safety-critical decisions. Our code is open-sourced at https://github.com/szrlee/GPT-HyperAgent/.", "sections": [{"title": "1 Introduction", "content": "Real-world decision-making often faces uncertainty due to a lack of comprehensive information about the environment. Intelligent agents must not only understand this uncertainty but also actively gather information to resolve it. This task is particularly challenging for real-time online decisions involving foundation models-large-scale AI models pretrained on vast datasets that process unstructured inputs like text and images.\nContent moderation on digital platforms, a real-world safety-critical task, exemplifies these challenges [Gorwa et al., 2020]. Traditionally, human reviewers detected violations of human value and community standards [Roberts, 2019], but the high volume of posts on platforms like Facebook [Meta, 2024], Twitter [Corp., 2024], and Reddit [Reddit, 2024] required automating content moderation. AI systems using foundation models [Weng et al., 2023] provide real-time capabilities and reduce human workload. However, pretrained on historical data, these models may struggle with uncertainty in online production traffic where new and rare situations exhibit, leading to errors [Markov et al., 2023]. Reliable content moderation requires real-time human feedback to correct AI errors, reduce uncertainty, and refine detection policies. This human-AI collaboration pipeline aims to minimize human intervention (by exploiting the current AI capability) while ensuring long-term reliability (by exploring uncertain content for human review to improve future ability), as illustrated in Figure 1. To achieve these goals, AI systems using foundation models need to quickly adjust uncertainty estimates and refine policies as new data continually arrives, necessitating fast incremental uncertainty estimation and scalable solutions to balance exploration and exploitation.\nThese challenges can be framed within the contextual bandit problem [Wang et al., 2005, Langford and Zhang, 2007]\u2014a fundamental online decision-making problem involving contextual information, including unstructured language and vision input, that affects decisions."}, {"title": "1.1 Key Contributions", "content": "We introduce GPT-HyperAgent, leveraging pretrained GPT for expressive feature embeddings and integrating HyperAgent [Li et al., 2024b] for scalable uncertainty-guided exploration in contextual bandits with unstructured language input. Vanilla HyperAgent [Li et al., 2024b] was designed based on hypermodel framework [Dwaracherla et al., 2020, Li et al., 2022] and achieves state-of-the-art computational and data efficiency for large-scale deep reinforcement learning benchmarks. Yet, the compatibility of vanilla HyperAgent with foundation models for contextual bandits has never been examined. More importantly, existing literature lack rigorous understanding on HyperAgent or hypermodel-type algorithm under function approximation, and thus cannot provide much guidance on the algorithmic configurations.\nIn this work, we provide an in-depth theoretical understanding that leads to practical advancement, and close a fundamental gap in the theory for scalable randomized exploration algorithms.\nTheoretical Understanding\n\u2022 Efficient and Scalable Uncertainty Estimation: We theoretically prove that HyperAgent achieves fast and scalable incremental uncertainty estimation with \u00d5(logT) per-step computational com- plexity over T periods under the linear realizable assumption. This enables real-time adaptation and efficient handling of increasing data volumes. The underlying mechanism is incrementally updating an approximate factor of the covariance matrix via the outer product of the feature vector and a random vector draw from perturbation distribution at each time period.\n\u2022 Distribution-Dependent Regret: We develop a general regret analysis framework that leads to a regret bound dependent on the reference distribution. Certain continuous-support reference"}, {"title": "Practical Guidance and Performance", "content": "\u2022 Separated Distributions & Empirical Validation: We demonstrate that update and perturbation distributions in HyperAgent can be chosen separately, unconventional to existing literature. This allows dual benefits by using discrete-support update distributions for lower computation cost while maintaining the advantages of continuous-support reference distributions. This algorithmic insight is theoretically justified under linear realizable assumption and empirically validated in neural contextual bandit setups.\n\u2022 Foundation Model Online Decisions: By addressing the challenges of uncertainty estimation and scalable exploration, GPT-HyperAgent advances the state-of-the-art in online decisions with foundation models. This is crucial for applications like content moderation with human feedback, where the balance between reducing human workload and ensuring long-term safety is paramount."}, {"title": "2 Problem formulation and HyperAgent algorithm", "content": null}, {"title": "2.1 Sequential decision-making under uncertainty", "content": "We consider a environment involving a set of actions A and a ground-truth real-valued functions f* : A \u2192 R. We will define random variables with respect to a probability space (\u03a9, F,P). The agent is uncertain about the function f* in the beginning. At each time t, the agent is presented with a possibly random subset At \u2286 A and selects an action At \u2208 At, after which she observes a reward Yt. We denote by Ht the o-algebra generated from history (A1, A1, Y1, . . ., At\u22121, At\u22121, Yt\u22121, At) of observations available to the agent when choosing an action At. The agent employs a policy \u03c0 = {\u03c0\u03c4 | t\u2208N}, which is a deterministic sequence of functions, each mapping the history Ht to a probability distribution over actions A. For each realization of Ht \u2208 Ht, \u03c0\u03b9 (Ht) is a distribution over A with support At, though with some abuse of notation, we will often write this distribution as \u03c0t. The action At is selected by sampling from the distribution \u03c0t, so that P(At \u2208\u00b7 |\u03c0t) =\nP (At \u2208 \u2022 | Ht) = \u03c0\u03b9(\u00b7). We assume that E [Yt | Ht, At] = f* (At). In other words, the realized reward is the mean reward value corrupted by zero mean noise. We will also assume that for each t\u2208 N, arg maxa\u2208At f*(a) is nonempty with probability one, though algorithms and results can be generalized to handle cases where this assumption does not hold. The T-period regret of a policy \u03c0 is the random variable defined by $R(T) = \\sum_{t=1}^T \\max_{a\\in A_t} f^*(a) \u2013 f^* (A_t)$.\nExample 1 (Contextual Bandit Models). The contextual bandit model [Langford and Zhang, 2007, Wang et al., 2005] is a special case of the formulation presented above. In such a model, an exogenous Markov process Xt taking values in a set X influences rewards. In particular, the expected reward at time t is given by f*(a, Xt). However, this is mathematically equivalent to a problem with stochastic time-variant decision sets At. In particular, one can define the set of actions to be the set of state-action pairs A := {(x,a) : x \u2208 X,a \u2208 A(x)}, and the set of available actions to be\n$A_t = \\{(X_t, a) : a \\in A (X_t)\\}$.\nWithin the online automated content moderation task as in Figure 1, the context Xt consists of text, image or video content that the user submits to the platform while the moderator needs to choose an action a \u2208 {publish, remove}.\nExample 2 (Linear Realizable Rewards). We say the reward function f* is linear realizable w.r.t a known feature map \u00a2 : A \u2192 Bd if there exists a vector 0* \u2208 Bd, such that $f^*(a) = (\\phi(a), \\theta^*)$.\nThese examples are widely studied in the literature of linear contextual bandits [Rusmevichientong and Tsitsiklis, 2010, Abbasi-Yadkori et al., 2011a, Dani et al., 2008]."}, {"title": "2.2 HyperAgent, hypermodel and index sampling", "content": "Vanilla HyperAgent [Li et al., 2024b] was shown state-of-the-arts performance in large-scale deep RL benchmarks. Its success can be attributed to several key mechanisms: hypermodel [Dwaracherla et al., 2020, Li et al., 2022], incremental updates, and index-based approximate Thompson sampling, known as index sampling.\nThe hypermodel $f_\\theta$, parameterized by \u03b8, is designed for uncertainty estimation. It takes an input x \u2208 Rd and a random index (draw from a fixed reference distribution P\u0163, producing an index sample $f_\\theta(x, \\zeta)$, reflecting a predictive sample from a desired distribution. For instance, if we want to approximate a linear-Gaussian distribution $N(x^\\top \\mu, x^\\top \\Sigma x)$, one can use linear hypermodel\n$f_\\theta(x,\\zeta) = (x, \\mu + A\\zeta)$, with $\\theta = (A \\in \\mathbb{R}^{d\\times M},\\mu\\in \\mathbb{R}^d)$ and Gaussian reference distribution $P_\\zeta = N(0, I_M)$. It essentially performs a Box-Muller transformation: when $AA^\\top = \\Sigma$, the index"}, {"title": "3 Theoretical analysis", "content": "We start by providing a general analytical framework for agent, potentially randomized, operating in the generic bandit environments. Let us introduce a few necessary definitions to facilitate the understanding and analysis. The confidence bound is used for uncertainty estimation over the ture function f* given the history Ht.\nDefinition 1 (Confidence bounds). Confidence bounds are a sequence of real-valued Ht-measurable functions Lt(\u00b7) and Ut(\u00b7) for t \u2208 [T] such that, w.p. at least 1 \u2013 8, the joint event E = \u2229te[T]Et holds, where Et := {$f^*(a) \\in [L_t(a), U_t(a)],\\forall a \\in A_t$\\}.\nThe agent may not perform well unless it is well-behaved, defined by reasonableness and optimism. Intuitively, an agent that explores too much or too little will incur a high regret. Reasonableness and optimism are the mechanisms for controlling these potential flaws respectively.\nDefinition 2 (Reasonableness). Given confidence bounds Lt(\u00b7) and Ut(\u00b7) for t \u2208 [T], an (randomized) agent is called reasonable if it produces a sequence of functions (ft(\u00b7), t \u2208 [T]) such that w.p. at least 1 \u2013 8, the joint event \u0190 = \u2229t\u2208[T]\u0190t holds, where Et := {$f_t(a) \\in [L_t(a), U_t(a)], \\forall a \\in A_t$\\}.\nIn short, reasonableness ensures that the chosen action according to ft is close to the best action which ensures agent does not explore actions unnecessarily. The following optimism guarantees the agent sufficient explores."}, {"title": "3.1 Insight from Linear HyperAgent", "content": "Consider the functional form for HyperAgent in a linear setup at time t:\n$f_t(a) := f_{\\theta_{t-1}} (a, \\zeta_t) = (\\phi(\u03b1), \u03b2_tA_{t-1}\\zeta_t + \\mu_{t-1}), \\forall \u03b1 \u2208 A$,\nwhere \u00dft is an inflation coefficient defined later, \u03c6(\u00b7) is a feature map introduced in Example 2, and 0t = (At, \u03bct) are parameters representing uncertainty.\nIn the context of GPT-HyperAgent, if the underlying reward function f* can be linearly approximated using GPT-2's pretrained feature embeddings, then we can freeze the GPT-2 torso during training Hyperagent, as shown in Figure 3. This approach, referred to as the frozen-GPT- torso method, is essentially an instance of linear HyperAgent and will be evaluated in Section 4. The assumption of linear realizability will be discussed formally in Assumption 1.\nWith the form of linear HyperAgent, we theoretically identify the general conditions for the update and perturbation distribution (P\u0118, P\u2082) that permit scalable uncertainty estimation via incremental posterior approximation. Then, we investigate the reasonableness and optimism condition through several reference distributions Pr."}, {"title": "4 Experiments", "content": "We conduct comprehensive experiments integrating HyperAgent into existing foundation models, thereby improving exploration in decision-making tasks. First, we analyze the advantages of HyperAgent in linear bandit, treating this scenario as case where the foundation model backbone is frozen. We than investigate the benefits of HyperAgent in neural bandit, utilizing MLP networks as the backbone for convenient analysis. Finally, we apply HyperAgent to existing foundation models (GPT-2) to demonstrate its superiority in handling natural language task that requires exploration."}, {"title": "4.1 Synthetic Experiments", "content": "We conduct experiments on synthetic bandit tasks, involving both linear and nonlinear reward functions, to validate the theoretical insights and provide practical guidance. We firstly demonstrate the superiority of HyperAgent over Ensemble+ [Osband et al., 2018] in linear contextual bandits and then we validate the advantage of separating the reference and update distributions in neural contextual bandits."}, {"title": "4.1.1 Linear Contextual Bandits", "content": "We begin by examining the advantages of HyperAgent in linear bandit, which can be understood as scenario where the foundation model backbone is fixed. In this experiment, we primarily focus on studying the impact of perturbations and reference distributions on HyperAgent."}, {"title": "4.1.2 Neural Contextual Bandits", "content": "We then extend HyperAgent to nonlinear tasks, using MLP networks as feature extractors for convenient analysis. In this experiment, we primarily focus on studying the impact of update distributions and demonstrating the advantages of HyperAgent compared to other algorithms that employ approximate posterior sampling."}, {"title": "4.2 Content Moderation with Human Feedback", "content": "In this experiment, we integrate HyperAgent with existing foundation models to enhance their decision-making capabilities. We utilize LLMs as the backbone to address language tasks that require exploration. Given the high computational cost associated with hyperparameter tuning in LLMs, we adopt the effective settings from the previous study on synthetic tasks. Specifically, we employ the Coord distribution for updates and use the Sphere distribution as both the reference and perturbation distribution."}, {"title": "5 Related works", "content": "Foundation Model Decision-making. Recent research applies foundation models, including large language models and generative models, to real-world decision-making applications [Yang et al., 2023, Shinn et al., 2024, Wang et al., 2023, Lee et al., 2023]. Although these systems grant agents decision-making and planning capabilities, they predominantly rely on pre-trained models using offline datasets and may struggle with actively seeking information and resolving uncertainties in an online decision environment that continually presents new, uncovered scenarios. Krishnamurthy et al. [2024] demonstrated that even the most advanced large language model, GPT-4 [OpenAI, 2023] with various advanced prompt design, is ineffective in making online decisions, as seen in a simple multi-armed bandit (MAB) problem, unless supplemented with an MAB-specific uncertain estimation method like external history summarization. More sophisticated algorithmic interventions, such as fine-tuning or dataset curation, might be necessary to enhance LLM-based decision-making agents in complex settings [Krishnamurthy et al., 2024]. Our work addresses this gap by integrating our uncertainty-aware HyperAgent techniques into foundation models, thereby enabling effective online decision-making in complex and uncertain environments.\nScalable Uncertainty Estimation & Exploration. Thompson Sampling (TS) is a favored exploration strategy in online sequential decision-making, balancing exploration and exploitation by sampling from the model's posterior distribution, a Bayesian uncertainty estimation principle. However, TS is computationally feasible primarily in straightforward scenarios where conjugacy permits efficient posterior updates as new data accumulates [Thompson, 1933, Russo et al., 2018]. In many practical"}, {"title": "6 Conclusion and future directions", "content": "In this work, we introduced GPT-HyperAgent, a novel integration of foundation models with HyperAgent for online decision-making tasks, focusing on contextual bandits with natural language input. Our contributions include both theoretical insights and practical advancements. Specifically, we proved a regret bound for HyperAgent under linear setups, closing a gap in the theory for scalable exploration algorithms. We showed that perturbation and update distributions in HyperAgent can be chosen separately for computational benefits, providing practical guidance. Empirical results in an online content moderation task validated GPT-HyperAgent's superior online decision capability, demonstrating scalable and efficient performance in real-world safty-critical applications. Promising future directions include\n\u2022 Black-Box & Multi-Modal Foundation Models: Extend GPT-HyperAgent to work with black- box foundation models accessed via APIs. This would allow leveraging powerful pretrained models without requiring access to their internal architectures, making the approach applicable to a wider range of commercial AI services. Specifically, current LLM/VLM APIs provide top-k token logits, text embedding or fine-tuning services by uploading private dataset. Additionally, integrate multi-modal inputs (e.g., combining vision, language, and audio) to tackle more diverse real-world scenarios and improve generalization capabilities.\n\u2022 Efficient Human-AI interplay & AI safety: The AI moderation system, a safety-critical scenario, inevitably relies on human-AI collaboration pipeline. It needs further in-depth studies. Other important applications require human-AI interplay include reward modeling from human feedback by actively query informative data for human review. This is critical for reinforcement learning from human feedback (RLHF) and the efficiency in data-centric AI."}, {"title": "A Additional related works", "content": "Broad Capabilities and Diverse Applications of Foundation Models. Foundation models, pre- trained on diverse datasets encompassing audio, vision, language, and other modalities, have demonstrated exceptional capabilities across a wide range of downstream tasks [Bommasani et al., 2021, Reid et al., 2024, OpenAI, 2023, Collaboration et al., 2024]. Their applications extend to real-world scenarios such as dialogue systems, autonomous driving, healthcare, robotics, and bio-engineering [OpenAI, 2023, Yan et al., 2024, Chen et al., 2023b, Zhang et al., 2023, Saab et al., 2024, Collaboration et al., 2024, Huang et al., 2024]. In these settings, foundation models encounter unique challenges like interacting with external environments, adapting to varied task modalities, and performing long-term reasoning and planning [Nakano et al., 2021, Yao et al., 2022].\nFoundation Models in Sequential Decision-Making. Sequential decision-making, which includes domains such as reinforcement learning (RL) and bandit problems, has traditionally focused on task-specific settings with limited prior knowledge, achieving notable success in tasks such as board games, video games, and robotics manipulation [Sutton and Barto, 2018, Lattimore and Szepesv\u00e1ri, 2020, Schrittwieser et al., 2020, Li et al., 2024b, Kalashnikov et al., 2018]. However, these traditional methods, learning from scratch, often face issues with generalization and data efficiency.\nThe integration of foundation models into this field represents a paradigm shift, giving rise to \"foundation agents\" that utilize extensive pretraining to solve a broader range of tasks more efficiently [Yang et al., 2023]. However, current foundation agents that use off-the-shelf models such as large language models (LLMs) and vision language models (VLMs) face significant challenges related to controllability, reproducibility, and efficiency [Chen et al., 2023a]. These models, not originally designed for decision-making tasks like action generation or self-evaluation, demonstrate limited capabilities in few-shot prompting and in-context learning, which do not effectively address the needs for exploration and exploitation in sequential decision-making [Brown et al., 2020, Krishnamurthy et al., 2024]. To address these shortcomings, (1) more robust algorithmic interventions such as fine-tuning or (2) bottom-up re-design of the foundation models for sequential decision-making may be necessary.\nBridging Theory and Practice. The transition from theoretical models to practical applications requires a deep understanding of both the capabilities and limitations of these advanced models. This understanding is essential for designing systems that are not only efficient but also scalable and adaptable to real-world complexities [Li et al., 2024b, Li, 2024a,b]."}, {"title": "B Incremental uncertainty estimation in Lemma 1", "content": "Before proving Lemma 1, we state the preliminary tools of sequential random projection for completeness, which is adapted form [Li, 2024a]. This tool was used to prove incremental posterior approximation argument of HyperAgent in tabular RL setup [Li et al., 2024b]. As the tool in [Li, 2024a] works only for the scalar process, we need additional technical innovations to deal with high-dimensional vector process. We make a novel utilization of this tool in the linear function"}, {"title": "B.1 Probability tools for sequential random projection", "content": "We define some important concept that would be useful in the analysis. Let (\u03a9, F, F = (Ft)ten, P) be a complete filtered probability space. We first consider the measurable properties within the filtered probability space.\nDefinition 5 (Adapted process). For an index set I of the form {$t \\in \\mathbb{N} : t > t_0$\\} for some $t_0 \\in \\mathbb{N}$, we say a stochastic process $(X_t)_{t\\in I}$ is adapted to the filtration $(F_t)_{t\\in I}$ if each $X_t$ is $F_t$-measurable.\nDefinition 6 ((Conditionally) \u03c3-sub-Gaussian). A random variable X \u2208 R is \u03c3-sub-Gaussian if\n$\\mathbb{E}[\\exp(\\lambda X)] \\leq \\exp\\left(\\frac{\\lambda^2 \\sigma^2}{2}\\right), \\ \\forall \\lambda \\in \\mathbb{R}$.\nLet $(X_t)_{t>1} \\subset \\mathbb{R}$ be a stochastic process adapted to filtration $(F_t)_{t>1}$. Let $\\sigma = (\\sigma_t)_{t>0}$ be a stochastic process adapted to filtration $(F_t)_{t\\geq 0}$. We say the process is $(X_t)_{t>1}$ is conditionally \u03c3-sub-Gaussian if\n$\\mathbb{E}[\\exp(\\lambda X_t) | F_{t-1}] \\leq \\exp\\left(\\frac{\\lambda^2 \\sigma_{t-1}^2}{2}\\right) \\ \\ \\text{a.s.} \\ \\ \\forall \\lambda \\in \\mathbb{R}$.\nSpecifically for the index t + 1, we can say Xt+1 is (Ft-conditionally) \u03c3t-sub-Gaussian. If \u03c3t is a constant \u03c3 for all t > 0, then we just say (conditionally) \u03c3-sub-Gaussian.\nFor a random vector X \u2208 RM or vector process (Xt)t\u22651 CRM in high-dimension, we say it is \u03c3-sub-Gaussian is for every fixed v \u2208 SM\u22121 if the random variable \u27e8v, X\u27e9, or the scalarized process \u27e8(\u03c5, Xt)\u27e9t\u22651 \u03af\u03c2 \u03c3-sub-Gaussian.\nDefinition 7 (Almost sure unit-norm). We say a random variable X is almost sure unit-norm if ||X||2 = 1 almost surely.\nRemark 5. When talking about the perturbation distribution Pz, we scale all specific distribution discussed in Appendix E by $\\frac{1}{\\sqrt{M}}$. Then the spherical distribution U(SM-1) and uniform over scaled cube U({1,-1}M) satisfy the sub-Gaussian condition in Definition 6 with parameter $o = \\frac{1}{\\sqrt{M}}$ and also satisfy the unit-norm condition in Definition 7 according to the discussion in Appendix E.\nAdditionally, we characterize the boundedness on the stochastic processes.\nDefinition 8 (Square-bounded process). For an index set I of the form {$t \\in \\mathbb{N} : t > t_0$\\} for some to \u2208 N, the stochastic process $(X_t)_{t\\in I}$ is c-square-bounded if X} < c almost surely for all t \u2208 I.\nNow, we are ready to state the important tool that is fundamental to our analysis.\nTheorem 3 (Sequential random projection in adaptive process [Li, 2024a]). Let \u025b \u2208 (0,1) be fixed and (Ft)t\u22650 be a filtration. Let $z_0 \\in \\mathbb{R}^M$ be an $F_0$-measurable random vector satisfies $\\mathbb{E}[||z_0||^2] = 1$ and $|| \\ |z_0||^2 \u2013 1| < (\\delta \\epsilon^2 /2)$. Let $(z_t)_{t\\geq 1} \\subset \\mathbb{R}^M$ be a stochastic process adapted to filtration $(F_t)_{t>1}$ such that it is $\\sqrt{c_0}/M$-sub-Gaussian and each zt is unit-norm. Let $(x_t)_{t>1} \\subset \\mathbb{R}$ be a stochastic process adapted to filtration $(F_{t-1})_{t>1}$ such that it is $c_x$-square-bounded. Here, $c_0$ and $c_x$ are absolute constants. For any fixed xo \u2208 R, if the following condition is satisfied\n$M \\geq \\frac{16c_0(1+\\epsilon)}{\\delta \\epsilon^2} \\bigg( \\log(\\frac{3}{\\delta}) + \\log\\Big(1 + \\frac{c_x T}{\\epsilon \\sigma^2}\\Big) \\bigg);$"}, {"title": "B.2 Reduce Lemma 1 to sequential random projection", "content": "Without loss of generality, let us consider the set Sd-1. First, we define a fine-grained good event for desired approximation error \u03b5\u2208 [0,1]: the approximate posterior variance is \u025b-close to the true posterior variance for action a at time t \u2208 T := {0,1,..., T}.\n$\\mathcal{G}_t(a,\\epsilon) = \\Big\\{ |a^T \\Sigma_t^{-1}a - a^T \\hat{\\Sigma}_t^{-1}a| < \\epsilon a^T \\Sigma_t^{-1} a \\Big\\}$,\nand corresponding joint event over the set Sd-1,\n$\\mathcal{G}_t(\\epsilon) = \\bigcap_{a \\in \\mathcal{S}^{d-1}} \\mathcal{G}_t(a,\\epsilon)$.\nThe good event defined in Lemma 1 is indeed $\\mathcal{G}_t(1/2)$.\nA reduction. To fully utilize the probability tool for sequential random projection in Theorem 3, we make use of the following reduction from vector process to scalar process. For a fixed $a \\in \\mathcal{S}^{d-1}$, we let $s(a) = a^T \\Sigma^{1/2} z_0$, $s^2(a) = a^T \\Sigma^{-1} a$. Further define short notation $z_0 := \\frac{s(a)}{s^2(a)}$ and $x_0 := s(a)$. and $x_t = a^{-1} \\phi(A_t)$ for all $t \\in [T]$, then we can relate the incremental update in Proposition 1\n$a^T \\Sigma_t^{-1} A_t \\hat{A}_t \\Sigma_t a = \\frac{a^T \\Sigma^{1/2} z_0}{s(a)} \\hat{s_t}^2 + \\sum_{i=1}^{t} a \\phi(A_i)a \\phi(A_i) a = \\frac{a^T \\Sigma^{1/2} z_0}{s(a)} \\hat{s_t}^2 + \\sum_{i=1}^{t} a \\phi(A_i)\\phi(A_i)^T a$\nto the scalar sequence $(x_t)_{t>0}$ and the vector sequence $(z_t)_{t>0}$ that would be applied in Theorem 3. Recall that Ht the o-algebra generated from history (A1, A1, Y1, . . ., At\u22121, At\u22121, Yt\u22121, At). Denote $Z_1 = \\sigma(Z_0)$ and $Z_t = \\sigma(Z_0, Z_1,..., Z_{t-1})$ for $t \\geq 2$. We observe the following statistical relationship, which is further demonstrated in Figure 8\n\u2022 zt (Ht, At, Zt), xt is dependent on Ht, Zt,\n\u2022 At\u22121 \u2208 \u03c3(Ht, Zt),\n\u2022 \u03bc\u03c4\u22121, \u03a3\u03c4\u22121 \u2208 Ht.\nFor all t > N, let us define the sigma-algebra Ft = \u03c3(Ht+1, Zt+1, At+1). We can verify Fk \u2286 Fi for all k \u2264 1. Thus F = (Ft)ten is a filtration. Now, we could verify (zt)t>o is adapted to (Ft)t\u22650 and (xt)t>1 is adapted to (Ft)t>0, satisfying the conditions in Theorem 3.\nPrior approximation. First, we state a standard covering argument on sphere.\nLemma 2 (Covering number of a sphere). There exists a set $\\mathcal{C}, \\mathcal{C} \\subset \\mathcal{S}^{d-1}$ with $|\\mathcal{C}_\\iota| \\leq (1 + 2/\\iota)^d$ such that for all $x \\in \\mathcal{S}^{d-1}$ there exists a y \u2208 C\u2081 with ||x - Y||2 \u2264 1.\nLemma 3 (Computing spectral norm on a covering set). Let A be a symmetric d \u00d7 d matrix, and let C\u2081 be the an i-covering of Sd\u22121 for some i \u2208 (0,1). Then,\n$||A|| = \\sup_{x\\in \\mathcal{S}^{d-1}} |x^T A x| \\leq (1 \u2013 2\\iota)^{-1} \\sup_{x\\in \\mathcal{C}_L} |x^T A x|.$"}, {"title": "C Regret analysis", "content": "To make the proof easy to access, we restate the core results and a few notations that is needed for the proof of the propositions.\nAdapting the results from [Abbasi-Yadkori et al., 2011b, Abeille and Lazaric, 2017], let $\u03b2_t = \\sqrt{\\lambda} + \\sqrt{2\\log(1/\\delta)} + \\sqrt{\\log \\det(\\Sigma_t^{-1} /\\lambda)} $. Under assumption 1, we define the confidence bound as\n$L_t(\u00b7) = (\u22121) \\vee (\\langle \\mu_{t-1}, \\phi(\u00b7)\\rangle - \\beta_t ||\\phi(\u00b7)||_{\\Sigma_{t-1}}) , U_t(\u00b7) = 1 \\wedge (\\langle \\mu_{t-1}, \\phi(\u00b7)\\rangle + \\beta_t ||\\phi(\u00b7)||_{\\Sigma_{t-1}})$\nFor the purpose of analysis within various reference distribution, we define a slightly inflated confidence bounds as\n$L_t(\u00b7; \\mathcal{P}_\\zeta) = (\\langle \\mu_{t-1}, \\phi(\u00b7)\\rangle \u2013 \u03b2_t \\rho(\\mathcal{P}_\\zeta) ||\\phi(\u00b7)||_{\\Sigma_{t-1}}) \\vee (\u22121)$,\n$U_t(\u00b7; \\mathcal{P}_\\zeta) = (\\langle \\mu_{t-1}, \\phi(\u00b7)\\rangle + \u03b2_t \\rho(\\mathcal{P}_\\zeta) ||\\phi(\u00b7)||_{\\Sigma_{t-1}}) \\wedge 1$.\n\u03c1(Pt) is defined via $\u03c1_1 = O(\\sqrt{M \\log(MT)})$, $\u03c1_2 = O(\\sqrt{M})$, and $\u03c1_3 = O(\\log(|A|T))$ and Table 1. An immediate observation is that [$L_t(\u00b7), U_t(\u00b7)$] \u2282 [$L^*_t(\u00b7; \\mathcal{P}_\\zeta), U^*_t(\u00b7; \\mathcal{P}_\\zeta)$]. Thus, $L^*_t(\u00b7; \\mathcal{P}_\\zeta)$ and $U^*_t(\u00b7; \\mathcal{P}_\\zeta)]$ are also confidence bounds. We consider the the following functional form for HyperAgent under linear setup: for time t,\n$f_t(\u03b1) := f_{\\theta_{t-1}} (\u03b1, \u03b6_t) = (\\phi(\u03b1), \u03b2_t A_{t-1} \u03b6_t + \\mu_{t-1}), \\ \\ \\ \\forall \u03b1 \u2208 A$,\nThe condition on the propositions and theorem for regret analysis is when Equation (9) is satisfied, that is when M = O(dlog T), the Lemma 1 implies that with high probability, the good events G = NteTGt hold jointly, where\n$\\mathcal{G}_t := \\{ \\frac{1}{2} x^T \\Sigma_t x < x^T \\hat{\\Sigma}_t \\hat{A}_t x < \\frac{3}{2} x^T \\Sigma_t x ,\\forall x \\in \\mathbb{R}^d \\}$.\nIn the following section, we discuss the proof conditioned on the joint event G and also the confidence event that $f^*(\u03b1) \\in [L_t(\u03b1), U_t(\u03b1)]$ for all $t \\in [T]$ and $\u03b1 \u2208 A."}, {"title": "C.1 Proof of Proposition 2", "content": "Notice that from Equation (6), we derive\n$\\vert f_t(\u03b1) - (\u03bc_{t-1}, \\phi(\u03b1)) \\vert = \\vert (\\phi(\u03b1), \u03b2_t A_{t-1} \u03b6_t ) \\vert$\n=$\\frac{\u03b2_t\\vert \\phi(\u03b1) A_{t-1} A^{-1}_{t-1} \u03b6_t \\vert }{\\sqrt{||\\phi(\u03b1) A_{t-1}||_{\\Sigma_t^{-1}"}]}, {}]