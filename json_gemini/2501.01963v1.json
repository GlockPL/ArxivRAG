{"title": "STATISTICAL LEARNING DOES NOT ALWAYS ENTAIL KNOWLEDGE", "authors": ["DANIEL ANDR\u00c9S D\u00cdAZ-PACH\u00d3N", "H. RENATA GALLEGOS", "OLA H\u00d6SSJER", "J. SUNIL RAO"], "abstract": "In this paper, we study learning and knowledge acquisition (LKA) of an\nagent about a proposition that is either true or false. We use a Bayesian ap-\nproach, where the agent receives data to update his beliefs about the propo-\nsition according to a posterior distribution. The LKA is formulated in terms\nof active information, with data representing external or exogenous informa-\ntion that modifies the agent's beliefs. It is assumed that data provide details\nabout a number of features that are relevant to the proposition. We show that\nthis leads to a Gibbs distribution posterior, which is in maximum entropy rel-\native to the prior, conditioned on the side constraints that the data provide\nin terms of the features. We demonstrate that full learning is sometimes not\npossible and full knowledge acquisition is never possible when the number of\nextracted features is too small. We also distinguish between primary learning\n(receiving data about features of relevance for the proposition) and secondary\nlearning (receiving data about the learning of another agent). We argue that\nthis type of secondary learning does not represent true knowledge acquisi-\ntion. Our results have implications for statistical learning algorithms, and we\nclaim that such algorithms do not always generate true knowledge. The the-\nory is illustrated with several examples.", "sections": [{"title": "Introduction.", "content": "In the current era of scientific computing,\nwhen large language models have seemingly achieved surprising levels of understanding and\ndiscussions about artificial general intelligence are as abundant as nebulous, proper defini-\ntions that can be accurately quantified are conspicuous by their absence. For instance, what\ndo we mean by \u201cunderstanding\u201d and \u201cintelligence\u201d in the previous paragraph? If explainable\nAI is going to explain anything, it does require clear concepts capable of guiding the dis-\ncussion to reach valid conclusions. Philosophers usually define knowledge as \u201cjustified true\nbelief\" [21, 25, 37]. This means that an agent A knows a proposition p if the following three\nproperties are satisfied:\nLK1 A believes p,\nLK2 p is true,\nLK3 A's belief about p is justified.\nIf only properties LK1 and LK2 are satisfied, A learns p. Clearly, acquiring knowledge\nrequires more than learning. Therefore, even before further theoretical developments, we\nobtain a simple but revealing fact:\nCLAIM 1. Statistical learning does not always entail knowledge."}, {"title": "Active information.", "content": "In order to describe (i)-(iv) in more detail, we will introduce\nlocal measures of information since our approach to learning and knowledge acquisition de-\npends fundamentally on such measures. This is interesting because Shannon's information\ntheory has been almost exclusively focused on global averages such as entropy, mutual in-\nformation, relative entropy, etc. However, recent decades have seen a resurgence of unaver-\naged measures of information like local active information storage and local transfer entropy.\nThese measures have been used in origin of life [8, 40, 42], neuroscience [41, 43] as well as\ncancer research and cell communication [30, 31]. All such measures can be seen as math-\nematical extensions of the more basic active information, which was originally proposed to\nmeasure the amount of exogenous information infused by a programmer in a search, com-\npared to the endogenous information generated by a blind search [9, 10]. Formally, if the\ndistributions of the outcome of the programmer and the blind search are represented by two\nprobability measures P and $P_0$ defined on the same measurable space $(\\mathcal{X}, \\mathcal{F})$, active infor-\nmation for a specific target $T\\subset\\mathcal{X}$ is defined as\n$I^+(T) = I^+(T; P_0, P) = \\log \\frac{P(T)}{P_0(T)},$\nwhere we assume 0/0 = 0 by continuity. In particular, if the programmer reaches the target\nwith certainty (P(T) = 1), then (1) reduces to the self-information of T.\nTo this point, active information has been used in several areas. For instance, in genetics, to\nquantify functional information in genetic sequence data [38, 39], and to compare selectively\nnon-neutral models to neutral ones in population genetics, where T was the event that a given\nallele gets fixed [16]; in bump-hunting, using machine learning algorithms to find a bump T\n[19, 29]; and in decision theory, to construct hypothesis tests that quantify the amount of\ninformation added, or needed, to produce an event T [12, 18]."}, {"title": "A mixed frequentist-Bayesian framework for learning and knowledge acquisition.", "content": "Following [24], in this article we apply active information to formalize the concepts LK1-\nLK3 behind learning and knowledge acquisition. To this end, it is assumed that $\\mathcal{X}$ is a set of\nparameters of a statistical model; in this context, we take a mixed frequentist and Bayesian\napproach. On the one hand, it is postulated that one element $x \\in \\mathcal{X}$ is the true parameter\nvalue (a frequentist assumption). On the other hand, uncertainty about $x_0$ is formulated as a\nprobability measure on $\\mathcal{X}$ that varies between persons (a Bayesian assumption). More specif-\nically, P and $P_0$ represent degrees of beliefs about $x \\in \\mathcal{X}$, of an agent A and an ignorant\nperson I, respectively. It is assumed that A acquired data D that I lacks, so that P and $P_0$ are\nposterior and prior distributions on $\\mathcal{X}$ that represent degrees of beliefs of A about $x_0$, after"}, {"title": "The novelties of this article.", "content": "Given the mathematical framework outlined in Section\n1.3, the novelties (i)-(iv) for learning and knowledge acquisition mentioned above can be\nphrased as follows. Starting with (i), discernment is a crucial aspect of A's learning and\nknowledge acquisition process, which quantifies his ability to separate elements of $\\mathcal{X}$ from\neach other. We assume that A's discernment is larger than that of the ignorant person I. We\nmean by this that A's beliefs P are measurable with respect to a finer o-field on $\\mathcal{X}$ than the\nbeliefs $P_0$ of I. We prove general results on how A's o-field affects his potential to learn and\nacquire knowledge.\nAs for (ii), we assume that data provide agent A with details about (modifies his beliefs\nin) the values of a number of features of relevance for learning proposition p. Then A forms\nhis likelihood in such a way that P maximizes entropy relative to $P_0$, among all probability\nmeasures on $\\mathcal{X}$ that are consistent with the beliefs of A about the values of the features. As\nwe shall see, this implies that P belongs to a family of Gibbs distributions.\nNovelty (ii) also has relevance for (iii) since feature extraction is a commonly used tech-\nnique for data reduction within statistical learning (see, e.g., [22, Section 5.3]). But, as a\nconsequence of the data processing inequality, feature extraction potentially implies a loss of\ninformation, regardless of how large the data set used to form A's beliefs about the values\nof the features is [7, Section 2.8], [11, Problem 2.1]. This implies that the Gibbs distribution\nbeliefs of A about the value of $x_0$ are limited by which features are selected in the first place.\nWe give a number of examples of how this provides fundamental limits in terms of learning\nand knowledge acquisition.\nThe concept of secondary learning (iv) refers to the learning process of another agent $\\bar A$\nwho lacks data D but, on the other hand, uses other data $\\bar D$ to learn how much A learned and\nacquired knowledge about the proposition. In other words, $\\bar A$ learns and acquires knowledge\nabout A's learning, but not necessarily about the proposition p itself. This also has an impact\non (iii) since machine learning algorithms often recapitulate the beliefs of humans, thereby\nperforming secondary (rather than primary) learning and knowledge acquisition. We also\ndemonstrate that the long-term effects of secondary learning are very similar to those of\nsynthetic primary learning, whereby a third agent $\\tilde A$ learns from synthetic primary data D'\ngenerated by A."}, {"title": "Organization of article.", "content": "Our paper is organized as follows. In Section 2, we define\nwhat it means that agent A has learned whether a proposition is true or not and whether he"}]}