{"title": "TOWARDS EMPOWERMENT GAIN THROUGH CAUSAL STRUCTURE LEARNING IN MODEL-BASED RL", "authors": ["Hongye Cao", "Fan Feng", "Meng Fang", "Shaokang Dong", "Tianpei Yang", "Jing Huo", "Yang Gao"], "abstract": "In Model-Based Reinforcement Learning (MBRL), incorporating causal structures into dynamics models provides agents with the structured understanding of environments, enabling more efficient and effective decisions. Empowerment, as an intrinsic motivation, enhances the ability of agents to actively control environments by maximizing mutual information between future states and actions. We posit that empowerment coupled with the causal understanding of the environment can improve the agent's controllability over environments, while enhanced empowerment gain can further facilitate causal reasoning. To this end, we propose the framework that pioneers the integration of empowerment with causal reasoning, Empowerment through Causal Learning (ECL), where an agent with the awareness of the causal dynamics model achieves empowerment-driven exploration and optimizes its causal structure for task learning. Specifically, we first train a causal dynamics model of the environment based on collected data. Next, we maximize empowerment under the causal structure for exploration, simultaneously using data gathered through exploration to update the causal dynamics model, which could be more controllable than dynamics models without the causal structure. We also design an intrinsic curiosity reward to mitigate overfitting during downstream task learning. Importantly, ECL is method-agnostic and can integrate diverse causal discovery methods. We evaluate ECL combined with 3 causal discovery methods across 6 environments including both state-based and pixel-based tasks, demonstrating its performance gain compared to other causal MBRL methods, in terms of causal structure discovery, sample efficiency, and asymptotic performance in policy learning.", "sections": [{"title": "INTRODUCTION", "content": "Model-Based Reinforcement Learning (MBRL) uses predictive dynamics models to enhance decision-making and planning. Recent advances in integrating causal structures into MBRL have provided a more accurate description of systems, achieve better adaptation, generalization , and avoiding spurious correlations.\nHowever, these methods often passively rely on pre-existing or learned causal structures for policy learning or generalization. In this work, we aim to enable the agent to actively leverage causal structures, guiding more efficient exploration of the environment. The agent can then refine its causal structure through newly acquired data, resulting in improvements in both the causal model and policy. This could further enhance the agent's controllability over the environment and its learning efficiency.\nWe hypothesize that agents equipped with learned causal structures will have better controllability than those using traditional dynamics models without causal modeling. This is because causal structures inform agents to explore the environment more efficiently by nulling out the irrelevant system variables. This assumption serves as intrinsic motivation to guide the policy in exploring higher-quality data, which in turn improves both causal and policy learning. Specifically, we employ empowerment gain, an information-theoretic framework where agents maximize mutual information between their actions and future states to improve control , as the intrinsic motivation to measure the agent's controllability. Concurrently, through empowerment, agents develop a more nuanced comprehension of their actions' consequences, implicitly discovering the causal relationships within their environment. Hence, by iteratively improving empowerment gain with causal structure for exploration, refining causal structure with data gathered through the exploration, the agent should be able to develop a robust causal model for effective policy learning.\nWe give a motivating example (Fig.1(a)) in a manipulation task, where the robot aims to move a target node while avoiding noisy nodes. Three possible trajectories (rows 1-3) are shown with different levels of control, efficiency, and success. Row 1 (irrelevant states) represents the least effective trajectory that can not control nodes and find the target, while rows 2 and 3 (controllable states) demonstrate learned control and efficiency, with high empowerment focusing on movable objects. In the corresponding causal graphs, represented as a Dynamics Bayesian Network in Fig. 1(b), $s^1$, $s^2$, $s^3$ denote the states of three objects. For simplicity and clarity, we assume each object is represented by a single variable. The graph illustrates the causal relationships between these states, actions, and rewards. Assuming the agent follows the causal structure (Fig.1(b)), it will likely execute actions similar to rows 2 and 3 since there are causal relationships between actions and states of movable objects, effectively improving controllability. Through exploration with better control, agents can facilitate improved causal discovery of the task, leading to high-reward outcomes and resulting in more efficient task completion like row 3.\nTo this end, we propose an Empowerment through Causal Learning (ECL) framework that actively leverages causal structure to maximize empowerment gain, improving controllability and learning efficiency. ECL consists of three main steps: model learning, model optimization, and policy learning. In model learning (step 1), we learn the causal dynamics model with a causal mask and a reward model. We then integrate an empowerment-driven exploration policy with the learned causal structure to better control the environment (step 2). We alternately update the causal structure with the collected data through exploration and policy of empowerment maximization. Finally, the optimized causal dynamics and reward models are used to learn policies for downstream tasks with a curiosity reward to maintain robustness and prevent overfitting (step 3). Importantly, ECL is method-agnostic, being able to integrate diverse causal discovery (i.e., score-based and constraint-based) methods. The main contributions of this work can be summarized as follows:\n\u2022 To improve controllability and learning efficiency, we propose ECL, a novel method-agnostic framework that actively leverages causal structures to boost empowerment gain, facilitating efficient exploration and causal discovery."}, {"title": "PRELIMINARIES", "content": "2.1 MDP WITH CAUSAL STRUCTURES\nMarkov Decision Process In MBRL, the interaction between the agent and the environment is formalized as a Markov Decision Process (MDP). The standard MDP is defined by the tuple $M = (S, A, T, \\mu_0, r, \\gamma)$, where $S$ denotes the state space, $A$ represents the action space, $T(s'| s, a)$ is the transition dynamics model, $r(s, a)$ is the reward function, and $\\mu_0$ is the distribution of the initial state $s_0$. The discount factor $\\gamma \\in [0, 1)$ is also included. The objective of RL is to learn a policy $\\pi : S \\times A \\rightarrow [0, 1]$ that maximizes the expected discounted cumulative reward $\\eta_{\\mu}(\\pi) := E_{s_0\\sim\\mu_0, s_t\\sim T, a_t\\sim \\pi} [\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)]$.\nStructural Causal Model A Structural Causal Model (SCM)  is defined by a distribution over random variables $V = \\{s_t, \\dots, s_t, a_t, \\dots, a_t, r_t, s_{t+1}, ..., s_{t+1}\\}$ and a Directed Acyclic Graph (DAG) $G = (V, E)$ with a conditional distribution $P(v_i | PA(v_i))$ for node $v_i \\in V$. Then the distribution can be specified as:\n$p(v_1,..., v_{[V]}) = \\prod_{i=1}^{[V]} p(v_i | PA(v_i))$,\nwhere $PA(v_i)$ is the set of parents of the node $v_i$ in the graph $G$.\nCausal Structures in MDP We model a factored MDP with the underlying SCM between states, actions, and rewards (Fig.1b). In this factored MDP, nodes represent system variables (different dimensions of the state, action, and reward), while edges denote their relationships within the MDP. We employ causal discovery methods to learn the structures of $G$. We identify the graph structures in G, which can be represented as the causal mask M. Hence, the dynamics transitions and reward functions in MDP with causal structures are defined as follows:\n$\\begin{aligned} s_{i+1} &= f (M_{s\\rightarrow s} \\odot s_t, M_{a\\rightarrow s}\\odot a_t, \\epsilon_{s,i,t}) \\\\ r_t &= R(\\phi_c(s_t | M), a_t) \\end{aligned}$,\nwhere $s_{i+1}$ represents the next state in dimension $i$, $M_{s\\rightarrow s} \\in \\{0, 1\\}^{|s| \\times |s|}$ and $M_{a\\rightarrow s} \\in \\{0, 1\\}^{|a| \\times |s|}$ are the causal masks indicating the influence of current states and actions on the next state, respectively, $\\odot$ denotes the element-wise product, and $\\epsilon_{s,i,t}$ represents i.i.d. Gaussian noise. Each entry in the causal mask $M$ (represented as the adjacency matrix of the causal graph $G$) indicates the presence (1) or absence (0) of a causal relationship between elements. The reward $r_t$ is a function of the state abstraction $\\phi_c(\\cdot | M)$ under the learned causal mask $M$, which filters out the state dimensions without direct edges to the target state dimension, and the action $a_t.$\n2.2 EMPOWERMENT\nEmpowerment is to quantify the influence an agent has over its environment and the extent to which this influence can be perceived by the agent . Within our framework, the empowerment is the mutual information between the agent action $a_t$ and its subsequent state $s_{t+1}$ under the causal mask $M$ as follows:\n$\\mathcal{E} := \\max_{\\pi(\\cdot | s_t)} I(s_{t+1}; a_t | M)$,\nwhere $\\mathcal{E}$ is used to represent the channel capacity from the action to state observation. $\\pi(\\cdot|s_t)$ is the conditional distribution of actions given states."}, {"title": "EMPOWERMENT THROUGH CAUSAL LEARNING", "content": "An illustration of the ECL framework is shown in Fig. 2, comprising three main steps: model learning, model optimization, and policy learning. In model learning (step 1), we learn causal dynamics model with the causal mask and reward model. This causal dynamics model is trained using collected data to identify causal structures (i.e., causal masks M), by maximizing the likelihood of observed trajectories. The reward model is trained based on state abstraction that masks irrelevant state dimensions with the causal structure. With the learned causal structure, we integrate empowerment-driven exploration for model optimization (step 2). This process involves learning the empowerment policy $\\pi_e$ that enhances the agent's controllability by actively leveraging the causal mask. We alternately update the policy $\\pi_e$ for empowerment maximization and generate data with $\\pi_e$ to optimize the causal mask M and reward model $P_{r}$. Finally, in step 3, the learned causal dynamics and reward models are used to learn policies for the downstream tasks. In addition to the task reward, to maintain robustness and prevent overfitting, an intrinsic curiosity reward is incorporated to balance the causality.\n3.1 STEP 1: MODEL LEARNING WITH CAUSAL DISCOVERY\nWe first learn causal dynamics model with the causal mask and reward model for the empowerment and downstream task learning. Specifically, a dynamics encoder is trained by maximizing the likelihood of observed trajectories $\\mathcal{D}$. Then, the causal mask is learned based on the dynamics model and a reward model is trained with the state abstraction under the causal mask and action.\nCausal Dynamics Model The causal dynamics model is composed with a dynamics model $P_{\\phi_c}$ and a causal mask $M$. The dynamics model maximizes the likelihood of observed trajectories $\\mathcal{D}$ as follows:\n$\\mathcal{L}_{dyn} = E_{(s_t, a_t, s_{t+1})\\sim \\mathcal{D}} \\sum_{i=1}^{d_s} log P_{\\phi_c}(s_{t+1}^i | s_t, a_t; \\phi_c)$,\nwhere $d_s$ is the dimension of the state space, and $\\phi_c$ denotes the parameters of the dynamics model. We train the dynamics model as a dense dynamics model that incorporates all state dimensions to capture the state transitions within the environment, facilitating subsequent causal discovery and empowerment. Additionally, we assess the performance of the dense model, specifically the baseline MLP, within the experimental evaluations detailed in Section 5. Next, we use this learned dynamics model for causal discovery.\nCausal Discovery For causal discovery, with the learned dynamics model $P_{\\phi_c}$, we further embed the causal masks $M_{s\\rightarrow s}$ and $M_{a\\rightarrow s}$ into the learning objective. To learn the causal mask, we employ both conditional independence testing (constraint-based) and mask learning by sparse regularization (score-based) . We further maximize the likelihood of states by updating the dynamics model and learned masks. Thus, the learning objective for the causal dynamics model is as follows:\n$\\mathcal{L}_{c-dyn} = E_{(s_t, a_t, s_{t+1})\\sim \\mathcal{D}} \\sum_{i=1}^{d_s} log P_{\\phi_c}(s_{t+1}^i | M_{s\\rightarrow s} \\odot s_t, M_{a\\rightarrow s} \\odot a_t; \\phi_c) + \\mathcal{L}_{causal}$,"}, {"title": "STEP 2: MODEL OPTIMIZATION WITH EMPOWERMENT-DRIVEN EXPLORATION", "content": "In Step 2, we optimize the learning of the causal structure and empowerment. As depicted in Fig. 2, this procedure alternates between optimizing the empowerment-driven exploration policy $\\pi_e$ and update the causal mask M using data gathered through exploration. Furthermore, to ensure the stability, we update the reward model to adapt to changes in state abstraction induced by updates to the causal mask M. Note that the dynamics model $P_{\\phi_c}$ learned in Step 1 remains fixed, allowing for a focused optimization of both the causal structure and the empowerment in an alternating manner. The causal structure is optimized by the causal mask M through maximizing $\\mathcal{L}_{causal} \\text{(Eq. 5)}$, while keeping the parameters of $\\phi_e$ fixed during this learning step.\nEmpowerment-driven Exploration To enhance the agent's control and efficiency given the causal structure, instead of maximizing $I(s_{t+1}, a_t|s_t)$ at each step, we consider a baseline that uses the dense dynamics model $P_{\\phi_e}$ without the causal mask M. We then prioritize causal information by maximizing the difference in empowerment gain between the causal and dense dynamics models.\nWe first denote the empowerment gain of the causal dynamics model and dense dynamics model as $\\mathcal{E}_{P_{\\phi_c}}(s|M) = \\max_a I(s_{t+1}; a_t | s_t; \\phi_c, M)$ and $\\mathcal{E}_{P_{\\phi_e}}(s) = \\max_a I(s_{t+1}; a_t | s_t; \\phi_c)$, respectively. Here, $\\mathcal{E}_{P_{\\phi_e}}(s)$ corresponds to the dynamics model without considering causal structures.\nThen, we have the following learning objective:\n$\\max_{\\alpha \\sim \\pi_e (a|s)} E_{(s, a, s') \\sim \\mathcal{D}} [\\mathcal{E}_{P_{\\phi_c}}(s|M) \u2013 \\mathcal{E}_{P_{\\phi_e}}(s)]$.\nIn practice, we employ the estimated $\\hat{\\mathcal{E}}_{P_{\\phi_c}}(s | M)$ and $\\hat{\\mathcal{E}}_{P_{\\phi_e}}(s)$ with the policy $\\pi_e$ for computing, specifically:\n$\\hat{\\mathcal{E}}_{P_{\\phi_c}}(S / M) = \\max_{\\alpha \\sim \\pi_e (a|s)} E_{\\pi_e (a_t|S_t) P_{\\phi_c}(S_{t+1} | s_t, a_t, M)} [log P_{\\phi_c} (s_{t+1} | s_t, a_t; M, \\phi_c) \u2013 log P(s_{t+1}|s_t)]$,\nand:\n$\\hat{\\mathcal{E}}_{P_{\\phi_e}}(S_t) = \\max_{\\alpha \\sim \\pi_e (a|s)} E_{\\pi_e (a_t|S_t) P_{\\phi_c}(S_{t+1} | s_t, a_t)} [log P_{\\phi_c} (s_{t+1} | s_t, a_t; \\phi_c) \u2013 log P(s_{t+1}|s_t)]$,\nwhere $P(s_{t+1}|s_t)$ is the conditional distribution of the current state. Hence, the objective function Eq. 7 is derived as:\n$\\max_{\\alpha \\sim \\pi_e (a|s)} H(s_{t+1} | s_t; M) - H(s_{t+1} | s_t) + E_{\\alpha \\sim \\pi_e (a|s)} [KL (P_{\\phi_c}(s_{t+1} | s_t, a_t; M) || P_{\\phi_c}(s_{t+1} | s_t, a_t))]$,\nwhere $H(s_{t+1} | s_t; M)$ and $H(s_{t+1} | s_t)$ denote the entropy at time $t + 1$ under the causal dynamics model and dense dynamics model, respectively. For simplicity, we update $\\pi_e$ by optimizing the KL term.\nModel Optimization In Step 2, we fix the dynamics model $P_{\\phi_e}$ and further fine-tune the causal mask M and the reward model $P_{r}$. We adopt an alternating optimization with the policy $\\pi_e$ to optimize the causal mask. Specifically, given M, we first optimize $\\pi_e$. The policy $\\pi_e$ is designed to collect controllable trajectories by maximizing the distance of empowerment between causal and dense models. These collected trajectories are then used to optimize both the causal structure M and reward model $P_{r}$."}, {"title": "STEP 3: POLICY LEARNING WITH CURIOSITY REWARD", "content": "We learn the downstream task policy based on the optimized causal structure. To mitigate potential overfitting of the causality learned in Steps 1&2, we incorporate a curiosity-based reward as an intrinsic motivation objective or exploration bonus, in conjunction with a task-specific reward, to prevent overfitting during task learning:\n$r_{cur}(s, a) = E_{(s_t, a_t, s_{t+1}) \\sim \\mathcal{D}} KL (P_{env} (s_{t+1} | s_t, a_t) || P_{\\phi_c, M} (s_{t+1} | s_t, a_t; \\phi_c, M)) -KL (P_{env} (s_{t+1} | s_t, a_t) || P_{\\phi_c} (s_{t+1}| s_t, a_t; \\phi_c))$,\nwhere $P_{env}$ is the ground truth dynamics collected from the environment. By taking account of $r_{cur}$, we encourage the agent to explore states that the causal dynamics cannot capture but the dense dynamics can from the true environment dynamics, thus preventing the policy from being overly conservative due to model learning with trajectories. Hence, the shaped reward function is:\n$r(s, a) = r_{task}(s, a) + \\lambda r_{cur}(s, a)$,\nwhere $r_{task}(s, a)$ is the task reward, $\\lambda$ is a balancing hyperparameter.\n4 PRACTICAL IMPLEMENTATION\nWe introduce the practical implementation of ECL for casual dynamics learning with empowerment-driven exploration and task learning.\nStep 1: Model Learning Initially, following , we use a transition collection policy $\\pi_{collect}$ by formulating a reward function that incentivizes selecting transitions that cover more state-action pairs to expose causal relationships thoroughly. We train the dynamics model $P_{\\phi_e}$ by maximizing the log-likelihood $\\mathcal{L}_{dyn}$, following Eq. 4. Then, we employ the causal discovery approach for learning causal mask M by maximizing the log-likelihood $\\mathcal{L}_{c-dyn}$ followed Eq. 5. Subsequently, we train the reward model $P_{r}$ with the state abstraction $\\phi_c(s | M)$ by maximizing the likelihood.\nStep 2: Model Optimization We execute the empowerment-driven exploration by $\\max_{\\alpha \\sim \\pi_e(a|s)} E_{s_t, a_t, s_{t+1} \\sim \\mathcal{D}} [\\mathcal{E}_{P_{\\phi_c}}(s|M) \u2013 \\mathcal{E}_{P_{\\phi_e}}(s)]$ followed Eq. 7 with causal dynamics model and dense dynamics model for policy $\\pi_e$ learning. Furthermore, the learned policy $\\pi_e$ is used to sample transitions for updating casual mask M and reward model. We alternately perform empowerment-driven exploration for policy learning and causal model optimization.\nStep 3: Policy Learning During downstream task learning, we incorporate the causal effects of different actions as curiosity rewards combined with the task reward, following Eq. 12. We maximize the discounted cumulative reward to learn the policy by the cross entropy method (CEM). Specifically, The causal model is used to execute dynamic state transitions defined in Eq. 2. The reward model evaluates these transitions and provides feedback in the form of rewards. The CEM handles the planning process by leveraging the predictions from the causal and reward models to optimize the task's objectives effectively."}, {"title": "EXPERIMENTS", "content": "We aim to answer the following questions in experimental evaluation: (i) How does the performance of ECL compare to other causal and dense models across different environments for tasks and dynamics learning, including pixel-based tasks? (ii) Does ECL improve causal discovery by eliminating more irrelevant state dimensions interference, thereby enhancing learning efficiency and generalization towards the empowerment gain? (iii) Whether different causal discovery methods in step 1 and 2, impact policy performance? What are the effects when combine the step 1 and 2? (iv) What are the effects of the components and hyperparameters in ECL?\n5.1 SETUP\nEnvironments. We select 3 different environments for basic experimental evaluation. Chemical : The task is to discover the causal relationship (Chain, Collider & Full) of chemical items which proves the learned dynamics and explains the behavior without spurious correlations. Manipulation : The task is to prove dynamics and policy for difficult settings with spurious correlations and multi-dimension action causal influence. Physical : a dense mode Physical environment. Furthermore, we also include 3 pixel-based environments of Modified Cartpole , Robedesk  and Deep Mind Control  for evaluation in latent state environments.\n5.2 RESULTS\n5.2.1 TASK LEARNING\nWe evaluate each method with the following 7 downstream tasks in the chemical (C), physical (P) and the manipulation (M) environments. Match (C): match the object colors with goal colors individually. Push (P): use the heavier object to push the lighter object to the goal position. Reach (M): move the end-effector to the goal position. Pick (M): pick the movable object to the goal position. Stack (M): stack the movable object on the top of the unmovable object."}, {"title": "CAUSAL DYNAMICS LEARNING", "content": "Causal Graph Learning. To evaluate the efficacy of our proposed method for learning causal relationships, we first conduct experimental analyses across three chemical environments, employing five evaluation metrics. We conduct causal learning based on the causal discovery with Con and Sco respectively. The comparative results using the same causal discovery methods are presented in Table 1, with each cell containing the comparative results for that method across different scenarios. These results demonstrate the superior performance of our approach in causal reasoning, exhibiting both effectiveness and robustness as evinced by the evaluation metrics of F1 score and ROC AUC . All results exceed 0.90. Notably, our approach exhibits exceptional learning capabilities in chemical chain and collider environments. Moreover, it significantly enhances models performance when handling more complex full causal relationships, underscoring its remarkable capability in grasping intricate causal structures. This proposed causal empowerment framework facilitates more precise uncovering of causal relationships by actively using the causal structure.\nVisualisation. Moreover, we visually compare the inferred causal graph with the ground truth graph in terms of edge accuracy. Furthermore, ECL-Sco mitigates interference from irrelevant causal nodes more proficiently than GRADER. The causal graph learned in the complex manipulation environment shown in Figure 15, demonstrates that ECL effectively excludes irrelevant state dimensions to avoid the influence of spurious correlations. These findings substantiate that the proposed method attains superior performance compared to other causal discovery methods in causal learning.\nPredicting Future States. Given the current state and a sequence of actions, we evaluate the accuracy of each method's prediction, for states both ID and OOD."}, {"title": "PIXEL-BASED TASK LEARNING", "content": "In complex pixel-based robodesk task, where video backgrounds serve as distractors, ECL effectively learns controllable policies for changing background colors to green, as shown in Figure 8. Additionally, ECL surpasses IFactor in terms of average return. These results further validate ECL's efficacy in pixel-based tasks and its ability to overcome spurious correlations (video backgrounds)."}, {"title": "RELATED WORK", "content": "Causal MBRL MBRL involves training a dynamics model by maximizing the likelihood of collected transitions, known as the world model. Due to the exclusion of irrelevant factors from the environment through state abstraction, the application of causal inference in MBRL can effectively improve sample efficiency and generalization. Wang proposes a constraint-based causal dynamics learning that explicitly learns causal dependencies by action-sufficient state representations. GRADER executes variational inference by regarding the causal graph as a latent variable. CDL  is a causal dynamics learning method based on CIT. CDL employs conditional mutual information to compute the causal relationships between different dimensions of states and actions.\nEmpowerment in RL Empowerment is an intrinsic motivation to improve the controllability over the environment . This concept is from the information-theoretic framework, wherein actions and future states are viewed as channels for information transmission."}, {"title": "CONCLUSION", "content": "This paper proposes a method-agnostic framework of empowerment through causal structure learning in MBRL to improve controllability and learning efficiency by iterative policy learning and causal structure optimization. We maximize empowerment under causal structure to prioritize controllable information and optimize causal dynamics and reward models to guide downstream task learning. Extensive experiments across 6 environments included pixel-based tasks substantiate the remarkable performance of the proposed framework.\nLimitation and Future Work ECL implicitly enhances the controllability but does not explicitly tease apart different behavioral dimensions. In our future work, we plan to extend this framework in several directions. First, we aim to disentangle directable behaviors and explore entropy relaxation methods to enhance empowerment, particularly for real-world robotics tasks . Second, while the current framework does not account for changing dynamics, we intend to incorporate insights from recent advancements in local causal discovery  and modeling non-stationary change factors to enhance the causal discovery component. Third, we plan to leverage pre-trained 3D or object-centric visual dynamics models to scale our approach to real-world robotics applications. These directions will be pursued in future work."}, {"title": "BROADER IMPACT", "content": "Our work explores leveraging causal structure to enhance empowerment for efficient policy learning, enabling better control of the environment in MBRL. We propose a framework that can effectively combine diverse causal discovery methods. This holistic approach not only refines policy learning but also ensures that the causal model remains adaptable and accurate, even when faced with novel or shifting environmental conditions. ECL demonstrates improved learning efficiency and generalization compared to other causal MBRL methods across six different RL environments, including pixel-based tasks. Simultaneously, ECL achieves more accurate causal relationship discovery, overcoming spurious correlation present in the environment.\nWhile ECL demonstrated strengths in accurate causal discovery and overcoming spurious correlation, disentangling controllable behavioral dimensions remains a limitation. Our implicit empowerment approach enhances the policy's control over the environment, but does not explicitly tease apart different behavioral axes. Explicitly disentangling controllable behavioral dimensions could be an important future work to further improve behavioral control and empowerment. Additionally, our current approach involves substantial data collection and model optimization efforts, which can hinder training efficiency. Moving forward, we aim to further streamline our framework to enable more efficient policy training and causal structure learning. Enhancing computational performance while maintaining accuracy will be a key focus area for future iterations of this work. In the empowerment maximization described by Eq. 10, we currently omit two entropy terms. In our future work, we plan to explore additional entropy relaxation methods to further optimize this causal empowerment learning objective."}, {"title": "ADDITIONAL RELATED WORKS", "content": "B.1 MODEL-BASED REINFORCEMENT LEARNING\nMBRL involves training a dynamics model by maximizing the likelihood of collected transitions, known as the world model, as well as learning a reward model. Based on learned models, MBRL can execute downstream task planning, data augmentation and Q-value estimation. MBRL can easily leverage prior knowledge of dynamics, making it more effective at enhancing policy stability and generalization. However, when faced with high-dimensional state spaces and confounders in complex environments, the dense models learned by MBRL suffer from spurious correlations and poor generalization. To tackle these issues, causal inference approaches are applied to MBRL for state abstraction, removing unrelated components.\nB.2 CAUSALITY IN MBRL\nDue to the exclusion of irrelevant factors from the environment through causality, the application of causal inference in MBRL can effectively improve sample efficiency and generalization. Wang proposes a regularization-based causal dynamics learning method that explicitly learns causal dependencies by regularizing the number of variables used when predicting each state variable. GRADER executes variational inference by regarding the causal graph as a latent variable. IFactor  is a general framework to model four distinct categories of latent state variables, capturing various aspects of information. CDL is a causal dynamics learning method based on conditional independence testing. CDL employs conditional mutual information to compute the causal relationships between different dimensions of states and actions, thereby explicitly removing unrelated components. However, it is challenging to strike a balance between explicit causal discovery and prediction performance, and the learned policy has lower controllability over the system. In this work, we aim to actively leverage learned causal structures to achieve effective exploration of the environment through empowerment, thereby learning controllable policies that generate data to further optimize causal structures."}, {"title": "NOTATIONS, ASSUMPTIONS AND PROPOSITIONS", "content": "Assumption 1 (d-separation ) d-separation is a graphical criterion used to determine", "conditions": "n(i) $n_k \\in S$", "converse": "if $a_i \\nvdash s_{t+1}^i |$ {$a_t \\setminus a_i, s_t$}, then there exists an edge from $a_i$ to $s_{t+1}^i$. Again, we use proof by contradiction. Suppose there is no edge between $a_i$ and $s_{t+1}^i$ in the graph. Due to the Markov assumption, the lack of an edge between these variables implies their conditional independence given {$a_t \\setminus a_i, s_t$}. This contradicts our initial statement that $a_i \\amalg s_{t+1}^i |$ {$a_t \\setminus a_i, s_t$}.\nTherefore, there must exist an edge from $a_i$ to $s_{t+1}^i$.\nProposition 2 Under the assumptions that the causal graph is Markov and faithful to the observations, there exists an edge from $s_t^i \\rightarrow s_{t+1}^i$ if and only if $s_t^i \\amalg s_{t+1}^i |$ {$a_t, S_t \\setminus s_t^i$}.\nThe proof of Proposition 2 follows a similar line of reasoning to that of Proposition 1. Consequently, the two propositions collectively serve"}]}