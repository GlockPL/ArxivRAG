{"title": "Concept Conductor: Orchestrating Multiple Personalized Concepts in\nText-to-Image Synthesis", "authors": ["Zebin Yao", "Fangxiang Feng", "Ruifan Li", "Xiaojie Wang"], "abstract": "The customization of text-to-image models has seen signifi-\ncant advancements, yet generating multiple personalized con-\ncepts remains a challenging task. Current methods struggle\nwith attribute leakage and layout confusion when handling\nmultiple concepts, leading to reduced concept fidelity and\nsemantic consistency. In this work, we introduce a novel\ntraining-free framework, Concept Conductor, designed to en-\nsure visual fidelity and correct layout in multi-concept cus-\ntomization. Concept Conductor isolates the sampling pro-\ncesses of multiple custom models to prevent attribute leak-\nage between different concepts and corrects erroneous lay-\nouts through self-attention-based spatial guidance. Addition-\nally, we present a concept injection technique that employs\nshape-aware masks to specify the generation area for each\nconcept. This technique injects the structure and appearance\nof personalized concepts through feature fusion in the at-\ntention layers, ensuring harmony in the final image. Exten-\nsive qualitative and quantitative experiments demonstrate that\nConcept Conductor can consistently generate composite im-\nages with accurate layouts while preserving the visual details\nof each concept. Compared to existing baselines, Concept\nConductor shows significant performance improvements. Our\nmethod supports the combination of any number of concepts\nand maintains high fidelity even when dealing with visu-\nally similar concepts. The code and models are available at\nhttps://github.com/Nihukat/Concept-Conductor.", "sections": [{"title": "Introduction", "content": "Text-to-image diffusion models(Nichol et al. 2021; Saharia\net al. 2022; Ramesh et al. 2022; Rombach et al. 2022; Podell\net al. 2023) have achieved remarkable success in generat-\ning realistic high-resolution images. Building on this foun-\ndation, techniques for personalizing these models have also\nadvanced. Various methods for single-concept customiza-\ntion(Dong, Wei, and Lin 2022; Ruiz et al. 2023; Gal et al.\n2022; Voynov et al. 2023; Alaluf et al. 2023) have been pro-\nposed, enabling the generation of images of the target con-\ncept in specified contexts based on user-provided visual con-\nditions. These methods allow users to place real-world sub-\njects into imagined scenes, greatly enriching the application\nscenarios of image generation.\nDespite the excellent performance of existing methods for\nsingle-concept customization, handling multiple concepts\nremains challenging. Current methods(Kumari et al. 2023;"}, {"title": "Related Work", "content": ""}, {"title": "Text-to-Image Diffusion Models", "content": "In recent years, text-to-image diffusion models have ex-\ncelled in generating realistic and diverse images, becom-\ning the mainstream approach in this field. Trained on large-\nscale datasets like LAION(Schuhmann et al. 2022), mod-\nels such as GLIDE(Nichol et al. 2021), DALL-E 2(Ramesh\net al. 2022), Imagen(Saharia et al. 2022), and Stable Dif-\nfusion(Rombach et al. 2022) can produce high-quality and\ntext-aligned outputs. However, these models struggle to un-\nderstand the relationships between multiple concepts, result-\ning in generated content that fails to fully convey the original\nsemantics. This issue is exacerbated when dealing with vi-\nsually similar concepts. In this work, we apply our method\nto the publicly available Stable Diffusion(Rombach et al.\n2022), which is based on the Latent Diffusion Model (LDM)\narchitecture. LDM operates in the latent space of a Varia-\ntional Autoencoder (VAE), iteratively denoising to recover\nthe latent representation of an image from Gaussian noise.\nAt each timestep, the noisy latents z\u0142 are fed into the denois-\ning network \u20ac\u03b8, which predicts the current noise eo (zt, y, t)\nbased on the encoded prompt y."}, {"title": "Customization in T21 Diffusion Models", "content": "Several works(Ruiz et al. 2023; Gal et al. 2022; Voynov et al.\n2023; Alaluf et al. 2023) have customized text-to-image\nmodels to generate images of target concepts in new con-\ntexts by learning new visual concepts from user-provided ex-\nample images. For instance, DreamBooth(Ruiz et al. 2023)\nembeds specific visual concepts into a pre-trained model by\nfine-tuning its weights. Textual Inversion(Gal et al. 2022)\nrepresents new concepts by optimizing a text embedding,\nlater improved by P+(Voynov et al. 2023) and NeTI(Alaluf\net al. 2023). Custom Diffusion(Kumari et al. 2023) explores\nmulti-concept customization through joint training, but it re-\nquires training a separate model for each combination and\noften faces severe attribute leakage. Recent works(Liu et al.\n2023; Gu et al. 2024) propose frameworks that combine\nmultiple single-concept models and introduce manually de-\nfined layouts in attention maps to aid generation. For exam-\nple, Cones 2(Liu et al. 2023) proposes residual embedding-\nbased concept representations for textual composition and\nemphasizes or de-emphasizes a concept at a specific loca-\ntion by editing cross-attention. Mix-of-show(Gu et al. 2024)\nmerges multiple custom models into one using gradient fu-\nsion and restricts each concept's appearance area through\nregion-controlled sampling. These works cannot fully avoid\ninterference between concepts, leading to low success rates\nwhen handling similar concepts or more than two con-\ncepts. Additionally, by only manipulating cross-attention\nand neglecting the impact of self-attention on image struc-\nture, these methods often result in mismatched structures\nand appearances, causing layout control failures. In con-\ntrast, our method prevents attribute leakage by isolating the\nsampling processes of different single-concept models and\nachieves stable layout control through self-attention-based\nspatial guidance."}, {"title": "Spatial Control in T2I Diffusion Models", "content": "Using text prompts alone is insufficient for precise control\nover image layout or structure. Some methods(Avrahami\net al. 2023; Li et al. 2023; Zhang, Rao, and Agrawala 2023;\nMou et al. 2024) introduce layout conditions by training ad-\nditional modules to generate controllable images. For ex-\nample, GLIGEN(Li et al. 2023) adds trainable gated self-\nattention layers to allow extra input conditions, such as\nbounding boxes. To achieve finer spatial control, Control-\nNet(Zhang, Rao, and Agrawala 2023) and T2I-Adapter(Mou\net al. 2024) introduce image-based conditions like key-\npoints, sketches, and depth maps by training U-Net encoder\ncopies or adapters. These methods can stably control image\nstructure but limit the target subjects' poses, and these condi-\ntional images are difficult for users to create. Some training-\nfree methods achieve spatial guidance by manipulating at-\ntention layers during sampling. Most works(Ma et al. 2024;\nKim et al. 2023; He, Salakhutdinov, and Kolter 2023) at-\ntempt to alleviate attribute leakage and control layout by\ndirectly editing attention maps but have low success rates.\nSeveral gradient-based methods (Couairon et al. 2023; Xie\net al. 2023; Phung, Ge, and Huang 2024) calculate the loss\nbetween attention and the given layout and introduce layout\ninformation into the latent space representation by optimiz-\ning the loss gradients. These methods align the generated\nimage layout with coarse visual conditions (e.g., bounding\nboxes and semantic segmentation maps), often resulting in\nhigh-frequency detail loss and image quality degradation,\neven with complex loss designs. In this work, we propose\nextracting layout information from an easily obtainable ref-\nerence image as a supervisory signal, which not only sta-\nbly controls the layout but also preserves the diversity of the\ngenerated subjects' poses while avoiding image distortion."}, {"title": "Method", "content": ""}, {"title": "Preliminary: Attention Layers in LDM", "content": "LDM employs a U-Net as the denoising network, consisting\nof a series of convolutional layers and transformer blocks. In\neach block, intermediate features produced by the convolu-\ntional layers are passed to a self-attention layer followed by\na cross-attention layer. Given an input feature hin, the output\nfeature in each attention layer is computed as hout = AV,\nwhere A = softmax(QKT). Here, Q = fq(hin), K =\nfk(c), and V = fv(c) are obtained through learned pro-\njectors fq, fk, and fv, with c = hin for self-attention and\nc = y for cross-attention. Self-attention enhances the quality\nof the generated image by capturing long-range dependen-\ncies in the image features, while cross-attention integrates\ntextual information into the generation process, enabling the\ngenerated image to reflect the content of the text prompt.\nFurthermore, extensive researches (Liu et al. 2024; Patash-\nnik et al. 2023; Hertz et al. 2022) have shown that self-\nattention controls the structure of the image (e.g., shapes, ge-\nometric relationships), whereas cross-attention controls the\nappearance of the image (e.g., colors, materials, textures)."}, {"title": "Preliminary: ED-LORA", "content": "ED-LORA is a method for single-concept customization, pri-\nmarily involving learnable hierarchical text embeddings and\nlow-rank adaptation (LoRA) applied to pre-trained weights.\nTo learn the representation of a concept within the pre-\ntrained model's domain, it creates layer-wise embeddings\nfor the target concept's token following P+(Voynov et al.\n2023). Additionally, to capture out-of-domain visual details,\nit fine-tunes the pre-trained text encoder and U-Net using\nLORA(Hu et al. 2021). In this paper, ED-LoRA is used as\nour single-concept model by default."}, {"title": "Overview of Concept Conductor", "content": "Our method comprises three components: multi-path sam-\npling, layout alignment, and concept injection, as illustrated\nin Figure 2. At each denoising step, we first correct the in-\nput latents 2t through layout alignment, obtaining new la-\ntents zt/ that carry the layout information from the reference\nimage. Then, we inject the personalized concepts from the\ncustom models into the base model and denoise zt/ to obtain\nthe next latents 2t-1. Multi-path sampling is implemented\nin both layout alignment and concept injection to ensure the\nindependence of each concept and coordination between dif-\nferent subjects."}, {"title": "Multipath Sampling", "content": "Joint training or model fusion methods often lead to attribute\nleakage between different concepts (as shown in Figure 1)\nand require additional optimization steps for each combi-\nnation. To directly utilize multiple existing single-concept\nmodels for composite generation without attribute leakage,\nwe propose a multipath sampling structure. This structure\nincorporates a base model base and multiple custom models\ne (implemented with ED-LoRA(Gu et al. 2024)) for per-\nsonalized concepts Vi, as illustrated in Figure 3.\n V\n V\nGiven several custom models er and a text prompt p, at\n V\neach timestep t, we maintain the independent denoising pro-\ncess for each model: \u20ac exi = ei (zt, t,p). When the prompt\n Vi\ncontains similar concepts, models may struggle to distin-\nguish them, leading to attribute leakage. Therefore, we edit"}, {"title": "Layout Alignment", "content": "Existing multi-concept customization methods often suffer\nfrom layout confusion (as shown in Figure 1), especially\nwhen the target concepts are visually similar or numerous.\nTo address these challenges, we introduce a reference im-\nage to correct the layout during the generation process. For\nexample, to generate an image of a specific dog and cat\nin a specific context, we only need a reference image con-\ntaining an ordinary cat and dog. One simple approach to\nachieve layout control is to convert the reference image into\nabstract visual conditions (e.g., keypoints or sketches) and\nthen use ControlNet(Zhang, Rao, and Agrawala 2023) or\nT2I-Adapter(Mou et al. 2024) for spatial guidance, which\nlimits variability and flexibility and may reduce the fidelity\nof the target concepts. Another approach is to directly inject\nthe full self-attention of the reference image into the gen-"}, {"title": "", "content": "eration process, transferring the overall structure of the im-\nage(Hertz et al. 2022). This strictly limits the poses of the\ntarget subjects, reducing the diversity of the generated im-\nages. Additionally, it requires structural similarity between\nthe reference image subjects and target concepts to avoid\ndistortions caused by shape mismatches.\nTo align the layout while preserving the structure of the\ntarget concepts, we propose a gradient-guided approach, as\nshown in Figure Figure 4. Given a reference image, we per-\nform DDIM inversion to obtain the latent space representa-\ntion zef and record the self-attention features Fref at each\nttimestep. Similarly, we record the self-attention features of\nthe base model and each custom model during the generation\nprocess, denoted as Fbase and FVi, respectively, as shown in\nFigure 2. An optimization objective is set to encourage the\ngenerated image's layout to align with the given layout:\n Clayout = || Fbase \u2013 Fref ||2 + N FV -Fref ||2  (1)\nt=1\ni=1\nwhere a represents the weighting coefficient, and N denotes\nthe number of personalized concepts. We use gradient de-\nscent to optimize this objective, obtaining the corrected la-\ntent space representation:\nZt/ = Zt  Clayout\n  (2)\nwhere A represents the gradient descent step size. Through\nlayout alignment, we ensure that the generated image mim-\nics the reference image's layout without any confusion."}, {"title": "Concept Injection", "content": "After layout alignment, the original latents zt are replaced\nwith the corrected latents zt, and multipath sampling is\nused to generate the next latents 2t-1. The goal is to inject\nthe subjects generated by different custom branches into the\nbase branch to create a composite image. A naive way is to"}, {"title": "", "content": "spatially fuse the noise predicted by different models:\n efuse = base Mbase + V  MV  (3)\ni=1\nwhere base represents the noise predicted by the base model,\neV represents the noise predicted by the custom model for\nconcept Vi, and Mbase and Mv, are predefined masks. This\nmethod ensures the fidelity of the target concepts but often\nresults in disharmonious images.\nTo address this issue, we propose an attention-based con-\ncept injection technique, including feature fusion and mask\nrefinement, as shown in Figure 5. Spatial fusion is imple-\nmented on the output feature maps of all attention layers in\nthe U-Net decoder, as self-attention controls the structure of\nthe subjects and cross-attention controls their appearance,\nboth crucial for reproducing the attributes of the target con-\ncepts. For each selected attention layer, the fused output fea-\nture is computed as:\nht = hbase Mbase + h Mi\n   (4)\ni=1\nwhere Mbase =   MY Vi. Here, hbase and hy represent\nthe output features of the attention layers of the base model\nand the custom models, respectively, and MV represents the\nbinary mask of concept Vi at timestep t, specifying the dense\ngeneration area of the target concept. The fused feature ht\nis then sent back to the corresponding position in the base\nmodel to replace hbase and complete the denoising process."}, {"title": "", "content": "Since the poses of the generated subjects are uncertain,\npredefined masks may not precisely match the shapes and\ncontours of the target subjects, leading to incomplete ap-\npearances. To address this, we use mask refinement to al-\nlow the masks to adjust according to the shapes and poses\nof the target subjects during the generation process. Inspired\nby local-prompt-mixing(Patashnik et al. 2023), we use self-\nattention-based semantic segmentation to obtain the masks\nof the target subjects. For each target concept Vi, we clus-\nter the self-attention of the custom model ev; to obtain a\nsemantic segmentation map SV and extract the subject's\nmask MV,custom. We perform the same operation on the\nbase model base, obtaining the semantic segmentation map\nSpase and several masks Mi,base, i \u2208 [1,N], each cor-\nresponding to a subject Vi. To reconcile the shape differ-\nences between the subjects in the base model and the cus-\ntom models, the corresponding masks are merged: MV =\nMV custom U MV, base\n For initialization, we perform DDIM inversion on the ref-\nerence image and extract the original masks My from the\nself-attention layers in the same way. An alternative way is\nto use grounding models and segmentation models to extract\nmasks in the pixel space, which provides higher resolution\nmasks but requires additional computation and storage over-\nhead. Through concept injection, we ensure the harmony of\nthe image while fully preserving the attributes of the target\nconcepts."}, {"title": "Experiments", "content": ""}, {"title": "Dataset", "content": "We construct a dataset covering representative categories\nsuch as humans, animals, objects, and buildings, including\n30 personalized concepts. Real and anime human images\nare collected from the Mix-of-show dataset(Gu et al. 2024),\nwhile other categories are sourced from the DreamBooth\ndataset(Ruiz et al. 2023) and CustomConcept101(Kumari\net al. 2023), with 3-15 images per concept. For quantita-\ntive evaluation, we select 10 pairs of visually similar con-\ncepts and generate 5 text prompts for each pair using Chat-\nGPT(OpenAI 2023). We produce 8 samples for each text\nprompt using the same set of random seeds, resulting in a\ntotal of 10\u00d75\u00d78=400 images per method. More details about\nthe dataset are provided in Appendix A.1."}, {"title": "Implementation Details", "content": "Our method is implemented on Stable Diffusion v1.5, us-\ning images generated by SD XL(Podell et al. 2023) as lay-\nout references. In layout alignment, the key of the first self-\nattention layer in the U-Net decoder is used as the layout fea-\nture Ft. For mask refinement, we cluster the attention proba-\nbilities in the sixth self-attention layer of the U-Net decoder\nto extract semantic segmentation maps and scale them to dif-\nferent sizes for feature fusion in different attention layers. In\nall experiments, the weighting coefficient a is set to 1, and\nthe gradient descent step size A is set to 10. More implemen-\ntation details are provided in Appendix A.2."}, {"title": "Evaluation Metrics", "content": "We evaluate multi-concept customization methods from two\nperspectives: text alignment and image alignment. For text\nalignment, we report results on CLIP(Radford et al. 2021)\nand ImageReward(Xu et al. 2024). For image alignment,\nwe introduce a new metric called \"SegSim\" to address the\nlimitations of traditional image similarity methods, which\ncannot reflect attribute leakage and layout conflicts. SegSim\nevaluates fine-grained fidelity by using text-guided ground-\ning models and segmentation models to extract subject seg-\nments from generated and reference images, then calcu-\nlating their similarity. Detailed information is in Appendix\nA.3. We use CLIP(Radford et al. 2021) and DINOV2(Oquab\net al. 2023) to calculate segment similarity and report image\nalignment based on these models. To systematically evalu-\nate omission and redundancy in multi-concept generation,\ngrounding models are used to automatically count the num-\nber of target category subjects in each generated image."}, {"title": "Qualitative Comparison", "content": "We evaluate our method and all baselines on various com-\nbinations of similar concepts, as shown in Figure 6. Custom\nDiffusion and Cones 2 struggle to retain the visual details of\ntarget concepts (e.g., the cat's fur pattern and the backpack's\ndesign), exhibiting severe attribute leakage (e.g., two identi-\ncal boots) and layout confusion (e.g., missing or redundant\nteddy bears). Mix-of-Show demonstrates higher fidelity and"}, {"title": "Quantitative Comparison", "content": "As reported in Table 1, our Concept Conductor significantly\noutperforms previous methods in both image alignment and\ntext alignment. The improvement in image alignment in-\ndicates that our method can preserve the visual details of\neach concept without attribute leakage, primarily due to\nour proposed multipath sampling framework and attention-\nbased concept injection. The improvement in text align-\nment is mainly because our method effectively avoids the\nlayout confusion that leads to unfaithful or disharmonious\nimages through layout alignment, thereby enhancing text-\nimage consistency. The significant reduction in omission\nand redundancy rates also supports this."}, {"title": "Ablation Study", "content": "To verify the effectiveness of the proposed components,\nwe conduct qualitative comparisons of various settings, as\nshown in Figure 8. In Figure 8(a), removing layout align-\nment lead to incorrect layouts, including appearance trunca-\ntion (e.g., two dogs incorrectly stitched together) and con-\ncept omission (e.g., missing turquoise cup). Figures 8(b)\nand 8(c) show that disabling either self-attention or cross-\nattention features during concept injection results in a de-\ncline in fidelity, indicating that both are crucial for preserv-"}, {"title": "", "content": "ing the visual details of the target concepts. Figure 8(d)\ndemonstrates that ablating mask refinement can cause gen-\nerated subjects' contours to not match the target concepts,\nleading to incomplete appearances (e.g., chow chow's fur,\ncorgi's ears, green cup's handle). To avoid randomness, we\nconduct quantitative comparisons of various settings using\nthe same data and evaluation metrics as in the previous sec-\ntion, with results reported in Table 2. As shown in Table\n2, layout alignment effectively avoids omission and redun-\ndancy, significantly improving the alignment of generated\nimages with textual semantics. Feature fusion in both self-\nattention and cross-attention layers leads to higher image\nalignment, as both are crucial for reproducing the attributes\nof the target concepts. Mask refinement further improves\ntext alignment and image alignment by optimizing the edge\ndetails of the generated subjects."}, {"title": "Conclusion", "content": "We introduce Concept Conductor, a novel inference frame-\nwork designed to generate realistic images containing mul-\ntiple personalized concepts. By employing multipath sam-\npling and layout alignment, we addressed the common is-\nsues of attribute leakage and layout confusion in multi-\nconcept personalization. Additionally, concept injection is\nused to create harmonious composite images. Experimental\nresults demonstrate that Concept Conductor can consistently\ngenerate composite images with correct layouts, fully pre-\nserving the attributes of each concept, even when the target\nconcepts are highly similar or numerous."}]}