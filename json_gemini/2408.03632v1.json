{"title": "Concept Conductor: Orchestrating Multiple Personalized Concepts in Text-to-Image Synthesis", "authors": ["Zebin Yao", "Fangxiang Feng", "Ruifan Li", "Xiaojie Wang"], "abstract": "The customization of text-to-image models has seen significant advancements, yet generating multiple personalized concepts remains a challenging task. Current methods struggle with attribute leakage and layout confusion when handling multiple concepts, leading to reduced concept fidelity and semantic consistency. In this work, we introduce a novel training-free framework, Concept Conductor, designed to ensure visual fidelity and correct layout in multi-concept customization. Concept Conductor isolates the sampling processes of multiple custom models to prevent attribute leakage between different concepts and corrects erroneous layouts through self-attention-based spatial guidance. Additionally, we present a concept injection technique that employs shape-aware masks to specify the generation area for each concept. This technique injects the structure and appearance of personalized concepts through feature fusion in the attention layers, ensuring harmony in the final image. Extensive qualitative and quantitative experiments demonstrate that Concept Conductor can consistently generate composite images with accurate layouts while preserving the visual details of each concept. Compared to existing baselines, Concept Conductor shows significant performance improvements. Our method supports the combination of any number of concepts and maintains high fidelity even when dealing with visually similar concepts.", "sections": [{"title": "Introduction", "content": "Text-to-image diffusion models(Nichol et al. 2021; Saharia et al. 2022; Ramesh et al. 2022; Rombach et al. 2022; Podell et al. 2023) have achieved remarkable success in generating realistic high-resolution images. Building on this foundation, techniques for personalizing these models have also advanced. Various methods for single-concept customization(Dong, Wei, and Lin 2022; Ruiz et al. 2023; Gal et al. 2022; Voynov et al. 2023; Alaluf et al. 2023) have been proposed, enabling the generation of images of the target concept in specified contexts based on user-provided visual conditions. These methods allow users to place real-world subjects into imagined scenes, greatly enriching the application scenarios of image generation.\nDespite the excellent performance of existing methods for single-concept customization, handling multiple concepts remains challenging. Current methods(Kumari et al. 2023; Liu et al. 2023; Gu et al. 2024) often mix the attributes of multiple concepts or fail to align well with the given text prompts, especially when the target concepts are visually similar (e.g., a cat and a dog). We categorize these failures as attribute leakage and layout confusion. Layout confusion can be further divided into concept omission, subject redundancy, and appearance truncation, as shown in Figure 1. Attribute leakage denotes the application of one concept's attributes to another (e.g., a cat acquiring the fur and eyes of a dog). Concept omission indicates one or more target concepts not appearing in the image (e.g., the absence of the target cat). Subject redundancy refers to the appearance of extra subjects similar to the target concept (e.g., an extra cat). Appearance truncation signifies the target concept's appearance being observed only in a partial area of the subject (e.g., the upper half of a dog and the lower half of a cat).\nTo address these challenges, we introduce Concept Conductor, a novel inference framework for multi-concept customization. This framework aims to seamlessly integrate multiple personalized concepts with accurate attributes and layout into a single image based on the given text prompts, as illustrated in Figure 2. Our method comprises three key components: multi-path sampling, layout alignment, and concept injection. Multi-path sampling allows the base model and different single-concept models to retain their independent denoising processes, thereby preventing attribute leakage between concepts. Instead of training a model containing multiple concepts to directly generate the final image, we let each single-concept model first focus on generating its corresponding concept. These generated subjects are then integrated into a single image, avoiding interference and conflict between concepts. Layout alignment ensures each model produces the correct layout, fundamentally addressing layout confusion. Specifically, we borrow the layout from a normal image and align the intermediate representations produced by each model in the self-attention layers with it. This reference image is flexible and easy to obtain. It can be a real photo, generated by advanced text-to-image models, or even a simple collage created by the user. Concept injection enables each concept to fully inject its visual features into the final generated image, ensuring harmony. We use shape-aware masks to define the generation area for each concept and inject the visual details (including structure and appearance) of personalized concepts through feature fusion in the attention layers. At each step of the denoising process, we first use layout alignment to correct the input latent space representation, and then use concept injection to obtain the next representation. Multi-path sampling is implemented in both layout alignment and concept injection to ensure the independence of each subject and coordination between different subjects.\nTo evaluate the effectiveness of the proposed method, we create a new dataset containing 30 concepts, covering representative categories such as humans, animals, objects, and buildings. We also introduce a fine-grained metric specifically designed for multi-concept customization to measure the visual consistency between the generated subjects and the given concepts. Extensive experiments demonstrate that our method can consistently generate composite images with correct layouts while fully preserving the attributes of each personalized concept, regardless of the number or similarity of the target concepts. Both qualitative and quantitative comparisons highlight the advantages of our method in terms of concept fidelity and alignment with textual semantics.\nOur contributions can be summarized as follows:\n\u2022 We propose Concept Conductor, an inference framework for multi-concept customization that utilizes multiple single-concept models to combine personalized concepts into a harmonious image, preserving visual details of each concept.\n\u2022 We develop a self-attention-based spatial guidance method to align the layout of the generated image with an easily obtainable reference image, ensuring correct layout. It maintains the diversity of the target subjects' poses and avoids the degradation of image quality caused by coarse visual conditions.\n\u2022 Extensive experiments validate our method's effectiveness and superiority over existing methods."}, {"title": "Related Work", "content": "In recent years, text-to-image diffusion models have excelled in generating realistic and diverse images, becoming the mainstream approach in this field. Trained on large-scale datasets like LAION(Schuhmann et al. 2022), models such as GLIDE(Nichol et al. 2021), DALL-E 2(Ramesh et al. 2022), Imagen(Saharia et al. 2022), and Stable Diffusion(Rombach et al. 2022) can produce high-quality and text-aligned outputs. However, these models struggle to understand the relationships between multiple concepts, resulting in generated content that fails to fully convey the original semantics. This issue is exacerbated when dealing with visually similar concepts. In this work, we apply our method to the publicly available Stable Diffusion(Rombach et al. 2022), which is based on the Latent Diffusion Model (LDM) architecture. LDM operates in the latent space of a Variational Autoencoder (VAE), iteratively denoising to recover the latent representation of an image from Gaussian noise. At each timestep, the noisy latents $z_t$ are fed into the denoising network $\\epsilon_\\theta$, which predicts the current noise $\\epsilon_\\theta(z_t, y, t)$ based on the encoded prompt $y$.\nSeveral works(Ruiz et al. 2023; Gal et al. 2022; Voynov et al. 2023; Alaluf et al. 2023) have customized text-to-image models to generate images of target concepts in new contexts by learning new visual concepts from user-provided example images. For instance, DreamBooth(Ruiz et al. 2023) embeds specific visual concepts into a pre-trained model by fine-tuning its weights. Textual Inversion(Gal et al. 2022) represents new concepts by optimizing a text embedding, later improved by P+(Voynov et al. 2023) and NeTI(Alaluf et al. 2023). Custom Diffusion(Kumari et al. 2023) explores multi-concept customization through joint training, but it requires training a separate model for each combination and often faces severe attribute leakage. Recent works(Liu et al. 2023; Gu et al. 2024) propose frameworks that combine multiple single-concept models and introduce manually defined layouts in attention maps to aid generation. For example, Cones 2(Liu et al. 2023) proposes residual embedding-based concept representations for textual composition and emphasizes or de-emphasizes a concept at a specific location by editing cross-attention. Mix-of-show(Gu et al. 2024) merges multiple custom models into one using gradient fusion and restricts each concept's appearance area through region-controlled sampling. These works cannot fully avoid interference between concepts, leading to low success rates when handling similar concepts or more than two concepts. Additionally, by only manipulating cross-attention and neglecting the impact of self-attention on image structure, these methods often result in mismatched structures and appearances, causing layout control failures. In contrast, our method prevents attribute leakage by isolating the sampling processes of different single-concept models and achieves stable layout control through self-attention-based spatial guidance."}, {"title": "Spatial Control in T2I Diffusion Models", "content": "Using text prompts alone is insufficient for precise control over image layout or structure. Some methods(Avrahami et al. 2023; Li et al. 2023; Zhang, Rao, and Agrawala 2023; Mou et al. 2024) introduce layout conditions by training additional modules to generate controllable images. For example, GLIGEN(Li et al. 2023) adds trainable gated self-attention layers to allow extra input conditions, such as bounding boxes. To achieve finer spatial control, ControlNet(Zhang, Rao, and Agrawala 2023) and T2I-Adapter(Mou et al. 2024) introduce image-based conditions like key-points, sketches, and depth maps by training U-Net encoder copies or adapters. These methods can stably control image structure but limit the target subjects' poses, and these conditional images are difficult for users to create. Some training-free methods achieve spatial guidance by manipulating attention layers during sampling. Most works(Ma et al. 2024; Kim et al. 2023; He, Salakhutdinov, and Kolter 2023) attempt to alleviate attribute leakage and control layout by directly editing attention maps but have low success rates. Several gradient-based methods (Couairon et al. 2023; Xie et al. 2023; Phung, Ge, and Huang 2024) calculate the loss between attention and the given layout and introduce layout information into the latent space representation by optimizing the loss gradients. These methods align the generated image layout with coarse visual conditions (e.g., bounding boxes and semantic segmentation maps), often resulting in high-frequency detail loss and image quality degradation, even with complex loss designs. In this work, we propose extracting layout information from an easily obtainable reference image as a supervisory signal, which not only stably controls the layout but also preserves the diversity of the generated subjects' poses while avoiding image distortion."}, {"title": "Method", "content": "LDM employs a U-Net as the denoising network, consisting of a series of convolutional layers and transformer blocks. In each block, intermediate features produced by the convolutional layers are passed to a self-attention layer followed by a cross-attention layer. Given an input feature $h_{in}$, the output feature in each attention layer is computed as $h_{out} = AV$, where $A = \\text{softmax}(QK^T)$. Here, $Q = f_q(h_{in})$, $K = f_k(c)$, and $V = f_v(c)$ are obtained through learned projectors $f_q, f_k,$ and $f_v$, with $c = h_{in}$ for self-attention and $c = y$ for cross-attention. Self-attention enhances the quality of the generated image by capturing long-range dependencies in the image features, while cross-attention integrates textual information into the generation process, enabling the generated image to reflect the content of the text prompt. Furthermore, extensive researches (Liu et al. 2024; Patashnik et al. 2023; Hertz et al. 2022) have shown that self-attention controls the structure of the image (e.g., shapes, geometric relationships), whereas cross-attention controls the appearance of the image (e.g., colors, materials, textures)."}, {"title": "Preliminary: Attention Layers in LDM", "content": "Text-alignment\nMethod CLIP-T\nIR\nCD\n0.2939\n0.0133\nCones2 0.2954 0.0973\nImage-alignment\nCLIP-I\nDINO\n0.8333 0.6829\n0.8415\n0.6928\n0.8928\n0.7961\nCounting\nn < 2 n > 2\n0.2750\n0.1050\n0.3600\n0.0650\n0.3400\n0.0400\n1.  2542\n0.9190\n0.8569\n0.0075\n0.0350\nTable 1: Quantitative Comparison of Multi-Concept Custo-\nmization Methods. IR stands for ImageReward, CD for\nCustom Diffusion, and MoS for Mix-of-Show. n < 2 in-dicates omission, while n > 2 indicates redundancy."}, {"title": "Overview of Concept Conductor", "content": "Our method comprises three components: multi-path sampling, layout alignment, and concept injection, as illustrated in Figure 2. At each denoising step, we first correct the input latents $z_t$ through layout alignment, obtaining new latents $z_t'$ that carry the layout information from the reference image. Then, we inject the personalized concepts from the custom models into the base model and denoise $z_t'$ to obtain the next latents $z_{t-1}$. Multi-path sampling is implemented in both layout alignment and concept injection to ensure the independence of each concept and coordination between different subjects."}, {"title": "Multipath Sampling", "content": "Joint training or model fusion methods often lead to attribute leakage between different concepts (as shown in Figure 1) and require additional optimization steps for each combination. To directly utilize multiple existing single-concept models for composite generation without attribute leakage, we propose a multipath sampling structure. This structure incorporates a base model $\\epsilon_{\\theta}^{base}$ and multiple custom models $\\epsilon_{\\theta}^{V_i}$(implemented with ED-LoRA(Gu et al. 2024)) for personalized concepts $V_i$, as illustrated in Figure 3.\nGiven several custom models $\\epsilon_{\\theta}^{V_i}$ and a text prompt $p$, at each timestep $t$, we maintain the independent denoising process for each model: $\\epsilon_{\\theta}^{V_i} = \\epsilon_{\\theta_i}(z_t, t, p)$. When the prompt contains similar concepts, models may struggle to distinguish them, leading to attribute leakage. Therefore, we edit the input text prompt for each custom model to help them focus on generating the corresponding single concept. Given a base prompt $P_{base}$, we replace tokens visually similar to the target concept $V_i$ with tokens representing the target concept, creating a prompt variant $P_{V_i}$. For example, for the prompt \u201cA dog and a cat on the beach\u201d and concepts of a dog $V_1$ and a cat $V_2$, we edit the text to obtain two modified prompts: $P_{V_1}$ = \u201cA < $V_1$ > and a < $V_1$ > on the beach\u201d and $P_{V_2}$ = \u201cA < $V_2$ > and a < $V_2$ > on the beach\u201d.\nAfter editing the prompts, the denoising process for the custom models can be expressed as: $\\epsilon_{\\theta}^{V_i} = \\epsilon_{\\theta_i}(z_t, t, P_{V_i})$. Meanwhile, the base prompt is sent to the base model to retain global semantics: $\\epsilon^{base} = \\epsilon_{\\theta}^{base}(z_t, t, P_{base})$. Through multipath sampling, each custom model receives only information relevant to its corresponding concept, fundamentally preventing attribute leakage between different concepts."}, {"title": "Layout Alignment", "content": "Existing multi-concept customization methods often suffer from layout confusion (as shown in Figure 1), especially when the target concepts are visually similar or numerous. To address these challenges, we introduce a reference image to correct the layout during the generation process. For example, to generate an image of a specific dog and cat in a specific context, we only need a reference image containing an ordinary cat and dog. One simple approach to achieve layout control is to convert the reference image into abstract visual conditions (e.g., keypoints or sketches) and then use ControlNet(Zhang, Rao, and Agrawala 2023) or T2I-Adapter(Mou et al. 2024) for spatial guidance, which limits variability and flexibility and may reduce the fidelity of the target concepts. Another approach is to directly inject the full self-attention of the reference image into the generation process, transferring the overall structure of the image(Hertz et al. 2022). This strictly limits the poses of the target subjects, reducing the diversity of the generated images. Additionally, it requires structural similarity between the reference image subjects and target concepts to avoid distortions caused by shape mismatches.\nTo align the layout while preserving the structure of the target concepts, we propose a gradient-guided approach, as shown in Figure Figure 4. Given a reference image, we perform DDIM inversion to obtain the latent space representation $z_{ref}$ and record the self-attention features $F_{ref}^t$ at each timestep. Similarly, we record the self-attention features of the base model and each custom model during the generation process, denoted as $F_{base}^t$ and $F_{V_i}^t$, respectively, as shown in Figure 2. An optimization objective is set to encourage the generated image\u2019s layout to align with the given layout:\n$C_{layout} = ||F_{base}^t - F_{ref}^t||_2 + \\sum_{i=1}^N \\alpha \\sum_{t} ||F_{V_i}^t - F_{ref}^t||_2$ (1)\nwhere $\\alpha$ represents the weighting coefficient, and $N$ denotes the number of personalized concepts. We use gradient descent to optimize this objective, obtaining the corrected latent space representation:\n$z_t' = z_t - \\lambda \\frac{\\partial C_{layout}}{\\partial z_t}$ (2)\nwhere $\\lambda$ represents the gradient descent step size. Through layout alignment, we ensure that the generated image mimics the reference image\u2019s layout without any confusion."}, {"title": "Concept Injection", "content": "After layout alignment, the original latents $z_t$ are replaced with the corrected latents $z_t'$, and multipath sampling is used to generate the next latents $z_{t-1}$. The goal is to inject the subjects generated by different custom branches into the base branch to create a composite image. A naive way is to spatially fuse the noise predicted by different models:\n$\\epsilon_{fuse} = \\epsilon^{base} M^{base} + \\sum_{i=1}^N \\epsilon^{V_i} M^{V_i}$ (3)\nwhere $\\epsilon^{base}$ represents the noise predicted by the base model, $\\epsilon^{V_i}$ represents the noise predicted by the custom model for concept $V_i$, and $M^{base}$ and $M^{V_i}$ are predefined masks. This method ensures the fidelity of the target concepts but often results in disharmonious images.\nTo address this issue, we propose an attention-based concept injection technique, including feature fusion and mask refinement, as shown in Figure 5. Spatial fusion is implemented on the output feature maps of all attention layers in the U-Net decoder, as self-attention controls the structure of the subjects and cross-attention controls their appearance, both crucial for reproducing the attributes of the target concepts. For each selected attention layer, the fused output feature is computed as:\n$h_t = h^{base} M^{base} + \\sum_{i=1}^N h^{V_i} M^{V_i}$ (4)\nwhere $M^{base} = 1 - \\bigcup_{i=1}^N M^{V_i}$. Here, $h^{base}$ and $h^{V_i}$ represent the output features of the attention layers of the base model and the custom models, respectively, and $M^{V_i}$ represents the binary mask of concept $V_i$ at timestep $t$, specifying the dense generation area of the target concept. The fused feature $h_t$ is then sent back to the corresponding position in the base model to replace $h^{base}$ and complete the denoising process.\nSince the poses of the generated subjects are uncertain, predefined masks may not precisely match the shapes and contours of the target subjects, leading to incomplete appearances. To address this, we use mask refinement to allow the masks to adjust according to the shapes and poses of the target subjects during the generation process. Inspired by local-prompt-mixing(Patashnik et al. 2023), we use self-attention-based semantic segmentation to obtain the masks of the target subjects. For each target concept $V_i$, we cluster the self-attention of the custom model $\\epsilon_{\\theta_i}$ to obtain a semantic segmentation map $S^{V_i}$ and extract the subject\u2019s mask $M_t^{V_i,custom}$. We perform the same operation on the base model $\\epsilon_{\\theta}^{base}$, obtaining the semantic segmentation map $S^{base}$ and several masks $M_t^{V_i,base}$, $i \\in [1,N]$, each corresponding to a subject $V_i$. To reconcile the shape differences between the subjects in the base model and the custom models, the corresponding masks are merged: $M_t^{V_i} = M_t^{V_i,custom} \\bigcup M_t^{V_i, base}$.\nFor initialization, we perform DDIM inversion on the reference image and extract the original masks $M_t^{V_i}$ from the self-attention layers in the same way. An alternative way is to use grounding models and segmentation models to extract masks in the pixel space, which provides higher resolution masks but requires additional computation and storage overhead. Through concept injection, we ensure the harmony of the image while fully preserving the attributes of the target concepts."}, {"title": "Experiments", "content": "We construct a dataset covering representative categories such as humans, animals, objects, and buildings, including 30 personalized concepts. Real and anime human images are collected from the Mix-of-show dataset(Gu et al. 2024), while other categories are sourced from the DreamBooth dataset(Ruiz et al. 2023) and CustomConcept101(Kumari et al. 2023), with 3-15 images per concept. For quantitative evaluation, we select 10 pairs of visually similar concepts and generate 5 text prompts for each pair using ChatGPT(OpenAI 2023). We produce 8 samples for each text prompt using the same set of random seeds, resulting in a total of 10\u00d75\u00d78=400 images per method."}, {"title": "Implementation Details", "content": "Our method is implemented on Stable Diffusion v1.5, using images generated by SD XL(Podell et al. 2023) as layout references. In layout alignment, the key of the first self-attention layer in the U-Net decoder is used as the layout feature $F_t$. For mask refinement, we cluster the attention probabilities in the sixth self-attention layer of the U-Net decoder to extract semantic segmentation maps and scale them to different sizes for feature fusion in different attention layers. In all experiments, the weighting coefficient $\\alpha$ is set to 1, and the gradient descent step size $\\lambda$ is set to 10."}, {"title": "Baselines", "content": "We compare our method with three multi-concept customization methods: Custom Diffusion, Cones 2, and Mix-of-Show. For Custom Diffusion, we use the diffusers(von Platen et al. 2022) version implementation, while for the other methods, we use their official code implementations. All experimental settings followe the official recommendations. For Cones 2 and Mix-of-Show, grounding models are used to extract bounding boxes of target subjects from the layout reference image to ensure consistent spatial conditions. To ensure a fair comparison, no additional control models like ControlNet or T2I-Adapter are used."}, {"title": "Evaluation Metrics", "content": "We evaluate multi-concept customization methods from two perspectives: text alignment and image alignment. For text alignment, we report results on CLIP(Radford et al. 2021) and ImageReward(Xu et al. 2024). For image alignment, we introduce a new metric called \u201cSegSim\u201d to address the limitations of traditional image similarity methods, which cannot reflect attribute leakage and layout conflicts. SegSim evaluates fine-grained fidelity by using text-guided grounding models and segmentation models to extract subject segments from generated and reference images, then calculating their similarity. We use CLIP(Radford et al. 2021) and DINOV2(Oquab et al. 2023) to calculate segment similarity and report image alignment based on these models. To systematically evaluate omission and redundancy in multi-concept generation, grounding models are used to automatically count the number of target category subjects in each generated image."}, {"title": "Qualitative Comparison", "content": "We evaluate our method and all baselines on various combinations of similar concepts, as shown in Figure 6. Custom Diffusion and Cones 2 struggle to retain the visual details of target concepts (e.g., the cat\u2019s fur pattern and the backpack\u2019s design), exhibiting severe attribute leakage (e.g., two identical boots) and layout confusion (e.g., missing or redundant teddy bears). Mix-of-Show demonstrates higher fidelity and mitigates attribute leakage but still faces significant concept omission (e.g., missing cartoon backpack) and appearance truncation (e.g., stitched teddy bear). In contrast, our Concept Conductor generates all target concepts with high fidelity without leakage through multipath sampling and concept injection, ensuring correct layout through layout alignment. Our method maintains stable performance across different concept combinations, even when the target concepts are very similar, such as two teddy bears.\nWe further explore more challenging scenarios and compare our method with Mix-of-Show, as shown in Figure 7. When handling three similar concepts, Mix-of-Show exhibits severe attribute leakage (e.g., both men wearing glasses) and concept omission (e.g., one person missing). Additionally, Mix-of-Show struggles with dense layouts, often resulting in appearance truncation when faced with complex spatial relationships (e.g., the upper half of a cat and the lower half of a dog stitched together). In contrast, our method maintains the visual features of each concept without attribute leakage and faithfully reflects the layout described in the text, even in these complex scenarios."}, {"title": "Quantitative Comparison", "content": "As reported in Table 1, our Concept Conductor significantly outperforms previous methods in both image alignment and text alignment. The improvement in image alignment indicates that our method can preserve the visual details of each concept without attribute leakage, primarily due to our proposed multipath sampling framework and attention-based concept injection. The improvement in text alignment is mainly because our method effectively avoids the layout confusion that leads to unfaithful or disharmonious images through layout alignment, thereby enhancing text-image consistency. The significant reduction in omission and redundancy rates also supports this."}, {"title": "Ablation Study", "content": "To verify the effectiveness of the proposed components, we conduct qualitative comparisons of various settings, as shown in Figure 8. In Figure 8(a), removing layout alignment lead to incorrect layouts, including appearance truncation (e.g., two dogs incorrectly stitched together) and concept omission (e.g., missing turquoise cup). Figures 8(b) and 8(c) show that disabling either self-attention or cross-attention features during concept injection results in a decline in fidelity, indicating that both are crucial for preserving the visual details of the target concepts. Figure 8(d) demonstrates that ablating mask refinement can cause generated subjects\u2019 contours to not match the target concepts, leading to incomplete appearances (e.g., chow chow\u2019s fur, corgi\u2019s ears, green cup\u2019s handle). To avoid randomness, we conduct quantitative comparisons of various settings using the same data and evaluation metrics as in the previous section, with results reported in Table 2. As shown in Table 2, layout alignment effectively avoids omission and redundancy, significantly improving the alignment of generated images with textual semantics. Feature fusion in both self-attention and cross-attention layers leads to higher image alignment, as both are crucial for reproducing the attributes of the target concepts. Mask refinement further improves text alignment and image alignment by optimizing the edge details of the generated subjects."}, {"title": "Conclusion", "content": "We introduce Concept Conductor, a novel inference framework designed to generate realistic images containing multiple personalized concepts. By employing multipath sampling and layout alignment, we addressed the common issues of attribute leakage and layout confusion in multi-concept personalization. Additionally, concept injection is used to create harmonious composite images. Experimental results demonstrate that Concept Conductor can consistently generate composite images with correct layouts, fully preserving the attributes of each concept, even when the target concepts are highly similar or numerous."}]}