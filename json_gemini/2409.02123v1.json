{"title": "PUYUN: MEDIUM-RANGE GLOBAL WEATHER FORECASTING USING LARGE KERNEL ATTENTION CONVOLUTIONAL NETWORKS", "authors": ["Shengchen Zhu", "Yiming Chen", "Peiying Yu", "Xiang Qu", "Yuxiao Zhou", "Yiming Ma", "Zhizhan Zhao", "Yukai Liu", "Hao Mi", "Bin Wang"], "abstract": "Accurate weather forecasting is essential for understanding and mitigating weather-related impacts. In this paper, we present PuYun, an autoregressive cascade model that leverages large kernel attention convolutional networks. The model's design inherently supports extended weather prediction horizons while broadening the effective receptive field. The integration of large kernel attention mechanisms within the convolutional layers enhances the model's capacity to capture fine-grained spatial details, thereby improving its predictive accuracy for meteorological phenomena.\nWe introduce PuYun, comprising PuYun-Short for 0-5 day forecasts and PuYun-Medium for 5-10 day predictions. This approach enhances the accuracy of 10-day weather forecasting. Through evaluation, we demonstrate that PuYun-Short alone surpasses the performance of both GraphCast and FuXi-Short in generating accurate 10-day forecasts. Specifically, on the 10th day, PuYun-Short reduces the RMSE for Z500 to 720 m\u00b2/s\u00b2, compared to 732 m\u00b2/s\u00b2 for GraphCast and 740 m\u00b2/s\u00b2 for FuXi-Short. Additionally, the RMSE for T2M is reduced to 2.60 K, compared to 2.63 K for GraphCast and 2.65 K for FuXi-Short. Furthermore, when employing a cascaded approach by integrating PuYun-Short and PuYun-Medium, our method achieves superior results compared to the combined performance of FuXi-Short and FuXi-Medium. On the 10th day, the RMSE for Z500 is further reduced to 638 m\u00b2/s\u00b2, compared to 641 m\u00b2/s\u00b2 for FuXi. These findings underscore the effectiveness of our model ensemble in advancing medium-range weather prediction. Our training code and model will be open-sourced.", "sections": [{"title": "1 Introduction", "content": "Accurate weather forecasting bolsters emergency management and mitigation strategies, thus safeguarding lives. It also helps mitigate substantial financial losses incurred by severe weather incidents and yields significant economic benefits [36, 35, 27, 34, 23, 17]. Traditional methods of weather forecasting rely on numerical weather prediction (NWP), which uses numerical techniques to analyze the atmospheric state as a grid and determine transitions between states[12, 14]. NWP is an essential tool for providing accurate and reliable meteorological data, with positive impacts across various societal sectors [2, 30]. However, due to the highly non-linear and variable nature of weather phenomena[19], NWP based forecasts still struggle with accurately predicting specific scenarios (e.g. monsoons, cyclones and etc.)[4, 18, 37]. Furthermore, the computational expenditure associated with these predictions is exorbitantly high [1, 38].\nIn contrast to NWP models, machine learning (ML) methods exhibit the advantage of not relying on a prior understanding of intricate physical processes. Instead, ML models autonomously discern and leverage patterns inherent in data [24, 7, 8, 33, 6]. Recent strides, exemplified by methods like FourCastNet[29], Pangu[3], Graphcast[21], and FuXi[9],"}, {"title": "2 Method", "content": "In this section, we introduce a new medium-range global weather forecasting model using LKA ConvNets called \"PuYun\". An overview of our method is given in Fig. 1. PuYun leverages the innovative LKA-FCN, offering the unique capability of resolution expansion, a feature absent in ViTs. The input data combines both atmospheric and surface variables, forming a 2 \u00d7 69 \u00d7 721 \u00d7 1440 tensor. Here, 2, 69, 721, and 1440 correspond to the two preceding time steps (t - 1 and t), the total number of variables (C), latitude (H) and longitude (W) grid points, respectively. This combined data is then fed into the PuYun model. Subsequent sections delve into the specifics of each component within the PuYun model."}, {"title": "2.1 PuYun Model", "content": ""}, {"title": "2.1.1 Patch embedding", "content": "The technique of patch embedding, commonly used in computer vision, is applied for dimensionality reduction. This entails transforming the data from its original space into a latent space of C dimensions. As depicted in Fig. 1 and referred to as Patch Embedding, this operation reduces the data dimensionality to C \u00d7 90 \u00d7 180, with a patch size of 8 \u00d7 8 and a stride of 8 \u00d7 8. This optimization enhances computational and memory efficiency for self-attention calculations."}, {"title": "2.1.2 LKA-FCN layers", "content": "Instances of LKA-FCN layer are stacked to constitute the foundational core of the PuYun model, each contributing to the hierarchical feature extraction process. Specifically, the PuYun model incorporates 4 LKA-FCN layers, each comprising 12 blocks. The structure of the block is illustrated in Fig. 2."}, {"title": "2.1.3 Patch merging", "content": "During the patch merging process, the data fed into a fully-connected layer, refining the features from the C channels to dimensions of 69 \u00d7 8 \u00d7 8. Following this transformation, the features undergo a pixel shuffle operation designed to upscale the spatial resolution of the data to 720 \u00d7 1440, subsequently resized to 721 \u00d7 1440. As the final step, the input Xt is added, culminating in the production of the ultimate result."}, {"title": "2.1.4 Autoregressive Forecasting", "content": "PuYun refines input from two weather states, (Xt\u22121, Xt), representing the current time, t, and the immediately preceding time, t - 1, to forecast the weather state at the subsequent time step. The time step considered in this model is 6 hours. Formally, it is expressed as follows:\nXt+1 = PuYun (Xt-1, xt) \\tag{1}\nTo generate a T-step forecast, Xt+1:t+T = (x+1,..., t+T), PuYun employs Equation (1) iteratively in an autoregressive manner. It feeds its own predictions back as input to forecast subsequent steps. For example, to predict step t + 1, the input is (Xt-1, Xt). To predict step t + 2, the input is (xt, xt+1).\nThe pipeline for PuYun's 10-day forecast, with predictions made every 6 hours, is depicted in Fig. 3."}, {"title": "2.2 Loss Function", "content": "PuYun undergoes pre-training with the goal of minimizing a loss function for a single time step against ERA5 targets, employing the gradient descent optimization technique. The specific loss function utilized for single-step training is"}, {"title": "2.3 Datasets", "content": "Our work utilizes ECMWF ReAnalysis v5 (ERA5) [13]. The ERA5 dataset, produced by the ECMWF, represents the fifth generation of ECMWF reanalysis data. Covering from 1940 to the present, it offers comprehensive information on Earth's climate and weather conditions. The data is provided at a 0.25\u00b0 latitude-longitude resolution and spans 37 vertical pressure levels, ranging from 1000 hPa to 1 hPa, making it suitable for various applications in climate research, weather forecasting, and environmental monitoring. We sample 00/06/12/18 time slots from the original ERA5 dataset.\nTo train and test the PuYun model, we utilize 40 years of historical weather data (1979-2018) from ERA5 reanalysis archive. Specifically, data from 1979 to 2016 is used for training, data from 2017 for validation, and data from 2018 for testing. For each pressure level, five variables are considered: geopotential height (Z), relative humidity (R), temperature (T), and the u and v components of wind speed (U, V). Additionally, four surface-level variables are included: 2-meter temperature (2T), u-component and v-component of 10m wind speed (10U, 10V), and mean sea level pressure (MSL).\nDuring the training phase on ERA5, our observations revealed that the inclusion or exclusion of precipitation has negligible effects on the ultimate accuracy of other meteorological variables. As a result, we made the decision to strategically remove precipitation from our model."}, {"title": "2.4 Implementation Details", "content": "The PuYun model is constructed utilizing the PyTorch framework [28]. The pretraining of the model necessitates approximately 96 hours, employing a cluster comprising 32 Nvidia A100 GPUs. The training process employs a batch size of 1 on each GPU and ultimately achieves a total batch size of 32 (we observed that the batch size significantly influences the accuracy. Even with just 8 A100 GPUs, employing gradient accumulation to raise the batch size to 32 allows us to attain comparable outcomes). This process involves 120,000 iterations. We normalized the ERA5 data and compressed it into the Zarr format, resulting in a final data size of 4.3T. The I/O becomes the main bottleneck in the early phase of distributed training in our clusters. We used a data cache strategy to cache a copy of the data on the disk of each GPU machine along the training process, which reduces the entire training process from 492 hours to 96 hours.\nThe AdamW optimizer [16] is employed with parameters \u03b2\u2081 = 0.9 and \u03b22 = 0.95, utilizing an initial learning rate of 1 \u00d7 10-3 and a weight decay coefficient of 0.1. To mitigate overfitting, Scheduled DropPath [22] is implemented with a dropping ratio of 0.2. For efficient memory management during model training, Fully-Sharded Data Parallel (FSDP) [39], bfloat16 floating-point precision, and gradient check-pointing [10] are incorporated."}, {"title": "2.5 Training Procedure", "content": "After training the single-step model, we fine-tuned it to obtain two refined models: PuYun-Short, and PuYun-Medium. The fine-tuning process employed dynamic step autoregressive training, where different GPUs were assigned varying autoregressive steps sampled randomly from 2 to 12. After pre-training, the PuYun-Short model underwent training on data from 1979 to 2016 for 10,000 steps. Subsequently, the PuYun-Short model generated a five-day forecast for the period 2010-2017, serving as input for the PuYun-Medium model. The PuYun-Medium model then underwent another training for 10,000 steps. This finetuning process is specifically designed to achieve optimal performance in generating 6-hourly forecasts for up to 10 days. During finetuning, the learning rate is fixed at 3 \u00d7 10-7, and the batch size is 32."}, {"title": "3 Model Evaluation", "content": "To evaluate the performance of the PuYun model, it is imperative to conduct a comparative analysis with state-of-the-art NWP and ML-bsaed models. In alignment with previous methodologies[21, 3], we have selected 00z and 12z as the initial forecast times. For deterministic forecasts, the Root Mean Square Error (RMSE) and Anomaly Correlation Coefficient (ACC) are utilized as the primary evaluation metrics.\nRMSE represents the latitude-weighted Root Mean Square Error. Given the prediction result $x_{c,i}^{t_{o} + \\tau}$ and its target (ground truth) $x_{c,i}^{t_{o} + \\tau}$, the RMSE is defined as follows:\nCRMSE = \\sqrt{\\frac{1}{|T|} \\sum_{t_{o} \\in T} \\sqrt{ \\frac{1}{|G_{0.25}|} \\sum_{i \\in G_{0.25}} a_{i}(x_{c,i}^{t_{o} + \\tau} - \\hat{x}_{c,i}^{t_{o} + \\tau})^{2}}} \\tag{3}\nHere, c indicates the index for channels, which could be either surface variables or atmospheric variables at certain pressure levels. i \u2208 G0.25 are the location (latitude and longitude) coordinates in the grid. ai is the area of the latitude-longitude grid cell (normalized to unit mean over the grid) which varies with latitude.\nACC is the Latitude-weighted Anomaly Correlation Coefficient that evaluates the performance of dynamical models by comparing their predictions of anomalies (departures from the long-term averaged climatology) to observed anomalies.\nACC = \\frac{\\sum_{t_{o} \\in T}  \\sum_{i \\in G_{0.25}} (\\hat{x}'_{c,i}x'_{c,i})}{\\sqrt{\\sum_{t_{o} \\in T}  \\sum_{i \\in G_{0.25}}  (\\hat{x}'_{c,i})^{2} \\sum_{t_{o} \\in T}  \\sum_{i \\in G_{0.25}} (x'_{c,i})^{2}}} \\tag{4}\nwhere $\\hat{x}'_{c,i} = a_{i}(\\hat{x}_{c,i}^{t_{o} + \\tau} - m_{c,i}^{t_{o} + \\tau})$ and $x'_{c,i} = a_{i}(x_{c,i}^{t_{o} + \\tau} - m_{c,i}^{t_{o} + \\tau})$. In this context, $m_{c,i}^{t_{o} + \\tau}$ is the climatological mean over the day-of-year containing the validity time to + \u03c4 for a given weather variable c at longitude w and latitude h. It is averaged from daily data using ERA5 data from 1993 to 2016."}, {"title": "4 Experimental Results", "content": "PuYun can predict 69 meteorological variables. We selected several representative meteorological variables for comparison, which are as follows.\n\u2022 Z500: Geopotential at 500 hPa.\n\u2022 T500: Temperature at 500 hPa.\n\u2022 U500, V500: U and V components of wind at 500 hPa.\n\u2022 2T: 2 meter temperature.\n\u2022 10U, 10V: U and V components of wind at 10 meter.\n\u2022 MSL: Mean sea level pressure.\n2T, 10U, 10V and MSL are surface meteorological variables that are directly perceptible to humans. The 500hPa level, representing atmospheric pressure at 500 millibars, holds significance in meteorology due to its typical location in the mid-level atmosphere, approximately halfway up in the atmosphere's vertical extent. This positioning enables meteorological variables at the 500hPa level to provide crucial insights into large-scale weather systems.\nWe compared PuYun with leading ML based models Pangu, GraphCast and FuXi."}, {"title": "4.1 Quantitative Comparison", "content": "To evaluate the performance of PuYun, we utilize data from the year 2018 and selected two daily initialization times (00:00 UTC and 12:00 UTC) to generate 10-day forecasts with 6-hour intervals."}, {"title": "4.1.1 Quantitative Skill Evaluation", "content": "We conduct a comparative analysis of PuYun's forecast performance against state-of-the-art ML-based models (Pangu, GraphCast and FuXi) using 0.25\u00b0 resolution ERA5 data. For 10-day forecasts with a single model, we compared Pangu (using the official greedy strategy across four models), Graphcast, FuXi-Short, and PuYun-Short. Fig. 4 and Fig.5 present the globally averaged latitude-weighted ACC and RMSE results for four atmospheric layer variables (Z500, T500, U500, and V500) and four surface variables (MSL, 2T, 10U, and 10V). In terms of the RMSE metric,"}, {"title": "4.1.2 Quantitative Predication Evaluation", "content": "We present visualizations of PuYun's predicted results at lead days 3, 5, and 10 for two variables: Z500 and 2T, comparing them with ERA5. In Fig. 6 and Fig. 7, the first two rows display the state sequences from PuYun and ERA5, while the third row shows the absolute error between PuYun and ERA5. The results indicate that PuYun's predictions closely align with ERA5 on the third day. As the forecast lead time increases, the absolute error gradually grows. These visualizations confirm PuYun's ability to approximate real data and provide accurate weather state estimates."}, {"title": "4.2 Ablation Experiments", "content": "To scrutinize the pivotal components influencing our model's performance, we conducted a series of ablation experiments focusing on the one-step (6h) prediction RMSE values of meteorological elements (Z500, 2T, 10U, MSL) under various structural configurations. These experiments are configured with a default iteration of 30,000 steps, a batch size of 8, and a learning rate of 1e-3, maintaining consistency with all other conditions as described in Section 2.5. The first row of Table 2 outlines the adjustable structures, including the number of dimensions in the basic block (768d), the number of blocks within each FCN-layer((6,6,6,6)), the kernel size of corresponding convolutions(@K5), and the method of patch merging([resize]).\nEach subsequent row presents a modification relative to the first row's structure, keeping all other aspects unchanged. For instance, rows 2/3/4 only vary in kernel size relative to row 1, whereas row 5 changes both kernel size and the patch merging strategy compared to row 1. The results indicate that larger kernel sizes generally enhance performance, but there is negligible difference between sizes 9 and 11, making size 9 a preferable choice for reducing memory usage. Employing a resize strategy leads to rapid initial convergence, which further improves with the incorporation of the pixelshuffle strategy as the number of steps increases. A substantial reduction in RMSE is observed when the number of dimensions in the basic block is increased from 768 to 1536, and the number of blocks in the FCN-layer is doubled from 6 to 12."}, {"title": "4.3 Evaluating the Effectiveness of Dynamic Step Autoregressive Training", "content": "After obtaining the model from single-step prediction, we applied dynamic step autoregressive training for fine-tuning to enhance the model's performance in multi-step forecasting. In dynamic step autoregressive training, a maximum step length M is set, and during each training iteration, different GPUs are randomly assigned autoregressive steps ranging from 2 to M. At the end of each iteration, the gradients from all GPUs are aggregated, followed by a model parameter update.\nWe gradually increased M from 2 to 12, training for 3000 iterations at each experiment. The results, as shown in the Fig. 8, indicate that larger M lead to lower RMSE at longer prediction steps. Specifically, when M=12, the RMSE for 20-step predictions is more than 10% lower compared to M=1."}, {"title": "4.4 Effectiveness of the Cascade Model Design", "content": "In this section, we ablate over the influence of the cascade ML model architecture on alleviating accumulation errors in weather forecasting. We utilize a single PuYun model (PuYun-Short) to generate 10-day forecasts and evaluate its efficacy in comparison to PuYun (A cascade model of PuYun-Short and PuYun-Medium). Fig. 9 illustrates the performance PuYun-Short and PuYun, spanning lead times from 0 to 10 days. The performance of single PuYun-Short predictions diminishes significantly with increasing lead times. We assert this is primarily attributed to accumulation errors."}, {"title": "5 Conclusion", "content": "In recent years, data-driven ML models have made significant advancements in global medium-range weather forecasting, surpassing even the HRES. However, these ML models, while resource-intensive during training, are currently confined to a spatial resolution of 0.25\u00b0. Such resolution falls significantly short of practical operational demands in critical industries like renewable energy generation, agriculture, and transportation. Furthermore, the persistent challenge of accumulation errors in medium-range forecasts adds another layer of complexity to accurate predictions.\nIn response to these challenges, we introduce PuYun, a novel approach based on LKA-FCN featuring a cascading structure. PuYun excels in generating 10-day forecasts, updating every 6 hours, encompassing four surface variables and five atmospheric variables across 13 vertical pressure levels.\nLeveraging LKA convolutions enhances PuYun's effective receptive field, allowing it to capture meteorological changes in adjacent areas and gain a deeper understanding of the meteorological system. PuYun introduces dynamic step autoregressive training, extending the model's predictive capability from single-step to multi-step forecasts. Using only a single model, PuYun-Short achieved state-of-the-art performance in predicting global weather for the next 10 days, highlighting the potential of dynamic step autoregressive training and large kernel attention (LKA) in meteorological forecasting. When employing a model cascading strategy for forecasting global weather over the next 10 days, PuYun also achieved world-leading performance on over 80% of meteorological variables. This result further underscores the effectiveness of the model cascading approach.\nPuYun demonstrates the capability to be fine-tuned on the HRES-fc0-0.25d dataset and subsequently utilize the HRES-fc0-0.1d dataset as input for generating global forecasts at a higher resolution of 0.1\u00b0. This capability highlights our forthcoming objective of achieving high-resolution global predictions."}, {"title": "6 Future Work", "content": "It is essential to recognize that current machine learning models heavily rely on initial fields produced by numerical weather prediction (NWP) models. We are witnessing an exponential increase in global meteorological observation instruments, accompanied by a continual enhancement in the temporal resolution of observations. Looking ahead, our objective is to develop end-to-end global weather forecasting algorithms that utilize global observational data as input, enabling real-time generation of medium-range weather forecasts, while also advancing the forecasting resolution to 0.1\u00b0."}]}