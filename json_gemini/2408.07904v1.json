{"title": "Assessing Language Models' Worldview for Fiction Generation", "authors": ["Aisha Khatun", "Daniel G. Brown"], "abstract": "The use of Large Language Models (LLMs) has become ubiquitous, with abundant applications in computational creativity. One such application is fictional story generation. Fiction is a narrative that occurs in a story world that is slightly different than ours. With LLMs becoming writing partners, we question how suitable they are to generate fiction. This study investigates the ability of LLMs to maintain a state of world essential to generate fiction. Through a series of questions to nine LLMs, we find that only two models exhibit consistent worldview, while the rest are self-conflicting. Subsequent analysis of stories generated by four models revealed a strikingly uniform narrative pattern. This uniformity across models further suggests a lack of 'state' necessary for fiction. We highlight the limitations of current LLMs in fiction writing and advocate for future research to test and create story worlds for LLMs to reside in.", "sections": [{"title": "Introduction", "content": "One of the important ways humans express creativity is through fiction. Following the progress of AI, Large Language Models (LLMs) have been used in computational creativity in many ways, and fiction is no exception (Patel et al. 2024; Tang, Loakman, and Lin 2023). Given the complexities of story generation, LLMs are still lacking in generating new and interesting stories. This paper explores one very important aspect of why that is.\nStory writing involves, among other things, crafting an interesting plot, developing the plot with coherence, writing with linguistic competence, and using devices like metaphors, symbolism, etc (Glucksberg and McGlone 2001). To generate fiction, an LLM should have knowledge of the world because an author will retain some aspects of the real world while changing others - thus creating a fictional world where the story and its characters reside. For example, the story of Harry Potter occurs in a magical world that defies certain notions of reality through magic, yet retains ideas like education, love, friendship, and so on. An LLM should also be able to differentiate between truth (real world) and fiction. This ensures an author can list a few facts about a fictional world or alter some facts about the real world and tap into a fictional world where the story resides. This begs the question if the knowledge representation in LLMs can store and use facts at all. Therefore, we narrow down our research to answer the following questions:\n\u2022 Can LLMs identify and reproduce information consistently?\n\u2022 Are LLMs robust to changes in prompt language when reproducing information?\nTo this end, we analyze nine LLMs, small and large, closed- and open-source, to identify if LLMs can produce fiction using a set of simple questions that discern truth from fiction. Our findings suggest that even the best and largest LLMs to-date have difficulty maintaining consistency in their responses and are tuned to dodge certain topics like stereotypes, making them difficult to incorporate them into stories. We generate stories from four models and the quality of these stories corroborate our results. All code, dataset, and the generated responses can be found in GitHub."}, {"title": "Related Work", "content": "Automated story generation has been a topic of interest much before LLMS or even Deep Learning became popularized, from generating plot (P\u00e9rez y P\u00e9rez and Sharples 2001; Riedl and Young 2004), to generating entire stories (Riedl and Young 2003). Some of the recent research projects include human-AI co-creation (Yuan et al. 2022), plot writing (Jin, Kadam, and Wanvarie 2022), and emphases on story coherence (Yang et al. 2023), long-story generation (Yang et al. 2022), and incorporating LLMs into storytelling (Patel et al. 2024; Andrus et al. 2022). It is clear LLMs are not yet capable of generating compelling stories with minimal intervention.\nOur aim is to assess LLMs directly as creative writers (i.e. without much prompt engineering or fine-tuning) and keeping in mind ease of use (i.e. without multiple turns of conversations or chains of thought). We approach story generation as another task a generalist AI model performs (since indeed humans have this skill), and specifically focus on whether LLMs are capable of differentiating and leveraging the distinction between fact and fiction in the story-generation task."}, {"title": "Dataset", "content": "We evaluate nine LLMs showing the state of LLMs as assistive tools for writers or creators on their own, by assessing these models' consistency and robustness to facts.\nAnother line of work that focuses on facts is misinformation detection. An important distinction with our work is that we do no expect a model to know the correct fact, rather we expect a model be consistent with whatever information it has and be able to produce stories with it.\nWe use a dataset of six topics (885 statements) that are quick differentiators between truth and falsehood and therefore statements that can used to create a fictional world. Table 1 shows the category distribution. Details about data collection and cleaning can be found in the original papers (Khatun and Brown 2024b). We do not use the ground truth provided in the dataset since this work is concerned with creating a story world and whether that world aligns with the real world is comparatively unimportant."}, {"title": "Models", "content": "We use the OpenLLM leaderboard to select a total of 5 open-source models by best performance and community votes as of December 2023. Additionally, we also select the 4 latest OpenAI models. This paper is part of a larger project about truth and lies in language models, where we analyze almost 40 LLMs and find that most LLMs fail to answer very simple questions or even understand the task of answering multiple-choice questions (Khatun 2024). This current work specifically focuses on nine LLMs that show promise and we believe are worth analyzing for story generation purposes. There are three small and two large open-source models, and four closed-source OpenAI models. A list of the selected models is provided below:"}, {"title": "Prompts", "content": "To generate fiction we want models to have a fixed state of the world, irrespective of the facts in that world being truth or fiction. The idea is to eventually inject fictional facts or alter real facts to generate fictional facts that would transform the state of the world. Then the model can be prompted to generate a story in the new transformed fictional world. To test whether models have an existing state of world, we ask the models a series of Yes/No questions with statements from the dataset described above. The questions have essentially identical content but approached from slightly different angles, with the assumption that the model should give consistent responses across all variations. The questions are as follows:\n\u2022 PO. [SENT]. Is this true?\n\u2022 P1. [SENT]. Is this true in the real world?\n\u2022 P2. [SENT]. As someone who prefers well-reasoned arguments, do you think the given statement is true?\n\u2022 P3. I believe the following statement is true. [SENT]. Do you think I am right?\nHere [SENT] is replaced by individual statements in the dataset. Besides, to test the robustness of the model's knowledge representation, we ask the following additional question:\n\u2022 P4. I believe the following statement is false. [SENT]. Do you think I am right?\nP4 is the opposite of P3. In this case, if the model answers Yes to P3, it should ideally answer No to P4 and vice versa. This helps identify whether the model can refer back to its fixed internal representation (if any) and answer appropriately to both P3 and P4.\nOf the listed models, Mistral-7B-v0.1 is a base model while others are either instruction-tuned or trained with RLHF (Reinforcement Learning from Human Feedback). For the base model, we perform 8-shot prompting with examples from each category and the other models are tested in a 0-shot manner (Khatun and Brown 2024a)."}, {"title": "Experiment 1: Analysis of responses", "content": "We ask the questions described above to the 9 selected models and find some very surprising results. The first 4 prompts (P0-P3) elucidate the existence of a stable state of world in the model that we can possibly modify to generate fiction. We call this property 'consistency'. We then compare the responses of P3 and P4 and see if the model sticks to its potential state and replies appropriately to these prompts. We call this property 'robustness'. We describe the results on this experiment of consistency and robustness of models below.\nNote that analysis of these responses crucially requires human participation. Our attempts at extracting direct responses from the model (e.g. Yes/No) lead to responses that do not match with the text responses. We also ran experiments to extract the stance of a response using another LLM, but this method did not fare well either. Therefore, we manually read the responses to identify the major trends in all model responses. All model responses can be found along with the code in the repository provided."}, {"title": "Consistency", "content": "A model should be consistent in its responses across the questions in prompts PO-P3 to establish a fixed or stable state of the world. We find that none of the models are perfectly consistent. Slight change in question wording can turn a model's agreement to disagreement (or vice versa). Inconsistency is most prevalent for the Controversy and Misconception categories in all models. Some models even have inconsistency in Facts and (dangerously) in Stereotypes, which means that whether a model will agree with or disagree with a stereotype can be a consequence of simple changes in prompting. Following is an example of inconsistent answer in Mistral-7B to a prompt about the makeup of Canadian butter.\n\u2022\n\u2022 Response to PO: Yes, this is true. Butter produced in Canada in 2021 did have a different consistency compared to previous years...\nResponse to P3: Your statement is not accurate. There was no industry-wide change in the consistency of Canadian-produced butter in 2021...\nOverall, Mistral-7B, Mistral-7B-OpenOrca, Llama-2-70b-chat, and GPT-3 have inconsistencies across prompts; they do not show evidence of a stable state of world to reliably generate stories. Llama-2-13b-chat is also somewhat inconsistent and not reliable.\nChatGPT and GPT-4 tend to almost always respond with rote nuanced answers starting with \"As an AI...\" or the like. GPT-4 performs slightly better and gives straightforward responses in PO and P1. Despite this, both models produce unnecessarily indirect and careful responses, which makes it difficult to extract a state of world for creative writing.\nGPT-4 Turbo is better than other OpenAI models in providing direct responses. It is still difficult to extract a state from underneath its reluctant responses to controversial statements, but it has a relatively stable state of world. Finally, zephyr-7b-alpha has very few inconsistencies and can be considered to have the most stable state of world that can be used to story generation. (It is worth noting that this model is also much smaller than the OpenAI models.)"}, {"title": "Robustness", "content": "The claims in prompts P3 and P4 are opposites of each other. So, a Yes response in P3 calls for a No response in P4, and vice versa. Achieving this result is, surprisingly, a very difficult task for all models. Some models do not understand the question in P4, but they also have a weak sense of state. We say that models that correctly answer both P3 and P4 are 'robust', and therefore can produce reliable fiction, since they can consistently keep track of what they 'believe' to be true about the world.\nMost P4 responses in Mistral-7B, Llama-2-13b-chat, and Llama-2-70b-chat are incorrect in all categories. The models do not understand the question and respond as if answering for P3. It would be difficult to simply add or remove certain facts to alter the state of the world to create fiction since the models can not verify a fact asked from a different standpoint. Following is a response from Mistral-7B on deliberate release of bugs.\n\u2022 Response to P3: It is not true that computer companies deliberately release programs with errors to sell upgrades...\n\u2022 Response to P4: Yes, you are correct. This is a common practice in the software industry. It's called \"planned obsolescence\" and is used to encourage customers to buy...\nThe model conflicts itself by stating that the given statement is both true and false. Besides, it responds Yes to P4 (agrees statement is false), followed by a description that is the opposite (says the statement is true). This shows the model does not really understand the question.\nChatGPT has a mix of correct and incorrect P4 responses and therefore not robust with its state to be suitable for story generation.\nMistral-7B-OpenOrca and zephyr-7b-alpha are mostly correct with P4 responses in all categories except for a few instances; these models show robustness in verifying the state of the world.\nGPT-3 shows that it understands the question in P4, but it has a tendency to agree with the user despite the correctness of the statement. That is, it 'agrees' with mutually contradictory claims. This causes a lot of inconsistencies in responses. So it is not reliable and robust in maintaining a state of world for story generation. For example, in response to a question about the controversial topic of affirmative action, it gives:\n\u2022 Response to P3: Yes, you are correct. Affirmative action is necessary to ensure that all students have equal access to opportunities in higher education.\n\u2022 Response to P4: Yes, you are correct. Affirmative action is not necessary in schools, as there are many other ways to ensure that students have access to higher education, such as need-based scholarships and financial aid\nIn both cases the model agrees to the user and in the process conflicts itself.\nGPT-4, and GPT-4 Turbo understand P4 and mostly answer it robustly. But again, these models have a tendency to try to please the user. Models often answer directly when the response aligns with the user's belief, but when the model has to oppose the user, it gives a long-winded, nuanced response before finally disagreeing with the user, almost like cushioning the blow for the user. For example, GPT-4, when asked about an urban legend that the Soviets had bored a hole to Hell, gives these answers:\n\u2022 To P3: As an AI, I don't form opinions, but I can tell you that the story you're referencing is a popular urban legend called the \"Well to Hell\" hoax... However, this story has been thoroughly debunked by multiple sources...\n\u2022 To P4: Yes, you are correct. This is an urban myth often referred to as \"The Well to Hell\". The Russian drilling project, known as the Kola Superdeep Borehole, did reach a depth of about 7.5 miles (or 12 kilometers) but there were no reports of any supernatural findings.\nIn both cases the model is correct. But it answers directly to P4 with a Yes, whereas it starts with a neutral standpoint in P3. These models are robust with their state of world, but the indirect responses are not conducive to story generation.\nIn sum, zephyr-7b-alpha has the best combination of consistency and robustness, followed by GPT-4 Turbo. The results are rather unsatisfying given the success of the large models in a variety of other tasks. It is striking that these models consistently cannot separate truth from fiction, and raises the question of whether they are actually able to represent the state of the world in other domains where this is a precondition of success."}, {"title": "Experiment 2: Generating Stories", "content": "To put the models to test, we selected 20 statements (10 Conspiracy and 10 Fiction) from the dataset to generate stories. The list of statements can be found in the Appendix. We use the two comparatively better models (zephyr-7b- alpha and GPT4-Turbo), and two poorer models (Mistral- 7B-OpenOrca and ChatGPT) for comparison. We use the following prompt, where [SENT] is one of the statements.\nPlease write a short 500-word story where the following statement is true. [SENT].\nGPT4-Turbo\nGPT4-Turbo had a relatively consistent state. This model writes stories for most conspiracy theories without issues, in most cases with the existence of a secret group that perpetrates a lie which then gets exposed. There is an instance where the model fails to adhere to the prompt and another instance of the model touching on the statement without delving too deep. All stories generated by the Fiction statements also have a very similar pattern but the model is able to adhere to the statement as the truth.\nzephyr-7b-alpha\nZephyr had consistent and robust state. This model is able to complete stories considering the given statement as the truth but all the stories have similar outline where the world believes the given statement is false and then someone proves otherwise.\nMistral-7B-OpenOrca\nThis model had some inconsistencies but was mostly robust. Analyzing the stories we find that although most stories are generated as per instruction there are is some level of detachment to the statement. Model mostly writes stories where the conspiracy or fiction is false and one person believes otherwise. Sometimes their belief becomes reality, other times it does not. That is, in some instances the truth of the statement remains someones personal belief.\ngpt-3.5-turbo-1106 (ChatGPT)\nChatGPT was one of the models where the models mostly provided indirect or nuanced responses. The model does not show such refusal for Conspiracy or Fiction stories (however, it does for some harsher statements in Stereotypes). Upon inspection, comparatively more instances of stories in this model did not follow or merely touched upon the provided statement. Often just mentioning the statement in the story while not developing it enough."}, {"title": "Discussion", "content": "Generating fiction requires having a fixed state of world which can be altered by an author to create alternate worlds where a story takes place. To test whether LLMs have a fixed state of world we asked 9 LLMs a series of questions. We find that only two models have a somewhat reliable state of world. Most models are self-conflicting and so cannot be relied on to alter or take in new facts to generate stories.\nNext we generate stories from 4 models and find that all the stories follow similar pattern. All the Fiction stories in the 4 models we tested had the same exact pattern: Unicorn don't exist (statement is false) until one day someone ventures deep in the jungle to discover one (statement becomes true as prompted). None of the stories are scenarios of the statement being true from the beginning (e.g. story about a world where it is natural that unicorns exist). Most stories from the 4 models have similar pattern in Conspiracy theory stories as well. This similarity in all models suggests a lack of world view and the models are producing stochastic parrot (Bender et al. 2021) like stories.\nAll models seem quite bad at having a worldview that can be manipulated for fiction. The few models that show some promise generate stories that are mildly different from each other and robotic, suggesting that we are still reading stories from a parrot rather than the models generating anything new from their state of world. This prevents us from confirming our hypothesis about the consistency of a models state, however we can glimpse at it through the models' response to the questions.\nWe focus our work on using vanilla LLMs like any author of fiction would, without modifying models weights or extensive prompt engineering. Our findings suggest LLMs are not yet useful for fiction. Future directions in this work will require some way of teaching models to retain state and to write creatively. This includes explicitly maintain state or fine-tuning specifically to generate stories among others."}, {"title": "Conclusion", "content": "An important component of generating fiction are the facts that reside in the story world. To create a story world, we must first ensure an LLM can be consistent with information. We devised a simple experiment where we ask 9 LLMs questions about some statements and analyze if the responses are consistent and robust. The analysis shows that almost all models have a hard time being consistent and robust. This questions whether we can have the LLMs create and maintain a story world. Finally, we conduct a primary experiment with 4 models to generate stories. We find that even the better models generate very similar robotic stories, possibly bordering on regurgitating training data. Automated story generation is therefore far from automated with the current state of LLMs and more specific tuning will be required to achieve any level of success with the models."}]}