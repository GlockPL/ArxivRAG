{"title": "A Toolkit for Virtual Reality Data Collection", "authors": ["Tim Rolff", "Niklas Hypki", "Markus Lappe", "Frank Steinicke"], "abstract": "Due to the still relatively low number of users, acquiring large-scale and multi-dimensional virtual reality datasets remains a significant challenge. Consequently, VR datasets comparable in size to state-of-the-art collections in natural language processing or computer vision are rare or absent. However, the availability of such datasets could unlock groundbreaking advancements in deep-learning, psychological modeling, and data analysis in the context of VR. In this paper, we present a versatile data collection toolkit designed to facilitate the capturing of extensive VR datasets. Our toolkit seamlessly integrates with any device, either directly via OpenXR or through the use of a virtual device. Additionally, we introduce a robust data collection pipeline that emphasizes ethical practices (e.g., ensuring data protection and regulation) and ensures a standardized, reproducible methodology.", "sections": [{"title": "1 INTRODUCTION", "content": "The vast availability of (open) internet data and media has become a cornerstone of modern machine learning, particularly fueling advancements in large language models (LLMs) and multimodal large language models (MLLMs). However, when applying machine learning to virtual reality (VR), data availability emerges as a critical limitation. Compared to the extensive and often terabytes-in-size datasets driving breakthroughs in fields like natural language processing (NLP) [23, 24, 34] and computer vision (CV) [11, 22, 28], datasets for VR problems in similar size remain relatively scarce to non-existent. Compounding this challenge is the complexity introduced by diverse hardware and input devices within and between manufacturers, which makes data collection and standardization even more intricate. Despite these obstacles, constructing comprehensive VR datasets holds immense potential. Such datasets could unlock progress in different areas. Examples of these areas are movement prediction [7, 30], human motion tracking [9, 35], task prediction [8, 13], gaze forecasting [14, 27], locomotion [10], or the development of systems for predicting motion sickness [21]. Furthermore, the availability of such extensive VR datasets would empower researchers to develop novel and innovative algorithms and gain more profound insights into behavioral patterns of humans.\nYet, a significant challenge in creating comprehensive VR datasets lies in developing a robust capturing system capable of handling the wide variety of head-mounted displays (HMDs) (e.g., Meta Quest or Apple Vision Pro), game engines (e.g., Unity3D or Unreal), and data modalities (e.g., IMU data from controller inputs, head position, gaze data, videos, depth data, and many more). On one side, such a system requires a robust data format and capturing tool that allows capturing a wide range of modalities from virtual environments (VEs) and hardware. On the other side, it should be as easily accessible as possible, enabling researchers from multiple disciplines to provide their data to the growing corpus of datasets. Fortunately, many modern HMDs provide a standardized OpenXR [5] interface, exposing the functionality of each device. It was designed such, that it standardizes across multiple devices, including HMDs, controllers, base stations haptic devices, and other trackers, such as body trackers, eye trackers, or hand trackers. This upside of the converging OpenXR standard enables a data-capturing tool that can support any device supported by the standard itself."}, {"title": "2 RELATED WORK", "content": "Data collection within VEs is a fundamental element of most VR experiments [19, 20], serving as a critical instrument to validate or challenge the hypotheses of papers. For studies, involving human participants, it is not unusual to frequently employ standardized or custom-designed questionnaires, obtaining insights either during or after the experiment. The choice of questionnaire is typically tailored to the research domain and the specific hypothesis. In addition to questionnaires, interviews and other qualitative methods are often employed to provide a more profound understanding of human interaction with technology.\nAnother quite common research element is the recording of metrics during experiments, capturing both discrete and sequential data. This includes single-value metrics, for example, game scores [17] or task completion speed [16], but also temporal data, like heart rate [26], or eye movement data [27]. Most likely, most studies performed will capture multiple modalities, including quantitative and qualitative data through questionnaires, metrics, videos, and other modalities [13, 14]. These metrics may either be used on their own for the confirmation of a hypothesis or in combination with other qualitative measures. Together, these diverse data collection methods provide a robust foundation for understanding and interpreting human behavior in virtual settings.\nHowever, some types of data acquisition might be more challenging than others or have to follow so determined process. To simplify this and to avoid errors, previous literature focuses on providing toolkits for the correct execution of the experimental procedure [1, 15, 32, 36]. For example, Zenner et al. [36] provide a toolkit for the staircase procedure for estimating psychophysical detection thresholds. Other toolkits, such as the Toggle toolkit [31], focus on providing a way to trigger specific events and actions inside the VE; however, their toolkit lacks full recording capabilities. In contrast, some proprietary tools such as NVIDIA Virtual Reality Capture and Replay (NVIDIA VCR) [2], Tobii's Ocumen [3], cognitive3D [1], or open access tools, like XREcho, aim at recording the full application state. Yet, these are often restricted to PC-VR or Web-VR, making them impractical to record for standalone VR (i.e., for devices such as the Meta Quest). To solve this gap, Javerliat et al. [15] present a more universal tool to record user behavior by capturing the state of the application for a wide range of devices. Their PLUME toolkit [15] provides an effortless way to capture a full experimental setup, with a wide variety of captured data."}, {"title": "3 TOOLKIT", "content": "In this section, we will introduce our OpenXR Data Recorder (OXDR) toolkit. We base our toolkit on the Unity3D game engine. Contrary to previous toolkits like XREcho [32] or PLUME [15], we do not want to capture the full state, but rather directly record the output of the hardware that is provided to the Unity game engine. This has two reasons: First, it allows us to capture the data at a fixed update rate, independent of the frame rate. This is especially important for captured data with high change rates or fast movements, such as eye movement or controller input data. Second, the data captured can be directly used for inference on a trained machine learning model, since the captured data can be directly used to train the model and does not deviate between capture and runtime.\nFor clarity, we split our toolkit into three different parts:\n1.  A data format designed to capture any data provided by OpenXR, which also allows being easily extended for special devices that do not provide OpenXR support yet.\n2.  A recording tool for the Unity3D game engine with a minimal setup that directly stores the OpenXR data provided to the Unity3D engine in our aforementioned data format.\n3.  A set of Python scripts, that act as a toolset to generate machine learning data for external analysis.\nIn the following, we will first discuss our data format. Afterward, we then go into the details of our data recording tool and give a short overview of the Python toolkit."}, {"title": "3.1 Data Format", "content": "Before, detailing our capturing system, we want to provide an overview of our data format used to store any arbitrary data outputted by the Unity3D interface of OpenXR. Here, we propose two different types of data storage, providing a trade-off between data sizes and data handling. To store the data, we either provide New-line Delimited JSON (NDJSON) or binary storage through MessagePack, which provides an efficient binary serialization format similar to the JSON format. To store all possible data types, we define a hierarchy of different structures (c.f. Fig. 2):"}, {"title": "3.1.1 Metadata", "content": "The initial entry in our data format is defined by a series of metadata elements (see Fig. 3) that provide essential information for external analysis and validation. These metadata entries include details such as the startup time, end time, and framerate/polling rate, enabling the alignment of the recorded data. Additionally, attributes such as video resolution (width and height) and the corresponding filename are stored. To ensure the validity of the recordings, we also incorporate details about the utilized HMD and the storage medium."}, {"title": "3.1.2 Snapshot", "content": "Our tool is specifically designed to integrate seamlessly with Unity3D; hence aligning its data structures with the Unity3D Input System [4]. At the core, we use a snapshot structure, representing all data captured during a single update cycle. We define an update cycle as the interval in which the input system updates according to a specified polling rate. Each snapshot encapsulates information about the associated frame, timestamp, and device metadata. Since multiple devices can be updated within the same cycle, a single snapshot may represent data from multiple devices. This structured approach ensures that our data format efficiently stores precisely one snapshot for every input system update cycle."}, {"title": "3.1.3 Device", "content": "A device refers to a physical or virtual hardware component being recorded, whether partially or in its entirety. This might be, for example, a controller, eye tracker, or the HMD. A device provides metadata such as timestamps, names, and serial numbers to enable identification. Additionally, each device is associated with a set of features that encapsulate and represent its hardware capabilities. To accommodate diverse use cases, we propose a flexible data abstraction layer designed to (i) integrate seamlessly with the Unity Input System (refer to Sec. 3.1.4) and (ii) support the extension of custom user-defined virtual device types, even for those that do not natively expose an OpenXR interface to Unity."}, {"title": "3.1.4 Feature and value types", "content": "A feature represents an abstraction of a Unity3D Input System control type, encompassing its name, type, and associated data. As previously mentioned, to support extensibility and flexibility, our approach provides an abstraction layer for the value data that stores the captured data, allowing us to integrate custom user-defined types while maintaining compatibility with the control types offered by the Unity3D Input System; hence requiring us to store the type of the data for reconstruction during external analysis.\nBy default, the system supports fundamental data types, such as Integer and Double, ensuring the representation and storage of numerical information. Beyond these, we include commonly used mathematical constructs like Vector2, Vector3, and Quaternions. All these already allow for storing complex input systems, such as controllers, eye trackers, or heart rate sensors. For ease of use, we also add the missing input options of the Unity3D Input System, supporting controls such as Axis, Button, Key, Stick, DPad, and Touch, thereby offering a wide range of input scenarios.\nGiven all outlined value types, it enables us to define physical or virtual hardware as a composition of multiple control types. A basic controller, equipped with three buttons, a touchpad, and two triggers, can be characterized by five button controls (corresponding to the three primary buttons and two trigger buttons), a touch control for the touchpad, a Vector3 control representing its positional coordinates, and a Quaternion control capturing its rotational orientation."}, {"title": "3.2 Data Collection Toolkit & Procedure", "content": "In this section, we detail a standardized approach for data collection using our toolkit alongside its technical details. While this is not the only possibility to collect data, we want to provide a guideline to ensure a standardized, ethical, and correctly captured dataset. For a detailed overview, see Fig. 4."}, {"title": "3.2.1 Ethical and Data Protection Considerations:", "content": "We designed our data collection system from the ground up to be as ethical as possible, aligning with established recommendations [6, 25]. Therefore, we want to outline some guidelines for the ethical use of our system. Before initiating the recording, all participants must be thoroughly informed about the nature and purpose of the data being collected via a detailed consent form. The consent form must educate participants about the potential authorized users that will have access to the data. Participants are required to provide written consent, which is tied to a unique identifier to facilitate their rights to data deletion, in full compliance with the General Data Protection Regulation (GDPR) enacted by the European Union. To reinforce the transparency and autonomy of participants and the developers of the captured application, a second consent confirmation step should be performed within the captured application, following the recommendation of our institutional ethics committee. This step requires participants to actively affirm their agreement via a button press. All raw data collected is securely stored on a GDPR-compliant server located within the European Union, ensuring rigorous adherence to data protection standards."}, {"title": "3.2.2 Standardized Surveys:", "content": "In addition to the collection of quantitative data, we incorporate a series of questionnaires into our toolkit to establish connections between various data modalities and quantitative metrics. This approach enables multimodal data analysis without requiring a purpose-built experimental setup, while also providing opportunities to generate novel hypotheses from pre-existing datasets. For example, this would allow replicating already performed studies, such as the finding that mental workload can be associated with pupil diameter [33]. Given the wide range of available surveys, we selected questionnaires that are broadly applicable across diverse VR studies, ensuring their applicability for data collection:\nTo ensure consistency in data collection, we begin with a standardized demographics' questionnaire (see Appendix A). This gathers essential details such as participants' age, gender, native language, use of vision correction, and prior experience with VR. Additionally, we recommend administering the NASA-TLX [12], the Simulator Sickness Questionnaire (SSQ) [18], and the Igroup Presence Questionnaire (IPQ) [29] both at the study's beginning (onset) and ending (offset). These tools may provide valuable insights into participants' experiences and ensure a comprehensive assessment of qualitative metrics without an extensive time impact."}, {"title": "3.2.3 Technical Details:", "content": "In contrast to many data capture toolkits, we designed our system such that it runs independent of the frame rate of the application. This is critical, as a stutter due to a longer performed rendering computation of the graphics card may have an impact on the captured data. This may influence the predictive performance of any deep-learning (DL) model. As we want to support using the captured output directly for training and inference, frame dependence may make a difference, for example, due to changed computation or different scenes, resulting in differently aligned data. Furthermore, frame dependence also adds another layer of complexity that needs to be taken care of as it may require performing multiple alignments due to multiple reported data points for a singular frame (i.e., gaze data from an eye tracker).\nBeyond that, we do not want users of the collection system to specify the intricate details of the captured details and captured objects. Instead, we fully collect all connected hardware, even including the keyboard and mouse in a PC-VR setup. Implementation-wise, we hook directly into the input system and specify a polling frequency at which new device information, will be reported. As a result, we capture all events sent to Unity, including OpenXR input events. This allows us to filter for necessary hardware later through Python (c.f. Fig. 5).\nDue to some hardware not supporting all OpenXR features, we provide implementations for virtual devices, such as an eye tracker for the Meta Quest Pro, Pico Neo3 Pro, or the HTC Vive Pro Eye. We also support capturing video on a subset of hardware."}, {"title": "3.2.4 Analysis Pipeline", "content": "Our analysis pipeline is based on python and implements all data types supported by our capturing system (c.f. Sec. 3.1). This allows users to easily parse files outputted by the system and their conversion to different file formats, such as comma separated values (CSV) for data analysis or model training."}, {"title": "4 DISCUSSION & LIMITATIONS", "content": "This paper introduces a versatile data collection toolkit designed specifically for virtual reality studies. The proposed toolkit has a streamlined integration, offers frame rate-independent recording, and provides a standardized methodology for data acquisition. Central to this approach is the inclusion of ethical guidelines and standardized questionnaires to ensure robust and reproducible research practices. Furthermore, we provide a Python-based analysis tool for processing the defined data format. By incorporating frame rate independence, our framework eliminates the need for distinct implementations when capturing or interfacing with deep learning models, enhancing both efficiency and scalability.\nDespite its strengths, our work has certain limitations worth noting. Foremost among them is that the current iteration of our toolbox is restricted to supporting Unity. Expanding this compatibility could be achieved by implementing an interceptor plugin for OpenXR, which would enable the interception of all native OpenXR calls. Additionally, while our framework excels in its targeted simplicity, it does not capture the complete state of the application, unlike solutions such as PLUME [15]. This design trade-off means that, at present, we cannot automatically capture in-application metrics such as game scores without integrating custom virtual devices."}]}