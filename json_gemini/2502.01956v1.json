{"title": "DHP: DISCRETE HIERARCHICAL PLANNING FOR HIERARCHICAL REINFORCEMENT LEARNING AGENTS", "authors": ["Shashank Sharma", "Janina Hoffmann", "Vinay Namboodiri"], "abstract": "In this paper, we address the challenge of long-horizon visual planning tasks using Hierarchical Reinforcement Learning (HRL). Our key contribution is a Discrete Hierarchical Planning (DHP) method, an alternative to traditional distance-based approaches. We provide theoretical foundations for the method and demonstrate its effectiveness through extensive empirical evaluations.\nOur agent recursively predicts subgoals in the context of a long-term goal and receives discrete rewards for constructing plans as compositions of abstract actions. The method introduces a novel advantage estimation strategy for tree trajectories, which inherently encourages shorter plans and enables generalization beyond the maximum tree depth. The learned policy function allows the agent to plan efficiently, requiring only log N computational steps, making re-planning highly efficient. The agent, based on a soft-actor critic (SAC) framework, is trained using on-policy imagination data. Additionally, we propose a novel exploration strategy that enables the agent to generate relevant training examples for the planning modules. We evaluate our method on long-horizon visual planning tasks in a 25-room environment, where it significantly outperforms previous benchmarks at success rate and average episode length. Furthermore, an ablation study highlights the individual contributions of key modules to the overall performance.", "sections": [{"title": "Introduction", "content": "Reinforcement learning (RL) has achieved significant success in solving complex decision-making problems, ranging from playing games to robotic control [17, 27]. An important characteristic of high-performing agents is planning and reasoning for long-term goals. Traditional sequential planning methods such as Monte-Carlo Tree Search (MCTS) [27, 5], Visual Foresight [8], Imagination-based search [12], Graph search [9] often struggle to scale efficiently with environments with long-time horizons and intricate dependencies, as the search space grows exponentially with problem complexity [28]. Hierarchical planning methods address this challenge by recursively decomposing tasks into simpler subtasks [21, 7, 23, 2, 13]. Unlike sequential planning, hierarchical planning enables the reuse of learned sub-policies or abstractions across different tasks or environments, promoting generalization and reducing the need for retraining from scratch [29].\nDespite these advantages, training hierarchical planning agents has been challenging. The current hierarchical Planning methods typically optimize to minimize a temporal distance metric between a pair of states [23, 2, 13]. Though the distance-based metric has given good results, they are highlighted as both crucial for success and challenging to learn.\nFor instance, [9] use an explicit graph search for planning and emphasize that the success of their SearchPolicy depends"}, {"title": "Related work", "content": "Planning algorithms aim to solve long-horizon tasks efficiently by exploring future states and selecting optimal actions [16, 6]. Monte Carlo Tree Search (MCTS) [5] expands a tree of possible future states by sampling actions and simulating outcomes. While effective in discrete action spaces, MCTS struggles with scalability in high-dimensional or continuous environments. Visual Foresight methods [8, 10, 12] learned visual dynamics models to simulate future states, enabling planning in pixel-based environments. However, they require accurate world models and can be computationally expensive. Some use explicit graph search over the replay buffer data [9]. Model Predictive Control (MPC) [18, 19] is an online planner that samples future trajectories and optimizes actions over a finite horizon. These methods rely on sampling the future state and thus do not scale well with the horizon length.\nTo address the challenges of long-horizon tasks, some planning algorithms decompose complex problems into manage- able subtasks by predicting intermediate subgoals. MAXQ [7] decomposes the value function of the target Markov Decision Process (MDP) into an additive combination of smaller MDPs, enabling hierarchical planning. Sub-Goal Trees [13] learn a subgoal prediction policy optimized to minimize the total predicted distance measures of the decomposed subtasks. Long-Horizon Visual Planning [23] utilizes Goal-Conditioned Predictors (GCPs) to reduce the prediction space and employs a Cross-Entropy Method (CEM)-based planner optimized to minimize distance costs. CO-PILOT [2] is a collaborative framework where a planning policy and an RL agent learn through mutual feedback to optimize the total distance cost."}, {"title": "Methodology", "content": "Let \\$\\pi_\\theta\\$ be a goal-conditioned planning policy that takes the initial \\$\\{s\\}\\_t \\in \\mathcal{S}\\$ and goal \\$\\left\\{s\\right\\}\\_g \\in \\mathcal{S}\\$ states as input, and outputs a subgoal in the same space \\$\\left\\{s\\right\\}\\_\\text{g1} \\in \\mathcal{S}\\$. The planning policy allows the agent to break a long-horizon task (\\$\\left\\{s\\right\\}\\_t, \\left\\{s\\right\\}\\_g\\$) into two smaller subtasks (\\$\\left\\{s\\right\\}\\_t, \\left\\{s\\right\\}\\_\\text{g1}\\$) and (\\$\\left\\{s\\right\\}\\_\\text{g1},\\left\\{s\\right\\}\\_g\\$). The following sections describe our method that helps train the planning policy. The overall process can be summarized as: generation of plans via recursive application of the planning policy, reward estimation for the plans, advantage estimation to measure the plan quality, and using policy gradients to update the planning policy using the estimated advantages."}, {"title": "Plan Unrolling", "content": "Since the subgoal is in the same state space, the policy can also be applied to the subtasks. The recursive application of the subgoal operator further breaks the task, leading to a tree of subtasks \\$\\tau\\$ where each node \\$n\\_i\\$ is a task (Fig. 1a). Let the preorder traversal of the subtask tree \\$\\tau\\$ of depth \\$D\\$ be written as \\$\\left[n\\_0, n\\_1, n\\_2, ..., n\\_{2^{D+1} - 2}\\right]\\$. The root node \\$n\\_0\\$ is the original task and the other nodes are the recursively generated subtasks. The formulation means the child nodes (\\$n\\_{2i+1}\\$, \\$n\\_{2i+2}\\$) represent the subtasks generated by the application of the planning policy at node \\$n\\_i\\$. The tree leaf nodes indicate the sequence of smallest subtasks that can be executed sequentially to complete the original problem."}, {"title": "Discrete Rewards Scheme", "content": "Next, we want to identify the nodes/subtasks in the unrolled tree that are directly achievable by the worker. Nodes that the worker can directly complete do not need to be expanded further. We want our planning policy to act such that all branches of the tree end in reachable nodes and with minimal possible leaf nodes. For this, we first test the reachability"}, {"title": "Advantage Estimation for Trees", "content": "Ideally, we would like our planning policy to maximize the sum of all received rewards \\$\\sum\\_{i=1}^{2^{D-1}} R\\_i\\$. In our formulation, Sub-Goal trees [13] minimize the sum of the distances predicted in the subtree under the node to optimize the policy at that node. CO-PILOT [2] minimizes the sum of the distances predicted at all nodes. Long-Horizon Visual Planning [23] uses a CEM-based optimizer to minimize the sum of predicted distances for the child nodes. Since we work with reachability-based rewards, we cannot optimize the sum of rewards because the policy can converge to a degenerate solution. This can happen if the policy predicts either the initial or goal state as the subgoal leading to two nodes, one degenerate and the other containing the original problem. Such predictions reward policy with an immediate non-zero reward and it gets stuck in a local optima. It can also encourage the policy to generate maximum possible leaf nodes or longer plans. Ideally, the return should be high when all leaf subtasks end in reachable nodes and the plans are short.\nTaking inspiration from the discounted return formulation for linear trajectories, \\$\\left\\{G\\right\\}\\_t = \\left\\{R\\right\\}\\_{t+1} + \\gamma \\left\\{G\\right\\}\\_{t+1}\\$, we propose writing the return formulation for trees as, \\$\\left\\{G\\right\\}\\_i = \\text{min}(\\left\\{R\\right\\}\\_{2i+1} + \\gamma \\left\\{G\\right\\}\\_{2i+1}, \\left\\{R\\right\\}\\_{2i+2} + \\gamma \\left\\{G\\right\\}\\_{2i+2})\\$. All branches should end in directly reachable subtasks to score a high return with this formulation. Also, since the discount factor diminishes the return with each additional depth, the agent can score higher when the constructed tree has less depth similar to linear trajectories where the agent gets a higher return for shorter paths to the goal [30]. Thus, given a tree trajectory \\$\\tau\\$, we write the Monte-Carlo (MC) return, the 1-step return, and the lambda return for each non-terminal and non-leaf node as:\n```latex\n\\begin{aligned}\nG_i &= \\text{min}(R_{2i+1} + \\gamma G_{2i+1}, R_{2i+2} + \\gamma G_{2i+2}) \\\\\nG_i^1 &= \\text{min}(R_{2i+1} + \\gamma v_\\phi(n_{2i+1}), R_{2i+2} + \\gamma v_\\phi(n_{2i+2})) \\\\\nG_i^\\lambda &= \\text{min}(R_{2i+1} + \\gamma((1 - \\lambda)v_\\phi(n_{2i+1}) + \\lambda G_{2i+1}), R_{2i+2} + \\gamma((1 - \\lambda)v_\\phi(n_{2i+2}) + \\lambda G_{2i+2}))\n\\end{aligned}\n```\nIllustrations of example return evaluations for some sample trees are shown in Fig. 10. We show that the Bellman operators for the above returns are contractions and repeated applications cause the value function \\$v\\$ to approach a stationary \\$v^\\*\\$ (see Sec. A.2.1 for the proof).\nThese returns can be used to learn a value function \\$v\\_\\phi(n\\_i) \\rightarrow \\mathbb{R}\\$ using loss:\n```latex\n\\mathcal{L}(v_\\phi) = \\mathbb{E}\\_{\\tau \\sim \\pi_\\theta} \\bigg[\\sum_{i=0}^{2^{D}-2} (v_\\phi(n_i) - G_i)^2\\bigg]\n```"}, {"title": "Policy Gradients", "content": "Given a tree trajectory \\$\\tau\\$, sampled using a planning policy \\$\\pi\\_\\theta\\$ parameterized by \\$\\theta\\$, and \\$A^i(\\tau)\\$ being the advantage estimated for node \\$n\\_i\\$ of the trajectory \\$\\tau\\$. We derive the policy gradients for a tree trajectory (see Sec. A.1 for proof).\nGiven a tree trajectory \\$\\tau\\$ specified as a list of nodes \\$n\\_i\\$, generated using a policy \\$\\pi\\_\\theta\\$. The policy gradients can be written as:\n```latex\n\\nabla_\\theta J(\\theta) = \\mathbb{E}\\_{\\tau} \\sum_{i=0}^{2^{D}-2} A^i(\\tau) \\nabla_\\theta \\log \\pi_\\theta(a_i|n_i)\n```\nWe also show that if the advantage estimate is independent of the policy \\$\\pi\\_\\theta\\$, the expectation reduces to 0, implying that we can use the value function as a baseline (see Sec. A.1 for proof).\nIf \\$A(\\tau)\\$ is independent of \\$\\tau\\$, say \\$b(n\\_i)\\$, then its net contribution to the policy gradient is 0.\n```latex\n\\mathbb{E}\\_{\\tau} \\sum_{i=0}^{2^{D}-2} b(n_i) \\nabla_\\theta \\log \\pi_\\theta(a_i|n_i) = 0\n```\nUsing the policy gradients and an entropy term to encourage random exploration, we construct the loss function for the policy \\$\\pi\\_\\theta\\$ as (sum over all non-leaf and non-terminal nodes):\n```latex\n\\mathcal{L}(\\pi_\\theta) = -\\mathbb{E} \\sum_{i=0}^{2^{D}-2} [(G_i - v_\\phi(n_i)) \\log \\pi_\\theta(a_i|n_i) + \\eta H[\\pi_\\theta(z|S_t)]]\n```"}, {"title": "Agent Architecture", "content": "Our agent uses an HRL architecture to plan using discrete combinations of abstract actions. Our architecture consists of three modules broadly: perception, worker, and manager. Perception is implemented using the Recurrent State Space Module (RSSM) [12] that learns state representations using a sequence of observations. RSSM enables imagination which allows on-policy agent training by generating rollouts efficiently. Both, the worker (Fig. 8) and the manager (Fig. 9) are implemented as Goal-Conditioned SAC agents optimized for different rewards. The worker is optimized to increase the cosine_max similarity measure between the current and a prescribed subgoal state. The manager is the planning policy described in the previous section that refreshes the worker goal every K steps. The manager predicts in the latent space using a Conditional State Recall (CSR) module which helps in search space reduction."}, {"title": "Conditional State Recall", "content": "To help reduce the search space for subgoal prediction, we train a Conditional Variational AutoEncoder (CVAE) that learns to predict midway states given an initial and a final state from replay data. Thus, given initial and final states \\$\\left(s\\_t, s\\_{t+q}\\right)\\$ that are \\$q\\$ steps apart, we want to be able to learn a distribution over the midway states \\$s\\_{t+q/2}\\$. The CVAE module consists of an Encoder and a Decoder (Fig. 7b in appendix). The encoder takes the initial, midway, and final states as input to output a distribution over a latent variable \\$Encg(z|s\\_t, s\\_{t+q/2}, s\\_{t+q})\\$. The decoder uses the initial, and final states, and a sample from the latent distribution \\$z \\sim Encg(z|s\\_t, s\\_{t+q/2}, s\\_{t+q})\\$ to predict the midway state \\$Dec(s\\_t, s\\_{t+q}, z) \\rightarrow \\hat{s}\\_{t+q/2}\\$. The module is optimized using the replay buffer data to minimize the ELBO objective. We extract the triplets at multiple temporal resolutions \\$q \\in \\left\\{2K, 4K, 8K, ...\\right\\}\\$ (subjected to task horizon) allowing the policy to function at all temporal resolutions (Fig. 13b).\nSimilarly, we train another CVAE that helps predict nearby goal states given an initial state. Given an initial and final state separated by K steps \\$\\left(s\\_t, s\\_{t+K}\\right)\\$ (Fig. 13a). We learn a CVAE where the encoder encodes the state pair to a latent space \\$Enc\u0131(z|s\\_t, s\\_{t+K})\\$ and the decoder predicts the final state in the context of the initial state \\$Dec\u0131(s\\_t, z) \\rightarrow \\hat{s}\\_{t+K}\\$ (Fig. 7a). This CVAE helps the explorer predict nearby goals for the worker during the exploration phase and helps efficiently check reachability during inference. We call these modules Goal Conditioned State recall (GCSR) and Initial Conditioned State recall (ICSR). Intuitively, the GCSR and ICSR modules learn path segments and state transitions at the manager's temporal resolution, respectively. See Sec. A.4 for more training details."}, {"title": "Plan Inference", "content": "Unlike non-policy-based approaches like CEM [18, 23], which construct multiple plans and evaluate the best at runtime, a policy-based agent outputs the optimal best plan. Therefore, the agent does not need to predict the entire tree but only the left child node for each node, corresponding to evaluating the immediate steps only (Fig. 1b). The evaluation requires \\$(D = \\log N)\\$ policy steps for planning horizons of N steps. Though all tree nodes can be constructed and evaluated in parallel in O(log N) time, they still require computing \\$2^{D+1}-1\\$ nodes to construct plans with length \\$2^{D}\\$ nodes (the leaf nodes). The agent's resource and time efficiency allow for cheap plan regenerations.\nSince the model is not trained for nodes beyond the terminal nodes, we need to evaluate the reachability of the predictions in real time. For this, we use the ICSR module to reconstruct the subgoal states in the context of the initial state. The node is directly achievable if the cosine_max similarity between the goal states and the reconstructions is above the threshold \\$\\Delta\\_R\\$. The goal from the lowest depth reachable node is the worker's goal (Fig. 11). Since the agent generalizes beyond the maximum training depth, the plan inference can be unrolled for depths \\$D\\_{\\text{inf}}\\$ greater than the maximum training depth. We use \\$D = 5\\$ and \\$D\\_{\\text{inf}} = 8\\$ for our experiments to validate this."}, {"title": "Memory Augmented Exploration", "content": "When using self-generated exploratory data, a separate exploratory manager SAC \\$\\left(\\pi\\_\\epsilon(s\\_t), V\\_\\epsilon(s\\_t)\\right)\\$ is used to act for \\$N\\_\\epsilon\\$ steps, and then switched to the task planning manager. The exploration policy generates goals for the worker using the initial conditional state recall (ICSR) (Fig. 12). The explorer is trained to maximize intrinsic exploratory rewards. Vanilla exploratory reward schemes positively reward the agent for visiting states with non-optimal representations. The reward encourages the agent to seek novel states in the environment. However, these resulted in trajectories that were non-optimal for the current task. We observed that as per the exploration objective the agent sought out the unclear state representations. However, once at the state, it stayed near the state. Data generated from this behavior led to suboptimal planning policies with lower performance. We therefore instead propose an alternate exploratory reward scheme that rewards the agent positively for maneuvering novel path segments \\$\\left(s\\_t, s\\_{t+q/2}, s\\_{t+q}\\right)\\$ and state transitions \\$\\left(s\\_t, s\\_{t+K}\\right)\\$. The novelty is measured as the reconstruction errors using the conditional state recall modules. Thus, we roll out an imagined trajectory, measure the reconstruction errors using the conditional state recall modules as mean-squared errors (MSE), and use them as rewards for the agent at appropriate time steps (Fig. 14a). The exploratory rewards \\$R\\_t\\$ at step \\$t\\$ can be written as:\n```latex\n\\begin{aligned}\nR^I_t &= \\lambda \\|\\|s_t - Dec_I(s_{t-K}, z) \\|\\|^2 \\quad \\text{where} \\quad z \\sim Enc_I(z | s_{t-K}, s_t)\\\\\nR^G(q)_t &= \\|\\|s_{t-q/2} - Dec_G(s_{t-q}, s_t, z) \\|\\|^2 \\quad \\text{where} \\quad z \\sim Enc_G(z | s_{t-q}, s_{t-q/2}, s_t) \\text{and} q \\in \\mathcal{Q} \\\\\nR_t &= R^I + \\sum_{q \\in \\mathcal{Q}} R^G(q)\n\\end{aligned}\n```\nSince the exploratory rewards for the current step depend on the past states. The explorer needs to know the states \\$\\left[s\\_t, s\\_{t-K}, s\\_{t-2K}, ...\\right]\\$ to guide the agent accurately along rewarding trajectories. To address this, we provide a memory of the past states as an additional input to the exploratory manager SAC \\$\\left(\\pi\\_E(s\\_t, mem\\_t), V\\_E(s\\_t, mem\\_t)\\right)\\$ (Fig. 12). To do this, a memory buffer stores every K-th state the agent encounters, and then a memory input is constructed consisting of states \\$mem\\_t = \\left\\{s\\_{-K}, s\\_{-2K}, s\\_{-4K}, ...\\right\\}\\$ (Fig. 14). The imagination horizon and maximum temporal resolution of the CSR modules constrain the memory length. We call this memory-augmented exploration that uses memory to achieve long-term dependencies in the exploration trajectories. See Sec. 5.2.2 for performance comparisons of the different exploration strategies, Sec. A.4.3 for more training details, and Sec. A.6 for examples of generated trajectories."}, {"title": "Results and Evaluation", "content": "We test our agent in the 25-room environment where it has to navigate a maze of connected rooms to reach the goal state. Benchmarks from the previous methods show the average episode length as > 150 steps, indicating a long-horizon task. The agent is provided the initial and goal states as 64 \u00d7 64 images. Each episode lasts 400 steps before terminating with a 0 reward. If the agent reaches the goal within 400 steps, the episode terminates and is given a reward \\$0 < R \\leq 1\\$ depending on the episode length."}, {"title": "Conclusion", "content": "The proposed approach shows that learning a discrete reachability-based planning method can be used to learn highly performant hierarchical reinforcement learning agents. We obtain real-time planning with shorter trajectories and higher completion rates compared to previous distance-based approaches. While the method works well, it also provides opportunities for future work to address the limitations. We observe that learning good state representations is crucial and one could explore methods for generating state representations from textual prompts. One could also explore automatic generation of goals in the future."}, {"title": "Policy Gradients for Trees", "content": "Given a goal-directed task as a pair of initial and final states \\$\\{s\\_t,s\\_g\\}\\$, a subgoal generation method predicts an intermediate subgoal \\$\\{s\\}\\_1\\$ that breaks the task into two simpler subtasks \\$\\{s\\_t, s\\_1\\}\\$ and \\$\\{s\\_1, s\\_g\\}\\$. The recursive application of the subgoal operator further breaks the task, leading to a tree of subtasks \\$\\tau\\$ where each node \\$n\\_i\\$ is a task. Let the preorder traversal of the subtask tree \\$\\tau\\$ of depth D be written as \\$n\\_0, n\\_1, n\\_2, ..., n\\_{2^{D+1}-2}\\$. The root node \\$n\\_0\\$ is the given task and the other nodes are the recursively generated subtasks. Ideally, the leaf nodes should indicate the simplest reduction of the subtask that can be executed sequentially to complete the original task. The tree can be viewed as a trajectory where each node \\$n\\_i\\$ is a state, and taking action \\$\\pi\\_\\theta(a|n)\\$ simultaneously places the agent in two states given by the child nodes \\$\\{n\\_{2i+1}, n\\_{2i+2}\\}\\$. Thus, the policy function can be written as \\$\\pi\\_\\theta(a|n)\\$, the transition probabilities as \\$p\\_T(n\\_{2i+1}, n\\_{2i+2}|a, n)\\$, and the probability of the tree trajectory under the policy \\$\\tau\\_{\\pi\\_\\theta}\\$ can be represented as:\n```latex\n\\begin{aligned}\np_{\\pi_{\\theta}}(\\tau) =& p(\\pi_{\\theta}) * [\\pi_{\\theta}(a_0|n_0) * p_T(n_1, n_2|a_0, n_0)]*\\\\&[\\pi_{\\theta}(a_1|n_1) * p_T(n_3, n_4|a_1, n_1)]*\\\\&[\\pi_{\\theta}(a_2|n_2) * p_T(n_5, n_6|a_2, n_2)] *\\\\&[\\pi_{\\theta}(a_{2^{D-2}}|n_{2^{D-2}}) * p_T(n_{2^{D+1}-3}, n_{2^{D+1}-2}|a_{2^{D-2}}|n_{2^{D-2}})]\n\\end{aligned}\n```\n```latex\np_{\\pi_{\\theta}}(\\tau) = p(\\pi_{\\theta}) \\prod_{i=0}^{2^{D-2}} \\pi_{\\theta}(a_i|n_i) \\prod_{i=0}^{2^{D-2}} p_T(n_{2i+1}, n_{2i+2}|a_i, n_i)\n```\nGiven a tree trajectory \\$\\tau\\$ specified as a list of nodes \\$n\\_i\\$, generated using a policy \\$\\pi\\_\\theta\\$. The policy gradients can be written as:\n```latex\n\\nabla_\\theta J(\\theta) = E_{\\tau} \\sum_{i=0}^{2^{D-2}} A^i(\\tau) \\nabla_{\\theta} log \\pi_{\\theta}(a_i|n_i)\n```\nThe log-probabilities of the tree trajectory and their gradients can be written as:\n```latex\nlog p_{\\pi_{\\theta}}(\\tau) = log p(n_0) + \\sum_{i=0}^{2^{D-2}} log \\pi_{\\theta}(a_i|n_i) + \\sum_{i=0}^{2^{D-2}} log p_T(n_{2i+1}, n_{2i+2}|a_i, n_i)\n```\n```latex\n\\nabla_{\\theta} log p_{\\pi_{\\theta}}(\\tau) = 0 + \\nabla_{\\theta} \\sum_{i=0}^{2^{D-2}} log \\pi_{\\theta}(a_i|n_i) + 0 = \\sum_{i=0}^{2^{D-2}} \\nabla_{\\theta} log \\pi_{\\theta}(a_i|n_i)\n```\nThe objective of policy gradient methods is measured as the expectation of advantage or some scoring function A(\\$\\tau\\$):\n```latex\nJ(\\theta) = E A(\\tau) = \\sum_{\\tau} A(\\tau) * p_{\\pi_{\\theta}}(\\tau)\n```"}, {"title": "Policy Evaluation for Trees", "content": "We present the return and advantage estimation for trees as an extension of current return estimation methods for linear trajectories. As the return estimation for a state \\$\\left\\{s\\right\\}\\_t\\$ in linear trajectories depends upon the next state \\$\\left\\{s\\right\\}\\_{t+1}\\$, our tree return estimation method uses child nodes \\$\\left(n\\_{2i+1}, n\\_{2i+2}\\right)\\$ to compute the return for a node \\$n\\_i\\$. We extend the previous methods like lambda returns and Gen realized Advantage estimation (GAE) for trees.\nThe objective of our method is to reach nodes that are directly reachable. Such nodes are marked as terminal, and the agent receives a reward. For generalization, let's say that when the agent takes an action \\$\\{a\\}\\_i\\$ at node \\$n\\_i\\$, it receives a pair rewards \\$\\left(R\\_{2i+1}(n\\_i, a\\_i), R\\_{2i+2}(n\\_i, a\\_i)\\right)\\$ corresponding to the child nodes. Formally, the rewards \\$R(\\tau)\\$ is an array of length equal to the length of the tree trajectory with \\$R\\_0 = 0\\$. Then, the agent's task is to maximize the sum of rewards received in the tree trajectory \\$\\mathbb{E}\\_\\tau \\sum\\_0 R\\_i\\$. To consider future rewards, the returns for a trajectory can be computed as the sum of rewards discounted by their distance from the root node (depth), \\$\\mathbb{E}\\_{\\tau} \\sum\\_0 \\gamma^{\\left[log\\_2(i+1)\\right]-1} R\\_i\\$. Thus, the returns for each node can be written as the sum of rewards obtained and the discount-weighted returns thereafter:\n```latex\nG_i = (R_{2i+1}+\\gamma G_{2i+1}) + (R_{2i+2} + \\gamma G_{2i+2})\n```\nAlthough this works theoretically, a flaw causes the agent to collapse to a degenerate local optimum. This can happen if the agent can generate a subgoal very similar to the initial or goal state \\$\\|S\\_t, S\\_{sub}\\| < \\epsilon\\$ or \\$\\|S\\_g, S\\_{sub}\\| < \\epsilon\\$. A usual theme of reward systems for subgoal trees will be to have a high reward when the agent predicts a reachable or temporally close enough subgoal. Thus, if the agent predicts a degenerate subgoal, it receives a reward for one child node and the initial problem carries forward to the other node.\nTherefore, we propose an alternative objective that optimizes for the above objective under the condition that both child subtasks \\$\\left(n\\_{2i+1}, n\\_{2i+2}\\right)\\$ get solved. Instead of estimating the return as the sum of the returns from the child nodes, we can estimate it as the minimum of the child node returns.\n```latex\nG_i = \\min(R_{2i+1} + \\gamma G_{2i+1}, R_{2i+2} + \\gamma G_{2i+2})\n```\nThis formulation causes the agent to optimize the weaker child node first and receive discounted rewards if all subtasks are solved (or have high returns). It can also be noticed that the tree return for a node is essentially the discounted return along the linear trajectory that traces the path with the least return starting at that node. Next, we analyze different return methods in the tree setting and try to prove their convergence."}, {"title": "Lambda Returns", "content": "TD(\\$\\lambda\\$) returns for linear trajectories are computed as:\n```latex\nG_t^\\lambda = R_{t+1} + \\gamma((1 - \\lambda)V(s_{t+1}) + \\lambda G_{t+1}^\\lambda)\n```\nWe propose, the lambda returns for tree trajectories can be computed as:\n```latex\nG_i^\\lambda = \\min(R_{2i+1} + \\gamma((1 - \\lambda)V(n_{2i+1}) + \\lambda G_{2i+1}^\\lambda), R_{2i+2} + \\gamma((1 - \\lambda)V(n_{2i+2}) + \\lambda G_{2i+2}^\\lambda))\n```\nWhich essentially translates to the minimum of lambda returns using either of the child nodes as the next state.\nNext, we check if there exists a fixed point that the value function approaches. The return operators can be written as:\n```latex\nTV(n_i) = E[\\min(R_{2i+1} + \\gamma((1 - \\lambda)V(n_{2i+1}) + \\lambda G_{2i+1}^\\lambda),\nR_{2i+2} + \\gamma((1 - \\lambda)V(n_{2i+2}) + \\lambda G_{2i+2}^\\lambda))]\nTV^0(n_i) = E[\\min(R_{2i+1} + \\gamma V(n_{2i+1}), R_{2i+2} + \\gamma V(n_{2i+2}))]\nTV^1(n_i) = E[\\min(R_{2i+1} + \\gamma G_{2i+2}, R_{2i+2} + \\gamma G_{2i+2})]\n```"}, {"title": "Bootstrapping with D-depth Returns", "content": "When the subtask tree branches end as terminal (or are masked as reachable), the agent receives a reward = 1, which provides a learning signal using the discounted returns. However, when the branches do not end as terminal nodes, it does not provide a learning signal for the nodes above it, as the return is formulated as min of the returns from child nodes. In this case, we can replace the returns of the non-terminal leaf nodes with their value estimates. Therefore, in case the value estimate from the end node is high, indicating the agent knows how to solve the task from there onwards, it still provides a learning signal. The n-step return for a linear trajectory is written as:\n```latex\nG_t^{(n)} = R_{t+1} + \\gamma G_{t+1}^{(n-1)}\n```\nwith the base case as:\n```latex\nG_t^{(1)} = R_{t+1} + \\gamma V(s_{t+1})\n```\nWe write the n-step returns for the tree trajectory as:\n```latex\nG_i^{(d)} = \\min(R_{2i+1} + \\gamma G_{2i+1}^{(d-1)}, R_{2i+2}+ \\gamma G_{2i+2}^{(d-1)})\n```\nwith the base case as:\n```latex\nG_i^{(1)} = \\min(R_{t+1} + \\gamma V(n_{2i+1}), R_{t+2} + \\gamma V(n_{2i+2}))\n```\nValue estimates help bootstrap at the maximum depth of the unrolled subtask tree D and allow the policy to learn from incomplete plans."}, {"title": "Properties of Tree Return Estimates", "content": "In section A.2.1 it can be seen how the tree return formulation for a node essentially reduces to the linear trajectory returns along the branch of minimum return in the subtree under it. When the value function has reached the stationary point. For a subtask tree, if all branches end as terminal nodes, the return will be \\$\\gamma^{D'}\\$ where D' is the depth of the deepest node. Otherwise, it would be \\$\\gamma^D V'\\' where V' is the non-terminal leaf node with the minimum return. Thus, it can be seen how higher depth penalizes the returns received at the root node with the discount factor \\$\\gamma\\$. This property is true with the linear trajectories where the policy converges to shortest paths to rewards to counter discounting [28, 24]. Thus, our goal-conditioned policy similarly converges to plans trees with minimum maximum depth: \\$\\min\\_O (max d\\_i)\\$, where \\$d\\_i\\$ is the depth of node \\$n\\_i\\$."}, {"title": "Architecture & Training Details", "content": "In the context of the task, we train an agent with an I-CSR module and a G-CSR module. The manager refreshes the worker's goal every K = 8 steps and the worker executes environmental actions for the given goal every step. Two policies are trained, an exploratory policy and a goal-"}]}