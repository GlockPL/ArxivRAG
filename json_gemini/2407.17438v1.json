{"title": "HumanVid: Demystifying Training Data for\nCamera-controllable Human Image Animation", "authors": ["Zhenzhi Wang", "Yixuan Li", "Yanhong Zeng", "Youqing Fang", "Yuwei Guo", "Wenran Liu", "Jing Tan", "Kai Chen", "Tianfan Xue", "Bo Dai", "Dahua Lin"], "abstract": "Human image animation involves generating videos from a character photo, allow-\ning user control and unlocking potential for video and movie production. While\nrecent approaches yield impressive results using high-quality training data, the\ninaccessibility of these datasets hampers fair and transparent benchmarking. More-\nover, these approaches prioritize 2D human motion and overlook the significance\nof camera motions in videos, leading to limited control and unstable video gener-\nation. To demystify the training data, we present HumanVid, the first large-scale\nhigh-quality dataset tailored for human image animation, which combines crafted\nreal-world and synthetic data. For the real-world data, we compile a vast collection\nof copyright-free real-world videos from the internet. Through a carefully designed\nrule-based filtering strategy, we ensure the inclusion of high-quality videos, re-\nsulting in a collection of 20K human-centric videos in 1080P resolution. Human\nand camera motion annotation is accomplished using a 2D pose estimator and a\nSLAM-based method. For the synthetic data, we gather 2,300 copyright-free 3D\navatar assets to augment existing available 3D assets. Notably, we introduce a\nrule-based camera trajectory generation method, enabling the synthetic pipeline\nto incorporate diverse and precise camera motion annotation, which can rarely be\nfound in real-world data. To verify the effectiveness of HumanVid, we establish a\nbaseline model named CamAnimate, short for Camera-controllable Human An-\nimation, that considers both human and camera motions as conditions. Through\nextensive experimentation, we demonstrate that such simple baseline training on\nour HumanVid achieves state-of-the-art performance in controlling both human\npose and camera motions, setting a new benchmark. Code and data will be publicly\navailable at https://github.com/zhenzhiwang/HumanVid/.", "sections": [{"title": "1 Introduction", "content": "High-quality and highly controllable human image animation has significantly progressed as an\nemerging popular task [17, 29, 31, 71]. Imagine the possibilities of recreating iconic movie perfor-\nmances using just a single photo of the characters, capturing them from any desired angle. This\ntechnique has the potential to significantly impact video and movie production. In this study, we\nfocus on animating characters from a single image, considering both human and camera motions as\ncrucial factors for generating realistic human videos.\nPreprint. Under review."}, {"title": "2 Related Work", "content": "Human Image Animation. The task of human image animation aims to generate coherent human\nvideos from a single image. To enhance controllability, the mainstream works in this field often\nemploy explicit human skeleton representation, e.g., OpenPose [14, 56, 69] and DensePose [20], as\nadditional guidance. Early solutions are majorly developed upon GANs for image animation and\npose transfers [16, 48, 54, 53, 55, 77, 83]. More recently, diffusion models (DMs) [25, 59, 41, 65]\nhave been drawing attention from human image animation considering their remarkable success and\nhigh-quality results in image [50, 42, 47, 52, 7, 45] and video [11, 85, 57, 27, 26, 51, 75, 64, 22]\nsynthesis. For instance, MagicDance [17] proposes a two-stage training strategy to disentangle the\nlearning of appearance and human motion. Animate Anyone [29] utilizes a reference network to\nextract the appearance representation from the source image and adopts a motion module similar\nto AnimateDiff [22] to enhance temporal consistency. It also incorporates a lightweight pose\nguider to encode pose information to the pre-trained models. Similarly, MagicAnimate [71] adopts\nDensePose [20] as the motion representation and uses a ControlNet [80] to encode pose information.\nChamp [86] further introduces the SMPL [38] model sequence and the rendered depth and normal for\nbetter alignment. Though with remarkable visual quality, these works mostly adopt a static camera\nsetting and do not consider camera viewpoint movement.\nCamera-aware Video Generation. As a significant component in video and movie production,\ncamera viewpoint movement determines the content dynamics and the overall feeling of the audience.\nWhile many works focus on guiding video generative models with structural signals [18, 75, 63,\n82, 32, 21], less attention has been paid to controlling the pose/viewpoint of camera in generating\nvideos [70, 76]. To control camera motion with reference videos, MotionDirector [84] proposes\na dual-path LoRA [28] adapter to decouple the motion and appearance learning and can roughly\ncontrol camera movements to produce a surrounding shot. For more precise control, MotionCtrl [68]\ndirectly injects the camera extrinsic matrix to the temporal attention layer in pre-trained text-to-video\nmodels and can precisely specify the camera viewpoint by providing camera poses at inference.\nCameraCtrl [23] further enhances the controllability by representing the camera pose with Pl\u00fccker ray\nembeddings [58, 36]. CamViG [40] explores the camera control in token-based video generator [35]\nby introducing camera embedding as a new modality."}, {"title": "3 Dataset", "content": "Given that diffusion models typically require large amounts of data, we are pioneering the use of\nsynthetic data in human video generation. While previous datasets [10, 73] only contain single-view\nimage data or clips with basic camera movements (i.e. zoom-in, orbit), we show that accurate\nannotations, extensive scale and rich camera trajectories from synthetic data could be vital for\ngeneration. Our synthetic videos are rendered by Unreal Engine 5 (UE5) [3] or Blender [12].\nTo enhance the diversity of human appearance, we also curate human-centric internet videos from\ncopyright-free platforms and leverage pose estimation methods [72] for automatic annotation. Both\nsynthetic and internet data are fully scalable without any human supervision."}, {"title": "3.1 Synthetic Data Construction", "content": "The synthetic video data are rendered with one or more characters moving in various 3D scenes using\ndiverse camera trajectories. Consequently, constructing the synthetic data involves three key steps:\ncharacter creation, motion retargeting, and 3D scene and camera placement."}, {"title": "3.1.1 Character Creation", "content": "We create two types of characters for animating images in the diverse domains: (1) Human-like\ncharacters from SMPL-X [44] meshes and clothing. (2) Anime characters from user-uploaded assets.\nDiverse body shapes and skin textures, 3D clothing and textures are considered for highly varied\nhuman representation.\nBody shapes and skin tone. For human-like characters, we sample body shapes from a diverse set\nof 271 body shapes with different BMI collected from the ARGOA [43] and CAESAR [49] datasets\nfollowing Bedlam [10]. To reduce the gender and ethnicity bias, we use 50 female and 50 male\ncommercial skin albedo textures collected from Meshcapade [19] with a resolution of 4096 \u00d7 4096,\nspanning over seven ethnic groups.\n3D Clothing and textures. To generate realistic human videos, it's crucial to have 3D clothing\nmotions that are physically plausible and consistent with human body movements. For instance,\nthe LSMPL-X representation from Synbody [73] adds a clothing layer to SMPL-X [44], but lacks\nrealistic physics simulation for clothing motion, leading to unnatural movements in loose-fitting"}, {"title": "3.1.2 Motion Retargeting", "content": "Given the character assets, we transfer diverse motions to these characters by re-targeting motion data\nfrom various sources, including motion capture datasets [39] and open-source software Rokoko [4].\nSPML-X characters. For human-like SMPL-X [44] characters, we sample human motions from\nlarge-scale motion capture datasets [39]. To enhance motion diversity, we sample based on motion\nannotations from [46], following the approach of Bedlam [10].\nAnime characters. Conversely, anime character assets can have diverse skeleton lengths. We utilize\nthe automatic re-targeting software Rokoko to transfer existing motions to the anime character assets.\nThe clothing and hair are treated as part of the body, so their motion is also determined by source\nmotions."}, {"title": "3.1.3 3D Scenes and Camera Placements", "content": "3D Scenes. The realistic and diverse 3D scene backgrounds for synthetic video are constructed from\nabout 100 panoramic HDRI images [2] or high-quality 3D scenes to cover both indoor and outdoor\nenvironment. We manually select panorama images with flat ground for characters to move on, while\navoiding excessive scene components that might lead to unnatural human-scene interactions. We\nalso exclude images with uniform visual patterns across different views, such as grasslands, deserts,\nor farms. The selected panorama backgrounds feature high-quality, complex texture details that"}, {"title": "Camera Trajectory Design", "content": "Unlike [10, 73, 43], our dataset highlights rich and diverse camera\ntrajectories in human-centric videos. Each camera trajectory consists of a sequence of 6-DoF\ntranslations and rotations. We carefully design a rule-based camera motion generation pipeline\nto obtain diverse trajectories. This pipeline randomly sample camera locations adaptive to human\npositions and orientations in the keyframes, and use spline interpolation to get smooth camera\nlocations and rotations in the whole sequence. Specifically, in each keyframe scene space, we\nrandomly sample camera locations within a semi-cylinder of radius \u2208 [3m, 5m] and height \u2208\n[0.6m, 1.2m] in front of the human. Then, we set the camera orientation's yaw and pitch to point at\nthe person. To create a more natural camera trajectory that smoothly follows the person, we adjust\nthe camera's position by adding the human's position offset from the keyframe to the camera position\nin each frame. Finally, we also sample the roll of camera rotation \u2208 [-30\u00b0, 30\u00b0] in keyframes. Our\ndesign of camera keyframe sampling enables all types of camera trajectories, significantly enhancing\nthe camera trajectory diversity and cinematic effect of human videos compared to existing video\ndatasets."}, {"title": "Rendering and Annotations", "content": "We render the image sequences of SMPL-X characters using UE5\ngame engine and the built-in movie render function (Movie Render Queue) for high-quality images.\nThe anime characters are rigged and rendered with blender. With our synthetic data source, a variety\nof ground-truth annotations, including camera trajectories, human skeletons, segmentation masks,\ndepth maps and normal maps, could be obtained without manual efforts. Although our dataset is\ncurated for human video generation, these ground-truth annotations could also be useful for other\ndownstream applications."}, {"title": "3.2 Internet Data Curation", "content": "To enhance the appearance diversity on top of synthetic videos, we collect real human-centric videos\nfrom copyright-free internet platforms [5] with the same distribution of synthetic data: the pixel\nmotions in such videos is only resulted from human skeleton motion or camera motion, not object\nmovements or background dynamics.\nWe utilize the Pexels API [5] to scrape data based on around 100 keywords and employed a pose\ndetector [72] to analyze the data. The pose detector focused on measuring the upper body keypoints'\nconfidence, the ratio of the largest human bounding box over the frame r, the average number of\nhumans present in each frame n, and the average motion (position offsets) of the keypoints Ap. With\nthese statistics, we apply a specific filtering criteria: a) the human should occupy the main part of the\nimage (r > 0.07); b) there should be few people (n < 4); c) there should be a noticeable motion in\nkeypoints to remove static videos (Ap > 0.01); d) No exits, entrances or occlusions of individuals in\nvideos (n \u2208 Z). As a result, we collect around 20k high-quality, real human-centric video clips with\nvarious appearances."}, {"title": "Camera Trajectory Estimation", "content": "Reconstructing global camera trajectories from in-the-wild videos\nis a challenging problem. Following the curation of human-centric videos from previous steps,\nwe adopt TRAM [66] to utilize a SLAM method [60] for recovering camera extrinsic parameters\nfrom in-the-wild videos with explicit human movement. To ensure camera parameters are robust to\ndynamic humans, we employ a masking technique [34] that removes dynamic regions from both input\nimages and dense bundle adjustment steps. By compelling SLAM to rely solely on the background\nfor camera estimation from the outset, we mitigate the risk of catastrophic failure. To convert camera\nestimation to metric scale, we leverage semantic cues from the background by utilizing noisy depth\npredictions [9]. Consequently, we recover accurate, metric-scale camera motion that serves as an\noptimal camera condition for training diffusion models. For videos with texture-less backgrounds,\nwhere the SLAM system reports large reconstruction errors, we empirically set such samples to static\ncameras. Additionally, we filter out videos with very large rotations or translations, such as cycling,\nor those with sudden shot changes, as these videos fall outside the scope of human image animation."}, {"title": "3.3 CamAnimate", "content": "To validate our dataset's capability for animating humans with moving cameras, we propose a\nsimple baseline for the camera-controllable human image animation task, named CamAnimate. By\nleveraging CameraCtrl [23]'s advanced camera pose control and Animate Anyone [29]'s character\nanimation framework, CamAnimate ensures consistent and high-quality human video generation\nwith simultaneous human and camera movements. As shown in Fig. 5, it utilizes pl\u00fccker embeddings\nto accurately parameterize camera trajectories and incorporates an additional camera pose encoder to\nencode camera information for the Denoising UNet via zero-convolution [79], while ReferenceNet\nand a pose guider maintain appearance consistency and pose controllability. By training our method\non general human videos with both camera and human movement, these two types of motion can be\ndecoupled in the network and learned by separate modules in an end-to-end manner. In addition to\nthe moving camera setting, CamAnimate can seamlessly handle traditional human image animation\ngiven a static camera parameter.\nImplementation Details. We use the checkpoint of Stable Diffusion 1.5 [50] to initialize the\nDenoising UNet and ReferenceNet, and use the weights of ControlNet [80] on OpenPose [15] to\ninitialize the Pose Guider. The camera encoder weights from CameraCtrl [23] are used to initialize\nour camera encoder. We mix train horizontal and vertical videos with a resolution of (long side,\nshort side) = (896,512), i.e., for horizontal videos (w, h) = (896,512) and for vertical videos\n(w, h) = (512,896). Each batch only samples either all horizontal or all vertical videos, and the\nchoice between horizontal and vertical videos is made randomly between batches. The setting of"}, {"title": "4 Experiments", "content": "4.1 Evaluation Benchmark\nDue to the lack of a unified benchmark for previous methods, the testing protocols for each method\nhave been varied significantly. However, when the form of the samples differs, the disparity between\nthe reference image and the target image can vary greatly, leading to highly inconsistent results\nfor the same method when inferring different reference and target images. Therefore, as the first\nlarge-scale dataset, we provide a unified testing protocol for human video generation. Specifically,\nwe use the middle (12th) frame as the reference image, predict frames in the range [1,72) with a stride\nof 3, resulting in a sequence of 24 frames. Finally, we evaluate each video under this setting using\nPSNR [67], SSIM [67], LPIPS [81], FID [24], and FVD [61] metrics."}, {"title": "4.2 Comparison with the State-of-the Art", "content": "In this section, we compare our baseline model with previous state-of-the-art methods, namely\nAnimate Anyone [29], Magic-animate [71] and Champ [86]. As animate anyone is not open-sourced,\nwe use a third-party implementation. We use the official implementations for other two methods.\nAs shown in Tab. 2, although our method is trained on videos with moving cameras, it is still able\ngenerate high-quality static camera videos on TikTok dataset and achieves best performance in all\nmetrics due to our precise camera control ability. For human videos with camera movement on our test\nset, previous methods commonly do not consider the camera condition, so they struggle to produce\nnatural videos with camera movements. Such result could be observed from both reconstruction\nmetrics like SSIM, PSNR, and LPIPS and generation metrics like FID and FVD.\nUser-study. We also conduct a user-study to compare our method with previous methods, as shown in\nTab 4. We collect 2 videos from Tiktok test set and 8 videos from our test set, and compare our results\nwith other three methods' results as a ranking question with 4 options. A total of 20 participants take\npart in our user study and we assign 3, 2, 1 points for the first, second and third method respectively.\nThe final average score is normalized by total points, so the upper bound of this metric is 0.5. We\nconclude the average points and top-1 preference of each method in Tab 4, which shows a dominate\nadvantage (0.44 points and 0.73 top-1 preference) over previous methods due to their artifacts in\nappearance, human pose and camera movements.\nQualitative comparisons. In Fig. 6, we show the qualitative comparison with previous SOTA\nmethods, where the artifacts are highlighted by red boxes. We show that previous SOTA methods may\nhave different artifacts when applied to our challenging evaluation test sets. For example, animate\nanyone suffers from inaccurate and low-quality appearances of humans. Champ suffers from missing\n3D skeletons due to the difficulty of estimating accurate 3D skeletons, especially in a crowded scene.\nMagic-animate is not able to correctly generate human face expressions due to the ambiguity of face\nrepresentation of Densepose. Our method is able to accurately generate facial expressions, body pose\nand background motions from the camera movement."}, {"title": "5 Conclusion", "content": "In this paper, our study addresses the significant challenges in the field of human image animation by\nintroducing a novel combination of a high-quality real-world video dataset and a meticulously crafted\nsynthetic dataset. Our proposed dataset not only enhances the visual quality and controllability of\nhuman animations, but also introduces a new benchmark for camera control in human videos. Without\nbells and whistles, our proposed simple baseline demonstrate superior performance when it is trained\non our combined dataset, particularly in scenarios involving complex human and camera motions. We\nbelieve that our contributions will pave the way for more transparent and comprehensive evaluations\nin this field, fostering further advancements and innovations in video and movie production.\nLimitations. The annotation of our Internet data heavily rely on pose estimation [72, 20] and\nSLAM [66, 60] methods, which could introduce noises into the camera and pose annotations.\nBroader Impacts. Our dataset and baseline method are highly effective at creating realistic human\nvideos. Nonetheless, it's important to recognize that improvements in generative model technologies\ncould lead to the creation of realistic deepfakes, which may be misused to spread misinformation."}]}