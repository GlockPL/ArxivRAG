{"title": "HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation", "authors": ["Zhenzhi Wang", "Yixuan Li", "Yanhong Zeng", "Youqing Fang", "Yuwei Guo", "Wenran Liu", "Jing Tan", "Kai Chen", "Tianfan Xue", "Bo Dai", "Dahua Lin"], "abstract": "Human image animation involves generating videos from a character photo, allowing user control and unlocking potential for video and movie production. While recent approaches yield impressive results using high-quality training data, the inaccessibility of these datasets hampers fair and transparent benchmarking. Moreover, these approaches prioritize 2D human motion and overlook the significance of camera motions in videos, leading to limited control and unstable video generation. To demystify the training data, we present HumanVid, the first large-scale high-quality dataset tailored for human image animation, which combines crafted real-world and synthetic data. For the real-world data, we compile a vast collection of copyright-free real-world videos from the internet. Through a carefully designed rule-based filtering strategy, we ensure the inclusion of high-quality videos, resulting in a collection of 20K human-centric videos in 1080P resolution. Human and camera motion annotation is accomplished using a 2D pose estimator and a SLAM-based method. For the synthetic data, we gather 2,300 copyright-free 3D avatar assets to augment existing available 3D assets. Notably, we introduce a rule-based camera trajectory generation method, enabling the synthetic pipeline to incorporate diverse and precise camera motion annotation, which can rarely be found in real-world data. To verify the effectiveness of HumanVid, we establish a baseline model named CamAnimate, short for Camera-controllable Human Animation, that considers both human and camera motions as conditions. Through extensive experimentation, we demonstrate that such simple baseline training on our HumanVid achieves state-of-the-art performance in controlling both human pose and camera motions, setting a new benchmark. Code and data will be publicly available at https://github.com/zhenzhiwang/HumanVid/.", "sections": [{"title": "1 Introduction", "content": "High-quality and highly controllable human image animation has significantly progressed as an emerging popular task [17, 29, 31, 71]. Imagine the possibilities of recreating iconic movie performances using just a single photo of the characters, capturing them from any desired angle. This technique has the potential to significantly impact video and movie production. In this study, we focus on animating characters from a single image, considering both human and camera motions as crucial factors for generating realistic human videos."}, {"title": "2 Related Work", "content": "Human Image Animation. The task of human image animation aims to generate coherent human videos from a single image. To enhance controllability, the mainstream works in this field often employ explicit human skeleton representation, e.g., OpenPose [14, 56, 69] and DensePose [20], as additional guidance. Early solutions are majorly developed upon GANs for image animation and pose transfers [16, 48, 54, 53, 55, 77, 83]. More recently, diffusion models (DMs) [25, 59, 41, 65] have been drawing attention from human image animation considering their remarkable success and high-quality results in image [50, 42, 47, 52, 7, 45] and video [11, 85, 57, 27, 26, 51, 75, 64, 22] synthesis. For instance, MagicDance [17] proposes a two-stage training strategy to disentangle the learning of appearance and human motion. Animate Anyone [29] utilizes a reference network to extract the appearance representation from the source image and adopts a motion module similar to AnimateDiff [22] to enhance temporal consistency. It also incorporates a lightweight pose guider to encode pose information to the pre-trained models. Similarly, MagicAnimate [71] adopts DensePose [20] as the motion representation and uses a ControlNet [80] to encode pose information. Champ [86] further introduces the SMPL [38] model sequence and the rendered depth and normal for better alignment. Though with remarkable visual quality, these works mostly adopt a static camera setting and do not consider camera viewpoint movement.\nCamera-aware Video Generation. As a significant component in video and movie production, camera viewpoint movement determines the content dynamics and the overall feeling of the audience. While many works focus on guiding video generative models with structural signals [18, 75, 63, 82, 32, 21], less attention has been paid to controlling the pose/viewpoint of camera in generating videos [70, 76]. To control camera motion with reference videos, MotionDirector [84] proposes a dual-path LoRA [28] adapter to decouple the motion and appearance learning and can roughly control camera movements to produce a surrounding shot. For more precise control, MotionCtrl [68] directly injects the camera extrinsic matrix to the temporal attention layer in pre-trained text-to-video models and can precisely specify the camera viewpoint by providing camera poses at inference. CameraCtrl [23] further enhances the controllability by representing the camera pose with Pl\u00fccker ray embeddings [58, 36]. CamViG [40] explores the camera control in token-based video generator [35] by introducing camera embedding as a new modality."}, {"title": "3 Dataset", "content": "Given that diffusion models typically require large amounts of data, we are pioneering the use of synthetic data in human video generation. While previous datasets [10, 73] only contain single-view image data or clips with basic camera movements (i.e. zoom-in, orbit), we show that accurate annotations, extensive scale and rich camera trajectories from synthetic data could be vital for generation. Our synthetic videos are rendered by Unreal Engine 5 (UE5) [3] or Blender [12]. To enhance the diversity of human appearance, we also curate human-centric internet videos from copyright-free platforms and leverage pose estimation methods [72] for automatic annotation. Both synthetic and internet data are fully scalable without any human supervision."}, {"title": "3.1 Synthetic Data Construction", "content": "The synthetic video data are rendered with one or more characters moving in various 3D scenes using diverse camera trajectories. Consequently, constructing the synthetic data involves three key steps: character creation, motion retargeting, and 3D scene and camera placement."}, {"title": "3.1.1 Character Creation", "content": "We create two types of characters for animating images in the diverse domains: (1) Human-like characters from SMPL-X [44] meshes and clothing. (2) Anime characters from user-uploaded assets. Diverse body shapes and skin textures, 3D clothing and textures are considered for highly varied human representation.\nBody shapes and skin tone. For human-like characters, we sample body shapes from a diverse set of 271 body shapes with different BMI collected from the ARGOA [43] and CAESAR [49] datasets following Bedlam [10]. To reduce the gender and ethnicity bias, we use 50 female and 50 male commercial skin albedo textures collected from Meshcapade [19] with a resolution of 4096 \u00d7 4096, spanning over seven ethnic groups.\n3D Clothing and textures. To generate realistic human videos, it's crucial to have 3D clothing motions that are physically plausible and consistent with human body movements. For instance, the LSMPL-X representation from Synbody [73] adds a clothing layer to SMPL-X [44], but lacks realistic physics simulation for clothing motion, leading to unnatural movements in loose-fitting"}, {"title": "3.1.2 Motion Retargeting", "content": "Given the character assets, we transfer diverse motions to these characters by re-targeting motion data from various sources, including motion capture datasets [39] and open-source software Rokoko [4].\nSPML-X characters. For human-like SMPL-X [44] characters, we sample human motions from large-scale motion capture datasets [39]. To enhance motion diversity, we sample based on motion annotations from [46], following the approach of Bedlam [10].\nAnime characters. Conversely, anime character assets can have diverse skeleton lengths. We utilize the automatic re-targeting software Rokoko to transfer existing motions to the anime character assets. The clothing and hair are treated as part of the body, so their motion is also determined by source motions."}, {"title": "3.1.3 3D Scenes and Camera Placements", "content": "3D Scenes. The realistic and diverse 3D scene backgrounds for synthetic video are constructed from about 100 panoramic HDRI images [2] or high-quality 3D scenes to cover both indoor and outdoor environment. We manually select panorama images with flat ground for characters to move on, while avoiding excessive scene components that might lead to unnatural human-scene interactions. We also exclude images with uniform visual patterns across different views, such as grasslands, deserts, or farms. The selected panorama backgrounds feature high-quality, complex texture details that"}, {"title": "Camera Trajectory Design", "content": "Unlike [10, 73, 43], our dataset highlights rich and diverse camera trajectories in human-centric videos. Each camera trajectory consists of a sequence of 6-DoF translations and rotations. We carefully design a rule-based camera motion generation pipeline to obtain diverse trajectories. This pipeline randomly sample camera locations adaptive to human positions and orientations in the keyframes, and use spline interpolation to get smooth camera locations and rotations in the whole sequence. Specifically, in each keyframe scene space, we randomly sample camera locations within a semi-cylinder of radius $ \u2208 [3m, 5m]$ and height $ \u2208 [0.6m, 1.2m]$ in front of the human. Then, we set the camera orientation's yaw and pitch to point at the person. To create a more natural camera trajectory that smoothly follows the person, we adjust the camera's position by adding the human's position offset from the keyframe to the camera position in each frame. Finally, we also sample the roll of camera rotation $ \u2208 [-30\u00b0, 30\u00b0]$ in keyframes. Our design of camera keyframe sampling enables all types of camera trajectories, significantly enhancing the camera trajectory diversity and cinematic effect of human videos compared to existing video datasets.\nRendering and Annotations. We render the image sequences of SMPL-X characters using UE5 game engine and the built-in movie render function (Movie Render Queue) for high-quality images. The anime characters are rigged and rendered with blender. With our synthetic data source, a variety of ground-truth annotations, including camera trajectories, human skeletons, segmentation masks, depth maps and normal maps, could be obtained without manual efforts. Although our dataset is curated for human video generation, these ground-truth annotations could also be useful for other downstream applications."}, {"title": "3.2 Internet Data Curation", "content": "To enhance the appearance diversity on top of synthetic videos, we collect real human-centric videos from copyright-free internet platforms [5] with the same distribution of synthetic data: the pixel motions in such videos is only resulted from human skeleton motion or camera motion, not object movements or background dynamics.\nWe utilize the Pexels API [5] to scrape data based on around 100 keywords and employed a pose detector [72] to analyze the data. The pose detector focused on measuring the upper body keypoints' confidence, the ratio of the largest human bounding box over the frame $r$, the average number of humans present in each frame $n$, and the average motion (position offsets) of the keypoints $A_p$. With these statistics, we apply a specific filtering criteria: a) the human should occupy the main part of the image ($r > 0.07$); b) there should be few people ($n < 4$); c) there should be a noticeable motion in keypoints to remove static videos ($A_p > 0.01$); d) No exits, entrances or occlusions of individuals in videos ($n \u2208 Z$). As a result, we collect around 20k high-quality, real human-centric video clips with various appearances.\nCamera Trajectory Estimation. Reconstructing global camera trajectories from in-the-wild videos is a challenging problem. Following the curation of human-centric videos from previous steps, we adopt TRAM [66] to utilize a SLAM method [60] for recovering camera extrinsic parameters from in-the-wild videos with explicit human movement. To ensure camera parameters are robust to dynamic humans, we employ a masking technique [34] that removes dynamic regions from both input images and dense bundle adjustment steps. By compelling SLAM to rely solely on the background for camera estimation from the outset, we mitigate the risk of catastrophic failure. To convert camera estimation to metric scale, we leverage semantic cues from the background by utilizing noisy depth predictions [9]. Consequently, we recover accurate, metric-scale camera motion that serves as an optimal camera condition for training diffusion models. For videos with texture-less backgrounds, where the SLAM system reports large reconstruction errors, we empirically set such samples to static cameras. Additionally, we filter out videos with very large rotations or translations, such as cycling, or those with sudden shot changes, as these videos fall outside the scope of human image animation."}, {"title": "3.3 CamAnimate", "content": "To validate our dataset's capability for animating humans with moving cameras, we propose a simple baseline for the camera-controllable human image animation task, named CamAnimate. By leveraging CameraCtrl [23]'s advanced camera pose control and Animate Anyone [29]'s character animation framework, CamAnimate ensures consistent and high-quality human video generation with simultaneous human and camera movements. As shown in Fig. 5, it utilizes pl\u00fccker embeddings to accurately parameterize camera trajectories and incorporates an additional camera pose encoder to encode camera information for the Denoising UNet via zero-convolution [79], while ReferenceNet and a pose guider maintain appearance consistency and pose controllability. By training our method on general human videos with both camera and human movement, these two types of motion can be decoupled in the network and learned by separate modules in an end-to-end manner. In addition to the moving camera setting, CamAnimate can seamlessly handle traditional human image animation given a static camera parameter.\nImplementation Details. We use the checkpoint of Stable Diffusion 1.5 [50] to initialize the Denoising UNet and ReferenceNet, and use the weights of ControlNet [80] on OpenPose [15] to initialize the Pose Guider. The camera encoder weights from CameraCtrl [23] are used to initialize our camera encoder. We mix train horizontal and vertical videos with a resolution of (long side, short side) = (896,512), i.e., for horizontal videos (w, h) = (896,512) and for vertical videos (w, h) = (512,896). Each batch only samples either all horizontal or all vertical videos, and the choice between horizontal and vertical videos is made randomly between batches. The setting of"}, {"title": "4 Experiments", "content": null}, {"title": "4.1 Evaluation Benchmark", "content": "Due to the lack of a unified benchmark for previous methods, the testing protocols for each method have been varied significantly. However, when the form of the samples differs, the disparity between the reference image and the target image can vary greatly, leading to highly inconsistent results for the same method when inferring different reference and target images. Therefore, as the first large-scale dataset, we provide a unified testing protocol for human video generation. Specifically, we use the middle (12th) frame as the reference image, predict frames in the range [1,72) with a stride of 3, resulting in a sequence of 24 frames. Finally, we evaluate each video under this setting using PSNR [67], SSIM [67], LPIPS [81], FID [24], and FVD [61] metrics.\nWe use the last 40 videos from the TikTok dataset [30] out of a total of 340 videos as the test set for evaluation on static camera. Additionally, we provide 40 videos each in both portrait and landscape orientations as a test set for evaluation on moving camera human video generation, sampled from our"}, {"title": "4.2 Comparison with the State-of-the-Art", "content": "In this section, we compare our baseline model with previous state-of-the-art methods, namely Animate Anyone [29], Magic-animate [71] and Champ [86]. As animate anyone is not open-sourced, we use a third-party implementation. We use the official implementations for other two methods. As shown in Tab. 2, although our method is trained on videos with moving cameras, it is still able generate high-quality static camera videos on TikTok dataset and achieves best performance in all metrics due to our precise camera control ability. For human videos with camera movement on our test set, previous methods commonly do not consider the camera condition, so they struggle to produce natural videos with camera movements. Such result could be observed from both reconstruction metrics like SSIM, PSNR, and LPIPS and generation metrics like FID and FVD.\nUser-study. We also conduct a user-study to compare our method with previous methods, as shown in Tab 4. We collect 2 videos from Tiktok test set and 8 videos from our test set, and compare our results with other three methods' results as a ranking question with 4 options. A total of 20 participants take part in our user study and we assign 3, 2, 1 points for the first, second and third method respectively. The final average score is normalized by total points, so the upper bound of this metric is 0.5. We conclude the average points and top-1 preference of each method in Tab 4, which shows a dominate advantage (0.44 points and 0.73 top-1 preference) over previous methods due to their artifacts in appearance, human pose and camera movements.\nQualitative comparisons. In Fig. 6, we show the qualitative comparison with previous SOTA methods, where the artifacts are highlighted by red boxes. We show that previous SOTA methods may have different artifacts when applied to our challenging evaluation test sets. For example, animate anyone suffers from inaccurate and low-quality appearances of humans. Champ suffers from missing 3D skeletons due to the difficulty of estimating accurate 3D skeletons, especially in a crowded scene. Magic-animate is not able to correctly generate human face expressions due to the ambiguity of face representation of Densepose. Our method is able to accurately generate facial expressions, body pose and background motions from the camera movement."}, {"title": "5 Conclusion", "content": "In this paper, our study addresses the significant challenges in the field of human image animation by introducing a novel combination of a high-quality real-world video dataset and a meticulously crafted synthetic dataset. Our proposed dataset not only enhances the visual quality and controllability of human animations, but also introduces a new benchmark for camera control in human videos. Without bells and whistles, our proposed simple baseline demonstrate superior performance when it is trained on our combined dataset, particularly in scenarios involving complex human and camera motions. We believe that our contributions will pave the way for more transparent and comprehensive evaluations in this field, fostering further advancements and innovations in video and movie production.\nLimitations. The annotation of our Internet data heavily rely on pose estimation [72, 20] and SLAM [66, 60] methods, which could introduce noises into the camera and pose annotations.\nBroader Impacts. Our dataset and baseline method are highly effective at creating realistic human videos. Nonetheless, it's important to recognize that improvements in generative model technologies could lead to the creation of realistic deepfakes, which may be misused to spread misinformation."}]}