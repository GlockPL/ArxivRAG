{"title": "SCALING LARGE VISION-LANGUAGE MODELS FOR ENHANCED MULTIMODAL COMPREHENSION IN BIOMEDICAL IMAGE ANALYSIS", "authors": ["Robinson Umeike", "Neil Getty", "Fangfang Xia", "Rick Stevens"], "abstract": "Large language models (LLMs) have demonstrated immense capabilities in understanding textual data and are increasingly being adopted to help researchers accelerate scientific discovery through knowledge extraction (information retrieval), knowledge distillation (summarizing key findings and methodologies into concise forms), and knowledge synthesis (aggregating information from multiple scientific sources to address complex queries, generate hypothesis and formulate experimental plans). However, scientific data often exists in both visual and textual modalities. Vision language models (VLMs) address this by incorporating a pretrained vision backbone for processing images and a cross-modal projector that adapts image tokens into the LLM dimensional space, thereby providing richer multimodal comprehension. Nevertheless, off-the-shelf VLMs show limited capabilities in handling domain-specific data and are prone to hallucinations. We developed intelligent assistants finetuned from LLaVA models to enhance multimodal understanding in low-dose radiation therapy (LDRT)\u2014a benign approach used in the treatment of cancer-related illnesses. Using multilingual data from 42,673 articles, we devise complex reasoning and detailed description tasks for visual question answering (VQA) benchmarks. Our assistants, trained on 50,882 image-text pairs, demonstrate superior performance over base models as evaluated using LLM-as-a-judge approach, particularly in reducing hallucination and improving domain-specific comprehension.", "sections": [{"title": "1. INTRODUCTION", "content": "Large language models (LLMs) are rapidly transforming scientific research due to their ability to create plausible natural language outputs supporting tasks such as text generation, translation, question answering, and content summarization [1]. However, processing scientific data often requires comprehending modalities beyond text. Vision language models (VLMs) address this limitation by introducing visual data comprehension for richer information retrieval.\nVLMs, trained on extensive visual and textual datasets, exhibit considerable potential as intelligent assistants. Their training process involves two key phases: adapting vision tokens from the foundation model into the LLM space (feature alignment) and enabling specific downstream tasks (fine-tuning). In this study, we fine-tune LLaVA v1.6-vicuna-\n13B and LLaVA v1.5-13B on machine-generated biomedical data derived from low-dose radiation therapy (LDRT) articles\n[2], specifically using captions and textual artifacts related to the extracted images. Through a series of comprehensive experiments on our curated dataset, we demonstrate substantial improvement in LDRT-based visual question answering tasks compared to base models."}, {"title": "2. RELATED WORKS", "content": "Language-Only Scientific Assistants: Domain-specific\nLLMs have emerged as resourceful digital assistants or\nchatbots capable of engaging in meaningful dialogue across\na variety of disciplines [3]. These models are typically trained\non large, carefully curated corpora of finetuned textual data, enabling them to generate human-like responses that often surpass general-purpose models on field-specific benchmarks. For instance, Galactica, an open-source\nlanguage model developed by Meta AI, demonstrates\nadvanced capabilities in processing technical knowledge across multiple modalities including LaTeX, text, DNA/AA\nsequences, code, and SMILES notation [1]. Trained on over\n48 million scientific documents, Galactica with 120B\nparameters outperforms general-purpose models like GPT-3\n175B and PaLM 540B [3, 4]. Similarly, domain-specific models like SciBERT and BioBERT have achieved state-of-the-art performance in biomedical applications and\nbenchmarks such as BC5CDR and NCBI-disease [5-8]. However, their effectiveness in tasks requiring visual\ncomprehension remains limited due to the lack of multimodal understanding.\nGeneralist VLMs: Recent breakthroughs in general-purpose\nVLMs that process multiple modalities-images, videos, and\naudio alongside text-have sparked investigations into their\nadaptability for specialized applications [2]. These versatile models are gaining widespread recognition due to their\npotential relevance in many real-world tasks, such as\nanalyzing medical imaging datasets for surgical planning or\ninterpreting microscopy images for pathology diagnosis [9]. However, a major hurdle to adapting these generalist VLMs\nand LLMs to domain-specific benchmarks is the prevalence\nof hallucinations. In VLMs, this phenomenon occurs when\nthe model makes up information that isn't in the source\nmaterial (ground truth text or image), which presents a critical\nchallenge in biomedical imaging applications where accurate\ninterpretation of visual and linguistic data is crucial. For\nexample, hallucination could lead to incorrect interpretations\nof X-ray or MRI scans, potentially compromising tasks such\nas automated diagnosis, report generation, and clinical\ndecisions (Figure 1). While some studies have proposed\nincreasing image resolution as an approach to mitigate this\nproblem [10], our research takes a different approach. We\nseek to bridge this gap in biomedical image analysis by fine- tuning pretrained generalist VLMs specifically for scientific literature, with emphasis on preserving factual accuracy and reducing hallucinations through specialized preprocessing techniques and domain-specific training data."}, {"title": "3. METHOLODY", "content": null}, {"title": "3.1. Data Preprocessing", "content": "GPT-4 was used to generate 100 keyword search strings related to LDRT, cancer, radiation biology, and related fields"}, {"title": "3.2. Model Architecture", "content": "Our model builds upon the LLaVA architecture (Figure 2), which consists of three main components: a pre-trained language model, a vision encoder, and a cross-modal projector [2]. The model's open-source nature and well- documented architecture make it ideal for research"}, {"title": "3.3. Finetuning and Optimization", "content": "Finetuning Large VLMs is a computationally intensive task requiring careful balance among model performance, runtime efficiency, GPU utilization, memory constraints, dataset size, and hyperparameter configurations. The optimization objective during fine-tuning can be formalized as:\n$0^* = argmin_{\\theta_p, \\theta_l} [L_2(\\theta_p, \\theta_l) + \\lambda R(\\theta_p, \\theta_l)]$\nwhere $L_2$ is the primary loss function, $R$ is a regularization term, and $\\lambda$ is the regularization coefficient controlling its contribution. To efficiently optimize this objective, we employ the following techniques:\nMemory Optimization. We employed two key strategies to optimize memory usage: Gradient checkpointing, which reduces memory footprint by recomputing intermediate activations during backpropagation, and FlashAttention-2, which implements memory-efficient attention computation with O(N) (linear) memory complexity instead of the traditional O(N2) (quadratic) [14].\nComputational Efficiency: DeepSpeed ZeRO3 (Zero\nRedundancy Optimizer) enables efficient model and data\nparallelism and portioning [15]. This partitioning strategy, combined with FlashAttention-2 [13], achieves significant runtime acceleration.\nParameter Efficiency. Low-Rank Adaptation (LoRA) was\nused to minimize the trainable parameter space during fine- tuning, W = Wo + BA, where Wo \u2208 Rdxk represents the frozen pretrained weights, B\u2208 Rdxr, A \u2208 Rrxk are trainable low-rank matrices, and r is the adaptation rank, such that r < min(d,k)\n[15]. Without these optimization strategies, training would be computationally infeasible given our available resources."}, {"title": "4. EXPERIMENT AND RESULT", "content": "Our training was initialized from the instruction-tuned\nLLaVA checkpoint. The experimentation was performed\nusing a compute node equipped with 4 A40G GPU, training on 50,882 entries for 1 epoch with a batch size of 16 (4 per\nGPU \u00d7 4 GPUs) and a learning rate of 2e-4 with cosine decay.\nThe model was configured with LoRA parameters (r=128, a=256) for both the vision module and the base model. The\ncross-modal projector, implemented as a 2-layer MLP with\nGELU activation, was trained with a learning rate of 2e-5.\nWe utilized a 3% warmup ratio for learning rate scheduling and set the maximum sequence length to 2048 tokens.\nModel Evaluation and Inference. For model evaluation, we\nperformed inference on both base LLaVA models (v1.5-13B\nand v1.6-vicuna-13B) and our fine-tuned variants. The\ninference process utilized a temperature (balance between\ncreativity and predictability) of 0.2 with a maximum\ngeneration length of 1024 tokens. Each image-question pair\nwas processed by prepending image tokens to the question text, followed by model generation of responses. We\nevaluated models using both the base checkpoints from\nLLaVA and our fine-tuned versions, testing them on our\nevaluation set using identical image inputs and question\nprompts to ensure fair comparison.\nVQA Evaluation. We used Qwen2-72B-Instruct [12] and\nLlama-3.1-70B-Instruct [16], as independent judges to\nevaluate both base and fine-tuned models on our image-text\nevaluation set. Following the evaluation protocol in LLaVA\n2], the judges were prompted to assign integer scores (0-10)\nbased on response relevance, helpfulness, and accuracy. The\nconsistency in scoring between judges validates our\nevaluation methodology, as illustrated in Table 1.\nHallucination Analysis. The models' hallucination tendency\nwas estimated using two complementary approaches. First,\nusing ROUGE metrics to measure response alignment with\nground truth (captions and contextual sentences), where higher scores indicate better factual consistency. Second, by\nanalyzing linguistic markers of uncertainty, where we\nobserved base models frequently employ hedging language. Notably, LLaVA v1.6-vicuna-13B uses \"appears\" 1,451\ntimes in the evaluation set compared to only 49 times in our fine-tuned version. This significant reduction in hedging language, combined with improved ROUGE scores and length ratios closer to ground truth (Figure 4), suggests our fine-tuned models exhibit both higher confidence and better factual consistency."}, {"title": "5. DISCUSSION AND CONCLUSION", "content": "In this paper, we present two scientific assistant models, fine- tuned from two LLaVA versions and capable of understanding LDRT images from scientific articles. They generally outperform LLaVA across model versions and question types; however, we observe an interesting trade-off\nin LLaVA v1.6-vicuna-13B, which shows better performance on detail-type tasks but exhibits verbosity as seen in its length ratio. This suggests a potential balance in our model between verbosity and reasoning depth. Given that complex reasoning represents a more challenging and valuable capability in scientific applications, our models' clear advantage in these tasks, combined with reduced hallucinations and improved confidence, is particularly significant. Future work will focus on expanding the data curation and hallucination analysis pipeline to enhance performance further and adapting the data for other specific biomedical applications."}]}