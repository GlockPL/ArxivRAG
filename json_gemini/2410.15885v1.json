{"title": "HOW TO BUILD A PRE-TRAINED MULTIMODAL MODEL FOR SIMULTANEOUSLY CHATTING AND DECISION-MAKING?", "authors": ["Zuojin Tang", "Bin Hu", "Chenyang Zhao", "De Ma", "Gang Pan", "Bin Liu"], "abstract": "Existing large pre-trained models typically map text input to text output in an\nend-to-end manner, such as ChatGPT, or map a segment of text input to a hierar-\nchy of action decisions, such as OpenVLA. However, humans can simultaneously\ngenerate text and actions when receiving specific input signals. For example, a\ndriver can make precise driving decisions while conversing with a friend in the\npassenger seat. Motivated by this observation, we consider the following ques-\ntion in this work: is it possible to construct a pre-trained model that can provide\nboth language interaction and precise decision-making capabilities in dynamic\nopen scenarios. We provide a definitive answer to this question by developing a\nnew model architecture termed Visual Language Action model for Chatting and\nDecision Making (VLA4CD), and further demonstrating its performance in chal-\nlenging autonomous driving tasks. We build VLA4CD on the basis of transformer-\nbased LLM architecture. Specifically, we leverage LoRA to fine-tune a pre-trained\nLLM with data of multiple modalities covering language, visual, and action. Un-\nlike the existing LoRA operations used for LLM fine-tuning, we have designed\nnew computational modules and training cost functions for VLA4CD. These de-\nsigns enable VLA4CD to provide continuous-valued action decisions while out-\nputting text responses. In contrast, existing LLMs can only output text responses,\nand current VLA models can only output action decisions. Moreover, these VLA\nmodels handle action data by discretizing and then tokenizing the discretized ac-\ntions, a method unsuitable for complex decision-making tasks involving high-\ndimensional continuous-valued action vectors, such as autonomous driving. The\nextensive experimental results on the closed-loop autonomous driving platform\nCARLA validate that: (1) the model construction method we proposed is effec-\ntive; (2) compared to the state-of-the-art VLA model, VLA4CD can provide more\naccurate real-time decision-making while retaining the text interaction capability\ninherent to LLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Since the emergence of ChatGPT, large-scale pre-trained models, represented by large language\nmodels (LLMs), have garnered increasing attention. LLMs are trained on vast amounts of text and\ncode data on the internet, encoding a significant amount of general knowledge about the real world.\nThis equips them with better generalization capabilities compared to traditional AI models, such as\nin-context learning abilities and certain reasoning capabilities (through techniques such as chain-of-\nthought (Wei et al., 2022)). A development trend in the field of large-scale pre-trained models is that"}, {"title": "2 RELATED WORK", "content": "their application domains are expanding from tasks like dialogue and text generation to decision-\nmaking tasks in the open physical world.\nHow to build large-scale pre-trained models for decision-making tasks in the open physical world?\nCurrently, there are three major approaches. An approach is to serialize the decision-making pro-\ncess and then train a sequence model, such as the decision transformer (Chen et al., 2021), in the\nsame way as processing text. This method relies on the construction of large-scale high-quality\ndecision-making datasets. The second approach involves adopting a hierarchical modular system\ndesign, where the pre-trained LLM provides high-level planning, such as breaking down the target\ntask into a series of subtasks and then completing each subtask by calling tools or small models\naimed at the subtasks (Chen et al., 2024; Carta et al., 2023; Hu et al., 2024; Zhou et al., 2024). This\napproach requires manual pre-design of system modularization and the establishment of interfaces\nbetween modules. Additionally, after the model is deployed, in addition to the latency caused by\nLLM inference, it also introduces the working latency of other modules, making it unsuitable for\ndecision-making scenarios with high-time requirements. The last approach is to train a multimodal\nvisual language action model (VLA) based on LLM (Padalkar et al., 2023; Kim et al., 2024). Un-\nlike the hierarchical modular method, the VLA model can provide end-to-end decision generation,\neliminating the need for manual module design and interface design between modules.\nTo the best of our knowledge, existing LLM or VLA models, given an input signal (a piece of text\nprompt, an image, or a video), produce outputs that are single-modal (a piece of text or an action\ndecision). However, we know that for us humans, we can simultaneously generate text and actions\nwhen receiving specific input signals. For example, a driver can make precise driving decisions\nwhile conversing with a friend in the passenger seat. Inspired by the above observations, we attempt\nto answer the following question in this paper:\nIs it possible to develop a pre-trained model that can provide both action decision-making and\ntext interaction capabilities in an end-to-end manner?\nWe provide a definitive answer to it by developing a new model architecture termed Visual Lan-\nguage Action model for Chatting and Decision Making (VLA4CD), and further demonstrating its\nperformance in challenging automonous driving tasks. Like existing VLA models, VLA4CD is a\nmultimodal pre-trained large model developed based on the transformer architecture. However, it\nhas significant differences from current VLAs (such as RT-X (Brohan et al., 2022; 2023)):\n\u2022 The operational mechanism of VLA involves executing serialized decisions after receiv-\ning text instructions, without generating text data during the decision-making process. In\ncontrast, VLA4CD allows for the synchronous generation of text data during real-time\ndecision-making.\n\u2022 Current VLA models typically handle action data by discretizing it and then tokenizing\nthe discrete values. This approach is not suitable for complex decision-making scenarios\nsuch as autonomous driving, where actions are high-dimensional continuous value vectors.\nOur VLA4CD processes actions directly as continuous values, eliminating the need for\ndiscretization and making it more suitable for such scenarios.\nIn summary, the main contributions of this work are as follows.\n\u2022 We propose a new problem setting: how to synthesize the capabilities of LLM and VLA us-\ning a single model to achieve end-to-end simultaneous action decision-making and chatting\nwith people.\n\u2022 We present a solution to the aforementioned problem. Specifically, we propose a method\nfor constructing VLA4CD based on pre-trained LLM and have validated the effectiveness\nand superiority of this method through extensive closed-loop autonomous driving experi-\nments on CARLA (Dosovitskiy et al., 2017). The experimental results show that the result-\ning VLA4CD model not only outputs more accurate real-time action decisions compared\nto the SOTA models but also perfectly retains real-time text-based dialogue functionality.\nOur method combines several experimentally validated ideas: (1) a computational module\nand cost function term for generating continuous action values; (2) an image reconstruction\nloss term added in the training cost function to ensure the exploitation of rich information\nfrom the visual modality data during text generation and decision-making processes; (3) a\nlabel smoothing strategy to maintain dialogue capabilities and enhance decision-making."}, {"title": "3 METHODOLOGY", "content": "In this section, we present how to build VLA4CD in detail, including the model architecture and the\ntraining procedure, with a focus on the loss designs in the last output hiddens layer. An overview of\nVLA4CD is illustrated in Figure 1. To begin with, we present the problem setting of our concern."}, {"title": "3.1 PROBLEM SETTING", "content": "We consider a multimodal setting similar as (Xiao et al., 2020), wherein, at each time step $t$,\nupon the agent performs an action $a_t$, the environment returns an observation consisting of both\nvisual and textual modalities, denoted by ${o_t, w_t}$. Our objective is to build a generative model\n$\\pi(a_\\tau, w_t | O_{t-H}, W_{t-H}, a_{t-H}, ..., O_t, w_t)$, which can generate both high-quality action decisions and"}, {"title": "3.2 MODEL ARCHITECTURE", "content": "Our model supports three different input modalities: text, image, and numeric vector. We use Llama-\n7b (Touvron et al., 2023b) as the backbone model, and encode textual inputs by its pre-trained\nembedding layers. To encode the visual inputs, we follow the standard practice used in visual\nlanguage models (VLMs) (Liu et al., 2024) and VLAs (Kim et al., 2024). Specifically, we first\nsegment each input image $o$ into $L$ patches $p_i, i=1, ..., L$, then train a 2D convolution network\nthat directly maps the patches to the vector space. In addition, to deal with the input of the action\nvalue, we train a multi-layer perceptron (MLP) module that encodes the action values to the vector\nspace. Finally, We concatenate encoded embeddings of all modalities together to form a sequence\nof embedded trajectory $\\tau$ at time $t$ as follows:\n$\\tau_t = \\{(p^v_{1-H},...,p^v_L_{1-H}), (w^t_{1-H},...,w^t_{j-H}), a_{t-H}, ..., (p^v_1,...,p^v_L), (w^t_1,...,w^t_j)\\},$ (1)\nwhere $p_i$ and $w_j$ denote the embeddings of i-th patch for visual observation and j-th token for\ntextual observation at at time $t$, respectively.\nDuring the inference stage, the transformer backbone in VLA4CD generates the hidden embed-\ndings $s_{t+1},..., s_{t+n+1}$ as shown in Figure 1, then these embeddings are decoded into the outputs\nof different modalities. Specifically, VLA4CD supports two different output modalities: text for\nchatting and numeric vector for action-level decision making. For the chatting part, we use the\npre-trained output MLP layers and tokenizer of the Llama-7b model to generate texts. For action\ndecision-making, our model generates one more embedding vector after the \"< EOS >\u201d, an empty\nplaceholder token. Unlike previous work like OpenVLA (Kim et al., 2024) and RT-X (Brohan et al.,\n2023), in which action prediction is formalized as a token generation task by splitting the action\nspace into discrete action bins, we train an action head consisting of a two-layer MLP module. This\naction head directly maps the output embedding to action values. We empirically find that using our\napproach leads to better performance compared to discretizing action values."}, {"title": "3.3 TRAINING PROCEDURE", "content": "We fine-tune the transformer backbone with LoRA (Hu et al., 2021) and train the image encod-\ning module, text encoding module, action encoding, and decoding modules with an offline dataset\n$D_{expert}$, which contains demonstrated trajectories of driving vehicles with question-answer pairs\nrelated to this driving scenario. The training objective is to predict accurate actions for vehicle con-\ntrol and answer domain questions such as \u201cSummarize the current driving scenario at a high level\u201d.\nMoreover, to encourage the model to abstract key information from the images and prevent over-\nfitting, we consider image reconstruction as an auxiliary task, adding a 2D transposed convolution\nlayer to reconstruct input images patches from the output last hidden embeddings $s_{t,\u2026\u2026,s_{t + n}}$, as illus-\ntrated in Figure 1. As a result, our training loss is composed of three items, corresponding to text\ngeneration, action prediction, and image reconstruction, respectively. Next, we describe each loss\nitem in detail. For ease of presentation, we denote the parameters in the auxiliary image decoder as\n$\\phi$ and all other trainable parameters as $\\theta$.\nText Generation In our experiment, we found that merely replacing specific numerical values in\nthe translation template (Chen et al., 2024) results in minimal representational differences caused\nby the sequential nature of the data, making the phenomenon of model overfitting easy to happen if\nwe use the conventional cross-entropy loss for text generation. Refer to Appendix A.7 for details.\nTo mitigate this, we use the label smoothing technique to regularize the training process (Szegedy\net al., 2016). Specifically, the hard label for token $w_i$ is smoothed by assigning a small portion of\nthe probability mass to incorrect classes:\n$L_i =\\begin{cases}\n1 - \\epsilon & \\text{if } k = Y_i, \\\\\n\\frac{\\epsilon}{K} & \\text{otherwise},\n\\end{cases}$ (2)\nwhere $\\epsilon$ is the smoothing factor and $K$ is the number of total classes, i.e., vocabulary size. That is to\nsay, the loss item for text generation we finally use is:\n$L_{language}(\\theta) = \\frac{1}{N} \\sum_{i=1}^T\\sum_{k=1}^K L_i log (p(w_i|\\tau_{1:i-1}, \\theta))$, (3)\nwhere $\\tau_{1:i-1}$ denotes the input token sequence before position $i$, used for predicting token $i$. N\ndenotes the maximum padding length to unify the input text.\nAction Prediction To directly predicts continuous action values instead of discrete action bins,\nwe train our model with a mean square error (MSE) loss between the ground-truth action value $a_t$\nand the predicted value, as follows:\n$L_{action} (\\theta) = \\frac{1}{T} \\sum_{i=1}^T \\frac{1}{D} \\sum_{d=1}^D [(a^d_t - \\pi( \\tau_t, \\theta))^2]$ (4)\nwhere $D$ denotes the dimension of the action space. In our experiments, the action dimension is 2,\ncorresponding to the acceleration and steering of the vehicle, respectively.\nImage Reconstruction The visual modality data contains rich information about the states of the\nenvironment. However, we find that, with a limited dataset, directly training the image encoder from\nlanguage and action losses is not sufficient, as it leads to information losses. Inspired by Hafner\net al. (2019), we consider an auxiliary image reconstruction task to introduce additional supervision\nin the visual modality. Specifically, we use a 2D transposed convolution layer $f_{\\phi}$ to reconstruct\neach image patch from its corresponding output embedding and train the model to minimize the\npixel-wise Euclidean distance between the original and reconstructed image patches:\n$L_{image}(\\theta, \\phi) = \\frac{1}{L} \\sum_i^L MSE(o_i, f_{\\phi}(\\pi(g_{\\theta}(\\tau_p),\\theta))),$ (5)\nwhere $o_i$ is the input image, and $\\tau^p_i$ is the input sequence up to this patch token, and $g_{\\theta}$ represents\na trainable 2D convolutional network that directly maps image patches $p_1,\u2026\u2026,p_L$ to the language\nembedding space $p_1,\u2026\u2026,p_L$.\nTraining Loss Function In summary, our training loss function is defined as follows:\n$L(\\theta, \\phi) = \\alpha_1L_{language}(\\theta) + \\alpha_2L_{action}(\\theta) + \\lambda L_{image}(\\theta, \\phi),$ (6)"}, {"title": "4 EXPERIMENTS", "content": "In this section, we validate through experiments on the autonomous driving simulation platform\nCARLA that VLA4CD can make fine-grained action decisions while maintaining dialogue function-\nality. We also examine the impact of each loss term in our loss function design on the performance of\nVLA4CD, as well as the quality of textual modality data in training data affects the decision-making\nperformance of the model."}, {"title": "4.1 EXPERIMENTAL SETTING", "content": "We conducted our experiments in a benchmark environment called gym-carla (Chen, 2020), which\nis a third-party environment for OpenAI Gym, integrated with the closed-loop autonomous driving\nsimulator CARLA 0.9.10 (Dosovitskiy et al., 2017). This experimental environment can provide\nimage observations and supplementary textual descriptions relevant to the target task, with high\ndemands on decision-making. During LoRA fine-tuning, we only fine-tuned the Q projection and\nV projection modules, the fine-tuned parameters accounting for only 0.06% of Llama-7B's whole\nparameters. For more details on the hyperparameter settings for VLA4CD, parameters for the linear\nmapping layer, and parameter settings in gym-carla, refer to Appendix A.1."}, {"title": "4.2 COMPARISON METHODS", "content": "The Behavior Cloning (BC) method performed in gym-carla (Chen, 2020) was used as a baseline.\nThe other methods involved for comparison include RL methods Dreamer (Hafner et al., 2019)\nand Forbes (Chen et al., 2022), Decision Transformer (DT) (Chen et al., 2021), and VLA models\nOpenVLA (Kim et al., 2024) and DriverGPT4 (Xu et al., 2024)."}, {"title": "4.3 TRAINING DATASETS", "content": "We trained all comparison methods based on an expert dataset $D_{expert}$, which is 5.69GB in size,\ncontaining 13,761 frames. We used 90% of it as the training set and the remaining as the test set.\nWe evaluated these comparison methods online in the random mode of CARLA town03. Following\nthe work on DT (Chen et al., 2021), we investigated the performance of sequence fusion for both\nsingle time steps and multiple time steps. We set the context length $H = 1$, resulting in a fusion se-\nquence length of 489. This includes dividing the 128\u00d7128 image into 64 tokens and padding the text"}, {"title": "4.4 PERFORMANCE METRICS", "content": "Performance metrics for evaluating the chatting ability We used the powerful model GPT-\n4$\\$\\U0001f9e0$\\$(OpenAI, 2023) to compare the quality of answers given by VLA4CD with baseline models.\nSpecifically, we first chose 50 pieces of randomly generated environment information and questions\nin CARLA. Then, given a piece of environment information and a question, we let VLA4CD and\na baseline model each generate an answer. Then we used GPT-4$\\$\\U0001f9e0$\\$$ to score them, with a maximum\nscore of 10. The scoring criteria are as follows: Not Acceptable (< 3), Acceptable (3 < score < 6),\nGood (\u2265 6). Additionally, to assess the impact of the language and image components on dialogue\ncapabilities, we included VLA4CD (no-language) and VLA4CD (no-image), two simplified ver-"}, {"title": "4.5 EXPERIMENTAL RESULT ON CHATTING ABILITY EVALUATION", "content": "As shown in Figures 2 and 4, VLA4CD performs significantly better that others in terms of chat-\nting ability. In contrast, OpenVLA performs poorly in question-answering because it focuses solely\non optimizing the action loss. DriverGPT4 faces challenges as both tasks share the same decoder,\ncausing the model to misinterpret inputs as only for action prediction, making it difficult to generate\ncomplete text. Despite having two independent loss items, the model has not effectively balanced\nthese two losses. Furthermore, VLA4CD (no language) shows a significant gap in conversational\nability compared to VLA4CD, while VLA4CD (no image) performs similarly to VLA4CD, high-\nlighting the importance of the language loss component for enhancing chatting abilities."}, {"title": "4.6 EXPERIMENTAL RESULT ON DECISION-MAKING ABILITY EVALUATION", "content": "We define the \u201c-\u201d in Tables 1, 2, and 3 as a failure standard if a complete action value is not generated\nwithin 50 seconds. As shown in Table 1, VLA4CD significantly outperforms BC and OpenVLA in\nterms of DS, AR, and ASD at a single time step, while DriverGPT4 fails to generate precise action\nvalues. VLA4CD also shows significant improvements over other methods across multiple time\nsteps in Table 2, indicating sustained benefits over longer durations. We evaluated these models\u2019\ngeneralization capability by training them on the town03 dataset and then evaluating them online"}, {"title": "4.7 ABLATION STUDIES ON THE LOSS FUNCTION DESIGN", "content": "As shown in Equation (6), our loss function is composed of three losses, namely action loss $L_{action}$,\nlanguage loss $L_{language}$, and image loss $L_{image}$. We conducted ablation studies to investigate the\neffect of each loss on the performance of VLA4CD. The experiment result is shown in Table 4,\nwhere the action-bins loss $L_{action-bins}$ denotes the action loss used by OpenVLA and RT2. They\ndeal with continuous valued actions by value discretization. We included VLA4CD (no-language)\nand VLA4CD (no-image), two simplified versions of VLA4CD trained by using $L_{action} +L_{image}$ and\n$L_{action} +L_{language}$, respectively.\nOn the effect of $L_{action}$ As shown in Table 4, if we compare the performance metrics of $L_{image} +$\n$L_{language} + L_{action-bins}$ with that of $L_{image} + L_{language} + L_{action}$, we can see a clear advantage of using\nour action loss $L_{action}$ over using $L_{action-bins}$. This explains why VLA4CD outperforms VLA models\nthat use the type of action loss similar to $L_{action-bins}$, as shown in Tables 1, 2, and 3. Specifically,\nfrom our experiments, we found that doing action discretization and tokenization as in current VLA\nmodels lead to low training loss but bad inference performance. This is because adjacent action\nintervals are represented by consecutive token IDs (e.g., 31830 and 31831), which are close in token\nspace. Consequently, the model tends to output the same token (31830 or 31831) in inference, while\nthe actual action values corresponding to them can have significant differences. In contrast, our\napproach proposed to deal with continuously valued actions can avoid this phenomenon to happen.\nOn the effect of $L_{language}$ As shown in Table 4, if we compare performance metrics between\n$L_{image} + L_{action}$ (corresponding to VLA4CD (no-language)) and $L_{image} + L_{language} + L_{action}$ (corre-"}, {"title": "4.8 HOW THE QUALITY OF TEXTUAL MODALITY DATA IN TRAINING DATA AFFECTS THE\nDECISION-MAKING PERFORMANCE OF THE MODEL?", "content": "Imagine a driver is operating a car, with a friend sitting inside the vehicle and conversing with the\ndriver. If this friend provides valuable reminders, such as alerting the driver to a car approaching\nfrom the blind spot, the friend's words would be beneficial to the driver's decision-making. On the\ncontrary, if the friend's words are irrelevant noise to the current situation, it might interfere with\nthe driver's ability to make accurate decisions. Therefore, we designed a set of experiments to test\nwhether our model exhibits similar performance to that of human drivers in decision making. The\nresult is presented in Table 5. As is shown, when we add more and more noisy information unrelated\nto driving scenarios into the text modality data in the training dataset, the quality of the decisions\noutput by our model rapidly decreases. This indicates that the performance of our model is very\nsimilar to that of human drivers."}, {"title": "5 CONCLUSION", "content": "In this paper, we investigated how to develop a multimodal pre-trained model that simultaneously\nachieves the dialogue function of LLM and the decision-making function of VLA. We use the au-\ntonomous driving scenario as an example to explain our problem setup and model development\nprocess. Unlike the instruction-following setup used behind VLA models, our problem setup can\nbe described as making decisions while conversing. In the former, text data appears in the form of\ninstructions before the decision-making process; in the latter, text data and decision data are inter-\nwoven (imagine a large pre-trained model making driving decisions while chatting with people in\nthe car). For the aforementioned problem setup, we provide a method for constructing a multimodal\nVisual Language Action model for simultaneously Chatting and Decision making (VLA4CD). Ex-\nperimental results show that, thanks to our proposed way to deal with continuous valued actions,\nour design of the training cost function, and the use of label smoothing technique, our VLA4CD\nmodel significantly outperforms the SOTA VLA model, RL, and decision transformer methods in\ndecision-making performance, while also possessing smooth dialogue capabilities.\nVLA4CD can be seen as a functional extension of the VLA model, while its performance depends\non the quality of the training data set. Interesting future research directions include: further testing\nand validation using large-scale real-world driving datasets; and applying our approach to scenarios\nbeyond autonomous driving, such as home robots."}, {"title": "A APPENDIX", "content": "In this section, we respectively introduce the model parameters of VLM4EDM, the parameters of\nthe custom linear layers, as well as the parameters of gym-carla and evaluation, as shown in Tables\n6, 7, and 8."}, {"title": "A.2 CARLA MAPS", "content": "In order to comprehensively evaluate the performance of our EGADS, we utilized five maps in\nCARLA, including town03, town04 as shown in Figure 6. Town03 is a larger town with features\nof a downtown urban area. The map includes some interesting road network features such as a"}, {"title": "A.3 IS MODEL DECISION-MAKING ABILITY ENHANCED WITH LONGER CONTEXT OF\nTRAJECTORIES?", "content": "As shown in Table 9, we observed that although the context length H of input trajectories is longer,\nthe overall DS and AR of VLA4CD show some improvement, but the increase is not significant.\nThis improvement is primarily attributed to the higher route completion and lower collision rates\nassociated with longer time steps. According to Section 4.3, when H = 4, the sequence length"}, {"title": "A.4 REWARD FUNCTION", "content": "We use the default reward function of the Gym-Carla benchmark (Chen, 2020) to evaluate all exper-\nimental methods, as follows:\n$f = 200r_c + v_{lon} + 10r_f + r_o - 5a^2 + 0.2r_{lat} - 0.1$ (7)\nwhere $r_c$ is the reward related to collision, which is set to -1 if the ego vehicle collides and 0\notherwise. $v_{lon}$ is the longitudinal speed of the ego vehicle. $r_f$ is the reward related to running too\nfast, which is set to -1 if it exceeds the desired speed (8 m/s here) and 0 otherwise. $r_o$ is set to -1\nif the ego vehicle runs out of the lane, and 0 otherwise. $a$ is the steering angle of the ego vehicle in\nradians. $r_{lat}$ is the reward related to lateral acceleration, which is calculated by $r_{lat} = -|a| \\cdot v_{lon}$.\nThe last constant term is added to prevent the ego vehicle from standing still."}, {"title": "A.5 MEASURE PERFORMANCE METRICS", "content": "We use multiple key metrics to evaluate the performance of autonomous driving models in various\ndriving scenarios. Collision Rate (CR): the frequency at which the vehicle collides with obstacles\nor other vehicles. This metric is critical for assessing the safety of the driving model. Outlane Rate\n(OR): the rate at which the vehicle deviates from its designated lane. This metric evaluates the ability\nof modes to maintain proper lane discipline. Episode Completion Rate (ER): the percentage of driv-\ning tasks or episodes that the vehicle successfully completes. Higher completion rates indicate better\ntask performance. Average Safe Driving Distance (ASD): the average distance driven without inci-\ndents, such as collisions or off-road events. This metric highlights the capability to drive safely over\nextended periods. Average Return (AR): A metric that measures the cumulative reward collected by\nthe vehicle during its driving tasks, often reflecting both task performance and adherence to safety\nguidelines. Driving Score (DS): A comprehensive metric that reflects the overall performance of the\nvehicle in terms of safety, efficiency, and compliance with traffic rules.\n$CR = \\frac{N_{collisions}}{N_{total\\_episodes}}, OR = \\frac{N_{off\\_road\\_events}}{N_{total\\_episodes}},ER = \\frac{N_{completed\\_steps}}{N_{total\\_steps}}$ (8)"}, {"title": "A.6 THE NATURAL LANGUAGE TEMPLATE FOR TEXT INPUT", "content": "We obtained information from the CARLA environment using other sensors (such as speed sensors\nand position sensors), excluding the acceleration and steering (action) of the ego vehicle). This\ninformation is transformed into a natural language template that the VLA can understand, as shown\nbelow:\n<lateral_dis, delta_yaw, speed, vehicles_info> = <observation_vehicle_state>\n<vehicles_num> = <len(vehicles_info)>\n<multi_dis += str(vehicles_info[i][0])+\"\", multi_yaw += str(vehicles_info[i][1])+\"\", multi_speed\n+= str(vehicles_info[i][2])+\"\">\n<if vehicles_num=1:>\n<new_input=\u201cYou can see that there is a car. It is speed, straight-line distance from you, and angle\nin the direction your heading are respectively {multi_speed} m/s, {multi_dis} m, {multi_yaw}\u00b0.\u201d\n\u201cYou are now {lateral_dis}m laterally away from your driving route. \u201c>\n<elif vehicles_num>1:>\n<new_input=\u201cYou can see that there are vehicles_num cars. Their speed, straight-line distance\nfrom you, and angle in the direction your heading are respectively {multi_speed} m/s, {multi_dis}\nm, {multi_yaw}\u00b0.\u201d \u201cYou are now {lateral_dis}m laterally away from your driving route. \u201c>\n<elif vehicles_num=0:>\n<new_input=\u201cYou see no car here, and you are now {lateral_dis}m laterally away from your\ndriving route.\u201c>"}, {"title": "A.7 THE BENEFITS OF CROSS-ENTROPY LOSS AND LABEL SMOOTHING LOSS FOR VLA4CD", "content": "We found that merely replacing specific numerical values in the translation template (Chen et al.,\n2024) results in minimal representational differences caused by the sequential nature of data, making\nit easy for conventional cross-entropy loss to lead to overfitting in text generation tasks. As shown\nin Table 10, we tested on both town03 and town04, which led to a decline in the decision-making\nperformance of model. Compared to cross-entropy loss, cross-entropy loss with smoothed labels\nperformed better. Therefore, we chose cross-entropy loss with smoothed labels as the loss for text\ngeneration in VLA4CD in our experiments."}, {"title": "A.8 THE IMPACT OF TRAINING DATA-RELATED FACTORS ON THE DECISION PERFORMANCE\nOF MODEL", "content": "In the multimodal ablation experiments on the VLA4CD model, as shown in Table 11, we systemat-\nically removed or replaced individual modalities to evaluate their contribution to decision-making.\nThe results show that models utilizing image and text fusion significantly outperform those with\nonly a single image or text input in terms of decision accuracy and stability. This indicates that\nthe text modality in our dataset provides higher-level semantic abstraction to complement visual in-\nputs, thereby enhancing overall decision-making ability. In addition, as shown in Table 11, a single\ntext input performs better than a single image input, indicating that the information provided by the\ntext modality in our dataset (especially from \u201cother sensors input\u201d, as shown in Figure 2) is highly\nbeneficial for improving the decision-making ability of model."}, {"title": "A.9 THE NOISE CONSISTED OF INFORMATION DATASETS", "content": "The noise consisted of information completely unrelated to the current driving scenario as follow:\n{\u201cA playful puppy brings joy and laughter to our days\u201d, \u201cThe whisper of the wind carries secrets\nof the universe\u201d, \u201cA hidden garden blooms with the magic of nature's colors\u201d, \u201cThe aroma of fresh\ncoffee awakens the senses each morning\u201d, \u201cA handwritten letter feels like a warm hug from afar\u201d,\n\u201cThe glimmer of fireflies creates a magical summer night\u201d, \u201cA spontaneous adventure can lead to\nunforgettable memories\u201d, \u201cThe serenity of a quiet lake reflects the beauty of the world\u201d, \u201cA gentle\ntouch can convey love without a single word\u201d, \u201cThe laughter of friends is the sweetest melody of\nall\u201d, \u201cA warm hug is a universal language of comfort\u201d, \u201cThe dance of leaves in the breeze tells stories\nof change\u201d, \u201cA cozy fire invites stories and shared moments\u201d, \u201cThe beauty of art inspires creativity\nand self-expression\u201d, \u201cA day spent volunteering fills the heart with purpose\u201d, \u201cThe excitement of a\nnew book is like embarking on a journey\u201d, \u201cA delicious meal shared brings people closer together\u201d,\n\u201cThe sound of laughter can brighten even the gloomiest day\u201d, \u201cA fleeting moment can hold the\nweight of a thousand memories\u201d, \u201cThe charm of small towns lies in their simple beauty\u201d, \u201cA gentle\nrain nurtures the earth and inspires growth\u201d, \u201cA colorful painting captures the essence of joy\u201d,\n\u201cThe peace of a mountain retreat refreshes the soul\u201d, \u201cA favorite mug"}]}