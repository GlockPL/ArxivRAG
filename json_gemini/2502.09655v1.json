{"title": "Bidirectional Diffusion Bridge Models", "authors": ["Duc Kieu", "Kien Do", "Toan Nguyen", "Dang Nguyen", "Thin Nguyen"], "abstract": "Diffusion bridges have shown potential in paired image-to-image (I2I) transla-\ntion tasks. However, existing methods are limited by their unidirectional nature,\nrequiring separate models for forward and reverse translations. This not only\ndoubles the computational cost but also restricts their practicality. In this work,\nwe introduce the Bidirectional Diffusion Bridge Model (BDBM), a scalable ap-\nproach that facilitates bidirectional translation between two coupled distributions\nusing a single network. BDBM leverages the Chapman-Kolmogorov Equation\nfor bridges, enabling it to model data distribution shifts across timesteps in both\nforward and backward directions by exploiting the interchangeability of the initial\nand target timesteps within this framework. Notably, when the marginal distribu-\ntion given endpoints is Gaussian, BDBM's transition kernels in both directions\npossess analytical forms, allowing for efficient learning with a single network. We\ndemonstrate the connection between BDBM and existing bridge methods, such\nas Doob's h-transform and variational approaches, and highlight its advantages.\nExtensive experiments on high-resolution I2I translation tasks demonstrate that\nBDBM not only enables bidirectional translation with minimal additional cost but\nalso outperforms state-of-the-art bridge models. Our source code is available at\nhttps://github.com/kvmduc/BDBM.", "sections": [{"title": "1 Introduction", "content": "Diffusion models (DMs) [40, 43, 13] have emerged as a powerful class of generative models,\nsurpassing GANs [11] and VAEs [21] in generating high-quality data [7]. These models learn to\ntransform a Gaussian prior distribution into the data distribution through iterative denoising steps.\nHowever, the Gaussian prior assumption in diffusion models limits their application, particularly in\nimage-to-image (I2I) translation [16], where the distributions of the two domains are non-Gaussian.\nA straightforward solution is to incorporate an additional condition related to one domain into\ndiffusion models for guidance [6, 35]. This approach often overlooks the marginal distribution of\neach domain, which may hinder its generalization ability, especially when the two domains are\ndiverse and significantly different. In contrast, methods that construct an ODE flow [25, 29, 1]\nor a Schr\u00f6dinger bridge [4, 39, 18] between two domains focus mainly on matching the marginal\ndistributions at the boundaries, neglecting the relationships between samples from the two domains.\nConsequently, these methods are not well-suited for paired I2I tasks.\nTo solve the paired I2I problem, recent methods [31, 53] leverage knowledge of the target sample\ny in the pair (x, y) and utilized Doob's h-transform [9] to construct a bridge that converges to\ny. This involves learning either the h function [41] or the score function of the h-transformed\nSDE [53], both of which depend on y. Other methods [24] extend the unconditional variational\nframework for diffusion models to a conditional one given y for constructing such bridges, thereby\nlearning a backward transition distribution conditioned on y. Despite their success in capturing the\ncorrespondence between x and y, these methods share a common limitation: they can only generate"}, {"title": "2 Preliminaries", "content": ""}, {"title": "2.1 Markov Processes and Diffusion Processes", "content": "A Markov process is a stochastic process satisfying the Markov property, i.e., the future (state) is\nindependent of the past given the present:\n\\(p(x_s|x_t, x_u) = p(x_s|x_t)\\)\nwhere \\(x_u\\), \\(x_t\\), \\(x_t\\) denote random states at times u, t, s satisfying that 0 < u < t < s < T. Here, \\(p(x_s|x_t)\\)\nis the transition distribution of the Markov process.\nDiffusion processes are special cases of Markov processes where the transition distribution is typically\na Gaussian distribution. A diffusion process can be either discrete-time [13] or continuous-time\n[44]. A continuous-time diffusion process can be described by the following (forward) stochastic\ndifferential equation (SDE):\n\\(dX_t = \\mu (t, X_t) dt + \\sigma (t, X_t) dW_t\\)   (1)\nwhere \\(W_t\\) denotes the Wiener process (aka Brownian motion) at time t. Eq. 1 can be solved via\nsimulation provided that the distribution of \\(X_0\\) is known. One can derive the forward and backward\nKolmogorov equations (KFE and KBE) for this SDE as follows:\nKBE:\n\\(\\frac{\\partial p (t, x)}{\\partial t} = G^*p (t, x); p (0,\\cdot) \\text{ is given}\\)   (2)\nKFE:\n\\(\\frac{\\partial p (T, y|t, x)}{\\partial t} = -Gp (T, y|t, x); p (T,\\cdot) \\text{ is given}\\)   (3)\nwhere G denotes the generator corresponding to the SDE in Eq. 1 and \\(G^*\\) is the adjoint of G. When\n\\(\\sigma (t, x)\\) is a scalar depending only on t (i.e., \\(\\sigma (t, x) = \\sigma (t)\\), for a real-valued function f, Gf (t, x)\nand \\(G^* f (t, x)\\) are given by:\n\\(Gf (t, x) = \\nabla f (t, x)' \\mu (t, x) + \\frac{\\sigma(t)^2}{2} \\Delta f (t, x)\\)\n\\(G^* f (t, x) = -\\nabla \\cdot (f (t, x) \\mu (t, x)) + \\frac{\\sigma(t)^2}{2} \\Delta f (t, x)\\)"}, {"title": "2.2 Chapman-Kolmogorov Equations", "content": "A Markov process can be described via the Chapman-Kolmogorov equation (CKE) [17] as follows:\n\\(p(x_s|x_t) = \\int p(x_s|x_r) p(x_r|x_t) dx_r\\)   (4)\nwhich holds for all times t, r, s satisfying that 0 < t < r < s < T. The CKE in Eq. 4 can be\nconsidered as the integral form of the KFE and KBE in Eqs. 2, 3. Compared to the Kolmogorov\nequations, the CKE is easier to work with since (i) it does not involve the partial derivatives of the\ntransition kernel, (ii) it is applicable to both continuous- and discrete-time Markov processes, and (iii)\nit encapsulates both forward and backward transitions. Regarding the last point, we can apply Eq. 4\neither in the forward manner (from 0 to T) to evaluate the distribution of the next state \\(x_s\\) given the\ndistribution of the current state \\(x_t\\):\n\\(p(x_s|x_0) = \\int p(x_s|x_t) p(x_t|x_0) dx_t; p(x_t|x_0) \\text{ is given}\\)   (5)\nor in the backward manner (from T to 0) to evaluate the distribution of the previous state \\(x_t\\) given the\ndistribution of current state \\(x_s\\):\n\\(p(x_T|x_t) = \\int p(x_T|x_s) p(x_s|x_t) dx_s; p(x_T|x_s) \\text{ is given}\\)   (6)\nIn the discrete-time setting, Eq. 5 can be interpreted as given a Markov process with \\(p(x_{t+1}|x_t)\\)\nspecified for every time t. If we have known the marginal distribution \\(p(x_t|x_0)\\) at time t, then\nby solving the CKE forwardly, we can compute \\(p(x_{t+1}|x_0)\\) at time t + 1. Similarly, in Eq. 6, if\nwe have known \\(p(x_T|x_{t+1})\\) at time t + 1, then by solving the CKE backwardly, we can compute\n\\(p(x_T|x_t)\\) at time t. For example, in DDPM [13], given \\(p(x_t|x_0) = N(x_t|\\sqrt{\\bar{a}_t}x_0, (1 - \\bar{a}_t) I)\\)\nand \\(p(x_{t+1}|x_t) = N(x_{t+1}|\\sqrt{1 - \\beta_{t+1}}x_t, \\beta_{t+1}I)\\), we can use Eq. 5 to compute \\(p(x_{t+1}|x_0)\\) as\n\\(N(x_{t+1}|\\sqrt{a_{t+1}}x_0, (1 - \\bar{a}_{t+1}) I)\\).\nInterestingly, the backward CKE in Eq. 6 can be written in another way according to Bayes' rule:\n\\(p(x_t|x_T) = \\int \\frac{p(x_T|x_s) p(x_s|x_t)}{p(x_T|x_t)} dx_s; p(x_s|x_T) \\text{ is given}\\)   (7)\nThe mathematical derivation is detailed in Appdx. A.1. Eq. 7 is akin to the forward CKE in Eq. 5 but\nin reverse time."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Chapman-Kolmogorov Equations for Bridges", "content": "In many real-world problems (e.g., paired/unpaired image translation), the joint boundary distribution\n\\(p(y_A, y_B)\\) of samples from two domains A, B is given in advance rather than just either \\(p(y_A)\\) or\n\\(p(y_B)\\), and we need to design a stochastic process such that if we start from \\(y_A\\) (\\(y_B\\)), we should\nreach \\(y_B\\) (\\(y_A\\)) with a predefined probability \\(p(y_B|y_A)\\) (\\(p(y_A|y_B)\\)). Such stochastic processes are\nreferred to as stochastic bridges or simply bridges [30, 31, 24, 53]. In this section, we will develop\nmathematical models for stochastic bridges based on the CKEs for Markov processes in Section 2.\nWithout loss of generality, we associate two domains A, B with samples at time 0, T, respectively.\nLet \\(\\{X_t\\}_{t=0}^T\\) be a stochastic process in which the initial distribution \\(p(x_0|\\hat{y}_A)\\) is a Dirac distribution at\n\\(y_A\\) (i.e., \\(p(x_0 = y_A|\\hat{y}_A) = \\delta_{y_A}\\)). To conform to the notation used in prior works, we denotes \\(x_0 := y_A\\).\nThe symbol \\(\\hat{}\\) indicates that \\(x_0\\) is a specified value rather than a random state like \\(x_0\\)."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Experimental Settings", "content": ""}, {"title": "4.1.1 Datasets and evaluation metrics", "content": "We validate our method on 4 paired image-to-image (I2I) translation datasets namely Edges Shoes,\nEdges Handbags, DIODE Outdoor [49], and Night Day [16]. Following [53], we rescale images\nto 64\u00d764 resolution for the first two datasets and 256\u00d7256 for the latter two. We construct bridges\nin the pixel space for the first three datasets and in the latent space of dimensions 32\u00d732\u00d74 for the\nNight Day dataset. To map images to latent representations, we use a pretrained VQ-GAN encoder\n[34]. Following prior work [24], we use FID [12], IS [36], and LPIPS [51] to measure the fidelity\nand perceptual faithfulness of generated images. These metrics are computed on training samples, as\nin [53]."}, {"title": "4.1.2 Model and training configurations", "content": "Unless stated otherwise, we use Brownian bridges, as described in Appdx. A.6.4, with \\(a_t = 1 - \\frac{t}{T}\\),\n\\(\\beta_t = \\frac{t}{T}\\) and \\(\\sigma_t^2 = k a_t \\beta_t (1 - \\frac{t}{T})\\) for our experiments. We consider discrete-time models with"}, {"title": "4.2 Experimental Results", "content": ""}, {"title": "4.2.1 Unidirectional I2I translation", "content": "Following [53], we experiment with the Edges Shoes, Edges Handbags, and DIODE Outdoor\ndatasets, focusing on translating sketches or normal maps to color images, as this translation is more\nchallenging than the reverse. Results for the reverse translation are provided in Appdx. B.2.\nAs shown in Table 1 and Fig. 3, BDBM significantly outperforms BDBM-1 and other unidirectional\nbaselines in most metrics and datasets. This improvement is also evident in the superior quality of\nsamples generated by our method compared to the baselines, as displayed in Fig. 2. Notably, BDBM\nwas trained using the same number of iterations as the baselines. This means that the actual number\nof model updates w.r.t. a specific direction in BDBM is only half that of the baselines, as the two\nendpoints \\(x_0\\), \\(x_T\\) are sampled with equal probability in the loss \\(L_{BDBM}\\) (Eq. 23). This demonstrates\nthe clear advantage of our proposed bidirectional training over the unidirectional counterpart.\nWe hypothesize that allowing either \\(x_0\\) or \\(x_T\\) to serve as the condition for the shared-parameter noise\nmodel \\(z_\\theta\\) during training enables the optimizer to leverage the endpoint that yields more accurate\npredictions for effective parameter updates. Intuitively, this endpoint is likely the one closer in time\nto the input \\(x_t\\) of the noise model. For instance, consider two noise predictions \\(z_\\theta(t, x_t, x_0)\\) and\n\\(z_\\theta(t, x_t, x_T)\\) for \\(x_t\\) at time t closer to 0 than to T, where \\(x_0\\) and \\(x_T\\) are chosen with equal probability.\nSince \\(x_0\\) generally provides more reliable information about the noise in \\(x_t\\) compared to \\(x_T\\), the"}, {"title": "4.2.2 Bidirectional I2I translation", "content": "We compare BDBM with bidirectional baselines DDIB and RF, presenting quantitative and qualitative\nresults in Table 2 and Fig. 4. BDBM outperforms the two baselines by large margins for translations\nin both directions. DDIB struggles to maintain pair consistency between boundary samples due to\nrandom mapping into shared Gaussian latent samples, resulting in translations that often differ greatly\nfrom the ground truth. Meanwhile, RF performs reasonably well for the color-to-sketch translation\nbut poorly for the reverse. This is because different color images can have very similar sketch images.\nThis causes the learned velocity for the sketch-to-color translation to point toward the average of\nmultiple target color images associated with a source sketch image, as evident in Fig. 4."}, {"title": "4.3 Ablation Study", "content": ""}, {"title": "4.3.1 Impacts of different parameterizations", "content": "As discussed in Section 3.3, the transition kernel of BDBM can be modeled by predicting the noise\nz or endpoints (either by predicting \\(x_0 + x_T\\) and inferring the missing endpoint given the known\none, or by directly predicting one endpoint given the other). We compare the effectiveness of these\napproaches on the Edges\u2192Shoes and Edges\u2192Handbags translation tasks, with results shown in\nTable 3. In addition to FID and LPIPS metrics, we evaluate Diversity [2, 24], which measures the\naverage pixel-wise standard deviation of multiple color images generated from a single sketch on a\nheld-out test set of 200 samples. We observe that predicting noise achieves slightly better FID scores\nand produces more diverse samples than predicting endpoints. We hypothesize that since \\(x_0\\), \\(x_T\\) are"}, {"title": "4.3.2 Effect of the noise variance \\(\\sigma_t^2\\)", "content": "In Section 4.1.2, the noise variance \\(\\sigma_t^2\\) of BDBM is defined as \\(\\sigma_t^2 = k a_t \\beta_t (1 - \\frac{t}{T})\\), which means\nwe can control \\(\\sigma_t^2\\) by changing the value of k. Table 4 shows the results on Edges\u2192Shoes for\ndifferent values of k \u2208 {1,2,4,8}. Increasing k generally yields more diverse samples but worsens\nFID and LPIPS scores. This trade-off occurs because higher k values increase the variance of the\ndistribution \\(q(x_t|x_0, x_T)\\), enlarging the path space and consequently making the model optimization\nmore challenging. Conversely, when k is too small, the noise variance becomes insufficient to corrupt\ndomain information for effective translation. Our results indicate that k = 2 offers the best balance\nbetween diversity and quality."}, {"title": "4.3.3 Effect of the variance \\(\\delta_{s,t}^2\\) of the transition kernel", "content": "We study the impact of varying the variance \\(\\delta_{s,t}^2\\) of the transition kernel via changing \u03b7 (Section 4.1.2)\non generation quality, with the results presented in Table 5 and Fig. 5. We observe that increasing"}, {"title": "4.3.4 Translation in latent spaces", "content": "To validate BDBM's translation capability in latent spaces, we adopt the Day Night translation\nexperiment from [53]. For a fair comparison, we maintain the same experimental settings as in\n[53], including the model architecture, training iterations, and NFE=53 for sample generation. We\nalso follow [53] and compute metrics using the reconstructed versions of the ground-truth target\nimages. This helps mitigate the impact of the VQ-GAN decoding process and ensures that the results\naccurately reflect the translation quality. Table 6 presents the results of BDBM and baseline methods,\nwith the baseline results taken from [53]. It is evident that BDBM significantly outperforms the\nbaselines, demonstrating its consistent performance in both pixel and latent spaces. We also observed\nthat BDBM effectively captures the statistics of the two domains, where in the dataset, nighttime\nimages are much less diverse than daytime ones, leading to the generation of duplicated nighttime\nimages when using different random seeds, as illustrated in Fig. 8."}, {"title": "5 Related Work", "content": ""}, {"title": "5.1 Schr\u00f6dinger Bridges and Diffusion Bridges", "content": "Recent bridge models can broadly be classified into Schr\u00f6dinger bridges (SB) and diffusion bridges\n(DB). The Schr\u00f6dinger Bridge problem [38, 32] aims to find a stochastic process that connects two\narbitrary marginal distributions \\(p_A\\), \\(p_B\\) while remaining as close as possible to a reference process.\nWhen the reference process is a diffusion process initialized at \\(p_A\\), the solution to the SB problem\ncan be characterized by two coupled partial differential equations (PDEs) governing the forward and\nbackward diffusion processes initialized at \\(p_A\\) and \\(p_B\\), respectively [23, 48, 4, 5, 26].\nSB models are typically trained using iterative proportional fitting which requires expensive simulation\nof the forward and backward processes [10, 4]. Several approaches have been proposed [33, 39, 47] to\nimprove the scalability of training SB models by leveraging the score and flow matching frameworks\n[15, 44, 25, 29]. However, SB models overlook the relationships between samples from the two\ndomains, making them unsuitable for paired translation tasks.\nDiffusion bridges simplify Schr\u00f6dinger bridges by assuming a Dirac distribution at one endpoint,\nallowing them to model the coupling between the two domains for paired translations. I2SB [27] is a\ndiffusion bridge derived from the general theory of SBs. On the other hand, methods like SBALIGN\n[41], \\(\\Omega\\)-bridge [30, 31], and DDBM [53] leverage Doob's h-transform to obtain the formula of a\ncontinuous-time h-transformed process that converges almost surely to a specific target sample while\naligning closely with the reference diffusion process. SBALIGN and \\(\\Omega\\)-bridge create a h-transform\nprocess that generates data and learn the drift of this process, whereas DDBM designs a h-transformed\nprocess that converges to a latent sample. For data generation, DDBM learns the score with respect\nto the reverse process via conditional score matching, following the approach in [44]. BBDM [24]"}, {"title": "5.2 Diffusion and Flow Models for I2I", "content": "Diffusion models (DMs) [40, 43, 13, 44] are powerful generative models that progressively denoise\nlatent samples from a standard Gaussian distribution to generate images. For image-to-image (I2I)\ntranslation, DMs can incorporate source images as conditions through either classifier-based [7] or\nclassifier-free [14] guidance techniques during the denoising process to generate corresponding target\nimages [37, 35, 52, 50]. However, since one of the two boundary distributions in DMs is always a\nstandard Gaussian, bidirectional translation requires training two distinct DMs conditioned on source\nand target images. DDIB [45] exemplifies this approach by combining two separate diffusion models\nfor source and target domains through a shared Gaussian latent space for bidirectional translation.\nFlow models (FMs) [29, 25, 1, 8] build an ODE map between two arbitrary boundary distributions\nand can be trained via the flow matching loss [25] related to the score matching loss for diffusion\nmodels [44]. FMs can be viewed as special cases of diffusion bridges where the variance of the\ntransition kernel is zero. Due to their deterministic nature, FMs are less suitable for capturing the\ncoupling between two domains, as demonstrated by our experimental results in Sections 4.2.2 and\n4.3.2. Nonetheless, FMs can be useful for unpaired translation and can be specially designed to\nrepresent optimal transport maps [28, 25, 46]."}, {"title": "6 Conclusion", "content": "We introduced the Bidirectional Diffusion Bridge Model (BDBM), a novel framework for bidirectional\nimage-to-image (I2I) translation using a single network. By leveraging the Chapman-Kolmogorov\nEquation, BDBM models the shared components of forward and backward transitions, enabling effi-\ncient bidirectional generation with minimal computational overhead. Empirical results demonstrated\nthat BDBM consistently outperforms existing I2I translation methods across diverse datasets.\nDespite these strengths, BDBM has so far been applied exclusively to the image domain. Extending\nit to other domains, such as text, presents an exciting direction for future research. In particular,\nexploring BDBM for multimodal tasks like image text generation would be a promising avenue."}]}