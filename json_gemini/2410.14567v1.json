{"title": "RAG-ConfusionQA: A Benchmark for Evaluating LLMs on Confusing Questions", "authors": ["Zhiyuan Peng", "Jinming Nian", "Alexandre Evfimievski", "Yi Fang"], "abstract": "Conversational AI agents use Retrieval Augmented Generation (RAG) to provide verifiable document-grounded responses to user inquiries. However, many natural questions do not have good answers: about 25% contain false assumptions (Yu et al., 2023), and over 50% are ambiguous (Min et al., 2020). RAG agents need high-quality data to improve their responses to confusing questions. This paper presents a novel synthetic data generation method to efficiently create a diverse set of context-grounded confusing questions from a given document corpus. We conduct an empirical comparative evaluation of several large language models as RAG agents to measure the accuracy of confusion detection and appropriate response generation. We contribute a benchmark dataset to the public domain.", "sections": [{"title": "Introduction", "content": "Retrieval Augmented Generation (RAG) has become a standard approach to building context-grounded conversational AI agents (Lewis et al., 2020; Guu et al., 2020; Gao et al., 2024). The RAG technology enables a large language model (LLM) to generate responses to users' inquiries with information from a curated knowledge base stored as a collection of text documents. For each inquiry, a typical RAG pipeline retrieves a relevant document and uses it to generate an LLM response. Since an LLM is not a reliable source of knowledge due to its propensity to hallucinate, in business critical settings its role must be limited to language manipulation and common-sense reasoning. Every domain-specific claim in the response must be verifiable and traceable to its source document.\nThe performance of a RAG system is determined by evaluating its responses along a number of dimensions, including accuracy, faithfulness, completeness, answer relevance, and context relevance, as well as style metrics (Es et al., 2024; Saad-Falcon et al., 2024; Liu et al., 2024; Zheng et al., 2023). Besides, the assumption that a user's question is answerable may not hold. A well-designed RAG system should implement an exception-handling pathway that detects confusing or unanswerable questions and responds by pointing out the confusion or seeking help. Confusing questions in RAG are the subject of this work.\nThere is substantial prior work on using LLM to identify and respond to questions with a false premise, as well as using such questions as benchmarks for LLM evaluation (Zhu et al., 2024; Yuan et al., 2024; Hu et al., 2023). Most of that work focuses on \"common-sense\u201d false premises that contradict basic knowledge learned by LLMs at pretraining. They do not consider premises that contradict a retrieved document, or appear to do so due to the lack of clarifying context such as timing. Our goal is to help LLMs respond to a variety of types of confusing questions, especially those confusing in the context of the document.\nBuilding a \"deconfusion\" system involves addressing 3 challenges: creating a labeled dataset with many diverse examples of confusing questions; building a classifier to detect confusion; and, developing a proper response generator for each supported confusion type. As human content creation is expensive, we focus on LLM-based synthetic generation of confusing questions for a given set of documents. Our contributions are as follows:\n\u2022 We design, implement, and evaluate a novel synthetic data generation method to efficiently generate diverse high-quality confusing questions for a set of provided documents.\n\u2022 We discuss the types of confusing questions in RAG setting and the common situations where confusion arises.\n\u2022 We perform an empirical comparative evaluation of several LLMs as RAG agents to measure the accuracy of confusion detection and appropriate response generation.\n\u2022 We will release our code and the RAG-ConfusionQA dataset upon acceptance of the paper\u00b9."}, {"title": "Related Work", "content": "Benchmark Datasets Most of the recent work on LLM response to confusing questions focuses on stand-alone questions to be answered based on the LLM's pretraining knowledge, without a context document. The questions may be naturally collected from online sources (Li et al., 2024; Yu et al., 2023), written by human annotators (Hu et al., 2023), or synthetically generated (Zhu et al., 2024; Yuan et al., 2024). Released benchmark datasets with confusing questions include KG-FPQ (Zhu et al., 2024), FalseQA (Hu et al., 2023), FLUB (Li et al., 2024), and CREPE (Yu et al., 2023). Ambiguous open-domain questions were studied in an earlier work (Min et al., 2020) that released AmbigNQ, a dataset of natural ambiguous questions.\nQuestion Generation (Zhu et al., 2024; Yuan et al., 2024) describe a method to synthetically generate stand-alone false premise questions by selecting a set of factual triples of subject + relation + object using Wikipedia data, replacing the object with a similar, but wrong, object, then filling in a template to generate a false premise question. Content-grounded (non-confusing) synthetic data generation is studied in (Yehudai et al., 2024; Wang et al., 2023b; Lee et al., 2023). Given the grounding passages, they generate data entries by few-shot prompting with task-specific examples, then filter to ensure data quality, faithfulness, and diversity.\nFalse Premise Detection by LLMs is commonly achieved with fine-tuning, chain-of-thought (CoT), and in-context learning (Zhu et al., 2024; Li et al., 2024; Hu et al., 2023). Experiments show that fine-tuning outperforms the other two methods (Hu et al., 2023). CoT does not bring stable improvements, while in-context learning can bring performance improvement if more examples are shown to the LLM (Li et al., 2024). In (Yu et al., 2023) a retrieval model finds relevant Wikipedia passages to send to a ROBERTa-based multi-passage classifier. (Yuan et al., 2024) deep-dives into the LLM and finds ~1% of attention heads primarily responsible for getting confused by false premise questions. Constraining these attention heads during inference yields a performance increase of nearly 20%. They also observed a strong correlation between hallucination and response uncertainty metrics.\nContext Based Verification Natural questions unanswerable due to unverifiable premises were studied in (Kim et al., 2021). This work is closer to RAG because it associates a document with each question: a Wikipedia page retrieved by the Google search engine. A premise is unverifiable (rather than false) if it not entailed by the document. The proposed system extracts all premises from a question, verifies them by checking entailment from each sentence of the document, then generates a response stating the unverifiable premise if found."}, {"title": "Methods", "content": "We built benchmark RAG-ConfusionQA to evaluate LLMs on the context-grounded response or answer generation step of the RAG pipeline when the questions are confusing (Section 3.1). In this step, given a standard prompt p (Appendix F.6), a confusing question q, and a retrieved document d, the LLM, generates a response r. Ideally, r should clarify the confusing part of the question, a process we refer to as defusion (or de-confusion), rather than attempting to answer the question directly. Document retrieval and ranking are out of scope for this work."}, {"title": "Confusing Questions", "content": "We call question q confusing given document d if q cannot be answered faithfully and completely based on d alone, without requesting - or providing \u2013 some clarification. Common confusion types in RAG-ConfusionQA (Appendix D) include:\nFalse premise: q makes a wrong or a highly biased assumption that must be clarified. The contradiction may be with d or with basic knowledge.\nNot Mention: Some in-domain information required to answer q is not provided in d.\nAmbiguous: q is missing some context or contains generic words with multiple interpretations, allowing different answers in the context of d.\nNote that q is interpreted within the context of d. If the name \"John Smith\" appears in both, it is assumed to refer to the same person unless stated otherwise. Similarly, q and d are assumed to refer to the same time, place, or use-case unless clarified, otherwise leading to confusion."}, {"title": "Data Generation", "content": "Here we describe our method of generating confusing questions for a given set of documents. We require each document to be short enough to fit in the LLM prompt along with instructions and examples, but long enough to make at least 5 to 10 separate claims (i.e. a few paragraphs long). We prefer documents that are novel to the LLMs under evaluation, e.g. news articles published later than all pretraining cutoff dates, so that the LLMs cannot reproduce claims or answer questions correctly by pure hallucination. Thus, we collected 200 news articles published in 2024, each containing more than 150 words for each topic (Appendix E). Sentences were selected sequentially from the top until the total word count exceeded 300, using the Newscatcher(Bugara et al., 2020). Given a set of such documents, we generate multiple confusing questions about each document, along with some non-confusing ones for comparison.\nWe start by picking LLMq (GPT-40-mini by default in this paper), the \"teacher\u201d LLM that will generate the confusing questions. We prefer, but do not require, LLM, to be larger and stronger than the LLM, under evaluation. For each document d, we call LLMq to generate a list of factual claims, then iteratively replace subsets of claims with hallucinated claims while maintaining semantic similarity to d using guided hallucination, see Algorithm 1. We also generate some non-confusing questions for each document d by calling LLM, either directly."}, {"title": "Evaluation and Metrics", "content": "We first assessed the quality of RAG-ConfusionQA by sampling a subset of questions and manually verifying whether they were indeed confusing or not (Section 3.4). Next, we evaluated how well various LLM models could defuse the confusing questions in RAG-ConfusionQA. Given the large number of confusing questions, manual verification of LLM responses is not feasible. Therefore, we proposed AutoDefuseEval using GPT-40-mini to automatically detect whether a response successfully defuses the question. To validate the effectiveness of this defusion detection method, we compared its performance with human annotations on a sampled dataset (Section 3.4). Formally, we define our tasks as follows:\nQuestion generation: What is the quality of confusing and non-confusing questions generated by Algorithm 1? (Section 3.4)\nDefusion detection: How accurately can AutoDefuseEval detect if an LLM, response to a confusing question defuses the confusion? (Section 3.4)\nConfusion response: How frequently does an LLM, when responding to a confusing question using the standard prompt, generate a response that successfully defuses the confusion? (Section 4.1)\nConfusion detection: How accurately can LLMs identify which context-grounded questions are confusing and require special handling? (Section 4.2)\nWe evaluate \"question generation\u201d and \u201cdefuse detection\" through accuracy and confusion matrix. We only apply accuracy for \u201cconfusion response\" and \"confusion detection\u201d. Algorithm A has the steps we run for each LLM, and for each (q, d) pair. We run steps 1, 2, and 3 for \u201cconfusion detection\" and 1, 4, 5, and 6 for \u201cconfusion response\u201d.\nFollowing the self-consistency method (Wang et al., 2023a), we perform multiple LLM calls in steps 2 and 5 of Algorithm A, taking the majority vote to determine the final label. We also present the LLMq with few-shot exemplars of questions, responses, and explanations (Section 4.3)."}, {"title": "Human Annotation", "content": "To evaluate \"question generation\u201d and \u201cdefuse detection\" (Section 3.3), we recruited six computer science graduate students for manual labeling (Appendix B). As reflected in \"Ground Truth Acc,\u201d of Appendix B, Table 3, our method achieved 94.91% accuracy in generating confusing and non-confusing questions, while our proposed AutoDe-"}, {"title": "Experimental Results", "content": "4.1 Confusion Response\nWe selected several open-source LLMs (Appendix 4) to evaluate their ability to generate responses that effectively defuse confusing questions with a standard prompt (Appendix F.6). As shown in Table 1, a) GPT-3.5 performs the worst among all six LLMs but with the smallest Std Dev across all topics, indicating consistent performance. On the other hand, Llama 3.1 70B and Llama 3.1 8B achieve comparable top performances with similar Std Dev, despite their differences in model size; b) most LLMs perform the worst in the \"science\" topic, except for GPT-3.5, for which \"science\" is the second worst. This is expected, as science topics often require more specialized domain knowledge to defuse confusion effectively; c) all LLMs achieve their best performance on the \u201csport\u201d topic, demonstrating a clear trend. The strong performance across LLMs on the \u201csport\" topic might be attributed to the relative simplicity and unambiguous nature of sports-related content.\n4.2 Confusion Detection\nBy default, we use GPT-40-mini as our LLM, for the task of \"confusion detection,\" which identifies whether the generated questions are confusing. In this section, we compare GPT-40-mini with other LLMs. As illustrated in Figure 1, GPT-40-mini achieves the best performance, while the smallest model, Llama 3.2 3B, performs the worst.\n4.3 Ablation Study\nWe examined the impact of various prompts on the task of \"defusion detection.\" As seen in Table 2, incorporating examples increased accuracy from 87.61% to 97.35%, and applying self-consistency with n = 3 further boosted performance to 98.23%. However, increasing the number of generated sequences beyond three did not lead to additional improvements. Since RAG-ConfusionQA-Golden is a smaller dataset, using n = 9 did not yield gains, but this doesn't rule out its potential benefit on the larger RAG-ConfusionQA-Silver dataset. Therefore, we adopted n = 9 in our experiments to balance accuracy and expenses (Appendix F.10)."}, {"title": "Conclusion", "content": "We introduced RAG-ConfusionQA, a benchmark designed to assess how effectively an LLM defuses confusion when responding to confusing questions using a standard prompt. Through human annotation, we demonstrated that RAG-ConfusionQA achieves a 94.91% accuracy in generating confusing and non-confusing questions, while our proposed AutoDefusionEval achieves a 98.23% accuracy in detecting whether an LLM's response defuses confusion. Additionally, our evaluation highlights that current LLMs often attempt to answer confusing questions to varying degrees. For future work, we aim to develop methods to better detect confusing questions and address the tendency of LLMs to respond to them directly instead of defusing the confusion."}, {"title": "Confusing Types", "content": "Document:\nJustin Jefferson, CeeDee Lamb, NFL Injury Statuses and Fantasy Impact for Week 3 Stephen Ma-turen/Getty Images\nTwo of the best wide receivers in the NFL are expected to be on the field for their respective Week 3 games. Justin Jefferson and CeeDee Lamb each received positive prognosis about their injury issues, which is something that can't be said for the rest of the stars across the NFL. Christian McCaffrey is already on injured reserve, Deebo Samuel is out for a weeks and a slew of other running backs and wide receivers are dealing with ailments that could keep them out of Week 3. Below is a look at all of the significant injuries that could affect fantasy football matchups across Week 3. Justin Jefferson Off Injury Report\nJustin Jefferson was taken off the Minnesota Vikings injury report on Friday. Jefferson's status was up in the air because of a quad injury, but he practiced well enough this week that the injury is not a concern. The superstar wide out will be needed for Minnesota's home clash with the Houston Texans, which has the potential to be a high-scoring affair. Jefferson is always the primary target in Minnesota when healthy, but he should have more targets in Week 3 because Jordan Addison and T.J. Hockenson are still out. Jefferson earned seven targets from Sam Darnold in Week 2. Only running back Aaron Jones had more than four targets against the New York Giants. Houston's defense may give Jefferson some fits, led by cornerback Derek Stingley Jr. but it has allowed 330 receiving yards to opposing wide outs on just 20 catches through two weeks. Jefferson is an automatic start whenever he's healthy, and his star power may be needed more on certain fantasy rosters in Week 3 depending on how many injuries affect a single roster.\nQuestion 1: How does Justin Jefferson's exceptional catching ability influence the Vikings' decision to select him in the first round of the 2020 NFL Draft despite his lack of speed?\nThis question is confusing as it asks about \"Vikings' decision to select Jefferson in the first round of the 2020 NFL Draft\" which was never discussed in the document. So this belongs to Not Mention\nQuestion 2: How will Justin Jefferson's struggles with hamstring issues in Week 3 affect the Vikings' chances of winning against the Houston Texans?\nThis question assumes that Justin Jefferson is dealing with hamstring issues, which is not mentioned in the document. The document only refers to a quad injury that Jefferson had, which is no longer a concern as of Week 3. The document makes it clear that Jefferson is healthy and will play without any restrictions. Therefore, the question is based on a False Premise\nQuestion 3: How will his performance impact the team's chances this season, considering they are missing key players?\nThis question uses the Ambiguous pronoun \u201chis\u201d without specifying who it refers to, making it unclear whether the question is asking about Justin Jefferson, CeeDee Lamb, or another player mentioned in the"}, {"title": "Statistics of RAG-ConfusionQA", "content": "Table 5: Statistics of RAG-ConfusionQA. Golden is sampled from Silver and annotated by annotators (Appendix B)."}, {"title": "Extract Claims Prompt", "content": "You will be provided with a document delimited by triple quotes.\nRead the document and follow user's instructions.\nRead the document and list {num_fact} most important facts it contains. Each fact should be stated in a clear, standalone sentence with sufficient context to be understood independently, avoiding undefined pronouns. Ensure that each fact is directly derived from the document and does not include any information not mentioned within it.\nDocument:\n\"\"\"{document}\"\"\"\n{num_fact} most important facts:"}, {"title": "Recover Missing Claims Prompt", "content": "Read the document below with a list of {num_fact} facts it contains. Note that some of the facts are missing represented by (missing). Your task is to guess the missing facts could have said and complete the missing facts. Each fact should be stated in a clear, standalone sentence with sufficient context to be understood independently, avoiding undefined pronouns. Please complete the missing facts and return all the {num_fact} facts in original order. You must return all the {num_fact} facts.\nDocument:\n\"\"\"{document}\"\"\"\nCompleted list of facts:"}, {"title": "Confusion Judgement Prompt", "content": "You will be provided with a document delimited by triple quotes.\nRead the document and follow user's instructions.\nRead the document and the question. Check if the question contains any confusing parts, false assumptions, incorrect information, or mentions of people, locations, organizations, or other entities that are not present in the document.\nDocument:\n\"\"\"{document}\"\"\"\nQuestion:\n{question}\nDoes the question contain any confusing parts, false assumptions, incorrect information, or mentions of people, locations, organizations, or other entities that are not present in the document? Please start by explaining your reasoning. End your response with 'The answer is: Yes or No.':"}, {"title": "Defusion Judgement Prompt", "content": "You will be provided with a document delimited by triple quotes.\nRead the document and follow user's instructions.\nRead the document and answer the question based on the document.\nDocument:\n\"\"\"{example_document}\"\"\"\nQuestion:\n{example_question}\n{defuse_answer_examples}\n{Defusion Instruction}\n{defuse_reasoning_examples}\nRead the document and answer the question based on the document.\nDocument:\n\"\"\"{document}\"\"\"\nQuestion:\n{question}\n{response_from_gpt3.5}\n{Defusion Instruction}"}, {"title": "Defusion Instruction", "content": "The question presented is confusing and unanswerable based on the document provided. Read the response carefully. If the response attempts to answer the question, like answering 'Yes' to agree the question or 'No' to disagree the question, you should immediately conclude with 'The answer is: No.' even if it later acknowledges the question's confusion. Otherwise, determine if the response clarifies that the question cannot be answered or confusing or matches any of the issues listed:\nFalse Premise: The response points out that the question is based on an incorrect assumption.\nNot Mentioned: The response indicates that the information needed to answer the question is not available or not mentioned in the document.\nAmbiguous: The response points out unclear pronouns or vague terms in the question that prevent a clear answer.\nBegin with your reasoning and finally conclude with 'The answer is: Yes.' or 'The answer is: No.'. Note that if the response attempts to answer the question at any point, like answering 'Yes' to agree the question or 'No' to disagree the question, you should immediately conclude with 'The answer is: No.' even if it later acknowledges the question's confusion:"}, {"title": "RAG Prompt", "content": "You will be provided with a document delimited by triple quotes.\nRead the document and follow user's instructions.\nRead the document and answer the question based on the document.\nDocument:\n\"\"\"{document}\"\"\"\nQuestion:\n{question}"}, {"title": "Non-confuing Question Generation Prompt", "content": "You will be provided with a document delimited by triple quotes.\nRead the document and follow user's instructions.\nRead the document attentively and compile a numbered list of the top {num_q} questions that the document directly answers. Ensure each question is clear, accurate, and devoid of confusion, false assumptions, undefined pronouns, or misinformation. Avoid referencing people, locations, organizations, or other entities not explicitly mentioned in the document. Construct each question to be thought-provoking, containing between 13 to 18 words, and sufficiently detailed to avoid being overly straightforward.\nDocument:\n\"\"\"{document}\"\"\"\nQuestions:"}, {"title": "Confusing Question Generatioin Prompt", "content": "You will be provided with a document delimited by triple quotes.\nRead the document and follow user's instructions.\nRead the document and review the list of hallucinated facts. For each hallucinated fact, craft a single, specific and concise question containing 13 to 18 words that incorporates the key element of the fact, ensuring the question is intentionally confusing.\nThe question should not be answerable using any information present in the document. The question should not combine multiple queries and each question should address only one specific aspect. If a question cannot be formulated for a particular hallucinated fact, you may omit it.\nDocument:\n\"\"\"{document}\"\"\"\nhallucinated facts:\n{hallucinated_facts}\nQuestions:"}, {"title": "Remove Claims Prompt", "content": "You will be provided with a document delimited by triple quotes.\nRead the document and follow user's instructions.\nRead the document below with a list of {num_true_fact} ground-truth facts it contains and a list of {num_false_fact} hallucinated facts that are not supported by the document. Your task is to remove any hallucinated facts that can be supported by either the document or the {num_true_fact} ground-truth facts.\nPlease only return the remaining hallucinated facts, along with their original order numbers.\nDocument:\n\"\"\"{example_document}\"\"\"\n4 ground-truth facts:\n{true_facts}\n{num_false_fact} hallucinated facts:\n{false_facts}\nRemaining hallucinated facts:\n{remained_facts}\nRead the document below with a list of {num_true_fact} ground-truth facts it contains and a list of {num_false_fact} hallucinated facts that are not supported by the document. Your task is to remove any hallucinated facts that can be supported by either the document or the {num_true_fact} ground-truth facts.\nPlease only return the remaining hallucinated facts, along with their original order numbers.\nDocument:\n\"\"\"{document}\"\"\"\n{num_true_fact} ground-truth facts:\n{true_facts}\n{num_false_fact} hallucinated facts:\n{hallucinated_facts}\nRemaining hallucinated facts:"}, {"title": "Expenses", "content": "Generating the RAG-ConfusionQA dataset using GPT-40-mini costs approximately $50. For LLMs not provided by OpenAI, we used the API service from https://www.together.ai/. Excluding the dataset generation, all other experiments cost around $60."}]}