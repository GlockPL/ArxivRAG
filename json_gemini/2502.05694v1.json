{"title": "Zero-Shot End-to-End Relation Extraction in Chinese: A Comparative Study of Gemini, LLaMA, and ChatGPT", "authors": ["Shaoshuai Du", "Yiyi Tao", "Yixian Shen", "Hang Zhang", "Yanxin Shen", "Xinyu Qiu", "Chuanqi Shi"], "abstract": "This study investigates the performance of various large language models (LLMs) on zero-shot end-to-end relation extraction (RE) in Chinese, a task that integrates entity recognition and relation extraction without requiring annotated data. While LLMs show promise for RE, most prior work focuses on English or assumes pre-annotated entities, leaving their effectiveness in Chinese RE largely unexplored. To bridge this gap, we evaluate ChatGPT, Gemini, and LLaMA based on accuracy, efficiency, and adaptability. ChatGPT demonstrates the highest overall performance, balancing precision and recall, while Gemini achieves the fastest inference speed, making it suitable for real-time applications. LLaMA underperforms in both accuracy and latency, highlighting the need for further adaptation. Our findings provide insights into the strengths and limitations of LLMs for zero-shot Chinese RE, shedding light on trade-offs between accuracy and efficiency. This study serves as a foundation for future research aimed at improving LLM adaptability to complex linguistic tasks in Chinese NLP.", "sections": [{"title": "I. INTRODUCTION", "content": "With the rapid development of AI, numerous applications have emerged across various domains, including image processing, financial sentiment analysis, and natural language understanding [1, 2, 3, 4, 5]. Among these, zero-shot end-to-end relation extraction (RE) has gained increasing attention as a promising approach for extracting structured knowledge from unstructured text without requiring task-specific training data.\nZero-shot RE aims to extract entities and their semantic relationships without annotated examples, relying solely on a model's pre-trained knowledge and inference capabilities. Unlike traditional pipeline approaches that separate entity recognition and relation classification, end-to-end RE integrates these two processes into a single framework, reducing error propagation and improving contextual understanding."}, {"title": "II. MODELS AND RELATED WORK", "content": "However, zero-shot end-to-end RE remains highly challenging, particularly in Chinese, due to:\n\u2022 Lack of explicit word boundaries: Unlike English, where words are separated by spaces, Chinese requires implicit segmentation, making entity boundary detection more ambiguous.\n\u2022 Complex character composition: Many entity names share overlapping characters, increasing segmentation difficulty.\n\u2022 Implicit relation expression: Chinese often conveys relationships through contextual inference rather than explicit syntactic patterns, making relation extraction harder.\nDespite these challenges, recent advancements in Large Language Models (LLMs), such as GPT-4 and Gemini, have demonstrated strong capabilities in zero-shot learning, offering a potential solution for RE without annotated training data. While existing studies have primarily focused on English datasets, the effectiveness of different LLMs on zero-shot end-to-end RE in Chinese remains underexplored.\nTo bridge this gap, this study systematically evaluates the zero-shot end-to-end RE performance of three prominent LLMs: ChatGPT, Gemini, and LLaMA. Our contributions are as follows:\n\u2022 We provide a comparative analysis of these models' capabilities in handling zero-shot Chinese RE, highlighting their respective strengths and weaknesses.\n\u2022 We evaluate inference latency alongside accuracy metrics, offering insights into their practical feasibility.\n\u2022 We introduce a semantic matching-based evaluation method to mitigate inconsistencies in entity recognition and relation phrasing.\nThis study advances the understanding of LLMs in zero-shot RE for Chinese, addressing both the challenges of linguistic complexity and the trade-offs between accuracy and efficiency."}, {"title": "A. Models", "content": "ChatGPT[6], developed by OpenAI, excels in multilingual understanding and generating human-like text. Its pre-training makes it a strong candidate for zero-shot RE, but it is sensitive to prompt design and may produce verbose outputs without clear instructions. Gemini [7] is a multilingual LLM with Chinese support. We evaluate it using its API with prompts tailored to its input requirements. While versatile, Gemini may struggle with complex entity relations without fine-tuning due to limitations in its training data and architecture. LLaMA[8] is efficient and adaptable, with a Chinese-adapted version supporting structured outputs like triples. However, it may face challenges with nuanced entity recognition in Chinese without task-specific training.\nHere are the specific models we used, as shown in Table I"}, {"title": "B. Related Work", "content": "Relation extraction is a fundamental NLP task that identifies semantic relationships between entities. Traditional pipeline-based RE treats entity recognition and relation classification as separate steps [9], leading to error propagation. In contrast, end-to-end RE integrates both tasks into a unified model, reducing error accumulation [10].\nTo address the reliance on labeled data, zero-shot RE leverages pre-trained knowledge to infer relations without task-specific annotations [11]. Recent studies show that prompt-based zero-shot learning enables LLMs to extract relations from context [12]. Wei et al. [13] used ChatGPT for conversational information extraction, while Zhu et al. [14] demonstrated LLMs' capabilities in knowledge graph construction. Wu et al. [15] applied zero-shot RE to medical knowledge extraction using GPT models.\nHowever, prior work primarily focuses on English datasets with pre-annotated entities, simplifying the task. Moreover, most evaluations center on OpenAI models, leaving the performance of alternatives like Gemini and LLaMA unexamined. More critically, zero-shot end-to-end RE in Chinese remains underexplored, despite its unique linguistic complexities."}, {"title": "III. METHODOLOGY", "content": "To fill this gap, we systematically evaluate the zero-shot end-to-end RE performance of ChatGPT, Gemini, and LLaMA on Chinese datasets. Our study compares accuracy and inference efficiency while introducing semantic matching to enhance evaluation robustness beyond traditional string-based metrics."}, {"title": "A. Workflow", "content": "As illustrated in Figure 1, the workflow begins with raw data, such as sentences from the DuIE 2.0 dataset, which are directly fed into the models' API without manual annotations. For example, given the prompt and the input sentence:\u201c\u65f6\n\u96944\u5e74,\u8fd9\u4f4d\u5bfc\u6f14\u2014\u2014\u9ec4\u771f\u771f\u5e26\u7740\u5979\u7684\u4f5c\u54c1\u300a\u95fa\u871c2\u300b\u534e\n\u4e3d\u56de\u5f52\u3002\u201das shown in Figure 1, the models are prompted\nto extract entity-relation triples. In this case, an ideal output would be:\u201c\u9ec4\u771f\u771f-\u5bfc\u6f14-\u300a\u95fa\u871c2\u300b\u201d\nEach model-Gemini, LLaMA, and ChatGPT-processes the input using its pre-trained knowledge, generating structured outputs in the form of triples. These outputs are then compared against the ground truth using joint evaluation metrics and semantic matching techniques, as detailed in Section III-B."}, {"title": "B. Joint Evaluation Metrics and Semantic Matching", "content": "Evaluating zero-shot end-to-end RE requires metrics that assess both entity recognition and relation extraction. Key metrics include:\n\u2022 Joint Precision: The proportion of correctly extracted entity-relation tuples out of all tuples extracted by the model.\n\u2022 Joint Recall: The proportion of correctly extracted tuples out of all true tuples in the ground truth.\n\u2022 Joint F1 Score: The harmonic mean of joint precision and recall, balancing accuracy and completeness.\nTo avoid the problem of semantic variability, evaluation methods incorporating semantic similarity measures have been introduced[16]:\n\u2022 Semantic Matching: Techniques like word embeddings and contextualized models measure similarity between outputs and ground truth, accommodating variations in wording.\n\u2022 Relaxed Criteria: Outputs are considered correct if they partially overlap with or provide acceptable alternatives to the ground truth."}, {"title": "IV. EXPERIMENTS", "content": ""}, {"title": "A. Experimental Setup", "content": "API: We utilize the latest APIs and libraries provided by each model's developers.\nDataset We use the DuIE 2.0 dataset[17], a widely recognized benchmark for Chinese information extraction tasks. It contains a diverse set of sentences annotated with entities and their relations, covering various domains and relation types."}, {"title": "B. Results and Analysis", "content": "In Table IV-B, we present\nthe precision, recall, and F1 scores of various models evaluated\nin our experiments. The results highlight that OpenAI's mod\nels, particularly gpt-4-turbo, demonstrate superior overall\nperformance, achieving the highest F1 score of 0.367, which\nreflects a well-balanced trade-off between precision and recall.\ngpt-4 achieves the highest precision at 0.363, indicating its\nstrength in generating accurate predictions, with a slightly\nlower recall of 0.353. On the other hand, gpt-40-mini\nachieves the highest recall of 0.432, showing strong coverage\nbut at the expense of lower precision of 0.284.\nThe Gemini models exhibit mixed performance. For in\nstance, gemini-1.5-flash shows a relatively high recall\nof 0.424, though its precision and F1 score remain mod\nerate at 0.231 and 0.293, respectively. In contrast, Llama\nmodels generally perform poorly across all metrics. The\nllama3.1-405b model achieves the lowest F1 score of 0.117, indicating its limited effectiveness in the zero-shot CN\nRE task.\nThese results suggest that OpenAI's models, particularly\ngpt-4-turbo, are better suited for zero-shot relation extrac\ntion tasks compared to the Gemini and Llama series. However,\nthe trade-offs between precision and recall in some models,\nsuch as gpt-40-mini, indicate potential areas for optimiza\ntion to improve the balance between prediction accuracy and\ncoverage."}, {"title": "Latency Comparison", "content": "Beyond accuracy, inference latency\nis a crucial factor for practical deployment, as shown in Fig\nure 2. OpenAI's models generally exhibit higher latency, with\ngpt-4 reaching 3.98 seconds, reflecting its computational\ncomplexity. In contrast, optimized versions such as gpt-40\nand gpt-40-mini achieve lower latencies of 1.86 and 2.31\nseconds, respectively, making them more viable for real-time\napplications.\nGemini models offer the lowest latency, with\ngemini-1.5-flash-8b achieving 0.57 seconds, followed\nby gemini-1.5-flash at 0.91 seconds. This efficiency\nmakes them well-suited for scenarios requiring fast response\ntimes, such as online reasoning tasks where real-time\ninteraction is critical. However, their moderate accuracy\nsuggests a trade-off between speed and extraction quality.\nLLaMA models show\nvarying latency, with\nllama3.1-405b reaching 3.36 seconds, comparable\nto gpt-4, while lighter versions such as llama3.2-3b\nand llama3.2-90b-vision demonstrate latencies of\n2.23 and 2.77 seconds, respectively."}, {"title": "C. Discussion", "content": "The findings reveal a trade-off between accuracy and infer\nence speed in zero-shot end-to-end RE for Chinese. OpenAI's\nmodels, such as gpt-4-turbo, deliver strong extraction\nperformance but have higher latency, limiting their suitability\nfor real-time applications. In contrast, Gemini models, espe\ncially gemini-1.5-flash-8b, offer rapid inference, mak\ning them preferable for latency-sensitive applications like real\ntime decision-making. However, their moderate accuracy may\nimpact performance in high-precision tasks. LLaMA models\nexhibit neither leading accuracy nor efficiency, suggesting\na need for further adaptation. These insights highlight the\nimportance of balancing accuracy and efficiency based on the\nspecific requirements of real-world applications, whether in\nreal-time processing or batch inference workflows."}, {"title": "V. CONCLUSION", "content": "This study evaluates ChatGPT, Gemini, and LLaMA for\nzero-shot end-to-end relation extraction in Chinese using se\nmantic matching. ChatGPT achieves the best accuracy but\nsuffers from higher latency, while Gemini offers the lowest\nlatency with moderate performance. LLaMA underperforms\nacross all metrics, indicating the need for further adaptation.\nThese findings highlight the trade-offs between accuracy and\nefficiency in applying LLMs to Chinese RE.\nFuture work should explore finer-grained evaluation metrics,\nsuch as performance on specific relation categories and long\ntail relations, to better assess model capabilities. Additionally,\nimproving model adaptability and optimizing prompt design\nremain key directions for enhancing zero-shot Chinese RE."}]}