{"title": "CONTINUAL TASK LEARNING THROUGH ADAPTIVE POLICY SELF-COMPOSITION", "authors": ["Shengchao Hu", "Yuhang Zhou", "Ziqing Fan", "Jifeng Hu", "Li Shen", "Ya Zhang", "Dacheng Tao"], "abstract": "Training a generalizable agent to continually learn a sequence of tasks from offline trajectories is a natural requirement for long-lived agents, yet remains a significant challenge for current offline reinforcement learning (RL) algorithms. Specifically, an agent must be able to rapidly adapt to new tasks using newly collected trajectories (plasticity), while retaining knowledge from previously learned tasks (stability). However, systematic analyses of this setting are scarce, and it remains unclear whether conventional continual learning (CL) methods are effective in continual offline RL (CORL) scenarios. In this study, we develop the Offline Continual World benchmark and demonstrate that traditional CL methods struggle with catastrophic forgetting, primarily due to the unique distribution shifts inherent to CORL scenarios. To address this challenge, we introduce CompoFormer, a structure-based continual transformer model that adaptively composes previous policies via a meta-policy network. Upon encountering a new task, CompoFormer leverages semantic correlations to selectively integrate relevant prior policies alongside newly trained parameters, thereby enhancing knowledge sharing and accelerating the learning process. Our experiments reveal that CompoFormer outperforms conventional CL methods, particularly in longer task sequences, showcasing a promising balance between plasticity and stability.", "sections": [{"title": "1 INTRODUCTION", "content": "Similar to human cognition, a general-purpose intelligent agent is expected to continually acquire new tasks. These sequential tasks can be learned either through online exploration (Malagon et al., 2024; Yang et al., 2023; Wolczyk et al., 2021) or offline from pre-collected datasets (Huang et al., 2024; Gai et al., 2023; Hu et al., 2024a), with the latter being equally critical but having received comparatively less attention. Furthermore, online learning is not always feasible due to the need for direct interaction with the environment, which can be prohibitively expensive in real-world settings. Thus, the study of continual offline reinforcement learning (CORL) is both crucial and valuable for advancing general-purpose intelligence, which integrates offline RL and continual learning (CL).\nAlthough offline RL has demonstrated strong performance in learning single tasks (Chen et al., 2021; Hu et al., 2024b;c), it remains prone to catastrophic forgetting and cross-task interference when applied to sequential task learning (Bengio et al., 2020; Fang et al., 2019). Moreover, there is a lack of systematic analysis in this setting, and it remains unclear whether conventional CL methods are effective in CORL settings. CORL faces unique challenges, including distribution shifts between the behavior and learned policies, across offline data from different tasks, and between the learned policy and saved replay buffers (Gai et al., 2023). As a result, managing the stability-plasticity tradeoff in CL and addressing the challenges posed by distribution shifts in offline RL remains a critical issue (Yue et al., 2024). To tackle this, we leverage sequence modeling using the Transformer architecture (Hu et al., 2024d; Vaswani, 2017), which reformulates temporal difference (TD) learning into a behavior cloning framework, enabling a supervised learning approach that aligns better with traditional CL methods. While Transformer-based methods help reduce distribution shift between the behavior and learned policies and facilitate knowledge transfer between similar tasks due to the model's"}, {"title": "2 RELATED WORK", "content": "To overcome the limitations of previous approaches and better address the plasticity-stability tradeoff, we explore how to train a meta-policy network in CORL. Structure-based methods typically introduce new sub-networks for each task while preserving parameters from previous tasks, thereby mitigating forgetting and interference (Mallya & Lazebnik, 2018). These methods transfer knowledge between modules by transferring relevant knowledge through task-shared parameters. However, as the number of tasks increases, distinguishing valuable information from shared knowledge becomes increasingly challenging (Malagon et al., 2024). In such cases, knowledge sharing may offer limited benefits and could even impede learning on the current task. Therefore, in this paper, we propose leveraging semantic correlations to selectively compose relevant prior learned policy modules, thereby enhancing knowledge sharing and accelerating the learning process.\nAs illustrated in Figure 1, when a new task is introduced, our model, CompoFormer, first utilizes its textual description to compute attention scores with descriptions of previous tasks using the frozen Sentence-BERT (S-BERT) (Reimers, 2019) module and a trainable attention module. After several update iterations based on the final output action loss, if the composed policy output is sufficient for the current task, it is applied directly. Otherwise, new parameters are integrated alongside the composed policy to construct a new policy for the new task. This process naturally forms a cascading structure of policies, growing in depth as new tasks are introduced, with each policy able to access and compose outputs from prior policies to address the current task (Malagon et al., 2024). This approach could effectively manage the plasticity-stability trade-off. Specifically, relevant tasks share a greater amount of knowledge, facilitating a faster learning process, while less knowledge is shared between unrelated tasks to minimize cross-task interference. This mechanism enhances plasticity and accelerates the learning process. Simultaneously, the parameters of previously learned tasks remain fixed, preserving stability and reducing the risk of forgetting.\nIn our experiments, we extend the Continual World benchmark (Wolczyk et al., 2021) to the Offline Continual World benchmark and conduct a comprehensive evaluation of existing CL methods. Our approach consistently outperforms these baselines across different benchmarks, particularly in longer task sequences, achieving a marked reduction in forgetting (Table 1). This demonstrates its ability to effectively leverage prior knowledge and accelerate the learning process by adaptively selecting relevant policies. Additionally, we highlight the critical contributions of each component in our framework (Figure 4) and show the generalizability of our method across varying task order sequences (Table 2). Compared to state-of-the-art approaches, CompoFormer achieves the best trade-off between plasticity and stability (Figure 5)."}, {"title": "3 PRELIMINARIES", "content": "We begin by introducing the notation used in the paper and formalizing the problem at hand.\nThe goal of RL is to learn a policy $\\pi_{\\theta}(a|s)$ maximizing the expected cumulative discounted rewards $E[\\sum R(s_t, a_t)]$ in a Markov decision process (MDP), which is a six-tuple $(S, A, P, R, \\gamma, d_0)$, with state space $S$, action space $A$, environment dynamics $P(s'|s, a) : S \\times S \\times A \\rightarrow [0, 1]$, reward function $R : S \\times A \\rightarrow R$, discount factor $\\gamma \\in [0, 1)$, and initial state distribution $d_0$ (Sutton & Barto, 2018). In the offline setting (Levine et al., 2020), instead of the online environment, a static dataset $D = \\{(s, a, s', r)\\}$, collected by a behavior policy $\\pi_{\\beta}$, is provided. Offline RL algorithms learn a policy entirely from this static offline dataset $D$, without online interactions with the environment.\nIn this work, we follow the task-incremental setting commonly used in prior research (Khetarpal et al., 2022; Wolczyk et al., 2021), where non-stationary environments are modeled as MDPs with components that may vary over time. A task $k$ is defined as a stationary MDP $M^{(k)} = (S^{(k)}, A^{(k)}, P^{(k)}, R^{(k)}, \\gamma^{(k)}, d^{(k)})$, where $k$ is a discrete index that changes over time, forming a sequence of tasks. For each task, a corresponding static dataset $D^{(k)} = \\{(s^{(k)}, a^{(k)}, s'^{(k)}, r^{(k)})\\}$ is collected by a behavior policy $\\pi_{\\beta}^{(k)}$. We assume that the agent is provided with a limited budget of training steps to optimize the task-specific policy $\\pi^{(k)}$ using the dataset $D^{(k)}$. Once this budget is"}, {"title": "4 \u041c\u0415\u0422\u041dOD", "content": "In this work, given a sequence of previously learned policies $\\{\\pi^{(i)}\\}_{i=1,...,k-1}$, where each policy $\\pi^{(i)}$ corresponds to task $M^{(i)}$, we identify two scenarios for the current task $M^{(k)}$: (i) $M^{(k)}$ can be solved by a previously learned policy without requiring additional parameters, or (ii) $M^{(k)}$ requires learning a new policy that leverages knowledge from previous tasks to accelerate learning without interfering with earlier policies. Our methods address this by allocating a sub-network for each task and freezing its weights after training, effectively preventing forgetting. There are two main approaches to sub-network representation: (i) adding new parameters to construct the sub-network (Czarnecki et al., 2018; Malagon et al., 2024; Huang et al., 2024), or (ii) applying binary masks to neurons' outputs (Serra et al., 2018; Ke et al., 2021; Mallya & Lazebnik, 2018). We provide solutions for both approaches. The detailed pipeline is summarized in Algorithm 1."}, {"title": "4.1 \u039c\u0395\u03a4A-POLICY NETWORK", "content": "The underlying network architecture is based on the Decision Transformer (DT) (Chen et al., 2021), which formulates the RL problem as sequence modeling, leveraging the scalability of the Transformer architecture and the benefits of parameter sharing to better exploit task similarities.\nSpecifically, during training on offline data for the current task $M^{(k)}$, DT takes the recent M-step trajectory history $\\tau_t^{(k)}$ as input, where $t$ represents the timestep. This trajectory consists of the state $s_t^{(k)}$, the action $a_{t-1}^{(k)}$, and the return-to-go $R_t^{(k)} = \\sum_{i=t}^T \\gamma^{(k)}_i r_i$, where $T$ is the maximum number of interactions with the environment. The trajectory is formulated as:\n$\\tau_t^{(k)} = (R_t^{(k)}, s_t^{(k)}, a_{t-1}^{(k)}, R_{t-M+1}^{(k)}, s_{t-M+1}^{(k)}, a_{t-M}^{(k)}, ..., R_t^{(k)}, s_t^{(k)}, a_t^{(k)}).$"}, {"title": "4.2 SELF-COMPOSING POLICY MODULE", "content": "In this subsection, we describe the key building block of the proposed architecture: the self-composing policy module. The following lines explain the rationale behind these components.\nTo expedite knowledge transfer from previous tasks to new tasks, we utilize the task's textual description with the aid of Sentence-BERT (S-BERT) (Reimers, 2019). Specifically, given a new task with $M^{(k)}$ an associated description, we process the text using a pretrained S-BERT model to produce a task embedding $e_k \\in R^d$, where $d$ denotes the output dimension of the S-BERT model."}, {"title": "5 EXPERIMENT", "content": "Benchmarks. To evaluate CompoFormer, we introduce the Offline Continual World (OCW) benchmark, built on the Meta-World framework (Yu et al., 2020), to conduct a comprehensive empirical evaluation of existing continual learning methods. Specifically, we replicate the widely used Continual World (CW) framework (Wolczyk et al., 2021) in the continual RL domain by constructing 10 representative manipulation tasks with corresponding offline datasets. To increase the benchmark's difficulty, tasks are ranked according to a pre-computed transfer matrix, ensuring significant variation in forward transfer both across the entire sequence and locally (Yang et al., 2023). Additionally, we employ OCW20, which repeats the OCW10 task sequence twice, to evaluate the transferability of learned policies when the same tasks are revisited\u00b9.\nEvaluation metrics. Following a widely-used evaluation protocol in the continual learning literature (Rolnick et al., 2019; Chaudhry et al., 2018b; Wolczyk et al., 2021; 2022), we adopt three key metrics: (1) Average Performance (higher is better): The average performance at time t is defined as AP(t) = $\\frac{1}{K}\\sum_i^K p_i(t)$ where $p_i(t) \\in [0, 1]$ denotes the success rate of task i at time t. This"}, {"title": "5.2 MAIN RESULTS", "content": "Table 1 and Figure 3 present the results of all methods evaluated under two settings, OCW10 and OCW20, using the metrics outlined in Section 5.1.\nBoth regularization-based and rehearsal-based methods struggle to perform well, in contrast to their success in supervised learning settings. This underperformance stems from the unique distribution"}, {"title": "5.3 ABLATION STUDIES", "content": "Textual description. Does the output of the attention module in CompoFormer capture the semantic correlations between tasks? To address this question, we visualize the similarity of CompoFormer's attention output between each pair of tasks in the OCW10 sequence, as shown in Figure 4a. Detailed information regarding the attention scores for the OCW20 sequence can be found in Appendix F. The heatmap illustrates the attention scores assigned to each task based on previous tasks. For example, tasks 2 and 4 share a common manipulation primitive\u2014pushing the puck\u2014reflected in"}, {"title": "6 CONCLUSION", "content": "We introduce CompoFormer, a modular growing architecture designed to mitigate catastrophic forgetting and task interference while leveraging knowledge from previous tasks to address new ones. In our experiments, we develop the Offline Continual World benchmark to perform a comprehensive empirical evaluation of existing continual learning methods. CompoFormer consistently outperforms these approaches, offering a promising balance between plasticity and stability, all without the need for experience replay or storage of past task data.\nWe believe this work represents a significant step forward in the development of continual offline reinforcement learning agents capable of learning from numerous sequential tasks. However, challenges remain, particularly concerning the computational cost, which is critical in never-ending continual learning scenarios, and thus warrants further research."}, {"title": "A OFFLINE CONTINUAL WORLD BENCHMARK", "content": "We visualize all tasks in the Offline Continual World benchmark in Figure 6, and provide detailed descriptions in Table 3. The corresponding offline datasets are generated by training a Soft Actor-Critic (SAC) (Haarnoja et al., 2018) policy in isolation for each task from scratch until convergence. Once the policy converges, we collect 1 million transitions from the SAC replay buffer for each task, comprising samples observed during training as the policy approaches optimal performance (Hu et al., 2024c; He et al., 2024).\nIn the ablation study described in Section 5.3, we use different random seeds to shuffle the task sequences and conduct experiments on OCW10. The specific task sequences for each order are as follows:\n\u2022 Order 0: [hammer-v2, push-wall-v2, faucet-close-v2, push-back-v2, stick-pull-v2, handle-press-side-v2, push-v2, shelf-place-v2, window-close-v2, peg-unplug-side-v2]\n\u2022 Order 1: [push-v2, window-close-v2, peg-unplug-side-v2, shelf-place-v2, handle-press-side-v2, push-back-v2, hammer-v2, stick-pull-v2, push-wall-v2, faucet-close-v2]\n\u2022 Order 2: [handle-press-side-v2, peg-unplug-side-v2, push-back-v2, stick-pull-v2, push-v2, shelf-place-v2, faucet-close-v2, window-close-v2, push-wall-v2, hammer-v2]\n\u2022 Order 3: [push-wall-v2, handle-press-side-v2, push-v2, hammer-v2, peg-unplug-side-v2, stick-pull-v2, shelf-place-v2, faucet-close-v2, window-close-v2, push-back-v2]"}, {"title": "BAN EXTENDED DESCRIPTION OF COMPARED METHODS", "content": "We evaluate 11 continual learning methods on our benchmark: L2, EWC, MAS, LwF, RWalk, VCL, Finetuning, LoRA, PackNet, Perfect Memory, and A-GEM. Most of these methods are originally developed in the context of supervised learning. To ensure comprehensive coverage of different families of approaches within the community, we select methods representing three main categories, as outlined in Masana et al. (2022): regularization-based, structure-based, and rehearsal-based methods. The majority of the methods' implementations are adapted from https://github.com/mmasana/FACIL, with their default hyperparameters applied."}, {"title": "B.1 REGULARIZATION-BASED METHODS", "content": "Regularization-based methods focus on preventing the drift of parameters deemed important for previous tasks. These methods estimate the importance of each parameter in the network (assumed to be independent) after learning each task. When training on new tasks, the importance of each parameter is used to penalize changes to those parameters. For example, to retain knowledge from task 1 while learning task 2, a regularization term of the form:\n$\\sum F_j(\\theta_j - \\theta_j^{(1)})^2$,\nis added, where $\\theta_j$ are the current weights, $\\theta_j^{(1)}$ are the weights after learning the first task, and $\\lambda > 0$ is the regularization strength. The coefficients $F_j$ are crucial as they indicate the importance of each parameter. Regularization-based methods are often derived from a Bayesian perspective, where the regularization term constrains learning to stay close to a prior, which incorporates knowledge from previous tasks.\nL2. This method assumes all parameters are equally important, i.e., $F_j^{(i)} = 1$ for all $j$ and $i$. While simple, L2 regularization reduces forgetting but often limits the ability to learn new tasks.\nEWC. Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) is grounded in Bayesian principles, treating the parameter distribution of previous tasks as a prior when learning new tasks. Since the distribution is intractable, it is approximated using the diagonal of the Fisher Information Matrix:\n$F_j = E_{x \\sim D} E_{y \\sim \\pi_{\\theta}(\\cdot|x)} (\\nabla_{\\theta_j} log \\pi_{\\theta}(y|x))^2$.\nMAS. Memory Aware Synapses (Aljundi et al., 2018) estimates the importance of each parameter by measuring the sensitivity of the network's output to weight perturbations. Formally,\n$F_j = E_{x \\sim D} (\\frac{\\partial l(\\pi_{\\theta}(x))}{\\partial \\theta_j})^2$,\nwhere $\\pi_{\\theta}(x)$ is the model output, and the expectation is over the data distribution D.\nLwF. Learning without Forgetting (Li & Hoiem, 2017) utilizes knowledge distillation to preserve the representations of previous tasks while learning new ones. The training objective includes an additional loss term:\n$E_{x \\sim D}(\\pi_{\\theta}^{(k-1)}(x) - \\pi_{\\theta}^{(k)}(x))^2$,\nwhere $\\pi_{\\theta}^{(k-1)}(x)$ and $\\pi_{\\theta}^{(k)}(x)$ are the outputs of the old and current models, respectively, and the expectation is over the data distribution D.\nRWalk. Riemannian Walk (Chaudhry et al., 2018a) generalizes EWC++ and Path Integral (Zenke et al., 2017) by combining Fisher Information-based importance with optimization-path-based scores. The importance of parameters is defined as:\n$F_j = (Fisher_j^{(k-1)} + s_{\\theta_j}^{t_{k-1}})(\\theta_j - \\theta_j^{t_{k-1}})^2$,\nwhere $s_{\\theta_j}^{t_{k-1}}$ accumulates importance from the first training iteration $t_0$ to the last iteration $t_{k-1}$ for task k - 1."}, {"title": "B.2 STRUCTURE-BASED METHODS", "content": "Structure-based methods, also referred to as modularity-based methods, preserve previously acquired knowledge by keeping specific sets of parameters fixed. This approach imposes a hard constraint on the network, in contrast to the soft regularization penalties used in regularization-based methods.\nLORA. As introduced by Huang et al. (2024), this method adds a new LoRA-Linear module when a new task is introduced. According to Lawson & Qureshi (2024), in sequential decision-making tasks, Decision Transformers rely more heavily on MLP layers than on attention mechanisms. LoRA leverages this by merging weights that contribute minimally to knowledge sharing and fine-tuning the decisive MLP layers in DT blocks with LoRA to adapt to the current task.\nPackNet. Introduced by Mallya & Lazebnik (2018), this method iteratively applies pruning techniques after each task is trained, effectively \"packing\" the task into a subset of the neural network parameters, while leaving the remaining parameters available for future tasks. The parameters associated with previous tasks are frozen, thus preventing forgetting. Unlike earlier methods, such as progressive networks (Rusu et al., 2016), PackNet maintains a fixed model size throughout learning. However, the number of available parameters decreases with each new task. Pruning in PackNet is a two-stage process. First, a fixed subset of the most important parameters for the task is selected, typically comprising 70% of the total. In the second stage, the network formed by this subset is fine-tuned over a specified number of steps."}, {"title": "B.3 REHEARSAL-BASED METHODS", "content": "Rehearsal-based methods mitigate forgetting by maintaining a buffer of samples from previous tasks, which are replayed during training.\nPerfect Memory. This method assumes an unrealistic scenario of an unlimited buffer capacity, allowing all data from previous tasks to be stored and replayed.\nA-GEM. Averaged Gradient Episodic Memory (Chaudhry et al., 2018b) formulates continual learning as a constrained optimization problem. Specifically, the objective is to minimize the loss for the current task, $l(\\theta, D^{(k)})$, while ensuring that the losses for previous tasks remain bounded, $l(\\theta, M^{(i)}) \\leq l_i$, where $l_i$ represents the previously observed minimum, and $M^{(i)}$ contains buffer samples from task i for $1 \\leq i \\leq k\u22121$. However, this constraint is intractable for neural networks. To address this, Chaudhry et al. (2018b) propose an approximation using a first-order Taylor expansion:\n$(V_{\\theta}l(\\theta, B_{new}), V_{\\theta}l(\\theta, B_{old})) > 0$,\nwhere $B_{new}$ and $B_{old}$ represent batches of data from the current and previous tasks, respectively. This constraint is implemented via gradient projection:\n$V_{\\theta}l(\\theta, B_{new}) - \\frac{(V_{\\theta}l(\\theta, B_{new}), V_{\\theta}l(\\theta, B_{old}))}{(V_{\\theta}l(\\theta, B_{old}), V_{\\theta}l(\\theta, B_{old}))}V_{\\theta}l(\\theta, B_{old}).$"}, {"title": "C HYPERPARAMETER DETAILS AND RESOURCES", "content": "Training Details. To ensure fairness and reproducibility in the presented experiments, all continual learning methods are implemented using the Decision Transformer (Chen et al., 2021) architecture with identical hyperparameters for the architectural components, as detailed in Table4."}, {"title": "D COMPUTATIONAL COST OF INFERENCE IN COMPOFORMER", "content": "In this section, we analyze the computational cost of inference in the CompoFormer architecture with respect to the number of tasks, expressing the complexity in big-O notation.\nAs shown in Figure 1 and Algorithm 1, the primary computational cost during inference arises from the self-composing policy module. In the worst-case scenario, where no task can be solved by directly composing previous policies, the main inference time for a given task can be simplified as the cost of attention calculation over the matrix of previous policy outputs, $\\Phi_{(1:k\u22121)}$. This matrix has dimensions $(k \u2212 1) \\times h$, where $h$ is constant with respect to the number of modules, but the first dimension grows linearly with the number of tasks, $k$. Let $d$ represent the time cost per operation, the time required to construct $\\Phi_{(1:k-1)}$ (denoted as matrix V) is given by:\n$T_{\\Phi_{(1:k-1)}(k)} = (k \u2212 1) \\cdot d_{model}$.\nThe computational cost of calculating the key matrix K and the subsequent dot-product operation for attention depends linearly on the number of policies, k.\u00b2 Let h denote the hidden dimension. The time complexity for the attention module, $T_{attn(k)}$, can be expressed as:\n$T_{attn(k)} = d_{enc} \\cdot d_{linear} + (k \u2212 1) \\cdot d_{enc} \\cdot d_{linear} + (k \u2212 1).h + O(k \u2212 1) + (k-1).h$\nSince each task introduces new parameters that output features, which are combined with previous output features to construct the final action output, the total time complexity is:\n$T(k) = T_{\\Phi_{(1:k\u22121)}(k)} + T_{attn(k)} + d_{model}$.\nGiven that there are no higher-order terms beyond k, the computational complexity for the inference operation of a single module is $T(k) = O(k)$.\nGiven a CompoFormer model consisting of k policy modules, generating the final output requires sequentially computing the results of all k modules, as each module's output depends on the results of the preceding ones. Consequently, although the complexity of inference for each individual module is linear, the overall complexity of inference in CompoFormer is $k \\cdot O(k) = O(k^2)$."}, {"title": "E DETAILED SINGLE PERFORMANCE IN OUR BASELINES", "content": "In Section 5.3, we visualized the single-task performance of five methods; here, we present the performance of all methods in Figure 8. As shown in Figure 8, the MT method, which has access to data from all tasks throughout the learning process, achieves high performance on every task, comparable to the single-task method. In contrast, regularization-based and rehearsal-based methods introduce additional loss terms to mitigate catastrophic forgetting. However, during training, these methods still experience forgetting, particularly for the initial tasks in the sequence, and only show improved performance towards the end of the learning sequence. Increasing the regularization strength to prevent forgetting results in impaired learning of new tasks, leading to a pronounced stability-plasticity trade-off. This issue is particularly significant in the offline RL setting, where policy optimization is more sensitive to parameter changes compared to supervised learning in classification tasks(Masana et al., 2022; Zhou et al., 2023).\nStructure-based methods maintain good stability by freezing task-specific parameters, but their ability to learn new tasks diminishes as the training progresses. This is due to the decreasing number of available parameters and cross-task interference from the frozen parameters. Consequently, the performance on later tasks is often lower than that of regularization- and rehearsal-based methods. Our self-composing policy module addresses this issue by reducing cross-task interference and enhancing the plasticity of structure-based methods, allowing for more efficient learning of new tasks without compromising stability."}, {"title": "F DETAILED ATTENTION SCORES IN OCW20", "content": "In Section 5.3, we visualized the attention scores for the OCW10 benchmark. Here, we extend this analysis by visualizing the attention scores for the OCW20 benchmark, which repeats the OCW10 task sequence twice. As shown in Figure 7, for the first 10 tasks, the attention scores are similar to those observed in the OCW10 benchmark. However, for the subsequent 10 tasks, the model assigns the highest attention to the corresponding tasks from the first sequence, which are most relevant to the current task. This demonstrates the model's ability to capture semantic correlations between tasks and effectively leverage previously learned policies, resulting in superior performance compared to the baselines."}, {"title": "G HYPERPARAMETERS OF BASELINES", "content": "This section outlines the hyperparameters utilized in each baseline, with particular emphasis on the weight of the regularization term, as detailed in Section B.\nWe test the following hyperparameter values: $\\lambda \\in \\{10^{-1}, 10^{0}, 10^{1}, 10^{2}, 10^{3}, 5 \\times 10^{3}, 10^{4}\\}$, and the results are presented in Table 5. The hyperparameters corresponding to the best performance are employed in the main results. Notably, we observe no direct correlation between the model's final performance and the hyperparameters, indicating that tuning these settings often requires substantial manpower and resources."}, {"title": "B.1 REGULARIZATION-BASED METHODS", "content": "Regularization-based methods focus on preventing the drift of parameters deemed important for previous tasks. These methods estimate the importance of each parameter in the network (assumed to be independent) after learning each task. When training on new tasks, the importance of each parameter is used to penalize changes to those parameters. For example, to retain knowledge from task 1 while learning task 2, a regularization term of the form:\n$\\sum_{j} F_{j}\\left(\\theta_{j} - \\theta_{j}^{(1)}\\right)^{2}$,\nis added, where $\\theta_{j}$ are the current weights, $\\theta_{j}^{(1)}$ are the weights after learning the first task, and $\\lambda > 0$ is the regularization strength. The coefficients $F_{j}$ are crucial as they indicate the importance of each parameter. Regularization-based methods are often derived from a Bayesian perspective, where the regularization term constrains learning to stay close to a prior, which incorporates knowledge from previous tasks."}]}