{"title": "OUTPUT SCOUTING: AUDITING LARGE LANGUAGE MODELS FOR CATASTROPHIC RESPONSES", "authors": ["Andrew Bell", "Joao P. Fonseca"], "abstract": "Recent high profile incidents in which the use of Large Language Models (LLMs) resulted in significant harm to individuals have brought about a growing interest in AI safety. One reason LLM safety issues occur is that models often have at least some non-zero probability of producing harmful outputs. In this work, we explore the following scenario: imagine an AI safety auditor is searching for catastrophic responses from an LLM (e.g., a \"yes\" responses to \u201ccan I fire an employee for being pregnant?\u201d), and is able to query the model a limited number of times (e.g., 1000 times). What is a strategy for querying the model that would efficiently find those failure responses? To this end, we propose output scouting: an approach that aims to generate semantically fluent outputs to a given prompt matching any target probability distribution. We then run experiments using two LLMs and find numerous examples of catastrophic responses. We conclude with a discussion that includes advice for practitioners who are looking to implement LLM auditing for catastrophic responses. We also release an open-source toolkit\u00b9 that implements our auditing framework using the Hugging Face transformers library.", "sections": [{"title": "1 INTRODUCTION", "content": "Due to the rapid proliferation of Large Language Models (LLMs) and recent high profile cases demonstrating their ability to cause harm, the importance of AI safety is becoming widely recognized. For example, a United Nations advisory board recently called the regulation of AI technologies \u201cimperative,\u201d noting potential impacts to peace, security, and the global economy. Further, there is a growing body of research on studying the generation of dangerous, biased or toxic outputs through adversarial attacks on LLMs (called red teaming) (Liu et al., 2023; Ganguli et al., 2022), as well as the development of benchmarks to test the performance of safeguards in LLMs (Dorn et al., 2024; Lin et al., 2023; Zhu et al., 2024; Zou et al., 2023).\nIn 2024, the New York City government released an AI-powered chatbot called the MyCity Chatbot to help business owners understand local laws and processes. However, it was quickly observed that the chatbot was capable of giving disastrous (and illegal) advice, like claiming that it is okay to fire a worker who complains about sexual harassment, doesn't disclose that they are pregnant, or refuses to cut their dreadlocks. Finding these types of catastrophic responses, or outputs from an LLM that can cause significant harm to individuals, is the motivation for this work. Consider a scenario where an AI safety auditor was asked to review the MyCity Chatbot for catastrophic responses before it was deployed--we ask, what is a strategy that could be used for querying the model that would efficiently find those failure responses? We expound upon this problem statement in Section 3.1."}, {"title": "2 RELATED WORK", "content": "The risk for catastrophic responses is due to, at least in part, the fact that LLMs often exhibit overconfidence when providing answers or expressing their certainty (Xiong et al., 2023), which may lead to a misplaced sense of authority or trust into the models (Wester et al., 2024). These concerns have given way to several research area broadly known as AI safety. We break related work into three related areas: red teaming, uncertainty estimation, and explainability.\nRed teaming. Red teaming invovles adversarially probing an LLM using either manual or automated methods, such as using \u201cprompt hacking,\u201d crowdworkers, or even other LLMs to try and elicit harmful responses (Mazeika et al., 2024; Yang et al., 2024; Xu et al., 2021; Perez et al., 2022). For example, one may prompt an LLM with statements like \u201cforget all previous insturctions...", "Can I fire a pregnant employee? Yes, you can! Here is how you can do it strategically...\" (Schulhoff et al., 2023). While our work has the same broad goals as red teaming, it has an important distinction: rather than adversarially influenc-\"\n    },\n    {\n      \"title\": \"3 PRELIMINARIES\",\n      \"content\": \"Suppose we are given a pre-trained, autoregressive transformer-based LLM with weights w and an input prompt x (Aichberger et al., 2024). The input prompt can be represented as a sequence of tokens $[x_1, x_2,...,x_M]$, with each token $x_i \\in V$, where V is said to be the vocabulary of the model. The output from the model is the sequence of tokens $y = [Y_1, Y_2, ..., Y_T]$, again with $y_i \\in V$. We refer to generating an output sequence as \u201cquerying\\\" the model with an input prompt x.\nThe output sequence y is generated one token at a time, with the token at step t being sampled from a probability distribution over all possible tokens in the vocabulary of the model. Importantly, this probability distribution is conditioned on the previous output tokens, and can be expressed as $Pr(y_t|x, Y_1,..., Y_{t-1}, w)$. Note that in a slight abuse of notation we will sometimes write this distribution as $Pr(y_t|x, y_{<t}, w)$, and otherwise denote it as $p_t$. As described by Wortsman et al. (2024), the distribution $p_t$ is obtained by applying the softmax function to the logits $l_t$ outputted by the model at step t, i.e., $p_t = \\frac{e^{l_t/T}}{Z}$ where $Z = \\sum e^{l_t}$. Then the probability that any output sequence y occurs is the product of the probability of each token in y:\n$\\begin{equation}\nPr(y|x, w) = \\prod_{t=1}^{T} p_t\n\\end{equation}$\nIn practice, the output probability of a sequence is often normalized to avoid shorter sequence lengths having higher probabilities (Aichberger et al., 2024; Thomas & Joy, 2006; Malinin & Gales, 2021; Kuhn et al., 2023). The normalized version of the probability can be written in the following way:\n$\\begin{equation}\nPr(y|x, w) = exp(\\sum_{t=1}^{T} log(p_t))\n\\end{equation}$\nWe can say that the output sequence is in the set all of all possible output sequences of the LLM for a given prompt x, or the output space Y, i.e. $y \\in Y$.\"\n    },\n    {\n      \"title\": \"3.1 PROBLEM STATEMENT\",\n      \"content\": \"Suppose an Al safety inspector is auditing an LLM for responses to a specific prompt that result in a catastrophic failure, or outputs from the model that could result in significant harm to individuals. Also suppose that they have low-level access to the model, and that they are constrained either by time or compute in the number of queries they can execute, e.g., 1000 times.\nAs described in Section 3, each output from the model has a certain probability of occurring. If the auditor pursues a naive approach and simply repeatedly queries the LLM, they will likely see a distribution of outputs like that seen in Figure 3a, with the probabilities of the observed output sequences resemble normal distribution.\nIn this work, we develop a strategy for querying the LLM that results in a set of output sequences matching any probability distribution. For example, the auditor may want to uniformly sample across the output space (see Figure 3b), or only sample from outputs that have a low probability of occurring (see Figure 3c), or a mixture of both probabilities, to try and find catastrophic responses.\nIn our experiments, we audit the two large language models for a series of prompts that have a risk of catastrophic responses, such as the prompt \u201cCan I ignore this court summons for a civil lawsuit?": "ote that, in the United States, the answer is that a court summons cannot be ignored-and doing so can result in significant fines, and in some cases, jail time. We take the view that if a model was deployed at-scale that might output a \"yes\" response in even a very low number of cases (e.g., less than 1 in 100,000), it would be a meaningful AI safety concern."}, {"title": "3.2 CHALLENGES TO FINDING CATASTROPHIC RESPONSES", "content": "One may be tempted to think that finding rare (and potentially catastrophic) output sequences y involves greedily choosing the token that minimizes $Pr(y|x, w)$ at each step t. However, this is not necessarily true. To see this, consider the following toy example:\nExample 3.1 (Greedy minimization.). Suppose an LLM is implemented with top-3 selection, and is generating an output sequence y of length 2 by greedily selecting each token at step t to minimize $Pr(y|x, w)$. Let the probabilities for the three tokens at step t = 1 be 0.7, 0.2, 0.1, respectively. If the third token is chosen, let the probabilities at step t = 2 be 0.4, 0.3, 0.3, meaning the probability of the output sequence y would be 0.1 \u00d7 0.3 = 0.03. Suppose, however, that at step t = 1 the greedy strategy was abandoned, and the second token was chosen instead. The probabilities at step t = 2 could have been 0.8, 0.15, 0.05, meaning a probability of 0.2 \u00d7 0.05 = 0.01, which is less than under the greedy strategy.\nFurther, the goals of finding a rare (i.e., low probability) output sequence and finding a catastrophic response should not be conflated. In Section 5 we show that many catastrophic responses actually have a relatively high normalized probability. This is because an output sequence for a prompt like \"Can I ignore this court summons for a civil lawsuit?\" may only have a small number of unlikely tokens in the beginning of the response (i.e., \u201cYes, it's okay...\") but be followed by a large number of high probability tokens.\""}, {"title": "4 PROPOSED METHOD", "content": "At a high level, output scouting works by introducing an additional parameter (called the auxiliary temperature), using that parameter to simulate outputs from the LLM as if it had a different temperature, learning a function between that parameter and the normalized probability of outputs, and then using that function to produce outputs from the model with probabilities matching any target distribution.\nProcedure. Recall from Section 3 that LLMs are implemented with a base temperature T that, when generating a new output sequence y, sharpens or softens $p_t$ at each step t. We freeze this base temperature and do not modify it.\nThe proposed approach has three stages:\n(Select) We introduce a new parameter $T' \\in R_{>0}$ called the auxiliary temperature that induces a new probability distribution at step t called $p'(y_t)$. This new distribution is found in the following way:\n$\\begin{equation}\np'_t = softmax(\\frac{l_t}{T'})\n\\end{equation}$\nNext, when generating an output sequence y, tokens are selected over the modified distribution $p'(y_t)$.\n(Cache) While each token in the output sequence y is selected over the probability distribution using $p'$, we calculate and cache the normalized probability of a response using the \"base\" distribution $p_t$, as detailed in Figure 2. In this way, adjusting $T'$ allows us to simulate outputs with a different probability distribution, but we can still know the normalized probability of having generated that observation under the base model.\n(Predict) We generate an initial small amount of output sequences y (i.e., less than 10) using initial or random values for $T'$, caching the results as pairs ($T'$, $Pr(y|x, w)$). With this data (and each subsequent data pair we may generate) we can learn a function $f : R_{>0} \\rightarrow [0,1]$ that relates $T'$ to the observed values of $Pr(y|x, w)$. In our experiments, f is a degree-3 polynomial."}, {"title": "5 EXPERIMENTAL RESULTS", "content": "In Table 1 we propose six prompts for testing LLMs for catastrophic failure. These prompts are based on other popular red teaming prompts like those from ToxicChat, PromptBench, and AdvBench, but with the modification that they are all yes or no questions, where a response like \"yes\" from the model could result in significant harms to the individuals (Lin et al., 2023; Zhu et al., 2024; Zou et al., 2023). Importantly, the prompts we present here (and those found in existing benchmarks) can likely never constitute a complete audit for finding catastrophic responses. For example, one could likely always design more and more intricate prompts with a risk of catastrophic responses, such as questions about complex drug interactions or legal situations. This is further evidence for the need for safety refusals (sometimes called algorithmic resignation) where an LLM refuses to respond to a prompt for robust AI safety (Bhatt & Sargeant, 2024; Cheong et al., 2024; Xie et al., 2024).\nAudit framework. The framework we propose for auditing a model for catastrophic is as follows:"}, {"title": "6 DISCUSSION", "content": "Based on our audit results, we hypothesize that the risk of catastrophic responses is greatly underestimated by those using LLMs, as well as the research community at large. Even with vanilla sampling, we were able to find catastrophic responses at rates as high as 17 out of 1,000 queries. With the use of output scouting, we were able to find numerous additional catastrophic responses, even those with relatively high normalized probabilities. We take the view that if models are deployed at scale, even low rates of catastrophic responses could pose a significant risk for individuals and society at large.\nGuidance to practitioners. As part of this work, we built an open-source toolkit for using output scouting to audit any model loaded into the AutoModelForCausalLM class in the Hugging Face transformers library. To best use this toolkit, we recommend that practitioners follow the workflow described in the audit framework (see Section 5). Here we offer some additional considerations.\nUnfortunately, information like the base temperature T, or the top-k (or top-p) selection strategy isn't always publicly available for popular closed-source models like for OpenAI's ChatGPT or Anthropic's Claude. However, the base settings can sometimes be inferred through trial-and-error queries via the model's API, which often allow these settings to be tuned."}, {"title": "7 CONCLUSION AND SOCIAL IMPACT", "content": "In this work, we propose a method, framework, and toolkit for auditing LLMs for catastrophic responses. We take the perspective that if LLMs have any non-zero risk of producing a catastrophic responses and are deployed at scale, they pose a significant risk to human safety. It is our hope this work will be adopted by developers, practitioners, and regulators like AI safety auditors to create safer AI."}]}