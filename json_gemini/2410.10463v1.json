{"title": "TABCF: Counterfactual Explanations for Tabular Data Using a Transformer-Based VAE", "authors": ["Emmanouil Panagiotou", "Manuel Heurich", "Tim Landgraf", "Eirini Ntoutsi"], "abstract": "In the field of Explainable AI (XAI), counterfactual (CF) explanations are one prominent method to interpret a black-box model by suggesting changes to the input that would alter a prediction. In real-world applications, the input is predominantly in tabular form and comprised of mixed data types and complex feature interdependencies. These unique data characteristics are difficult to model, and we empirically show that they lead to bias towards specific feature types when generating CFs. To overcome this issue, we introduce TABCF, a CF explanation method that leverages a transformer-based Variational Autoencoder (VAE) tailored for modeling tabular data. Our approach uses transformers to learn a continuous latent space and a novel Gumbel-Softmax detokenizer that enables precise categorical reconstruction while preserving end-to-end differentiability. Extensive quantitative evaluation on five financial datasets demonstrates that TABCF does not exhibit bias toward specific feature types, and outperforms existing methods in producing effective CFs that align with common CF desiderata.", "sections": [{"title": "1 INTRODUCTION", "content": "Although Deep Neural Networks (DNN) are highly effective, their complexity makes them difficult to explain, hindering their adoption in crucial fields like healthcare and finance. Explainable AI (\u03a7\u0391\u0399) aims to overcome this by making their decisions interpretable [1]. Counterfactual (CF) explanations are one way to gain insight into a black-box model's decision by offering meaningful changes to the input that would result in a favorable outcome. Most initial works find CFs by searching in the input space [22, 28]. More recent studies leverage generative models, such as Variational Autoencoders [18] (VAE). These models capture the underlying structure of the data to generate more realistic and semantically meaningful CFs. However, most of these methods are developed for the vision domain [7, 14], where generative models are particularly effective. In business applications, especially finance, data most often comes in tabular form presenting specific challenges like mixed (numerical and categorical) feature types, inherent imbalances, and complex feature interdependencies. Existing tabular CF methods handle the mixed feature space using simple pre/post-processing [2, 9, 17, 25] or regularization functions [22]. Our experiments show that such"}, {"title": "2 RELATED WORK", "content": "Recent literature is abundant with studies on CFs that detail numerous desired properties (desiderata), target different data modalities, and employ various methodologies [26] and criteria for evaluation [24]. This section provides an overview of the methods most pertinent to our field of work of tabular CFs.\nCounterfactual desiderata. Are desirable properties for effective CF explanations. The most essential property is validity, ensuring the proposed changes alter the decision of the black-box model we aim to explain. The vast majority of works simultaneously optimize to find the most minimal changes that lead to the desired outcome [17, 22, 25, 28]. This objective involves the desiderata of proximity and sparsity, i.e., CFs that are close to the original instance and alter as few features as possible. Finally, some methods find more robust CFs that withstand minor perturbations [3], account for predictive uncertainty [2], causal constraints [17, 23], and plausibility [5].\nCounterfactual methods. Wachter et al. [28] first introduced CF explanations as an optimization problem in input space, aimed at changing a model's decision while minimizing the distance to the original instance. This approach inspired various other methods"}, {"title": "3 TABCF: COUNTERFACTUAL EXPLANATIONS FOR TABULAR DATA", "content": "In this section, we introduce our method TABCF. We start with preliminaries, then we present the architecture of our transformer-based VAE in Section 3.1 and we describe the CF generation process in Section 3.2.\nWe assume a differentiable black-box classifier $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$ where $\\mathcal{X}$ is the input feature space and $\\mathcal{y} = \\{0, 1\\}$ is the binary class. For an instance classified in the undesired class $f(x) = 0$, the goal is to find a CF example that belongs to the target class, i.e. $f(x') = 1$. Additionally, we assume a mixed input space of $|N|$ numerical and $|C|$ categorical features $x = [x_{num}, x_{cat}] \\in \\mathbb{R}^{[N]+[C]}$ We train $f$ by pre-processing the data in the usual fashion so that all numerical features are min-max normalized, and all categorical features are one-hot encoded. Therefore, each row is presented as a $k$-dimensional vector $x = \\left[x^{num}_{1}, x^{num}_{2},..., x^{num}_{|N|}, x^{oh}_{1}, x^{oh}_{2},...,x^{oh}_{|C|}\\right]$, with $k = |N| + \\sum_{i=1}^{C} C_i$, where $C_i$ are the discrete domains of each categorical feature."}, {"title": "3.1 Transformer-based VAE for tabular data", "content": "As previously mentioned, we build on the architecture used in [30] for synthetic mixed tabular data generation, and adapt it for CF generation. In particular, we employ learnable tokenizers to process the input data and transformers to learn the latent space. To reconstruct precise one-hot samples while maintaining end-to-end differentiability we propose a Gumbel detokenizer. The entire training pipeline of the VAE is presented in Figure 2, and is described in detail hereafter."}, {"title": "Feature Tokenizer", "content": "To adapt the transformer architecture for tabular data, [12] proposes a Feature Tokenizer as a learnable preprocessing step that converts features into tokens for the subsequent feature-level transformer layers. Specifically, given an input vector $x$ of size $k$,\n$x = [x^{num}_1, x^{num}_2,...,x^{num}_{|N|}, x^{oh}_1, x^{oh}_2,...,x^{oh}_{|C|}]$ a tokenized vector $X_{token}$ of size $k \\times d$ is created\n$X_{token} = [t^{num}_1, t^{num}_2,...,t^{num}_{|N|}, t^{cat}_1, t^{cat}_2,...,t^{cat}_{|C|}]$ via linear transformation of numerical, and \"lookup tables\" for each categorical feature. In detail we have,\n$t^{num} = x^{num} \\cdot W^{num} + b^{num}$ and\n$t^{cat}_i = x^{oh}_i \\cdot W^{cat}_i + b^{cat}_i$ for $i \\in \\{1, ..., |C|\\}$ where each column is a token $t^{num}, t^{cat}_i \\in \\mathbb{R}^{1\\times d}$. All weights and biases of the tokenizer, i.e., $W^{num}, b^{num} \\in \\mathbb{R}^{|N|\\times d}$ and $W^{cat}_i \\in \\mathbb{R}^{C_i\\times d}, b^{cat}_i \\in \\mathbb{R}^{1\\times d}$ are learnable parameters.\nThe learned column-wise token embeddings are passed to the transformer-based encoder that captures the rich feature interdependencies to output the mean and log variance. The latent vector $z = \\mu + \\sigma \\cdot \\epsilon$ is obtained via parameterization [18]. The same inverse procedure is followed for reconstructing the token vector $\\hat{x}_{token}$ through the decoder. To get the final output $\\hat{x}$ we use our specialized Gumbel detokenizer, described hereafter."}, {"title": "Gumbel-Detokenizer", "content": "A key requirement for TABCF is a fully differentiable pipeline that allows for optimizing CF samples in the latent space through the back-propagation of gradients. Additionally, it is essential to ensure that the decoded samples adhere to feature-type constraints. For instance, decoded categorical vectors $\\hat{x}^{cat}_i$ should be one-hot. Considering this need to reconstruct one-hot data while enabling gradient flow, we introduce a Gumbel detokenizer that uses the Gumbel-softmax [16] trick to generate categorical features. In particular the gumbel(.) function,\n$gumbel(x_i) = \\frac{exp((log(x_i) + g_i)/\\tau)}{\\sum_{j=1}^{C_i} exp((log(x_j) + g_j)/\\tau)}$ where $C_i$ denotes the number of modes per categorical feature, $g$ denotes the probabilistic variable sampled from the Gumbel distribution, and $\\tau$ denotes the temperature hyperparameter. It is important"}, {"title": null, "content": "to note that (for Section 5.2) the gumbel function outputs the discretized one-hot samples but uses the soft samples for differentiation. After decoding we get the reconstructions,\n$\\hat{x}^{num} = sigmoid(\\hat{t}^{num} \\cdot \\hat{W}^{num} + \\hat{b}^{num})$\n$\\hat{x}^{oh}_i = gumbel(\\hat{t}^{cat}_i \\cdot \\hat{W}^{cat}_i + \\hat{b}^{cat}_i)$ for $i \\in \\{1, ..., |C|\\}$\nThis ensures that all input constraints are respected for the reconstructed samples $\\hat{x} = \\left[\\hat{x}^{num}_1, \\hat{x}^{num}_2,...,\\hat{x}^{num}_{|N|}, \\hat{x}^{oh}_1, \\hat{x}^{oh}_2,...,\\hat{x}^{oh}_{|C|}\\right]$, i.e. numerical columns are in the range of [0, 1] and that all categorical vectors follow a one-hot distribution. In conclusion, TABCF enables gradient-based CF generation in the latent space and inherently guarantees tabular feature constraints. In contrast, other gradient-based methods resort to postprocessing techniques or rely on additional regularization losses to maintain categorical constraints [22, 24]. In our experiments, we show that this leads to unwanted feature-type bias.\nVAE training. We train the transformer-based VAE using the $\\beta$-VAE loss, $L = ||x - \\hat{x}|| + \\beta \\cdot L_{KL}$ [13], where $L_{KL}$ denotes the discrete Kullback-Leibler divergence between the latent variable and a standard gaussian. Following [30], we opt for a better reconstruction than a perfectly Gaussian distributed latent space by gradually decreasing $\\beta = [\\beta_{max}, \\beta_{min}]$ during training."}, {"title": "3.2 Counterfactuals in the latent space", "content": "After training the transformer-based VAE, we use the latent representations to search for CFs by traversing the latent space via gradient steps. We use a loss term comprised of three components designed for Validity, Proximity, and Sparsity (referring to the desiderata in Section 2).\nMore specifically, given an input instance $x_0$, we obtain the initial latent representation $z_0 = Enc(x_0)$ through the encoder. We then initialize the optimization with $z = z_0$ and take gradient steps updating $z$ with $\\nabla_z L_{CF}$, minimizing the following loss function:\n$L_{CF}(z) = hinge\\_yloss[f(Dec(z)), y = 1]$\n$+ \\lambda_{prox\\_input} ||x_0 - Dec(z)||_1$\n$+ \\lambda_{prox\\_latent} ||z_0 - z||_2$"}, {"title": null, "content": "Validity. The first component computes the difference between the target class (y = 1) and the current prediction of the black-box model f(.). The latent vector z being optimized, is first passed through the decoder Dec(z). This reconstructed vector is in the original tabular form, by design of our architecture, and can be directly used to get the prediction of the model f (Dec(z)). Following [22], we use the hinge-loss, defined as\n$hinge\\_yloss = max \\left[0, 1 - logit (f(.))\\right]$\nwhich has two functionalities, i) it heavily penalizes predictions that do not belong to the target class, i.e. when P(f(x) = 1) < 0.5, and ii) it returns a penalty when the target class is achieved, i.e. when P(f(x) = 1) \u2265 0.5, proportional to the difference P(f(x) = 1) \u2013 0.5 between the predicted target probability and the decision threshold. The intended effect is pushing instances across the decision boundary to ensure validity, while optimizing for better trade-off solutions (i.e. proximity and sparsity) afterward.\nInput proximity and sparsity. The second term, $||x_0 - Dec(z) ||_1$ measures the L1 distance of the original instance $x_0$ and the reconstructed sample, where $\\lambda_{prox\\_input}$ is a weighting hyperparameter. This term serves as a proximity loss in the input space. Using the L1 norm (instead of e.g. L2) additionally encourages feature sparsity [31]. It is important to note that although sparsity can be computed using the L0 norm, i.e. count distance, this operation is non-differentiable.\nLatent proximity. The last term, $||z_0 - z||_2$ measures the L2 distance to the original latent representation $z_0$, to encourage proximity in the latent space, where $\\lambda_{prox\\_latent}$ is a weighting hyperparameter.\nMotivated by other works, that either contain the search in a neighborhood of the latent space [25], or minimize the distance in the input space [2, 17], we decide to follow a combined approach ensuring both latent and input proximity. Our ablation study (Section 5.3) empirically demonstrates that this combined method yields superior results in terms of proximity and sparsity. Our optimization process is illustrated in Figure 1."}, {"title": "4 EXPERIMENTAL EVALUATION", "content": "This section presents the datasets, baselines, metrics, and the general setup of our quantitative evaluation."}, {"title": "4.1 Experimental setup", "content": "We initialize TABCF by training the VAE for 4.000 epochs, gradually decreasing \u03b2 from $\u03b2_{max} = 10^{\u22123}$ to $\u03b2_{min} = 10^{\u22125}$, and use \u03c4 = 1.0 for the Gumbel distribution. We choose a maximum of 30.000 training samples for larger datasets to improve comparability. After the VAE is sufficiently trained, we perform Stochastic Gradient Descent (SGD) to find CFs for a maximum of 5.000 steps or until the loss converges and the target class is reached. For loss weighting, we set $\\lambda_{prox\\_input}$ = 1 and $\\lambda_{prox\\_latent}$ = 1, as these hyperparameters yield the best overall results. We discuss this further in our ablation study (Section 5.3). For all baseline competitors we use the implementations of the CARLA framework [24]."}, {"title": "4.2 Real world financial datasets", "content": "We choose five real-world financial datasets, with a range of different data types, feature counts, and ratios of categorical to numerical features, to ensure a thorough experimental evaluation. In Table 1 we list the characteristics of each dataset, such as the size of the training set used in our experiments, the number of numerical and categorical features, and a description of the binary classification problem."}, {"title": "4.3 Baseline competitors", "content": "We compare TABCF to related methods on counterfactual generation that either directly optimize in the input space or employ generative models to first learn latent representations (see Section 2). The descriptions of all methods are listed below."}, {"title": "4.3.1 Baselines operating in input space", "content": "We compare to two state-of-the-art gradient-based methods that do not leverage latent representations.\nWachter [28] first mathematically defined CF explanations, using Stochastic Gradient Descent (SGD) to optimize a loss function. The objective is twofold, optimizing for the label flip of the black-box model, while minimizing the distance to the original sample. However, this method tends to find CFs very close to the decision boundary [26], and clamps categorical features to their original values [24, 28].\nDiCE [22] extends the previous approach, taking into account more practical considerations regarding the feature types, such as incorporating a regularization loss for enforcing one-hot representations during optimization (see Section 5.2)."}, {"title": "4.3.2 Baselines operating in latent space", "content": "More recent works, similar to our approach, utilize generative autoencoders to learn latent representations of the data, before searching for CFs in that latent space. We have identified two such approaches for VAE-based tabular data CFs.\nREVISE [17] employs a VAE of fully connected neural network layers to learn a structured latent space. As with other gradient-based approaches [22, 28], a loss function is optimized to change the predicted class while minimizing the distance to the original instance. This distance is measured in the reconstructed input, similar to our input-proximity loss term. Gradient descent updates the latent representations until the loss converges, after which the final CFs are returned by the decoder.\nCCHVAE [25] similarly uses a conditional VAE for representation learning. Unlike earlier methods, it does not optimize a loss; rather CFs are found in the latent space using a model-agnostic search algorithm. Specifically, multiple points are sampled in the latent space with a growing-sphere approach, until some decoded sample matches the CF requirements, i.e., minimal proximity. Although such heuristic-based methods are more efficient, they do not guarantee optimal results and can be more difficult to adapt to new objectives or constraints.\nAlthough the competitors employ various methods to handle categorical data, they all share the common practice of discretization (e.g. rounding) after each optimization step. Our experiments in Section 5.2 demonstrate that this results in feature-type bias."}, {"title": "4.4 Metrics", "content": "We evaluate TABCF and all baselines along several metrics to assess the effectiveness of each method. To ensure a fair comparison, each metric is calculated directly in the input space. Additionally, we evaluate all methods on the same test set, selecting n = 1000, previously unseen, instances that are not part of the target class, i.e., $X_{test} = \\{x | f(x) = 0, i = 1, 2, ..., n\\}$. Each method generates a set of valid counterfactuals $CF_{test} = \\{x | f(x) = 1\\}$. Because CF generation is a difficult non-convex problem [22], it is not guaranteed that a valid CF will be found for each instance. Therefore, the number of valid CFs, $n_{val} = |CF_{test}|$, might be less than the number of test instances, i.e., $n_{val} \\leq n$. We define all metrics in detail hereafter.\nValidity. The validity score is the most important metric, as it measures the success rate, i.e. the percentage of instances for which"}, {"title": null, "content": "optimization successfully switched the decision of the black-box classifier to the target class. More formally,\n$Validity (\\uparrow) = \\frac{1}{n} \\sum_{i=1}^{n_{val}}(f(x') = 1)$\nEach of the following metrics is calculated only for valid CFs.\nSparsity. The sparsity scores measures the percentage of features changed to achieve a CF. We specifically differentiate between categorical and numerical sparsity,\n$Sparsity Cat (\\downarrow) = \\frac{1}{n_{val}} \\sum_{i=1}^{n_{val}} \\frac{||x_{cat} - x'_{cat}||_0}{|C|}$\n$Sparsity Num (\\downarrow) = \\frac{1}{n_{val}} \\sum_{i=1}^{n_{val}} \\frac{||x_{num} - x'_{num}||_0}{|N|}$\nWhere the L0 norm $||x^0 \u2013 x'||_0$, counts the number of features that have different values in $x^0$ compared to $x'$. The result is normalized by the total number of features, i.e. $|N|$ for numerical and for $|C|$ categorical.\nProximity. Proximity uses a distance function to measure how close the CF is to the original instance. It is defined only for numerical features since the categorical sparsity metric essentially plays the role of the proximity metric for categorical features.\n$Proximity Num (\\downarrow) = \\frac{1}{n_{val}} \\sum_{i=1}^{n_{val}} ||x_{num} - x'_{num}||_1$\nThe numerical features are standard-normalized, and the L1 norm is used to measure the distance."}, {"title": "4.5 Feature importance and utilization", "content": "In our feature utilization experiment, we aim to examine the different features altered by each method during CF generation. The general assumption motivating this study is that features with a positive impact on the model prediction (target class) are good candidates for modification, towards a potential label flip. This is especially true if the positive impact comes with a minimal difference to the original value. Such features in the XAI domain, are referred to as important features, and corresponding scores, given a black-box model, can be estimated via feature importance methods [19]. We compute feature importance for $X_{test}$ for the black-box model to study whether essential features are primarily subject to change for TABCF and the competitors.\nFor this study, we choose Shapley Additive Explanations (SHAP) [19] as the feature importance baseline. SHAP is a field-tested method for ML-based modeling in the finance domain [21], even further for modeling credit scoring systems [4, 20]. SHAP, originating from cooperative game theory, provides a way to distribute the payoff among players fairly based on their contributions. When applied to neural networks to explain feature importance, SHAP attributes the model's output to its input features by considering all possible feature combinations."}, {"title": "5 RESULTS AND DISCUSSION", "content": "In this section, we discuss the results of our quantitative evaluation. Then we examine how the processing of tabular data by competitors"}, {"title": null, "content": "can cause feature-type bias. Finally, we present the results of our ablation study on the hyperparameters of our loss functions."}, {"title": "5.1 Results for all baselines and datasets", "content": "We evaluate all methods on the five tabular datasets and report the average metric values across the test set. Detailed results for each dataset are presented in Table 3. For a comprehensive overview, we further average the results across all datasets to rank the methods, as shown in Table 2."}, {"title": "5.2 Feature utilization", "content": "Our observations indicate that competitors exhibit bias towards numerical features when identifying CFs, rather than using categorical features. This bias is problematic because an effective method should use features based solely on their influence on finding CFs, irrespective of their type.\nTo measure the importance of features on the model output, we can use the well-established Shapley explanation method [19] (as detailed in Section 4.5). For example, in Figure 3, we display the impact of various features from the Adult dataset on the output of"}, {"title": null, "content": "the black-box classifier. The visualization reveals that categorical features such as education and occupation positively affect the model's predictions in some cases. Moreover, certain numerical features, like age and hours/week, can have a positive impact even with moderate changes in the feature value (indicated in purple). On the other hand, the capital gain/loss features only show a positive impact when their values are maximal (highlighted in pink). Given that CF methods aim to identify minimal changes with positive outcomes, we expect these methods to favor categorical features, like education and occupation, or numerical features like age and hours/week, when generating CFs for the Adult dataset.\nHowever, we empirically show that this is not the case, by computing the feature utilization of each method on the Adult dataset. The resulting histogram in Figure 6 reveals that all competitors primarily use numerical features when generating CFs. DiCE exhibits a predominant imbalance towards numerical features, utilizing them"}, {"title": "5.3 Ablation losses", "content": "The ablation study includes a five-step weight increase in the range [0,1] for $\\lambda_{prox\\_latent}$ and $\\lambda_{prox\\_input}$. Therefore, we conduct 25"}, {"title": null, "content": "runs in total, measuring the metrics of Validity, numerical Proximity, and numerical and categorical Sparsity, on each weight combination, for the Adult dataset.\nAs anticipated, we observe the conflicting nature of the validity desideratum, to proximity and sparsity. This trade-off arises because the validity loss term aims to push instances toward the target class, while the proximity loss terms (in the latent and input space), work to keep the CFs close to the original instance. Hence, sparsity and proximity metrics show better scores (more saturated in the plot)"}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "This paper presents TABCF, a method that leverages a transformer-based VAE for generating CF explanations for mixed tabular data. Our differentiable Gumbel-Softmax architecture allows precise reconstruction, overcoming feature-type bias present in previous approaches. Additionally, TABCF outperforms competitors in generating valid, proximal, and sparse counterfactuals, thus enhancing the interpretability of black-box models in real-world applications.\nIn future work, we would like to address user input constraints, such as immutable features or causal relationships between features, which could be achieved by conditioning the latent space. Furthermore, we plan to investigate the effect of distance-preserving Lipschitz-continuous VAEs on proximal counterfactual generation."}]}