{"title": "CAMEL-Bench: A Comprehensive Arabic LMM Benchmark", "authors": ["Sara Ghaboura", "Ahmed Heakl", "Omkar Thawakar", "Ali Alharthi", "Ines Riahi", "Abduljalil Saif", "Jorma Laaksonen", "Fahad Shahbaz Khan", "Salman Khan", "Rao Muhammad Anwer"], "abstract": "Recent years have witnessed a significant interest in developing large multimodal models (LMMs) capable of performing various visual reasoning and understanding tasks. This has led to the introduction of multiple LMM benchmarks to evaluate LMMs on different tasks. However, most existing LMM evaluation benchmarks are predominantly English-centric. In this work, we develop a comprehensive LMM evaluation benchmark for the Arabic language to represent a large population of over 400 million speakers. The proposed benchmark, named CAMEL-Bench, comprises eight diverse domains and 38 sub-domains including, multi-image understanding, complex visual perception, handwritten document understanding, video understanding, medical imaging, plant diseases, and remote sensing-based land use understanding to evaluate broad scenario generalizability. Our CAMEL-Bench comprises around 29,036 questions that are filtered from a larger pool of samples, where the quality is manually verified by native speakers to ensure reliable model assessment. We conduct evaluations of both closed-source, including GPT-4 series, and open-source LMMs. Our analysis reveals the need for substantial improvement, especially among the best open-source models, with even the closed-source GPT-4o achieving an overall score of 62%. Our benchmark and evaluation scripts are open-sourced.", "sections": [{"title": "1. Introduction", "content": "Large multimodal models (LMMs) have recently achieved significant advancements across a broad spectrum of tasks, including visual reasoning, perception, and multimodal understanding. Closed-source models such as GPT-4V and open-source LMMs, such as LLaVA [28] have demonstrated effectiveness in tasks like image captioning [43], visual question answering (VQA) [24, 25], and complex visual reasoning [12]. These recent developments have led to the introduction of different benchmarks to evaluate the performance of open and closed-source LMMs. Despite these advances, most existing LMM benchmarks are English-centric, limiting their applicability to other languages [44]. With over 400 million speakers, Arabic is the 5th most widely spoken languages globally. In the context of large"}, {"title": "2. CAMEL-Bench", "content": "Our dataset encompasses eight diverse domains to ensure a versatile multi-task Arabic LMM benchmark for different real-world scenarios. Each domain is further sub-divided into different sub-domains, each focusing on a distinct aspect. During the data collection process, we either utilize"}, {"title": "2.1. Data Collection", "content": "Our dataset encompasses eight diverse domains to ensure a versatile multi-task Arabic LMM benchmark for different real-world scenarios. Each domain is further sub-divided into different sub-domains, each focusing on a distinct aspect. During the data collection process, we either utilize"}, {"title": "2.2. Question-Answers Pairs Generation", "content": "We note that a major part of our original Arabic data is not derived from ready-made VQA datasets. Some sub-domains, such as celebrities and food, consist of image-only data, while others, like Pexel's countries and landmarks, contain image-caption pairs. To create a rich and diverse VQA corpus, we first ensure that each image is accompanied by detailed contextual information. This context is sourced from a combination of Wikipedia (e.g., for food-related data), manual curation (e.g., for countries and landmarks in videos), and AI-generated content based on a manually provided context (e.g., for diagrams and infographics). Next, we generate multiple-choice questions (MCQs) for each sample using the GPT-40 model. The prompt is meticulously crafted to adhere to key criteria: each sample generates three multiple-choice questions (MCQs), with four distinct, non-synonymous options per question, only one of which is correct. The questions contain no embedded hints, ensuring that answers are derived exclusively from the image, without requiring prior knowledge. Additionally, the image must provide enough information to fully support the correct answer, eliminating the need for guesswork. In total, this process produces a corpus of 4.4K generated questions with 17.7K answers, enabling a comprehensive set of questions for evaluation."}, {"title": "2.3. Data Filtering and Verification", "content": "The data collection and question-answer pair generation process lead to over 41k questions in total which then undergoes to filtering and verification process. The CAMEL-Bench filtering and verification process (see Fig. 3) is carefully conducted based on whether the QA text is originally Arabic or translated into Arabic from English language. For all sub-domains derived from original Arabic context, we take a 20% randomly sampled subset for manual verification. In case if the error remains less below a 40% threshold, the sub-category is accepted into CAMEL-Bench. Alternatively, the entire sub-category undergoes manual review.\nIn case of the translated Arabic data from English, the original English context is also incorporated into the filtering and verification process. Here, Qwen7B [8] is used to compare the semantic similarity between the English and the English-translated data at the QA-pair level using fuzzy evaluation. To ensure the model understands semantic similarity in Arabic, we provided 5 few-shots prompting. Subsequently, QA-pairs rejected by Qwen7B [8] are manually reviewed, resulting in one of three outcomes. Manual Handling implying that data requires full re-translation. Refine and Verify referring that the translation can be refined using the model. Non-Translated Review implying that the non-translated data is re-sent to the model for translation. Consequently, we obtain 29,036 high-quality questions."}, {"title": "3. CAMEL-Bench Benchmark Evaluation", "content": "Evaluation Metrics: Our evaluation framework is designed with three specialized metrics, each carefully aligned to different types of datasets and tasks. For MCQ datasets like MMT [56] and MMMU [58], we utilize exact match accuracy to ensure precise evaluation. For optical character recognition (OCR) datasets, such as PATS [3] and Evarest [18], where accurate text extraction is critical, we adopt edit distance [45] as the key metric. For more flexible datasets like VQAv2 [17], MathVista [33], and GeoChat [22], where multiple synonymous answers can be considered correct. we implement a fuzzy evaluation method for all such datasets. This approach uses GPT-4o to compare the predicted answer with the ground truth, while accounting for the context of the question. By incorporating these diverse metrics, our evaluation provides a robust and comprehensive assessment that adapts to the unique demands and response formats of each dataset."}, {"title": "4. Conclusion, Limitations and Societal Impact", "content": "We present a comprehensive and diverse benchmark, named CAMEL-Bench, for Arabic LMM evaluation. To the best of our knowledge, CAMEL-Bench is the first comprehensive Arabic LMM benchmark comprising eight diverse domains and 38 sub-domains with around 29k questions that are filtered from a larger pool of 41k samples with the quality verified by native speakers. We conduct extensive evaluations of open- and closed-source LMMs, highlighting the need for substantial improvements in different areas for future Arabic LMM development. Although our CAMEL-Bench strives to significantly contribute towards developing sophisticated Arabic LMMs, we note that it mainly covers modern standard Arabic and does not fully explore other Arabic dialects. As the data samples are either based on existing datasets or new data that is crawled from the internet, it is possible that CAMEL-Bench exhibits biases already existing in the benchmarks. Nevertheless, we believe CAMEL-Bench is a step towards the inclusion of Arabic language and Arabic-speaking populations in accessing the benefits of LMMs."}, {"title": "A. Appendix", "content": null}, {"title": "B. More on Dataset Curation", "content": "The dataset utilized in this work was carefully curated with a rigorous focus on data quality, relevance, and diversity. Our curation process involved selecting multimodal data from various domains, including images, text, videos, and specialized fields such as medical imaging, agriculture, and remote sensing. To ensure the integrity and accuracy of the dataset, we employed multiple stages of data verification. This process involved cross-validation, thorough verification procedures for Arabic content, and the integration of standardized data sources where applicable."}, {"title": "C. Dataset Overview and Task Splits", "content": "This section provides a comprehensive breakdown of the datasets used across eight distinct categories, illustrating the diversity and depth of our evaluation framework. Each category is further divided into sub-domains, ensuring that the multimodal models are rigorously tested on a wide range of tasks and datasets. This structure guarantees comprehensive coverage and introduces varied challenges to thoroughly assess model performance. Refer to Tab. 2 for a detailed breakdown of the data categories with their statistics."}, {"title": "C.1. Multimodal Understanding and Reasoning", "content": "This category encompasses various sub-domains such as visual understanding, object hallucination evaluation, and complex visual perception. Key datasets include MME, MMBench, ScienceQA-IMG, and VQA2. These datasets test the model's ability to handle intricate reasoning tasks across both visual and textual inputs, with a total of 3,971 questions under the visual understanding sub-domain, and significant representation from other tasks like scientific reasoning (1,624 questions) and object-level perception (60 questions)."}, {"title": "C.2. OCR and Document Understanding", "content": "Document understanding covers scanned documents, scene understanding, text extraction, and more. This category emphasizes precise OCR and textual recognition from images and scanned materials. Datasets like ArabicDatasetOCR and ISI-PPT-Dataset challenge the model to process a diverse range of document types. A substantial number of questions come from Handwritten Text datasets (1,400 questions) and PPT OCR (2,354 questions), ensuring the model is evaluated across both structured and unstructured document types."}, {"title": "C.3. Chart and Diagram Understanding", "content": "In chart and diagram interpretation, models are tested on understanding visual representations of data, such as charts, diagrams, and tables. This includes datasets like ChartQA, MMMU, and BCE-Arabic. The evaluation focuses on tasks such as understanding diagrammatic reasoning and tabular data with 1,994 questions from diagram datasets and 745 questions involving charts, providing a robust examination of the model's ability to interpret visual data efficiently."}, {"title": "C.4. Video Understanding", "content": "This category assesses the model's ability to process and comprehend video data, focusing on tasks like recognizing countries, landmarks, and occasions. Video-MME is a prominent dataset, contributing 654 questions to the evaluation. The inclusion of diverse sub-domains, such as recognizing cultural aspects through video, highlights the importance of temporal and visual information synthesis in multimodal reasoning."}, {"title": "C.5. Cultural Specific Understanding", "content": "The cultural understanding domain tests the model's capacity to handle tasks specific to certain cultures, including food, landmarks, and celebrities. Datasets like arabic-food-101 and Pexel challenge the model to recognize culturally significant items, with 444 questions focused on celebrities and 494 on countries/landmarks. These tasks highlight the model's ability to adapt and generalize across different cultural contexts."}, {"title": "C.6. Medical Imaging", "content": "Covering a range of sub-domains in the medical field, this category includes tasks related to basic medical science, clinical medicine, and public health, using datasets like MMMU and MMT-MI-Bench. These datasets assess the model's potential in specialized medical contexts, with over 1,200 questions spanning across diagnosis, medical understanding, and pharmacy, ensuring a rigorous evaluation of the model's performance in handling critical medical information."}, {"title": "C.7. Agricultural Image Understanding", "content": "The agricultural domain is represented through datasets like AgroGPT, with 769 questions focused on agricultural understanding tasks. These tasks test the model's capacity to process and interpret images related to agricultural settings, reinforcing the model's ability to work with real-world scenarios in agriculture and environment-based challenges."}, {"title": "C.8. Remote Sensing Understanding", "content": "This category evaluates the model's ability to handle remote sensing data, specifically focusing on geographical data interpretation through datasets like GeoData VQA and GeoChat. With 709 questions in this domain, the model is tested on its spatial reasoning and understanding of complex remote-sensing imagery, crucial for applications in fields like environmental monitoring and geography."}, {"title": "D. CAMEL-Bench Data Samples", "content": "Fig. 2 showcases CAMEL-Bench's versatility across eight distinct domains, covering tasks like Multimodal Reasoning, OCR & Document Understanding, Chart & Diagram Interpretation, Video Scene Analysis, and more specialized areas like Remote Sensing, Agricultural Image Analysis, Medical Image Interpretation, and Cultural-Specific Knowledge. Each domain presents unique challenges, from logical reasoning and handwritten text recognition to medical diagnostics and cultural symbol identification. This variety emphasizes CAMEL-Bench's strength in supporting the development of AI systems capable of addressing real-world applications in healthcare, agriculture, geospatial analysis, and cross-cultural contexts."}]}