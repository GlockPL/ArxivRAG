{"title": "Rethinking Oversmoothing in Graph Neural Networks: A Rank-Based Perspective", "authors": ["Kaicheng Zhang", "Piero Deidda", "Desmond Higham", "Francesco Tudisco"], "abstract": "Oversmoothing is a fundamental challenge in graph neural networks (GNNs): as the number of layers increases, node embeddings become increasingly similar, and model performance drops sharply. Traditionally, oversmoothing has been quantified using metrics that measure the similarity of neighbouring node features, such as the Dirichlet energy. While these metrics are related to oversmoothing, we argue they have critical limitations and fail to reliably capture oversmoothing in realistic scenarios. For instance, they provide meaningful insights only for very deep networks and under somewhat strict conditions on the norm of network weights and feature representations. As an alternative, we propose measuring oversmoothing by examining the numerical or effective rank of the feature representations. We provide theoretical support for this approach, demonstrating that the numerical rank of feature representations converges to one for a broad family of nonlinear activation functions under the assumption of nonnegative trained weights. To the best of our knowledge, this is the first result that proves the occurrence of oversmoothing without assumptions on the boundedness of the weight matrices. Along with the theoretical findings, we provide extensive numerical evaluation across diverse graph architectures. Our results show that rank-based metrics consistently capture oversmoothing, whereas energy-based metrics often fail. Notably, we reveal that a significant drop in the rank aligns closely with performance degradation, even in scenarios where energy metrics remain unchanged.", "sections": [{"title": "1. Introduction", "content": "Graph neural networks (GNNs) have emerged as a powerful framework for learning representations from graph-structured data, with applications spanning knowledge retrieval and reasoning (Peng et al., 2023; Tian et al., 2022), personalised recommendation systems (Damianou et al., 2024; Peng et al., 2022), social network analysis (Fan et al., 2019), and 3D mesh classification (Shi & Rajkumar, 2020). Central to most GNN architectures is the message-passing paradigm, where node features are iteratively aggregated from their neighbours and transformed using learned functions, such as multi-layer perceptrons or graph-attention mechanisms.\nHowever, the performance of message-passing-based GNNs is known to deteriorate after only a few layers, essentially placing a soft limit on the depth of GNNs. This is often linked to the observation of increasing similarity between learned features as GNNs deepen and is named oversmoothing (Li et al., 2018; Nt & Maehara, 2019).\nOver the years, oversmoothing in GNNs, as well as methods to alleviate it, have been studied based on the decay of some node feature similarity metrics, such as the Dirichlet energy and its variants (Oono & Suzuki, 2019; Cai & Wang, 2020; Bodnar et al., 2022; Nguyen et al., 2022; Di Giovanni et al., 2023; Wu et al., 2023; Roth & Liebig, 2023). At a high level, most of these metrics directly measure the norm of the absolute deviation from the dominant eigenspace of the message-passing matrix. In linear GNNs without bias terms, this eigenspace is often known and easily computable via e.g. the power method. However, when nonlinear activation functions or biases are used, the dominant eigenspace may change, causing these oversmoothing metrics to fail and give false negative signals about the oversmoothing state of the learned features.\nTherefore, these metrics are often considered as providing sufficient but not necessary evidence for oversmoothing (Rusch et al., 2023a). Despite this, there is a considerable body of literature using these somewhat unreliable metrics as part of their evidence for non-occurrence of oversmoothing in GNNs (Zhou et al., 2021; Chen et al., 2022; Rusch et al., 2022; Wang et al., 2022; Maskey et al., 2023;"}, {"title": "2. Background", "content": "Let G = (V, E) be an undirected graph with V denoting its set of vertices and E \u2286 V \u00d7 V its set of edges. Let A \u2208 IRN\u00d7N be the unweighted adjacency matrix with N = |V| being the total number of nodes, |E| being the total number of edges of G and A the corresponding symmetric adjacency matrix normalized by the node degrees:\n$A = \\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}$,\nwhere D = D + I, D is the diagonal degree matrix of the graph G, and I is the identity matrix. The rows of the feature matrix X \u2208 RN\u00d7d are the concatenation of the d-dimensional feature vectors of all nodes in the graph. At each layer l, the GCN updates the node features as follows\nX(1+1) = \u03c3(AX(1)W(1))\nwhere o is a nonlinear activation function, applied componentwise, and W(1) is a trainable weight matrix."}, {"title": "2.2. Graph Attention Network", "content": "While GCNs use a fixed normalized adjacency matrix to perform graph convolutions at each layer, Graph Attention Networks (GATs) (Veli\u010dkovi\u0107 et al., 2017; Brody et al., 2021) perform graph convolution through a layer-dependent message-passing matrix A(1) learned through an attention mechanism as follows\n$A_{ij}^{(l)} = softmax_j(\\sigma_a(\\overrightarrow{p_1}^{(l)})^T W^{(l)} X_i + \\overrightarrow{p_2}^{(l)})^T W^{(l)}X_j)),$\nwhere $\\overrightarrow{p}$ are learnable parameter vectors, Xi, Xj denote the feature of the ith and jth nodes respectively, the activation \u03c3\u03b1 is typically chosen to be LeakyReLU, and softmaxj corresponds to the row-wise normalization\n$softmax_j (A_{ij}) = \\frac{exp(A_{ij})}{\\Sigma_{j'} exp(A_{ij'})}$\nThe corresponding feature update is\nX(1+1) = \u03c3(A(1)X(1)W(1))."}, {"title": "3. Oversmoothing", "content": "Oversmoothing can be broadly understood as an increase in similarity between node features as inputs are propagated through an increasing number of message-passing layers, leading to a noticeable decline in GNN performance. However, the precise definition of this phenomenon varies across different sources. Some works define oversmoothing more rigorously as the alignment of all feature vectors with each other. This definition is motivated by the behaviour of a linear GCN:\nX(1+1) = A... AX(0)W(0) ...W(l).\nIndeed if A is the adjacency matrix of a fully connected graph G, A will have a spectral radius equal to 1 with multiplicity 1, and A\u00b9 will thus converge towards the eigenspace spanned by the dominant eigenvector. Precisely, we have\n$A^l \\underset{l \\rightarrow \\infty}{=} Av^T$\nwhere Au = u, and Av v, see e.g. (Tudisco et al., 2015).\nAs a consequence, if the product of the weight matrices W(0) ... W(l) converges in the limit l \u2192 \u221e, then the features degenerate to a matrix having rank at most one, where all the features are aligned with the dominant eigenvector u. Mathematically, if we assume u to be such that ||u|| = 1, this alignment can be expressed by stating that the difference between the features and their projection onto u, given by ||X(1) \u2013 uuT X (1) ||, converges to zero."}, {"title": "3.1. Existing Oversmoothing Metrics", "content": "Motivated by the discussion about the linear case, oversmoothing is thus quantified and analysed in terms of the convergence of some node similarity metrics towards zero. In particular, in most cases, it is measured exactly by the alignment of the features with the dominant eigenvector of the matrix A. The most prominent metric that has been used to quantify oversmoothing is the Dirichlet energy, which measures the norm of the difference between the degree-normalized neighbouring node features (Cai & Wang, 2020; Rusch et al., 2023a)\n$E_{Dir}(X) = \\sum_{(i,j) \\in E} |\\frac{X_i}{u_i} - \\frac{X_j}{u_j}|^2$\nwhere ui is the i-th entry of the dominant eigenvector of the message-passing matrix in (1). It thus immediately follows from our discussion on the linear setting that EDir(X(1)) converges to zero as l \u2192 \u221e for a linear GCN with converging weights product W(0) . . . W(l). This intuition suggests that a similar behaviour may occur for \"smooth-enough\" nonlinearities."}, {"title": "3.2. A unifying perspective based on the eigenvectors of nonlinear activations", "content": "In the following discussion, we present a unified and more general perspective of the necessary conditions to have oversmoothing in the sense of the classical metrics, based on the eigenvectors of a nonlinear activation function. In the interest of space, longer proofs for this and the subsequent sections are moved to Appendix A.\nDefinition 3.1. We say that a vector u \u2208 RN \\ {0} is an eigenvector of the (nonlinear) activation function \u03c3 : RN \u2192 Rn if for any t \u2208 R \\ {0}, there exists pt such that \u03c3(tu) = \u03bc\u2081u.\nWith this definition, we can now provide a unifying characterization of message-passing operators A(1) and activation functions o that guarantee the convergence of the Dirichlet-like energy metric Eproj to zero for the feature representation sequence defined by X(1+1) = \u03c3(A(1)X(1)W(1)). Specifically, Theorem 3.3 shows that this holds provided all matrices A(1) share a common dominant eigenvector u, which is also an eigenvector of \u03c3.\nThis assumption recurs throughout our theoretical analysis, aligning with existing results in the literature. For example, (a) the proof by Cai & Wang (2020) applies to GCNs with LeakyReLU, where the dominant eigenvector of A is nonnegative by the Perron-Frobenius theorem and, therefore, an eigenvector of LeakyReLU; and (b) the proof by Wu et al. (2023) holds for stochastic message-passing matrices A(1), which inherently share a common dominant eigen-"}, {"title": "4. Energy-like metrics: what can go wrong", "content": "Energy-like metrics such as EDir and Eproj are among the most commonly used oversmoothing metrics. However, they suffer from inherent limitations that hinder their practical usability and informational content.\nOne important limitation of these metrics is that they indicate oversmoothing only in the limit of infinitely many layers, when their values converge exactly to zero. Since they measure a form of absolute distance, a small but nonzero value does not provide any meaningful information. On the other hand, convergence to zero corresponds to either perfect alignment with the dominant eigenspace or the collapse of the feature representation matrix to the all-zero matrix. While the former is a symptom of oversmoothing, the latter does not necessarily imply oversmoothing. Moreover, this convergence property requires the weights to be bounded. However, in practical cases, performance degradation is observed even in relatively shallow networks, far from being infinitely deep, and with arbitrarily large (or small) weight magnitudes. This aligns with our intuition and what occurs in the linear case. Indeed, for a linear GCN, even when the features X(1) grow to infinity as l \u2192 \u221e, one observes that X(1) becomes dominated by the dominant eigenspace of A, even for finite and possibly small values of l, depending on the spectral gap of the graph.\nMore precisely, the following theorem holds:\nTheorem 4.1. Let X(1+1) = AX(1)W(1) be a linear GCN. Let 11, 12 be the largest and second-largest eigenvalues (in modulus) of A, respectively. Assume the weights {W(1)}1 are randomly sampled from i.i.d. random variables with distribution v such that\n$\\int log^+(\\|W\\|) dv + \\int log^+(\\|W^{-1}\\|) dv < \\infty$\nwith log+(t) = max{log(t), 0}. If |12/11| < 1, then almost surely, it holds that\n$lim_{l \\rightarrow \\infty} \\frac{\\|(I - P)X^{(l)} \\|_F}{\\|PX^{(l)} \\|_F} = 0$\nwith a linear rate of convergence O(|\u03bb2/11|\u00b2).\nIn particular, the theorem above implies that for a large spectral gap |12/11| \u226a 1, X(1) is predominantly of rank one, namely\nX(1) = \u03bb\u03af (uv + R(1))"}, {"title": "5. The Rank as a Measure of Oversmoothing", "content": "Inspired by the behaviour observed in the linear case, we argue that measuring the rank of feature representations provides a more effective way to quantify oversmoothing, in alignment with recent work on oversmoothing (Guo et al., 2023). However, since the rank of a matrix is defined as the number of nonzero singular values, it is a discrete function and thus not suitable as a measure. A viable alternative is to use a continuous relaxation that closely approximates the rank itself.\nExamples of possible continuous approximations of the rank include the numerical rank, the stable rank, and the effective rank (Roy & Vetterli, 2007; Rudelson & Vershynin, 2006; Arora et al., 2019). The stable rank is defined as StableRank(X) = ||X|||||X||, where ||X||* = \u03a3\u03af\u03c3\u03b9 is the nuclear norm. The numerical rank is given by NumRank(X) = ||X||/||X||3. Finally, given the singular values \u03c3\u2081 > \u03c32 > \u2026 > 5min{N,d} of X, the effective rank is defined as\n$Erank(X) = exp(-\\sum_k p_k log p_k),$\nwhere pk = \u03c3\u03ba/\u03a3\u2081 \u03c3\u2081 is the k-th normalized singular value. These rank relaxation measures exhibit similar empirical behaviour as shown by our numerical evaluation in Section 6.\nIn practice, measuring oversmoothing in terms of a continuous approximation of the rank is a reasonable approach that helps address the limitations of Dirichlet-like measures. Specifically, it offers the following advantages: (a) it is scale-invariant, meaning it remains informative even when the feature matrix converges to zero or explodes to infinity; (b) it does not rely on a fixed, predetermined eigenspace but instead captures convergence of the feature matrix toward an arbitrary lower-dimensional subspace; (c) it allows for the detection of oversmoothing in shallow networks without requiring exact convergence to rank one\u2014since a small"}, {"title": "5.1. Theoretical Analysis of Rank Decay", "content": "In this section, we provide an analytical study proving the decrease of the numerical rank for a broad class of graph neural network architectures under the assumption that the weight matrices are entrywise nonnegative. While this is a somewhat restrictive setting, our result is the first theoretical proof that oversmoothing occurs independently of the weight (and thus feature) magnitude.\nWe begin with several useful observations. Let u be the dominant eigenvector of A corresponding to \u5165\u2081 and satisfying ||u|| = 1. Consider the projection matrix P = uuT. Given a matrix X, we can decompose it as X = PX + (I\u2013P)X. Since u is a unit vector, it follows that ||P||2 = 1, and"}, {"title": "6. Experiments", "content": "Empirical studies on the evolution of oversmoothing measures often use untrained, hundred-layer-deep GNNs (Rusch et al., 2022; Wang et al., 2022; Rusch et al., 2023b; Wu et al., 2023). We emphasize that this is an overly simplified setting. In more realistic settings, as the ones considered in this section, a trained GNN may suffer from significant performance degradation after only few-layers, at which stage the convergence patterns of most of the metrics are difficult to observe. The experiments that we present in this section validate the robustness of the effective rank and numerical rank in quantifying oversmoothing in GNNs against the other metrics.\nIn particular, we compare how different overmoothing metrics behave compared to the classification accuracy, varying the GNN architectures for node classification on real-world graph data. In our experiments, we consider the following metrics:\n\\bullet The Dirichlet Energy EDir (Cai & Wang, 2020; Rusch et al., 2023a) and its variant Eproj (Wu et al., 2023). Both are discussed in Section 3.1, see in particular (8) and (9).\n\\bullet Normalized versions of the Dirichlet energy and its variant, namely EDir(X)/||X|| and Eproj(X)/||X||F. Indeed, from our previous discussion, a robust oversmoothing measure should be scale invariant with respect to the features. Metrics with global normalization like the ones we consider here have also been proposed in (Di Giovanni et al., 2023; Roth & Liebig, 2023; Maskey et al., 2023).\n\\bullet The Mean Average Distance (MAD) (Chen et al., 2020)\n$MAD(X) = \\frac{1}{|E|} \\sum_{(i,j) \\in E} |1 - \\frac{X_i^T X_j}{\\|X_i\\| \\|X_j\\|}|$\nIt measures the cosine similarity between the neighbouring nodes. Unlike previous baselines, this oversmoothing metric does not take into account the dominant eigenvector of the matrices A(1).\n\\bullet Relaxed rank metrics: We consider the Numerical Rank and Effective Rank. Both are discussed in Section 5. We point out that from our theoretical investigation, in particular from (12), the numerical rank decays to 1 faster than the decay of the normalized Eproj energy to zero. This further supports the use of the Numerical Rank as an improved measure of oversmoothing with respect to Eproj.\nIn Table 2 and Figure 1, we train GNNs of a fixed hidden dimension equal to 32 on the Cora dataset. More results on Cora, as well as other datasets, are reported in Appendix B. We follow the standard setups of GCN and GAT as stated in equation (2) and (5), and use homogeneous LeakyReLU (LReLU) and subhomogenous Tanh as activation functions. For the GCN model, we also consider adding additional"}, {"title": "7. Conclusion", "content": "In this paper, we have discussed the problem of quantifying oversmoothing in GNNs. After discussing the limitations of the leading oversmoothing measures, we have proposed the use of the rank of the features as a better measure of oversmoothing. The experiments that we provided validate the robustness of the effective rank against the classical measures. In addition, we have proved theoretically the decay of the rank of the features for linear and nonnegative GNNs."}, {"title": "A. Proofs of the main results", "content": "Let \u03c0 := span{uvT | v \u2208 Rd} be the 1-dimensional matrix subspace of the rank-1 matrices having columns aligned to u. Then it is easy to note that given some matrix X, (I \u2013 P)X provides the projection of the matrix X on the subspace \u03c0, i.e.\n$(I-P)X = proj_{\\pi}(X)$.\nIndeed ((I \u2013 P)X, uvT)F = Tr(vuT (I \u2013 uuT)X) = 0. In particular, since the projection realizes the minimal distance, we have that\n$\\|X - PX\\|_F \\le \\|X - uv^T\\|_F$\n$\\\\ \\forall v \\in R^d$.\nNow observe that o(uuT A(1\u22121) X(1\u22121)W(l\u22121) = \u0e19\u1fe6 for some \u1fe6. Indeed, writing vT = uT A(1\u22121) x(l\u22121)W(l\u22121), we have that the i-th column of o(uuT A(1\u22121) X(l\u22121)W(l\u22121)) is equal to o(v\u2081u) = \u016b\u017eu for some vi, because u is an eigenvector of \u03c3. As a consequence we have\n$\\|(I - P)X^{(l)} \\|_F \\le \\|X^{(l)} - \\sigma(uu^T A^{(l-1)} X^{(l-1)} W^{(l-1)}) \\|_F$\n$= \\| \\sigma(A^{(l-1)} X^{(l-1)} W^{(l-1)}) - \\sigma(uu^T A^{(l-1)} X^{(l-1)} W^{(l-1)}) \\|_F$\n$\\le \\|(I - P)A^{(l-1)} X^{(l-1)}W^{(l-1)} \\|_F$\nwhere we have used the 1-Lipschitz property of \u03c3."}, {"title": "A.2. Proof for Theorem 4.1", "content": "Start by studying the norm of (I \u2013 P)X(1). Then looking at the shape of the powers of the Jordan blocks matrix it is not difficult to note that T\u00b9 = O(()) for 1 larger than N. In particular if we look at the explicit expression of (I \u2013 P)X(1)\n$\n(I-P)X^{(l)} = (0...0 (I-PM) \n\\begin{pmatrix}\n0 & 0\\\\\n\\frac{\\lambda_2}{\\lambda_1} & 0\\\\\n\\end{pmatrix}\nM^{(l-1)}X^{(0)}W^{(0)}...W^{(l-1)},$\nwe derive the upper bound\n$\\|I - P)X^{(l)} \\|_F \\le C(\\frac{\\lambda_2}{\\lambda_1})^l \\|X^{(0)}W^{(0)} ...W^{(l-1)}\\|_F,$\nfor some positive constant C that is independent on l.\nSimilarly we can observe that\n$\\|PX^{(l)} \\|_F \\ge \\|u^T A^l X^{(0)} W^{(0)} ... W^{(l-1)} \\|_F =$\n$\\|(\\lambda_1^l v + u^T MO((l))) X^{(0)}W^{(0)} ... W^{(l-1)} \\|_F \\ge$\n$\\|\\lambda_1^l (\\| v^T X^{(0)}W^{(0)} ... W^{(l-1)} \\|_F - \\|MO((\\frac{\\lambda_2}{\\lambda_1})^l) X^{(0)}W^{(0)}...W^{(l-1)}\\|) \\ge$\n$|\\lambda_1|^l (\\|v^T X^{(0)} W^{(0)} ... W^{(l-1)} \\|_F - \\|O((\\frac{\\lambda_2}{\\lambda_1})^l X^{(0)} W^{(0)} ... W^{(l-1)} \\|_F).\nNow observe that under the randomness hypothesis from (Furstenberg & Kifer, 1983) and more generally from the Oseledets ergodic multiplicative theorem, we have that for almost any w \u2208 Rd the limit exists and is equal to the maximal Lyapunov exponent of the system, i.e. a constant c(v) \u2265 0 depending only on the distribution v, $\\lim_n \\frac{1}{n} log \\|w^T W^{(0)} ... W^{(l-1)}\\| = c(\\nu)$. In particular for any w and \u20ac > 0 there exists lw,e sufficiently large such that for any l > lw,\u0454\n$c(\\nu) - \\epsilon < \\frac{1}{l} log \\|w^T W^{(0)} ... W^{(l-1)}\\| < c(\\nu) + \\epsilon$."}, {"title": "A.3. An illustrative example", "content": "Consider the stochastic nonnegative primitive matrix\n$A:= \\begin{pmatrix}\n0 & 1 \\\\\n1/2 & 1/2\\\\\n\\end{pmatrix}$\nSince A is row stochastic, the dominant eigenvector is given by the constant vector u = (1, 1). In particular given a vector x = (x1, x2) \u2208 K we have that\n$d_H(x, u) \\lessapprox log \\frac{maxi=1,2{xi}}{\\|x\\|_1 mini=1,2{xi}} = \\begin{cases}\nlog (x_1/x_2) & \\text{if} x_1 > X2\\\\\nlog (X2/X1) & \\text{if} X2 \\ge X1\\\\\n\\end{cases}$\nOn the other hand Ax = (x2, (x1 + x2)/2), thus\n$d_H(Ax, u) = \\begin{cases}\nlog (\\frac{(x1+x2)/2x2}) & \\text{if} x1 \\ge X2\\\\\nlog (2x2/(x1+x2)) & \\text{if} x2 \\ge X1\\\\\n\\end{cases}$\nNow observe that the set of vectors x such that d\u2081(x, u) < C is given by the points {x | x1 < x2 \u2264 C*x1} \u222a {x | x2 \u2264 x1 < C*x2} with C* = eC > 1. We want to prove the existence of some \u00dfc = 1 - ec \u2208 (0, 1) such that dH (Ax, u) \u2264 BcdH (x, u) for any x with d\u043d (\u0445, \u0438) \u2264 \u0421.\nTo this end consider first the case of x \u2208 N\u2081 = {x | x1 < X2 < C*x1}. In this case\n$dH(Ax, u) = log (\\frac{2x2}{X1+X2}) = \\log \\left((1-e_C)\\frac{x_2}{x_1} \\right) = (1 - ec)d(x, u)$\n$ \\Leftrightarrow e_c < \\log \\frac{2x2}{x1+x2} < \\log(2/( \\frac{x_1}{x_2} + 1)) \\forall x \\in \\Omega_1$\n$<  ec \\log(t) < \\log\\left( \\frac{2t}{1 + t} \\right)$\n$t  < \\frac{1 + t}{2}\\$\nvt \\in [1, C^*]$\n\\Vt \\in [1, C^*]$\n12"}, {"title": "A.4. proof of Lemma 5.3", "content": "By the contraction properties of the matrix A we know that if maxi d\u00ed ((AX)i, u) \u2264 C, then\nd\u043d((AX)i, u) = dH ((AX);, 11(A)u) = d ((AX)i, Au) \u2264 \u00df\u0430\u043d (Xi, u)\nVi.\nfor some \u03b2 < 1, where we have used X1 (A) > 0 and the scaling invariant property of the Hilbert distance.\nThen note that, for any i, we can write F(X)\u00bf as follows\n$(F(X))_i = \\sum_j W_{ij} (AX)_j$.\nThus we CLAIM that given x1, x2, y \u2208 K then\ndh(x1+x2,y) \u2264 max{dH(x1,y), dH (x2,y)}.\nObserve that if the claim holds we have conlcuded the proof, indeed by induction it can trivially be extended from 2 to d points yielding\nH(F(X)j, u) <= \\max dH (W_{ij} (AX)_i, u) <= \\max dH ((AX)_i, u) <= \\beta \\maxdH(x_j, u),\nwhere we have used the scale-invariance property of the Hilbert distance and the fact that maxi Wij > 0 for all j.\nIt remains to prove the claim. To this end, exploiting the expression of the Hilbert distance we write\ndH(x1+x2,y) = log \\sup_i \\sup_j \\frac{(x_1)_i + (x_2)_i}{(y)_i}\n <=\\log \\left(\\sup\\sup\\max\\frac{(x1)_i}{(y)_i},\\frac{(x2)_i}{(y)_i}) \\right) =  \\max {d_H(x_1,y), d_H(x_2, y)}\nconcluding the proof."}, {"title": "A.5. proof of Theorem 5.4", "content": "To prove the theorem we start proving that, a continuous subhomogeneous and order-preserving function with an eigenvector u in the cone, is not nonexpansive in Hilbert distance with respect to u. Formally we claim that\nd\u00ed (\u03c3(y), u) \u2264 d\u043d (\u0443, \u0438)\nVy \u2208 K."}, {"title": "A.6. proof of Proposition 5.5", "content": "We start from the homogenous case. Note that since 4 is homogeneous we have that necessarily f(t) = ct for all t, c \u2265 0, this in particular means that every u \u2208 R is an eigenvector of o with corresponding eigenvalue \u5165\u2081 = c.\nThen we can consider the subhomogeneous case. Assume that we have u \u2208 R that is an eigenvector of o with eigenvalue A and ui > 0 for all i, then\n$\u03c8(u_i) = \\lambda u_i \\hspace{1cm} \\forall i = 1,..., N.$\nBy strict subhomogeneity this means that necessarily u is constant, indeed if ui > uj > 0 then\n$\\lambda u_j = \\psi(u_j) = \\psi (\\frac{u_j}{u_i}u_i) > \\frac{u_j}{u_i} \\psi(u_i) = \\lambda u_j$,\nyielding a contradiction. In particular any constant vector u in R\u00a5 is easily proved to be an eigenvector of o relative to the eigenvalue \u03bb = ||\u03c3(u)||1/||u||1 = \u03c8(uz)/u\u017c where u\u2081 is any entry of u."}]}