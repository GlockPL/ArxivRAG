{"title": "The Prevalence of Neural Collapse in Neural Multivariate Regression", "authors": ["George Andriopoulos", "Zixuan Dong", "Li Guo", "Zifan Zhao", "Keith Ross"], "abstract": "Recently it has been observed that neural networks exhibit Neural Collapse (NC) during the final stage of training for the classification problem. We empirically show that multivariate regression, as employed in imitation learning and other applications, exhibits Neural Regression Collapse (NRC), a new form of neural collapse: (NRC1) The last-layer feature vectors collapse to the subspace spanned by the n principal components of the feature vectors, where n is the dimension of the targets (for univariate regression, n = 1); (NRC2) The last-layer feature vectors also collapse to the subspace spanned by the last-layer weight vectors; (NRC3) The Gram matrix for the weight vectors converges to a specific functional form that depends on the covariance matrix of the targets. After empirically establishing the prevalence of (NRC1)-(NRC3) for a variety of datasets and network architectures, we provide an explanation of these phenomena by modeling the regression task in the context of the Unconstrained Feature Model (UFM), in which the last layer feature vectors are treated as free variables when minimizing the loss function. We show that when the regularization parameters in the UFM model are strictly positive, then (NRC1)-(NRC3) also emerge as solutions in the UFM optimization problem. We also show that if the regularization parameters are equal to zero, then there is no collapse. To our knowledge, this is the first empirical and theoretical study of neural collapse in the context of regression. This extension is significant not only because it broadens the applicability of neural collapse to a new category of problems but also because it suggests that the phenomena of neural collapse could be a universal behavior in deep learning.", "sections": [{"title": "Introduction", "content": "Recently, an insightful phenomenon known as neural collapse (NC) [Papyan et al., 2020] has been empirically observed during the terminal phases of training in classification tasks with balanced data. NC has three principal components: (NC1) The features of samples within each class converge closely around their class mean. (NC2) The averages of the features within each class converge to form the vertices of a simplex equiangular tight frame. This geometric arrangement implies that class means are equidistant and symmetrically distributed. (NC3) The weight vectors of the classifiers in the final layer align with the class means of their respective features. These phenomena not only enhance our understanding of neural network behaviors but also suggest potential simplifications in the architecture and the training of neural networks."}, {"title": "Related work", "content": "Neural collapse (NC) was first identified by Papyan et al. [2020] as a symmetric geometric structure observed in both the last layer features and classification vectors during the terminal phase of training of deep neural networks for classification tasks, particularly evident in balanced datasets. Since then, there has been a surge of research into both theoretical and empirical aspects of NC.\nSeveral studies have investigated NC under different loss functions. For instance, [Han et al., 2021, Poggio and Liao, 2020, Zhou et al., 2022a] have observed and studied neural collapse under the Mean Squared Error (MSE) loss, while papers such as [Zhou et al., 2022b, Guo et al., 2024] have demonstrated that label smoothing loss and focal loss also lead to neural collapse. In addition to the last layer, some papers [He and Su, 2023, Rangamani et al., 2023] have also examined the occurrence of the NC properties within intermediate layers. Furthermore, beyond the balanced case, researchers have investigated the neural collapse phenomena in imbalanced scenarios. [Fang et al., 2021] identified a phenomenon called minority collapse for training on imbalanced data, while [Hong and Ling, 2023, Thrampoulidis et al., 2022, Dang et al., 2023] offer more precise characterizations of the geometric structure under imbalanced conditions.\nTo facilitate the theoretical exploration of the neural collapse phenomena, [Fang et al., 2021, Mixon et al., 2020] considered the unconstrained feature model (UFM). The UFM simplifies a deep neural network into an optimization problem by treating the last layer features as free variables to optimize over. This simplification is motivated by the rationale of the universal approximation theorem [Hornik et al., 1989], asserting that sufficiently over-parameterized neural networks can be highly expressive and can accurately approximate arbitrary smooth functions. Leveraging the UFM, studies such as [Zhu et al., 2021, Zhou et al., 2022a, Thrampoulidis et al., 2022, Tirer and Bruna, 2022, Tirer et al., 2023, Ergen and Pilanci, 2021, Wojtowytsch et al., 2020] have investigated models with different loss functions and regularization techniques. These studies have revealed that the global minima of the empirical risk function under UFMs align with the characterization of neural collapse observed by [Papyan et al., 2020]. Beyond the UFM, some work [Tirer and Bruna, 2022, S\u00faken\u00edk et al., 2024] has extended the model to explore deep constrained feature models with multiple layers, aiming to investigate neural collapse properties beyond the last layer.\nIn addition to its theoretical implications, NC serves as a valuable tool for gaining deeper insights into DNN models and various regularization techniques [Guo et al., 2024, Fisher et al., 2024]. It provides crucial insights into the generalization and transfer learning capabilities of neural networks [Hui et al., 2022, Kothapalli, 2022, Galanti et al., 2021], inspiring the design of enhanced model architectures for diverse applications. These include scenarios with imbalanced data [Yang et al., 2022, Kim and Kim] and contexts involving online continuous learning [Seo et al., 2024].\nDespite extensive research on the neural collapse phenomena and its implications in classification, to the best of our knowledge, there has been no investigation into similar issues regarding neural regression models. Perhaps the paper closest to the current work is [Zhou et al., 2022a], which applies the UFM model to the balanced classification problem with MSE loss. Although focused on classification, [Zhou et al., 2022a] derive some important results which apply to regression as well as to classification. Our UFM analysis leverages this related paper, particularly their Lemma B.1."}, {"title": "Prevalence of neural regression collapse", "content": "We consider the multivariate regression problem with M training examples {(xi, yi), i = 1, . . ., M}, where each input x\u2081 belongs to $R^D$ and each target vector y\u017c belongs to $R^n$. For the regression task, the deep neural network (DNN) takes as input an example x \u2208 $R^D$ and produces an output y = fo,w,b(x) \u2208 $R^n$. For most DNNs, including those used in this paper, this mapping takes the form fo,w,b(x) = Who(x) + b, where ho(\u00b7) : $R^D$ \u2192 $R^d$ is the non-linear feature extractor consisting of several nonlinear layers, W is a n \u00d7 d matrix representing the final linear layer in the model, and b\u2208 $R^n$ is the bias vector. For most neural regression tasks, n << d, that is the dimension of the target space is much smaller than the dimension of the feature space. For univariate regression, n = 1. The parameters 0, W, and b are all trainable.\nWe train the DDN using gradient descent to minimize the regularized L2 loss:\n$\\min_{\\theta,W,b} \\frac{1}{2M} \\sum_{i=1}^{M} \\|f_{\\theta,W,b}(x_i) - y_i\\|^2 + \\frac{\\lambda_{\\theta}}{2} \\|\\theta\\|^2 + \\frac{\\lambda_W}{2} \\|W\\|_F^2,$\nwhere || \u00b7 ||2 and || \u00b7 ||F denote the L2-norm and the Frobenius norm, respectively. As commonly done in practice, in our experiments we set all the regularization parameters to the same value, which we refer to as the weight-decay parameter AWD, that is, we set \u03bb\u06be = dw = AWD.", "subsections": [{"title": "Definition of neural regression collapse", "content": "In order to define Neural Regression Collapse (NRC), let \u2211 denote the n\u00d7 n covariance matrix corresponding to the targets {yi, i = 1,...,M}: \u2211 = $M^{-1}(Y - \\bar{Y})(Y - \\bar{Y})^T$, where Y = [\u04231\u2026\u0443\u043c], \u00dd = [\u04ef\u2026\u04ef], and \u04ef = $M^{-1} \\sum_{i=1}^M y_i$. Throughout this paper, we make the natural assumption that Y and \u2211 have full rank. Thus \u2211 is positive definite. Let Amin > 0 denote the minimum eigenvalue of \u03a3.\nAlso denote H := [h\u2081\u2026hM], where hi is the feature vector associated with input xi, that is, hi := ho(xi). Of course, W, H, and b are changing throughout the course of training. For any p \u00d7 q matrix C and any p-dimensional vector v, let proj (v|C) denote the projection of v onto the subspace spanned by the columns of C. Let HPCA be the d\u00d7 n matrix with the columns consisting of the n principal components of H.\nWe say that Neural Regression Collapse (NRC) emerges during training if the following three phenomena occur:\n* NRC1 = $\\frac{1}{M} \\sum_{i=1}^M \\|h_i - \\text{proj}(h_i | H_{PCA_n})\\|^2 \\rightarrow 0$.\n* NRC2 = $\\frac{1}{M} \\sum_{i=1}^M \\|h_i - \\text{proj}(h_i | W^T)\\|^2 \\rightarrow 0$.\n* There exists a constant \u03b3 \u2208 (0, Amin) such that:\nNRC3 = $\\frac{W W^T}{\\|W W^T\\|_F} - \\frac{\\Sigma^{1/2} - \\gamma^{1/2} I_n}{\\|\\Sigma^{1/2} - \\gamma^{1/2} I_n\\|_F} \\rightarrow 0$.\nNRC1 \u2192 0 indicates that there is feature-vector collapse, that is, the d-dimensional feature vectors hi, i = 1,..., M, collapse to a much lower n-dimensional subspace spanned by their n principal components. In many applications, n = 1, in which case the feature vectors are collapsing to a line in the original d-dimensional space. NRC2 \u2192 0 indicates that there is a form of self duality, that is, the feature vectors also collapse to the n-dimensional space spanned by the rows of W. NRC3 \u2192 0 indicates that the last-layer weights have a specific structure within the collapsed subspace. In particular, it gives detailed information about the norms of the row vectors in W and the angles between those row vectors. NRC3 \u2192 0 indicates that angles between the rows in W are influenced by \u22111/2. If the targets are uncorrelated so that \u2211 and \u22111/2 are diagonal, then NRC3 \u2192 0 implies that the rows in W will be orthogonal. NRC3 \u2192 0 also implies a specific structure for the feature vectors, as discussed in Section 4."}, {"title": "Experimental validation of neural regression collapse", "content": "In this section, we validate the emergence of NRC1-NRC3 during training across various datasets and deep neural network (DNN) architectures."}]}, {"title": "Unconstrained feature model", "content": "As discussed in the related work section, the UFM model has been extensively used to help explain the prevalence of neural collapse in the classification problem. In this section, we explore whether the UFM model can also help explain neural collapse in neural multivariate regression.\nSpecifically, we consider minimizing L(H, W, b), where\n$L(H, W, b) = \\frac{1}{2M} \\|WH + b\\mathbf{1}_M^T - Y\\|_F^2 + \\frac{\\lambda_H}{2M} \\|H\\|_F^2 + \\frac{\\lambda_W}{2} \\|W\\|_F^2,$\nwhere 1 := [1\u00b71] and \u03bb\u1e24, \u03bbw are non-negative regularization constants.\nThe optimization problem studied here bears some resemblance to the standard linear multivariate regression problem. If we view the features hi, i = 1, . . ., M, as the inputs to linear regression, then \u0177\u00bf := Wh\u2081 + b is the predicted output, and ||yi - \u0177i||\u00bd is the squared error. In standard linear regression, the h\u2081's are fixed inputs. In the UFM model, however, not only are we optimizing over the weights W and biases b but also over all the \"inputs\" H.\nFor the case of classification, regularization is needed in the UFM model to prevent the norms of H and/or W from going to infinity in the optimal solutions. In contrast, in the UFM regression model, the norms in the optimal solutions will be finite even without regularization. However, as regularization is typically used in neural regression problems to prevent overfitting, it is useful to include regularization in the UFM regression model as well."}, {"title": "Regularized loss function", "content": "Throughout this subsection, we assume that both Aw and \u03bb\u04a3 are strictly positive. We shall consider the \u03bbw = \u03bb = 0 case subsequently. We also make a number of assumptions in order to not get distracted by less important sub-cases. Throughout we assume n \u2264 d, that is, the dimension of the targets is not greater than the dimension of the feature space. As stated in a previous subsection, for problems of practical interest, we have n << d. Recall that \u2211 is the covariance matrix of the target data. Since \u2211 is a covariance matrix and is assumed to have full rank, it is also positive definite. It therefore has a positive definite square root, which we denote by \u22111/2. Let Xmax := >1 > >2 > \u2026 > \u03bb\u03b7 := Amin > 0 denote the n eigenvalues of \u03a3. We further define the n \u00d7 n matrix\nA := $\\Sigma^{1/2} - \\sqrt{c}I_n,$\nwhere c := \u03bbw\u03bb\u03bc. Also for any p \u00d7 q matrix C with columns C1, C2, . . ., cq, we denote [C]; to be the p x q matrix whose first j columns are identical to those in C and whose last q \u2013 j columns are all zero vectors, i.e., [C]j = [C1 C2 \u00b7\u00b7\u00b7 cj 0\u00b7\u00b7\u00b70]. All proofs are provided in the Appendix.\nTheorem 4.1. Any global minimum (W, H, b) for (1) takes the following form: If 0 < c < >max, then for any semi-orthogonal matrix R,\nW = $\\sqrt[4]{\\frac{\\lambda_H}{\\lambda_W}} [A^{1/2}]_jR, H = \\sqrt[4]{\\frac{\\lambda_W}{\\lambda_H}} W^T \\Sigma [\\Sigma^{1/2}]^{-1} (Y - \\bar{Y}), b = \\bar{y},$"}, {"title": "One-dimensional univariate case", "content": "In this subsection, we highlight the important special case n = 1, which often arises in practice (such as with Carla 1D and the UTKface datasets). When n = 1, \u2211 is simply the scalar \u03c3\u00b2, which is the variance of the one-dimensional targets over the M samples. Also, W is a row vector, which we denote by w. Theorem 4.1, for n = 1 provides the following insights:\nDepending on whether 0 < c < \u03c3\u00b2 or not, the global minimum takes on strikingly different forms. In the case, c > \u03c3\u00b2, corresponding to very large regularization parameters, the optimization problem ignores the MSE and entirely focuses on minimizing the norms ||H||\u00bd and ||w||3, giving ||H||3 = 0, ||w||3 = 0.\nWhen 0 < c < \u03c3\u00b2, the optimal solution takes a more natural and interesting form: For any unit vector e \u2208 Rd, the solution (H, w, b) given by\nwT = $\\sqrt[4]{\\frac{\\lambda_H}{\\lambda_W}} (\\frac{\\sigma}{\\sqrt{\\lambda_H}} - 1) e, H = \\sqrt[4]{\\frac{\\lambda_W}{\\lambda_H}} \\frac{w}{\\sqrt{\\lambda_W}} w^T (Y - \\bar{Y}), b = \\bar{y},$\nis a global minimum. Thus, all vectors w on the sphere given by ||w||2 = ( \u2212 1) are optimal solutions. Furthermore, hi, i = 1,..., M, are all in the one-dimensional subspace spanned by w. Thus the optimal solution of the UFM model provides a theoretical explanation for NRC1-NRC2. (NRC3 is not meaningful for the one-dimensional case.) Note that the hi's have a global zero mean and the norm of hi is proportional to yi - Y."}, {"title": "General n-dimensional multivariate case", "content": "In most cases of practical interest, we will have c < >min, so that [A1/2]j* = A1/2 in Theorem 4.1.\nCorollary 4.2. Suppose 0 < c < >min. Then the global minima given by (3) have the following properties: (i) All of the d-dimensional feature vectors hi, i = 1, . . ., M, lie in the n-dimensional subspace spanned by the n rows of W. (ii) $\\frac{W W^T}{\\|W\\|_F} = \\frac{[\\Sigma^{1/2} - \\sqrt{c} I_n]}{\\|\\Sigma^{1/2} - \\sqrt{c} I_n\\|_F}$, (iii) $\\lambda_H \\|H\\|_F = M \\lambda_W \\|W\\|_F$, (iv) L(H, W,b) = $\\frac{nc}{2} + \\sqrt{c} ||A^{1/2}||_F$, (v) $WH + b\\mathbf{1}_M^T - Y = -\\sqrt{c} [\\Sigma^{1/2}]^{-1} (Y - \\bar{Y})$.\nFrom Theorem 4.1 and Corollary 4.2, we make the following observations:\nMost importantly, the global minima in the UFM solution match the empirical properties (NRC1)-(NRC3) observed in Section 3. In particular, the theory precisely predicts NRC3, with \u03b3 = c. This confirms that the UFM model is an appropriate model for neural regression.\nUnlike the one-dimensional case, the feature vectors are no longer colinear with any of the rows of W. Moreover, after rotation and projection (determined by the semi-orthogonal matrix R), the angles between the target vectors in Y \u2013 Y do not in general align with the angles between the feature vectors in H. However, if the target components are uncorrelated, so that \u2211 is diagonal, then A is also diagonal and there is alignment between H and Y \u2013 Y.\nTheorem 4.1 also provides insight into the \u201cstrong regularization\u201d case of c > >min. In this case, the rows of W and the feature vectors H in the global minima belong to a subspace that has dimension even smaller than n, specifically, to dimension j* < n. To gain some insight, assume that the target components are uncorrelated so that \u2211 is diagonal and \\\u2081 = 0, i.e., of is the variance of the j-th target component. Then for a target component for which c > 03, the corresponding row row in W will be zero and the component prediction will be \u0177i) = y(i) for all examples i = 1, . . ., M. For more details, we refer the reader to Section D.1 in the Appendix."}, {"title": "Removing regularization", "content": "In the previous theorem and corollary, we assumed the presence or L2 regularization for W and H, that is, we assumed w > 0 and > > 0. Now we explore the structure of the solutions to the UFM when w = \u03bb\u2081 = 0. In this case, the UFM model is modeling the real problem with AWD equal to or close to zero. The loss function becomes:\n$L(W, H) = \\frac{1}{2M} \\|WH - Y\\|_F^2.$\nFor this case, we do not need bias since we can obtain zero loss without it.\nTheorem 4.3. The solution (W,H) is a global minimum if and only if W is any n \u00d7 d full rank matrix and\nH = W+Y + (Ia \u2013 W+W)Z,\nwhere W+ is the pseudo-inverse of W and Z is any d \u00d7 M matrix. Consequently, when there is no regularization, for each full-rank W there is an infinite number of global minima (W,H) that do not collapse to any subspace of Rd.\nFrom Theorem 4.3, when there is no regularization, the feature vectors do not collapse. Moreover, any full rank W provides an optimal solution. For example, for n = 2, the two rows of W can have any angle between them except angle 0 and angle 180. This is very different from the results we have for \u03bb\u03b9, \u03bbw > 0, in which case W depends on the covariance matrix \u03a3. Note that if we set > = >w and let > \u2192 0, then the limit of W still depends on \u03a3. Thus there is a major discontinuity in the solution when \u03bb\u03b7, \u03bbw goes to zero. We also observed this phase shift in the experiments (see Figure 4). We can therefore conclude that neural regression collapse is not an intrinsic property of neural regression alone. The geometric structure of neural regression collapse is due to the inclusion of regularization in the loss function."}, {"title": "Empirical results with UFM assumptions", "content": "We also provide empirical results for the case when we train with the same form of regularization as assumed by the UFM model. Specifically, we turn off weight decay and add an L2 penalty on the last-layer features hi, i = 1, ..., M, and on the layer linear weights W. Additionally, we omit the ReLU activation function in the penultimate layer, allowing the feature representation produced by the feature extractor to take any value, thus reflecting the UFM model. For these empirical results, when evaluating NRC3, rather than searching for y as in the definition of NRC3, we use the exact value of y given by Theorem 4.1, that is, y = \u03bbw\u03bb\u043d = c."}, {"title": "Conclusion", "content": "We provided strong evidence, both empirically and theoretically, of the existence of neural collapse for multivariate regression. This extension is significant not only because it broadens the applicability of neural collapse to a new category of problems but also because it suggests that the phenomena of neural collapse could be a universal behavior in deep learning. However, it is worth acknowledging that while we have gained a better understanding of the model behavior of deep regression models in the terminal phase of training, we have not addressed the connection between neural regression collapse and model generalization. This crucial aspect remains an important topic for future research."}, {"title": "Supplementary lemmas", "content": "Let us recall the form of the objective:\n$L(H, W, b) = \\frac{1}{2M} \\|WH + b\\mathbf{1}_M^T - Y\\|_F^2 + \\frac{\\lambda_H}{2M} \\|H\\|_F^2 + \\frac{\\lambda_W}{2} \\|W\\|_F^2,$\nwhere 1 = [1\u00b7\u00b71] and >, >w > 0 regularization constants.\nIn Lemma C.1, we demonstrate that if (H, W, b) is critical for (7), then W can be written as a closed-form function of H and the residual error. In an analogous way, H can be written as a closed-form function of W and the residual error. Furthermore, b = \u04ef, where y is the mean of the targets. In addition, we provide the identity that connects the matrix norms of the two, see (iii) below.\nLemma C.1. i) If (H, W, b) is a critical point of (7), then\nH = $-\\frac{\\lambda_H}{M} W^T (WH + \\bar{Y} - Y),$\nW = $-\\frac{\\lambda_W}{M} (WH + \\bar{Y} - Y)H^T,$\nb = \u04ef.\nii) If (H, W, b) is a critical point of (7), for fixed (H, W), b \u00fd minimizes L(H, W, b).\niii) \u0410\u043d||H|| = M>w||W||.\nProof. i) To prove the first part of the lemma, we will proceed by equating to zero the gradients w.r.t. the variables of the optimization objective L. Those can be written in the form of a matrix in the following way:\n$\\frac{\\partial \\mathcal{L}}{\\partial H} = \\frac{1}{M} W^T (WH + b\\mathbf{1}_M^T - Y) + \\frac{\\lambda_H}{M} H,$\n$\\frac{\\partial \\mathcal{L}}{\\partial W} = \\frac{1}{M} (WH + b\\mathbf{1}_M^T - Y) H^T + \\lambda_W W,$\n$\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{M} (WH + b\\mathbf{1}_M^T - Y) \\mathbf{1}_M.$\nWe set $\\frac{\\partial \\mathcal{L}}{\\partial b} = 0$ in (10) and observe that\nb = $\\frac{1}{M} (Y - WH) \\mathbf{1}_M = \\frac{Y \\mathbf{1}_M}{M} - \\frac{W H \\mathbf{1}_M}{M} = \\bar{y} - W h,$\nrecalling that y = $\\frac{1}{M} \\sum_{i=1}^M y_i$ and h = $\\frac{1}{M} \\sum_{i=1}^M h_i$.\nWe set $\\frac{\\partial \\mathcal{L}}{\\partial W} = 0$ in (9) and observe that\n$\\lambda_W W = -\\frac{1}{M} (WH + b\\mathbf{1}_M^T - Y) H^T.$\nWe set $\\frac{\\partial \\mathcal{L}}{\\partial H} = 0$ in (8) and observe that\n$\\lambda_H H = -W^T (WH + b\\mathbf{1}_M^T - Y),$\n$\\lambda_H h_i = -W^T (W h_i + b - y_i), \\forall i = 1, .., M,$\n$\\lambda_H \\bar{h} = -W^T (W \\bar{h} - \\bar{y}).$\nWe derived (15) by summing both sides of (14) over i, and subsequently dividing them by M. Substituting b for \u04ef \u2013 Wh, see (11), we get\nH = 0,\nb = \u04ef.\nThus, combining (12), (13), and (16) completes the first part of the proof of i).\nii) If (H, W, b) is a critical point of (7), noting that for fixed (H, W), the objective L(H, W, b) is convex w.r.t. b, readily yields that b = \u04ef minimizes L(H, W, b)."}, {"title": "Proof of corollary 4.2", "content": "(i) is derived in the proof of Lemma C.1, see (14) and (16). It is also easy to derive them from the form of H as given in (3). (ii) follows by the form W, see (3). By Lemma C.1(iii) and Lemma C.2, (iii)-(v) follow immediately."}, {"title": "Proof of theorem 4.1", "content": "The proof of Theorem 4.1 leverages [Zhou et al., 2022a, Lemma B.1]. For clarity, we now restate their lemma in our notation.\nn\nLemma D.1. [Zhou et al., 2022a, Lemma B.1] For n, d, M with d > n, and \u00dd := Y \u2212 \u1ef8 \u2208 Rn\u00d7M with SVD given by Y = U\u2211VT = $\\sum_{i=1}^n \\sigma_i u_i v_i^T$, where 01 \u2265 62 > \u2026 > \u03c3\u03b7 \u2265 0 are the singular values, the following problem\n$\\min_{H \\in R^{d \\times M}, W \\in R^{n \\times d}} \\mathcal{L}(H, W, \\bar{y})$\nis a strict saddle function with no spurious local minima, in the sense that\ni) Any local minimum (H, W, \u1ef9) of (7) is a global minimum of (7), with the following form\nWH = U[\u03a3 \u2013 \u221aM>w>HIn]+VT.\nCorrespondingly, the minimal objective value of (7) is\n$\\mathcal{L}(H, W, \\bar{y}) = \\frac{1}{2M} \\sum_{i=1}^n (\\sigma_i - \\eta_i)^2 + \\sqrt{M \\lambda_W \\lambda_H} \\sum_{i=1}^n \\eta_i,$\nwhere ni := 7\u2081(\u03bb, \u03bbw) is the i-th diagonal entry of [\u2211 \u2013 \u221aM>w>HIn]+.\nii) Any critical point (H, W, \u1ef9) that is not a local minimum is a strict saddle point with negative curvature, i.e., the Hessian at this critical point has at least one negative eigenvalue.\nLet Y = Y \u2013 Y = U\u017dVT = $\\sum_{i=1}^n \\hat{\\sigma}_i u_i v_i^T$, denote the compact SVD of \u1ef8 \u2208 Rn\u00d7M, where 01 \u2265 02 > \u2026 > \u03c3\u03b7 > 0 are the singular values, and \u2211 \u2208 Rn\u00d7n is diagonal, containing the aforementioned singular values. Furthermore, U \u2208 Rn\u00d7n, V \u2208 RM\u00d7n are orthogonal and semi-orthogonal respectively, i.e., UUT = UTU = In and VTV = In respectively. For the proof, recall the value of c = \u03bbw\u03bb\u03bc.\nProof of Theorem 4.1. Let (H, W, \u1ef9) be a global minimum of (1). By Lemma D.1, (H, W, \u1ef9) has the following form:\nWH = U[\u03a3 \u2013 \u221a McIn]+VT.\nIn light of Lemma C.1 and the identity >\u1e24||H|| = M>w||W||, from (23), we have that\nW = $\\sqrt[4]{\\frac{\\lambda_H}{M \\lambda_W}} [\\Sigma - \\sqrt{M}c I_n] R,$\nH = $\\sqrt[4]{\\frac{M \\lambda_W}{\\lambda_H}} R^T [\\Sigma - \\sqrt{M}c I_n] U^T \\bar{V},$\nfor all R \u2208 Rn\u00d7d such that RRT = In. Furthermore, using the SVD of Y = U\u017dVT,\n$\\Sigma = \\frac{YY^T}{M} = \\frac{U \\hat{\\Sigma} \\hat{\\Sigma}^T U^T}{M},$\nwhich deduces \u22111/2 = $U \\frac{\\hat{\\Sigma}}{\\sqrt{M}} U^T$. Since UT = U-1, this further yields\n$\\sqrt{M} [\\Sigma^{1/2} - \\sqrt{c} I_n] = U [\\Sigma - \\sqrt{M}c I_n] U^{-1},$\nwhich implies that the matrices \u221aM[\u22111/2 \u2013 \u221acIn] and \u2211 \u2013 \u221a McIn are similar. As a result, they have the same eigenvalues. The n\u00d7 n matrix on the left-hand side of (26) has eigenvalues given by"}, {"title": "Examples for theorem 4.1 (uncorrelated target components)", "content": "In this subsection, we examine closely the case when n = 3 and the target components are uncorre-lated. This simplifies considerably the problem as now the covariance matrix \u2211 is a diagonal matrix with entries given (in order) by 03, where of denotes the variance of the j-th target component, for j = 1, 2, 3. The unique positive definite and symmetric matrix A1/2, see (18), is given by\nA1/2 =\n$\n(01-\u221ac)\n0\n0\n\n0\n(02-\u221ac)\n0\n\n0\n0\n(03-\u221ac)\n,\nWithout loss of generality, assume that \u03c3max:= \u03c31 \u2265 \u03c32 \u2265 \u03c33 := \u03c3min > 0.\nIf 0 < c < \u03c3min = \u03c33, by Theorem 4.1, j* = 3, and therefore any global minimum (H, W, b) of (7) takes the following form:\nW = 4\u221a\u03bb\u03bbH\\sqrt {\u2212 (\u03a31/2\u2212c),b=\n,H=4\u221a\u03bbh\u03bbwWT(Y),foranysimi orthogonalmatrixR\u2208R3\u00d7d.TheformofA1/2,see(30),readilyyields\\wTj=\u03bb\u221a\u03c3j\u2212cTje,\nj=1,2,3,\nc.f., (4), where {ej : j = 1, 2, 3} is any collection of vectors lying in Rd such that ej is a unit vector, for all j = 1, 2, 3, and ek is orthogonal to ek', for all k \u2260 k'.\nTo interpret the landscape of global minima in the case when the target components are uncorrelated, the UFM \u201cforces\u201d the angle between the weight matrix rows to be \u03c0/2 (fixes the weight matrix rows to be orthogonal). Then, the configuration of the bwj's is exactly as in the 1-dimensional target case, that is those are restricted to lie on spheres of certain radiuses. The feature vector hi that corresponds to the i-th training example is then on the 3-dimensional subspace spanned by w1, w2 and w3.\nIf \u03c3min < c < \u03c3max, by Theorem 4.1, j* = 1 or or j* = 2. We analyze the latter, in which case c < \u03c31, c < \u03c32 but c > \u03c33. By Theorem 4.1, any global minimum (H, W, b) of (7) takes the form below:\nW = 4\u221a\u03bbA\u221a \u221a1/2 \u221a c0 0Hw\u03bbWT(Y,b=\u03bd\u03a3 \u221a c0 0,H=4\u221a\u03bbw\u03bbHWT(Y),b=\n\u03c3,j=1,2,\\wT=0,"}, {"title": "Proof of theorem 4.3 (no regularization)", "content": "We first show\nmin L(W, H) = 0\nW,H\nClearly L(W, H) \u2265 0 for all W and"}]}