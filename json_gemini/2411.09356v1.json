{"title": "Multi-scale Generative Modeling for Fast Sampling", "authors": ["Xiongye Xiao", "Shixuan Li", "Luzhe Huang", "Gengshuo Liu", "Trung-Kien Nguyen", "Yi Huang", "Di Chang", "Mykel J. Kochenderfer", "Paul Bogdan"], "abstract": "While working within the spatial domain can pose problems associated with ill- conditioned scores caused by power-law decay, recent advances in diffusion-based generative models have shown that transitioning to the wavelet domain offers a promising alternative. However, within the wavelet domain, we encounter unique challenges, especially the sparse representation of high-frequency coefficients, which deviates significantly from the Gaussian assumptions in the diffusion process. To this end, we propose a multi-scale generative modeling in the wavelet domain that employs distinct strategies for handling low and high-frequency bands. In the wavelet domain, we apply score-based generative modeling with well-conditioned scores for low-frequency bands, while utilizing a multi-scale generative adversarial learning for high-frequency bands. As supported by the theoretical analysis and experimental results, our model significantly improve performance and reduce the number of trainable parameters, sampling steps, and time. The source code is available at https://anonymous.4open.science/r/WMGM-3C47.", "sections": [{"title": "Introduction", "content": "Generative models (GMs) have revolutionized the machine learning field by their ability to create novel, high-quality data that closely mimic real-world distributions. Many GM frameworks have been proposed, such as variational autoencoders (VAE) [2], recurrent neural networks (RNNs) [22], generative adversarial networks (GANs) [14], and hybrid approaches (i.e., combining strategies). Such models have been applied to generate high-quality audio waveforms or speech [33], constructing natural-looking images [14, 4, 26], generating coherent text [3], and designing molecules [13]. Score-based generative models (SGMs) [43, 21, 42], also known as denoising diffusion models, encode the probability distributions through a scoring approach (e.g., a vector field that points in the direction of increasing likelihood of data) and recover the actual data distribution through a learnable reverse process to the forward Gaussian diffusion process. Although these SGMs have been highly successful, many physical, chemical, biological, and engineering systems have complex multiscale and non-Gaussian properties, leading to ill-conditioned scores [19].\nTo make the discussion more concrete, let us consider the noise-adding process in the frequency domain (i.e., wavelet domain), where noise contains a uniform power spectrum in each frequency band. However, in the wavelet domain, the high-frequency coefficients of natural images are sparse and contain minimal energy, while the low-frequency coefficients encapsulate most of the energy, a distribution characteristic that mirrors the power law decay of the power spectrum of natural images. Given the disparity between image and noise power spectra, low-frequency components, which hold the majority of the energy, receive the same magnitude of noise during the noise addition process, and high-frequency coefficients, despite being sparse, obtain a relatively larger amount of noise. This dynamics, also depicted in Fig. 1, offers inspiration for analyzing diffusion in the frequency domain, incorporating corresponding generative strategies tailored for each frequency sub-domain (namely,"}, {"title": "Related Work", "content": "Diffusion models have emerged as state-of-the-art generative models, which are stable and capable to generate high fidelity images. They use Markov chains to gradually introduce noise into the data and learn to capture the reverse process for sampling. To optimize the reverse process, Denoising Diffusion Probabilistic Models (DDPMs) [21, 10] predict the added noises from the diffused output at arbitrary time steps. Another common approach is score-based generative models (SGMs) [42, 43], which aims to predict the score \\nabla log px\u2081. However, a significant drawback of diffusion models is their lengthy sampling time. In response, recent research has explored methods to expedite this process, leading to more efficient sampling strategies [49, 47, 39, 44, 30].\nEmerging works attempt to incorporate the wavelet domain into diffusion models, either to facilitate the training or to speed the sampling process. WSGM Guth et al. [19] proposes a multi-scale score- based diffusion model and theoretically analyzes the advantages of wavelet-domain representation."}, {"title": "Methodology", "content": ""}, {"title": "Preliminaries: Ill-conditioned Score in Spatial Domain", "content": "For the Score-based Generative Model (SGM) [43, 21, 42], the forward/noising process can be mathematically formulated as the Ornstein-Uhlenbeck (OU) process. The general time-rescaled OU process can be written as follows:\n$$dXt = -g(t)^2X+dt + \\sqrt{2g(t)}dBt.$$\nHere, (Xt)t\u2208 [0,T] is the noising process with the initial condition Xo sampled from the data distribu- tion and (Bt)t\u2208 [0,T] is a standard d-dimensional Brownian motion. We use X\u0142 to denote the reverse process, such that (X)t\u2208 [0,T] = (XT\u2212t)t\u2208[0,T]\u00b7 With the common assumption that g(t) = 1 in standard diffusion models, the reverse processes are as follows:\n$$dX = (X + 2\\nabla log p_{T-t}(X)) dt + \\sqrt{2}dBt,$$\nwhere pt denotes the marginal density of Xt, and \\nabla log pt is called the score. To generate Xo from XT via the time-reversed SDE, it is essential to accurately estimate the score \\nabla log pt at each time t and to discretize the SDE with minimal error.\nAn approximation of the reverse process, as given in Eq. 2, can be computed by discretizing time and approximating \\nabla log pt by a score matching loss st. This results in a Markov chain approximation of the time-reversed Stochastic Differential Equation (SDE). Accordingly, the Markov chain is initialized by \u0161\u012b ~ N(0, Ia) and evolves over uniform time steps \u2206t that decrease from t= T to t= 0. The discretized process is as follows:\n$$xt\u22121 = xt + \u2206t (xt + 2st(xt)) + \\sqrt{2\u2206t}zt,$$\nwhere zt is a realization of Brownian motion Bt.\nTo explore the impact of the regularity of the score log pt on the discretization process in Eq. 3, let us assume that the data distribution is Gaussian, p = N(0, \u03a3), with a covariance matrix \u03a3 in a d-dimensional space. Consider pt as the distribution corresponding to xt, the approximation error between the data distribution p and po stems from (i) \u03a8\u0442: the misalignment between the distributions of xy and xt; (ii) \u03a8\u2206t: time discretization error. The sum of two errors & is connected to \u2211, specifically in the case of uniform time sampling with intervals \u2206t. We standardize the signal energy by enforcing Tr(\u03a3) = d and define k as the condition number of \u03a3.\nTheorem 1. Suppose the Gaussian distribution p = N(0, \u03a3) and distribution po from time reversed SDE, the Kullback-Leibler divergence between p and p\u00f5 relates to the covariance matrix \u2211 as:\n$$KL(p || \u0440\u043e) \u2264 \u03a8\u0442 + \u03a8\u2206t + \u03a8\u0442,\u2206t,$$\nwith:\n$$\u03a8\u0442 = f (e^{-4T} |Tr ((\u03a3 \u2013 Id)\u03a3)|),$$\n$$\u03a8\u2206t = f \\Big( \u2206t Tr \\Big(\\frac{\u03a3}{2}\u03a3^{-1} \u2013 \u03a3(\u03a3 \u2013 Id)^{-1} + \\frac{(Id \u2013 \u03a3\u03a3^{-1})^2}{3}\\Big)\\Big)$$\n$$\u03a8\u0442,\u2206t = 0 (\u2206t + e^{-4T}), \u2206t \u2192 0,T \u2192 +\u221e$$\nwhere f(t) = t \u2013 log(1 + t) and d is the dimension of \u03a3, Tr (\u03a3) = d."}, {"title": "Diffusion in Wavelet Domain", "content": "Here we demonstrate the duality between the spatial and wavelet domains in terms of diffusion processes. Subsequently, we delve into the well-conditioning properties inherent to the low-frequency coefficients and the sparsity of the high-frequency coefficients, serving as a pivotal inspiration for our proposed model. Technical details on the wavelet transform and multiresolution analysis (MRA) have been provided in Appendix B."}, {"title": "Duality between Spatial Domain and Wavelet Domain", "content": "The discrete wavelet transform (DWT), such as Haar DWT, and we can smoothly write the DWT as:\n$$\\tilde{X} = AX, \\quad X\\in R^d.$$\nIn this expression, X represents the wavelet coefficients of the signal X in Rd, and A denotes the discrete wavelet matrix. Importantly, A is an orthogonal matrix, satisfying the condition AAT = I, where I is the identity matrix. Following this transformation, we introduce the processes for score- based generative modeling in the wavelet domain.\nWhen we explore the forward (noising) process within score-based generative models, particularly after applying the Discrete Wavelet Transform (DWT) to the initial data Xo, we observe that the representation in the wavelet domain, denoted by Xt, similarly follows an Ornstein-Uhlenbeck (OU) process akin to that in the spatial domain as shown in Eq. 1. This observation underscores the consistency of the noising process across different representations of data. Based on this, we can succinctly define the forward and reverse process in the wavelet domain as follows:"}, {"title": "Well-conditioning Property of Low-frequency Coefficients", "content": "A signal x = x[n] with index n can be decomposed as low-frequency and high-frequency coefficients. Low-frequency coefficients, often referred to as the \"base\" or \"smooth\" components, capture the primary structures and general trends within the data:\n$$x_l^H = (H*x)_l = \\sum_{n=0}^{N-1} f[n]\u00b7 \u03c6_{\u03ba,\u03b9} [\u03b7],$$"}, {"title": "Sparsity of High-frequency Coefficients", "content": "Within the wavelet domain, the high-frequency coefficients usually represent rapid changes or transitions of signals, which are sparse in natural images. The sparsity in the high-frequency band primarily results from the inherent characteristics of the natural images and is well summarized by the power-law decay: most of the information and energy are concentrated in the low-frequency band, while the high-frequency band captures sparse fine textures and a small amount of energy. For a given shift parameter l, the high-frequency signal can be represented in discrete form as:\n$$x_l^H = (G^kx)_l = \\sum_{n=0}^{N-1} x[n] \u00b7 \u03c8_{\u03ba,\u03b9} [\u03b7],$$\nwhere \u03c8k,1[n] represents the discrete wavelet function and G is the high-pass operator.\nIt is well known that natural images intrinsically present distinct distribution patterns that are not Gaussian in nature. In fact, the wavelet coefficient distribution of natural images is often modeled as a Generalized Laplacian, Gaussian scale mixture, or Generalized Gaussian scale mixture [40, 46, 17, 5]. While the whitening effect is applied to the low-frequency coefficients, the high-frequency signals, represented by the long-tail and peak features of the original distribution are magnified and distilled in the high-frequency bands at each scale through the differential operations. Consequently, high- frequency coefficients are usually sparse (see detailed proofs in Appendix E.2)."}, {"title": "Wavelet Multi-scale Generative Model", "content": "According to the analysis of wavelet coefficients features in Appendix E, we formulate the multi-scale factorization of the generative model in the wavelet domain and design the architecture as shown in Fig. 2. We start with the multi-scale wavelet decomposition and factorize the probability of target image p(x) as below:\n$$p(x) = \\prod_{k=1}^S p(x_l^k)p(x_H^k|x_l^k).$$\nNotice the LL band at a finer scale k is jointly determined by the low-frequency and high-frequency bands at scale k + 1, i.e.,\n$$x_l^k = AT (x_l^{k+1}, x_H^{k+1})^T.$$\nHere, p(x_l^k) can be effectively approximated through the reverse diffusion process due to the whitening effect of multi-scale wavelet decomposition. On the other hand, the conditional probability of high-frequency bands on the LL band p(x_H^k|x_l^k) is often a long-tail and peaked multimodal distribution and differs from simple Gaussian models considerably (See Appendix A.5 for the proof). A more detailed depiction of this conditional distribution can be derived using the Gaussian scale mixture modeling the joint distribution of wavelet coefficients and the Gaussian-like distribution of the LL band.\nScore-based generative model in the coarsest wavelet layer. A deep neural network se approxi- mates the score function in the reverse process [21, 43, 42]. Following Eq. 14, the loss function for"}, {"title": "Implementation", "content": "For training, the input images are based on 128\u00d7128 resolution. We use the Adam optimizer [27] with the learning rate 10-4 for the diffusion model, and AdamW optimizers [29] for the generator and discriminator using learning rates of 10\u20134 and 10-5, respectively. The diffusion model is trained with a batch size of 64 for 50000 iterations, while the generator and discriminator are trained with a batch size of 128 for 150 epochs. For evaluation metrics, we use the Fr\u00e9chet inception distance (FID) [20] to measure the image quality.The sampling time is averaged over 10 trials when generating a batch of 64 images. The training code and model weights are publicly available. All tasks are conducted on a NVIDIA V100 GPU."}, {"title": "Results and Comparison", "content": "For our experiments, we used three diverse datasets: CelebA-HQ, AFHQ, and the Colon dataset (See Appendix H for more details). We assess the proposed Weighted Multiscale Generative Model (WMGM) across varied contexts, focusing on its fast sampling capabilities and universality. We ensure consistent experimental settings for all models across all datasets, using the FID as our primary evaluation metric. We showcase our method's fast sampling capabilities through experiments on varying datasets and with different numbers of discretization steps, demonstrating the WMGM's adaptability and efficiency.\nVarying datasets: With a consistent setting of 16 discretization steps, our model outperforms both SGM [43] and WSGM [19], as demonstrated in Table 1. This superior performance is achieved alongside significant reductions in model size and inference time. Specifically, our model, which combines a diffusion process and GAN-based sampling, requires only 89M parameters, making it"}, {"title": "Multi-scale Adversarial Learning", "content": "To efficiently construct the mappings from low-frequency to high-frequency coefficients among multi-scale (MS) and single-scale (SS), we compare different learning approaches including standard learning (SL), operator learning (OL) and adversarial learning (AL) on dataset CelebA-HQ (5k) with training picture number of 4750 and valid picture number of 250. For detailed definition of MS and SS, please refer to Appendix I. As shown in the Table 2, the AL model demonstrates significant advantages over other models. Also, among the same model, multi-scale performance is better than single scale with half model parameters, indicating that the function mapping relationships in different scales can capture and relate features more effectively, thereby improving generation quality."}, {"title": "Conclusion", "content": "In summary, our work directly addresses the challenges of ill-conditioning in score-based generative models within the spatial domain and navigates the intricacies of the wavelet domain, leading to the introduction of the Wavelet Multi-Scale Generative Model (WMGM). We clarify the duality of the diffusion process between the spatial and wavelet domains and delve into the characteristics of wavelet coefficients. Our model innovatively capitalizes on the whitening effect in low-frequency coefficients and integrates a multi-scale adversarial learning (MSAL) to effectively manage the non-Gaussian distribution of high-frequency wavelet coefficients. A pivotal aspect of our model is its emphasis on fast sampling, a critical advancement that positions WMGM notably ahead of existing approaches in terms of sampling efficiency and image generation quality."}, {"title": "Proof of Theorem 1", "content": "Theorem 5. Let N \u2208 N, \u2206t > 0, and T = N\u2206t. Then, we have that \u00d1 ~ N(\u00b5\u00d1, \u03a3\u00d1) with\n$$\u03a3\u00d1 = \u03a3 + exp(\u22124T)\u03a3\u00ce + \u2206t\u03a8\u2191 + (\u2206t)\u00b2RT,\u2206t,$$\n$$\u00b5\u00d1 = \u00b5 + exp(\u22122T)\u00b5\u03c4 + \u2206te\u2191 + \\frac{(\u2206t)\u00b2}{2} rT,\u2206t,$$\nwhere \u03a3\u00ce, \u03a8\u2191, RT,\u2206t \u2208 Rd\u00d7d, \u00b5\u03c4, e\u00ce, rT,\u2206t \u2208 Rd, and ||RT,\u2206t|| + ||rT,\u2206t|| < R, not dependent on T > 0 and At > 0. We have that\n$$\u03a3\u00ce = \u2212(\u03a3 \u2013 Id)(\u03a3\u03a3\u22121)2,$$\n$$\u00b5\u03c4 = \\frac{1}{4}Id \u2212 \u03a3\u00b2(\u03a3 \u2013 Id)\u22121 log(\u03a3) + exp(\u22122T)\u03a8\u0622.$$\nIn addition, we have\n$$\u00b5\u03c4 = \u2212\u03a3\u22121\u03a4\u03a3\u00b5,$$\n$$ef=\\Big\\{-2\u03a3^{-2}\u03a3^{-1} - \u03a3(\u03a3 \u2013 Id)^{-1} log(\u03a3)\\Big}\u00b5+exp(-2T)\u00b5\u03c4,$$\nwith \u03a8\u2191, \u00b5\u03c4 bounded and not dependent on T. Please refer to Proof S5 in [19] for the detialed proof outline for Theorem 1 and Proposition 1, based on the above Theorem 5.\nTheorem 6. Suppose that \u2207 log pt(x) is p\u00b2 in both t and x such that:\nsup ||\u2207\u00b2 log pt(x)|| \u2264 K, ||\u2202t\u2207log pt(x)|| \u2264 Me\u2212at||x||,\nfor some \u039a, \u039c, a > 0. Then, ||p \u2013 po||tv < \u03a8\u0442 + \u03a8\u2206t + \u03a8\u0442,\u2206t, where:\n$$\u03a8\u0442 = \\sqrt{2}e^{-T} KL (p || N(0, Id))1/2$$\n$$\u03a8\u2206t = 6\\sqrt{\u2206t} [1 + Ep (||x||4)1/4] [1 + K + M (1 + 1/20)1/2]$$\n$$\u03a8T,At = 0 (\\sqrt{\u2206t} + e^{-T}) \u2206t \u2192 0,T \u2192 +\u221e$$\nTheorem 6 provided in [19] generalizes Theorem 1 to non-Gaussian processes."}, {"title": "Wavelet Transform", "content": "The wavelets represent sets of functions that result from dilation and translation from a single function, often termed as 'mother function', or \u2018mother wavelet'. For a given mother wavelet f(x), the resulting wavelets are written as\n$$\u03c8a,b(x) = \\frac{1}{|a|^{1/2}} f(\\frac{x-b}{a}) a, b \u2208 R, a \u2260 0, x \u2208 D,$$\nwhere a, b are the dilation, translation factor, respectively, and D is the domain of the wavelets under consideration. In this work, we are interested in the compactly supported wavelets, or D is a finite interval [m, n], and we also take \u03c8 \u2208 L\u00b2. For the rest of this work, without loss of generality, we restrict ourself to the finite domain D = [0, 1], and extension to any [m, n] can be simply done by making suitable shift and scale."}, {"title": "1D Discrete Wavelet Transform", "content": "Multiresolution analysis (MRA). The Wavelet Transform employs Multiresolution Analysis (MRA) as a fundamental mechanism. The basic idea of MRA is to establish a preliminary basis in a subspace Vo of L\u00b2(R), and then use simple scaling and translation transformations to expand the basis of the subspace Vo into L2(R) for analysis on multiscales."}, {"title": "2D Discrete Wavelet Transform", "content": "The 2D DWT is a pivotal technique for image analysis. Unlike the traditional Fourier Transform which primarily provides a frequency view of data, the 2D DWT offers a combined time-frequency perspective, making it especially apt for analyzing non-stationary content in images.\nFor a given image I(x, y), its wavelet transform is achieved using a pair of filters: low-pass and high- pass, followed by a down-sampling operation. The outcome is four sets of coefficients: approximation coefficients, horizontal detail coefficients, vertical detail coefficients, and diagonal detail coefficients.\nThis transformation can be mathematically represented as:\nApproximation Coefficients:\nLL = DWT(I(x, y) * h(x) * h(y))\nHorizontal Detail Coefficients:\nLH = DWT(I(x, y) * h(x) * g(y))\nVertical Detail Coefficients:\nHL = DWT(I(x, y) * g(x) * h(y))\nDiagonal Detail Coefficients:\nHH = DWT(I(x,y) * g(x) * g(y))\nwhere h(x, y) and g(x, y) are the low-pass and high-pass filters, respectively."}, {"title": "Haar Wavelet Transform", "content": "As one of the most basic wavelets, Haar wavelet has simplicity and orthogonality, ensuring its effective implementation in digital signal processing paradigms. The straightforward and efficient characteristics of Haar wavelets make them highly practical and popular for various applications, and their successful implementation in other works prompted us to utilize them in our project as well. When we perform wavelet decomposition on an image using the discrete wavelet transform (DWT), we can obtain an approximate representation that captures the main features or overall structure of the image, as well as finer details that capture the high-frequency information in the image. Through further multi-resolution analysis (MRA), we can view the image at different scales or levels, resulting in a view containing more detail.\nThe Haar wavelet and its associated scaling function:\n$$\u03c8(t) = \\begin{cases} 1 & \\text{for } 0 < t < 0.5 \\\\ -1 & \\text{for } 0.5 < t < 1, \\\\ 0 & \\text{elsewhere} \\end{cases} ,$$\n$$\u03c6(t) = \\begin{cases} 1 & \\text{for } 0 \\le t < 1 \\\\ 0 & \\text{elsewhere} \\end{cases} ,$$"}, {"title": "Diffusion in Wavelet Domain", "content": "We first investigate the effect of noise evolution in the spatial domain. In our image perturbation analysis, we observed the ramifications of incrementally introduced Gaussian noise in the spatial domain. Initially, the superimposed Gaussian noise manifests predominantly as high-frequency perturbations. With an increase in the strength and duration of noise injection, these perturbations start to mask the primary structures of the original image, causing the entire image to be progressively characterized by Gaussian noise attributes. This renders the image increasingly homogeneous, dominated by high-frequency disturbances.\nThe influence of noise evolution extends beyond the spatial domain, and its progressive perturbation to different frequency bands of the image can be better understood from a wavelet domain perspective. When the perturbed images are subjected to a wavelet transform, we noted a series of effects:"}, {"title": "Duality of Diffusion Process in Spatial Space and Wavelet Domain", "content": ""}, {"title": "Forward Process", "content": "To simplify the notations, let \u00e6 be an image vector, and we can write the discrete wavelet transform (DWT) as:\n$$X = AX, \\quad X \\in R^d.$$\nHere, A is the discrete wavelet matrix. This matrix is an orthogonal matrix, i.e., AAT = I. Several choices of A are widely applied, such as Haar wavelets.\nFor the score-based generative modeling process, we consider the forward/noising process. This process can be mathematically formulated as the Ornstein-Uhlenbeck (OU) process. The general time-rescaled OU process can be written as\n$$dX\u2081 = \u2212g(t)^2X+dt + \\sqrt{2g(t)}dBt.$$\nHere, Bt is a standard d\u2013dimensional Brownian motion. We perform DWT to Xt and figure out Xt also observes the same OU process.\n$$dxt = -g(t)^2X+dt + \\sqrt{2g(t)}AdBt, \\quad X = AX0.$$\nLet Bt = ABt, Bt is also a standard Brownian motion. We let Xo be sampled from distribution p. Then Xo is from the distribution\n$$q = TA#p.$$\nHere, TA is the A linear transform operation, and # is the pushforward operation, which gives\n$$q(x) = p(ATx).$$\nLet pt be the density distribution of Xt, qt be the density distribution of Xt. We have\n$$qt=Ta#Pt, \\quad qt(x) = pt(ATx).$$\nLet\n$$St = \\nabla log Pt, \\quad rt = \\nabla log qt.$$\nbe the score functions of two processes. We have\n$$rt(x) = \\frac{\\nabla qt(x)}{qt(x)} = A \\frac{A\\nabla pt(ATx)}{Pt(ATx)} = A S_{t}(A^T x).$$"}, {"title": "Denoising/Reverse Process", "content": "We use X and X to denote the reverse process. With the common assumption that g(t) = 1 in standard diffusion models, the reverse processes follow:\n$$dX = (X+2sT-t(X+)) dt + \\sqrt{2}dBt,$$\n$$dxt = (x+2rT-t(x)) dt + \\sqrt{2}dBt.$$\nHere, B\u2081 = ABt. We look into the second SDE.\n$$dx = (x+2rT-t(X)) dt + \\sqrt{2}dBt$$\n$$= (x+2AST-(ATX)) dt + \\sqrt{2}dBt$$\n$$Adx = (A+X+2sT-t(ATX)) dt + \\sqrt{2}A dB.$$\nReplacing ATX by X. We can get back to the first equation. The training processes for so, rwith X(), X() also following the same standard denoising score matching loss function as follows:\n$$Et {x(t)Ex\uff61Ex:\\x0 [||80(Xt, t) \u2013 \u2207X log Pot (Xt|xo)||2]}$$\n$$Et {x(t)ExEx, *o [||ra(xt, t) \u2013 \u2207x log got (\u0160, \u0160o)||"}, {"title": "Analysis of Wavelet Coefficient Features", "content": ""}, {"title": "Gaussian Tendency of Low-Frequency Coefficients in Higher Scales", "content": "We first experimentally showcase the KL divergence between sample distribution of low-frequency coefficients and standard Gaussian distribution on CelebA-HQ and AFHQ-Cat datasets. 2-scale wavelet decomposition was implemented to each image, and the sample mean and covariance were calculated accordingly to the raw image (scale 0) and LL subbands at scale 1 and 2. The KL divergence of sample distribution to standard Gaussian is detailed in I.\nIn an image, pixel intensities are represented as random variables, with adjacent pixels exhibiting correlation due to their spatial proximity. This correlation often follows a power-law decay:\n$$C(d) = \\frac{1}{(1+ad)^{\\beta}}$$\nwhere C(d) is the correlation between pixels separated by distance d, and a and \u03b2 characterize the rate of decay.\nThe wavelet transform (i.e., Haar wavelet transform), particularly its down-sampling step, increases the effective distance d among pixels, thereby reducing their original spatial correlation. This reduction is crucial for applying the generalized Central Limit Theorem [37, 11], which requires that the individual variables (pixels, in this case) are not strongly correlated.\nAt scale k in the wavelet decomposition, the low-frequency coefficients, Xk, representing the average intensity over nk pixels, are calculated as:\n$$X_k = \\frac{1}{n_k}(X_1 + X_2 + \u00b7\u00b7\u00b7 + X_{n_k}),$$\nwhere nk is the number of pixels in each group at scale k.\nAs the scale increases, the effect of averaging over larger groups of pixels, combined with the reduced correlation due to down-sampling, leads to a scenario where the generalized Central Limit Theorem can be applied. Consequently, the distribution of Xk tends towards a Gaussian distribution:\n$$X_\u03ba - N(\\mu_\u03ba, \\frac{\\sigma_\u03ba^2}{n_k}),$$\nwhere \u03bc\u03b5 and \u03c3\u03b5 are the mean and variance of the averaged intensities at scale k, respectively. This Gaussian tendency becomes more pronounced at higher scales due to the combination of reduced pixel correlation and the averaging process."}, {"title": "Sparse Tendency of High-frequency Wavelet Coefficients", "content": "By examining the statistical sparsity of images in the CelebA-HQ dataset, we show that the distribution of high-frequency wavelet coefficients is highly non-Gaussian. For a given image \u00e6 and threshold t, the sparsity of its high-frequency coefficients at k-scale is defined as:\n$$s(x_i^k) = \\frac{||1{|x_i^k| \\leq t}||}{L^2}, k = 1,2,...$$\nHere ||.|| is the norm counting the number of 1s in the vector. In this way, we could estimate the expected sparsity of the true marginal distribution p(x). Considering that the LL coefficients with approximate Gaussian distribution given the whitening effect of wavelet decomposition, we have the following proposition.\nProposition 2. For a sufficiently large k, if the expected sparsity of x\u2081 has a lower bound a\n$$E(s(x_i^k)) \u2265 \u03b1,$$\nwhere a \u2208 [0,1]. Then the conditional expected sparsity of xon x is bounded by\n$$E(s(x_i^k)|x_i^k) \u2265 \u03b1 - \u03b5,$$\nwhere \u025b > 0 is a small positive number determined by k."}, {"title": "WMGM Training and Sampling Algorithms Description", "content": "The training and sampling algorithms for WMGM."}, {"title": "Datasets Description", "content": "CelebA-HQ [28] dataset is an extension of the original CelebA [28] dataset. It contains high-quality images of celebrity faces at a higher resolution compared to the original CelebA dataset. This dataset was created to cater to the needs of tasks that require high-resolution facial images. The images in the CelebA-HQ dataset are typically at a resolution of 1024 \u00d7 1024 pixels, providing a significant improvement in image quality compared to the original CelebA dataset, which had lower-resolution images. Similar to the original CelebA dataset, CelebA-HQ comes with a set of facial attribute annotations. These annotations include information about attributes such as gender, age, and presence of accessories like glasses. CelebA-HQ also contains a substantial number of images. While the exact number may vary depending on the specific release, it generally consists of thousands of"}, {"title": "Multi-scale (MS) and Single-scale (SS) Learning", "content": ""}, {"title": "Definition", "content": "Here, we define two different model scales: Multi-scale and Single-scale. Multi-scale refers to the process where a single model is used during training to learn the mapping from low-frequency to high-frequency information across different scales. As depicted in Figure 2, the GAN used in Scale2 and Scalel is the same and shares parameters. Single-scale, on the other hand, means that during training, a separate model is trained for each scale to perform the mapping, with no parameter sharing between the models."}, {"title": "Learning Approaches", "content": "Standard learning. Here, we utilized the Unet as the representative model for standard learning in our experiments, comprising 5 encoder layers and 4 decoder layers. Each encoder layer is composed by 2 Convolution Layers with kernel_size = 2, kernel_size = 4 and padding = 1, followed by batch normalization and ReLU activation. The output of each encoder layer is progressively downsampled through a max pooling layer. The decoder part includes transposed convolution operations for upsampling, where the features are concatenated with the corresponding encoder outputs after each upsampling, followed by processing through a Convolution module.\nOperator learning. Here, we utilized the Fourier Neural Operator (FNO) as the representative model for operator learning in our experiments, adopting the default parameter settings: n_modes = (16, 16) and hidden_channels = 256. The total number of parameters for a single model is 75,896,329.\nAdversarial learning. Here, we utilized the Generative Adversarial Network (GAN) as the represen- tative model for Adversarial Learning in our experiments. Detailed model information can be found in ?? as well as the source code."}, {"title": "Evaluation Metrics", "content": ""}, {"title": "Kullback-Leibler Divergence", "content": "We utilized KL divergence to quantify the similarity between the distribution of LL coefficients and the standard Gaussian distribution No = N(0, I). Suppose the image dataset of interest has N images X\u00bf \u2208 RL\u00b2, i = 1, 2, . . ., N. We could calculate the sample mean and covariance matrix as below:\n$$\\mu = \\frac{\\sum_{i=1}^{N} X_i}{N}$$\n$$\\Sigma = \\frac{1}{N} \\sum_{i=1}^{N} X_i X_i^T  - \\mu \\mu^T$$\nDue to the non-negative data range of image data, we perform pixel-wise normalization to the sample mean and covariance. Denote A = (diag(\u03a3))\u22121/2, the normalized sample mean and covariance are\n$$\\bar{\\mu} = \u039b \\bar{\\mu}$$\n$$\\bar{\\Sigma} = \u039b^{-1} \u03a3 \u039b^{-1}$$\nBy CLT, we know the normalized sample distribution can be approximated by a L2-dimension Gaussian distribution N\u2081 = N(\u016b, \u017d). Therefore, the KL divergence between the normalized sample distribution and standard Gaussian is:\n$$D_{KL}(N_0 || N_1) = \\frac{1}{2} (tr(\\bar{\\Sigma}^{-1}) - L^2 + \\bar{\\mu} \\bar{\\Sigma}^{-1} \\bar{\\mu} + ln(det{\\bar{\\Sigma}}))$$"}, {"title": "Fr\u00e9chet"}]}