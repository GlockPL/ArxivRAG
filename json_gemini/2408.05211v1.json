{"title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM", "authors": ["Chaoyou Fu", "Haojia Lin", "Zuwei Long", "Yunhang Shen", "Meng Zhao", "Yifan Zhang", "Xiong Wang", "Di Yin", "Long Ma", "Xiawu Zheng", "Ran He", "Rongrong Ji", "Yunsheng Wu", "Caifeng Shan", "Xing Sun"], "abstract": "The remarkable multimodal capabilities and interactive experience of GPT-40 underscore their necessity in practical applications, yet open-source models rarely excel in both areas. In this paper, we introduce VITA, the first-ever open-source Multimodal Large Language Model (MLLM) adept at simultaneous processing and analysis of Video, Image, Text, and Audio modalities, and meanwhile has an advanced multimodal interactive experience. Starting from Mixtral 8\u00d77B as a language foundation, we expand its Chinese vocabulary followed by bilingual instruction tuning. We further endow the language model with visual and audio capabilities through two-stage multi-task learning of multimodal alignment and instruction tuning. VITA demonstrates robust foundational capabilities of multi- lingual, vision, and audio understanding, as evidenced by its strong performance across a range of both unimodal and multimodal benchmarks. Beyond foundational capabilities, we have made considerable progress in enhancing the natural multi- modal human-computer interaction experience. To the best of our knowledge, we are the first to exploit non-awakening interaction and audio interrupt in MLLM. We design additional state tokens, and corresponding training data and strategies to perceive various interaction scenarios. The deployment of VITA employs a duplex scheme, where one model is responsible for generating responses to user queries, and the other continuously tracks environmental inputs, selectively outputting new responses with updated interactions. This allows VITA to feature impressive human-machine interaction functionalities such as non-awakening interaction and audio interrupt interaction. VITA is the first step for the open-source community to explore the seamless integration of multimodal understanding and interaction. While there is still lots of work to be done on VITA to get close to close-source counterparts, we hope that its role as a pioneer can serve as a cornerstone for subsequent research.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have undergone significant evolution [39, 21, 9, 48, 12] and rencently, we have witnessed the flourishing of Multimodal Large Language Models (MLLMs) [52, 29, 31, 30, 40], which exhibit surprising multimodal capabilities. Particularly, GPT-40 [40] has significantly advanced the field of MLLMs, distinguishing itself with two key attributes: (1) a unified framework that processes text, vision, and audio signals in an end-to-end manner, and (2) the capability to enable natural multimodal human-computer interaction. These breakthroughs have established a new standard in the discipline. However, there is a conspicuous absence of open-source"}, {"title": "VITA", "content": "As depicted in Fig. 3, the overall training pipeline of VITA consists of three stages: LLM instruction tuning, multimodal alignment, and multimodal instruction tuning. The development of VITA is also an important part."}, {"title": "LLM Instruction Tuning", "content": "Mixtral 8x7B\u00b9 [22] is a representative LLM with an architecture of sparse mixture of experts (SMoE). Its performance is among the top-tier open-source LLMs, making it an ideal starting point of our work. Nonetheless, we observe that the official Mixtral model exhibits limited proficiency in understanding Chinese. To infuse bilingual (Chinese and English) comprehension capabilities, we broaden the vocabulary of the base model with Chinese, increasing the vocabulary size from 32, 000 to 51, 747. This extension can also reduce the number of tokens under the same text, thus improving inference efficiency. With the extended vocabulary in place, we use 5 million synthetic bilingual corpus for pure-text instruction tuning."}, {"title": "Multimodal Alignment", "content": "In this stage, we aim to bridge the representation gap between text and other modalities, thereby laying the groundwork for multimodal understanding."}, {"title": "Visual Modality", "content": "Visual Encoder. We employ InternViT-300M-448px as the visual encoder\u00b2, which accepts a 448\u00d7448 image as input, generating 256 tokens after using a visual connector that is a simple two-layer MLP. For high-resolution image input, we implement the dynamic patching strategy [7] to capture local details. Videos are treated as special cases of images. If the video length is less than 4 seconds, we uniformly sample 4 frames. If the video length is between 4 and 16 seconds, we sample one frame per second. For videos longer than 16 seconds, we uniformly sample 16 frames. To prevent"}, {"title": "Audio Modality", "content": "Audio Encoder. The input audio is initially processed through a Mel Filter Bank block. This block breaks down the audio signal into individual frequency bands on the mel frequency scale, mimicking the nonlinear human perception of sound. Subsequently, we utilize 4\u00d7CNN downsampling layers followed by a 24 layers of transformer, totaling 341M parameters, to process the input features. We employ a simple two-layer MLP as the audio-text modality connector. In the end, each 2 seconds of audio input is encoded into 25 tokens.\nAudio Alignment. For one of the alignment tasks, we have opted for Automatic Speech Recognition (ASR). Our dataset includes Wenetspeech [54], which encompasses over 10,000 hours of multi- domain speech recognition data, with a primary focus on Chinese tasks. Similarly, Gigaspeech [5] also contains 10,000 hours of high-quality audio data, with the majority of the data geared towards English speech recognition tasks. The other task is audio captioning, which relies on the AudioSet SL subset of Wavcaps [38]. This dataset features 400K audio clips along with their corresponding audio captions. During alignment, both the audio encoder and connector are trained."}, {"title": "Multimodal Instruction Tuning", "content": "During this stage, we perform instruction tuning on the model to enhance its instruction following capability, whether text or audio."}, {"title": "Training Data", "content": "Data Construction. The data source in the instruction tuning phase are same as the alignment phase in Table 1, and we make the following improvements: (1) the questions are randomly (about half) replaced with their audio versions, using TTS technique such as GPT-SoVITS, to enhance the model's understanding of audio queries and its instruction following capabilities. The number of audio questions and text questions can be found in Table 1. (2) Different system prompts are set to avoid conflicts between different types of data, as listed in Table 2. For instance, some questions can be answered based on visual information or based on the model's own knowledge, leading to conflicts. Additionally, since the image data have been patched that are similar to multiple frames of"}, {"title": "Noisy Audio Construction", "content": "During human-computer interaction, not all audio inputs require a response, which are collectively referred to as noisy audio. A system with good interactive capabilities should be able to actively identify the type of audio [11] and selectively execute subsequent outputs. To this end, we need to construct various noisy audio samples for the model to recognize. Specifically, we randomly sample 474K sentences from answers of existing multimodal and unimodal QA data. These negative sample texts, focusing on non-query-related content that does not require a user response, have a length distribution consistent with the positive question length distribution. Then, we use the TTS tool to convert these sentences into audio. The construction of noisy audio samples enables the model to recognize audio inputs that do not require a response, which is beneficial for implementing Non-awakening Interaction. The specific training strategy will be elaborated in the following section."}, {"title": "Training Process", "content": "In accordance with the QA pairs constructed in the above section, the model needs to distinguish three types of queries:\nQuery Audio: The question is initiated by audio.\nNoisy Audio: The input is audio, but it does not contain a question.\nQuery Text: The question is initiated by text.\nBased on these query types, we have designed three state tokens <1>, <2>, and <3>. During the training phase, we insert corresponding state tokens at the beginning of the answers, allowing the model to flexibly handle different interactive behaviors. Specifically:\nState token <1> denotes that the question input is the query audio. In this case, the output of the model needs to be presented to the user, either as text or speech converted by TTS tools.\nState token <2> indicates that the question input is the noisy audio. The model should output an EOS token as a terminator. However, we observe that abruptly terminating the output during training can significantly degrade performance. Consequently, we send the text corresponding to the noisy audio to a LLM and use its output text as the training target. During inference, <2> serves as another special EOS token.\nState token <3> signifies the question of pure text, which is used to distinguish between the above two queries in the training set.\nDuring training, both visual and audio encoders are frozen, and the connectors are trained in conjunc- tion with Mixtral 8\u00d77B."}, {"title": "Development with Duplex Pipeline", "content": "In this section, we primarily discuss how we implement two interaction functionalities, namely non-awakening interaction and audio interrupt interaction."}, {"title": "Non-awakening Interaction", "content": "Non-awakening interaction implies that the model can be activated and respond to user audio questions in the environment without the need for a wake-up word or button. The deployment process must meet the following requirements:\nReal-time Tracking of Environmental Sounds. This involves determining whether the audio content constitutes human speech.\nFiltering out noisy audio. The model should only respond to effective human query audio.\nFor the first requirement, existing Voice Activity Detection (VAD) can provide assistance. It is also known as speech activity detection or speech detection, identifying the presence of human speech. VITA employs SileroVAD [45], which is trained on huge corpora that include over 6, 000 languages and performs well with various background noise. For the second requirement, we leverage the state"}, {"title": "Audio Interrupt Interaction", "content": "Audio interrupt interaction enables users to interrupt the model's generation at any time with new questions. To accomplish this, the deployment environment must fulfill the following requirements:\nReal-time Tracking and Filtering of External Queries. While generating responses, the system must simultaneously track and filter external queries in real time.\nAnswering New Questions. When a new question emerges, the system must cease its current generation, consolidate the historical context, and respond to the present query.\nTo achieve this, we propose the duplex deployment framework. As illustrated in Fig. 1, two VITA models are deployed concurrently. Under a typical condition, the Generation model answers user queries. Simultaneously, the Monitoring model detects environmental sounds during the generation process. It disregards non-query user sounds, i.e., noisy audio, but ceases the Generation model's progress when it identifies query audio. The Monitoring model subsequently consolidates the historical context and responds to the latest user query. At this point, the identities of the Generation model and the Monitoring model are transformed."}, {"title": "Evaluation", "content": "Language Performance. To validate the efficacy of our training process for language model, we evaluate our trained model \u201cMixtral 8x7B Ours\u201d against the official version \u201cMixtral 8x7B Instruct\u201d, on four datasets: C-EVAL [20], AGIEVAL [58], MMLU [18], and GSM8K [10]. These datasets encompass a variety of scenarios including general multiple-choice questions, multidisciplinary QA, as well as mathematical and logical reasoning tasks, covering both Chinese and English contexts. The results presented in Table 3 demonstrate that our training significantly enhances the language model's capabilities on Chinese evaluation sets (C-EVAL and AGIEVAL), while maintaining original performance levels on the English related benchmark (MMLU) and showing notable improvement in the mathematical reasoning task (GSM8K).\nAudio Performance. To validate the robustness of the speech representations learned by our model, we test it on the Wenetspeech and Librispeech datasets. The Wenetspeech features two evaluation splits: test_net and test_meeting. The former has data sources that are more closely aligned with the training data, making it easier, while the latter presents a greater challenge. As a held-out dataset for our model, Librispeech assesses the model's generalization ability on unseen datasets. It has four evaluation splits: those starting with \"dev\" are validation sets, and those starting with \u201ctest\u201d are test"}, {"title": "Multimodal Performance", "content": "To assess multimodal capabilities, we evaluate VITA on four representa- tive benchmarks, including MME [13], OCRBench [32], HallusionBench [16], and Video-MME [14]. As depicted in Fig. 5, in terms of image understanding, VITA outperforms image specialized open- source model LLaVA-Next [30] and is close to closed-source model Gemini 1.5 Pro [44]. In video understanding, VITA surpasses video specialized open-source model Video-CCAM. Although there is a gap between VITA and the video-specialized LLaVA-Next-Video [57], this is acceptable given that VITA supports a broader range of modalities and prioritizes interaction. However, it is worth noting that a substantial gap still exists between current open-source models and proprietary models in terms of video understanding capabilities."}, {"title": "Conclusion and Future Work", "content": "In this paper, we have introduced VITA, a strong open-source MLLM that integrates video, image, text, and audio understanding into a unified framework, with advanced interactive experience. Apart from robust multimodal fundational capabilities, VITA pioneers novel multimodal interactions for the open-source community, through non-awakening interaction and audio interrupt interaction. However, the current version still has the following limitations:\nEnhancement of Foundational Capabilities. While VITA demonstrates competitive performance in unimodal and multimodal tasks relative to leading open-source models, there remains a notable gap compared to proprietary counterparts.\nRefinement of Noisy Audio Construction. Using non-query responses of existing data as noisy audio samples is simple yet effective. However, there are instances where VITA misclassifies noisy audio as query audio, highlighting the need for a more nuanced construction approach.\nBuilding end-to-end TTS in conjunction with LLM. We currently use an additional TTS tool to convert LLM generated text into speech, which is quite time-consuming. If TTS can be combined with LLM to achieve end-to-end speech output, it may greatly boost the real-time interaction."}]}