{"title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM", "authors": ["Chaoyou Fu", "Haojia Lin", "Zuwei Long", "Yunhang Shen", "Meng Zhao", "Yifan Zhang", "Xiong Wang", "Di Yin", "Long Ma", "Xiawu Zheng", "Ran He", "Rongrong Ji", "Yunsheng Wu", "Caifeng Shan", "Xing Sun"], "abstract": "The remarkable multimodal capabilities and interactive experience of GPT-4o underscore their necessity in practical applications, yet open-source models rarely excel in both areas. In this paper, we introduce VITA, the first-ever open-source Multimodal Large Language Model (MLLM) adept at simultaneous processing and analysis of Video, Image, Text, and Audio modalities, and meanwhile has an advanced multimodal interactive experience. Starting from Mixtral 8\u00d77B as a language foundation, we expand its Chinese vocabulary followed by bilingual instruction tuning. We further endow the language model with visual and audio capabilities through two-stage multi-task learning of multimodal alignment and instruction tuning. VITA demonstrates robust foundational capabilities of multi-lingual, vision, and audio understanding, as evidenced by its strong performance across a range of both unimodal and multimodal benchmarks. Beyond foundational capabilities, we have made considerable progress in enhancing the natural multi-modal human-computer interaction experience. To the best of our knowledge, we are the first to exploit non-awakening interaction and audio interrupt in MLLM. We design additional state tokens, and corresponding training data and strategies to perceive various interaction scenarios. The deployment of VITA employs a duplex scheme, where one model is responsible for generating responses to user queries, and the other continuously tracks environmental inputs, selectively outputting new responses with updated interactions. This allows VITA to feature impressive human-machine interaction functionalities such as non-awakening interaction and audio interrupt interaction. VITA is the first step for the open-source community to explore the seamless integration of multimodal understanding and interaction. While there is still lots of work to be done on VITA to get close to close-source counterparts, we hope that its role as a pioneer can serve as a cornerstone for subsequent research.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have undergone significant evolution and rencently, we have witnessed the flourishing of Multimodal Large Language Models (MLLMs) , which exhibit surprising multimodal capabilities. Particularly, GPT-4o has significantly advanced the field of MLLMs, distinguishing itself with two key attributes: (1) a unified framework that processes text, vision, and audio signals in an end-to-end manner, and (2) the capability to enable natural multimodal human-computer interaction. These breakthroughs have established a new standard in the discipline. However, there is a conspicuous absence of open-source models that echo these highlights. The pressing need for open-source community to further promote development in this field cannot be overstated.\nThis paper introduces VITA, as a pioneer that has initially achieved the both two attributes, by integrating architectural innovations with advanced training and development strategies. The functionality and architecture of VITA are depicted in Fig. 1 and Fig. 2, respectively. The implementation of VITA mainly comprises three steps:\nBilingual Instruction Tuning of LLM. The official Mixtral 8\u00d77B  always lacks proficient Chinese language expression and comprehension. To tackle this, we expand the vocabulary of the base model and continued with further instruction tuning using the collected high-quality bilingual text corpus. This makes the LLM proficient in both Chinese and English.\nMultimodal Alignment and Instruction Tuning. To align the text feature space with video, image, and audio, we collect massive high-quality multimodal data to align individual encoders and connectors, which process different modalities respectively, to the LLM. Multimodal instruction tuning data are meticulously constructed. While giving VITA a powerful multimodal foundational capability, we teach it to recognize the type of input queries end-to-end by introducing a state token. This makes it possible to interact without audio awakening while inference.\nDevelopment with Duplex Pipeline. In terms of model deployment, we introduce a duplex scheme. As shown in Fig. 2, two VITA models are deployed simultaneously: one is responsible for generating responses to the current audio query, and the other continuously monitors for the new one. If any, the current generation is interrupted, and the model outputs the response to the new query. In order to improve the efficiency of the interaction, we have carried out massive engineering optimizations, such as adapting multimodal vLLM .\nThe contributions of this paper are as follows:\n\u2022 We develop an open-source high-performance multimodal base model that simultaneously supports video, image, text, and audio inputs in both English and Chinese. The model accepts either pure text/audio inputs or video/image combined with text/audio inputs. We design a comprehensive training process, which includes enhancing the Chinese capabilities of the LLM, constructing multimodal training data, and a multi-stage training pipeline."}, {"title": "2 Related Work", "content": "Leveraging advanced LLMs such as GPTs , LLaMA , Alpaca , Vicuna , and Mistral , MLLMs exhibit enhanced multimodal capabilities, particularly through end-to-end training techniques. Recent open-source MLLMs, such as Otter , mPLUG-Owl , LLaVA , Qwen-VL , Cambrian-1 , Mini-Gemini , MiniCPM-V 2.5 , DeepSeek-VL , SliME , and Bunny , have made progress in solving multimodal fundamental problems, such as vision-language alignment and instruction following.\nAmong them, some representative open-source models like InternLM-XComposer-2.5  and InternVL-2  have been rapidly advancing, demonstrating strong multimodal understanding capabilities and closely rivaling proprietary models in various multimodal benchmarks. However, compared to proprietary models such as GPT-4o  and Gemini-Pro 1.5 , which support more than two modalities like audio, image, and text, most open-source models focus on image-text modalities . Furthermore, open-source models rarely concentrate on user interaction capabilities, leaving this area relatively unexplored. In comparison, the proposed VITA not only exhibits impressive performance in perceiving data across four modalities, i.e., video, image, text, and audio, but also makes preliminary strides in enhancing user interaction capabilities. Through the comprehensive open-sourcing of VITA, we hope to accelerate developments in this field."}, {"title": "3 VITA", "content": "As depicted in Fig. 3, the overall training pipeline of VITA consists of three stages: LLM instruction tuning, multimodal alignment, and multimodal instruction tuning. The development of VITA is also an important part.\n3.1 LLM Instruction Tuning\nMixtral 8x7B\u00b9  is a representative LLM with an architecture of sparse mixture of experts (SMoE). Its performance is among the top-tier open-source LLMs, making it an ideal starting point of our work. Nonetheless, we observe that the official Mixtral model exhibits limited proficiency in understanding Chinese. To infuse bilingual (Chinese and English) comprehension capabilities, we broaden the vocabulary of the base model with Chinese, increasing the vocabulary size from 32, 000 to 51, 747. This extension can also reduce the number of tokens under the same text, thus improving inference efficiency. With the extended vocabulary in place, we use 5 million synthetic bilingual corpus for pure-text instruction tuning.\n3.2 Multimodal Alignment\nIn this stage, we aim to bridge the representation gap between text and other modalities, thereby laying the groundwork for multimodal understanding.\n3.2.1 Visual Modality\nVisual Encoder. We employ InternViT-300M-448px as the visual encoder\u00b2, which accepts a 448\u00d7448 image as input, generating 256 tokens after using a visual connector that is a simple two-layer MLP. For high-resolution image input, we implement the dynamic patching strategy [7] to capture local details. Videos are treated as special cases of images. If the video length is less than 4 seconds, we uniformly sample 4 frames. If the video length is between 4 and 16 seconds, we sample one frame per second. For videos longer than 16 seconds, we uniformly sample 16 frames. To prevent the introduction of an excessive number of visual tokens, we do not perform dynamic patching on individual frames of the video."}, {"title": "3.2.2 Audio Modality", "content": "Audio Encoder. The input audio is initially processed through a Mel Filter Bank block. This block breaks down the audio signal into individual frequency bands on the mel frequency scale, mimicking the nonlinear human perception of sound. Subsequently, we utilize 4\u00d7CNN downsampling layers followed by a 24 layers of transformer, totaling 341M parameters, to process the input features. We employ a simple two-layer MLP as the audio-text modality connector. In the end, each 2 seconds of audio input is encoded into 25 tokens.\nAudio Alignment. For one of the alignment tasks, we have opted for Automatic Speech Recognition (ASR). Our dataset includes Wenetspeech , which encompasses over 10,000 hours of multi-domain speech recognition data, with a primary focus on Chinese tasks. Similarly, Gigaspeech  also contains 10,000 hours of high-quality audio data, with the majority of the data geared towards English speech recognition tasks. The other task is audio captioning, which relies on the AudioSet SL subset of Wavcaps . This dataset features 400K audio clips along with their corresponding audio captions. During alignment, both the audio encoder and connector are trained."}, {"title": "3.3 Multimodal Instruction Tuning", "content": "During this stage, we perform instruction tuning on the model to enhance its instruction following capability, whether text or audio.\n3.3.1 Training Data\nData Construction. The data source in the instruction tuning phase are same as the alignment phase in Table 1, and we make the following improvements: (1) the questions are randomly (about half) replaced with their audio versions, using TTS technique such as GPT-SoVITS, to enhance the model's understanding of audio queries and its instruction following capabilities. The number of audio questions and text questions can be found in Table 1. (2) Different system prompts are set to avoid conflicts between different types of data, as listed in Table 2. For instance, some questions can be answered based on visual information or based on the model's own knowledge, leading to conflicts. Additionally, since the image data have been patched that are similar to multiple frames of video data, which may confuse the model. The system prompt explicitly distinguishes different data types, making it more intuitive to understand.\nNoisy Audio Construction. During human-computer interaction, not all audio inputs require a response, which are collectively referred to as noisy audio. A system with good interactive capabilities should be able to actively identify the type of audio [11] and selectively execute subsequent outputs. To this end, we need to construct various noisy audio samples for the model to recognize. Specifically, we randomly sample 474K sentences from answers of existing multimodal and unimodal QA data. These negative sample texts, focusing on non-query-related content that does not require a user response, have a length distribution consistent with the positive question length distribution. Then, we use the TTS tool to convert these sentences into audio. The construction of noisy audio samples enables the model to recognize audio inputs that do not require a response, which is beneficial for implementing Non-awakening Interaction. The specific training strategy will be elaborated in the following section.\n3.3.2 Training Process\nIn accordance with the QA pairs constructed in the above section, the model needs to distinguish three types of queries:\nQuery Audio: The question is initiated by audio.\n- Noisy Audio: The input is audio, but it does not contain a question.\nQuery Text: The question is initiated by text.\nBased on these query types, we have designed three state tokens <1>, <2>, and <3>. During the training phase, we insert corresponding state tokens at the beginning of the answers, allowing the model to flexibly handle different interactive behaviors. Specifically:\nState token <1> denotes that the question input is the query audio. In this case, the output of the model needs to be presented to the user, either as text or speech converted by TTS tools.\nState token <2> indicates that the question input is the noisy audio. The model should output an EOS token as a terminator. However, we observe that abruptly terminating the output during training can significantly degrade performance. Consequently, we send the text corresponding to the noisy audio to a LLM and use its output text as the training target. During inference, <2> serves as another special EOS token.\n- State token <3> signifies the question of pure text, which is used to distinguish between the above two queries in the training set.\nDuring training, both visual and audio encoders are frozen, and the connectors are trained in conjunction with Mixtral 8\u00d77B.\n3.4 Development with Duplex Pipeline\nIn this section, we primarily discuss how we implement two interaction functionalities, namely non-awakening interaction and audio interrupt interaction.\n3.4.1 Non-awakening Interaction\nNon-awakening interaction implies that the model can be activated and respond to user audio questions in the environment without the need for a wake-up word or button. The deployment process must meet the following requirements:\n- Real-time Tracking of Environmental Sounds. This involves determining whether the audio content constitutes human speech.\n- Filtering out noisy audio. The model should only respond to effective human query audio.\nFor the first requirement, existing Voice Activity Detection (VAD) can provide assistance. It is also known as speech activity detection or speech detection, identifying the presence of human speech. VITA employs SileroVAD , which is trained on huge corpora that include over 6, 000 languages and performs well with various background noise. For the second requirement, we leverage the state"}, {"title": "3.4.2 Audio Interrupt Interaction", "content": "Audio interrupt interaction enables users to interrupt the model's generation at any time with new questions. To accomplish this, the deployment environment must fulfill the following requirements:\n- Real-time Tracking and Filtering of External Queries. While generating responses, the system must simultaneously track and filter external queries in real time.\nAnswering New Questions. When a new question emerges, the system must cease its current generation, consolidate the historical context, and respond to the present query.\nTo achieve this, we propose the duplex deployment framework. As illustrated in Fig. 1, two VITA models are deployed concurrently. Under a typical condition, the Generation model answers user queries. Simultaneously, the Monitoring model detects environmental sounds during the generation process. It disregards non-query user sounds, i.e., noisy audio, but ceases the Generation model's progress when it identifies query audio. The Monitoring model subsequently consolidates the historical context and responds to the latest user query. At this point, the identities of the Generation model and the Monitoring model are transformed."}, {"title": "4 Evaluation", "content": "Language Performance. To validate the efficacy of our training process for language model, we evaluate our trained model \u201cMixtral 8x7B Ours\u201d against the official version \u201cMixtral 8x7B Instruct\u201d, on four datasets: C-EVAL , AGIEVAL , MMLU , and GSM8K . These datasets encompass a variety of scenarios including general multiple-choice questions, multidisciplinary QA, as well as mathematical and logical reasoning tasks, covering both Chinese and English contexts. The results presented in Table 3 demonstrate that our training significantly enhances the language model's capabilities on Chinese evaluation sets (C-EVAL and AGIEVAL), while maintaining original performance levels on the English related benchmark (MMLU) and showing notable improvement in the mathematical reasoning task (GSM8K).\nAudio Performance. To validate the robustness of the speech representations learned by our model, we test it on the Wenetspeech and Librispeech datasets. The Wenetspeech features two evaluation splits: test_net and test_meeting. The former has data sources that are more closely aligned with the training data, making it easier, while the latter presents a greater challenge. As a held-out dataset for our model, Librispeech assesses the model's generalization ability on unseen datasets. It has four evaluation splits: those starting with \"dev\" are validation sets, and those starting with \u201ctest\u201d are test"}, {"title": "5 Conclusion and Future Work", "content": "In this paper, we have introduced VITA, a strong open-source MLLM that integrates video, image, text, and audio understanding into a unified framework, with advanced interactive experience. Apart from robust multimodal fundational capabilities, VITA pioneers novel multimodal interactions for the open-source community, through non-awakening interaction and audio interrupt interaction. However, the current version still has the following limitations:\n- Enhancement of Foundational Capabilities. While VITA demonstrates competitive performance in unimodal and multimodal tasks relative to leading open-source models, there remains a notable gap compared to proprietary counterparts.\nRefinement of Noisy Audio Construction. Using non-query responses of existing data as noisy audio samples is simple yet effective. However, there are instances where VITA misclassifies noisy audio as query audio, highlighting the need for a more nuanced construction approach.\n- Building end-to-end TTS in conjunction with LLM. We currently use an additional TTS tool to convert LLM generated text into speech, which is quite time-consuming. If TTS can be combined with LLM to achieve end-to-end speech output, it may greatly boost the real-time interaction."}]}