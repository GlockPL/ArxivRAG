{"title": "Self-Guided Generation of Minority Samples Using Diffusion Models", "authors": ["Soobin Um", "Jong Chul Ye"], "abstract": "We present a novel approach for generating minority sam-ples that live on low-density regions of a data manifold. Our framework is built upon diffusion models, leveraging the principle of guided sampling that incorporates an arbitrary energy-based guidance during inference time. The key defining feature of our sampler lies in its self-contained nature, i.e., implementable solely with a pretrained model. This distinguishes our sampler from existing techniques that require expensive additional components (like external classifiers) for minority generation. Specifically, we first estimate the likelihood of features within an intermediate latent sample by evaluating a reconstruction loss w.r.t. its posterior mean. The generation then proceeds with the minimization of the estimated likelihood, thereby encouraging the emergence of minority features in the latent samples of subsequent timesteps. To further improve the performance of our sampler, we provide several time-scheduling techniques that properly manage the influence of guidance over inference steps. Experiments on benchmark real datasets demonstrate that our approach can greatly improve the capability of creating realistic low-likelihood minority instances over the existing techniques without the reliance on costly additional elements. Code is available at https://github.com/soobin-um/sg-minority.", "sections": [{"title": "1 Introduction", "content": "Contemporary large-scale datasets often exhibit long-tailed distributions, con-taining minority samples that lie on low-density regions of the data manifold. The minority samples are less common and often possess unique characteristics rarely seen in the majority of the data. Generating these less probable data points are indispensable in a variety of applications like classification [40], anomaly detection [12,13], and medical diagnosis [53] where augmenting additional instances of rare attributes could enhance the predictive capabilities of the focused tasks. Such augmentation is also significant in promoting fairness, aligning with social vulnerabilities often associated with minority instances [43,53]. Moreover, the unique features within these minority instances are of paramount importance in use-cases like creative AI applications [17,44], where the ability to generate samples with exceptional creativity is crucial."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Diffusion-based generative models", "content": "Diffusion models [20,49,51] are latent variable models described by a forward diffusion process and the associated reverse process. The forward process is basically a Markov chain with a Gaussian transition, where data is gradually perturbed by Gaussian noise according to a variance schedule {$\u03b2t$}T t=1: \nq(xt|xt\u22121) := N(xt; \u221a1 \u2013 \u03b2txt\u22121,\u03b2tI) where {xt}T t=1 are latent variables with the same dimensionality as data xo ~ q(x0). One important property of the forward process is that it admits one-shot sampling of xt at any timestep t\u2208 {1, ..., T}:\n\nqat (Xt | Xo) = N(xt; \u221aatxo, (1 \u2013 at)\u2160), (1)\n\nwhere at := \u03a0t s=1(1 - \u03b2s). The variance schedule is designed to respect ar \u2248 0 so that xT ~ N(0, I). The reverse process is another Markov Chain with learnable Gaussian transition p\u03b8(xt-1|xt) := N(xt\u22121; \u03bco(xt, t), \u03a3\u03b8(x,t)). The mean is expressible in terms of a noise-conditioned score network as \u03bc\u03b8(xt,t) = 1\u221a\u03b1t(Xt+\u03b2tSo(xt, t)), where the score network is parameterized to approximate the score function of the perturbed distribution: se(x,t) := \u221a1\u2212atVx+ log po (xt) \u2248 Vx+ log qat (xt). The variance of the reverse process is often fixed, e.g., \u03a3\u0189(xt,t) = \u03b2tI [20]. One common way to construct the score network is through a denoising score matching [52,54]:\n\nmin WtEq(x) 40 (22) [||So(x, t) \u2013 V\u0101 log qa\u2081 (X | x)||2],\n\u03b8\nt=1\n\nwhere wt := 1 - at. One notable point is that this procedure is equivalent to training a noise-prediction network e\u03b8(x,t) that predicts noise added on clean data xo through the forward process in Eq. (1) [52, 54]. This establishes an intimate connection between the two networks: se(xt,t) = \u2212Eo(xt,t)/\u221a1 \u2013 At."}, {"title": "2.2 Guided sampling with diffusion models", "content": "One instrumental feature of diffusion models is that their generative processes are often amenable to various optimization signals for conditioning generations in post-hoc fashions. Specifically at each time step t, one can incorporate an arbitrary energy-based guidance into the sampling process (e.g., Eq. (2)) to encourage the evolution toward a desired direction [14]:\n\nXt\u22121 = \u03bc\u03bf(xt, t) + \u03a3\u00b2 (xt, t)z + wtg(xt,t), (3)\n\nwhere g(xt,t) is a (sign-flipped) energy-based guidance function, and wt corresponds to the strength of the guidance term possibly scheduled over time. The guidance function may incorporate a target condition c, in which case the function becomes g(xt, t; c). Notice that plugging the gradient of a classifier log-likelihood (e.g. Vx+ log pp(y|xt)) into Eq. (3) (alongside wt = wo(xt, t) where w is a fixed constant) recovers the famous classifier-guided sampler [11].\nGuidance for minority data. The principles of existing minority samplers are centered around the classifier guidance [11]. Particularly for low-likelihood generation with a conditional diffusion model, [47] propose to leverage the classifier guidance in the opposite direction. Their guidance function is expressible as:\n\ng(xt,t; y) = -\u2207xt log pp(y|xt).\n\nwhere y indicates a target class for the focused conditional generation. The de-scending gradient makes the sampling process get closer to low-likelihood regions (w.r.t. the target class y), thereby encouraging generation of low-probability in-stances of the focused class y. On the other hand, the guidance developed by [53] uses the same sign of the guidance as [11] while incorporating a distinct classifier, specifically trained to predict the degree of uniqueness of features within xt:\n\ng(xt,t; 1) = x+ log py (l|xt), (4)\n\nwhere I indicates the uniqueness level w.r.t. noisy latent instance xt. Their fo-cused uniqueness metric, which is called minority score, is shown as being in-versely correlated with the likelihood (i.e., higher minority score, lower the like-lihood) [53], and therefore the gradient ascent w.r.t. the metric can serve to encourage generation of highly unique (i.e., less-probable) instances.\nWhile both techniques offer great improvements in the capability of produc-ing minority instances [47,53], their guidance functions bear inherent reliance on"}, {"title": "3 Method", "content": ""}, {"title": "3.1 Towards an inference-time minority metric", "content": "Our approach starts by investigating a metric to be incorporated in the guidance function (i.e., g in Eq. (3)). Specifically in the context of self-guided generation of minority data, the metric should satisfy two criteria: (i) the ability to assess the likelihood of features underlain in an intermediate latent sample xt; (ii) accessibility via a pretrained diffusion model.\nOne can naturally think of leveraging an ODE-based likelihood estimator [52] to compute log po(xt) and incorporating the estimate in the guidance function. However, despite its capability of providing highly-accurate estimates, ODE-based estimators are often computationally expensive [52], e.g., requiring many Jacobian computations proportional to the number of diffusion timestep T. More importantly, the direct use of the log-likelihood in the guidance function (e.g., g(xt,t) = x+logpo(xt)) may drive the sampling process out-of-manifold. This is because a low-likelihood in a perturbed distribution may imply a noisy instance that does not belong to the data manifold. The downside is evident by poor performance of the high-temperatured sampler of diffusion models; see details in Sec. G in [11].\nWe take a distinct approach that sidesteps the above challenges. To this end, we first introduce minority score, a low-likelihood measure proposed in [53]. The metric quantifies the degree of uniqueness (i.e., low-densitiness) of features contained in a given clean sample xo, mathematically written as:\n\nL(x0;t) := Eqaz(xt|x0) [d(x0, X0(xt))], (5)\n\nwhere t refers to the timestep used for perturbing \u00e6, and d(\u00b7,\u00b7) is a discrepancy measure (e.g., LPIPS [58]). 20 denotes the posterior mean obtained via Tweedie's formula [8,42] implemented with a pretrained model se:\n\nxo(xt) := E[xo|Xt] = 1\u221a\u03b1t (Xt+ (1 - at)So(xt,t)). (6)\n\nIntuitively, minority score can be interpreted as a reconstruction loss of a clean sample, measured with the posterior mean of a noise-perturbed version. The key benefit of this metric is in its computational efficiency, requiring only a single forward pass of a diffusion model while serving as a good proxy for the log-likelihood [53]. The problem is that the metric is defined w.r.t. clean samples xo, making it impossible to be directly employed in the guidance function (that should work with xt). The authors in [53] circumvented this issue by introducing"}, {"title": "3.2 Self-guidance for low-density regions", "content": "Our next step is to develop the guidance function that incorporates our metric for minority generation. Since we are interested in encouraging xt to evolve toward low-likelihood regions (that could yield high values of Eq. (7)), a natural choice for g would be to use the gradient of the proposed metric. Employing the gradient of our measure as g gives:\n\ng(xt,t; s) := \u221ax+L(xt; 8) = \u221ax+Eqas (@s|20) [d(X0 (xt), 10 (\u00c2s(xt)))]. (8)\n\nNotice that this guidance function does not require any external elements for computation, which is in stark contrast with the prior methods on low-density guidance [47,53]. We empirically found that simply adopting the above guidance function can yield great improvements in the capability to produce minority instances. However, the gradient computation in Eq. (8) requires two backward passes through the model se, which often comes with considerable computational overhead.\nWe handle the issue by leveraging the stop-gradient technique [5]. More specifically, we employ the stop-gradient on to that incurs the additional back-ward pass. Our modified guidance reflecting the stop-gradient reads:\n\ng* (xt, t; s) := \u2207xtEqas (@s|20) [d(xo (xt), sg(10(\u00c2s(xt))))],\nXt\nwhere sg() indicates the stop-gradient operator. Notice that only a single back-ward pass now suffices for computing the gradient. Importantly, we found that it"}, {"title": "3.3 Time-scheduling for improved sample quality", "content": "Now we move onto the story on wt, a scaling factor that controls the strength of the guidance over time. A naive approach is to use a constant scale (i.e., wt = \u03c9), but we observed that it often leads to non-trivial degradation in sample quality for high values of w. We hypothesized that this comes from conflicting influences between the reverse process and our guidance, particularly occur-ring during the later timesteps. Specifically, the sampling process in these later steps often focuses on articulating fine details of images [6, 24, 27]. If our guid-ance remains consistently strong during these stages, it may impede the articu-lation process since our guidance could potentially encourage structural changes diverging from the refinement task. To avoid the conflict, we explored several time-scheduled scaling methods that employ decreasing wt's over time. We found that they exhibit the same trend of yielding better sample quality over constant scales with a slight compromise in the low-densitiness.\nWe provide two distinct time schedules herein. The first one is a simple switch-off-type schedule that discontinues incorporating the guidance after a specific timestep:\n\nwt = w \u2022 1{t > tmid},"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Setup", "content": "Datasets and pretrained models. We employ four real-world benchmarks that include both unconditional and conditional data. Our unconditional bench-marks are CelebA 642 [33] and LSUN-Bedrooms 2562 [56]. For the class-conditional datasets, we employ ImageNet 642 and 2562 [10]. The pretrained model for CelebA was constructed by ourselves by following the settings in [53]. The mod-els for LSUN-Bedrooms and ImageNet were taken from the checkpoints provided in [11]. In addition to our primary focus on these four benchmarks, we further"}, {"title": "4.2 Results", "content": "Qualitative comparisons. Fig. 3 compares generated samples on the LSUN-Bedrooms dataset. Notice that both minority generators (i.e., Um and Ye [53] and ours) are more likely to yield low-likelihood features of the dataset (e.g., complex visual attributes [2,48]) compared to a standard ancestral sampling im-plemented with ADM [11]. An important distinction herein is that our method yields this performance benefit solely with a pretrained model, in contrast to [53] that requires significant resources to train a separate classifier. Fig. 4 exhibits generated samples on another challenging benchmark, ImageNet 256 \u00d7 256. We see similar benefits of our method compared to the baselines, further demon-strating the effectiveness of our sampler on challenging large-scale benchmarks.\nQuantitative evaluation. Tab. 1 exhibits quality and diversity evaluations on our focused benchmarks. For the baseline real data, we employ the most unique samples that yield the highest AvgkNN values. Notice that our sampler yields better (or comparable) results than the baseline approaches in all datasets,"}, {"title": "5 Conclusion", "content": "We develop a novel framework for generating minority data using diffusion mod-els. Our self-guided sampler, based on our new minority metric, optimizes the generation process of diffusion models to evolve towards low-likelihood minority features. We additionally provide several techniques to further improve the com-plexity and fidelity due to our sampler. Extensive experiments across real data benchmarks demonstrate significant improvements over existing minority sam-plers. Importantly, the benefits stem solely from a pretrained diffusion model, distinguishing our approach from existing frameworks requiring additional com-ponents to improve the minority-generating capability over standard samplers.\nLimitation and potential negative impact. One disadvantage is that the proposed sampler introduces additional inference costs compared to standard samplers. A potential concern is the misuse of our sampler to intentionally sup-press the generation of minority-featured samples. This malicious use could be realized by employing negative values of w in Eq. (9), directing the focus towards producing instances dominated by high-likelihood majority features. It is crucial to acknowledge and address this risk, emphasizing the need for responsible usage of our framework to uphold fairness and inclusivity in generative modeling."}, {"title": "A Proofs", "content": ""}, {"title": "A.1 Proof of Proposition 1", "content": "1\nProposition 1 Consider minority score in Eq. (5) with the squared-error dis-tance loss || ||2. Its weighted sum over timesteps is equivalent (upto a constant factor) to the negative ELBO considered in [20]:\n\nT\nT\n\u2211WtL(xo; t) = \u2211Ep(e) [|| \u20ac \u2013 60 (\u221aatxo + \u221a1 \u2013 ate, t)||2] \u2265 \u2212 log po(xo),\nt=1\nt=1\n\nwhere wt := at/(1 - at) and p(\u20ac) := N(\u20ac; 0, I).\nProof. We start from the definition of minority score in Eq. (5):\n\nL(x0;t) := Eqaz(xt|x0) [d(x0, X0(xt))].\n\nPlugging the squared-error loss and further manipulations then yield:\n\nL(X0;t) := Eqat (xt|xo) [d(x0, Xo (xt))] = Eqat (xt|xo) [||X - 2o||2]\n= Eqat (xt|xo) XO\n1-at\n- {xt \u2013 \u221a1 \u2013 at\u20ac0(xt,t)}\n\u221a1-at\n= \u0395\u03c1(\u03b5) \u221aat\n{\u20ac \u2013 \u20ac (x, t)} \u221a1-at\n2\n2\n2\n= WtEp(\u20ac) [||\u20ac \u2013 \u20ac\u03b8(\u221aatxo + \u221a1 \u2013 ate,t)||2], (10)\n\nwhere wt := (1 - at)/at. The second equality is due to the Tweedie's for-mula (i.e., Eq. (6)) together with the noise-predicting expression se(xt,t) = -\u20ac0(xt,t)/\u221a1 \u2013 at. A weighted sum of the expression in Eq. (10) over timesteps with wt := 1/\u1ff6t = at/(1 \u2212 at) gives:\n\nT\nT\n\u2211WtL(xo; t) = \u2211Ep(e) [||\u20ac \u2013 \u20ac\u03b8(\u221aatxo + \u221a1 \u2013 ate,t)||2] .\nt=1\nt=1\n\nNotice that the RHS is equivalent (up to a constant) to the expression of the negative ELBO considered in DDPM [20,30]. This completes the proof."}, {"title": "A.2 Proof of Corollary 1", "content": "Corollary 1 The proposed metric in Eq. (7) with the squared-error loss is equiv-alent to the negative ELBO w.r.t. log po(xo(xt)) when integrated over timesteps with ws := a/(1 \u2013 \u03b1\u03c2)."}, {"title": "B Additional Details on Experimental Setup", "content": "Pretrained models. The pretrained model for CelebA was constructed by ourselves by respecting the settings in [53]. The models for LSUN-Bedrooms and ImageNet were taken from the checkpoints provided in [11]. As in [47], we leveraged the upscaling model developed in [11] for the results on ImageNet-256.\nBaselines. The ADM [11] baselines on the four main benchmarks leveraged the same pretrained models as our approach. For implementing the sampler due to [53], we respected the settings provided in their manuscript for all considered datasets. Specifically based on the ADM pretrained models (i.e., the same ones as ours), we employed encoder architectures of U-Net for minority classifiers and incorporated all training samples to construct the classifiers, except for the one on LSUN-Bedrooms where only a 10% of the training set were used. For the ImageNet-256 results of [53], we employed the upscaling model [11] as in [53].\nThe BigGAN model for our CelebA experiments is based on the same archi-tecture used in [7]\u00b9, and we respect the training setup provided in the official project page of BigGAN2. For the additional baseline on CelebA with the clas-sifier guidance targeting minority annotations (i.e., ADM-ML in Tab. 1), the classifier was trained to predict four minority attributes: (i) \"Bald\"; (ii) \"Eye-glasses\"; (iii) \"Mustache\u201d; (iv) \u201cWearing_Hat\". During inference time, we gener-ated samples with random combinations of the four attributes (e.g., bald hair yet not wearing glasses) using the classifier guidance. The backbone model used for ADM-ML is the same as ours. To implement the sampler by [47] on CelebA, we constructed an out-of-distribution (OOD) classifier that predicts whether a given input is from CelebA or other datasets (e.g., ImageNet). We then incor-porated the gradient of negative log-likelihood of the classifier (targeting the"}, {"title": "C Additional Analyses and Discussions", "content": ""}, {"title": "C.1 Problem formulation: a mathematical version", "content": "We present a refined statement herein of our focused problem of generating high-quality low-likelihood instances. Let us consider a data distribution char-acterized by the density function q(x0). We assume that q is supported on the data manifold M that contain high-quality data instances. We further assume the continuity of Pdata across its support, under which samples with high (low) density correspond to high (low) likelihood, and vice versa. Our goal is then to generate on-manifold (i.e., high-quality) instances x \u2208 M that yield low den-sity values (i.e., with low-likelihoods) under a certain threshold 7th > 0. More formally, it is to produce x \u2208 S where S := {x \u2208 M : q(x) < Tth}."}, {"title": "C.2 Effectiveness of the proposed metric", "content": "We argued in the manuscript that our proposed metric (i.e. Eq. (7)) is powerful for evaluating the uniqueness of features within intermediate latent instances xt. To support this, we provided both theoretical and empirical evidence, by showing the connection to the log-likelihood and offering the visualizations of generated samples sorted by our metric. As a further validation, we illustrate herein the correlations of the proposed metric and existing low-density metrics; see Fig. 6 for details. Notice that our metric demonstrates positive correlations with the existing ones, providing an additional empirical validation as a minority metric applicable during inference time."}, {"title": "C.3 Manifold-preserving aspect of the proposed guidance", "content": "We argue that our guidance function is inherently robust to the off-manifold issue where generated samples do not lie on the data manifold M. Note that this is contrary to a naive low-density guidance approach that employs g(xt,t) = Vx+log po (xt) (which we mentioned in Sec. 3.1). To show this, we borrow the set-tings considered in [8,9] and invoke a manifold-based interpretation of diffusion models developed therein.\nLet us consider a (clean) data manifold M constructed by a given dataset xo ~ q(x0). We further consider a set of noisy manifolds {Mt}{=1 defined by the perturbed intermediate instances xt ~ qat (xt). As in [8,9], we assume that the clean data manifold M is low-dimensional (compared to the ambient space) and locally linear. The forward and reverse processes of diffusion models can then be interpreted as transitions between adjacent manifolds [8,9]. For instance, the reverse process from timestept-1 tot can be understood as a jump from a point on Mt\u22121 to another point on Mt (e.g., the black arrow in Fig. 7)."}, {"title": "C.4 Further ablation studies", "content": "Distance metric. Fig. 8a exhibits an ablation study that investigates various discrepancy metrics for d(, ) in our minority metric (i.e., Eq. (7)). Notice that while pixel-level distances offer significant gain when compared to the baseline ancestral sampling (i.e., the one with w = 0 in the figure), the use of perceptual distances like LPIPS [58] is more beneficial. This corroborates with the previous"}, {"title": "C.5 Controllability of the proposed approach", "content": "One may concern that the pro-posed guidance approach could lose some controllability (e.g., over seman-tics) potentially offered by previous classifier-based methods [47,53]. How-ever, we contend that the controlla-bility of our approach does not fall behind the prior works. Specifically given an external classifier capable of recognizing desired semantics (e.g., a gender predictor), our method enables semantically-controlled low-likelihood generation by integrating classifier guidance (CG) into our sampler; see Fig. 10 for instance on CelebA."}, {"title": "C.6 Computational complexities", "content": "Tab. 5 presents a comparison of computational burdens between our approach and focused baselines using the CelebA dataset. Thanks to the intermittent"}, {"title": "D Further Applications", "content": ""}, {"title": "D.1 Text-to-image generation", "content": "We demonstrate the practical significance of our approach herein by investigating the application on text-to-image (T2I) generation a challenging-yet-important task that draws substantial attention these days. Specifically, our goal is to create low-likelihood minority images w.r.t. given prompts, which are rarely produced via standard sampling techniques. To do so, we incorporate our guidance term into the standard sampling process of Stable Diffusion [44] (v2.1)."}, {"title": "D.2 Medical imaging", "content": "To demonstrate a broad applicabil-ity of our approach, we push the boundary beyond natural images and explore the domain of medial imaging. Specifically, we consider an in-house (IRB-approved) brain MRI dataset containing 13,640 axial slice images, where low-likelihood instances are ones that exhibit de-generative brain disease like cerebral atrophy. The MRI images are stan-dard 3T T1-weighted with 2562 res-olution. Our brain MRI experiments were conducted with two baselines. The first one is ADM [11] with ancestral sam-pling [20]. The second baseline is StyleGAN2-ADA [25], a powerful GAN-based framework that has demonstrated its effectiveness in medical imaging applica-tions [55]. We constructed the pretrained backbone by ourselves by respecting the same architecture and the setting used for LSUN-Bedrooms in [11]. To obtain the baseline StyleGAN2-ADA model, we respected the settings provided in the official codebase\u00b9\u00b2 and trained the model by ourselves. As our other experiments, we evaluated sample quality and diversity by comparing generated samples with low-likelihood real data yielding the highest AvgkNN values."}, {"title": "D.3 Image editing", "content": "We investigate the application of our approach in image editing, one promi-nent application area of generative models widely employed in practice. The interest herein is to introduce dis-tinctive elements into a target refer-ence image, which is a key focus in industries such as creative AI [17,44]. To this end, we integrate our ap-proach into the editing pipeline of SDEdit [34]. Specifically, we incorpo-rate the proposed guidance term into the reverse process of the SDEdit pipeline to yield minority features during reconstruction.\nFig. 14 visualizes the application of our approach on LSUN-Bedrooms. Ob-serve that our method introduces novel visual attributes when compared to the baseline SDEdit framework. We highlight that this demonstrates the practical importance of our work and its potential applicability across a wide variety of practical scenarios."}, {"title": "E Additional Experimental Results", "content": "Generated samples on CelebA. Fig. 15 visualizes generated samples on CelebA. Notice that the generated samples by both minority samplers are more likely to contain unique features of the dataset compared to the samples from a standard sampler. However, we highlight that our performance gain stems ex-clusively from the pretrained model, which is in stark contrast with the other minority sampler helped by an external classifier to yield the minority-enhanced generation.\nDensity results on other datasets. Fig. 16, 18, and 19 illustrate the neighbor-hood density outcomes on CelebA, ImageNet-64 and 256 respectively. Note that across all three metrics evaluated in these benchmarks, our self-guided sampler consistently outperforms or achieves comparable performance to the baselines in generating low-likelihood minority instances. This highlights the generic advan-tages of our approach, which are not limited to a specific dataset. For complete-ness, we include herein the neighborhood density results of LSUN-Bedrooms, which are already reported in the manuscript; see Fig. 17 for details.\nAdditional generated samples. To facilitate a more comprehensive quali-tative comparison among the samplers, we provide an extensive showcase of generated samples for all the considered datasets. See Figures 20-23 for details."}]}