{"title": "Self-Guided Generation of Minority Samples\nUsing Diffusion Models", "authors": ["Soobin Um", "Jong Chul Ye"], "abstract": "We present a novel approach for generating minority samples\nthat live on low-density regions of a data manifold. Our framework is\nbuilt upon diffusion models, leveraging the principle of guided sampling\nthat incorporates an arbitrary energy-based guidance during inference\ntime. The key defining feature of our sampler lies in its self-contained\nnature, i.e., implementable solely with a pretrained model. This dis-\ntinguishes our sampler from existing techniques that require expensive\nadditional components (like external classifiers) for minority generation.\nSpecifically, we first estimate the likelihood of features within an in-\ntermediate latent sample by evaluating a reconstruction loss w.r.t. its\nposterior mean. The generation then proceeds with the minimization\nof the estimated likelihood, thereby encouraging the emergence of mi-\nnority features in the latent samples of subsequent timesteps. To fur-\nther improve the performance of our sampler, we provide several time-\nscheduling techniques that properly manage the influence of guidance\nover inference steps. Experiments on benchmark real datasets demon-\nstrate that our approach can greatly improve the capability of creating\nrealistic low-likelihood minority instances over the existing techniques\nwithout the reliance on costly additional elements.", "sections": [{"title": "1 Introduction", "content": "Contemporary large-scale datasets often exhibit long-tailed distributions, con-\ntaining minority samples that lie on low-density regions of the data manifold.\nThe minority samples are less common and often possess unique characteristics\nrarely seen in the majority of the data. Generating these less probable data points\nare indispensable in a variety of applications like classification [40], anomaly de-\ntection [12,13], and medical diagnosis [53] where augmenting additional instances\nof rare attributes could enhance the predictive capabilities of the focused tasks.\nSuch augmentation is also significant in promoting fairness, aligning with social\nvulnerabilities often associated with minority instances [43,53]. Moreover, the\nunique features within these minority instances are of paramount importance\nin use-cases like creative AI applications [17,44], where the ability to generate\nsamples with exceptional creativity is crucial."}, {"title": "2 Background", "content": "2.1 Diffusion-based generative models\nDiffusion models [20, 49, 51] are latent variable models described by a forward\ndiffusion process and the associated reverse process. The forward process is\nbasically a Markov chain with a Gaussian transition, where data is gradu-\nally perturbed by Gaussian noise according to a variance schedule {$\\beta_t$}$_{t=1}^T$:\n$q(x_t|x_{t-1}) := \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI)$ where {$x_t$}$_{t=1}^T$ are latent variables with\nthe same dimensionality as data $x_0 \\sim q(x_0)$. One important property of the\nforward process is that it admits one-shot sampling of $x_t$ at any timestep\n$t \\in \\{1, ..., T\\}$:\n$q_{at}(x_t | x_0) = \\mathcal{N}(x_t; \\sqrt{a_t}x_0, (1 - a_t)I),$ (1)\nwhere $a_t := \\prod_{s=1}^t(1 - \\beta_s)$. The variance schedule is designed to respect $a_T \\approx 0$\nso that $x_T \\sim \\mathcal{N}(0, I)$. The reverse process is another Markov Chain with\nlearnable Gaussian transition $p_\\theta(x_{t-1}|x_t) := \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x,t))$. The\nmean is expressible in terms of a noise-conditioned score network as $\\mu_\\theta(x_t,t) =$\n$\\frac{1}{\\sqrt{a_t}}(x_t + \\beta_t s_\\theta(x_t, t))$, where the score network is parameterized to approximate\nthe score function of the perturbed distribution: $s_\\theta(x,t) := \\sqrt{1 - a_t} \\nabla_{x_t} \\log p_0 (x_t) \\approx$\n$\\nabla_{x_t} \\log q_{a_t} (x_t)$. The variance of the reverse process is often fixed, e.g., $\\Sigma_\\theta(x_t,t) =$\n$\\beta_tI$ [20]. One common way to construct the score network is through a denoising\nscore matching [52,54]:\n$\\min_\\theta \\sum_{t=1}^T \\mathbb{E}_{q(x)}[w_t \\mathbb{E}_{q_{a_t} (x_t|x)} [||s_\\theta(x, t) - \\nabla_{x_t} \\log q_{a_t} (x | x)||^2]],$\nwhere $w_t := 1 - a_t$. One notable point is that this procedure is equivalent to\ntraining a noise-prediction network $\\epsilon_\\theta(x,t)$ that predicts noise added on clean\ndata $x_0$ through the forward process in Eq. (1) [52, 54]. This establishes an\nintimate connection between the two networks: $s_\\theta(x_t,t) = -\\epsilon_\\theta(x_t,t)/\\sqrt{1 - a_t}$.\n2.2 Guided sampling with diffusion models\nOne instrumental feature of diffusion models is that their generative processes\nare often amenable to various optimization signals for conditioning generations\nin post-hoc fashions. Specifically at each time step $t$, one can incorporate an\narbitrary energy-based guidance into the sampling process (e.g., Eq. (2)) to\nencourage the evolution toward a desired direction [14]:\n$x_{t-1} = \\mu_\\theta(x_t, t) + \\Sigma_\\theta^{1/2}(x_t, t)z + w_t g(x_t,t),$ (3)\nwhere $g(x_t,t)$ is a (sign-flipped) energy-based guidance function, and $w_t$ cor-\nresponds to the strength of the guidance term possibly scheduled over time.\nThe guidance function may incorporate a target condition $c$, in which case the\nfunction becomes $g(x_t, t; c)$. Notice that plugging the gradient of a classifier log-\nlikelihood (e.g. $\\nabla_{x_t} \\log p_p(y|x_t)$) into Eq. (3) (alongside $w_t = w_0(x_t, t)$ where\n$w$ is a fixed constant) recovers the famous classifier-guided sampler [11].\nGuidance for minority data. The principles of existing minority samplers are\ncentered around the classifier guidance [11]. Particularly for low-likelihood gen-\neration with a conditional diffusion model, [47] propose to leverage the classifier\nguidance in the opposite direction. Their guidance function is expressible as:\n$g(x_t,t; y) = -\\nabla_{x_t} \\log p_p(y|x_t).$\nwhere $y$ indicates a target class for the focused conditional generation. The de-\nscending gradient makes the sampling process get closer to low-likelihood regions\n(w.r.t. the target class $y$), thereby encouraging generation of low-probability in-\nstances of the focused class $y$. On the other hand, the guidance developed by [53]\nuses the same sign of the guidance as [11] while incorporating a distinct classifier,\nspecifically trained to predict the degree of uniqueness of features within $x_t$:\n$g(x_t,t; l) = \\nabla_{x_t} \\log p_y (l|x_t),$ (4)\nwhere $l$ indicates the uniqueness level w.r.t. noisy latent instance $x_t$. Their fo-\ncused uniqueness metric, which is called minority score, is shown as being in-\nversely correlated with the likelihood (i.e., higher minority score, lower the like-\nlikelihood) [53], and therefore the gradient ascent w.r.t. the metric can serve to\nencourage generation of highly unique (i.e., less-probable) instances.\nWhile both techniques offer great improvements in the capability of produc-\ning minority instances [47,53], their guidance functions bear inherent reliance on"}, {"title": "3 Method", "content": "3.1 Towards an inference-time minority metric\nOur approach starts by investigating a metric to be incorporated in the guidance\nfunction (i.e., $g$ in Eq. (3)). Specifically in the context of self-guided generation\nof minority data, the metric should satisfy two criteria: (i) the ability to assess\nthe likelihood of features underlain in an intermediate latent sample $x_t$; (ii)\naccessibility via a pretrained diffusion model.\nOne can naturally think of leveraging an ODE-based likelihood estimator [52]\nto compute log $p_0(x_t)$ and incorporating the estimate in the guidance function.\nHowever, despite its capability of providing highly-accurate estimates, ODE-\nbased estimators are often computationally expensive [52], e.g., requiring many\nJacobian computations proportional to the number of diffusion timestep $T$. More\nimportantly, the direct use of the log-likelihood in the guidance function (e.g.,\n$g(x_t,t) = \\nabla_{x_t} \\log p_0(x_t)$) may drive the sampling process out-of-manifold. This\nis because a low-likelihood in a perturbed distribution may imply a noisy instance\nthat does not belong to the data manifold. The downside is evident by poor\nperformance of the high-temperatured sampler of diffusion models; see details\nin Sec. G in [11].\nWe take a distinct approach that sidesteps the above challenges. To this end,\nwe first introduce minority score, a low-likelihood measure proposed in [53].\nThe metric quantifies the degree of uniqueness (i.e., low-densitiness) of features\ncontained in a given clean sample $x_0$, mathematically written as:\n$L(x_0;t) := \\mathbb{E}_{q_{a_t}(x_t|x_0)} [d(x_0, \\hat{x}_0(x_t))],$ (5)\nwhere $t$ refers to the timestep used for perturbing $x_0$, and $d(\\cdot,\\cdot)$ is a discrepancy\nmeasure (e.g., LPIPS [58]). $\\hat{x}_0$ denotes the posterior mean obtained via Tweedie's\nformula [8, 42] implemented with a pretrained model $s_\\theta$:\n$\\hat{x}_0(x_t) := \\mathbb{E}[x_0|x_t] = \\frac{1}{\\sqrt{a_t}} (x_t + (1 - a_t)s_\\theta(x_t, t)).$ (6)\nIntuitively, minority score can be interpreted as a reconstruction loss of a clean\nsample measured with the posterior mean of a noise-perturbed version. The key\nbenefit of this metric is in its computational efficiency, requiring only a single\nforward pass of a diffusion model while serving as a good proxy for the log-\nlikelihood [53]. The problem is that the metric is defined w.r.t. clean samples\n$x_0$, making it impossible to be directly employed in the guidance function (that\nshould work with $x_t$). The authors in [53] circumvented this issue by introducing\n3.2 Self-guidance for low-density regions\nOur next step is to develop the guidance function that incorporates our metric\nfor minority generation. Since we are interested in encouraging $x_t$ to evolve\ntoward low-likelihood regions (that could yield high values of Eq. (7)), a natural\nchoice for $g$ would be to use the gradient of the proposed metric. Employing the\ngradient of our measure as $g$ gives:\n$g(x_t,t; s) := \\nabla_{x_t}L(x_t; s) = \\nabla_{x_t}\\mathbb{E}_{q_{as} (x_s|x_0)} [d(\\hat{x}_0 (x_t), \\hat{x}_0 (\\hat{x}_s(x_t)))].$ (8)\nNotice that this guidance function does not require any external elements for\ncomputation, which is in stark contrast with the prior methods on low-density\nguidance [47,53]. We empirically found that simply adopting the above guidance\nfunction can yield great improvements in the capability to produce minority\ninstances. However, the gradient computation in Eq. (8) requires two backward\npasses through the model $s_\\theta$, which often comes with considerable computational\noverhead.\nWe handle the issue by leveraging the stop-gradient technique [5]. More\nspecifically, we employ the stop-gradient on $\\hat{x}_0$ that incurs the additional back-\nward pass. Our modified guidance reflecting the stop-gradient reads:\n$g^* (x_t, t; s) := \\nabla_{x_t}\\mathbb{E}_{q_{as} (x_s|x_0)} [d(\\hat{x}_0 (x_t), sg(\\hat{x}_0 (\\hat{x}_s(x_t))))],$\nwhere sg($\\cdot$) indicates the stop-gradient operator. Notice that only a single back-\nward pass now suffices for computing the gradient. Importantly, we found that it\n3.3 Time-scheduling for improved sample quality\nNow we move onto the story on\n$w_t$, a scaling factor that controls\nthe strength of the guidance over\ntime. A naive approach is to use\na constant scale (i.e., $w_t = \\omega)$,\nbut we observed that it often\nleads to non-trivial degradation\nin sample quality for high values\nof $w$. We hypothesized that this\ncomes from conflicting influences\nbetween the reverse process and\nour guidance, particularly occur-\nring during the later timesteps. Specifically, the sampling process in these later\nsteps often focuses on articulating fine details of images [6, 24, 27]. If our guid-\nance remains consistently strong during these stages, it may impede the articu-\nlation process since our guidance could potentially encourage structural changes\ndiverging from the refinement task. To avoid the conflict, we explored several\ntime-scheduled scaling methods that employ decreasing $w_t$'s over time. We found\nthat they exhibit the same trend of yielding better sample quality over constant\nscales with a slight compromise in the low-densitiness.\nWe provide two distinct time schedules herein. The first one is a simple\nswitch-off-type schedule that discontinues incorporating the guidance after a\nspecific timestep:\n$w_t = w \\cdot \\mathbb{1}\\{t > t_{mid}\\},$\nwhere $t_{mid}$ is a pre-defined timestep that determines when to stop. The other\nproposal is one that leverages the noise variance of the reverse process (i.e., the\nsame choice as [11]):\n$w_t = w \\cdot \\Sigma_\\theta(x_t,t),$\nwhere $\\Sigma_\\theta(x_t,t)$ can be either learned or fixed; see Sec. 2.1 for its formal defini-\ntion. While the switch-off yields some improvements over the sampler with fixed\nscales, we empirically observed that the variance-based schedule generally yields\nbetter performance. The proposed minority sampler\nwith the variance schedule is formulated as:\n$x_{t-1} = \\mu_\\theta(x_t, t) + \\Sigma_\\theta^{1/2}(x_t, t)z + w \\Sigma_\\theta(x_t, t)g^* (x_t, t; s).$ (9)\nSee Algorithm 1 for pseudocode of our sampler that incorporates the intermit-\ntent technique. The generation due to Eq. (9) can be interpreted as sampling\nfrom a modified density $p_\\theta(x_t) \\propto p_\\theta(x_t)e^{L(x_t;s)}$. We note that instances with\nhigh values of $L(x_t; s)$ would have more chances to be generated compared to\nthe original density $p_\\theta(x_t)$, which aligns well with our focus to encourage the\ngeneration of minorities."}, {"title": "4 Experiments", "content": "4.1 Setup\nDatasets and pretrained models. We employ four real-world benchmarks\nthat include both unconditional and conditional data. Our unconditional bench-\nmarks are CelebA 642 [33] and LSUN-Bedrooms 2562 [56]. For the class-conditional\ndatasets, we employ ImageNet 642 and 2562 [10]. The pretrained model for\nCelebA was constructed by ourselves by following the settings in [53]. The mod-\nels for LSUN-Bedrooms and ImageNet were taken from the checkpoints provided\nin [11]. In addition to our primary focus on these four benchmarks, we further"}, {"title": "5 Conclusion", "content": "We develop a novel framework for generating minority data using diffusion mod-\nels. Our self-guided sampler, based on our new minority metric, optimizes the\ngeneration process of diffusion models to evolve towards low-likelihood minority\nfeatures. We additionally provide several techniques to further improve the com-\nplexity and fidelity due to our sampler. Extensive experiments across real data\nbenchmarks demonstrate significant improvements over existing minority sam-\nplers. Importantly, the benefits stem solely from a pretrained diffusion model,\ndistinguishing our approach from existing frameworks requiring additional com-\nponents to improve the minority-generating capability over standard samplers.\nLimitation and potential negative impact. One disadvantage is that the\nproposed sampler introduces additional inference costs compared to standard\nsamplers. A potential concern is the misuse of our sampler to intentionally sup-\npress the generation of minority-featured samples. This malicious use could be\nrealized by employing negative values of $w$ in Eq. (9), directing the focus towards\nproducing instances dominated by high-likelihood majority features. It is crucial\nto acknowledge and address this risk, emphasizing the need for responsible usage\nof our framework to uphold fairness and inclusivity in generative modeling."}]}