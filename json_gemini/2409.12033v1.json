{"title": "Topological Deep Learning with State-Space Models: A Mamba Approach for Simplicial Complexes", "authors": ["Marco Montagna", "Simone Scardapane", "Lev Telyatnikov"], "abstract": "Graph Neural Networks based on the message-passing (MP) mechanism are a dominant approach for handling graph-structured data. However, they are inherently limited to modeling only pairwise interactions, making it difficult to explicitly capture the complexity of systems with n-body relations. To address this, topological deep learning has emerged as a promising field for studying and modeling higher-order interactions using various topological domains, such as simplicial and cellular complexes. While these new domains provide powerful representations, they introduce new challenges, such as effectively modeling the interactions among higher-order structures through higher-order MP. Meanwhile, structured state-space sequence models have proven to be effective for sequence modeling and have recently been adapted for graph data by encoding the neighborhood of a node as a sequence, thereby avoiding the MP mechanism. In this work, we propose a novel architecture designed to operate with simplicial complexes, utilizing the Mamba state-space model as its backbone. Our approach generates sequences for the nodes based on the neighboring cells, enabling direct communication between all higher-order structures, regardless of their rank. We extensively validate our model, demonstrating that it achieves competitive performance compared to state-of-the-art models developed for simplicial complexes.", "sections": [{"title": "Introduction", "content": "Graph neural networks (GNNs) [1] are widely used across various fields due to their ability to leverage the underlying structure of data. By aggregating features from neighboring nodes and edges, GNNs can learn effective representations that often lead to better performance compared to models that do not use structural information. This principle forms the foundation of Message Passing Models [2], which is the approach employed by many of the most popular GNNs. However, the limitations of these methods have been explored in previous works [3]. A key drawback is that graphs inherently represent only pairwise interactions, which makes it difficult to capture more complex relationships between nodes. Additionally, GNNs face challenges in modeling interactions between distant nodes, as multiple message-passing steps are required for information to propagate over long distances [4]. To try and overcome some of the GNNs limitations several efforts have been made to adapt Transformers [5] for use with graph-structured data [6, 7]. These approaches treat the graph as a complete graph, computing attention between all pairs of nodes to facilitate communication, regardless of the distance between them. However, this method has the significant drawback of the attention matrix scaling quadratically with the number of nodes, making it impractical for large graphs. Recently State Space Models (SSMs) [8] have become a popular alternative to Transformer-based systems for sequence modeling. They can be seen as a combination of recurrent neural networks and convolutional neural networks [9], and their main advantage is that they have linear or non-linear scaling in the sequence length. By modeling long-range interactions with principled mechanisms [10] they have achieved"}, {"title": "Related Work", "content": "Graph Neural Networks [1] commonly rely on the message-passing mechanism [2], where each node representation is updated by aggregating \"messages\" from its neighboring nodes. This process typically involves a weighted sum of the features of adjacent nodes. Consequently, two non-adjacent nodes can only influence each other through multiple message-passing steps. GNNs have been effectively applied to a wide range of tasks, including graph classification [18], link prediction [19], protein folding [20], and recommendation systems [21].\nRecurrent Neural Networks (RNNs) are capable of capturing dependencies across sequences, which makes them suitable for problems involving temporal correlations [22, 23]. However, they face challenges such as vanishing gradients [24] and high computational costs due to their inherently sequential nature, which limits parallelization. State Space Models [8] provide a computationally efficient alternative to RNNs. The Mamba model [9] has recently gained traction for its ability to offer a more flexible state representation while preserving computational efficiency.\nThis concept is leveraged in [13] to develop a graph recurrent encoding-by-distance block, which creates sequences for each node by aggregating features from equidistant nodes, starting from the furthest nodes and ending with direct neighbors. This sequence can then be input into a recurrent neural network, providing an alternative to traditional message-passing architectures. Similarly, the authors in [12] construct sequences by performing random walks of varying lengths from each node and ordering these walks by their lengths, allowing for the generation of arbitrarily long sequences.\nBuilding on these ideas, we explore the creation of sequences when working with simplicial complexes. Topological Deep Learning [14-16] is increasingly recognized for its capacity to model complex relationships among graph nodes, leveraging various topological domains, including simplicial complexes, cellular complexes, hypergraphs, and combinatorial complexes [17].\nMany models have been developed to work directly with simplicial complexes. In the SCN model presented in [25] the 0-cell representations are calculated by aggregating the 1-cell and the 2-cell representations by using the adjacency between nodes and edges and nodes and triangles, and by adding a learnable weight matrix. The same is done for the edge and the triangle representations, effectively having the cells of each rank influence every other rank directly. The main limitation of this approach is that it considers only 0, 1, and 2-cells, while simplicial complexes can in theory consider cells of any arbitrary rank. In [26] the authors propose the SCCNN model that learns the representations of the simplices by performing convolutions considering lower and upper adjacencies independently. While this approach can be applied to cells of any rank, when cells differ in rank by more that one, they no longer directly influence each other. As it will be presented shortly our work directly tackles both limitations presented by introducing a model in which cells of any rank directly influence the cells of any other rank."}, {"title": "Notation and Background", "content": "A graph G = (V, E) is a tuple composed of a finite set of nodes V and a finite set of edges E connecting pairs of nodes. Considering a graph with no nodes, n\u2081 edges, node features of dimension do, and edge features with dimension d\u2081, we obtain a featured graph GF = (V, E, FV, FE) where Fy: V\u2192 Rdo and FE : E \u2192 Rd1 are functions that map the nodes and the edges to their feature vectors respectively. One common choice to encode the structure of the graph is to use the adjacency matrix A \u2208 Rnoxno where\n$(A)_{i,j} = \\begin{cases} 1 & e_{ij} \\in E, \\\\ 0 & otherwise, \\end{cases}$\nwith eij representing an edge between nodes i and j.\nSimplicial complexes expand on graphs by considering cells with different numbers of nodes.\nDefinition 1 A simplicial complex is a tuple X = (X0, X1, ..., XK) of finite ordered sets, where Xo is a set of nodes (or 0-cells), X\u2081 is a set of edges (or 1-cells) with each edge e \u2208 X\u2081 being an ordered pair of nodes e = [v1, v2] for V1, V2 \u2208 X0. Similarly, X2 is a set of triangles (2-cells) with each face \u03c3\u2208 X2 being an ordered sequence of edges \u03c3 = [e1, e2, e3] forming a closed path. Xk is a set of k-cells, where each is an ordered sequence of cells of rank k - 1. A featured simplicial complex is a tuple XF = (X0, X1, ..., XK, Fxo, Fx1,..., FxK) where Fx : X\u2084 \u2192 Rdr, Vr \u2208 [0,1,..., K] maps each cell in X to a feature vector in Rdr."}, {"title": "Clique lifting", "content": "While higher-order data naturally emerge in a variety of systems, from social networks [27] to proteins [28], the complexity of accurately capturing these interactions limits the collection of such data. Consequently, higher-order data are frequently derived by transforming graph datasets and augmenting them with higher-order features. Following the literature [29] we call lifting the transformation of a graph into a simplicial complex.\nDefinition 2 Let GF = (V, E, Fv, FE) be a featured graph. A lifting of GF is a triplet (XF, 1, L), where XF = (X0, X1, ..., XK, Fx0, Fx1, ..., FxK), is a featured simplicial complex, i is an embedding map that maps the nodes and edges of G to cells in XF, and L is a collection of functions that map features of GF to features of XF. The embedding map \u03b9 satisfies \u03b9(v) = sv for v \u2208 V with Sv \u2208 Xo being a 0-cell corresponding to node v, and i(e) = xe for e \u2208 E with xe \u2208 X\u2081 being a cell corresponding to edge e. The collection of functions in L consists of a lifting procedure for the higher-order cells in XF that are not part of G and a lifting procedure for the feature maps {Fxi}Ko based on GF.\nConnectivity lifting. The lifting chosen is the clique lifting: given a graph G and a maximum rank R this lifting first finds the set of cliques C of the graph. Then for each clique C\u2208 C the following sets of cells are added to the simplicial complex\nX\u2081 = \u222a {X \u2286 C | |X| = r}, \u22001 \u2264 r < R.\nCEC\nThis approach has the advantage of respecting the initial connectivity of the graph since the 1-rank cells of the obtained simplicial complex will be all the edges of the original graph but no pair of unconnected nodes will be included in the 1-cells. It is important to notice that given a clique of n elements, the number of rank r cells that the algorithm creates is the binomial coefficient $\\binom{n}{r}$ which can lead to an exploding number of high-rank cells when the dataset presents large cliques. Despite this, the lifting works well in all the datasets considered in our experiments and we leave an exploration of other lifting techniques to future work.\nFeature lifting. Since the obtained higher-order cells do not have predefined features, it is necessary to lift the features as well. Many approaches are possible, using both fixed or learnable functions, but the most common approach is to obtain the features of a cell by simply summing the features of the lower rank cells that compose it. When using the sum, obtaining the features of the cells of any rank is straightforward when the features of the lower-rank cells are known, since we can use\nH(r) = BH(r-1)."}, {"title": "Method", "content": "This section presents in detail the proposed model architecture, which we refer to as TopoMamba, and then discusses batching for simplicial complexes, starting with presenting the general approach and then detailing how our model architecture allows for a more efficient batching implementation."}, {"title": "Model architecture", "content": "TopoMamba consists of three parts that will be explained in the following paragraphs: (i) the feature encoder, (ii) the Mamba block, and (iii) the task head."}, {"title": "Feature encoder", "content": "The feature encoder performs the transformation of the input features via an initial mapping function\nH(0) = ReLU (H) Win + bin),\nin\nwhere H(0) \u2208 Rn\u00d7din is the matrix of input features for the n nodes of the simplicial complex. The output of the feature encoder is H(0) \u2208 Rn\u00d7dh, where d\u0127 is the hidden dimension of the transformed features. Win \u2208 Rdn\u00d7do is a learnable weight matrix, and bin \u2208 Rdh is a learnable bias vector."}, {"title": "Mamba block", "content": "The Mamba model takes as input a batch of sequences and outputs their updated representation. To be able to use Mamba we construct a sequence for each node from the simplicial complex structure. We use a parameter-free transformation (see Figure 1 steps 2, 3). In particular, for a given node A, we collect all the simplices it belongs to and divide them by rank: we obtain R sets of the form\nZA Z(r) = {h(r) | A <x(r)},\nwith R the maximum rank of the simplicial complex. The features for the higher-order cells are obtained from the node features using Equation 1. Next, the collected simplices of the same rank are aggregated using the AGG function, which pools a set of i-cells into a single representation. The AGG function can be any invariant aggregation operation, as there is no natural order among the cells of the same rank. The obtained sequence for node A is\nSA = [AGG(Z(R)), AGG(Z(R),..., AGG(Z)), h)].\nThe aggregation operation employed in our model is the sum, since it has better expressive power than the mean and the max aggregators [30]. The features of the node being considered are also added as the last element of the sequence. This ensures that the model can distinguish between different nodes with similar neighborhoods. The obtained sequences are then passed to the Mamba model M which outputs new sequences\nSA = M(SA).\nWe also employ a separate Mamba model and apply it to the inverse of the sequence. This makes it possible to have the features of the higher-order cells be influenced by the node being studied, since as pointed out before the node vector is the last element. If we indicate with inv(\u00b7) the operator that inverts a sequence, then we can define\nS = M(inv(SA)).\nThe results of the two Mamba layers are then summed to obtain a single sequence\nSA = S + inv(S).\nThis equation demonstrates how cells of different ranks can directly influence one another. In sequential models, earlier elements in a sequence affect those that follow. Thus, in S, higher-order cells influence lower-rank cells, while in S2, the influence is reversed, with lower-rank cells affecting higher-rank ones. This enables interactions across all ranks within a single Mamba block. The new features for node A are then obtained by summing the elements of the newly obtained sequence\nA = \u2211 SA\nIn parallel to this whole block, we also employ skip connections which were shown to improve the model performance (Section 5.2). The final node features can be written as\nhA h(0) \u2190h(0) hA + (0)\nEquations 2 to 7 form the Mamba block. This module outputs the node features but does not update the higher-order cells, which can be obtained by applying Equation 1 again. This allows us to stack Mamba blocks one after the other, obtaining a final h. By exploiting the parallelization capabilities of the Mamba model we can stack the sequences for each node into a single matrix and obtain the desired output matrix H(0)."}, {"title": "Task head", "content": "Finally, the task head is another linear layer that transforms the hidden representations for each node into the shape needed for the desired task. In particular\nH(0) = ReLU (H(0). WT + bout bout),\nout\nwhere Hout \u2208 Rn\u00d7dout is the final output of the network, Wout \u2208 Rdout\u00d7dh is the learnable weight matrix for the transformation, and bout \u2208 Rdout is the bias vector. For classification dout corresponds to the number of possible classes, while dout = 1 is used for regression.\nFor clarity, we omitted from the previous discussions the inclusion of dropout layers and normalization layers. In particular, dropout is implemented after both the feature encoder and the task head, while layer normalization is used after creating the sequences."}, {"title": "Batching", "content": "Batching is essential for reducing memory requirements when working with large datasets. Unlike standard machine learning, batching for relational data requires careful handling to avoid loss of information. In message-passing networks, the limited spread of information, restricted by the number of message-passing steps, is leveraged by sampling algorithms. In fact, to avoid losing network information, they select only the batch nodes along with their corresponding n-hop neighborhoods. In TDL, the challenge of handling large higher-order networks becomes more pronounced due to the inclusion of higher-order structures. Generally, since the neighborhood structures of simplicial complexes are more complicated than those in graphs, it is required to handle each incidence matrix Br independently. This involves first removing nodes that are too distant, followed by edges, triangles, and so on for each higher-order structure. While this approach can be effective, it requires a series of matrix operations, which slows down model training.\nTopoMamba allows us to address the batching issue just mentioned by a sampling mechanism that is conceptually similar to, and just as straightforward as, graph neighborhood batching. For the creation of the sequences needed as input to the Mamba block, it is enough to rely solely on the boundary relations between higher-order structures and nodes, referred to as B*, defined as:\nDefinition 3 Let B* \u2208 Rno\u00d7n* be a matrix with n* = \u2211r=1R nr the sum of the number of all the higher order structures starting from rank 1 to the maximum rank. The node incidence is the matrix\n$(B^*)_{i,j} = \\begin{cases} \\pm 1 & x^{(r)} \\prec x_0, \\\\ 0 & otherwise. \\end{cases}$\nThe rank r can be any 1 < r < R.\nBy utilizing the node incidence matrix, we can directly apply existing neighborhood sampling algorithms developed for graphs to our model. In simplicial complexes, nodes that are part of a higher-order structure are all connected among themselves (see Definition 1). This ensures that when performing neighborhood sampling, all the cells belonging to the higher-order structures adjacent to a node are included in the selection. This method is efficient because it relies on a single incidence matrix and enables the use of well-optimized, graph-based libraries."}, {"title": "Empirical Analysis", "content": "In this section, we present a comprehensive evaluation of our proposed model, comparing its performance against state-of-the-art models for simplicial complexes. To ensure generalizability, we evaluate the models on datasets from diverse domains. Section 5.1 outlines the experimental setup, while Section 5.2 discusses the main results, examines the impact of batching on the proposed and simplicial models, and includes an ablation study to analyze the effects of the model components."}, {"title": "Setup", "content": "We use several datasets in this study, including Cora [31], Citeseer [32], Pubmed [33], Minesweeper, Amazon Ratings, Roman Empire [34], and US-county-demos [35]. Table 1 provides key statistics for each dataset, where the numbers of triangles and tetrahedra correspond to the datasets after their transformation using clique lifting. Cora, Citeseer, and Pubmed are well-known cocitation datasets"}, {"title": "Experiments", "content": "Models comparison. To assess our model's performance, we compare it with two state-of-the-art models for simplicial complexes: SCN [25] and SCCNN [26]. We also test a variant of our model that substitutes the Mamba backbone with a recurrent neural network based on gated recurrent units [37].\nTable 2 presents the performance results for the models evaluated in our experiments. TopoMamba generally performs better or is comparable to state-of-the-art simplicial models across all datasets, except for the Roman Empire dataset. The fact that our model relies solely on graph structures to generate sequences for the Mamba model highlights its ability to effectively capture and leverage topological information. However, the Roman Empire dataset poses a challenge due to its chain-like structure, characterized by an average degree of 2.9 and a diameter of 6824, which makes it difficult for our model to learn effective node representations.\nBatching. The three models TopoMamba, SCN, and SCCNN are compared both when batching and when using the full-batch approach, where the entire graph is used for training. For our model the batching technique introduced in Section 4.2 is used. Since it cannot be adapted for the other models, with them we employ the standard batching approach also described in the same section. In addition to performance, we measure the maximum memory usage during training and the time required for each epoch.\nTraining times per epoch are reported in Table 4. As expected, the batching approach tends to result in longer training times compared to full-batch training. However, it is important to note that the model typically requires fewer training epochs when using batching. Our model also consistently demonstrates faster training times compared to other models while achieving comparable or superior performance.\nAdditional metrics are detailed in Table 5. The results suggest that there is no optimal batching method; for certain datasets, batching offers performance advantages, while for others, full-batch training is more effective. In terms of memory usage, the results confirm that our batching method is an effective technique for enabling large-scale dataset processing with limited memory resources."}, {"title": "Ablation Study: Effect of Model Components", "content": "An ablation study is also performed to understand the impact of the various components of TopoMamba, including the presence or absence of skip connections and the backward Mamba model, as well as variations in hidden size and the number of blocks. The findings from the ablation study are summarized in Table 3. The results indicate that using two layers and a larger hidden dimension generally improves performance. As anticipated, the inclusion of skip connections is particularly beneficial when increasing the number of layers. Furthermore, considering the standard deviation in the results, our model appears robust to minor changes in architecture and hyperparameters."}, {"title": "Conclusions", "content": "This paper introduces a novel architecture for processing simplicial complexes. Our approach first constructs a sequence for each node by ordering and aggregating the representations of neighboring cells according to their rank. Then the Mamba model is used to iteratively update the representations of the nodes in the simplicial complex. The proposed approach eliminates the need to devise a specific higher-order message-passing schema, as it allows cells of different ranks to communicate directly (see Section 4.1). The effectiveness of our model is evaluated against several state-of-the-art networks across various datasets, demonstrating that it either outperforms or matches these models while TopoMamba is considerably faster (see Section 5.2). Furthermore, the proposed architecture allows for a novel batching strategy relying on the introduced concept of node incidence (see Section 4.2) that is easy to implement, with minimal computational overhead during training, thereby enhancing the scalability of the approach.\nFor future work, TopoMamba can be extended to other topological domains, offering a novel paradigm for information propagation across different structures and potentially addressing the challenge of the increased time complexity in topological neural networks compared to GNNs. For example, this approach could be adapted to cellular and combinatorial complexes, which also exhibit hierarchical relationships among their constituent cells. Additionally, the node incidence matrix introduced in this work is analogous to the incidence matrix used for hypergraphs, suggesting that TopoMamba could be adapted to operate effectively in this domain as well. Additionally, Mamba can potentially allow"}]}