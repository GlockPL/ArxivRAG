{"title": "MULTIMODAL OBJECT DETECTION USING DEPTH AND IMAGE DATA FOR MANUFACTURING PARTS", "authors": ["Nazanin Mahjourian", "Vinh Nguyen"], "abstract": "Manufacturing requires reliable object detection methods for precise picking and handling of diverse types of manufacturing parts and components. Traditional object detection methods utilize either only 2D images from cameras or 3D data from lidars or similar 3D sensors. However, each of these sensors have weaknesses and limitations. Cameras do not have depth perception and 3D sensors typically do not carry color information. These weaknesses can undermine the reliability and robustness of industrial manufacturing systems. To address these challenges, this work proposes a multi-sensor system combining an red-green-blue (RGB) camera and a 3D point cloud sensor. The two sensors are calibrated for precise alignment of the multimodal data captured from the two hardware devices. A novel multimodal object detection method is developed to process both RGB and depth data. This object detector is based on the Faster R-CNN baseline that was originally designed to process only camera images. The results show that the multimodal model significantly outperforms the depth-only and RGB-only baselines on established object detection metrics. More specifically, the multimodal model improves mAP by 13% and raises Mean Precision by 11.8% in comparison to the RGB-only baseline. Compared to the depth-only baseline, it improves mAP by 78% and raises Mean Precision by 57%. Hence, this method facilitates more reliable and robust object detection in service to smart manufacturing applications.", "sections": [{"title": "1. INTRODUCTION", "content": "Employing Artificial Intelligence (AI) for automation of manufacturing has resulted in increased efficiency, precision, and flexibility and created a pardigm shift in the design of manufacturing systems. Al has been successfully applied to a vast array of manufacturing tasks in the industry [1, 2]. While AI-based methods have improved the manufacturing process, there are still challenges in ensuring that the AI-based black box systems continue to be reliable and robust. The most fundamental layer underlying all smart manufacturing systems is object detection, which allows the system to identify the type and position of the objects that it needs to handle. Object detection is a established computer vision problem, which involves identifying and categorizing specific objects of interest within a larger image by placing a bounding box around each detected object [3].\nAn effective automation system also requires proper sensor design to provide adequate coverage over the environment and allow the system to properly observe the scene and the objects [4]. The past decades have seen great advancement in sensor hardware. Camera resolutions have increased and they have become more affordable at the same time. Similarly, consumer applications have facilitated mass manufacturing of 3D sensors like lidars and stereo cameras, which are great sensors for smart manufacturing. Despite these improvements, sensors have inherent limitations rooted in their physics. For example, a single image captured by camera does not carry depth information. 3D sensors capture point cloud data which addresses this issue, but these sensors are typically low resolution and do not provide color information.\nAn effective and reliable automation system requires selecting the right sensors and an object detection system that can effectively ingest the data provided by these sensors. Prior work has shown the limitations of object detection systems that rely solely on cameras [5-8]. Image distortions like blur and noise can significantly lower the detection accuracy [5]. Although cameras provide color information, there are environments where there is low contrast between objects and the background, and as a result detection accuracy may suffer [9] and cameras may not be enough for handling these environments. Moreover, camera-based object detection systems are sensitive to illumination and can be fragile if there are changes to the lighting conditions. It has also been shown that illumination can negatively affect [6] camera-based object detection systems, because they struggle to generalize to operate under lighting conditions different from what they have experienced during training. Similarly, the performance of these systems diminishes in scenarios where objects vary significantly in size or when they blend indistinguishable with the background in camera's view [7].\nWhile 3D sensors are less sensitive to lighting, they pose their"}, {"title": "2. MULTIMODAL OBJECT DETECTION MODEL", "content": "Figure 1 illustrates the design of RGBD-Man, our multimodal object detection model for smart manufacturing. Our method combines data from two separate sensors which provide images and 3D point clouds. This two-sensor setup maximizes the generalizability of the method to be applicable to a wide range of manufacturing environments.\nThe process starts with calibrating the two sensors so that the image and point cloud data are aligned when they are fed to the model. For each scene, the 3D point cloud is projected onto the image plane in 2D using the intrinsic and extrinsic projection matrices obtained through the sensor calibration process. This 3D to 2D projection converts the point cloud into a single-channel depth map. The depth map contains values which show each point's distance from the RGB camera. The depth map is then concatenated with the RGB image to form a four-channel input. This input is fed to a convolutional backbone which extracts feature maps from the combined RGB+depth data. Section 3 will outline the process for calibrating the camera and point cloud sensors and obtaining the depth maps using 3D-to-2D projections in more detail. This section discusses only the architecture of the object detector.\nThe object detection model is based on Faster R-CNN [22], which is a popular deep learning architecture for its ability to efficiently localize and classify objects within images. A number of modifications are made to customize the Faster R-CNN model for accepting the fused RGB-D inputs. The first convolutional layer is replaced with a custom layer which accepts four input channels instead of three. The depth values are normalized and scaled to map to the range 0 - 255 to match the scale of the RGB inputs. In addition, mean and standard deviation of depth values are computed over the dataset and used inside the model to normalize the depth inputs. Section 3 will outline more details about the training and evaluation setup, including the architecture of RGB-only and Depth-only variants of this object detection model."}, {"title": "3. EXPERIMENTAL SETUP", "content": "This section outlines the experimental setup for training and evaluating the multimodal object detection framework. First, the assembly task board and the manufacturing components are introduced. Then, the sensors and the sensor calibration process is discussed. Then, the steps for generating input depth maps are outlined. Next, three model variants are introduced to help us quantify the impact of depth inputs for object detection. Lastly, the setup and hyperparameters for training and evaluating these models are discussed."}, {"title": "3.1 Assembly Task Board", "content": "The task board used in this work is derived from the modified NIST task board configuration which contains nine components serving as the training and testing ground for the proposed object detection system. This board which is shown in Figure 2 includes a large gear, a small gear, a USB-C connector, a nut, a waterproof connector, pairs of small and large rectangular pins, and pairs of small and large round pins. The usage of this task board introduces realistic scenarios where lighting conditions, component orientation, and material finishes can affect the accuracy of object identification. This selection of components provides a comprehensive framework for evaluating the capabilities of the system under study. Variations in surface texture, from gear smoothness to the metallic finish of connectors, generate reflections and shadows that may obscure features or mimic other objects, risking misclassification. In addition, material properties including pin translucency and intricate details of components such as USB-C connectors introduce visual variability that demands sophisticated interpretation by the network. These factors underscore the need for an advanced, adaptable neural network and thorough pre-processing to ensure accurate detection amidst the multifaceted visual conditions of the task board."}, {"title": "3.2 Sensors and Calibration", "content": "The object detection framework ingests data from two independent sensors capturing image and depth data. Figure 3 shows the multi-sensor setup used in our experiments. A Basler camera captures images, while an Intel RealSense depth camera senses the world in 3D in the form of point clouds. The sensors are mounted together and affixed to the robot's table. This combination of sensors was selected to provide visual details of the scene in addition to the spatial relationships and dimensions of the objects under study. Although certain types of hardware exist which can provide both RGB and depth data from a single sensor, our multi-sensor framework provides greater compatibility and generalizability to diverse manufacturing problems and environments.\nThe two sensors need to be calibrated against each other since the assembly task and the object detection model require precise alignment between the image and depth data received from the environment. Camera calibration [23] involves the precise estimation of camera parameters, including intrinsic and extrinsic, to infer accurate geometric features from captured sequences. To conduct the calibration, an asymmetric 10x7 checkerboard pattern was placed in various positions and orientations within the two sensor's field of view and 60 pairs of image and point clouds were captured. MATLAB's Lidar-Camera Calibration application [24] was used to tune the calibration parameters, as shown in Fig. ??. Fig. ?? shows that the camera intrinsics are properly calibrated, therefore the red boundary perfectly lines up with the checkerboard's perimeter. However, the extrinsics are not calibrated yet, so the point cloud does not align with the checkerboard in the camera view. At the end of the process, the visual display and the error metrics signal a proper calibration. Hence, this process involved iteratively removing image and point cloud pairs with high calibration errors until the maximum translation error was below 0.0045m and the maximum rotation error was below 4.5\u00b0. At this point, a total of 37 image - point cloud pairs were used to compute the calibration parameters."}, {"title": "3.3 Generating Depth Maps", "content": "The 3D sensor captures point clouds, which are sets of disjoint points in 3D space. For higher efficiency, our multimodal object detector consumes depth maps, which are generated from the point clouds. A depth map is essentially a single-channel image similar to a grayscale image. Unlike a grayscale image where each pixel indicates a shade of gray, each pixel in a depth map specifies the distance from the sensor to the object existing at that pixel. If a pixel has no corresponding points, it will record a value of zero.\nThe point cloud data is projected onto the 2D image plane using a 4 x 4 homogeneous transformation matrix obtained from sensor calibration. This transformation matrix produces metric depth values. To make the values easier to consume in the model, they are normalized as\n\n$D_{norm} = \\frac{D - D_{min}}{D_{max} - D_{min}}$\n\nwhere D is the original depth value, Dmin and Dmax are the minimum and maximum depth values observed across the entire dataset, and Dnorm is the normalized depth value in range [0, 1].\nSince the model receives RGB images encoded as unsigned integers in range [0, 255], this work further scale the depth values as\n\n$D_s = D_{norm} \\times 255$\n\nwhere Ds is the scaled depth value. After this transformation, the depth map is concatenated with the RGB image to form a four-channel image where all values range consistently from 0 to 255, which makes the data suitable for feeding to the model. The four-channel RGB-D images can be conveniently precomputed and stored to disk since many image formats like PNG support a fourth alpha channel mainly used for encoding transparency information for each pixel.\nThe mean and standard deviation of depth values across the dataset were computed and used to renormalize the depth channel inside the object detection model similarly to the normalization of RGB channels. This renormalization increases the training efficiency of the model since it changes the inputs to small positive and negative values around zero."}, {"title": "3.4 Data Collection and Labeling", "content": "A dataset containing 301 pairs of images and associated point clouds captured from varied configurations of the assembly components on and off the assembly board were created. Placing the objects on and off the board enables the robot to detect and localize the components both before and after they get installed on the assembly board. The position and orientation of the assembly board within the sensors' field of view were also varied so that the model can generalize to varied task configurations. It was also ensured that each class of components is adequately present in the dataset to avoid an imbalance between classes [25] leading to suboptimal performance.\nThe Roboflow annotation tool [26] was used to label every one of the 301 examples in the dataset, which were then stored into the COCO JSON format. The examples in the dataset were randomly split between a train set with 226 examples, a validation set with 45 examples, and a test set with 30 examples. After annotating the full RGB-D dataset, special variants of the dataset containing only image data (RGB-only) and only depth data (Depth-only) were made to enable training model variants for ablations discussed in the next section. All dataset variants shared the original class and bounding box labels and used the same train/validation/test split."}, {"title": "3.5 Model Variants", "content": "To help isolate the impact of feeding depth data to the object detection model alongside the images, two additional variants of object detection model were created. The first variant, called RGB-only, receives only camera data and predicts object class scores and bounding boxes. The second variant, called Depth-only, receives only single-channel depth maps and predicts the same outputs. These two variants are unimodal since they each receive a single input modality. These two variants, alongside the original multimodal RGBD-Man model form three distinct models that are fully trained and evaluated in our experiments. Fig. 4 compares the three variants. Each model variant is trained and evaluated on the corresponding dataset variant. Other than the first convolutional layers, all three variants shared the same architecture, built on top of a ResNet-50 [27] backbone.\nIn all experiments, models are trained from scratch with all weights initialized to random values. No weights are shared between the three model variants, and no weights are initialized from pretraining on any other dataset."}, {"title": "3.6 Training and Evaluation Setup", "content": "All models were trained on NVIDIA Tesla T4 GPUs with a driver version of 535.104.05 and CUDA version 12.2. All models used a batch size of 4, and were trained using the Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.001, Nesterov momentum of 0.9, and weight decay of le-4."}, {"title": "4. RESULTS", "content": "This section presents the evaluations results comparing the performance of the RGB-only, Depth-only, and RGB-D model variants using the mAP and Mean Precision metrics over the test set. Table 1 and Fig. 5 compare the metrics from the three models. Fig. 5 also shows error bars indicating standard deviation of the metrics over the 10 runs for each model.\nThe RGB-D model, which uses both images and depth, achieves a mean mAP of 0.480, outperforming the RGB-only model's mean mAP of 0.425, and the Depth-only model's mean mAP of 0.269. Similarly, the mean precision metric further demonstrates the RGB-D model's superior performance with a score of 0.474 in contrast to the RGB-only model's 0.424 and the Depth-only model's 0.301. These results are visually summarized in Figure 5, where the RGB-D model demonstrates a clear advantage over the RGB-only and depth-only models across both metrics. Specifically, the RGB-D model's mean mAP is 13% higher than that of the RGB-only model and 78% higher than the depth-only model. Similarly, the RGB-D model's mean precision shows an increase of 11.8% compared to the RGB-only and a significant 57% enhancement when measured against the Depth-only model. This demonstrates a pronounced enhancement in object detection capabilities when depth information is employed in conjunction with RGB data.\nFigure 6 shows the performance of three model variants on two example scenes from the test set arranged side by side for a direct comparison. The qualitative results reinforce the quantitative findings. The RGB-D detections show a notable improvement in identifying objects that the RGB-only model misses-typically metallic objects whose colors are not very different from the scene background. Objects that are misdetected or completely undetected in the RGB-only model are accurately identified by the RGB-D model. This visual comparison aligns with the quantitative results and emphasizes how the integration of depth information with RGB data enhances detection accuracy. The depth component aids in overcoming the limitations observed with RGB-only detections, particularly in complex scenarios where depth cues are crucial for accurate object localization and classification. The Depth-only model's performance is notably weaker compared to the RGB-only and RGB-D models. The Depth-only model tends to fail detecting objects which are short in height and small objects which may not be captured by many points in the point cloud from the 3D sensor. While depth information can be crucial for adding spatial context in a multimodal setting, it is not sufficient to reliably identify and localize objects, particularly in complex scenes or where depth data lacks the necessary detail and contrast to differentiate objects from the background.\nThe enhanced performance of the RGB-D model can be attributed to the additional spatial cues provided by the depth data, which complement the visual cues from the RGB data. This extra information allows the RGB-D model to better interpret the three-dimensional structure of the scene, leading to improved detection of objects that may be challenging to recognize based solely on their appearance in RGB images."}, {"title": "5. CONCLUSIONS", "content": "This work demonstrates a notable advancement in the field of object detection for manufacturing by presenting an enhanced multimodal approach that combines RGB and depth data in an efficient manner. This work developed and validated an object detection model using an early fusion of depth information with standard RGB data that results in an efficient four-channel input that can be processed using standard convolutional networks. The experiments have demonstrated that the integration of depth information with RGB data can significantly improve object detection performance and advance the capabilities of object detection systems, particularly in complex manufacturing environments where visual ambiguity and presence of varied objects and components can pose problems for image-only object detection systems. Future work will integrate and evaluate this object detection framework in a robotic object manipulation task [29]. Alternative designs for multimodal object detection will also be explored and evaluated, including the augmentation of 3D detection models with RGB data from cameras."}]}