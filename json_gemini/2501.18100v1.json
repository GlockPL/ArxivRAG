{"title": "Panacea : Mitigating Harmful Fine-tuning for Large Language Models via Post-fine-tuning Perturbation", "authors": ["Yibo Wang", "Tiansheng Huang", "Li Shen", "Huanjin Yao", "Haotian Luo", "Rui Liu", "Naiqiang Tan", "Jiaxing Huang", "Dacheng Tao"], "abstract": "Harmful fine-tuning attack introduces significant security risks to the fine-tuning services. Main-stream defenses aim to vaccinate the model such that the later harmful fine-tuning attack is less effective. However, our evaluation results show that such defenses are fragile- with a few fine-tuning steps, the model still can learn the harmful knowledge. To this end, we do further experiment and find that an embarrassingly sim-ple solution- adding purely random perturbations to the fine-tuned model, can recover the model from harmful behaviors, though it leads to a degradation in the model's fine-tuning per-formance. To address the degradation of fine-tuning performance, we further propose Panacea, which optimizes an adaptive perturbation that will be applied to the model after fine-tuning. Panacea maintains model's safety alignment per-formance without compromising downstream fine-tuning performance. Comprehensive experi-ments are conducted on different harmful ratios, fine-tuning tasks and mainstream LLMs, where the average harmful scores are reduced by up-to 21.5%, while maintaining fine-tuning perfor-mance. As a by-product, we analyze the adap-tive perturbation and show that different layers in various LLMs have distinct safety coefficients. Source code available at https://github.com/w-yibo/Panacea.", "sections": [{"title": "1. Introduction", "content": "Fine-tuning-as-a-service (OpenAI) is a popular business service to enhance model's performance for customized datasets, domain-specific tasks, and private needs. However, recent studies (Qi et al., 2023; Zhan et al., 2023; Yang et al.,"}, {"title": "2. Related Work", "content": "Safety Alignment. The safety alignment requires the model to output content that is both helpful and harmless, and to be able to output a refusal answer when given harmful in-structions. Existing methods typically rely on supervised fine-tuning (SFT), RLHF (Ouyang et al., 2022), and vari-ations of RLHF (e.g., PPO, DPO) (Bai et al., 2022; Wu"}, {"title": "3. Revisiting Alignment Defenses", "content": "3.1. Problem Setup\nHarmful Fine-tuning. Harmful fine-tuning poses a sig-nificant threat to LLMs service providers (OpenAI). The scenario is illustrated in Figure 1, where LLMs service providers use an alignment dataset to perform safety align-ment on a pretrained model, transforming it into an aligned model. Users then upload a fine-tuning dataset containing harmful data to the service provider. The fine-tuned dataset is deployed on the service provider's server and used to generate personalized outputs for the users.\nThreat Models. Following (Qi et al., 2023; Rosati et al., 2024b; Huang et al., 2024b), we assume that, during the fine-tuning stage, $p$ (percentage) of the fine-tuning dataset consists of harmful data (i.e., harmful instruction-harmful response pairs), while the $1-p$ of data represents benign fine-tuning data (e.g., math question-answer pairs (Cobbe et al., 2021)). Furthermore, we assume that harmful and benign data are inseparable within the fine-tuning dataset.\nDefense Assumptions. Assume that LLM providers host an alignment dataset (harmful instruction-harmless response pairs) used during the alignment stage. Such a dataset is also assumed to be available by Vaccine (Huang et al., 2024e), RepNoise (Rosati et al., 2024b), BEA(Wang et al., 2024). Additionally, we assume availability of a harmful dataset (harmful instruction-harmful response pairs). This harmful dataset is also assumed to be available by existing meth-ods, e.g., RepNoise (Rosati et al., 2024b), TAR (Tamirisa et al., 2024), Booster (Huang et al., 2024b). Both the align-ment dataset and the harmful dataset can be obtained from existing open-sourced datasets (e.g., BeaverTails).\n3.2. Revisiting Alignment Defenses\nIn this section, we explore the existing alignment stage solutions during the harmful fine-tuning and discuss the feasibility of introducing random perturbation in the post-"}, {"title": "4. Methodology", "content": "At the fine-tuning stage, we aim to optimize an adaptive perturbation that maximally increases the harmful loss. This perturbation is then added to the fine-tuned model after the fine-tuning process. Therefore, our method is a post-fine-tuning stage solution, formulated as a max-maximize optimization, as follows:\n$\\max_{w} \\max_{\\epsilon:||\\epsilon||<\\rho} (h(w + \\epsilon) - h(w)) - \\lambda g(w)$\nwhere $w$ and $\\epsilon$ are the vanilla supervised aligned model parameters and the adaptive perturbation, respectively, $g(w)$ is the empirical loss over the fine-tuning dataset (contains harmful data) and $h(w)$ is the empirical loss over the harm-ful dataset. The outer optimization is maximizing the in-crease in harmful loss when adding the perturbation, while minimizing its fine-tuning loss. The term $h(w + \\epsilon) - h(w)$ represents the harmful loss ascent when adding the pertur-bation, and $A$ is a balancing hyper-parameter. The inner optimization $max_{\\epsilon}$ finds the optimal perturbation $\\epsilon$ that maximizes the increase in harmful loss $h(w + \\epsilon)$. The con-straint $||e|| \\leq \\rho$ ensures that the perturbation remains within a norm-bound $p$, preventing excessive perturbation.\nTo solve this max-maximize optimization problem, we adopt the alternative optimization. We alternatively solve the inner problem fixing w and solve the outer problem fixing $\\epsilon$.\nClose-form solution for the inner problem. Fixing $w$, the inner optimization over $\\epsilon$ could be solved with the following equation (See Appendix A for a proof):\n$\\epsilon = \\rho \\frac{\\nabla h(w_t)}{||\\nabla h(w_t) ||}$\nwhere $\\nabla h(w_t)$ denotes the gradient of the harmful loss with respect to the model parameters $w_t$, and $||\\nabla h(w_t) ||$ denotes its norm. This formulation ensures that the perturbation $\\epsilon$ is directed along the gradient of the harmful loss and scaled by the norm bound $p$.\nIterative update rule for the outer problem. Fixing $\\epsilon$, the iterative update rule of $w$ for the outer problem could the following equation:\n$w_{t+1} = w_t + \\eta(\\lambda(\\nabla h(w_t + \\epsilon_t) - \\nabla h(w_t)) - \\nabla g(w_t))$\nwhere $\\eta$ is the learning rate.\nAs shown in Algorithm 1, the optimization process consists of four key steps: First, a batch of fine-tuning data $(x_t, Y_t)$"}, {"title": "5. Experiment", "content": "5.1. Experiment Settings\nDataset. Three distinct datasets are utilized: the align-ment dataset, harmful dataset, and fine-tuning dataset. The alignment dataset and harmful dataset are derived from the RepNoise (Rosati et al., 2024b), which extracts subsets from the BeaverTails dataset (Ji et al., 2023). Specifically, 5,000 examples are sampled for the alignment dataset, and 1,000 examples for the harmful dataset. The fine-tuning dataset is constructed from four downstream fine-tuning tasks: GSM8K (Cobbe et al., 2021), SST2 (Socher et al., 2013), AlpacaEval (Li et al., 2023), and AGNEWS (Zhang et al., 2015), with 1,000 samples collected from each task (700 samples from AlpacaEval). To simulate the harmful"}, {"title": "5.2. Main Results", "content": "In this sub-section, we conduct a comprehensive evaluation of Panacea for the effectiveness and generalization.\nHarmful Ratio. Fine-tuning datasets with different harm-ful ratios are employed, specifically 0 (clean), 0.05, 0.1, 0.15, 0.2, and 0.25. The results are presented in Table 1, Panacea achieves the lowest harmful score across different harmful ratios while maintaining competitive fine-tuning performance (ranked as the second-best on average), indi-cating that the expected adaptive perturbation is obtained, and the analysis is in Sec 5.3. Compared to SFT method, it reduces the harmful score by an average of 21.5% and improves fine-tuning accuracy by 0.3%. Furthermore, as the harmful ratio increases, Panacea consistently maintains a lower harmful score compared to other methods. By miti-gating the impact of harmful loss, the model achieves the best fine-tuning performance. However, since Panacea is de-signed to weaken harmful loss, its fine-tuning performance on clean data (without explicit harmful loss) is slightly re-duced, while Panacea still achieves the lowest harmful score on this benign fine-tuing.\nFine-tuning Tasks. Table 2 presents the comparative re-sults across various fine-tuning tasks (GSM8K, SST2, Al-pacaEval, AGNEWS). The results demonstrate that Panacea achieves the lowest harmful scores across all fine-tuning tasks, reducing harmful scores by 25.7%, 23.3%, 4.4%, and 9.8% compared to SFT method. Additionally, Panacea is the only method that outperforms SFT in fine-tuning accu-"}, {"title": "5.3. Perturbation Analysis", "content": "Ablation Study. After fine-tuning, we perform an ablation study on different fine-tuning tasks and LLMs, compar-ing the results with and without the adaptive perturbation. The experimental results in Table 4 and Table 5 show that the adaptive perturbation is the primary factor in reducing harmful scores. After applying the adaptive perturbation, harmful scores decrease by 24.2%, 23.9%, 4.3%, and 10.3% on different fine-tuning tasks, and it proves effective across various LLMs. Notably, Gemma2-9B experiences a 28.9% reduction in harmful scores. Furthermore, our adaptive per-turbation has minimal impact on fine-tuning performance. For the complicated AlpacaEval dataset, it even improves\nVisualization. Visualization analysis is conducted by visu-alizing the weights of the perturbations obtained in Table 5. The result in Figure 5 reveals the layers of different LLMs that are most critical for model's safety. Specifically, in Llama2-7B, the adaptive perturbation has a larger weight in the earlier layers and a smaller effect on the later lay-ers, suggesting that the earlier layers are more critical for the model's safety. This observation aligns with previous research (Du et al., 2024a; Rosati et al., 2024b; Liu et al., 2024a; Yi et al., 2024b). Additionally, we observe that in Gemma2-9B, the middle and final layers hold greater impor-tance for safety, while in Qwen2-7B, the safety importance gradually increases across layers, reaching its peak in the final layers. This may indicate that the model implements stricter safety measures in the output layer. Therefore, our experiments also suggest that these models may require tar-"}, {"title": "5.4. Statistical Analysis", "content": "We compare the statistical results between Panacea and the SFT method: harmful score, harmful training Loss, harmful testing Loss in Figure 6. Panacea introduces the adaptive perturbation only after fine-tuning is complete. Thus, during the evaluation phase, Panacea adds the perturbation opti-mized at specific step and subtracts it after the evaluation is finished.\nHarmful Score. As shown in the top figure, both Panacea and SFT exhibit an increase in harmful scores during the fine-tuning process. However, the harmful score increase for Panacea is generally smaller. In the final few hundred steps, the defense of SFT is significantly degraded, causing a sharp rise in harmful scores, while the harmful score of Panacea remains stable. This is because, at this stage, the perturbation optimized by Panacea enhances the model's ability to mitigate harmful behaviors.\nHarmful Training Loss. As shown in the middle figure, since Panacea does not apply additional defense during the alignment stage, it starts with the same harmful training loss as SFT. Although both harmful training losses decrease, Panacea exhibits a smaller reduction, reaching a higher harmful training loss than SFT in the end. With the relatively high loss, Panacea's harmful score shows a significant im-provement. Finally, we observe an interesting phenomenon that the harmful training loss and harmful score of Panacea follow the same trend in the last few hundred steps. We attribute this to the near-successful optimization of the adap-tive perturbation at this stage. See more analysis of harmful loss in Appendix B.3."}, {"title": "5.5. System Evaluation", "content": "Table 6 presents a comparison of the clock time and GPU memory usage during training for different methods.\nClock Time. Panacea requires only 0.25 more hours than SFT for total training time, and it outperforms other state-of-the-art methods in terms of time efficiency. Specifically, other methods double or even more than double the time spent on the time-consuming alignment stage. We acknowl-edge that the adaptive perturbation optimized during the fine-tuning stage in Panacea significantly improves safety, but it incurs an additional time cost-more than doubling the time. This is because we perform two additional gra-dient computations during training. However, compared to alignment-stage methods, our approach offers a time ad-vantage as it can be directly applied to the aligned model, making it more practical even time-efficient.\nGPU Memory. Panacea achieves the lowest memory usage. In contrast, Vaccine and RepNoise introduce an additional 9.52GB and 37.57GB of memory usage compared to SFT. Panacea reduces memory usage slightly compared to SFT, which we believe is due to the reduced gradient size after the final gradient summation."}, {"title": "5.6. Hyper-parameter Analysis", "content": "Perturbation Intensity p. Table 7 shows the impact of perturbation intensity $p$ on Panacea. It can be observed that, overall, the variation of $p$ and the harmful score are linearly related, as $p$ affects the rate of change of the perturbation during each optimization, ensuring that the magnitude of the perturbation does not become too large. Therefore, when $p$"}, {"title": "5.7. Case Study", "content": "We used the finetuned model over default attack setting for evaluation. The responses of different methods to malicious queries are shown in Figure 7, Panacea is able to reject malicious queries and provide harmless outputs, whereas other methods fail to do so."}, {"title": "6. Conclusion", "content": "In this paper, we first explore that mainstream defenses still suffer from the harmful fine-tuning attack when with more fine-tuning steps. Based on this finding, we find an embarrassingly simple solution that adding purely random perturbations can restore the model's safety alignment but causes a loss of fine-tuning performance. We further pro-pose Panacea, this post-fine-tuning method could maintain model'ss afety alignment without compromising fine-tuning performance. The comprehensive experiments demonstrate the effectiveness and generalization of Panacea and the vi-sualization of the adaptive perturbation reveals the different lays in various LLMs have distinct safety coefficients."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to address the harm-ful fine-tuning and make LLMs helpful and harmless. We acknowledge that the phenomena or issues identified in this paper may pose potential risks. Disclaimer: this paper contains red-teaming data (from open dataset) and model-generated content that can be offensive in nature."}, {"title": "A. Proof of Inner Optimization", "content": "In the inner maximization problem, we aim to solve the following problem:\n$\\arg \\max_{\\epsilon:||\\epsilon||<\\rho} \\lambda (h(w + \\epsilon) - h(w)) - g(w)$\nwhich is equal to the following equation:\n$\\arg \\max_{\\epsilon:||\\epsilon||<\\rho} h(w + \\epsilon)$\nApproximating the harmful loss with first-order Taylor expansion on w, we can get:\n$\\arg \\max_{\\epsilon:||\\epsilon||<\\rho} h(w + \\epsilon) = \\arg \\max_{\\epsilon:||\\epsilon||<\\rho} h(w) + \\epsilon^T \\nabla h(w)$\nwhich is equivalent to solve:\n$\\arg \\max_{\\epsilon:||\\epsilon||<\\rho} \\epsilon^T \\nabla h(w)$\nBy H\u00f6lder's inequality, we have:\n$\\epsilon^T \\nabla h(w) \\leq ||\\epsilon|| ||\\nabla h(w)||$\nSince $||\\epsilon|| \\leq \\rho$, we maximize the term $||5||$ by setting $||\\epsilon|| = \\rho$. Substituting this into the expression, we get:\n$\\epsilon^T \\nabla h(w) \\leq \\rho ||\\nabla h(w)||$\nAs $\\epsilon^T\\epsilon = ||\\epsilon||^2$, we get that:\n$\\epsilon^T \\nabla h(w) = \\rho ||\\nabla h(w)||$\nAnd due to the definition of the L2 norm, it is easy to verify:\n$|| \\hat{\\epsilon} || = \\rho$\nCombining Eq. 10 and Eq. 11, we can infer that $\\hat{\\epsilon}$ is a solution that satisfies the L2 norm ball constraint with function value $\\rho||\\nabla h(w)||$. By Eq. 9, we know that all feasible solutions must have function values smaller than $\\rho||\\nabla h(w)||$. Therefore, $\\hat{\\epsilon}$ is the optimal solution within the feasible set, i.e., $\\epsilon^* = \\hat{\\epsilon}$. This completes the proof.\nB. More Detail of Experiments.\nB.1. Experiment Details\nTraining details. The alignment dataset is sampled from BeaverTail (Ji et al., 2023) with 5000 instances, while the harmful dataset is also sampled from BeaverTail with 1000 instances. The fine-tuning dataset is a mixture of benign fine-tuning samples and harmful samples. The benign fine-tuning samples come from GSM8K, SST2, AlpacaEval, and AGNEWS, with 1000, 1000, 700 (due to the limited training data for this task), and 1000 instances, respectively. The harmful samples are also sampled from BeaverTail but follow a different distribution than the harmful/alignment dataset.\nIn the alignment stage, the learning rate is set to 5e \u2013 4, the batch size is 10, and the total number of alignment epochs is 20. In the fine-tuning stage, the learning rate is set to 2e \u2013 5, the batch size is 10, and the total number of fine-tuning epochs is 20. Most experiments are conducted on a single L40S, while RepNoise and other LLMs (Gemma2-9B and Qwen2-7B) are run on a single A100-80G.\nTesting details. Following (Huang et al., 2024b), the test dataset for harmful score (HS) is sampled from the BeaverTail test set with 1000 instances, while the test datasets for fine-tuning accuracy (FA) are sampled from the GSM8k, SST2, AlpacaEval, and AGNEWS test sets with 1000, 872, 105, and 1000 instances, respectively.\nB.2. Prompt Template.\nWe follow (Huang et al., 2024b) to use the prompt template in the following box for constructing supervised dataset for alignment/fine-tuning."}, {"title": "B.3. Harmful Loss.", "content": "This section discusses more analysis of harmful loss in current defense methods. As shown in Figure 8, at the beginning of fine-tuning, although the harmful loss decreases, which is inevitable, the model's harmful score does not rise significantly, and the defense remains effective. However, in the last few hundred steps, the model's harmful loss tends to converge, and the model's harmful score sharply increases. Therefore, we argue that the convergence of harmful loss is considered crucial for the effectiveness of the defense and is also consistent with common sense."}, {"title": "B.4. More Examples.", "content": "Below we try to provide the responses of five methods to two malicious instructions. We used the fine-tuned model over default attack setting for evaluation. In the first instruction, all four other defense methods fail to refuse the response, while Panacea successfully rejects the answer and provides harmless suggestions. In the second instruction, although Vaccine, RepNoise, and Booster initially attempt to refuse the response, they ultimately output harmful content."}]}