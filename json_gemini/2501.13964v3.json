{"title": "Advancing the Understanding and Evaluation of AR-Generated Scenes: When Vision-Language Models Shine and Stumble", "authors": ["Lin Duan", "Yanming Xiu", "Maria Gorlatova"], "abstract": "Augmented Reality (AR) enhances the real world by integrating virtual content, yet ensuring the quality, usability, and safety of AR experiences presents significant challenges. Could Vision-Language Models (VLMs) offer a solution for the automated evaluation of AR-generated scenes? In this study, we evaluate the capabilities of three state-of-the-art commercial VLMS-GPT, Gemini, and Claude-in identifying and describing AR scenes. For this purpose, we use DiverseAR, the first AR dataset specifically designed to assess VLMs' ability to analyze virtual content across a wide range of AR scene complexities. Our findings demonstrate that VLMs are generally capable of perceiving and describing AR scenes, achieving a True Positive Rate (TPR) of up to 93% for perception and 71% for description. While they excel at identifying obvious virtual objects, such as a glowing apple, they struggle when faced with seamlessly integrated content, such as a virtual pot with realistic shadows. Our results highlight both the strengths and the limitations of VLMs in understanding AR scenarios. We identify key factors affecting VLM performance, including virtual content placement, rendering quality, and physical plausibility. This study underscores the potential of VLMs as tools for evaluating the quality of AR experiences.", "sections": [{"title": "1 INTRODUCTION", "content": "Augmented Reality (AR) is poised to transform how we interact with the world, unlocking groundbreaking applications across education, entertainment, and healthcare. However, some AR applications can negatively impact user experience, either unintentionally or through deliberate malice. For example, AR content that does not blend seamlessly with the physical environment can reduce immersion and undermine the intended objectives of the AR experience, due to issues like spatial misalignment [22] or stylistic mismatches [11]. Additionally, some AR content may mislead or confuse users [18]. These challenges not only compromise user experience but also raise safety and ethical concerns. Therefore, evaluating the quality of AR content has become a critical challenge to ensure usability and safety in AR applications.\nWhile prior studies have investigated methods for assessing general image quality [12, 21], evaluating AR scenes introduces unique challenges that surpass those of traditional image analysis. Unlike conventional approaches that primarily rely on mathematical metrics (e.g., SSIM [9]) or, more recently, machine learning models (e.g., IQAGPT [4]), the quality of an AR scene is deeply intertwined with the user's subjective experience. This is affected by factors such as content placement, lighting, rendering quality, and the seamless integration of virtual elements into the physical environment. Typically, these aspects are assessed through user studies [14], which gather feedback after users interact with the AR app. However, this approach has some drawbacks: experiencing an AR scene is often time-consuming, and variations in real-world environments or AR content during interactions can skew the evaluation. Real-time AR scene quality estimation offers a promising alternative, enabling immediate and holistic assessments. Yet, collecting real-time user feedback may not always be practical, as it could disrupt the interaction and detract from the experience. This highlights the need for objective and real-time methods to automatically evaluate AR content.\nRecent developments in Vision Language Models (VLMs) present potential solutions to the challenges of AR experience evaluation. These models enable a holistic, human-like understanding of complex scenes [24], excelling in context-aware analysis by capturing relationships between objects and generating detailed descriptions that align with human perception. However, it is essential to understand VLMs' ability toward AR content understanding before using them for AR content evaluation. Examples in Figure 1 present AR scenes together with GPT's responses to the question, \"Is this an augmented reality (AR) scene? If yes, what virtual content does it include, and could you explain why each element is considered virtual?\" These examples suggest that VLMs can recognize AR images but are sometimes unreliable. To address this, in this work, we provide an assessment of the feasibility of using modern VLMs for evaluating AR content. Toward this goal, we collected AR images from different sources and tested three state-of-the-art VLMs, GPT [16], Gemini [7], and Claude [1], with different prompt questions on these images. The contributions of this paper include:\n\u2022 We curate and release a dataset\u00b9, DiverseAR, comprising 318 images collected from one public website [5], two commercial AR platforms (Amazon and Scaniverse), three AR applications previously developed by our lab running on Magic Leap, Android, and HoloLens [6, 17,23], along with two custom-built AR applications for this project, running on Apple Vision Pro and Android. To the best of our knowledge, this dataset is the first of its kind for assessing the capabilities of VLMS in identifying and describing AR content, encompassing diverse AR scenarios."}, {"title": "2 RELATED WORK", "content": "Recently, the advancement of VLMs has made them a transformative technology, capable of providing comprehensive analysis of complex scenes. This positions VLMs as efficient tools for evaluating AR content quality. Previous studies have explored their utility in various domains, including analyzing image perception for advertisements and medical images [4, 13], detecting visual anomalies in the generated images [2, 25], and assessing the look and feel of images [21], highlighting their potential for content assessment. Yet, to our knowledge, no prior efforts have applied VLMs to evaluate AR content.\nDespite their promise, concerns persist regarding VLMs' perception and reasoning capabilities. Studies have shown that VLMs can hallucinate, producing information not grounded in the data [20]. Additionally, VLMs exhibit limited ability to comprehend depth information [3], further raising questions about their reliability in evaluating the quality of AR experiences, particularly in aspects such as spatial alignment and contextual integration. In light of these issues, this work evaluates the ability of VLMs to analyze and interpret AR images, offering a pioneering investigation into their effectiveness in AR content evaluation."}, {"title": "3 DIVERSEAR DATASET", "content": "In this section, we introduce DiverseAR, a new dataset curated as part of this work. The dataset is publicly available on GitHub\u00b9.\n3.1 Motivation and Collection Process\nTo evaluate the AR scene understanding capabilities of VLMs, we curated the DiverseAR dataset, specifically designed to capture a broad spectrum of AR scenarios. The dataset consists of 298 AR images collected from diverse sources and environments. It includes 23 images captured using a custom-developed Apple Vision Pro AR application in laboratory and kitchen environments, and 151 images collected from a custom-developed Android AR application in bedroom and dining room environments. Additionally, 42 images were collected during the exploration of AR-specific research topics, such as attention patterns [17], virtual content arrangements [23], and surgical guidance [6]. The dataset also features 7 images from the Amazon app's AR view and 46 images collected from the Scaniverse [15] app's AR view, captured in laboratory, kitchen, and dining room environments. Finally, 29 images were sourced from a website showcasing AR advertisement videos [5]. Additionally, we included 20 non-AR images to supplement the dataset.\n3.2 Dataset Composition\nThe DiverseAR dataset encompasses a wide spectrum of AR and non-AR scenarios, showcasing diverse characteristics of both virtual and real-world content."}, {"title": "4 METHODS", "content": "To evaluate the AR scene understanding capabilities of VLMs and identify factors influencing their performance, we assess three commercial VLMs using our DiverseAR dataset: 1) GPT-40-2024-08-06 by OpenAI; 2) Claude-3.5-Sonnet-20241022 by Anthropic; 3) Gemini-1.5-Pro-002 by Google. These models represent state-of-the-art capabilities in vision-language understanding and generation. We also design targeted prompts to assess the accuracy of their perception and description of AR scenes. Additionally, we define three levels of AR scene complexity based on AR content attributes and categorize AR samples into five groups based on VLM responses. These categories facilitate a detailed quantitative analysis of VLM performance in recognizing and describing AR scenes.\n4.1 Prompt Design for AR Scene Understanding\nTo assess the capacity of VLMs to generate comprehensive textual descriptions of images, we design two types of prompts: general and task-aware.\nGeneral Image Captioning Prompt: \"Can you explain what is happening in this image?\"\nThis prompt is designed to assess the model's ability to generate a general caption of the image without any specific guidance toward identifying AR content. It tests whether the model can naturally recognize and describe AR elements without explicit textual cues.\nTask-aware Image Captioning Prompts: 1) Without explicitly referencing AR: \"Does this image include virtual content superimposed on the real world? If yes, could you list all the virtual elements present and explain why each one is considered virtual?\" 2) Explicitly referencing AR: \u201cIs this an augmented reality (AR) scene? If yes, what virtual content does it include, and could you explain why each element is considered virtual?\"\nThese task-aware image captioning prompts are designed to assess the model's ability to leverage guidance for more accurate identification and description of AR-specific elements. Since the responses from VLMs were similar for both task-aware prompts, we chose to use the prompt without explicitly referencing AR in our experiments for consistency and simplicity.\n4.2 Classification of AR Scene Complexity Levels\nTo analyze VLM performance across varying complexities of AR scenes, we define three distinct levels based on the attributes of virtual elements as follows:\n\u2022 Easy: Images with obvious virtual content, such as transparent or glowing overlays, or virtual objects with low rendering quality that are easily distinguishable from the real world.\n\u2022 Medium: Images with high-quality virtual content that exhibits inconsistencies with physical laws, such as floating or intersecting objects, or virtual objects with unrealistic attributes like informal size or placement relative to the real world.\n\u2022 Hard: Images with high-quality virtual content seamlessly integrated into the real-world environment, including proper shadows, realistic size and shape, and adherence to physical laws, making them more challenging to distinguish as virtual.\nLabeled by an annotator with extensive expertise in AR, there are 91, 128, and 79 images in the easy, medium, and hard levels, respectively.\n4.3 Categorization of VLM Responses\nTo facilitate the analysis of factors affecting VLM performance, we categorize AR samples into five groups based on the VLMs' responses:\n\u2022 Category 1: Accurate AR Recognition. AR images are correctly identified as AR scenes, and the virtual content is accurately recognized and described. This includes proper classification of real versus virtual elements, their attributes, and their spatial relationships.\n\u2022 Category 2: Partial AR Recognition. AR images are recognized as AR scenes, but errors in identifying virtual content persist, including misclassifying real elements as virtual (or vice versa), incorrect object classification, or inaccurate descriptions of object properties such as location.\n\u2022 Category 3: Missed AR Recognition. AR images are not recognized as AR scenes. Instead, they are misidentified as real-world images or as digital content displayed on screens, failing to acknowledge the presence of AR elements.\n\u2022 Category 4: False AR Detection. Non-AR images are incorrectly identified as AR scenes. The VLM mistakenly attributes virtual content to images that contain only real-world elements.\n\u2022 Category 5: Accurate Non-AR Recognition. Non-AR images are correctly identified as not being AR scenes, with no virtual content mistakenly attributed to them.\nWe apply this categorization across the whole dataset. For each group, we analyze the key characteristics of the samples that influence VLM performance in Section 5.\n4.4 Evaluation Metrics\nMetrics for AR Scene Perception: To assess whether VLMs can accurately identify AR scenes, we evaluate their performance using the True Positive Rate, TPRp, and the True Negative Rate, TNRp. These metrics are formulated as follows:\n$TPRP = \\frac{N_1 + N_2}{N_1+N_2+N_3}$ , $TNRP = \\frac{N_5}{N_4+N_5}$',\nwhere N1, N2, N3, N4, and N5 denote the number of images in Category 1, 2, 3, 4, and 5, respectively. TPRP represents the proportion of AR images identified as containing AR elements among all AR images, regardless of the accuracy of the AR scene descriptions. Similarly, TNRP indicates the proportion of non-AR images described without referencing AR elements among all non-AR images, irrespective of the accuracy of the non-AR scene descriptions.\nWe use these metrics instead of simple accuracy because the DiverseAR dataset contains relatively few non-AR images, which were collected only to supplement the study. These metrics enable a more nuanced evaluation, especially for imbalanced datasets.\nMetric for AR Scene Description: To evaluate AR scene description performance, we measure the VLMs' ability to correctly identify and describe virtual content within AR scenes. This is assessed using the metric TPRD, which is defined as follows:\n$TPRD = \\frac{N_1}{N_1+N_2+N_3}$'\nTPRD represents the percentage of AR images that are both correctly identified as containing AR elements and accurately described in terms of the AR scene, out of all AR images. This metric, alongside perception metrics, allows for a detailed evaluation of the VLMs' strengths and limitations in AR scene understanding. By focusing on the quality of both perception and description, we aim to provide detailed insights into VLM performance across varying levels of AR scene complexity. These results are analyzed further in Section 5."}, {"title": "5 RESULTS", "content": "We present the results of VLMs' AR scene understanding capabilities, focusing on their perception and description performance. We also provide an in-depth categorical analysis of VLM responses and compare their performance with that of human counterparts.\n5.1 Performance of VLMs' AR Understanding Abilities\nPerception Performance:\nWe evaluate VLMs' ability to perceive AR scenes at different complexity levels, using the DiverseAR dataset as well as the prompts defined in Section 4.\nAcross the dataset, the Perception True Negative Rate (TNRp) remains consistently at 100% for all models. This demonstrates that VLMs rarely misclassify real-world images as AR content, showcasing their robustness in distinguishing non-AR scenes.\nWhen comparing the Perception True Positive Rate (TPRP) using the task-aware prompt (T) and the general prompt (G), we observe significant improvements with prompt T across all models, as illustrated in Figure 3. We observe performance improvements ranging from 30.5% to 46.7%, highlighting the importance of explicitly specifying AR-related textual cues. Among the three models we evaluated, Gemini consistently achieves the highest TPRp, with 61.4% under prompt G and 93.3% under prompt T, outperforming GPT and Claude. These results underscore Gemini's superior performance and its potential suitability for AR scene perception tasks.\nFurther analysis of TPRp across different AR scene complexity levels reveals a consistent decline in performance as complexity increases. For instance, under prompt G, GPT's TPRP decreases from 71.4% for easy scenes to just 11.4% for hard scenes, with similar trends observed for all models. This suggests that while VLMs excel at identifying simpler AR objects (e.g., easy-level AR scenes), their performance deteriorates when faced with more complex AR content characterized by polished rendering and realistic integration.\nDescription Performance: In addition to perception, we evaluate the correctness of VLMs' virtual content descriptions across varying scene complexity levels and the full dataset.\nSimilar to perception, the Description True Negative Rate (TNRD) remains consistently at 100% across all models and the dataset. This indicates that VLMs do not mistakenly describe non-AR scenes as AR content, even in scenarios involving unusual object placements.\nThe prompt T significantly improves description accuracy compared to the prompt G, as shown in Figure 4. For instance, GPT's Description True Positive Rate (TPRD) increases from 38.9% with prompt G to 73.8% with prompt T, emphasizing the importance of tailored prompts. While Gemini outperforms GPT and Claude under prompt G, GPT achieves the highest TPRD under prompt T (73.8%), surpassing Gemini (71.8%) by 2%. This result indicates that GPT has the potential to leverage explicit task prompts more effectively when analyzing AR scenes. In contrast, Claude consistently underperforms, with TPRD as low as 14.1% under prompt G, improving to only 34.9% under prompt T. Detailed analysis reveals that Claude often generates high-level summaries of objects in the scene without capturing the relationships among them, limiting its ability to accurately describe AR-specific content.\nAnalyzing TPRD across different levels of complexity reveals a consistent decline in performance as complexity increases, mirroring the trends observed in perception results. Notably, GPT outperforms both Gemini and Claude on medium and hard AR scenes. This highlights GPT's strength in tasks requiring detailed AR content descriptions, particularly in more complex scenes where spatial reasoning and contextual understanding are critical.\n5.2 Categorical Analysis of VLMs' Responses\nWe analyze AR samples across the five response categories defined in Section 4, examining key observations, contributing factors, and potential reasons for the VLMs' behavior.\nCategory 1: Accurate AR Recognition. In this category, VLMs demonstrate strong performance in identifying AR objects. We observe that AR content with characteristics such as text overlays, low-quality rendering (e.g., uniform pixel distributions), or implausible physical attributes (e.g., lighting, shape, size mismatches, unnatural object placement like floating objects, or unrealistic transparency) is often recognized correctly, as shown in Figure 2(k, 1, e, d). These attributes are prevalent in common AR scenarios and are likely well-represented in the VLMs' training datasets, making the models adept at detecting them.\nInterestingly, certain visual cues enhance recognition accuracy. For instance, the presence of QR codes or hands provides strong contextual clues. As shown in Figure 5, a virtual sign with a QR code is correctly identified, while the same virtual sign without it is misclassified. Similarly, Figure 6 shows that the presence of hands improves AR recognition, while their absence causes VLMs to misclassify real grids as virtual content. Additionally, implausible physical interactions, such as a pot placed on a cat, further help the models identify AR content. The ability of VLMs to identify these features can be attributed to the self-attention mechanism in transformer-based architectures [8], which processes an image as a sequence of patches, capturing global context and spatial relationships. This design enables the models to effectively understand and contextualize relationships, actions, and attributes within an image.\nCategory 2: Partial AR Recognition. In this category, VLMs misinterpret AR content, in three primary ways. First, real objects are recognized as virtual. For instance, as shown in Figure 7, a real pair of sneakers is identified correctly when no virtual sneakers are present. However, adding virtual sneakers to the scene causes the VLM to misclassify the real sneakers as virtual. Second, virtual content is only partially recognized. For example, in a mixed scene, a virtual toy elephant and a virtual pot are not recognized as virtual objects, as illustrated in Figure 8. Shadows rendered on these virtual objects can enhance their realism, allowing them to evade detection.Lastly, virtual objects may be misclassified entirely or have their properties misunderstood. For example, a toy butterfly might be incorrectly identified as another type of object, possibly due to gaps in the training data. These issues are most common when virtual and real elements are combined, creating ambiguity that misleads VLMs. Consequently, the model's response to a certain object can vary depending on its surrounding elements, suggesting that these errors likely stem from the spatial reasoning capabilities of transformer-based models.\nCategory 3: Missed AR Recognition. Failures in this category arise when realistic virtual content adheres to physical laws or is seamlessly integrated into real-world settings. These scenarios can mislead VLMs, making it difficult for them to identify virtual elements. For instance, virtual objects with realistic shadows frequently evade detection, especially in environments with weak visual cues. These include dark surfaces, low-light conditions, or inverse lighting directions (Figure 2(j)). Similarly, virtual objects positioned on digital screens can also mislead VLMs, as seen in Figure 9(a).\nWe also observe several interesting cases within this category. For example, virtual objects with inadequate transparency are misidentified as real (Figure 9(b)). Some virtual content with low rendering quality may be misclassified as realistic \"fake objects\", as such \"fake objects\" are often encountered in daily life (Figure 9(c)). Likewise, objects that defy physical laws-such as floating slightly but remaining attached to the surfaces of real objects-still appear plausible enough to evade detection (Figure 2(j)). Moreover, virtual objects that do not typically appear in AR settings further exacerbate the recognition problem (Figure 2(h)).\nThese observations highlight several limitations of VLMs. For instance, they struggle with depth perception in 2D images and have trouble interpreting situations that challenge common-sense knowledge [25], making it difficult to identify violations of some physical laws. Additionally, we speculate that many models may have limited exposure to diverse or atypical AR-specific samples during training, potentially hindering their ability to effectively understand AR scenes.\nCategory 4: False AR Detection. No instances of false AR detection were observed, showcasing the model's robustness in accurately distinguishing non-AR images. To further investigate the generalizability of this observation, our ongoing work involves constructing real-world scenarios with implausible lighting and object placements. Examples include shining a light onto a real apple or attaching a pen unnaturally to a shelf.\nCategory 5: Accurate Non-AR Recognition. VLMs show strong performance in recognizing non-AR images. Notably, even real-world objects with unconventional placements (Figure 9(d)) were correctly identified as real rather than virtual, suggesting that VLMs are less likely to be confused by physically plausible yet atypical arrangements.\n5.3 User Study\nTo compare human performance with VLMs in identifying and describing AR scenarios, we conducted a user study approved by the Duke University Campus Institutional Review Board (protocol number: 2020-0292). We recruited five participants from the Duke community, comprising four males and one female, aged between 23 and 29 years. Participants were informed that they would be shown a mix of AR and non-AR images and were tasked with providing responses based on the prompt T. To facilitate the study, we developed a custom Jupyter Notebook interface that displayed each image at 8 \u00d7 8 inches (20.32 \u00d7 20.32 cm), adjusted to fit the height of a laptop screen, allowing the entire image to be viewed clearly without scrolling. Participants' responses were recorded through the interface by clicking buttons to label their evaluation outcomes.\nPerception Performance: The average True Positive Rate for Perception (TPRP) across easy and medium levels closely aligned with the performance of VLMs, trailing the best TPRP of VLMs by only 1.7% and 0.7%, respectively. However, on hard-level examples, the average TPRp surpassed the best TPRp of VLMs by 8.1%. This improvement can be attributed to the higher object density in hard-level images, which heightened participants' suspicion and improved their ability to accurately identify virtual objects. For the same reason, each participant misclassified 1 to 8 non-AR images as AR images.\nDescription Performance: The average True Positive Rate for Description (TPRD) across medium and hard levels exceeded the best TPRD of VLMs by 7.7% and 27.1%, respectively, while falling only 1.1% short at the easy level. The superior performance at medium and hard levels can likely be attributed to humans' ability to learn from observed images and apply reasoning skills. Despite their enhanced reasoning, humans were prone to fatigue after reviewing approximately 100 images. Furthermore, we observed that the overall decline in human performance with increasing complexity levels closely mirrored the pattern exhibited by VLMs.\nFactors Influencing Performance: Recognition of AR content among participants was influenced by various factors, such as object texture, size, lighting, shadows, and positioning. Across the study, participants misclassified virtual objects as real or vice versa in 7.6% to 13.8% of the images, amounting to 24 to 44 misclassifications per participant. Interestingly, most misclassified images overlapped with those where VLMs also struggled, indicating similarities in the challenges faced by humans and VLMs. Moreover, 2 participants showed a tendency to focus on objects that were closer and centrally positioned in the image, particularly during the later stages of the study. This finding provides guidance for fine-tuning VLMs to prioritize closer and central areas of images, aligning their attention more closely with human perception.\nThe study highlights both similarities and differences between human performance and VLMs, demonstrating the potential of VLMs to assess AR experience quality while also revealing areas where they fall short. To bridge this gap and align VLM performance more closely with human users, we plan to fine-tune VLMs using a manually curated dataset featuring diverse AR scenarios accompanied by corresponding textual descriptions."}, {"title": "6 CONCLUSION AND FUTURE WORK", "content": "In this work, we evaluate VLMs for AR scene understanding using the DiverseAR dataset, the first AR dataset to assess VLMs' ability to identify and describe virtual content. Our results reveal that while VLMs perform well in detecting prominent AR elements, they struggle with seamlessly integrated virtual content, especially under conditions of realistic lighting, shadows, and physical plausibility.\nBuilding on this study, we are enhancing the DiverseAR dataset by gathering a larger collection of AR images, each annotated with quantifiable properties such as virtual object size, contrast, and position, while also collecting human perception data for comparison. To further refine VLM performance, we are exploring AR-specific fine-tuning methods to align model capabilities with human perception, focusing on factors like shadow realism, lighting consistency, and object alignment.\nBeyond image-level assessment, we aim to expand our evaluation to include quality factors associated with temporal data modalities, such as the drift and jitter of virtual content. Building on the success of our previous non-VLM-based approaches [10,19], we will explore VLM-based methods to further analyze and assess these factors."}]}