{"title": "DISC: Plug-and-Play Decoding Intervention with Similarity of Characters for Chinese Spelling Check", "authors": ["Ziheng Qiao", "Houquan Zhou", "Yumeng Liu", "Zhenghua Li", "Min Zhang", "Bo Zhang", "Chen Li", "Ji Zhang", "Fei Huang"], "abstract": "One key characteristic of the Chinese spelling check (CSC) task is that incorrect characters are usually similar to the correct ones in either phonetics or glyph. To accommodate this, previous works usually leverage confusion sets, which suffer from two problems, i.e., difficulty in determining which character pairs to include and lack of probabilities to distinguish items in the set. In this paper, we propose a light-weight plug-and-play DISC (i.e., decoding intervention with similarity of characters) module for CSC models. DISC measures phonetic and glyph similarities between characters and incorporates this similarity information only during the inference phase. This method can be easily integrated into various existing CSC models, such as ReaLiSe, SCOPE, and ReLM, without additional training costs. Experiments on three CSC benchmarks demonstrate that our proposed method significantly improves model performance, approaching and even surpassing the current state-of-the-art models.", "sections": [{"title": "1 Introduction", "content": "Given an input sentence, Chinese spelling check (CSC) aims to detect incorrect characters and modify each into an correct character (Yu and Li, 2014; Xu et al., 2021). Spelling errors degrade reading efficiency, and sometimes even lead to misunderstanding. The authority or attitude of the writer may be doubted if their document contains simple spelling errors. Moreover, spelling errors substantially hurt the performance of subsequent NLP models.\nAs is well known, spelling errors in Chinese texts have three major sources, i.e., 1) from keyboard typing with some input methods, 2) from image or document scanning with some optical character recognition (OCR) software, and 3) from speech-to-text translation with some automatic speech recognition (ASR) software. Nowadays, most Chinese"}, {"title": "2 The Basic CSC Model", "content": "Given an input sentence consisting of n characters, denoted as x = x_1x_2 ... x_n, the goal of a CSC model is to output a corresponding correct sentence, denoted as y = y_1y_2 ... y_n, in which all erroneous characters in \u00e6 are replaced with the correct ones.\nPresently, mainstream approaches treat CSC as a character-wise classification problem (Zhang et al., 2020; Liu et al., 2021; Xu et al., 2021), i.e., determining whether a current character should be kept the same or be replaced with a new character.\nEncoding. Given \u00e6, the encoder of the CSC model generates representations for each character:\nh_1...h_n = Encoder(x).  (1)\nTo leverage the power of pre-trained language models, a BERT-like encoder is usually employed.\nClassification. For each character position, for instance h_i, the CSC model employs MLP and softmax layers to obtain a probability distribution over the whole character vocabulary V:\np(y | x, i) = softmax(MLP(h_i) )[y].  (2)\nDuring the evaluation phase, the model selects the character with the highest probability, i.e., y* =\narg max_{y\\in V} p(y | x, i).\nTraining. The typical training procedure consists of 2-3 steps for the CSC task. First, automatically synthesize large-scale CSC training data by replacing some characters with others randomly, sometimes constrained by a given confusion set. Second, train the CSC model on the synthesized training data. Third, fine-tune the model on a small-scale in-domain training data, if the data is available."}, {"title": "3 Our Approach", "content": "In this paper, we propose a simple plug-and-play module to intervene in the classification (or prediction) process of any off-the-shelf CSC model. The basic idea is to adjust the probability distribution according to the similarity between a candidate character y and the original character x_i:\nScore(x, i, y) = p(y | x,i) + \u03b1 \u00d7 Sim(x_i, y),  (3)\nwhere Sim(\u00b7, \u00b7) gives the similarity between two characters, and \u03b1 is a hyperparameter and we set \u03b1 = 1.1 for all datasets and basic models according to a few preliminary experiments. We use Score(\u00b7, \u00b7, \u00b7) to denote the replacement likelihood since the value is no longer a probability.\nOur experiments show that by encouraging the model to prefer similar characters, our approach achieves a consistent and substantial performance boost on all CSC benchmark datasets."}, {"title": "3.1 Phonetic Similarity", "content": "Given two characters, we employ the pypinyin library to obtain the Pinyin sequences, e.g., \u201c\u5fe0\u201d (zhong) and \u201c\u4ef2\u201d (zhong), and then compute the phonetic similarity based on the edit distance over their Pinyin sequences:\nSim^p(c_1, c_2) = 1 - \\frac{LD(py(c_1), py(c_2))}{len(py(c_1) + py(c_2))},  (5)\nwhere LD(\u00b7, \u00b7) gives the Levenshtein distance, and len(\u00b7) gives the total length of the two sequences."}, {"title": "Handling polyphonic characters", "content": "Given two characters, we enumerate all possible Pinyin sequences of each character, and adopt the combination that leads to the highest similarity.\nWe have also tried more sophisticated strategies. For instance, we follow Yang et al. (2023b) and give higher weights to certain phoneme (consonant or vowel) pairs, since they are more likely to cause spelling errors. However, our preliminary experiments show that our simple strategy in Eq. (5) works quite robustly."}, {"title": "3.2 Glyph Similarity", "content": "According to Liu et al. (2010), 83% of Chinese spelling errors are related to pronunciation, while 48% are with glyphs, indicating that a considerable proportion is related to both. Therefore, it is necessary to consider the glyph information when computing character similarity.\nPinyin sequences can largely encode the phonetics of Chinese characters. In contrast, it is much more complex to represent character glyphs. In this work, we compute and fuse glyph similarity from four aspects:\nSim^g(c_1, c_2) = \\frac{\\sum_{i=1}^{4} Sim_i(c_1, c_2)}{4}  (6)"}, {"title": "Four-corner code", "content": "The four-corner method is widely used in Chinese lexicography for indexing characters. Given a character, it gives four digits ranging from 0 to 9, corresponding to the shapes at the four corners of the character\u2019s glyph, respectively. For instance, the four-corner code is 5033 for \u201c\u5fe0\u201d, and 2520 for \u201c\u4ef2\u201d.\nThen, we use the digit-wise matching rate between two codes as the similarity:\nSim_1(c_1, c_2) = \\frac{\\sum_{i=1}^{4} 1(FC(c_1)[i] = FC(c_2)[i])}{4}  (7)\nwhere FC(\u00b7) gives the four-digit code, and 1 is the indicator function."}, {"title": "Structure-aware four-corner code", "content": "One important feature of Chinese characters is that a complex character can usually be decomposed into simpler parts, and each part corresponds to a simpler character or a radical. Most radicals are semantically equivalent to some character, e.g., \u201c\u4ebb\u201d to \u201c\u4eba\u201d. Such structural decomposition directly reveals how characters are visually similar to each other. Motivated by this observation, we design a structure-aware four-corner code for each character. For example,\n\u201c\u5fe0\u201d:C5000C3300 (\u201c\u4e2d\u201d: 5000;\u201c\u5fc3\u201d: 3300)\n\u201c\u4ef2\u201d: B8000B5000 (\u201c\u4eba\u201d: 8000;\u201c\u4e2d\u201d: 5000)\nwhere \"C\" leading a four-coner code means up-down structure, and \u201cB\u201d means left-right structure.\nThen we compute the similarity based on the Levenshtein distance as follows:\nSim_2(c_1, c_2) = 1 - \\frac{LD(SFC(c_1), SFC(c_2))}{len(SFC(c_1) + SFC(c_2))}  (8)\nwhere SFC(\u00b7) gives the structure-aware code of a character."}, {"title": "Stroke sequences", "content": "Four-corner codes focus on the shapes of the four corners. Some very similar characters may obtain quite different codes, e.g., \u201c\u6728\u201d(4090) vs. \u201c\u672c\u201d (5023). To address this issue, we utilize stroke sequence information, which encodes how a character is handwritten stroke by stroke. For example,\n\u201c\u6728\u201d:\u4e00\u4e00\u30ce\u3001(4 strokes)\n\u201c\u672c\u201d:\u4e00\u4e00\u30ce\u3001 (5 strokes)\nThen we compute two similarity metrics from two complementary viewpoints. The first metric is based on Levenshtein distance:\nSim_3(c_1, c_2) = 1 - \\frac{LD(SS(c_1), SS(c_2))}{len(SS(c_1) + SS(c_2))}  (9)\nwhere SS(\u00b7) gives the stroke sequence of a character.\nThe second metric considers the longest common subsequence, i.e., LCS(\u00b7):\nSim_4(c_1, c_2) = \\frac{LCS(SS(c_1), SS(c_2))}{max(len(SS(c_1)), len(SS(c_2)))}  (10)\nAccording to Eq. (4), and supposing \u03b2 = 0.7, we get the similarity between \u201c\u5fe0\u201d and \u201c\u4ef2\u201d being:\n0.  7 \u00d7 1 + 0.3 \u00d7 \\frac{0 + 0.56 + 0.57 + 0.5}{4} = 0.82."}, {"title": "4 Experimental Setup", "content": "4.  1 Datasets\nFollowing the conventions of previous work, we employ the test sets of the SIGHAN 13/14/15 datasets (Wu et al., 2013; Yu et al., 2014; Tseng et al., 2015) as our evaluation benchmarks.\nHowever, many previous studies have pointed out that the SIGHAN datasets may not represent real-world CSC tasks, as they are derived from Chinese learner texts. To address this limitation, we also conduct experiments on the ECSpell (Lv et al., 2023) and LEMON (Wu et al., 2023) datasets, which are derived from Chinese native-speaker (CNS) texts and encompass a wide range of domains. It is worth noting that LEMON does not have a dedicated training set, making it an excellent test set for evaluating a model\u2019s generalization ability."}, {"title": "4.2 Baseline Models", "content": "We select three representative BERT-style models as our baselines: ReaLiSe, SCOPE, and ReLM.\nThe ReaLiSe model (Xu et al., 2021) employs multi-modal technology to capture semantic, phonetic, and glyph information. The SCOPE model (Li et al., 2022) is one of the SOTA models for CSC, which enhances model correction performance by introducing a character pronunciation prediction task. The ReLM model (Liu et al., 2024) treats CSC as a non-autoregressive paraphrasing task, standing out as a new SOTA model.\nAdditionally, we include some of the latest work (Cheng et al., 2020; Huang et al., 2023) for performance comparison.\nIn the era of LLMs, researchers have begun using LLMs to explore the CSC field. We present the results of representative LLMs on certain benchmarks for comparison, including the top-performing GPT series in terms of overall capa-"}, {"title": "4.3 Evaluation Metrics", "content": "The CSC task comprises two subtasks: error detection and error correction. Following the previous work (Zhang et al., 2020), we report the precision (P), recall (R), and F1 scores at the sentence level for both subtasks. Additionally, we also evaluate the models with the False Positive Rate (FPR) metric (Liu et al., 2024), which quantifies the CSC model\u2019s frequency of over-correction, i.e., incorrectly identifying correct sentences as erroneous."}, {"title": "4.4 Hyperparameters", "content": "Hyperparameters \u03b1 and \u03b2 denote the weights assigned to overall similarity and phonetic similarity, respectively. As detailed in Section 6 on grid search results, we set \u03b1 = 1.1 in Eq. 3 and \u03b2 = 0.7 in Eq. 4 for all experiments."}, {"title": "5 Main Results", "content": "Results on SIGHANs. demonstrates that the addition of the DISC module in the decoding process leads to notable improvements across all the compared models, reaching state-of-the-art performance. Specifically, ReaLiSe +"}, {"title": "Robustness of similarity hyperparameters", "content": "As illustrated in , the model's precision steadily improves as \u03b1 increases. This is be-"}]}