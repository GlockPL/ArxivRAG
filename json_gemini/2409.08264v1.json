{"title": "WINDOWSAGENTARENA: EVALUATING MULTI-MODAL OS AGENTS AT SCALE", "authors": ["Rogerio Bonatti", "Dan Zhao", "Francesco Bonacci", "Dillon Dupont", "Sara Abdali", "Yinheng Li", "Justin Wagle", "Kazuhito Koishida", "Arthur Bucker", "Lawrence Jang", "Zack Hui"], "abstract": "Large language models (LLMs) show remarkable potential to act as computer\nagents, enhancing human productivity and software accessibility in multi-modal\ntasks that require planning and reasoning. However, measuring agent performance\nin realistic environments remains a challenge since: (i) most benchmarks are lim-\nited to specific modalities or domains (e.g. text-only, web navigation, Q&A,\ncoding) and (ii) full benchmark evaluations are slow (on order of magnitude of\ndays) given the multi-step sequential nature of tasks. To address these challenges,\nwe introduce the WINDOWSAGENTARENA: a reproducible, general environment\nfocusing exclusively on the Windows operating system (OS) where agents can\noperate freely within a real Windows OS and use the same wide range of appli-\ncations, tools, and web browsers available to human users when solving tasks.\nWe adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse Win-\ndows tasks across representative domains that require agent abilities in planning,\nscreen understanding, and tool usage. Our benchmark is scalable and can be\nseamlessly parallelized in Azure for a full benchmark evaluation in as little as 20\nminutes. To demonstrate WINDOWSAGENTARENA's capabilities, we also intro-\nduce a new multi-modal agent, Navi. Our agent achieves a success rate of 19.5%\nin the Windows domain, compared to 74.5% performance of an unassisted hu-\nman. Navi also demonstrates strong performance on another popular web-based\nbenchmark, Mind2Web. We offer extensive quantitative and qualitative analysis\nof Navi's performance, and provide insights into the opportunities for future re-\nsearch in agent development and data generation using WINDOWSAGENTARENA.", "sections": [{"title": "INTRODUCTION", "content": "The concept of an agent is not new to the field of artificial intelligence, with its definition and\ntaxonomy being subject to debate for decades (Minsky, 1988; Shoham, 1993; Franklin & Graesser,\n1996; Jennings et al., 1998). In the broadest sense, an agent is \"anything that can be viewed as\nperceiving its environment through sensors and acting upon that environment through actuators\"\n(Russell & Norvig, 2016). As most of our daily personal and work tasks move to and concentrate\nwithin the digital realm, the development of computer agents that can perceive, plan, and act in\nvirtual environments holds great potential to enhance human productivity and software accessibility.\nHigh-capacity multi-modal models, such as GPT-4V (Achiam et al., 2023), GPT-40, and Gem-\nini (Team et al., 2023), now serve as the main backbone of agents tested in benchmarks focus-\ning on domains such as web navigation (Deng et al., 2023) mobile screens (Rawles et al., 2024b),\nand software development (Jimenez et al., 2023). The complexity of realistic human workflows,\nhowever, is not fully captured in most testing platforms. We often perform complex cross-domain"}, {"title": "2 PRIOR WORK", "content": "Advances in LLMs and foundation models have led to signif-\nicant interest in their potential as autonomous agents. Recent improvements in multi-modal (e.g.,\nlanguage and vision) capabilities like image captioning and visual reasoning have spurred applica-\ntions to agent perception and action grounding such as cross-attention between modalities (Alayrac\net al., 2022), trainable visual expert models (Wang et al., 2023), mapping images into frozen LLM\nembedding spaces (Tsimpoukelli et al., 2021), fine-tuning with images/instruction pairs (Liu et al.,\n2023a), and more. Many closed-source LLMs have followed suit to incorporate vision and other\nmodalities such as GPT-4V (OpenAI, 2023) and Gemini (Google-Research, 2023). Various ex-\namples of text-only and vision-language agent models have seen testing on the web, desktop, and\nmobile environments: UFO (Zhang et al., 2024a), CC-Net (Humphreys et al., 2022), AiTW (Rawles\net al., 2024b), CogAgent (Hong et al., 2023), MM-Navigator (Yan et al., 2023), SeeAct (Zheng et al.,\n2024b), WebAgent (Gur et al., 2023), OS-Copilot (Wu et al., 2024), among others.\nSeveral benchmarks attempt to test different facets of agentic capabilities. Some benchmarks rely on a static dataset of human-collected data to evaluate\nagent performance, such as Mind2Web (Deng et al., 2023), WebLinx (L\u00f9 et al., 2024), PixelHelp (Li\net al., 2020), MoTiF (Burns et al., 2021), AiTW (Rawles et al., 2024b), and OmniAct (Kapoor et al.,\n2024). The downside of static benchmarks is that they are unable to capture the full range of trajec-\ntories which an agent might take to solve tasks as these can differ from human demonstration.\nInteractive benchmarks, in contrast, can provide reward signals at task completion that allow for\nmore dynamic and open-ended agent evaluation. These evaluations can occur in pre-defined environ-\nments or in open-ended fashion allowing the agent to make use of any tools available. GAIA (Mialon\net al., 2023), for instance, allows the agent to use the web and tools to answer complex questions,\nbut does not specify a closed environment in which the agent can actually operate. On acting in\nspecific environments, notable examples are MiniWoB++ (Shi et al., 2017), WebArena (Zhou et al.,\n2024), VisualWebArena (Koh et al., 2024), WorkArena (Drouin et al., 2024), MMInA (Zhang et al.,\n2024b), AndroidWorld (Rawles et al., 2024a), and OSWorld (Xie et al., 2024). Closest to our work is\nOSWorld, which provides a benchmark for agents to operate within OS environments, with a focus\non Linux. We extend and build upon this work to focus on Windows OS, as discussed in Section 3.2.\nA key challenge which is often not discussed is the time it takes to com-\nplete an evaluation on a benchmark. Several of the works cited here have tasks which require about\n10-20 steps to complete; assuming each step takes 10-30 seconds to process using local models\ncombined with a cloud API call, this translates to an order of magnitude of one or more days to\nevaluate hundreds of tasks sequentially in a single thread. Most works do not address this challenge\ndirectly. While OSWorld allows simulations to be parallelized within a single machine, this does\nnot scale beyond a low, single digits number of agents within a typical development machine due to\nmemory and CPU constraints. We address this challenge by providing native cloud parallelization\nwhich can scale up to the number of benchmark tasks, as discussed in Section 3.3."}, {"title": "3 THE WINDOWSAGENTARENA ENVIRONMENT", "content": "Similar to Koh et al. (2024) and Xie et al. (2024), we formalize the agent behavior as a partially\nobservable Markov decision process (POMDP) (S, O, A, T,R) with state space S, observation\nspace O (3.1.1), action space A (3.1.2), transition function $T:S \\times A \\rightarrow S$, and reward func-\ntion $R: S \\times A \\rightarrow R$. Given current observation $o_t \\in O$, an agent generates executable action\n$a_t \\in A$, resulting in a new state $s_{t+1} \\in S$ and a new partial observation $O_{t+1} \\in O$. The process\nrepeats until the agent outputs a termination action (DONE or FAIL) or when the maximum number\nof steps is reached (t > tmax).\nThe reward function $R : S \\times A \\rightarrow [0,1]$ returns a non-zero value at the final step if the agent\nstate achieves the task objective, or if the agent accurately predicts failure for an infeasible task;\ndepending on the task, the reward either returns 1 if the outcome is binary (correct vs. incorrect)\nor a non-zero decimal value if task evaluation depends on similarity of agent outcome to the task's\nobjective. In all other scenarios, it returns 0. We note that the benchmark can potentially be modified\nto allow for intermediate non-zero rewards along the episode to account for partial completion via\nmore complex evaluation functions\u2014we leave this as possible future work."}, {"title": "3.1.1 OBSERVATION SPACE", "content": "The observation space O represents the OS information available to the agent at each time step. In\nits most general form, this includes the task instruction, the clipboard content, metadata about the\ncurrent session (e.g. title of the current foreground window, titles for all other windows or browser\ntabs currently open), and a representation of the current screen. The screen can be represented in\nvarious formats such as a screenshot (RGB array), DOM (Document Object Model) tree, UI Au-\ntomation tree, or using pixel-based models to augment the raw data and create Set-of-Marks (Yang\net al., 2023) visual lists of interactive elements (e.g. buttons, images, text fields) as described in\nSection 4.1. For each task, we use a deterministic script to set the initial state of the system st=0\n(used to generate the initial observation ot=0) which might involve opening specific applications,\ndownloading files, or navigating to webpages. We provide more details in Appendix A.1."}, {"title": "3.1.2 \u0410\u0421TION SPACE", "content": "We provide two types of action spaces: free-form\npyautogui/python code execution, and function wrap-\npers through the Computer class. The Computer class\nis described in the prompt and allows the agent to use\nsimple and complex OS-related functions with relatively\nhigh precision or, in some cases, to interact directly with\nthe IDs of interactive elements on the screen rather using\nabsolute pixel coordinates (Table 2). We provide more\ndetails on the full action space in Appendix A.2."}, {"title": "3.1.3 REWARD EVALUATION BASED ON EXECUTION", "content": "We follow the guidelines from OSWorld Xie et al. (2024),\nand use post-processing scripts to evaluate task comple-\ntion. Depending on the task, this might involve reading\nthe state of system or app settings, checking the content\nof files, or dynamic functions that use the web to evalu-\nate a result. We provide examples in Table 3 and leave\nadditional details in Appendix A.3."}, {"title": "3.2 TASK CURATION FOR WINDOWS OS", "content": "The initial release of WINDOWSAGENTARENA consists of 154 diverse tasks spanning applications\nwhich represent the typical user workloads within Windows OS: editing documents and spreadsheets"}, {"title": "3.3 DEPLOYMENT INFRASTRUCTURE", "content": "We designed the infrastructure behind WINDOWSAGENTARENA to support flexible local execution\nas well as scalable and secure cloud parallelization via Azure. The core of our system is a Docker\ncontainer that hosts the Windows 11 virtual machine (VM). Within the container, we deploy a client\nprocess for task scheduling and configuration as well as the agent and the evaluation scripts. The\nVM is our main simulation environment: a Python Flask server acts as the bridge between the\ncontainer and the VM by receiving commands from the client processes, executing them within the\nVM, and sending observations and files back to the client. Fig. 3 illustrates the setup.\nDue to licensing restrictions, we cannot provide a pre-built Windows 11 snapshot; however, our\nopen-source instructions explain how users can download a trial image from Microsoft's servers and\nuse our scripts to automatically prepare it for the benchmark. The VM is deployed using QEMU\nand KVM with a process adapted from the dockur/windows Docker image."}, {"title": "4 BASELINE RESULTS AND ANALYSIS", "content": "In this section, we present the implementation details and results of our agent, Navi, on WIN-\nDOWSAGENTARENAalong with an analysis of its strengths and limitations."}, {"title": "4.1 Navi: AN AGENT FOR WINDOWS NAVIGATION", "content": "We develop a new multimodal agent, Navi, to operate autonomously within the Windows environ-\nment. We use chain-of-thought prompting (Wei et al., 2022) to instruct the agent to reason about\nthe current state of the computer, its own past actions, and decide on the most appropriate next ac-\ntion (full prompts found in Appendix D). All variations of our agent receive as input the title of the\ncurrent foreground window, titles for all other windows or browser tabs currently open, and a rep-\nresentation of the current screen. We consider several methods to process the screen representation\nfor the agent as input and create Set-of-Marks (SoMs):\n\u2022 UIA tree parsing: extracts the visible elements from the Windows UI Automation tree;\n\u2022 DOM tree parsing: extracts the visible elements from the DOM tree (browser only);\n\u2022 OCR: proprietary and open models (Tesseract (Smith, 2007));\n\u2022 Icon and image detection: proprietary and open models (Grounding DINO (Liu et al., 2023b));\n\u2022 OmniParser: proprietary model trained on multi-element detection in screens (Lu et al., 2024).\nIt detects text, icons, and images, and provides icon captioning in text format."}, {"title": "4.2 WINDOWSAGENTARENA BASELINE RESULTS", "content": "We summarize the main baseline results in Table 4 where we compare variants Navi using dif-\nferent SoM and reasoning models. Fig. 5 displays an example of a successful task in WIN-\nDOWSAGENTARENA using proprietary models. Additional examples can be found in Appendix B.\nWe highlight some key observations and insights below:\nGeneralist zero-shot VLM agents are still far from human performance: The best of our agents\n(UIA combined with Omniparser and GPT-4V-1106) achieves a success rate of 19.5%, which is low\nin absolute terms when compared to human performance at 74.5%. This discrepancy is not uni-\nform across categories as the agent performs significantly better in tasks with clear text-dominant\ninterfaces (Web Browser, Windows System) and worse in tasks that rely heavily on keyboard short-\ncuts and icons (Office, Windows Utils). However, Navi's performance is comparable to those from\nsimilar benchmarks like OSWorld and AndroidWorld which also test zero-shot generalist agents.\nPrecise Set-of-Marks are crucial for agent performance: The quality of visual prompting tech-\nniques has a significant impact on the agent's performance. As shown in Fig. 6a, we commonly\nfind that imprecise SoM bounding box boundaries lead to incorrect element selection. As seen in\nTable 4, adding high-quality UIA markers in addition to pixel-based element detectors boosts per-\nformance by 57% for Omniparser, 52% for open-sourced models, and 15% for proprietary pixel\nmodels when using GPT-4V-1106. Using UIA markers, however, can take from a few seconds up to\nseveral minutes to be queried depending on the complexity of the screen.\nVisual-language mis-alignment is a common cause of failures: We find that a common cause\nof agent failures spanned from the agent's inability to correctly align its text output with its visual\nunderstanding of the screen. In Fig. 6b we show an example where the agent correctly outputs the\naction textual description (move to the red color), but fails to select the visual ID (#59 corresponds to\nthe yellow palette). This behavior is likely caused by the mismatch between the training data distri-\nbution of VLMs and OS tasks, which typically contain hundreds of small-scale markers. We believe\nthat a main reason behind Omniparser's higher performance lies in its icon captioning capabilities,\nwhich help the agent directly link visual context IDs with textual descriptions.\nPerformance gap between base models: We find a large performance gap between Phi3-V, GPT-\n40-mini, GPT-40, and GPT-4V-1106 with the latter achieving better performance in most categories,\nmore than double of GPT-40's success rate in the case of UIA+Omniparser. As expected, smaller\nmodels like Phi3-V and GPT-40-mini perform worse than larger models. We note that Phi3-V tends\nto hallucinate when planning multiple steps ahead or presented with long input contexts."}, {"title": "4.3 ADDITIONAL Navi RESULTS FOR THE MIND2WEB BENCHMARK", "content": "In Table 5, we evaluate Navi on the Mind2Web benchmark (Deng et al., 2023) in order to assess our\nagent's performance and robustness in a separate setting. Navi achieves state-of-the-art performance\nwhen compared with the original SeeAct agent Zheng et al. (2024a). We find that a combination of\nimage and DOM SOM prompting provides the best agent performance, similarly to the results from\nWINDOWSAGENTARENA combining UIA tree with pixel-based markers. We provide more details\non evaluation setup in Appendix C."}, {"title": "5 DISCUSSION", "content": "Based on the results presented in this paper, we would like to pose different discussion points related\nto the development of generalist OS agents:\nThough our work focuses on autonomous agents, we\nbelieve that considering a human-in-the-loop approach could benefit performance and effectiveness\nin helping the end user. At a cost, agents could choose a question action to actively seek user input\nfor clarification, additional info, or direct human intervention. However, this approach would be\nharder to evaluate and benchmark, likely needing human involvement in the evaluation process.\nMost modern agents are based on large models with generalist\nknowledge. However, we believe that a fruitful research direction would be to treat an OS agent as\nan agent of agents which uses specialized fine-tuned sub-systems for specific domains.\nA key challenge in agent development is the lack of ground-\ntruth action data for model training/tuning, resembling difficulties found in robotics research. Rein-\nforcement learning could be a potential solution, by leveraging data from imperfect actions, so long\nas the objective functions for rewards are well-defined (e.g. task completion in our benchmark). One\ndirection is to use WINDOWSAGENTARENA to generate data at scale for agent RL training.\nWhile we focus on fully autonomous agents with free-form\nactions, we also see potential for agents that can use a stricter predefined action function or skill\nlibrary. This can offer higher execution precision as errors would instead stem mostly from the\nagent's planning and decision-making in stitching together action primitives.\nIn the AI agent design process, it's crucial to adhere to ethical guidelines\nand principles of responsible AI use. Users should have the capability to understand, direct, diag-\nnose, and if needed, override AI actions. We are committed to the continued development of AI\ntechnologies that respect user privacy, ensure fairness, and contribute positively to society."}, {"title": "6 CONCLUSION", "content": "We introduced WINDOWSAGENTARENA, an open-source and reproducible environment for evalu-\nating multi-modal agents within a real Windows OS. Our platform provides 154 diverse tasks that re-\nquire abilities including, but not limited to, planning, screen understanding, and tool usage, offering\na realistic framework for the development and testing of autonomous computer agents. Our bench-\nmark is scalable and can be parallelized in Azure, significantly reducing the evaluation time for full\nbenchmark runs. We also introduced a multi-modal agent, Navi, and demonstrated its capabilities\nacross different configurations. WINDOWSAGENTARENA's extensive quantitative and qualitative\nanalysis of agent performance provides valuable insights for future research in agent development.\nBy making our code, benchmark, and baseline models available, we aim to facilitate further research\nand accelerate the development of more efficient and capable computer control agents."}, {"title": "A.1 OBSERVATION SPACE", "content": "The observation space available to the agent consists of the following components:\nForeground and background window titles. Extracted using the pygetwindow library.\nClipboard content. If text, we copy the clipboard content with pyperclip. If image, we store an\nVLM-generated description of the copied area.\nWe extract the Windows UI Automation tree (UIA tree) using the pywinauto\nlibrary. We do not feed the UIA tree directly to the Navi agent. Instead,\nsome agent configurations parse the tree to extract relevant information, such as the names of the\nelements, their types, and their positions on the screen. These elements are then used to create a\nSet-of-Marks (SoM) in the screenshot to guide the agent's actions.\nWe use screenshot of the previous screen to help the agent under-\nstand the context of the current screen and also identify if the task has been completed. We capture\nit as an RGB array with resolution of 1440 \u00d7 900 \u00d7 3 pixels.\nWe capture the current screenshot as an RGB array with resolution\nof 1440 x 900 \u00d7 3 pixels. Depending on the agent configurations we use different methods for\nannotating the screenshot with Set-of-Marks (SoM) to guide the agent's actions. We display diverse\nexamples of annotations generated by our internal models, open-sourced models, and\nUIA-based parsing."}, {"title": "A.2 ACTION SPACE", "content": "As described in the main paper, we utilize two types of action spaces: pyautogui and a custom\ndefined Computer class wrapper. Our Computer functions/actions can be characterized into three\nbroad categories: GUI functions, keyboard functions, and OS functions. See Table 6 for details."}, {"title": "A.3 REWARD EVALUATION BASED ON AGENT EXECUTION", "content": "In our benchmark, task evaluation through the appropriate evaluator function can return either zero\nreward or non-negative reward. As mentioned in Table 1, evaluation occurs on the device state: after\nthe agent is done, or perceives itself to be done, with a task, the relevant evaluator function queries\nthe VM device state (e.g., a certain configuration file, setting, etc.) for differences compared to the\ninitial state to determine whether or not said task is accomplished. In the case of the agent failing\nsaid task, the evaluator returns a reward of zero. However, if the agent succeeds at the task (i.e., the\nVM device state is changed successfully from its default), the reward returns a non-negative reward.\nThe reward (R) is determined by the task's evaluator function but can\ngenerally fall into two types: binary reward and continuous reward. A binary reward (R \u2208 {0,1}) is\none that returns either zero or one and is typically involved in tasks that uses a strict rule to evaluate a\ntask outcome; in other words, the device state either agrees with the rule (e.g., turn a program setting\non or off) or it does not (i.e., fails and returns reward zero). A continuous reward (R \u2208 [0,1]) can\ntake values between zero and one inclusive on a continuous scale. In contrast to tasks with binary\noutcomes, tasks whose outcomes can vary in quality or scale such as those requiring the matching of\nvisual or audio output (e.g., take a screenshot of a video frame and set it as the desktop background;\nevaluation then uses a \u201cgolden\" reference image to compare against the agent's generated image via\nan image similarity measure) among others are typically characterized by this type of reward."}, {"title": "A.4 TASK & PROGRAM SELECTION", "content": "In Table 7, we provide a detailed list of the 154 tasks in WINDOWSAGENTARENA along with the\nprograms/applications they are associated with. We note that in both our choices of tasks adapted\nfrom OSWorld Xie et al. (2024) and new tasks created for Windows, our goal was to ensure a di-\nverse representation across contexts and difficulty levels while maintaining a degree of realism in\nreflecting the common kinds of tasks users of Windows OS regularly perform and the programs/ap-\nplications they use."}, {"title": "A.5 WINDOWSAGENTARENA HUMAN EVALUATION", "content": "In this section of the Appendix, we provide a more detailed overview of human evaluation metrics\non our WINDOWSAGENTARENA benchmark. In total, our human participant is a casual user of a\nWindows PC and spent approximately 1.5 hours in five sittings. For each task, the participant was\nprovided the task in natural language and had to first determine if said task was feasible or not. A\ntask can either be feasible or infeasible: while a feasible task tests the agent's ability complete a\ntask, an infeasible task is meant to test if an agent can recognize whether while a task can be done\nat all in the first place. If the task is infeasible (i.e., impossible to carry out) and the agent/user\nrecognizes that it is infeasible, then such an outcome is considered a success; in contrast, if the task\nis infeasible but is recognized as feasible, then the outcome is a failure. If the task is feasible, the\nparticipant then attempts to complete the task. Tasks are completed without any external human or\ninternet assistance.\nDuring the task attempt, the user/participant records the number of steps taken along the way. If\nthe participant succeeds at the task, as measured by whether the task evaluation scores the end state\nas a success, then the number of steps for task completion is recorded. If the participant fails, the\ntask is recorded as a failure and the participant is shown a correct way to do the task before the\ntask is done again to record the number of steps required theoretically to complete it. Task order is\nrandomized and ongoing task success/failure rates are not recorded and made known to the human"}, {"title": "A.6 TASK DEFINITION & CONFIGURATION", "content": "To better illustrate how tasks are defined, configured, and implemented, we refer to an example of\na task JSON in Figure 12. Each task is defined following a similar format. The \"id\" key serves\nas an unique identifier for each and every task in the WINDOWSAGENTARENA benchmark library.\nThe \"instruction\" serves to define the task at hand via natural language: in this example, the\ntask is to \"Help me modify the folder used to store my recordings to the Desktop\".\nThe \"config\" key defines the context of the task at hand and sets up the initial state for the agent\nto start execution in its attempt to accomplish the task at hand as defined by the instruction. The\n\"config\" can invoke different types of actions to setup the initial state, ranging from launching a\nprogram/application relevant for the task instruction to invoking commands through Powershell or\ndownloading a file for the task and more. In this case, we see that the example JSON's \"config\"\ninvokes two setup types: one \"type\" is a \"launch\" which communicates with the VM to launch"}, {"title": "A.7 WINDOWS AGENTARENA: AZURE PARALLELIZATION DETAILS", "content": "We note that during Azure deployment, the user needs to select a VM type that support nested\nvirtualization with KVM. Table 10 provides a non-exhaustive list of CPU VMs that support our\nsetup as well as their current costs as of August 2024 (subject to change). We rely primarily on the\nStandard D8 v3 machine for our experiments."}, {"title": "C.1 SETUP", "content": "We evaluate on a processed version of the Mind2Web Deng et al. (2023) benchmark that removes\na small number of tasks with a missing ground truth element or broken HTML. Each entry consists\nof a task description, a sequence of actions, a screenshot, and a list of entities proposed by the"}, {"title": "C.2 VISUAL EXAMPLES", "content": "This list consists of the task input data, as part of the prompt, available to the Screen Helper. It also shows actions selected at every step as recorded during the actual agent run in the system. Also, note that the original system outputs the actions\nscreen parsing module. During evaluation, as part of its prompt, the agent is given the screenshot\nwith set-of-marks prompting for bounding box visualization along with a textual representation\nof the elements (i.e., a description of their contents and corresponding element ID). We use few-\nshot Chain-of-Thought prompting and in-context examples with each agent and a python interpreter\naction space. For Mind2Web, two of the agents have text input but our multi-modal agent employs\nset-of-marks prompting (Table 5). During offline evaluation, the agent's outputted Python code is\nexecuted and the resulting coordinates are fed into Mind2Web's evaluation function."}, {"title": "D AGENT PROMPTS", "content": "You are Screen Helper, a world-class reasoning engine that can complete any goal on a computer to help a user by executing code.\nWhen you output actions, they will be executed **on the user's computer**. The user has given you **full and complete permission**\nto execute any code necessary to complete the task. In general, try to make plans with as few steps as possible. As for actually executing\nactions to carry out that plan, **don't do more than one action per step**. Verify at each step whether or not you're on track.\nA text string with the user's goal for the task, which remains constant until the task is completed.\nA string with the title of the foreground active window.\nA list with the names of all the windows/apps currently open on the user's computer. These names can be used\nin case the user's objective involves switching between windows.\nA string with the current content of the clipboard. If the clipboard contains copied text this will show the text\nitself. If the clipboard contains an image, this will contain some description of the image. This can be useful for storing information\nwhich you plan to use later.\nA multi-line block of text with the screen's text OCR contents, rendered with their approximate screen locations.\nNote that none of the images or icons will be present in the screen rendering, even though they are visible on the real computer screen.\nA list of candidate screen elements which which you can interact, each represented with the\nfollowing fields:\n- ID: A unique identifier for the element.\nType: The type of the element (e.g., image, button, icon).\n- Content: The content of the element, expressed in text format. This is the text content of each button region, or empty in the case of\nimages and icons classes.\nLocation: The normalized location of the element on the screen (0-1), expressed as a tuple (x1, y1, x2, y2) where (x1, y1) is the\ntop-left corner and (x2, y2) is the bottom-right corner.\n7. Images of the current screen:\n7.0 Raw previous screen image.\n7.1 Raw screen image.\n7.2 Annotated screen with bounding boxes drawn around the image (red bounding boxes) and icon (green bounding boxes) elements,\ntagged with their respective IDs. Note that the button text elements are not annotated in this screen, even though they might be the most\nrelevant for the current step's objective.\nVery important note about annotated screen image: the element IDs from images and icons are marked on the bottom right corner of"}, {"title": "D.2 MIND2WEB AGENT", "content": "Below is the prompt used for our Navi agent and its variants on an example task from the Mind2Web\nbenchmark.\nYou are Screen Helper", "Example": "nVerify at each step whether or not you're on track.\nA text string with the user's goal for the task", "fields": "n- ID: A unique numeric identifier for the element.\n- Type: The type of the element (e.g.", "icon).\nContent": "The content of the element", "classes.\nLocation": "The normalized location of the element on the screen (0-1)", "screen": "nAnnotated screen with bounding boxes drawn around the image (red bounding boxes), icon (green bounding boxes), and input (blue\nbounding boxes) elements, tagged with their respective IDs. Note that the button text elements are not annotated in this screen, even\nthough they might be the most relevant for the current step's objective.\nThis is a list of possible actions that you can take to complete the task. Execute these actions by outputting them within a code block,\nset off by triple backticks and the word \"python\" on the first line and triple backticks on the last line.\nHistory of past actions taken to reach the current screen, which can help you understand the context of the current screen.\nThis is a string that you can use to store information for future steps. You can use this to store information that you plan to use in later\nsteps, such as a summary, a description of a previous page, or a song title that you will type or use as context later.\nYour goal is to analyze all the inputs then output each of the following items\nIn a few words, what is happening on the screen?\nHow does the screen content relate to the current step's objective?\nOn a high level, what are the next actions and screens you expect to happen between"}]}