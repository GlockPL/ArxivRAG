{"title": "GaRField++: Reinforced Gaussian Radiance Fields for Large-Scale 3D Scene Reconstruction", "authors": ["Hanyue Zhang", "Zhiliu Yang", "Xinhe Zuo", "Yuxin Tong", "Ying Long", "Chen Liu"], "abstract": "This paper proposes a novel framework for large-scale scene reconstruction based on 3D Gaussian splatting (3DGS) and aims to address the scalability and accuracy challenges faced by existing methods. For tackling the scalability issue, we split the large scene into multiple cells, and the candidate point-cloud and camera views of each cell are correlated through a visibility-based camera selection and a progressive point-cloud extension. To reinforce the rendering quality, three highlighted improvements are made in comparison with vanilla 3DGS, which are a strategy of the ray-Gaussian intersection and the novel Gaussians density control for learning efficiency, an appearance decoupling module based on ConvKAN network to solve uneven lighting conditions in large-scale scenes, and a refined final loss with the color loss, the depth distortion loss, and the normal consistency loss. Finally, the seamless stitching procedure is executed to merge the individual Gaussian radiance field for novel view synthesis across different cells. Evaluation of Mill19, Urban3D, and MatrixCity datasets shows that our method consistently generates more high-fidelity rendering results than state-of-the-art methods of large-scale scene reconstruction. We further validate the generalizability of the proposed approach by rendering on self-collected video clips recorded by a commercial drone.", "sections": [{"title": "I. INTRODUCTION", "content": "The recent advances in 3D reconstruction of large-scale urban scenes have reshaped modern society. It can serve as a visualization medium for AR/VR [1], aerial surveying [2], and city planning [3], [4], a high definition (HD) map for autonomous driving [5], [6], [7], [8], [9], [10], or a photorealistic simulator for unexpected cases in end-to-end autonomous driving and unmanned aerial vehicles (UAVs) [3], [11], [12], [13].\nThe task consists of high-fidelity reconstruction and real-time rendering for large areas that typically span more than 1.5 km\u00b2 [2]. In recent years, the field has been dominated by methods based on Neural Radiance Fields (NeRFs) [14]. Representative works include Block-NeRF [5], GgNeRF [15], Switch-NeRF [16] and Mega-NeRF [2]. However, these methods still lack the fidelity in preserving details. Recently, the 3D Gaussian Splatting (3DGS) technique [17] has gained significant attention for its outstanding performance in visual quality and rendering speed, achieving near-photorealistic rendering effects at 1080p resolution in real time. It has also been successfully applied to the reconstruction of dynamic scenes [18], [19], [20] and the generation of 3D content [21], [22]. However, the 3DGS still faces several scalability and accuracy challenges when dealing with large-scale environments.\nFirstly, large-scale scenes typically encompass various objects, including the complex geometry structure such as grass, plants [23], and a large area of background such as the sky and water body [12]. Traditional 3DGS-based reconstruction methods do not adequately model normal depth and opacity information. Secondly, uneven lighting conditions in large-scale scenes may lead to significant appearance differences in captured images. When dealing with these variations, 3DGS tends to generate large-size 3D Gaussians with low opacity [12], which results in floating artifacts in novel views. Third, optimizing the entire large-scale scene requires multiple iterations, which become extremely time consuming and unstable without the proper regularization term and loss function design [24].\nRecent efforts of large-scale scene reconstruction based on the 3DGS have mitigated some of the aforementioned shortcomings. Methods like visibility-based camera selection [12], appearance modeling [25], multimodal fusion [26], level of details [27] etc. are proposed correspondingly to improve the rendering quality. Although these methods produce reasonable results, they are still limited to some areas that are too blurred in the rendered images.\nWe propose a reinforced Gaussian radiance field for large-"}, {"title": "II. RELATED WORK", "content": "A. Rendering with Radiance Fields\n1) Neural Radiance Fields: Neural Radiance Fields (NeRF) [14] implicitly represents 3D scenes as a mapping from position and direction into radiance using a multi-layer perceptrons (MLPs), and achieves novel view synthesis through volumetric rendering techniques. Despite the significant progress made for 3D scene reconstruction and rendering by NeRF [14], they still face challenges in efficiency and memory usage when dealing with large-scale scenes. To improve rendering efficiency, researchers have proposed various strategies [29], [30], [31]. InstantNGP [29] firstly encodes the scene into a multi-resolution hashing table. Mip-NeRF [2] enhances NeRF's representation capacity for outdoor scenes by introducing the down sampling of conical frustums. Zip-NeRF [32] employs a hexagonal sampling strategy to address aliasing issues in the rendering.\n2) 3D Gaussian Splatting: Rendering methods based on points utilize 3D Gaussian functions as geometric primitives, achieving the rapid rendering and a scene editing ability [17]. The 3D Gaussian Splatting (3DGS) further enhances rendering efficiency by employing optimized rasterization. Although 3DGS can produce high-fidelity 3D reconstruction results, methods such as Mip-splatting [33], LightGaussian [34], GSCore [35], Gaussianpro [36], Fregs [37], Eagles [38], Compact3d [39] are proposed to improve the rendering process. Motivated by the method of EWA-Splatting [40], the Mip-Splatting [33] limits the frequency of the 3D representation and introduces a 2D Mip filter. Eagles [38], Compact3d [39], and others are committed to applying the VQ [41] trick to compress a large number of Gaussian primitives. Unlike FreGS [37], C3DGS [42], which optimizes on the software algorithms, GSCore [35] proposes a hardware acceleration unit to optimize the 3DGS pipeline in the rendering of the radiance field. GaussianPro [36] introduces an innovative paradigm for joint 2D-3D training to reduce the dependence on SfM initialization.\nB. Large-scale Scene Reconstruction\nThe neural rendering and the 3DGS-based rendering are naturally extended to the domain of large-scale scene reconstruction. Block-NeRF [5] divides large scenes into blocks and introduces appearance embeddings, learned pose refinement, and controllable exposure for the training of each individual block. Mega-NeRF [2] analyzes the data visibility of large-scale scenes, thereby proposes a sparse network structure where parameters are dedicated to different areas of the scene. Urban Radiance Fields [26] utilizes LiDAR and 2D optical flow data for large-scale scene reconstruction. Switch-NeRF [43] introduces a Mixture of Experts (MoE) system for end-to-end large-space modeling. A 3D point is assigned to an expert through a gating network, and the final rendering outcome is determined by the combined output of the expert and the gate value. VastGaussian [12] and CityGaussian [27] are representative works that take advantage of 3DGS for scalability and rendering fidelity of large-scale scene reconstruction. Additionally, DrivingGaussian [44] and StreetGaussians [25] aim at reconstructing large-scale dynamic scenes in autonomous driving using multimodal data. StreetGaussians [25] uses Fourier transforms to effectively represent the temporal changes of spherical harmonics. DrivingGaussian [44] leverages the LiDAR priors and employs multi-frame multi-view data for hierarchical scene modeling. 3DGS-Calib [45] introduces LiDAR point clouds as reference points for Gaussian positions to construct a continuous scene representation.\nWhile the aforementioned studies have effectively improved the rendering quality in the large-scale scene reconstruction compared to the methods proposed before inventing the NeRFs and the 3DGS, there is the space for improving the rendering precision of geometric structure and large homogeneous areas."}, {"title": "III. METHODOLOGY", "content": "Our GaRField++framework processes the input images through a structure-from-motions module, a scenes partition-ing, a cells rendering, and a seamless stitching to construct a reinforced Gaussian radiance field, which gives its capability to synthesize photorealistic views. The overview of the entire framework is shown in Fig. 2."}, {"title": "A. Scenes Partitioning", "content": "We employ a divide-and-conquer strategy similar to [12] and [27], divide the large-scale scene into multiple cells, then render each cell independently.\n1) Sparse Reconstruction: The input images of the scene are denoted as \\( {I_t\\vert t = 1,2,...,T} \\). Then the Structure-from-Motion (SfM) method, COLMAP [46], is adopted to generate a sparse point cloud P, and the initial camera pose \\( E_t \\) is estimated for each image \\( I_t \\). The camera views are defined as \\( V_t = {I_t,t} \\). The Z axis of the point cloud P is adjusted to be perpendicular to the ground plane by performing Manhattan world alignment [12].\n2) Visibility-based View Selection: The best illumination condition and geometry visibility can be obtained by applying the coverage-wise point selection strategy, and details of the view selection are given below.\n*   Coordinate-based Regionalization: The large-scale scene is first divided into N cells and we distribute parts of the point cloud to a specific cell. The point cloud within a cell is defined as \\( {P_i\\vert i = 1, 2, 3, ..., N} \\).\n*   Point Clouds Extension: Boundaries of the cell i are expanded to enroll the common views between adjacent cells. The original bounded area of cell i is \\( L_W \\times L_H \\), which now extends to i is \\( (1 + B)L_W \\times (1+ )L_H \\) by a certain percentage B. The set of point clouds \\( P_i \\) is slightly dilated to \\( P^d \\).\n*   Cameras and Points Selection for Data Partitioning: Given a cell i, the camera views from the adjacent cell j is enrolled by checking the visibility criterion R, which is calculated by the following equation:\n\\[\nR_i = {\\frac{\\vert A_{proj}^i\\vert}{A_I^i} = \\frac{W_i\\times H_i}{W^I \\times H^I} }\n\\]\n(1)\nWhere \\( A_{proj}^i \\) is the projected area of ith cell in image \\( I \\) and \\( A_I \\) is the area of pixels in image I by multiplying the width of the image \\( W^I \\) and height \\( H^I \\). Cameras whose R is larger than a predefined threshold [12] are selected to join the cell i. And more point cloud from the adjacent cell j is selected in the current partition, only if those points can be observed from the newly added camera views \\( V_t \\). The final point cloud inside a cell i is further extended to \\( P^f \\)."}, {"title": "B. Cells Rendering", "content": "The previous step produces the best point set, \\( P^f \\), for modeling one of the partitions of large-scale areas, which represents a coarse description of the geometry distribution. Here, we further correlate these points with Gaussian primitives [17]. And our GaRField++framework strengthens the radiance fields made up of Gaussian primitives with the following three reinforcements.\n1) Ray-Gaussian Intersection Model & Improved Gaussian Density Control: The sparse point clouds of the scene is further depicted with a set of 3D Gaussian primitives \\( {G_k\\vert k = 1,..., K} \\) correspondingly. The properties of each 3D Gaussian \\( G_k \\) are parameterized by view-dependent color \\( c_k \\in R^{3\\times1} \\), opacity \\( \\alpha_k \\in [0,1] \\), center \\( u_k \\in R^{3\\times1} \\), scale \\( s \\in R^{3\\times1} \\), and rotation \\( R \\in R^{3\\times3} \\).\nThe Gaussian primitive \\( G_k \\) of any point \\( x \\in R^{3\\times1} \\) is depicted as:\n\\[\nG_k(x) = \\alpha_k e^{-(x-u_k)^T \\Sigma_k^{-1}(x-u_k)}\n\\]\n(2)\nDifferent from original 3DGS [17] method which projects Gaussian balls into 2D screen space and examine the Gaussian in 2D, ray-Gaussian intersection [23] is utilized here to convert 3D Gaussians at any point x into a 1D Gaussian \\( G_D(x) \\). For a given camera pose \\( \\xi_t \\) of the image \\( I_t \\),"}, {"title": "C. Seamless Stitching & Novel view Synthesis", "content": "The Gaussian radiance fields within each cell is well-trained, and the Gaussian points outside the original boundary presented by \\( P_i \\) (before the boundaries extension step) is cut out for seamless merging. Then we directly stitch different cells together, and the entire large-scale area model can now support cross-boarder rendering for novel view synthesis. Given a random camera pose \\( \\xi_{nus} \\), the novel view \\( I_{nus} \\) can be rendered."}, {"title": "IV. EXPERIMENTS", "content": "A. Experimental Setup\n1) Dataset and Metrics: The experiments are conducted across five large-scale scenarios: the Rubble and the Building from the Mill-19 dataset [2], the Residence from the Urban-Scene3D dataset [48], the small_city, which is a synthetic scene from the MatrixCity dataset [49], and Campus-YNU dataset collected by ourselves. The Campus-YNU dataset covers a region around 1km \u00d7 1km, which is captured simply using a DJI drone (Mini 3 Pro). SSIM, PSNR, and LPIPS [50] architecture serve as our evaluation metrics to quantitatively analyze the rendering results. All experiments were conducted on NVIDIA L40 GPUs with 48 GB memory for each card."}, {"title": "B. Performance of Novel view Synthesis", "content": "1) Comparison with SOTA: As demonstrated in Table I, our method outperforms the state-of-the-art (SOTA) methods in terms of SSIM, PSNR, and LPIPS metrics for all four scenes (Building, Rubble, Residence, and MatrixCity. Here, small city is selected from the MatrixCity dataset.). The qualitative results presented in Fig. 4 also validate the high fidelity of our rendering results. As shown in Fig. 4, our rendering results achieve more realistic results, which is much closer to Ground Truth in the aspects of lighting and color. Specifically, in the building scenario, our renderings better preserve the detail of sunlight reflection on solar panels, which validates the effectiveness of our ray-Gaussian-intersection rendering, density control strategy, and the color decoupling module based on KAN and CNN.\n2) Experiments on Self-collected Data: To validate the effectiveness and generalization capability of our framework in large-scale scenarios, we employ a DJI drone (Mini 3 Pro) to fly over a 1km \u00d7 1km area, captured a dataset comprising 1,600 images at a resolution of 3768 \u00d7 2118 pixels. This scene was selected in our experiment for its intricate details, including solar panels, window arrays, and construction sites. Comparative experiments are conducted between 3DGS and our GaRField++. As shown in Table"}, {"title": "2) Loss:", "content": "Our loss function, which is composed of depth distortion loss, normal consistency loss, and RGB loss derived from 3DGS [17], can better enhance the rendering quality of images compared to using only the RGB loss from 3DGS [17]. As illustrated in Fig. 6.h and Fig. 6.i, the rendered text is obviously clearer after using our loss function."}, {"title": "3) Decoupled Color Model:", "content": "Our color decoupling module, which employs a network combining KAN [28] and CNN, has achieved superior results in reducing color variations in rendered images. As illustrated in Fig. 6.b and Fig. 6.c, compared to a color decoupling module composed solely of CNN, our approach can more effectively learn consistent geometric shapes and colors from training images with varying appearances. During the ablation experiments of the color decoupling module, we utilized a camera visibility of 0.25 and employed a full loss function."}, {"title": "1) Camera Visibility Calculation:", "content": "As shown in the Table. III, we conduct an investigation into camera visibility within a Rubble scenario. Specifically, we establish three levels of camera visibility, designated as Vis R0, Vis R1, and Vis R2, with settings of 0, 0.50, and 0.25, respectively. Throughout the experimental process, we use a full loss function and a color decoupling module integrated with CNN and KAN [28]. Subsequently, our approach employs the proposed camera visibility technique to test all these levels of visibility. As demonstrated in Fig. 6.d, Fig. 6.e, and Fig. 6.f, the camera visibility is found to be helpful in enhancing the quality of the rendering."}, {"title": "V. CONCLUSION", "content": "In this work, we introduce GaRField++, a high-fidelity reconstruction and rendering method for large-scale scenes based on 3D Gaussian splatting. We employ a ray-Gaussian-intersection volume rendering and a density control strategy for large-scale reconstruction, a color decoupling module that combines KAN and CNN, a data partitioning method based on coordinates and camera visibility, and depth-normal consistency. We have achieved state-of-the-art rendering fidelity in mainstream benchmark tests and excellent rendering fidelity in our self-collected data set. However, we have not yet explored the optimal solutions for camera visibility and coordinate partitioning. In some scenarios, we still require hyper-parameter tuning to provide better rendering quality, and our model relies on the accuracy of the initial sparse point cloud. Additionally, our research may be applied to the 3D mesh extraction in the large-scale scenes. These works are left for our future endeavors."}]}