{"title": "Guiding Word Equation Solving using Graph Neural Networks (Extended Technical Report)", "authors": ["Parosh Aziz Abdulla", "Mohamed Faouzi Atig", "Julie Cailler", "Chencheng Liang", "Philipp R\u00fcmmer"], "abstract": "This paper proposes a Graph Neural Network-guided algorithm for solving word equations, based on the well-known Nielsen transformation for splitting equations. The algorithm iteratively rewrites the first terms of each side of an equation, giving rise to a tree-like search space. The choice of path at each split point of the tree significantly impacts solving time, motivating the use of Graph Neural Networks (GNNs) for efficient split decision-making. Split decisions are encoded as multi-classification tasks, and five graph representations of word equations are introduced to encode their structural information for GNNs. The algorithm is implemented as a solver named DragonLi. Experiments are conducted on artificial and real-world benchmarks. The algorithm performs particularly well on satisfiable problems. For single word equations, DragonLi can solve significantly more problems than well-established string solvers. For the conjunction of multiple word equations, DragonLi is competitive with state-of-the-art string solvers.", "sections": [{"title": "Introduction", "content": "Over the past few years, reasoning within specific theories, including arithmetic, arrays, or algebraic data structures, has become one the main challenges in automated reasoning. To address the needs of modern applications, new techniques have been developed, giving rise to SMT (Satisfiability Modulo Theories) solvers. SMT solvers implement efficient decision procedures and reasoning methods for a wide range of theories, and are used in applications such as verification.\nAmong the theories supported by SMT solvers, the theory of strings has in particular received attention in the last years. Strings represent one of the most important data-types in programming, and string constraints are therefore relevant in various domains, from text processing to database management systems"}, {"title": "Preliminaries", "content": "We start by defining the syntax of word equations, as well as the notion of satisfiability. Then, we explain the fundamental mechanism of Graph Neural Networks (GNNs), along with a description of the specific GNN model we have employed in our experiments.\nWord Equations. We assume a finite non-empty alphabet \u2211 and write * for the set of all strings (or words) over \u03a3. We work with a set \u0393 of string variables, ranging over words in \u03a3*, and denote the empty string by e. The symbol \u00b7 denotes the concatenation of two strings; in our examples, we often write uv as shorthand for u \u00b7 v. The syntax of word equations is defined as follows:\nFormulae $ ::= true | e\u2227 \u0444\nWords w ::= etw\nEquations e ::= w = w\nTerms t ::= Xc\nwhere X \u2208 \u0393 ranges over variables and c\u2208\u03a3 over letters.\nDefinition 1 (Satisfiability of word equations). A formula o is satisfiable if there exists a substitution \u03c0 : \u0393 \u2192 \u03a3* such that, when each variable X \u2208 \u0393 in & is replaced by \u03c0(X), all equations in o are satisfied.\nGraph Neural Networks. A Graph Neural Network (GNN) [13] uses Multi-Layer Perceptrons (MLPs) to extract features from a given graph. MLPs, also known as multi-layer neural networks [25], transform an input space to make different classes of data linearly separable, and this way learn representations of data with multiple levels of abstraction. Each layer of an MLP consists of neurons that apply a nonlinear transformation to the inputs received from the previous layer. This allows MLPs to learn increasingly complex patterns as data moves from the input layer to the output layer.\nMessage passing-based GNNs (MP-GNNs) [24] are designed to learn features of graph nodes (and potentially the entire graph) by iteratively aggregating and transforming feature information from the neighborhood of a node. For instance, if we represent variables in a word equation by nodes in a graph, then node features could represent symbol type (i.e., being a variable), possible assignments, or the position in the word equation.\nConsider a graph G = (V, E), with V as the set of nodes and EC V \u00d7 Vas the set of edges. Each node v \u2208 V has an initial representation x \u2208 Rn and a set of neighbors N C V. In an MP-GNN comprising T message-passing steps,"}, {"title": "Search Procedure and Split Algorithm", "content": "In this section, we define our proof system for word equations, the notion of a proof tree, and show soundness and completeness. We then introduce an algorithm to solve a conjunction of word equations.\n3.1 Split Rules\nWe introduce four types of proof rules in Figure 2, each corresponding to a specific situation. The proof rules are inspired by [4], but streamlined and formulated differently. Each proof rule is of the form:\nP\nName\n[cond1]\nC1\n[condn]\nCn"}, {"title": "GNN-Guided Split Algorithm", "content": "We use the proof rules in Figure 2 and the idea of iterative deepening from [32] (combination of depth- and breadth-first search in a tree) to solve word equa- tions, as shown in Algorithm 1. This algorithm aims to check the satisfiability of word equations \u03c6 = (N=1 wi = \u03c9i).\nAlgorithm 1 receives as parameter a backtrack strategy BT \u2208 {BT1, BT2, BT3}, which determines when to stop exploring a path of the proof tree and re-"}, {"title": "Representing Word Equations by Graphs", "content": "Graph representations can capture the structural information in word equations and are the standard input format for GNNs. To understand the impact of the graph structure on our framework, we have designed five graph representations for word equations.\nIn order to extract a single graph from the equations, we first translate a conjunction \u2081 w = w of word equations to a single word equation, by inserting a distinguished letter # & \u2211 as follows:\nw\u2081#w2#...#wn = wi#w2#...#wn,\n(3)\nThen, we construct the graph representations for the word equation in (3). A graph representation G = (V, E, VT, Vvar) of a word equation consists of a set of nodes V, a set of edges E CV \u00d7 V, a set of terminal nodes VT CV, and a set of variable nodes Vvar V. We start constructing the graph by drawing the \"=\" symbol as the root node. Its left and right children are the leftmost terms of both sides of the equation, respectively. The rest of the graph is built following the choice of the graph type:\nGraph 1: Inspired by Abstract Syntax Trees (ASTs). Each letter and vari- able is represented by its own node, and words are represented by singly- linked lists of nodes.\nGraph 2: An extension of Graph 1, introducing additional edges from each term node back to the root node.\nGraph 3: An extension of Graph 1 which incorporates unique variable nodes. In this design, nodes representing variables are added, which are con- nected to their respective occurrences in the linked lists. This representation aims at facilitating the learning of long-distance variable relationships by GNNs."}, {"title": "Training of Graph Neural Networks", "content": "Forward Propagation. In the orderBranches function of Algorithm 1, we sort the branches by using the predictions from a trained GNN model. This GNN model performs a multi-classification task. Given a list of branches (b1,...,bn) resulting from a rule application, we expect the trained GNN model to output a list of floating-point numbers Yn = (\u01771,..., \u0177n), representing priorities of the branches. A higher value for \u0177i indicates a higher priority of the branch. For instance, given a node with two children b\u2081 and b2, the output from the model could be Y2 = (0.3, 0.7), expressing the prediction that b2 will lead to a solution more quickly than b\u2081 and should be explored first. We detail the process of deriving Yn at each split point using GNNs, exemplified by using n = 2.\nPropagation on Graphs. To explain forward propagation, suppose a node labelled with formula o in the proof is rewritten by applying rule R7, resulting in direct children labelled with $1, $2. The situation is similar for applications of R8.\nFormulas 0, 1, and $2 are transformed to graphs Go = (V\u00ba, E\u00ba, V+, Var), G\u2081 = (V1, E1, V+, VVar), and G2 = (V2, E2, V, Var), respectively, according to one of the encodings in Section 4.1. Each node in those graphs is then assigned"}, {"title": "Training Data Collection", "content": "With our current algorithm, UNSAT problems always require an exhaustive exploration of a proof tree; branch ordering therefore does not affect the solving time. We have thus focused on optimizing the process of finding solutions and only extract training data from SAT problems.\nTo collect our training labels, we construct the complete proof tree for given conjunctions of word equations, up to a certain depth. The tree enables us to identify cases of multiple SAT pathways within the tree, and to identify situations where one branch leads to a solution more quickly than other branches.\nEach node v of the proof tree with multiple children is labelled based on two criteria: the satisfiability status (SAT, UNSAT, or UNKNOWN) of the formula, and the size of the proof sub-tree underneath each of the direct children. As- sume that node v has n children, each of which has status SAT, UNSAT, or UNKNOWN, respectively. If there is exactly one child of v, say the i'th child, that is SAT, then the label of v is a list of integers (x1,...,xn) with Xi = 1 and xj = 0 for j \u2260 i. If multiple children are SAT, we examine the size of the sub-tree underneath each of those children, and label all children with minimal sub-trees with 1 in the list (x1,...,xn)."}, {"title": "Guidance for the Split Algorithm using the GNN Model", "content": "In Algorithm 1, we introduce five strategies for the orderBranches function im- plementation, designed to evaluate the efficiency of deterministic versus stochas- tic methods in branch ordering and to investigate the interplay between fixed and variable branch ordering approaches:\nFixed Order: Use a predetermined branch order, defined before execution. In our experiments, we simply use the order in which the branches are dis- played in Figure 2.\nRandom Order: Reorder branches randomly.\nGNN (S1): Exclusively use the GNN model for branch ordering.\nGNN-fixed (S2): A balanced approach with a 50% chance of using the GNN model and a 50% chance of using the fixed order.\nGNN-random (S3): Similar to S2, but with the alternative 50% chance dedicated to random ordering."}, {"title": "Experimental Results", "content": "This section presents the benchmarks used for our experiments and details the results with the different versions of our algorithm. It also provides a compre- hensive comparison with other state-of-the-art solvers."}, {"title": "Implementation of DragonLi", "content": "DragonLi [2] is developed from scratch using Python 3.8 [49]. We train the mod- els with PyTorch [42] and construct the GNNs using the Deep Graph Library (DGL) [51]. For tracking and visualizing training experiments, mlflow [15] is em- ployed. Proof trees and graph representations of word equations are stored in JSON [43] format, while graphviz [21] is utilized for their tracking and visual- ization."}, {"title": "Benchmarks Selection", "content": "We consider two kinds of benchmarks: benchmarks that are artificially gen- erated based on the benchmarks used to evaluate the solver Woorpje [19], as well as benchmarks extracted from the non-incremental QF_S, QF_SLIA, and QF_SNLIA track of the SMT-LIB benchmarks [1]. We summarize the bench- marks as following:\nBenchmark 1 is generated by the mechanism used in Woorpje track I. Given finite sets of letters Cand variables V, we construct a strings with maximum length of k by randomly concatenating selected letters from C. We then form a word equation s = s and repeatedly replace substrings in s with the concatenation of between 1 and 5 fresh variables. This procedure guarantees that the constructed word equation is SAT.\nBenchmark 2 is generated by the mechanism used in Woorpje track III. It first generates a word equation using the following definition:\nXnaXnbXn-1bXn-2bX1\u2081 = aXnXn-1Xn-1bXn-2Xn-2bbX1X\u2081baa\n(6)\nwhere X1, ..., Xn are variables and a and b are letters. We then generate a word equation using the mechanism for Benchmark 1, and replace letters b in (6) randomly with the left-hand side or the right-hand side of that equation.\nBenchmark 3 is generated by conjoining multiple word equations that were randomly generated using the mechanism described in Benchmark 1. This procedure mainly produces benchmarks that are UNSAT.\nBenchmark 4 is extracted from benchmarks from the non-incremental QF_S, QF_SLIA, and QF_SNLIA tracks of SMT-LIB. We obtain word equa- tions by removing length constraints, regular expressions, and unsupported Boolean operators, which are not considered in this paper. As a result, bench- marks after transformation can be SAT even if the original SMT-LIB bench- marks were UNSAT."}, {"title": "Experimental Settings", "content": "DragonLi was parametrized with values 1BT2 = 500, IstepBT2 = 250, \u0406\u0432\u0442\u0437 = 20 for Algorithm 1. In addition, we chose a hidden layer of size 128 for all neural networks, and used a two message-passing layer (i.e. t = 2 in Equation 1) for the GNN. Each problem in the benchmarks is evaluated on a computer equipped with two Intel Xeon E5 2630 v4 at 2.20 GHz/core and 128GB memory. The GNNs are trained on A100 GPUs. We measured the number of solved problems and the average solving time (in seconds), with timeout of 300 seconds for each proof attempt."}, {"title": "Comparison with Other Solvers", "content": "In Table 2, we evaluate three versions of our algorithm based on their imple- mentation of the orderRules function: the fixed, random, and GNN-guided order versions (listed in Section 4.4). The performance of the GNN-guided DragonLi (row GNN in Table 2) for each benchmark is selected from the best results out of 45 experiments (see Table 3). These experiments use different combinations of five graph representations, three backtrack strategies, and three GNN guidance strategies, as shown in bold text in Table 3. We compare the results with those of five other solvers: Z3 (v4.12.2) [39], Z3-Noodler (v1.1.0) [17], cvc5 (v1.0.8) [10], Ostrich (v1.3) [16], and Woopje (v0.2) [19].\nThe primary metric is the number of solved problems. DragonLi outperforms all other solvers on SAT problems in benchmark 2. Notably, the GNN-based DragonLi solves the highest number of SAT problems. For the conjunction of multiple word equations (benchmark 3 and 4), DragonLi's performance is com- parable to the other solvers. The order of processing word equations is crucial for"}, {"title": "Ablation Study", "content": "Table 3 displays the number of problems solved in 45 experiments across all benchmarks using the GNN-guided version.\nIn terms of backtrack strategies, BT\u2081 performs a pure depth-first search, but it already has good performance. BT2 performs a depth-first search controlled by parameters IBT, and I, IBT, and in many cases, it delivers the best performance. BT3 conducts a systematic search on the proof tree, which is complete for prov- ing problems SAT, but turns out to be relatively inefficient in the experiments and solves the fewest problems given a fixed timeout. This indicates that more sophisticated search strategies may lead to even better performance.\nIn terms of the guiding strategies (S1, S2, S3), using the GNN alone (S1) to guide the branch order is better than combining it with predefined and random orders (S2 and S3) in most cases. This indicates that the GNN model success- fully learns useful patterns at each split point and can be used as a stand-alone heuristic for branching.\nIn terms of the five graph representations, Graph 1 has the simplest structure, which represents the syntactic information of the word equations and thus incurs the least overhead when we call the model at each split point. This yields av- erage performance compared to other graph representations. The performances of Graph 2 are weaker than others; this is probably due to the extra edges not providing any benefits for prediction, but leading to additional computational overhead. Graphs 3 and 4 emphasize the relationships between terminals and variables, respectively, thus the performance is biased by individual problems. Graph 5 considers the relationships for both terminals and variables, thus it has bigger overhead than Graphs 1, 3, and 4, but it offers relatively good perfor- mance. This shows that representing semantic information of the word equations well in graphs helps the model to learn important patterns. In summary, the set- ting (BT2, S1, Graph5) performs the best."}, {"title": "Related Work", "content": "There are many techniques within solvers supporting word equations, as well as in stand-alone word equation solvers. For instance, the SMT solvers Norn [5] and"}, {"title": "Conclusion and Future Work", "content": "This study introduces a GNN-guided split algorithm for solving word equations, along with five graph representations to enhance branch ordering through a multi-classification task at each split point of the proof tree. We developed our solver from scratch instead of modifying a state-of-the-art SMT solver. This de- cision prevents the confounding influences of pre-existing optimizations in state- of-the-art SMT solvers, allowing us to isolate and evaluate the specific impact of GNN guidance more effectively.\nWe investigate various configurations, including graph representations, back- track strategies, and the conditions for employing GNN-guided branches, aiming to analyze the behaviors of the algorithm across different settings.\nThe evaluation tables reveal that while the split algorithm effectively solves single word equations, it does not demonstrate marked improvements for mul- tiple conjunctive word equations relative to other solvers. This discrepancy is attributed to the significance of the processing order for conjunctive word equa- tions, where our current solver employs a predefined order. It is possible to make use of a GNN to compute the best equation to start with. However, this involves ranking a list of elements with variable lengths, rather than performing a fixed-category classification task, and requires completely different training"}, {"title": "Proof of Lemma 1", "content": "Proof. Outline of the proof:\nR1: Trivial.\nR2: Simplify \u20ac = \u20ac / $ to 4, then the premise and conclusion are identical.\nR3: Assign e to X for both the premise and conclusion, then simplify it to make the premise and conclusion identical.\nR4: Does not exist an assignment to make the premise SAT due to a cannot be \u20ac.\nR5, R9: Simplify the premise to u = \u03c5 \u039b by eliminating the same prefix, then the premise and conclusion are identical.\nR6: Does not exist an assignment to make the premise SAT due to mis- matched prefix.\nR7, R8: Assign e or a. X' to X to make the premise identical to one of the conclusions\nR8: Assign Y to X, X to Y, Y \u00b7 Y' to X, or XX' to Y to make the premise identical to one of the conclusions."}]}