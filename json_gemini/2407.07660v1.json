{"title": "Boosting Medical Image Synthesis via Registration-guided Consistency and Disentanglement Learning", "authors": ["Chuanpu Li", "Zeli Chen", "Yiwen Zhang", "Liming Zhong", "Wei Yang"], "abstract": "Medical image synthesis remains challenging due to misalignment noise during training. Existing methods have attempted to address this challenge by incorporating a registration-guided module. However, these methods tend to overlook the task-specific constraints on the synthetic and registration modules, which may cause the synthetic module to still generate spatially aligned images with misaligned target images during training, regardless of the registration module's function. Therefore, this paper proposes registration-guided consistency and incorporates disentanglement learning for medical image synthesis. The proposed registration-guided consistency architecture fosters task-specificity within the synthetic and registration modules by applying identical deformation fields before and after synthesis, while enforcing output consistency through an alignment loss. Moreover, the synthetic module is designed to possess the capability of disentangling anatomical structures and specific styles across various modalities. An anatomy consistency loss is introduced to further compel the synthetic module to preserve geometrical integrity within latent spaces. Experiments conducted on both an in-house abdominal CECT-CT dataset and a publicly available pelvic MR-CT dataset have demonstrated the superiority of the proposed method.", "sections": [{"title": "1 Introduction", "content": "Medical image synthesis can offer advantages by predicting a target imaging modality in situations where acquisition is impractical due to time and cost constraints, or when image registration introduces unacceptable uncertainty be-tween images of different modalities [2,23]. For example, MR-to-CT synthesis is a clinically significant solution for MRI-only radiation therapy treatment plan-ning [6,24], and CECT-to-CT synthesis is beneficial for reducing the registration errors between contrast-enhanced CT (CECT) and CT, the additional exposure, and treatment cost [14,19].\nExisting medical image synthesis methods can be categorized into super-vised and unsupervised learning paradigms. In the supervised setting, source and target modalities are registered prior to training [5, 14, 24]. However, as shown in Fig. 1(a), the standard image pre-processing steps cannot entirely eliminate misaligned errors, particularly in anatomical areas like the abdomen or pelvis. This misalignment can result in supervised methods learning inac-curate mappings [5, 24, 26, 27]. Unsupervised methods alleviate the necessity of paired images. Unfortunately, these methods cannot fully utilize retrospective images from the same patient, and the performance of unsupervised synthesis methods is inferior to supervised ones [6,15,25]. Therefore, better image quality may be obtained by trying to solve the misalignment problem in the supervised paradigm.\nRecent methods impose the registration-guided module to mitigate the mis-alignment challenge [7, 11, 18, 25-27]. For example, Kong et al. proposed Reg-GAN to train the generator using an additional registration network to fit the misaligned noise distribution adaptively for T1-to-T2 MR image synthesis [11]. Zhong et al. proposed quartet attention aware closed-loop learning (QACL) framework for MR-to-CT synthesis via simultaneous registration [25]. However, these methods neglect the task-specific constraints on synthetic and registra-tion modules. Specifically, the synthetic module should ideally preserve geomet-ric properties, while the registration module should effectively learn geometric transformations from images suffering from misalignment. This neglect could po-tentially cause the synthetic module to directly generate spatially aligned images with the misaligned target image during training, regardless of the function of the registration block [1]. In the testing phase, images synthesized solely using the"}, {"title": "2 Method", "content": "Let $S \\subset \\mathbb{R}^{D_s \\times H_s \\times W_s}$ and $T \\subset \\mathbb{R}^{D_T \\times H_T \\times W_T}$ represent domains of source image and target image, where D, H, W are the slice numbers, height, and weight. We denote $S = \\{I_s \\in S\\}_{s=1}^{N}$ and $T = \\{I_t \\in T\\}_{t=1}^{N}$ as paired image sets. Medical image synthesis aims to learn a synthetic network G that maps the source image $I_s$ to its target image $O_t = G(I_s)$. However, the inevitable misalignment between S and T often results in inaccurate mapping. To address this challenge, we propose a registration-guided consistency disentanglement learning method. As shown in Fig. 2, the proposed scheme consists of two major parts: (a) a registration-guided consistency architecture, (b) an anatomy consistency disentanglement synthetic (ACDS) module. Below, we describe the design and objective of each part."}, {"title": "2.1 Registration-guided Consistency", "content": "A key challenge of registration-guided medical synthesis lies in delineating the respective roles of the synthetic network and registration network. Specifically, it's crucial to avoid situations where the synthetic network ignores the function of the registration network and directly synthesizes spatially aligned images during training. To achieve this, we assume that the synthetic and registration processes are commutative and propose a registration-guided consistency structure (As shown in Fig. 2(a)).\nRegistration-guided Module. The registration module is composed of a deformation generator $R_{\\phi}$ and a re-sampler $R_s$. $R_{\\phi}$ is designed to take $I_s$ and $I_t$ as input, and produce a deformation field $\\phi$ describing the non-rigid alignment from $I_s$ to $I_t$:\n$\\phi = R_{\\phi}(I_s, I_t).$   (1)\n$R_s$ receives the deformation field $\\phi$ and applies it to an image desired to be moved. We have adopted the architectural framework detailed in [4] as the foun-dation of our registration module, and incorporated its smoothing loss to preserve the smooth displacement of the deformable field:\n$L_{smooth}(\\phi) = \\mathbb{E}_{s,T}[||\\nabla \\phi||^2].$   (2)\nSynthesis Before Registration. Synthesis before registration first applies the synthetic module G on $I_s$, followed by the utilization of the re-sampler layer $R_s$ on the synthesized image. This process yields the final output, which can be formally expressed as:\n$O_t^f = R_s(G(I_s), \\phi).$   (3)\nSynthesis After Registration. In contrast, synthesis after registration first applies the re-sampler layer $R_s$ on $I_s$, followed by the utilization of the synthetic module G on the warped image. This process yields the other final output, which can be formally expressed as:\n$O_t^b = G(R_s(I_s, \\phi)).$   (4)\nAlignment Loss. In Eq.3 and Eq.4, the deformation field $\\phi$ used in the re-sampler $R_s$ is both given by the $R_{\\phi}(I_s, I_t)$ and designed to be the same. The sole difference is that the synthetic module G accepts geometrically different images as inputs ($I_s$ and $R_s(I_s, \\phi)$). Thus, if we assume that the synthetic and registra-tion processes are commutative, which expect the same output from synthesis before and after registration, the synthetic module G is required to be geometry preserving and finally alleviate the misalignment issue. This assumption can be easily realized by a simple alignment loss, which can be formulated as:\n$L_{align}(G, R) = ||O_t^f - I_t||_1 + ||O_t^b - I_t||_1.$   (5)"}, {"title": "2.2 Anatomy Consistency Disentanglement Synthetic Module", "content": "Given that variations in styles across modalities may be falsely interpreted as motions by the registration network [12], it is beneficial for the synthetic module G to disentangle the specific style for each modality. Moreover, disentangling anatomical structures and enforcing their consistency across identical content images is important in mitigating misaligned noise. Therefore, we propose an anatomy consistency disentanglement synthetic module.\nSynthetic process. As shown in Fig. 2(b), the ACDS module is composed of two content encoders $\\{E_s, E_t\\}$, two style encoders $\\{E_s^S, E_t^S\\}$, and two generators $\\{G_s, G_t\\}$. The encoders in both domains respectively encode an input image to an anatomical-invariant space and a modality-specific style space, and the generators respectively combine the anatomical space features and style features to synthesize images. This can be formulated as:\n$O_t = G_t(c_s, s_t) = G_t(E_s^C(I_s), E_t^S(d_t)),$   (6)\n$O_s = G_s(c_t, s_s) = G_s(E_t^C(I_t), E_s^S(d_s)),$   (7)\nwhere $O_t$ and $O_s$ denote the synthetic images, $d_t$ and $d_s$ denote the learned domain code, which is empirically set as an 8-bit vector."}, {"title": "Disentanglement Learning", "content": "Inspired by [8,13], the synthetic module real-izes disentanglement via generative adversarial loss, self-reconstruction loss, and cycle-consistency loss. We define them as follows:\n$L_{adv} = \\mathbb{E}_T[log D_t(I_t)] + \\mathbb{E}_S[1 - log D_t(O_t)] + \\mathbb{E}_S[log D_s(I_s)] + \\mathbb{E}_T[1 - log D_s(O_s)],$   (8)\n$L_{self} = ||G_s(E_s^C(I_s), E_s^S(d_s)) - I_s||_1 + ||G_t(E_t^C(I_t), E_t^S(d_t)) - I_t||_1,$   (9)\n$L_{cycle} = ||G_s(E_s^C(O_t), E_s^S(d_s)) - I_s||_1 + ||G_t(E_t^C(O_s), E_t^S(d_t)) - I_t||_1,$   (10)\nwhere $D_t$ and $D_s$ represent the discriminators. If the encoders can successfully disentangle the anatomical content from images and the generators uphold ge-ometric fidelity, anatomical content features should be identical. Therefore, we propose an anatomy consistency loss:\n$L_{anatomy} = ||E_t^C(O_t) - E_s^C(I_s)||_1 + ||E_s^C(O_s) - E_t^C(I_t)||_1.$   (11)\nThe implementation of the anatomy consistency loss necessitates the ACDS module to further learn and retain anatomical structure information in the latent space. Then, the overall loss function is:\n$L_{total} = L_{adv} + L_{self} + L_{cycle} + \\lambda_1 L_{anatomy} + \\lambda_2 L_{smooth} + \\lambda_3 L_{align},$   (12)\nwhere $\\lambda$ is the hyperparameter. As shown in Fig. 2(c), after training, only $E_s^C$, $E_t^S$ and $G_t$ are required to translate the source image into target image in the testing phase. The content encoder consists of 3 Conv-IN-ReLU blocks with kernel sizes of 3, strides of 2, channels of 32, 64, 128, followed by four residual blocks. The style encoder consists of 3 MLP layers."}, {"title": "3 Experiments", "content": "Datasets. We evaluated our methods on an in-house abdominal CECT-CT dataset and a publicly available pelvic MR-CT derived from the SynthRAD2023 Grand Challenge dataset [20]. The CECT-CT dataset comprised 278 patients diagnosed with liver cancer. Preprocessing of images involved sequential imple-mentation of affine spatial normalization [10] and nonlinear registration tech-niques [3], aimed at aligning the CECT images with the corresponding CT images. For evaluation, we manually selected 15 well-aligned subjects for the validation set and 40 well-aligned subjects for the test set. The pelvic MR-CT dataset consists of 180 patients. Similarly, the validation set comprised 6 aligned subjects, while the test set included 30 appropriately aligned subjects."}, {"title": "Implementation Details", "content": "Our method was implemented with PyTorch, using 2 GPU of NVIDIA RTX3090 GPUs. The hyperparameter $\\lambda_1$, $\\lambda_2$, $\\lambda_3$ were set as 0.5, 10 and 20. For the anatomy consistency disentanglement synthetic module, we used a similar structure as [8] and changed it into 3D. We employed Adam optimizer with a poly decay strategy, a batch size of 2, and a maximum of 200 epochs to dynamically adjust the learning rate from 2 \u00d7 10\u22124."}, {"title": "Evaluation Metrics", "content": "To evaluate the efficacy of our proposed model, we em-ployed three standard evaluation metrics: the mean absolute error (MAE), the structural similarity index measurement (SSIM), and the peak signal-to-noise ratio (PSNR). Calculations for MAE, SSIM, and PSNR were confined within the body mask, with SSIM parameters set under the guidelines established by Wang et al. [22]."}, {"title": "3.2 Results", "content": "Comparisons with State-of-the-Art. We compared our method with several synthesis methods including supervised methods such as Pix2pix [9], ResViT [5], unsupervised methods such as CycleGAN [28], disentanglement methods such as UNIT [13], and registration-guided method such as RegGAN [11].\nAs illustrated in Table 1, our proposed method demonstrates superior per-formance, exhibiting a MAE of 16.38 \u00b1 1.83, a PSNR of 36.52 \u00b1 1.66 and an SSIM of 89.22 \u00b1 2.83 for CECT-to-CT synthesis, and an MAE of 47.47 \u00b1 4.59, a PSNR of 30.86 \u00b1 0.88 and a SSIM of 83.70 \u00b1 2.47 for MR-to-CT synthesis. Fig. 3 shows the visual results. The supervised and previous registration-guided methods still retain misalignment noise and have blurred boundaries due to the averaging effect. In contrast, our methods preserve anatomical consistency and clear boundaries in the regions where misalignment is often encountered during training.\nAblation Study. An ablation study was undertaken to assess the efficacy of the registration-guided consistency and ACDS module. Table 2 shows the results ob-"}, {"title": "4 Conclusion", "content": "This paper introduces a registration-guided consistency architecture and an anatomy consistency disentanglement synthetic module to address the misalign-ment issue for medical image synthesis. The proposed registration-guided consis-tency design customizes the registration and synthetic module to be task-specific and encourages the synthetic module to eliminate the influence of misalignment-induced noise. Additionally, disentanglement learning and an anatomy consis-tency loss are applied to the synthetic module, enhancing its ability to preserve geometric integrity, thus further avoiding the misalignment issue. Experimental results conducted on both an in-house CECT-CT dataset and a publicly available MR-CT dataset have demonstrated the superiority of the proposed method."}]}