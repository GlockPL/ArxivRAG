{"title": "Towards a Domain-Specific Modelling Environment for Reinforcement Learning", "authors": ["Natalie Sinani", "Sahil Salma", "Paul Boutot", "Sadaf Mustafiz"], "abstract": "In recent years, machine learning technologies have gained immense popularity and are being used in a wide range of domains. However, due to the complexity associated with machine learning algorithms, it is a challenge to make it user-friendly, easy to understand and apply. Machine learning applications are especially challenging for users who do not have proficiency in this area.\nIn this paper, we use model-driven engineering (MDE) methods and tools for developing a domain-specific modelling environment to contribute towards providing a solution for this problem. We targeted reinforcement learning from the machine learning domain, and evaluated the proposed language, reinforcement learning modelling language (RLML), with multiple applications. The tool supports syntax-directed editing, constraint checking, and automatic generation of code from RLML models. The environment also provides support for comparing results generated with multiple RL algorithms. With our proposed MDE approach, we were able to help in abstracting reinforcement learning technologies and improve the learning curve for RL users.", "sections": [{"title": "1 Introduction", "content": "The advent of artificial intelligence and machine learning technologies marks a significant shift in the landscape of software and systems, with groundbreaking developments\n1"}, {"title": "2 Background", "content": "This section provides necessary background on model-driven engineering and reinforcement learning.\n2"}, {"title": "2.1 Model-Driven Engineering", "content": "Model-Driven Engineering (MDE) is an approach to software development where models take a central role [5]. In MDE, abstract representations of the knowledge and activities of a domain are created, which form the basis for code generation, analysis, and simulation. This approach emphasizes the separation of concerns, allowing developers to focus on domain-specific logic rather than low-level implementation details. MDE is particularly effective in managing complexity and enhancing productivity, as it allows for high-level abstraction and automation in the software development process.\nModels in MDE have to conform to some modelling language, which may be graphical or textual. These languages have three main ingredients: abstract syntax, concrete syntax, and semantics. In addition to general-purpose modelling languages (e.g., UML), domain-specific languages (DSL) can also be developed, which are tailored to specific problem domains and simplify complex concepts, making models more accessible and intuitive for users. Expert knowledge is used to define DSLs that work at a higher level of abstraction than general-purpose languages. This allows a tight fit with the domain and changes in the domain can be easily incorporated into the language. The use of such languages separate the domain-experts work from the software developers work and also make it easier for non-technical people to learn and avoid errors in the initial development phases. They also provide support for text or code generation, which automates the process of creating executable source code and/or textual artifacts (e.g., documentation, task lists, etc.) from the domain-specific models. The developers are not required to manually write and maintain these artifacts, which significantly improves productivity and reduces defects in the implementation, thus improving software quality."}, {"title": "2.2 Reinforcement Learning", "content": "Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of rewards. It is a self-teaching system trying to find an appropriate action model that would maximize an agent's total cumulative reward, by following the trial and error method. In general, the RL algorithms reward the agent for taking desired actions in the environment, and punishes i.e., grants negative or zero rewards, for the undesired ones [1]. The following are the key components that describe RL problems.\n\u2022 Environment: The RL environment [6] represents all the existing states that the agent can enter. It produces information that describe the states of the system. The agent interacts with the environment by observing the state space and taking an action based on the observation. Each action receives a positive or negative reward, which informs the agent on selecting the next state.\n\u2022 Agent: This is represented by an intelligent reinforcement learning algorithm that learns and makes decisions to maximize the future rewards while moving within the environment.\n\u2022 State: The state represents the current situation of the agent. It should be noted that term state in RL is different from the meaning of state in MDE in the context of state machines [7].\n3"}, {"title": "3 Modelling Language Design", "content": "In this section, we describe the proposed DSL for reinforcement learning, RLML. The core language concepts was designed based on the main elements representing the reinforcement learning problem and solution algorithms. This includes the environment representing the reinforcement learning problem, and the agent that is learning and exploring in the environment."}, {"title": "3.1 RLML Abstract Syntax", "content": "Reflecting the RL domain concepts, RLML mainly consists of an environment element and an agent element, and additionally, the result element. Successively these elements contain all the other details involved in solving an RL problem. Similarly, the RLML Comparator consists of the same elements as RLML, except it can have multiple agent elements as well as corresponding number of result elements. Fig. 2 presents the metamodel for RLML.\n\u2022 RLML: The RLML element is the root element of all the other elements in the language, and contains the environment element, RL agent and the result. It also has a string property which represents the name of the project, along with other properties that lets the user decide an input method and a run language method.\n\u2022 RLML Comparator: This is another root element which is almost a replica of RLML and contains all the other elements in the language except RLML. It contains the environment element, multiple RL agent elements, multiple result elements and a string property which represents the name of the project.\n5"}, {"title": "3.2 RLML Concrete Syntax", "content": "One of the goals of RLML is to reduce the complexity involved in implementing RL applications. RLML uses textual concrete syntax and can be modelled as a simple configuration-like properties file, as shown in the sample model in Fig. 4.\nThe inspiration of the RLML concrete syntax comes from YAML representation, which is a human-readable data format used for data serialization. It is used for reading and writing data independent of a specific programming language. Another significant aspect for this concrete syntax is its relevance in model-free algorithms, where the a dynamic state-action space is required for the agent's actions. This interaction-focused approach is key in model-free reinforcement learning, allowing the agent to effectively learn and refine its strategy through direct experience, even in complex and variable scenarios. As per the abstract syntax, the model needs to specify the project's name, the environment element properties (the states, actions, rewards and terminal states), and the agent's RL algorithm type and the settings for that algorithm."}, {"title": "3.3 RLML Constraints", "content": "The property values are considered valid when they are in a format that RLML can use to implement the chosen RL algorithm. To ensure that the user is entering valid properties, we defined the following DSL validation constraints.\n\u2022 States Constraint: States property value is expecting a string representation of all the possible states an agent can move within the current world or the environment of the current task. The value of the states property must be a comma-separated list of state strings, within square brackets. The individual state names cannot have comma or spaces.\nValid example: [A, B, C, D, E, F]\n\u2022 Actions Constraint: The possible actions that the agent can take for each state of the states array. This value is also in string format and expects a two-dimensional array of indexes. The array of indexes contain the index values of the states that the agent can go to, starting from the given state. Each array is a comma-separated list within square brackets. The constraint validates the format of the provided string value and checks that the length of actions array element is equivalent to the length of the states array. In the valid example below, we can see that there are six arrays of indexes to match the length of the example array for states.\nStates example: [A, B, C, D, E, F]\nValid Example: [[1,3], [0,2,4], [2], [0,4], [1,3,5], [2,4]]\n\u2022 Rewards Constraint: The rewards property value is similar to the actions property value. In this case, the two-dimensional array contains reward values the agent will receive when moving from the given state to other states in the environment. The RL algorithm will eventually learn to move towards the states that give maximum future rewards and ignore the ones that do not give rewards. Each array is a comma-separated list within square brackets. Similar to actions value validation, the rewards constraint validates the format of the string and checks that the length of the rewards array is equal to the length of states array and the length of individual rewards elements, is equivalent to the length of the states array. In the valid example below,\n7"}, {"title": "4 Domain-Specific Modelling Environment", "content": "This section describes the tool support developed for RLML."}, {"title": "4.1 RLML Features", "content": "A modelling environment has been designed and developed to create RLML models. Translational semantics have been implemented to support execution of the models through the environment. The modelling environment supports use of different agents as well as displays the output of the RL training in the environment.\nIn our project, which supports both Java and Python, we focus on maintaining algorithmic uniformity. Thus we have the same algorithms for both general purpose languages. Concurrently, efforts were made to enhance Java's RL capabilities, ensuring it remains a viable option for those preferring or requiring it. Our balanced approach enhances the project's overall utility, catering to the diverse needs of the RL community and maintaining inclusivity across programming preferences.\nWe have also incorporated a capability for users to save their trained RL models. This addition serves to facilitate the retention and subsequent utilization of these models, offering researchers a valuable resource. Additionally, we developed support for running multiple algorithms simultaneously, presenting data for each distinct variation. This functionality not only allows users to compare and analyze different algorithmic approaches side by side but also facilitates a deeper understanding of how variations in parameters affect outcomes (see Fig. 5). It provides a robust platform for experimentation, enabling users to efficiently identify the most effective algorithms and parameter settings for their specific use cases. This multi-algorithm capability greatly enhances the tool's utility in complex scenarios, making it an invaluable asset for both research and practical applications in diverse fields where nuanced algorithmic comparisons are essential.\nRecognizing the complexity of RL inputs and the impracticality of manual entry in some cases, we have enhanced RLML with the capability to import values through a text file. This feature allows users to select a file (see Fig. 6), which is then processed to ensure it contains valid data. Upon confirmation of valid input, the system\n8"}, {"title": "4.2 RLML Editor", "content": "MPS is a language workbench which provides a tool or set of tools to support language definition, and it implements language-oriented programming. MPS is an integrated development environment (IDE) for DSL development, which promotes reusability and extensibility. The language definition in MPS consists of several aspects: structure, editor, actions, constraints, behaviour, type system, intentions, plugin and data flow. Only the structure aspect is essential for language definition and the rest are for additional features. These aspects describe the different facets of a language.\nWe have employed the structure, editor and constraints aspects in the RLML definition. The structure aspect, defines the nodes of the language Abstract Syntax Tree (AST), known as concepts in MPS. The editor aspect describes how a language is presented and edited in the editor and it enables the language designer to create a user interface for editing their concepts. Constraints describe the restrictions on the AST.\n\u2022 RLML structure: The structure aspect contains the concepts that represent the RLML metamodel. Each concept consist of properties and children, reflecting the properties and the relationships in the RLML metamodel, shown in Fig. 2.\n\u2022 RLML editor: The RLML editor aspect is configured to define RLML's concrete syntax as described and illustrated earlier in Fig. 4). The concept editor for RLML root element contains \"Click Here\", \"Browse File\", \"Change Run Language\", \"Run Program\u201d and \u201cClear Result\" buttons (see Fig. 4). The first button (Click Here) refers to input via file, clicking the button will show or hide the \"Browse File\"\n9"}, {"title": "4.3 RLML Code Generation", "content": "This work aims to provide abstractions to reduce the complexity associated with reinforcement learning problems and algorithms by generating runnable code from the RLML models. Generators define possible transformations between a source modelling language and a target language, typically a general purpose language, like Java or Python.\nFor our proposed language, we implemented the model to code transformation to generate code from RLML models. We have used a root mapping rule and reduction rules for our code generation. While Java is directly supported by MPS, it is limited in generating code for Python. Since we want to generate python code, we utilize an open-source MPS module. This module allows us to extend MPS's capabilities to generate Python, applying similar model-to-text transformations as with Java. This integration enhances the versatility of our tool, supporting a wider range of programming languages and accommodating a broader user base.\n\u2022 Root mapping rule: RLML's generator module contains two root mapping rules, one for the RLML element and other for RLMLComparator element, which are the root"}, {"title": "4.4 Discussion", "content": "Most machine learning libraries are widely available as Python libraries and not as Java libraries, hence it was challenging to find Java libraries to support RL algorithms. For the few available libraries, they were not fully supported by MPS. We were able to overcome this challenge by implementing algorithms in Java manually and using an open source MPS module to support Python code generation.\nA second issue was that RL problems do not have fixed input format from actions, rewards and states perspective. Therefore, it was not straightforward to come up with a format for those inputs. More validation and mapping is needed for broader RL problem coverage and code generation. This could be fixed by using more model-based algorithms where the agent is given an environment it can interact with to solve the problem.\nAddressing the challenge of handling large data set inputs, our tool offers a feature for data input through a file. This method is particularly useful for adding large inputs efficiently. However, it is important to note that this feature requires the text files to adhere to a specific format (as defined in the concrete syntax of the language).\n12"}, {"title": "5 Validation", "content": "We have validated the proposed reinforcement learning domain-specific language on four applications: path finding, simple game, blackjack, and frozen lake. These applications are well-known applications used in the RL domain [13]. We present the first three applications here. For details on the use of RLML on the frozen lake application, please refer to [14]. The artifacts are available at https://github.com/mde-tmu/RLML.\nImplementation of the Monte Carlo and DQN algorithms in Java is currently work in progress, hence the validation does not cover these algorithms."}, {"title": "5.1 Path Finding Application", "content": "The path finding problem [15] is a common application in the machine learning domain that can be solved with different algorithms, including RL algorithms. In the path finding environment, the agent's goal is to learn the path to a target state, starting from a randomly selected state (see Fig. 7). There are in total six states in this application, represented by the alphabetical letters A to F. On each episode the agent will be placed in a random state, from there, it will take actions, and move to new states trying to reach the goal state, which is C for this application. Once the agent reaches the goal state, that episode will be considered complete. The agent will repeat the training episodes a given number of times, as configured in the RL algorithm. In an RLML model, this is set as the total episodes in the RL algorithm entity. At the end of the training, the agent will learn the best path to the goal state C, starting from the random initial state. The agent learns the path to the goal state by updating what is referred to as Q-Table and aims to calculate the optimal action value function that can be used to derive the optimal policy.\nThe RL environment needs to be modelled in a format that conforms to the RLML abstract and concrete syntax (described in Sec. 3). We model the path finding environment as states, actions, rewards and terminal states arrays, as shown in Fig. 8a. Next, the path finding application environment variables and reinforcement learning algorithm option needs to be selected in the RLML model. Sample RLML model instances for the path finding algorithm are shown in Fig. 8b and Fig. 8c.\nThe general purpose language code is automatically generated from the RLML model for each selected algorithm. At a high level, it is a Java/Python file named according to the RLML root element name property, e.g., PathFindingQLearning (similar to the code presented in Fig. 12 and Fig. 13). It contains a method to implement the chosen algorithm, a method to run it and one to print the results.\nThe RLML modelling environment contains the Run Program button (explained earlier in Sec. 4.2). Once we click on the Run Program button, the environment is dynamically updated with the calculated results and we can see the result of running\n14"}, {"title": "5.2 Simple Game Application", "content": "The second example we used to validate RLML is a simple game application [16] (see Fig. 11). This application has a similar environment to the path finding application."}, {"title": "6 Related Work", "content": "While machine learning is widely applied in the MDE area, there is a need for MDE application in the ML area [2]. There is limited work available in this area. DSMLs for the artificial intelligence domain and more specifically, machine learning domain, are recent contributions, driven by the need to make ML algorithms easier to understand by improving the learning curve, as well as more attractive to apply and faster to test and compare different solutions to one another. The application of MDE in the RL\n19"}, {"title": "6.1 Application of MDE in ML", "content": "Our work takes inspiration from the Classification Algorithm Framework (CAF) [17]. The primary difference between CAF and RLML is that CAF is developed for machine learning classification algorithms, and RLML is developed for reinforcement learning algorithms. CAF expects inputs to apply classification algorithms, and RLML expects inputs to implement reinforcement learning algorithm. They have similar configuration-like interface where user can interact with it. CAF supports code generation in the Java language, while RLML supports both Java and Python. Unlike CAF, RLML offers a comparator feature.\nLiaskos et al. [18] present a modelling and design process for generating simulation environments for RL based on goal models defined using iStar. This allows model-based reasoning to be carried out and for agents to be trained prior to deployment in the target environment. The paper mentions that high-level RL models can be automatically mapped to these simulation components. While this work has a very\n20"}, {"title": "6.2 RL Libraries and Toolkits", "content": "As the field of reinforcement learning evolves, various platforms and libraries have been established to facilitate the development of RL applications, each with its distinct features and focus areas. For instance, the Reinforcement Learning Toolkit developed with Unreal Engine [21] emphasizes immersive simulation environments. Python libraries\n21"}, {"title": "7 Conclusion", "content": "We proposed a domain-specific language, RLML, for the reinforcement learning domain. With the use of the language workbench MPS, we built a domain-specific modelling environment for RLML, which supports model editing, syntax checking, constraints checking and validation, as well as code generation. Our proposed language is developed to be easily extensible to support a wide range of RL algorithms. However in this paper, we targeted model-free, gradient-free category of reinforcement learning algorithms. To the best of our knowledge, this work is a first step in this direction for reinforcement learning.\nThrough our reinforcement learning applications, we validated our proposed language. With our validation, we showcased how RLML achieved the abstraction needed in RL applications, by providing a configuration-like model where it is only expecting input values of the RL problem environment and a choice of RL algorithm. From that point, the RLML modelling environment can generate executable code, run it and display the results. The environment also provides a comparator to compare results obtained with different RL algorithms. It supports both Java and Python implementations.\nRLML can also be used on business case studies and to get feedback from reinforcement learning users at different levels of expertise. Moreover, RLML can be helpful in academia for making reinforcement learning accessible for non-technical students. This work is a starting point towards developing an environment for supporting various types of RL technologies, both model-free and model-based. As discussed earlier,\n22"}]}