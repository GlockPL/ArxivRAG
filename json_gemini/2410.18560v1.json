{"title": "Explainable News Summarization - Analysis and mitigation of Disagreement Problem", "authors": ["Seema Aswani", "Sujala D. Shetty"], "abstract": "Explainable AI (XAI) techniques for text summarization provide valuable understanding of how the summaries are generated. Recent studies have highlighted a major challenge in this area, known as the \"disagreement problem.\" This problem occurs when different XAI methods offer contradictory explanations for the summary generated from the same input article. This inconsistency across XAI methods has been evaluated using predefined metrics designed to quantify agreement levels between them, revealing significant disagreement. This impedes the reliability and interpretability of XAI in this area. To address this challenge, we propose a novel approach that utilizes sentence transformers and the k-means clustering algorithm to first segment the input article and then generate the explanation of the summary generated for each segment. By producing regional or segmented explanations rather than comprehensive ones, a decrease in the observed disagreement between XAI methods is hypothesized. This segmentation-based approach was used on two news summarization datasets, namely Extreme Summarization (XSum) and CNN-DailyMail, and the experiment was conducted using multiple disagreement metrics. Our experiments validate the hypothesis by showing a significant reduction in disagreement among different XAI methods. Additionally, a JavaScript visualization tool is developed, that is easy to use and allows users to interactively explore the color-coded visualization of the input article and the machine-generated summary based on the attribution scores of each sentences.", "sections": [{"title": "1. Introduction", "content": "Text summarization is a process that effectively condenses lengthy documents into concise summaries by emphasizing the most significant and pertinent information from the original document (Widyassari et al., 2022). It has now turned into a critical tool that can help cope with the ever-growing amount of information available in recent times. Automated text summarization systems are often driven by the powerful deep learning architectures like BART (Lewis, 2019) and PEGASUS (Zhang et al., 2020). These pretrained transformer based architectures helps in producing exceptionally good, coherent, and fluent summary. However, the intrinsic black box nature of these models raises concerns for their reliability, transparency, and potential biases.\nExplainable AI is an area of research in AI that tries to address these concerns with techniques that provide understandable explanations of a complex AI model (Saranya & Subhashini, 2023). In text summarization, the methods of XAI are intended to provide transparent and interpretable explanation for why certain words/sentences were included in or excluded from the summary. This makes it easier to make the model output more comprehensible and plausible for the users, which is extremely helpful to developers in pinpointing areas of possible biases or flaws to correct.\nOur research delves into the internal workings of two widely used abstractive text summarization models: a pre-trained BART model that has been fine-tuned using the XSum dataset (Narayan et al., 2018), and a Pegasus model that has been fine-tuned using the CNN/Daily Mail dataset (Nallapati et al., 2016). A rigorous testing ground for our analysis is provided by two news summarization datasets: Xsum and CNN/Daily Mail. The XSum dataset is a challenging benchmark consisting of BBC news articles and their corresponding human-written summaries. In contrast, the CNN/Daily Mail dataset provides a wide range of news articles together with their corresponding highlights. To Analyze the decision making process of the LLMs used in our study we have employed a wide range of Explainable AI (XAI) techniques, including DeepLIFT (Shrikumar et al., 2017), LIME (Ribeiro et al., 2016), Gradient SHAP (Lundberg, 2017), and attention mechanism (Bahdanau, 2014). Each of these approaches provides a distinct perspective to"}, {"title": "3. Methodology", "content": "In this study we have divided the methodology framework into two primary phases, each of which is aimed at addressing a particular problem with interpretability and consistency in text summarization driven by XA\u0399.\nPhase A: Disagreement Analysis: The process starts with laying a strong foundation using SOTA(state-of-the-art) text summarization mod-els. Here, we apply and assess different Explainable Artificial Intelligence (XAI) techniques to elucidate the results of the model. Subsequent to the generation of explanations, we employ pre-defined disagreement metrics to assess the degree of agreement between two Explainable Artificial Intelligence (XAI) methodologies regarding the explanations they have generated. This methodology facilitates a thorough pairwise agreement analysis, which ex-plicitly identifies and quantifies the discrepancies in the explanations offered by the model."}, {"title": "Phase B: Addressing Disagreement through Regional Explanations", "content": "The purpose of the following stage is to address the problem of disagreement that was identified in the first stage. The present study introduces an innovative methodology that utilizes regional explanations to offer more intricate understanding of the decision-making process of the model."}, {"title": "Text Summarization Model and Dataset", "content": "We create concise and accurate summaries of news articles using the PE-GASUS (Pre-training with Extracted Gap-sentences for Abstractive Sum-marization) (Lewis, 2019) model and the BART (Bidirectional and Auto-Regressive Transformers) model (Zhang et al., 2020). We specifically used BART model fine-tuned on the XSum (Extreme Summarization) dataset and applied the PEGASUS model fine-tuned on the CNN/Daily Mail dataset. BART is highly effective at capturing the nuances and context of the input text. This is because of the way it is designed, combining the bidirectional understanding of BERT (Devlin, 2018) with the autoregressive generating powers of GPT. Similarly, architecture of PEGASUS incorporates a special pre-training goal designed for abstractive summarization, which enables it to succeed in generating informative and coherent summaries by predicting masked sentences within the text. We take advantage of capabilities of both"}, {"title": "Pre-processing Steps", "content": "1. Handling Periods within Quotations:\nIn an article if the quotation mark is followed by a space and a period then this step will remove the space between the period and a quotation\nmark. In the same way, we changed patterns where a period is placed outside a quotation mark to a period placed within quotation mark.\n2. Ensuring Sentence Termination:\nTo ensure that all the sentences end with a period in the article we added a check in this step. We first use newline characters to break the article apart in sentences, and then place a period at the end of the sentence if the period is missing.\n3. Reducing Redundant Periods:\nTo prevent misinterpretation during token aggregations, we substituted a single period for several consecutive periods.\n4. Handling Question Marks:\nThis step makes sure to standardize the placement of question mark with the period followed by a question mark.\n5. Handling Abbreviations, Initials and web address periods:\nIf the period is placed between name initials we substituted it with a special token [NAME_PERIOD_TOKEN]. In the same manner in a website address a period is also replaced with a special token [WEB_PERIOD_TOKEN].\n6. Protecting Email Addresses and Decimal Numbers:\nTo avoid misinterpretation of periods used between numbers and email address, we substituted a special token [EMAIL_PERIOD_TOKEN] for pe-riods within email addresses and [NUMBER_PERIOD_TOKEN] for periods expressed in decimal numbers."}, {"title": "Model Architecture and Traning", "content": "1. BART Model Implementation:\nIn this study the pre-trained BART based model checkpoint facebook/bart-large-xsum, from huggingface was employed to summarize the Xsum news articles. BART (Bidirectional and auto-regressive transformers ) is transformer-based encoder-decoder architecture that embeds the bidirectional encoding of models like BERT with the auto-regressive decoding methodology of models like GPT. The encoder analyses input news articles, transforming them into comprehensive contextual repre-sentations that encapsulate semantic meanings and structural relation-ships. The decoder produces summaries in an autoregressive manner, predicting each token by utilizing the encoder's output and the pre-viously generated tokens. Its pre-training as a denoising autoencoder\nis what makes BART so effective. The original text is reconstructed from the corrupted versions introduced through sentence shuffling, to-ken masking and deletion. Due to the extensive pre-training the model is able to develop a deep understanding of linguistic structure and long-range dependencies. In line with the dataset's emphasis on extreme summarization the pre-fine tuned BART model, trained on the XSum dataset is utilized, to take advantage of its ability to generate succinct summaries that capture the main idea of the input text in a single, powerful sentence. Input articles were tokenized, up to a maximum of 1024 tokens of input.\n2. PEGASUS Model Implementation: To summarize the CNN/-Daily mail news articles the huggingface model checkpoint, google/pegasus-cnn dailymail model was employed. PEGASUS (Pre-training with Ex-tracted Gap-sentences for Abstractive Summarization) is a Transformer-based encoder-decoder architecture built for abstractive summarization tasks. The distinctive pre-training objective, Gap Sentence Generation (GSG), entails masking and reconstructing complete sentences(gap sen-tences) within a document. Due to the pre-training objective of PEGA-SUS architecture, the model is able to generate high quality abstarctive summaries of news articles. The input news articles are transformed into rich contextual representations by the PEGASUS encoder. The en-coder's output and the previously created tokens are both used by the decoder to generate summaries in an autoregressive manner. The pre-finetuned PEGASUS model, trained on the CNN/Daily Mail dataset was employed, to leverage its capacity for producing coherent and con-textually relevant multi-sentence summaries to accurately reflect the fundamental points of the input articles. The input was tokenized using the integrated sentence-piece tokenizer with a maximum input length of 1,024 tokens.\nThe need of additional fine-tuning was eliminated due to the use of these pre-finetuned models. This allows us to focus on the evaluation of text sum-marization model outcome and the explainability methods applied. The employed pretrained models have used the default parameters offered by the Huggingface Transformers library. This ensures consistency and repro-ducibility in our experiments."}, {"title": "XAI methods", "content": "In this study four different XAI methods - LIME, Gradient SHAP, Attention, and DeepLIFT were used to explain the text summa-rization model outcome. Every technique offers a unique perspective on the significance of different input features, to explain the behavior of the model. The reason of adapting four different categories of XAI methods is to observe how much do they agree with each other based on the explanation that they provide considering the algorithmic differences between these methods."}, {"title": "3.1. LIME (Local Interpretable Model-agnostic Explanations)", "content": "LIME locally approximates a complex model using an interpretable model to explain individual predictions (Ribeiro et al., 2016). It alters the input data, monitors the resultant changes in output, and constructs an inter-pretable model, typically a linear regression model, to these alterations. The method produces local explanations by generating a new dataset of perturbed examples and assigning weights according to their proximity to the original instance. In the text summarization task, LIME tries to explain the impor-tance of a particular word or sentence by removing some of them or omitting them from the input text and also observes the effects of such changes on the generated summary. The interpretable model describes local behavior of the original model around a given prediction. To generate the attribution LIME minimizes the objective function stated below :\n$f(x) = arg min L(f, g, \u03c0_{x}) + \u03a9(g)$\n$gEG$\nwhere:\n\u2022 \u00a7(x): Explanation of the input x.\n\u2022 g\u2208G: A selected model g from the class of interpretable models.\n\u2022 L(f, g, \u03c0\u03b1): The fidelity loss function, the measure of how inaccurately the interpretable model g approximates the original model f locally around x. The function \u03c0\u03b1 defines the locality around the instance x (usually based on proximity).\n\u2022 \u03c0\u03c7: The proximity measure, which indicates how similar or \"close\" a perturbed instance z is to the original instance x.\n\u2022 \u03a9(g): A complexity measure of the interpretable model g. For example, in linear models, this might be the number of non-zero weights, and in decision trees, it might be the depth of the tree."}, {"title": "3.2. Gradient SHAP", "content": "Gradient SHAP offers an accurate and efficient feature attribution estima-tion by combining SHAP (SHapley Additive exPlanations) values (Lundberg, 2017) with Integrated Gradients (Sundararajan et al., 2017). The two key steps in Gradient SHAP calculation are :\n1. Calculating the expectation of gradients by interpolating between base-line and input values\n2. sampling from the baseline distribution to account for multiple poten-tial baseline inputs\nThis approach guarantees consistent and additive explanations inline with Shapely values and solves the issue of baseline value assignment. Gradient SHAP can be applied in text summarization for assigning the importance of every word or token by calculating the gradient with respect to the output for every input feature that resulted in a given summary. This method very effectively points out which part of the input text is contributing much to the resulting summary. It uses the following formula for the estimation of an attribution score :\nAttribution = Ebaselines [gradients \u00d7 (inputs - baselines)]"}, {"title": "3.3. Attention", "content": "Attention mechanism, first introduced by (Bahdanau, 2014) for neural machine translation, is widely used for in transformer models to focus on relevant parts of the input text when generating outputs. The attention mechanism computes a weighted sum of the input features where weights are determined by how relevant each feature is to the output being generated at a particular step. Self-attention (Vaswani et al., 2017) enhances this concept by enabling the model to assess the significance of various tokens through the entire input sequence. In text summarization, attention mechanism en-able the model to selectively concentrate on different parts of the input text\nduring the generation of each summary word, enabling to capture the most important information.\nFormula:\nAttention(Q, K, V) = softmax($\\frac{QKT}{\\sqrt{dk}}$)V\nwhere Q (query), K (key), and V (value) are matrices derived from the in-put, and dk is the dimension of the key vectors."}, {"title": "3.4. DeepLIFT (Deep Learning Important FeaTures)", "content": "DeepLIFT (Shrikumar et al., 2017) is a technique for attributing the score of a deep learning model's prediction to its input features by contrasting the activation of each neuron with a reference activation. This method calculates an attribution score by backpropagation from the difference in activations of the output layer to the initial input layer. This highlights that it does not solely rely on gradients. DeepLIFT can assign the meaning of each word or phrase in the input text by analyzing how changes in the input affect the output summary to provide insights into the decision-making process of the text summarization. DeepLIFT employs a \"summation-to-delta\" principle that is pointed out in the Equation: 4 below.\nn\n\u03a3\u03b1\u03af\u03bf = \u0394\u03bf\ni=1\nwhere (o = f(x) ) is the model output, ( \u0394 \u03bf = f(x) - f(r) ), ( \u2206 Xi = Xi -\nri ), and r is the reference input.\nFour XAI techniques are applied in this paper to explain the output of the text summarization model. Using different XAI techniques enables a comparison to find the most consistent method for explaining results of text"}, {"title": "An Interactive Javascript Visualization tool", "content": "An interactive visualization tool is developed to improve the interpretabil-ity of the text summarization models and to offer a visual understand-ing of the link between the input text and the model generated summary. JavaScript-based implementation of this tool helps to visualize sentence-wise attribution scores obtained from several XAI techniques.\nThe visualization tool accepts three main inputs:\n1. Input text sentences, which represents the sentences from the source document and are seperated by commas.\n2. Sentence-wise attribution scores calculated with inseq by the chosen XAI technique and normalized within the range [0, 1].\n3. Model generated Summary\nAfter these inputs are fed, the tool generates a color-coded plot highlight-ing how each input sentence influences the final summary. Every sentence in the input text is given a color gradient shade depending on the attribu-tion score that shows its importance. The tool creates a color-coded plot emphasizing how each input sentence contributes to the resultant summary. Based on the attribution score, every sentence in the input text is given a shade of color gradient that exhibits its significance. The XAI techniques generate inherently distinct attribution score on the scale and magnitude. These scores are normalized between 0 and 1 using the following formula to guarantee a consistent and interpretable visualizing:\nNormalized Score = $\\frac{Score - Min}{Max - Min}$\nThis normalization guarantees different degrees of importance are visually distinguishable and allows clear comparisons between sentences. To Visually illustrate the significance of every sentence the normalized attribution scores are mapped to a color gradient spanning from light blue to dark sky-blue color. Here the darker the color, the higher the attribution weight of that particular sentence is. Users of the tool can view the exact attribution score of any given sentence through the help of an interactive hover feature. By connecting input sentences to their attribution scores, this visualization tool helps to clarify the inner workings of text summarization models."}, {"title": "Disagreement Metrics", "content": "To measure the degree of disagreement among various XAI methods, mul-tiple metrics that assess various aspects of feature importance and ranking were utilized after the explanations are generated (Krishna et al., 2022).\nAgreement Measurement based on Top-k features: In this category of measurement following two metrics were employed to quantify the degree of agreement between two pairs of explanations based on their most important (top- k) features:\n\u2022 Feature Agreement\n\u2022 Rank Agreement\nThe value of k initiates at 2, denoting the top-2 features, and may be ex-tended to any k value, dependent upon the length of the articles under anal-ysis."}, {"title": "1. Feature Agreement", "content": "Feature agreement was the first metric applied in this study, and it referred to the degree to which two explanations agree on their most important features. It calculates the ratio of the number of common features between the two top-k feature sets from both explanations.\nMathematically, it is defined as:\nFeatureAgreement(Ea, Eb, k) = $\\frac{|TF(Ea, k) \u2229 TF(E\u266d, k)|}{k}$\nwhere based on the magnitude of feature importance values, TF(E, k) returns the set of top-k features of explanation E based on the magni-tude of feature importance values. Ea and Eb are explanations of two methods a and b."}, {"title": "2. Rank Agreement", "content": "The rank agreement metric compares the sets of top-k features includ-ing their relative ordering in addition to their intersection of common features. It offers a more rigorous measure than simple feature agree-ment because it includes the positional ordering of features, which can greatly influence the interpretation of model explanations. The metric computes the fraction of features that are both present in the top-k feature sets of both explanations and share the same rank inside those sets considering two explanations Ea and Es. It can be computed using the formula below: RankAgreement(Ea, Eb, k) =\n$\\frac{|{s | s \u2208 Ta \u2229 To \u2227 rank(Ea, s) = rank(Eb, s)}|}{k}$\nHere,\nTa = top_features(Ea, k) and T\u2081 = top_features(Eb, k).\nS represents the entire set of features, top_features(E, k) is the set of top-k features in explanation E, and rank(E, s) returns the rank or position of the feature s within the explanation E."}, {"title": "Agreement Measurement based on Relative ordering of features", "content": "Following two measures rely on the relative feature ordering. Two explanations are considered to be different if the relative ordering of the features of interest varies between them."}, {"title": "3.4.1. Rank Correlation", "content": "To quantify the agreement between the relative ordering of features in two explanations, Ea and Et Spearman's rank correlation coefficient is used. This metric quantifies the degree of correlation between the rankings of a selected set of features. The rank correlation between two explanations RankCorrelation(E_a, E-b, F) can be computed as:\nrs(Ranking(Ea, F), Ranking(E\u266d, F))\nWhere Fis a selected set of features, rs represents Spearman's rank correlation coefficient, and Ranking(E, F) is the rank assigned to the features in F based on explanation E. Lower correlation values indicate a higher level of disagreement in the ordering of features."}, {"title": "3.4.2. Pairwise Rank Agreement", "content": "It quantifies the agreement of the relative ordering of feature pairs across two explanations. This metric shows whether the relative im-portance of every pair of features in one explanation matches with the same pair in another. Specifically it evaluates the proportion of feature pairs for which the relative ranking stays constant across both models. The formula of Pairwise Rank Agreement is given below:\nPRA(Ea, E\u266d, F) = $\\frac{\u03a3i<j 1[Ranka(fi, fj) = Ranks(fi, fj)]}{(F\\choose{2})}$\nWhere F = {f1, f2,...} is the set of selected features, and 1 is an indicator function that returns 1 if the relative ranking of fi and f; is the same in both explanations, and 0 otherwise. The denominator ($F\\choose{2}$) represents the total number of feature pairs. Ranka(fi, fj) rep-resents the relative ranking of features fi and f; in explanation Ea. Rank(fi, fj) represents the relative ranking of the same feature pair in explanation Eb."}, {"title": "Algorithm 2 Regional Explanations and Agreement Calculation", "content": "1: Step 1: Preprocessing and Truncation\n2: for each article in sample_news_articles do\n3:\nApply preprocessing steps to clean text and handle punctuation\n4: Tokenize and truncate article to 1024 tokens using PEGASUS/BART to-kenizer\n5: Store preprocessed article in preprocessed_articles\n6: end for\n7: Step 2: Determine Optimal k for Clustering\n8: for each article in preprocessed_articles do\n9:\nSplit article into sentences and encode using Sentence Transformers\n10: Determine optimal number of clusters k using elbow method and silhouette score\n11: Store optimal k for each article\n12: end for\n13: Compute the average optimal k to ensure uniform clustering across all articles\n14: Step 3: Segment Articles Using K-Means Clustering\n15: for each article in preprocessed_articles do\n16:\nSegment the article using K-means clustering with average k\n17: Ensure no clusters have fewer than two sentences by merging small clusters\n18: Store segmented article in preprocessed_segments\n19: end for\n20: Step 4: XAI Methods and Agreement Calculation (Segment Level)\n21: for each segmented_article in preprocessed_segments do\n22:\nfor each segment in segmented_article do\n23:\nfor each method in methods do\n24:\nApply XAI method to segment\n25:\nCalculate attribution and disagreement scores for the method pair\n26: end for\n27: end for\n28: Aggregate segment-level scores into article-level disagreement scores\n29: end for\n30: Step 5: Compute Average Agreement Across All Articles\n31: Average the article-level disagreement scores across all articles\n32: Compare pairwise agreement between methods and generate heatmap"}, {"title": "4. Results", "content": "The results section is divided into depicting the results of three primary objectives such as Global Agreement Analysis, Analysis of Agreement over text segmentation and providing an interactive tool for visualization of ex-plainable text summarization plot."}, {"title": "4.1. Global Agreement Analysis on News Articles", "content": "Based on top-k features and relative ranking, agreement scores were cal-culated using the predefined metrics stated in the methods section with evalu-ations made across pairs of X\u0391\u0399 methods (e.g., Attention, DeepLIFT, LIME, Gradient_SHAP). The evaluation was conducted depending on relative rank-ing of features as well as top-k features, where k denotes the total count of selected top features. The findings of this study are discussed in detail in the subsequent subsections."}, {"title": "Agreement Analysis of XAI methods on Xsum Dataset", "content": "Following section depict the results obtained on the random sample of 500 test articles from Xsum dataset."}, {"title": "Agreement Measurement based on Top-k features", "content": "To analyze the level of disagreement between XAI methods based on top-k features feature agreement and rank agreement was used. The results are visualized in two ways: a line plot showing the average feature agreement scores across differ-ent values of k (from 2 to 10) and a box plot displaying the distribution of feature agreement scores across method pairs with highlighted k-values."}, {"title": "4.2. Agreement measurement based on Relative Rank of features", "content": "Figure: 6 depicts the heatmaps of relative rank agreements between dif-ferent explainability methods on the XSum dataset. On the left the Pairwise Rank Agreement is shown, which highlights the degree by which rank order-ings of features agree between pairs of methods. The right heatmap presents the Spearman-Rank Correlation, which is a more robust ranking agreement\nthat considers the monotonic relationship between the ranks given by differ-ent methods. From the pairwise rank agreement heatmap shown in figure: 6a it is clearly visible that attention and deeplift methods exhibit the high-est rank agreement (0.75), reflecting that these two methods assign similar feature importance orders. Conversely, lime consistently shows lower agree-ment with other methods, particularly with attention(0.54), which suggests a divergence in how lime ranks the top-k features compared to the other methods. In the Spearman rank correlation heatmap shown in figure: 6b, a similar trend is observed. The attention and deeplift methods have the high-est Spearman correlation score(0.64), which indicates a consistent ranking of feature importance between them. On the other hand, lime has much lower correlations with the all other methods, specifically with attention (0.1) and gradient shap (0.15), indicating that the ranking approach that lime is taking is very different from others.\nThis finding indicates that attention and deeplift tend to agree more in their ranking of important features, while lime tends to deviate from it, which lead to more diverse explanations."}, {"title": "Agreement Analysis of XAI methods on CNN/Dailymail Dataset", "content": "Following section depict the results obtained on CNN/Dailymail dataset us-ing different Disagreement metrics."}, {"title": "Agreement Measurement based on Top-k features", "content": "\u2022 The line plot in Figure: 7a shows a consistent increase for all method\npairs, the agreement increases smoothly as k increases. Among all pairs, attention vs deeplift has the highest feature agreement of 0.6 at k=11, which is lower than the peak agreement of 0.75 for Xsum. Figure: 7b depicts a box plot of feature agreement scores for CNN/DM dataset. The scores have higher variability especially for pairs like attention vs lime. This reflects that due to the increased complexity given the longer documents there is a higher variation in feature agreement scores."}, {"title": "Agreement Measurement based on Relative rankig of features", "content": "\u2022 Figure: 9a shows that the maximum pairwise rank agreement among all the pairs is between attention and gradient shap, which is 0.66, and between attention and deeplift, which is 0.64. This indicates that these methods tend to provide somewhat similar rankings of features. Overall, most of the agreements between different pairs of methods fall in the range between 0.56 and 0.66, reflecting that there is a general level of consistency between methods when considering feature ranking between pairs of features in terms of agreement scores."}, {"title": "4.3. Agreement Analysis based on Segmented Explanations on Xsum and CNN/DMdataset", "content": "Due to the high level of disagreement observed between XAI methods we adapted the approach of segmented explanations. This approach is utilized to segment the text and then generate the explanation; first segment wise and then article wise. This article wise segmentation are finally averaged out and\ncompared with global average agreement scores to see if text segmentation has helped in reduced disagreement or not. Following subsections displays the result of the proposed approach."}, {"title": "Agreement Analysis of Segmented Explanations based on top-k features", "content": "\u2022 Figure 10a shows a consistent increase in the feature agreement scores with the number of top-k features for all pairs of XAI methods for Xsum dataset. Attention vs. Deeplift continues to have the highest score, which is above 0.85 for k = 4, indicating the strong alignment between these two methods after text segmentation. Lime vs. Attention and Lime vs. Deeplift still have the lowest feature agreement, however there is an improvement in scores compared to the global agreement results for all the method pairs of LIME. The highest value of k in text segmentation analysis across both datasets is taken as 4 since input news article is broken into semantically coherent clusters, smaller in size compared to the length of original news article. From the results it is evident that regional explanations seem to increase the feature agreement among all XAI methods. This validates the hypothesis that text segmentation can better capture the nuances of the input and helps in improved agreement across methods."}, {"title": "5. Conclusion", "content": "In this paper, we provide an in-depth study of disagreement among mul-tiple XAI methods on text summarization models. We have analyzed two state-of-the-art news summarization datasets: CNN/DM and XSum, with respect to feature attributions, about how different explainability methods - Attention, DeepLIFT, LIME, and Gradient SHAP - agree upon them. To quantify the degree of disagreement, we consider the following metrics: fea-ture agreement, rank agreement, pairwise rank agreement, and Spearman rank correlation. Our findings indicate a considerable disagreement between XAI methods. Attention and DeepLIFT have always shown the highest agreement in both datasets, whereas LIME has always had a lower agree-ment with other methods. The segmentation approach for both XSum nd CNN/DM datasets partially supported our hypothesis, as the feature agree-ment and rank agreement increased when we divided the articles into seman-tically coherent segments. However, the rank correlation was still low suggest that while methods may agree on important features within localized regions, they still differ in ranking those features.\nFuture Work This analysis can be further extended in the future on other news summarization datasets to investigate how \u03a7\u0391AI methods perform on health, finance, and legal text domains. It would present the behavior of XAI methods on different text genres. Furthermore, it would be interest-ing to study the effect of domain-specific training on the global and regional agreement of XAI methods. A comparison of agreement scores between mod-els trained on domain-specific datasets and those trained on general-purpose datasets will allow researchers to determine whether specialized training af-fects the amount of agreement amongst explainability techniques.\nOne limitation of the current study is in the sample size usage. For this study, in total, 500 test articles were analyzed from both datasets. This choice of sample size was made due to the extensive computational requirement"}]}