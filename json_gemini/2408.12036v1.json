{"title": "Reasoning and Tools for Human-Level Forecasting", "authors": ["Elvis Hsieh", "Preston Fu", "Jonathan Chen"], "abstract": "Language models (LMs) trained on web-scale datasets are largely successful due to their ability to memorize large amounts of training data, even if only present in a few examples. These capabilities are often desirable in evaluation on tasks such as question answering but raise questions about whether these models can exhibit genuine reasoning or succeed only at mimicking patterns from the training data. This distinction is particularly salient in forecasting tasks, where the answer is not present in the training data, and the model must reason to make logical deductions. We present Reasoning and Tools for Forecasting (RTF), a framework of reasoning-and-acting (ReAct) agents that can dynamically retrieve updated information and run numerical simulation with equipped tools. We evaluate our model with questions from competitive forecasting platforms and demonstrate that our method is competitive with and can outperform human predictions. This suggests that LMs, with the right tools, can indeed think and adapt like humans, offering valuable insights for real-world decision-making.", "sections": [{"title": "1 Introduction", "content": "Forecasting is an essential tool today, playing a critical role in government, corporate, and personal decision-making. Weather forecasting provides essential information for agriculture, natural disaster preparedness for governments, and travel plans for individuals. During the COVID-19 pandemic, lockdown policies were largely determined by forecasts, which were required to be sufficiently accurate due to their global impact (Dub\u00e9 et al., 2020).\nForecasting methodologies fall into two main categories (Webby and O'Connor, 1996): statistical and judgmental. Statistical forecasting leverages time-series modeling and excels with abundant data under stable conditions. Conversely, judgmental forecasting, which we refer to simply as \"forecasting,\" typically relies on human expertise, integrat-"}, {"title": "2 Related Work", "content": "Information retrieval Reliable and accurate predictions are largely dependent on the information available to the predictor. This is especially the case of LMs, which are trained on data preceding a knowledge cutoff and have been shown to perform better with information retrieval (Shuster et al., 2021).\nLanguage models model the likelihood"}, {"title": "3 Reasoning and Tools for Forecasting", "content": "Forecasting is a complex task solving environment, for which we would like to leverage a frozen LM $p_{\\theta}$ as reasoning. Successful forecasting agents rely on the most up-to-date information, and accordingly operate as agents that collect observations $o_t \\in \\mathcal{O}$ and take actions $a_t \\in \\mathcal{A}$. The observation space $\\mathcal{O}$ is natural language, as collected from the prompt itself or information on the internet. The agent's actions are distributed according to $a_t \\sim \\pi(a_t|c_t)$, where $c_t = (o_1, a_1, ..., o_{t-1}, a_{t-1})$ is the context to the agent.\nOur proposed approach $\\pi$ satisfies the following criteria:\n(i) It is simple, scalable, and time-invariant. As we consider different datasets of forecasting questions or language models at least as capable as the current state-of-the-art, we would like our approach to work at least as well.\n(ii) It can produce comprehensive responses through zero-shot prompting from factual information, which can be used to reliably support decision-making in downstream scenarios.\n(iii) These responses should be consistent, i.e. they should correctly synthesize the up-to-date information the model collects.\n(Yao et al., 2023) shows that CoT prompting, even with in-context examples, can iteratively hallucinate to produce incorrect responses on complex tasks. CoT satisfies (i) but neither (ii) nor (iii). We find that CoT's lack of interaction with the environment (i.e. sole reliance on its training data) limits its reasoning abilities and over-emphasizes irrelevant information.\n(Yao et al., 2023) proposes ReAct for this setting: $\\mathcal{A} = \\{\\text{search, lookup, finish}\\}$, and observations $o_t$ from search and lookup are collected from $\\mathcal{O} \\coloneqq \\text{Wikipedia web API}$. The context is then augmented a thought $\\hat{a}_t \\sim p_{\\theta}(\\hat{a}_t|c_t)$ that composes information about the existing context. This method has shown to significantly enhance the model's ability to refine its responses continuously, reducing the likelihood of erroneous outputs due to lacking critical context information. Vanilla ReAct satisfies (i); as part of our framework, we show that it can additionally satisfy (ii) and (iii)."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Setup", "content": "Models and data (Jin et al., 2021; Zou et al., 2022b) have proposed forecasting benchmarks to assess models' forecasting abilities, simulating forecasting by leveraging that models are only trained up to a cutoff date. However, these benchmarks, consisting of questions that resolved in 2022, are now outdated for evaluating the performance of models such as GPT-4$\\theta$ due to answer leakage in training data (knowledge cutoff October 2023; see Appendix A.1).\nWe curated the dataset on April 15, 2024, when we scraped the platform for questions resolving within the next two weeks and corresponding human crowd predictions. We then filtered out vague questions, and ran every prediction method on these questions, enabling a fair comparison between each method and the human crowd. To prevent answer leakage from the Google API, we set the search range to prior to this date.\nNone of our baselines have direct access to prediction market data, and empirically we found that this information was never scraped via Google search. That is, the prediction given by the ensemble of agents relies on only the agents themselves, with no human crowd influence. (By contrast, if deployed in the real world, this approach could benefit from incorporating the current human crowd performance as an input to the prediction due to the wisdom-of-crowds effect. Indeed, we observe in our experiments that human crowds are fairly well-calibrated.)"}, {"title": "4.2 Results and Observations", "content": "Table 1 demonstrates that RTF significantly improves over CoT and scratchpad with fine-tuning. We also achieve comparable Brier score (0.169 vs. 0.172) and superior accuracy (73.9% vs. 73.8%) compared to human predictors using the median and mean of our ensemble, respectively.\nThe optimal strategy to minimize Brier scores is to forecast $f_i = P(o_i = 1)$, so this scoring metric is unbiased. It is typical to compare Brier scores to 0.25, which can be achieved by $f_i = 0.5$ for all $i$.\nWe also demonstrate that ensembles for RTF yield better performance than individual agents (Brier 0.169 vs. 0.180). However, this is not the case for base LMs (Brier 0.218 vs. 0.21$\\theta$ for GPT-4$\\theta$). Base LMs tend to produce higher-variance outputs (standard deviation in ensemble size 4 of 0.150) compared to our better-calibrated ReAct agents (standard deviation in ensemble size 3 of 0.092), which satisfied (iii) as defined in Section 3. Ensembles only contribute to the final performance if each ensemble member is already sufficiently calibrated. Indeed, Brier scores given by randomly sampling our ReAct ensemble outputs, \"React Sampled\" in the table, achieved a score of 0.180, far better than was achieved by any of the base methods (which, aside from GPT-4$\\theta$, perform worse than guessing 0.5 every time by Brier score).\nAblation study To demonstrate the effectiveness of our introduced components, we conduct the ablation study. We showed each component is necessary for the fully functioning RTF framework.\n\\bullet ReAct: RTF itself without adequate guidance from ReAct struggles to properly use the tools provided by our low-level agents, which leads to misguided lines of reasoning that cascade downstream. This is consistent with the observation (B) in (Yao et al., 2023), where groundedness and trustworthiness come at the cost of higher reasoning error rates.\n\\bullet Hierarchical Planning: Empirically, without the cooperation of high- and low-level agents, a single agent fails to call APIs and perform necessary reasoning, as it exhausted available tokens on API schemas. In our experiments, the single-agent approach frequently encountered time-out errors or exceeded rate limits when handling complex queries.\nQualitative analysis While the baselines systematically evaluate multiple considerations, they do not consider interactions between these considerations. Empirically, we find in our samples that the prompting style we present is useful in generating a wide variety of arguments and providing reasonable estimates for how to weight each of those arguments. On the other hand, we see that this same prompt GPT-4$\\theta$ directly does this calibration in a sequential manner to update its final estimate, which may result in over- or under-estimate based on the recency of its considerations. In general, we find that RTF yield human-like reasoning trajectories, showing the robustness of interactive decision making, supporting goal (ii) from Section 3 (see Appendix E).\nCalibration index In Table 2, we evaluate our methods by calibration index, which compares, for each bin of forecast probabilities, the model probabilities to the outcomes (see Appendix B.1). Comparing GPT-4$\\theta$ and React Mean, we see a significant decrease in calibration index (0.0194 vs. 0.0129), which shows that ensembling with ReAct not only increases forecasting accuracy, but also more accurately measures the specific magnitudes with which events occur."}, {"title": "5 Conclusion", "content": "We present Reasoning and Tools for Forecasting, a framework to leverage LMs' reasoning capabilities by interacting with the latest information. It is competitive with the predictive capabilities of human forecasters on forecasting platforms. The RTF synthesizes information through a structured decision-making process, ensuring that the predictions are both current and relevant. Additionally, while previous work has shown that ensembling can improve prediction accuracy, a carefully calibrated smaller set of models is often more cost-effective than larger ensembles.\nBy advancing LMs' abilities to reason and dynamically interact with new data, RTF offers a robust tool for real-world decision-making for tasks like forecasting.\nLimitations The evaluation dataset is based on prediction market data and popular questions rather than domain-specific questions. This facilitates a comparison with crowd prediction performance, but may not fully capture the nuances of more specialized domains. In addition, our work focused on predicting binary rather than numerical outcomes."}, {"title": "A.1 Models", "content": "We list the details of models we evaluated in Table 3, where the cutoffs have been retrieved from the model cards. For GPT models, we run them via the OpenAI API. We host Llama-3-7B on a single NVIDIA TITAN RTX 24GB via Ollama for roughly 0.5 GPU-hours. All other approaches are run through the OpenAI API, for roughly 1 hour per naive baseline, 6 hours for our reproduction of (Halawi et al., 2024), and 2.5 hours for our proposed method. For GPT models, we use temperature 0.5 for all the experiments.\n(Halawi et al., 2024) finds that GPT-3.5 and GPT-4 do not have leakage due to post-training. We find that the same is true of GPT-4$\\theta$ and Llama-3-7B: prompting with \u201cAnswer this question without searching the web: Who was appointed to the Governor-General of Australia in 2024?\" yielded a statement about its cutoff date, whereas the correct answer was given when prompted for the answer in 2019."}, {"title": "A.2 Crowd Predictions", "content": "On Manifold Markets, players make bets on the outcome of various events where the prices of bets are determined by a current aggregate of crowd predictions, which are prices in [0, 1]. As bets are made, the prices are adjusted by their automated market-makers (Markets, 2022). As shown in (Metaculus, 2023), the crowd prediction is a strong baseline and consistently outperform top forecaster in the prediction market."}, {"title": "B Evaluations", "content": ""}, {"title": "B.1 Calibration Index", "content": "The calibration index is a statistical measure used to assess the reliability of a forecasting system; specifically it quantifies the extent to which observed outcomes correspond to forecast probabilities. A well-calibrated model means that if a forecast predicts an event with a certain probability, the event should occur approximately that fraction of the time over many predictions.\nWe calculate the calibration index as\n$\\text{CI} = \\frac{1}{N} \\sum_{k=1}^{K} N_k (f_k - o_k)^2,$\nwhere N is the total number of forecasts, $N_k$ is the number of forecasts in bin k, $f_k$ is the mean forecast probability in bin k, and $o_k$ is the observed probability with which events occur in bin k. We select bins as the K-quantiles of the forecasts."}, {"title": "C Dataset", "content": ""}, {"title": "C.1 Questions", "content": "Our final dataset consisted of 201 questions from Manifold Markets. These question were all resolved after April 15, 2024, which was the knowledge cutoff date for our low-level agent supporting the Google Search API. We include a subset of the dataset for reference."}, {"title": "C.2 Knowledge Evaluation by Category", "content": "We show the diversity of our dataset in Table 5, with categories determined by GPT-3.5. Due to the popularity of Economics & Business and Politics & Governance questions on the forecasting platform, we have higher proportions of data in those categories."}, {"title": "E.2 RTF Output", "content": "[Entering new AgentExecutor chain...]\nTo forecast the probability that ETH (Ethereum) will close\nabove $3700 on April 30, 2024, I'll follow a structured\napproach:\n1. **Reference Classes and Base Rates**:\nHistorical price trends of Ethereum.\nHistorical performance of similar cryptocurrencies.\nGeneral market conditions and trends in the\ncryptocurrency market.\n2. **Recent News and Distinctive Features**:\nRecent developments in Ethereum technology.\nRegulatory news affecting cryptocurrencies.\nBroader economic conditions impacting the cryptocurrency\nmarket.\n3. **Statistical Modeling**:\nUse historical price data to model potential future price\nmovements.\nConsider volatility and distribution patterns typical for\nEthereum.\n### Step-by-Step Analysis\n#### 1. Reference Classes and Base Rates\nFirst, let's look at the historical price data of Ethereum. We\nwant to understand how often Ethereum has closed above\n$3700 in the past and under what conditions it has done\nso.\nThought: Do I need to use a tool? Yes\nAction: Google Search Snippets\nAction Input: \"historical price data of Ethereum\"\nTo forecast the probability that ETH (Ethereum) will close\nabove $3700 on April 30, 2024, I'll follow a structured\napproach:\n1. **Reference Classes and Base Rates**:\nHistorical price trends of Ethereum.\nHistorical performance of similar cryptocurrencies.\nGeneral market conditions and trends in the\ncryptocurrency market.\n2. **Recent News and Distinctive Features**:\nRecent developments in Ethereum technology.\nRegulatory news affecting cryptocurrencies.\nBroader economic conditions impacting the cryptocurrency\nmarket.\n3. **Statistical Modeling**:\nUse historical price data to model potential future price\nmovements.\nConsider volatility and distribution patterns typical for\nEthereum.\n### Step-by-Step Analysis\n#### 1. Reference Classes and Base Rates\nFirst, let's look at the historical price data of Ethereum. We\nwant to understand how often Ethereum has closed above\n$3700 in the past and under what conditions it has done\nso.\nThought: Do I need to use a tool? Yes\nAction: Google Search Snippets\nAction Input: \"historical price data of Ethereum\"\nEthereum Price History. Daily. Weekly. Monthly. 3/28\nHistorical Data. Frequently Asked Questions (FAQS)\nWhat Does Ethereum's Market Cap History Reveal About...\nJan 8, 2023 Price history of ETH: On August 7, 2015,\nthe first Ethereum transaction was carried out using\nthe transaction hash of block 46147. Since its...\nEthereum's (ETH) highest price in late 2021 was much\nhigher than any other price seen in a chart that looks\nat the previous 5 years as well as 2023. Track the\nlatest Ethereum price, market cap, trading volume, news\nand more with CoinGecko's live ETH price chart and\npopular... Features real-time (live) charts, ethereum\nblockchain, news and videos. Learn about ETH, the\ncurrent hashrate, crypto trading and more. Dec 18, 2016\nThere is no canonical ETH or BTC price, each\nexchange has its own. Exchange may or may not collect\nhistorical data, and granularity of such data... Sep 1,\n2017\nEthereum price history API? I am working on a"}, {"title": "E.3 GPT-40 Output", "content": "To forecast whether ETH (Ethereum) will close above $3700 by\nApril 30, 2024, we need to consider several factors,\nincluding historical price data, market trends, and\ncurrent events that could influence the price. Let's\nbreak down the analysis step-by-step.\n### Step 1: Historical Data and Base Rates\nFirst, we need to look at the historical price data for ETH to\nunderstand its past performance and volatility. We can\nuse this data to establish a base rate for how often ETH\nhas closed above $3700 in the past.\n#### Historical Price Data\nETH has been highly volatile, with significant price swings\nover the past few years.\nETH reached an all-time high of around $4800 in November\n2021.\nSince then, the price has fluctuated, with periods of both\nsignificant gains and losses. Let's assume we have the\nfollowing historical data for ETH closing prices over\nthe past few years: 2021: ETH closed above $3700 on\n50 days.\n2022: ETH closed above $3700 on 20 days.\n2023 (up to October): ETH closed above $3700 on 10 days.\nGiven this data, we can calculate the base rate for ETH\nclosing above $3700.\n#### Base Rate Calculation Total number of days in the dataset:\n365 (2021) + 365 (2022) + 273 (2023 up to October) =\n1003 days Total number of days ETH closed above $3700:\n50 (2021) + 20 (2022) + 10 (2023) = 80 days Base rate =\n80/1003 approximate to 0.08 (8%)\n### Step 2: Current Market Trends and News\nNext, we need to consider the current market trends and any\nrecent news that could impact the price of ETH.\n#### Market Trends\nThe cryptocurrency market has been experiencing increased\ninstitutional interest and adoption.\nRegulatory developments could either positively or\nnegatively impact the market.\nMacroeconomic factors, such as inflation and interest rates,\ncould influence investor sentiment. #### Recent News\nEthereum's transition to a proof-of-stake (PoS)\nconsensus mechanism (Ethereum 2.0) could have a\nsignificant impact on its price.\nAny major security breaches or technological advancements\ncould also affect the price. ### Step 3: Adjusting the\nBase Rate Based on the current market trends and recent\nnews, we need to adjust our base rate. #### Positive\nFactors\nIncreased institutional interest and adoption could drive\nthe price higher.\nThe successful implementation of Ethereum 2.0 could boost\ninvestor confidence.\n#### Negative Factors\nRegulatory crackdowns could negatively impact the price.\nMacroeconomic factors such as rising interest rates could\nlead to reduced investment in riskier assets like\ncryptocurrencies.\n### Step 4: Combining Estimates\nWe can combine our base rate with adjustments for current\ntrends and news to arrive at a final probability\nestimate.\n#### Adjusted Probability\nBase rate: 8%\nPositive adjustment: +5% (for increased adoption and\nEthereum 2.0)\nNegative adjustment: -3% (for potential regulatory and\nmacroeconomic risks)\nFinal adjusted probability = 8% + 5% - 3% = 10%\n### Step 5: Final Probability Estimate\nAfter considering all factors, my best prediction for the\nprobability of ETH closing above $3700 by April 30, 2024,\nis: **0.10 (10%)**\nThis estimate balances the historical base rate with current\nmarket trends and potential risks, aiming to minimize\nthe Brier score by being well-calibrated and resolved."}, {"title": "D Forecasting Principles Guidance", "content": "Our system prompt requested that the output satisfy elementary forecasting principles. Due to its length, the full prompt will be released along with the codebase and dataset. The forecasting principles are as follows:\n\u2022 You may find relevant time series data online. If the data is recent, you can use zeroth-order or first-order approximations (but for longer time horizons, recall that these may break down due to saturation effects).\n\u2022 Don't ignore base rates for low-probability events that haven't happened, but rather attempt to estimate those probabilities to inform your estimate.\n\u2022 Attempt to approximate the same quantity in multiple different ways. You are welcome to split your reasoning into multiple sections, where in each section you describe how you reached an estimate independently. If your estimates in these sections are very far off, reason through why the estimates are so different, potentially update those estimates, and theorize how confident you are in each estimate. Then, combine those forecasts using a weighted or trimmed mean or median.\n\u2022 Some data can be modeled well using a common distribution, e.g., normal, log-normal, power law, or Poisson distribution (but this is by no means a guarantee). If there's a good reason to believe this is the case, you should use information about those distributions."}, {"title": "E Prompts and Workflows", "content": "Due to length, we have only included one full sample below. In general, we observe that RTF yields more robust CoT-style outputs, whereas base LMs use more linear reasoning that may result in outputs unnecessarily biased towards specific considerations."}, {"title": "E.1 Prompt", "content": "Will ETH close above $3700?\nBackground: None\nResolution criteria: None\nClosure time: April 30, 2024\nFor reference, this prompt resolved as NO (0)."}]}