{"title": "Prompt Selection Matters: Enhancing Text Annotations for Social Sciences with Large Language Models", "authors": ["Louis Abraham", "Charles Arnal", "Antoine Marie"], "abstract": "Large Language Models have recently been applied to text annotation tasks from social sciences, equalling or surpassing the performance of human workers at a fraction of the cost. However, no inquiry has yet been made on the impact of prompt selection on labelling accuracy. In this study, we show that performance greatly varies between prompts, and we apply the method of automatic prompt optimization to systematically craft high quality prompts. We also provide the community with a simple, browser-based implementation of the method at https://prompt-ultra.github.io/.", "sections": [{"title": "Introduction", "content": "Throughout the social sciences, many research questions are answered through annotation and classification of large volumes of text, such as tweets or Facebook comments. Researchers may be interested in knowing, for instance, how politically slanted (liberal vs. conservative) a claim or headline is; how emotional or hostile its tone is, or which one of the basic emotions it reflects (sadness, joy, anger, etc) [BLHJ16, GL12, SM10, RBOP24]. Text annotation so far had to be performed either by human experts or by unskilled crowd workers, depending on the nature of the task. As a result, it was typically costly and time-consuming; crowd workers are also likely to mislabel the data.\n\nThe recent progress of Large Language Models (LLMs) has opened new avenues and could rev-\nolutionize text mining by allowing huge volumes of text data to be analyzed in an unsupervised\nway in a matter of minutes [HCvH24, GAK23, T\u00f6r23, WR23]. Early results suggest that LLMs\ncan perform extremely well, extremely fast and at almost no financial cost, with levels of accuracy\nrivalling those of experts and superior to those of unskilled workers. Moreover, these results are\nachieved by off-the-shelf, general purpose models, that do not need any specialized training, unlike\nsome pre-existing machine learning-based techniques.\n\nNonetheless, crucial aspects of the automatic annotation of text data using LLMs have not been\nstudied yet. In particular, earlier studies did not consider the importance of prompt choice: they used\nsimple hand-crafted prompts, such as \u201cDoes the following message express liberal or conservative\nviews?\u201d or \u201cIs this tweet pro-life or pro-choice?\u201d, to annotate their corpus. However, it has been\nobserved outside the context of text annotation that applying distinct yet similar versions of a given\nprompt to certain tasks can result in large differences in accuracy, of the order of more than 10%\n[KGR+22, BMR+20]. Going from 10% to 20% of mislabelled data can greatly impact the quality of\na study's conclusions, especially if the classification errors are biased (e.g., if all mislabelled tweets\nexpress conservative views)."}, {"title": "Automatic text labelling using LLMs", "content": "In this section, we present the recently proposed method of automatic text labelling using LLMs, and\ndiscuss some of its shortcomings.\n\nState-of-the-art general purpose language models, such as GPT-4 [Ope23], have now achieved human-\nlike performance on a variety of tasks. In particular, it has been recently demonstrated that they can\nbe applied off-the-shelf, i.e., without needing any specialized training, to a variety of text-labelling\ntasks needed in social sciences. Among other examples, they have been shown to reach around 95%\nof accuracy\u00b9 on negativity detection in tweets and news articles [HCvH24], around 75% of accuracy\non stance detection in tweets (largely outperforming untrained crowd workers) [GAK23], and more\nthan 92% of accuracy on political orientation in tweets [T\u00f6r23].\n\nUp until one or two years ago, similar performances were either unachievable, or only achievable\nby models specifically trained on the task at hand, the creation of which was complex and time-\nconsuming [JD16, NR13, AAS+20, PP11, DMB23].\n\nLet us give a quick explanation of how to automatically label text using LLMs. Though variants are\npossible, the simplest method is to simply ask in simple and clear terms an LLM-based chatbot, such\nas ChatGPT [LHM+23], to give a label to a given piece of text. For instance:\n[Social scientist]\nDoes the following tweet express liberal or conservative opinions?\nOutput only \"liberal\" or \"conservative\" without quotes.\n\nPlease support our Capitol Police and Law Enforcement.\nThey are truly on the side of our Country. Stay peaceful!\n\n[ChatGPT] conservative\n\nIt is useful to note in passing that while it is desirable to maximise accuracy at classification tasks, it is\ngenerally impossible to reach 100% in virtue of the fact that some statements are intrinsically ambiguous and\nthus impossible to classify even by professional raters."}, {"title": "Limitations of automatic text labelling", "content": "As for any method, automatic text annotation using LLMs suffers from some limitations, some of\nwhich have not been extensively discussed in the literature. Though early studies are very encouraging,\nthe range of tasks that have been studied so far remains rather limited; the vast majority of them\nconsists in attributing one of a handful of labels to short- to medium-length texts (most of which being\nsocial media posts or news headlines). Moreover, some preliminary results find that accuracy tends to\nbe lower in languages other than English [HCvH24], which is coherent with general observations\nregarding LLMs' performances. LLMs are also known to suffer from certain biases, including\npolitical ones [LJW+22, MPNR24, ZLS+24]; those could result in systematic biases on certain tasks\nthat would have a greater impact on the research performed on the annotated dataset than the raw\naccuracy might suggest. As an example, consider an LLM tasked with labelling a set of tweets as\nRepublican-leaning or Democrat-leaning ; if all mislabelled tweets are Republican tweets erroneously\nlabeled as Democrat, the conclusions drawn from the dataset might be wrong, despite the overall\naccuracy being high.\n\nThe issue can also be exacerbated by conscious moderating efforts by the LLM's creators: various\ntechniques are in place to ensure that the answers provided by LLM-based chat agents avoid using\nstereotypes susceptible to be seen as offensive, which could negatively affect their performance on\nsome politically sensitive tasks, such as race.\n\nOther problems arise not from the intrinsic nature of LLMs, but rather from the way in which they\nare currently being trained and deployed. State-of-the-art LLMs, such as GPT-4, are trained on\nimmense datasets of text scrapped from the web, and regularly updated in a similar fashion. Beyond\nthe possible privacy issues, this has crucial implications regarding the performance of LLMs: they\nwill often perform better on data on which they have been trained than on yet unseen data (e.g., data\nthat was produced after their training). As a result, the same LLM might be much better at classifying\na certain set of tweets than a similar yet more recent set which it has not yet \u201cseen\u201d. As the exact\nnature of the training data is typically not shared with the general public, such phenomena are very\nhard to predict - see some of our observations in Section 4. In particular, we suspect that some\nof the most impressive accuracy scores achieved by LLMs on annotation tasks and reported in the\nliterature might be partially explained by this. This problem, in conjunction with the regular updating\nof models, creates important issues of replicability: results might vary greatly between apparently\nsimilar tasks, or when applying different versions of the same model to the same task. This can be\npartially offset by using \u201cfrozen\u201d LLMs, i.e. by keeping a copy of a certain version of an LLM at a\ncertain point in time and using it on all tasks."}, {"title": "Experiments", "content": "We evaluate various hand-crafted prompts and automatically optimized prompts (see Subsections 3.2\nand 3.3 below) on a range of datasets and classical social sciences tasks using OpenAI's GPT-3.5\nTurbo's API [YCX+23] (see our code base for details). Our objective is twofold: first, we want to\nmeasure the impact of the precise formulation of the prompt on GPT's accuracy by comparing perfor-\nmance across various prompts. The goal here is not to find some optimal prompt crafting technique\nthat would systematically outperform all others (such panacea is unlikely to exist, as suggested by our\nexperimental results further below); in particular, we do not claim to have examined every possible\nprompt-crafting trick. Rather, we only want to see whether two reasonable, semantically similar\nprompts can result in significantly different accuracies on a given task. Second, we want to check\nwhether automatic prompt optimization can help achieve consistently good (though not necessarily\noptimal) results on all tasks without the need for manual tweaking by the experimenter."}, {"title": "Datasets and tasks", "content": "We have selected a range of diverse yet typical annotation tasks and datasets. When the sets have a\npredefined train and test sets split, we only use the test set for reasons explained in Section 4.\n\nTweetEval - hate, emotion, sentiment, offensive (TE-hate, TE-emotion, TE-sent, TE-off)\nTweetEval [BCCEAN20] consists of seven heterogeneous tasks performed on Twitter data in English,\nof which we have selected four:\n\n\u2022 Hate detection (hateful or non-hateful); the set contains 2'970 tweets [BBF+19].\n\n\u2022 Emotion recognition (anger, joy, optimism or sadness); the set contains 1'421 tweets\n[MBMSK18].\n\n\u2022 Sentiment recognition (negative, neutral, positive); the set contains 12\u2032284 tweets, and we\nrandomly draw and use 10'000 of them [RFN17].\n\n\u2022 Offensive language detection (non-offensive, offensive); the set contains 860 tweets\n[ZMN+19].\n\nTweet Sentiment Multilingual (TML-sent)\nTweet Sentiment Multilingual [BEACC22] is a dataset of tweets in 8 different languages (Arabic,\nEnglish, French, German, Hindi, Italian, Portuguese, Spanish) with sentiment analysis labels (negative,\nneutral and positive). The test set contains 6'960 messages.\n\nArticle Bias Prediction (AS-pol)\nA dataset of 37554 news articles from major US newspapers [BDSMGN20], with labels representing\ntheir political inclination (left, center\u00b3 or right), from which we randomly sample 10'000 articles.\n\nLiberals vs Conservatives on Reddit (LibCon)\nA dataset of 13'000 Reddit posts in English collected from liberal- and conservative-leaning subreddits\n[lib]. We randomly select and use 10'000 of them."}, {"title": "Hand-crafted prompts", "content": "It has been observed outside the context of automatic text annotation that various tricks can help\nincrease LLMs' performance. As an example, simply adding \"Let's think step by step\" at the end of\na prompt can result in impressive improvements [KGR+22]. Though there is no systematic theory of\n\"good prompting\", many such tricks have been catalogued (see e.g., [pro]). We have applied several\nof these standard techniques to craft five different prompts for each task; below are examples from\nthe TweetEval - hate task. The other prompts can be found in the accompanying code.\n\n\u2022 Simple - A simple, minimalist prompt:\n\nClassify the following message as hateful if it contains hate speech or non-hateful if it does\nnot contain hate speech. Output only \u201chateful\u201d or \u201cnon-hateful\u201d without quotes."}, {"title": "Automatic prompt optimization (APO)", "content": "A more systematic way to craft good prompts has been recently proposed: automatic prompt\noptimization [ZMH+22, YWL+24, SDZ23] (also called automatic prompt engineering).\n\nThe core idea is to ask an LLM to repeatedly rephrase prompts, then to select the one that offers the\nbest performance. We translated this general concept into our specific text labelling setting as follows:\nwe first require a subset of the dataset to have already been labelled by human annotators; typically, a\nfew hundred or a few thousand messages. We then start from any reasonable prompt for the task,\ne.g., \"Classify the following message as hateful if it contains hate speech or non-hateful if it does\nnot contain hate speech.\" in the case of the hate detection task. We ask the LLM to reformulate the\nprompt several times, e.g. with \u201cGenerate a variation of the following instruction while keeping the\nsemantic meaning.\" We then repeat the following steps as many times as desired: we evaluate the\ncurrent set of prompts on the labelled subset, we keep only the top prompts (in terms of scores), and\nwe ask the LLM to reformulate them to create a new generation of prompts (in addition with the top\nprompts themselves). The best prompt of the last generation is then used to label the remainder of the\ndataset. Before that, one can estimate its associated accuracy by testing it on another pre-labelled\nsubset, distinct from the one used during the optimization process to avoid any bias caused by a lack\nof independence.\n\nIn our experiments and for a given task, each generation had 8 prompts; each was evaluated on a fixed\nsubset of 400 samples from the dataset. The top 2 prompts were kept, and each was reformulated\n3 times to generate a new generation of 2 + 2 \u00d7 3 = 8 prompts. We repeated this process for 15\ngenerations, after which we kept the best performing prompt of the last generation. Finally, we"}, {"title": "Results and discussion", "content": "The accuracy on each task of our five types of prompts and of the best prompt generated using\nautomatic prompt generation are reported in Table 1 We observe that the choice of prompt has a\nsignificant impact on performance for most tasks: depending on the task, the worst prompt results in\nbetween 14% (LibCon) and 47% (TE-off) more errors than the best prompt. This confirms our claim\nthat careful prompt selection is crucial.\n\nWe also note that there is no \u201cmiracle\u201d hand-crafted prompt that consistently outperforms all the\nothers. Some heuristics and specific wordings work well on certain tasks, and poorly on others. This\nis in line with observations made in the literature; as an example, adding \u201cLet's think step by step\u201d at\nthe end of a prompt is shown to outperform adding the semantically similar \u201cLet's work this out in a\nstep by step way to be sure we have the right answer\u201d for the task studied in [YWL+24] (see Table\n1), while the converse is true in [ZMH+22] (Table 7).\n\nWe now turn to the specific performances of Automatic prompt optimization (APO). APO beats\nhand-crafted prompts at all tasks but two; among those two, it is second best for TE-sent (with a\nscore nearly equal to that of the best hand-crafted prompt), and is roughly at the level of the median\nof the hand-crafted prompts for TE-off. This shows that automatically optimizing prompts results in\nconsistently good performance without the need for prior testing of various hand-crafted prompts on\na subset of the dataset of interest.\n\nInterestingly, and in line with the remarks of the previous paragraph, the final prompts generated\nusing APO do not differ much from the ones used to start the optimization process, despite the\noften large gap between their associated accuracies. For TE-hate, for example, the initial prompt\nis \"Classify the following message as hateful if it contains hate speech or non-hateful if it does not\ncontain hate speech. Output only \u201chateful\u201d or \u201cnon-hateful\u201d without quotes\u201d and the automatically\noptimized prompt is \u201cCheck for hate speech in the following message to determine if it is hateful,\nthen classify it as either \"hateful\" or \"non-hateful\"\"."}, {"title": "Is ChatGPT cheating?", "content": "We have observed a curious phenomenon: as shown in table 2, all prompts result in much higher\naccuracy when tested on the training set of the TE-hate dataset than on its test set (where \u201ctraining\u201d\nand \"test\" refer to the split made by the creators of the dataset [BBF+19]). This is a priori surprising,\nas both subsets should be drawn from the same distribution, and as such our prompts should not\nperform any better on the training set than on the test set7. Our explanation is that the training set\nof TE-hate is part of the enormous amount of data on which GPT-3.5 Turbo, the model which we\nused, has been trained; as a result, it performs better on it than on the test set (which was either\nnot included, or included in a different fashion). As the exact data on which the model was trained\nwas not made public, we cannot confirm this hypothesis with absolute certainty. As a result, we\ncan say that ChatGPT is \u201ccheating\u201d when labelling the training set of TE-hate, in the sense that its\nperformance is superior to what it would be on a similar dataset to which it did not have access\nduring its training (e.g. data that was produced after its training). This explains why we used the test\nsplits of our datasets whenever possible, in the hope that as (we assume it to be the case) for TE-hate,\nthey have not been included in the training set of the model, making for a fairer assessment of the\ncapabilities of the method.\n\nThis problem makes it hard to predict future performances based on past experiments, as we have no\nguarantee as to what was and was not used in the models' training. Note for example that the same\ngap in performance between the training set and the test set is not observable for TE-emotion. Ideally,\none should only test the models on data that was produced after their training. This could however\nprove difficult and overly restrictive: curating useful datasets is a challenging and time-consuming\ntask in itself, and most datasets available online are at least a few years old, thereby predating all last\ngeneration LLMs.\n\nAs a side note, this phenomenon could explain why we have observed slightly lower accuracies\non average than those reported in recent literature [HCvH24, GAK23, T\u00f6r23] on similar tasks: we\nsuspect that the authors of these articles might not have been aware of this pitfall, and have taken\nno measures to circumvent it. It could also be due to intrinsic differences in difficulty between the\ndatasets used, as we tried to include both easy and hard datasets for diversity (in particular, it has\nbeen noted before, e.g. in [HCvH24], that performance drops for very long texts, such as the articles\nfrom the AS-pol dataset). We cannot verify this hypothesis, as (most of) the datasets used in the\nreferences that we cite are not available anymore due to a policy change from Twitter."}, {"title": "Conclusion", "content": "This paper meant to illustrate the importance of prompt selection and the effectiveness of automatic\nprompt optimization within the context of text annotation tasks frequently used in the social sciences.\nIn particular, our results suggest that the prompt optimization procedure described in Subsection 3.3\ncan be applied off-the-shelf to various tasks to achieve accuracies that equal or exceed those obtained\nusing hand-crafted prompts. This prompt optimization method is made particularly easy by our user-\nfriendly implementation, which is freely available at https://prompt-ultra.github.io/."}, {"title": "Brief overview of our browser-based service", "content": "We summarily describe our automatic text labelling service https://prompt-ultra.github.\nio/."}, {"title": "Optimized prompts", "content": "For each task, we report the best prompt generated by the prompt optimization process:\nTE-hate \"Check for hate speech in the following message to determine if it is hateful, then classify it\nas either \"hateful\" or \"non-hateful\"\".\"\nTE-emotion \"Identify the emotion displayed in the following message as joy, anger, sadness or\noptimism.\"\nTE-sent \"Label the emotion in the given message as positive, negative, or neutral.\"\nTE-off \"Determine if the following message is offensive or non-offensive and provide the correspond-\ning label.\"\nTML-sent \"Categorize the sentiment in the following message as positive, negative, or neutral.\"\nAS-pol \"Identify if the text below belongs to the left, center, or right categories.\"\nLibCon \"Identify the text as either liberal or conservative.\""}]}