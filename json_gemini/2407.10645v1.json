{"title": "Prompt Selection Matters: Enhancing Text Annotations for Social Sciences with Large Language Models", "authors": ["Louis Abraham", "Charles Arnal", "Antoine Marie"], "abstract": "Large Language Models have recently been applied to text annotation tasks from social sciences, equalling or surpassing the performance of human workers at a fraction of the cost. However, no inquiry has yet been made on the impact of prompt selection on labelling accuracy. In this study, we show that performance greatly varies between prompts, and we apply the method of automatic prompt optimization to systematically craft high quality prompts. We also provide the community with a simple, browser-based implementation of the method at https://prompt-ultra.github.io/.", "sections": [{"title": "1 Introduction", "content": "Throughout the social sciences, many research questions are answered through annotation and classification of large volumes of text, such as tweets or Facebook comments. Researchers may be interested in knowing, for instance, how politically slanted (liberal vs. conservative) a claim or headline is; how emotional or hostile its tone is, or which one of the basic emotions it reflects (sadness, joy, anger, etc) [BLHJ16, GL12, SM10, RBOP24]. Text annotation so far had to be performed either by human experts or by unskilled crowd workers, depending on the nature of the task. As a result, it was typically costly and time-consuming; crowd workers are also likely to mislabel the data.\n\nThe recent progress of Large Language Models (LLMs) has opened new avenues and could rev-olutionize text mining by allowing huge volumes of text data to be analyzed in an unsupervised way in a matter of minutes [HCvH24, GAK23, T\u00f6r23, WR23]. Early results suggest that LLMs can perform extremely well, extremely fast and at almost no financial cost, with levels of accuracy rivalling those of experts and superior to those of unskilled workers. Moreover, these results are achieved by off-the-shelf, general purpose models, that do not need any specialized training, unlike some pre-existing machine learning-based techniques.\n\nNonetheless, crucial aspects of the automatic annotation of text data using LLMs have not been studied yet. In particular, earlier studies did not consider the importance of prompt choice: they used simple hand-crafted prompts, such as \u201cDoes the following message express liberal or conservative views?\u201d or \"Is this tweet pro-life or pro-choice?\u201d, to annotate their corpus. However, it has been observed outside the context of text annotation that applying distinct yet similar versions of a given prompt to certain tasks can result in large differences in accuracy, of the order of more than 10% [KGR+22, BMR+20]. Going from 10% to 20% of mislabelled data can greatly impact the quality of a study's conclusions, especially if the classification errors are biased (e.g., if all mislabelled tweets express conservative views)."}, {"title": "2 Automatic text labelling using LLMs", "content": "In this section, we present the recently proposed method of automatic text labelling using LLMs, and discuss some of its shortcomings.\n\nState-of-the-art general purpose language models, such as GPT-4 [Ope23], have now achieved human-like performance on a variety of tasks. In particular, it has been recently demonstrated that they can be applied off-the-shelf, i.e., without needing any specialized training, to a variety of text-labelling tasks needed in social sciences. Among other examples, they have been shown to reach around 95% of accuracy\u00b9 on negativity detection in tweets and news articles [HCvH24], around 75% of accuracy on stance detection in tweets (largely outperforming untrained crowd workers) [GAK23], and more than 92% of accuracy on political orientation in tweets [T\u00f6r23].\n\nUp until one or two years ago, similar performances were either unachievable, or only achievable by models specifically trained on the task at hand, the creation of which was complex and time-consuming [JD16, NR13, AAS+20, PP11, DMB23].\n\nLet us give a quick explanation of how to automatically label text using LLMs. Though variants are possible, the simplest method is to simply ask in simple and clear terms an LLM-based chatbot, such as ChatGPT [LHM+23], to give a label to a given piece of text. For instance:\n\n[Social scientist]\nDoes the following tweet express liberal or conservative opinions?\nOutput only \"liberal\" or \"conservative\" without quotes.\n\nPlease support our Capitol Police and Law Enforcement.\nThey are truly on the side of our Country. Stay peaceful!\n\n[ChatGPT] conservative\n\nIt is useful to note in passing that while it is desirable to maximise accuracy at classification tasks, it is generally impossible to reach 100% in virtue of the fact that some statements are intrinsically ambiguous and thus impossible to classify even by professional raters."}, {"title": "2.1 Limitations of automatic text labelling", "content": "As for any method, automatic text annotation using LLMs suffers from some limitations, some of which have not been extensively discussed in the literature. Though early studies are very encouraging, the range of tasks that have been studied so far remains rather limited; the vast majority of them consists in attributing one of a handful of labels to short- to medium-length texts (most of which being social media posts or news headlines). Moreover, some preliminary results find that accuracy tends to be lower in languages other than English [HCvH24], which is coherent with general observations regarding LLMs' performances. LLMs are also known to suffer from certain biases, including political ones [LJW+22, MPNR24, ZLS+24]; those could result in systematic biases on certain tasks that would have a greater impact on the research performed on the annotated dataset than the raw accuracy might suggest. As an example, consider an LLM tasked with labelling a set of tweets as Republican-leaning or Democrat-leaning ; if all mislabelled tweets are Republican tweets erroneously labeled as Democrat, the conclusions drawn from the dataset might be wrong, despite the overall accuracy being high.\n\nThe issue can also be exacerbated by conscious moderating efforts by the LLM's creators: various techniques are in place to ensure that the answers provided by LLM-based chat agents avoid using stereotypes susceptible to be seen as offensive, which could negatively affect their performance on some politically sensitive tasks, such as race.\n\nOther problems arise not from the intrinsic nature of LLMs, but rather from the way in which they are currently being trained and deployed. State-of-the-art LLMs, such as GPT-4, are trained on immense datasets of text scrapped from the web, and regularly updated in a similar fashion. Beyond the possible privacy issues, this has crucial implications regarding the performance of LLMs: they will often perform better on data on which they have been trained than on yet unseen data (e.g., data that was produced after their training). As a result, the same LLM might be much better at classifying a certain set of tweets than a similar yet more recent set which it has not yet \"seen\". As the exact nature of the training data is typically not shared with the general public, such phenomena are very hard to predict - see some of our observations in Section 4. In particular, we suspect that some of the most impressive accuracy scores achieved by LLMs on annotation tasks and reported in the literature might be partially explained by this. This problem, in conjunction with the regular updating of models, creates important issues of replicability: results might vary greatly between apparently similar tasks, or when applying different versions of the same model to the same task. This can be partially offset by using \"frozen\u201d LLMs, i.e. by keeping a copy of a certain version of an LLM at a certain point in time and using it on all tasks."}, {"title": "3 Experiments", "content": "We evaluate various hand-crafted prompts and automatically optimized prompts (see Subsections 3.2 and 3.3 below) on a range of datasets and classical social sciences tasks using OpenAI's GPT-3.5 Turbo's API [YCX+23] (see our code base for details). Our objective is twofold: first, we want to measure the impact of the precise formulation of the prompt on GPT's accuracy by comparing performance across various prompts. The goal here is not to find some optimal prompt crafting technique that would systematically outperform all others (such panacea is unlikely to exist, as suggested by our experimental results further below); in particular, we do not claim to have examined every possible prompt-crafting trick. Rather, we only want to see whether two reasonable, semantically similar prompts can result in significantly different accuracies on a given task. Second, we want to check whether automatic prompt optimization can help achieve consistently good (though not necessarily optimal) results on all tasks without the need for manual tweaking by the experimenter."}, {"title": "3.1 Datasets and tasks", "content": "We have selected a range of diverse yet typical annotation tasks and datasets. When the sets have a predefined train and test sets split, we only use the test set for reasons explained in Section 4.\n\nTweetEval - hate, emotion, sentiment, offensive (TE-hate, TE-emotion, TE-sent, TE-off)\nTweetEval [BCCEAN20] consists of seven heterogeneous tasks performed on Twitter data in English, of which we have selected four:\n\n\u2022 Hate detection (hateful or non-hateful); the set contains 2'970 tweets [BBF+19].\n\n\u2022 Emotion recognition (anger, joy, optimism or sadness); the set contains 1'421 tweets [MBMSK18].\n\n\u2022 Sentiment recognition (negative, neutral, positive); the set contains 12\u2032284 tweets, and we randomly draw and use 10'000 of them [RFN17].\n\n\u2022 Offensive language detection (non-offensive, offensive); the set contains 860 tweets [ZMN+19].\n\nTweet Sentiment Multilingual (TML-sent)\nTweet Sentiment Multilingual [BEACC22] is a dataset of tweets in 8 different languages (Arabic, English, French, German, Hindi, Italian, Portuguese, Spanish) with sentiment analysis labels (negative, neutral and positive). The test set contains 6'960 messages.\n\nArticle Bias Prediction (AS-pol)\nA dataset of 37554 news articles from major US newspapers [BDSMGN20], with labels representing their political inclination (left, center\u00b3 or right), from which we randomly sample 10'000 articles.\n\nLiberals vs Conservatives on Reddit (LibCon)\nA dataset of 13'000 Reddit posts in English collected from liberal- and conservative-leaning subreddits [lib]. We randomly select and use 10'000 of them."}, {"title": "3.2 Hand-crafted prompts", "content": "It has been observed outside the context of automatic text annotation that various tricks can help increase LLMs' performance. As an example, simply adding \"Let's think step by step\" at the end of a prompt can result in impressive improvements [KGR+22]. Though there is no systematic theory of \"good prompting\", many such tricks have been catalogued (see e.g., [pro]). We have applied several of these standard techniques to craft five different prompts for each task; below are examples from the TweetEval - hate task. The other prompts can be found in the accompanying code.\n\n\u2022 Simple - A simple, minimalist prompt:\n\nClassify the following message as hateful if it contains hate speech or non-hateful if it does\nnot contain hate speech. Output only \u201chateful\u201d or \u201cnon-hateful\u201d without quotes."}, {"title": "3.3 Automatic prompt optimization (APO)", "content": "A more systematic way to craft good prompts has been recently proposed: automatic prompt optimization [ZMH+22, YWL+24, SDZ23] (also called automatic prompt engineering).\n\nThe core idea is to ask an LLM to repeatedly rephrase prompts, then to select the one that offers the best performance. We translated this general concept into our specific text labelling setting as follows: we first require a subset of the dataset to have already been labelled by human annotators; typically, a few hundred or a few thousand messages. We then start from any reasonable prompt for the task, e.g., \"Classify the following message as hateful if it contains hate speech or non-hateful if it does not contain hate speech.\" in the case of the hate detection task. We ask the LLM to reformulate the prompt several times, e.g. with \u201cGenerate a variation of the following instruction while keeping the semantic meaning.\" We then repeat the following steps as many times as desired: we evaluate the current set of prompts on the labelled subset, we keep only the top prompts (in terms of scores), and we ask the LLM to reformulate them to create a new generation of prompts (in addition with the top prompts themselves). The best prompt of the last generation is then used to label the remainder of the dataset. Before that, one can estimate its associated accuracy by testing it on another pre-labelled subset, distinct from the one used during the optimization process to avoid any bias caused by a lack of independence.\n\nIn our experiments and for a given task, each generation had 8 prompts; each was evaluated on a fixed subset of 400 samples from the dataset. The top 2 prompts were kept, and each was reformulated 3 times to generate a new generation of 2 + 2 \u00d7 3 = 8 prompts. We repeated this process for 15 generations, after which we kept the best performing prompt of the last generation. Finally, we evaluated this prompt on the remainder of the dataset (without the 400 prompts, to guarantee an unbiased estimate of the prompt's accuracy).\n\nThe method is both conceptually simple, and easy to implement; its main a priori downside compared to using hand-crafted prompts is the increased number of calls to the chatbot API required, though the costs remain almost negligible. It can be tested on our free browser-based service, which we further describe in Appendix A."}, {"title": "4 Results and discussion", "content": "Inter-prompts performance variability\nThe accuracy on each task of our five types of prompts and of the best prompt generated using automatic prompt generation are reported in Table 1 We observe that the choice of prompt has a significant impact on performance for most tasks: depending on the task, the worst prompt results in between 14% (LibCon) and 47% (TE-off) more errors than the best prompt. This confirms our claim that careful prompt selection is crucial.\n\nWe also note that there is no \"miracle\" hand-crafted prompt that consistently outperforms all the others. Some heuristics and specific wordings work well on certain tasks, and poorly on others. This is in line with observations made in the literature; as an example, adding \u201cLet's think step by step\u201d at the end of a prompt is shown to outperform adding the semantically similar \u201cLet's work this out in a step by step way to be sure we have the right answer\u201d for the task studied in [YWL+24] (see Table 1), while the converse is true in [ZMH+22] (Table 7).\n\nAutomatic prompt optimization outperforms hand-crafted prompts\nWe now turn to the specific performances of Automatic prompt optimization (APO). APO beats hand-crafted prompts at all tasks but two; among those two, it is second best for TE-sent (with a score nearly equal to that of the best hand-crafted prompt), and is roughly at the level of the median of the hand-crafted prompts for TE-off. This shows that automatically optimizing prompts results in consistently good performance without the need for prior testing of various hand-crafted prompts on a subset of the dataset of interest.\n\nInterestingly, and in line with the remarks of the previous paragraph, the final prompts generated using APO do not differ much from the ones used to start the optimization process, despite the often large gap between their associated accuracies. For TE-hate, for example, the initial prompt is \"Classify the following message as hateful if it contains hate speech or non-hateful if it does not contain hate speech. Output only \u201chateful\u201d or \u201cnon-hateful\u201d without quotes\u201d and the automatically optimized prompt is \u201cCheck for hate speech in the following message to determine if it is hateful, then classify it as either \"hateful\" or \"non-hateful\"\u201d."}, {"title": "Is ChatGPT cheating?", "content": "We have observed a curious phenomenon: as shown in table 2, all prompts result in much higher accuracy when tested on the training set of the TE-hate dataset than on its test set (where \u201ctraining\u201d and \"test\" refer to the split made by the creators of the dataset [BBF+19]). This is a priori surprising, as both subsets should be drawn from the same distribution, and as such our prompts should not perform any better on the training set than on the test set7. Our explanation is that the training set of TE-hate is part of the enormous amount of data on which GPT-3.5 Turbo, the model which we used, has been trained; as a result, it performs better on it than on the test set (which was either not included, or included in a different fashion). As the exact data on which the model was trained was not made public, we cannot confirm this hypothesis with absolute certainty. As a result, we can say that ChatGPT is \u201ccheating\u201d when labelling the training set of TE-hate, in the sense that its performance is superior to what it would be on a similar dataset to which it did not have access during its training (e.g. data that was produced after its training). This explains why we used the test splits of our datasets whenever possible, in the hope that as (we assume it to be the case) for TE-hate, they have not been included in the training set of the model, making for a fairer assessment of the capabilities of the method.\n\nThis problem makes it hard to predict future performances based on past experiments, as we have no guarantee as to what was and was not used in the models' training. Note for example that the same gap in performance between the training set and the test set is not observable for TE-emotion. Ideally, one should only test the models on data that was produced after their training. This could however prove difficult and overly restrictive: curating useful datasets is a challenging and time-consuming task in itself, and most datasets available online are at least a few years old, thereby predating all last generation LLMs.\n\nAs a side note, this phenomenon could explain why we have observed slightly lower accuracies on average than those reported in recent literature [HCvH24, GAK23, T\u00f6r23] on similar tasks: we suspect that the authors of these articles might not have been aware of this pitfall, and have taken no measures to circumvent it. It could also be due to intrinsic differences in difficulty between the datasets used, as we tried to include both easy and hard datasets for diversity (in particular, it has been noted before, e.g. in [HCvH24], that performance drops for very long texts, such as the articles from the AS-pol dataset). We cannot verify this hypothesis, as (most of) the datasets used in the references that we cite are not available anymore due to a policy change from Twitter."}, {"title": "5 Conclusion", "content": "This paper meant to illustrate the importance of prompt selection and the effectiveness of automatic prompt optimization within the context of text annotation tasks frequently used in the social sciences. In particular, our results suggest that the prompt optimization procedure described in Subsection 3.3 can be applied off-the-shelf to various tasks to achieve accuracies that equal or exceed those obtained using hand-crafted prompts. This prompt optimization method is made particularly easy by our user-friendly implementation, which is freely available at https://prompt-ultra.github.io/.\n\nNonetheless, we have also seen that any automatic annotation procedure relying on last generation LLMs raises important questions of replicability due to their training processes (not to mention environmental and privacy-related concerns).\n\nAmong other potential future research directions, it would be particularly interesting to test whether LLMs tasked with labelling a corpus can give robust justifications for their choices \u2013 e.g. explain that they classified a given tweet as left-wing due to the presence of such and such opinions that are typically associated with the Left. Though more sophisticated solutions are conceivable (e.g. using the LLMs' attention mechanisms, see [VSP+17]), directly asking the chatbot \"Can you justify your decision?\" would already make for an interesting experiment. What would also be extremely valuable is to be able to ask the LLMs to associate a (reliable) confidence score to its labels, so that human annotators can review the small percentage of labels the model was unsure of, and increase accuracy."}, {"title": "A Brief overview of our browser-based service", "content": "We summarily describe our automatic text labelling service https://prompt-ultra.github.io/.\nIn the EV\u00c5L tab (see Figure 1), you upload a dataset, which can be labelled or unlabelled, using the topmost button. You then enter a prompt and press the Run button. At that point, a pop-up window requires you to input your ChatGPT access key. Once you have done so, the prompt is applied to each entry of the dataset, and the resulting labelled dataset is output. If the original dataset was labelled, its labels and the predicted labels are compared and the resulting accuracy is computed.\n\nPrompt optimization is performed in the \u00d8PTIM tab (see Figure 2). You first upload a labelled dataset using the topmost button, and input a starting prompt. A macroprompt is then applied to generate successive generations of prompts, with the best prompts of each generation surviving to the next generation. A graph illustrates which prompt descends from which prompt of the previous generation. After a specified number of generations, the prompt whose performance was the best on the labelled dataset is output, as well as its accuracy score.\n\nThe SPL\u00cbT tab (see Figure 3) simply provides a convenient way to split a dataset in two.\n\nFinally, the KLEAR C\u00c4CHE button allows you to clear the LLM's cache, and D.SK\u00d6NNECT to erase the ChatGPT access key that you had entered."}, {"title": "B Optimized prompts", "content": "For each task, we report the best prompt generated by the prompt optimization process:\n\nTE-hate \"Check for hate speech in the following message to determine if it is hateful, then classify it as either \"hateful\" or \"non-hateful\"\u201d.\n\nTE-emotion \"Identify the emotion displayed in the following message as joy, anger, sadness or optimism.\"\n\nTE-sent \"Label the emotion in the given message as positive, negative, or neutral.\"\n\nTE-off \"Determine if the following message is offensive or non-offensive and provide the corresponding label.\"\n\nTML-sent \"Categorize the sentiment in the following message as positive, negative, or neutral.\"\n\nAS-pol \"Identify if the text below belongs to the left, center, or right categories.\"\n\nLibCon \"Identify the text as either liberal or conservative.\u201d"}]}