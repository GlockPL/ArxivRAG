{"title": "ARTIFICIAL NEURAL NETWORKS ON GRADED VECTOR SPACES", "authors": ["T. SHASKA"], "abstract": "We develop new artificial neural network models for graded vector spaces, which are suitable when different features in the data have different significance (weights). This is the first time that such models are designed mathematically and they are expected to perform better than neural networks over usual vector spaces, which are the special case when the gradings are all 1s.", "sections": [{"title": "1. INTRODUCTION", "content": "Artificial neural networks are widely used in artificial intelligence for a variety of problems, including problems that rise from pure mathematics. A neural network model is a function $f: k^n \\rightarrow k^m$, for some feld k and in the majority of cases k = R. Many different architectures and models are used for such networks. The coordinates of v \u2208 kn are called input features and the coordinates of the vector u = f(v) the output features.\nThere are many scenerios when the input features are characterized by different values from some set, say I. For example, if the entries of the data are document and each one has a different significance and could be associated with different values. Consider for example if v = [xo,...,xn] we can assign to any xi some value wt(xi) \u2208 I. Such values are called weights. A vector space in which coordinates of each vector are assigned some other value are known in mathematics as graded vector spaces (cf. Section 4). In this paper we investigate whether one can design neural networks over such graded vector spaces. One can think of many scenarios where neural networks defined over graded vector spaces can make a lot os sense from the applications point of view.\nOur motivation came from studying the weighted projective space WP(2,4,6,10),Q which is the moduli space of genus two curves; see [10], in which case the weights are positive integers. The space of homogenous polynomials graded by their degree is a classical example of such graded vector spaces, when again the grading is done over the set of positive integers.\nIf one intends to carry the theory of neural networks to such graded vector spaces there are some mathematical obstacles that need to be cleared. Are there linear maps between such spaces? How will the activation functions look like? Will such graded or weighted neural networks have any advantages over the classical neural networks?\nThis paper is organized as follows. In Section 2 we give the mathematical back- ground of artificial neural networks. We briefly define group action on sets, invariant and equivariant maps, quotient spaces, group and quotient representations, tensor products, topological groups, and state the Clebsch-Gordan decomposition. While some of these definitions are basic knowledge for mathematicians, they become necessary since this paper is intended to a larger audience of the AI community. Section 2 is a prelude to defining equivariant neural networks what we intend to develop for the analog of such networks over graded vector spaces.\nIn Section 3 we give the basic definitions of equivariant neural networks. We define convolutional neural networks or translation equivariant networks, integral"}, {"title": "2. MATHEMATICAL FOUNDATIONS OF ARTIFICIAL NEURAL NETWORKS", "content": "In this section we establish the notation and give basic definitions of equivariant neural networks. We assume the reader has basic knowledge on the subject on the level of [12], [7] Throughout this paper k denotes a field, An(k) := kn the affine space, and Pn (k) the projective space over k."}, {"title": "2.1. Artificial Neural Networks.", "content": "Let the input vector be x = (x0,...,xm) and the output say some y = (yo,..., Yn). We denote by X the space of in-features and Y the space of out-features. A neuron is a function f: kk such that\n$f(x) = \\Sigma W_ix_i + b,$\nwhere b\u2208k is a constant called bias. We can generalize neurons to tuples of neurons via\n$L:k^n \\rightarrow k^n$\n$x \\rightarrow (f_0(x), ..., f_n(x))$\nThen L is a function given by\n$L(x) = W x + b,$\nwhere W is an n\u00d7n matrix (of weights) with integer entries and b\u2208k", "g": "kn \u2192 kn is called an activation function.\nA network layer is a function\n$k^n \\rightarrow k^n$\n$x\\rightarrow g (W.x + b)$\nfor some g some activation function. A neural network is the composition of many layers. The i-th layer\n... $\\stackrel{L}{\\rightarrow}$ kn kn...\n$X \\rightarrow L_i(x) = g_i (W_ix + b_i),$\nwhere gi, Wi, and bi are the activation, matrix, and bias of the corresponding to this layer.\nAfter m layers the output (predicted values) will be denoted by \u0177 = [\u01771,..., \u0177n]t, where\n$\\hat{y}$ = Lm (Lm-1 (. . . (L\u2081 (x)) . . .)),\nwhile the true values by y = [Y1,..., Yn]t. The composition of all layers is called the model function, say\n$M : X \\rightarrow Y$"}, {"title": "2.2. Symmetries.", "content": "Assume that the input has symmetries. The simplest one could be permuting coordinates, but other not so obvious symmetries could be present as well."}, {"title": "Example 1 (Symmetric polynomials).", "content": "Consider the following: x = (a1,..., An) and y = (yo,...,Yn\u22121) where coordinates of y are coefficients of the polynomial\n$F(x) := \\Pi (x - \\alpha_i)$"}, {"title": "2.3. Groups acting on sets.", "content": "Let X be a set and Ga group. We say that the group G acts on X if there is a function\n\u25ba: Gx X \u2192 X such that (g,x) \u2192 g X\nwhich satisfies the following properties:\ni) e x = x for every x \u2208 X\nii) g (hx) = (gh) x, for every g, h \u2208 G.\nThe set X is called a G-set. When there is no confusion g x is simply denoted by gx. Let Gacts on X and x,y \u2208 X. We say that x and y are G-equivalent if there exists g\u2208 G such that gx = y. If two elements are G-equivalent, we write X ~Gy or x ~ y."}, {"title": "Proposition 1.", "content": "Let X be a G -set. Then, G-equivalent is an equivalence relation in X.\nThe kernel of the action is the set of elements\nker(f) = {g \u2208 G | gx = x, for all x \u2208 X}\nFor x \u2208 X, the stabilizer of x \u2208 G is defined as\nStabg(x) = {g \u2208 G|gx = x}\nsometimes denoted by Gr. The stabilizer Stabg(x) is a subgroup of G."}, {"title": "Lemma 1.", "content": "Let X be a G -set and assume that x ~ y. Then, the stabilizer Stabg(x) is isomorphic to the stabilizer Stabg(y)."}, {"title": "2.4. Invariant and equivariant maps.", "content": "From now on G acts on X via \u25ba: GxX \u2192 X. A function T : X \u2192 Y is called G-invariant if and only if,\nT(g\u25bax) = T(x), \u2200g\u2208G, \u2200x \u2208 X"}, {"title": "2.5. Quotient spaces.", "content": "The set of orbits (left) of G acting on X is denoted by\nG\\X := {Orb(x) | x \u2208 X}\nand is called a quotient space. The corresponding quotient map is called the map\n\u03c0: X \u2192 G\\X, x \u2192 Orb(x)\nNotice that in the case of right action, the symbol X/G is used for the quotient space."}, {"title": "2.6. Group representations.", "content": "Let V be a vector space (finite dimension) over a field k. GL(V) the general linear group of V (i.e., group of invertible linear maps L : V \u2192 V). Let Ga locally compact group (i.e., finite groups, compact groups, Lie groups are all locally compact) A linear representation of Gon V is a tuple (p, V) such that\n\u03c1: G \u2192 GL(V)\nis a group homomorphism. V is called the representation space. Sometimes (PG, V) is used. If V = kn then \u2200g \u2208 G, we have p(g) \u2208 GLn(k), so p(g) is ann\u00d7n invertible matrix when a basis in V is chosen.\nAny G-representation (p, V) defines an action\n\u25b7:GXVV,\n(g, v) \u2192 \u03c1(g)v\nConversely, from any linear Gaction \u25b7 : G\u00d7 V \u2192 V we get a representation\nPD: G \u2192 GL(V), g\u2192 Lg\nwhere Lg (v) = g \u25b7 (v), for all v \u2208 V. Hence, there is a one to one correspondence between G-representations on V and (linear) G-group actions on V. Here are some common representations (we will skip details)"}, {"title": "Definition. 2.", "content": "A representation (p, V) is called irreducible representation (irrep) it it has only the two trivial subrepresentations W V and W {0}."}, {"title": "Example 2.", "content": "Of course the fact that (p, V) is irreducible or not depends on the field k. For example, let G = SO(2, R). Its real valued irreducible representation are\n$\\rho^{(R)}_{($) = ( cos(mo) - sin(mo)\n sin(mo)\n cos(mo)\n), MEN$\nHowever, over C"}, {"title": "Definition. 3 (Equivalent (isomorphic) representations).", "content": "Two representations (P1, V\u2081) and (p2, V2) are called equivalent or isomorphic if there exists an iso- morphism\n$L: V_1 \\rightarrow V_2, such that L\u00b0 p\u2081(g) = p2(g) \u25cb L, for all gEG$\nThis is equivalent as matrix representations p\u2081(g) and p2(g) are similar for every gEG."}, {"title": "Definition. 4 (Endomorphisms).", "content": "Intertwines from (p, V) to itself are called en- domorphisms. In other words, an endomorphism is a linear map L: V \u2192 V such that\n$Lop(g) = p(g) 0 L,$\nfor all g \u2208 G. The endomorphism space is denoted by EndG(V) = Homg(V, V)"}, {"title": "Lemma 3 (Schur's lemma).", "content": "Let (p1, V\u2081) and (p2, V2) be G-irreps over k Ror k = C. Then:\n(1) If (p1, V\u2081) and (p2, V2) are not isomorphic, then there is no (non-trivial) intertwiner between them\n(2) If (p1, V\u2081) = (p2, V2) =: (p, V) are identical, any intertwiner is an isomor- phism and"}, {"title": "2.8. Tensor products.", "content": "The tensor product Vk W of two vector spaces V and W over a field k is the k-vector space based on elements vw, and with relations for all k \u2208 C, v \u2208 V, w \u2208 W\n(V1 + V2) & w = v\u2081 \\ w + V2 & W\nv & (W1 + W2) = v \\ W\u2081 + v & W2\n(k \u2022 v) \u2297 w = v \u2297 (k \u2022 w) = k \u00b7 (v & w)\nIf {1,..., Un} is a basis for V and {W1,...,Wm} is a basis for W, then {vi Wj} is a basis for VW.\nLet (p1, V\u2081) and (p2, V2) be two representations of a group G. The tensor product representation (p1 p2, V1 V2) is defined as\n$(p_1 \\bigotimes p_2)(g) (v_1 \\bigotimes v_2) := p_1 (g)(v_1) p_2(g)(v_2)$\nand extended to all vectors in V\u2297W by linearity. It has dimension dim(V\u2081)\u00b7dim(V2).\nIf dim V, dim W < \u221e, then there is a natural isomorphism of vector spaces (preserving G-actions, if defined) from WV to Hom(V, W)."}, {"title": "2.9. Topological groups.", "content": "A topological group is a group which is also a topo- logical space and for which the group operation is continuous. It is called compact if it is so as a topological space.\nA representation of a topological group Gon a finite-dimensional vector space V is a continuous group homomorphism\n\u03c1 : G \u2192 GL(V),\n1\nwith the topology of GL(V) inherited from the space End(V) of linear self-maps. Notice that now, we can naturally replace EgEG with fc dg (see Haar measure, Borel measure, etc)."}, {"title": "3. EQUIVARIANT NEURAL NETWORKS", "content": "Let us see now how to construct some Equivariant Neural Networks. Let X be the space of input features and Y the space of output features. Let M: X \u2192 Y be a model. Usually we want to approximate some target function\nT : X \u2192 Y"}, {"title": "3.1. Equivariant neural networks.", "content": "A feed forward neural network is a sequence\n$X_0 \\stackrel{L_1}{\\longrightarrow} X_1 \\stackrel{L_2}{\\longrightarrow} X_2 ... \\stackrel{L_N}{\\longrightarrow} X_N$\nof parametrization layers Li : Xi\u22121 \u2192 Xi, where Xi is a feature space (vector space) and Li the i-th layer. Constructing equivariant networks typically involves designing each layer to be individually equivariant. Therefore, each feature space Xi has it's own group action:\n\u25bai: G \u00d7 Xi \u2192 Xi, (g,x) \u2192 g\u25baix.\nIn the network the input Xo and output XN actions o and N are determined by learning task, while the intermediate actions are selected by the user.\nFor a layer Li to be invariant it's input and output actions must satisfies the following,\n$L_i(g\\triangleright_{i-1} x) = g_i L_i(x), \\forall g \\in G, x \\in X_{i-1}$"}, {"title": "3.2. Convolution Neural Networks: translation equivariance.", "content": "A Euclidean feature map in d dimensions with c channels is a function F: Rd \u2192 R that as- signs a c-dimensional feature vector F(x) for every point x \u2208 Rd.\nLet E(d,c) be the set of all Euclidean feature maps Rd \u2192 RC. A translation group is called the additive group of the Euclidean space V = Rd. It acts on Rd by shifting (or translation)\n$(R^d, +) \\times R^d \\rightarrow R^d, (t,x) \\rightarrow x + t$\nIt induces an action on E(d,c) via\n$(R^d, +) \\times E(d,c) \\rightarrow E(d,c), F\\rightarrow (t\\triangleright F)(x) = F(x - t)$"}, {"title": "3.3. Integral transforms.", "content": "Let\n$I_k: L^2(R^d, R^{cin}) \\rightarrow L^2(R^d, R^{cout})$\nbe integral transform map that is parametrized by a square integrable two- argument kernel K\n$\\kappa:R^d \\times R^d \\rightarrow R^{cin \\times Cout}, (x,y) \\rightarrow \\kappa(x,y)$\ndefined by\n$I_k(F)(x) := \\int \\kappa(x, y) F(y) dy$\nLet\nK : Rd \u2192 RCin\u00d7Cout, \u2206x \u2192 K(\u2206x)\ndefined by K(Ax) := \u03ba(\u0394,0)."}, {"title": "Theorem 7 (Regular translations intertwiners are convolutions).", "content": "The integral transform Ik is equivariant if and only if the two-argument kernel k satisfies\n$\\kappa(x + t, y + t) = \\kappa(x, y), for any x, y, t \\in R^d$.\nMoreover, the integral trasform reduces to a convolution integral\n$I_k(F)(x) = \\int K(x,y)F(y) dy$"}, {"title": "3.4. Translation equivariant bias summation.", "content": "Let b : Rd \u2192 Rein be a bias field. The bias preserves the number of channels, so c:= Cin = Cout Consider a bias operation\n$B_b : C^2(R^d, R^{c}) \\rightarrow C^2(R^d, R^{c}),$\n$F\\rightarrow F+b$\nbe an unconstrained bias summation that is parametrized by a square integrable bias field b: Rd \u2192 R"}, {"title": "Theorem 8 (Translation equivariant bias summation).", "content": "The bias summation Bb is equivariant if and only if b is constant (i.e., b(x) = b for some b \u2208 RC)."}, {"title": "3.5. Translation equivariant local nonlinearities.", "content": "Let\n$S_o: L^2(R^d, R^{in}) \\rightarrow L^2(R^d, R^{cout}), F \\rightarrow S_o(F)$\ndefined by S.(F)(x) =: \u03c3x(F(x)), where is a spatially dependent localized non- linearity\n$\\sigma: R^d \\times R^{in} \\rightarrow R^{cout}, (x,y) \\rightarrow \\sigma_x(y)$"}, {"title": "Theorem 9 (Translation equivariant local nonlinearities).", "content": "The spatially dependent localized nonlinearity operation S, is translation equivariant if :"}, {"title": "3.6. Translation equivariant local pooling operations.", "content": "Local max pooling is a nonlinear operation that generates a feature field where the value at a point xo \u2208 Rd is determined by taking the maximum feature value across channels within a defined pooling region RoC Rd centered around xo.\n$P: L^2(R^d, R^{C}) \\rightarrow C^2(R^d, R^{C}), F \\rightarrow max F(y)$"}, {"title": "Theorem 10 (Translation equivariance local max pooling).", "content": "The local max pooling operation. P is translation equivariant if and only if g\u00af\u00b9Rx = Rx\u2212g for all x \u2208 Rd, g\u2208 (R, +)."}, {"title": "3.7. Affine group equivariance and steerable Euclidean CNNs.", "content": "Let G < GLd(R) be a given group. Affine groups Aff(G) are semi-direct products of trans- lations t \u2208 (Rd, +) and G, Aff(G) := (Rd, +) \u00d7 G. The affine group Aff(G) acts on Euclidean spaces,\n$Aff(G) \\times R^d \\rightarrow R^2$\n$(t_g,x) \\rightarrow gx+t$\nNotice that\n$((t_h)^{-1},x) \\rightarrow g^{-1}(x - t)$"}, {"title": "4. GRADED VECTOR SPACES", "content": "Here we give the bare minimum of the background graded vector spaces. The interested reader can check details at [3], [7], [4] among other places.\nA graded vector space is a vector space that has the extra structure of a grading or gradation, which is a decomposition of the vector space into a direct sum of vector subspaces, generally indexed by the integers. For the purposes of this paper we will focus on graded vector spaces indexed by integers, but we also give below the definition of such spaces for a general index set I."}, {"title": "4.1. Integer gradation.", "content": "Let N be the set of non-negative integers. An N-graded vector space, often called simply a graded vector space without the prefix N, is a vector space V together with a decomposition into a direct sum of the form\n$V = \\bigoplus V_n$\nNEN\nwhere each Vn is a vector space. For a given n the elements of Vn are then called homogeneous elements of degree n.\nGraded vector spaces are common. For example the set of all polynomials in one or several variables forms a graded vector space, where the homogeneous elements of degree n are exactly the linear combinations of monomials of degree n."}, {"title": "Example 4.", "content": "Let k be a field and consider V(2,3) the space of degree 2 and 3 homoge- nous polynomials in k[x,y]. It is decomposed as V(2,3) = V2 + V3, whwre V2 is the space of binary quadratics and V3 the space of binary cubics. Let u = [f, g] \u2208 V2\u2295V3. Then the scalar multiplication works as\n$\\lambda * u = \\lambda * [f, g] = [\\lambda^2 f, \\lambda^3 g]$\nWe will use this example repeatedly for the rest of the paper."}, {"title": "Example 5 (Moduli space of genus 2 curves).", "content": "Assume char k \u2260 2 and C a genus 2 curve defined over k. Then C has affine equation y\u00b2 = f(x) where f(x) is a degree 6 polynomial. The ismorphism class of C is determined by its invariants J2, J4, 16, J10, which are homogenous polynomials of degree 2, 4, 6, and 10 respec- tively in terms of the coefficients of C. The moduli space of genus 2 curves defined over k is isomorphic to the weighted projective space WP(2,4,6,10),k."}, {"title": "4.2. General gradation.", "content": "The subspaces of a graded vector space need not be indexed by the set of natural numbers, and may be indexed by the elements of any set I. An I-graded vector space V is a vector space together with a decomposition into a direct sum of subspaces indexed by elements i of the set I:\n$V = \\bigoplus V_i$\n\u0395\u0399\nThe case where I is the ring Z/2Z (the elements 0 and 1) is particularly important in physics. A (Z/2Z)-graded vector space is known as a supervector space."}, {"title": "4.3. Graded linear maps.", "content": "For general index sets I, a linear map between two I-graded vector spaces f: V \u2192 W is called a graded linear map if it preserves the grading of homogeneous elements,\n$f(V_i) \\subset W_i, for all i\\in I.$\nA graded linear map is also called a homomorphism (or morphism) of graded vector spaces, or homogeneous linear map.\nWhen I is a commutative monoid (such as N), then one may more generally define linear maps that are homogeneous of any degree i in I by the property\n$f(V_j) \\subset W_{i+j}, for all j\\in I$\nwhere \"+\" denotes the monoid operation. If moreover I satisfies the cancellation property so that it can be embedded into an abelian group A that it generates (for instance the integers if I is the natural numbers), then one may also define linear maps that are homogeneous of degree i in A by the same property (but now \"+\" denotes the group operation in A). Specifically, for i \u2208 I a linear map will be homogeneous of degree -i if\n$f(V_{i+j}) \\subset W_j, for all j\\in I, while f(V_j) = 0 if j \u2212 i \\notin I$\nLet us see a simple example of a graded linear map."}, {"title": "Example 6.", "content": "Consider V(2,3) V2 V3 as in Example 4. Then a linear map L: V(2,3) \u2192 V(2,3) satisfies\n$L([\\lambda*u]) = L([\\lambda^2f, \\lambda^3g]) = [\\lambda^2L(f), \\lambda^3L(g)] = \\lambda* [L(f), L(g)] = \\lambda * L(u)$\nand\n$L ([f, g] \\oplus [f', g']) = L ([f + f', g + g']) = [L(f) + L(f'), L(g) + L(g')]\n= [L(f), L(g)] + [L(f'), L(g')] = L([f, g]) + L([f', g'])"}, {"title": "4.4. Operations over graded vector spaces.", "content": "Some operations on vector spaces can be defined for graded vector spaces as well. For example, given two I-graded vector spaces V and W, their direct sum has underlying vector space VW with gradation\n$(V\\oplus W)_i = V_i \\oplus W_i$\nIf I is a semigroup, then the tensor product of two I-graded vector spaces V and W is another I-graded vector space,\n$(V\\otimes W)_i = \\bigoplus (V_j \\otimes W_k)$\nWe will come back back to tensor products of vectors spaces when more details are needed."}, {"title": "4.5. Inner graded vector spaces.", "content": "Consider now the case when each Vi, is a finite fdimensional inner space and let(,); denote the corresponding inner product. Then we can define an inner product on V as follows. For u = u1 + ... + un and v = v1 + ... + Un we define\n$(u, v) = (u_1, v_2)_1 + ... + (u_n, v_n)_n$\nwhich is the standart product. Then the Euclidean norm is as expected\n$||u|| = \\sqrt{u_1^2 + ... + u_n^2}$\nIf such Vi are not necessary finite dimensional then we have to assume that Vi is a Hilbert space (i.e. a real or complex inner product space that is also a complete metric space with respect to the distance function induced by the inner product). This case of Hilbert spaces is especially important in machine learning and artificial intelligence as pointed out by Thm. 5.\nObviously having a norm on a graded vector space is important for machine learning if we want to define a cost function of some type. The simpler case of Euclidean vector spaces and their norms was considered in [6], [11]."}, {"title": "Example 7.", "content": "Let us continuw with the space V(2,3) from Example 4. We continue with with bases B\u2081 = {x2,xy,y2} and B2 = {x3,x\u00b2y, xy\u00b2, y\u00b3} as in Example 6. Hence, a basis for V(2,3) can be picked as B = {x2,xy, y2, x3, x\u00b2y, xy\u00b2, y3}. Let us see how we could define an inner product in V(2,3). Let u, v \u2208 V(2,3) given by\n$u = a + b = (u_1 x^2 + u_2 \u0445\u0443 + u_3 y^2) + (u_4 x^3 + u_5 x^2y + u_6 xy^2 + u_7 y^3)$\n$v = a' + b' = (v_1 x^2 + v_2 xy + v_3 y^2) + (v_4 x^3 + v_5 x^2y + v_6 xy^2 + v_7 y^3)$\nThen\n$(u, v) = (a + b, a' + b') = (a + a') + (b + b')\n= u\u2081v2 + u2v2 + u3u3 + u4v4 + u5v5 + u6v6 + u7v7\nand the Euclidean norm is defined as expected $||u|| = \\sqrt{u_1^2 + ... u_7^2}.$\nThere are other ways to define a norm on graded spaces. Consider a Lie algebra g. It is called graded if there is a finite family of subspaces V\u2081,..., V, such that g = V\u2081 \u2295 \u2026 \u2295 V, and [Vi, Vj] C Vi+j, where [Vi, Vi] is the Lie bracket. When gis graded we define for t \u2208 R, at: g\u2192g such that\n$A_t(V_1,..., V_n) = (t_v1, t_2v_2, ..., t^rv_r).$"}, {"title": "5. ARTIFICIAL NEURAL NETWORKS OVER GRADED VECTOR SPACES", "content": "Let us now try to design artificial neural networks over graded vector spaces. Let k be a field and for any integer n \u2265 1 denote by Ar (resp. Pr) the affine (resp. projective) space over k. When k is an algebraically closed field, we will drop the subscript. A fixed tuple of positive integers w = (qo,..., qn) is called set of weights. The weight of a \u2208 k will be denoted by wt(a). The set\n$V_w (k) := {(x_1,...,x_n) \\in k^n | wt(x_i) = q_i, i = 1, ..., n}$\nis a graded vector space over k. From now on, when there is no confusion we will simply use Vw for a graded vector space.\nWe follow the analogy with the classical case of artificial neural networks. A neuron on a graded vector space Vw is a function f: Vnk such that\n$a_w (x) = \\sum x_i + b,$\nwhere b\u2208k is a constant called bias. We can generalize neurons to tuples of neurons via\n$ := V_n (k) \\rightarrow V_n (k)$\n$x \\rightarrow g (a_0(x),..., A_n (x))$\nfor any gives set of weights Wo,..., wn. Then is a k-linear function with matrix written as\n$(x) = W x + b,$\nfor some b \u2208 kn+1 and Wann\u00d7n matrix with integer entries."}, {"title": "Remark 1.", "content": "There is a big confusion here when it comes to terminology. The elements wi are called weights in classical neural networks, but these are different from weights of the graded vector space qi. The matrix W is called the matrix of weights since it is the matrix W = [wi,j], but again those weights are not the same as weights qo,..., qn."}, {"title": "5.1. Artificial neural networks on weighted projective spaces.", "content": "Our inten- tion is to build a complete theory of equivariant nueral networks over graded vector spaces and make it possible to design machine learning models to study weighted projective spaces and weighted varieties among other applications. It is unclear how such models would perform, however mathematically there is every reason to believe that computations on weighted projective varieties are more efficient than over classical projective varieties.\nIn [10] we used current techniques of machine learning to study the weighted projective space WP(2,4,6,10) which is the moduli soace of genus two curves. The input features were invariants J2, J4, J6, and J10, which represent a point in a graded space. However, current techniques are designed for classical vector spaces. Hence, while we got some interesting results in [10] we weren't sure what these results represent. In other words, one can't really understand what is happening in a graded space unless the gradation becomes part of the training.\nThis paper is the first attempt in what we envision as a long project of machine learning in graded vector spaces. Most of mathematical details have yet to be worked out and one has to explore graded Lie algebras, graded manifolds and also different gradings. Especially interesting is the case when the ground field is not Ror C, but or a a field of positive characteristic. Such tasks present challenges mathematically and from the implementation point of view and their performance and efficiency are still open questions."}]}