{"title": "ARTIFICIAL NEURAL NETWORKS ON GRADED VECTOR SPACES", "authors": ["T. SHASKA"], "abstract": "We develop new artificial neural network models for graded vector spaces, which are suitable when different features in the data have different significance (weights). This is the first time that such models are designed mathematically and they are expected to perform better than neural networks over usual vector spaces, which are the special case when the gradings are all 1s.", "sections": [{"title": "1. INTRODUCTION", "content": "Artificial neural networks are widely used in artificial intelligence for a variety of problems, including problems that rise from pure mathematics. A neural network model is a function $f: k^n \\rightarrow k^m$, for some feld k and in the majority of cases k = R. Many different architectures and models are used for such networks. The coordinates of $v \\in k^n$ are called input features and the coordinates of the vector $u = f(v)$ the output features.\nThere are many scenerios when the input features are characterized by different values from some set, say I. For example, if the entries of the data are document and each one has a different significance and could be associated with different values. Consider for example if $v = [x_0,...,x_n]$ we can assign to any $x_i$ some value $wt(x_i) \\in I$. Such values are called weights. A vector space in which coordinates of each vector are assigned some other value are known in mathematics as graded vector spaces (cf. Section 4). In this paper we investigate whether one can design neural networks over such graded vector spaces. One can think of many scenarios where neural networks defined over graded vector spaces can make a lot os sense from the applications point of view.\nOur motivation came from studying the weighted projective space $WP_{(2,4,6,10),Q}$ which is the moduli space of genus two curves; see [10], in which case the weights are positive integers. The space of homogenous polynomials graded by their degree is a classical example of such graded vector spaces, when again the grading is done over the set of positive integers.\nIf one intends to carry the theory of neural networks to such graded vector spaces there are some mathematical obstacles that need to be cleared. Are there linear maps between such spaces? How will the activation functions look like? Will such graded or weighted neural networks have any advantages over the classical neural networks?\nThis paper is organized as follows. In Section 2 we give the mathematical back-ground of artificial neural networks. We briefly define group action on sets, invariant and equivariant maps, quotient spaces, group and quotient representations, tensor products, topological groups, and state the Clebsch-Gordan decomposition. While some of these definitions are basic knowledge for mathematicians, they become necessary since this paper is intended to a larger audience of the AI community. Section 2 is a prelude to defining equivariant neural networks what we intend to develop for the analog of such networks over graded vector spaces.\nIn Section 3 we give the basic definitions of equivariant neural networks. We define convolutional neural networks or translation equivariant networks, integral"}, {"title": "2. MATHEMATICAL FOUNDATIONS OF ARTIFICIAL NEURAL NETWORKS", "content": "In this section we establish the notation and give basic definitions of equivariant neural networks. We assume the reader has basic knowledge on the subject on the level of [12], [7] Throughout this paper k denotes a field, $A^n(k) := k^n$ the affine space, and $P^n(k)$ the projective space over k.\nLet the input vector be $x = (x_0,...,x_m)$ and the output say some $y = (y_0,...,y_n)$. We denote by X the space of in-features and Y the space of out-features. A neuron is a function $f: k^k$ such that\n$f(x) = \\Sigma_{i=0}^{n} w_i x_i + b,$\nwhere $b \\in k$ is a constant called bias. We can generalize neurons to tuples of neurons via\n$L:k^n \\rightarrow k^n$\n$x \\rightarrow (f_0(x), ..., f_n(x))$\nThen L is a function given by\n$L(x) = W x + b,$\nwhere W is an $n \\times n$ matrix (of weights) with integer entries and $b \\in k^n$. A non-linear function $g : k^n \\rightarrow k^n$ is called an activation function.\nA network layer is a function\n$k^n \\rightarrow k^n$\n$x\\rightarrow g (W.x + b)$\nfor some g some activation function. A neural network is the composition of many layers. The i-th layer\n$\\xrightarrow{L}$ $k^n \\xrightarrow{} k^n...\n$X \\rightarrow L_i(x) = g_i (W_ix + b_i),$\nwhere $g_i, W_i$, and $b_i$ are the activation, matrix, and bias of the corresponding to this layer.\nAfter m layers the output (predicted values) will be denoted by $\\hat{y} = [\\hat{y}_1,..., \\hat{y}_n]^t$, where\n$\\hat{y} = L_m (L_{m-1} (. . . (L_1(x)) . . .)),$\nwhile the true values by $y = [y_1,..., y_n]^t$. The composition of all layers is called the model function, say\n$M : X \\rightarrow Y$\nAssume that the input has symmetries. The simplest one could be permuting coordinates, but other not so obvious symmetries could be present as well.\nConsider the following: $x = (a_1,..., a_n)$ and $y = (y_0,...,y_{n-1})$ where coordinates of y are coefficients of the polynomial\n$F(x) := \\Pi_{i=1}^{n} (x - a_i)$"}, {"title": "3. EQUIVARIANT NEURAL NETWORKS", "content": "Let us see now how to construct some Equivariant Neural Networks. Let X be the space of input features and Y the space of output features. Let $M: X \\rightarrow Y$ be a model. Usually we want to approximate some target function\n$T : X \\rightarrow Y$\nLet $H_{full}$ denote the space of all models under consideration during the training, we call this the hypothesis space. Assume Gacts on X and Y as\n$G \\times X \\rightarrow X, (g, x) \\rightarrow g \\rightarrow X$ and $G\\times Y \\rightarrow V, (g, y) \\rightarrow gy$\nWe denote by $H_{inv}$ the space of invariant models and by $H_{equiv}$ the space of equivariant models. So we have\n$H_{inv} C H_{equiv} C H_{full}$\nConsider now instead of having a network sending $x \\rightarrow M(x)$ we have one which sends Orb(x) $\\rightarrow M(x)$.\nSo $M\\downarrow$ is an invariant map and $H_{inu}$ can be thought of the space of models $M\\downarrow$.\nA feed forward neural network is a sequence\n$X_0 \\xrightarrow{L_1}X_1 \\xrightarrow{L_2}X_2... \\xrightarrow{L_{N-1}}X_N$\nof parametrization layers $L_i : X_{i-1} \\rightarrow X_i$, where $X_i$ is a feature space (vector space) and $L_i$ the i-th layer. Constructing equivariant networks typically involves designing each layer to be individually equivariant. Therefore, each feature space $X_i$ has it's own group action:\n$\triangleright_i: G \\times X_i \\rightarrow X_i, (g,x) \\rightarrow g\\triangleright_ix.$\nIn the network the input $X_0$ and output $X_N$ actions $\\triangleright_0$ and $\\triangleright_N$ are determined by learning task, while the intermediate actions are selected by the user.\nFor a layer $L_i$ to be invariant it's input and output actions must satisfies the following,\n$L_i(g\\triangleright_{i-1} x) = g_i L_i(x), \\forall g \\in G, x \\in X_{i-1}$\nThe visualization of equivariant neural networks is given below\nA Euclidean feature map in d dimensions with c channels is a function $F: R^d \\rightarrow R^c$ that assigns a c-dimensional feature vector F(x) for every point $x \\in R^d$.\nLet $E(d,c)$ be the set of all Euclidean feature maps $R^d \\rightarrow R^C$. A translation group is called the additive group of the Euclidean space $V = R^d$. It acts on $R^d$ by shifting (or translation)\n$(R^d, +) \\times R^d \\rightarrow R^d, (t,x) \\rightarrow x + t$\nIt induces an action on $E(d,c)$ via\n$(R^d, +) \\times E(d,c) \\rightarrow E(d,c), F\\rightarrow (t\\triangleright F)(x) = F(x - t)$"}, {"title": "4. GRADED VECTOR SPACES", "content": "Here we give the bare minimum of the background graded vector spaces. The interested reader can check details at [3], [7], [4] among other places.\nA graded vector space is a vector space that has the extra structure of a grading or gradation, which is a decomposition of the vector space into a direct sum of vector subspaces, generally indexed by the integers. For the purposes of this paper we will focus on graded vector spaces indexed by integers, but we also give below the definition of such spaces for a general index set I.\nLet N be the set of non-negative integers. An N-graded vector space, often called simply a graded vector space without the prefix N, is a vector space V together with a decomposition into a direct sum of the form\n$V = \\bigoplus_{n \\in N} Vn$\nwhere each $V_n$ is a vector space. For a given n the elements of $V_n$ are then called homogeneous elements of degree n.\nGraded vector spaces are common. For example the set of all polynomials in one or several variables forms a graded vector space, where the homogeneous elements of degree n are exactly the linear combinations of monomials of degree n.\nLet k be a field and consider $V_{(2,3)}$ the space of degree 2 and 3 homogenous polynomials in k[x,y]. It is decomposed as $V_{(2,3)} = V_2 + V_3$, whwre $V_2$ is the space of binary quadratics and $V_3$ the space of binary cubics. Let $u = [f, g] \\in V_2 \\oplus V_3$. Then the scalar multiplication works as\n$\\lambda * u = \\lambda * [f, g] = [\\lambda^2 f, \\lambda^3 g]$\nNext we give another example that was our main motivation for machine learning models over graded vector spaces.\nAssume char k \\neq 2 and C a genus 2 curve defined over k. Then C has affine equation $y^2 = f(x)$ where f(x) is a degree 6 polynomial. The ismorphism class of C is determined by its invariants $J_2, J_4, J_6, J_{10}$, which are homogenous polynomials of degree 2, 4, 6, and 10 respectively in terms of the coefficients of C. The moduli space of genus 2 curves defined over k is isomorphic to the weighted projective space $WP_{(2,4,6,10),k}$.\nThe subspaces of a graded vector space need not be indexed by the set of natural numbers, and may be indexed by the elements of any set I. An I-graded vector space V is a vector space together with a decomposition into a direct sum of subspaces indexed by elements i of the set I:\n$V = \\bigoplus_{i \\in I} V_i$\nThe case where I is the ring Z/2Z (the elements 0 and 1) is particularly important in physics. A (Z/2Z)-graded vector space is known as a supervector space."}, {"title": "5. ARTIFICIAL NEURAL NETWORKS OVER GRADED VECTOR SPACES", "content": "Let us now try to design artificial neural networks over graded vector spaces. Let k be a field and for any integer n \\geq 1 denote by $A^r$ (resp. $P^r$) the affine (resp. projective) space over k. When k is an algebraically closed field, we will drop the subscript. A fixed tuple of positive integers $w = (q_0,..., q_n)$ is called set of weights. The weight of $a \\in k$ will be denoted by wt(a). The set\n$V_w (k) := \\{(x_1,...,x_n) \\in k^n | wt(x_i) = q_i, i = 1, ..., n\\}$\nis a graded vector space over k. From now on, when there is no confusion we will simply use $V_w$ for a graded vector space.\nWe follow the analogy with the classical case of artificial neural networks. A neuron on a graded vector space $V_w$ is a function $f: V_n \\rightarrow k$ such that\n$a_w (x) = \\Sigma_{i=0}^{n} x_i + b,$\nwhere $b \\in k$ is a constant called bias. We can generalize neurons to tuples of neurons via\n$\\varphi := V^n (k) \\rightarrow V^n (k)$\n$x \\rightarrow g (a_0(x),..., a_n (x))$\nfor any gives set of weights $w_0,..., w_n$. Then $\\varphi$ is a k-linear function with matrix written as\n$\\varphi (x) = W x + b,$\nfor some $b \\in k^{n+1}$ and W an $n \\times n$ matrix with integer entries.\nThere is a big confusion here when it comes to terminology. The elements $w_i$ are called weights in classical neural networks, but these are different from weights of the graded vector space $q_i$. The matrix W is called the matrix of weights since it is the matrix $W = [w_{i,j}]$, but again those weights are not the same as weights $q_0,..., q_n$.\nA non-linear function $g: V^n \\rightarrow V^n$ is called an graded activation function. A graded network layer is a function\n$V^n (k) \\rightarrow V^n (k)$\n$x\\rightarrow g (W.x + b)$\nfor some some activation function g. A graded neural network is the composition of many layers. The l-th layer\n$... \\rightarrow V(k) \\xrightarrow{\\varphi_l}V^n (k)...\n$x \\rightarrow \\varphi_l(x) = g_l (W^l x + b^l),$"}]}