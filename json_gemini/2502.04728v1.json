{"title": "Generating Symbolic World Models via Test-time Scaling of Large Language Models", "authors": ["Zhouliang Yu", "Yuhuan Yuan", "Tim Z. Xiao", "Fuxiang Frank Xia", "Jie Fu", "Ge Zhang", "Ge Lin", "Weiyang Liu"], "abstract": "Solving complex planning problems requires Large Language Models (LLMs) to explicitly model the state transition to avoid rule violations, comply with constraints, and ensure optimality a task hindered by the inherent ambiguity of natural language. To overcome such ambiguity, Planning Domain Definition Language (PDDL) is leveraged as a planning abstraction that enables precise and formal state descriptions. With PDDL, we can generate a symbolic world model where classic searching algorithms, such as A*, can be seamlessly applied to find optimal plans. However, directly generating PDDL domains with current LLMs remains an open challenge due to the lack of PDDL training data. To address this challenge, we propose to scale up the test-time computation of LLMs to enhance their PDDL reasoning capabilities, thereby enabling the generation of high-quality PDDL domains. Specifically, we introduce a simple yet effective algorithm, which first employs a Best-of-N sampling approach to improve the quality of the initial solution and then refines the solution in a fine-grained manner with verbalized machine learning. Our method outperforms ol-mini by a considerable margin in the generation of PDDL domain, achieving over 50% success rate on two tasks (i.e., generating PDDL domains from natural language description or PDDL problems). This is done without requiring additional training. By taking advantage of PDDL as state abstraction, our method is able to outperform current state-of-the-art methods on almost all competition-level planning tasks.", "sections": [{"title": "Introduction", "content": "Enabling large language models (LLMs) to plan in complex scenarios like Barman, Floortile, and Termes remains an open problem. While recent LLMs like OpenAI-01 excel at complex reasoning tasks, including coding and mathematics, they still struggle with deductive reasoning and principled planning that requires the consideration of optimality, constraints, and complex state transitions. This limitation persists in ol even using self-critique techniques and multiple answer re-sampling strategies. A natural solution is translating the world abstraction from natural language into Planning Domain Definition Language (PDDL), which utilizes first-order logic (FOL) to explicitly describe states and relationships. Compared to natural language, the formal nature of PDDL simplifies verification and enables the precise specification of constraints and objectives, facilitating the seamless integration of off-the-shelf planning algorithms., However, it remains a huge challenge to translate natural language descriptions into PDDL domains with satisfactory accuracy. Current LLMs perform poorly in this translation task due to two key challenges: the scarcity of high-quality PDDL training data and the complexity of maintaining logical consistency across predicates and actions. Traditionally, the translation process has heavily relied on human expertise and manual refinement, making it difficult to automate and scale [GVSK23]."}, {"title": "Related Work", "content": "Recent advances in large language models (LLMs), such as OpenAI-01 [ZCY+24] and Qwen [YYH+24], have shown promise in handling common reasoning tasks such as GSM8K [CKB+21] and HumanEval [CTJ+21]. The gain of reasoning and planning capabilities of LLMs can be attributed to (but not limited to) several factors: (1) Extensive training on reasoning datasets: [YXWK24] fine-tunes LLMs with distilled chain-of-thought datasets to improve plausible reasoning, and recent models have scaled this approach using larger and more diverse datasets, which has enhanced performance on tasks such as mathematical reasoning, coding, and logical reasoning. However, such an approach raises concerns about whether the observed gains are attributable to data contamination [SKN24, MAS+24]. (2) Self-improvement during inference time: Some approaches incorporate verifiers that provide synthetic feedback during inference, using self-critique [XBSL24], process reward models [ZHB+24] or simple sparse objective reward [ZCS+23] guide improvement. However, [SKN24] shows that imperfect verifiers increase false positive samples during scaling up reasoning at inference time. Despite these plausible advancements in common benchmarks, LLMs face complex reasoning and planning challenges. Results from constraint-heavy and spatially complex planning tasks, for example, Termes (see Figure 2) demonstrate that LLMs continue to struggle with planning tasks requiring intricate multi-step logical reasoning, simultaneous management of multiple constraints, and manipulation of spatial relationships [VMO+24, WLB+24, QLF+25]. These challenges often lead to inconsistent or suboptimal outcomes or worse, hallucinations (see"}, {"title": "LLMs for Task Planning", "content": "However, automatically generating scalable PDDL-based world models by LLMs is still challenging. Current LLMs rely heavily on either human-in-the-loop or extensive training data to plausibly be superior in generating PDDL on limited scenarios. For example, [GVSK23] leverages multiple human experts to refine the logical error in individual PDDL action expressions generated by LLMs. [ZVL+24] collect more than 132,027 SFT data to train LLM on limited simple planning scenarios, (e.g., BlockWorld and Grippers)."}, {"title": "World Model Generation", "content": "In contrast, our approach focuses on automation and scalability across diverse planning scenarios without additional training (no training data is needed). We compare our method and current works in Table 1. Several methods have also explored using LLMs to generate world model representations other than PDDL. For example, GIF-MCTS [DMAM24] and World-coder [TKE24] translate natural language descriptions of world models into Gym-like Python code [BCP+16], and using pre-collected trajectories to validate and providing feedback for wrong state-transition predictions iteratively. Our work, however, focuses on more general planning scenarios without pre-collected validation datasets to provide critique feedback and ensuring the correctness of world modeling."}, {"title": "Adaptation of LLMs", "content": "In the absence of pre-collected validation datasets, the adaptation of LLMs for the PDDL-based world model presents unique challenges. Parameter-efficient finetuning methods (e.g., [HWAZ+22, QLF+23, LQF+24, DQY+23]) enable effective adaptation of LLMs to downstream tasks, they still require high-quality training data. If there lacks a sufficient amount of training data, in-context learning [BMR+20, WTB+22, DLD+22] offers an alternative adaptation approach. Prompt optimization techniques [ZMH+22, PIL+23, YWL+24] further enhance adaptation performance by deriving improved instruction prompts from limited training data. Recently, [XBSL24] proposes an iterative framework to update model-characterizing text prompts with LLMs. [PIL+23, YBB+24] introduce the concept of textual gradients as criteria for updating text prompts. Although finetuning methods can achieve effective adaptation, they risk causing catastrophic forgetting in pretrained LLMs, potentially compromising their general instruction-following capabilities. Therefore, test-time adaptation of LLMs to downstream tasks has emerged as a practical solution. In our paper, we introduce a simple yet effective test-time adaptation method for PDDL-based world model generation that scales efficiently with test-time computing and requires no model finetuning."}, {"title": "The Proposed Test-time Compute Scaling Approach", "content": "Our proposed methods aim to explore the following key questions:\nHow can PDDL serve as a good world representation for planning? Natural language task planning faces significant challenges in state estimation, constraint-based plan generation, and plan validation. How can we create explicit, unambiguous world models? (see Section 3.1)\nHow can we effectively generate PDDL-based world models? Generating symbolic world models requires not only natural language understanding but also sophisticated deductive reasoning to maintain logical consistency across all model components. Without such formal modeling sophistication, models risk generating inconsistent state transitions and producing suboptimal plans. To address this challenge, we enhance LLMs' reasoning capabilities through an instance verbalized machine learning algorithm (see Section 3.3), initializing it with good candidates generated by best-of-N sampling."}, {"title": "PDDL-based World Model Representation", "content": "Classical planning problems are inherently complex. Even determining plan satisfiability [RN16]\u2014whether any solution exists for a given planning problem\u2014is NP-hard. Planning problems that involve optimization under constraints pose even greater challenges for natural language planners. Our approach, which utilizes the PDDL-based representation, offers several advantages: (1) PDDL employs a logical system to express atoms and predicates derived from STRIPS [FN71] (i.e., Stanford Research Institute Problem Solver), and therefore it provides a formal and unambiguous syntax for representing world models. PDDL-based world modeling employs explicit representations that not only enrich the description of actions and states but also ensure precise and straightforward validation, thereby eliminating ambiguity. (2) Natural"}, {"title": "Best-of-N Sampling for PDDL Initialization", "content": "Our test-time scaling approach adopts a two-stage coarse-to-fine optimization process. The first stage is to coarsely search a good initial solution, and the second stage is to iteratively refine this solution in a fine-grained manner. To efficiently get a diverse set of plausible solutions, before LLM iteratively the internal logic with iVML, we adopt Best-of-N sampling to find a good solution as the initial PDDL-based world model. For each problem, the LLM generates N candidate solutions in parallel and retains the K"}, {"title": "iVML: Instance Verbalized Machine learning", "content": "With the BoN sampling to select the candidates as the initial solution, we introduce instance verbalized machine learning to refine both the generated PDDL domain and the natural language chain of thought. iVML is an adaptation of verbalized machine learning [XBSL24] to the instance optimization setting, where the goal is to optimize and refine a single instance (i.e., PDDL domains in this paper). In iVML, functions are parameterized using natural language rather than numerical values. Viewing an LLM as the inference engine, we can evaluate such a natural language parameterized function, and optimize its model parameters in the natural language space. In our setting, we are given a description G from the planning domain either in natural language for NL2Domain or in problem code for Prob2Domain, and we aim to generate an accurate corresponding PDDL domain description D*, i.e.,\n\nD* = arg \\min L(G, D)\n\nwhere L() is a loss function defining the closeness between G and D. Solving Equation (3.2) is difficult as both G and D are text, and L(\u00b7) is hard to define unless abstractly using natural language. Using the iVML framework, we can approximately solve Equation (3.2) with an iterative algorithm that alternates between two natural language parameterized functions at the iteration i:\n\nFi = f_{opt}(L, G, T_{i-1}, D_{i-1}),\nT_{i}, D_{i} = f_{update}(F_{i}, T_{i-1}, D_{i-1}),\n\nwhere Ti-1 and Di\u22121 correspond to the current thoughts and the current PDDL domain, Fi is the feedback from the optimizer function fopt(\u00b7), Ti and Di are the updated thoughts and PDDL domain output from the update function fupdate(\u00b7). Do is initialized from the best-of-N sampleing. These two functions are evaluated through separate LLMs calls. We show the prompt templates for fopt (\u00b7) and fupdate(\u00b7) below:"}, {"title": "Experiments and Results", "content": "We conduct extensive experiments to compare our test-time scaling algorithm to existing state-of-the-art methods on competition-level PDDL domain synthesis tasks. Our method improves PDDL generation across nearly all tested LLMs. By using PDDL as an intermediate abstraction layer, we have shifted the role of LLMs from acting as planners to generating PDDL-based world models. The generated PDDL-based world model, combined with a classical planner in the loop, helps to reduce hallucinations when using LLMs directly as planners."}, {"title": "Experiment Setup", "content": "Evaluation tasks and datasets. We evaluate several test-time scaling methods on the International Planning Competition benchmark\u00b9, which encompasses diverse complex planning domains and problems. Our evaluation focuses on two key PDDL domain synthesis tasks, including (1) NL2Domain which aims to convert Natural Language Descriptions to PDDL Domains; and (2) Prob2Domain which aims to derive necessary PDDL domains from PDDL problems. The evaluation metric used here is the success rate of the generated PDDL domain passing the PDDL validation system [HL03].\nLarge language model settings. The backbone LLMs in our experiment include Qwen2.5-Instruct (0.5B-72B parameters) [YYH+24], LLaMA3.1-Instruct (8B and 70B parameters) [DJP+24], and Yi-1.5-Chat (6B, 9B, and 34B parameters) [YCL+24]. We also incorporate specialized code-oriented LLMs, specifically Qwen2.5-Coder and Yi-1.5-Coder. In addition to open-source LLMs, we benchmark against OpenAI's proprietary models, including GPT-40, 01-mini, and o1-preview. We test our proposed methods on Qwen models in a zero-shot setting without model finetuning.\nChain of thought prompting. All baselines here utilize chain-of-thought (CoT) prompting by default. Current LLMs have been extensively trained on datasets that include step-by-step reasoning besides"}, {"title": "Main Results in PDDL Domain Synthesis", "content": "Current LLMs perform poorly in PDDL domain synthesis. Despite advances in code and math reasoning, LLMs exhibit fundamental limitations in PDDL-based formal synthesis. For instance in Table 2,"}, {"title": "Convergence Comparison between BoN and iVML", "content": "Experiment settings. This section presents a comparative analysis of the convergence behavior of BoN and iVML in PDDL domain synthesis tasks. The computational efficiency and synthesis success"}, {"title": "Ablation Study of Initialization Strategies", "content": "Experiment settings. This section investigates the effect of initialization strategies on iVML through a controlled experiment. We evaluate three LLMs Qwen2.5-Coder (7B), Deepseek-Coder-Instruct-v1.5 (7B), and LLaMa-3.1-Instruct (8B)\u2014across two different initialization settings: single-pass and BoN-8. Among these models, LLaMa-3.1-Instruct (8B) is defined as a \u201cweak model\u201d in PDDL synthesis, as it achieves a zero success rate in the main experiment in Table 2. The purpose of including it in this study is to investigate whether our approach can enhance LLaMa's capabilities in PDDL synthesis, enabling its transition from a weak to a strong model.\nBoN vs. Single-pass sampling. BoN sampling, as an initialization strategy, provides iVML with the dual advantages of accelerated convergence and improved solution quality. For example, in Figure 4, with BoN-8 initialization in NL2Domain, Deepseek-Coder saturates earlier than its single-pass counterpart at T = 16, while achieving higher accuracy (reaching approximately 270 successful domains compared to fewer than 200 in single-pass). Unlike single-pass sampling, which is analogous to random initialization in traditional optimization, e.g., stochastic gradient descent, BoN generates a diverse set of initial candidate solutions. The solution diversity can effectively improve iVML with expanded exploration. By covering a broader range of the solution space, BoN helps to avoid early convergence to suboptimal solutions. This means that the algorithm is less likely to get stuck in local minima, which are common challenges in complex optimization problems [Bak19]. By selecting high-quality initial solution candidates, BoN guides"}, {"title": "PDDL Problem Generation", "content": "Experiment settings. This section investigates the effectiveness of our approach while generalized to PDDL problem synthesis. In contrast to the PDDL domain, which outlines the general framework or environment defined for planning tasks, the PDDL problem defines a specific instance of the planning task within that domain. This involves defining two main components (1) Initial state: the starting state of the world, defined by the predicates that are true initially, and (2) Goal state: The objective that the planner aims to achieve. We adopt the Planetarium [ZVL+24] benchmark, which evaluates LLMs' capacity to"}, {"title": "Comparison to LLM-as-Planner Methods", "content": "In the previous sections, we demonstrated that iVML enhances LLMs' ability to generate high-quality PDDL-based world models. In this section, we compare our method, which utilizes synthesized PDDL domains as world models, with LLMs-as-a-Planner methods that use natural language for world modeling and planning. A detailed description of the tested planning cases is provided in Appendix C.\nExperimental settings. LLMs often hallucinate for tasks such as generating feasible or optimal plans, understanding the planning problem, and strictly following the rules, particularly in complex planning problems (e.g., Termes and Barman) [WLB+24]. To investigate whether these hallucinations arise from limitations in prompt engineering, we provide LLMs with explicit instructions on the rules they must follow and require them to verify rule compliance at each step of the planning process. Additionally, we employ two strategies to reduce uncertainty in LLM-generated plans: (1) Introducing the Pass@8 metric to evaluate the probability that at least one of the top 8-generated plans is correct, and (2) Allowing LLMs to self-evaluate their plans and refine them based on these assessments. The baseline implementations of LLM-as-Planner methods are based on GPT-40, 01-mini, and o1-preview, respectively.\nDiscussion on LLM-as-Planner methods. The observations are as follows: (1) rule violation: Despite explicitly informing LLMs to examine rule violation at each step, the generated plans still contain elements that inherently violate the predefined rules. For example, in the step 3 and 4 of Figure 2, o1 incorrectly places the block from pos-0-1 to pos-1-2, mistakenly assuming that they are neighboring positions. This action violates the rule governing the placement of blocks. (2) incorrect state transition estimation: The LLMs fail to accurately estimate state transitions. For instance, after moving the block from pos-1-1 to pos-1-2, o1 incorrectly assumes that the heights of both positions are 0, reflecting an inability to track state changes correctly. (3) incorrect goal achievement estimation: o1 attempts to achieve the goal in a manner that disregards all constraints and relies on flawed state estimations, which results in a plan that"}, {"title": "Concluding Remarks and Current Limitations", "content": "Our work introduces a test-time scaling framework for automated PDDL synthesis that integrates best-of-N sampling with instance verbalized machine learning. This approach demonstrates that effectively scaling test-time computation of open-source LLMs can outperform state-of-the-art closed-source LLM planners, including OpenAI's o1-mini.Our hybrid method employs a two-phase optimization paradigm: (1) BoN initialization which generates diverse candidate solutions to explore critical regions of the search space, addressing the cold-start problem in formal language synthesis. (2) iVML refinement which iteratively improves the BoN initial solutions through self-critique and natural language feedback, resolving logical inconsistencies and syntactic errors. Leveraging BoN's stochastic search to initialize iVML's refinement process, our method achieves faster convergence and higher-quality PDDL domains. The effectiveness of iVML in PDDL problem synthesis even surpasses models specifically fine-tuned for this task. The results show that our proposed test-time compute scaling approach can enhance LLMs' formal reasoning and planning capabilities. By generating PDDL-based symbolic world models, we enable explicit model-based planning with classical search algorithms (e.g., A*), avoiding the error-prone state transitions inherent in direct LLM-as-planner approaches. Beyond PDDL synthesis, our work provides a general framework for scaling up test-time compute of LLMs for formal language synthesis.\nThe limitations of our work include: (1) challenges in semantic verification for autoformalization: Consistent with prior work in PDDL synthesis (e.g., [GVSK23, ZVL+24, VMO+24]), our evaluation relies on VAL [HL03] for syntax validation and plan verification. While VAL ensures syntactic correctness (e.g., predicate arity, type consistency) and plan executability (e.g., action preconditions and effects), it cannot detect semantic inconsistencies that violate domain intent or commonsense logic. This limitation parallels broader challenges in autoformalization, where even formal mathematical proof [ZHP21] struggles to verify semantic alignment between informal specifications and formal outputs through compiler checking. (2) simulation assumptions: Our evaluation relies on an idealized simulation environment with two key assumptions. First, actions execute perfectly, with no execution misalignment. Second, the state space is fully observable, with no sensor noise or occlusions. These idealized conditions differ significantly from real-world robotic manipulation scenarios."}, {"title": "Planning Problem Formulation", "content": "The classical planning problem [FN71] in artificial intelligence involves finding a sequence of actions that transition an agent from an initial state to a desired goal state within a deterministic and fully observable environment. It is formalized as a tuple (S, A, T, 80, G), where: S is the set of all possible states; A is the set of all possible actions; T : S \u00d7 A \u2192 S is the state transition function, specifying the outcome state resulting from applying an action in a state; so \u2208 S is the initial state; G is the goal condition, a predicate over states. The objective is to find a sequence of actions a1, a2, ..., an \u2208 A such that applying these actions successively transitions the system from so to a state sn satisfying the goal condition G:\n\nS_{n} = T(S_{n-1}, a_{n}) = T(T(...T(T(S_{0}, a_{1}), a_{2}), ..., a_{n-1}), A_{n})\n\nwith\n\nS_{n} = G\n\nPrevious works that adopt LLMs as planners estimate state transitions implicitly within language latent spaces, lacking explicit representations of the state space required for classical planning. This implicit representation can make it challenging to ensure consistency, validity, and completeness in the planning process. In contrast, our work leverages LLMs to generate explicit representations of the state space by creating PDDL domains. We utilize the generative capabilities of LLMs to produce formal PDDL models from high-level descriptions of the planning tasks. This approach bridges the gap between natural language specifications and formal planning models.\nBy generating PDDL domains using LLMs, we obtain: 1. explicit definitions of the set of states S through predicates and objects, 2. formal specifications of actions A, including their preconditions and effects, 3. a deterministic state transition function T derived from the action definitions, 4. abilities to cooperate with clearly defined initial state so and goal condition G in PDDL syntax. Thus the new objective under this background is: Given high-level descriptions of planning tasks, our objective is to leverage LLMs to create PDDL domains that can represent state transitions explicitly. By generating these explicit representations, we enable classical planning algorithms to efficiently search for plans using the defined state transitions during the planning process."}, {"title": "STRIPS Formulation", "content": "The states are expressed through a set of predicates that describe the properties of objects in the environment. Each predicate represents a relationship or characteristic, such as on (A, B), which indicates that object A is on top of object B. A state can be represented as:\n\nS_{0} = {on(A, B), clear(C)}\n\nThis representation includes relationships that capture the positions and statuses of the objects within the environment. Actions in PDDL are tightly bound to the representation of states through the use of preconditions and effects. Each action is defined by specifying what must be true in the current state for the action to be applicable (preconditions), as well as what changes in the state when the action is performed (effects). For example, consider an action move(A, B, C), indicating that move the object A from B to C. The preconditions and effects could be defined as follows: Preconditions: Pre(move(A, B, C)) = {on(A, B), clear(C)}and Effects: Eff(move(A, B, C)) = {on(A, C), clear(B)}. They indicate that for the action move(A, B, C) to be executed, object A must be positioned on B, and C must be clear of any objects. And after moving A from B to C, A is now on C, and B is clear. The transition can be formulated as: T(S, A) \u2192 S'. State transitions occur according to a defined sequence of"}, {"title": "Tasks in Case Study", "content": "Barman. The main goal of the Barman domain is to simulate the task of a bartender who prepares and serves cocktails by manipulating ingredients, tools, and glassware within a bar setting. In this scenario, the agent is tasked with creating a specific cocktail by following a series of actions that involve: 1. Identifying and dispensing the necessary ingredients required for the cocktail from the available dispensers. 2. Using bar tools such as shakers and shot glasses effectively, ensuring they are clean and suitable for use. 3. Coordinating the use of both hands to pick up and handle objects while ensuring that hands are free when needed and that objects are properly placed on surfaces like the bar counter when not in use. 4. Adhering to the proper sequence of steps for cocktail preparation, which includes dispensing ingredients into the shaker, mixing them, and then pouring the mixture into a shot glass. 5. Maintaining the correct state of all objects involved, such as keeping the shaker and glasses clean and empty before use, and updating their states appropriately as actions are performed. 6. Successfully preparing the cocktail and having it contained in the shot glass, thereby fulfilling the goal of serving the drink as intended.\nGripper. Gripper problem is designed to test the agent's ability to manage resources and plan actions in a scenario involving manipulating multiple objects across different locations. The agent, represented by one or more robots, must strategically perform the following tasks: Pick Up Balls: The agent must use its available grippers to pick up the balls from their initial locations. This requires careful hand management to ensure grippers are free and available when needed. Transport Balls: The robot needs to navigate between rooms to move balls to their specified target locations efficiently. This involves planning the correct sequence of moves and ensuring that the robot is in the correct room with the appropriate objects. Drop Balls: The agent must release the balls in the designated rooms. This requires ensuring that the robot's grippers are properly aligned and that the release actions are performed at the correct time. Manage Resources: Throughout the task, the agent must effectively manage both its grip and position within the environment, making sure that it follows constraints such as carrying capacity and room access.\nTyreworld. The main goal of the Tyreworld problem is to simulate the challenges of vehicle maintenance and tire management in a scenario where a vehicle may suffer random tire failures. The primary objectives include: 1. Replacing Flat Tires: The agent must effectively replace flat tires with intact ones on the vehicle's hubs. 2. Inflating Tires: The intact tires must be inflated before being mounted onto the vehicle. 3. Ensuring Secure Fastening: After replacing and inflating the tires, the nuts on the hubs must be securely tightened to ensure the wheels are safely attached. 3. Resource Management: The agent must manage limited resources (e.g., spare tires, tools like jacks and wrenches) strategically to minimize the risk of failure or being stranded. 4. Navigating Uncertainty: The agent must effectively plan actions while accounting for the possibility of tire failures and other uncertainties in the environment. 5. Reaching the Destination: Ultimately, the goal is to ensure the vehicle is properly equipped with intact, inflated tires, allowing it to continue its journey successfully. 6. The Tyreworld problem serves to test an agent's planning, resource management, and adaptability in unpredictable scenarios related to vehicle maintenance.\nFloor-tile. The main goal of Floor-tile is to enable the robot to navigate the environment, manage its colors, and paint the tiles according to specific requirements. The robot must efficiently utilize its"}, {"title": "Prompt Template for Chain-of-Thought", "content": "Prompts for Our Methods\nYou will be given a natural language description of a planning problem. Your task is to translate this description into PDDL domain code. This includes defining predicates and actions based on the information provided.\nInformation about the Al agent will be provided in the natural language description. Note that individual conditions in preconditions and effects should be listed separately. For example, \"object1 is washed and heated\" should be considered as two separate conditions \"object1 is washed\" and \"object1 is heated\". Also, in PDDL, two predicates cannot have the same name even if they have different parameters. Each predicate in PDDL must have a unique name, and its parameters must be explicitly defined in the predicate definition. It is recommended to define predicate names in an intuitive and readable way. Remember: Ignore the information that you think is not helpful for the planning task.\nYou are only responsible for domain generation. Before you generate the concrete domain code, you should first generate a natural language thought about the meaning of each variable, and the step-by-step explaination of the domain code. Even if I didn't provide the exact name of the predicates and actions, you should generate them based on the information provided in the natural language description.\nTemplate is:\n### Thought:\npredicates1: the name of predicate1, explanation of predictate1 predicaten: the name of predicaten, explanation of predictaten action1: the name of action1, explanation of action1 actionn: the name of action, explanation of actionn \n### Domain: \"'pddl\nThe concrete pddl code for domain.pddl\nNow its your time to generate the solution, you have to follow the format I provided above.\nNL_Description: Natural language description of the planning domain"}, {"title": "The Prompt of Our Methods on Termes", "content": "Prompts for Our Methods on Termes\nYou will be given a natural language description of a planning problem. Your task is to translate this description into PDDL domain code. This includes defining predicates and actions based on the information provided.\nInformation about the Al agent will be provided in the natural language description. Note that individual conditions in preconditions and effects should be listed separately. For example", "object1 is washed and heated\" should be considered as two separate conditions \"object1 is washed\" and \"object1 is heated\". Also, in PDDL, two predicates cannot have the same name even if they have different parameters. Each predicate in PDDL must have a unique name, and its parameters must be explicitly defined in the predicate definition. It is recommended to define predicate names in an intuitive and readable way. Remember": "Ignore the information that you think is not helpful for the planning task.\nYou are only responsible for domain generation. Before you generate the concrete domain code", "is": "n### Thought: predicates1: the name of predicate1", "predicaten": "the name of predicaten", "action1": "the name of action1", "actionn": "the name of action", "above.\nNL_Description": "nYou control a robot that can take the following actions to build complex structures.\nMove from a position to another. The new position and the old position must be at the same height. pddl action name: move\nMove up from a position to another", "name": "move-up\nMove down from a position to another"}, {"name": "move-down\nPlace a block at a neighboring position from the robot's current position. The robot must have a block. The current height at the robot's position and the block's position must be the same. A block cannot be placed at the depot. The height at the block's position will be one block higher than the current height. pddl action name: place-block\nRemove a block at a neighboring position from the robot's current position. The robot must not have a block. A block cannot be removed from the depot. The current height at the robot's position must be the same as the new height at the block's position. The new height at the block's position will be one block lower than the current height. pddl action name: remove-block\nCreate a block at the depot. The robot will have the block. pddl action name: create-block\nDestroy a block at the depot. The robot must have a block. pddl action name: destroy-block\nAn example problem PDDL file to the domain is:\n\"\"pddl (define (problem prob) (:domain termes); Initial state: ; 0 0 ROD ; 0 0 0 ;000;Goal state: ; 000;010;000; Maximal height: 1 (:objects n0 - numb n1 - numb pos-0-0 - position pos-0-1 - position pos-0-2 - position pos-1-0 - position pos-1-1 - position pos-1-2 - position pos-2-0 - position pos-2-1 - position pos-2-2 - position) (:init (height pos-0-0 n0) (height pos-0-1 n0) (height pos-0-2 n0) (height pos-1-0 n0) (height pos-1-1 n0) (height pos-1-2 n0) (height pos-2-0 n0) (height pos-2-1 n0) (height pos-2-2 n0) (at pos-2-0) (SUCC n1 n0) (NEIGHBOR pos-0-0 pos-1-0) (NEIGHBOR pos-0-0 pos-0-1) (NEIGHBOR pos-0-1 pos-1-1) (NEIGHBOR pos-0-1 pos-0-0) (NEIGHBOR pos-0-1 pos-0-2) (NEIGHBOR pos-0-2 pos-1-2) (NEIGHBOR pos-0-2 pos-0-1) (NEIGHBOR pos- 1-0 pos-0-0) (NEIGHBOR pos-1-0 pos-2-0) (NEIGHBOR pos-1-0 pos-1-1) (NEIGHBOR pos-1-1 pos-0-1) (NEIGHBOR pos-1-1 pos-2-1) (NEIGHBOR pos-1-1 pos-1-0) (NEIGHBOR pos-1-1 pos-1-2) (NEIGHBOR pos-1-2 pos-0-2) (NEIGHBOR pos-1-2 pos- 2-2) (NEIGHBOR pos-1-2 pos-1-1) (NEIGHBOR pos-2-0 pos-1-0) (NEIGHBOR pos-2-0 pos-2-1) (NEIGHBOR pos-2-1 pos-1-1) (NEIGHBOR pos-2-1 pos-2-0) (NEIGHBOR pos-2-"}]}