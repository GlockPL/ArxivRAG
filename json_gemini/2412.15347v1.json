{"title": "Exploring Machine Learning Engineering for Object\nDetection and Tracking by Unmanned Aerial\nVehicle (UAV)", "authors": ["Aneesha Guna", "Parth Ganeriwala", "Siddhartha Bhattacharyya"], "abstract": "With the advancement of deep learning methods it\nis imperative that autonomous systems will increasingly become\nintelligent with the inclusion of advanced machine learning\nalgorithms to execute a variety of autonomous operations. One\nsuch task involves the design and evaluation for a subsystem of\nthe perception system for object detection and tracking. The\nchallenge in the creation of software to solve the task is in\ndiscovering the need for a dataset, annotation of the dataset,\nselection of features, integration and refinement of existing algo-\nrithms, while evaluating performance metrics through training\nand testing. This research effort focuses on the development\nof a machine learning pipeline emphasizing the inclusion of\nassurance methods with increasing automation. In the process,\na new dataset was created by collecting videos of moving object\nsuch as Roomba vacuum cleaner, emulating search and rescue\n(SAR) for indoor environment. Individual frames were extracted\nfrom the videos and labeled using a combination of manual\nand automated techniques. This annotated dataset was refined\nfor accuracy by initially training it on YOLOv4. After the\nrefinement of the dataset it was trained on a second YOLOv4\nand a Mask R-CNN model, which is deployed on a Parrot\nMambo drone to perform real-time object detection and tracking.\nExperimental results demonstrate the effectiveness of the models\nin accurately detecting and tracking the Roomba across multiple\ntrials, achieving an average loss of 0.1942 and 96% accuracy.", "sections": [{"title": "I. INTRODUCTION", "content": "Search and Rescue (SAR) operations have traditionally been\nlabor-intensive and inherently risky for both the missing indi-\nviduals and the rescue personnel. However, recent technologi-\ncal advancements, particularly in the field of unmanned aerial\nvehicles (UAVs), have the potential to transform these missions\nby introducing autonomous capabilities [1]. While human\nrescuers remain essential, UAVs simplify the search process,\nassist in formulating optimal rescue strategies, and reduce risks\nin hazardous environments. This is particularly beneficial in\nmountainous regions, where traditional SAR methods expose\nrescuers to extreme conditions. The value of UAVs in SAR\nmissions has been widely recognized, resulting in the devel-\nopment of remote-controlled drone systems for emergency\nassistance. However, the ultimate aim is to develop fully\nautonomous rescue drones capable of independent operation.\nIn response to this need, Lygouras et al. [2] proposed an aerial\nrescue system to operate autonomous \"ROLFERS\" (Robotic\nLifeguard for Emergency Rescue) designed to provide imme-\ndiate assistance during maritime emergencies and alleviate the\nworkload of rescue teams. In the field of autonomous UAVs\nfor SAR, Waharte and Trigoni [3] explored search algorithms\nfocused on optimizing the efficiency of locating missing per-\nsons in minimal time. The algorithms tested included greedy\nheuristics, potential-based heuristics, and Partially Observable\nMarkov Decision Process-based heuristics, which were applied\nin a simulated outdoor environment. The results were promis-\ning, showing the potential to search for victims in a reasonable\ntime, though the high computational cost of these algorithms\nposed challenges for real-world implementation.\nThis work aims to address this gap by focusing on the\ndevelopment of an autonomous drone system specifically for\nindoor environments, with the potential for adaptation to real-\nworld human detection in various settings. We illustrate this\nby demonstrating the system's ability to autonomously detect\nRoomba vacuum cleaners indoors, showcasing the technol-\nogy's potential for future SAR applications. The remainder of\nthis paper is organized as follows: Section II provides a review\nof related works in the field of UAV-based SAR. Section III\noutlines the proposed methodology for our autonomous drone\nsystem. Section IV details the experimental setup and pro-\ncedures, followed by the presentation of experimental results\nin Section V. Finally, Section VI concludes the paper with a\nsummary of findings and future directions for research."}, {"title": "II. RELATED WORKS", "content": "Object detection, a widely researched computer vision tech-\nnique, aims to identify and locate specific objects within an\nimage, providing pose information for each detection [4].\nDepending on the method employed, outputs can range from\nsimple object location to more complex representations like\nbounding boxes or segmentation masks. Image classification\ninvolves considering various variables during training, such as\nchanges in lighting, diverse camera angles and positions, dif-"}, {"title": "III. PROPOSED METHODOLOGY", "content": "Unmanned Aerial Vehicles (UAVs), equipped with cam-\neras provide a unique vantage point for surveying large\nareas. However, the challenge lies in developing computer\nvision based software to process the captured video data\nand efficiently detect and track objects of interest. In this\nresearch effort, we propose a Machine learning engineering\nframework, Automated Labeling and Detection of Objects for\nTracking (ALDOT) for automated labeling with assurance,\nfeature engineering for selecting and evaluating the features,\nand finally, deep learning methods to address the challenge of\ndetecting and tracking multiple moving objects. The proposed\nmethodology (Figure 1) utilizes ALDOT, and is discussed\nfurther below."}, {"title": "A. Data Acquisition and Preprocessing", "content": "In this research, the motivation was to create a real time\nmachine learning based software for object tracking. The\ntestbed had moving Roombas, but there was a lack of dataset\nwith moving Roombas, as a result the new datatset was\ncreated. A Parrot Mambo drone was selected as the UAV.\nIt is utilized for data acquisition, equipped with a high-\nresolution camera to capture video footage of Roomba vacuum\ncleaners (target objects) in an indoor environment. The optimal\naltitude for the drone is determined by considering the physical\ndimensions of the Roomba, the camera's pixel resolution, and\nits field of view (FOV), ensuring that the captured images\ncontain sufficient detail for accurate object detection. Video\ndata is recorded at various angles and heights to create the\ndataset (https://github.com/ParthGaneriwala/RoombaDataset).\nFrom the recorded videos, individual frames are extracted\nfor further analysis. A subset of these frames (20 images) is\nmanually annotated using an online annotation tool, creating\na labeled dataset that includes bounding box coordinates for\nthe Roombas."}, {"title": "B. Initial Object Detection with YOLOv4 for labelling", "content": "The initial phase of object detection employs the YOLOv4\n(You Only Look Once version 4) model [10], leveraging its\ncapability for real-time object detection. YOLOv4 is imple-\nmented using the Darknet framework. The manually labeled\nimages are used to train the YOLOv4 model, employing a\ntransfer learning approach. A pre-trained YOLOv4 model on\nthe COCO dataset [15] is fine-tuned on our labeled dataset,\nsignificantly reducing training time and enhancing detection\nperformance."}, {"title": "C. Automated Labeling and Label Assurance", "content": "The trained YOLOv4 model is then applied to the remaining\nunlabeled images to generate bounding box predictions for the\nRoombas (Figure 2). To ensure the quality of these automated\nannotations, a random subset of images is selected for visual\ninspection. Our objective was to identify the frames repre-\nsenting scenario shifts [16]. Instead of conducting a granular\nframe-by-frame analysis, we reviewed the videos manually\nand marked the specific frames that captured the scenario\nshifts, amounting to a total of 100 images. Our selection\nis underpinned by the Law of Large Numbers (LLN), as\nillustrated in Equation 1:\n\n$\u03a7\u03b7 - \u03bc$ as $n\u2192\u221e$                                                                                                                                                                                                                                                                                                                                                                                          (1)"}, {"title": "D. Training for Detection and Segmentation with YOLOv4 and\nMask R-CNN", "content": "To further refine object detection and achieve precise in-\nstance segmentation, the filtered bounding box information is\nused to train a Mask R-CNN model [8]. Mask R-CNN, a state-\nof-the-art architecture for object detection and segmentation,\ncomprises a backbone network (e.g., ResNet) for feature\nextraction, a Region Proposal Network (RPN) for generating\nobject proposals, and a mask prediction head for creating\nsegmentation masks. The Mask R-CNN model is trained on the\nrefined dataset, and its performance is evaluated on a separate\ntest set to assess both detection and segmentation accuracy.\nMetrics such as mean Average Precision (mAP) and Intersec-\ntion over Union (IoU) are used to quantify performance. We\nalso trained a YOLOv4 model to evaluate against."}, {"title": "E. Real-Time Deployment and Autonomous Tracking", "content": "Upon successful evaluation, the trained Mask R-CNN and\nYOLOv4 model is deployed on the Parrot Mambo drone for\nreal-time object detection and tracking. The drone's control\nsystem is integrated with the detection model to enable au-\ntonomous tracking (Figure 3). The system is programmed to\nadjust the drone's position and camera orientation to keep\nthe detected Roomba centered in the camera's field of view.\nThis control mechanism involves calculating the center of the\ndetected bounding box and adjusting the drone's movements\nto minimize the offset between the bounding box center and\nthe center of the camera's field of view. The results of this\nprocess are detailed in the following section."}, {"title": "IV. EXPERIMENTATION AND RESULTS", "content": "The evolution of deep learning has led to significant ad-\nvancements in object detection, resulting in two main cate-\ngories: one-stage and two-stage detectors. Each offers distinct\ntrade-offs in terms of accuracy, speed, and computational effi-\nciency. One-stage detectors, such as YOLO, SSD (Single Shot\nDetector), and RetinaNet, directly predict object categories\nand locations in a single forward pass using dense anchor\nboxes or points across various spatial positions and scales.\nThese models are known for their computational efficiency and\nreal-time performance. For instance, YOLO divides the input\nimage into an S x S grid, assigning detection responsibility\nto the grid cell containing the object's center. In contrast,\ntwo-stage detectors, such as Faster R-CNN, generate initial\nobject proposals using Region Proposal Networks (RPNs)\nbefore refining the locations and predicting object categories.\nWhile more accurate, two-stage detectors typically require\nmore computational resources compared to one-stage models.\nThe choice between one-stage and two-stage detectors depends\non the specific needs of the task, balancing accuracy, speed,\nand available resources. The Mask R-CNN architecture further\nextends object detection by adding instance segmentation. It\noperates in two parallel stages: one for generating object\nproposals and bounding box offsets, and another for predicting\nbinary masks for each Region of Interest (RoI). This parallel"}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In conclusion, ALDOT a Machine Learning Engineering\nframework successfully demonstrated how dataset creation,\nautomated labeling, and deep learning algorithms can be\nintegrated with drones to effectively detect and track objects.\nThis application holds significance in real-world scenarios as\nit minimizes risk for first responders by enabling the drone to\ndetect and monitor individuals initially. This approach would\nfacilitate image segmentation, potentially strengthening the\nmodel's tracking and detection capabilities. Additionally, ex-\npanding the scope to detect humans instead of Roombas would\nalign more closely with the search and rescue objectives,\nfurther advancing the study's relevance and applicability."}]}