{"title": "WalnutData: A UAV Remote Sensing Dataset of Green Walnuts and Model Evaluation", "authors": ["Mingjie Wu", "Chenggui Yang", "Huihua Wang", "Chen Xue", "Yibo Wang", "Haoyu Wang", "Yansong Wang", "Can Peng", "Yuqi Han", "Ruoyu Li", "Lijun Yun", "Zaiqing Chen", "Songfan Shi", "Luhao Fang", "Shuyi Wan", "Tingfeng Li", "Shuangyao Liu", "Haotian Feng"], "abstract": "The UAV technology is gradually maturing and can provide extremely powerful support for smart agriculture and precise monitoring. Currently, there is no dataset related to green walnuts in the field of agricultural computer vision. Thus, in order to promote the algorithm design in the field of agricultural computer vision, we used UAV to collect remote- sensing data from 8 walnut sample plots. Considering that green walnuts are subject to various lighting conditions and occlusion, we constructed a large-scale dataset with a higher-granularity of target features - WalnutData. This dataset contains a total of 30,240 images and 706,208 instances, and there are 4 target categories: being illuminated by frontal light and unoccluded (A1), being backlit and unoccluded (A2), being illuminated by frontal light and occluded (B1), and being backlit and occluded (B2). Subsequently, we evaluated many mainstream algorithms on WalnutData and used these evaluation results as the baseline standard. The dataset and all evaluation results can be obtained at https://github.com/1wuming/WalnutData.", "sections": [{"title": "I. INTRODUCTION", "content": "Currently, UAV technology is approaching maturity and is sufficient to provide reliable assistance in fields such as agroforestry production management [13, 41], emergency res- cue [24, 35], and security monitoring [43]. In the production of agricultural and forestry crops, UAV, by carrying multi-modal or high-resolution camera sensors, can quickly acquire image information of large-scale farmland and orchards, providing strong technical support for crop detection, yield estima- tion, and automated management in precision agriculture [2]. Therefore, UAV technology is widely applied in the agricul- tural field. However, object detection combined with UAV technology faces many challenges, such as lighting changes, foliage occlusion, and the diversity of target scales [19, 9, 36]. These problems significantly increase the detection difficulty and limit the performance of existing algorithms in practical applications.\nAs a crop of great value [39], the green walnut has a complex surface texture, a high color similarity to the back- ground, and is often affected by the occlusion of branches, leaves, and lighting changes. These characteristics make it a research object with unique scientific challenges and en- gineering application value in agricultural object detection. In the future of the smart walnut industry, in automated applications such as aerial UAV picking robots, accurate object detection is not only the basis for crop positioning but also the core prerequisite for robot path planning, obstacle avoidance decision-making, and picking priority judgment. If the impact of environmental interference on the apparent characteristics of the target is ignored, the reliability of the detection algorithm will directly affect the efficiency and success rate of the robot's task execution. Therefore, data-driven automated management methods for walnut production will greatly need a large-scale dataset with higher-granularity target features.\nCurrently, most UAV-based object detection datasets are related to urban road environments, such as VisDrone [42] and UAVDT [8], or datasets for maritime object detection, such as SeaDronesSee [30] and SDS-ODv2 [16]. There are only relatively few open-source datasets for UAV-based object"}, {"title": "II. RELATED WORKS", "content": "In this section, we review the main annotated datasets that can be used for supervised learning models in the field of UAV vision and agricultural scenarios."}, {"title": "A. Annotated Image Datasets Collected by UAV", "content": "In recent years, most of the mainstream annotated datasets collected by UAV are used to describe data of traffic roads or marine environments, such as VisDrone [42], UAVDT [8], SeaDronesSee [30], and SDS-ODv2 [16]. As can be seen from Table I, the images captured by these UAVs have a height range of 5-260 meters, a shooting angle range of 0-90\u00b0, and an image width range of 960-5,456 pixel. These datasets cover various scenarios such as cities, villages, and oceans, mainly focusing on road traffic environment analysis and maritime rescue, but lacking coverage of agricultural target scenarios. WalnutData is an agricultural scenario dataset different from the traffic and maritime fields. The UAV images are collected in the range of 12-30m, with an aerial shooting angle of 90\u00b0. The width of the dataset images is 1,024 pixel after the original images are segmented. WalnutData has a significant advantage over other datasets in terms of the number of images and instances."}, {"title": "B. Annotated Datasets in Agriculture", "content": "With the rapid popularization of deep learning technology and the urgent needs of precision agriculture, more and more datasets in the field of computer vision for agriculture have been constructed and made public. The main objects of study in these datasets listed in Table II include apple, potato, tomato, mango, etc. However, there is still a lack of research on green walnut targets.\nIn studies such as tomato [38], apple [3], and mango [28], the image data are mainly collected by DSLR camera, UGV, or mobile phone. These shooting methods are affected by the ground environment. Moreover, the planting terrains of crops such as tomato, cherry, or apple are relatively flat, which is quite different from the growing terrain of walnut trees. In addition, in the research on agricultural UAV-related datasets, the apple trees studied by Santos T et al. [27] have a neat interval, which is very conducive to collecting relatively regular data information. Thus, effective algorithms can be used for apple detection, tracking, and positioning. Butte Set al. [5] proposed a potato dataset. Through the model they designed, it is possible to accurately identify healthy or drought-stressed potatoes, providing a new idea for precision agriculture.\nIn Yunnan Province, China, most walnut trees are planted in mountainous areas with large altitude differences and complex terrains, and the fruit trees are unevenly distributed [34]. Therefore, in this study, UAV aerial photography is used for data collection to obtain WalnutData. Compared with other datasets, WalnutData has a more detailed division of crop characteristic states and a larger amount of data, which can provide a more solid foundation for model design. In addition, WalnutData provides three types of labels (VOC, COCO, and YOLO), which are suitable for many current mainstream object detection models and offer multiple choices for researchers in related fields."}, {"title": "III. WALNUTDATA CONSTRUCTION", "content": ""}, {"title": "A. Data Collection", "content": "We carried out data collection on 8 walnut sample plots between July 18 and September 14, 2024. These sample plots are all located in Yangbi County, Dali Bai Autonomous"}, {"title": "B. Dataset Construction", "content": "Setting the overlap rate of the flight paths above 70% can ensure that certain contents will not be missed during shooting. However, this will cause the UAV to capture similar areas during the data collection task, resulting in the situation where the same walnut tree appears in multiple aerial images. In order to avoid a large amount of duplicate content in the final dataset, we organized multiple members to carefully screen the aerial images of each walnut sample plot at the same time, so as to achieve the situation where there are almost no overlapping areas in the selected images.\nSince the resolution of the UAV aerial images (8,192\u00d75,460 pixels) is too large, which is not conducive to the training of the model, in this study, the selected original images were all cut with a step size of 512. The resolution of the cut images is 1,024\u00d71,024 pixels. After the processing of the above steps, the dataset of this study was finally formed, with a total of 30,240 images."}, {"title": "C. Data Annotation", "content": "In this study, four label categories were defined: A1 (frontal light without occlusion), A2 (backlight without occlusion), B1 (frontal light with occlusion), and B2 (backlight with occlusion). The Labelme annotation tool was used to manually annotate the dataset, and the annotation format is bounding box. During this work process, we organized multiple mem- bers to spend about 3 months on data annotation, and finally obtained 24,673 labels."}, {"title": "D. Dataset Split", "content": "According to the way accepted by the current mainstream object detection models, we divided the dataset into a Train, a Val, and a Test. The ratio of the Train, the Val, and the Test is 7:2:1, with 21,167 images, 6,048 images, and 3,025 images respectively. In addition, in the arrangement of the distribution of the number of categories, we tried our best to ensure the similarity and balance of the distribution. The distribution information of category instances after the dataset partition is shown in Fig. 2. We will release the Train and the Val containing label annotations. At the same time, the Test will also be provided to researchers for evaluating their own models, but the label annotations of the Test images will not be provided."}, {"title": "E. Dataset Analysis", "content": "We have counted the number of instances and the average number of instances in WalnutData (Table IV). The average number of targets per image in the Training, the Val, and the Test is approximately 23.353.\nWe analyzed the lighting conditions of the green walnut fruits in WalnutData. Since the lighting conditions of the non-target backgrounds around the green walnut fruits are almost similar, we first extracted the pixels of the images within the instance rectangular boxes. Then, we converted the RGB images into grayscale images and calculated the average grayscale value to analyze the lighting intensity received by the green walnut fruits. The distribution of the average grayscale values of each instance in the Train, the Val, and the Test is shown in Fig. 3. The average grayscale values of the Train, the Val, and the Test are 107.316, 108.048, and 107.544 respectively. The proportions of values lower than the intermediate grayscale value of 127.5 are 76.31%, 75.59%, and 75.81% respectively. This indicates that most of the green walnuts in WalnutData are in backlight conditions or are shaded by leaves in relatively dark places.\nIn addition, according to the definition of large (pixel>96), medium (96>pixel>32), and small (32>pixel) targets in the COCO dataset [20], we counted the quantity distribution of large, medium, and small targets in WalnutData (Table V). In WalnutData, the proportion of medium and small targets is higher, which is in line with the morphological characteristics of green walnut fruits from the perspective of a UAV. There- fore, the model trained on WalnutData can better adapt to the distribution of target sizes in practical application scenarios."}, {"title": "IV. EXPERIMENTAL EVALUATION", "content": "We evaluated some of the more popular object detection models in recent years on WalnutData, and implemented one- stage and two-stage object detection algorithms using the ul- tralytics framework [14] and the mmdetection framework [23] respectively. The one-stage object detection algorithms include YOLOv3 [25], YOLOv4 [4], YOLOv5 [14], DETR [7], etc.; the two-stage object detection algorithms include Fast R- CNN [11], Faster R-CNN [26], TridentNet [18], etc. In the following content, the evaluation results of each algorithm on instances of various categories and sizes in WalnutData will be announced. All the experiments of the models in this study were carried out on servers equipped with 8 RTX 3090 GPUs or A800 GPUs, and the hyperparameters of the baseline models all used the default parameter values. In addition, the evaluation results of these baseline models will be provided as benchmark values of WalnutData for researchers as a reference."}, {"title": "A. Baselines of One-Stage Object Detection Algorithms", "content": "We conducted benchmark evaluations on WalnutData using YOLOv3 [25], YOLOv4 [4], YOLOv5 [14], YOLOv6 [17, 32, 14, 33, 31] to YOLOv11 [15], as well as YOLOX [10], DETR [7], Deformable DETR [44], DINO [40], and Condi- tional DETR [22]. From the evaluation results on the val- idation set of WalnutData (Table VI), it can be seen that YOLOv3 (154.6) and YOLOv3-SPP (155.4) with relatively high GFLOPs rank first and second in terms of the mAP50 metric. However, in terms of the mAP50:95 metric, YOLOv8x (72.7%) and YOLOv10x (72.6%) have better detection perfor- mance. Under the mmdetection framework, we used AP50:95, AP50, and AP75 as evaluation metrics. Among the evaluation metric results of these models (Table VII), YOLOX shows the strongest performance.\nIn Table VIII and Table IX, we will present the detection accuracies of these one-stage object detection algorithms for each category and for small, medium, and large-sized targets. In the evaluation results of the benchmark algorithms in Table VIII, YOLOv3-SPP performs the best in terms of the mAP50 metric. In addition, in the evaluation results of the mAP50:95 metric, YOLOv8x achieves 77.0%, 73.1%, 68.8%, and 71.8% for the A1, B1, B2, and A2 categories respectively, and its overall strength is the highest among other algorithms. Moreover, as can be seen from Table IX, in the detection of small and medium-sized targets, YOLOX-x has the highest AP values, reaching 53.5% and 56.3% respectively, followed by YOLOX-1 (50.1%, 53.3%) and Deformable DETR (43.7%, 55.4%). In the detection of large-sized targets, compared with other algorithms, Deformable DETR has a huge advantage in detection performance, and the corresponding AR metric (72.5%) also ranks second."}, {"title": "B. Baselines of two-stage object detection algorithms", "content": "This study uses several popular two-stage object detec- tion algorithms in recent years: Fast R-CNN [11], Faster R-CNN [26], Cascade R-CNN [6], Grid R-CNN [21], Tri- dentNet [18], Double Head R-CNN [37], and Sparse R- CNN [29], and evaluates these algorithms on WalnutData. As shown in Table X, in the evaluation of two-stage object detection algorithms, Cascade R-CNN ranks first in overall performance, with the best detection results for small objects (52.7%) and medium-sized objects (65.7%). Grid R-CNN ranks second, with slightly lower detection performance for small and medium-sized objects compared to Cascade R-CNN, achieving 51.2% and 64.9%, respectively. However, Grid R- CNN outperforms Cascade R-CNN in detecting large objects."}, {"title": "V. CONCLUSION", "content": "This research aims to address the computer vision chal- lenges of walnut fruit detection from a drone perspective, such as the impacts of lighting variations and occlusions on the algorithms. To this end, we have constructed a fine-grained agricultural drone dataset for walnut detection, which is the first large-scale dataset in the field of smart walnut farming. The dataset's scale and fine-grained feature segmentation give it significant research value and engineering application po- tential in the field of agricultural computer vision.In addition, by conducting benchmark evaluations of WalnutData using a series of mainstream object detection models, we hope to drive the development of precision agriculture and the smart walnut sector.\nIn the future, research based on WalnutData can further advance the application of automated harvesting robots and precision management systems in smart agriculture. By op- timizing existing object detection algorithms and integrating more agricultural data, more efficient and accurate crop mon- itoring and yield prediction can be achieved."}]}