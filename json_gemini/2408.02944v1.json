{"title": "LLM-Empowered Resource Allocation in Wireless Communications Systems", "authors": ["Woongsup Lee", "Jeonghun Park"], "abstract": "The recent success of large language models (LLMs) has spurred their application in various fields. In particular, there have been efforts to integrate LLMs into various aspects of wireless communication systems. The use of LLMs in wireless communication systems has the potential to realize artificial general intelligence (AGI)-enabled wireless networks. In this paper, we investigate an LLM-based resource allocation scheme for wireless communication systems. Specifically, we formulate a simple resource allocation problem involving two transmit pairs and develop an LLM-based resource allocation approach that aims to maximize either energy efficiency or spectral efficiency. Additionally, we consider the joint use of low-complexity resource allocation techniques to compensate for the reliability shortcomings of the LLM-based scheme. After confirming the applicability and feasibility of LLM-based resource allocation, we address several key technical challenges that remain in applying LLMs in practice.", "sections": [{"title": "I. INTRODUCTION", "content": "In wireless communication systems, the allocation of resources such as transmit power, bandwidth, or beamforming is of utmost importance because the openness nature of wireless medium causes interference to neighboring nodes [1]. Recently, the number of transmitting nodes has increased, while the required communication objectives have become more diverse and stringent. At the same time, more complicated system models and stringent constraints, such as extremely low computation time, are being considered. These factors complicate resource allocation, spurring extensive research in this field.\nSince finding the optimal resource allocation strategy is typically formulated as an optimization problem, analytical optimization frameworks such as convex optimization have been widely applied [2]. However, given that the Shannon capacity formula is nonconvex with respect to transmit power and many control variables, such as channel selection, are discrete, finding the optimal resource allocation is challenging.\nTo address these issues, novel mathematical approaches, such as game-theoretic frameworks or convex relaxation techniques, have been developed [2]. Despite these advances, most analytical approaches face limitations in efficiently managing large numbers of nodes at low computational times and struggle to adapt to dynamically changing wireless environments.\nThe deep learning (DL)-based resource allocation has received considerable attention to address the above-mentioned challenges [3], [4]. In DL-based schemes, the optimal resource allocation strategy is approximated using specially structured deep neural network (DNN) models. Since DNNs primarily perform simple matrix operations, once properly trained, they can achieve near-optimal performance with significantly low computation times, on the order of a few milliseconds. Despite these advantages, the design and training of DNNs is task-specific, requiring unique structures and training for each resource allocation task. This limits the applicability of the DL-based approach.\nVery recently, the large language model (LLM), which is the foundational model built on natural language understanding, has achieved significant success in various domains, exemplified by technologies such as OpenAI's ChatGPT and Google's Bard. These models are considered game changers in the next wave of artificial intelligence (AI). LLM-based technology can also open new dimensions in wireless network design and operation, potentially leading to much higher performance [5]. In particular, the multimodal data relevant to wireless communication systems, such as radio frequency signals and visual representations of the wireless environment, can be properly exploited in LLM-based approaches, which enables situational and temporal awareness and predictability, thus facilitating proactive control of wireless communication systems.\nThanks to the human-like reasoning and inference capabilities of LLM, efforts have been made to use them to solve mathematical problems, and it has been demonstrated that LLMs can indeed be effective in this domain. Consequently, it is promising to employ the LLMs to address various optimization problems in wireless communication systems, such as finding optimal resource allocation strategies. Compared to the conventional DL-based approaches, LLM-based approaches offer the significant advantage of providing reasonable outputs without the need for designing task-specific model design and training, since LLMs are already pre-trained on extensive datasets. Thus, the LLM can be considered as a general-purpose solver capable of finding optimal solutions to a wide range of optimization problems, making it easily adaptable to different environments.\nIn this article, we investigate the feasibility of LLM-based resource allocation in wireless communication systems. We begin by describing the basic principles of LLMs and exploring how they can be leveraged to determine resource allocation strategies. Next, we provide an illustrative example of a simple resource allocation problem that aims to maximize either spectral efficiency (SE) or energy efficiency (EE), and design an LLM-based resource allocation framework. In addition, we also propose a hybrid resource allocation strategy that"}, {"title": "II. BASIC PRINCIPLE OF LLM AND ITS APPLICATION IN RESOURCE ALLOCATION", "content": "In this section, we describe the basic principles of LLM. Then, we turn our attention to the feasibility and benefits of using LLMs for resource allocation.\n\nA. Principle of LLM\nA DNN, trained on extremely large amounts of unlabeled data using self-supervision techniques to adapt to a wide range of tasks, is referred to as a foundation model. The foundation model is designed to be highly generalizable and can be fine-tuned for specific applications, making it a versatile and powerful base for a wide range of tasks. Notably, the foundation models typically have deep architectures with many layers and a large number of parameters (e.g., 175 billion parameters for GPT-3), allowing them to capture complex relationships in data and perform complicated tasks that require logical reasoning and understanding of the data. The LLM is a representative example of a foundation model, trained on large amounts of text data. Accordingly, the LLM generates text and performs text-based tasks based on given prompts, which are natural language descriptions of the tasks.\nLLM is primarily based on the Transformer architecture, which has become the mainstream framework for modern language models. The Transformer contains two main innovations: the attention mechanism and positional encoding [6]. The attention mechanism computes the relevance of each word in a sentence relative to other words, allowing the model to capture dependencies between words regardless of their distance in the text. This mechanism is enhanced by multi-head attention, which enables the model to focus on different parts of the input sequence simultaneously, capturing different contextual aspects. In addition, positional encoding is added to the input embeddings to encode information about the position of each word in the sequence, ensuring that the order and relative positioning of words are taken into account.\nMost well-known LLMs are pre-trained on billions to trillions of tokens over millions of iterations. Consequently, fine-tuning with task-specific labeled datasets is often employed to further adapt the LLM to specific tasks [7]. However, given the enormous number of trainable parameters, the computational overhead for fine-tuning can be substantial and costly. Given the reasoning capabilities of the LLM, few-shot or zero-shot learning approaches, where only a few concrete examples of a task are provided, or no examples at all, can be effective. Despite the minimal input, the LLM's inductive reasoning capabilities allow it to produce reasonable results. For example, simply adding \"Let's think step by step\" to the prompt can guide the LLM to follow a human-like reasoning process, leading to better results [7].\nGiven their importance, leading AI companies have developed their own LLMs. Among the best known are GPT, developed by OpenAI; and LLaMA, developed by Meta [8].\n\u2022 GPT: The GPT, or Generative Pre-trained Transformers, are decoder-only Transformer-based language models developed by OpenAI. The early models, such as GPT-1 and GPT-2, are open source, while more recent models, such as GPT-3 and GPT-4, are closed source. Although the basic GPT model is trained on general language data, specific versions such as InstructGPT and ChatGPT are trained for specific tasks. These models use reinforcement learning from human feedback to follow instructions in a more human-like manner [8].\n\u2022 LLaMA: LLaMA, or Large Language Model Meta AI, is a collection of basic language models published by Meta. A notable aspect of LLaMA is that, unlike GPT, it is open source, meaning that model weights are made available. In addition, the size of LLaMA models is relatively small compared to GPT-based models, ranging from 7 billion to 65 billion parameters. As a result, LLaMA has been widely used in the research community and in the development of task-specific LLMs for mission-critical applications.\nB. Conventional Resource Allocation Strategies\nIn resource allocation for wireless communication systems, parameters such as transmit power are adjusted to optimize network performance, such as maximizing SE, while satisfying constraints. Finding the optimal strategy is typically formulated as an optimization problem, which can be addressed by using either conventional optimization-based approaches or DL-based approaches as follows [9]:\n\u2022 Optimization-based approach : The formulated optimization problem is tackled analytically. In general, the optimization problem is often complicated and hard to solve, involving aspects such as integer-valued control parameters and non-convex functions. As a result, a variety of mathematical approaches are typically used to transform and simplify the problem, to make it more tractable. The optimal solution is then often obtained through iterative methods.\n\u2022 DL-based approach: The optimal resource allocation is approximated using the output of a specially designed DNN, which provides the resource allocation strategy based on the current wireless network conditions. To achieve this, the DNN must be trained, which can be done either through supervised learning exploiting optimal resource allocation as labeled data, or through unsupervised learning without such labeled data. Although training the DNN can take a considerable amount of time, inference from the trained DNN can typically be performed in the order of a few milliseconds.\nAlthough resource allocation has been extensively investigated in the literature, conventional methods face significant limitations, especially with respect to multimodality and flexibility. In particular, resource allocation in future wireless"}, {"title": "III. LLM-BASED RESOURCE ALLOCATION", "content": "In this section, we illustrate the feasibility of using LLMs for resource allocation. To this end, we first describe a simplified resource allocation problem. We then explain how LLMS can be utilized for the resource allocation and present a scheme designed to compensate for the shortcomings of a purely LLM-based approach. Finally, the feasibility of LLM-based resource allocation is evaluated through simulation.\n\nA. System Model and Considered Resource Allocation\nWe consider a resource allocation strategy for two transmit pairs, which are randomly distributed. Specifically, we assume that two transmitters send data to their respective receivers over the same channel. Let $h_{ij}$ denote the channel gain between transmitter $i$ and receiver $j$, where $i, j \\in \\{1, 2\\}$, and $P_i$ denotes the transmit power of the transmitter $i$, where the transmit power for a transmitter must not exceed the maximum transmit power, $P_T$.\nWe consider two resource allocation strategies to maximize either total SE or EE by properly adjusting the transmit power. The achievable SE of the transmit pair $i$, denoted as $SE_i$, can be expressed as\n$log_2(1 + \\frac{h_{i,i}P_i}{N_0W + \\Sigma_{l \\in \\{1, 2\\}\\\\{i\\}}h_{l,i}P_l})$\nwhere $N_0$ and $W$ are noise power density and bandwidth, respectively. Then the objective for maximizing the total SE can be formulated as $\\Sigma_{i=1}^2 SE_i$. Conversely, the objective for maximizing total EE can be formulated as $\\Sigma_{i=1}^2 \\frac{SE_i}{P_i+P_c}$, where $P_c$ is the constant circuit power of the transmitter. Although we do not explicitly show it in the paper, the strategy for transmit power control behaves differently depending on whether the goal is to maximize SE or EE. In particular, binary transmit power control is generally optimal for maximizing SE, whereas a range of transmit power levels can be used to maximize EE. This distinction will later affect the performance of the LLM-based resource allocation.\nB. Proposed Resource Allocation based on LLM\nIn the proposed LLM-based resource allocation strategy, the transmit power of each transmitter is determined based on the channel gain, $h_{i,j}$. In this approach, the channel gain serves as the input, and the transmit power is the output of the LLM, such that the channel gain is provided as a prompt, and the resource allocation can be derived from the text generated by the LLM. Unlike the conventional DNN-based approach, which requires a customized structure, a generic LLM can be used for resource allocation. The procedure of proposed LLM-based resource allocation is depicted in Fig. 2.\nIn our work, we adopt a few-shot learning-based approach, where the channel gains and the corresponding transmit power strategies are provided as references in the prompt of LLM. Notably, we do not provide a formal description of the optimization objective; instead, we simply provide the channel gain and its corresponding optimal strategy. As a result, our framework can be applied to other optimization problems as well. In the generation of prompts for the LLM, we normalize the channel gain to have zero mean and unit variance, then multiply it by 100. This value is subsequently rounded to an integer. Similarly, the transmit power is normalized by its maximum value, i.e., $P_T$, and multiplied by 100, and is rounded to an integer. This preprocessing of numerical values is crucial because the LLM recognizes numeric values as strings, making it important to simplify these values for efficiency. Specifically, the original channel gain is typically on the order of $10^{-9}$, which is inefficient to express in string format. Additionally, given that the number of inputs to the LLM (i.e., the number of tokens) is limited, it is desirable to reduce the number of characters used to express each numeric value. For example, the formulated prompt to maximize the total SE can be illustrated as follows:\n\"If A is -29, 30, 128, -26, then B is 0, 100. If A is -31, -31, -19, -31, then B is 100, 0... If A is 51, -32, -22, -19, then B is\"\nIn the illustrative example above, the channel gain is de-noted as A in the order of $h_{1,1}, h_{1,2}, h_{2,1}, h_{2,2}$, where $h_{i,j}$ is the preprocessed channel gain $h_{i,j}$. On the other hand, the transmit power aimed at maximizing the total SE, is denoted as B in the order of $P_1, P_2$, where $P_i$ represents the preprocessed transmit power. In the considered few-shot learning approach, the training data includes the values of both A and B. However, for the channel gain in the last line, where we need to determine the transmit power, the value of B is omitted. Since the LLM is designed to generate text that"}, {"title": "IV. RESEARCH CHALLENGES", "content": "In the following, we discuss the research challenges associated with the future use of LLMs for resource allocation.\n\nA. Latency and Computation Time\nIn general, the size of LLMs is extremely large due to their large number of parameters. GPT-4, for example, has 1.7 trillion parameters. As a result, running LLMs requires significant computation resources, often necessitating the use of cloud-based services, which can introduce long latency. In addition, due to the size of LLMs, the computation time required can be large. To address the issues of long latency and computation time, the development of small LLMs (sLLMs) tailored for resource allocation is essential. These sLLMs, combined with edge AI technology, can distribute the computational load across distributed nodes, thereby reducing latency and improving efficiency.\nB. Optimized LLM Architecture\nTo achieve higher performance, it is necessary to optimize the LLM architecture, including the proper selection of a pre-trained LLM model, which requires extensive performance"}, {"title": "V. CONCLUSIONS", "content": "In this article, we discussed the application of LLM for re-source allocation. Unlike conventional DL-based approaches, the LLM-based approach eliminates the need to build and train dedicated DNNs for specific tasks. We considered a simple resource allocation problem and demonstrated the ability of LLMs for the resource allocation strategy. To address the drawbacks of the LLM-based scheme, we also devised a joint consideration of conventional resource allocation strategies. Through performance evaluation, we confirmed that the LLM-based resource allocation can achieve near-optimal performance and investigated its characteristics. In addition, we outlined challenges associated with the application of LLMs for resource allocation."}]}