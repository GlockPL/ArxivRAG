{"title": "ON-THE-FLY PREFERENCE ALIGNMENT VIA PRINCIPLE-GUIDED DECODING", "authors": ["Mingye Zhu", "Yi Liu", "Lei Zhang", "Junbo Guo", "Zhendong Mao"], "abstract": "With the rapidly expanding landscape of large language models, aligning model\ngenerations with human values and preferences is becoming increasingly impor-\ntant. Popular alignment methods, such as Reinforcement Learning from Human\nFeedback, have shown significant success in guiding models with greater control.\nHowever, these methods require considerable computational resources, which is\ninefficient, and substantial collection of training data to accommodate the diverse\nand pluralistic nature of human preferences, which is impractical. These lim-\nitations significantly constrain the scope and efficacy of both task-specific and\ngeneral preference alignment methods. In this work, we introduce On-the-fly\nPreference Alignment via Principle-Guided Decoding (OPAD) to directly align\nmodel outputs with human preferences during inference, eliminating the need\nfor fine-tuning. Our approach involves first curating a surrogate solution to an\notherwise infeasible optimization problem and then designing a principle-guided\nreward function based on this surrogate. The final aligned policy is derived by\nmaximizing this customized reward, which exploits the discrepancy between the\nconstrained policy and its unconstrained counterpart. OPAD directly modifies\nthe model's predictions during inference, ensuring principle adherence without\nincurring the computational overhead of retraining or fine-tuning. Experiments\nshow that OPAD achieves competitive or superior performance in both general\nand personalized alignment tasks, demonstrating its efficiency and effectiveness\ncompared to state-of-the-art baselines.", "sections": [{"title": "1 INTRODUCTION", "content": "As tremendous strides have been made in the\ndevelopment of large language models (LLMs),\nit remains challenging to align these models\nwith specific principles, such as ethical guide-\nlines or factual consistency, during genera-\ntion. Popular alignment methods focus pri-\nmarily on training-time optimization, such as\nReinforcement Learning from Human Feed-\nback (RLHF) (Ouyang et al., 2022; Stiennon\net al., 2020) and Direct Preference Optimiza-\ntion (DPO) (Rafailov et al., 2023). While these\ntechniques significantly improve the alignment\nof model outputs, they still face certain limita-\ntions (Lin et al., 2023). RLHF, for instance, is\nsensitive to hyperparameters and is complex to\ntrain (Casper et al., 2023). DPO, on the other"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1\nTRADITIONAL PREFERENCE ALIGNMENT", "content": "Among the wide range of algorithms proposed for preference alignment (Stiennon et al., 2020;\nYuan et al., 2023; Rafailov et al., 2023; Zhu et al., 2024a), the most prominent are perhaps RLHF\nand DPO. Both methods rely on human feedback to fine-tune model generations and align them with\nhuman preferences. RLHF follows a three-step process: first, supervised fine-tuning (SFT) is applied\nto the initial model; second, a reward model is trained to reflect human preferences; and finally,\nReinforcement Learning (RL) techniques such as Proximal Policy Optimization (PPO) (Schulman\net al., 2017) are used to optimize the policy based on the reward model. DPO simplifies the RLHF\nprocess by introducing a new parameterization of the reward model, reducing the problem to a\nsimple classification loss, which makes DPO easier and more stable to train. However, despite\nthese improvements, both RLHF and DPO still require substantial amounts of annotated data and\nsignificant computational resources, posing limitations for their practical application, which motives\nanother line of research that focuses on tuning-free alignment."}, {"title": "2.2 TUNING-FREE ALIGNMENT", "content": "Currently, alignment methods are underscoring a shift toward flexible, decoding-time techniques\nthat adapt LLM outputs to diverse human preferences and ethical standards. One popular technique\nis In-Context Learning (ICL), which adapts models to new information or tasks by using a few\ninstruction-output examples in the prompt. Another powerful inference-time algorithm is Best-of-\nN (BON), which involves generating N different samples from the model and selecting the best\none, y*, based on a predefined evaluation criterion S(yi). However, this method is inefficient,\nas generation must be executed N times, prompting the search for more efficient inference-time\napproaches.\nRecently, we have also seen the emergence of more advanced methods. Lin et al. (2023) discov-\nered that token distribution shifts between aligned and unaligned policies diminish over time during\ndecoding. Li et al. (2023b) introduced a self-evaluation and rewind mechanism that directly aligns\nbase LLMs with human preferences via self-boosting. Huang et al. (2024) proposed DeAL, a frame-\nwork that enables decoding-time alignment through a heuristic-guided search process and leverages\nprogrammatic constraints as well as abstract objectives to achieve alignment. (Shi et al., 2024b) in-\ntroduced Multi-Objective Decoding, a method that combines predictions from multiple base models\nto achieve adaptable alignment during decoding. DARWIN (Hung et al., 2024) proposes to strike the\nbalance between exploration and exploitation of rewards during decoding with evolutionary heuris-\ntics. Additionally, Zhu et al. (2024b) focused on alignment with personal traits and developed an\nactivation intervention optimization method to align with individual behavioral preferences. Lately,\nLinear Alignment (LA) (Gao et al., 2024) was proposed as a method for aligning language models\nwith human preferences in a single inference step. This approach relies on a novel parameterization\nfor policy optimization under divergence constraints and estimates the preference direction using\nself-contrastive decoding. Despite the significant progress and achievements made, there are still\nmany gaps to be filled in this field. Therefore, in this work, we propose OPAD to further improve\nthe efficiency of inference-time alignment."}, {"title": "3 METHODOLOGY", "content": "Before delving into this section, it is essential to highlight two foundational tenets underlying the\nproposed method. 1. The model itself already preserves enough knowledge or capability to answer\nthe request. 2. Even with instructions to follow certain principles, the model can only partially com-\nply with or even still fails the request. Otherwise, training is ultimately needed to encode necessary\nknowledge."}, {"title": "3.1 PROBLEM FORMULATION", "content": "We begin by specifying the notations and formally framing the problem. Given a query x and\nprinciple c, the simplest approach is to directly prompt the model to generate under the guidance\nof principle c. For an autoregressive language model, token prediction probabilities conditioned on\ninput x and principle care denoted as $P_{ng} (yt|x, c) \\in \\mathbb{R}^{L \\times V}$, where L is the sequence length and V\nthe vocabulary size. The probability of generating a sentence y with T tokens takes the following\nform:\n$\\pi_{\\theta} (y|x, c) = \\prod_{t=1}^{T} P_{\\pi_{\\theta}} (y_t|x, C, y<t).$\n(1)\nHowever, the model often struggles to align with principle c through direct prompting alone. There-\nfore, the core objective of this work is to devise a strategy for dynamically modifying next-token\npredictions during inference, thereby enforcing adherence to the principle."}, {"title": "3.2 PRINCIPLE-ALIGNED DIVERGENCE REWARD", "content": "RLHF for preference alignment typically begins by formulating an optimization problem that max-\nimizes rewards, which reflect the quality of model outputs during training. However, since our ap-\nproach focuses solely on inference time, we must reconceptualize the original optimization problem\nand adapt it into a form that can be addressed within this context. Please note that in the following\nsection, we use principles and constraints interchangeably.\nProposition 1 Suppose we have a target principle c, maximizing the KL divergence between the\nconstrained policy $P_c$ and the unconstrained policy P serves as a surrogate for minimizing the KL\ndivergence between the true data distribution $P_{data}$ and $P_c$:\n$P_{opt} = arg \\max_{P_c} D_{KL}(P_c||P),$\n(2)\nunder the following conditions:\n1. The unconstrained policy P is a poor approximation of the true preference data distribution\n$P_{data}$;\n2. The constraint c aligns well with the data distribution $P_{data}$;\n3. The unconstrained policy P has broader support than the constrained policy $P_c$.\nProof Sketch: Our objective is to minimize the KL divergence $D_{KL}(P_{data}||P_c)$, thereby aligning\nthe constrained policy $P_e$ with the true data distribution $P_{data}$. Direct optimization is infeasible\nsince we have no access to the training data. Instead, we propose maximizing the KL divergence\n$D_{KL} (P_c||P)$ under the given conditions. Please find a more detailed proof in Appendix B.\nBy maximizing $D_{KL}(P_c||P)$ under these conditions, we indirectly promote policies that respect the\nprinciples encoded in c and ensure that $P_e$ is distinct from a potentially suboptimal P.\nNext, we write the KL term between $P_c$ and P as the expectation of the log ratio between model\npredictions over the constrained distribution in T time steps:\n$D_{KL}(\\pi_{\\theta}(y|x, c) || \\pi_{\\theta}(y|x)) = E_{\\pi_{\\theta}(y|x,c)} \\left[ \\sum_{t=1}^{T} log \\frac{\\pi_{\\theta}(y_{t}|x, C, y<t)}{\\pi_{\\theta}(y_{t}|x, y<t)} \\right]$\n$=\\sum_{t=1}^{T} E_{\\pi_{\\theta}(y|x,c)} \\left[ log \\frac{\\pi_{\\theta}(y_{t}|x, C, y<t)}{\\pi_{\\theta}(y_{t}|x, y<t)} \\right]$\n(3)"}, {"title": "Reward design via sequential divergence. Formally, we define the reward as:", "content": "$\\Upsilon_{\\pi_{\\theta}} (x, y<t, C) = \\sum_{t'=t-1}^{t} log \\frac{\\pi_{\\theta} (y_{t'}|x, C, y<t')}{\\pi_{\\theta}(y_{t'}|x, y<t')}$\n(4)\nOur motivation is that sequential models exhibit dependencies across time steps, and including both\nt-1 and t in the reward function captures the contributions of consecutive time steps, while still con-\ncentrating on the current decoding step. This helps comprehend temporal dynamics and propagate\ndivergence, aligning the per-step reward with the sequence-level KL divergence.\nDifference in reward mechanism. In traditional RLHF, the reward function serves to quantize\nthe quality of model responses based on human feedback, often guiding long-term policy updates\nthrough reinforcement learning. However, in our approach, the reward design plays a different role:\nrather than quantifying sequences, it is designed to modulate token-wise model predictions, which\ncan be perceived as a token-level assessment of the adherence to the guiding principle at each step\nof token generation."}, {"title": "3.3 PRINCIPLE-GUIDED INFERENCE-TIME ALIGNMENT", "content": "Deriving the optimal solution via reward maximization. Next we denote the final principle-\nguided policy as $p_{\\theta}$ and consider the following optimization problem:\n$\\max_{p_{\\theta}} E_{p_e} [\\Upsilon_{\\pi_{\\theta}}(x, y<t, c)] - \\beta D_{KL}(P_e(y|x, c) || \\pi_{\\theta}(y|x, c))$,\n(5)\nwhere $\\beta$ is a hyperparameter balancing the reward and the divergence from the base policy\n$\\pi_{\\theta} (y|x, c)$. The solution to this optimization at time step t yields (please find derivation in Ap-\npendix C):\n$P_{\\theta} (y_{t} x, c, y<t) = \\frac{1}{Z(x, c, y<t)} \\pi_{\\theta} (y_{t}|x, c, y<t) exp(\\frac{1}{\\beta} \\Upsilon_{\\pi_{\\theta}} (x, y<t, c)),$\n(6)\nwhere $Z(x, c, y<t) = \\sum_{y} \\pi_{\\theta}(y|x, c, y'<t) exp(\\Upsilon_{\\pi_{\\theta}}(x, y'<t,c))$ is the partition function. It is\nimportant to note that the reward function in Equation 6 operates entirely within the probability\nspace, so the partition function computation does not require explicit decoding of tokens or summing\nover all sequences, which makes it tractable."}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 EXPERIMENTAL SETTINGS", "content": "Datasets. To comprehensively evaluate the effect of the proposed OPAD, we focus on general\nalignment and personalized alignment tasks. For general alignment, we use two widely employed"}, {"title": "4.2 GENERAL ALIGNMENT RESULTS", "content": "Performance analysis on general alignment. Both base models (Vicuna-7B-v1.5 and Mistral-\n7B-Instruct) are fined-tuned on instruction datasets that involve user interactions, and the model's\nability to follow instructions inherently leads to better alignment with general preferences such as\nhelpfulness and safety (Jiang et al., 2023; Zheng et al., 2023), suggesting that the unconstrained\ndistribution P is not necessary a poor approximation of the ground truth data (condition 2 in Propo-\nsition 1). However, we can carefully curate the principle c to better illustrate what this universal\npreference (such as in common dialog and summarization tasks) means, thus granting the model a\nbetter understanding of these universal principles. This makes OPAD stand out compared to most\nof the baselines. Please find more case studies in Appendix H. Notably, the performance of BoN is\nheavily dependent on the quality of the reward model. In our experiments, we use the DeBERTa-\nlarge reward model, which has been thoroughly trained on data sample pairs from both HH-RLHF\nand Summarization tasks, leading to its strong performance."}, {"title": "4.3 PERSONALIZED ALIGNMENT RESULTS", "content": "OPAD effectively handles out-of-distribution tasks in personalized alignment. When the uncon-\nstrained policy P is trained on generic, domain-agnostic data, it may poorly approximate the real\ndata distribution $P_{data}$ especially if the latter belongs to a specific domain or personalized preference.\nIn contrast to general alignment, where universal preferences are implicitly incorporated during\nthe instruction-tuning phase, personalized alignment tasks better showcase the flexibility and effi-\nciency of OPAD in catering to user-specific requests. As illustrated in Figure 3, OPAD consistently\noutperforms baseline methods across various models and tasks. Notably, unlike the comparable per-\nformance with LA on the HH-RLHF dataset, OPAD achieves superior results. Two possible reasons\ncontribute to this advantage: (1) the out-of-distribution principle prompts may lead to less accu-\nrate gradient estimation for LA, and (2) the poor approximation of P to $P_{data}$ is more effective in\nboosting OPAD's capabilities. Please find some representative samples in Appendix H."}, {"title": "4.4 FURTHER ANALYSIS AND DISCUSSION", "content": "The scaling effects of OPAD. We evaluate OPAD on models of varying sizes, including\nPythia-2.8B, Vicuna-13B, and Vicuna-33B, across general and personalized alignment tasks"}, {"title": "5 CONCLUSION", "content": "In this work, we propose OPAD, a simple yet effective framework for aligning model outputs with\ntarget principles during inference, without fine-tuning.Leveraging a principle-guided reward mecha-\nnism and maximizing this reward under KL divergence constraints, our approach enables on-the-fly\nadjustments to the model's predictions.\nLimitations and future work: While OPAD demonstrates promising results, several limitations re-\nmain. First, the current reward design relies heavily on KL divergence, which may fail to capture the\nnuances of alignment when the constrained and unconstrained policy has few overlaps. Moreover,\nOPAD's strict adherence to principles may sometimes lead to overfitting, resulting in formulaic or\nrigid outputs. Balancing principle adherence with creativity and flexibility in ambiguous contexts\nremains an open challenge that future work should address."}, {"title": "6 ETHICS STATEMENT", "content": "With the increasing capabilities of LLMs, the risks of generating untruthful, biased, or harmful con-\ntent are also amplified, which could result in significant negative impacts. To mitigate these risks and\nensure that model outputs align with human values and intentions, it is essential to develop robust\ntechniques that promote ethical behavior in AI systems. Extensive research has been dedicated to de-\nsigning ethical frameworks, addressing various aspects from data collection and algorithm design to\nmodel deployment and application. We hope that our contribution in this area helps to make LLMs\nmore secure, transparent, and aligned with human interests, ensuring safer and more controllable\ninteractions."}, {"title": "A RELATION WITH THE RESIDUAL EBMS", "content": "Residual energy-based models (EBMs) (Parshakova et al., 2019; Deng et al., 2020) combine glob-\nally normalized EBMs with more tractable, locally normalized autoregressive language models. In\nessence, they refine a base distribution using an energy-based adjustment, which typically captures\ndependencies not accounted for by the base model alone. The general form of a residual EBM is:\n$P(y| x) = \\frac{1}{Z(x)} P_{LM}(y|x) exp(-E(x,y))$,\n(7)\nwhere Z(x) is the partition function. It is straightforward to see that the probability distribution\ninduced by OPAD relates mathematically to a residual EBM, by expressing the reward function as a\nnegative energy term. The key differences lie in the following aspects.\nIn residual EBMs, the energy term is trained over the entire sequence, introducing global normal-\nization. One advantage of global normalization is its ability to mitigate exposure bias (Ranzato\net al., 2015), which arises due to discrepancies between teacher-forcing training and autoregressive\ninference. However, global normalization involves summing over all possible sequences y, which\nmakes the partition function in Equation 7 intractable. Therefore, during inference, sampling-based\ntechniques (Grover et al., 2019; Shapiro, 2003) are typically employed to first sample from the base\nmodel and then re-weight or correct the samples using the energy function.\nIn contrast, OPAD operates purely as an inference-time algorithm and generates tokens in an au-\ntoregressive manner. This obviates the need to address exposure bias through global normalization,\nas OPAD inherently aligns token generation with the desired principles without relying on teacher\nforcing. Here, the negative energy term\u2014the reward function\u2014acts as a token-level adjustment fac-\ntor. This design allows for efficient computation of the partition function in Equation 6, enabling a\nlocal normalization process that is both computationally tractable and straightforward to implement.\nThe resemblance of OPAD to residual EBMs endows it with the ability to leverage the strengths of\nthe base distribution while introducing additional flexibility through the residual energy. Specifi-\ncally, while residual EBMs typically involve global adjustments based on the entire energy function,\nOPAD implements token-by-token updates during inference, allowing dynamic and fine-grained\npolicy adjustments."}, {"title": "B DETAILED PROOF OF PROPOSITION 1", "content": "Proof: We want to prove that maximizing $D_{KL}(P_c || P)$ serves as a surrogate for minimizing\n$D_{KL} (P_{data} || P_c)$. Firstly, since the unconstrained policy P poorly approximates $P_{data}e$ (Condi-\ntion 1), the divergence between them is significant. Secondly, because the constraint c aligns well\nwith $P_{data}$ (Condition 2), adhering to c inherently guides $P_e$ closer to $P_{data}$. supp$(P_c)$ $\\subseteq$ supp$(P)$\n(Condition 3) suggests maximizing $D_{KL}(P_c||P)$ allows $P_e$ to concentrate on regions where $P_{data}$\nis significant, effectively filtering out less relevant areas of P's support. This condition also ensures\nthat $D_{KL} (P_e||P)$ is well-defined and valid, ensuring that the surrogate optimization problem is both\nfeasible and meaningful. Therefore, maximizing $D_{KL}(P_c || P)$ serves as a surrogate for minimizing\n$D_{KL} (P_{data} || P_c)$."}, {"title": "C SOLVING THE KL-CONSTRAINED OPTIMIZATION", "content": "In this section, we derive the optimal solution to the KL-constrained optimization problem in Equa-\ntion 5. We aim to solve the following optimization problem:\n$\\max_{P_e} E_{p_e} [\\Upsilon(x, y<t, c)] - \\beta D_{KL}(p_e(y|x, c) || \\pi_{\\theta}(y|x, c))$,\n(8)\nwhere $\\pi_{\\theta}$ is the base policy, $p_{\\theta}$ is the policy to be optimized, and $\\beta$ is a positive scalar balancing the\nreward and the KL divergence.\nAssuming x and c are given and fixed, the objective function can be expressed as:\n$\\max_{P_e} \\sum_{y} p_{\\theta} (y|x, c) \\cdot r(x, y<t, c) - \\beta \\sum_{y} p_{\\theta} (y|x, c) log(\\frac{p_{\\theta} (y|x, c)}{\\pi_{\\theta} (y|x, c)})$,(9)\nTo ensure that $p_{\\theta}$ is a valid probability distribution (i.e., $\\sum_{y}p_{\\theta}(y|x, c) = 1$), we introduce a La-\ngrange multiplier $\\lambda$. For simplicity we omit x and c in the expression. The Lagrangian $\\mathcal{L}$ thus\nbecomes:\n$\\mathcal{L} = \\sum_{y} p_{\\theta} (y) \\cdot r(y) - \\beta \\sum_{y} p_{\\theta} (y) log(\\frac{p_{\\theta}(y)}{\\pi_{\\theta}(y)}) + \\lambda(1-\\sum_{y}p_{\\theta}(y))$ (10)\nTo find the optimal $p_{\\theta}$, take the derivative of $\\mathcal{L}$ with respect to $p_{\\theta}(y)$ and set it to zero:\n$\\frac{d\\mathcal{L}}{dp_{\\theta}(y)} = r(y) - \\beta \\cdot (1+log(\\frac{p_{\\theta}(y)}{\\pi_{\\theta}(y)})) - \\lambda = 0$\n$\\Rightarrow$\nr(y) - \\beta - \\beta log(\\frac{p_{\\theta} (y)}{\\pi_{\\theta}(y)}) - \\lambda = 0\n$\\Rightarrow$\nlog(\\frac{p_{\\theta}(y)}{\\pi_{\\theta}(y)}) = \\frac{r(y) - \\beta - \\lambda}{\\beta}$ (11)\nNext we exponent both sides to solve for $p_{\\theta} (y)$:\n$\\frac{p_{\\theta}(y)}{\\pi_{\\theta}(y)} = exp(\\frac{r(y) - \\beta - \\lambda}{\\beta})$ (12)\n$p_{\\theta}(y) = \\pi_{\\theta}(y) \\cdot exp(\\frac{r(y)}{\\beta} - 1 - \\frac{\\lambda}{\\beta})$ (13)\nFactor out the terms that do not depend on y and recall the property of a probability distribution:\n$\\sum_{y}p_{\\theta}(y) = [exp(-1 - \\frac{\\lambda}{\\beta})] \\sum_{y}\\pi_{\\theta}(y) \\cdot exp(\\frac{r(y)}{\\beta}) = 1$. (14)\nNext we introduce the partition function Z to simplify the notation:\n$Z = \\sum_{y}\\pi_{\\theta}(y) \\cdot exp(\\frac{r(y)}{\\beta})$ (15)"}, {"title": "1.4 DAGGREGATING OVER MULTIPLE TIME STEPS IN REWARD DESIGN", "content": "In this section, we conduct an empirical\nanalysis by varying the number of time\nsteps in the reward function (Equation 4).\nIn sequential generation tasks, the influ-\nence of earlier tokens on the current step\noften diminishes over time, and appropri-\nate weighting is required to accurately re-\nflect the varying levels of importance in\nachieving global alignment. Based on this\nassumption, we introduced a discount fac-\ntor, specifically choosing a discount value\nof 0.6, and aggregated the reward over 4\ntime steps. As evidenced by the win-lose\nratio against PP, aggregating\nover more time steps did not provide additional alignment benefits while incurring higher com-\nputational costs compared to a simple two-step design.\nThe reason may be that the nature of language modelling means that prior information is already\nincorporated when decoding the current token, strictly enforcing a global adjustment can result in\na coarse-grained alignment that is not as precise as a 2-token adjustment. Using two tokens is\nsufficient for effective modeling and demonstrates transferable characteristics across different tasks\nwith less computational cost. Based on this consideration, we chose the two-token design."}, {"title": "1.5 E EFFECT OF DIFFERENT 3 VALUES ON DIFFERENT TASKS", "content": "In Section 4.4, we explore how the policy and reward landscapes evolve as 3 changes in the DSP\ndataset. Here, we extend this analysis to a general alignment task by applying the same methodol-\nogy to the HH-RLHF dataset. As shown in Figure 7, the scaled reward distribution in the general\nalignment dataset appears narrower and steeper compared to the personalized alignment dataset.\nThis results in less deviation for the aligned model with the same 3 value. Moreover, the reward\ndistribution is more zero-centered, indicating that the model has more consistent predictions with\nand without principles, exhibiting lower variability when predicting HH-related values."}, {"title": "1.6 F TASK-SPECIFIED PRINCIPLES", "content": "In this section, we give the principles for each task. For HH-RLHF and Summarization, the principle\naims to explain the general human preferences in detail (e.g., helpfulness and harmlessness) to better\nguide model generation. Specifically:\nFor HH-RLHF:"}]}