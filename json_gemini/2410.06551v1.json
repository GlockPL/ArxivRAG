{"title": "INSTANTIR: BLIND IMAGE RESTORATION WITH INSTANT GENERATIVE REFERENCE", "authors": ["Jen-Yuan Huang", "Haofan Wang", "Hao Ai", "Peng Xing", "Qixun Wang", "Xu Bai", "Jen-Tse Huang"], "abstract": "Handling test-time unknown degradation is the major challenge in Blind Image Restoration (BIR), necessitating high model generalization. An effective strategy is to incorporate prior knowledge, either from human input or generative model. In this paper, we introduce Instant-reference Image Restoration (INSTANTIR), a novel diffusion-based BIR method which dynamically adjusts generation condition during inference. We first extract a compact representation of the input via a pre-trained vision encoder. At each generation step, this representation is used to decode current diffusion latent and instantiate it in the generative prior. The degraded image is then encoded with this reference, providing robust generation condition. We observe the variance of generative references fluctuate with degradation intensity, which we further leverage as an indicator for developing a sampling algorithm adaptive to input quality. Extensive experiments demonstrate INSTANTIR achieves state-of-the-art performance and offering outstanding visual quality.", "sections": [{"title": "1 INTRODUCTION", "content": "Image restoration seeks to recover High-Quality (HQ) visual details from Low-Quality (LQ) images. This technology has a wide range of important applications. It can enhance social media contents to improve user experience (Chao et al., 2023). It also functions at the heart in industries like autonomous driving (Patil et al., 2023) and robotics (Porav et al., 2019) by improving adaptability in diverse environments, as well as assists object detector in adverse conditions (Sun et al., 2022).\nImage restoration remains a long-standing challenge extending beyond its practical application. The information loss during degradation makes a single LQ image corresponds to multiple plausible restorations. This ill-posed problem is further exacerbated in Blind Image Restoration (BIR), where models are tested under unknown degradation. A common strategy is to leverage prior knowledge. Reference-IR models use other HQ images to modulate LQ features, requiring additional inputs with similar contents but richer visual details (Lu et al., 2021). Generative approaches, on the other hand, directly learn the HQ image distribution. The input is first encoded into a hidden variables z, which servers as the generation condition to sample HQ image from the learned distribution p(yz). Although generative methods achieve single-image restoration, they are prone to hallucinations that produce artifacts in restoration (Yang et al., 2020). This happens when the encoder fails to retrieve accurate hidden variable due to the input distribution shift in degradation. Existing methods improve robustness by training on more diverse synthetic degradation data or introduce discrete feature codebook. We argue that these are only shot-term solutions. Alternative methods are pendding to be explored to better address unknown inputs in BIR.\nIn this paper, we present INSTANTIR, a dynamic restoration pipeline that iteratively refines generation condition using a pre-trained Diffusion Probabilistic Model (DPM). INSTANTIR employs two complementary way for processing input LQ image. First, a pre-trained vision encoder extracts compact representation from degraded content. The encoder's high compression rate enhances the robustness in the extracted representation, while retaining only high-level semantics and structural information. Next, we introduce the Previewer module, a distilled DPM capable of one-step generation. At each generation step, the previewer decodes current diffusion latent using the compact representation, providing a restoration preview resembles original input in high-level features. This preview serves as an instant generative reference to guide the Aggregator in encoding identity and other fine-grained missing from the compact representation. We observe in experiments that the previewer tends to decode aggressively when the input is clear, resulting in high variance in restoration previews. We take this as a reliable indicator of input image quality, and develop an adaptive sampling algorithm that amplifies the fine-grained encoding with relatively high quality inputs. Additionally, we find the previewer is controllable through text prompts, which produces diverse generative references and enables semantic editing with restoration. Our contributions are as follows:\n1. We explore a novel BIR method that iteratively aligns with the generative prior to address unknown degradation;\n2. We introduce a novel architecture based on pre-trained DPM, which dynamically adjusts the generation condition by previewing intermediate outputs;\n3. We develop sampling algorithms tailored for our pipeline, enabling both adaptive and controllable restoration to text prompts;\n4. We perform extensive evaluations to validate the effectiveness of the proposed methods."}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 DIFFUSION MODEL", "content": "DPM is a class of generative model that generate data by iteratively denoising from Gaussian noise (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020b). Typically, a neural network with a UNet architecture (Ronneberger et al., 2015) is trained to predict the noise added at"}, {"title": "2.2 BLIND IMAGE RESTORATION", "content": "The task setting makes BIR particular valuable in real-world applications. The major challenge in BIR is the input distribution gap between training and testing data. Previous work have explored multiple ways to address this issue. Feature quantification is widely used in generative-based methods (Esser et al., 2021; Van Den Oord et al., 2017; Zhou et al., 2022). They align the encoded LQ image features to a learnable feature codebook, ensuring the input to generator is unaffected by domain shifts. However, this hard alignment constraints the generation diversity and quality by the capacity of the discrete codebook. Previous work have also explored the application of powerful DPM in BIR. Some approaches design specialized architectures and train DPMs from scratch (Saharia et al., 2022b; Sahak et al., 2023; Li et al., 2022), while the others apply additional modules on pre-trained T2I model (Wang et al., 2024b; Yu et al., 2024; Sun et al., 2024a), leveraging their large-scale prior. In many practical scenarios, HQ images with similar contents, such as those from photo albums or video frames, are available. This has spurred interest in restoring images using reference-based methods (Cao et al., 2022; Jiang et al., 2021; Lu et al., 2021; Xia et al., 2022; Yang et al., 2020; Zhang et al., 2019). They adopt regression models to learn how to transfer high-quality features to LQ images, enhancing details restoration."}, {"title": "3 \u041c\u0415\u0422\u041dODOLOGY", "content": "The distribution gap between training and testing data exacerbates the ill-posed nature of BIR, causing hallucinations in generation-based IR models and producing artifacts. We attribute this to the error in encoding LQ image, and propose a generative restoration pipeline that refines the LQ encodings with generative references. This is achieved by exploiting the reverse process of DPM. Specifically, we first encode the LQ image into a compact representation via pre-trained vision encoder, capturing global structure and semantics to initiate diffusion generation. Conditioned on this embedding, our Previewer module generates a restoration preview at each diffusion time-step. The preview resembles to the input image with more plausible details, and they are further fused in the Aggregator module to preserve fidelity. Finally, the adjusted LQ encoding is used to control the pre-trained DPM for a fine-grained diffusion step."}, {"title": "3.1 PRELIMINARIES", "content": "DPM involves two stochastic processes named forward and reverse process (Ho et al., 2020). In the forward process, i.i.d. Gaussian noise is progressively added to the image x. The marginal distribution of diffusion latent \u00e6t follows N (at\u00e6, \u03b2tI), where at and \u1e9et are hyperparameters defining the forward process. xt converges to pure noise as t increases, and the reverse process generates images by inverting the forward process. Generally, we train a neural-network to predict the noise added at each time-step by minimizing the diffusion loss:\nLdiff = E [|| Eo (xt, t) - \u20ac||2], (1)\nwhere e denotes the noise-prediction network. At each step in the reverse process, we can retrieve a denoising sample with the predicted noise and re-parameterization (Karras et al., 2022):\nXt = x - Bte (xt, t) (2)\nAt\nIn the open-sourced T2I model Stable Diffusion (SD) (Rombach et al., 2022), the noise-prediction network ee is additionally conditioned on a text input that describes the target image. Moreover, SD employs a VAE to move the input \u00e6t into latent space zt, compressing inputs by a factor of 48 and significantly reduces the memory usage to enable image generation up to 5122 resolution."}, {"title": "3.2 ARCHITECTURE", "content": "The restoration pipeline of INSTANTIR consists of three key modules: Degradation Content Perceptor (DCP) for compact LQ image encoding, Instant Restoration Previewer for generating references on-the-fly during the reverse process, and Latent Aggregator for integrating restoration references.\nDegradation Content Perceptor We employ the pre-trained DINO (Oquab et al., 2023) for providing compact LQ image representation. Compared to CLIP (Radford et al., 2021), a common choice in image editing (Ye et al., 2023), DINO's self-supervised training with data augmentation improves robustness of the encoded features. The extracted LQ representation is modulated by a learnable Resampler (Han et al., 2024) and projected as context to the cross-attention layers of diffusion UNet. For the l-th cross-attention block, we introduce an additional cross-attention operation:\nfout = fin + CrossAttn (fin, Ctxt) + w\u00b9\u00b7 CrossAttn (fin, I (c\u0131q, t)), (3)\nwhere I denotes the DCP module and ciq is the LQ context matrix. We retain the text cross-attention here as it is a crucial part of the pre-trained T2I model that synthesizes high-level semantics. Jointly training DCP with textual transformation allows it to focus on low-level information absent in the other modality. We introduce a hyper-parameter w\u00b9 to regulate their behaviors. Note that the DCP also takes time-step t as input to establish temporal dependency in the output. Specifically, we use adaptive layer-normalization to modulate the context matrix from the DCP according to time-step t:\n\u03a6 (x, t) = Tscale LayerNorm (c\u0131q) + Tshift, (4)\nwhere, Tscale, Tshift are calculated from the time-step. We train the DCP module on a frozen diffusion model using the standard diffusion loss in Eq. 1."}, {"title": "Instant Restoration Previewer", "content": "The compact representation encoded by the DCP, while robust against degradation, losses high-level information. We introduce Previewer, a diffusion model generates from current diffusion latent instead of noise, to decode generative references from the DCP encoding. Decoding at each diffusion time-step requires (T (T + 1)/2) network forward passes with the vanilla T2I model. To streamline this process, we fine-tune the Previewer using consistency distillation (Luo et al., 2023) to make it a one-step generator. For diffusion latent zs at time-step s, we first obtain the Previewer output conditioned solely on c\u0131q. Then, we perform a diffusion step using the pre-trained model from zs, conditioned on both c\u0131q and ctxt, to reach zt. zt is regarded as the ground-truth diffusion latent at time-step t in the sampling trajectory. Finally, we get the preview of zt, again conditioned solely on clq. The consistency distillation loss is then calculated by:\nLdist = ||\u03a8 (zs, s, \u03a6 (Clq, s)) \u2013 StopGrad (\u03a8 (zt, t, \u03a6 (c\u0131q, t)))||2, (5)\nwhere I denotes the previewer model. Additionally, Eq. 5 trains the previewer to follow the sampling trajectory without ctxt, removing its dependency on text conditions which are typically unavailable in BIR tasks. The consistency constraint (Song et al., 2023) of enforcing consistent outputs across time-step enabling the Previewer to decode generative references on-the-fly."}, {"title": "Latent Aggregator", "content": "The primary challenge in the BIR task is the input distribution shift. Previous work address this by aligning LQ features with reference HQ images or a learned feature codebook. The former takes extra inputs, while the latter is limited to a specific domain by the codebook capacity. In contrast, we generate reference features directly from diffusion prior. Since the compact embedding ciq retains only high-level information, it is insufficient for the Previewer to reconstruct HQ images at larger time-steps, as shown in Fig. 6. Relying solely on reference preview incurs error accumulation, so the Aggregator anchors preview to the original input to prevent divergence in the reverse process. The input LQ image is encoded into SD's latent space and spatially concatenated with the preview. This expanded input remains compatible to the diffusion UNet, allowing the Aggregator to be initialized as a trainable copy of UNet compression path following (Zhang et al., 2023). We remove text cross-attention layers to make the Aggregator lightweight and independent of textual conditions like the Previewer. The preview and LQ hidden featrues are fused in the spatial-attention layers, which are further integrated via Spatial Feature Transform (SFT) (Wang et al., 2018). For hidden feature H\u00b9 at the l-th layer in the Aggregator, we first split it spatially into h and h, corresponding to the hidden features of preview and LQ latent, and integrate them with SFT:\nhres = (1 + a') h + B\u00b2; hp, h = Split(H'), (6)\nwhere a\u00b9, \u03b2\u00b2 = M(h) are two affine transformation parameters calculated from the feature map of LQ latent at this level. We extract multi-level features {hres}, from Aggregator using Eq. 6, and inject them into the corresponding part of U-Net expansion path through residual connections."}, {"title": "3.3 ADAPTIVE RESTORATION", "content": "INSTANTIR processes LQ image through two complementary ways: 1) extracting compact representation using the DCP, which is robust to degradation but loses fine-grained information; 2) encoding"}, {"title": "Algorithm 1 Adaptive Restoration", "content": null}, {"title": "4 EXPERIMENTS", "content": null}, {"title": "4.1 IMPLEMENTATION DETAILS", "content": "INSTANTIR is built on SDXL (Podell et al., 2023) accompanied by a two-stage training strategy. In Stage-I, we train the DCP module on a frozen SDXL, followed by consistency distillation of the Previewer (see Sec. 3.2). The Previewer is trained by applying Low-Rank Adaptation (LoRA) (Hu et al., 2021) on the base SDXL model for efficiency. By toggling the Previewer LoRA, we can seamlessly switch between the Previewer and SDXL, reducing memory footprint. After obtaining the DCP and Previewer LoRA, we proceed to Stage-II Aggregator training. The two-stage training ensures the Aggregator receives high-quality previews since the beginning of its training course.\nWe adopt SDXL's data preprocessing and conduct training on 10242 resolution. In both two stages we use the AdamW (Loshchilov, 2017) optimizer with a learning rate of 1 \u00d7 10\u22124. In Stage-I, we train the DCP module using a batch size of 256 over 200K steps, and distill the Previewer for another 30K steps with the same batch size. We train the Aggregator with a batch size of 96 over 200K steps in Stage-II. The entire training process spans approximately 9 days on 8 Nvidia H800 GPUs."}, {"title": "4.2 EXPERIMENTAL CONFIGURATION", "content": "Training Data We synthesis LQ-HQ image pairs using Real-ESRGAN (Wang et al., 2021) with the default setting. As mentioned in Sec. 3.2, we conduct Stage-I training on the JourneyDB dataset (Sun et al., 2024b), a generated dataset with descriptive captions. While JourneyDB images are of extreme quality, they lack the textures in real-world images. Hence for Stage-II training, we incorporate publicly available texture-rich datasets to enhance model's ability to produce realistic visual details. Specifically, we use DIV2K (Agustsson & Timofte, 2017), LSDIR (Li et al., 2023), Flickr2K (Timofte et al., 2017) and FFHQ (Karras et al., 2019).\nTest Setting For a comprehensive evaluation, we test INSTANTIR on a synthetic dataset and public benchmarks following previous work. We synthesize 2, 000 multi-degradation samples from DIV2K and LSDIR validation sets using Real-ESRGAN pipeline, filtering out images smaller than 10242 to ensure ground-truth quality. We include a small portion of JourneyDB validation data to enhance benchmark diversity. We conduct evaluations on RealSR (Cai et al., 2019) and DRealSR (Wei et al., 2020) to assess model performance on real-world LQ images. We report full-reference metrics PSNR, SSIM, LPIPS (Zhang et al., 2018), if ground-truth targets are available, and non-reference"}, {"title": "4.3 COMPARING TO EXISTING METHODS", "content": "We compare INSTANTIR with state-of-the-art models, including StableSR (Wang et al., 2024b), CoSeR (Sun et al., 2024a), SUPIR (Yu et al., 2024), BSRGAN (Zhang et al., 2021) and Real-ESRGAN (Wang et al., 2021). Since some of them are limited to 5122 resolution, we consider two test scenarios for a fair comparison: 1) models are tested on 5122 images with outputs of 1024-models scaled accordingly; 2) following SUPIR, the models are tested on 10242 images by cropping 5122 patch as inputs to 512-models, metrics are evaluated on the cropped area only.\nQuantitative Comparison The results are summarized in Tab. 1. INSTANTIR continuously achieves the highest MUSIQ and MANIQA scores across all test settings, outperfoming the second best by large margins up to 22% in MANIQA and 8% in MUSIQ. Notably in scenario 1, despite halving the input data, INSTANTIR still performs comparably to SOTA models. While CoSeR achieves the best CLIPIQA scores closely followed by INSTANTIR, restorations from 1024-models SUPIR and INSTANTIR are rich in details as shown in Fig. 4. We also observe the misalignment of PSNR and SSIM scores with visual quality as reported in the literature (Yu et al., 2024; Wang et al., 2024b). We include these metrics here for reference purpose.\nQualitative Comparison We provide some restoration samples on real-world LQ images in Fig. 4. Through leveraging the previewing mechanism, INSTANTIR actively aligns with generative prior, reducing hallucinations and producing sharp yet realistic details. In the second row of Fig. 4, while SUPIR's result contains rich textures, the absence of global semantic guidance causes the diver's body and mask to blend together. In contrast, the cognitive encoder in CoSeR helps it identifies statues in the second example. CoSeR employs a feature codebook to handle unknown degradations, which limits the generation of complex textures on the statues. Notably in the first row of Fig. 4,"}, {"title": "4.4 RESTORATION WITH PREVIEWING", "content": "In-domain Reference for Detail Enhancement Reference-based BIR models improve detail restoration by transferring high-quality textures from HQ references. INSTANTIR achieves this by querying the T2I model, eliminating additional inputs. In Fig. 5a, we disable the Previewer to see the effect of generative references. Here INSTANTIR infers solely with LQ images, which is beneficial to fidelity preservation but bad for visual quality. This is also reflected in Tab. 2b where all quantitative metrics deteriorate except PSNR and SSIM. Moreover, INSTANTIR equipped with Alg. 1 further improves the non-reference metrics, suggesting its flexibility in to different conditions.\nOut-domain Reference for Creative Restoration Fig. 5b shows more creative restoration samples. Owing to the efficiency of our Aggregator in integrating reference latents, INSTANTIR is able to perform high-level semantic editing during restoration, altering specific attributes of the subject and leaving other visual details like global structure and layout intact. We empirically find INSTANTIR offer better prompt-following ability under heavy degradation."}, {"title": "4.5 ABLATION STUDY", "content": "DCP Training on Text Domain We compare training DCP module with and without textual condition. Due to limited computational resources, we did not proceed to train the subsequent Previewer and Aggregator for the DCP trained on image-only data. For comparison, we provide some visual examples of their generative references across diffusion time-steps in Fig. 6. As shown in the first row of Fig. 6, the generative references from the image-only DCP differ significantly from the input LQ image at early stage, retaining only coarse semantic like \"a bird standing on a rocky surface.\" In contrast, DCP trained with text descriptions preserves most of the low-level information, including global hue, structure, layout, and even the subject's category (penguin) and its pose.\nPreviewer Consistency Distillation We validate the necessity of consistency constraints in Previewer. We experiment with using predictions from Eq. 2 as reference inputs to the Aggregator. The second row in Tab. 2b shows a significant drop in the non-reference metrics. In fact, the prediction in Eq. 2 is close to the distribution mean at each time-step (Karras et al., 2022). Previewer with consistency distillation can directly sample from the data distribution, providing more informative generative references.\nFresh Noise to Restoration Previews We additionally train an Aggregator that injects fresh noise to reference latents according to diffusion time-step. The noisy preview latent follows the same distribution as current diffusion latent, making the overall pipeline resemble a ControlNet model (Zhang et al., 2023). As shown in the third row of Tab. 2b, INSTANTIR significantly outperforms ControlNet with LQ image as conditional inputs. This highlights the effectiveness of the previewing mechanism in INSTANTIR for adjusting generation conditions during inference."}, {"title": "5 CONCLUSION", "content": "In this paper, we explore a novel method to address unknown degradations in BIR tasks. Through exploiting the generation process of DPM, we propose to actively align with the generative prior to reduce the errors in encoding LQ image. Our pipeline is implemented based on pre-trained SDXL model, referred to as INSTANTIR. Extensive experiments demonstrate the exceptional restoration capability of INSTANTIR, delivering SOTA performance in quantitative metrics and visual quality. However, we observe some disparity in reference metrics such as PSNR and SSIM compared to SOTA models, which might because of the excessive generative prior diminishes fidelity. Future work will explore approach to improve the interaction between generative prior and conditions, as well as ways to refine the previewer to produce more reliable references."}]}