{"title": "Benchmarking Mental State Representations in Language Models", "authors": ["Matteo Bortoletto", "Constantin Ruhdorfer", "Lei Shi", "Andreas Bulling"], "abstract": "While numerous works have assessed the generative performance of language models (LMs) on tasks requiring Theory of Mind reasoning, research into the models' internal representation of mental states remains limited. Recent work has used probing to demonstrate that LMs can represent beliefs of themselves and others. However, these claims are accompanied by limited evaluation, making it difficult to assess how mental state representations are affected by model design and training choices. We report an extensive benchmark with various LM types with different model sizes, fine-tuning approaches, and prompt designs to study the robustness of mental state representations and memorisation issues within the probes. Our results show that the quality of models' internal representations of the beliefs of others increases with model size and, more crucially, with fine-tuning. We are the first to study how prompt variations impact probing performance on theory of mind tasks. We demonstrate that models' representations are sensitive to prompt variations, even when such variations should be beneficial. Finally, we complement previous activation editing experiments on Theory of Mind tasks and show that it is possible to improve models' reasoning performance by steering their activations without the need to train any probe.", "sections": [{"title": "1. Introduction", "content": "Modern language models (LMs) trained on next token prediction have demonstrated impressive capabilities, spanning coding, mathematical reasoning, fact verification, and embodied interaction (Wei et al., 2022; Bubeck et al., 2023). As these models are designed with the ultimate goal of collaborating with humans, it becomes imperative that they complement these skills with an understanding of humans, in particular their beliefs, emotions, desires, and intentions (Li et al., 2023a). Core to this understanding is Theory of Mind (ToM) - the ability to attribute mental states to oneself and others (Premack & Woodruff, 1978). ToM is essential for effective communication and cooperation with other agents, facilitating interaction and learning from feedback and demonstrations (Saha et al., 2023). Given its significance, ToM has emerged as a critical milestone in artificial intelligence (AI) and an important capability when evaluating cutting-edge LMs (Bubeck et al., 2023). Interest in LMs' generative performance on tasks requiring ToM reasoning has resulted in a wide variety of benchmark datasets, typically involving question-answering tasks (Le et al., 2019; Gandhi et al., 2023; Kim et al., 2023; He et al., 2023; Tan et al., 2024; Xu et al., 2024).\nDespite showing improved performance on ToM benchmarks compared to earlier models, modern LMs are still far from perfect (Sap et al., 2022). Text generated by LMs often contains errors that limit their performance on ToM tasks (Martindale et al., 2019). Previous work has shown that it is sometimes possible to still obtain correct predictions by probing LMs' internal representations (Li et al., 2021; Liu et al., 2023b; Gurnee et al., 2023). In particular, Zhu et al. (2024) have shown that LMs, when prompted with a story and a belief statement, can represent beliefs from their own perspective and, to a lesser extent, from the perspective of a character in the story. Their work is an important first step towards understanding how LMs represent mental states, but it is limited in the number of models and settings studied, leaving many questions unanswered.\nBuilding and extending on (Zhu et al., 2024), we benchmark mental state representations of self and others in language models through extensive experiments of different LM families, model sizes, and fine-tuning approaches. Specifically, we design a set of experiments to address the following research questions: RQ1. What is the relation between model size and probing accuracy? RQ2. Does fine-tuning with instruction-tuning (Wei et al., 2021) and/or reinforcement learning from human feedback (Christiano et al., 2017; Ouyang et al., 2022, RLHF) have an effect on probing accuracy? RQ3. Are models' internal representations of beliefs sensitive to prompt variations? RQ4. Is there a risk of probes memorising training data due to the large dimensionality of LM representations? RQ5. Can we enhance LMs' performance by editing their activations without training dedicated probes?"}, {"title": "2. Related Work", "content": "Machine Theory of Mind Theory of Mind (ToM) has been studied in cognitive science and psychology for decades (Gurney et al., 2021). Mirroring efforts to understand ToM in humans, an increasing number of works in the computational sciences have investigated means to equip AI with similar capabilities. Previously proposed models that aim to implement a machine ToM have been based on partially observable Markov decision processes (POMDP) (Doshi et al., 2010; Han & Gmytrasiewicz, 2018), Bayesian methods (Baker et al., 2011; 2017) and deep learning methods (Rabinowitz et al., 2018; Bara et al., 2021; Wang et al., 2022; Duan et al., 2022; Liu et al., 2023a; Bortoletto et al., 2024b;a). Recent advances in LMs have sparked interest in evaluating their ToM capabilities. Various benchmarks have been proposed, aiming to measure LMs' ability to understand and reason about the beliefs, goals, and intentions of others (Le et al., 2019; He et al., 2023; Kim et al., 2023; Gandhi et al., 2023; Xu et al., 2024; Tan et al., 2024). Additionally, efforts have been made to enhance LMs' ToM through prompting techniques (Zhou et al., 2023b; Moghaddam & Honey, 2023; Wilf et al., 2023).\nA new direction of research explores LMs' internal representation of mental states. Zhu et al. (2024) demonstrated that LMs linearly encode beliefs from different agents' perspectives, and manipulating these representations can enhance ToM task performance. While Zhu et al.'s work is a crucial initial step, our work dives deeper into LMs' internal belief representations, offering a broader insight into these mechanisms.\nProbing Neural Representations Initially proposed by Alain & Bengio (2017), probing has emerged as a common method for determining if models represent particular features or concepts. In the realm of LMs, numerous works used probing to demonstrate that these models acquire rich linguistic representations. These representations span syntactic and semantic concepts such as syntactic categories, dependency relations, co-reference, and word meaning (Conneau et al., 2018; Tenney et al., 2018; 2019; Rogers et al., 2021; Li et al., 2021; Hernandez & Andreas, 2021; Marks & Tegmark, 2023; Liu et al., 2023b). A separate line of work explored if and how LMs represent the world, i.e., whether they possess a world model. Li et al. (2021) showed that LMs track the states of entities within a context. Other works showed that LMs exhibit representations reflecting non-linguistic concepts in the world, which LMs have never observed (Abdou et al., 2021; Patel & Pavlick, 2022; Li et al., 2023b; Nanda et al., 2023). An emergent line of work that is particularly relevant to our work used probing to explore if LMs have agent models, for example, if they can"}, {"title": "3. Experimental Setup", "content": "3.1. Probing\nIn line with previous work (Zhu et al., 2024) we linearly decode belief status from the perspective of different agents by using probing (Alain & Bengio, 2017). Probing involves localising specific concepts in a neural model by training a simple classifier (called a probe) on model activations to predict a target label associated with the input data. To provide a formal definition, we adopt a similar notation to the one introduced in (Belinkov, 2022). Let us define an original model $f : x \\rightarrow \\hat{y}$ that is trained on a dataset $D^O = \\{x^{(i)}, y^{(i)}\\}$ to map input $x$ to output $\\hat{y}$. Model performance is evaluated by some measure, denoted $PERF(f, D^O)$.\nA probe $g: f_l(x) \\rightarrow z$ maps intermediate representations of $x$ in $f$ at layer $l$ to some property $z$, which is the label of interest. The probe $g$ is trained on a probing dataset $D^P = \\{x^{(i)}, z^{(i)}\\}$ and evaluated using some performance measure $PERF(g, f, D^O, D^P)$. In our case, $f$ is an autoregressive language model that given a sequence of tokens $x$ outputs a probability distribution over the token vocabulary to predict the next token in the sequence. Our probe is a logistic regression model $z = \\sigma(Wa_l + b)$ trained on neural activations $f_l(x) = a_l$ to predict belief labels $y = \\{0,1\\}$."}, {"title": "3.2. Dataset", "content": "Following Zhu et al. (2024) we use the BigToM benchmark (Gandhi et al., 2023). BigToM is constructed using GPT-4 (Achiam et al., 2023) to populate causal templates and combine elements from these templates. Each causal template is set up with a context and a description of the protagonist (e.g. \"Noor is working as a barista [. . . ]\"), a desire (\"Noor wants to make a cappuccino\u201d), a percept (\u201cNoor grabs a milk pitcher and fills it with oat milk", "Noor believes that the pitcher contains oat milk": ".", "A coworker swaps the oat milk in the pitcher with almond milk": ".", "datasets": "D_P^P = \\{x_f^{(i)}, z_p^{(i)}\\}$, where the labels $z_p^{(i)}$ correspond to ground-truth beliefs from the protagonist perspective, and $D_O^P = \\{x_f^{(i)}, z_o^{(i)}\\}$, where the labels $z_o^{(i)}$ reflect the perspective of an omniscient oracle. $D_P^P$ and $D_O^P$ are built by pairing each story in BigToM with a belief statement, as shown in Figure 1. After prompting the model with a story-belief pair we cache the residual stream activations at the final token position for all residual streams (Figure 5)."}, {"title": "3.3. Models", "content": "Zhu et al. (2024) have used two models for their experiments: Mistral-7B-Instruct (Jiang et al., 2023) and DeepSeek-7B-Chat (Bi et al., 2024) \u2013 both being the same size and fine-tuned. In contrast, we study two families of LMs that offer us options in model sizes and fine-tuning: Pythia (Biderman et al., 2023) and Llama-2 (Touvron et al., 2023). While Llama-2 offers \u201cchat\u201d versions fine-tuned using supervised learning and RLHF, Pythia's open-source training set ensures that there is no data leakage. Additionally, we consider a version of Pythia-6.9B fine-tuned on a mixture of open-source instruction datasets (Wang et al., 2024), which we refer to as Pythia-6.9B-chat. A summary of the models we study is reported in Table 2."}, {"title": "3.4. Probing Experiments", "content": "We aim to contribute to understanding how LMs represent beliefs of self and others by proposing a set of extensive probing experiments across LMs that differ in architecture, size, and fine-tuning approach. Our approach is generally similar to the one used by (Zhu et al., 2024), but we make a different operational choice: While (Zhu et al., 2024) train probes on each attention head for every layer, we train probes on the residual stream for every layer. We opted to use the residual stream as it integrates information from both the attention and feed-forward components, potentially encoding richer representations. Additionally, since the residual activations directly contribute to the final output predictions, probing them may better align with understanding the model's behaviour for downstream tasks.\nModel Size and Fine-tuning We first report experiments to better understand the effect of model size and fine-tuning on belief probing accuracy. Specifically, we ask the following questions: Is there a relation between model size and probing accuracy? (RQ1) Does fine-tuning an LM with instruction-tuning or RLHF have an effect on probing accuracy? (RQ2) To answer these questions we performed the same probing experiment across all our models and compared the results.\nSensitivity to Prompting By using a single prompt design, previous work left the impact of prompt design on probing accuracy unclear (Zhu et al., 2024). Our second set of experiments aims to explore how belief representations are sensitive to different prompts. Research on prompt robustness in language models is still in its infancy and focused mainly on revealing vulnerability to prompt alternations on downstream performance. In contrast, we study how the input influences models' representations by asking: Are models' internal belief representations robust to prompt variations? (RQ3) To answer this question we define four prompt variations:\n\u2022 Random: Following (Gurnee & Tegmark, 2024), we add 10 random tokens to the belief statement.\n\u2022 Misleading: Each story is followed by two belief statements, one pertinent to the story and one randomly chosen from another.\n\u2022 Time Specification: The prompt specifies that the belief statement refers to the end of the story. We study this variation because some belief statements can be true (false) at the story's beginning but false (true) at the end. For example, consider the story in Figure 1: if Noor does not witness the swap, in the end, she will believe the pitcher contains almond milk ($y_p$ = True). However, if the same belief is referred to at the beginning of the story, then it is false ($y_p$ = False).\n\u2022 Initial Belief: We explicitly reveal the protagonist's initial belief (e.g. \"Noor believes that the pitcher contains oat"}, {"title": "3.5. Contrastive Activation Addition", "content": "Our final set of experiments builds upon the findings of (Zhu et al., 2024), who showed that employing trained probes with inference time intervention (Li et al., 2023c, ITI) could enhance LMs' performance on ToM tasks. We take a step further and ask: Can we enhance LMs' performance by manipulating their activations without the need for training dedicated probes? (RQ5) To find an answer we use contrastive activation addition (Rimsky et al., 2023, CAA), an extension of activation addition (Turner et al., 2023, AA) that computes steering vectors to control LMs' behaviour. Steering vectors are computed as the average difference in residual stream activations between pairs of positive and negative instances of a specific behaviour. Formally, given a dataset D of triplets $(p, c_p, c_n)$, where $p$ is a prompt, $c_p$ is a positive completion, and $c_n$ is a negative completion, CAA computes a mean difference vector $v_{md}^l$ for layer $l$ as:\n$v_{md}^l = \\frac{1}{\\vert D \\vert} \\sum_{p, c_p, c_n} a_l(p, c_p) - a_l(p, c_n)$\nDuring inference, these steering vectors are multiplied with an appropriate coefficient $\\alpha$ and added at every token position of the generated text after the user's prompt. CAA has two main advantages over ITI: First, it eliminates the need to train probes. Second, it operates at the residual stream level, making it easier to use than methods that intervene on specific attention heads like ITI. While CAA has been used to control alignment-relevant behaviour, such as hallucinations, refusal, and sycophancy (Rimsky et al., 2023), we are the first to apply it to enhance LMs' ToM reasoning. This can be understood as isolating the direction in the LMs' latent space corresponding to taking the perspective of another agent. To evaluate both base and fine-tuned LMs, we rank their answers to the ToM questions according to PLM ($\\alpha_q$) (Petroni et al., 2019). We adopt the Forward Belief task split used in (Zhu et al., 2024) to compute the steering vectors. Additionally, we evaluate the transferability of the CAA steering vectors by applying them to two other BigToM tasks: Forward Action and Backward Belief. We provide details about these tasks in Appendix A.1.1, and a more detailed explanation of how ITI works in Appendix A.5."}, {"title": "4. Results", "content": "4.1. Effect of Model Size and Fine-tuning\nResults from our study on model size and fine-tuning are shown in Figure 2. When considering oracle beliefs, probing accuracy rapidly converges to 100, with larger models showing faster convergence rates. The smallest Pythia-70m that performs slightly worse but still achieves 95% accuracy despite having less than 0.6% of the parameters of Pythia-12B. This finding suggests that even small LMs can effectively represent beliefs from an omniscient perspective.\nFor protagonist beliefs, accuracy also increases with model size, although there is a performance gap between Llama-2 and Pythia. For example, Llama2-13B reaches around 80%, while Pythia-12B achieves approximately 60%. This gap is likely due to Llama-2 being trained on nearly seven times more tokens than Pythia. The figure also shows that accuracy at early layers is particularly low across all models. We speculate that this is due to the initial coding strategy of LMs that uses the first layers to combine individual tokens into more semantically meaningful representations (Gurnee et al., 2023). Probes on fine-tuned LMs show significantly better accuracy with improvements of up to 29% for Llama2-7B-chat and 26% for Pythia-6.9B-chat with respect to their base version. Fine-tuned 7B LMs outperform (Llama-2) or are on par (Pythia) with twice as large base models (12/13B), highlighting the importance of fine-tuning in developing representations of others' beliefs. This resonates with cognitive"}, {"title": "4.2. Sensitivity to prompting", "content": "Figure 3 compares protagonist probe accuracy across various prompt variations for different models, considering their architecture, size, and fine-tuning. As can be seen from the figure, providing the protagonist's Initial Belief in the story yields higher probe accuracy compared to the Original prompt (Figure 1). Accuracy for all the other prompt variations is generally lower than Original. On one hand, misleading prompts hurt performance across all models. This finding resonates with Webson & Pavlick (2022) who found that instruction-tuned models, despite being more robust, are still sensitive to misleading prompts. On the other hand, Time Specification unexpectedly does not help in disambiguating belief states in different time frames, as we hypothesised in \u00a73.4. Additionally, models show sensitivity to Random tokens placed before the belief statement. Results for oracle beliefs are reported in Figure 7 and indicate that models maintain high accuracy. Misleading prompts slightly reduce performance to around 95%. In summary, these experiments show that LMs possess robust belief representations when taking an omniscient perspective, whereas their representations of others' beliefs are more susceptible to prompt variations."}, {"title": "4.3. Memorisation Effects in the Probes", "content": "Figure 4 and Figure 8 show probe accuracies obtained by training a probe on the top k principal components of the intermediate representations for protagonist and oracle, respectively. Specifically, we consider k = {2, 10, 100, 1000}, spanning several orders of magnitude. For models with hidden dimensions smaller than 1000, we skip this value. For all models, it is generally possible to recover most of the original accuracy by training probes on a number k of principal components of the activations that is more than one order of magnitude smaller, indicating no strong evidence of memorisation in the probes."}, {"title": "4.4. Contrastive Activation Addition", "content": "We finally compare models' accuracy on three BigToM tasks in Table 1. Each model has been evaluated three times: without any intervention, using ITI, and using CAA. Hyperparameter details can be found in Appendix A.6. Note that we use steering vectors computed using the Forward Belief task for all three tasks to test their generalisability (Zhu et al., 2024). As can be seen from the table, performance without intervention is generally lower across tasks and model sizes, with the larger Llama-2-70B and Llama-2-70B-chat models exhibiting higher accuracy. Performance for Pythia models of different sizes does not change much, with the fine-tuned Pythia-6.9B-chat showing better performance on single true belief (TB) and false belief (FB) tasks but not on their conjunction (Both). ITI demonstrates modest improvements"}, {"title": "5. Limitations and Future Work", "content": "Our study focused on expanding experiments from the model perspective, examining architectures, sizes, fine-tuning, and prompt design, all within the same dataset. A natural extension of our work is replicating these experiments across multiple datasets and more model families. Given the rapid pace of new language model releases, studying all available models is impractical, particularly considering computational resource constraints. Nevertheless, our approach can be adopted to support new benchmarks or to evaluate newly released models as they become available."}, {"title": "6. Conclusion", "content": "Our study addresses a significant gap in understanding LMs by investigating their internal representation of mental states. We conducted an extensive benchmark involving various LM types, sizes, fine-tuning approaches, and prompt designs to examine the robustness of these representations. Our findings reveal that scaling LMs' size and, in particular for smaller LMs, fine-tuning are key to developing representations of others' beliefs. We are the first to demonstrate that such prompt variations influence model representations, and we also demonstrate the feasibility of enhancing models' ToM reasoning by steering their activations without training any probe. Overall, our work contributes valuable insights into the factors influencing LMs' mental state representations, shedding light on avenues for improving their performance in ToM tasks."}, {"title": "Societal Impact", "content": "While our work is foundational and remains distant from specific applications with direct societal impact, it's important to recognise the ethical implications of modelling and predicting mental states. Handling sensitive aspects of individuals' inner experiences and emotions requires careful consideration to avoid reinforcing biases or misunderstanding psychological nuances."}, {"title": "Author Contributions Statement", "content": "M. Bortoletto designed and directed the project, performed the experiments, analysed the results, and wrote the manuscript.\nC. Ruhdorfer provided feedback on the experimental results and contributed to editing the final manuscript.\nL. Shi contributed to proofreading the final manuscript.\nA. Bulling supervised the project and contributed to the writing of the manuscript."}, {"title": "A. Appendix", "content": "A.1. Experimental Setup\nA.1.1. BIGTOM\nBigToM (Gandhi et al., 2023) is constructed using GPT-4 (Achiam et al., 2023) to populate causal templates and combine elements from these templates. Each causal template is set up with a context and a description of the protagonist (e.g. \"Noor is working as a barista [. . . ]\"), a desire (\u201cNoor wants to make a cappuccino\u201d), a percept (\u201cNoor grabs a milk pitcher and fills it with oat milk", "Noor believes that the pitcher contains oat milk\"). The state of the world is changed by a causal event (\u201cA coworker swaps the oat milk in the pitcher with almond milk\"). The dataset constructs different conditions by changing the percepts of the protagonist after the causal event, which will result in different beliefs. In this work, following Zhu et al. (2024) and Gandhi et al. (2023), we focused on the 6 most important conditions, corresponding to true and false beliefs on the following three tasks": "n\u2022 Forward Belief: given the protagonist's percepts of the causal event, infer their belief: P(belief|percept).\n\u2022 Forward Action: infer the protagonist's action given their desire and percepts of the causal event. Before inferring the action, one would need to first implicitly infer the protagonist's belief: $\\sum_{belief}$ P(action|percept, belief, desire).\n\u2022 Backward Belief: infer the protagonist's belief from observed actions. This requires to first implicitly infer the protagonist's percepts: $\\sum_{percepts}$ P(belief|action, percept, desire).\nThe dataset was released under the MIT license and can be accessed at We report one example for each task in the boxes below, where the text defining true belief or false belief task is shown in blue and red, respectively."}, {"title": "A.1.2. LINEAR PROBES", "content": "Our probing approach is illustrated in Figure 5. We cache activations both at the attention and residual stream level. In our experiments, we use residual stream activations. To perform ITI and compare it to CAA, we also cache attention heads' activations. We trained the probes using the L-BFGS solver with L2 penalty with inverse of regularisation strength 10 for a maximum of 1000 iterations. We use zero as random seed."}, {"title": "A.1.3. LANGUAGE MODELS", "content": "A detailed summary of the models we use in this work is shown in Table 2. Pythia was released under the Apache 2.0 license. Llama-2 is licensed by Meta for both researchers and commercial entities (Touvron et al., 2023). For all the models, we set the temperature to zero."}, {"title": "A.1.4. EXAMPLES OF PROMPT VARIATIONS", "content": ""}, {"title": "A.7. Compute Resources", "content": "We ran our experiments on a server running Ubuntu 22.04, equipped with eight NVIDIA Tesla V100-SXM2 GPUs with 32GB of memory and Intel Xeon Platinum 8260 CPUs."}, {"title": "A.8. Code", "content": "Our code will be made public under the MIT licence at"}, {"title": "A.5. Inference-time intervention", "content": "Inference-time intervention (Li et al., 2023c, ITI) employs a two-step process. First, it trains a probe for each attention head across all layers of a LM. These probes are evaluated on a validation set, and the top-k heads with the highest accuracy are selected. Subsequently, during inference, ITI steers the activations of these top heads along the directions defined by their corresponding probes. Formally, ITI can be defined as an additional term to the multi-head attention:\n$x_{l+1} = x_l + \\sum_{h=1}^{H} \\alpha (Attr^h(x) + \\alpha \\sigma_h \\Theta_h)$\nwhere $x_l$ is the residual stream at layer $l$, $H$ is the number of attention heads, $\\alpha \\in R^+$ is a coefficient, $\\sigma_h^l$ is the standard deviation of activations along the direction identified by the probe trained on attention head h at layer $l$, and $\\Theta_h$ is zero ofr not-selected attention heads."}, {"title": "A.6. Activation editing hyperparameters", "content": "Table 3 reports results obtained on the three BigToM tasks with the hyperparameters used for ITI (Li et al., 2023c) and CAA (Rimsky et al., 2023). We report an example of prompt used for evaluation in the box below."}]}