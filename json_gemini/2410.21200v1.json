{"title": "BongLLaMA: LLaMA for Bangla Language", "authors": ["Abdullah Khan Zehady", "Safi Al Mamun", "Naymul Islam", "Santu Karmaker"], "abstract": "Bangla (or \"Bengali\") is a language spoken by approximately 240 million native speakers and around 300 million people worldwide. Despite being the 5th largest spoken language in the world, Bangla is still a \"low-resource\" language, and existing pretrained language models often struggle to perform well on Bangla Language Processing (BLP) tasks. This work addresses this gap by introducing BongLLaMA (i.e., Bangla-LLaMA), an open-source large language model fine-tuned exclusively on large Bangla corpora and instruction-tuning datasets. We present our methodology, data augmentation techniques, fine-tuning details, and comprehensive benchmarking results showcasing the utility of BongLLaMA on BLP tasks. We believe BongLLaMA will serve as the new standard baseline for Bangla Language Models and, thus, facilitate future benchmarking studies focused on this widely-spoken yet \"low-resource\" language\u00b9.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have revolutionized the landscape of NLP research and contributed significantly to the mission of democratizing AI among the general public. LLMs like ChatGPT (Achiam et al., 2023), LLaMA (Touvron et al., 2023a), and Mistral (Jiang et al., 2023) have demonstrated unprecedented prowess, reshaping how humans can naturally interact with computers. Despite this remarkable progress, much of the focus has been on high-resource languages like English, while low-resource languages like Bangla (\"Bengali\") have received little attention.\nInterestingly, Bangla is the 5th largest spoken language in the world (Brunn and Kehrein, 2018), yet it is still a \u201clow-resource\u201d language regarding"}, {"title": "2 Background on LLMs and Bangla NLP", "content": "The momentum of Large Language Models (LLMs) surged with the introduction of OpenAI's GPT-2 in 2019 (Radford et al., 2019) and GPT-3 in 2020 (Brown et al., 2020), boasting decoders with increasingly vast parameter counts. ChatGPT-3.5 (Gao et al., 2023) further popularized LLMs by introducing simple interactive user interfaces and human-like conversational capability by incorporating Reinforcement Learning from Human Feedback (RLHF) and refinement. The trend toward larger models culminated in the release of GPT-4 (Achiam et al., 2023), signaling a steady progression toward larger model sizes.\nAfter GPT's success, open-source LLM endeav-"}, {"title": "3 Pre-training BongLLaMA", "content": "We pre-trained five different variants of Meta's LLaMA models using one of the largest Bangla text corpora (see below) to develop corresponding variants of BongLLaMA. The models are:\n1. BongLLaMA2-7B from MetaLLaMA2-7B\n2. BongLLaMA3-8B from MetaLLaMA3-8B\n3. BongLLaMA3.1-8B from MetaLLaMA3.1-8B\n4. BongLLaMA3.2-1B from MetaLLaMA3.2-1B\n5. BongLLaMA3.2-3B from MetaLLaMA3.2-3B\nCorpus for Pre-training: We leveraged the Bangla subset of CulturaX, a comprehensive multilingual dataset developed by Nguyen et al."}, {"title": "4 Instruction-Tuning BongLLaMA", "content": "Following pretraining, we conducted rigorous instruction-finetuning to enhance BongLLaMA's performance on specific Bangla language tasks.\nCorpus for Instruction Finetuning: For instruction finetuning, we curated a special dataset called Bangla-Alpaca-Orca that includes 172,000 instructions translated into Bangla and culturally relevant responses tailored to enrich the model's understanding of Bengali cultural nuances. To create this dataset, we utilized the Google Translation API to produce a Bangla version of the original Alpaca dataset (Taori et al., 2023) and combined it with a carefully selected portion of the OpenOrca dataset (Lian et al., 2023), both specifically adapted for Bangla instruction fine-tuning. Following the translation process, we manually handcrafted instructions to incorporate Bangla cultural aspects and Bangladeshi factual events, ensuring Bengali cultural relevance."}, {"title": "5 Evaluation of BongLLaMA", "content": "We rigorously assessed the performance of BongLLaMA against Meta-LLaMA (original LLaMA models released by Meta) using a diverse set of 120 queries across nine different tasks: 1) Coding, 2) Translation, 3) Entertainment, 4) Generation, 5) Open_QA, 6) Factual_QA, 7) Reasoning, 8) Ethics and 9) Literature. Refer to appendix A.5 for the complete list of queries. To account for the stochastic nature of language model outputs, we employed a triple-sampling strategy. Each query was prompted to both BongLLaMA and Meta-LLaMA three times, with a consistent temperature setting of 0.6 (see Table 5f - Table 5j in the appendix for example prompts and responses).\nEvaluation Metrics: Our primary evaluation metric leverages the capabilities of GPT-40, as an \"omni-evaluator\u201d. GPT-4o assesses each response on a scale of 1 to 100, providing a high-resolution view of performance differences (appendix A.3 details GPT-40 scoring parameters). This automated scoring is applied to all 120 queries in our test set, and an average score is reported.\nTo mitigate potential biases or errors in the automated scoring, we supplement the GPT-4 evaluations with meticulous manual reviews. Expert human evaluators examined a subset of the responses, paying particular attention to edge cases or instances where GPT-4's scoring seems inconsistent or questionable. This human oversight allowed us to calibrate the prompting techniques for the GPT-4 evaluator and mitigate any systematic biases in the automated scoring process."}, {"title": "6 Bong vs. Meta-LLaMA Results", "content": "Table 1 presents the summary of our evaluation results, which reveals several interesting insights and highlights both the strengths and limitations of our models compared to Meta's LLaMA counterparts, as discussed below.\nComparison Across Tasks: BongLLaMA models consistently outperform Meta-LLaMA models in tasks that require a nuanced understanding of the Bangla language. These tasks include Open_QA, Reasoning, Generation, Factual_QA, and Literature. For instance, in Open_QA, BongLLaMA3-8b scores 66.96 compared to Meta's 43.40, indicating an enhanced capability to comprehend and generate accurate responses to open-ended Bangla queries. Similarly, in Reasoning, BongLLaMA3-8b scores a remarkable 82.75, vastly outperforming Meta's 37.50, which suggests superior logical reasoning and the ability to handle complex Bangla contexts.\nFor example, consider the Reasoning task, \u201c\u09e9x+\u09e7=\u09e7\u09e6 \u09b9\u09b2\u09c7, x \u098f\u09b0 \u09ae\u09be\u09a8 \u0995\u09a4?\u201d which translates to \u201cIf 3x+1=10, what is x equal to?\u201d. BongLLaMA3-8b effectively solves this equation, demonstrating its strong reasoning capabilities in Bangla.\nIn the Literature category, BongLLaMA3.1-8b scores 53.37 against Meta's 24.63, showcasing its proficiency in understanding and generating literary Bangla text. This proficiency is likely due to targeted training on literary datasets, enabling the model to grasp intricate linguistic patterns and idiomatic expressions prevalent in Bangla literature.\nIn contrast, BongLLaMA models generally underperform in tasks that require cross-lingual understanding or technical expertise, specifically Coding and Translation. For example, Meta's LLaMA3-8b scores 68.33 in Coding, significantly higher than BongLLaMA3-8b's 32.50. Coding tasks often involve programming languages and technical terminologies predominantly documented in English, which BongLLaMA's Bangla-focused training may not adequately cover. Similarly, in Translation, BongLLaMA3-1b achieves a score of 21.67, while Meta's LLaMA3.1-8b scores 60.42. This disparity suggests that BongLLaMA's limited exposure to bilingual corpora hurts its performance in translation tasks, especially between Bangla and English.\nFor instance, consider a Coding task: \u201c\u09aa\u09be\u0987\u09a5\u09a8\u09c7 'Quick Sort' \u0985\u09cd\u09af\u09be\u09b2\u0997\u09b0\u09bf\u09a6\u09ae \u09aa\u09cd\u09b0\u09df\u09cb\u0997 \u0995\u09b0\u09c1\u09a8\u0964\u201d translated as \u201cImplement \u2018Quick Sort' algorithm in Python.\u201d While Meta's LLaMA3-8b handles this task with higher proficiency, BongLLaMA3-8b failed to generate a relevant response.\nComparison Across Model Generations: The progression from LLaMA2 to LLaMA3 and its subsequent iterations demonstrates substantial advancements in model performance. Transitioning from BongLLaMA2-7b to BongLLaMA3-8b results in significant performance gains across most of the tasks. For example, in Reasoning, the score jumps from 12.08 to 82.75, and in Generation, from 14.71 to 66.46. Additionally, BongLLaMA3.1-8b shows exceptional per-"}, {"title": "7 Discussion and Final words", "content": "In this work, we present the BongLLaMA model, the first open-source LLM exclusively tailored to the Bangla language. We created five different variants of BongLLaMA models\u00b2 by extending the five original variants of LLaMA models released by Meta. BongLLaMA models demonstrate significant improvements in terms of accurately processing complex instructions for various tasks. The superior performance of BongLLaMA in Bangla-centric tasks can be attributed to extensive fine-tuning on large, diverse Bangla datasets. Our focused approach enhances BongLLaMA's ability to understand and generate nuanced Bangla text for various tasks compared to their original LLaMA counterparts (see Table 1).\nHowever, the lack of sufficient bilingual and technical data limits BongLLaMA's performance in cross-lingual (translation) and coding tasks. Incorporating more diverse datasets, including bilingual corpora and technical documents, could address these shortcomings. Additionally, advancements from LLaMA2 to LLaMA3 and its iterations introduce architectural improvements that facilitate better language understanding and reasoning capabilities. The substantial performance gains in tasks like Reasoning and Generation highlights how increased model capacity and refined architectures contribute to enhanced performance. Further, while larger models generally perform better due to their capacity to learn complex patterns, the competitive performance of smaller models like BongLLaMA3.2-3b in specific tasks underscores the effectiveness of targeted fine-tuning. This indicates that strategic training approaches can partially compensate for smaller model sizes, enabling them to perform well in specialized areas.\nFinally, we believe BongLLaMA will evolve"}, {"title": "8 Limitations", "content": "While the BongLLaMA suite of models represents a significant advancement in Bangla language processing, it is crucial to acknowledge and discuss the inherent limitations of our work. First, due to computational and resource constraints, our pre-training was conducted on a relatively modest corpus size. This limitation may result in knowledge gaps, particularly regarding nuanced aspects of Bangla culture, literature, and contemporary issues. Expanding the pretraining corpus will significantly enhance the model's contextual understanding and generalization capabilities.\nSecond, our reliance on the Google Translation API for converting English instructions to Bangla may have introduced inaccuracies or nuanced losses in meaning. This \u201ctranslation loss\" could potentially affect the models' performance in both text generation and comprehension tasks, as subtle linguistic nuances might be misrepresented or lost in translation.\nThird, our use of a fixed temperature of 0.6 across all categories may have limited the models' performance in tasks that could benefit from different levels of randomness. A more nuanced approach, tailoring temperature settings to specific task categories, could potentially yield improved results."}, {"title": "9 Adherence to Ethics Policy", "content": "Our work adheres to the ACL Ethics Policy. We recognize the broader impact of our research on society and acknowledge the ethical considerations involved in developing large language models (LLMs) such as BongLLaMA. While LLMs offer tremendous potential for advancing natural language processing (NLP) tasks, including translation, summarization, and question-answering, they also raise concerns regarding biases, privacy, and misuse.\nTo mitigate these risks, we have taken several steps. Firstly, we have focused on enhancing the representation of the Bengali language in LLMs, addressing a significant gap in linguistic resources. By enriching the pre-training phase with a comprehensive Bangla corpus and expanding the vocabulary, we aim to improve the model's proficiency in understanding and generating Bangla text across various domains.\nSecondly, we have employed the Low-Rank Adaptation (LoRA) approach, Flash Attention, and bits-and-bytes quantization to optimize the training process, reducing computational overhead and memory footprint while maintaining high performance. These techniques not only improve the efficiency of our models but also contribute to reducing the environmental impact of training large models.\nThirdly, we have conducted experiments using NVIDIA A100 GPUs, leveraging hardware acceleration to achieve high-performance computing for deep learning tasks. This choice of hardware reflects our commitment to efficient resource utilization and sustainable AI development practices.\nLastly, we have fine-tuned our models on datasets that include translated instructions and contextual data, ensuring that our models are culturally aware and capable of generating relevant responses in diverse contexts. Our use of the existing dataset is consistent with their intended use. We will release all artifacts that we created for public use as long as they are compatible with the original access conditions.\nOverall, we believe that our work contributes positively to the field of NLP by advancing the capabilities of LLMs for the Bengali language while adhering to ethical guidelines and promoting responsible AI development.\nFinally, we used Large Language Models (especially ChatGPT-40) to improve our writing by"}, {"title": "A Appendix", "content": "A.1 BongLLaMA Variants\nBongLLaMA2-7b-Base: This model is based on the LLaMA 2 architecture with 7 billion parameters. It has been pre-trained exclusively on the Bengali subset of the CulturaX dataset, focusing on Bengali language understanding and generation. The model features a context window of 4,096 tokens, which is a 100% increase from the original LLaMA's 2,048 tokens. This expanded context allows for improved handling of longer text sequences and enhanced performance on reasoning tasks.\nBongLLaMA2-7B-Instruct: This is the instruction-tuned version of BongLLaMA2-7b-Base. It has been fine-tuned on the bangla-alpaca-orca dataset, significantly enhancing its ability to follow instructions and perform various NLP tasks in Bengali. Like its base counterpart, it maintains the 4,096 token context window. The instruction-tuning process improves the model's task-oriented language understanding and generation capabilities in Bengali.\nBongLLaMA3-8b-Base: Built on the LLaMA 3 architecture, this 8 billion parameter model represents an advancement in the BongLLaMA series. It has been pre-trained solely on the Bengali subset of the CulturaX dataset. A key improvement is its expanded context window of 8,192 tokens, representing a 100% increase from LLaMA 2 and a 300\nBongLLaMA3-8B-Instruct: This is the instruction-tuned variant of BongLLaMA3-8b-Base, fine-tuned on the bangla-alpaca-orca dataset. It maintains the 8,192 token context window, offering enhanced capabilities in understanding and executing specific instructions in Bengali. The expanded context window and instruction-tuning make this model particularly suitable for complex, task-oriented applications in Bengali.\nBongLLaMA3.1-8b-Base: This model incorporates improvements from the LLaMA 3.1 architecture while maintaining 8 billion parameters. It is pre-trained on the Bengali subset of CulturaX. The most significant advancement is its dramatically expanded context length of 128,000 tokens. This represents a 1,462.5% increase from LLaMA 3, a 3,025% increase from LLaMA 2, and a massive 6,150% increase from the original LLaMA. This extensive context window allows for unprecedented handling of long-form content and complex reasoning tasks in Bengali.\nBongLLaMA3.1-8B-Instruct: As the instruction-tuned version of BongLLaMA3.1-8b-Base, this model is fine-tuned on the bangla-alpaca-orca dataset. It retains the 128,000 token context window, enabling it to process extremely long documents or conversations in Bengali. This makes it ideal for tasks requiring deep contextual understanding and generation, such as long-form document analysis or extended dialogue processing.\nBongLLaMA3.2-1b-Base: This is the most compact model in the BongLLaMA series with only 1 billion parameters. Despite its smaller size, it leverages the LLaMA 3.1 architecture improvements, including the 128,000 token context window. Pre-trained on the Bengali subset of CulturaX, it aims to provide efficient Bengali language processing capabilities for resource-constrained environments.\nBongLLaMA3.2-1B-Instruct: The instruction-tuned version of BongLLaMA3.2-1b-Base, fine-tuned on bangla-alpaca-orca. It maintains the 128,000 token context window, allowing for sophisticated language tasks even on devices with limited computational power. This model demonstrates the potential for deploying advanced language models in resource-constrained environments while still benefiting from the expansive context window.\nBongLLaMA3.2-3b-Base: With 3 billion parameters, this model strikes a balance between the 1b and 8b versions. It incorporates LLaMA 3.1 architecture improvements, including the 128,000 token context window, and is pre-trained on the Bengali subset of CulturaX. It offers a middle ground for applications requiring advanced language understanding with moderate computational demands.\nBongLLaMA3.2-3B-Instruct: This instruction-tuned variant of BongLLaMA3.2-3b-Base is fine-tuned on the bangla-alpaca-orca dataset. It leverages the 128,000 token context window and delivers enhanced instruction-following capabilities in a more compact form factor compared to the 8B model. This makes it suitable for applications requiring advanced Bengali language understanding and generation within moderate computational constraints. All models in the series benefit from being specifically trained on Bengali language data, with the base models pre-trained on the Bengali subset of CulturaX and the instruct models fine-tuned on bangla-alpaca-orca, tailoring them for Bengali language processing tasks."}, {"title": "A.2 Pre-training BongLLama 2 & 3", "content": "A.2.1 BongLLama 7B tokenizer\nTable 2 illustrates the difference in tokenization between the original LLaMA tokenizer and our Bangla-LLaMA tokenizer. The example sentence \"\u0986\u09ae\u09be\u09b0 \u099c\u09a8\u09cd\u09ae \u09ac\u09be\u0982\u09b2\u09be\u09a6\u09c7\u09b6\u09c7 (I was born in Bangladesh)\" is tokenized using both methods. The Bangla-LLaMA tokenizer achieves a more compact representation with only 15 tokens compared to 32 tokens from the original LLaMA tokenizer, demonstrating improved efficiency in processing Bangla text."}, {"title": "A.2.2 Configuration", "content": "Key details of our pre-training configuration include:\nEach model was trained for one epoch over the"}, {"title": "A.3 GPT-40 Parameters for Automatic Evaluation", "content": "The evaluation settings are crucial for maintaining the consistency and reliability of results. Table 4 outlines the generation parameters adopted during model evaluations:"}, {"title": "A.4 Instruction-Tuning Configuration", "content": "The fine-tuning is executed under a 16-bit floating-point precision setting, which optimizes the balance between computational efficiency and numerical precision. Table 5 shows the summary of the fine-tuning hyperparameters used in our models:"}, {"title": "A.5 Task Details", "content": ""}]}