{"title": "How Do Students Interact with an LLM-powered Virtual Teaching Assistant in Different Educational Settings?", "authors": ["Pratyusha Maiti", "Ashok K. Goel"], "abstract": "Jill Watson, a virtual teaching assistant powered by LLMs, answers student questions and engages them in extended conversations on courseware provided by the instructors. In this paper, we analyze student interactions with Jill across multiple courses and colleges, focusing on the types and complexity of student questions based on Bloom's Revised Taxonomy and tool usage patterns. We find that, by supporting a wide range of cognitive demands, Jill encourages students to engage in sophisticated, higher-order cognitive questions. However, the frequency of usage varies significantly across deployments, and the types of questions asked depend on course-specific contexts. These findings pave the way for future work on AI-driven educational tools tailored to individual learning styles and course structure, potentially enhancing both the teaching and learning experience in classrooms.", "sections": [{"title": "1. INTRODUCTION", "content": "The integration of Generative AI tools into classroom settings marks a significant evolution in educational technologies, shifting towards more interactive and personalized learning experiences. As these technologies become increasingly prevalent, understanding the dynamics of real interactions between students and AI becomes crucial. This understanding not only helps in assessing the effectiveness of AI tools in enhancing learning outcomes but also in refining their design to better address student needs and behaviors.\nJill Watson is a conversational vitual teaching assistant that answers students questions and engages them in extended conversations based on teacher-prescribed courseware using LLMs and retrieval augmented generation in the backend[14]. Jill Watson has been equipped with OpenAI's GPT-3.5 Turbo model, accessed via the OpenAI API, and coupled with several other technologies to facilitate more nuanced, context-aware, and safe interactions with students. Jill has been deployed in both online and offline classrooms [10] across different educational institutes and courses. This paper examines student interactions with Jill Watson, to understand how AI-based educational tools may engage students in meaningful and deeper learning experiences.\nThe paper has two main contributions: (i) We report trends in usage of an LLM-powered conversational question-answering (QA) agent observed through organic interactions of students with the tool deployed in classroom settings. This analysis provides insights into how students interact with the tool naturally, without guided prompts or interventions. (ii) Our findings reveal that by supporting a diverse range of student questions across varying levels of cognitive complexity, the QA agent facilitates and encourages students to pose more complex, higher-order questions. Such interactions not only foster deeper cognitive involvement but also potentially boost student motivation by engaging them in more challenging and intellectually stimulating discussions.\nIn subsequent sections, we provide theoretical motivation for our evaluation and describe the datasets, tools and analysis employed to study the impact of Jill Watson on frequency of usage and complexity of interactions. Section 2 explores previous work on student usage of AI tools in classrooms and discusses our work in this context. Section 3 details the various sources of data and tools for analyzing student interaction with the tool as well as the evaluation metrics focusing on the trends of tool usage and the cognitive dimensions of student queries. In section 4, we present our findings and compare usage trends across two different educational settings. We conclude with a summary of key takeaways, limitations and future work on studying the impact of AI tools in classrooms."}, {"title": "2. RELATED WORK", "content": "Since the introduction of ChatGPT by OpenAI\u00b9 in November 2022, there has been a rising interest in exploring its applications within educational settings. The capability of ChatGPT to mimic human-like conversations and generate coherent, detailed responses [16] has been particularly noted for its potential in educational environments. Previous research [12] categorizes potential education functions for ChatGPT to work as a virtual teaching assistant into learning support answering questions, summarizing, and facilitating collaboration and assessment tools concept checking, exam preparation, drafting, and feedback provision. Empirical studies such as those conducted by Gilson et al. [7] observed that responses generated by ChatGPT are structured in a way that they could lead to more in-depth questioning and stimulate students to use their knowledge and reasoning abilities. Conversely, Rudolph et al. [13] warned against relying on ChatGPT as a substitute for critical thinking and originality. Despite several challenges, studies identify the potential of using ChatGPT and similar generative AI tools in classrooms as learning aid in form of teaching assistants or personalized tutors[1].\nPreviously, Goel and Polepeddi introduced a virtual teaching assistant, Jill Watson [4, 8] built on top of IBM's Watson platform. Jill answered students' questions on course logistics in online discussion forums and was trained on historic human TA-student interactions.\nThe current version of Jill Watson is designed to engage students in extended conversations on verified courseware for a classroom. Taneja et al. [14] discuss the foundational architecture of Jill Watson, highlighting the design decisions that made Jill's responses contextually relevant, safe and robust to toxic prompting. Kakar et al.[10] introduced modifications to this architecture to tailor Jill for classroom use. Our study focuses on student interactions with Jill deployed in real classrooms along two dimensions usage patterns and the nature of questions posed by students, thereby enhancing our understanding of Jill's educational impact and effectiveness.\nThe existing literature predominantly relies on surveys to examine student perceptions of such tools. These studies provide valuable insights into students' attitudes and self-reported experiences but may not fully capture the nuances of real-time, interactive dynamics. For instance, previous research often focuses on students' perceived benefits and drawbacks without observing how students actually use and react to these tools in live scenarios [3]. In this paper, we report trends in usage observed through organic interactions of students with the AI tool in classrooms.\nThe application of Bloom's Taxonomy [2] in educational settings has been extensively explored, mainly for designing educational objectives and assessment tasks. The original taxonomy consists of six levels - Knowledge, Comprehension, Application, Analysis, Evaluation, and Synthesis. Each level represents a step in the cognitive process, from basic recall of facts to higher-order thinking skills such as critical analysis and creative problem-solving. Krathwohl (2002)[11] provided a foundational revision of Bloom's original cognitive taxonomy by emphasizing the dynamic nature of learning objectives and categorized them into cognitive processes that are crucial for structuring educational content and assessments. We have adapted this framework to analyze the cognitive complexity of student questions. Cognitive complexity refers to the level of mental processes required to perform a task or understand a concept, ranging from basic recall and comprehension to higher-order thinking skills such as analysis, synthesis, and evaluation. Questions that demand higher-order thinking skills reveal a deeper level of understanding and engagement with the educational content[5]."}, {"title": "3. METHODOLOGY", "content": "In this paper, we will examine student interactions from real classroom deployments with Jill Watson. Our goal is to understand how desirable characteristics of LLMs enhance tool usage and whether inherent adaptability of LLMs has the capability to engage students in critical thinking.\nTo achieve this, we have divided our work into three focus areas: (1) investigate usage frequencies, (2) explore tool utility by examining which document students ask most questions from and examining the distribution of these questions over time throughout the semester, and (3) explore the complexity of questions that students ask from the tool under different educational settings. In our context, an education setting refers to a distinct classroom that Jill is deployed in, marked by the course structure and contents, and the student body taking the course. In the subsequent sections, we will expand on each of these focus areas from the point of view of theory, data, and tools used for analysis."}, {"title": "3.1 Agent Design", "content": "To examine the effect of LLM integration into Jill Watson on frequency of usage, we will explore student interactions with Jill Watson both pre and post LLM integration. Hence, we will briefly describe the architecture for both these versions of the tool to highlight how LLM integration significantly enhanced tool functionality and how it reflects on adoption.\nTable 1 shows which version of Jill has been deployed in the academic semesters under study. To provide context for our findings later in the paper, we will also describe the courses where Jill has been deployed."}, {"title": "3.1.1 Legacy Jill Watson", "content": "Pre-LLM version of Jill[9] employed a two-dimensional hybrid classification process as shown in Figure 1. A pre-trained NLP model functioning as an intent classifier classifies student questions into predefined question categories. A rule-based semantic processor utilizes the classified intent to select an appropriate response template for generating responses. This iteration of Jill Watson was extensively deployed in classes at Georgia Institute of Technology. For our study, we analyze interaction and testing data from deployment of this version in one particular classroom - the Knowledge-Based AI (KBAI) course at Georgia Tech to understand how the use of Jill Watson has evolved with the introduction of more advanced technologies and LLMs."}, {"title": "3.1.2 LLM-powered Jill Watson", "content": "Post LLM integration, Jill Watson, described by Taneja et al.[14], features a modular, Retrieval Augmented Generation (RAG)-based pipeline for question-answering. Figure 2 shows a simplified version of the architecture that provides an intuitive understanding of the LLM-powered QA process. Jill retrieves conversation history for a student from memory, relevant information from the knowledge base (which is pre-configured by processing verified courseware received from instructors), and constructs a prompt for OpenAI API to generate a response restricted to the given context. The response is validated before being sent to the student. By restricting the context to relevant information sourced exclusively from verified course materials and combining moderation modules to ensure the safety and accuracy of responses, Jill goes beyond ChatGPT in response relevancy, accuracy and safety, which are further elaborated in [14]. Further design modifications to tailor Jill for classroom integration are provided by Kakar et al. [10]. These adjustments facilitate Jill Watson's deployment across diverse educational environments, providing rich insights into student interactions."}, {"title": "3.1.3 Deployments", "content": "Table 2 lists the courses where LLM-powered Jill has been deployed in Spring 2024, the institutes where these courses are taught, and the courseware to answer student questions, and the main course deliverables. These details serve to provide context for the variability within each deployment. From Fall 2021 through Fall 2022, the legacy version of Jill Watson was deployed in the Knowledge-Based AI (KBAI) course at Georgia Tech. Starting in Fall 2023, the updated version of Jill Watson has been deployed not only in the KBAI course but also in the Introduction to Cognitive Science (CogSci) course at Georgia Tech, and expanded to the English: Composition and Rhetoric course at Wiregrass Georgia Technical College.\nThe KBAI and Cognitive Science courses are part of Georgia Tech's Online Master of Science in Computer Science program\u00b2, designed primarily for part-time graduate students who typically maintain full-time employment. These courses aim to provide advanced knowledge and skills in AI and Cognitive Science through a flexible online format. In contrast, Wiregrass College, a two-year community college within the Technical College System of Georgia (TCSG) recognized for its workforce development programs, integrates Jill Watson into its undergraduate English course. The course teaches various modes of writing and includes a review of standard grammatical and stylistic usage in proofreading and editing.\nIn each of these courses, Jill Watson is deployed as a private conversational assistant within the Learning Management System (LMS) using Learning Tools Interoperability (LTI)\u00b3. Jill can be accessed through the course's Canvas\u2074 or Blackboard page. This integration ensures that students can access assistance directly within their course platform, encouraging a seamless and supportive learning experience. Once a question is asked on the platform, a student typically receives a structured response within a few seconds. Much of the delay is caused due to multiple calls to OpenAI's API."}, {"title": "3.2 Data Collection", "content": "For this study, we use data from three different sources: (i) student interaction data from deployments of the LLM-powered Jill Watson in Spring 2024, and (ii) student interaction data from deployments of the pre-LLM version of Jill Watson in the KBAI class from Fall 2021 to Fall 2022 and (iii) a set of synthetically generated context-question-response tuples based on various courseware for Spring 2024."}, {"title": "3.2.1 Student Interaction with Jill Watson", "content": "We collected and stored exhaustive student interaction data for each deployment of Jill Watson in various courses in a persistent memory on the hosting server, following established protocols. This dataset includes de-anonymized student IDs, timestamps, questions, responses, and feedback, along with inputs and outputs for each component of the question-and-answer (QA) pipeline. This structured data collection allows for a robust analysis of usage patterns."}, {"title": "3.2.2 Student Interaction with Legacy Jill", "content": "We have sourced the number of questions asked of legacy Jill Watson in the KBAI course spanning from Fall 2021 to Fall 2022. We will use these numbers to compare the frequency of usage with the current version of Jill."}, {"title": "3.3 Pre-deployment Testing of Jill Watson", "content": "We generate synthetic context-question-answer (CQA) tuples using Google's PaLM API for supported courseware, ranging a variety of documents like syllabi, textbook, instructor handouts and guides across five different courses. These question sets are used to test Jill's accuracy for courseware for a given class prior to deployments. We prompt the API to create a diverse set of questions that cover a range of cognitive levels and types (e.g., factual, conceptual, procedural) as well as different question types (e.g., is, which, when, where, why, how). To ensure these CQAs are representative of the questions Jill Watson is expected to address, we incorporate guidelines that require the API to generate questions needing multiple pieces of information and to reference specific values within the course materials. This approach helps simulate the complexity and diversity of real student inquiries. These synthetic CQAs are used as a benchmark to compare the expected versus actual usage of Jill Watson in classroom settings."}, {"title": "3.4 Classifying Question Complexity with Fine-Tuned BERT", "content": "To understand the type and complexity of questions asked in classrooms, we use cognitive levels defined by Bloom's Revised Taxonomy [11]. We compiled a dataset of 3,600 labeled questions derived from various publicly accessible sources [6, 15]. This dataset contains questions across various disciplines and labels them into one of the six cognitive levels defined in the taxonomy - Knowledge, Comprehension, Application, Analysis, Synthesis, and Evaluation, which range from basic recall of facts to generating new ideas.\nFirst, we investigated zero-shot classification techniques using GPT-3.5, GPT-4, and BERT for categorizing student queries. Given the nuanced nature of this classification, zero-shot methods proved ineffective, with accuracies lower than 50%, prompting us to explore fine-tuning strategies on a curated dataset to improve performance.\nFor fine-tuning, we divided the dataset with 3600 questions into training and testing subsets with a 75:25 split. Standard preprocessing methods were applied to each question to ensure consistency across the data. These methods included tokenization, stopwords removal, and text normalization, which were essential for minimizing variability and enhancing the efficiency of model training. We opted for a fine-tuning strategy on the bert-base-uncased BERT model to classify questions based on cognitive complexity. The model underwent fine-tuning over five epochs, using a batch size of 32 and a learning rate of 2e-5, parameters selected from initial tests that indicated optimal results. We settled on using the fine-tuned model that achieved an accuracy of 91.9%."}, {"title": "4. RESULTS AND DISCUSSION", "content": "In this section, we analyze usage patterns, interaction frequencies, the complexity of questions, and the qualitative aspects of Jill Watson's responses, to provide a comprehensive understanding of how LLMs enhance the educational experience. Each subsection addresses specific aspects of the tool's performance and interaction with students across different educational settings."}, {"title": "4.1 Usage Patterns", "content": "To explore the breadth of tool usage, we compared the number of questions asked in the KBAI class across six semesters, showing a substantial increase post-LLM integration. The average number of questions rose from 84.75 pre-LLM to 2173.0 post-LLM, as seen in Figure 3, likely due to the enhanced capabilities of Jill Watson and increased student familiarity with AI tools.\nTo understand our sample size and set the stage for further analysis of student interaction with Jill, we look at the frequency of usage and number of unique users that used Jill multiple times over the course of the Spring 2024 semester."}, {"title": "4.2 Interaction Patterns", "content": "Since we have established that Jill has been used extensively in courses, we next look at which category student questions fall into, and how they vary across the semester. Jill has been equipped to answer questions based on courseware which can be bucketed into two main categories - course content which includes textbooks, guides, handouts and course logistics which include course syllabi and schedule.\nBy analyzing the count of questions related to logistics versus content on a weekly basis, we gain a preliminary understanding of the usage patterns. Logistics questions tend to be straightforward, due-diligence queries that help students navigate the course structure. Higher frequency early in the semester typically reflects students' efforts to understand course logistics. Content questions indicate deeper cognitive engagement, as students seek to understand and apply course material. High frequency of content questions suggests heightened cognitive engagement.\nFigure 4 shows the number of questions asked weekly for each course in Spring 2024. For KBAI, we observe a substantial spike in 'course content' inquiries during week 8 and week 17, which coincide with the examination periods, suggesting that students find Jill useful for looking up information for exam preparation or testing their understanding of concepts. The significant increase in usage also coincides with the increased complexity of topics covered as the semester advances.\nIn contrast, the Intro to Cognitive Science course demonstrates significantly higher engagement, particularly in the 'course content' category, with consistent activity throughout the semester. The peak in logistics-related questions during the early weeks may reflect an initial exploration of concepts before stabilizing as the semester progresses. The sustained high volume of content-related questions underscores the tool's effectiveness in supporting complex conceptual understanding and course-specific learning. The data for the English course shows relatively low overall interaction across all categories, with notable peaks in 'course content' queries during the mid-semester weeks.\nOur observations suggest two takeaways. First, we notice higher frequency of logistics questions early in the semester which is unsurprising as students navigate the course. Second, in the KBAI and CogSci classes, there is a clear pattern of high content-related questions, but the pattern differs with course structure. Courses with evenly distributed deliverables might encourage consistent engagement and tool usage, leading to a more steady learning process. Conversely, courses with concentrated assessments might need targeted interventions to help students prepare effectively.\nWhat stands out is the high adoption of Jill Watson in these courses, reinforcing the premise that students view Jill as a valuable tool for navigating course content. This success can be attributed to Jill's advanced design, which addresses some of the inherent limitations of LLMs. By using RAG to restrict context and incorporating advanced prompting, Jill ensures relevant and safe answers to student questions while suppressing biases and hallucinations."}, {"title": "4.3 Complexity of Questions", "content": "First, we look at the complexity of the questions that Jill is tested on, pre-deployment, to set a baseline of what we expected Jill to be primarily used for. Next, to assess actual usage, we will look at the complexity of the real student questions collected.\nWe classified all questions in the CQAs using the fine-tuned BERT model described in the previous section. Since we explicitly prompt PaLM API to generate a variety of questions as part of the CQAs for testing, we expect the CQAs to somewhat reflect types of questions that would naturally arise from the courseware. Table 7 shows the distribution of these test questions across the six cognitive levels for each of the three courses Jill was deployed in Spring 2024. We observe that a majority of the questions fall under the 'Knowledge' category or deal with recalling or recognizing factual information. This provides a baseline for the kind of questions Jill has tested on and the expectation that Jill Watson would perform well as an information retrieval tool.\nTable 8 shows the percentage of real student questions within each cognitive level. The numbers show that the 'Knowledge' dimension contains the highest portion of questions across all courses. This aligns with our expectation that Jill Watson could serve as an advanced information retrieval tool in classrooms. We also notice that, in general, there was a notable presence of higher-order cognitive questions, such as those requiring analysis and evaluation, challenging the tool to deliver beyond basic factual responses. These numbers are observed to vary by course, leading us to speculate whether how a course is structured plays a role in this distribution.\nTo examine the relationship between course/semester and the cognitive dimensions of student questions, we ran a chi-squared test of independence ($\\chi^2(15) = 194.53$, p < 0.0001) on the question frequencies for each dimension, as seen in Table 9, suggesting that the distribution of question types across cognitive dimensions is indeed strongly dependent on the course and semester. Table 10 shows some of the real student questions per cognitive dimension to further shed a light on the type of questions that students have been asking Jill.\nWhen analyzing the usage and complexity of questions over the entire semester, we also notice a relatively uniform distribution of questions in case of Intro to Cognitive Science, which is inline with how critical deliverables are distributed throughout the semester. In contrast, we have previously seen that in case of KBAI, there is a spike in Jill Watson usage during the exam weeks. Figure 5 illustrates that this period also corresponds with a surge in critical questions from students. This leads us to believe that the way a course is structured has an influence on the critical question-asking behavior of students. However, further studies through student survey will be required to quantify the dependence between course structure and critical-question asking behavior.\nIt is important to note that the number of questions asked in the English course is significantly lower than that in the KBAI or Intro to Cognitive Science course. Therefore, the observed distribution of questions and variations between semesters for this course may not necessarily indicate a substantive dependence on the pedagogical focus."}, {"title": "4.4 Qualitative Analysis of Responses", "content": "The integration of Jill Watson with LLMs has introduced several desirable characteristics in its responses which could further have an effect on usage. Qualitative analysis of the responses generated by Jill Watson reveals structured and educational patterns in its output such as:\n\u2022 Definition and Examples: Responses start with a clear definition of the concept, followed by relevant examples, reinforcing theoretical points with practical instances.\n\u2022 Comparison and Contrast: Another frequent response pattern involves comparing and contrasting different concepts, which is systematically followed by examples. This method aids students in understanding differences and similarities between concepts, fostering deeper analytical skills.\n\u2022 Summary of Key Points: Responses typically conclude with a summary of key points, ensuring clarity and retention of the discussed information.\n\u2022 Adaptive Tone and Sentiment: Responses are tailored to the tone and sentiment of the student's query, maintaining politeness and engagement, which is specifically prompted to the agent.\nTables 11 show one such real student interaction with Jill Watson that supports our observations listed above."}, {"title": "4.5 Student Feedback to Jill's Responses", "content": "The Jill Watson interface allows students to provide feedback for Jill's responses by marking them as either helpful or not helpful. Across all three courses, we observed a substantial amount of positive feedback, as seen in Table 12. Specifically, in the KBAI course, 191 responses (78% of responses that received feedback) were marked helpful compared to 53 marked not helpful. In the Cognitive Science course, 47 responses (67% of responses that received feedback) were marked as helpful while 24 were not, while for the English course, 17 (94% of responses that received feedback) were marked helpful and 1 marked not helpful. Responses that are marked as helpful typically contain elements such as examples and summaries, which aid in understanding the material. The consistently high positive feedback across different courses highlights the effectiveness of Jill Watson in providing valuable assistance to students."}, {"title": "5. LIMITATIONS", "content": "Our study has several limitations that must be acknowledged and that pave the way for future work. While we have identified a trend towards higher-order question-asking in certain course structures, the study does not deeply explore the underlying factors driving these behaviors. More granular investigations into how specific pedagogical approaches, course designs and external factors like digital literacy influence student interaction with AI tools are necessary to draw more precise conclusions. Second, while our analysis covers a range of question types and cognitive levels, it does not fully address the quality of the responses provided by Jill Watson. Future studies should include a more detailed assessment of response accuracy and pedagogical soundness to ensure that the tool not only engages students but also supports effective learning."}, {"title": "6. CONCLUSIONS", "content": "Our study shows that incorporating LLMs into Jill Watson has led to increased tool usage and exploration among students under varying settings. We have observed that a significant portion of student inquiries are related to course content, indicating that students are utilizing Jill as a resource for understanding classroom material. We also see a notable presence of questions with high cognitive complexity, suggesting that students enagaged deeply with the material and used Jill for more than just fact-seeking. Although further work is needed, our qualitative analysis of Jill's responses highlights several ideal features that may encourage students to ask more critical questions. Jill's capability to address questions of varying complexities, ensure the relevance and safety of responses, and mitigate biases and hallucinations through the implementation of RAG demonstrates the potential of AI-based educational tools to engage students in meaningful and deeper learning experiences.\nThe frequency and complexity of these higher-order questions vary significantly with the course structure, suggesting that the way a course is designed and implemented can influence how students utilize AI-based educational tools. Courses that emphasize problem-solving tend to elicit a higher frequency of higher-order cognitive questions, whereas courses focused on writing primarily generate questions centered on understanding and comprehension. Moreover, the utility of Jill evolves over the semester, indicating that adapting to student needs is a critical factor in design of such tools."}]}