{"title": "Projection Optimization: A General Framework for Multi-Objective and Multi-Group RLHF", "authors": ["Nuoya Xiong", "Aarti Singh"], "abstract": "Reinforcement Learning with Human Feedback (RLHF) is a widely used fine-tuning approach that aligns machine learning model, particularly Language Model (LM) with human preferences. There are typically multiple objectives driving the preference, hence humans find it easier to express per-objective comparisons rather than a global preference between two choices. Multi-Objective RLHF (MORLHF) aims to use per-objective preference feedback and achieve Pareto optimality among these objectives by aggregating them into a single unified objective for optimization. However, nearly all prior works rely on linear aggregation, which rules out policies that favor specific objectives such as the worst one. The only existing approach using non-linear aggregation is computationally expensive due to its reward-based nature and the need for retraining whenever the aggregation parameters change. In this work, we address this limitation by transforming the non-linear aggregation maximization problem into a series of sub-problems. Each sub-problem involves only linear aggregation, making it computationally efficient to solve. We further extend our framework to handle multi-group scenarios, where each group has distinct weights for the objectives. Our method enables achieving consensus or maximizing the aggregated objective across all groups. Theoretically, we demonstrate that our algorithmic framework achieves sublinear regret and can be easily adapted to a reward-free algorithm. Empirically, leveraging our theoretical insights, we propose a nearly training-free algorithm once the optimal policies for individual objectives are obtained.", "sections": [{"title": "1 Introduction", "content": "In recent years, there has been considerable effort to fine-tune a machine learning model, particularly Large Language Model (LLM), to perform better on particular tasks. RLHF is a popular fine-tuning approach, which receives the human's preference feedback and aligns the LLM model with human values using fine-tuning. Standard RLHF exploits human preference feedback between two outputs to maximize the expectation of the implicit or explicit reward function.\nHowever, there are two main challenges for the application of RLHF in the real world. First, standard RLHF only maximizes a single reward function. However, people often find it hard to evaluate choices in an overall sense as, in reality, there are often multiple objectives. For example, comparing two papers or essays overall is harder than comparing them on specific objectives such as novelty, clarity, correctness etc. Similarly, recommending a city for vacation is harder than comparing cities on food options, nightlife, safety, etc. Each objective has its own implicit or explicit reward function, and the LLM needs to achieve a Pareto optimal trade-off between them by, for example, maximizing an aggregation of these reward function. Second, there are multiple groups of users in the real world who may prefer different aggregations of the objectives. For example, groups with different genders, political views, marital status, etc. This requires that the LLM either (a) satisfies the requirements of all the groups simultaneously, or (b) optimizes some aggregation across multiple groups.\nMulti-Objective Problem There are some works [Rame et al., 2024; Yang et al., 2024; Shi et al., 2024] that consider balancing the utilities of multiple objectives to get the Pareto optimal point or maximize the average expectation. Some works [Zhong et al., 2024; Park et al., 2024] consider multi-party problem in which each reward represents a group, which can also be regarded as a multi-objective problem. We assume that we have m different objectives, and each objective has its own reward function ri(x,y)(1 \u2264 i \u2264 m). Each reward corresponds to an objective of the response y like safety or helpfulness of the LLM. Nearly all of the previous work consider only linear aggregation, i.e., optimizing r(x,y) = \\sum_{i=1}^m a_i r_i(x, y), where a = {ai}i\u2208[m] is the weight of all objectives that is assumed to be known.\nHowever, this kind of aggregation may not lead to an LLM that treats all objectives fairly. For example, the LLM may favor one objective significantly at the expense of another. In social choice theory, certain natural axioms such as monotonicity, symmetry, scale invariance, etc. which apply to multi-objective aggregation as well, lead to a more general function class [Cousins, 2021]\nr(x,y) = (\\sum_{i=1}^m a_i r_i(x,y)^p)^{1/p} ,p\u2264 1,\t\t(1)\nThe general p-norm aggregation with p < 1 promotes fairness across multiple objectives, which is particularly useful when aiming for a machine model that achieves a balanced performance among different objectives. Only one paper [Zhong et al., 2024] addresses the p-norm aggregation setting. In that work, the authors first learn a reward function for each objective, aggregate them into a new reward, and then attempt to optimize this new reward directly. However, this reward-based approach is computationally inefficient compared to the reward-free, DPO-based algorithm [Rafailov et al., 2024]. Moreover, it requires retraining the entire policy whenever the aggregation method changes, which becomes even more time-consuming.\nTo reduce the computational cost of the reward-based RLHF algorithm, the paper [Shi et al., 2024] shows that for p = 1, once the optimal policy \\pi_{r_i} for each individual objective is obtained, the optimal policy \\pi_{r} for the linear averaged sum can be calculated as \\pi_r(y | x) \\propto \\prod_{i=1}^m \\pi_{r_i} (y | x)^{a_i}. However, the derivation heavily depends on the linear structure of the aggregated reward r(x,y). When p\u2260 1, this approach breaks and the optimal policy cannot be written as a simple closed-form of the optimal policies of each objective. That raises the first question:\nQuestion 1: Can we derive a computationally efficient MORLHF algorithm\nwith non-linear aggregation?\nIn our work, we propose a projection-based algorithm both in offline and online preference data settings, which transforms the nonlinear objective maximization problem into a sequence of sub-problems, each involving only a linear maximization problem. Theoretically, we provide a thorough analysis for both offline and online setting, showing that it can converge to the optimal policy with"}, {"title": "2 Related Works", "content": "RLHF Fine-tuning LLMs with human feedback and RL is known as RLHF. The reward-based RLHF first extracts a reward model with a Bradley-Terry (BT) assumption on human preferences, and then optimizes the reward model [Ouyang et al., 2022; Bai et al., 2022; Touvron et al., 2023; Azar et al., 2024]. On the other hand, the reward-free RLHF avoids explicit reward modeling by directly formulating the preference loss as a function of the policy and then using supervised learning [Wang et al., 2023; Rafailov et al., 2024], which is more stable and computation-friendly.\nMORLHF Multi-Objective RLHF (MORLHF) aims to align an LLM with human preferences while optimizing for multiple objectives, such as harmlessness, helpfulness, and humor. Most previous works aggregate rewards or models as the weighted sum of individual components. MORLHF [Wu et al., 2023; Bai et al., 2022] directly optimizes the aggregated reward using PPO, while MODPO [Zhou et al., 2023] provides a lightweight reward-free alternative. RS [Rame et al., 2024] combines individual models by averaging them. MOD [Shi et al., 2024] calculates the closed-form solution of the optimal policy for aggregated reward directly and derives a training-free algorithm. Only one work [Zhong et al., 2024] consider non-linear aggregation, and they optimize the aggregated reward function directly. However, this approach is computationally expensive and requires retraining when the aggregation changes. Instead, we propose a theoretical framework that can be easily adapted to a reward-free algorithm, along with a training-free empirical algorithm built on the same theoretical framework. Detailed comparisons are shown in Table 1.\nPluralistic Alignment and Preference Aggregation There is a growing body of work on aligning machine learning models with diverse preferences, accounting for different values and perspectives. The works [Chakraborty et al., 2024; Ramesh et al., 2024] focus on optimizing the worst-case group loss, ensuring that the model achieves reasonable performance across all groups. [Park et al., 2024; Sorensen et al., 2024; Conitzer et al., 2024] explore how to aggregate preferences using social choice and voting theory, outlining a high-level roadmap for pluralistic AI alignment. [Ge et al., 2024] technically demonstrate that the BTL model fails to satisfy well-known standards in social choice theory and propose a novel rule-based approach for learning reward functions. [Chen et al., 2024] further study the generalization of the BTL model and introduce an ideal point model that better accommodates diverse groups."}, {"title": "3 Preliminaries and Notations", "content": "Denote the prompt space of the LLM as X and the response space as Y. The distribution \u03c1\u2208 \u0394(X) represent the distribution of the prompt. A policy \u03c0 : X \u2192 \u2206(Y) represents an LLM that generates a response distribution given prompt x. In RLHF, we assume that we can get a pre-trained LLM \\pi_{ref} that is usually trained on supervised data. The goal is to fine-tune the pre-trained model to align the model with the human preference on one particular task. To be more specific, given prompt x ~ p, LLM can generate two responses Y1, Y2, then the human gives a preference feedback on the response pairs as either y1 \\succ y2 or y1 \\prec y2. The responses Y1, Y2 are labeled as yw, yl respectively with probability P(y1 \\succ y2 | x), and are labeled as yl, yw with probability 1 \u2013 P(y1 \\succ y2 | x). It is further assumed that the human preference is modeled by a Bradley-Terry (BT) model with the reward function r*(x, y) : X \u00d7 \u0423 \u2194 [0, B]:\nP(y_1 \\succ y_2 | x) = \\sigma(r^*(x, y_1) - r^*(x, y_2)),\nwhere \u03c3(z) = \\frac{1}{1+exp(-z)} and B > 1. Given the reward function r, the optimal policy \\pi_{\\rho}^* = arg max_{\u03c0} J(\u03c0) maximizes the expected reward function, with an additional KL divergence term that prevents the policy from deviating too much from \\pi_{ref}:\n\\pi^* = arg max_{\u03c0} J(\u03c0) = arg max_{\u03c0} \\mathbb{E}_{x\\sim\\rho} \\mathbb{E}_{y\\sim\\pi(\\cdot|x)} [r^*(x, y) \u2013 \\beta D_{KL}(\\pi || \\pi_{ref})].\t\t(2)\nIn this paper, we consider both offline and online RLHF. For the offline RLHF setting, the LLM has access to a pre-collected offline data D consisting of prompts and corresponding winning and losing responses, and the expectation in the optimal policy is calculated on the offline data. For the online setting, at each round LLM can generate two responses Y1, Y2 following the policy \u03c0, and then receive the preference feedback by human for data collection.\nWe assume there are m known representations {\\phi_i(x,y) \u2208 \\mathbb{R}^d}_{i\u2208[m]} and the corresponding reward function class {r_i(x,y) = \\theta_i^T\\phi_i(x,y) \u2208 [0, B], ||\\phi_i||_2 \u2264 1, ||\\theta_i||_2 \u2264 B} for each objective i \u2208 [m]. The true reward r_i^* for objective i can be written as r_i^*(x,y) = (\\theta_i^*)^T\\phi_i(x, y). This assumption is purely theoretical. In practice, the reward can be parameterized as r^{\u03b8} using a neural network, and our practical algorithm 5 also does not rely on this assumption."}, {"title": "3.1 Multi-Objective Learning", "content": "Since the preference only contains the information of ri(x, y1) \u2014 ri(x, y2) for each objective i, rewards are invariant to constant shifts in feedback. Follow [Cen et al., 2024], we can assume there is a known policy \\pi_{base} and constant C, such that for each i \u2208 [m], the reward parameter space \\Theta_i is defined as\n\u0398_i = {\u03b8 \u2208 Rd : \\mathbb{E}_{\\pi_{base}}(\u03b8i, \u03c6i(x, y)) = C}.\t\t(3)\nWe assume that there are m different objectives, and each objective has reward function ri(x, y) \u2208 [0, B] for i \u2208 [m]. As discussed in the introduction, we apply the definition of social welfare function in social choice theory to multi-objective setting and consider the weighted p-norm aggregation across objectives\nr(x,y) = (\\sum_{i=1}^m a_ir_i(x,y)^p)^{1/p} ,p\u2264 1,\nwhere a \u2208 \u0394m-1 are weights of the objectives. Note that for positive rewards, aggregation yields Pareto optimality.\nThe goal is to find the optimal policy for the aggregated reward function r. One natural approach to solving multi-objective RLHF is to first learn a reward model for each individual objective, and then aggregate these models to formulate a new reward. Finally, RL methods like PPO can be applied to optimize this new reward. However, this reward-based approach is significantly more computationally inefficient and unstable compared to reward-free approaches, such as DPO [Rafailov et al., 2024]. Additionally, it requires retraining the entire model for all possible reward aggregations, which becomes time-consuming when the aggregation parameters change. In this work, we first provide a theoretical algorithmic framework for multi-objective RLHF, which naturally leads to the derivation of a reward-free algorithm. Based on this theoretical framework, we propose a nearly training-free practical algorithm that incurs almost zero computational cost once the optimal policy for each objective is obtained.\nPrevious techniques cannot be easily applied to this setting. In fact, for the linear aggregation when p = 1, the paper [Shi et al., 2024] finds that the optimal policy \\pi_{r} can be written as a closed-form of the optimal policy \\pi_{r_i} as \\pi_r(\u00b7 | x) \\propto \\pi_{ref}(\u00b7 | x)\u00b7exp(\\sum_{i=1}^m (a_i r_i(x))),\nand conduct a decoding algorithm MOD using this derivation. By the linear aggregation r(x,y) = \\sum_{i=1}^m a_i r_i(x,y) and \\sum_{i=1}^m a_i = 1, it is easy to verify that \\pi_r(y | x) \\propto \\prod_{i=1}^m \\pi_{r_i}(y | x)^{a_i}. Hence, one natural reward-free algorithm is to first learn the optimal policy \\pi_{r_i} for each objective using DPO, then calculate the optimal policy \\pi_r. It is also a training-free algorithm once the optimal policy for each objective is known. However, when we choose the general aggregation with p < 1, this derivation will fail due to the non-linear structure of the reward, making the problem much more complicated.\nTo avoid this technical difficulty, we draw inspiration from RL with Blackwell-approachability [Yu et al., 2021], which focuses on minimizing the distance between the reward vector and a specified target set. This approach makes the problem more tractable since we can incorporate the non-linear aggregation into the definition of the target set. To be more specific, a target set W \\subset \\mathbb{R}^m is a convex set that is defined by\nW_{p,c}^a = { z\u2208 \\mathbb{R}^m : (\\sum_{i=1}^m a_i z_i^p)^{1/p}  \u2265 c}"}, {"title": "3.2 Multi-Group Learning", "content": "Beyond the single group setting, we also study the multi-group setting, where each group has a different aggregation approach (parameterized by c,p and a). For each group n, we assume there is a target set\nW^{(n)} = {z \u2208 \\mathbb{R}^m : (\\sum_{i=1}^m a_i^{(n)} z_i^{p^{(n)}})^{p^{(n)}} \u2265 c^{(n)}}\nrepresenting the aggregation rule across objectives for them. We consider two types of goals that represent the effectiveness of alignment across diverse groups.\nConsensus The first goal is called \"consensus\", in which we wants to minimize the distance between the expected reward vector and the intersection of all target sets from diverse groups. Formally, the goal is to choose the optimal policy that minimizes the Euclidean distance\n\\pi^* = arg min_{\u03c0} d (S(\u03c0), \\bigcap_{n=1}^N W^{(n)})\t\t(5)\nMalfare Function Minimization Another goal is to minimize the aggregated malfare function, where the malfare function for each group is the square of the distance between the expected reward vector and the group's target set. Formally, with group weight \u03b6n > 0 and \\sum_{m=1}^N \u03b6_m = 1, the goal is to find the optimal policy \u03c0* that\n\\pi^* = arg min_{\u03c0} (\\sum_{n=1}^N \u03b6_n (d\u00b2 (S(\u03c0), W^{(n)}))^{q})^{1/q} ,q\u2265 1."}, {"title": "4 Algorithms for Multiple Objectives with Linear Aggregation", "content": "In this section, we consider the simplest setting where the reward function is a linear aggregation, i.e. r(x,y) = \\sum_{i=1}^m d_i r_i(x, y), where d \u2208 \\mathbb{R}^m is called the direction. In fact, the linear aggregation can be viewed as projecting the reward vector onto a specific direction d. As we will show later, this will become an essential sub-problem in our final algorithm for non-linear aggregation.\nGiven the dataset D_i = {(x_j, (y_w, y_l))}_{j\u2208[M]} containing M data points for objective i, we provide offline and online algorithms to learn the optimal policy with respect to multiple objectives in a consistent way. Now we aim to minimize the negative log-likelihood loss of preference data\nL_i(\\theta_i) = - \\sum_{(x,y_w,y_l) \u2208 D_i} log(\\sigma(r^{\\theta_i}_i (x, y) \u2013 r^{\\theta_i}_i (x, y)))\nfor each objective i. Following [Cen et al., 2024], we can refine our estimation of the reward by adding an additional exploration term max_{\u03c0} J(r^{\\theta}, d, \u03c0) = max_{\u03c0} \\mathbb{E}_{\u03c0}[\\sum_{i=1}^m d_i r_i^{\\theta} \u2013 \\beta D_{KL}(\u03c0||\\pi_{ref})], which represents the optimism/pessimism principle of the online/offline learning process. To be more specific, for the offline and online setting, LLM learns the \u03b8offline and \u03b8online respectively by\n\\theta_{offline} = arg max_{\\theta_1,\u2026, \\theta_m} (\\max_{\u03c0} J(r^{\\theta}, d, \u03c0) \u2013 \\sum_{i=1}^m \\eta \\Omega_i(\\theta_i))\t\t(6)"}, {"title": "5 General Algorithm for Preference Aggregation", "content": "In this section, we introduce general offline and online algorithms that work for both linear and non-linear preference aggregation, and provide their theoretical guarantees. Both algorithms transform the non-linear aggregation into a series of linear aggregation sub-problem, using Algorithm 1 and 2 as their core sub-procedures."}, {"title": "5.1 Offline Algorithm", "content": "Now we introduce our algorithm Multi-Objective Projection Optimization (MOPO), which follows from the competitive RL with Blackwell-approachability literature [Yu et al., 2021]. We receive the offline data set D = {Di}i\u2208[m] which contains M data points Di for each objective i. The algorithm learns the reward or optimizes the policy directly from the offline data. Our algorithm contains T iterations. In each iteration t, we first project the reward vector on the direction dt \u2208 Rm defined in the last iteration, i.e. r(x,y) = \u2211\u2081dri(x,y), and then using the sub-procedure in the previous section to find the estimated parameter et and determine the corresponding policy wt. Finally, we derive the estimated expected reward vector Vt \u2208 Rm as (Vt); = Ent[rit (x, y) \u2013 DKL(\u03c0\u207a||\u3160ref)], and calculate the averaged reward vector as Vt = \\frac{1}{t}\\sum_{t=1}^T V_i. Finally, the direction is updated based on the projection of the estimated point V onto the target set, guided by either the consensus problem or the malfare function minimization problem. The pseudocode is in Algorithm 3.\nThe key component of our algorithm is the direction calculation in each iteration. Intuitively, the algorithm aims to optimize the reward to guide the expected reward vector toward the target set as effectively as possible. Suppose the target set is W, the direction can be calculated by dt+1 = Proj(W, V\u00b2) = \\frac{\\Pi_w(V)-V}{\\|\\Pi_w(V)-V\\|} For the consensus problem, we can substitute into W = \\bigcap_{n=1}^N W^{(n)} and get\ndt+1 = Proj(\\bigcap_{n=1}^N W^{(n)}, Vt)\t\t(9)\nFor the malfare function minimization problem, we can first calculate the projection to each target set W(n) and then aggregate them as\ndt+1 = \\sum_{n=1}^N \\frac{\u03b6_n Proj_{(W^{(n)}, V)}}{{\\sum_{n=1}^N \\|\\Pi_{(W^{(n)}, V)}\\|^(2q-1)}}\t\t(10)\nNote that if we apply MOPO with p = 1, it reduces to the classical MORLHF algorithm. This is because the direction dt = Proj(Vt, W\u2081c) = a for each t as long as c is large. However, for p \u2260 1, MOPO solves the non-linear aggregation maximization problem by transforming into a series of subproblems, in which each subproblem only contains the linear aggregation and can be easily solved using any previous algorithm. Thus, MOPO serves as a general framework for MORLHF"}, {"title": "5.2 Online Algorithm", "content": "Now we provide the online version of MOPO, which is similar to the offline setting. The main difference is the adoption optimism principle (Eq. (7)) rather than the pessimism principle (Eq. (6)). Additionally, the dataset is collected incrementally online, and we also estimate the importance weight a instead of assuming it is known.\nAdditionally, rather than assuming the weight is known, we estimate it based on the frequency with which humans report the objective. This method also works offline by using the frequency of related data in the dataset. At each round t, given a prompt xt ~ p and two responses y1 and y2, each group n identifies an objective It,(n) \u2208 [m] showing the greatest difference and provides preference feedback (yw(n), yl(n)) on that objective. The model collects the data (xt, yw(n), yl(n), It,(n)) into D(n) for all group n. Next, we model how humans select the objective index. For responses Yw and yi, the gap on objective i is quantified as |ai \u00b7 (ri(x, Yw) - ri(x, y\u0131))|, with the selection following a softmax distribution:\nP(I | a, r*, x, Yw, Y\u0131) \\propto exp(a_I \u00b7 |r(x, yw) - r (x,y1)|).\nThen if we define the likelihood function as\nL(a, D^{(n)}, \u03b8) = \\sum_{(x,y_w,y_l, I)\u2208D^{(n)}} P(I | a, x, Yw, Y\u0131, r^\u03b8),\nwe can estimate the importance weight vector for each group by MLE as\n\\hat{a}_{t,(n)} = arg max_{\u03b1\u2208\u0394_{M-1}} L(a, D^{(n)}, \\hat{\u03b8}),\t\t(11)\nwhere we use an estimated reward parameter \\hat{\u03b8}t to approximate \u03b8*. Before we present our results, we assume there is a gap between the reward obtained by following the optimal policy \u03c0* and the reference policy \\pi_{ref}. This gap is reasonable since the expected reward should be improved after fine-tuning."}, {"title": "7 Conclusion", "content": "In this paper, we study efficient multi-objective and multi-group RLHF problems under non-linear aggregation. By transforming the non-linear aggregation maximization into a series of linear aggregation maximization sub-problems, we find a computationally efficient algorithm that can converge to the optimal policy. Theoretically, we establish a general framework with converge guarantees for both offline and online settings, and the framework is also adaptable to a reward-free version. Empirically, we present a training-free framework given the reward functions and optimal policies for all objectives.\nThere are many future directions worth exploring. First, one can study how to learn the parameter p in the aggregation function like [Pardeshi et al., 2024] using the preference feedback. Second, one can further study the token-level MORLHF [Zeng et al., 2024] based on our idea. Last, it is interesting to further study the multiple preference aggregation in Stochastic Transitivity model [Fishburn, 1973] instead of BTL model, and further discuss the relationship between them and previous distortion negative results [Anshelevich et al., 2021]."}]}