{"title": "Towards Adaptive Feedback with AI: Comparing the Feedback Quality of LLMs and Teachers on Experimentation Protocols", "authors": ["Kathrin Se\u00dfler", "Arne Bewersdorff", "Claudia Nerdel", "Enkelejda Kasneci"], "abstract": "Effective feedback is essential for fostering students' success in scientific inquiry. With advancements in artificial intelligence, large language models (LLMs) offer new possibilities for delivering instant and adaptive feedback. However, this feedback often lacks the pedagogical validation provided by real-world practitioners. To address this limitation, our study evaluates and compares the feedback quality of LLM agents with that of human teachers and science education experts on student-written experimentation protocols. Four blinded raters, all professionals in scientific inquiry and science education, evaluated the feedback texts generated by 1) the LLM agent, 2) the teachers and 3) the science education experts using a five-point Likert scale based on six criteria of effective feedback: Feed Up, Feed Back, Feed Forward, Constructive Tone, Linguistic Clarity, and Technical Terminology. Our results indicate that LLM-generated feedback shows no significant difference to that of teachers and experts in overall quality. However, the LLM agent's performance lags in the Feed Back dimension, which involves identifying and explaining errors within the student's work context. Qualitative analysis highlighted the LLM agent's limitations in contextual understanding and in the clear communication of specific errors. Our findings suggest that combining LLM-generated feedback with human expertise can enhance educational practices by leveraging the efficiency of LLMs and the nuanced understanding of educators.", "sections": [{"title": "1 Introduction", "content": "Effective feedback is a cornerstone of student development, influencing learning outcomes, student satisfaction and motivation (Gan et al, 2021; Wisniewski et al, 2020; Monteiro et al, 2021). Personalization and contextualization significantly enhance intrinsic motivation and engagement in learning tasks (Cordova and Lepper, 1996). However, traditional automatic feedback systems often fall short by providing generic, static responses that lack adaptiveness and fail to meet individual learner needs.\nAdvancements in generative Artificial Intelligence (AI), particularly large language models (LLMs), offer new opportunities to significantly improve feedback mechanisms in education (Kasneci et al, 2023). LLMs can overcome previous limitations by delivering adaptive, real-time, and personalized feedback that mimics human-like interaction. This integration combines the efficiency and scalability of computer-based feedback with the nuanced, adaptive qualities traditionally associated only with human educators. In their review, Maier and Klotz (2022) highlight the necessity for studies to adopt AI systems capable of providing personalized feedback to individual learners.\nDespite these advancements, current research on LLM-based feedback systems often lacks validation with real-world student data or focuses on proof of concept studies without inviting domain experts as collaborators (Nguyen et al, 2023; Se\u00dfler et al, 2023; Gabbay and Cohen, 2024). This gap underscores the need for developing and validating feedback systems that are not only technically sound but also pedagogically effective and aligned with didactic principles.\nIn science education, experimentation is a fundamental practice essential for fostering analytical thinking and understanding the empirical process of scientific inquiry (Council, 2012). However, students frequently encounter challenges and make mistakes in designing and conducting experiments such as forming testable hypotheses, controlling variables, and interpreting data (Kranz et al, 2023; Baur, 2018). As experimentation protocols are the central student outcomes of the experimentation process they are valuable sources for assessment and feedback. Addressing errors in experimentation protocols typically requires personalized feedback from educators, a process that is both time-consuming and resource-intensive.\nResponding to these challenges and following the call by Zhai and Nehm (2023) for innovation in science education through generative AI, we aim to develop and validate an LLM feedback agent that (1) detects errors in students' experimentation protocols and (2) provides adaptive feedback to learners in real-time. By leveraging LLMs, our system seeks to reduce the workload on educators while enhancing the learning experience for students. Our research addresses a significant gap that still exists between theoretical work on generative AI in science education and practical applications, including a focus on real-world implications.\nBuilding upon our previous work (see: Bewersdorff et al (2023)), where we developed an AI system to automatically detect errors in experimentation protocol, we now extend our approach to deliver actionable feedback that addresses specific student misconceptions and mistakes. We aim to investigate whether LLMs possess the capacity to comprehend context and provide meaningful, adaptive feedback that aligns with pedagogical best practices."}, {"title": "2 Theoretical Background", "content": ""}, {"title": "2.1 Characteristics of effective feedback", "content": "Focusing on the learning process, formative assessment tries to continuously adapt the teaching to the needs of the students (Filsecker and Kerres, 2012). It has been, despite some critics (Bennett, 2010), identified as one of the most significant influencing factors for effective learning (Hattie, 2008). Feedback is a key part of formative assessment (Brookhart, 2007), helping to bridge the gap between current understanding and learning goals (Sadler, 1989). Shute (2008) defined formative feedback as \"information communicated to the learner that is intended to modify his or her thinking or behavior to improve learning\". According to the model of Hattie and Timperley (2007), effective feedback answers the following the questions \"Where am I going?\" (Feed Up), \u201cHow am I going?\" (Feed Back), and \u201cWhere to go next?\u201d (Feed Forward). Each feedback question again can be applied on four levels: The task level, the process level, the self-regulation level and the self level (see Hattie and Timperley (2007)).\nTo guide students towards their academic goals, content-related feedback is fundamental. Firstly, Feed Up involves clearly summarizing the objectives of the student, establishing a clear goal, and providing a sense of direction (Hattie and Timperley, 2007). Secondly, Feed Back focuses on the student's current performance, highlighting errors in context. This helps students understand their situation and identify specific areas for improvement (Hattie and Timperley, 2007). Lastly, Feed Forward offers guidance on the next steps. Overall, effective feedback facilitates the learning process, by identifying errors but also recalling the objectives and providing hints for solutions or giving explanations to address identified challenges (Hattie and Timperley, 2007; Ossenberg et al, 2019)."}, {"title": "2.2 Automated computer-based feedback", "content": "The effectiveness of automated, computer-based feedback is known for decades in educational research, as it shows some benefits to human-generated feedback. One central advantage is the instance generation of feedback. Especially for low-achieving learners this immediate feedback has proven to be effective (Mason and Bruning, 2001). In their meta-analysis of the effects of feedback in computer-based instruction, Azevedo and Bernard (1995) found effects of .80 for immediate feedback and .35 for delayed feedback across all learners. Another advantage can be the lower bias that is perceived when using automated feedback (Kluger and DeNisi, 1996). However, one-to-one tutoring is generally very effective (Bloom, 1984), and computer-based feedback can have difficulties mimicking the adaptiveness of human tutoring. Unlike conventional computer-based feedback systems, LLM-based systems can offer a higher degree of interactivity, thus enabling a more adaptive and engaging learning experience."}, {"title": "2.3 LLM-based feedback", "content": "LLM-generated feedback has been explored in various educational fields. In mathematics education, Nguyen et al (2023) evaluated the effectiveness of GPT-3.5 in addressing student errors related to decimal numbers and found it generally effective in providing appropriate feedback. For writing instruction, Se\u00dfler et al (2023) utilized GPT-3.5 to generate instant feedback, demonstrating its utility in supporting writing development through constructive critiques. A significant focus has been placed on programming education, where several studies have investigated the capabilities of LLMs. Gabbay and Cohen (2024) found that while GPT-3.5 and GPT-4 are effective at detecting errors in code assignments within MOOCs, they often fall short in providing accurate or actionable feedback. Est\u00e9vez-Ayres et al (2024) observed limitations in LLMs' ability to evaluate exercises involving concurrency errors, highlighting challenges in understanding complex programming concepts. Additionally, Koutcheme et al (2024) noted a tendency for GPT-4 to provide overly positive feedback in introductory programming courses, potentially overlooking critical issues that require attention.\nWhile these advancements are promising, research on LLM-based feedback generation in science education is still developing. From a theoretical standpoint, Zhai and Nehm (2023) underscore the importance of embracing AI's potential in formative assessment within science education, advocating for constructive dialogue rather than dismissing its impact. In an earlier review, Zhai et al (2020) examined machine learning-based science assessments across three axes -technical implementation, validity, and pedagogical features - and found that most studies concentrated on validity, often neglecting technical and pedagogical aspects.\nRecent efforts have begun to explore the application of language models in science education assessment. Wu et al (2023) applied a pre-trained BERT model with zero-shot prompting to score students' written responses, demonstrating the feasibility of using LLMs for assessment purposes. Similarly, Latif and Zhai (2024) compared a fine-tuned BERT model with GPT-3.5 for automatically scoring student answers, finding that GPT-3.5 significantly outperforms BERT in terms of scoring accuracy. However, these studies primarily focus on scoring student responses rather than generating detailed feedback that can guide learning. Guo et al (2024) developed a multi-agent system to generate feedback on science education items. However, the study has notable limitations. It does not compare the system's feedback to that of real-world teachers, leaving its alignment with practical classroom feedback untested. Additionally, the evaluation focuses solely on over-praise and over-inference, neglecting other critical pedagogical aspects such as clarity or specificity. This narrow scope, combined with limited validation for diverse classroom contexts, raises questions about its broader applicability in education.\nThere remains a significant gap in research regarding the generation of feedback texts for hands-on student experimentation in science education, particularly in benchmarking against real-world teacher feedback and validating critical feedback quality dimensions. Addressing this gap, our study analyzes the performance of LLM-generated feedback on students' experimentation protocols, comparing it to feedback from practicing teachers and science education experts. Our evaluation spans multiple dimensions and incorporates real-world student data."}, {"title": "3 Methodology", "content": "In our study, we conduct a multidimensional comparison of feedback generated by our LLM agent for student errors in real-world experimentation protocol to feedback texts given by teachers and experts in science education. In the following, we introduce our used rating scheme, explain the data collection and automated feedback generation and the analyzes we conducted to evaluate the LLM-generated feedback texts."}, {"title": "3.1 The Research Context: Students' Errors in Experimentation Protocols", "content": "Planning and conducting investigations, along with analyzing and interpreting data, are considered key scientific practices essential for developing scientific literacy (Council, 2012). These practices are effectively fostered through inquiry-based learning methods, particularly student-centered experimentation. However, while experimentation is central to scientific inquiry, it demands high levels of scientific reasoning and literacy from students.\nStudents frequently encounter challenges in planning and conducting experiments, which often result in errors. These errors have been well-documented and empirically confirmed in numerous studies (Kranz et al, 2023). Common errors include formulating hypotheses that focus on expected observations rather than the dependent variable (Baur, 2018) or neglecting to include control trials (Dasgupta et al, 2014; Germann et al, 1996).\nBuilding on these documented errors, in a previous study, we developed and validated an LLM-based system to identify 16 common student errors related to experimentation (Bewersdorff et al, 2023). Each protocol included sections for 'Hypothesis', 'Materials', 'Sketch of the Experimental Setup', 'Description of the Implementation', 'Observation', and 'Result'. The protocols were collected from 37 sixth to eighth-grade students attending secondary schools in Germany and are publicly available in the supplementary material of our previous work (see Bewersdorff et al (2023)). The student protocols were based on two experimental contexts:\n\u2022 Yeast Experiment Students were tasked with determining the conditions necessary for yeast to produce carbon dioxide (\"Find out what yeast needs to produce carbon dioxide\").\n\u2022 Pine Cone Experiment Students explored the factors that cause pine cone scales to close (\"Find out what triggers cone scales to close\").\nIn total, the 65 experimentation protocols from the two experimental contexts contained 159 errors. We used this dataset as a basis for our study."}, {"title": "3.2 Feedback collection from human participants", "content": "From the protocol dataset, we selected 40 protocols to create a test dataset containing a total of 109 student errors. The size of the dataset is limited by the inherent challenges of collecting data on student experimentation. Despite its modest size, the dataset was carefully curated to capture a wide range of error types and reflect realistic classroom scenarios (Baur, 2018). The data was collected from a sample of 37 students in grades six through eight (ages 10-12) across multiple schools. Efforts were made to ensure an approximately equal distribution of genders (boys and girls). Additionally, the sample was designed to ensure diversity regarding performance by including students with high, average, and low performance levels in mathematics, science, and German.\nFor each error, we collected two human feedback texts to serve as benchmarks for our LLM agent. To achieve this, we engaged 11 science teachers (6 in-service teachers and 5 pre-service teachers in their final semester) and 5 experts in science education who are currently working as university researchers. These two sources provided both real-world feedback from teachers and additional benchmark feedback from domain experts in science education. This selected sample serves as a preliminary foundation for investigating the potential of LLMs in providing feedback on scientific inquiry tasks.\nAll experts were provided with the student protocol, a list of the identified errors, and general feedback guidelines based on our rating scheme (see Section 3.4). They also received detailed information about the structure of an experimentation protocol and the tasks assigned to the students. The participation was voluntary, all teachers received a \u20ac10 voucher for their support."}, {"title": "3.3 Technical background of the LLM agent", "content": "For automatic feedback generation, we exploited a zero-shot approach, optimizing the prompt for our LLM agent with clear instructions but without requiring prior task-specific examples. The model was role-prompted to act as a science teacher. For the context, it was then provided with the student's task and the relevant section of the protocol. For example, if the error related to the hypothesis, only the first part of the protocol was included. For errors associated with the result, the entire student-written protocol was provided. The model also received a definition of the specific error identified. Finally, it was instructed to follow a step-by-step approach and return its output in the defined format. \nThe prompt was created and refined using 15 protocols selected as train dataset including 50 error of the real-world dataset. The 40 test protocols later used for the comparison with the humans were not considered for the prompt adjustments.\nWe generated all feedback texts using the OpenAI pipeline, leveraging GPT-3.5 (gpt-3.5-turbo-0125) as baseline LLM. At the time of running the experiments, this version was the most cost-effective option, making it the most practical choice for a classroom setting. Notably, since then, the newer GPT-40 Mini model (gpt-4o-mini) has become available, offering even lower costs and improved reasoning capabilities Se\u00dfler et al (2024b), which could further enhance the feasibility and effectiveness of automated feedback generation in educational contexts."}, {"title": "3.4 Feedback rating scheme", "content": "After collecting the feedback tests from humans and our LLM agent, we used a multi-dimensional rating scheme to evaluate them. Drawing from the theoretical foundations of effective feedback in science education (see 2.1), we derived a rating scheme based on six dimensions for our analysis.\nAccording to Hattie and Timperley (2007), feedback comprises three core components: Feed Up, Feed Back, and Feed Forward. Feed Up clarifies the current goals and learning objectives, ensuring that students understand what is expected of them. Feed Back identifies specific errors within the context of the current work, providing students with clear insights into their mistakes. Feed Forward suggests possible courses of action to address the identified errors, guiding students on how to improve and proceed with their learning.\nIn addition to these components, the format of the feedback plays a crucial role in its effectiveness (2.1). Therefore, we included three language-related dimensions: Constructive Tone, Linguistic Clarity, and Technical Terminology. A Constructive Tone ensures that the feedback is delivered in an encouraging and supportive manner without being overly positive, fostering a positive learning environment. Linguistic Clarity involves the use of clear and straightforward language that is appropriate for students in grades 6 to 8, making the feedback easily understandable. Lastly, Technical Terminology refers to the appropriate use of subject-specific terms to enhance understanding and ensure that the feedback is both precise and relevant to the scientific context.\nEach of these six dimensions was evaluated using a 5-point Likert scale, ranging from Strongly Disagree to Strongly Agree. This quantitative approach enables us to systematically rate and compare the quality of feedback provided by both human experts in the field of science education and the LLM agent."}, {"title": "3.5 Expert feedback ratings", "content": "After the data collecting phase, we presented the three feedback texts (generated by teacher, expert and LLM agent) blinded and shuffled to four human raters, that are professionals in science inquiry as well as in teaching and learning. The criteria for selection as an rater included: 1) a PhD in a natural science or science education, 2) several years of teaching experience with pre-service science teachers, and 3) publications on experimentation as a method of scientific inquiry. The raters were provided with a batch of the original protocols and the task to the assess the texts based on the six aspects in Table 1."}, {"title": "3.6 Analysis", "content": "To thoroughly compare the feedback texts written by teachers, experts, and the LLM agent, we conducted several analyses, using human-generated feedback as the benchmark for evaluating the agent's performance. First, we assessed the overall performance by averaging all six categories of the rating scheme into a single overall score and analyzing the distribution of these scores across the three groups, providing a general impression of performance (Section 4.1).\nNext, we conducted a detailed aspect-level analysis to examine each dimension of the rating scheme individually. By comparing the mean and variance of scores for each category, we identified significant differences among the feedback sources-teachers, experts, and agent using independent t-tests (Section 4.2). Since feedback length is also a critical factor for adequacy, particularly because LLMs tend to produce verbose responses, we analyzed the number of words in each feedback text to investigate this possible challenge (Section 4.3)."}, {"title": "4 Results", "content": "In following we report the results of the ratings in the multi-dimensional criteria to analyze the feedback quality of our LLM agent compared to real-world teachers and experts. We analyze how well LLMs perform in terms of content and language aspects in providing feedback on experimental protocols."}, {"title": "4.1 Overall scoring", "content": "As a first step, we compare the mean score, averaging all six multi-dimensional aspects for each feedback text. On average, the LLM agent reaches a mean value of 3.784 (SD = 1.238), the teacher feedback texts 3.805 (SD = 1.266) and the experts 3.831 (SD = 1.287), with no significant difference between the three groups. Therefore, from an overall perspective, the feedback generated by an LLM agent can be considered similar quality to the one written by a human teacher or expert."}, {"title": "4.2 Multidimensional scoring", "content": "To get more detailed insights, we analyze the ratings of the feedback texts in the multi-dimensional aspects. Here, In general, all three groups achieved a higher rating in the three language related aspects and slightly worse results on the content-related ones. \nInvestigating the language categories, we see that while experts seem to have a more constructive tone, they suffer more in linguistic clarity. Teachers seem to have a small advantage using technical terminology for the young target group. But, overall the variations are minor and there are no significant differences between the three"}, {"title": "4.3 Length analysis", "content": "A good feedback need to be comprehensive as well as precise. Therefore, we take a look into the length of the provided feedbacks of the three groups In the prompt for the LLM agent we asked to write 100 words, interestingly most generated feedbacks contain 50 words. This aligns with the length of the formative feedback provided by the teachers and is therefore probably a realistic length for a real-world classroom. On the other hand, the experts provided clearly more lengthy feedback texts, which is not suitable for students of grade 6 to 8 anymore. Even though their"}, {"title": "4.4 Correlation analysis", "content": "To assess whether humans and LLMs excel or fall short for similar feedback texts or struggle with different kinds of errors, we analyzed the correlations of the expert ratings.\nCorrelations for language-related aspects were generally strong. Constructive Tone showed moderate to strong correlations across all constellations (p \u2265 .48), reflecting meaningful alignment. Linguistic Clarity demonstrated robust agreement across all sources (p \u2265 .60), while Technical Terminology exhibited moderate positive correlations (p \u2265 .41). \nIn contrast, content-related aspects showed consistently weaker correlations. Between experts and teachers, correlations ranged from weak to moderate (p = .26 for Feed Forward to p = .41 for Feed Back), suggesting some overlap in performance on distinct feedback texts. However, correlations between human-written feedback and LLM-generated feedback were low to non-existent. For example, correlations between teachers and the LLM ranged from p = .06 for Feed Back to p = .21 for Feed Up, while correlations between expert feedback ratings and LLM feedback ratings were negligible (|p| <.1 for all aspects). These results suggest that humans and LLMs struggled with different types of feedback texts, with limited alignment in their strengths."}, {"title": "4.5 Qualitative analysis", "content": "To gain deeper insight into why our LLM agent performs sometimes worse than the humans, we qualitatively compared the feedback provided by these three entities regarding a specific student error. The error analyzed in Table 4 was selected based on two criteria: (1) poor performance by the LLM agent on content-related aspects, and (2) the largest overall gap between human and LLM feedback across all dimensions. The second criterion highlights cases where humans excel at providing effective feedback while the LLM agent falls short."}, {"title": "5 Discussion", "content": "The present study evaluated the efficacy of an LLM feedback agent on experimentation protocols compared to human experts and teachers across multiple dimensions of feedback quality: Feed Up, Feed Back, Feed Forward, Constructive Tone, Linguistic Clarity, and Technical Terminology. Our findings indicate that the LLM agent performed on par with human experts and teachers in most dimensions, showing no significant differences. However, it was significantly less effective in the Feed Back dimension, which involves describing the error in the context of the current situation.\nLLMs can provide actionable and linguistically appropriate feedback.\nThe comparable performance of the LLM agent in the content-related dimensions such as Feed Up, Feed Forward as well as the language-related dimensions shows that an LLM can effectively mimic feedback from educational experts in summarizing goals, providing forward-looking guidance, maintaining an encouraging tone, and using appropriate language and terminology for middle school students. This aligns with previous research indicating that AI systems can deliver structured and pedagogically sound feedback \nLLMs fall short in nuanced context understanding.\nThe small but significant shortfall in the Feed Back dimension highlights a critical area where LLM agents need improvement. Describing errors within the specific con-text of a student's work requires a nuanced understanding of the learning material"}, {"title": "Implementation of AI in the classroom", "content": "The analysis of content-related aspects reveals notable differences between human and LLM-generated feedback, indicating that humans and LLMs excel in distinct error scenarios. While their overall performance is comparable (see Section 4.2), this suggests that integrating LLMs together with human assessments could enhance feedback quality by combining the strengths of teachers and the LLM agent.\nThe design of the LLM agent as a tool for hybrid intelligence aligns with the principle of augmenting human capabilities in the classroom rather than replacing them . Teachers can integrate the LLM agent's strengths into their practice without making big changes to their teaching methods. By supporting adaptive and instant assessment and feedback for all students, the AI system enables teachers to focus on personalized instruction while retaining control over the learning process.\nDespite the promising results regarding LLM-generated feedback quality,  report that students tend to prefer human feedback due to perceived deficiencies in the genuineness, usefulness, and objectivity of AI feedback. A teacher-in-the-loop approach could address these issues by ensuring quality control and enhancing trust. There is a spectrum ranging from student-centered use of AI, through teacher-only use, to the complete absence of AI in the classroom . Finding the right level of AI integration into classroom settings remains challenging and is subject to future research. Ideally, these LLM agents are designed to provide different levels of AI integration, providing teachers with options to choose the level that best aligns with their pedagogical goals. For effective AI integration there is a need to build AI literacy among STEM teachers, enabling them to make informed pedagogical decisions based on an understanding of the strengths and limitations of current LLM agents ."}, {"title": "5.1 Limitations", "content": "One major limitation of this study is the small sample size for many of the identified errors, with some errors represented by only a single instance. This restricts the generality of our findings and may not capture the full range of possible feedback scenarios. Additionally, the LLM capabilities were constrained by financial considerations essential for creating a real-world, school-usable system. As a result, we utilized the cost-efficient version of GPT at the time of conducting the user study, GPT-3.5, which possess significantly lower reasoning abilities compared to more advanced, but expensive, models like GPT-4 or 01. Also, the GPT-40 Mini model (published after running our experiments) is more cost-efficient and shows improved capabilities . Choosing this version likely impacted the agent's performance,"}, {"title": "5.2 Future work", "content": "Future work should involve conducting a real-world study where the LLM agent is implemented in classroom settings to evaluate its effectiveness and practicality in everyday educational environments and assess the effectiveness beyond the proof-of-concept given in this study. Additionally, upgrading the agent to more advanced models, such as GPT-40 or o1, would likely enhance the agent's reasoning capabilities as shown by the findings of current research allowing for more sophisticated and accurate feedback.\nTo address limitations related to sample size, strategies such as data augmentation using generative AI  could be employed to increase the number of samples. These methods would allow for more robust findings without the challenging task of collecting large numbers of student protocols in real-world classroom settings. Importantly, synthetic data generation can be designed to be privacy-preserving by retaining the statistical properties of the original dataset while ensuring that no individual student's data is directly replicated . Such approaches not only expand the dataset and increase the reliability but also align with ethical considerations and data privacy regulations, which are critical in educational research.\nCurrently, our LLM agent is limited to text-based interactions. At its core, science learning, and experimentation protocols, like science itself are inherently multimodal . Regarding experimentation protocols, these include reading and writing scientific arguments and explanations, drawing and interpreting diagrams, analyzing and visualizing data, and creating flowcharts and diagrams. Future systems could adopt a multi-modal approach, such as providing feedback on experimental drawings or generating sketches of experimental designs, thereby addressing the diverse modalities inherent in scientific inquiry (Lee et al, 2023; Bewersdorff et al, 2024)."}, {"title": "6 Conclusion", "content": "We conducted a study to compare feedback on experimentation protocols written by teachers, science education experts, and an LLM agent. Human raters assessed the feedback across six dimensions to evaluate its quality in both content and language-related aspects. Our findings show that, on average, there is no significant difference in overall quality between the three groups, indicating that LLMs are capable of producing valuable feedback for students. However, the LLM agent tends to focus on the feed forward aspect of feedback, partly neglecting the equally important feed back component. This highlights the need for further refinement of LLMs to ensure they provide balanced and comprehensive feedback for students. Additionally, since teachers and LLMs appear to encounter challenges with different types of student errors, integrating feedback from both could offer a promising approach for enhancing educational practices in the future."}, {"title": "Declarations", "content": "The science education experts and the science teachers voluntarily participated in the study. All participants provided informed consent before participating. The study was designed to minimize any potential risks or discomfort for the participants. All procedures were in accordance with the ethical standards of the German Society of Psychology (DGPs). Student data was obtained from the publicly available dataset from Bewersdorff et al (2023)."}, {"title": "6.1 Author Contribution Information", "content": "Kathrin Se\u00dfler: Writing original draft, Writing review & editing, Visualization, Data curation, Methodology, Investigation, Conceptualization, Software. Arne Bewersdorff: Writing - original draft, Writing - review & editing, Data curation, Methodology, Investigation, Conceptualization. Claudia Nerdel: Writing - review & editing, Supervision, Conceptualization. Enkelejda Kasneci: Writing - review & editing, Supervision, Project administration, Conceptualization."}, {"title": "6.2 Declaration of competing interest", "content": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper."}, {"title": "6.3 Declaration of AI and AI-assisted technologies in the writing process", "content": "During the preparation of this work, the authors used ChatGPT (GPT-4 and o1) as well as Grammarly in order to improve the readability, structure and language of sentences as some authors are not native English speakers. After using these tools, the authors reviewed and edited the content as needed and took full responsibility for the content of the publication."}]}