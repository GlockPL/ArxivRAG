{"title": "Integrating Domain Knowledge into Large Language Models for Enhanced Fashion Recommendations", "authors": ["Zhan Shi", "Shanglin Yang"], "abstract": "Fashion, deeply rooted in sociocultural dynamics, evolves as individuals emulate styles popularized by influencers and iconic figures. In the quest to replicate such refined tastes using artificial intelligence, traditional fashion ensemble methods have primarily used supervised learning to imitate the decisions of style icons, which falter when faced with distribution shifts, leading to style replication discrepancies triggered by slight variations in input. Meanwhile, large language models (LLMs) have become prominent across various sectors, recognized for their user-friendly interfaces, strong conversational skills, and advanced reasoning capabilities. To address these challenges, we introduce the Fashion Large Language Model (FLLM), which employs auto-prompt generation training strategies to enhance its capacity for delivering personalized fashion advice while retaining essential domain knowledge. Additionally, by integrating a retrieval augmentation technique during inference, the model can better adjust to individual preferences. Our results show that this approach surpasses existing models in accuracy, interpretability, and few-shot learning capabilities.", "sections": [{"title": "1 INTRODUCTION", "content": "Fashion plays a significant role in various aspects of life, including social and cultural identity. People often imitate styles suggested by fashion experts, icons, or Key Opinion Leaders (KOLs) from social media. In the rapidly evolving fashion technology sector, creating advanced, personalized recommendation systems is a major academic and commercial endeavor. This research introduces an innovative approach using Large Language Models (LLMs) to transform fashion recommendations. Unlike traditional methods relying on static datasets and supervised learning, our generative model leverages LLMs to provide personalized, flexible fashion advice. This approach overcomes previous limitations and offers a deeper connection with individual preferences, reflecting the dynamic nature of fashion. Fashion trends change rapidly, posing challenges for recommendation systems to forecast and adapt to personal preferences. Conventional systems, though somewhat successful in predicting item compatibility and ensuring stylistic consistency, often fail to cater to diverse individual styles and rely heavily on static datasets, limiting their relevance over time.\nTo address these challenges, our study presents an innovative solution using LLMs' generative and adaptive strengths [20], offering new methods for generating recommendations that include items, occasions, and styles. We propose a new training paradigm for"}, {"title": "2 RELATED WORK", "content": "In recent years, the use of deep learning neural networks, high- lighted by [3], has drawn significant attention in various tasks within the fashion industry. These tasks include category and at- tribute classification ([13]), trend forecasting ([1]), popularity pre- diction ([14]), and the development of fashion recommendation systems ([6], [16]), particularly in outfit recommendation. It's cru- cial to determine which garments go well together to recommend complete outfits that are compatible and cohesive.\nInitial attempts to address outfit compatibility treated it as a series of pairwise comparisons among all garments in an outfit, as explored by [17]. These pairwise-based methods used Siamese networks ([19]) and triplet loss networks, with either type-aware embeddings ([18]) or similarity-aware embeddings ([18]). In con- trast, some researchers have aimed to capture a holistic view of outfit-level representations through bidirectional LSTMs ([5]) or graph neural networks ([4], [10]). Some studies also have shifted to attention-based methods, including the Transformer architecture for personalized outfit recommendations and complementary item retrieval ([12], [21], [15]).\nResearchers have increasingly turned to Large Language Models (LLMs) for their outstanding text generation capabilities, particu- larly for data augmentation ([7], [9]). [2], for instance, used LLMs to create multimodal datasets that blend language and images for instruction-following tasks. Their method, which involved tuning the instructions based on this data, significantly improved both vision and language comprehension.\nIn the domain of personalized recommendation systems, LLMs have also played a crucial role. [23] employed LLMs to generate user profiles by incorporating behaviors like clicks, purchases, and ratings. These profiles, when combined with the history of user interactions and potential items, were key in formulating the final recommendation prompt. LLMs were then applied to determine the likelihood of user-item interactions from this prompt. In a similar vein, [11] developed a technique that leverages LLMs to merge reasoning about user preferences with factual knowledge of items. However, our research explores a new area: there is no established work on using LLMs in fashion recommendation. Therefore, our study establishes a novel baseline for LLM application in the fashion recommendation field."}, {"title": "3 METHODOLOGY", "content": null}, {"title": "3.1 Problem Formulation", "content": "Figure 2 illustrates our sophisticated fashion recommendation system, which utilizes a finely-tuned Large Language Model (LLM) integrated with domain-specific retrieval processes. This system, optimized with a dedicated fashion dataset, is adept at parsing the complexities of fashion nuances. It employs the LLM to create specific prompts that trigger searches within a comprehensive fashion product dataset, and a dual-component mechanism retrieves vital domain context by merging industry trends and design principles with user preferences to tailor recommendations.\nThe system processes both user and system-generated text via tokenizers, converting them into tokens suitable for the LLM. The model then analyzes these inputs to generate customized recommendations, such as suggesting \"floral-print pants\" among various item choices. Fashion-related documents are processed into embeddings through a sentence transformer and stored in a vector database. These embeddings are then matched with user query embeddings to further refine the recommendations.\nUltimately, the LLM delivers sophisticated responses to user queries, resulting in either well-coordinated outfit recommendations or detailed fashion advice. This ensures both contextual relevance and personalization. By integrating system prompts, item description and processed contextual data, the system achieves precise and personalized fashion recommendations."}, {"title": "3.2 Fintuning LLM with auto-prompts generation", "content": "The diagram illustrated in Figure 3 delineates the comprehensive training and refinement process for a specialized Fashion Large Language Model (FLLM), which is tasked with dispensing tailored fashion advice. Initially, the FLLM is acquainted with Polyvore outfit training data. This data encompasses two primary types: binary question data, which prompts the model to discern the appropriateness of an outfit, and Fill-in-the-Blank (FITB) data.\nTo enhance the robustness and proficiency of our Fashion Large Language Model (FLLM), we employ a dual-strategy approach in generating training data. Initially, we source training data directly from existing datasets. Additionally, we utilize the advanced capabilities of a domain-specific model to further strengthen the FLLM.\n(1) Template QA Generation: This data comprises a list of items for which the model generates a binary response-assessing whether the items constitute a compatible outfit and suggesting potential item candidates as suitable choices.\n(2) LLM Auto QA Generation: The training prompts are crafted to elicit detailed descriptions that highlight the style and fit of outfits. This enriches the model with comprehensive textual content, preserving and utilizing domain knowledge to refine its general understanding. Initially, the model generates multiple QAs based on fashion style, which are then converted into template training data. This also supports the domain knowledge needed for the retrieval module.\nThese strategies, as reflected in the designed prompts, ensure that the FLLM not only retains its core language processing capabilities but is also finely tuned to analyze and address fashion-related queries with enhanced precision. Leveraging insights from domain knowledge, including broad fashion trends and specific user purchasing behaviors, the model crafts tailored training prompts. These prompts are instrumental in boosting the FLLM's predictive accuracy and its capacity to deliver customized fashion recommendations. By diversifying the retrieval process through these multi-path queries, the model significantly broadens its ability to gather information from various facets, culminating in a richly layered final context. This multi-aspect query enhancement allows the model not only to retrieve more relevant information but also to provide more nuanced and comprehensive fashion item recommendations."}, {"title": "3.3 Retrieval augmented inference", "content": "We introduce a novel architecture that incorporates a Retrieval- Augmented Generation (RAG) model within a Large Language Model (LLM) to enhance fashion item recommendations, as shown in Figure 2. This architecture improves upon traditional retrieval methods by dividing the retrieval process into multiple query pathways, each tailored to increase the contextual depth of the information retrieved.\nThe architecture employs direct embedding-based queries, utilizing the capabilities of neural embeddings for immediate and accurate retrieval of fashion items. It also incorporates queries tailored to style and occasion, enabling contextually aware recommendations that match specific user scenarios and stylistic preferences. The queries are aligned with the knowledge documented in our generated knowledge path, as outlined in Figure 3. Additionally, it leverages queries derived from LLM-generated questions. This innovative approach generates dynamic questions that reflect current fashion trends and individual user profiles, further refining the retrieval process."}, {"title": "4 EXPERIMENTS", "content": null}, {"title": "4.1 Dataset", "content": "The Polyvore dataset [5] is a robust, community-generated collection of fashion ensembles, each meticulously curated to ensure that the individual items within them are complementary. It encompasses an extensive compilation of 68,000 manually curated outfits. A specialized subset known as Polyvore-disjoint is derived from the primary Polyvore dataset. This subset is crafted by meticulously filtering out any outfits that share items across the training, validation, and testing sets, thereby ensuring a strict separation of data."}, {"title": "4.2 Evaluation", "content": "FITB accuracy. We carried out a fill-in-the-blank accuracy assessment using the Polyvore dataset, as detailed in Table 1. The results indicate that, compared to traditional methods, the LLM demonstrates superior proficiency in grasping more complex contexts and achieves the highest accuracy in both Joint and Disjoint Dataset.\nFew-shot ability. We also evaluated the performance of our model across different training data ratios to assess the impact of data volume on its effectiveness. A lower ratio indicates reduced data use for training. This evaluation compared our method to the state-of-the-art Type-Aware methods in fashion prediction. Our results show that although the final performance of both algorithms is similar when using the full dataset, a significant difference emerges at lower data ratios as shown in Figure 4. Here, our algorithm surpasses the baseline by about 10 percent in accuracy improvement. This demonstrates the Large Language Model's (LLM) ability to effectively learn from limited data and maintain high prediction accuracy, which is especially advantageous in situations where gathering extensive datasets is challenging, providing a significant advantage in data-sparse environments.\nVisualizing Recommendation Results. The visualization of recommendation results, as depicted in Figure 5, demonstrates how our approach can achieve diverse matching outcomes compatible with both the item and the style preferences. It effectively maps query items to style preferences and subsequently to corresponding recommendation items, showcasing the model's adaptability and effectiveness in aligning fashion choices with user preferences."}, {"title": "5 REMARKS AND FUTURE WORK", "content": "Our model has established a Large Language Model (LLM)-based fashion recommendation system characterized by high accuracy and dynamic adaptation. The LLM model can quickly adjust to user preferences and new data distributions. Moving forward, our focus will be on a hybrid approach that combines the outputs of the LLM and a Conventional Recommendation Model (CRM) to form a model ensemble. This ensemble will also incorporate multimodal information, including images and metadata to further develop the system's ability to process and integrate multimodal data, improving the accuracy and personalization of fashion recommendations."}]}