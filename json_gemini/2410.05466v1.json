{"title": "Herd Mentality in Augmentation - Not a Good Idea!\nA Robust Multi-stage Approach towards Deepfake Detection", "authors": ["Monu", "Rohan Raju Dhanakshirur"], "abstract": "The rapid increase in deepfake technology has raised significant concerns about digital media integrity. Detecting deepfakes is crucial for safeguarding digital media. However, most standard image classifiers fail to distinguish between fake and real faces. Our analysis reveals that this failure is due to the model's inability to explicitly focus on the artefacts typically in deepfakes. We propose an enhanced architecture based on the GenConViT model, which incorporates weighted loss and update augmentation techniques and includes masked eye pretraining. This proposed model improves the F1 score by 1.71% and the accuracy by 4.34% on the Celeb-DF v2 dataset. The source code for our model is available at https://github.com/Monu-Khicher-1/multi-stage-learning.", "sections": [{"title": "1 Introduction", "content": "Background: Deepfakes represent manipulative media generated through advanced Deep Learning (DL) techniques, achieved by superimposing the faces of two different individuals. These manipulations encompass a variety of techniques, including entire face synthesis, identity swapping, face morphing, attribute manipulation, and expression swapping (also known as face reenactment or talking faces) [1, 2]. With the progression of DL methods, the realism and believability of such content have significantly increased [3]. The emergence of deepfakes has raised public concerns due to their potential to deceive, misuse, and disseminate false information [2, 4]. To mitigate the spread of misleading information, researchers have introduced numerous deepfake detection techniques [5, 6, 7]. While most of these approaches employ binary classifiers and demonstrate high performance in terms of accuracy and AUC, they often exhibit poor F1 scores, highlighting their limitations in effectively distinguishing between authentic and manipulated images.\nDatasets & Existing Models: Deepfake Detection has been a well-explored research topic. Researchers also proposed multiple datasets for the problem because of the data-driven nature of existing computer vision techniques. Andreas Rossler [8] proposed a deepfake videos dataset in 2019. After that, Yuezun Li [9] also proposed a deepfake video dataset, CelebDF. CelebDF includes 590 real videos and 5,639 high-quality fake videos crafted by the improved DeepFake algorithm [9]. We evaluated our proposed model and existing models on CelebDF. Further, we converted this video dataset into an image dataset by extracting frames at a 1fps rate. Then, we divided this image dataset into train, validation, and testing datasets (6:2:2) to evaluate and train existing models and our model.\nThe number of papers published in the deepfake detection domain is increasing with advancements in deepfake generative models. The publication of deepfake detection papers has increased in the past few years. Most papers are based on binary classifiers where the deepfake detection problem is treated as a general image classification problem. Broadly, the deepfake detection problem can be solved by two methods: binary classifier & others.\nBinary Classifiers: Deepfake Detection is treated as a binary image classification problem [17, 18] in the initial few years. Usually, they use a backbone (e.g. Xception [10], VGGNet [11]) encoder to extract higher-level feature maps for input images and then use these to classify them using a binary classifier. Recently, an integrated feature extraction backbone is proposed after publishing the Attention paper [12]. For example, Zhou et al. [13] designed a multi-attentional face forgery detector that aggregates the texture features and high-level semantic features of multiple local parts to classify real and fake [7, 13].\nOthers: Many attempts are made to improve the capability of the general deep fake detectors by various handcrafted techniques. One of these methods is Reconstruction Learning in unsupervised settings [14, 15, 16, 17, 18]. It helps the model learn more about the input and reconstruct it using encoded information. Reconstruction learning is also done in the forgery detection domain [7, 19]. Junyi Cao [7] has proposed a reconstruction encoder which learns to regenerate real images and is later used as a classifier for fake and real. In some supervised methods, a reconstructor learns latent data distribution [20]. Shichao Dong [6] proposed an architecture comprising an artefact detection"}, {"title": "2 Proposed Architecture", "content": "We utilized GenConViT [20] as backbone architecture for our model. As illustrated in Figure 1b, it uses an Encoder-Decoder for reconstructing images and learns latent data distribution through this Autoencoder. Backbone of GenConViT consists of the SwinTansformer & ConvNeXt layer [20].\nThe ConvNeXt and Swin transformer models form a hybrid ConvNeXt-Swin, represented as the backbone in Figure 1b. The ConvNeXt model acts as the backbone of the hybrid model, using a CNN to extract features from the input frames. The Swin Transformer, with its hierarchical feature representation and attention mechanism, further extracts the global and local features of the input images [20]. Backbone (ConvNeXt-Swin) extracts out visual features of the input image. Later, these features are used by the classifier. The model takes N images as input; the Encoder will generate N x 256 x 7 x7 feature space for N x 3 x 224 x 224, which will be input to the decoder to reconstruct the original image. Backbone (ConvNeXt-Swin) will generate an N x 100 feature map for input and reconstructed images. These feature maps are concatenated and used as input in the binary classifier to make final predictions."}, {"title": "2.2 Problems with GenConViT:", "content": "The model predicts various outputs using the feature map generated by the backbone. We observed some problems during our analyses:\n1. Our analysis reveals that real faces are misclassified more than fake faces. Out of 13,543 fake images, 863"}, {"title": "2.3 Modified Data Augmentation(NA & RF):", "content": "Standard augmentation techniques such as Gaussian noise, random brightness contrast, and sharpening generate fake images and disrupt the ideal conditions for deepfake detection. Augmentation can be used for this task, which doesn't introduce any noise and just rotates the image. So, we revert to basic augmentations, such as rotation and flipping only. This minor adjustment results in an 8.21% increase in the F1 score and a 3.85% increase in accuracy for the CelebDF V-2 [5] dataset Table 1. We proposed 2 methodology for it: (1) No Augmentation (NA) & (2) Random Flips (RF) (flipping-based augmentation)"}, {"title": "2.4 Hardness-inspired Curriculum Learning (MEP):", "content": "Multi-stage Training: Proposed new Multi-stage training methodology (Figure 2) for better training of model weights such that it makes predictions based on facial features other than eyes also. For this, we trained the model Augmented Masked Eye Training dataset and then fine-tuned on Augmented Cropped Faces Dataset & then finetuned on Augmented Masked Eye Training dataset respectively. More details: Figure 2."}, {"title": "2.5 Weighted Loss Function (WL):", "content": "The dataset exhibits class imbalance, with fake images being nearly ten times more prevalent than real images in the test dataset. GenConViT [20] misclassifies real faces more frequently than fake faces. To address this issue, we introduced a weighted loss function:\n\\(L_{fake} = w * L_{real}\\)\nAfter tuning the hyperparameter w, we found the optimal value to be w = 1.85. This adjustment improved the F1 score by 4.46% for the base model. Ablation analyses with other approaches are shown in Table 2."}, {"title": "3 Results and Discussion", "content": "We demonstrated the performance of the proposed methodology on CelebDF-v2 [9]. CelebDF includes 590 real videos and 5,639 high-quality fake videos crafted by the improved DeepFake algorithm [9]. This video dataset is preprocessed to extract frames from videos (1fps). This image dataset is split into training, validation and testing (6:2:2). Then, the training dataset is balanced. After preprocessing and all other steps, the training dataset contains 4000 real images and 4000 fake images. Test Dataset is imbalanced and contains 1,415 real and 13,543 fake images. Further, validation have 1,137 real and 10,833 fake images. Some models used this dataset."}, {"title": "3.2 Evaluation Metrics:", "content": "We employed several evaluation metrics, including the F1 score, accuracy, and area under the curve (AUC), to analyze the results of different methodologies and models. Although accuracy is commonly used for comparing different models, it is not an appropriate parameter when dealing with imbalanced test datasets. Given that most deepfake datasets exhibit significant class imbalance, relying on accuracy alone can be misleading and fails to provide a comprehensive understanding of a model's performance.\nIn scenarios with imbalanced datasets, the F1 score is more suitable as it considers both precision and recall, offering a balanced measure of a model's performance. Therefore, we utilized the F1 score for evaluating deepfake detection models. Additionally, we also employed the AUC, specifically the area under the receiver operating characteristic curve (AuROC), for further analysis."}, {"title": "3.3 Quantitative Results on CelebDF:", "content": "The results of the proposed method & comparison with SOTA deepfake detection models are shown in Table 1. It can be observed that the proposed method outperforms the best-performing SOTA method by 1.71%. Table 2 compares the performance of our base model with other models using different combinations of techniques. Table 1 shows the performance of our approach compared with existing state-of-the-art models."}, {"title": "3.4 Ablation Analysis:", "content": "We proposed various methodologies to enhance GenConViT [20], addressing several network issues. During our analyses, we identified multiple problems within the model and proposed corresponding solutions: (1) Class Imbalance:"}, {"title": "4 Conclusion", "content": "In this paper, we proposed methodologies to improve GenConvNet. Towards this, first, we proposed a weighted loss concept for deepfake models & augmentations suitable for deepfakes. Then, we proposed a multi-stage training architecture for fixing architecture ablations. Overall, the proposed method outperforms the best-performing SOTA method by 1.71% in the publicly available dataset Celeb-DF."}]}