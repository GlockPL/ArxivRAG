{"title": "Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction", "authors": ["Qintong Zhang", "Victor Shea-Jay Huang", "Bin Wang", "Junyuan Zhang", "Zhengren Wang", "Hao Liang", "Shawn Wang", "Matthieu Lin", "Wentao Zhang", "Conghui He"], "abstract": "Document parsing is essential for converting unstructured and semi-structured documents-such as contracts, academic papers, and invoices into structured, machine-readable data.Document parsing extract reliable structured data from unstructured inputs, providing huge convenience for numerous applications. Especially with recent achievements in Large Language Models, document parsing plays an indispensable role in both knowledge base construction and training data generation.This survey presents a comprehensive review of the current state of document parsing, covering key methodologies, from modular pipeline systems to end-to-end models driven by large vision-language models. Core components such as layout detection, content extraction (including text, tables, and mathematical expressions), and multi-modal data integration are examined in detail. Additionally, this paper discusses the challenges faced by modular document parsing systems and vision-language models in handling complex layouts, integrating multiple modules, and recognizing high-density text. It emphasizes the importance of developing larger and more diverse datasets and outlines future research directions.", "sections": [{"title": "Introduction", "content": "As digital transformation accelerates, electronic documents have increasingly supplanted paper documents as the primary medium for information exchange across various industries. This shift has significantly expanded the diversity and complexity of document types, including contracts, invoices, and academic papers. Consequently, there is an escalating need for efficient systems to manage and retrieve information [1, 2]. However, a substantial proportion of historical records, academic publications, and legal documents remain in scanned or image-based formats, posing considerable challenges to tasks such as information extraction, document comprehension, and enhanced retrieval [3-5].\nTo address these challenges, document parsing (DP), also known as document content extraction, has emerged as an essential tool for converting unstructured and semi-structured documents into structured information. Document parsing recognizes and extracts various elements such as text, equations, tables, and images from various document inputs while preserving their structural relationships. The extracted content is then transformed into structured formats like Markdown or JSON, enabling seamless integration into modern workflows [6].\nDocument parsing is critical for document-related tasks, reshaping how information is stored, shared, and applied across numerous applications. It provides a foundation for various downstream processes, including the development of Retrieval-Augmented Generation(RAG) systems in various practical fields, the automated construction of electronic storage and retrieval libraries for paper materials [7-10]. Besides that,there are plenty of potential information in document which remains largely underdeveloped. Document parsing technology can effectively extract and organize these rich knowledge, laying a solid foundation for the development of the next generation of intelligent systems. For example, training more professional and powerful multimodal models [5, 11].\nHowever, recent years have seen significant advancements in document parsing technologies, particularly those based on deep learning, leading to the proliferation of document parsing tools and the emergence of promising document parsers. Nevertheless, research in this field still faces certain limitations. Many surveys on document parsing are outdated, resulting in pipelines that lack rigor and comprehensiveness, with technological descriptions failing to capture recent advancements and changes in application scenarios [3, 4]. Additionally, high-quality reviews often focus on specific sub-technologies within document parsing, such as layout analysis [12-14], mathematical expression recognition [15-17], table structure recognition [18-20], and chart-related work in documents [21], without providing a comprehensive overview of the entire document parsing process.\nGiven these limitations, a comprehensive review of document parsing is urgently needed. In this survey, we analyze advancements in document parsing from a holistic perspective, providing researchers and developers with a broad understanding of recent developments and future directions in the field. The key contributions of this survey are as follows:\n\u2022 Comprehensive Review of Document Parsing. This paper systematically integrates and evaluates recent advancements in document parsing technologies across the stages of the document parsing pipeline.\n\u2022 Consolidation of Datasets and Evaluation Metrics. We consolidate widely used datasets and evaluation metrics, addressing gaps in existing reviews within the document parsing field.\n\u2022 Holistic Insight for Researchers and Practitioners. This work provides a holistic perspective on the current state and future directions of document parsing, bridging the gap between academic research and practical applications.\n\u2022 Introductory Guide for Newcomers. It serves as a guide for newcomers to quickly understand the field's landscape and identify promising research directions.\nThe organization of this article is as follows: Section 2 outlines two main approaches to document parsing. From Section 3 to Section 6.4 examines key algorithms used in modular document parsing systems. Section 7 presents visual language macromodels applicable to document-related tasks, with a focus on document parsing and OCR. Section 8 and Section 9 covers datasets and evaluation metrics in document parsing. In Section 11, we discuss current challenges in the field and highlight significant future directions. Finally, Section 12 provides a concise and insightful conclusion."}, {"title": "Methodology", "content": "Document parsing can be broadly categorized into two methodologies: the modular pipeline document parsing system and the end-to-end approach based on large vision-language models."}, {"title": "Document Parsing System", "content": null}, {"title": "Layout Analysis", "content": "Layout detection identifies the structural elements of a document\u2014such as text blocks, paragraphs, headings, images, tables, and mathematical expressions\u2014along with their spatial coordinates and reading order. This foundational step is crucial for ensuring accurate content extraction. Notably, the detection of mathematical expressions, especially inline ones, is often handled separately due to their complexity."}, {"title": "Content Extraction", "content": "\u2022 Text Extraction: This process leverages Optical Character Recognition (OCR) technology to convert document images into machine-readable text. By analyzing the shapes and patterns of characters, OCR accurately recognizes and processes the text contained within the images.\n\u2022 Mathematical Expression Extraction: In this step, mathematical symbols and structures within document regions are detected and converted into standardized formats such as LaTeX or MathML. Due to the complexity of the symbols and their spatial arrangements, this task presents a unique challenge.\n\u2022 Table Data and Structure Extraction: Table recognition involves detecting and interpreting table structures by identifying the layout of cells and the relationships between rows and columns in document images. The extracted table data is typically combined with OCR results and converted into formats like LaTeX for further use.\n\u2022 Chart Recognition: This step focuses on identifying different types of charts and extracting the underlying data as well as their structural relationships. The visual information from charts is converted into raw data tables or structured formats like JSON."}, {"title": "Relation Integration", "content": "Each step builds upon the previous, ensuring a seamless flow from text to mathematical expressions, tables, and charts, all while leveraging advanced recognition technologies to convert document content into structured, machine-readable formats.Once individual content elements are extracted, relation integration combines them into a unified structure. This step uses the spatial coordinates identified during layout detection, ensuring that the spatial and semantic relationships between elements are preserved. Rule-based systems or specialized reading order models are commonly applied to maintain the logical flow of content."}, {"title": "End-to-End Approaches and Multimodal Large Models", "content": "While traditional modular document parsing systems perform effectively within specific domains, their architecture often leads to limitations in joint optimization and generalization across diverse document types. Recent advances in multimodal large models, particularly vision-language models (VLMs), offer promising alternatives. Models such as GPT-4, Qwen, LLaMA, and InternVL can simultaneously process visual and textual data, facilitating end-to-end conversion of document images into structured outputs. Due to the unique challenges posed by document images such as dense text, complex layouts, and high variability in visual elements-specialized large models like Nougat, Fox, and GOT have emerged. These models represent a significant leap forward in automating document parsing and comprehension."}, {"title": "Layout Analysis", "content": null}, {"title": "Introduction to Layout Analysis Technology", "content": "Research on document layout analysis (DLA) for scanned images began in the 1990s. Early studies focused on simple document structures, often as a preprocessing step, and primarily used rule-based methods [173\u2013186, 71, 187\u2013196] or statistical techniques [197, 198, 178].\nBy the 2000s, DLA incorporated feature engineering and machine learning, framing the task as pixel-based semantic segmentation [199\u2013201]. Since 2015, deep learning techniques, particularly convolutional neural networks (CNNs) and Transformers, have dominated the field, treating DLA as a pixel-wise segmentation problem and leveraging visual features to analyze physical layouts [202-207, 22].\nMoreover, graph convolutional networks (GCNs) have been employed to model relational representations between document components [208, 23, 31, 209, 24, 25]. Grid-based approaches [210-212, 26, 27] have emphasized the importance of preserving spatial structures. Recent studies have also integrated multiple data sources into these models [28, 33, 31, 34, 32, 29, 30]. Around 2020, self-supervised pretraining in multimodal natural language processing (NLP) influenced DLA research, leading to models that jointly integrate text and visual layout information for end-to-end learning."}, {"title": "Based on Visual Feature", "content": "Early deep learning-based DLA primarily focused on analyzing physical layouts using visual features from document images. Documents were treated as images, with elements such as text blocks, images, and tables detected and extracted through neural network architectures [202]."}, {"title": "CNN-based Methods", "content": "The introduction of Convolutional Neural Networks (CNNs) marked a significant advancement in DLA. Originally designed for object detection, these models were adapted for tasks like page segmentation and layout detection. R-CNN, Fast R-CNN, and Mask R-CNN were especially influential for detecting components such as text blocks and tables [203]. Later studies improved the region proposal process and architecture for enhanced page object detection [204]. Models like Fully Convolutional Networks (FCN) and ARU-net were developed to handle more complex layouts [205, 206]."}, {"title": "Transformer-based Methods", "content": "Recent advances in Transformer models have extended their application in DLA. BEiT (Bidirectional Encoder Representation from Image Transformers), inspired by BERT, employs self-supervised pretraining to learn robust image representations, excelling at extracting global document features such as titles, paragraphs, and tables [207]. The Document Image Transformer (DiT), with its Vision Transformer (ViT)-like architecture, splits document images into patches to enhance layout analysis. However, these models are computationally intensive and require extensive pretraining [22].Recent work such as [213, 214] also focuses on using transformers to complete classification tasks based on document visual features."}, {"title": "Graph-based Methods", "content": "While image-based approaches have significantly advanced DLA, they often rely heavily on visual features, limiting their understanding of semantic structures. Graph Convolutional Networks (GCNs) address this issue by modeling relationships between document components, enhancing the semantic analysis of layouts [208, 23, 31]. For instance, Doc-GCN improves understanding of semantic and contextual relationships among layout components [24]. GLAM, another prominent model, represents a document page as a structured graph, combining visual features with embedded metadata for superior performance [25]."}, {"title": "Grid-Based Methods", "content": "Grid-based methods preserve spatial information by representing document layouts as grids, which aids in retaining spatial details [210\u2013212, 26, 27]. For instance, BERTGrid adapts BERT to represent layouts while maintaining spatial structures [26]. The VGT model integrates Vision Transformer (ViT) and Grid Transformer (GiT) modules to capture features at both token and paragraph levels. However, grid-based methods often face challenges such as large parameter sizes and slow inference speeds, limiting their practical application [27]."}, {"title": "Integrate with Semantic Information", "content": "As document analysis becomes more complex, physical layout analysis alone is insufficient. Although there is work that proves that excellent object detection models such as YOLO v8 are still relatively leading in the layout analysis of documents in some small languages based on graphemes [215], and relevant improvements have been made [216]., DLA methods that combine semantic information are still an important development direction. Logical layout analysis is needed to classify document elements by their semantic roles, such as titles, charts, or footers. With the rise of multimodal models, methods that combine visual, textual, and layout information have gained prominence in DLA research.\nLogical layout analysis, driven by the need to classify document elements based on their semantic roles, has led to the development of multimodal models that integrate text and layout information for more comprehensive analysis. Studies have explored multimodal data integration by combining supervised learning with pre-trained natural language processing (NLP) or computer vision (CV) models. For example, LayoutLM was the first model to fuse text and layout information within a single framework, using the BERT architecture to capture document features through text, positional, and image embeddings [28].\n[33] extended this by combining RoBERTa with GCNs to capture relational layout information from both text and images. [31] introduced a multi-scale adaptive aggregation module to fuse visual and semantic features, producing an attention map for more accurate feature alignment.\nSelf-supervised pretraining in multimodal NLP has also significantly advanced the field. During pretraining, models jointly process text, images, and layout information using a unified Transformer architecture, enabling them to learn cross-modal knowledge from various document types. This approach improves model versatility, requiring minimal supervision for fine-tuning across different document types and styles.\nIn 2020, [34] proposed a multimodal document pre-training framework that encodes information from multi-page documents end-to-end, incorporating tasks such as document topic modeling and random document prediction. This framework enables models to learn rich representations of images, text, and layout. Notable work such as UniDoc [32] uses a Transformer and ResNet-50 architecture to extract linguistic and visual features, aligned through a gated cross-modal attention mechanism.\nAdvancements include LayoutLMv2 and LayoutLMv3, which refine LayoutLM by optimizing the fusion of text, image, and layout information. These models improve feature extraction through deeper multimodal interactions and masking mechanisms, achieving more efficient and comprehensive document analysis [29, 30]. In addition, LayoutLLM [35] attempts to use a large language model to integrate certain semantic information to complete tasks related to document layout."}, {"title": "Optical Character Recognition", "content": null}, {"title": "Introduction to Document OCR", "content": "Optical Character Recognition (OCR) has a long history, originating from the early development of computers. The concept was first introduced by Tausheck in 1929. Today, OCR is a critical area of research in computer vision and pattern recognition, aiming to identify text in visual data and convert it into editable digital formats for subsequent analysis and organization.\nIn the 1950s and 1960s, OCR research concentrated on handwritten document recognition, such as check processing and mail sorting. During this period, OCR systems primarily utilized preprocessing techniques and rule-based or template-matching methods. For instance, early versions of ABBYY OCR employed image binarization, noise reduction, and layout analysis to identify characters through template matching.\nBefore the advent of deep learning, OCR systems mainly relied on feature engineering and traditional machine learning techniques for character recognition. These approaches were commonly applied to tasks like postal code recognition, form processing, and banking. A notable example is Tesseract OCR, developed by HP Labs in 1984, which used such techniques in its earlier versions (prior to version 4.x)."}, {"title": "Text Detection", "content": "Traditional, non-deep learning text detection algorithms are generally effective in simple scenes with high contrast backgrounds. However, they often require manual parameter adjustments to achieve optimal performance in different contexts, limiting their generalization capabilities. In contrast, deep learning-based text detection algorithms, which improve upon object detection and instance segmentation techniques, can be categorized into four main approaches: one-stage regression-based methods, two-stage region proposal methods, instance segmentation-based methods, and hybrid methods."}, {"title": "Regression-Based Single-Stage Methods", "content": "Regression-based methods, also called direct regression approaches, directly predict the corner coordinates or aspect ratios of text boxes from specific points in the image, bypassing the need for multi-stage candidate region generation and subsequent classification. Algorithms like YOLO and SSD have been adapted for text detection, with modifications to handle text-specific challenges, such as varied aspect ratios and orientations [36, 37]. For instance, CTPN [38] achieves precise text line localization through regression of vertical positions and lateral offsets. Methods such as SegLink [217] and DRRG [39] apply regression techniques to handle irregular text shapes, while Fourier transforms [40] enable compact representation of complex text contours. Although regression-based approaches are computationally efficient and integrate well with deep learning models, they can struggle with blurred edges and cluttered backgrounds."}, {"title": "Region Proposal-Based Two-Stage Methods", "content": "Region proposal-based methods treat text blocks as specific detection targets, processing them using two-stage object detection techniques like Fast R-CNN and Faster R-CNN. These methods aim to generate candidate boxes optimized for text and improve detection accuracy for arbitrarily oriented text. DenseBox [218], for example, introduced an end-to-end Fully Convolutional Network (FCN) framework that incorporates position and scale information through multi-task learning, enhancing detection accuracy. Similarly, DeepText [41] introduced an Inception-RPN to generate more detailed text candidate boxes. While Faster R-CNN was primarily designed for horizontal text, several studies have enhanced its ability to detect irregular text regions [42-46]. Research by [47] extends these methods to handle text in any orientation, including curved text blocks, further increasing their robustness."}, {"title": "Segmentation-Based Methods", "content": "Text detection can also be approached as an image segmentation problem, where pixels are classified to identify text regions. This approach provides flexibility for handling various text shapes and orientations. Early methods [48] used Fully Convolutional Networks (FCNs) for detecting text lines, while later algorithms like PAN [49] improved efficiency and accuracy. CRAFT [50] represents a key advancement by employing character-level detection, eliminating the need for large receptive fields. Instance segmentation methods like [48] address challenges such as closely adjacent text blocks by treating each block as a distinct instance. Techniques like SPCNET [51] and LSAE [52] further improve this approach using pyramid attention modules and dual-branch architectures, respectively. Post-processing, such as binarization, is critical in segmentation-based methods, with Differentiable Binarization (DB) [53] improving both detection speed and accuracy by integrating binarization into the network."}, {"title": "Hybrid Methods", "content": "Hybrid methods combine the strengths of regression and segmentation approaches to capture both global and local text details, enhancing localization accuracy while reducing the need for extensive post-processing. Techniques like EAST [54] and MOST incorporate Position-Aware Non-Maximum Suppression (PA-NMS) to optimize detection across varying scales. Recent methods like Centripetal-Text [55] refine text localization using centripetal offsets. Additionally, innovations such as graph networks and transformer architectures [56, 57] further enhance detection by utilizing adaptive boundary proposals and attention mechanisms. Advances in transfer learning and multimodal integration [58, 59], particularly in transformer-based architectures, have addressed challenges in detecting small text areas and improved accuracy by integrating visual-textual representations.\nIn conclusion, text detection has advanced significantly, leveraging improvements in object detection, segmentation, and novel architectural innovations, making it a robust tool for various applications."}, {"title": "Text Recognition", "content": "Text recognition is a crucial component of Optical Character Recognition (OCR), referring to the automated process of converting images of written or printed text into machine-readable formats. The primary goal of a text recognition system is to interpret characters and words from visual data for subsequent computational tasks. Over time, various text recognition methods have emerged, primarily categorized into three groups: vision feature-based methods, connectionist temporal classification (CTC) loss-based methods, and sequence-to-sequence (seq2seq) techniques."}, {"title": "Vision Feature-Based OCR Technology", "content": "\u2022 Image Feature-Based Methods: Recent advancements in OCR technology leverage image processing, particularly Convolutional Neural Networks (CNNs), to capture spatial features from text images. These methods localize and recognize characters by eliminating the need for traditional feature engineering, directly deriving features from images. This simplifies model design and implementation while effectively capturing spatial structural information, making these techniques especially useful for regular or semi-structured text images.\nFor instance, [219] proposed a model using CNNs for detecting text regions and classifying characters, effectively managing character spatial arrangements. Similarly, [60] introduced a synthetic data generator combined with a deep CNN architecture to improve adaptability across various text types. The CA-FAN model [61] enhances character recognition accuracy by employing a character attention mechanism. Additionally, TextScanner [62] combines CNNs with Recurrent Neural Networks (RNNs) to improve character segmentation and positioning accuracy. Despite their effectiveness, these methods face challenges with complex or irregular text, particularly in cases involving significant background noise or intricate text structures, often requiring additional post-processing for enhanced recognition accuracy."}, {"title": "CTC Loss-Based Methods", "content": "\u2022 CTC Loss-Based Methods: The Connectionist Temporal Classification (CTC) loss function addresses sequence alignment issues by enabling models to optimize without explicit alignment between input and output sequences during training. CTC computes probabilities over all possible alignment paths, making it particularly suited for processing texts of variable lengths.\nA notable application of CTC is the CRNN model by [63], which integrates CNN and RNN architectures with CTC loss for sequence generation. Deep TextSpotter [64] com-bines CNN feature extraction with CTC to improve text detection and recognition accuracy. ADOCRNet [65] further applies CTC with CNN and Bidirectional Long Short-Term Mem-ory (BLSTM) networks for Arabic document recognition. However, CTC struggles with extended text and contextual nuances, which can increase computational complexity and affect model training efficiency and real-time performance."}, {"title": "Sequence-to-Sequence Methods", "content": "\u2022 Sequence-to-Sequence Methods: Sequence-to-sequence (seq2seq) techniques use an encoder-decoder architecture to encode input sequences and generate corresponding outputs. These methods manage long-distance dependencies between input and output sequences through attention mechanisms, facilitating end-to-end training. Traditional approaches often employ RNNs and CNNs to convert image features into one-dimensional sequences, which are then processed by attention-based decoders. Despite their effectiveness, convert-ing images into one-dimensional sequences for Transformer-based architectures presents challenges, particularly with arbitrarily oriented and irregular texts.\nTo address these issues, models use strategies like input correction and two-dimensional feature maps. Spatial Transformer Networks (STNs), for instance, rectify text images into rectangular, horizontally aligned characters, as seen in ASTER [66], ESIR [68], and MORAN [66]. Other models avoid input modification by learning 2D feature maps to di-rectly extract characters from 2D space, accommodating irregular and multi-directional text, as demonstrated by SAR [70], AON [67], and SATRN [72]. The rising use of Transformer architectures represents a shift from traditional CNN and RNN models toward attention-based encoder-decoder systems. NRTR [69], for example, employs a fully self-attention architecture, using convolutional layers to convert 2D input images into 1D sequences for the encoder-decoder framework. Vision Transformer models like ViTSTR [71] dispense with traditional backbone networks, using the Vision Transformer (ViT) architecture exclusively for encoding, while TrOCR [73] fully relies on Transformer architectures for both image processing and text generation, avoiding CNNs entirely.\nPerformance improvements for irregular or elongated text sequences focus on better handling two-dimensional geometric positional information. For example, the method proposed by [220] integrates a correction module akin to traditional R-CNN approaches, along with text grouping and arrangement modules. LOCR [74] enhances OCR performance on long texts in documents by incorporating positional information of document elements alongside positional encoding from image blocks. OCR research continues to evolve, especially in using Transformer architectures to improve performance for complex image texts [221, 81]."}, {"title": "Incorporation of Semantic Information", "content": "Text recognition, traditionally approached as a visual classification task, benefits significantly from the integration of semantic information and contextual understanding, especially when dealing with irregular, blurred, or occluded text. Recent research emphasizes incorporating semantic understanding into text recognition systems, broadly classified into three approaches: character-level semantic integration, enhancements through dedicated semantic modules, and training refinements to improve contextual awareness.\n\u2022 Character-Level Semantic Integration: Enhancing OCR performance with character-level semantic information involves leveraging character-related features, such as counts and orders. The RF-L (Reciprocal Feature Learning) framework proposed by [75] highlights the benefit of using implicit labels, such as text length, for improved recognition. RF-L incorporates a counting task (CNT) to predict character frequencies, aiding the recognition task. Similarly, [76] presents a context-aware dual-parallel encoder (CDDP), using cross-attention and specialized loss functions to integrate sorting and counting modules. Despite improving performance by integrating character information, challenges remain in capturing diverse prior knowledge from large-scale, unlabeled texts for language model pre-training.\n\u2022 Enhancements Through Semantic Modules: While character-level semantic integration is valuable, some approaches focus on independent semantic modules to capture higher-level semantic features. These strategies align visual and semantic data via contextual relationships within specialized modules. SRN [77], for instance, introduces a Parallel Visual Attention Module (PVAM) and a Global Semantic Reasoning Module (GSRM) to align 2D visual features with characters, transforming character features into semantic embeddings for global reasoning. Similarly, SEED [78] adds a semantic module between the encoder and decoder, enhancing feature sequences through semantic transformations. ABINet [79] refines character positions through iterative feedback, using a separately trained language model for contextual refinement. These strategies align semantic and visual data efficiently, but challenges remain in fully leveraging semantic relationships.\n\u2022 Training Advancements for Contextual Awareness: Pre-training strategies adapted from natural language processing (NLP), such as BERT, have played a pivotal role in enhancing context-awareness in OCR tasks. Methods like VisionLAN [80] use masking to improve contextual understanding, introducing a Masked Language Perception Module (MLM) and a Visual Reasoning Module (VRM) for parallel reasoning. Similarly, Text-DIAE [81] applies degradation methods like masking, blurring, and noise addition during pre-training to im-prove OCR capabilities. PARSeq [82] modifies Permutation Language Modeling (PLM) to enhance text recognition by reordering encoded tags for better contextual sequences. While these pre-training approaches improve semantic learning, they often increase computational complexity and resource demands."}, {"title": "Text Spotting", "content": "Text spotting involves the detection and transcription of textual information from images, which encompasses both text detection and recognition tasks. Traditionally, these tasks have been handled independently, with a detector identifying text regions and a recognition module subsequently transcribing the identified text. Although this approach is straightforward, separate processing of detection and recognition may impact performance, as the final result's effectiveness largely hinges on the accuracy of the text detection model. With advancements in deep learning, recent efforts have increasingly focused on developing end-to-end models that integrate text detection and recognition, thereby enhancing efficiency and accuracy by sharing feature representations. End-to-end text spotting models based on deep learning fall into two primary categories: two-stage and single-stage methods, each offering unique advantages. While both methods have been explored, recent research primarily emphasizes one-stage approaches.\n\u2022 Two-Stage Methods:\nTwo-stage methods integrate text detection and recognition architectures, enabling joint training and feature alignment. This approach permits the sharing of information between detection and recognition tasks by extracting common features, often through shared convolutional layers, and linking tasks using a Region of Interest (RoI) mechanism. In the detection phase, the model identifies potential text regions and maps them onto the shared feature map in the recognition phase for transcription.\nFor instance, a foundational two-stage method combined a single-scan text detector with a sequence-to-sequence recognizer using rectangular RoIs [222]. Subsequent work improved multi-directional text detection using a similar architecture [64]. However, rectangular RoIs are most suited for organized text layouts and can be compromised by background elements, prompting researchers to explore alternative RoI methods. Some methods employed object detection technologies, such as FOTS [223] with the RoIRotate mechanism, and the Mask TextSpotter series [224, 225], AE TextSpotter [226], and ABINet++ [227] using RoIAlign. Notably, Mask TextSpotter v1 was the first to fully implement end-to-end OCR, allowing feedback between detection and recognition during joint training, while Mask TextSpotter v3 [225] introduced a Segmentation Proposal Network (SPN) to provide flexible text region representations.\nOther two-stage methods, such as [228], integrated attention mechanisms with text alignment layers instead of RoI. Innovations in RoI mechanisms include TextDragon's [229] RoLSide operator, which extracts and aligns arbitrary text regions, and BezierAlign in ABCNet [230], which adapts to text contours rather than rectangular boundaries. PAN++ [231] uses a masked region of interest attention recognition head to balance accuracy and speed, while SwinTextSpotter [232] introduced a mechanism for detection-informed recognition. In 2022, GLASS [233] proposed Rotated-RoIAlign to enhance text feature extraction from shared backbones, addressing challenges posed by varying text sizes and orientations through a global attention module.\nDespite these innovations, two-stage methods possess inherent limitations. Their reliance on precise detection results increases demands on the detection modules and necessitates high-quality annotated datasets. Furthermore, the RoI operations and post-processing steps are computationally intensive, especially for arbitrary text shapes.\n\u2022 One-Stage Methods:\nOne-stage methods unify text detection and recognition within a single architecture, obviat-ing the need for distinct modules. By sharing loss functions, the two tasks can be trained and optimized conjointly, avoiding potential performance losses from module separation. The first one-stage approach, proposed by [234], introduced Convolutional Character Networks that detect characters as fundamental units and produce character boundaries and labels, eliminating the need for RoI cropping. While effective for English text, this approach is computationally demanding. CRAFTS [235] continued this character-based approach, integrating detection results into an attention-based recognizer to propagate recognition loss across the network.\nSubsequent developments, such as [236], incorporated Shape Transformer Modules to optimize end-to-end detection and recognition, while MANGO [237] employed a position-aware mask attention module to apply attention weights directly to character sequences. Recent encoder-decoder models have further evolved, with PGNet [238] and PageNet [239] decoding feature maps into sequences, while the SPTS series [240, 241] and TESTR [242] adopted Transformer-based architectures. Enhanced with cross-attention mechanisms, CLIP-based models [243] improved collaboration between image and text embeddings. In [244], the application of text spotting to video text is introduced with TransDETR, a Transformer-based framework that simplifies tracking and recognition of text over time, which may benefit document text spotting tasks.\nAlthough one-stage models have demonstrated versatility and improved accuracy, they require more complex training processes than two-stage models and may not perform equally effectively in certain specialized text-processing tasks."}, {"title": "Mathematical Expression Detection and Recognition", "content": null}, {"title": "Introduction to Mathematical Expression Detection and Recognition", "content": "Mathematical expression detection and recognition focus on identifying and interpreting mathematical expression within documents. This process is typically categorized by the type of mathematical expression: handwritten or printed. Handwritten mathematical expressions, facilitated by advances in stylus technology, can be further divided into online (real-time) and offline recognition; this discussion centers exclusively on offline mathematical expressions.\nIn documents, mathematical expressions manifest as either displayed mathematical expressions, which are distinct from regular text, or in-line expressions, which are embedded within text lines. Displayed mathematical expressions are easier to identify using document layout analysis, whereas in-line mathematical expressions present challenges due to their proximity to regular text, necessitating specialized detection techniques.\nEarly approaches to mathematical expression detection relied on rule-based methods [245-253] or adaptations of document layout analysis [254-256]. For in-line mathematical expressions, statistical and machine learning techniques, including Support Vector Machines and Bayesian models, were commonly used for feature extraction and classification [252, 253, 257, 256, 258].\nWith the advent of deep learning, mathematical expression detection increasingly resembles object detection in document images, utilizing bounding boxes or instance segmentation to isolate mathe-matical expression regions. Continuous mathematical expression blocks can be managed through advanced segmentation techniques."}, {"title": "Mathematical Expression Detection", "content": null}, {"title": "Early Work and Convolutional Neural Networks", "content": "Initial endeavors in mathematical expression detection (MED) utilized convolutional neural net-works (CNNs) for mathematical expression localization. Studies such as [271, 204, 272] harnessed CNNs alongside traditional manual feature extraction to generate bounding boxes for mathematical expression identification. Notably, [271] employed recurrent neural networks (RNNs) to extract character sequences; however, these models did not support fully end-to-end detection, limiting their generalization and performance. The Unet model, introduced for end-to-end detection in [83], concentrated on printed documents and circumvented complex segmentation tasks. While effective in detecting in-line mathematical expressions, it lacked robustness against noise."}, {"title": "Advances in Object Detection Algorithms", "content": "MED has evolved through adaptations of generic object detection algorithms into specialized forms, including both single-stage and two-stage approaches. Single-stage detectors, like DS-YOLOv5 [90], incorporated deformable convolutions and multi-scale architectures to enhance detection accuracy and speed. Similarly, the Single Shot MultiBox Detector (SSD) [85] accelerated computations using a sliding window strategy for scale-invariant detection. The 2021 ICDAR competition showcased advancements such as Generalized Focal Loss (GFL) to address class imbalance, bolstered by Feature Pyramid Networks to improve small mathematical expression detection."}, {"title": "Instance Segmentation Techniques", "content": "Instance segmentation algorithms align well with MED, effectively managing non-linear and dense mathematical expression configurations through pixel-level segmentation. Mask R-CNN [277] advanced the field by incorporating pixel mask predictions within its framework, resulting in superior region recognition. PANet [278] and Hybrid Task Cascade (HTC) [279] further enhanced these approaches by improving semantic localization and integrating detection with segmentation tasks. In 2024, FormulaDet [91] innovated by framing MED as an entity and relation extraction problem, successfully utilizing contextual and layout-aware networks. This integrated approach demonstrated substantial improvements in understanding and detecting complex formula structures."}, {"title": "Mathematical Expression Recognition (MER)", "content": "Mathematical Expression Recognition (MER) models frequently utilize encoder-decoder architectures to convert visual representations into structured formats like LaTeX. These models predominantly rely on CNN-based encoders, with recent advancements integrating Transformer-based encoders. On the decoder side, RNN and Transformer architectures are commonly employed, with numerous enhancements improving model performance."}, {"title": "Encoder Strategies in MER", "content": "The fundamental task"}]}