{"title": "Are Deep Learning Models Robust to Partial Object Occlusion in Visual Recognition Tasks?", "authors": ["Kaleb Kassaw", "Francesco Luzi", "Leslie M. Collins", "Jordan M. Malof"], "abstract": "Image classification models, including convolutional neural networks (CNNs), perform well on a variety of classification tasks but struggle under conditions of partial occlusion, i.e., conditions in which objects are partially covered from the view of a camera. Methods to improve performance under occlusion, including data augmentation, part-based clustering, and more inherently robust architectures, including Vision Transformer (ViT) models, have, to some extent, been evaluated on their ability to classify objects under partial occlusion. However, evaluations of these methods have largely relied on images containing artificial occlusion, which are typically computer-generated and therefore inexpensive to label. Additionally, methods are rarely compared against each other, and many methods are compared against early, now outdated, deep learning models. We contribute the Image Recognition Under Occlusion (IRUO) dataset, based on the recently developed Occluded Video Instance Segmentation (OVIS) dataset (Qi et al. (2022)). IRUO utilizes real-world and artificially occluded images to test and benchmark leading methods' robustness to partial occlusion in visual recognition tasks. In addition, we contribute the design and results of a human study using images from IRUO that evaluates human classification performance at multiple levels and types of occlusion, including diffuse occlusion. We find that modern CNN-based models show improved recognition accuracy on occluded images compared to earlier CNN-based models, and ViT-based models are more accurate than CNN-based models on occluded images, performing only modestly worse than human accuracy. We also find that certain types of occlusion, including diffuse occlusion, where relevant objects are seen through \"holes\" in occluders such as fences and leaves, can greatly reduce the accuracy of deep recognition models as compared to humans, especially those with CNN backbones.", "sections": [{"title": "1 Introduction", "content": "Deep learning models, particularly deep neural networks (DNNs), have demonstrated tremendous success on visual recognition tasks, even matching or exceeding human performance on some benchmarks (e.g., Xie et al. (2017); Dosovitskiy et al. (2021); Liu et al. (2021)). However, one important real-world condition in which DNNs still struggle is occlusion, wherein the target object is partially obscured, or occluded, by other objects in the scene. In this work we aim to address two important and fundamental open questions regarding image recognition under occlusion.\n(i) Which existing models are most accurate on occluded imagery?\nThe answer to this question is useful for practitioners facing occlusion in imagery and would provide guidance for future research. Several recent publications have proposed occlusion-robust recognition models (see Sec. 2), and demonstrated that their proposed approaches improve visual recognition accuracy over conventional DNNs. However, these comparisons, e.g., in Kortylewski et al. (2020b); Xiao et al. (2019); Cen et al. (2021), suffer from several limitations, e.g., comparisons to only early deep learning models such as AlexNet (Krizhevsky et al. (2012)) and VGG (Simonyan and Zisserman (2015)) and other models expressly designed for occluded scenes, datasets containing few classes (e.g., vehicles), and datasets with few examples of real-world occlusion. This is an especially important set of comparisons to make, as deep learning has advanced significantly since these early models, greatly improving top-1 classification accuracy on large-scale datasets such as ImageNet (Deng et al. (2009)). Therefore, to our knowledge, no existing work has performed a rigorous empirical comparison or benchmark - of both existing conventional DNNs and/or recent occlusion-robust models.\n(ii) Are existing models robust to occlusion? Human and model comparison\nFor a model to be \"robust\" to occlusion, we mean that it does not degrade much faster than necessary (e.g., compared to an optimal model) in the presence of occlusion. Numerous publications have demonstrated that DNN recognition accuracy degrades in the presence of occlusion. Note however that the accuracy of any recognition model even an optimal model might necessarily degrade in the presence of occlusion, due to loss of image information. Therefore it is unclear whether or not the accuracy degradation observed in existing research can be mitigated through improved modeling and, therefore, whether existing models are robust to occlusion.\nTo evaluate whether a particular model is occlusion-robust, one must compare against an optimal model, yet estimating such a model's performance is difficult. Human visual recognition accuracy is often used as such an estimator, and one recent publication has compared human and DNN-based recognition accuracy as occlusion increases (Zhu et al. (2019)). This study concluded that DNN-based models do indeed degrade more quickly than human observers, and it provides one occlusion-robustness estimate (i.e., how much performance do we stand to gain with research investment). Although this work provides valuable insights, the test dataset used for this study suffers from several limitations that undermine the ability to draw generalizable conclusions. First, the testing dataset is limited in both size (e.g., only 500 test images) and variability (e.g., only vehicles classes). Second, the authors predominantly use synthetic occlusions consisting of highly unrealistic objects and scenarios, and it is unclear if such scenarios are a good proxy for real-world occlusions. Lastly, the work utilized relatively old models that exhibit inferior performance to modern models, and as we confirm in this work, also yield inferior performance under occlusion."}, {"title": "1.1 Contributions of This Work", "content": "In this work we attempt to provide comprehensive answers to the aforementioned questions. First, we curated a large and diverse dataset of real-world occlusions termed the Image Recognition Under Occlusion (IRUO) dataset, which we built upon the Occluded Video Instance Segmentation (OVIS) dataset (Qi et al. (2022)). The IRUO contains 23 classes of objects and 88 thousand images and addresses several limitations associated with previous datasets, such as their limited size, limited target object diversity, or their lack of real-world occlusion. Using IRUO, we address question (i) by comparing state-of-the-art representatives from each of three relevant model types: convolutional models (e.g., ResNeXt (Xie et al. (2017))), Vision Transformers (e.g., Swin (Liu et al. (2021))), and models that are especially designed to be occlusion robust (e.g., CompositionalNet (Kortylewski et al. (2020b))). We also leverage our benchmark results to examine whether the usage of synthetically occluded imagery is an accurate proxy for real-world occlusion. Synthetic occlusions are much easier to generate and control, and therefore a positive answer to this question would empower future investigation of occlusion, as well as help validate historical studies that made use of synthetic occlusion. To address question (ii) we obtain classification predictions from twenty human observers on our IRUO dataset, and compare their accuracy under occlusion against all of the data-driven models in our study.\nContributions of this work can be summarized as follows.\n1. The Image Recognition Under Occlusion (IRUO) Dataset: the first large-scale public benchmark for image recognition under occlusion). This dataset is built upon the OVIS dataset (Qi et al. (2022)) and comprises unprecedented size and diversity.\n2. A rigorous comparison of state-of-the-art image classification models under varying levels of occlusion. This includes convolutional models, Vision Transformers, and models specifically designed for occlusion.\n3. A rigorous comparison of model and human recognition accuracy on our benchmark, making it possible to determine whether, and to what extent, existing DNN-based models can be made more robust to occlusion.\n4. An investigation of whether synthetically occluded imagery is an accurate proxy for real-world occlusion.\nTo support future research, we publish all of our datasets, models, and results."}, {"title": "2 Related Work", "content": "Existing occlusion-robust methods. Recent methods have been proposed to address the decrease in model accuracy under conditions of occlusion. These methods include data augmentation , part-based modeling techniques , and development of architectures inherently more robust to partial occlusion . Several methods of data augmentation are used to encourage models to focus on image parts. Mixup and Cut Mix are augmentations that replace single training images with combinations of multiple images, through weighted averages over all pixels (Mixup) or cut-and-paste image areas (CutMix). Cutout , a closely related augmentation to CutMix, does not add pieces of a second image; instead, cutouts are replaced with gray boxes. The authors in each paper hypothesize that, by forcing deep networks to make inferences on parts, they can confer part-based knowledge to these models. In , in proposing TransMix, an extension of CutMix that re-weights proportions of images based on relative transformer attention values, the authors hypothesize that this part-based knowledge is linked to improved performance under occlusion. Data augmentation techniques, including CutMix and TransMix, are hypothesized to improve recognition accuracy on occluded images as they splice parts of images and encourage models to learn labels that are proportional to image area. Data augmentation has also been proposed at the convolutional deep feature level to improve recognition accuracy under occlusion ; this method collects differences in deep vectors between unoccluded and occluded objects to confer occlusion robustness to models. Part-based modeling techniques have been used to improve performance under some occluded scenes while preserving the convolutional backbones of many deep learning models; CompositionalNet, TDAPNet , and TDMPNet cluster deep feature representations in high-dimensional space and use various methods to assign these features to either object classes or occluders, explicitly for the purpose of robustness to occlusion. Although not explicitly designed for the purpose, the Vision Transformer has been shown to perform well under conditions of constant-valued occlusions coinciding with input image patches. We include CompositionalNet in our experiments, in addition to Deep Feature Augmentation, as these methods have been shown to improve performance under some conditions of occlusion, compared to VGG. We additionally include evaluations of CutMix and Mixup , as they are widely deployed in modern models to improve overall recognition accuracy and suggested to improve accuracy under conditions of occlusion .\nBenchmarking human and algorithm performance under occlusion. Human visual recognition performance on occlusion has been evaluated in , where accuracy of humans on a five-class vehicle dataset with artificial occlusion is compared to the accuracy of deep learning models including AlexNet and VGG. This study suggests that human performance is not attained by deep learning models on images with objects that are highly occluded, as human performance on highly occluded objects is significantly higher than that of any model tested. However, this study has several limitations; the dataset contains only five vehicle classes, compares against aging models instead of more modern models of the time, e.g., ResNet, and it is not publicly available.\nRecent research has suggested that Vision Transformer-based models may perform better under conditions such as mask occlusion and adversarial patches, suggesting that newer attention-based deep learning models may demonstrate higher performance than convolutional neural networks under conditions of real-world occlusion. We include these algorithms in a benchmark of visual recognition accuracy under occlusion, in addition to comparing these results with human accuracy, for the first time."}, {"title": "3 Background Methods", "content": "We evaluate several state-of-the-art vision models from three relevant categories: convolutional models, transformers, and models designed specifically for occlusion robustness. Among occlusion-specific models, we include the CompositionalNet , because it was recently shown to outperform other such models (e.g., TDAPNet , and \"two-stage voting\" ). Among convolutional models we include VGG for continuity with prior work, as VGG was included in several previous benchmarks . We also include state-of-the-art convolutional models such as ResNet and ResNeXt. Among state-of-the-art transformer-based models, we include the Vision Transformer , DeiT , and Swin .\nWe also propose the testing of various augmentation and model regularization methods, including Mixup , CutMix , and Deep Feature Augmentation . Mixup and CutMix are state-of-the-art augmentation methods that are commonly used in leading deep learning models regardless of the presence of occlusion in the images they classify, and their usage is hypothesized to improve model robustness to partial occlusion by conferring knowledge of object parts to models . Deep Feature Augmentation is explicitly designed to improve model performance"}, {"title": "3.3 Statistical Tests", "content": "Friedman Test . To determine whether or not models are ranked differently by real versus synthetic occlusions , we use the Friedman test in Friedman (1937). This test assumes that we have a collection of k objects to rank, and that we have n evaluation metrics, or \"judges\", for ranking the objects. The objective of the test is to determine whether the judges produce random rankings. Specifically, the null hypothesis of this test is that the rankings of each judge are produced by randomly drawing each of the k objects without replacement, and then assigning their rank based upon the order in which they were drawn. Rejection of the null hypothesis then suggests that the rankings of each judge are non-random so that some objects are ranked consistently above others across the judges. To test this hypothesis, the Friedman test prescribes the calculation of a Q statistic based upon the observed k rankings of each of the n available judges. For a sufficiently large k, the Q statistic is drawn from a \u03c7\u00b2 distribution with k - 1 degrees of freedom, and we reject the null hypothesis if Q is above the 95th percentile of this \u03c7\u00b2 distribution (equivalent to p < 0.05).\nTukey's Rule . In Sec. 6.3 we estimate human recognition accuracy in the presence of occlusion, and we use Tukey's rule to remove unrepresentative human subjects (i.e., outliers) from consideration. Tukey's rule prescribes that data is drawn from the same underlying distribution with a common mean and variance. By this rule, data points in a set falling 1.5 interquartile ranges outside the interquartile range are classified as outliers, and the removal of these points for calculating summary statistics is justified."}, {"title": "3.4 Human Labeling Procedure", "content": "We employ a method of human labeling known as Scalable Multi-Label Annotation, first used in . This method of data collection attempts to minimize human error due to factors other than strict identification of classes. For a (typically large) set of classes within a dataset, Scalable Multi-Label Annotation asks a series of informative categorical questions to narrow human responses to a much smaller set of classes within the dataset. These categorical questions can be broad at first, e.g., \"Is an animal present?\". This iterative collection procedure is designed to remove the need for 1-of-n-class labeling while also addressing humans' limited memory . DNN-based models, by nature of utilizing large computational resources, are capable of remembering characteristics distinctive to each class; humans, by comparison, perform best with few-class, hierarchical prompts . In addition, this hierarchical approach enables the analysis of whether or not humans are selecting classes that are relatively close or relatively distant , as categorical responses can be used in place of fine labels for certain analyses. This allows us to compare the types of errors made by both humans and models beyond simply comparing top-1 accuracy scores."}, {"title": "4 The Image Recognition Under Occlusion (IRUO) Dataset", "content": "We build the IRUO dataset based upon the Occluded Video Instance Segmentation (OVIS) dataset , first used in video segmentation problems. The OVIS dataset is comprised of a training set of approximately 3.5k objects in 600 videos, and validation and test sets of 750 objects in 150 videos each. Each of these objects is labeled with a corresponding occlusion level, (0) no occlusion; (1) some occlusion, in which up to 50 percent of the object is hidden from view; and (2) severe occlusion, in which more than 50 percent of the object is hidden from view. To create IRUO, we needed to curate the OVIS video dataset so that it is appropriate for the task of image classification. We chose to build an image classification dataset (as opposed to segmentation or detection) because, to our knowledge, all occlusion-robust image recognition methods (e.g., CompositionalNet ) are suitable only for the classification task. This was done by cropping individual target objects from OVIS video frames using the bounding boxes provided with OVIS for each target instance. To account for minor bounding box label errors and preserve object aspect ratios, we then expand the shorter dimension from the center to match the longer dimension, expand all dimensions 20 pixels from the center point, and use the subsequent crops to generate our image classification dataset. Sample images are shown in Fig. 2, where a parrot is shown at each level of occlusion defined in IRUO. These occlusion levels are exactly the same as those defined in the OVIS dataset.\nThere exists an official, author-defined train-test split for OVIS; however, labels for the test partition are unavailable due to the ongoing OVIS competition . Therefore, in this work we partitioned the official OVIS training set into training and test sets for IRUO. We then remove images in the boat and vehicle classes, as there are very few examples of each at occlusion level 0, leaving 23 remaining classes of objects. For each of these 23 classes of objects, we split the set of videos (not images) such that approximately 70 percent of instances of the class appear in the training data and 30 percent appear in the test data. We ensure that, per-class, no instances of objects in the training set appear in the test set, to prevent positively-biased estimates of model accuracy.\nFrom the base IRUO dataset, we derive three additional datasets: IRUO-HTS, IRUO-Synthetic, and IRUO-Diffuse. IRUO-HTS refers to the subset"}, {"title": "5 Human Study", "content": "In this section we describe the process for estimating human-level recognition accuracy under varying levels of occlusion. To do this, we recruited 20 human subjects to classify a curated subset of images from the testing set of our IRUO dataset, termed the Human Testing Subset (IRUO-HTS). Below we describe how we designed and utilized IRUO-HTS, as well as the testing protocol used to collect the subjects' predictions on IRUO-HTS."}, {"title": "5.1 The Human Testing Subset", "content": "To mitigate the risks of subject fatigue, each human subject was only asked to classify 200 test images which was chosen to require about 1 hour to complete using our testing protocol. These 200 images were sampled from a curated subset of 552 images from the full IRUO testing dataset, which we term the \"Human Testing Subset\" (IRUO-HTS). To ensure that IRUO-HTS was representative of the IRUO test dataset, we randomly sampled 8 images for each of the 23 target object classes, and did so for each of 3 occlusion levels, resulting in a total of 552 images. We sub-sample images from IRUO-HTS to ensure that multiple subjects classify each image, on average. This has the drawback of limiting the total number of unique images in the test set, while allowing us to identify and remove human subjects that consistently performed worse than others (i.e., outliers). Because each test image varies in its difficulty, some subjects may perform worse simply because their subset of testing images is more difficult. By enforcing some overlap in test images, we can compare the accuracy of each subject to several other subjects over the same images. Using our sampling procedure on IRUO-HTS, and our total of 20 human subjects, each image was classified by about 7 subjects on average.\nWe choose to include 8 samples per class per occlusion level to balance variation between humans and variation between images, i.e., too many images may result in few human observers seeing each image, and too few images may risk not accurately reflecting human accuracy. We additionally isolate the effect of occlusion from other factors, including small imagery and blur, by applying two additional selection criteria for images in this subset. We approximate blur using the variance of the Laplacian operator, as in , and we approximate image size by using the number of cropped pixels on image targets. We choose values for minimum Laplacian variance and image size that give us approximately 90% accuracy on top models at occlusion level 0; in our experiments, these values are 20 and 10,000, respectively."}, {"title": "5.2 Testing Protocol", "content": "We employ a method of human labeling known as Scalable Multi-Label Annotation, first used in . This method of data collection attempts to minimize human error due to factors other than strict identification of classes. As described in detail in Sec. 3.4, this collection procedure is designed to address the difference between model memorization of classes and humans' comparatively limited memory by relying on hierarchical prompts. We use the hierarchy in Fig. 5, derived from WordNet, a commonly-used lexical database of the English language . This hierarchy gives all classes in IRUO a hierarchy \"level,\" i.e., the number of clicks required to obtain this class in the human study program. For example, the class \"fish\" does not appear at level 1 but appears at levels 2-5, and the class \"dog\" does not appear at levels 1-3 but appears at levels 4-5.\nTo ensure that human observers select the correct object within an image (e.g., in cases where there are multiple classifiable objects in frame), we additionally ask humans to label which object they are selecting by clicking a point on the image. The correct object is usually centered on the image, so we place a cross directly at the center of each image for human observers. This guides human observers toward the correct object, but it does not give away information about which object to classify that is not available to models; this cross does not move throughout data collection."}, {"title": "6 Experimental Results", "content": "Each subsection below aims at addressing a specific scientific question, given by its title. In each case we report experimental results with our IRUO dataset that address the given question. All models in our experiments utilize encoders that have already been pre-trained on the ImageNet-1k dataset . We consider a large number of models that combine different architectures and augmentation strategies); we fine-tune each model on the IRUO dataset. Following previous work suggesting that training on occluded imagery is not beneficial for models , we fine-tune the models only on unoccluded imagery. We independently optimize several key hyperparameters for each model using a grid search to find highest overall accuracy on IRUO. Full details of the hyperparameter grid search, optimized hyperparameter choices , stopping criteria, and other training details can be found in the Appendix."}, {"title": "6.1 Which models are most accurate under occlusion?", "content": "To address this question, we report in Fig. 6 the accuracy of the models and augmentations described in Sec. 3.1 and Sec. 3.2, respectively, under varying levels of occlusion. The results indicate that Transformer-based models perform best, by a large margin, across all levels of occlusion, including Level 0 . The Swin model achieves the best overall performance among Transformers, making it the best-performing model in our benchmark. The convolutional models achieve the next best accuracy. Among the convolutional models we see that ResNeXt performs best, followed by ResNet, and then VGG. Similar to transformer-based models, the relative rank of the convolutional models is similar across all levels of occlusion. Surprisingly, the worst-performing model is the CompositionalNet. We observe mixed and minimal impact of the two augmentation schemes that we considered: Mixup and CutMix. The best-performing model is the Swin model with Mixup; however, the performance gain over Swin alone is small, and the impact of Mixup on other models is more modest, or even sometimes negative.\nOur findings expand upon and in some cases modify findings of prior work. Recent work found that transformers outperform convolutional models in the presence of occlusion; our work corroborates this finding on a more comprehensive and controlled experimental setting. The authors in hypothesized that this increase in accuracy under occlusion is due to the ability of receptive fields that result from self-attention layers to adapt to occlusion and attend to unoccluded data while down-weighting the relative importance of occluded data. We corroborate these results qualitatively in Fig. 7; an image of a highly-occluded tiger is shown alongside corresponding attention maps generated by ViT. To calculate the attention masks, we select all patches with object pixels, and we sum the values of attention at all other locations for all attention heads. As shown in Fig. 7, objects are clearly separated from background and occluders, and parts of the tiger in frame attend to each other. This suggests that Vision Transformer-based models can localize relevant objects and ignore occluders. Recent work reported that the CompositionalNet model outperforms convolutional models ; however, we find here that all DNN-based models outperform it. We hypothesize that this discrepancy is due to the significantly greater complexity of the IRUO dataset here compared to the VehicleOcclusion dataset utilized in . Lastly, Deep Feature Augmentation demonstrated higher accuracy under occlusion however we find more mixed results. This possibly reflects a difference in our implementation compared to the original; we do not use the same synthetic occlusions in training and testing, as is done in ."}, {"title": "6.2 Is synthetic occlusion a good proxy for real occlusion?", "content": "In this section we investigate whether synthetic occlusions are an accurate proxy for real-world occlusions, when estimating the relative accuracy of competing models. In other words, we ask whether the evaluation of model accuracy using synthetic occlusions would lead to the same rank-ordering of models as real-world occlusion. To address this question, we evaluate model accuracy using two datasets: one with real-world occlusions, and one with synthetic occlusions. If the rank-orderings produced by each dataset are highly dissimilar, then we conclude that synthetic occlusions are not a reasonable proxy for real-world occlusions. We use the IRUO-Base and IRUO-Synthetic partitions of the IRUO dataset to rank-order models on real-world and synthetic occlusions, respectively. The construction of the IRUO-Base dataset is detailed in Sec. 4. The IRUO-Synthetic dataset was created by adding artificial occlusion to all of the images in IRUO-Base that do not contain occlusion. To create synthetic occlusions, we replicate several strategies utilized in recent prior work , which share the same basic strategy of placing (i.e., inpainting) occluding objects at pseudo-random locations over the target object in the imagery. The primary difference among prior approaches is the design of the occluding objects. We consider several previously explored occluder designs: white boxes, black boxes, zebra-print texture boxes, uniformly-distributed noise boxes, and objects from the COCO dataset . Examples of the synthetic occlusions are presented in Fig. 8. Full details of our occluder designs and the occluder placement strategy can be found in the Appendix.\nQualitative Assessment. First we qualitatively evaluate the agreement of the rankings provided by real and synthetic occlusion. In Fig. 9(a), we present a line plot showing the ranking of each model as we change the occlusion type for occlusion level 1. The results indicate that no pair of occlusion types produces the same ranking of models; however, the the rankings are always very similar. For example, models often don't change ranking, and when they do, it is only by one or two positions. The variability of ranking is even more limited if we only consider changes in architecture (i.e., crossings between solid lines). In terms of similarity to real occlusion, it appears that solid boxes (white or black boxes), or juxtaposed objects, provide the most similar rankings. In Fig. 9(b), we present the rankings of models for occlusion level 2, where we observe somewhat lower consistency in the rankings; however, the conclusions are otherwise the same.\nOur qualitative results therefore suggest that synthetic occlusions often provide good approximate rankings of model accuracy under occlusion. The rankings provided by synthetic occlusion are not identical to real-world occlusions, however; therefore, synthetic occlusions will be a better proxy for large differences in model accuracy. It is also noteworthy that the variations in rankings by each occlusion type may not be due to systematic differences among occlusion types, and instead may be due to random variations in the data; i.e., testing with a bootstrap sample of the test imagery of one occlusion type may produce different rankings, as well.\nStatistical Assessment. To provide a more precise notion of similarity among the rankings, we utilize the Friedman Q-value test , summarized in Sec. 3.3. We treat the benchmark models as ranked \"objects\" in the Friedman test and each of occlusion type as \"judges\" that rank the objects. Using the rank-ordering of models provided by each occlusion type, we can then calculate the Friendman Q statistic to test the null hypothesis, which supposes that the rankings provided by the occlusion types are random. Therefore, if we reject the null hypothesis, particularly with very large values of Q, this suggests there is high similarity in the rankings provided by each occlusion. We perform the Friedman test separately for each of two occlusion levels: level 1 and level 2 . For each occlusion level, we treat each of the six types of occlusion (real occluders, white boxes, black boxes, noise boxes, texture boxes, and synthetically added objects) as a set of six judges. The calculated Friedman Q-value follows a chi-squared null distribution for either a large number of models or types of occlusion ; as our number of models, k, is 14, the former condition is met in our study, and therefore our calculated Q value is approximately X13-distributed."}, {"title": "6.3 How occlusion-robust are models compared to humans?", "content": "In this section we investigate how occlusion-robust DNNs are to occlusion. We determine robustness by comparing the accuracy of DNNs to the accuracy of a hypothetical optimal model, which we approximate using human recognition accuracy under occlusion .\nHuman Study Design. To assess human accuracy, we use the IRUO-HTS partition of our IRUO dataset, the creation of which is described in Sec. 5.1. We ask each of 20 people, or observers, to label 200 randomly-selected images from this partition of size 552 images. We follow the method in Sec. 5.2, asking respondents for image labels at each level of the hierarchy in Fig. 5 until a specific class label belonging to IRUO is selected. We then estimate the human accuracy for the jth image by\n$q_j = \\frac{1}{N_i}\\Sigma_{i \\epsilon S_j} 1(t_{ij}= t_j)$     (1)\nwhere Sj is the set of observers that labeled the jth image, tij is the label assigned to the jth image by the ith observer, and tj is the ground truth label for the jth image. Then, to calculate average human accuracy, we take the average of qj values across all images in the IRUO-HTS partition.\nIdentification of Outlier Observers. Some human observers may consistently obtain lower recognition accuracy than others (e.g., due to fatigue, or misunderstanding directions), implying that they may negatively bias our estimate of optimal recognition. To mitigate this potential problem, we remove human observers from our estimate that consistently perform worse than other observers when examined on the same imagery. It is crucial to compare humans (or any recognition model) on a shared set of imagery because classification difficulty varies greatly between images. Therefore we compute a \"normalized accuracy\" for each observer, which is their accuracy relative to other observers that rated the same imagery. Once we obtain the normalized accuracy for each observer, we use Tukey's Rule (see Sec. 3.3) to remove outliers: i.e., observers that consistently perform worse than others over the same imagery.\nWe define the normalized accuracy of the ith observer, as a = ai \u0101i, where ai is the proportion of top-1 accuracy of the ith observer, and \u0101i is an estimate of the average top-1 accuracy of all observers on the subset of 200 images shown to the ith observer. We compute \u0101i as\n$\\bar{a_i} = \\frac{1}{200} \\Sigma_{j\\epsilon S_i} q_j$  (2)\nwhere Si is the subset of 200 images shown to the ith observer, and qj is the human accuracy for image j, as defined in Eq. 1. We then apply Tukey's rule to identify and remove outliers from the set of normalized accuracy values for our 20 observers, denoted SA = {54}1. This resulted in the detection and removal of the two lowest-performing participants from the study.\nResults. The results are presented in Table 3, where we report the accuracy of all benchmark models, as well as an average of the human observers (with outliers removed), on the IRUO-HTS dataset. Among the benchmark models, the results are similar to those of Sec. 6.1 with the larger IRUO dataset, and the overall conclusions"}, {"title": "6.4 Further analysis: are models robust to diffuse occlusion?", "content": "The results in Sec. 5.2 suggest that contemporary DNNs (e.g.", "diffuseness\". We define diffuseness as the average proportion of neighbors of occluding pixels that are not occluders. Our primary hypothesis is that the occlusion-robustness of DNNs reduces as diffuseness increases, suggesting that there is still substantial potential to improve DNNs for some types of occlusion.\nExperimental Design. We conduct two experiments to evaluate the effect of diffuse occluders on the occlusion-robustness of DNNs. To conduct these experiments, we leverage our findings in Sec. 6.2 where we found that synthetic occlusions are a reasonable substitute for real-world occlusions if we wish to rank-order the performance of models under occlusion. Using this finding, we created an additional partition of the IRUO dataset, termed IRUO-Diffuse , wherein we generate several types of synthetic occlusions with varying types and levels of \"diffuseness\" to evaluate both experiments.\nOur first experiment evaluates whether DNNS might be less robust to diffuse occlusion than solid occluders. To conduct this experiment, in IRUO-Diffuse, we created solid gray box occluders and two types of diffuse gray occluders": "right-angle and oblique line occluders", "levels": 0, "diffuseness": "denoted 1", "occlusion": 0, "subset": "one for each of the two experiments outlined above. For each experiment we recruited five human observers", "sets": "one containing images with the solid and diffuse occluders described in the first experiment above and in Fig. 12, and the other containing images with the various-sized"}]}