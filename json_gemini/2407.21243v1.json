{"title": "Informed Correctors for Discrete Diffusion Models", "authors": ["Yixiu Zhao", "Jiaxin Shi", "Lester Mackey", "Scott Linderman"], "abstract": "Discrete diffusion modeling is a promising framework for modeling and generating data in discrete spaces. To sample from these models, different strategies present trade-offs between computation and sample quality. A predominant sampling strategy is predictor-corrector T-leaping, which simulates the continuous time generative process with discretized predictor steps and counteracts the accumulation of discretization error via corrector steps. However, for absorbing state diffusion, an important class of discrete diffusion models, the standard forward-backward corrector can be ineffective in fixing such errors, resulting in subpar sample quality. To remedy this problem, we propose a family of informed correctors that more reliably counteracts discretization error by leveraging information learned by the model. For further efficiency gains, we also propose k-Gillespie's, a sampling algorithm that better utilizes each model evaluation, while still enjoying the speed and flexibility of T-leaping. Across several real and synthetic datasets, we show that k-Gillespie's with informed correctors reliably produces higher quality samples at lower computational cost.", "sections": [{"title": "1 Introduction", "content": "Denoising diffusion models are powerful generative models for high-dimensional data [1-3]. The central idea of diffusion models is to gradually corrupt data into noise using a \"noising\" or \"forward\" process and then to train a parameterized model (usually a deep neural network) to learn its time reversal, commonly known as the \u201cdenoising\" or simply the \"backward\" process. In continuous domains such as image generation, the forward process is usually defined by gradual injection of Gaussian noise and scaling, transforming the data distribution close to a standard normal. The backward process is then approximated by learning the gradient of the log density (also known as the \"score function\") of the marginal distribution. Samples can be drawn from a trained model by first generating Gaussian noise and then simulating the backward process using the score information. Diffusion generative models are currently the dominant framework for image generation and can generate high-resolution images with stunning details and style given prompts by the users [4, 5].\nGiven the success of diffusion models in continuous domains, recent work has explored diffusion modeling in discrete domains [6-13], with applications to language [13], protein sequences [14, 15], graphs [16], and more. Notably, Campbell et al. [9] developed TauLDR, a general framework for discrete denoising diffusion in continuous time. They formulated the forward and backward processes as continuous time Markov chains (CTMCs), and their model learns to predict the distribution over denoised data via an evidence lower bound (ELBO) objective. Lou et al. [13] adopted a very similar strategy but instead directly parameterize the concrete score function an analog of the continuous score function in discrete spaces."}, {"title": "2 Background", "content": "Consider a data point x \u2208 S which is assumed to be sampled from the data distribution \\(P_{data}(x_0)\\). Denoising diffusion models [1-3] seek to model this unknown data distribution by progressively adding noise to data samples until their distribution converges to a simple target, such as a uniform distribution. Then, a deep neural network is trained to invert this process and remove the noise. This reverse process forms a generative model from which new samples can be drawn. The noising and generative processes are also called the forward and backward processes, respectively. We will adopt this convention for the rest of the paper.\nContinuous-time diffusion models define a forward process with noising distribution \\(q_t(x_t|x_0)\\) and marginals \\(q_t(x_t) = \\int q_t(x_t|x_0)q_0(x_0)dx_0\\), with \\(q_0 = P_{data}\\) and \\(q_T \\approx \\pi\\) where \\(\\pi\\) is the limiting distribution of the forward process, which is assumed to be a simple distribution. Then, a parameterized process with marginals \\(p_\\theta\\) is learned to match the true backward process.\nWhen the data of interest is discrete in nature (e.g., text, protein sequences, neural spike counts), the forward-process can be described with a continuous-time Markov chain (CTMC), which is captured by the initial distribution \\(q_0\\) and a transition rate matrix \\(R_t \\in \\mathbb{R}^{S \\times S}\\), where \\(S = |\\mathcal{S}|\\) denotes the cardinality of the data space. The off-diagonal terms \\(R_t(x, y)\\), \\(x \\neq y\\) of the matrix represent the rate of state \\(x\\) transitioning to state \\(y\\), and the diagonals \\(R_t(x, x)\\) are defined as the negative sum of all other elements in the same row: \\(R_t(x,x) = - \\sum_{y \\neq x} R_t(x, y)\\). Under the assumption that \\(R_s\\) and \\(R_t\\) commute for all \\(s\\) and \\(t\\), the finite-time transition probabilities satisfy\n\\[q_{t+\\Delta t|t}(y | x) = \\left[\\exp \\left{\\int_{t}^{t+\\Delta t} R_s ds\\right}\\right](x,y) = I(x,y) + R_t(x, y)\\Delta t + o(\\Delta t),\\tag{1}\\]\nwhere \\(I\\) represents the identity matrix, and in the square brackets we compute a matrix exponential. It can then be shown [9] that the time reversal of the above CTMC is another CTMC with the transition rate\n\\[R_t(y, x) = R_t(x, y) \\frac{q_t(x)}{q_t (y)} = R_t(x, y) \\frac{\\sum_{x_0} q_{t|0} (x | x_0) q_0(x_0)}{\\sum_{x_0} q_{t|0} (y | x_0) q_0|t(x_0 | y)}.\\tag{2}\\]\nCampbell et al. [9] approximate the backward rate by learning a parameterized denoising model \\(p_\\theta(x_0 | y) \\approx q_{0|t}(x_0 | y)\\). Alternatively, Meng et al. [18] and Lou et al. [13] observed that the"}, {"title": "2.1 Continuous-Time Discrete Diffusion Models", "content": "Despite the conceptual progress on discrete diffusion, many challenges in implementation still limit the effectiveness of these models. One major challenge lies in the trade-off between efficiency and accuracy in simulating the continuous-time backward process. Song et al. [17] proposed using approximate MCMC correctors to correct for the discretization error in simulating diffusion backward process. Campbell et al. [9] extended predictor-corrector sampling to discrete diffusion models and discovered that the corrector steps significantly impact generative performance. However, a clear understanding of how the choice of corrector steps influences sample quality is still lacking.\nCan further gains be made from leveraging different sampling methods? We give an affirmative answer. By analyzing the forward-backward corrector used in TauLDR, we identify failure modes in absorbing state diffusion and develop a simple synthetic \u201ccountdown\u201d dataset that exposes the pathologies of standard corrector steps. To address this shortcoming:\n\u2022 We propose a family of \"informed correctors\" and accompanying training schemes that further capitalize on the score information gathered by the model during training. We use them to guide the sampling process, correcting mistakes and accelerating mixing.\n\u2022 We propose an alternative to T-leaping called k-Gillespie's, which takes a fixed number of updates \\(k \\geq 1\\) per backward step. We show that it generally achieves better sample quality than T-leaping for the same number of model evaluations.\nWe compare these models on the countdown dataset as well as benchmarks for discrete sequence generation including gene expression the monophonic music sequences, We show that informed correctors, together with our other algorithmic and architectural proposals, produce higher quality samples with fewer model evaluations."}, {"title": "2.2 Sequence Modeling with the Absorbing Forward Process", "content": "For real-world applications, the data is often a vector or sequence \\(x_{1:D}\\) of length \\(D\\), where each dimension of the data \\(x_d\\) belongs to a vocabulary, and the full state space is the product of the vocabularies and thus exponentially large. With a shift in notation, we now use \\(S = \\{1, ..., S\\}\\) to denote the \u201cvocabulary\u201d set instead, whereas the full space is represented as \\(S^D\\).\nGiven the factorization of this space, it is natural to model the forward process as a composition of identical independent processes on each dimension, where the rate over the entire state \\(R_t\\) can be written as\n\\[R_t(x, y) = \\beta(t) \\sum_d R_\\flat(x_d, y_d) \\mathbb{1}\\{x_{\\backslash d}=y_{\\backslash d}\\}\\tag{5}\\]\nfor a base rate matrix \\(R_\\flat \\in \\mathbb{R}^{S \\times S}\\), absorbing the time dependence into the scalar coefficient \\(\\beta(t)\\). With this factorization of the forward rate, the backward rates are also factorized, but now with a different time-dependent rate \\(R^d_t\\) for each dimension \\(d\\). This means that the objective in (4) can be decomposed nicely into a sum over dimensions, as detailed by Campbell et al. [9].\nFor generic discrete data, two forward processes are most commonly used: the uniform process and the absorbing state process. We focus on the absorbing state process in this paper, for which the base rate matrix for each dimension can be written as\n\\[R^{\\text{absorb}}_\\flat(x, y) = \\begin{cases} 1 & y = \\text{MASK}, x \\neq \\text{MASK} \\\\ -1 & y = x, x \\neq \\text{MASK} \\\\ 0 & \\text{otherwise}, \\end{cases}\\tag{6}\\]\nwhere we augment the vocabulary set \\(S\\) by introducing the MASK token, which represents the absorbing state. The absorbing state diffusion is important to consider because it is very commonly used to model sequence data without ordinal structure (e.g., text and protein sequences). It is also intimately connected to masked language models [8, 20, 21].\nWe use special notation for the absorbing state process. For a sequence \\(x \\in \\{\\text{MASK}, 1, ..., S\\}^D\\), we use the mask operator \\(y = M_d(x)\\) to denote the sequence obtained by changing the \\(d\\)th component of \\(x\\) to MASK. Finally, we denote the masked indices of \\(x\\) by \\(\\mathcal{M}(x) = \\{d : x_d = \\text{MASK}\\}}\\). Then the set of non-mask indices is denoted by the set complement \\(\\mathcal{M}(x)\\)."}, {"title": "2.3 Sampling From the Backward Process", "content": "Gillespie's method Given learned rates \\(R^\\flat\\) for the backward process, one can approximately sample from the backward Markov jump process using Gillespie's algorithm [22] which computes"}, {"title": "2.4 Corrector Steps for \u03c4-leaping", "content": "Because of the approximate nature of the T-leaping steps, this approach accumulates simulation errors over the course of the backward process. To combat this problem, additional corrector steps can be used to make sure that the marginal distribution of samples \\(x_t\\) at time \\(t\\) matches \\(q_t\\). Campbell et al. [9] show that a corrector process with rates:\n\\[\\tilde{R}(x,y) = R_t(x,y) + \\hat{R}_t(x,y)\\tag{7}\\]\nleaves the stationary distribution \\(q_t\\) invariant. In practice, one must use the learned backward rates \\(\\hat{R}_t\\) as an approximation for the true backward rates \\(R_t\\). We refer to this as the forward-backward corrector, as it is simply a summation of the forward and backward rates. Campbell et al. [9] demonstrate that the forward-backward corrector increases sample quality in practice.\nCorrecting for the absorbing diffusion The necessity of corrector steps is particularly apparent when T-leaping is used in conjunction with an absorbing forward process. In the reversal of the absorbing process, only transitions from the absorbing state to other states are allowed, which is reflected by the structure of \\(R_t\\). Thus, once a token has been generated, it cannot be erased or changed, rendering it impossible for the predictor to fix its own errors. Forward-backward corrector steps mitigate this issue by introducing the forward term \\(R_t\\), allowing transitions in the forward direction, resulting in generated tokens being \"masked out\" again with uniform probability.\nThough forward-backward corrector steps solve the issue in theory, we note that the forward-backward corrector is far from optimal in practice. It can only correct mistakes by masking them at random. Ideally, we would like the corrector to make informed transitions based on which states are more likely under the model. This observation motivates us to find a better alternative to the forward-backward corrector."}, {"title": "3 Methods", "content": "This section presents our main contributions. Focusing on discrete diffusion models with an absorbing forward process, we propose a modification of Gillespie's algorithm, a classic algorithm for simulating CTMCs, to approximate the backward process with fewer model evaluations. Then, motivated by the discussion above, we develop novel corrector steps for the reverse process. Taking inspiration from the Stein operator literature, we present informed correctors, a family of Markov transition operators that leverage score information learned by the model to more efficiently address mistakes in the reverse process and improve sample quality."}, {"title": "3.1 k-Gillespie's Algorithm", "content": "In the absorbing state reverse process, a change in the state happens if and only if a new token is generated from the absorbing state. This means that the number of state changes is exactly equal to the number of positions D, which, in Gillespie's algorithm, also corresponds to D model evaluations.\nUnlike Gillespie's, T-leaping can freely choose the number of model evaluations by changing the step size \\(\\tau\\). This enables a trade-off between sample quality and computation. Unfortunately, a Poisson-distributed number of updates in the T-leaping algorithm means that on some steps, no"}, {"title": "3.2 Informed Correctors", "content": "We propose the following family of correctors for discrete diffusion sampling by combining the symmetrized forward process and with an additional \u201ccorrector operator,\" \\(A_t\\),\n\\[\\tilde{R}(x,y) = \\begin{cases} \\frac{1}{2} (R_t(x, y) + R_t(y,x)) \\cdot A_t(x, y) & x \\neq y \\\\ {-\\sum_z \\tilde{R}(x, z)} & x = y \\end{cases}\\tag{8}\\]\nHere, \\(A_t\\) is the matrix representation of a Markov transition operator that leaves the marginal distribution \\(q_t\\) invariant. That is, a continuous time Markov jump process generated by \\(A_t\\) has stationary distribution \\(q_t\\). One way to obtain a corrector operator is by appealing to the literature on Stein operators. For example, a Zanella Stein operator [23] is of the form,\n\\[A_t(x, y) = \\begin{cases} \\kappa(t) \\frac{q_t (y)}{q_t (x)} & y \\neq x \\\\ {-\\sum_z A_t(x,z)} & y = x \\end{cases},\\tag{9}\\]\nwhere \\(\\kappa\\) is a positive function satisfying \\(\\kappa(t) = t \\kappa(1/t)\\).\u00b9 In this work, we will focus on the minimum probability flow (MPF) Stein operator [24, 25], where \\(\\kappa(t) = \\sqrt{t}\\), and the Barker Stein operator [26, 27], where \\(\\kappa(t) = \\frac{t}{1+t}\\), two examples of Zanella Stein operators that have been found to work well in practice [26].\nIn practice, we would like to use \\(\\hat{A}^t\\) to approximate \\(A_t\\), where \\(\\hat{A}^t\\) uses the learned concrete score \\(s^\\flat_t\\) in place of the true \\(s_t\\). However, it is important to note that this approximation is not always viable, since the score information \\(s^\\flat_t(\\frac{q_t (y)}{q_t (x)})\\) is not learned by the model for all pairs of \\((x, y)\\), but only for those that are allowed by the forward process. This follows from the ELBO objective in (4), where the learning signal is weighted by the forward rates. Therefore, the model can learn poor approximations for \\(\\hat{A}^t\\) if the forward rate \\(R_t(x, y)\\) is 0 or very small.\nWhile diffusion with a uniform forward process is not affected by virtue of sampling transitions between all pairs of states during training, this limitation becomes apparent for absorbing state diffusion, which only allows mask to non-mask transitions but not vice versa. In the next section, we show why having informed correctors is especially important in the absorbing case and illustrate ways to learn informed correctors with small adjustments to the score function architecture."}, {"title": "4 Learning Informed Correctors for Absorbing State Diffusion", "content": "We develop an approach for learning informed correctors for discrete diffusion models with absorbing forward processes. Specifically, we modify the parameterization and architecture such that when trained with the same ELBO objective, the model learns the necessary score information efficiently. Note that this change in architecture is only required for the absorbing state diffusion, and for uniform process diffusion informed correctors can be applied directly without changing the learning scheme."}, {"title": "4.1 Alternative Parameterization for Efficient Score Evaluation", "content": "The corrector rates in (8) and (9) provide a nonzero rate of converting the token at the \\(d\\)-th dimension of \\(x\\) back into a mask. These rates depend on the score function \\(s_t(x) \\frac{q_t(M^d(x))}{q_t(x)}\\).\n\\qquad^{1}Zanella [23] defines a neighborhood of state x, but premultiplying by the symmetrized forward rate matrix \\(R_t(x, y) + R_t(y, x)\\) achieve a similar effect, allowing us to simplify the presentation of \\(A_t\\)."}, {"title": "4.2 Learning the Full Score Information Using Hollow Transformers", "content": "Despite the conceptual simplicity, for this parameterization to work in practice, one must make sure that for \\(d \\in \\mathcal{M}(x)\\) information about \\(x_d\\) does not leak to the output \\(f_\\theta (x,t)_{d,i}\\). To achieve this, we use a hollow transformer [28], a two-stream self-attention architecture where each stream runs causal attention in a different direction. The embeddings are then offset by one position so that the embedding at position d does not attend to the corresponding input position. By combining these two directional streams, the hollow transformer satisfies the property that, for continuous x,\n\\[\\frac{\\partial}{\\partial x_d} f_\\theta^{HT}(x,t)_{d,i} = 0 \\quad \\forall i \\in \\{1, ..., S\\}.\\tag{11}\\]\nIn other words, the network output at dimension d is agnostic to the input at the same dimension d. We provide more details of the model architecture in Section 6.\nLearning the Hollow Transformer with the ELBO Objective The parameterization in (10) can be learned with the hollow transformer without changing the ELBO objective (4). To see why, consider a pair of observations x and x' = \\mathcal{M}_d(x) such that \\(x_d \\neq \\text{MASK}\\). By the property of the hollow transformer, \\(f_\\theta^{HT}(x,t)_{d,i} = f_\\theta^{HT}(x',t)_{d,i}\\). However, since the forward rate \\(R_t(\\cdot, x)\\) is 0 under the absorbing forward process, the backward rate \\(\\tilde{R}_t(x, \\cdot)\\) is also 0. This means that during training, gradients only flow through \\(\\tilde{R}_t(\\mathcal{M}_d, \\cdot)\\), which encourages the network output \\(f_\\theta^{HT}(x,t)_{d,i}\\) to approach the true denoising distribution \\(q_{0|t}(x^d = i | \\mathcal{M}_d(x))\\) which is conditioned on the masked state \\(\\mathcal{M}_d(x) = x'\\). In other words, since the model does not see \\(x_d\\) when producing the output at dimension \\(d\\), it always tries to produce outputs as if \\(x^d = \\text{MASK}\\) to minimize the loss."}, {"title": "5 Related Work", "content": "Discrete diffusion models and predictor-corrector sampling. Sohl-Dickstein et al. [1] derived a discrete diffusion model but only for binomial variables. Hoogeboom et al. [6], Song et al. [7] extended the formulation to categorical variables using a uniform noising distribution, which was further generalized by Austin et al. [8] to enable any corruption processes specified with a transition matrix. Austin et al. [8] also discussed the continuous-time limit of the proposed discrete diffusion model. A continuous-time objective amenable to training was first proposed by Campbell et al. [9] and further developed by Benton et al. [12], Lou et al. [13], Shi et al. [19]. In addition to this line of"}, {"title": "6 Experiments", "content": "We evaluate k-Gillespie's algorithm and informed correctors on synthetic and real datasets for both unconditional and conditional sampling tasks. We train a hollow transformer architecture augmented for denoising diffusion using standard implementation techniques [17]. The inputs to the transformer are treated as one-hot variables, destroying all ordinal structure of the data. Our networks use 5M to 8M parameters depending on the task. Training is done using a single A100 GPU with 80GB of memory on an academic cluster.\nTo study efficiency-quality trade-offs, we measure different dataset-dependent metrics and also record the number of function evaluations (NFE) needed to produce each sample under different sampling schemes. For T-leaping, we control NFE via tuning the \\(\\tau\\) parameter, which is the inverse of the number of predictor steps. For our k-Gillespie's, we change k from 1 to 3, with higher values of k resulting in fewer NFEs. Keep in mind that each corrector step also requires one function evaluation."}, {"title": "6.1 The Countdown Dataset", "content": "To check our intuition that standard forward-backward correctors would perform poorly for absorbing diffusion processes (see Section 2.4), we create a synthetic sequence dataset with strong position-wise correlation structure and test generation quality for different corrector steps. Given a state size S and sequence length D, a data sequence \\(x_{1:D}\\) is drawn according to the following rule:\n\\[\\begin{aligned} x^1 &\\sim \\text{Uniform}\\{1, ..., S\\}, \\\\ x^{d+1} | x^d &\\sim \\begin{cases} \\delta_{x^d-1} & x^d\\neq 0 \\\\ \\text{Uniform}\\{1, ...,S\\} & x^d = 0 \\end{cases} \\end{aligned}\\tag{12}\\]\nWe evaluate sample quality for this dataset using the error rate measured by the proportion of generated tokens that fail to count downward from the previous token:\n\\[r_{\\text{err}}(\\{x_n\\}_{1:N}) = \\frac{1}{ND} \\sum_{n=1}^N \\sum_{d=1}^D \\mathbb{1}\\{[x_{n+1,d} - (x_{n,d}-1)] \\mathbb{1}\\{x_{n,d}\\neq 0\\}\\}.\tag{13}\\]\nThe results are shown in Figure 1. We confirm the observation from [9] that more corrector steps (thicker lines) improves sample quality. However, the forward-backward corrector (orange curves) notably benefits the least from increased corrector steps in proportion of errors. This confirms the intuition that the forward-backward corrector is bad at correcting mistakes due to masking tokens uniformly at random.\nExcept for the MPF corrector on the proportion of errors metric, k-Gillespie's (solid curves) greatly outperforms T-leaping across different setting, achieving better sample quality with fewer than half as many model evaluations. Notably, MPF (pink curves) outperforms other correctors by an order of magnitude in reducing errors. We demonstrate this visually in Figure 2."}, {"title": "6.2 Gene Expression Data", "content": "To test if these sampling schemes also capture correlations on real data, we use a gene expression time series dataset [36] and the evaluation metrics from Kerrigan et al. [37]. We quantize the values by binning them into S = 64 discrete bins and train our model on the discretized data. Kerrigan et al. [37] evaluates the quality of the generated sequences by computing point-wise statistics for each position along the sequence lengths and computes the mean squared error (MSE) of these statistics between the samples and true data.\nUnfortunately, these point-wise metrics are ineffective at testing the correlation structure of individual sampled sequences, since they only measure marginal distributions on each sequence dimension. However, as we have demonstrated previously, correctors are needed to compensate for the predictor's failures to capture dependencies in the data. In order to better test this, we calculate the point-wise metrics on the discrete cosine transform (DCT) of the sequences.\nThe results are shown in Figure 3. Each method uses 56 model evaluations and 2 corrector steps per predictor step. We find that k-Gillespie's still reliably outperforms T-leaping, while the MPF informed corrector yields the best performance on many metrics."}, {"title": "6.3 Conditional Generation on Monophonic Music", "content": "Following Campbell et al. [9], we test conditional generation by modeling songs from the Lakh pianoroll dataset. In this dataset, monophonic sequences of length 256 are extracted, with each coordinate representing one of 128 notes or a rest. Campbell et al. [9] trained a discrete denoising diffusion model with the uniform forward process. The training objective was altered to target the last 224 time steps of the input sequence given the first 32. We construct our hollow transformer model with approximately the same number of parameters and train it for over 1M iterations."}, {"title": "7 Discussion", "content": "We proposed informed correctors and modified algorithms for discrete diffusion models with absorbing forward processes. We showed that common scheme of 7-leaping with the forward-backward corrector is not well-suited to this setting. To address this problem, we develop novel, informed correctors and provided a modified training strategy to learn them. We showed that this approach produces superior sample quality with fewer model evaluations than previous methods. Overall, our work serves to further enhance the capabilities of diffusion models in discrete regimes.\nLimitations Using the informed correctors for absorbing diffusion requires architecture modifications such as using the hollow transformer. In practice, we cannot apply the correctors directly for pretrained models and have to retrain them from scratch. This limits the direct applicability of our methods.\nFuture work In developing the informed corrector, we observed that design decisions in the training and sampling phases can be strongly coupled. To leverage better sampling methods, the need for modifying the training setup naturally arises. Future work could explore this coupling in more depth, for example, by studying how the architecture and training objective can be further adapted to efficient sampling strategies."}, {"title": "A Unifying the TauLDR and score entropy objectives", "content": "We first reproduce the TauLDR objective:\n\\[\\begin{aligned} \\mathcal{L}(\\theta) & = \\mathbb{E}_{q_t(y)r_t(z|y)} \\left[ R_t(y) - \\sum_{z'\\neq y} R_t(y, z') \\log(\\tilde{R}_t(z',y)) \\right] dt \\tag{14} \\\\ & = \\mathbb{E}_{q_t(y)r_t(z|y)} \\left[ R_t(y) - R_t(y) \\log(\\tilde{R}_t(z,y)) \\right] dt, \\tag{15} \\end{aligned}\\]\nwhere \\(R_t(y) \\equiv -R_t(y,y) = \\sum_{z'\\neq y} R_t(y, z')\\). Here \\(r_t\\) represents the probability of a specific transition, given that a transition happens at time t:\n\\[r_t(z | y) = \\begin{cases} \\frac{R_t (y,z)}{R_t (y)} & y \\neq z \\\\ 0 & y = z \\end{cases}\\tag{16}\\]\nCampbell et al. [9] rewrote the joint distribution :\n\\[q_t(y)r_t(z | y) = \\sum_{x_0} q_0(x_0)\\psi_t(z | x_0)\\phi_t(y | z, x_0),\\tag{17}\\]\nand then analytically integrate out y to reduce sampling variance, given factorization assumptions on \\(R_t\\). The downside of this approach is that the resulting loss becomes very complicated and unintuitive.\nTo circumvent this complication, we propose to instead rewrite the objective with a label switching trick:\n\\[\\begin{aligned} \\mathcal{L}(\\theta) & = \\mathbb{E}_{q_0(x_0)q_t(y|x_0)r_t(z|y)} \\left[ R_t(y) - \\sum_{z\\neq y} R_t(y, z) \\log(\\tilde{R}_t(z,y)) \\right] dt \\tag{18} \\\\ & = \\int \\mathbb{E}_{q_0(x_0)q_t(y|x_0)} \\left[ R_t(y) - \\sum_{z\\neq y} R_t(y, z) \\log(\\tilde{R}_t(z,y)) \\right] dt \\tag{19} \\\\ & = \\int \\mathbb{E}_{q_0(x_0)q_t(y|x_0)} \\sum_{z\\neq y} \\left[ \\tilde{R}_t(y, z) - R_t(y, z) \\log(\\tilde{R}_t(z,y)) \\right] dt \\tag{20} \\\\ & = \\int \\mathbb{E}_{q_0(x_0)} \\sum_y \\sum_{z\\neq y} \\left\\{ q_t(y | x_0) \\tilde{R}_t(y, z) - q_t(y | x_0) R_t(y, z) \\log(\\tilde{R}_t(z,y)) \\right\\} dt \\tag{21} \\\\ & = \\int \\mathbb{E}_{q_0(x_0)} \\sum_y \\sum_{z\\neq y} \\left\\{ q_t(y | x_0) \\tilde{R}_t(y, z) - q_t(z | x_0) R_t(z, y) \\log(\\tilde{R}_t(z,y)) \\right\\} dt \\tag{22} \\\\ & = \\int \\mathbb{E}_{q_0(x_0)} \\sum_y \\sum_{z\\neq y} \\left\\{ q_t(y | x_0) \\tilde{R}_t(y, z) - q_t(z | x_0) R_t(z, y) \\log(\\tilde{R}_t(y, z)) \\right\\} dt \\tag{23} \\\\ & = \\int \\mathbb{E}_{q_0(x_0)} \\sum_y \\left\\{ \\sum_{x\\neq y} \\left[ \\tilde{R}_t(y, x) - R_{t|x_0}(y, x) \\log(\\tilde{R}_t(y, x)) \\frac{q_t(x|x_0)}{q_t(y|x_0)} \\right] \\right\\} dt, \\tag{24} \\end{aligned}\\]\nwhere \\(R_{t|x_0}(y, x) = R_t(x, y) \\frac{q_t(x|x_0)}{q_t(y|x_0)}\\) is the true reverse transition rate conditioned on \\(x_0\\). Note that we switched the labels y and z on line (22). We also renamed z to x on the last line to preserve alphabetical order of the variables for readability. Now we have a simple Monte Carlo estimator for this objective: we just need to sample y from the marginal distribution, and then evaluate the parameterized transition model \\(\\tilde{R}_t(y, x)\\) for all neighbors x of y, which can be done using only one model evaluation."}, {"title": "B Pseudocode for k-Gillespie's Algorithm", "content": "We give pseudocode for the k-Gillespie's algorithm presented in Section 3.1 in Algorithm 1. Note that here we use the generation operator \\(x = G_d(y)\\) to represent the sequence obtained by changing the dth component of y to the token n."}]}