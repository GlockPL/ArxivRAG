{"title": "Tutorial: VAE as an inference paradigm for\nneuroimaging", "authors": ["C. V\u00e1zquez-Garc\u00eda", "F. J. Martinez-Murcia", "F. Segovia Rom\u00e1n", "Juan M. G\u00f3rriz"], "abstract": "In this tutorial, we explore Variational Autoencoders (VAEs), an essential framework for unsupervised learning, particularly suited for high-dimensional datasets such as neuroimaging. By integrating deep learning with Bayesian inference, VAEs enable the generation of interpretable latent representations. This tutorial outlines the theoretical foundations of VAEs, addresses practical challenges such as convergence issues and overfitting, and discusses strategies like the reparameterization trick and hyperparameter optimization. We also highlight key applications of VAEs in neuroimaging, demonstrating their potential to uncover meaningful patterns, including those associated with neurodegenerative processes, and their broader implications for analyzing complex brain data.", "sections": [{"title": "1 Introduction", "content": "Variational Autoencoders (VAEs) have emerged as a powerful tool for unsupervised learning, offering a framework to model complex, high-dimensional data through probabilistic inference [1, 2]. Unlike traditional autoencoders, VAES integrate principles from Bayesian inference, allowing them to generate and reconstruct data by learning latent representations that are both interpretable and continuous [3]. This paradigm has proven particularly valuable in fields dealing with intricate and multidimensional datasets, such as neuroimaging.\nNeuroimaging data, which includes structural and functional brain scans, often exhibits high dimensionality, noise, and heterogeneity. These characteristics make traditional machine learning approaches prone to overfitting or limited generalization [4]. Moreover, the integration of neuroimaging data with other modalities such as cognitive assessments, cerebrospinal fluid markers, or genetic information-requires robust generative models capable of capturing complex relationships while preserving interpretability. This challenge is particularly crucial when integrating multimodal datasets, such as combining structural brain scans with genetic or cognitive markers.\nWhile the theoretical foundations of VAEs are well established, their practical implementation often comes with significant challenges. Issues such as convergence to suboptimal solutions, over-regularization leading to a loss of expressivity, or difficulties in normalizing multimodal data are common hurdles in real-world applications [5, 6]. Furthermore, in the context of neuroimaging, VAEs must tackle domain-specific challenges, such as the alignment of data across individuals or the scarcity of large, labeled datasets [7].\nThis tutorial aims to provide a clear and intuitive explanation of the core concepts of VAEs, providing a comprehensive yet intuitive explanation of their theoretical underpinnings. Additionally, it explores common pitfalls encountered during implementation, offering insights into strategies to mitigate these issues. Throughout the tutorial, we emphasize applications in neuroimaging, showcasing the potential of VAEs to uncover latent structures in complex brain data and enhance our understanding of neurological and psychiatric conditions. By bridging the gap between theory and practice, this tutorial serves as a guide for researchers interested in leveraging VAEs as an inference paradigm in neuroimaging."}, {"title": "1.1 An opportunity: latent generative models", "content": "Neuroimaging plays a central role in studying NDDs due to its non-invasive nature and ability to provide detailed insights into brain structure and function [8]. However, the high dimensionality of neuroimaging data presents challenges for modeling, often leading to the curse of dimensionality, where the number of features surpasses the number of available samples [9].\nThe manifold hypothesis offers a potential solution, suggesting that high-dimensional data tends to concentrate around lower-dimensional latent manifolds [10]. This concept has inspired a range of techniques to uncover these hidden structures, from classical approaches like PCA to advanced machine learning methods such as Autoencoders [11], GANs [12], and Latent Diffusion Models [13]. For a further mathematical insight of the manifold hypothesis refer to appendix A.\nThis manifold hypothesis provides a powerful foundation for neuroimaging analysis. Neuroimaging data often lie on lower-dimensional latent manifolds, which can be leveraged to uncover meaningful patterns associated with degeneration-patterns that may not be directly observable to the naked eye-. Techniques such as Principal Component Analysis (PCA) have long been used to model neuroimaging data in clinical settings through latent variables due to its popularity and powerful capability of uncovering hidden patterns in data. Beyond PCA, a wide array of methods aim to identify the underlying manifold structure hidden within complex datasets. For example, Tenenbaum et al. [10] demonstrated how data points that appear close in Euclidean space, such as those distributed along a spiral, may actually be far apart when considering the manifold's true geometry. Techniques like PCA or Multidimensional Scaling (MDS) [14] often fail to capture such complex non-linear structures, emphasizing the need for more sophisticated approaches.\nLocally Linear Embedding (LLE) [15] is one such method that reconstructs the latent manifold by capturing local geometry. It represents each data point as a linear combination of its neighbors, recovering non-linear structures effectively. However, LLE has limitations: it is sensitive to noise, which can distort the manifold, and relies on sufficient data density to ensure accurate connections between neighboring points. In scenarios where data is sparse or noisy, its performance may degrade significantly.\nRecent advances in Deep Neural Networks (DNNs) have introduced a new paradigm for representing data in non-linear latent spaces. Models like Autoencoders (AEs), Generative Adversarial Networks (GANs), and Latent Diffusion Models (LDMs) offer powerful tools for uncovering and representing latent structures. But, why favor deep learning over simpler methods? Unlike traditional techniques, which rely on predefined mathematical procedures, DNNs are highly flexible and adaptable, capable of learning complex patterns across diverse data types [16, 17]. Their ability to model non-linearity is particularly critical, as real-world data rarely conforms to linear assumptions.\nMoreover, DNNs draw inspiration from the brain, mimicking processes of neural encoding. The brain processes an immense volume of high-dimensional sensory input by encoding it into lower-dimensional, abstract representations. This principle is well-documented in vision research. For instance, Rowekamp and Sharpee [18] analyzed the response of V2 neurons in the visual cortex to natural stimuli. These neurons encode complex visual inputs from earlier cortical stages into simpler structures, such as multi-edge features, which help identify contrasts and object boundaries. Subsequent stages of neural processing abstract these features further, transforming them into complex ideas a process still under active investigation in psychology."}, {"title": "2 The variational autoencoder as a latent generator paradigm", "content": "When working with 2D images or 3D volumes, one of the most commonly used frameworks of DNN is the Variational Autoencoder (VAE), which is a modification of the usual Autoencoder, based on Bayesian inference. While Autoencoders learn a fixed latent representation, VAEs learn a probabilistic distribution of the latent representations. This model was introduced by Kingma in its work [1], where he proposed a way to learn the intractable posterior distribution that encode the real data into the latent representation. Imagine we have a dataset X of i.i.d. samples of a continuous variable. We assume that there is some unknown random hidden process that generates the data, concerning some unobserved variable z. This unobserved variable is drawn from some prior distribution p(z). Therefore, since every sample from X is generated by the hidden process, each value x is generated by the conditional distribution p(x|z). For simplicity, we assume that both the prior function and the likelihood p(x|z)"}, {"title": "2.1 The theory of the Variational Autoencoder", "content": "come from a parametric family of functions that we parameterise as 0 and that their PDFs are differentiable with respect to z and 0. The general idea is to find the parameter 0 that maximizes the likelihood p(x|z). In summary, we just want to find which parameters make it more believable that our real data x come from the latent representation z. This problem is modelled by the joint distribution of X and Z, known as the generative model, given by:\n$p(x, z) = p(x)p(z|x) = p(z)p(x|z).$ \n(1)\np(x) is the marginal likelihood of x, integrated over every possibility of z, and p(zx) is the true posterior density, that is, how probable is the value of z given the observation x. In order to encode the data and obtain the latent representation we need to find the true posterior density, which tells us how data is encoded. If we isolate it from (1) we obtain the expression:\n$p(z|x) =  \\frac{p(z)p(x|z)}{p(x)} $ \n(2)\nHowever, this is intractable in every case that p(x) is intractable, which is every practical case. It is not hard to image due to the high dimensionality that the integral p(x) = \u222b p(z)p(x|z)dz is intractable. In order to address this issue, Kingma introduced a variational inference model to approximate the true posterior density with an inference posterior q(zx), where $ is a parameter. In variational autoencoders, we substitute the intractable true posterior (2) with this approximating inference model q parameterized with a deep neural network known as encoder. The mathematical formalism of the VAE allows us to obtain a lower bound for the log-likelihood. If we assume i.i.d. data points, then we can write the marginal log-likelihood of a single observation as:\n$logp(x) = log( \\frac{p(z)p(x|z)}{p(z|x)}) = -logp(z|x) + log(p(z)p(x|z)).$\n$=log(p(x,z)) $ \n(3)\nIntroducing the approximating inference model logq$(z|x) by adding and sub-stracting in one side of the equation, we get:\n$logp(x) = -logp(z|x) + log(p(x, z)) + log( \\frac{q$(z|x)}{q$(zx)}) $\n(3)\n$ = log( \\frac{q$(z|x)}{p(z|x)}  + logp(x, z) - logq$(z|x). $\nNow, let us take the expectation with respect to q(z|x) of both sites:\n$logp(x) = E_{q$\\phi}(z|x)} [log \\frac{q$(z|x)}{p(z|x)}] +E_{q$\\phi}(z|x)} [logp(x, z) - logq(z|x)] . $\n(4)"}, {"title": "2.2 The practical implementation of the Variational Autoencoder and its complications", "content": "Now that we have seen the theory, it is appropriate to talk about its practical implementation and its use in neuroimaging. So far, we have talked about the generation of latent manifolds, but we have also mentioned that VAE has a true posterior density p(z|x) that accounts for the encoding of the real data in its latent representation, and a likelihood p(x|z) that accounts for the decoding of the latent manifold into real-like reconstructed data. So, the architecture of the VAE consists of both an Encoder E and a Decoder De. If we look at the ELBO function (6) we see that it consists simply of a reconstruction term E[logp(x|z)] that tells us how well the decoder reconstructs the data from the latent space under the inference model 4, and a regularization term $D_{KL}(q_{\\phi}(z|x)||p(z))$ that tells us how close the inference model is to the prior distribution. Hence, the loss of the model can be easily computed by computationally implementing both terms. For the reconstruction term one can choose a reconstruction function that best fits the needs of its problem. A simple and efficient choice is the mean squared error (MSE), which provides physical meaning. However, in the context of images, this reconstruction measure is not well suited to match perceived visual quality. The mean squared error cannot retrieve the structural features of the images because it does not take into account the spatial relationships throughout the image. In addition, it tends to focus on the data points that perform the worst [20, 21, 22]. For neuroimaging, a good choice is the Structural Dissimilarity Index (DSSIM) between two images or volumes, which"}, {"title": "2.2.1 The gradient problem and the reparameterization trick", "content": "Once the loss function computation is clear, all that is left is to backpropagate this loss through our network in order to update the weights. To do so, we need to compute the gradient of the ELBO (6) with respect to the parameters of the model 0, 4, which is a bit tricky. On the first hand, the gradient with respect to \u03b8 is:\n$V_{\\theta}L = \\nabla_{\\theta}E_{q_{\\phi} (z|x)}[logp(x|z)] - \\nabla_{\\theta}D_{KL}(q_{\\phi}(z|x)||p(z)).$ \n(8)\nTo compute the first term we can replace the expectation value with an unbiased Monte Carlo estimator [29]:\n$\\frac{1}{N} \\sum^{N}_{i=1}$ $\\log p(x/z_i)$,\nwhich allows us to rewrite the gradient as:\n$V_{\\theta}L\\approx \\frac{1}{N} \\sum^{N}_{i=1}$ $\\nabla_{\\theta}\\log p(x|z_i),$\n(9)\nwhere the term of the divergence is zero because it does not depend on the parameter 0. If we now take the gradient with respect to $ we have the following expression:\n$\\frac{1}{N} \\sum^{N}_{i=1} \\nabla_{\\phi} \\log p(x|z_i) - \\nabla_{\\phi} D_{KL}(q_{\\phi}(z|x)||p(z)).$\n(10)\nThe problem with the first term of the expression (10) is that, if we sample z directly from $q_{\\phi}(z|x) \\sim N(\\mu_{\\phi}(x), \\sigma_{\\phi}(x))$, the relationship between z and $ is not smooth due to the random nature of the sampling, and thus it is not differentiable. On the other hand, since $q_{\\phi}(z|x)$ is differentiable with respect to $ there is no issue with the gradient of the regularisation term. The solution to this issue, as proposed by Kingma [30], is straightforward. Given that the problem arises from the stochastic nature of the sampling process, it can be addressed by drawing a sample from a standard normal distribution and subsequently reparameterizing it as:\nz = \u03bc(\u03c6; x) + \u03c3(\u03c6; x)\u00b7 \u03b5,\nwhere \u20ac ~ N(0,1). Since the distribution of the random variable e is independent of every parameter or x, then the reparameterization z = f ($, x, e) is now"}, {"title": "2.2.2 The information preference problem and the hyperparameter space", "content": "Aside from overfitting, there is a more concerning issue for variational autoencoders, regarding its autoencoding capability. Even though VAEs are interpreted as autoencoders, meaning that the generated data is close to the real data, the conditions in which this happens are not discussed in the original article [1]. As discussed in the article [34], VAEs do not always autoencode and they may not even use the information in the latent space unless the decoder is weak. In this work they use a Bits-Back coding approach to prove that if the decoder p(xz) is able to reconstruct the data without using the information of the latent space, then it will not use it, in which case it would set q$(z|x) to simply be equal to the prior p(z). This means that we have to achieve a balance between the complexity of the model, which could lead to both overfitting and loss of latent information, and a significant latent representation, in order to obtain accurate patterns of neuroimaging. This issue occurs because, as we have seen in the ELBO formula (5), the KL divergence simply does not care about the information of the latent space, it only cares about the similarity between the variational model and the prior. We may think that if q4(z|x) is close to p(z) that would be good, because we are minimizing loss. However, if our q(zx) is too close to the prior, we would not capture the specific inter-subject variability and thus our representation would not be meaningful. To"}, {"title": "2.3 Latent representations as a benchmark for analysis in neuroimaging", "content": "Variational Autoencoders are explicit probabilistic models, meaning they learn an explicit distribution over the latent space, which is typically designed to be accessible and interpretable, opposed to other latent generative models such as GAN or LDM, which learn implicit representations that are useful for performing different tasks such as synthesis or reconstruction, but that do not provide great insights of the latent variables.\nNow that our VAE model is finally operative, we can perform some statistical analysis to these representations. Remember that all the information that we have extracted is encoded within our latent space. To exploit this latent variables there are many statistical techniques that we can apply. Firstly, we know that our model is correctly reconstructing volumes and inter-subject variability by using the latent variables, which means that there is information within those variables that understand the structural differences between subjects. For example, there may be some variables that encode the size of the ventricles, or the thalamus or even the deterioration of the temporal lobes. We are interested in which properties are codified within each latent variable. However, this relationship may not be simple. In the case of structural size, we may find a variable that is able to codify it, but if we try to find a variable that codifies dementia we would probably find nothing. Instead of a single latent variable, it could be codified as a linear (or non-linear) combination of the variables of the latent space. If we were able to find such a relationship we would have a model that is able to codify and understand the patterns related to dementia.\nFor instance, imagine we want to see whether dementia is encoded in our latent variables, and we want to identify which variables are more relevant for such encoding. One way is to use an SVR model, which learns a function to predict the target variable (e.g., dementia diagnosis) based on the latent variables. SVR uses kernels that map the input data into higher-dimensional spaces where non-linear relationships can become linear. However, this transformation is performed implicitly, meaning that there is no explicit formula showing how the latent variables combine to predict the target, which makes the model act like a black box. This is not desirable since our main goal is to interpret the information of the latent space.\nTo address this issue, alternative methods, such as the Generalized Linear Model (GLM), can be employed. GLMs extend linear regression by introducing a link function that connects the expected value of the response variable to a linear combination of the latent variables. This allows the model to capture non-linear relationships through explicit transformations. Unlike SVR, where the kernel implicitly transforms the data, GLMs offer direct control over the link function, enabling interpretability. While GLMs may be less flexible than SVR in terms of the transformations they can model, they provide valuable insights into how the latent variables contribute to the target variable.\nOther techniques, such as polynomial or spline regression, can also capture linear and non-linear relationships between the latent variables and the response"}, {"title": "3 Latent Representations for Longitudinal Patterns:", "content": "Even though most research in the literature focuses on latent representations for reconstruction or data synthesis, recent studies have explored their use in extracting meaningful insights. Below, we highlight notable works that leverage latent representations in neuroimaging:\n1. Fusing Functional and Structural Neuroimaging Data: In [41], the authors employ a VAE model to integrate functional and structural neuroimaging data into a shared latent space using a unified encoder-decoder scheme. This approach enables the model to learn cross-modal patterns, facilitating interpolation and interpretability between modalities. By integrating information from both modalities, the study demonstrates improved insights compared to analyzing each modality independently.\n2. Cross-Modality Latent Representations: The work in [42] develops a joint VAE model combining neuroimaging and clinical data. The authors demonstrate that specific latent variables can reconstruct clinical data (e.g., UPDRS scores) using neuroimaging representations, and vice versa\n3. This is achieved through a cross-modality loss function that enforces shared latent representations during training. The study highlights the potential of such models to capture cross-modal patterns, particularly in the context of Parkinson's disease, where the latent representations of medical volumes and clinical data collectively recover the UPDRS score, a key measure of symptom severity and progression."}, {"title": "Appendix A: Mathematical Implications of the Concentration Phenomenon", "content": "To better understand why high-dimensional data often concentrates around lower-dimensional structures, we can analyze the geometry of high-dimensional spaces. Consider the volume of a hypersphere, a generalization of a circle (2D) or a sphere (3D) to n-dimensional space. The volume of such a hypersphere with radius R is given by:\n$V_n(R) = \\frac{\\pi^{n/2}R^n}{\\Gamma(\\frac{n}{2}+1)},$ \n(12)\nwhere I denotes the Gamma function, a generalization of the factorial to real and complex numbers. While this formula captures the growth of volume in low dimensions, as n increases, an interesting phenomenom emerges: the volume of the hypersphere becomes increasingly concentrated near its surface. This counterintuitive result implies that, in high dimensions, most points are pushed toward the surface, leaving the center sparsely populated. Consequently, most of the space appears \"empty\", with the volume concentrating near the surface. To explore this, we calculate the volume of a thin shell near the surface of the hypersphere. The volume of the shell is given by:\n$V_{shell} = V_n(R) \u2013 V_n (R \u2013 \\epsilon),$ \n(13)\nwhere R \u2208 Rn+1 is the radius of the hypersphere and e << 1 is a small scalar. Using (12) we can write:\n$V_{shell} = V_n(R) \\frac{\\pi^{n/2}(R \u2013 \\epsilon)^n}{\\Gamma(\\frac{n}{2}+1)}$ \n(14)\nUsing Taylor approximation around R we can write (R - \u0454)n \u2248 Rn \u2013 nRn-1\u20ac, and therefore the volume of the shell is:\n$V_{shell} \\approx V_n(R) - \\frac{\\pi^{n/2}(R)^n}{\\Gamma(\\frac{n}{2}+1)} - n\\frac{\\pi^{n/2}R^{n-1}\\epsilon}{\\Gamma(\\frac{n}{2}+1)},$\n(15)\n$V_{shell} \\approx n \\frac{\\pi^{n/2}R^{n-1}\\epsilon}{\\Gamma(\\frac{n}{2}+1)}.$\nNow, we can compute the fraction of the total volume that the shell represents:\n$ratio_{shell} = \\frac{V_{shell}}{V_n(R)} = n\\frac{\\epsilon}{R}.$\nThis result demonstrates that, for a fixed e, the fraction of volume in the shell near the surface increases with n. In fact, even as e \u2192 0, the ratio approaches 1 as n\u2192\u221e. This implies that, in high dimensions, nearly all the volume is concentrated in a thin shell adjacent to the surface.\nTo gain further intuition, consider the equation for the radius of a hypersphere:\n$r_n = x_1^2 + x_2^2 + \u2026 + x_n^2 + x_{n+1}^2,$\n(16)\nwhere the sphere is centered at the origin. If we choose a random point and calculate the probability of it being close to the center, we find that this probability decreases as the number of dimensions increases. This is because the points have exponentially more configurations in higher dimensions, pushing them away from the center.\nIn summary, in high-dimensional spaces, points tend to cluster close to a lower-dimensional manifold. This phenomenon is a manifestation of the concentration of measure phenomenon [45, 46]. Note, however, that this is a mathematical abstraction, and latent manifolds of real-world data are often more complex."}]}