{"title": "HAPM - Hardware Aware Pruning Method for CNN hardware accelerators in resource constrained devices", "authors": ["Federico Nicol\u00e1s Peccia", "Luciano Ferreyro", "Alejandro Furfaro"], "abstract": "During the last years, algorithms known as Convolutional Neural Networks (CNNs) had become increasingly popular, expanding its application range to several areas. In particular, the image processing field has experienced a remarkable advance thanks to this algorithms. In IoT, a wide research field aims to develop hardware capable of execute them at the lowest possible energy cost, but keeping acceptable image inference time. One can get around this apparently conflicting objectives by applying design and training techniques. The present work proposes a generic hardware architecture ready to be implemented on FPGA devices, supporting a wide range of configurations which allows the system to run different neural network architectures, dynamically exploiting the sparsity caused by pruning techniques in the mathematical operations present in this kind of algorithms. The inference speed of the design is evaluated over different resource constrained FPGA devices. Finally, the standard pruning algorithm is compared against a custom pruning technique specifically designed to exploit the scheduling properties of this hardware accelerator. We demonstrate that our hardware-aware pruning algorithm achieves a remarkable improvement of a 45 % in inference time compared to a network pruned using the standard algorithm.", "sections": [{"title": "I. INTRODUCTION", "content": "ARTIFICIAL neural networks can be defined as a succession of n layers of neurons interconnected in a sequential manner, where the output is given by:\n$0 = \\sum_{k=0}^{j} w_k * i_k + bias$ (1)\n$s = f(0)$ (2)\nWhere @ is the weighted sum of the input of the layer plus a bias value, s refers to the output of the layer and f is an activation function.\nIn a convolutional layer the output is the result of a convolution process between the inputs and a matrix of weights. As each layer dispose of this matrices and a convolution process is performed, it's possible to think each layer as a filter and the weights as the filter's coefficients (a kernel). The increasingly complex networks that are still being developed based on this layers sometimes need to do millions or even billions of multiply and accumulate operations in order to process one single image.\nFor this reason, once trained, these algorithms can undergo different kinds of optimizations according to the hardware on which they will be executed. One of the most commonly used techniques is the absolute value pruning [1]\u2013[3], which eliminates redundant weights. Therefore, at running an algorithm's forward pass, the number of calculations and memory accesses needed decreases significantly.\nAn evolution of this technique was proposed by Zhu M. et. al. [4], who proposed to gradually force coefficients of each layer to zero until the amount of zero coefficients reaches a certain sparsity threshold while retraining to accommodate the remaining parameters and thus maintaining the quality of the prediction.\nOn the other hand, in hardware accelerator designs specifically developed to run a neural network in inference mode, the hardware resource and energy consumption savings are one of the main concerns. This kind of development presents very different challenges from those found during neural network training since the main focus is to find the most efficient way of executing the internal operations needed for an inference pass.\nThis type of architectures are very suited for implementing these algorithms in low-power applications [5]. Different architectures to implement this kind of algorithms on FPGAs were explored [6]\u2013[8]. There is also a wide field of research focused exclusively on ways to improve the efficiency of the convolution operation in hardware [9]\u2013[11], since according to an analysis carried out by researchers from Tsinghua University, 98 % of the mathematical operations performed by the nowadays most commonly used neural network architectures are done in the convolutional layers [12]. When this kind of digital designs are developed, certain characteristics are usually observed and taken into account as key aspects which need to be carefully analyzed, such as latency, partial additions storage, number of accesses to internal buffers, number of accesses to external buffers and data reuse, like spatial and/or temporal reuse [11].\nAnother aspect to take into account is the structure of an optimal processing element. Works like [13] [14] have shown that this could be achieved by using a multiplier and an adder used as an accumulator. In order to maximize the internal data reuse, these works propose a matrix-wise interconnect for this processing elements (PEs) conforming a Systolic Array [15], where the coefficients and the input pixels are shared across adjacent elements. Finally, it is proposed that each PE should have a minimum internal control to be able to implement different kinds of operations and improve its reusability.\nMost of current designs tend to use the algorithm known as im2col to transform the convolution operation into a matrix multiplication, which can be easily accelerated using the increasingly bigger Systolic Arrays that are being developed nowadays. But this algorithm also adds a memory overhead because data needs to be rearranged in specific ways, which produce values duplication in memory. This is unsuitable for low resource devices where memory is a limited resource.\nIn contrast, this work presents a novel hardware accelerator architecture targeting resource constrained devices based on small and reusable Systolic Arrays. In order to validate its operation, the design was implemented on multiple FPGAs and a ResNet type neural network architecture was executed on it, achieving a maximum of 7.468 GOPs classifying images of the CIFAR-10 dataset [16]. Finally, the Hardware Aware Pruning Method (HAPM) is presented, a custom pruning technique which exploits the scheduling properties of this accelerator. When compared against the standard pruning technique, we demonstrate that networks trained with HAPM achieve a remarkable improvement of a 45 % in the inference time per image without significant accuracy loss."}, {"title": "II. DESIGN OF THE HARDWARE ARCHITECTURE", "content": "A. Optimization of the convolution operation\nThe output of a convolutional layer can be calculated using a series of nested loops, as presented in algorithm 1. But in a FPGA, these loops can be parallelized in different ways. In the work by Ma et. al. [11] the impact of 3 ways of implementing these cycles is analyzed, and this is how this strategies were used in this work:\n\u2022 Loop unrolling: the loop at line 2 was unrolled, and the design has the flexibility to unroll it fully or partially, depending on the amount of FPGA resources available. Loops at lines 5,8 and 9 were also partially unrolled.\n\u2022 Loop tiling: the design can be configured to select which layers have their coefficients stored in the internal buffers of the FPGA (Block RAM) and which layers have their coefficients stored in the external RAM. In the latter case, the coefficients are first brought to the internal buffers before starting to execute the layer.\nB. Core processing element\nAs stated in algorithm 1, the basic operation is a MACC operation (multiply and accumulate). Using works [13], [14] as a reference, a processing element (PE) as seen in figure 1a was implemented. This PE consists of a multiplier and an adder that can be used both to accumulate the output of itself with the multiplied value, or to accumulate the partial sum that comes from another cascaded PE.\nC. Computation units matrix\nIn order to reuse data while executing the convolution operation, a structure like the one presented in figure 1b was proposed, composed of multiple cascaded PEs remembering a systolic array of dimensions CUx and CUy. By connecting the PEs in this way, it is possible to reuse the input data in multiple PEs: the green arrows indicate the reuse of the filter coefficients that convolve with the image, and the blue arrows indicate the reuse of the image data (or the input layer). This reuse decreases the number of memory accesses by implementing a spatial reuse of data. It also eliminates the need to store special transformations of the input feature maps of a convolutional layer (as opposed to accelerators which use GEMM to calculate the output of a convolution, which incur in memory overheads because of this transformations): data is continuously streamed into the array in columns of CUh = CUx + CUy \u2013 1 values, traversing the input matrix from left to right. The partial results of the convolution are first accumulated into each PE and then forwarded to the top ones. By pipelining the arrival of the filter and data to each PE, this matrix is capable of computing the output of two 3\u00d73 convolutions every 4 clock cycles.\nD. Matrix block\nThis module is designed to instantiate Ncu matrices in parallel as it is presented in figure 1c. This is the module which actually executes the complete convolution operation. By selecting Ncu one can control the desired parallelism level and the amount of resources used in the design.\nA particular design detail is highlighted regarding the buffers of each matrix. In contrast with the data, partial inputs and partial outputs buffers (FIFO buffers), the input coefficient buffer is a circular one, which allows the implementation of a temporal reuse, saving substantial memory access time and energy. This is similar to the Weight Stationary (WS) approach used by many Systolic Array based accelerators like Gemmini [17].\nIn addition, this module can be synthesized with extra hardware elements that dynamically verify if the data loaded in the data or coefficient buffers is zero, in order to avoid losing clock cycles by performing unnecessary multiplications. This feature will be referred during the rest of the paper as Dynamic Sparsity Bypass (DSB).\nE. Convolution scheduling\nThe order in which the data is dispatched to this last module, and the order in which the results of partial sums are read from it, is decided by the layer translator module (see figure 2). Every computation unit matrix receives the same input data but uses different convolution filters. Each one receives a patch of data to process, along with the appropriate filter and data to add to each output. This is called the scheduling of the convolution (see algorithm 2). Lines 5, 6 and 9 represent the loops over the input matrix, and line 13 represents the parallel processing of the matrix block module (section II-D. Function SysArray represents the internal processing that is done in each computation unit matrix (section II-C).\nF. Block diagram of the entire architecture\nFigure 2 shows all the modules interconnected with each other. A brief description of each of one them is presented:\n\u2022 CPU/DDR3 DRAM: this Zynq 7000 resources are used to run a Linux operating system. It's main function is to obtain the image to be analyzed, notify the hardware through a driver that must start processing it, and wait for the result.\n\u2022 CDMA: Performs data transfers between the external memory and the internal memory of the FPGA.\n\u2022 Computation units: already described in section II-A.\n\u2022 Principal modules: the controller, adder and pooling modules each represent one layer of a classical convolutional neural network. They obtain the necessary data from the internal memory, perform the operation (internally or by sending the data to the matrix block module, section II-D) and save the result to memory again.\n\u2022 Layer translator: this module owns the knowledge about the architecture of the neural network to be executed (that is, the layers sequence, the types of each layer, the specific parameters of each one of them, etc.). It coordinates the operation of all the other modules. This allows to make the design configurable, selecting the network to run during the implementation step 2."}, {"title": "III. HARDWARE AWARE PRUNING METHOD", "content": "A common method when using convolutional neural networks in inference mode is to prune the weights of the network, in order to reduce its memory footprint. Several different approaches exist [18]: pruning can be done in an unstructured manner, per layer, per channel; the weights to prune can be chosen based on its absolute value or using the gradient to determine how much every weight contributes to the output; pruning can also be applied gradually (pruning fixed amounts of weights every step) or in a single step.\nThe problem of all this approaches is that the remaining weights could be arranged in such a fashion that it would not be possible to speed that network using the already designed hardware accelerator. It is not enough to add sparsity to the weights of a network: this must be done in a structured way so that the hardware accelerator can take advantage of this sparsity, if it has the necessary logic to do so. This is why this paper presents our Hardware Aware Pruning Method (HAPM), whose pseudocode can be seen in algorithm 3."}, {"title": "IV. MATERIALS AND METHODS", "content": "In this work, a Zybo and a Zedboard boards were used to carry out the development. Each PE within the hardware design was implemented using one of the dedicated DSP48E1 found within all Xilinx 7-series FPGAs.\nA. Validation\nIn order to validate the operation of the developed hardware, a neural network based on the ResNet architecture [20] composed of 21 convolutional layers was trained to classify images from the CIFAR-10 dataset [16]. This dataset contains 60,000 images of 32\u00d732\u00d73 pixels, arranged in 10 mutually exclusive categories, which are separated into 50,000 images to train the network and 10,000 images to validate the performed training.\n1) Training: In each trained version (see table I and figure 3), the following techniques were used:\n\u2022 A variable learning factor was applied as the training progressed.\n\u2022 The class ReduceLROnPlateau of the Keras framework was used to also dynamically modify the learning factor.\n\u2022 Since the data set is limited to 50,000 images, data augmentation techniques were used to expand the data set during training. For this, the class ImageDataGenerator of the Keras framework was used."}, {"title": "V. DISCUSSION", "content": "On one hand, for the QKeras model (2), the activating of the DSB feature does not add a significant improvement to the mean inference time per image (this difference is as small as 0.79%). This was expected and not at all surprising, since this model was not trained with any pruning technique, and therefore the coefficients of its filters do not have any restriction applied to force them to zero and thus be able to take advantage of the DSB feature of the hardware.\nIt is also appreciated that the model trained with the uniform pruning technique (model 3) adds a slight improvement when compared to the previous model, if versions with and without the DSB feature are compared (the difference is around a 3% in the best case). However, this difference is still very small. For the model trained with the HAPM (model 4), a notable decrease in the mean inference time per image of the design is observed, which is around 45% in the best case, when feature DSB is activated. It is noteworthy that this model had been trained with a target pruning of only 50%, and yet obtained better results than model 3 in terms of inference speed, at the cost of a slight loss of accuracy. This can also be seen in figure 6, where the performance improvement across the different hardware implementations can be seen.\nFor model 4, the inference time per image was compared for a FIFO data buffer depth (for each computation unit) of 8 and 32 elements. If this buffer is too small, idle states appear in the internal state machines of the Controller module, thus increasing the mean inference time per image. As shown in the table, a design implemented with 32-element deep data buffers achieves an 8% improvement in the inference speed. But on the other hand, this depth increase also brings with it an increase in the FPGA resource usage (figure 7). Therefore, although the improvement is small in this particular case, if there are FPGA with more resources available, a considerable improvement can be obtained by increasing the size of this buffer even more.\nThere is also a difference between the theoretical performance of the design, and the actual one once implemented. For example, the theoretical performance of the Zedboard model with 72 DSPs should be around 11 GOPs according to figure 5, but in the best case, we achieved 7.468 GOPs. Part of this depends on the size of the FIFO buffers and the IDLE states, as described in the previous paragraph. But there are also scheduling issues, which should be addressed in following works. Because of the chosen scheduling policy, after the last channel of the input matrix of a convolutional layer is processed, the output data needs to be stored in its final memory position (see line 24 in algorithm 2). This data needs to be stored in a specific layout so that it can be used by the next layer without transformations. The problem is, that during this last storing of data, the output data of each computation units matrix needs to be stored in disjoint locations, and the entirety of the writing bus of the SRAM can not be used to pack together multiple writes. This generates back pressure on the output FIFOs and also delay the start of the processing of the next layer until all data is written to the SRAM."}, {"title": "VI. CONCLUSION", "content": "We developed a CNN hardware accelerator small enough to be implemented for resource-constrained FPGAs. We demonstrated that HAPM can be used to accelerate the inference of a CNN on a custom hardware accelerator by almost 2x."}, {"title": "VII. FUTURE WORK", "content": "This work opens the doors to a large set of future research branches, where extensions and improvements to this hardware accelerator will be analyzed. On the one hand, the use of the DSP slices in SIMD mode will be investigated, and its impact in the throughput and performance of the design will be analyzed. On the other hand, the energy consumption analysis of these implementations and its relationship with the HAPM and the DSB feature are also pending topics worth investigating. In addition, it is of great interest to analyze with more detail why a network trained with the standard pruning technique has so little improvement in the image processing time, and to review if the DSB characteristic can be improved for networks trained with this technique. On the other hand, in this version of the design, the coefficients of the neural network are stored and retrieved from the DRAM without any type of compression, which is not efficient when executing pruned networks. The analysis of dynamic compression and decompression techniques for these data and their integration into this architecture are also of interest. Additionally, the scheduling and memory layout issues described in Discussion should be addressed in order to leverage the difference between the theoretical performance and the real one. Finally, future work should demonstrate the flexibility of this design by running multiple different neural network architectures on more FPGAs."}]}