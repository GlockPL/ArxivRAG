{"title": "Urban Region Pre-training and Prompting: A Graph-based Approach", "authors": ["Jiahui Jin", "Yifan Song", "Dong Kan", "Haojia Zhu", "Xiangguo Sun", "Zhicheng Li", "Xigang Sun", "Jinghui Zhang"], "abstract": "Urban region representation is crucial for various urban downstream tasks. However, despite the proliferation of methods and their success, acquiring general urban region knowledge and adapting to different tasks remains challenging. Previous work often neglects the spatial structures and functional layouts between entities, limiting their ability to capture transferable knowledge across regions. Further, these methods struggle to adapt effectively to specific downstream tasks, as they do not adequately address the unique features and relationships required for different downstream tasks. In this paper, we propose a Graph-based Urban Region Pre-training and Prompting framework (GURPP) for region representation learning. Specifically, we first construct an urban region graph that integrates detailed spatial entity data for more effective urban region representation. Then, we develop a subgraph-centric urban region pre-training model to capture the heterogeneous and transferable patterns of interactions among entities. To further enhance the adaptability of these embeddings to different tasks, we design two graph-based prompting methods to incorporate explicit/hidden task knowledge. Extensive experiments on various urban region prediction tasks and different cities demonstrate the superior performance of our GURPP framework. The implementation is available at this repository: https://anonymous.4open.science/r/GURPP.", "sections": [{"title": "1\nIntroduction", "content": "An urban region is a geographical area comprising diverse spatial entities (e.g., shopping centers, restaurants, and roads) and their interactions (e.g., nearby and located in). Recently, urban region representation learning, which aims to extract vector embeddings from these entities, has become a significant research focus [60]. Effective urban region representations can be applied to various tasks, such as region popularity prediction [10, 24, 46, 65], house price prediction [45], crime prediction [45, 46, 59, 65], and population density inference [23], enabling urban planners, policymakers, and businesses to enhance decision-making processes. However, the complexity of urban environments and the diversity of applications pose significant challenges.\nBalancing urban generality and task specificity is crucial in urban region representation. (i) Urban regions often appear similar in their spatial structures and functional layouts, because the interactions among entities within these regions follow similar patterns. These shared patterns serve as generic knowledge across urban areas, facilitating the development of generalizable region representation models. As illustrated in Figure 1, Region A and Region B showcase typical urban regions. In these examples, spatial structures are demonstrated by the proximity of residences to parks and shops, which enhances community convenience and accessibility. Functional layouts are characterized by the clustering of similar entities within areas, such as shops in commercial areas and schools in teaching areas, and the close proximity of these distinct areas to each other. Therefore, an effective urban region representation model needs to capture the heterogeneous and generic patterns of interactions among entities within regions. (ii) Despite the broad similarities, each urban region and downstream task possesses unique details. Specific tasks may require attention to different features within urban data. For example, in traffic crash prediction, the density of roads and the number of residents living nearby, might be critical features, as they directly influence the probability of accidents occurring. On the other hand, crime prediction models might focus on the density and type of POIs within the region, such as bars or late-night convenience stores. As consequence, balance between generality and specificity raises a crucial question: Can we design an urban region representation framework that effectively integrates generalizable knowledge while accommodating the specific needs of distinct tasks?\nRecently, the pre-training and prompting paradigm that learns general representations that can be fine-tuned or prompted to adapt to specific tasks has shown remarkable success in the field of natural language processing [4, 11, 27, 39] and computer vision [38, 63, 64]. Inspired by these advancements, we hope to design a pre-training and prompting framework for urban region representation learning."}, {"title": "2 Preliminaries", "content": "Urban region. A city is divided into a set R of non-overlapping regions based on defined criteria, such as administrative boundaries [28, 34, 46, 59] and grid partitioning [8, 21, 26, 56]. Each region r\u2208 R consists of multiple spatial entities, including points of interest (POIs), roads, and junctions.\nUrban region representation learning. The urban region representation learning problem involves learning a mapping function \u03b6 : R \u2192 Rd, which generates a low-dimensional embeddingh \u2208 Rd for each region r\u2208 R. This mapping considers both the spatial structures and functional layouts of the spatial entities within r, with d being the size of the embedding.\nOnce the mapping function (\u03b6) has been learned, it can be applied to various downstream prediction tasks, such as crime and check-in predictions. Let the task dataset be {(ri, yi)}, where Yi \u2208 R is a numerical indicator for region ri. The downstream prediction tasks aim to predict yi based on the region embedding"}, {"title": "3\nOverview", "content": "As shown in Figure 3, we propose a graph-based framework to pre-train and prompt urban region representations. Our approach begins with the creation of an urban region graph that integrates specific urban entities, providing a comprehensive structure for representing spatial and functional relationships within urban areas (discussed in Section 4.1). From this urban region graph, we extract subgraphs corresponding to each region, using graph patterns that characterize the region's structure and interactions. These subgraph representations are then refined through multi-view learning using the GURP model, enabling the capture of generalizable information that can serve as the foundation for downstream tasks (explained in Section 4.2). To tailor these representations for specific tasks, we employ two prompt-tuning methods: a manually-designed prompt method and a task-learnable prompt learning method, each designed to incorporate task-specific knowledge into the pre-trained region embeddings (elaborated in Sections 5.1 and 5.2, respectively)."}, {"title": "4 Pre-train General Representation", "content": "In this section, we introduce the processes of constructing the urban region graph and pre-training subgraph-centric region embeddings using a multi-view learning approach."}, {"title": "4.1 Urban Region Graph Construction", "content": "An urban region graph is constructed to model the spatial structures and functional layouts of urban areas. The urban region graph, denoted as G = (V, E, Tv, Te), is a heterogeneous graph containing multiple types of nodes and relationships. Here, v \u2208 V and e \u2208 E represent a node and an edge, respectively, and \u03c6(v) \u2208 Tv (\u03c8(\u03b5) \u03b5 TE) is a type function that assign a type to a specific node (edge).\nThe urban region graph follows a schema that defines the relationships among various types of nodes and edges (see Figure 4). Without loss of generality, we consider three types of spatial entities: POIs, roads, and junctions. Each spatial entity is categorized: for instance, POI categories include shops and parks, road categories encompass highways and residential streets, and junction categories cover features like roundabouts.\nWe construct the urban region graph from POI datasets and online maps. Since the urban region graph contains heterogeneous nodes and edges, we adopt the knowledge graph embedding technique TransR [25] to initialize the representations of all nodes and edges. This aligns the heterogeneous information into a unified vector space."}, {"title": "4.2 Subgraph-centric Region Embedding", "content": "We introduce a subgraph-centric region embedding model to encode the spatial structures and functional layouts of a region r."}, {"title": "4.2.1 Extract region-induced subgraph.", "content": "The region-induced subgraph is a subgraph of G that includes the region node r and matches the graph pattern P = (PV, PE, \u03b8). Here, Py \u2286 Ty and PE TE are sets of node and edge types, respectively, which are used to filter entities and relationships specifically useful for region embeddings.\nThe set 0 Ty indicates the node types that serve as termina-tion points for subgraph extraction. As a result, the schema of the region-induced subgraph is a subset of the schema of the urban region graph. During the pre-training phase, we select all node and edge types and set Py = TV, PE = TE, i.e., P = (Tv, TE, \u03b8) with the aim of comprehensively characterising the rich information of a region and thereby learning general knowledge.\nThe region-induced subgraph gr = (Vr, Er, PV, PE) includes nodes V, and edges Er that can be reached from r and whose types match Py and PE. We extract gr recursively, starting with V, = {r} and Er = \u00d8 and repeatedly expanding V, and Er, until the termina-tion points are reached. Specifically, we use a breadth-first search (BFS) starting from r to expand the subgraph. During each iteration of BFS, we explore edges e(u, v) where one endpoint u is in V, and the other endpoint v is not yet in Vr. We then check whether the type of e is in PE and whether the type of the unseen node v is not in the termination type set 0. If e passes the checks, we add it to the set AEr. Using \u2206Er, we construct the node set AV,, which contains the endpoints in AEr that are not already in Vr. Formally, the sets of incremental edges (i.e., AEr) and nodes (i.e., AV,) are as follows: \u0394\u0395r = {e(u, v) | u \u2208 Vr\u2227 \u03c6(\u03c5) \u03b8\u043b\u0435\u0454\u0395\u039b\u03c8(e) \u2208 PE} and AV, = { v \u2209 V, \u2227 \u2203\u0438 \u0454 Vr,e(u, v) \u2208 \u0394\u0395,}. We then up-date the region-induced subgraph by setting V = V, U AV, and \u0395\u03b3 = \u0395\u03b3 \u039f \u0394\u0395\u03b3. The expansion terminates if AE, and AV, are empty."}, {"title": "4.2.2 Region subgraph encoding.", "content": "In Section 4.1, each node in the subgraph gr has been assigned a global initial embedding using TransR. Here, we encode each subgraph gr into a vector hy to fur-ther incorporate the local features of region r. Specifically, we use a heterogeneous graph encoder gEnc(.) to extract the heteroge-neous and generic patterns within gr, ultimately outputting the contextualized embedding of the subgraph.\nOur encoder gEnc() chooses the Heterogeneous Graph Trans-former (HGT) [17] as the subgraph node encoder due to HGT's capability of capturing node and edge-types semantics. By using HGT, gEnc() characterizes the heterogeneity of input graph by maintaining a set of node and edge-type dependent parameters, and the output embedding of each node HGT that integrates the global initial embeddings generated by TransR, and node and edge-type semantics of the subgraph gr. This trait aligns with our need to capture the semantic features of heterogeneous entities and rela-tionships within each region-induced subgraph.\nAfter obtaining the representations of each node in the subgraph, we further obtain the graph-level representation of gr through two steps of aggregation. For each subgraph gr, we first use SUM(.) function to aggregate the representations of nodes of the same type to integrate the impact of the number of entities on the representa-tion, obtaining the node-type embedding ti:\nti = \u2211 HGT \u03c4\u03b9 \u2208 Tv, Vo e Vr.\n\u03c6(\u03c5)=\u03c4\u03af\nThen we concatenate all representations of node types, and encode them with a Linear layer to capture the interaction between various types of nodes:\nhr = Linear (CONCAT|Tv| (ti))."}, {"title": "4.3 Multi-view Representation Learning", "content": "We learn the region representation with multiple views to characterize general region features by using the spatial proximity, region satellite imagery and region-wise traffic information."}, {"title": "4.3.1 Spatial proximity.", "content": "The First Law of Geography [43] emphasizes the concept of spatial proximity, stating that \u201ceverything is related to everything else, but near things are more related than distant things\", which means two regions with spatial proximity should exhibit greater similarity in their representation.\nWe utilize a triplet network [16] to capture region similarity based on spatial proximity. For a given subgraph gr of region r, which serves as the anchor sample, we construct its positive and negative samples based on spatial proximity. If region r' is adjacent to r (i.e., there is a \"NearBy\" type of edge connecting the corre-sponding nodes), we take the subgraph of r' as the positive sample, denoted by gt. Otherwise, we take the subgraph as the negative sample, denoted by gr . We input gr, g+, and g, to the graph encoder gEnc(\u00b7), obtaining the vector representations hr, ht, and he Rd, respectively. Then we use triplet loss to minimize the distance be-tween adjacent regions and maximize it between distant regions, with the loss function calculated:\nLsp = \u2211 max{||hr - ||2 - ||--||2 + 8,0}.\nreR\nwhere & represents the margin parameter of triplet loss."}, {"title": "4.3.2 Region imagery.", "content": "Urban imagery encompasses various types of image information that describe the visual characteristics of a city, such as satellite images, street view images, and more. In this paper, we use satellite images as the source of urban imagery. For a given region r, its imagery can be represented as the set {a1, a2,\u2026, am}. We employ a pre-trained visual model to extract visual features of the region himg and fuse these features with the region's representation with the contrastive loss Limg (see Appendix A.2 for more details)."}, {"title": "4.3.3 Human mobility.", "content": "Human mobility describes the dynamics of urban systems and is typically represented by trajectory data. For a given region r, the outflow feature sr and inflow feature tr are defined by calculating the number of outgoing and incoming trips within each time period. This results in the outflow and inflow feature sets {S1, S2,..., SN} and {t1, t2,..., t}, respectively, where si, t\u00a1 \u2208 R\u00b9, and I is the number of intervals (e.g., 24). We then encode the inflow and outflow features using an encoder (such as a Multi-Layer Perceptron) to obtain d-dimensional embeddings:\nhsrc = Encoder(sr) hdst = Encoder(tr).\nHere we adopt the reconstruction loss Lflow to optimize the two embeddings (see Appendix A.3 for more details)."}, {"title": "4.3.4 Region representation learning.", "content": "We aim to jointly learn region representations from spatial proximity, imagery, and flow views to extract general urban region knowledge and embed this into a unified representation space. To achieve this, we propose a view-fusion task that encodes the region embedding hr, the imagery embedding him, and the flow embeddings here and hast into a combined vector hy. We then decode h, using separate decoders to reconstruct the original embeddings.\nTo implement this idea, we employ a fusion layer to integrate the embeddings: h\u2081 = ReLU(W[h,,himg, here, hast] + b), where [.] denotes the concatenation operation, and W and b are trainable weights. We design a multi-view prediction task to train the fusion layer with the loss function Lfuse, defined by the ability to reconstruct the representation of each individual embedding hr, him, here, hast from the fused representation \u0125r:\n4\n\u00a3fuse = \u03a3\u03a3 - Decoder (hr)||.\nreRk=1\nHere, (k = 1, 2, 3, 4) represents one of the four embeddings, and Decoderk is the corresponding decoder (such as a Multi-Layer Perceptron).\nThe final loss function can be formulated as follows:\nL = Lsp + Limg + Lflow + \u00b5L fuse.\nwhere u is a parameter that balances the weight of the fusion loss."}, {"title": "5 Prompt Task-specific Knowledge", "content": "After obtaining a general region representation, a critical issue lies in how to integrate task-specific knowledge into pre-trained representations. To achieve this, we propose two graph-based prompt tuning methods: a manually-designed prompt method based on explicit domain knowledge and a task-learnable prompt learning method to extract implicit task-specific knowledge."}, {"title": "5.1 Manually-designed Prompt", "content": "In the manually-designed prompt, we adjust the subgraph structure, i.e., defining the graph pattern P(PV, PE, \u03b8) in Section 4.2, to emphasize specific relationships and entities relevant to the target task, as shown in Figure 5.\nWe further enhance this process by manually adjusting the proportion of different types of edges and entities in the subgraph, thereby influencing the resultant embeddings. To achieve this, we first define a set of task-specific entity weights Wtask that reflect the importance of various types of entities for the downstream task. These weights are designed based on domain knowledge and the specific requirements of the task. Formally, we define Wtask as:\nWtask = {Wti | \u03c4\u03af \u2208 Pv}.\nwhere Py is the set of all possible node types in the region-induced subgraph, and wr\u2081 \u2208 [0,1] represents the importance of entity type ti. The weights w\u2081\u2081 are manually defined based on domain knowledge and are used as the deletion proportion for each en-tity type. We define a function Adjust() to delete nodes based on Wtask: Adjust(gr, Wtask)\u2192g where gr = (Vr, Er) is the origi-nal subgraph. The function Adjust obtains the adjusted subgraph g = (V, E) as follows:\n(1) Node Deletion: For each entity type \u03c4\u2081 \u20ac &, delete WT\u2081 Vi nodes of type ti from gr, where | Vr\u2081 | is the number of nodes of type Ti in gr, such that V = V, \\ {v | \u03c6(v) = \u03c4\u2081 with probability 1 \u2013 wri}.\n(2) Edge Deletion: After node deletion, delete edges connected to the removed nodes, such that E = {e | e = (u, v) \u2208 \u0395; u, v \u2208 V'}.\nThe adjusted subgraph g' = (V', E') is then input to the pre-trained GURP to generate the task-specific embeddings.\nIn summary, the manually-designed prompt is the graph pat-tern P with the set of task-specific entity weights Wtask, which guide the Adjust() function in modifying the subgraph gr to gr. By incorporating these manually-designed prompts, we can tailor the urban region representations to better capture task-relevant entities and relationships. This approach also offers the advantage of adapting to downstream tasks without the need for additional training. It simply requires adjusting the input subgraph based on the manually-defined prompts."}, {"title": "5.2 Task-learnable Prompt", "content": "Apart from leveraging explicitly designed task-specific knowledge prompts to enrich region representations, in the presence of suffi-cient task data, we can also learn prompts based on the task data and pre-trained representations to induce implicit knowledge within the task data, thereby enhancing region representations.\nConsidering that we describe the attributes of an urban region through subgraph induced with region nodes from G, it is intu-itive to similarly describe the task adaptation to different region attributes with graphs as well. Therefore, we propose a prompt learning module to use prompt graphs to portray the adaptation"}, {"title": "6 Experiments", "content": "We conduct extensive experiments to investigate the following research questions (RQs):\n\u2022 RQ1: Can our proposed pre-train and prompt framework (GURPP) outperform previous approaches?\n\u2022 RQ2: Does GURP have the ability to capture and apply het-erogeneous and generic patterns?\n\u2022 RQ3: How does the manually-designed prompt work?\n\u2022 RQ4: How does the task-learnable prompt perform and how does it look like?\n\u2022 RQ5: How does each component contribute to GURPP?"}, {"title": "6.1 Experimental Setup", "content": "6.1.1 Datasets. We collect the datasets of region division, POI, urban imagery, taxi trips, road network, check-in, crime and crash for two representative cities in the United States: New York City (NYC) and Chicago (CHI). The detailed statistics and sources of the dataset is available in Appendix A.5.\n6.1.2 Baselines. We compare our model with the following repre-sentative baseline models: knowledge graph embedding methods and its variants (TransR [25], TransR-N, TransR-G), graph em-bedding methods (node2vec [12], GAE [22]), and urban region embedding methods (MVURE [59], MGFN [46], HREP [65], HRE, ReCP [24]). These baselines cover the range from traditional graph embedding to complex models that take into account urban-specific attributes, providing us with a comprehensive performance com-parison benchmark. Specific details and descriptions can be found in the Appendix A.6.\n6.1.3 Metrics and Implementation. To evaluate the prediction per-formance, we use three widely recognized evaluation metrics: the coefficient of determination (R2), root mean squared error (RMSE), and mean absolute error (MAE). Better performance is indicated by a higher R2 and lower RMSE and MAE values. Our model includes three version: 1) GURP represents the pre-train module, 2) GURPP M is GURP with manually-designed prompt, and 3) GURPPT is GURP with task-learnable prompt. For detailed implementation, please refer to Appendix A.7 and our provided code repository."}, {"title": "6.2 RQ1 & RQ2: Overall Performance", "content": "We evaluate the overall performance of baselines and our model on three downstream tasks. The results of the experiments are shown in Table 1, where the GURPP is all implemented by GURPPT. From the results, we find the following:\n(1) GURPP demonstrates superior performance to baselines in both pre-training and prompting phases. In the pre-training phase, the performance of GURP improves by 6.54%, 8.28%, 462.03%, and 113.01% over the best baseline of R2 on two urban datasets. Mean-while, on all downstream tasks, the average performance on MAE and RMSE metrics improves by 17.28% and 22.26%, respectively. Subsequently, the performance of GURPP is further improved by an average of 9.50%, 9.28%, and 11.703% of MAE, RMSE and R2, respectively, when we introduce the task-learnable prompt. The results validate the significant advantages of our proposed pre-train and prompt framework.\n(2) To validate the ability of our pre-training method to cap-ture generic urban knowledge, we compare the performance of state-of-the-art urban region embedding methods on datasets from two distinct cities, NYC and CHI. The results show that while these baselines perform well on the NYC dataset, their performance drops significantly on the CHI dataset. For instance, the performance on CHI decreases by an average of 71.8% in R\u00b2 compared to NYC. In contrast, our GURPP model demonstrates remarkable consistency, with only an 8.49% decrease in R2. This may be due to the fact that the baselines tend to be effective only at extracting information spe-cific to certain data types, but they do not consider spatial structure and functional layouts among urban entities. For example, existing baselines rely on the flow view to extract information. However, the content of flow view in the CHI dataset is sparse, leading to a significant degradation in the performance of downstream task."}, {"title": "6.3 RQ3: Manually-designed Prompt Study", "content": "In the real world, the incidence of crime is often closely linked to specific POI categories, roads and junctions, for example, ATMs, bars, parks, etc. are more likely to be the location of a crime. The density of roads and the complexity of junctions are also important factors affecting crime rates. As for the check-in behavior, the brand of the POI is given more attention, and certain users may be more inclined to check in at locations of well-known brands such as Starbucks, McDonald's, or other specific brands. Therefore, we designed two patterns as shown in Figure 7 to validate the performance of manually-designed prompt.\nThe results are shown in Table 2, from which it can be found that crime prediction does pay more attention to factors such as POI"}, {"title": "6.4 RQ4: Task-learnable Prompt Study", "content": "6.4.1 Effectiveness of Task-learnable Prompt. We utilize GURP pre-trained embedding with the simple prefix prompt (proposed by HREP[65]) and task-learnable prompt respectively for comparative"}, {"title": "6.5 RQ5: Ablation Studies", "content": "Finally, we conduct the ablation studies in NYC dataset to inves-tigate the effectiveness of the individual components of GURPP. Variants of GURPP are designed as follows: GURPP/P. We use only the pre-training process to learn urban region general repre-sentation.GURPP/F, GURPP/S, GURPP/I. We remove the flow view, spatial proximity view, and imagery view from GURPP, re-spectively. GURPP/K. We replace the knowledge graph embedding initialization with random initialization.\nFigure 9 presents the ablation results of our model and its vari-ants. Spatial proximity view and knowledge graph embedding method significantly enhance the model's performance, lifting it by 151.64% and 156.67% of R2, respectively. These components effectively capture the heterogeneous and generic patterns of in-teractions among entities by elucidating the spatial relationships and structured knowledge of the urban region. The flow view and"}, {"title": "7 Related Work", "content": "Extensive studies have been conducted to learn urban region rep-resentation from different perspectives. We combine the existing work categories in Table 4.\nTask-specific learning or fine-tuning. Urban region representa-tion learning aims to extract complex patterns and relationships within urban regions and embed them into a representation space. Existing research can be broadly classified into two categories: task-specific supervised region representation learning [2, 13, 50, 66] and general unsupervised region representation learning [19, 23, 24, 46, 51, 58, 59]. These works utilize feature aggregation mod-els to generate representation from predefined region statistics with limitations in capturing detailed spatial layout and semantics. Moreover, using predefined statistical features makes it challenging to further adapt the representation to the specificities of various downstream tasks. We address the limitations by leveraging a urban region graph and subgraph-centric region embeddings, which learn the generic knowledge of urban spatial layout and generate flexible region representation tailored to different tasks.\nPre-training and Prompting. Recent years have witnessed the significant success of pre-training and prompting paradigm in fields of NLP [4, 11, 27, 39], CV [38, 52, 63, 64] and graph learn-ing [31, 41, 53, 54]. Many works in urban computing have also made attempts in this regard [3, 47, 48, 56, 60]. For instance, Yuan et al. [56] design an urban spatio-temporal prediction pre-train"}, {"title": "8\nConclusion and Future Works", "content": "In this paper, we introduce the urban region graph and propose a graph-based pre-training and prompting framework for urban region representation learning, balancing urban generality and task specificity. We develop a subgraph-centric pre-training model to capture general urban knowledge and two prompt methods to incor-porate task-specific insights. Extensive experiments demonstrate the effectiveness of our approach. Future work may explore: 1) Integrating textual data and LLMs to uncover more universal ur-ban knowledge; 2) Developing arbitrary region representations for flexible urban modeling; 3) Assessing the transferability of urban representations across cities to enhance general applicability."}, {"title": "A Appendix", "content": "A.1 Appendix of urban region graph construction\nHere we provide more details for the urban region graph construc-tion. Specifically, we incorporate brand information for POI entities, such as Starbucks, to enhance the representation of POI functional-ities. Altogether, we define eight node types shared across regions and cities: region, POI, POI category, brand, road, road category, junc-tion, and junction category, comprising the node type set Ty. We also define six edge types, forming the edge type set TE. These in-clude the NearBy relationship, which connects region entities based on their geographical adjacency; the Contains relationship, which is based on spatial containment; the BrandOf relationship, which links brands to POI entities; and the CateOf, JCateOf, and RCateOf relationships, which are based on the category information of the entities.\nA.2 Appendix of region imagery encoding\nWe supplement more details for the urban imagery encoding. Specif-ically, we employ ResNet [14] to extract visual features for each region image, such that ak = ResNet(ak) and derive the comprehen-sive visual representation for the region by averaging the features of multiple images within the region. Then, a linear projection layer is utilized to project the visual feature into the representation space of region attribute, such that\nhimg = Linear(-\u03a3 ak).\n1-m\nm\nk=1\nThe contrastive loss Limg is then used to align the visual repre-sentation with the subgraph-centric region representation, ensuring a cohesive integration of visual and attribute information.\nLimg = -log exp + log exp\nreR [hThImg /\u03c4]\n B-1\nhThj\n/ exp  [h ThImg/\u03c4]\n  j=1 /\u03c4\nHere, B represents batch size and t is the temperature parameter.\nA.3 Appendix of human mobility encoding\nHere we give a detailed description for human mobility encoding. To learn the embeddings, we reconstruct the departure and arrival flow distribution. Let mij represent the number of trips from region ri to region rj. The source and destination urban flow distributions mij mij\nare computed as Psrc (rj|ri) = and Pdst (ri|rj) = respectively. We then reconstruct the source and destination distri-butions psrc(rj|ri) and pdst (ri|rj) using the encoded inflow and outflow features:\npsrc (rj|ri) =\npdst (ri/rj) =\nexp((hsrc)Thdst)\nEk exp((hsrc)Thdst)\nk\nexp((hdst)There)\nEk exp((hdst) There)\nk"}, {"title": "A.4 Appendix of attribute graph kernel function", "content": "We introduce how attribute graph kernel function Kp (\u00b7, \u00b7) works. We first define the graph kernel function K(, ). The graph kernel function is a P-step random walk kernel [33] to measure the sim-ilarity between graphs, which counts the number of common walks in both graphs within P steps with its calculation as follows. Given G1 = (V1, E1), G2 = (V2, E2), where V and E are the set of nodes and edges, respectively, we initially define the direct product graph Gx following the framework used in existing research [7, 33]:\nGx = G1 X G2 = (Vx, Ex)\n(1)\nwhere Vx = {(01, 02) : 01 \u2208 V1, 02 \u20ac V2}, Ex = {{(v1, v2), (U1, U2)} : {01, 41} \u20ac \u0395\u2081 \u039b {v2, 42} \u2208 E2}. Following the theorem [44] that a single random walk on the direct product graph Gx is equivalent to a random walk on the two graphs G1, G2 at the same time, the P-step random walk kernel is computed in the following way:\nP\nP Vx\nK(G1, G2) = Kp (G1, G2) = \u03a3 \u03a3 [Ax]i,j\np=0\np=0 i,j=1\n(2)\nwhere Ax is the adjacency matrix of the direct product graph, Ap is the sequence of weights and the (i, j) item of A represents the P-length common walks between the i-th and j-th nodes.\nExtending the above definition to the scenario of graphs with node attributes, we use X \u2208 Rn\u00d7d to denote the attribute matrix of a graph with n nodes and attribute dimension d. Specifically, the node attribute matrices of G1, G2 are denoted by X1 \u2208 Rn1\u00d7d, X2 \u20ac Rn2xd. Thus, for the graph Gx = G1 \u00d7 G2, the node attribute ma-trix is S = X1X, S \u2208 Rn1\u00d7n2, where the item (i, j) encodes the similarity between the i-th node in G\u2081 and the j-th node in G2. We flattened S to s\u2208 RN1n2 to obtain the formula for the random walk kernel:\n[Vx\nKp (G1, G2) = Sisj [A]ij = s\u00afAs.\ni,j=1\n(3)\nA.5 Appendix of dataset statistics and sources\nWe collect the datasets of region division, POI, urban imagery, taxi trips, road network, check-in, crime and crash for two representa-tive cities in the United States: New York City (NYC) and Chicago (CHI). The region division is based on census tracts and the urban imagery are 256 x 256 pixels with a resolution of 4.7 meters."}, {"title": "A.6 Appendix of baselines description", "content": "We compare our model with representative baseline models, in-cluding graph embedding methods, knowledge graph embedding methods and state-of-the-art region embedding methods.\nI. Knowledge graph embedding methods:\n\u2022 TransR [25] captures the complex interactions between en-tities and relationships in the knowledge graph by projecting them into different spaces and learning their representation in these spaces.\n\u2022 TransR-N method uses the embedding of the region node learned by TransR as the region representation.\n\u2022 TransR-G method is to average the embedding of the region subgraph learned by TransR as the region representation.\nII. Graph embedding methods:\n\u2022 node2vec [12] generates a sequence of nodes by performing random walks on the graph to capture the neighborhood information of nodes, and then learns the representation of nodes using the Skip-Gram model.\n\u2022 GAE [22] consists of an encoder and a decoder. The encoder is used to learn the representation of nodes, and the decoder tries to reconstruct the adjacency matrix from these node representation.\nIII. State-of-the-art urban region embedding methods:\n\u2022 MVURE [59] constructs intra- and inter-region data multi-view based on human mobility and region attributes, and then performs multi-view fusion to learn region representa-tion.\n\u2022 MGFN [46] builds human mobility patterns and then learns intra- and inter-pattern relationships to obtain region repre-sentation through a multi-level cross-attention mechanism.\n\u2022 HREP [65] designs a multi-relational heterogeneous graph of regions and used a relation-aware GCN to learn region representation, as well as a prefix prompt mechanism for downstream tasks.\n\u2022 HRE is a part of HREP without prompt learning.\n\u2022 ReCP [24] is a multi-view comparative prediction model that learns region representation by capturing the unique in-formation of a single view as well as the consistency between two views."}, {"title": "A.7 Appendix of Implementation", "content": "In our experiments, all the region embedding dimensions is set to 144. The number of HGT layer and head is 2 and 4, and the triplet margin parameter 8 = 2, fusion loss weight \u00b5 = 0.01. We set the learning rate to 0.001 and the weight"}]}