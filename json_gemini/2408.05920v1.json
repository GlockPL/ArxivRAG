{"title": "Urban Region Pre-training and Prompting: A Graph-based Approach", "authors": ["Jiahui Jin", "Yifan Song", "Dong Kan", "Haojia Zhu", "Xiangguo Sun", "Zhicheng Li", "Xigang Sun", "Jinghui Zhang"], "abstract": "Urban region representation is crucial for various urban downstream tasks. However, despite the proliferation of methods and their success, acquiring general urban region knowledge and adapting to different tasks remains challenging. Previous work often neglects the spatial structures and functional layouts between entities, limiting their ability to capture transferable knowledge across regions. Further, these methods struggle to adapt effectively to specific downstream tasks, as they do not adequately address the unique features and relationships required for different downstream tasks. In this paper, we propose a Graph-based Urban Region Pre-training and Prompting framework (GURPP) for region representation learning. Specifically, we first construct an urban region graph that integrates detailed spatial entity data for more effective urban region representation. Then, we develop a subgraph-centric urban region pre-training model to capture the heterogeneous and transferable patterns of interactions among entities. To further enhance the adaptability of these embeddings to different tasks, we design two graph-based prompting methods to incorporate explicit/hidden task knowledge. Extensive experiments on various urban region prediction tasks and different cities demonstrate the superior performance of our GURPP framework. The implementation is available at this repository: https://anonymous.4open.science/r/GURPP.", "sections": [{"title": "1 Introduction", "content": "An urban region is a geographical area comprising diverse spatial entities (e.g., shopping centers, restaurants, and roads) and their interactions (e.g., nearby and located in). Recently, urban region representation learning, which aims to extract vector embeddings from these entities, has become a significant research focus [60]. Effective urban region representations can be applied to various tasks, such as region popularity prediction [10, 24, 46, 65], house price prediction [45], crime prediction [45, 46, 59, 65], and population density inference [23], enabling urban planners, policymakers, and businesses to enhance decision-making processes. However, the complexity of urban environments and the diversity of applications pose significant challenges.\nBalancing urban generality and task specificity is crucial in urban region representation. (i) Urban regions often appear similar in their spatial structures and functional layouts, because the interactions among entities within these regions follow similar patterns. These shared patterns serve as generic knowledge across urban areas, facilitating the development of generalizable region representation models. As illustrated in Figure 1, Region A and Region B showcase typical urban regions. In these examples, spatial structures are demonstrated by the proximity of residences to parks and shops, which enhances community convenience and accessibility. Functional layouts are characterized by the clustering of similar entities within areas, such as shops in commercial areas and schools in teaching areas, and the close proximity of these distinct areas to each other. Therefore, an effective urban region representation model needs to capture the heterogeneous and generic patterns of interactions among entities within regions. (ii) Despite the broad similarities, each urban region and downstream task possesses unique details. Specific tasks may require attention to different features within urban data. For example, in traffic crash prediction, the density of roads and the number of residents liv-ing nearby, might be critical features, as they directly influence the probability of accidents occurring. On the other hand, crime prediction models might focus on the density and type of POIs within the region, such as bars or late-night convenience stores. As consequence, balance between generality and specificity raises a crucial question: Can we design an urban region representation framework that effectively integrates generalizable knowledge while accommodating the specific needs of distinct tasks?\nRecently, the pre-training and prompting paradigm that learns general representations that can be fine-tuned or prompted to adapt to specific tasks has shown remarkable success in the field of natural language processing [4, 11, 27, 39] and computer vision [38, 63, 64]. Inspired by these advancements, we hope to design a pre-training and prompting framework for urban region representation learning."}, {"title": "2 Preliminaries", "content": "Urban region. A city is divided into a set $R$ of non-overlapping regions based on defined criteria, such as administrative boundaries [28, 34, 46, 59] and grid partitioning [8, 21, 26, 56]. Each region $r \\in R$ consists of multiple spatial entities, including points of interest (POIs), roads, and junctions.\nUrban region representation learning. The urban region representation learning problem involves learning a mapping function $\\zeta : R \\rightarrow R^d$, which generates a low-dimensional embedding $h \\in R^d$ for each region $r \\in R$. This mapping considers both the spatial structures and functional layouts of the spatial entities within $r$, with $d$ being the size of the embedding.\nOnce the mapping function $\\zeta(\\cdot)$ has been learned, it can be applied to various downstream prediction tasks, such as crime and check-in predictions. Let the task dataset be $\\{(r_i, y_i)\\}$, where $y_i \\in \\mathbb{R}$ is a numerical indicator for region $r_i$. The downstream prediction tasks aim to predict $y_i$ based on the region embedding $\\zeta(r_i)$ using a simple regression model like a Ridge Regression [15]."}, {"title": "3 Overview", "content": "As shown in Figure 3, we propose a graph-based framework to pre-train and prompt urban region representations. Our approach begins with the creation of an urban region graph that integrates specific urban entities, providing a comprehensive structure for representing spatial and functional relationships within urban areas (discussed in Section 4.1). From this urban region graph, we extract subgraphs corresponding to each region, using graph patterns that characterize the region's structure and interactions. These subgraph representations are then refined through multi-view learning using the GURP model, enabling the capture of generalizable information that can serve as the foundation for downstream tasks (explained in Section 4.2). To tailor these representations for specific tasks, we employ two prompt-tuning methods: a manually-designed prompt method and a task-learnable prompt learning method, each designed to incorporate task-specific knowledge into the pre-trained region embeddings (elaborated in Sections 5.1 and 5.2, respectively)."}, {"title": "4 Pre-train General Representation", "content": "In this section, we introduce the processes of constructing the urban region graph and pre-training subgraph-centric region embeddings using a multi-view learning approach."}, {"title": "4.1 Urban Region Graph Construction", "content": "An urban region graph is constructed to model the spatial structures and functional layouts of urban areas. The urban region graph, denoted as $G = (V, E, T_v, T_e)$, is a heterogeneous graph containing multiple types of nodes and relationships. Here, $v \\in V$ and $e \\in E$ represent a node and an edge, respectively, and $\\varphi(v) \\in T_v$ ($\\psi(\\varepsilon) \\varepsilon T_E$) is a type function that assign a type to a specific node (edge).\nThe urban region graph follows a schema that defines the relationships among various types of nodes and edges (see Figure 4). Without loss of generality, we consider three types of spatial entities: POIs, roads, and junctions. Each spatial entity is categorized: for instance, POI categories include shops and parks, road categories encompass highways and residential streets, and junction categories cover features like roundabouts.\nWe construct the urban region graph from POI datasets and online maps. Since the urban region graph contains heterogeneous nodes and edges, we adopt the knowledge graph embedding technique TransR [25] to initialize the representations of all nodes and edges. This aligns the heterogeneous information into a unified vector space."}, {"title": "4.2 Subgraph-centric Region Embedding", "content": "We introduce a subgraph-centric region embedding model to encode the spatial structures and functional layouts of a region $r$.\n4.2.1 Extract region-induced subgraph. The region-induced subgraph is a subgraph of $G$ that includes the region node $r$ and matches the graph pattern $P = (P_V, P_E, \\theta)$. Here, $P_V \\subseteq T_V$ and $P_E T_E$ are sets of node and edge types, respectively, which are used to filter entities and relationships specifically useful for region embeddings."}, {"title": "4.2.2 Region subgraph encoding", "content": "In Section 4.1, each node in the subgraph $g_r$ has been assigned a global initial embedding using TransR. Here, we encode each subgraph $g_r$ into a vector $h_r$ to further incorporate the local features of region $r$. Specifically, we use a heterogeneous graph encoder $g_{Enc}(\\cdot)$ to extract the heterogeneous and generic patterns within $g_r$, ultimately outputting the contextualized embedding of the subgraph.\nOur encoder $g_{Enc}(\\cdot)$ chooses the Heterogeneous Graph Transformer (HGT) [17] as the subgraph node encoder due to HGT's capability of capturing node and edge-types semantics. By using HGT, $g_{Enc}(\\cdot)$ characterizes the heterogeneity of input graph by maintaining a set of node and edge-type dependent parameters, and the output embedding of each node $HGT$ that integrates the global initial embeddings generated by TransR, and node and edge-type semantics of the subgraph $g_r$. This trait aligns with our need to capture the semantic features of heterogeneous entities and relationships within each region-induced subgraph.\nAfter obtaining the representations of each node in the subgraph, we further obtain the graph-level representation of $g_r$ through two steps of aggregation. For each subgraph $g_r$, we first use $SUM(\\cdot)$ function to aggregate the representations of nodes of the same type to integrate the impact of the number of entities on the representation, obtaining the node-type embedding $t_i$:\n$t_i = \\sum_{v \\in V_r \\\\ \\varphi(v)=t_i} HGT \\quad \\tau_i \\in T_v, \\forall v \\in V_r$.\nThen we concatenate all representations of node types, and encode them with a Linear layer to capture the interaction between various types of nodes:\n$h_r = Linear (CONCAT_{|T_v|} (t_i))$."}, {"title": "4.3 Multi-view Representation Learning", "content": "We learn the region representation with multiple views to characterize general region features by using the spatial proximity, region satellite imagery and region-wise traffic information.\n4.3.1 Spatial proximity. The First Law of Geography [43] emphasizes the concept of spatial proximity, stating that \u201ceverything is related to everything else, but near things are more related than distant things\", which means two regions with spatial proximity should exhibit greater similarity in their representation.\nWe utilize a triplet network [16] to capture region similarity based on spatial proximity. For a given subgraph $g_r$ of region $r$, which serves as the anchor sample, we construct its positive and negative samples based on spatial proximity. If region $r'$ is adjacent to $r$ (i.e., there is a \"NearBy\" type of edge connecting the corresponding nodes), we take the subgraph of $r'$ as the positive sample, denoted by $g_{r^+}$. Otherwise, we take the subgraph as the negative sample, denoted by $g_{r^-}$. We input $g_r$, $g_{r^+}$, and $g_{r^-}$ to the graph encoder $g_{Enc}(\\cdot)$, obtaining the vector representations $h_r$, $h_{r^+}$ and $h_{r^-} \\in R^d$, respectively. Then we use triplet loss to minimize the distance between adjacent regions and maximize it between distant regions, with the loss function calculated:\n$\\mathcal{L}_{sp} = \\sum_{r \\in R} max\\{||h_r - h_{r^+}||_2 - ||h_r - h_{r^-}||_2 + \\delta, 0\\}.$\nwhere $\\delta$ represents the margin parameter of triplet loss.\n4.3.2 Region imagery. Urban imagery encompasses various types of image information that describe the visual characteristics of a city, such as satellite images, street view images, and more. In this paper, we use satellite images as the source of urban imagery. For a given region $r$, its imagery can be represented as the set $\\{a_1, a_2,\\ldots, a_m\\}$. We employ a pre-trained visual model to extract visual features of the region $h_{img}$ and fuse these features with the region's representation with the contrastive loss $\\mathcal{L}_{img}$ (see Appendix A.2 for more details)."}, {"title": "4.3.3 Human mobility", "content": "Human mobility describes the dynamics of urban systems and is typically represented by trajectory data. For a given region $r$, the outflow feature $s_r$ and inflow feature $t_r$ are defined by calculating the number of outgoing and incoming trips within each time period. This results in the outflow and inflow feature sets $\\{s_1, s_2,..., s_N\\}$ and $\\{t_1, t_2,..., t_N\\}$, respectively, where $s_i, t_i \\in \\mathbb{R}^I$, and $I$ is the number of intervals (e.g., 24). We then encode the inflow and outflow features using an encoder (such as a Multi-Layer Perceptron) to obtain $d$-dimensional embeddings:\n$h_{src} = Encoder(s_r) \\quad h_{dst} = Encoder(t_r)$.\nHere we adopt the reconstruction loss $\\mathcal{L}_{flow}$ to optimize the two embeddings (see Appendix A.3 for more details)."}, {"title": "4.3.4 Region representation learning", "content": "We aim to jointly learn region representations from spatial proximity, imagery, and flow views to extract general urban region knowledge and embed this into a unified representation space. To achieve this, we propose a view-fusion task that encodes the region embedding $h_r$, the imagery embedding $h_{img}$, and the flow embeddings $h_{src}$ and $h_{dst}$ into a combined vector $h_\\tau$. We then decode $h_\\tau$ using separate decoders to reconstruct the original embeddings.\nTo implement this idea, we employ a fusion layer to integrate the embeddings: $h_\\tau = ReLU(W[h_r, h_{img}, h_{src}, h_{dst}] + b)$, where $[.]$ denotes the concatenation operation, and $W$ and $b$ are trainable weights. We design a multi-view prediction task to train the fusion layer with the loss function $\\mathcal{L}_{fuse}$, defined by the ability to reconstruct the representation of each individual embedding $h_r$, $h_{img}$, $h_{src}$, $h_{dst}$ from the fused representation $\\hat{h}_r$:\n$\\mathcal{L}_{fuse} = \\sum_{r \\in R} \\sum_{k=1}^{4} || h_k - Decoder_k(\\hat{h}_r) ||_2^2$.\nHere, $h_k$ ($k=1,2,3,4$) represents one of the four embeddings, and $Decoder_k$ is the corresponding decoder (such as a Multi-Layer Perceptron).\nThe final loss function can be formulated as follows:\n$\\mathcal{L} = \\mathcal{L}_{sp} + \\mathcal{L}_{img} + \\mathcal{L}_{flow} + \\mu \\mathcal{L}_{fuse}.$\nwhere $\\mu$ is a parameter that balances the weight of the fusion loss."}, {"title": "5 Prompt Task-specific Knowledge", "content": "After obtaining a general region representation, a critical issue lies in how to integrate task-specific knowledge into pre-trained representations. To achieve this, we propose two graph-based prompt tuning methods: a manually-designed prompt method based on explicit domain knowledge and a task-learnable prompt learning method to extract implicit task-specific knowledge."}, {"title": "5.1 Manually-designed Prompt", "content": "In the manually-designed prompt, we adjust the subgraph structure, i.e.,, defining the graph pattern $P(P_V, P_E, \\theta)$ in Section 4.2, to emphasize specific relationships and entities relevant to the target task, as shown in Figure 5.\nWe further enhance this process by manually adjusting the proportion of different types of edges and entities in the subgraph, thereby influencing the resultant embeddings. To achieve this, we first define a set of task-specific entity weights $W_{task}$ that reflect the importance of various types of entities for the downstream task. These weights are designed based on domain knowledge and the specific requirements of the task. Formally, we define $W_{task}$ as:\n$W_{task} = \\{w_{\\tau_i} | \\tau_i \\in P_v\\}.$\nwhere $P_v$ is the set of all possible node types in the region-induced subgraph, and $w_{\\tau_i} \\in [0,1]$ represents the importance of entity type $\\tau_i$. The weights $w_{\\tau_i}$ are manually defined based on domain knowledge and are used as the deletion proportion for each entity type. We define a function $Adjust(\\cdot)$ to delete nodes based on $W_{task}$: $Adjust(g_r, W_{task}) \\rightarrow g'$, where $g_r = (V_r, E_r)$ is the original subgraph. The function Adjust obtains the adjusted subgraph $g' = (V', E')$ as follows:\n(1) Node Deletion: For each entity type $\\tau_i \\in &$, delete $w_{\\tau_i} \\vert V_{\\tau_i} \\vert$ nodes of type $\\tau_i$ from $g_r$, where $| V_{\\tau_i} |$ is the number of nodes of type $\\tau_i$ in $g_r$, such that $V' = V_r \\setminus \\{v | \\varphi(v) = \\tau_i \\\\text{ with probability } 1 - w_{\\tau_i}\\}$.\n(2) Edge Deletion: After node deletion, delete edges connected to the removed nodes, such that $E' = \\{e | e = (u, v) \\in E_r; u, v \\in V'\\}$.\nThe adjusted subgraph $g' = (V', E')$ is then input to the pretrained GURP to generate the task-specific embeddings.\nIn summary, the manually-designed prompt is the graph pattern $P$ with the set of task-specific entity weights $W_{task}$, which guide the $Adjust(\\cdot)$ function in modifying the subgraph $g_r$ to $g'$. By incorporating these manually-designed prompts, we can tailor the urban region representations to better capture task-relevant entities and relationships. This approach also offers the advantage of adapting to downstream tasks without the need for additional training. It simply requires adjusting the input subgraph based on the manually-defined prompts."}, {"title": "5.2 Task-learnable Prompt", "content": "Apart from leveraging explicitly designed task-specific knowledge prompts to enrich region representations, in the presence of sufficient task data, we can also learn prompts based on the task data and pre-trained representations to induce implicit knowledge within the task data, thereby enhancing region representations.\nConsidering that we describe the attributes of an urban region through subgraph induced with region nodes from $G$, it is intuitive to similarly describe the task adaptation to different region attributes with graphs as well. Therefore, we propose a prompt learning module to use prompt graphs to portray the adaptation of downstream task relative to the original region embedding, as shown in Figure 6.\nWe introduce the graph kernel method [7, 33, 44] to assess the similarity of between the prompt graphs and the region subgraphs due to its excellent performance and computational efficiency, then enhancing the pre-trained region embedding. Let $G$ denote the prompt graph set consisting of $m$ trainable (hidden) prompt graphs, where each prompt graph $G^P \\in G$ is defined as $G^P = (V^P, E^P, X^P)$. Here, $X^P \\in \\mathbb{R}^{|V^P|\\times d}$ represents the node attributes, with $d$ being the dimension of these attributes. For each region subgraph $g_r$, we utilize an attribute graph kernel function $K_p(\\cdot, \\cdot)$ to measure the similarity between $g_r$ and each prompt graph $G^P_1, G^P_2,..., G^P_m$. Specifically, we employ the $P$-step random walk kernel (more details in Appendix A.4) to compute the kernel value between the region subgraph $g_r$ and each prompt graph $G^P_i$. These values are concatenated to form an $m$-dimensional prompt vector $h_p \\in \\mathbb{R}^m$ for each region. We concatenate the prompt vector $h_p \\in \\mathbb{R}^m$ before the pre-trained region representation $\\hat{h}_r$, to derive region embedding $h = h_p||\\hat{h}_r$. The new embeddings are then fed into a linear layer $Linear(\\cdot)$. We update the prompt graphs and linear layer parameters, by optimizing on the downstream labeled task, to obtain the final prompt representation with the simple MSE loss function:\n$\\mathcal{L}_{task} = \\frac{1}{N} \\sum_{i=1}^{N} (y_{pred}^i - y_{label})^2$."}, {"title": "6 Experiments", "content": "We conduct extensive experiments to investigate the following research questions (RQs):\n*   RQ1: Can our proposed pre-train and prompt framework (GURPP) outperform previous approaches?\n*   RQ2: Does GURP have the ability to capture and apply heterogeneous and generic patterns?\n*   RQ3: How does the manually-designed prompt work?\n*   RQ4: How does the task-learnable prompt perform and how does it look like?\n*   RQ5: How does each component contribute to GURPP?"}, {"title": "6.1 Experimental Setup", "content": "6.1.1 Datasets. We collect the datasets of region division, POI, urban imagery, taxi trips, road network, check-in, crime and crash for two representative cities in the United States: New York City (NYC) and Chicago (CHI). The detailed statistics and sources of the dataset is available in Appendix A.5.\n6.1.2 Baselines. We compare our model with the following representative baseline models: knowledge graph embedding methods and its variants (TransR [25], TransR-N, TransR-G), graph embedding methods (node2vec [12], GAE [22]), and urban region embedding methods (MVURE [59], MGFN [46], HREP [65], HRE, ReCP [24]). These baselines cover the range from traditional graph embedding to complex models that take into account urban-specific attributes, providing us with a comprehensive performance comparison benchmark. Specific details and descriptions can be found in the Appendix A.6.\n6.1.3 Metrics and Implementation. To evaluate the prediction performance, we use three widely recognized evaluation metrics: the coefficient of determination (R2), root mean squared error (RMSE), and mean absolute error (MAE). Better performance is indicated by a higher R2 and lower RMSE and MAE values. Our model includes three version: 1) GURP represents the pre-train module, 2) GURPP M is GURP with manually-designed prompt, and 3) GURPPT is GURP with task-learnable prompt. For detailed implementation, please refer to Appendix A.7 and our provided code repository."}, {"title": "6.2 RQ1 & RQ2: Overall Performance", "content": "We evaluate the overall performance of baselines and our model on three downstream tasks. The results of the experiments are shown in Table 1, where the GURPP is all implemented by GURPPT. From the results, we find the following:\n(1) GURPP demonstrates superior performance to baselines in both pre-training and prompting phases. In the pre-training phase, the performance of GURP improves by 6.54%, 8.28%, 462.03%, and 113.01% over the best baseline of R2 on two urban datasets. Meanwhile, on all downstream tasks, the average performance on MAE and RMSE metrics improves by 17.28% and 22.26%, respectively. Subsequently, the performance of GURPP is further improved by an average of 9.50%, 9.28%, and 11.703% of MAE, RMSE and R2, respectively, when we introduce the task-learnable prompt. The results validate the significant advantages of our proposed pre-train and prompt framework.\n(2) To validate the ability of our pre-training method to capture generic urban knowledge, we compare the performance of state-of-the-art urban region embedding methods on datasets from two distinct cities, NYC and CHI. The results show that while these baselines perform well on the NYC dataset, their performance drops significantly on the CHI dataset. For instance, the performance on CHI decreases by an average of 71.8% in R\u00b2 compared to NYC. In contrast, our GURPP model demonstrates remarkable consistency, with only an 8.49% decrease in R2. This may be due to the fact that the baselines tend to be effective only at extracting information specific to certain data types, but they do not consider spatial structure and functional layouts among urban entities. For example, existing baselines rely on the flow view to extract information. However, the content of flow view in the CHI dataset is sparse, leading to a significant degradation in the performance of downstream task."}, {"title": "6.3 RQ3: Manually-designed Prompt Study", "content": "In the real world, the incidence of crime is often closely linked to specific POI categories, roads and junctions, for example, ATMs, bars, parks, etc. are more likely to be the location of a crime. The density of roads and the complexity of junctions are also important factors affecting crime rates. As for the check-in behavior, the brand of the POI is given more attention, and certain users may be more inclined to check in at locations of well-known brands such as Starbucks, McDonald's, or other specific brands. Therefore, we designed two patterns as shown in Figure 7 to validate the performance of manually-designed prompt.\nThe results are shown in Table 2, from which it can be found that crime prediction does pay more attention to factors such as POI categories, roads and junctions, which are closely related to criminal behavior. Meanwhile, removing POI brand information shows a significant decrease in the performance of check-in prediction. These results also demonstrate that the prediction performance of the model can be improved by manually-designed prompt."}, {"title": "6.4 RQ4: Task-learnable Prompt Study", "content": "6.4.1 Effectiveness of Task-learnable Prompt. We utilize GURP pre-trained embedding with the simple prefix prompt (proposed by HREP[65]) and task-learnable prompt respectively for comparative analysis. Both of these prompt methods are data-driven and seek to improve the region representation by learning data for specific downstream tasks. We conduct experiments on crime prediction task in NYC and crash prediction task in CHI, respectively, and the results are shown in Table 3.\nOur task-learnable prompt outperforms pre-trained embeddings across both tasks, with improvements of 8.83% of R\u00b2 in crime prediction and 149.66% of R\u00b2 in crash prediction, indicating that the prompt is highly effective in providing tailored guidelines for different downstream tasks. However, in crime prediction task, the prefix prompt have an negative effect, resulting in a performance decrease of 1.16% of R2. This demonstrates that our task-learnable prompt is more stable and effective compared to prefix prompt.\n6.4.2 Visualization of Learned Prompt Graph. Compared to prefix prompt, our task-learnable prompt is able to generate a series of prompt graphs during training, which enhances the clarity of our prompt method. We visualize prompt graphs that reveal key structural features in the urban region graph of interest in the task. In Figure 8, we show some prompt graphs for crime prediction tasks in CHI. (Additional prompt graphs can be found in Appendix A.8.) These results show that the prompt graphs generally exhibits circular structural features, implying that the model have identified and emphasized interactions between urban entities."}, {"title": "6.5 RQ5: Ablation Studies", "content": "Finally, we conduct the ablation studies in NYC dataset to investigate the effectiveness of the individual components of GURPP. Variants of GURPP are designed as follows: GURPP/P. We use only the pre-training process to learn urban region general representation.GURPP/F, GURPP/S, GURPP/I. We remove the flow view, spatial proximity view, and imagery view from GURPP, respectively. GURPP/K. We replace the knowledge graph embedding initialization with random initialization.\nFigure 9 presents the ablation results of our model and its variants. Spatial proximity view and knowledge graph embedding method significantly enhance the model's performance, lifting it by 151.64% and 156.67% of R2, respectively. These components effectively capture the heterogeneous and generic patterns of interactions among entities by elucidating the spatial relationships and structured knowledge of the urban region. The flow view and the imagery view can also greatly enhance the performance of the model, increasing it by 29.04% and 21.11%, respectively. This is because they capture dynamic flow information and visual features of urban regions, respectively, providing the model with a multi-dimensional context that enhances its ability to fully understand and predict urban dynamics. Prompt learning, with an improvement of 3.12%, effectively elevates model performance by introducing guiding prompts that allow the model to delve deeper into understanding and utilizing task-relevant knowledge."}, {"title": "7 Related Work", "content": "Extensive studies have been conducted to learn urban region representation from different perspectives. We combine the existing work categories in Table 4."}, {"title": "8 Conclusion and Future Works", "content": "In this paper, we introduce the urban region graph and propose a graph-based pre-training and prompting framework for urban region representation learning, balancing urban generality and task specificity. We develop a subgraph-centric pre-training model to capture general urban knowledge and two prompt methods to incorporate task-specific insights. Extensive experiments demonstrate the effectiveness of our approach. Future work may explore: 1) Integrating textual data and LLMs to uncover more universal urban knowledge; 2) Developing arbitrary region representations for flexible urban modeling; 3) Assessing the transferability of urban representations across cities to enhance general applicability."}, {"title": "A Appendix", "content": "A.1 Appendix of urban region graph construction\nHere we provide more details for the urban region graph construction. Specifically", "cities": "region", "14": "to extract visual features for each region image", "features": "n$\\hat{p"}, {"function": "n$\\mathcal{L"}, {"r_i|r_j)": "nwhich helps in accurately learning the inflow and outflow embeddings by comparing the true distributions with the reconstructed distributions.\nA.4 Appendix of attribute graph kernel function\nWe introduce how attribute graph kernel function $K_p (\\cdot", "33": "to measure the similarity between graphs"}, {"33": "n$G_X = G_1 X G_2 = (V_X", "v_2)": "v_1 \\in V_1", "u_2)\\}": {}}]}