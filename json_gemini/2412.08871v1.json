{"title": "Inference-Time Diffusion Model Distillation", "authors": ["Geon Yeong Park", "Sang Wan Lee", "Jong Chul Ye"], "abstract": "Diffusion distillation models effectively accelerate reverse sampling by compressing the process into fewer steps. However, these models still exhibit a performance gap compared to their pre-trained diffusion model counterparts, exacerbated by distribution shifts and accumulated errors during multi-step sampling. To address this, we introduce Distillation++, a novel inference-time distillation framework that reduces this gap by incorporating teacher-guided refinement during sampling. Inspired by recent advances in conditional sampling, our approach recasts student model sampling as a proximal optimization problem with a score distillation sampling loss (SDS). To this end, we integrate distillation optimization during reverse sampling, which can be viewed as teacher guidance that drives student sampling trajectory towards the clean manifold using pre-trained diffusion models. Thus, Distillation++ improves the denoising process in real-time without additional source data or fine-tuning. Distillation++ demonstrates substantial improvements over state-of-the-art distillation baselines, particularly in early sampling stages, positioning itself as a robust guided sampling process crafted for diffusion distillation models. Code: here.", "sections": [{"title": "1. Introduction", "content": "Diffusion models have significantly advanced image generation by producing high-quality samples through an iterative refinement process that gradually denoises an initial noise vector. This refinement can be viewed as solving the reverse generative Stochastic Differential Equation (SDE) or Ordinary Differential Equations (ODE), a counterpart to a prescribed forward SDE/ODE.\nDespite achieving unprecedented realism and diversity, diffusion models face a critical challenge: slow sampling speed. The progressive denoising process is computationally expensive because solving the reverse SDE/ODE requires fine discretization of time steps to minimize discretization errors considering the curvature of the diffusion sampling trajectory [15]. This leads to an increased number of function evaluations (NFE), where typical diffusion sampling necessitates tens to hundreds of NFE, limiting its use in user-interactive creative tools.\nTo address these limitations, various works have proposed to accelerate diffusion sampling. One promising avenue is distillation models, which distill the pre-trained diffusion models (teacher model) by directly estimating the integral along the Probability Flow ODE (PF-ODE) trajectory [41]. This effectively amortizes the computational cost of sampling into the training phase. Recent advances in distillation methods have led to the emergence of a one-step image generator; however, few-step distillation models (student models) are also often preferred in terms of image quality.\nDespite progress, distillation models still face challenges, particularly in bridging the performance gap between the teacher model and its distilled student counterpart. The primary issues include potential suffer from accumulated errors in multi-step sampling or iterative training. For example, [16] identifies potential issues of multi-step sampling with models estimating the zero-time endpoint of PF-ODE. As an empirical demonstration, they show that the generation quality of consistency models does not improve as NFE increases. Similarly, [9] shows that consistency errors can accumulate across time intervals, leading to unstable training. [8, 47] warn the general training/inference mismatch observed in the multi-step sampling of student models. While prior works aim to mitigate this gap by introducing real training datasets [38, 47], it may face a potential distribution shift between datasets of teacher and student, leading to suboptimal performance on out-of-distribution (OOD) prompts.\nIn this work, we aim to overcome this fundamental gap between teacher and student by proposing a novel inference-time distillation framework called Distillation++. Distillation++ is a novel symbiotic distillation framework that distills the teacher model to a student model throughout the sampling process, in contrast to prior works which distill only during training process. In particular, inspired by the recent advances in text-conditional sampling [4, 5, 17], we first recast the diffusion sampling of student models to the proximal optimization problem and regularize its sampling path with a score distillation sampling loss (SDS, [36]) which opens an important opportunity for external guidance. Based on this insight, we develop a teacher-guided sampling process that inherently minimizes the SDS loss by leveraging the pre-trained diffusion model as a teacher evaluating student's denoised estimates during sampling. Specifically, intermediate estimates of student models are refined towards the clean manifold by minimizing the SDS loss, computed using the teacher model.\nThis inference-time distillation fosters a life-long partnership between teacher and student, allowing continuous teacher guidance beyond the training phase. Our empirical results demonstrate that this data-free distillation approach significantly improves student model performance, particularly in the early stages of sampling with minimal computational costs. We believe this approach introduces a new opportunity for inference-time distillation, a concept that has not been previously explored.\nOur contributions can be summarized as follows:\n\u2022 We introduce Distillation++, a novel inference-time distillation framework where teacher guides sampling process so that it closes the gap between student and teacher during sampling with affordable computational costs.\n\u2022 The proposed framework is generally compatible with various student models, ranging from ones directly predicting the PF-ODE endpoint [28, 43, 48] to the progressive distillation branches [21, 37]. We also demonstrate its general applicability with various solvers, including Euler and DPM++ 2S Ancestral [26].\n\u2022 To the best of our knowledge, Distillation++ is a first tuning-free and data-free inference-time distillation framework that serves as a viable post-training option for improving distillation model sampling."}, {"title": "2. Background", "content": "Diffusion models. Diffusion models aim to generate samples by learning the reversal of a prescribed diffusion forward process. In discrete setting with a total of N noise scales, define the fixed forward diffusion kernel as follows:\np(xt|xt\u22121) = N(xt|\u221a\u03b1txt\u22121, (1 \u2212 \u03b2t)I), (1)\npt(xt|x0) = N(xt|\u221a\u03b1tx0, (1 \u2013 \u03b1t)I), (2)\nwhere xo \u2208 Rd ~ po(x) is given as a clean sample, \u03b2t denotes a noise schedule discretized from \u03b2(t) : R \u2192 R > 0, \u03b1t := 1 \u2212 \u03b2t and \u03b1t := \u03a0ti=1\u03b1i. Then as N \u2192 \u221e, the underlying forward noising process can be expressed as the forward It\u00f4 SDE [42] given x(t) \u2208 Rd:\ndx = \u2212 \u03b2(t)/2xdt + \u221a\u03b2(t)dw, (3)"}, {"title": "3. Main Contribution: Distillation++", "content": "While both improved sampling and distillation methods have made significant progress in addressing the speed-quality trade-off, there has been limited advances in integrating these methods through the design of specialized solvers for improving multi-step distillation models. This can be attributed to two primary reasons. First, off-the-shelf ODE solvers typically require the estimation of the tangent gradient direction along the solution trajectory. In contrast, many distillation models [28, 43, 47, 48] directly predict the endpoint of the trajectory on the side of data distribution at any given time point, avoiding the need for direct trajectory estimation. Consequently, the multi-step sampling procedures of these student models are reduced to simple iterative processes involving random noise injection and subsequent denoising steps [43]. Furthermore, distillation models often sample only 2-8 steps, which may restrict the design space for solvers, including higher-order ones.\nThese constraints limit post-training options for improving distillation model sampling, despite the performance gap between multi-step student and teacher models. To address this, Distillation++ leverages large-scale pre-trained diffusion models as a teacher signal during the early-stage sampling process (e.g. first 1-2 steps), which substantially improves the overall sampling trajectory as shown in Fig. 2. More details follow.\nFor a better multi-step sampling of student models, we derive a novel data-free distillation process by integrating the teacher guidance as an optimization problem within the reverse sampling process. Let 0 \u2208 RD and V \u2208 RD parameterize the residual denoiser of student and teacher diffusion models, respectively. Then, our objective is to define a guidance loss function  that, when minimized under the symbiotic guidance of the teacher model, progressively aligns the student's intermediate estimates \u03b8(xt) with the teacher model's distribution. We present such guidance loss function  as a score distillation sampling loss (SDS) with respect to the teacher model (\u03c8) as follow:\nldistill (x; \u03c8, s) = \u2225 xs \u2212 \u221a \u03b1s\u03c8(xs, c) / \u221a 1 \u2212 \u03b1s \u2225 2 2 = \u2225 \u221a \u03b1s \u03c7 \u2212 \u03c8 ( xs, c) / \u221a 1 \u2212 \u03b1s \u2225 2 2 where xs = \u221a \u03b1s x + \u221a 1 \u2212 \u03b1s \u03f5 with a perturbation time s > 0, \u03f5 \u223c N ( \u03f5 0, I ), x \u2208 M with a clean data manifold M, and \u03c8 ( s ) follows the Tweedie's formula in (5) which is denoised by the teacher model \u03c8 . For simplicity, we consider the text-unconditional version with the null-text embedding \u00d8\u2208 Rd in this subsection. This loss represents an ideal condition that high-quality denoised estimates should satisfy: the ideal student samples should be well reconstructed from random perturbations followed by denoising using large-scale pre-trained teacher diffusion models. Variants of the SDS framework [30, 32, 48] have thus frequently been employed as key components in recent diffusion distillation training procedure.\nThen, we can now integrate the optimization step of ldistill in terms of denoised student estimates  , resulting DDIM sampling process (5). A potential concern includes the feasibility of gradient descent due to the intractable Jacobian computation: \u2202\u03f50\u2202x = \u2202\u03f50\u2202xs \u2202xs\u2202x . To circumvent this, by following the prior work on Decomposed Diffusion Sampling (DDS) [4], which bypasses direct computation of the score Jacobian, we have\nxt\u22121 = xt \u2212 \u03b3t\u2207\u03b8(xt)ldistill (xt; \u03c8, s) / \u221a \u03b1t\u22121 xt + \u221a 1 \u2212 \u03b1t\u22121\u03f5\u03b8(xt, t), (8)\nwhere \u03b3t > 0 refers to the step size. [3, 5] supports that this update allows precise transition to the subsequent noisy manifold Mt\u22121 under some manifold assumption. This gives us a simple single DDIM sampling iterate as follows:\nxnew\u03b8(t) = xt \u2212 \u03bb ( xt \u2212 x\u03c8(s) ) = (1 \u2013 \u03bb)xt + \u03bbx\u03c8(s) xt\u22121 = \u221a \u03b1t\u22121xnew\u03b8(t) + \u221a 1 \u2212 \u03b1t\u22121\u03f5\u03b8(xt, t), (9)\nwhere \u03bb = \u03b3t \u221a 1 \u2212 \u03b1t / \u221a \u03b1t. Note that the updated estimate \u03b8(xt) can be obtained by the interpolation between initial student estimate x\u03b8(t) and the revised teacher estimate x\u03c8(s) which is the denoised estimated from different time index s != t. Thus, the goal is to update \u03b8(xt) towards the clean manifold well-aligned with the teacher model distribution.\nRenoising strategy. Note that (9) is not a simple interpolation between the student and teacher during the sampling. Indeed, the time step schedule of s != t plays a crucial role in performance. Considering that the teacher model is well-trained on fine level of timesteps, it is compatible with a broad range of renoising timestep s. That said, as our approach recasts distillation more as a guided sampling rather than a mere training process, we adopt a decreasing time step schedule for s as s = t - 1 following the sampling process (Fig. 2b), reminiscent of schedules used in [17, 45, 50]. This is in contrast to conventional random timestep s scheduling [36]. Intuitively, as the student model often learns to leap towards the end-point of each sub-interval (Fig. 2a), refining this large-step update direction at the terminal of each sub-interval with teacher model may better guide the sampling trajectory (Fig. 2c,d). Empirical evidences and more discussions are provided in Table 3.\nTeacher guidance. For a better understanding of Distillation++, we provide reparameterization of (9), by intentionally assuming \u03b1t \u2248 \u03b1s. This leads to the formulation of teacher guidance, drawing parallels to the CFG denoising"}, {"title": "4. Experimental results", "content": "In our experiments, we demonstrate the impacts of Distillation++ using SDXL backbone [34] and its open-sourced weights. All experiments are conducted using a single NVIDIA GeForce RTX 4090. All quantitative results are obtained with one-step teacher-guided distillation at the initial sampling step of the student models (i.e., t = T), minimizing additional computational costs, though more frequent guidance could further enhance quality. Detailed implementation settings are provided in the appendix and the following."}, {"title": "5. Conclusion", "content": "This paper fosters a symbiotic collaboration between two diffusion models: fast but suboptimal student models and slower, high-quality teacher models. Distillation++ serves as a teacher-guided sampling method, minimizing SDS loss by leveraging a pre-trained model to evaluate student estimates during sampling."}, {"title": "Supplementary Material", "content": "The supplementary sections are organized as follows. Section 6 introduces the pseudo training algorithm behind our inference-time diffusion distillation framework. In Section 7, we provide experimental details. Section 8 features additional results. Following this, we delve into the future directions and limitations of the proposed method in Section 9. Code will be released in https://github.com/anony-distillationpp/distillation_pp.\nAlgorithm 1 Inference-time Diffusion model distillation\n1: Input: Student model \u03b8 , Teacher model \u03c8 , N sampling steps, k number of steps of teacher guidance, CFG scale w, Teacher guidance scale \u03bb .\n2: Output: Improved generation xt .\n3:\n4: xT ~ N (xT|0, I), \u25b3t = T/N\n5: for t = T to At do\n6: Stage 1. Initial student estimation\n7: x\u03b8(t) = xt \u2212 \u221a 1 \u2212 \u03b1t\u03f5\u03b8(xt,c) / \u221a \u03b1t\n8: Stage 2. Revised teacher estimation\n9: if step < k then\n10: Renoising step s = t \u2212 At .\n11: xs = \u221a \u03b1sx\u03b8(t) + \u221a 1 \u2212 \u03b1s \u03f5. (\u03f5 ~ N (\u03f5|0, I))\n12: x\u03c8(s) = xs \u2212 \u221a 1 \u2212 \u03b1s\u03f5\u03c8(xs,c) / \u221a \u03b1s\n13: xnew,c\u03b8(t) = (1 \u2212 \u03bb)x\u03b8(t) + \u03bbx\u03c8(s)\n14: else\n15: xnew,c\u03b8(t) = x\u03b8(t)\n16: end if\n17: Update xt\u2212\u25b3t by forwarding xnew,c\u03b8(t) .\n18: end for\nWe fix typo in (11) with t\u2192 xt. Also, we note that k = 1 used in every quantitative analysis, ensuring computational efficiency. As the proposed framework revises estimation by interpolation, it can be seamlessly extended with a convex combination of multiple teacher revisions. Moreover, as Algorithm 1 is described with random renoising strategy (Line 12), it is fully compatible with general student models ranging from ones directly predicting the PF-ODE endpoint to the progressive distillation branches.\nFor completeness, we extend Distillation++ to accommodate a broader range of ODE/SDE solvers. The core principle lies in steering the denoising process with teacher models. Specifically, we consider solving the variance-exploding (VE) PF-ODE, commonly employed in standard diffusion model implementations1, which can be readily derived via reparameterization of VP diffusion models. Following the notation in Lu et al. [26], we consider a sequence of timesteps {ti} 0 M , where to = T denotes the initial starting point of the reverse sampling (i.e. Gaussian noise).\nEuler [15]. This is in line with DDIM [40] and thus included for completeness:\nxti+1 = xnew,c\u03b8(xti) + xti \u2212 x\u03b8(xti) / \u03c3ti \u03c3ti+1 .\nwhere xnew,c\u03b8(xt) refers to the revised estimate by interpolation. CFG++ [5] can be integrated by replacing x\u03b8(xt) with (xt).\"\nEuler Ancestral. The Euler Ancestral sampler extends the Euler method by introducing stochasticity, taking larger steps and adding a small random noise. This may potentially improve sampling diversity:\nxti+1 = xnew,c\u03b8(xt) + xti \u2212 x\u03b8(xti) / \u03c3ti + \u03c3t\u03f5,\u03f5 ~ N (\u03f5|0, I).\nwhere ti > tdi > ti+1 and \u2208 ~ N(e|0, I).\nDPM-solver++ 2M [26]. While many student models support only first-order solvers, customized distillation models in the open-source community are compatible with higher-order solvers like DPM-Solver++ 2M. Using an iterative process initialized with Gaussian noise, DPM-solver++ refines the sampling trajectory with higher-order corrections, enabling precise updates. Similarly as DPM-solver++ 2S, define ot := e-t, hi := ti \u2212 ti\u22121, and ri := hi\u22121/hi. Given xto initialized as Gaussian noise, the first iteration reads:\nXt1 = x(xt0) + e \u2212h1 (Xto \u2212 X(Xto)).\nThen, the following provides higher-order correction:\nD1 = x(xti\u22121) + 1/2r2 ((xti\u22121) \u2212 X(Xti\u22122)), (13)\nxt1 = e-hixt\u22121 \u2212 (e-hi \u2013 1)Di. (14)\"\n    }"}]}