{"title": "Robust RL with LLM-Driven Data Synthesis and Policy Adaptation for Autonomous Driving", "authors": ["Sihao Wu", "Jiaxu Liu", "Xiangyu Yin", "Guangliang Cheng", "Meng Fang", "Xingyu Zhao", "Xinping Yi", "Xiaowei Huang"], "abstract": "The integration of Large Language Models (LLMs) into autonomous driving systems demonstrates strong common sense and reasoning abilities, effectively addressing the pitfalls of purely data-driven methods. Current LLM-based agents require lengthy inference times and face challenges in interacting with real-time autonomous driving environments. A key open question is whether we can effectively leverage the knowledge from LLMs to train an efficient and robust Reinforcement Learning (RL) agent. This paper introduces RAPID, a novel Robust Adaptive Policy Infusion and Distillation framework, which trains specialized mix-of-policy RL agents using data synthesized by an LLM-based driving agent and online adaptation. RAPID features three key designs: 1) utilization of offline data collected from an LLM agent to distil expert knowledge into RL policies for faster real-time inference; 2) introduction of robust distillation in RL to inherit both performance and robustness from LLM-based teacher; and 3) employment of a mix-of-policy approach for joint decision decoding with a policy adapter. Through fine-tuning via online environment interaction, RAPID reduces the forgetting of LLM knowledge while maintaining adaptability to different tasks. Extensive experiments demonstrate RAPID's capability to effectively integrate LLM knowledge into scaled-down RL policies in an efficient, adaptable, and robust way. Code and checkpoints will be made publicly available upon acceptance.", "sections": [{"title": "Introduction", "content": "The integration of Large Language Models (LLMs) with emergent capabilities into autonomous driving presents an innovative approach [1, 2, 3]. Previous work suggests that LLM can significantly enhance the common sense and reasoning abilities of autonomous vehicles, effectively addressing several pitfalls of purely data-driven methods [4, 5, 6]. However, LLMs face several challenges, primarily in generating effective end-to-end instructions in real-time and dynamic driving scenarios. This limitation stems from two primary factors: the extended inference time required by LLM-based agents [5] and the difficulty these agents face in continuous data collection and learning [7], which renders them unsuitable for real-time decision-making in dynamic driving environments. Furthermore, faster and smaller models, which are often preferred for real-time applications, have a higher risk of being vulnerable to adversarial attacks compared to larger models [8, 9, 10, 11]. These challenges drive us to tackle the following questions:\nHow to develop an efficient, adaptable, and robust agent that can leverage the\ncampabilities of the LLM-based agent for autonomous driving?\nOne potential solution is to use the LLM as a teacher policy to instruct the learning of a lighter, specialized student RL policy through knowledge distillation [12, 13]. This allows the student model to inherit the reasoning abilities of the LLM while being lightweight enough for real-time infer-"}, {"title": "Preliminaries", "content": "Notation. We view a sequential decision-making problem, formalized as a Markov Decision Process (MDP), denoted by (S, A, T, R, \u03b3), where S and A represent the state and action spaces, respectively. The transition probability function is denoted by T : S \u00d7 A \u2192 P(S), and the reward function is denoted by R : S \u00d7 A \u00d7 S \u2192 R. Moreover, \u03b3 denotes the discount factor. The main objective is to acquire an optimal policy, denoted as \u03c0 : S \u2192 A, that maximizes the expected cu-mulative return over time, max \u0395[\u03a3t ytrt]. The policy parameter \u03b8, denoted in \u03c0, is crucial. A typical gradient-based RL algorithm minimizes a surrogate loss, J(0), employing gradient descent concerning 0. This loss is estimated using sampled trajectories, wherein each trajectory comprises a sequence of state-action-reward tuples.\nOffline RL. The aim is to learn effective policies from pre-collected datasets, eliminating the need for further interaction. Given a dataset D = {(s, a, r, s')} containing trajectories collected under an unknown behavior policy \u3160B, the iterative Q update step with learned policy \u03c0 is expressed as\nQk+1 \u2190 arg min J(Q, \u03c0, D), (Policy Evaluation)\nQ\n(1)\nwhere J(Q, \u00bb, D) := Es,a,s'~D [((r(s, a) + Ea'~(a'ls') [Qk(s', a')]) - Q(s,a))\u00b2]. (2)\nWith the updated Q-function, the policy is improved by\nk+1 \u2190 arg max Es~D,a~nk (als) [Qk+1(s,a)]. (Policy Improvement)\n\u03c0\n(3)\nConservative Q-Learning. Offline RL algorithms following this fundamental approach are often challenged by the issue of action distribution shift [19]. Therefore, [20] proposed conservative Q-learning, where the Q values are penalized by Out-Of-Distribution (OOD) actions\nQ*+1 \u2190 arg min J(Q, \u03c0, D) + \u03b1 (Es~D,a~\u00b5(\u00b7\\s) [Q(s, a)] \u2013 Es,a~D[Q(s, a)]), (4)\nQ\nwhere u is an approximation of the policy that maximizes the current Q-function. While [21] found that the effectiveness of Offline RL algorithms is significantly influenced by the characteristics of the dataset, which also motivated us to explore the influence of LLM-generated dataset."}, {"title": "RAPID: Robust Distillation and Adaptive Policy Infusion", "content": "3.1 Offline Dataset Collection\nAs shown in Fig. 2 (a), we conducted a closed-loop driving experiment on HighwayEnv [22] using GPT-3.5 [23] to collect the offline dataset. The vehicle density and number of lanes in HighwayEnv can be adjusted, and we choose LANE-3-DENSITY-2 as the base environment. As a text-only LLM, GPT-3.5 cannot directly interact with the HighwayEnv simulator. To facilitate its observation and decision-making processes, the experiment incorporated perception tools and agent prompts, enabling GPT-3.5 to effectively engage with the simulated environment. The prompts have the fol-lowing stages: (1) Prefix prompt: The LLM obtains the current driving scene and historical infor-mation. (2) Reasoning: By employing the ReAct framework [24], the LLM-based agent reasons about the appropriate driving actions based on the scene. (3) Output decision: The LLM outputs its decision on which meta-action to take. The agent has access to 5 meta-actions: lane_left, lane_right, faster, slower, and idle. More details about the prompt setup are instructed in Appendix F. Through an iterative closed-loop process described above, we collect the dataset DLLM = {(s, a, r, s') |a ~ \u3160LLM(a|s)}, where the \u3160LLM is the LLM agent.\n3.2 Robustness Regularized Distillation\nRecall the offline RL objective in Eq. (3-4). Let the LLM-distil policy be distil, with the collected dataset DLLM, the offline training is to optimize J(Q, distill, DLLM) for an improved Q function, then update the policy w.r.t Q. Empirically as shown in Fig. 1, the LLM-based agent ALLM is more robust against malicious state injection un-der the autonomous driving setting. However, a distilled offline policy is not as robust compared to LLMs, demon-strated by [25], where the Q value can change drastically over neighbouring states, leading to an unstable policy. Therefore, vanilla offline RL algorithms cannot robustly distil information to the LLM-distil agent. Inspired by [10, 11], we formulate a novel training objective by in-troducing a discrepancy term into Eq. (4), allowing the distillation of adversarial knowledge to the offline agent.\nJrobust (Q, \u03c0, D) := J(Q, \u03c0, D) + \u03b1 (Es~D,a~\u00b5(:\\s) [Q(s, a)] \u2013 Es,a~D [Q(s, a)])\n+B (Es,a~D [log [\n\u03c3(Q (\u0161, a))\nonehot(a)\n]] \u2013 Es,a~D [log [log\n\u03c3(Q (\u0161, a))\nonehot(a)\n]]) , where \u0161 = arg max Es,a~D log\n||S-S||2\u20ac\n(5)\nIn Eq. (5), \u03c3(\u00b7) denote the softmax(\u00b7) function, characterizing the probability that the Q network will choose each action. onehot(\u00b7) converts the selected action from the offline dataset to a one-hot vector, characterizing the definite events. Therefore, Es,a~D [log [\n\u03c3(Q(s,a))\nonehot(a)\n]] can be viewed as the KL divergence between the Q network output distribution and the categorical distribution of one-hot action. Essentially, the arg max constraint identifies the adversarial state that yields the worst Q performance, while the objective Jrobust seeks to find the optimal Q function to neutralize the adversarial attack. This process forms an adversarial training procedure, where a and \u1e9e are hyperparameters that balance the conservative and robustness terms, respectively.\n3.3 LLM Knowledge Infused Robust Policy with Environment Adaptation\nIn autonomous driving, the RL policy typically has the overview of the ego car, as well as sur-rounding cars' information. The ego action is predicted by considering the motion of all captured cars. Assuming V vehicles captured, F vehicle features and A action features\u00b9, the state can be"}, {"title": "Mixture-of-Policies via Vehicle Features Tokenization", "content": "Let the observation s \u2208 RV\u00d7F, assume we have N policies for joint decision. Our approach is to revise the state s as a sequence of tokens, where the sequence is of length V and each token is of dimension F. Borrowing ideas from language models [26, 27, 28], we implement mixture-of-policies (MoP) for joint-decision. As detailed below, we illustrate the process to obtain action a from state s. Assume the N policies \u03c01...N : RV\u00d7F \u2192 RV\u00d7D where D is the latent token dim. The state is first fed into a router network G : RV \u00d7 F \u2192 RV\u00d7N to get the policy weights G(s) \u2208RV\u00d7N. Then a topK and Softmax is applied column-wise to select the most influential K policies and get the normalization w = softmax(topK(G(s))) \u2208 RV\u00d7N. Next, we calculate output sequences of all policies: t = {\u03c0\u03af(s)}\u2081 \u2208 RN\u00d7V\u00d7D. The mixed policy is then the weighted mean of sequences, expressed as a = \u2211=1(W)iti\u2208RV\u00d7D. Finally the action is obtained via action decoder dec: RVXD \u2192 RA. In one equation, the joint policy s \u2192 a is formulated by\na = IMOP (S; Od, Or, Op) := decea \u2211 [([softmax (topK (Ga, (s)))])\u00b7\u03c0\u03b5 (;)\ni\ni=1\n] (6)\nwhere \u03b8\u03b1, \u03b8r, Op are respectively the parameters of action decoder, router, and policies. \u041f\u043c\u043e\u0440 represents the mixed policy for the joint decision from both distilled policy and online adaptation policy. In practice, we design the policies as full-attention transformers, and decoder decea as a ViT-wise transformer, taking 1 + V tokens as input sequence where the first token is an extra learnable token. The extra token's embedding is decoded as the predicted action. For detailed implementation of MoP policies and the action decoder, please refer to Appendix C.\nRemark 1. One simple alternative to joint action prediction is to mix actions instead of policies. Regarding N policies as mlp1...N : RV*F \u2192 RA, the final action a \u2208 RA is then the merge of their respective decisions, i.e. a = merge(mlp\u2081...v (flat(s)); W1...N) where flat : RV\u00d7F \u2192 RV*F. However, this approach presents several problems: Q1: Weights W1...N, despite learnable, are fixed vehicle-wise. This means all vehicles in policy 1 share the same w\u2081 for action prediction, same as for other policies. Q2: Weights are independent of observations, which are not generalizable"}, {"title": "Online Adaptation to Offline Policy with Mixture-of-Policies", "content": "Our employment of Eq. (6) is illustrated in Fig. 2, a special case where N = K = 2. The policies come from two sources, respectively, the distilled language model knowledge (LLM-distill Policy, \u03c01 = distil) and the online environment interaction (Online Adapter Policy, \u03c02 = adapt). The robust distillation principal (Eq. (5)) is integrated into the offline distillation phase (Fig. 2.(b)). Respectively, the objective for Q-network & joint policy update is expressed as\nQk+1 \u2190 arg min Jrobust (Q, \u03a0MoP(\u00b7|\u00b7; 0d, 0r, 0p), Doff), (Robust Offline Q-Learning) (7)\nMoP\nQ\n\u03c0(\u00b7s) a)).\narg max Es~Doff, a~ Mop Q+(s,a (LLM Policy Improvement) (8)\n\u03b8\u03b1,01,001)\nThe parameters 02 of 72 (adapt) during policy improvement (Eq. (8)) are frozen since LLM knowledge should only be distilled into \u03c0\u2081 (distil). With an arbitrary RL algorithm, the online adaptation phase objective can be expressed by\nMop = arg max Es~Don,a~IMop (\u00b7|s;01,01,0p) [Q(s, a)]. (Adapter Policy Optimization) (9)\n04,01,012)\nIn Eq. (9), the learned policy interacts with the environment rolling out the (s, a, r) tuple. With the frozen distilled knowledge of LLM in \u03c0distil, parameterized by (01), we fine-tune Tadapt, parameterized by 012), to adapt the MoP policy \u041f\u041c\u043e\u0440 with LLM prior to the actual RL environment."}, {"title": "Zero Gating Adapter Policy", "content": "To eliminate the influence of adapt on distil during the offline phase, we adopt the concept of zero gating [29] for initializing the policy. Specifically, the router network's corresponding expert weights for adapt are masked with a trainable zero vector. As a result, the MoP token in phase 2 only considers the impact of distil (s). During phase 3, we enable the training of zero gates, allowing adaptation tokens to progressively inject newly acquired online signals into the MoP policy \u041f\u041c\u043e\u0440."}, {"title": "Experiments", "content": "4.1 Experiment Setting\nWe conduct experiments using the HighwayEnv simulation environment [22]. We involve three driving scenarios with increasing levels of complexity: LANE-3-DENSITY-2, LANE-4-DENSITY-2.5, and LANE-5-DENSITY-3. Detailed task descriptions are delegated to Appendix D. This paper constructs three types of datasets, namely the random Drand, the LLM-collected DLLM, and the combined dataset Doff. The construction details and optimal ratio for Doff are discussed in Sec. 4.2.\nWe compare the RAPID performance under the offline phase with several state-of-the-art RL meth-ods, DQN [30], DDQN [31], and CQL [20] respectively. In the online phase, we employ the DQN algorithm as the adaptation method under our RAPID framework. To validate the efficacy of Jrobust term in Eq. (5), we utilize four various attack methods: Uniform, Gaussian, FGSM, and PGD, to evaluate the robustness of the distilled policy. All baselines are implemented with d3rlpy library [32]. The hyperparameters setting refers to Appendix. E. Each method is trained for a total of 10K training iterations, with evaluations performed per 1K iterations. We employ the same reward func-tion as defined in highway-env-rewards2."}, {"title": "Fusing LLM Generated Dataset (Phase 1)", "content": "To validate the efficacy of the LLM-collected dataset DLLM, we build the offline dataset by combining it with a random dataset Drand. We evaluate the cumulative rewards on Doff = sample({DLLM, Drand}; {p, 1 \u2013 p}) with two offline algorithms: DDQN [31] and CQL [20], to find the best ratio p. To clarify, Drand is sam-pled using a random behavioural policy, serving as a baseline for data collection. DLLM is sam-pled via a pre-trained LLM \u3160LLM. We gather 3K transitions using ALLM for each trail. Therefore, both Drandom and DLLM dataset contains 15K transitions.\nAccording to Fig. 3, the pure Drand with p = 0% cannot support a well-trained offline policy. Utilizing only the DLLM can also harm the performance. However, augmenting offline datasets with partial LLM-generated data can significantly boost policy cumulative reward. With such evidence, we choose p = 25% in between 12.5% ~ 50% as a sweet spot and define it as our final Doff"}, {"title": "Offline LLM Knowledge Distillation (Phase 2)", "content": "Phase 2 only uses distil for offline training as described in Sec. 3.3. We evaluate RAPID on three environments, respectively, LANE-3-DENSITY-2, LANE-4-DENSITY-2.5 and LANE-5-DENSITY-3 with three different types of datasets: Drand, DLLM and Doff as described in Sec. 4.2. In particular, we collect Doff with LANE-3-DENSITY-2 only, and apply this dataset to trails LANE-3-DENSITY-2-Doff, LANE-4-DENSITY-2.5-Dff, and LANE-5-DENSITY-3-Doff\nWe present the offline results in Tab. 1 and observe the following: (1) With DLLM, policies ex-hibit better offline performance than Drand in general, while Doff, as a mixture of above, improve upon both randomly- and LLM-generated datasets. This again confirms our conclusion in Sec. 4.2. (2) Our approach, RAPID, consistently outperforms conventional methods, with RAPID using the mixed Dorff achieving the highest rewards overall. (3) Jrobust does not impact the clean performance under the offline training phase. (4) Doff collected from LANE-3-DENSITY-2 achieves better per-formance across all tasks, indicating that the LLM-generated dataset contains general knowledge applicable to different tasks with the same state and action spaces."}, {"title": "Online Adaptation Performance (Phase 3)", "content": "To evaluate the online adaptation ability of RAPID, we employ the pre-trained distil from Phase 2 (with the collected Doff from LANE-3-DENSITY-2). Then train the adapt and its zero gate (as depicted in Fig. 2(c)) via interacting with different online environments. We compare it with the vanilla Online RL (DQN) and Offline RL (DQN) without using the RAPID MOP policy. We train the RAPID policy for 5K training epochs during the offline phase, followed by 10K online epochs. The Online DQN starts from the 5K-th epoch.\nAs depicted in Fig. 4: (1) The cumulative rewards for distil are sufficiently high, due to the in-troduced common sense knowledge and reasoning abilities from LLM. However, the conventional offline RL cannot perform well. (2) With the offline phase pre-trained on LANE-3-DENSITY-2, the online adaptation with RAPID MOP on the same dataset LANE-3-DENSITY-2 (Fig. 4(a)) achieves significantly high rewards. This demonstrates the necessity of online adaptation to generalize knowledge for practical application. (3) We further conduct zero-shot adaptation on offline-unseen dataset LANE-4-DENSITY-2.5 and LANE-5-DENSITY-3. In Fig. 4(b-c), we observe RAPID can achieve competitive performance not only compared to the vanilla online approach, but toward the large ALLM. This highlights the efficacy of the RAPID framework in task adaptation."}, {"title": "Robust Distillation Performance", "content": "We evaluate the robustness of multiple distillation algorithms using 4 different attack methods, re-spectively, Uniform, Gaussian, FGSM and PGD. Specifically, we employ a 10-step PGD with a designated step size of 0.01. For both FGSM and PGD attacks, the attack radius e is set to 0.1. For Uniform and Gaussian attacks, e is set to 0.2. The observation was normalized before the attack and then denormalized for standard RL policy understanding. In RAPID, we set the \u1e9e as 0.5 to balance the robustness distillation term according to Eq. (5).\nAs illustrated in Tab. 2, the conventional methods, Online DQN and Offline DQN, are not able to effectively defend against strong adversarial attacks like FGSM and PGD. Although Offline DQN, which is trained on the dataset Doff partially collected by the LLM policy #LLM. It still strug-gles to maintain performance under these attacks. RAPID (w/o Jrobust), which undergoes offline training and online adaptation without using Jrobust, performs weakly when facing strong adver-sarial attacks. In contrast, the full RAPID method with Jrobust demonstrates superior robustness against various adversarial attacks across all three environments. We note that the Jrobust might approximate the robustness of LLM by building the robust soft label in Eq. 5. Overall, the Jrobust regularizer plays a crucial role in bolstering the robustness of the model by effectively utilizing the robust knowledge obtained from the LLM-based teacher."}, {"title": "Ablation Study", "content": "MoP Routing Analysis. In Fig. 5, we visualize the contribution of each policy (distil and adapt) to the final predicted action at each vehicle position after online adaptation. In general, distil dom-"}, {"title": "Conclusion", "content": "We propose RAPID, a promising approach for leveraging LLMs' reasoning abilities and common sense knowledge to enhance the performance of RL agents in heterogeneous autonomous driving tasks. Meanwhile, RAPID overcomes challenges such as the long inference time of LLMs and policy knowledge overwriting. The robust knowledge distillation method enables the student RL policy to inherit the robustness of the LLM teacher.\nLimitation and future work. Although we have conducted tests in three distinct autonomous driv-ing environments and validated the closed-loop RL policy in real-time, the scope of the analysis is restricted. The 2D HighwayEnv remains overly simplistic for comprehensive autonomous driving evaluation. To further establish the efficacy of our online adaptation policy, it is necessary to assess its performance with Visual Language Models [33, 34] in more realistic and complex autonomous driving environments, like CARLA [35]."}, {"title": "Related Work", "content": "A.1 Offline RL for Autonomous Driving\nOffline RL algorithms are designed to learn policies from a static dataset, eliminating the need for interaction with the real environment [36]. Compared to conventional online RL [37] and the extended goal-conditioned RL [38], this approach is especially beneficial in scenarios where such interaction is prohibitively expensive or risky, such as in autonomous driving. Offline RL algorithms have demonstrated the capability to surpass expert-level performance [39, 20, 40]. In general, these algorithms employ policy regularization [39, 41] and out-of-distribution (OOD) penalization [20, 42] as strategies to prevent value overestimation. In this paper, we pioneer utilising the LLM-generated data to train the offline RL. Although there have been several works [43, 44] focus on distillation for LLM, none of them considers distilling to RL.\nA.2 LLM for Autonomous Driving\nRecent advancements in LLMs [16, 45, 46] demonstrate their powerful embodied abilities, provid-ing the possibility to distil knowledge from humans to autonomous systems. LLMs exhibit a strong aptitude for general reasoning [47, 48], web agents [49, 50], and embodied robotics [51, 52, 53]. Inspired by the superior capability of common sense of LLM-based agents, a substantial body of research is dedicated to LLM-based autonomous driving. Wayve [54] introduced an open-loop driv-ing commentator called LINGO-1, which integrates vision, language, and action to enhance the interpretation and training of driving models. DiLu [5] developed a framework utilizing LLMs as agents for closed-loop driving tasks, with a memory module to record experiences. To enhance the stability and generalization performance, [4] utilised reasoning, interpretation, and memorization of LLM to enhance autonomous driving. More research works for this vibrant field were summarised in [55]. However, all these methods require huge resources, long inference time, and unstable per-formance. To bridge this gap, we propose a knowledge distillation framework, from LLM to RL, which enhances the applicability and stability of real-world autonomous driving.\nA.3 Distillation for LLM\nKnowledge distillation has proven successful in transferring knowledge from large-scale, more competent teacher models to small-scale student models, making them more affordable for prac-tical applications [56, 57, 58]. This method facilitates learning from limited labelled data, as the larger teacher model is commonly employed to generate a training dataset with noisy pseudo-labels [59, 60, 61]. Further, [44] involves extracting rationales from LLMs as additional supervisory signals to train small-scale models within a multi-task framework. [43] utilises reverse Kullback-Leibler di-vergence to ensure that the student model does not overestimate the low-probability regions in the teacher distribution. Currently, knowledge distillation from LLM to RL for autonomous driving remains unexplored."}, {"title": "Additional Experiments", "content": "B.1 Comparison between MLP and RAPID(Attentive) based Policy\nTo verify the contribution of the RAPID(Attentive) architecture (as shown in Fig. 8) in the offline training phase, we conduct extra experiments in this section. As illustrated in Tab. 3, we compared the DQN, DDQN, and CQL under MLP and RAPID architecture, respectively. As the environment complexity increases, the performance gap between RAPID and MLP narrows, suggesting RAPID handles simpler environments more effectively. In summary, we conlude: (1) the RAPID + DQN method achieves the best performance among all methods, thus we choose DQN as the backbone of RAPID for offline training. (2) The attentive architecture demonstrates superior performance compared to MLP, particularly in less complex environments.\nB.2 Impact of Jrobust\nBuilding upon the results presented in Fig. 6, we provide an analysis of the impact of Jrobust among three environments. The experiment setup is the same as Sec. 4.3. The RAPID is trained 10K epochs based on LANE-3-DENSITY-2-Doff, LANE-4-DENSITY-2.5-Doff, and LANE-5-DENSITY-3-Doff. From Fig. 6, we observe that the Jrobust gradually decreased and converged to 0 during the training process. The experimental results align with our results in Tab. 2, as the defense mecha-nism demonstrates superior performance in both LANE-3-DENSITY-2 and LANE-4-DENSITY-2.5. In the more complex LANE-5-DENSITY-3 environment, the loss remains relatively high, around 1.6, which leads to suboptimal defense performance compared to the other scenarios. This suggests that the increased complexity and vehicle density in this setting pose additional challenges for the de-fense mechanism. Overall, the consistency across different settings highlights the robustness and effectiveness of our approach."}, {"title": "Impact of B", "content": "To investigate the effect of various \u1e9e values (hyperparameter in Eq. 5), we compare \u03b2\u03b5 {0.1,0.5,0.8} on LANE-3-DENSITY-2, LANE-4-DENSITY-2.5, and LANE-5-DENSITY-3, respec-tively. The results are illustrated in Tab. 4. Based on the results presented in Tab. 4, we make several observations regarding the impact of the \u1e9e on the performance of our robust distillation approach. When setting \u03b2 = 0.8, we notice a decline in the clean performance across all three environments. This degradation can be attributed to the increased emphasis on adversarial examples during the training process, which may lead to a trade-off with clean accuracy. On the other hand, when setting \u03b2 = 0.1, the attack return is relatively lower compared to other settings. This suggests that the adversarial training strength may not be sufficient to provide adequate robustness against adversar-ial perturbations. Considering these findings, we determine that setting \u03b2 = 0.5 strikes a balance between maintaining clean performance and achieving satisfactory robustness."}, {"title": "Visualization the of distil and adapt", "content": null}, {"title": "Network Implementation", "content": "C.1 Transformer Encoder\nWe employ the same encoder-only transformer fTranse as [62].\nC.2 Policy Networks\nThe policy network consist of a linear projection and a transformer encoder that take the projected state as input. Let state s \u2208 RV\u00d7F. The policy network first project s \u2192 s\u0129 \u2208 RV\u00d7F' with a trainable linear projection, then regard s as the input token for the transformer. The transformer then processes these embeddings through [0, L] layers of self-attention and feedforward networks, in our case, we set L = 2. Mathematically, for each layer l \u2208 [0, L], the distil/Tadapt computes the following: sp \u2190 Transe fpolicy\n(fProj(s)) \u2208 RV\u00d7D.\nC.3 Action Decoder\nThe action decoder transforms encoded state representations into an action using another transformer encoder. It takes the mix-of-policy token from the routed policy networks as input. Given the routed MoP token sm \u2208 RV\u00d7D, we first concatenate sm with an extra learnable token s\u1ebd \u2208 R1\u00d7D, then put concat(sm||Se) \u2208 R(V+1)\u00d7D into the transformer encoder. We regard the first output token as the action token. Essentially, dec : RV\u00d7D \u2192 RA computes: a \u2190 faction (concat(sm||Se))o,: \u2208 R1\u00d7A."}, {"title": "Environment Details", "content": "Each of these three environments has continuous state and discrete action space. The maximum episode horizon is defined as 30. Tab. 5 describes the configuration for the highway environment in a reinforcement learning setting. The observation configuration defines the type of observation uti-lized, which is specified as KinematicsObservation. This observation type represents the surround-ing vehicles as a V \u00d7 Farray, where V denotes the number of nearby vehicles and F represents the size of the feature set describing each vehicle. The specific features included in the observation are listed in the features field of the configuration. The KinematicsObservation provides essential information about the neighbouring vehicles, such as their presence, positions in the x-y coordinate system, and velocities along the x and y axes. These features are represented as absolute values, independent of the agent's frame of reference, and are not subjected to any normalization process. Fig. 9 demonstrates the state features under KinematicObservation setting, in which we introduce the vehicle feature tokenization in Sec. 3.3."}, {"title": "Hyperparameters", "content": "In this work, conventional DQN, DDQN and CQL share the same MLP network architectures. For all experiments, the hyperparameters of our backbone architectures and algorithms are reported in Tab. 6. Our implementation is based on d3rlpy [32], which is open-sourced."}, {"title": "Prompt Setup", "content": "In this section, we detail the specific prompt design and give an example of the interaction between the LLM-based agent and the environment.\nPrefix Prompt. As shown in Fig. 10, the Prefix Prompt part primarily consists of an introduction to the autonomous driving task, a description of the scenario, common sense rules, and instructions for the output format. The previous decision and explanation are obtained from the experience buffer. The current scenario information plays an important role while making decision, and it is dynamically generated based on the current decision frame. The driving scenario description contains information about the ego and surrounding vehicles' speed and positions. The common sense rules section embeds the driving style to guide the vehicle's behaviour. Finally, the final answer format is constructed to output the discrete actions and construct the closed-loop simulation on HighwayEnv.\nInteraction. We demonstrate one example to make readers better understand the reasoning process of GPT-3.5. As shown in Fig. 11, the ego car initially checks the available actions and related safety outcomes. On the first round of thought, GPT-3.5 tries to understand the situation of the ego car and checks the available lanes for decision-making. After several rounds of interaction, it checks whether the action keep speed is safe with vehicle 7. Finally, it outputs the decision idle and explains that maintaining the current speed and lane can keep a safe distance from surrounding cars."}]}