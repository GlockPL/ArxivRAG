{"title": "Explainable and Robust Millimeter Wave Beam Alignment for AI-Native 6G Networks", "authors": ["Nasir Khan", "Asmaa Abdallah", "Abdulkadir Celik", "Ahmed M. Eltawil", "Sinem Coleri"], "abstract": "Integrated artificial intelligence (AI) and communication has been recognized as a key pillar of 6G and beyond networks. In line with AI-native 6G vision, explainability and robustness in Al-driven systems are critical for establishing trust and ensuring reliable performance in diverse and evolving environments. This paper addresses these challenges by developing a robust and explainable deep learning (DL)-based beam alignment engine (BAE) for millimeter-wave (mmWave) multiple-input multiple-output (MIMO) systems. The proposed convolutional neural network (CNN)-based BAE utilizes received signal strength indicator (RSSI) measurements over a set of wide beams to accurately predict the best narrow beam for each UE, significantly reducing the overhead associated with exhaustive codebook-based narrow beam sweeping for initial access (IA) and data transmission. To ensure transparency and resilience, the Deep k-Nearest Neighbors (DkNN) algorithm is employed to assess the internal representations of the network via nearest neighbor approach, providing human-interpretable explanations and confidence metrics for detecting out-of-distribution inputs. Experimental results demonstrate that the proposed DL-based BAE exhibits robustness to measurement noise, reduces beam training overhead by 75% compared to the exhaustive search while maintaining near-optimal performance in terms of spectral efficiency. Moreover, the proposed framework improves outlier detection robustness by up to 5x and offers clearer insights into beam prediction decisions compared to traditional softmax-based classifiers.", "sections": [{"title": "I. INTRODUCTION", "content": "THE International Telecommunication Union's IMT-2030 framework identifies \"Integrated artificial intelligence (AI and Communication\" as one of the core directions for 6G networks. This shift toward AI-driven solution necessitates greater transparency in decision-making processes, where explainability is pivotal in establishing trust in AI systems, allowing network operators and engineers to understand, validate, and troubleshoot the decisions made by deep learning (DL) models. Additionally, robustness against out-of-distribution inputs and adversarial attacks is crucial to ensure reliable performance in diverse and evolving environments. These two factors-explainability and robustness-are essential to fulfilling the broader goals of AI-native 6G networks, where AI is not just an enhancement but a foundational component integrated into communication systems [1]. Among their various wireless communication use cases, DL models especially offer promising solutions to address the challenges posed by massive multiple-input multiple-output (mMIMO) millimeter wave (mmWave) systems, which rely on efficient beam alignment to determine the optimal beam pair between base stations (BS) and user equipment (UE) [2]. In the current 5G standard, beam alignment strategies are employed wherein the BS sweeps beams using reference signals, while the UE measures the received signal strength indicators (RSSIs) and reports the strongest beam back to the BS [3]. Standard approaches often use quantized beams, which distribute energy across the angular space via codebooks, such as discrete Fourier transform (DFT) codebooks. While DFT codebooks ensure broad coverage, they often lack the granularity needed for precise beam alignment. To address this, oversampled DFT (O-DFT) codebooks provide finer granularity at the cost of increased beam training overhead, as more beams need to be evaluated during the alignment process. In general, beam sweeping at the BS/UE is cyclically executed via an exhaustive search to refresh and maintain continuous beam alignment, where the feedback communication overhead emerges as a critical bottleneck.\nTo address the challenges with non-AI beam alignment methods, DL-based solutions have attracted great interest, enabling learning from data and adapting to dynamic conditions [4]. By incorporating real-world data, such as radar measurements [5], camera images [6], and global navigation satellite system (GNSS) coordinates [7], DL-based systems can predict optimal beams with greater speed and accuracy. However, while DL-aided approaches have demonstrated significant improvements in beam alignment efficiency [5]- [7], a key challenge remains: the lack of interpretability in the decision-making process and the vulnerability to out-of-distribution inputs. Radio engineers are often interested in mapping data inputs, algorithm design, to the projected wireless key performance indicators, where promoting explainability and robustness of DL-based solutions becomes necessary for automatic decision-making systems [8]. Ensuring that DL-based solutions are resilient to outliers and transparent in their decision-making is essential for standardization and commercial deployment.\nThis paper proposes a robust and explainable DL-based beam alignment engine (BAE) to predict mmWave beams during the initial access (IA) process with minimal beam sweeping overhead. The proposed solution is a convolutional neural network (CNN)-based beam predictor that utilizes RSSI feedback of a finite set of sensing beams (i.e., DFT codebook) to accurately predict the optimal narrow beams from the O- DFT codebook for IA and data transmission. The developed solution significantly enhances beam training efficiency by eliminating the need for exhaustive searches within the O-"}, {"title": "II. SYSTEM MODEL AND PROBLEM FORMULATION", "content": "We consider a downlink mmWave communication system, where the BS features a uniform linear array (ULA) with NBS antenna elements to communicate with Nu single-antenna UEs. We concentrate on the scenario of multi-user beamform- ing, where the BS communicates with very UE using only a single stream. The channel from the BS to the UEu can be expressed based on geometric channel modeling as\n$h_u = \\sum_{l=1}^{L} a_{u,l}b(\\phi_l),$ (1)\nwhere L denotes the number of paths, $a_{u,l}$ represents the complex path gain for the l-th path, $\\phi_l$ is the angle of departure for the 1-th path, and b (1) represents the array response vector, which is given by\n$b(\\phi_{u,l}) = \\frac{1}{\\sqrt{N_{BS}}} [1, e^{jd \\sin(\\phi_{u,l})}, ..., e^{j(N_{BS}-1)d \\sin(\\phi_{u,l})},$ (2)\nwhere A is the signal wavelength, and d = $ \\frac{\\lambda}{2}$ denotes the antenna spacing. To mitigate the hardware cost and power consumption of a fully digital system, we adopt analog- only beamforming where the BS has a single common trans- mit/receive radio frequency (RF) chain shared by NBs anten- nas. Hence, the beamforming vector designed for the BS is given by\n$W = \\frac{1}{N_{BS}} [e^{j\\varphi_1}, ..., e^{j\\varphi_i}, ..., e^{j\\varphi_{N_{BS}}}] \\in N_{BS}\\times1,$ (3)\nwhere $\\varphi_i$ is the phase shift of i-th antenna element. We assume that the BS adopt a beamforming codebook W = {$w_1$, ..., $w_Q$} incorporating Q pre-defined beamforming vectors. The vectors in W satisfy ||$w_q$||\u00b2 = 1, $ \\forall q \\in$ {1, ..., Q} to accommodate the constant-modulus constraint of analog beamforming architec- ture.\nDuring the beam sweeping process, the BS periodically transmits symbols $s_w \\in C$ to the UEs through the beams defined by the matrix $W \\in C^{N_{BS}XQ}$. Following the beam sweeping process, the complex received signal at the UEu, using the q-th beamforming vector, can be expressed as\n$RSSI,u = \\sqrt{PBS}h_wqS_w + Z_{RSSI,u},$ (4)\nwhere PBS denotes the BS transmit power, $h_u \\in C^{N_{BS}\\times1}$ is the channel vector, and $Z_{RSSI,u}$ represents the additive com- plex noise with power \u03c32. Then, with unit-power transmitted symbols, the signal-to-noise ratio (SNR) at the UEu can be expressed as\n$SNRU = \\frac{PBSh_w}{\\sigma^2},$ (5)\nFor a given BS-UE pair, the optimal beam index q* can be identified by selecting the beam that maximizes the SNR:\n$q^* = \\underset{qu\\in{1,2,...,Q}}{arg\\ max} \\frac{\\|hwa}{\\sigma^2} = \\underset{qu\\in{1,2,...,Q}}{arg\\ max} (hw^2).$ (6)\nIdentifying q through exhaustive beam search over Q code- words results in significant beam training overhead, which will be addressed by the proposed solution next."}, {"title": "III. DEEP LEARNING FRAMEWORK FOR BEAM ALIGNMENT", "content": "This section describes the details of the DL-based beam alignment method and the proposed DkNN-based explainable and robust beam selection framework."}, {"title": "A. DL-based Beam Alignment", "content": "To address the beam training overhead caused by exhaustive search and resulting feedback communication overhead, the proposed solution leverages the RSSI values over a set of compact wide sensing beams Mw (i.e., beams from DFT codebook), to select the best narrow beam from the O-DFT codebook, thus, avoiding exhaustive search over the O-DFT codebook. For beam-sweeping, the BS transmits the pilot signals over a smaller set of beams, each at a separate time slot. All UEs connected to the BS measure and report the RSSI values of the Mw beams. It is assumed that beam sweeping, measurement, and reporting occur within the coherence time during which the channel remains constant. The reported beam sweeping results for UE u can be written as\n$[rrssi,u]_i = \\sqrt{PBS}h_w \\delta_i + z_i$ (7)\nwhere $[rrssi,u]_i = \\sqrt{PBShwisw+zi}$ is the received signal using the i-th sensing beam, $ \\forall i \\in M_w$.\nIn the DL-based approach, this beam selection problem is formulated as a classification task. Particularly, the reported beam sweeping results from (7) are fed as input to the deep neural network (DNN)-based beam classifier denoted by f(.;0): X $\\rightarrow$ RO, where X$\\in R^{M_w}$ is the set of DNN's input corresponding to the RSSI values over $M_w$ beams and \u03b8 denotes the DNN's weight parameters vector. The beam classifier outputs the posterior probability distribution q = f(x, 0) of each narrow beam in Q being the optimal beam. The true beam index is represented as a one-hot vector q*$\\in R^Q$, and the loss function is formulated as the cross- entropy between the predicted and true distributions:\n$L(xu, q^*, \\theta) = - \\sum_{i=1}^{Q} q_i^*log q_i,$ (8)\nwhere i is the index representing each possible beam, q and $q_i$ denote the true one-hot encoded vector and the predicted probability for beam i$\\in Q$, respectively.\nWhile the aforementioned DL-aided approach reduces beam alignment overhead, it still faces issues with robustness and reliability in confidence estimates. DNN models often struggle to quantify prediction confidence due to their black-box nature, making it difficult to trust their decisions in real-world scenar- ios, where inaccurate beam selection can significantly degrade performance. Next, we propose a framework analyzing the internal representations of the neural networks to strengthen the explainability and robustness of its beam predictions."}, {"title": "B. Proposed Explainable and Robust Beam Classifier Frame- work", "content": "To quantify the confidence in the beam prediction task, we utilize the DkNN algorithm from [9], which incorporates concepts from conformal prediction to analyze the internal rep- resentations generated by the DNN/CNN during testing. This method identifies inconsistencies with training observations, thus providing a more reliable measure of confidence than the softmax-based prediction typically used to estimate the model's confidence. The DkNN algorithm is model-agnostic and can be applied to any pre-trained DL model with man- ageable computational complexity.\nLet f(xu;0) be a pre-trained DNN-based beam classifier composed of L layers, where each layer is indexed by n, with \u03b7\u2208 {0,...,1\u22121}. After training the DNN for beam selection, the algorithm records the output of each layer $f_n$ for every training point, thus obtaining a per-layer representation of the training data paired with their respective labels. This per-layer representation is used to build a nearest neighbor classifier in the space defined by each layer l to create a representation of the training data at each layer. To efficiently identify nearest neighbors in the high-dimensional spaces produced by these layers, we use locality-sensitive hashing (LSH) [10], which finds similar representations based on cosine similarity. For a given test input \u00c2u, we pass it through the DNN to obtain the layer outputs $f_n(x_u)$, then apply LSH to find the k nearest neighbors from the training data for each layer's representation. The labels associated with these nearest neighbors are collected for each layer, and these multi- sets of labels are used to compute the final beam selection prediction through a non-conformity check.\nFor a given test input \u00c2u, the non-conformity of a prediction is defined as the number of nearest neighboring representations found in training data whose label is different from the candidate label j \u2208 {1, 2, . . ., Q}, mathematically expressed as:\n$\\rho(\\hat{x_u}, j) = \\sum_{\\eta=1}^{l}|{i \\in \\Omega_{\\eta}: i \\neq j}],$ (9)\nwhere $ \\Omega_n$ is the multi-set of labels for the training points whose representations are closest to the test input's at layer \u03b7, and the operator || denotes the cardinality of a set, which is the number of elements contained within it.\nBefore making beam predictions, we compute the non- conformity of a labeled calibration dataset (X, Q\u00ba), sampled from the same distribution as the training data (X, Q) but not used for training. The nonconformity values are defined as C ={(xu, q) : (\u00c2u, q) \u2208 (X\u00ba, Q\u00ba)}, and are then compared to the test input's nonconformity score (\u00c2u, j) for each candidate beam label j. For input \u00c2u with label j, we calculate the empirical p-value, which represents the fraction of calibration nonconformity scores larger than the test input's score, as:\n$P_j(u) = \\frac{|{\\rho \\in C : \\rho \\geq \\rho(\\hat{x_u}, j)}|}{|C|}$ (10)\nThe predicted beam label is the one with the highest p- value, as given by\n$Prediction = arg\\ max_j P_j(u).$ (11)\nConfidence is defined as one minus the second-highest p- value, i.e., the probability that any label other than the predic- tion is true, as given by\n$Confidence = 1 - \\underset{j \\in {1,2,...,Q},j\\neq prediction} {max} P_j(u).$ (12)\nThe prediction credibility is the p-value of the predicted beam label, which measures the degree to which the test input conforms to the training data (equation 13).\n$Credibility = max P_j(u).$ (13)\nThe proposed DkNN-based beam classification algorithm is summarized in Algorithm 1. To evaluate the model's re- silience against adversarial/outlier inputs using Algorithm 1, we generate the out-of-training data (adversarial) examples by the Fast Gradient Sign Method (FGSM) [11] that aims to manipulate the inputs to a beam classifier by perturbing them in the direction that maximizes the loss function in 8 with respect to the true beam labels. To generate the adversarial example, the FGSM computes the perturbations as du = e sign (Vx L(xu, q*, 0)), where e controls the perturbation magnitude under some suitable power constraint with respect to the original RSSI values. The adversarial input computed as Xadv,u = Xu + du is used to assess the classifier's robustness to outlier inputs.\nTo characterize the credibility estimates, we adopt the standard reliability diagrams [12] to visualize the calibration of credibility scores. Reliability diagrams are histograms pre- senting accuracy as a function of credibility estimates of the model's prediction. The reliability diagrams bin the classifier's credibility score into S intervals of equal size. A test data point (u, q) from the test dataset (Xte, Qte) is placed in bin B if the model's credibility on u is contained within the bin, i.e., (\u00c2u, q) \u2208 Bs. For each bin Bs, the within-bin accuracy is defined as:\n$Acc (B_s) = \\frac{1}{B_s} \\sum_x 1 (arg\\ max P_j(x_u,q))=q,$ (14)\nwhich measures the fraction of test samples within the bin that are correctly classified.\nIt is noteworthy that rather than blindly trusting the model's predictions, the proposed approach enhances explainability by using nearest neighbors to provide example-based insights, aligning the DNN's intermediate computations with its output for more transparent decision-making"}, {"title": "IV. SIMULATION RESULTS", "content": "In this section, we provide details of the simulation setup, dataset acquisition, and the DL model architecture, followed by a discussion of the results."}, {"title": "A. Simulation Setup", "content": "To simulate the BS-UE mmWave communications, we adopt an urban scenario comprising the downtown sector of Boston with both line-of-sight (LOS) and non-line-of-sight (NLOS) users. The BS employs a ULA with NBS = 32 antennas and is placed at a height of 15 meters while oriented towards the negative y-axis. The BS communicates with single-antenna UEs with a height of 2 meters. The service area measures 200 meters by 230 meters and is discretized into a user grid with a spacing of 0.37 meters. Based on these configurations, a total number of 98,650 downlink UE channels are generated. We construct the channel matrix for every UE position using the DeepMIMO dataset generator [13] according to the specified parameters and system configuration summarized in Table I. To enhance the stability and efficiency of training, the channel vectors, the channel vectors h \u2208 CNBs\u00d71 are normalized by the largest absolute value in the channel's matrix. All simulations are performed on a 10-Core Intel(R) Xenon(R) Silver 4114, 2.2GHz system equipped with an Nvidia Quadro P2000 graphics processing unit (GPU).\nThe BS performs analog beamforming using Mw sensing beam using a NBS-DFT codebook, whereas, for the narrow beam codebook W, we use an O-DFT with OS factor of 4 to get a total of 128 narrow beams. The parameters of the neural network are learned from a labeled dataset D = {(xu,k, q) : k = 1, .., K} which is composed of K labeled training samples. Each sample consists of the received power values as the input features and the O-DFT beam index as the target label generated using (6). In all experiments, 70% of the data is allocated for training, 10% for validation, and the remaining 20% for testing. The calibration dataset is created by reserving a portion of the test data not utilized for evaluation."}, {"title": "B. Deep Learning Model Architecture", "content": "The proposed DkNN beam classifier is implemented using a CNN, owing to its powerful feature extraction capabilities [14]. The designed classifier has L = 4 layers with three convolution layers stacked with a fully connected layer. Each convolution layer is followed by the rectified linear unit (ReLU) activation to provide non-linearity to the convolutional layers. The fully connected layer takes the flattened input vector of local information and outputs the softmax probability distribution. For a quick nearest neighbor search on DkNN, we use an LSH from the FALCONN Python library [10] and employ a grid parameter search to configure the number of neighbors to k = 10. The hyperparameters of the DkNN beam classifier are summarized in Table II."}, {"title": "C. Performance Evaluation", "content": "For comparison purposes, we consider the following bench- mark methods: 1) the upper bound beamforming based on the SVD of perfectly known channels under 4-bit quantized phase shifter [15]; 2) the DFT-based codebook scanning directions with NBS candidate beams at the BS [7]; and 3) the O-DFT codebook [16] with an oversampling factor (OS) = 4 and NBSX4 candidate beams at the BS. It is worth mentioning that the SVD upper bound can only be reached when each user's perfect channel is known at the BS. To evaluate the explainability and robustness of the proposed DkNN-based beam classifier, we compare its credibility estimates with the outputs of a standard softmax classifier, typically interpreted as model's confidence estimates[9], with both classifiers using the same DNN architecture.\nTo assess the optimal beam alignment accuracy, we use the top-k accuracy metric, defined as the proportion of test samples where the optimal beam index falls within the top k predicted beams.\n1) Measurement Noise Analysis: The accuracy of beam prediction is influenced by the measured power of the beam- forming signals, where noise in these signals can significantly affect beam alignment performance. We consider two different cases: 1) DkNN trained without measurement noise (DkNNwo) where the CNN model is trained with no noise present in its training data, but it is then exposed to noisy signal during the testing/validation stage; 2) DkNN trained with measurement noise (DkNNwn) where the CNN model is trained with the expected measurement noise and is then deployed in a network with the expected noise distribution.\nFig. 2 compares the accuracy performance of different schemes across different SNR values. In our analysis, the noise power ranges considered while generating the training data are between -28dBm to -90dBm. It is evident that DkNNwn main- tains higher accuracy across expected noise levels compared to DkNNwo. In the NLOS environment, the DKNN trained for expected noise maintains top-5 and top-3 accuracy above 92% until SNR drops below -35 dB, while DkNNwo sees a more significant decline for higher noise levels. The reason for the improved performance of the DkNN is that it can train the neural network with the received signal containing channel measurement noise and is more robust at low SNR values. In comparison, the O-DFT (x4) codebook-based solution is less robust against noise, with a steady decrease in accuracy below -10dB SNR. On the other hand, the classical DFT codebook solution shows minimal adaptability, remaining below 45% accuracy across all SNR levels, illustrating its limitations in dynamic environments and susceptibility to measurement noise.\nFig. 3 illustrates the spectral efficiency versus SNR for various beam alignment methods. The proposed DkNNwn- based algorithm exhibits resilience against measurement noise and achieves approximately 98.5% of the 128-DFT codebook performance in terms of spectral efficiency at all SNR values while reducing the beam training overhead by ~ 75% com- pared to the O-DFT codebook solution and significantly out- performing classical DFT codebook based solution. Compared to the O-DFT codebook-based solutions, which require an exhaustive beam search over 128 beams, the proposed method requires beam sweeping with only a subset of N\u00dfs beams and an additional top-k predicted beams.\n2) Explainability and Robustness Evaluation: We evaluate the explainability and robustness of the proposed DkNN-based beam classifier by comparing its prediction credibility to that of the softmax classifier. The softmax classifier estimates confidence using output probability distributions but lacks explainability and robustness. In contrast, the DkNN provides interpretability through nearest neighbors, offering human- understandable explanations for intermediate computations at each layer, making it a valuable debugging tool. We show that the softmax classifier is poorly calibrated and overestimates confidence when predicting out-of-distribution inputs.\nFigure 4 presents the reliability diagrams for the DkNN and the naive softmax classifier. The distribution of credibil- ity/confidence values across the data is given by the number of data points in each credibility bin, reflected by the red line overlaid on the bars. The softmax classifier lacks calibration as it consistently exhibits high confidence on both test and adversarial data, making it ineffective in identifying outliers. Figure 4a demonstrates that the proposed DkNN classifier exhibits superior calibration by assigning low credibility to adversarial samples, effectively filtering outliers. It achieves 5x and 3.5\u00d7 robustness improvements at credibility thresholds of 0.2 and 0.4, respectively. Figure 4b illustrates the softmax classifier reliability diagram, which shows overconfidence, misclassifying more than 80% of adversarial examples with high confidence (>0.9). Note that because of the NLOS environment in the Boston-5G dataset, the test dataset contains many test inputs that are difficult to classify, reflected by the lower mean accuracy of the underlying CNN. Still, the DKNN recovers some accuracy on adversarial examples by leveraging representations from CNN's internal layers and, therefore, is better calibrated than its softmax equivalent: its reliability diagrams follow the linear relation between accuracy and CNN's credibility/confidence."}, {"title": "V. CONCLUSION", "content": "transmission. To improve the explainability and robustness of the trained BAE, we have proposed a model-agnostic DL-aided framework utilizing the DkNN algorithm, which inspects the internal representations of the model to evaluate how well their predictions conform with the training data, providing inter- pretable insights into beam selection and robustness against outlier inputs. Compared to the classical DFT codebook-based solutions, the proposed approach reduces beam training over- head by 75% while achieving near-optimal accuracy. More- over, the proposed DkNN-based algorithm effectively filters outlier inputs and provides interpretable insights rationalizing the beam prediction decisions, enhancing both explainability and robustness. Future works could involve benchmarking the proposed solution against other explainable AI methods and assessing the impact of different adversarial attacks on the robustness of beam prediction."}]}