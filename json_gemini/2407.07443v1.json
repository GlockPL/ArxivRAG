{"title": "Secondary Structure-Guided Novel Protein Sequence Generation with Latent Graph Diffusion", "authors": ["Yutong Hu", "Yang Tan", "Andi Han", "Lirong Zheng", "Liang Hong", "Bingxin Zhou"], "abstract": "The advent of deep learning has introduced efficient approaches for de novo protein sequence design, significantly improving success rates and reducing development costs compared to computational or experimental methods. However, existing methods face challenges in generating proteins with diverse lengths and shapes while maintaining key structural features. To address these challenges, we introduce CPDiffusion-SS, a latent graph diffusion model that generates protein sequences based on coarse-grained secondary structural information. CPDiffusion-SS offers greater flexibility in producing a variety of novel amino acid sequences while preserving overall structural constraints, thus enhancing the reliability and diversity of generated proteins. Experimental analyses demonstrate the significant superiority of the proposed method in producing diverse and novel sequences, with CPDiffusion-SS surpassing popular baseline methods on open benchmarks across various quantitative measurements. Furthermore, we provide a series of case studies to highlight the biological significance of the generation performance by the proposed method. The source code is publicly available at https://github.com/riacd/CPDiffusion-SS .", "sections": [{"title": "1. Introduction", "content": "Deep learning-based protein design provides an innovative and effective methodology, which promotes and creates novel or enhanced functionalities and physical properties of proteins varied from peptides to enzymes. Compared with traditional protein design approaches, such as directed evolution and rational design, deep learning-based protein design can significantly lower the human source, time, and financial cost (Chu et al., 2024) and create new proteins that do not exist in nature. Protein sequence is the foundation of protein structure and function, indicating that the sequence design is crucial for designing proteins with desired functions. There has been an increasing amount of work on designing protein sequences with deep generative models and validating the effectiveness of the designed protein products through bio-experiments (Ingraham et al., 2023; Zhou et al., 2023). These new techniques not only offer an opportunity to design novel protein sequences for a protein structure of interest, but also open a new way of designing proteins with significantly enhanced or novel functions for specific biological applications.\n The intricate connection between protein sequences and their functions remains largely unknown due to the vast high-dimensional space of protein sequences. Additionally, obtaining accurately labeled data that detail the sequence-function relationship presents a significant challenge. Thus, the sequence-based deep learning models are generated for finding the relationship between sequence and function. To enhance the generative capabilities, some autoregressive generative models have been developed that incorporate homologous wild-type proteins from closely related functional families or engage multiple sequence alignments. Including protein family data could direct the generated proteins to exhibit specified, desirable traits (Truong Jr & Bepler, 2024). Masked language models adopt a different approach by working with fragments of wild-type protein sequences and training the system to complete the remaining parts (Elnaggar et al., 2021; Lin et al., 2023). Even though protein language models have access to a wealth of sequence data to assimilate typical protein sequence patterns and to craft sequences with variable lengths, it remains a complex task to ensure an ample supply of homologous sequences for specific proteins (Rao et al., 2021). A notable shortcoming of these sequence-centric approaches is their tendency to neglect the vital structural features of proteins. These structural elements are critical since they largely dictate protein functionality. Without consideration of these three-dimensional attributes, the models may fail to fully capture the nuances of protein behavior and activity."}, {"title": "2. Related Work", "content": "Protein sequence generation typically seeks to achieve specific catalytic functions, often necessitating the integration of guiding principles or constraints from either the structural or sequence level to obtain the desired results. At the structural level, a common approach is to utilize a fixed protein backbone, such as the positions of amino acids (AAs) in three-dimensional space, and then output the appropriate AA type of each position, forming an AA sequence that is most likely to fold into the given structure (Hsu et al., 2022). This approach requires models capable of processing geometric structures, for example, using SE(3) equivariant neural networks to learn the geometric relationships between AAs (Satorras et al., 2021). Open benchmarks have validated these methods for their effectiveness in recovering AAs (Dauparas et al., 2022; Yi et al., 2024). Additionally, in some research, to demonstrate their models' effectiveness in generating desired proteins, wet lab experiments are conducted (Zhou et al., 2023). Beyond protein inverse folding, other conditional protein sequence generation tasks require"}, {"title": "3. CPDIFFUSION-SS: Secondary Structure-Guided Conditional Latent Protein Diffusion", "content": "In this section, we introduce the problem formulation to our research questions and propose our solution to it, i.e., CPDIFFUSION-SS. The notations used in this study is summarized in Table 1."}, {"title": "3.1. Problem Formulation", "content": "Our study aims to generate AA sequences with secondary structure constraints. Relevant notations can be defined as follows: Let P(Atype, Stype, Acoord) denote an arbitrary protein of n AAs, where the two sequences Atype = (a1,...,an) and Stype = (81,...,Sn) represent labels for AA types and secondary structure types, respectively. Acoord is the coordinates of each AA in the three-dimensional Euclidean space.\n The secondary structures (SS) are organized into a graph that illustrates the relationships between them within a protein, denoted as Gs(Xtype, Xcoord, E). Here Xtype denotes the sequence of SS type, which can be helix (H), sheet (E), or coil (C). Xcoord is the coordinates of secondary structures, which is calculated by averaging all AA coordinates within each secondary structure. For instance, in the visualized protein (PDB ID: 1Z25) in Figure 1, the highlighted node represents a helix structure, containing 7 AAs in the wild-type template. Suppose the structure is located at j-th position in Xtype and the 7 corresponding AAs are located sequentially starting from the i-th position in Atype, then the coordinates are computed as Xcoord;j = mean(Acoord;i,..., Acoord;i+6). Then, the SSs are connected to their k nearest neighbor in the Euclidean space, with edge features E encoding the Euclidean distance between each SS pair.\n The objective is to train a conditional generative model go() which generates the desired AA sequence \u0303A = (\u00e31,..., \u00e3n'), where n' does not necessarily equals n, i.e.,\n\u0303A = go (Gs (Xtype, Xcoord, E)) . \\tag{1}\n The major challenge is that only coarse information about the desired structures is provided. Conventional protein language models and inverse folding methods are inadequate: protein language models cannot directly constrain the structure, and inverse folding methods require precise structural inputs, including the exact number and positions of all AAs. To address this, we propose CPDIFFUSION-SS, a secondary structure-guided conditional latent protein diffusion method for approximating go (.)."}, {"title": "3.2. Model Architecture", "content": "CPDIFFUSION-SS comprises three components: a sequence encoder, a latent diffusion generator, and an autoregressive decoder. The encoder and decoder form a variational auto-encoder. The sequence encoder embeds amino acid (AA) sequences into a latent space characterized by secondary structure-level (SS-level) representations, while the decoder translates these SS-level latent representations back to the AA space. Both the encoder and decoder use protein language models for sequence embedding and reconstruction. The central component, a latent graph diffusion model, generates diverse SS-level hidden representations within the latent space conditioned on SS input. Below, we detail the construction of each module."}, {"title": "3.2.1. ENCODER-DECODER", "content": "Encoder For a protein P including n AAs and m SSs,\nthe encoder converts the discrete input AA sequence\n(a1,..., an) into a continuous representation sequence\n(h1,..., hm) using a protein language model and an at-\ntention pooling module. The pre-trained protein language\nmodel initially maps the AA sequences of proteins to AA-\nlevel vector representations Z = [Z1,..., Zn]. In this\nprocess, we utilize an evolutionary-scale protein language\nmodel (Lin et al., 2023) to effectively analyze the struc-\ntural and functional characteristics of proteins, employing\na masked language model training objective (Devlin et al.,\n2018), i.e.,\nLMLM :=\u2211i\u2208Mlog (P(ai|A\\M)),\nwhere A\\M represents the masked AA sequence obtained\nfrom Atype. To obtain secondary structure (SS)-level\nrepresentations, we utilize an attention pooling module\n(Yang et al., 2023), which aggregates amino acid (AA)-\nlevel representations H = [h1,h2,..., hm]. Using Xtype, we rearrange Z into m\ngroups:\nZ = [Z1,..., Zm] = [[Z1,..., Zn1],..., [Zn-nm+1,..., Zn]] ,\nwith ni being the number of AA in the i-th secondary struc-\nture, i.e., \u03a3i=1m ni = n. For the k-th (1 \u2264 k \u2264 m) sec-\nondary structure, the corresponding latent embedding hk is\nsummarized from Zk by\nhk = AttnPool(Zk) = softmax(Conv(Zk)). Zk, \\tag{2}\nwhere Conv() represents a 1-dimensional convolution\nalong the dimension of the AA sequence and calculates\nthe weighted average of AA embeddings within the same\nsecondary structure.\nDecoder The decoder converts the diffusion-generated\nSS-level representation (introduced in the following section)\nH = (h1,..., hm) into \u0303A = (\u00e31,..., \u00e3n'). To generate\nAA sequences of varying lengths, an autoregressive model\nwith multi-layer cross-attention is employed (Vaswani et al.,\n2017). The learning objective is structured as a sequence translation task. For training, the SS-level hidden representation (h1,..., hm) from the encoder is used. This continuous representation is fed into the decoder as context vectors, guiding the reconstruction of the AA sequence. The decoder's training target is to minimize the KL divergence.\nmin \u2211a\u017c\u2208A DKL (ai||Decoder(Encoder(A), a<i)). \\tag{3}\nRotary Position Embedding (RoPE) (Su et al., 2024) is\napplied for positional encoding of the AA sequences, en-\nhancing the model's ability to effectively capture positional\ninformation.\nIn summary, the encoder-decoder mechanism facilitates the\nmapping between AA-level protein sequences and SS-level\nlatent space. We utilize an evolutionary model to proficiently\nperform sequence embedding and train an autoregressive\ndecoder for translating AA sequences of varying lengths. To\nbetter align with secondary structure conditions and enrich\nthe diversity of generated outcomes, we incorporate latent\ngraph diffusion to generate SS-level vector representations."}, {"title": "3.2.2. LATENT DIFFUSION", "content": "For generating SS-level latent representations, we adhere\nto the standard pipeline of the denoising diffusion proba-\nbilistic model (Ho et al., 2020) within the latent space. For\neach protein AA sequence, we extract its secondary em-\nbeddings from the pre-trained encoder, denoted as H =\n[h1, h2, ..., hm]. The diffusion model is trained to gener-\nate representations of protein sequences that adhere to the\nsecondary structure properties. The architecture for the\ndenoising model is visualized in Figure 2.\nFollowing the denoising diffusion probabilistic model\n(DDPM), the forward diffusion process gradually adds Gaus-\nsian noise to the input embeddings over steps 0 \u2192 T. The"}, {"title": "3.3. Model Pipeline", "content": "Data Preparation CPDIFFUSION-SS utilizes two types\nof protein data as model input: the AA-level protein repre-\nsentation P(Atype, Stype, Acoord) and the SS-level graph\nrepresentation G5(Xtype, Xcoord, E; H), as previously dis-\ncussed in Section 3.1. For P, both Atype and Acoord are di-\nrectly obtained from structure-informed protein documents,\nsuch as PDB. The secondary structure Stype is assigned\nusing DSSP (Touw et al., 2015). Proteins with more than\n100 AAs in a single secondary structure are excluded, as\nthey are believed to be problematic or irregularly dominated\nby loops. The processed AA-level data P is used solely for training purposes. In contrast, SS-level information Gs is used for both training and inference. Constructing the associated graph representation requires additional data pro- cessing steps. Specifically, the SS-level graph for a protein is defined with each node representing a secondary struc- ture, labeled using one-hot encoding for its class (H, E, or C). Additionally, each secondary structure has a 1280- dimensional hidden representation H from the encoder that describes its AA compositions. During inference, this fea- ture is generated by latent diffusion and represented as \u0303H. The three-dimensional coordinate Xcoord is defined as the average position of all AAs (determined by the C\u03b1 atom) within it. Following the convention for constructing protein graphs, the connections in Gs are defined using k-nearest neighbor (kNN) graphs, with k = 3 based on the fact that secondary structures are less closely related than AAs. The edge matrix E is weighted by the fraction of the Euclidean distance between connected node pairs.\n Model Training and Inferencing CPDIFFUSION-SS undergoes a two-stage training process, with separate training phases for the encoder-decoder module and the latent dif- fusion module. For the encoder-decoder, we utilize the pre-trained ESM2-650M (Lin et al., 2023) and train our Transformer-style decoder to minimize the objective func- tion described in (3). This model is trained on a subset of the AlphaFoldDB (Barrio-Hernandez et al., 2023)1 clustered by FoldSeek (Van Kempen et al., 2024), which includes over 2 million wild-type proteins with ALPHAFOLD2 predictions. In the second stage, we freeze the trained encoder-decoder and train the latent graph diffusion model to reconstruct the latent secondary structure representation \u0303H. Given that the performance of the latent graph diffusion is closely related to protein structure, we train the model using CATH4.3 (Sillitoe et al., 2021), which provides over 30,000 protein domain structures with less than 40The inference process be- gins with the latent diffusion model, which uses the provided secondary structure graph and a randomly generated noise representation \u0303HT. It then proceeds through the denoising process using EGNN layers to generate latent representa- tions conditioned on the specified input secondary structure. Subsequently, the sampled \u0303H is fed into the trained de- coder to translate each secondary structure representation into explicit AA sequences."}, {"title": "4. Experiments", "content": "4.1. Experimental Protocol"}, {"title": "Generation Task", "content": "The models are evaluated through a secondary structure-based protein sequence generation task. We use 50 randomly selected structure templates from CATH4.3 for validation. These 50 test templates are ex- cluded from the training set to ensure unbiased evaluation. For each template, 200 sequences are generated and assessed based on their structures predicted by ESMFOLD2. Models are provided with the secondary structure and the minimum essential additional data required for each specific baseline model.\n Specifically, for any given template structure, CPDIFFUSION-SS receives an SS-level graph featur- ing secondary structure labels. Structure-based models (PROTEINMPNN and ESM-IF1) receive AA-level graph representations, where amino acids (AAs) within the same secondary structure are positioned at the group's center. Alternatively, sequence-based methods (ESM2 (Lin et al., 2023) and PROSTT5 (Raffel et al., 2020)) are given a small set of unmasked AA tokens. Both PROSTT5 and ESM2 (1) obtain a randomly selected unmasked AA token in each secondary structure, while ESM2 (0.8) and ESM2 (0.6) are supplied with randomly selected 20% and 40% unmasked AAs from the wild-type protein, respectively. We exclude a comparison with the model described in (Ni et al., 2023) due to the unavailability of the model implementation's checkpoint.\n Training Setup For the encoder module, we utilize ESM- 650, followed by a convolutional 1D-attention mechanism. The input channel for the convolution operator is set to 1280 (the output dimension of ESM2-650M), with the output channel being 1 and a kernel size of 1. In the latent graph diffusion module, we employ 4 EGNN layers as the denois- ing layers. The hidden and embedding dimensions are set to 640 and 1280, respectively. We use the sqrt noise sched- ule, with a learning rate of 5 \u00d7 10-4 and a weight decay of 10-5. For the decoder, we incorporate 3 Transformer layers, each with 8 attention heads and hidden dimensions of 4960 in the feed-forward network. All implementations are programmed using PyTorch Geometric (version 2.4.0) (Fey & Lenssen, 2019) and PyTorch (version 2.2). The training is conducted on 8 NVIDIA\u00ae Tesla A800 GPUs, each with 80GB HBM2, mounted on an HPC cluster. To ensure reproducibility, all details required to replicate our results are included in the submission."}, {"title": "4.2. Evaluation Measurements", "content": "Diversity assesses the variance of generated amino acid (AA) sequences from the same template structure. We evaluate the diversity of the generated results at both the sequence and structure levels by comparing the pairwise similarity of all generated sequences and reporting the average scores. For sequence-level evaluation, we calculate the AA sequence identity, expressed as a percentage. For structure-level evaluation, we use TM-score and RMSD (Root Mean Square Deviation), both calculated using TM-align (Zhang & Skolnick, 2005). These metrics are crucial as we aim for generated sequences from the same template to exhibit significant differences. Thus, we prefer models that generate sequences with lower average sequence identity, lower average TM-score, and higher average RMSD. In Table 2, these measurements are denoted as Seq. ID, TM new, and RMSD, respectively.\n Novelty evaluates whether the structures of generated proteins significantly differ from existing wild-type proteins. Maximizing novelty is a common design objective in de novo protein design (Watson et al., 2023; Yim et al., 2024). For efficient protein structure comparison, we use Foldseek (Van Kempen et al., 2024) to examine the alignment of the generated protein structures (predicted by ESMFold) with those in the training set. We report the TM-score between the most similar wild-type protein and the generated protein. In this context, a lower TM-score indicates higher novelty, which is desirable. The novelty evaluation is reported under TM wt. We provide the average TM-scores for all the proteins generated from the test templates.\n Consistency evaluates the alignment between the input secondary structure conditions and the predicted secondary structure of generated proteins. This is measured from two perspectives: SS-level sequence identity and structure composition. Similar to AA-level sequence identity, SS-level sequence identity is computed by aligning sequences and calculating the proportion of matched tokens. The best- aligned sequence is obtained by maximizing the alignment length corresponding to the secondary structure sequence alignment, using a penalty mechanism for mismatches and gaps. This follows the definition of AA sequence identity in BLAST (Altschul et al., 1990) for global sequence compar- ison, where identity = (Matches/AlignmentLength) \u00d7 100%. The alignment length includes the total number of tokens for matches, gaps, and mismatches. Secondary struc- ture composition calculates the proportions of helices (H), sheets (E), and coils (C) in both the input condition and the generated sequences. It then employs the Mean Squared Er- ror (MSE) measure to quantify their differences. The three introduced metrics are reported in Table 2 as ID, ID max, and MSE SS Composition. Additionally, since sheets and helices are generally considered more important and harder to generate than loops (coils), and their structures are more fixed, we also report the consistency score after removing loops as a reference."}, {"title": "4.3. Results Analysis", "content": "The generative performance scores are presented in Table 2, where we assess our proposed CPDIFFUSION-SS against both sequence and structure-based baseline methods across 10 evaluation metrics focusing on the diversity, novelty, and consistency of the generated sequences. In this evaluation, CPDIFFUSION-SS outperforms baseline methods on 9 out of the 10 metrics, except for TM new, where language models generally exhibit superior performance compared to structure-aware models. This disparity can be attributed to language models not explicitly integrating structural information, thus allowing for more unrestricted sequence generation. For instance, sequences generated by PROSTT5 for all three examined templates frequently exhibit repetitive patterns of certain amino acids, such as Glycine, Leucine, and Isoleucine. However, these amino acids are commonly found in all proteins for backbone stabilization and lack specificity to individual proteins. Moreover, such sequences are highly improbable to occur naturally, leading to lower sequence identity scores and TM scores.\n Additionally, language models tend to produce longer sequences compared to structure-constrained models like ESM-if1 and CPDIFFUSION-SS. This observation is evident in Figure 4, where ProstT5 generates significantly longer sequences compared to both baseline methods and the wild-type templates.\n Furthermore, we analyze the learning curve of the trained model and compare it with other hyperparameter configu- rations, as illustrated in Figure 3. All curves are visualized using wandb with moving average smoothing for better clarity. Both training and validation curves rapidly converge to a stable state after a reasonable number of training steps. To justify our choice of hyperparameters and architectures, we compare the learning curve with different pooling meth- ods (average pooling and attention pooling), learning rates, and dropout rates for the encoder-decoder, as well as noise schedules (sqrt, linear, and cosine) for the latent diffusion model."}, {"title": "4.4. Case Study", "content": "To demonstrate CPDIFFUSION-SS's efficacy in using sec- ondary structures for protein sequence generation, we con- ducted experiments generating novel sequences guided by specific secondary structures. We selected protein structures shown in Figure 4(a) as constraints. We then compared the structures of sequences generated by CPDIFFUSION-SS, PROSTT5, ESM-IF1, and PROTEINMPNN under the same conditions. Sequences from CPDIFFUSION-SS fold into plausible protein structures with similar secondary struc- ture compositions to the wild-type template. In contrast, sequences from PROSTT5, ESM-IF1, and PROTEINMPNN deviate significantly from the secondary structural condi- tions. Notably, PROSTT5 generates sequences much longer than the template, and PROTEINMPNN produces sequences forming only random coils, unlikely to fold into functional proteins."}, {"title": "5. Conclusion and Discussion", "content": "This study introduces a novel protein generation model guided by secondary structures, crucial elements for protein functionality. Leveraging powerful protein language models and latent graph diffusion models, we develop one of the first deep learning frameworks capable of generating diverse and reliable sequences conditioned on specific secondary structures.\n Our experimental findings underscore CPDIFFUSION-SS's ability to generate proteins with target structures while adhering to secondary structure constraints. This capability holds significant implications for protein design and protein- based biotechnology. Structural flexibility, crucial for protein stability and activity, is intricately linked to secondary structure. Proteins often encounter challenges in industrial applications within extreme environments like strong acids, bases, or high temperatures due to structural instability. CPDIFFUSION-SS offers a solution by introducing new helices and sheets on the protein surface, compacting the protein and enhancing its resistance to extreme conditions (Zheng et al., 2022). Additionally, the flexibility of a protein's catalytic pocket profoundly influences its bioactivity (Zheng et al., 2022). By employing CPDIFFUSION-SS to increase loops and turns around catalytic sites, conformational changes can be facilitated during biofunctions, thereby enhancing catalytic activity."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of protein de novo design. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}]}