{"title": "Secondary Structure-Guided Novel Protein Sequence Generation with Latent Graph Diffusion", "authors": ["Yutong Hu", "Yang Tan", "Andi Han", "Lirong Zheng", "Liang Hong", "Bingxin Zhou"], "abstract": "The advent of deep learning has introduced efficient approaches for de novo protein sequence design, significantly improving success rates and reducing development costs compared to computational or experimental methods. However, existing methods face challenges in generating proteins with diverse lengths and shapes while maintaining key structural features. To address these challenges, we introduce CPDiffusion-SS, a latent graph diffusion model that generates protein sequences based on coarse-grained secondary structural information. CPDiffusion-SS offers greater flexibility in producing a variety of novel amino acid sequences while preserving overall structural constraints, thus enhancing the reliability and diversity of generated proteins. Experimental analyses demonstrate the significant superiority of the proposed method in producing diverse and novel sequences, with CPDiffusion-SS surpassing popular baseline methods on open benchmarks across various quantitative measurements. Furthermore, we provide a series of case studies to highlight the biological significance of the generation performance by the proposed method. The source code is publicly available at https://github.com/riacd/CPDiffusion-SS .", "sections": [{"title": "1. Introduction", "content": "Deep learning-based protein design provides an innovative and effective methodology, which promotes and creates novel or enhanced functionalities and physical properties of proteins varied from peptides to enzymes. Compared with traditional protein design approaches, such as directed evolution and rational design, deep learning-based protein design can significantly lower the human source, time, and financial cost (Chu et al., 2024) and create new proteins that do not exist in nature. Protein sequence is the foundation of protein structure and function, indicating that the sequence design is crucial for designing proteins with desired functions. There has been an increasing amount of work on designing protein sequences with deep generative models and validating the effectiveness of the designed protein products through bio-experiments (Ingraham et al., 2023; Zhou et al., 2023). These new techniques not only offer an opportunity to design novel protein sequences for a protein structure of interest, but also open a new way of designing proteins with significantly enhanced or novel functions for specific biological applications.\n\nThe intricate connection between protein sequences and their functions remains largely unknown due to the vast high-dimensional space of protein sequences. Additionally, obtaining accurately labeled data that detail the sequence-function relationship presents a significant challenge. Thus, the sequence-based deep learning models are generated for finding the relationship between sequence and function. To enhance the generative capabilities, some autoregressive generative models have been developed that incorporate homologous wild-type proteins from closely related functional families or engage multiple sequence alignments. Including protein family data could direct the generated proteins to exhibit specified, desirable traits (Truong Jr & Bepler, 2024). Masked language models adopt a different approach by working with fragments of wild-type protein sequences and training the system to complete the remaining parts (Elnaggar et al., 2021; Lin et al., 2023). Even though protein language models have access to a wealth of sequence data to assimilate typical protein sequence patterns and to craft sequences with variable lengths, it remains a complex task to ensure an ample supply of homologous sequences for specific proteins (Rao et al., 2021). A notable shortcoming of these sequence-centric approaches is their tendency to neglect the vital structural features of proteins. These structural elements are critical since they largely dictate protein functionality. Without consideration of these three-dimensional attributes, the models may fail to fully capture the nuances of protein behavior and activity."}, {"title": "2. Related Work", "content": "Conditional Protein Sequence Generation Protein sequence generation typically seeks to achieve specific catalytic functions, often necessitating the integration of guiding principles or constraints from either the structural or sequence level to obtain the desired results. At the structural level, a common approach is to utilize a fixed protein backbone, such as the positions of amino acids (AAs) in three-dimensional space, and then output the appropriate AA type of each position, forming an AA sequence that is most likely to fold into the given structure (Hsu et al., 2022). This approach requires models capable of processing geometric structures, for example, using SE(3) equivariant neural networks to learn the geometric relationships between AAs (Satorras et al., 2021). Open benchmarks have validated these methods for their effectiveness in recovering AAs (Dauparas et al., 2022; Yi et al., 2024). Additionally, in some research, to demonstrate their models' effectiveness in generating desired proteins, wet lab experiments are conducted (Zhou et al., 2023). Beyond protein inverse folding, other conditional protein sequence generation tasks require different inputs, such as protein function (Kucera et al., 2022), protein family (Repecka et al., 2021), and secondary structure (Xie et al., 2023). Although there have been some methods that attempt to incorporate secondary structures for conditional sequence generation, these methods often have limitations, such as being unable to generate sequences of varying AA length (Ni et al., 2023) or secondary structures w fixed order (Ingraham et al., 2023).\n\nProtein Language Model Protein language models (PLMs) have been a hot spot in the field of AI-assisted protein design. PLMs are trained in a self-supervised manner and utilize extensive amino acid (AA) sequences to extract reliable AA representations, which are valuable for various downstream tasks, such as protein folding (Lin et al., 2023) and variant effect prediction (Tan et al., 2023; Truong Jr & Bepler, 2024). There are two prevalent types of PLMs: masked language models and autoregressive models. Masked language models are inspired by BERT (Devlin et al., 2018). These models are trained to predict masked AAs within the context of surrounding unmasked tokens. This approach is exemplified by models like ESM-1b (Rives et al., 2021). To improve the model's understanding of sequence characteristics, additional information including evolutionary data from multiple sequence alignments (MSA) (Rao et al., 2021) or functional annotations (Brandes et al., 2022) can be incorporated. Autoregressive models share an architecture similar to GPT-2 (Radford et al., 2019), generating protein sequences of varying lengths without conditioning (Nijkamp et al., 2023). Some PLMs are based on T5 (Raffel et al., 2020), such as ProtT5 (Elnaggar et al., 2021) and ProstT5 (Heinzinger et al., 2023)."}, {"title": "3. CPDIFFUSION-SS: Secondary Structure-Guided Conditional Latent Protein Diffusion", "content": "In this section, we introduce the problem formulation to our research questions and propose our solution to it, i.e., CPDIFFUSION-SS. The notations used in this study is summarized in Table 1."}, {"title": "3.1. Problem Formulation", "content": "Our study aims to generate AA sequences with secondary structure constraints. Relevant notations can be defined as follows: Let P(Atype, Stype, Acoord) denote an arbitrary protein of n AAs, where the two sequences Atype = (a1,...,an) and Stype = (81,...,Sn) represent labels for AA types and secondary structure types, respectively. Acoord is the coordinates of each AA in the three-dimensional Euclidean space.\n\nThe secondary structures (SS) are organized into a graph that illustrates the relationships between them within a protein, denoted as Gs(Xtype, Xcoord, E). Here Xtype denotes the sequence of SS type, which can be helix (H), sheet (E), or coil (C). Xcoord is the coordinates of secondary structures, which is calculated by averaging all AA coordinates within each secondary structure. For instance, in the visualized protein (PDB ID: 1Z25) in Figure 1, the highlighted node represents a helix structure, containing 7 AAs in the wild-type template. Suppose the structure is located at j-th position in Xtype and the 7 corresponding AAs are located sequentially starting from the i-th position in Atype, then the coordinates are computed as $Xcoord;j = mean(Acoord;i,..., Acoord;i+6)$. Then, the SSs are connected to their k nearest neighbor in the Euclidean space, with edge features E encoding the Euclidean distance between each SS pair.\n\nThe objective is to train a conditional generative model go(\u00b7) which generates the desired AA sequence A = (\u1fb61,..., \u0101n'), where n' does not necessarily equals n, i.e.,\n$A = 90 (Gs (Xtype, Xcoord, E)) . \t(1)$\n\nThe major challenge is that only coarse information about the desired structures is provided. Conventional protein language models and inverse folding methods are inadequate: protein language models cannot directly constrain the structure, and inverse folding methods require precise structural inputs, including the exact number and positions of all AAs. To address this, we propose CPDIFFUSION-SS, a secondary structure-guided conditional latent protein diffusion method for approximating go (.)."}, {"title": "3.2. Model Architecture", "content": "CPDIFFUSION-SS comprises three components: a sequence encoder, a latent diffusion generator, and an autoregressive decoder. The encoder and decoder form a variational auto-encoder. The sequence encoder embeds amino acid (AA) sequences into a latent space characterized by secondary structure-level (SS-level) representations, while the decoder translates these SS-level latent representations back to the AA space. Both the encoder and decoder use protein language models for sequence embedding and reconstruction. The central component, a latent graph diffusion model, generates diverse SS-level hidden representations within the latent space conditioned on SS input. Below, we detail the construction of each module."}, {"title": "3.2.1. ENCODER-DECODER", "content": "Encoder For a protein P including n AAs and m SSs, the encoder converts the discrete input AA sequence (a1,..., an) into a continuous representation sequence (h1,..., hm) using a protein language model and an attention pooling module. The pre-trained protein language model initially maps the AA sequences of proteins to AA-level vector representations Z = [Z1,..., Zn]. In this process, we utilize an evolutionary-scale protein language model (Lin et al., 2023) to effectively analyze the structural and functional characteristics of proteins, employing a masked language model training objective (Devlin et al., 2018), i.e.,\n$\\LMLM := \\sum_i log (P(a|A\\\u043c))$,\nwhere A\\M represents the masked AA sequence obtained from Atype. To obtain secondary structure (SS)-level representations, we utilize an attention pooling module (Yang et al., 2023), which aggregates amino acid (AA)-level representations Z into SS-level representations H = [h1,h2,..., hm]. Using Xtype, we rearrange Z into m groups:\nZ = [Z1,..., Zm] = [[Z1,..., Zn1],..., [Zn-nm+1,..., Zn]] ,\nwith ni being the number of AA in the i-th secondary structure, i.e., $\\\u03a3_i n_i = n$. For the k-th (1 \u2264 k \u2264 m) secondary structure, the corresponding latent embedding hk is summarized from Zk by\nhk = AttnPool(Zk) = softmax(Conv(Zk)). Zk, \t(2)\nwhere Conv() represents a 1-dimensional convolution along the dimension of the AA sequence and calculates the weighted average of AA embeddings within the same secondary structure.\n\nDecoder The decoder converts the diffusion-generated SS-level representation (introduced in the following section) H = (h1,..., hm) into A = (\u0101\u2081,\u2026\u2026\u2026, \u0101n'). To generate AA sequences of varying lengths, an autoregressive model with multi-layer cross-attention is employed (Vaswani et al., 2017). The learning objective is structured as a sequence translation task. For training, the SS-level hidden representation (h\u2081,..., hm) from the encoder is used. This continuous representation is fed into the decoder as context vectors, guiding the reconstruction of the AA sequence. The decoder's training target is to minimize the KL divergence.\n$\\min \\sum_\\a\u017c \\DKL (ai||Decoder(Encoder(A), a<\u2170)). \t(3)$\nRotary Position Embedding (RoPE) (Su et al., 2024) is applied for positional encoding of the AA sequences, enhancing the model's ability to effectively capture positional information.\n\nIn summary, the encoder-decoder mechanism facilitates the mapping between AA-level protein sequences and SS-level latent space. We utilize an evolutionary model to proficiently perform sequence embedding and train an autoregressive decoder for translating AA sequences of varying lengths. To better align with secondary structure conditions and enrich the diversity of generated outcomes, we incorporate latent graph diffusion to generate SS-level vector representations."}, {"title": "3.2.2. LATENT DIFFUSION", "content": "For generating SS-level latent representations, we adhere to the standard pipeline of the denoising diffusion probabilistic model (Ho et al., 2020) within the latent space. For each protein AA sequence, we extract its secondary embeddings from the pre-trained encoder, denoted as H = [h1, h2, ..., hm]. The diffusion model is trained to generate representations of protein sequences that adhere to the secondary structure properties. The architecture for the denoising model is visualized in Figure 2.\n\nFollowing the denoising diffusion probabilistic model (DDPM), the forward diffusion process gradually adds Gaussian noise to the input embeddings over steps 0 \u2192 T. The objective is to maximize the evidence lower bound, which is equivalent to minimizing the expected reconstruction loss\n$\\min Et, Ht || fo (Ht, t, Xcoord, Xtype, Gs) \u2013 H\u00b0||\n\u03b8\nwhere we ignore the weighting constants.\n\nTo incorporate conditions based on secondary structure information, we design the denoising neural network fo() using equivariant graph neural networks (Satorras et al., 2021). Each protein is represented as an SS-level graph Gs(Xtype, Xcoord, E), preserving the 3D geometric information of the secondary structures in Xcoord = [X1, ..., Xm]. As previously introduced, these coordinates are defined by the average Ca of the corresponding AAs within each secondary structure. In addition to the positions of the secondary structures, their types are encoded using one-hot encoding features Xtype.\n\nThe denoising network fo(\u00b7) is conditioned on the 3D positions of protein secondary structures, thus the predicted embeddings should be invariant to orthogonal transformations or translations of the input coordinates. This means that translating, reflecting, or rotating the input should result in equivalent transformations of the output. To achieve this, we use E(3) equivariant graph neural networks (EGNN) (Satorras et al., 2021) as the backbone for fo(.). EGNNS have proven effective for protein representation learning (Tan et al., 2023; Yi et al., 2024; Zhou et al., 2023). At the l-th layer, the hidden representation h+1 is updated by edge and position updates, followed by node aggregation:\n$\\mij l+1 = e(h_i, h_j, ||xi - xj||2, eij)$\n$xi l+1 = xi l + \\sum_j (x_j \u2013 xi)x_i(x_i)$\n$h_i l+1 = h(h_i, \\sum_j mij)$,\nwhere e() and h(\u00b7) are the edge and node propagation functions, respectively, and eij represents the edge feature between nodes i and j."}, {"title": "3.3. Model Pipeline", "content": "Data Preparation CPDIFFUSION-SS utilizes two types of protein data as model input: the AA-level protein representation P(Atype, Stype, Acoord) and the SS-level graph representation G5(Xtype, Xcoord, E; H), as previously discussed in Section 3.1. For P, both Atype and Acoord are directly obtained from structure-informed protein documents, such as PDB. The secondary structure Stype is assigned using DSSP (Touw et al., 2015). Proteins with more than 100 AAs in a single secondary structure are excluded, as they are believed to be problematic or irregularly dominated by loops. The processed AA-level data P is used solely for training purposes. In contrast, SS-level information G is used for both training and inference. Constructing the associated graph representation requires additional data processing steps. Specifically, the SS-level graph for a protein is defined with each node representing a secondary structure, labeled using one-hot encoding for its class (H, E, or C). Additionally, each secondary structure has a 1280-dimensional hidden representation H from the encoder that describes its AA compositions. During inference, this feature is generated by latent diffusion and represented as H. The three-dimensional coordinate Xcoord is defined as the average position of all AAs (determined by the Ca atom) within it. Following the convention for constructing protein graphs, the connections in Gs are defined using k-nearest neighbor (kNN) graphs, with k = 3 based on the fact that secondary structures are less closely related than AAs. The edge matrix E is weighted by the fraction of the Euclidean distance between connected node pairs.\n\nModel Training and Inferencing CPDIFFUSION-SS undergoes a two-stage training process, with separate training phases for the encoder-decoder module and the latent diffusion module. For the encoder-decoder, we utilize the pre-trained ESM2-650M (Lin et al., 2023) and train our Transformer-style decoder to minimize the objective function described in (3). This model is trained on a subset of the AlphaFoldDB (Barrio-Hernandez et al., 2023)\u00b9 clustered by FoldSeek (Van Kempen et al., 2024), which includes over 2 million wild-type proteins with ALPHAFOLD2 predictions. In the second stage, we freeze the trained encoder-decoder and train the latent graph diffusion model to reconstruct the latent secondary structure representation H. Given that the performance of the latent graph diffusion is closely related to protein structure, we train the model using CATH4.3 (Sillitoe et al., 2021), which provides over 30,000 protein domain structures with less than 40The inference process begins with the latent diffusion model, which uses the provided secondary structure graph and a randomly generated noise representation HT. It then proceeds through the denoising process using EGNN layers to generate latent representations conditioned on the specified input secondary structure. Subsequently, the sampled H is fed into the trained decoder to translate each secondary structure representation into explicit AA sequences."}, {"title": "4. Experiments", "content": "4.1. Experimental Protocol\n\nGeneration Task The models are evaluated through a secondary structure-based protein sequence generation task. We use 50 randomly selected structure templates from CATH4.3 for validation. These 50 test templates are excluded from the training set to ensure unbiased evaluation. For each template, 200 sequences are generated and assessed based on their structures predicted by ESMFOLD2. Models are provided with the secondary structure and the minimum essential additional data required for each specific baseline model.\n\nSpecifically, for any given template structure, CPDIFFUSION-SS receives an SS-level graph featuring secondary structure labels. Structure-based models (PROTEINMPNN and ESM-IF1) receive AA-level graph representations, where amino acids (AAs) within the same secondary structure are positioned at the group's center. Alternatively, sequence-based methods (ESM2 (Lin et al., 2023) and PROSTT5 (Raffel et al., 2020)) are given a small set of unmasked AA tokens. Both PROSTT5 and ESM2 (1) obtain a randomly selected unmasked AA token in each secondary structure, while ESM2 (0.8) and ESM2 (0.6) are supplied with randomly selected 20% and 40% unmasked AAs from the wild-type protein, respectively. We exclude a comparison with the model described in (Ni et al., 2023) due to the unavailability of the model implementation's checkpoint.\n\nTraining Setup For the encoder module, we utilize ESM-650, followed by a convolutional 1D-attention mechanism. The input channel for the convolution operator is set to 1280 (the output dimension of ESM2-650M), with the output channel being 1 and a kernel size of 1. In the latent graph diffusion module, we employ 4 EGNN layers as the denoising layers. The hidden and embedding dimensions are set to 640 and 1280, respectively. We use the sqrt noise schedule, with a learning rate of 5 \u00d7 10-4 and a weight decay of 10-5. For the decoder, we incorporate 3 Transformer layers, each with 8 attention heads and hidden dimensions of 4960 in the feed-forward network. All implementations are programmed using PyTorch Geometric (version 2.4.0) (Fey & Lenssen, 2019) and PyTorch (version 2.2). The training is conducted on 8 NVIDIA\u00ae Tesla A800 GPUs, each with 80GB HBM2, mounted on an HPC cluster. To ensure reproducibility, all details required to replicate our results are included in the submission."}, {"title": "4.2. Evaluation Measurements", "content": "Diversity assesses the variance of generated amino acid (AA) sequences from the same template structure. We evaluate the diversity of the generated results at both the sequence and structure levels by comparing the pairwise similarity of all generated sequences and reporting the average scores. For sequence-level evaluation, we calculate the AA sequence identity, expressed as a percentage. For structure-level evaluation, we use TM-score and RMSD (Root Mean Square Deviation), both calculated using TM-align (Zhang & Skolnick, 2005). These metrics are crucial as we aim for generated sequences from the same template to exhibit significant differences. Thus, we prefer models that generate sequences with lower average sequence identity, lower average TM-score, and higher average RMSD. In Table 2, these measurements are denoted as Seq. ID, TM new, and RMSD, respectively.\n\nNovelty evaluates whether the structures of generated proteins significantly differ from existing wild-type proteins. Maximizing novelty is a common design objective in de novo protein design (Watson et al., 2023; Yim et al., 2024). For efficient protein structure comparison, we use Foldseek (Van Kempen et al., 2024) to examine the alignment of the generated protein structures (predicted by ESMFold) with those in the training set. We report the TM-score between the most similar wild-type protein and the generated protein. In this context, a lower TM-score indicates higher novelty, which is desirable. The novelty evaluation is reported under TM wt. We provide the average TM-scores for all the proteins generated from the test templates.\n\nConsistency evaluates the alignment between the input secondary structure conditions and the predicted secondary structure of generated proteins. This is measured from two perspectives: SS-level sequence identity and structure composition. Similar to AA-level sequence identity, SS-level sequence identity is computed by aligning sequences and calculating the proportion of matched tokens. The best-aligned sequence is obtained by maximizing the alignment length corresponding to the secondary structure sequence alignment, using a penalty mechanism for mismatches and gaps. This follows the definition of AA sequence identity in BLAST (Altschul et al., 1990) for global sequence comparison, where identity = (Matches/AlignmentLength) \u00d7 100%. The alignment length includes the total number of tokens for matches, gaps, and mismatches. Secondary structure composition calculates the proportions of helices (H), sheets (E), and coils (C) in both the input condition and the generated sequences. It then employs the Mean Squared Error (MSE) measure to quantify their differences. The three introduced metrics are reported in Table 2 as ID, ID max, and MSE SS Composition. Additionally, since sheets and helices are generally considered more important and harder to generate than loops (coils), and their structures are more fixed, we also report the consistency score after removing loops as a reference."}, {"title": "4.3. Results Analysis", "content": "The generative performance scores are presented in Table 2, where we assess our proposed CPDIFFUSION-SS against both sequence and structure-based baseline methods across 10 evaluation metrics focusing on the diversity, novelty, and consistency of the generated sequences. In this evaluation, CPDIFFUSION-SS outperforms baseline methods on 9 out of the 10 metrics, except for TM new, where language models generally exhibit superior performance compared to structure-aware models. This disparity can be attributed to language models not explicitly integrating structural information, thus allowing for more unrestricted sequence generation. For instance, sequences generated by PROSTT5 for all three examined templates frequently exhibit repetitive patterns of certain amino acids, such as Glycine, Leucine, and Isoleucine. However, these amino acids are commonly found in all proteins for backbone stabilization and lack specificity to individual proteins. Moreover, such sequences are highly improbable to occur naturally, leading to lower sequence identity scores and TM scores.\n\nAdditionally, language models tend to produce longer sequences compared to structure-constrained models like ESM-if1 and CPDIFFUSION-SS. This observation is evident in Figure 4, where ProstT5 generates significantly longer sequences compared to both baseline methods and the wild-type templates.\n\nFurthermore, we analyze the learning curve of the trained model and compare it with other hyperparameter configurations, as illustrated in Figure 3. All curves are visualized using wandb with moving average smoothing for better clarity. Both training and validation curves rapidly converge to a stable state after a reasonable number of training steps. To justify our choice of hyperparameters and architectures, we compare the learning curve with different pooling methods (average pooling and attention pooling), learning rates, and dropout rates for the encoder-decoder, as well as noise schedules (sqrt, linear, and cosine) for the latent diffusion model."}, {"title": "4.4. Case Study", "content": "To demonstrate CPDIFFUSION-SS's efficacy in using secondary structures for protein sequence generation, we conducted experiments generating novel sequences guided by specific secondary structures. We selected protein structures shown in Figure 4(a) as constraints. We then compared the structures of sequences generated by CPDIFFUSION-SS, PROSTT5, ESM-IF1, and PROTEINMPNN under the same conditions. Sequences from CPDIFFUSION-SS fold into plausible protein structures with similar secondary structure compositions to the wild-type template. In contrast, sequences from PROSTT5, ESM-IF1, and PROTEINMPNN deviate significantly from the secondary structural conditions. Notably, PROSTT5 generates sequences much longer than the template, and PROTEINMPNN produces sequences forming only random coils, unlikely to fold into functional proteins."}, {"title": "5. Conclusion and Discussion", "content": "This study introduces a novel protein generation model guided by secondary structures, crucial elements for protein functionality. Leveraging powerful protein language models and latent graph diffusion models, we develop one of the first deep learning frameworks capable of generating diverse and reliable sequences conditioned on specific secondary structures.\n\nOur experimental findings underscore CPDIFFUSION-SS's ability to generate proteins with target structures while adhering to secondary structure constraints. This capability holds significant implications for protein design and protein-based biotechnology. Structural flexibility, crucial for protein stability and activity, is intricately linked to secondary structure. Proteins often encounter challenges in industrial applications within extreme environments like strong acids, bases, or high temperatures due to structural instability. CPDIFFUSION-SS offers a solution by introducing new helices and sheets on the protein surface, compacting the protein and enhancing its resistance to extreme conditions (Zheng et al., 2022). Additionally, the flexibility of a protein's catalytic pocket profoundly influences its bioactivity (Zheng et al., 2022). By employing CPDIFFUSION-SS to increase loops and turns around catalytic sites, conformational changes can be facilitated during biofunctions, thereby enhancing catalytic activity."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of protein de novo design. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}]}