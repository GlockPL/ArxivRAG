{"title": "Adversarial Domain Adaptation for Cross-user Activity Recognition Using Diffusion-based Noise-centred Learning", "authors": ["Xiaozhou Ye", "Kevin I-Kai Wang"], "abstract": "Human Activity Recognition (HAR) plays a crucial role in various applications such as human-computer interaction and healthcare monitoring. However, challenges persist in HAR models due to the data distribution differences between training and real-world data distributions, particularly evident in cross-user scenarios. This paper introduces a novel framework, termed Diffusion-based Noise-centered Adversarial Learning Domain Adaptation (Diff-Noise-Adv-DA), designed to address these challenges by leveraging generative diffusion modeling and adversarial learning techniques. Traditional HAR models often struggle with the diversity of user behaviors and sensor data distributions. Diff-Noise-Adv-DA innovatively integrates the inherent noise within diffusion models, harnessing its latent information to enhance domain adaptation. Specifically, the framework transforms noise into a critical carrier of activity and domain class information, facilitating robust classification across different user domains. Experimental evaluations demonstrate the effectiveness of Diff-Noise-Adv-DA in improving HAR model performance across different users, surpassing traditional domain adaptation methods. The framework not only mitigates distribution mismatches but also enhances data quality through noise-based denoising techniques.", "sections": [{"title": "I. INTRODUCTION", "content": "Human Activity Recognition (HAR) is a critical component of human-computer interaction, ubiquitous computing, and the Internet of Things. The primary objective of HAR is to accurately classify human activities using sensor data and contextual information [1]. Despite extensive research efforts, developing an effective HAR model remains challenging. A significant issue in current HAR methodologies, particularly those dealing with time series sensor data, is the presumption that training and testing data originate from identical distributions. This assumption is frequently invalidated by the diversity of real-world data and the occurrence of out-of-distribution (o.o.d.) instances [2]. This discrepancy, caused by factors such as sensor heterogeneity [3], evolving data patterns [4], varying sensor layouts [5], and individual behavioral differences [6], severely impairs the generalizability of HAR models. Our research specifically addresses the o.o.d. challenge in sensor-based HAR, focusing on the cross-user HAR problem arising from individual behavioral variances, commonly encountered in healthcare applications.\nTransfer learning and domain adaptation [7] present promising solutions for bridging the data distribution gap between different domains. In cross-user Human Activity Recognition (HAR), these approaches aim to leverage knowledge from a source domain (e.g., activities of one group of users) to enhance performance in a target domain (e.g., activities of a different group of users). Techniques such as Maximum Mean Discrepancy [8] and Optimal Transport [6] are utilized to align domain features, while more advanced methods like Deep Generative Domain Adaptation [9] and Deep Conditional Adaptation Network [10] focus on deeper, non-linear alignments. Recent advancements in generative modeling have incorporated diffusion models into domain adaptation tasks, showcasing their potential to bridge the gap between source and target domains. These models facilitate smooth, controlled transitions while preserving critical data features, as demonstrated by the Domain-Adaptive Diffusion method [11]. Moreover, their capacity to generate high-fidelity samples enhances accurate domain translation and feature adaptation, as evidenced by the Data Augmentation with Diffusion Models [12]. Moreover, IoT sensor data such as human activity recognition data often contains significant noise due to various environmental and operational factors. Diffusion models, which simulate the process of information spreading or diffusing through a system, are highly effective at noise reduction. They iteratively refine the sensor data, making it smoother and more accurate by filtering out random fluctuations and disturbances. This denoising capability enhances the quality of the data, leading to more reliable activity recognition [13]. By applying diffusion models, the integrity and quality of sensor data can be significantly improved for better domain adaptation.\nHowever, existing diffusion-based domain adaptation methods often neglect the crucial role of noise in improving domain adaptation, which can carry valuable information in both the forward diffusion process and the reverse diffusion denoising process. To address these overlooked aspects, we propose a novel approach that harnesses the potential of noise within diffusion models for domain adaptation, termed Diffusion-based Noise-centered Adversarial Learning Domain Adaptation (Diff-Noise-Adv-DA). Our method integrates noise-centered learning into the diffusion process, enabling the model to develop robust activity classification models that generalize effectively across different user domains by enhancing the utilization of noise. This method transforms the forward diffusion and reverse denoising processes into adversarial learning phases, aligning feature distributions between source"}, {"title": "1) Novel Deep Domain Adaptation Framework:", "content": "We propose a novel deep domain adaptation framework, Diff-Noise-Adv-DA, specifically designed for cross-user HAR. This framework integrates the concepts of adversarial learning and generative modeling in a unique manner to exploit the knowledge embedded in the diffusion model for better domain adaptation."}, {"title": "2) Innovative Generative Model Architecture:", "content": "We introduce an innovative generative model architecture, incorporating unique constraint conditions and network structures that facilitate data distribution alignment between source and target domains. This enhances the generalization capability of the diffusion model and its classification performance."}, {"title": "3) Noise-centered Adversarial Learning:", "content": "We develop a unique noise-centered adversarial learning mechanism that transforms the noise in both forward and reverse diffusion processes into critical information carriers for classification tasks. By integrating activity and domain class information into the noise, we enable the model to extract and leverage embedded information, significantly enhancing the robustness and adaptability of HAR models across different user domains."}, {"title": "II. RELATED WORK", "content": "Sensor-based Human Activity Recognition (HAR) has emerged as a key component in the field of ubiquitous computing, providing deep insights into human behaviors by analyzing sensor data and contextual clues. This technology employs a variety of sensor types, each suited to different contexts, and can be grouped into five primary categories: smartphones and wearables, ambient sensors, device-free sensing, vision-based systems, and miscellaneous sensor types, as indicated in several studies [2] [14]. Our research specifically focuses on the use of wearable sensors for HAR.\nIn machine learning, the problem of HAR using sensors is viewed as a challenge in time series classification [15]. Several classification techniques have been suggested, including Bayesian networks, Support Vector Machines, and Hidden Markov Models (HMM). Additionally, deep learning models have shown impressive results across many tasks. In the context of HAR, these deep learning methods [16] can automatically learn to identify and extract relevant features from extensive datasets. However, these methods typically rely on the assumption that both training and testing data share the same distribution, conforming to the independent and identically distributed (i.i.d.) assumption [2]. Yet, this assumption is often violated in practical applications, where training and testing data sets usually differ significantly, falling into what is known as out-of-distribution (o.o.d.) scenarios [17]. Our study explores the challenges of sensor-based HAR in these o.o.d. conditions."}, {"title": "A. Problem formulation", "content": "In the task of cross-user HAR domain adaptation, the term 'domain' refers to the 'user'. Domain adaptation, in this case, means transferring knowledge between different users.\nGiven a labelled source user $SSource=\\{(xSource_i, y Source_i)\\}_{i=1}^{nSource}$ drawn from a joint probability distribution $PSource$ and a target user $STarget=\\{(xTarget_i, y Target_i)\\}_{i=1}^{nTarget}$ drawn from a joint probability distribution $PTarget$, where $nSource$ and $nTarget$ are the number of source and target samples respectively. $SSource$ and $STarget$ have the same feature spaces (i.e. the set of features that describes the data from sensor readings) and label spaces (i.e. the set of activity classes). The source and target users have different distributions, i.e., $pSource \\neq PTarget$, which means that even for the same activity, the sensor readings look different between the two users. Given source user data $\\{(xSource_i, y Source_i)\\}_{i=1}^{nSource}$ and target user data $\\{(xTarget_i)\\}_{i=1}^{nTarget}$, the goal is to obtain the labels for the target user activities."}, {"title": "B. An overview of the Diff-Noise-Adv-DA method", "content": "Diffusion models, inspired by processes in physics, have recently shown great potential in various machine learning tasks. These models iteratively transform data from a simple distribution to a more complex one through a series of gradual steps, enabling the capture of intricate data structures. Diffusion models are particularly effective for domain adaptation due to their ability to learn complex transformations and smooth transitions between data distributions. By modeling the progressive changes needed to align source and target data distributions, diffusion models can effectively bridge the gap between different users' activity data."}, {"title": "C. Forward Diffusion Process", "content": "In this section, we compare the origianl design of forward diffusion process of diffusion model to our novel design of Diff-Noise-Adv-DA.\n1) Original Forward Diffusion Process\nThe forward diffusion process is designed to incrementally add noise to data over multiple timesteps.\n1. Forward Diffusion Process Definition:\n$q(xt | Xt-1) = N(xt | Atxt\u22121, \u03b2\u03b5\u0399)$\nwhere $\u03b2t = 1 \u2013 at$ and I represents the identity matrix.\nThis equation defines the forward diffusion process. It specifies that the distribution of $xt$ given $xt-1$ is a Gaussian distribution with mean $&txt_1$ and variance $\u03b2\u2081I$. $at$ and $\u03b2t$ are parameters controlling the noise added at each time step. $\u03b2t = 1 \u2212 at$ determines the variance of the noise. As t increases, $xt$ becomes progressively noisier, transitioning from a signal to pure noise.\n2. Mean of Forward Process:\n$E[xt | Xt-1] = AtXt\u22121$\nThis equation provides the mean of the Gaussian distribution for $xt$ given $xt-1$. The mean is $atxt\u22121$, showing that $xt$ is a scaled version of $xt\u22121$, with the scaling factor $at$. The mean is scaled by $at$, meaning the influence of $xt-1$ on $xt$ decreases as $at$ changes. This scaling factor controls how much of the previous state influences the current state.\n3. Variance of Forward Process:\n$Var[xt | Xt-1] = \u03b2\u2081I$\nThis equation provides the variance of the Gaussian distribution for $xt$ given $xt\u22121$. The variance represents the amount of noise added at each step. As t increases, Bt generally increases, making $xt$ more noisy and less similar to $xt\u22121$.\n4. Forward Noising Process:\n$Xt = AtXt-1 + \u03b2\u03c4\u03b5$\nwhere $\u2208 ~ N(0, I)$ is the noise added to the process. This equation describes how $xt$ is generated from $xt-1$ by adding noise. e is drawn from a standard normal distribution N(0, I). $xt$ is obtained by adding noise to a scaled version of $Xt-1$. $Atxt-1$ represents the signal component, and $\u03b2te$ represents the noise component. As the process progresses, the noise component becomes more significant, making $xt$ increasingly noisy and different from $xt-1$."}, {"title": "2) Forward Diffusion Process with Classification Information", "content": "In our new design, we incorporate classification information directly into the forward diffusion process. This integration allows the model to utilize activity labels and user labels to better guide the orientation of noise addition process, empowering the noise as a carrier of the classification information.\n1. Forward Diffusion Process Definition:\n$q(xt | Xt\u22121, Ca, cu) = N(xt | Xtxt\u22121+\u03b2t (ca \u2022 Ya + cu\u00b7 Yu), \u03b2tI)$\nwhere $\u03b2t = 1 - at$.\nThis equation defines the forward diffusion process with the inclusion of classification information. The distribution of $xt$ given $xt-1$, activity label $ca$, and user label $cu$ is a Gaussian distribution with a mean that combines the scaled previous data $txt-1$ and additional terms $caYa + cu Yu$ based on the classification labels, and variance $\u03b2tI$. $ca$ and $cu$ represent the activity and user labels, respectively. $Ya$ and $Yu$ are vectors that encode the influence of the activity and user labels on the data. The mean of the Gaussian distribution now incorporates these classification labels to guide the noising process. $\u03b2t = 1 \u2212 at$ determines the amount of the noise added.\n2. Mean of Forward Process with Classification:\n$E[xtXt-1, Ca, Cu] = AtXt\u22121 + \u03b2t (ca Ya + Cu Yu)$\nThis equation provides the mean of the Gaussian distribution for $xt$ given $xt-1$, activity label $ca$, and user label $cu$. The mean is a combination of the scaled previous data and the contributions from the classification labels. The mean now includes additional terms $ca. Ya$ and $cu Yu$ that encode the influence of the activity and user labels. This incorporation helps align the data with the classification information.\n3. Variance of Forward Process:\n$Var[xtxt-1, Ca, Cu] = \u03b2t$\nThis equation provides the variance of the Gaussian distribution for $xt$ given $xt-1$, activity label $ca$, and user label $cu$. The variance remains \u00dft, as it is independent of the classification labels. The variance of the noise is not affected by the classification information; it remains as \u00dft, which governs the amount of noise added at each step.\n4. Forward Noising Process with Classification:\n$Xt = AtXt-1 + \u03b2\u03c4\u03b5$\nwhere $\u2208 ~ N(ca\u00b7 Ya + Cu \u2022 Yu, I)$ is the noise added to the process.\nThis equation describes the forward noising process with the integration of classification information. The data $xt$ is generated from $xt\u22121$ by adding noise with the information that incorporate activity and user labels. The term $&txt_1$ represents the scaled previous data. $Ca Ya$ and $Cu Yu$ add classification-specific information to the noise. Be adds random noise to the process. This design improves how well the model aligns noise with classification labels."}, {"title": "D. Reverse Denoising Process", "content": "In this section, we compare the origianl design of reverse denoising process of diffusion model to our novel design of Diff-Noise-Adv-DA.\n1) Original Reverse Denoising Process\nThe original reverse denoising process aims to reconstruct the original data from the noisy data by learning the mean and variance parameters at each step. This process gradually removes the added noise and retrieves the underlying clean data.\n1. Reverse Process Definition:\n$Po(Xt\u22121 | xt) = N(xt\u22121 | \u00b5t(xt), \u03b2tI)$\nwhere $pt(xt)$ is the learned mean and Bt is the learned variance.\nThis equation defines the reverse denoising process. The distribution of $xt_1$ given $xt$ is a Gaussian distribution with mean $it(xt)$ and variance BtI. The reverse process aims to recover $xt-1$ from the noisy $xt$. $it(xt)$ is the mean that is learned by the model, which represents the most probable value of $xt-1$ given $xt$. Bt is the variance that the model learns, indicating the uncertainty in the prediction of $xt-1$."}, {"title": "2. Learned Mean of Reverse Process:", "content": "$\n\u03bct(xt) = \\frac{1}{At} (\\frac{Xt}{\\sqrt{at}} - \\frac{\u03b2t}{\\sqrt{1-\u0101t}} e)$\nwhere e is the noise term.\nThis equation provides the learned mean it(xt) for the reverse process. It is computed by adjusting x\u2081 based on the learned noise \u20ac and the parameters at and \u1e9et. it(xt) represents the estimate of the clean data xt-1. The term $\\frac{1}{\\sqrt{at}}$ scales xt to account for the signal contribution. The term $\\frac{\u03b2t}{\\sqrt{1-\u0101t}}$ adjusts for the noise added during the forward process. e represents the noise term that is used to estimate the noise component in the reverse process."}, {"title": "3. Learned Variance of Reverse Process:", "content": "$\n\u03b2t = \\frac{\u03b2t(1-at-1)}{1\nAt}$\nThis equation provides the learned variance Bt for the reverse process. It is computed based on the variance Bt and the cumulative product of at values, denoted \u0101t. Bet represents the variance of the noise added during the reverse process. It is derived from the variance Bt used in the forward process and adjusted by the ratio of cumulative a terms. This ensures that the noise level added during the reverse process is consistent with the noise level added during the forward process."}, {"title": "4. Reverse Noising Process:", "content": "$\nxt-1 = \u03bct(xt) + \u03b2\u03c4\u03b7$\nwhere \u03b7 ~ \u039d(0, I) is the noise sampled in the reverse process.\nThis equation describes the reverse noising process. The clean data xt-1 is obtained from xt by adding noise sampled from a standard normal distribution to the learned mean \u016bt(xt). Xt-1 is reconstructed by starting from the learned mean \u016bt(xt) and adding noise \u1e9etn. \u1e9et scales the noise term \u03b7 to account for the variance in the reverse process. \u03b7 is sampled from a standard normal distribution and represents the random noise added during the reverse process. This process iteratively denoises xt to recover the original clean data as xt-1."}, {"title": "2) Reverse Denoising Process with Adversarial Regularization", "content": "In our new design, we enhance the reverse denoising process by incorporating adversarial regularization, which aims to improve the alignment between source and target distributions during the denoising process.\n1. Reverse Process Definition with Adversarial Regularization:\n$Po(Xt\u22121 | Xt, Ca, Ccu, cas) = N(xt\u22121 | \u00b5t(Xt, Ca, Cu, Cas), \u03b2tI)$\nwhere pt(xt, ca, cu, cas) incorporates adversarial regularization.\nThis equation defines the reverse denoising process with adversarial regularization. The distribution of xt_1 given xt, activity label ca, user label cu, and source user activity label cas is a Gaussian distribution with a learned mean \u00b5t(xt, ca, cu, cas) and variance \u1e9etI. The reverse process aims to recover xt-1 from the noisy xt. The learned mean it now incorporates adversarial regularization to ensure better alignment between source and target distributions. Bt is the"}, {"title": "2. Learned Mean of Reverse Process with Adversarial Regularization:", "content": "$\n\u00b5t (xt, Ca, Cu, Cas) = \\frac{1}{At} (\\frac{Xt}{\\sqrt{at}} - \\frac{\u03b2t}{\\sqrt{1-\u0101t}} (AR(ca Ya + Cu Yu) +cas Yas)) \\qquad (1)$\nwhere AR(\u00b7) denotes the adversarial mechanism of Gradient Reversal technique applied to $ca Ya + Cu Yu$\nThis equation provides the learned mean it for the reverse process, incorporating adversarial regularization. The mean is computed by adjusting xt based on the learned noise and adversarial terms. The adversarial regularization AR(\u00b7) is applied to the classification information to ensure feature user-invariant. Ut (xt, Ca, Cu, Cas) represents the estimate of the clean data xt\u22121 with adversarial adjustments. The term $\\frac{1}{\\sqrt{at}}$ scales x\u2081 to account for the signal contribution. The term $\\frac{\u03b2t}{\\sqrt{1-\u0101t}}$ adjusts for the noise component. AR($ca Ya + Cu \u00b7 Yu$) applies adversarial regularization to ensure that the activity and user labels do not bias the learned mean. $cas. Vas$ represents the adversarial signal related to the source user activity label, helping to align features between the source and target domains."}, {"title": "3. Learned Variance of Reverse Process:", "content": "$\n\u03b2t = \\frac{\u03b2t(1-at-1)}{1\nAt}$\nThis equation provides the learned variance Bt for the reverse process. It is computed based on the variance Bt and the cumulative product of a values, denoted \u0101t. Bt represents the variance of the noise added during the reverse process. It is derived from the forward process variance $\u03b2t$ and adjusted by the ratio of cumulative a terms. This ensures that the noise level added during the reverse process is consistent with the forward process, while incorporating adversarial adjustments if needed."}, {"title": "4. Reverse Noising Process with Adversarial Regularization:", "content": "$\nxt\u22121 = \u00b5t(xt, Ca, Cu, Cas) + \u03b2\u03c4\u03b7$\nwhere \u03b7 ~ \u039d(0, I) is the noise sampled in the reverse process.\nThis equation describes the reverse noising process with adversarial regularization. The clean data xt-\u22121 is reconstructed from xt by adding noise sampled from a standard normal distribution to the learned mean it. xt-1 is obtained by starting from the learned mean \u016bt and adding noise \u03b2\u03c4\u03b7. \u1e9et scales the noise term \u03b7 to account for the variance in the reverse process. \u03b7 is sampled from a standard normal distribution and represents the random noise added during the reverse process. Adversarial regularization helps align features between different users, improving the quality of the reconstructed data and ensuring consistency with the source and target distributions.\nBy incorporating adversarial regularization terms into the learned mean, the reverse denoising process is guided to focus"}, {"title": "E. Training Objectives", "content": "1) Original Training Objectives\nThe original training objective for diffusion models is to maximize the data likelihood by minimizing the reconstruction error between the original data and the denoised data. This objective aims to ensure that the learned mean and variance parameters effectively reverse the noise addition process.\nNoise Prediction Objective:\n$Ltotal = Lnoise = Ex0,6,t [||\u20ac \u2013 \u20ac(xt, t)||^2]$\nwhere \u00ea(xt, t) is the predicted noise from the model.\nThis objective function measures the difference between the true noise e and the predicted noise \u00ea(xt, t) at a given timestep t. The goal is to minimize this difference. The expectation Exo,e,t is taken over the original data xo, the noise e, and the time step t. The term ||\u20ac \u2013 \u20ac(xt,t)||2 represents the squared error between the true noise and the predicted noise. By minimizing Lnoise, the model learns to accurately predict the noise added during the forward diffusion process. Accurate noise prediction ensures that the model can effectively reverse the noise addition during the reverse process, recovering the original data.\n2) New Training Objectives\nIn our new design, we introduce an adversarial regularization term to the training objective. This term encourages the model to generate denoised data that aligns more closely with the target distribution.\n1. Noise Prediction Objective:\n$Lnoise = Exo,e,t [|| - \u00ea(xt, t) ||\u00b2]$\nwhere \u00ea(xt, t) is the predicted noise from the model.\n2\nThis objective function measures the difference between the actual noise e and the predicted noise \u00ea(xt,t) at time t. The expectation Exo,e,t is taken over the original data xo, the noise \u20ac, and the time step t. The term ||\u20ac \u2013 \u20ac(xt, t) ||\u00b2 represents the squared error between the true noise and the model's prediction. Minimizing Lnoise ensures that the model learns to accurately predict the noise, which is crucial for effectively reversing the forward diffusion process.\n2. Classification Objectives in Forward Diffusion Pro-cess:\nActivity Classification ($ca$):\n$Lact = Exo,ca [CrossEntropy(\u0109a, ca)]$\nUser Classification ($cu$):\n$Lbinary = Exo, cu [CrossEntropy(\u0109u, Cu)]$\nThese objectives guide the model to classify activity and domain labels during the forward diffusion process. Lact ensures that the model can accurately predict the activity label ca from the data. For this classifier, we only have access to labels from the source users, while the labels for target users are not available. Even though we don't know the specific classes in the target user, it is often assumed that the relationship"}, {"title": "3. Classification Objectives in Reverse Diffusion Process:", "content": "Adversarial Learning Loss:\n$Ladva-Exo,ca, cu [CrossEntropy(\u0109a, ca)]$\n$Ladvu = -Exo,ca,cu [CrossEntropy(\u0109u, cu)]$\n$Ladv = Ladva + Ladvu$\nActivity Classification of Source User (cas):\n$Lact-source = Exo,cas [CrossEntropy(\u0109as, cas)]$\nThese objectives introduce adversarial regularization and source user activity classification into the reverse denoising process. Lady applies adversarial regularization to ensure that the model's predictions for activity and domain classifications align with the target distribution. It uses cross-entropy loss and aims to minimize the classification accuracy, encouraging the model to generalize better across domains. Lact-source focuses on classifying the activity label of the source user, helping to maintain the relevance of source-specific information in the reverse process.\n4. Combined Training Objective:\n$Ltotal = Lnoise + Aact Lact + AbinaryLbinary$ + lady Lady + Aact-source Lact-source (2)\nwhere Aact, Abinary, Aadv, and Aact-source are hyperparameters that control the contribution of each term.\nThe combined training objective integrates the noise prediction loss with classification and adversarial losses, weighted by their respective hyperparameters. Ltotal balances multiple objectives to guide the model's training. Lnoise ensures effective noise prediction. Lact and Lbinary ensure accurate classification during the forward process. Lady enforces adversarial regularization to improve generalization across users. Lact-source maintains the relevance of source user activity information. Hyperparameters Aact, Abinary, Aadv, and Aact-source are tuned to balance the trade-offs between different training objectives. The incorporation of adversarial learning into the training objective aims to improve the model's ability to generalize across different users by maximising and minimizing the discrepancy between source and target distributions. This approach leverages the strengths of both diffusion models and adversarial learning to achieve robust cross-user HAR domain adaptation."}, {"title": "IV. EXPERIMENTS", "content": "We conducted extensive experiments on public HAR dataset PAMAP2, which offer a variety of activities and sensor configurations. For PAMAP2, we choos user 1, 2, 5and 8. because other users' activities are not complete. there are 12 activities, i.e. lying, sitting, standing, walking, running, cycling, Nordic walking, ascending stairs, descending stairs, vacuum cleaning, ironing, rope jumping.\nDelving deeper into the analysis, we explore the cross-user data distribution differences of activities across the three datasets. We apply a common feature extraction method in HAR [6] to sliding windows for both source and target users in each dataset. To quantify the differences in feature distributions between users, we employ the Wasserstein distance [32]. This metric measures the effort required to transform one distribution into another, considering the cost of moving and transforming distribution mass, which aligns well with our case.\nThe observed distance represents the Wasserstein distance of the complete data of the feature distributions in each activity between source and target users, depicted as the red dashed lines. This metric gauges the degree of o.o.d. of an activity across users, where a higher value indicates greater dissimilarity between the two data distributions. Next, we conduct resampling with replacement from each distribution to generate new sample distributions, repeating this process 5000 times. The bootstrap distance represents the Wasserstein distance of the resampled feature distributions in each activity between source and target users, visualized as the blue distributions. Bootstrapped distances aid in understanding the variability and centrality of the distance measure, revealing how different the feature distributions of the sub-activities are between source and target users. If more bootstrap distances are higher than the observed distance, it indicates greater data distribution difference of the subset of the activity features, implying that the sub-activity feature distributions are more distant.\nFurthermore, we introduce the Bootstrapped proportion, which denotes the proportion of distances from the bootstrapped samples that are smaller than or equal to the observed distance from the original samples. This proportion serves to measure the degree of sub-activity data distribution difference between users. A lower Bootstrapped proportion suggests that the sub-activity data distribution difference between users is more significant.\nIn Figure 5, we observe that the feature distributions of the lying activity across users differ significantly compared to standing, as indicated by the varying distances from the red dashed lines. However, when examining the sub-activity feature distributions, represented by the blue part, the situation is reversed. This suggests that while the lying activity remains relatively consistent, there is more variability in the standing activity. The reason for this disparity could be attributed to the consistent sleep posture across users during the lying activity, whereas individuals tend to change their posture more frequently while standing."}, {"title": "V. CONCLUSION", "content": "In this paper, we have presented a novel method Diff-Noise-Adv-DA for cross-user HAR domain adaptation, leveraging diffusion models with adversarial regularization. Our approach enhances the forward diffusion process by incorporating classification information and improves the reverse denoising process by introducing adversarial regularization. The training objectives are designed to maximize data likelihood while minimizing domain discrepancy, promoting better alignment between source and target distributions. Experimental results demonstrate the effectiveness of our method in achieving robust cross-user HAR domain adaptation, highlighting its potential for real-world applications."}]}