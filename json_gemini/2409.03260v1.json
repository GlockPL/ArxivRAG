{"title": "In Search of Trees: Decision-Tree Policy Synthesis for Black-Box Systems via Search", "authors": ["Emir Demirovi\u0107", "Christian Schilling", "Anna Lukina"], "abstract": "Decision trees, owing to their interpretability, are attractive as control policies for (dynamical) systems. Unfortunately, constructing, or synthesising, such policies is a challenging task. Previous approaches do so by imitating a neural-network policy, approximating a tabular policy obtained via formal synthesis, employing reinforcement learning, or modelling the problem as a mixed-integer linear program. However, these works may require access to a hard-to-obtain accurate policy or a formal model of the environment (within reach of formal synthesis), and may not provide guarantees on the quality or size of the final tree policy. In contrast, we present an approach to synthesise optimal decision-tree policies given a black-box environment and specification, and a discretisation of the tree predicates, where optimality is defined with respect to the number of steps to achieve the goal. Our approach is a specialised search algorithm which systematically explores the (exponentially large) space of decision trees under the given discretisation. The key component is a novel pruning mechanism that significantly reduces the search space. Our approach represents a conceptually novel way of synthesising small decision-tree policies with optimality guarantees even for black-box environments with black-box specifications.", "sections": [{"title": "Introduction", "content": "Designing controllers for complex systems with the guarantee of specified behaviour remains an important challenge. Classical control synthesis can provide such guarantees given a (precise) model of the system (Belta and Sadraddini 2019). This requirement may in some cases be infeasible, which gave rise to black-box and approximate approaches, e.g., based on machine learning. As systems grow larger, interpretability is an increasingly desired specification for machine-learned policies to achieve alignment with human specifications (Rudin 2019). With the success of decision trees as interpretable machine-learning models, policies represented as decision trees have gained considerable traction (Du, Liu, and Hu 2020; Glanois et al. 2024).\nThere are diverse approaches to synthesising, or learning, decision-tree policies. Stratego (David et al. 2015) employs reinforcement learning dedicated to decision trees. Modifying the reinforcement learning process to produce decision trees has also been proposed (Topin et al. 2021). An alternative is to apply imitation learning to distil a neural-network policy into a decision tree (Bastani, Pu, and Solar-Lezama 2018). After using formal synthesis to construct a policy in tabular form, decision trees may be induced via specialised algorithms akin to algorithms used for solving standard classification problems (Ashok et al. 2021).\nWhile previous approaches have their strengths, none of the discussed methods provide guarantees in terms of decision-tree policy performance or size and/or require an existing expert policy or effective reinforcement learning algorithm. Policy synthesis may be posed as a mixed-integer linear programming problem (Vos and Verwer 2023), which can provide guarantees. However, this approach assumes that a model of the environment is given. When the above requirements are not met, decision-tree policy cannot be constructed using existing methods.\nIn contrast, we consider a unique setting: derive 1) small decision-tree policies when 2) the model and specification of"}, {"title": "Related Work", "content": "Our work covers a unique setting: constructing decision trees given a deterministic black-box system whilst providing optimal performance guarantees. As such, there are no directly applicable works that we are aware of in this setting. Nevertheless, to illustrate the challenges of our setting, we discuss previous works for synthesising decision-tree policies, albeit not fitting into our setting.\nReinforcement learning. A tree policy can be obtained via reinforcement learning, either by using dedicated tree algorithms (David et al. 2015) or by modifying reinforcement learning to output tree policies (Topin et al. 2021). Alternative approaches allow linear functions on the leaves (Gupta, Talvitie, and Bowling 2015), consider multiple predicates, branches and actions at a time, or fix the structure using expert knowledge (Likmeta et al. 2020) and then employ policy gradient updates (Silva et al. 2020; Paleja et al. 2022). These approaches perform exceptionally well, when an existing reinforcement-learning approach is available that is effective for the given system (Topin et al. 2021), and/or the model (Gupta, Talvitie, and Bowling 2015) of the environment is known. A tree policy may be derived by imitating an expert policy, e.g., a neural network (Bastani, Pu, and Solar-Lezama 2018). In contrast, we require neither model nor expert policy and provide optimality guarantees. In case of sparse rewards, reinforcement learning might struggle, while our framework by design has no such problem.\nLearning from tabular data. When the policy is given in tabular form, dedicated tree-learning algorithms for control policies can be employed (Ashok et al. 2020, 2021), which extend classical tree-learning algorithms (Breiman et al. 1984; Quinlan 1996). Recent advancements in optimal tree induction could potentially also be employed (Demirovi\u0107 et al. 2022; van der Linden, de Weerdt, and Demirovic 2023). However, obtaining the tabular policy requires an explicit model, which is not required in our setting.\nOptimal policy synthesis. The problem of constructing a tree policy may be posed as a mixed-integer linear program (Vos and Verwer 2023), after which off-the-shelf solvers may be used to obtain optimal policies. However, not all environments may be feasible to model with such an approach (e.g., differential equations or trigonometric functions), and in our setting we consider black-box environments, which are not amendable to linear programming."}, {"title": "Preliminaries", "content": "A state $S = (s_1, s_2, ..., s_d)^T \\in S$ is a d-dimensional real-valued vector from a bounded state space $S \\subset R^d$, where each state dimension $s_i$ belongs to an interval $s_i \\in [l_i, U_i]$ of a lower and an upper bound. An action $a \\in A$ comes from a finite set $A \\subset \\mathbb{Z}$ of integer-valued actions.\nAn environment is a function $E: S \\times A \\rightarrow S$ that takes as input a state S and an action a and computes a trajectory until asked to output the next observable state $S' = E(S, a)$. In our setting, the environment is treated as a black box, i.e., we are agnostic to its dynamics.\nA policy is a function $\\pi: S \\rightarrow A$ that chooses an action based on an input state. We write $C_A$ for the set of all policies over action set A. In this work, we are concerned with the special case of a decision-tree policy, which is given in the form of a binary tree where each inner node is called a predicate node and each leaf node is called an action node. Each predicate node is associated with a function $S \\rightarrow \\mathbb{B}$, where $\\mathbb{B}$ is the Boolean domain. Each action node is associated with one of the available actions $a \\in A$. Given a state S and a node of the tree, a decision-tree policy $\\pi$ computes the action $\\alpha = \\pi(S)$ using the following recursive procedure, starting at the root node. If the current node is an action node, the associated action is returned. Otherwise, the current node is a predicate node. If the state S makes the predicate evaluate to true (false), the procedure continues with the left (right) child node.\nGiven an environment E, a decision-tree policy $\\pi$, an initial state $S_0$, and a bound $k \\in \\mathbb{N}$, the trace with k steps is the sequence of observed states $T = S_0, S_1, S_2, ..., S_k$, obtained by applying the sequence of actions given by $\\pi$: $S_i = E(S_{i-1}, \\pi(S_{i-1}))$, for $i = 1,..., k$. We write $\\mathbb{T}$ for the set of all traces."}, {"title": "Optimal Decision-Tree Policies", "content": "To simplify the exposition, we focus our discussion on the case of a single initial state $S_0$. We generalise the problem to multiple initial states in Section 5.4.\nDiscretised Decision-Tree Predicates\nThe space of all decision trees is infinitely large. By discretising the tree predicates, we obtain a finite (but exponentially large) space. We restrict our attention to (axis-aligned) predicates of the form $[s_i \\geq v_0 + m \\cdot v_{+}]$, where $s_i$ is the"}, {"title": "Specification", "content": "Given environment E, decision-tree policy $\\pi$, and initial state $S_0$, we consider a specification to determine whether $\\pi$ satisfies the specification. We assume that the specification is given in terms of traces, again in the form of a black-box specification function $P : \\mathbb{T} \\rightarrow \\mathbb{B}$. In order to effectively determine whether $\\pi$ satisfies the specification, we restrict the class of specifications we consider. A bounded-time specification with a bound $k \\in \\mathbb{N}$ has the property that every trace T of length k either satisfies or violates the specification. As a consequence, we are guaranteed to obtain a Boolean verdict from a trace of length at most k. We call the (unique) trace of length k the witness trace.\nThis class of specifications includes common reach-avoid problems where a goal needs to be reached while undesired states need to be avoided. For instance, for the pendulum environment, the specification is to reach the vertical position within a step bound k. A trace satisfies the specification if and only if a prefix of length less than k satisfies the specification function. Conversely, any trace not reaching the goal withing k steps violates the specification."}, {"title": "Optimality", "content": "So far, we were only interested in finding any policy that satisfies a given specification. In general, there may exist multiple solutions. We are interested in identifying an optimal policy. For that, we assume a fitness function, which is a partial order $\\succeq : \\mathbb{T} \\times \\mathbb{T}$ to compare two traces. A trace that satisfies the specification always precedes a trace that violates the specification. The fitness function induces another partial order $\\succeq : C_A \\times C_A$ to compare two policies, as follows. We say that $\\pi_1$ is strictly better than $\\pi_2$, written $\\pi_1 \\succ \\pi_2$, if one of the following conditions holds: 1) The witness $\\tau_1$ has strictly better fitness than the witness $\\tau_2$. 2) Both witnesses have the same fitness, and $\\pi_1$ is strictly smaller.\nWe wrap the black-box environment E and the black-box specification function P into a black-box system $B : C_A \\times S \\rightarrow \\mathbb{B} \\times \\mathbb{T}$. This system takes as input a policy $\\pi$ and an initial state $S_0$ and outputs both the (Boolean) verdict and the trace $\\tau$. We say that a policy $\\pi$ satisfies the specification for initial state $S_0$ if B yields a positive verdict. We note that B can be implemented from E and P by simply generating the witness $\\tau$ and querying the specification function."}, {"title": "Searching In the Space of Decision Trees", "content": "Our algorithm to enumerate decision trees is based on backtracking search. We represent the search space using backtracking variables $b_i$, where each variable is associated with a node in the tree. The possible values that can be assigned to a backtracking variable depend on the type of node which the variable is associated with: predicate nodes may be assigned a predicate from the set of discretised predicates, whereas action nodes may be assigned an action from the set of available actions.\nBacktracking variables are considered in a predefined order, i.e., variable $b_1$ goes before variable $b_{i+1}$. As is standard in backtracking, all combinations for variable $b_{i+1}$ are exhausted before taking the next value for variable $b_i$.\nAssigning all backtracking variables thus results in a particular decision-tree policy, and consequently, enumerating all possible assignments to the backtracking variables corresponds to all possible policies in our available space. When enumerating a policy, it is used in combination with the black-box environment to compute the witness trace and subsequently evaluate the quality of the policy. Finally, the best policy is returned as the result.\nFor a tree with a fixed shape and n predicate nodes, $A$ number of actions, and |P| number of discretised predicates, the size of the total search space is $O(|P|^n \\cdot |A|^{n+1})$. Our key contributions are techniques for reducing this exponentially large search space in practice."}, {"title": "Intuition Behind Trace-based Pruning", "content": "The idea of pruning is to limit the exploration by avoiding to explicitly enumerate trees that are guaranteed to be suboptimal, i.e., do not satisfy the desired property with a higher fitness. We define sufficient conditions for pruning.\nTo provide the intuition behind our approach, consider an environment with only one state dimension $s_1$, and the process of enumerating all trees with exactly one predicate node, three possible predicates $[s_1 \\geq 1]$, $[s_1 \\geq 2]$, $[s_1 \\geq 3]$,"}, {"title": "Trace-Based Pruning", "content": "The intuition discussed above can be generalised to prune a potentially exponential number of trees that do not result in a different trace, which significantly speeds up the search process. In the following, we discuss incorporating a general version within a backtracking algorithm with more than one predicate node. We will consider predicates in increasing order, i.e., $[s_j \\geq V_1]$ before $[s_j \\geq V_2]$ if $V_1 < V_2$.\nGiven a backtracking variable $b_i$ associated with a predicate node, the algorithm needs to select the next predicate to assign to the variable. A naive approach would be to simply select the next bigger one, e.g., from the example in Section 5.2, assign predicate $[s_1 \\geq 2]$ after considering $[s_1 \\geq 1]$. However, we can leverage information about previous traces to avoid explicitly considering predicates that are guaranteed to not result in a trace that has not been previously observed.\nAfter assigning a predicate $[s_j \\geq v]$ to a backtracking variable $b_i$, the idea is to track the values of state dimension $s_i$ that have been observed during environment runs such that the predicate evaluated to true. In particular, we are interested in the smallest such value, which we refer to as the distance value $d_i \\in \\mathbb{R}$. Note that a tree policy is only run after having all backtracking variables assigned.\nThe key idea is that, when selecting the next predicate for backtracking variable $b_i$, its threshold value (v in the example above) should exceed the distance value $d_i$. Otherwise, the trace would be identical."}, {"title": "Searching In the Space of Decision Trees", "content": "To reiterate, for each backtracking variable $b_i$ associated with a predicate node with current predicate $[s_j \\geq v]$, we store a value $d_i \\in \\mathbb{R}$, which tracks the minimum value of state dimension $s_i$ for which the predicate was evaluated to true amongst all traces that were considered since the predicate $[s_j \\geq v]$ has been assigned. Note that selecting a new threshold value for the predicate that is smaller than $d_i$ is guaranteed to result in a trace already observed. Note that it is not necessary to track the values where the predicate evaluates to false, since the predicates are explored in increasing order of the thresholds and as such the future predicates would also evaluate to false on those values.\nInitially, the distance value $d_i$ is set to undefined each time a new predicate $[s_j \\geq v]$ is assigned as part of the search. The first time the node observes a state where its predicate is satisfied in a trace, the distance value $d_i$ is set to the corresponding value of $s_j$. Each subsequent time the predicate is satisfied, $d_i$ is updated to the smallest value for which the predicate still evaluates to true.\nAfter considering predicate $[s_j \\geq v]$ for node i, our algorithm does not consider the next predicate, but instead uses the predicate $[s_j \\geq v']$ where v' is the smallest available value such that v' > $d_i$. If the distance value $d_i$ is undefined, all predicates may be discarded for that backtracking variable for the currently considered state dimension $s_j$.\nFor the example in Section 5.2,the distance value $d_1$ is initially undefined, and upon completing the first trace, it is set to $d_1$ = 2.3. When selecting the next predicate, $[s_1 \\geq 2]$ is discarded since its threshold 2 does not exceed distance value $d_1$ = 2.3; so the next selected predicate is $[s_1 \\geq 3]$. The above idea is applied to every backtracking variable associated with a predicate.\nOur pruning strategy is computationally inexpensive: it amounts to tracking a single value for each backtracking variable, and updating this value as the tree is queried during trace computation. The algorithm retains completeness, as it is guaranteed to not discard optimal trees. Our trace-based pruning is the key component in the practical efficiency.\nAdditional Techniques: Trees explored in increasing size. The algorithm partitions the search space in terms of tree shapes, which are ordered by size. For example, after considering trees with exactly one predicate node, the algorithm considers trees which have a root node with one left predicate child, then trees which have a root node with one right predicate child, then complete trees with three predicate nodes, and so on until the size budget is reached.\nEarly stopping due to the objective. During the search, the best tree found so far is tracked. When evaluating a new candidate tree, its evaluation is preemptively stopped when it is determined that the trace cannot be extended to a trace that is better than the one obtained from the best tree so far.\nFor example, consider a setting where the policy should reach a goal state as quickly as possible. If the best policy so far reaches the goal state in k trace steps, and the partial trace associated with the current candidate policy has not reached the goal state in k - 1 steps, we may safely discard the candidate policy from further consideration, since it cannot result in a better trace. Note that, since we explore"}, {"title": "Construction of an optimal decision-tree policy", "content": "Algorithm 1 provides a high-level view on using backtracking variables. If available, the next unassigned backtracking variable is selected, or the last assigned variable otherwise. The next value is selected either as the next action for variables representing action nodes, and otherwise trace-based pruning is used to determine the threshold. Once a predicate has been exhausted on one state dimension, predicates for the next state dimension are selected.\nOnce all backtracking variables are assigned, the algorithm constructs a decision-tree policy, and uses the black-box system to produce the trace $\\tau$. If $\\tau$ is better than the globally best trace (initially null), that trace is updated to $\\mathbb{T}$. The distance values of the nodes of the policy are used to update the distance values of the backtracking variables. After all policies have been (implicitly) considered, the algorithm returns the best policy (Line 7)."}, {"title": "Extensions", "content": "Multiple initial states. The previous discussion was based on constructing a tree policy from a single initial state. However, we may be interested in finding a single tree policy that works well across multiple initial states. The algorithm remains similar, with impact on two components: 1) the objective function, and 2) trace-based pruning.\nWhen evaluating a tree with respect to multiple initial states, we generalise the fitness function. For example, if the goal is to minimise the trace length, then the generalisation aims to minimise the maximum trace length. This influences early stopping: the initial states are evaluated with respect to the tree policy one at a time, and as soon as a trace is encountered that is considered violating, the evaluation stops, i.e., the remaining initial states are not considered further.\nThe above idea interacts with trace-based pruning. In case the tree evaluation is stopped early, meaning the tree is deemed not better than the best tree found so far, only the last trace is used to update the distance values. The intuition is that, if we wish to find a better tree, it must lead to a trace different from the last trace, and we can ignore the distance-value updates of the other traces.\nAs a result, due to the interaction with pruning, finding the optimal tree with respect to multiple initial states may lead to lower running times than with a single initial state.\nMaximisation. Rather than satisfying the desired property in the least number of steps, we may be interested in maximising the number of steps. For example, the goal may be to balance a pole for as long as possible. The algorithm stays largely the same, with the only analogous changes needed in the evaluation of the tree. For maximisation it is important to specify an upper bound on the trace length; otherwise, the algorithm may potentially run infinitely long."}, {"title": "Experimental Study", "content": "We aim to illustrate the effectiveness of our approach with a proof-of-concept implementation. We show that our trace-based pruning approach is a key factor in making the approach feasible. Furthermore, we consider scalability from two perspectives: the granularity of the predicates, and the number of predicate nodes in the tree. While both are expected to have an exponential impact on the runtime, we observe that the runtime is still within practical use.\nWe consider three classical control problems: CartPole, MountainCar, and pendulum. The environment behaviour is defined as in Gymnasium\u00b9. MountainCar and pendulum are minimisation problems, whereas CartPole is a maximisation problem. We set the maximum trace size to 10,000, which is mainly only relevant for CartPole since it maximises the trace size, while for the other two environments, most traces\nOur code base is written in (pure) Rust 1.77.0. The experiments were run on consumer-grade hardware (Intel(R) Xeon(R) W-10855M @ 2.8 GHz). Our code will be made publicly available in due course."}, {"title": "Experiment #1: Trace-Based Pruning", "content": "Our trace-based pruning technique is clearly effective in pruning the search space. For CartPole and MountainCar, there is a one- to two-orders-of-magnitude difference, whereas for pendulum, it leads to a 4x reduction.\nThe runtime is roughly proportional to the number of trees explicitly considered, as expected, and is consistent based on the standard deviation across different initial states. Interestingly, we observe only a sub-linear increase in runtime with more initial states, and the total number of trees considered is roughly similar regardless of the number of initial states."}, {"title": "Experiment #2: Granularity of Predicates", "content": "The granularity of predicates impacts the runtime: the finer the discretisation, the larger the search space. Each state dimension in the environment has a predefined range of values"}, {"title": "Experiment #3: Number of Predicate Nodes", "content": "The number of predicate nodes directly influences the size of the search space. We study the runtime by considering trees of depth three and varying the maximum number of nodes from three to six.\nThe results indicate that the discretisation needs to be chosen carefully to balance runtime and quality of the final tree."}, {"title": "Further Discussion and Limitations", "content": "Our approach is effective at computing small and optimal decision-tree policies. There is an exponential runtime dependency with respect to the size of the tree and the discretisation of the state space. It may be infeasible with our approach to construct large tree policies or deal with high-dimensional environments. It is also possible that not all environments may be controlled by small decision trees.\nHowever when it is applicable, we believe small trees are valuable for interpretability reasons, and our approach provides the means to easily obtain such trees. The exponential runtime factor in our approach is inherent to every approach that aims to provide guarantees.\nOur approach is exceptionally flexible as it only requires black-box access to the system. This entails that the black box may be arbitrarily complex, as long as it can still be practically computed. Furthermore our algorithm provides performance guarantees, despite working with black boxes.\nThe optimality is important since it guarantees that we obtain the best performing tree under consideration, which may be relevant for some applications. It also allows us to conclude in cases when no such tree exists, and in general understand the limits of decision trees as control policies.\nGiven that our approach is a conceptually novel way to synthesise decision-tree policies in a unique setting, it opens many avenues for future work.\nParallelisation is promising as the search space can be naturally partitioned, and further heuristic pruning may lead to a principled trade-off between runtime and guarantees. Extending the approach to stochastic environments is another interesting direction. In our work, continuous actions ought to be discretised in a preferably smaller number of actions. Synthesising optimal trees for continuous actions remains an open challenge for decision-tree policies in general."}, {"title": "Conclusion", "content": "We presented a novel search-based method for computing an optimal decision-tree policy given a set of initial states and a black-box system. To the best of our knowledge, our approach is the first to consider such a setting. The key component is our trace-based pruning technique, which discards large portions of the search space at runtime. We illustrated the practicality of the approach on classical control benchmarks. When the environment is controllable by a small tree, our approach provides a way to obtain a small and optimal tree despite only requiring black-box access to the system."}, {"title": "Appendix / supplemental material", "content": "We considered three environments as defined in Gymnasium\u00b2. The dynamics may be found in their git repository, however, our algorithm does not have access to the internal dynamics, and only observes the state-action outputs in a black-box fashion. In the following, we describe the initial states and specifications. Note that specifications are also treated as black-box for the algorithm.\nThe system has four dimensions: cart position x, cart velocity $\\dot{x}$, pole angle $\\theta$, and pole angular velocity $\\dot{\\theta}$. We select the initial state by randomly assigning values in the range [-0.05, 0.05] to each state dimension. These values follow the initial states given in the Gymnasium.\nThe specification is to maintain that the cart position stays within the range [-2.4, 2.4] and the pole angle is within the range $[-\\alpha, \\alpha]$, where $\\alpha$ = 24$\\pi$/360, by applying force to the left (-1) and right (1).\nThe system has two dimensions: car position x and car velocity $\\dot{x}$. We select the initial state by randomly assigning the car position in the range [-0.6, -0.4] and setting the velocity to zero. These values follow the initial states given in the Gymnasium.\nThe specification is to reach the top of a hill, which means that the car position is greater or equal to 0.5, by applying force to the left (-1) and right (1).\nThe system has two dimensions: the position of the free end of the pole x and pole angular velocity $\\dot{\\theta}$. Note that in the Gymnasium, the position of the tip of the pole is given as two dimensions in terms of cos and sin of the position - presumably this is done to make it easier for neural networks to learn; however, in our case, we chose to directly represent the position in Cartesian coordinates.\nThe initial state is given by randomly assigning values to the position of the tip of the pole and the angular velocity from the ranges [-0.8, -0.5] and [-0.2, 0.2], respectively."}]}