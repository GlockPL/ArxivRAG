{"title": "SparseGrow: Addressing Growth-Induced Forgetting in Task-Agnostic Continual\nLearning", "authors": ["Yuqing Zhao", "Divya Saxena", "Jiannong Cao", "Xiaoyun Liu", "Changlin Song"], "abstract": "In continual learning (CL), model growth enhances adaptabil-\nity over new data, improving knowledge retention for more\ntasks. However, improper model growth can lead to severe\ndegradation of previously learned knowledge, an issue we\nname as growth-induced forgetting (GIFt), especially in task-\nagnostic CL using entire grown model for inference. Existing\nworks, despite adopting model growth and random initializa-\ntion for better adaptability, often fail to recognize the pres-\nence of GIFt caused by improper model growth. This over-\nsight limits comprehensive control of forgetting and hinders\nfull utilization of model growth. We are the first in CL to iden-\ntify this issue, and conduct an in-depth study on root cause of\nGIFt, where layer expansion stands out among model growth\nstrategies, widening layers without affecting model function-\nality. Nonetheless, direct adoption of layer expansion presents\nchallenges. It lacks data-driven control and initialization of\nexpanded parameters to balance adaptability and knowledge\nretention. In this paper, we propose a novel sparse model\ngrowth (SparseGrow) approach to overcome the issue of GIFt\nwhile enhancing adaptability over new data. SparseGrow em-\nploys data-driven sparse layer expansion to control efficient\nparameter usage during model growth, reducing GIFt from\nexcessive growth and functionality changes. It also com-\nbines the sparse growth with on-data initialization at train-\ning late-stage to create partially zero-valued expansions that\nfit learned distribution, enhancing retention and adaptability.\nTo further minimize forgetting, freezing is applied by calcu-\nlating the sparse mask, allowing data-driven preservation of\nimportant parameters. Through experiments across datasets\nwith various task-agnostic settings, use cases and large num-\nber of tasks, we demonstrate the necessity of layer expansion\nand showcase the effectiveness of SparseGrow in overcoming\nGIFt, highlighting its adaptability and knowledge retention\nfor incremental tasks. Code will be available upon publica-\ntion.", "sections": [{"title": "Introduction", "content": "In the field of continual learning (CL), model growth be-\ncomes essential for better adaptation to new data, improving\nscalability in managing incremental data. In real-life sce-\nnarios with dynamic environments or evolving needs such\nas autonomous vehicles, healthcare and smart cities, model\ngrowth is crucial for model deployment in handling in-\ncreased task complexity, incorporating new features or new\nmodality data. As the volume of incremental data exceeds\nthe model's learning capacity, it struggles to adapt to new\ninputs or retain previous knowledge. This necessitates aug-\nmenting the model's parameter capacity to sustain or en-\nhance its adaptability, thereby improving scalability.\nHowever, improper model growth often leads to a degra-\ndation of performance on previously learned tasks, an issue\nwe identify and name as growth-induced forgetting (GIFt),\nespecially in task-agnostic CL settings such as domain and\nclass incremental learning settings (Shim et al. 2021; Hu\net al. 2021). In these scenarios, the task label is unavail-\nable, and the entire model, including expanded parameters,\nis used for inference. Differing from catastrophic forgetting\n(CF), which arises from data distribution changes, growth-\ninduced forgetting is a distinct type of forgetting that occurs\nwhen a trained single model, upon increasing its parameter\ncapacity through model growth, is affected in its ability to\nretain or effectively utilize previously learned information.\nThis concept is echoed in neuroscience, where increasing\nneurogenesis in the brain after memory formation can pro-\nmote forgetting, referred to as neurogenesis-based forgetting\n(Davis and Zhong 2017).\nSome existing works in continual learning adopt differ-\nent model growth strategies and randomly initialize them\nto enhance adaptability. However, they fail to recognize the\npresence of GIFt resulting from improper model growth.\nThese methods may either be task-specific, requiring man-\nual task identification during inference to avoid forgetting\nissues, not applicable for domain or class incremental sce-\nnarios, or adopting improper model growth strategies that\nmay further lead to GIFt. Nevertheless, the lack of explicit\nacknowledgement of the growth-induced forgetting issue\nposes challenges in identifying suitable model growth ap-\nproaches. This not only limits comprehensive control of for-\ngetting but also hinders full utilization of model growth.\nWe are the first in CL to identify this issue and con-\nduct an in-depth study to identify appropriate model growth\nstrategies for increasing model capacity while minimizing\nthe occurrence of growth-induced forgetting. Our research\nrevealed that layer expansion, which widens layers with-\nout affecting model functionality, stands out compared with\nother model growth strategies, such as lateral connection\nand in-depth growth. The impacts of these growth meth-\nods on growth-induced forgetting (GIFt) differ: Layer ex-\npansion expands parameters in width, which are computed\ncollectively during inference. Lateral connections add new\nmodules in width connecting adjacent layers, overwriting\nold module values. In-depth growth adds new hidden lay-\ners, introducing new computations for preceding and suc-\nceeding parameters. Drawing inspiration from neurogenesis\n(Davis and Zhong 2017; Deng, Aimone, and Gage 2010),\nwe propose a novel sparse model growth (SparseGrow) ap-\nproach to overcome the issue of GIFt while enhancing adapt-\nability. SparseGrow employs data-driven sparse layer ex-\npansion to control efficient parameter usage during model\ngrowth, reducing growth-induced forgetting caused by ex-\ncessive growth and functionality changes. While achieving\ndata-driven control, the choice of initialization values signif-\nicantly impacts expansion, with zero-initialization stopping\nits update and random-initialization causing heavy GIFt.\nSparseGrow combines the sparse growth with on-data ini-\ntialization at final stage of training to create partially zero-\nvalued expansions that fit the learned distribution, mini-\nmizing GIFt while enhancing adaptability. To further mini-\nmize forgetting, freezing is applied by calculating the sparse\nmask, allowing data-driven preservation of important pa-\nrameters. To validate our findings, we conducted compre-\nhensive experiments on various task-agnostic settings with\ndomain and class incremental datasets and varying numbers\nof tasks. The results demonstrate the necessity of layer ex-\npansion, showcasing the effectiveness of our approach in\novercoming GIFt while highlighting the adaptability and\nknowledge retention of the method for incremental tasks."}, {"title": "RELATED WORK", "content": "Growing Approaches\nThere are three principal model growth methods: 1) Layer\nExpansion (LayerExp), which grows in width by widening\neach layer of the model; 2) Lateral Connection (LatConn),\nwhich also grows in width by introducing new lateral layers\nconnected to adjacent layers; and 3) In-depth growth (ID-\nGrow) that adds hidden layers to increase model depth.\nLateral Connection PNNs (Rusu et al. 2016) statically\ngrow the architecture with randomly initialized modules\nwhile retaining lateral connections to previously frozen\nmodules, limited to specific simple networks. (Schwarz et al.\n2018) use randomly initialized active lateral columns to\nlearn new tasks by connecting them to lateral columns\nthat store previous knowledge, applicable to Conv2d layers.\n(Zhang et al. 2020) adopts AutoML-based model growing\nwith both lateral connections and in-depth growth. VariGrow\n(Ardywibowo et al. 2022) proposes a task-agnostic CL ap-\nproach that detects new tasks through an energy-based nov-\nelty score and grows a new expert module to handle them.\n(Hu et al. 2023) add a new branch called the task tex-\npert while freezing existing experts, introducing dense con-\nnections between the intermediate layers of the task expert\nnetworks. (Li et al. 2019) proposes a hybrid solution in-\nvolving operations: 'new', 'adaptation', and 'reuse', appli-\ncable to specially designed Conv2d networks. These meth-\nods demonstrate the potential of lateral connections for CL\nbut highlight the need for more general approaches with less\nGIFt applicable for complex networks.\nLayer Expansion DEN (Yoon et al. 2017) uses layer ex-\npansion in a top-down manner, growing every layer if the\nloss does not meet a threshold. (Hung et al. 2019) ex-\npands the number of filters (weights) for new tasks and\nadopts gradual pruning to compact the model, limited to\ntask-specific settings. (Ostapenko et al. 2019) expands the\nsame number of neurons used in a layer in the GAN gener-\nator for rehearsal scalability. (Geng et al. 2021) expands the\nhidden size by the pruning ratio of task. (Yang et al. 2021)\ngrow a randomly initialized expanded filter and concatenate\nit into the network. (Xu and Zhu 2018) adaptively expands\neach layer of the network when new task arrives, applicable\nfor simple convolutional networks and fully-connected net-\nworks. (Yan, Xie, and He 2021) expand the model with new\nparameters by creating a separate feature extractor for in-\ncoming data, applicable for class-incremental learning. Our\nwork could further exploit layer expansion's potential by re-\nducing GIFt.\nIn-Depth Growth Kozal et al. (Kozal and Wozniak 2023)\nadd new layers atop existing ones. Zhang et al. (Zhang et al.\n2020) use AutoML for width and depth growth, incorporat-\ning lateral and in-depth layers with knowledge distillation.\nHowever, deepening the model tends to change the model\narchitecture, is not general enough for complex models, and\noften affects knowledge retention."}, {"title": "Overcoming Growth-Induced Forgetting", "content": "The issue of growth-induced forgetting might have been in-\ndirectly addressed in the context of task-agnostic contin-\nual learning, where certain studies have focused on pre-\nserving knowledge post model growth. In task-agnostic CL,\n(Madaan et al. 2023) propose Quick Deep Inversion to re-\ncover prior task visual features and enhance distillation.\n(Ostapenko et al. 2019) expand the same number of neu-\nrons used in a layer in the GAN generator for the scalabil-\nity of rehearsal. (Yan, Xie, and He 2021) freeze the previ-\nously learned representation and augment it with additional\nfeature dimensions from a new learnable feature extractor.\nVariGrow (Ardywibowo et al. 2022) detects new tasks and\ngrows a new expert module to handle them. By identifying\nGIFt, our work aims to address this issue at its root level,\noffering new strategies for enhancing knowledge retention."}, {"title": "Problem Formulation", "content": "We aim to enhance model adaptation to new data by ex-\npanding model capacity while addressing growth-induced\nforgetting in task-agnostic scenarios, covering domain-\nincremental and class-incremental learning. Given a se-\nquence of non-iid datasets {$D_1, D_2, ..., D_t$} and a model\n$f(\\cdot; \\theta_t)$ trained on previous datasets, where $\\theta_t$ are the model\nparameters at time t. Accessing only the current dataset $D_t$\nat time t, the goal is to grow the model capacity to improve\naccuracy on $D_t$ and future datasets while minimizing perfor-\nmance degradation on previous datasets. This is formalized\nas:\n$\\displaystyle \\text{minimize } J(\\theta_t) = L(D_t; f (\\cdot; \\theta_t)) + \\lambda \\sum_{i=1}^{t-1} L(D_i; f(\\cdot; \\theta_i))$\nwhere $J(\\theta_t)$ is the objective function at time t, $L(\\cdot)$ is the\nloss function, $\\lambda$ is a regularization parameter, $L(D_t; f(\\cdot; \\theta_t))$\nis the loss on the current dataset, and $\\sum L(D_i; f(\\cdot; \\theta_i))$\nis the cumulative loss on previous datasets."}, {"title": "Methodology", "content": "Sparse Neural Expansion\nSparse neural expansion consists of layer expansion and dy-\nnamic sparse training. It focuses on model expansion to in-\ncrease capacity, allowing better adaptation to new data. Ad-\nditionally, dynamic sparse training is integrated within lay-\ners to help better control expansion sparsity and granularity\nusing a data-driven way.\nLayer Expansion enables more general and fine-grained\nmodel growth without significantly altering model function-\nality, eliminating the need for manual selection of growth\nlocations. Unlike methods such as lateral connections and\nin-depth growth, which can result in the addition of more pa-\nrameters and changes in model functionality, this enhanced\ncontrol makes the method more suitable for complex neu-\nral networks. For each neural network layer (Conv2d, MLP,\nBatch Norm Layer), we implement an expansion technique\nwithin layer. Consider a neural network model with layers\ndenoted by $l$. The model has been trained on a previous\ndataset and is already equipped with learned parameters.\nGiven an original weight tensor, $W^{old} \\in \\mathbb{R}^{(C_{out}^{old}\\times C_{in}^{old})}$,\nand an original bias tensor, $b^{old} \\in \\mathbb{R}^{(C_{out}^{old})}$, where $C_{out}$\nrepresents the number of output channels after expansion,\nand $C_{in}$ represents the number of input channels, we expand\nthese tensors to accommodate the desired expansion:\n$W^{exp} \\in \\mathbb{R}^{(C_{out}^{}+n,C_{in}^{}+n)}, b^{exp} \\in \\mathbb{R}^{(C_{out}^{}+n)}$.\nwhere n is the expanded channel number. The expanded\nlayer $l$ now has the input channels $C_{in}^{(l)} + n$ and output chan-\nnels $C_{out}^{(l)} + n$. After expansion, the weights and bias of ex-\npanded layer are defined as:\n$W[: C_{out}^{old}, C_{in}^{oid}] = W^{old}, b[: C_{out}^{old}] = b^{old}$,\nWeights from the existing layer are transferred to their\noriginal positions in the new layer, while the expansion part\nremains initialized. The freezing and pruning masks are up-\ndated accordingly.\nExpansion for the Whole Model follows a logical rule\nto maintain the correspondence between input and output\nchannels of consecutive layers. The same logical expansion\napproach is also employed in complex structures such as\nResNet blocks and skip connections, ensuring consistent in-\nformation flow by expanding the involved layers while pre-\nserving the correspondence between input and output chan-\nnels. To expand the entire network, we adopt a channel ex-\npansion strategy that increases the channel size by a variable,"}, {"title": "Dynamic Sparse Training", "content": "Sparse training is integrated\nwithin layers and autonomously regulates the layer's spar-\nsity during training without compromising performance. In\na neural network with parameter set $W_i : 1 \\leq i \\leq C$ (where\n$W_i$ denotes the parameter matrix at layer $i$ and $C$ is the num-\nber of layers), pruning involves applying a binary mask $M^P$\nto each parameter $W$, setting unimportant parameters to 0.\nThis is done using a trainable pruning threshold vector $t$ and\na unit step function $S(x)$:\n$M^P_{ij} = S (|W_{ij}| - t_i ), 1 \\leq i \\leq c_o, 1 \\leq j \\leq C_i \\qquad(1)$\nDuring training, the dynamic sparse training method (Liu\net al. 2020) learns thresholds for each layer to distinguish\nimportant from unimportant parameters. Higher pruning\nthresholds lead to higher sparsity. A sparse regularization\nterm $L_s$ penalizes low threshold values, aiding in high spar-\nsity:\n$R_i = \\sum \\exp(-t_i ), L_s = \\sum R_i\\qquad(2)$\n$i=1\\qquad i=1$\nThe training loss function incorporates $L_s$ to train a sparse\nneural network directly using backpropagation:\n$\\displaystyle W^*,t^* = \\text{argmin}[L (D; W) + \\alpha L_s] \\qquad(3)$\nHere, $L(D; W)$ denotes the loss function, $\\alpha$ scales the\nsparse regularization term, and a new round of fine-grained\ndata-driven pruning restarts for a new dataset."}, {"title": "On-dataset Frozen Initialization", "content": "On-dataset frozen initialization involves employing both\nrandom initialization and on-dataset finetuning, while also\nfreezing learned parameters. While random initialization of\ngrown parameters enables better adaptation to new data, it\nwill cause higher GIFt. We further apply on-dataset frozen\nfinetuning to fill the gap of randomly initialized grown pa-\nrameters to the learned data pattern, therefore ensuring less\nforgetting.\nOn-dataset Initialization Proper initialization of new pa-\nrameters is crucial for adaptability and less forgetting, with\nzero-init stopping its update and random-init causing heavy\nGIFt. To address this, we propose a simple yet effective ap-\nproach: random and then on-data initialize the expanded part\nto adapt it to the current distribution at the final phase of\ntraining.\nThe random initialization for a weight parameter $W_{ij}$ in a\nneural network layer is defined as:\n$\\displaystyle W[C_{out}^{old},:]_{init} \\sim \\mathcal{N} \\Big(0, \\frac{2}{N_{in}}\\Big),$\nwhere $\\mathcal{N}$ denotes the normal distribution, and $n_{in}$ rep-\nresents the number of input units to $W_{ij}$. However, ran-\ndom initialization of expanded parameters can disrupt the\nmodel's inference. We then selectively fine-tune the ran-\ndomly initialized layer with sparse training to adapt it to\nthe current distribution. On-data initialization involves up-\ndating the expanded parameters $W_{exp}$ and $b_{exp}$ using the cur-\nrent dataset by minimizing the sparse expanded loss func-\ntion $L(D; W) + \\alpha L_s$. During this, we update the expanded\nparameters by computing their gradients with respect to the\nsparse expansion loss and freezing technique, thus applying\nan optimization algorithm:\n$\\displaystyle\\begin{aligned}W_{exp}^{(l)} &\\leftarrow W_{exp}^{(l)} - \\nabla_{W_{exp}} \\Big[L(D; W) + \\alpha L_s\\Big] \\circ (1 - M_f) \\end{aligned}$\nWhere $\\circ$ denotes element-wise multiplication between the\nmatrices, $M_f$ is the binary freezing mask where impor-\ntant parameters are frozen, keeping knowledge from being\nreplaced, as explained below. This process allows the ex-\npanded part to adapt to the learned distribution, prevent-\ning the overwriting of previously learned knowledge while\nincorporating the expanded parameters' capacity for better\nadaptation and faster learning.\nTask-Agnostic Freezing To ensure that the correspond-\ning gradient of the parameters in the freeze mask $M_{fij}$ is\nset to 0 when $W_{ij}$ needs to be frozen. This technique in-\nvolves setting the gradients of masked parameters to zero.\nLet's denote the gradient of the weights in a layer as $\\nabla W$,\nthe masked gradients of the weights as $\\nabla W_{masked}$, and the\nmask that specifies which weights to freeze as $M_f$. This can\nbe represented as:\n$\\displaystyle\\nabla W_{masked} = (1 - M_f) \\bullet \\nabla W$\nThis equation essentially performs an element-wise multi-\nplication between the gradient vector and the mask. Wher-\never the mask is 0, the gradient will be zeroed out, effec-\ntively freezing the corresponding weights during the update\nstep of the optimization algorithm. This approach helps to\npreserve certain knowledge within the model, particularly\nbeneficial in scenarios that require task-agnostic adaptation\nwhile retaining previously learned information. The freeze\nmask will be updated after training each dataset."}, {"title": "Experimental Setup", "content": "Datasets\nThe domain-incremental datasets we use include the Per-\nmuted MNIST, FreshStale, and DomainNet. For evaluation\nin class-incremental setting, we use the class-incremental\nMNIST. Permuted MNIST is a variation of the MNIST\ndataset in which the pixel positions of the images are ran-\ndomly permuted, with no shared features among different\npermutations. Permuted MNIST includes an infinite num-\nber of permutations. Each dataset consists of 70,000 images.\nClass-incremental MNIST dataset partitions the MNIST\ndataset into distinct groups of classes. These groups may\ncontain an uneven number of classes or class overlaps.\nFreshStale (Joseph et al. 2021) dataset comprises a to-\ntal of 14,683 images of six domains of fruits and vegeta-\nbles, classified as either fresh or stale. The total size of\nthe dataset is approximately 2GB. DomainNet (Peng et al."}, {"title": "Addressing Growth-Induced Forgetting Evaluation", "content": "To assess the ability of different CL methods in reducing\nGIFt across diverse applications, we evaluated their layer-\nexpanded versions (having GIFt) using the FreshStale and\nDomainNet datasets. Figure 4 and 5 visualizes the epoch-\nwise average accuracy (AAC) of different baseline meth-\nods on the FreshStale and DomainNet datasets. And Table\n3 shows the AAC, BWT and FWT on the two application\ndatasets. Figure 4 suggests that these CL methods can par-\ntially alleviate the effects of layer expansion-induced GIFt.\nSparseGrow consistently demonstrates the most stable re-\nduction in model forgetting, achieving the highest AAC.\nIn Figure 5, SparseGrow exhibits consistent effectiveness\nin combating GIFt, surpassing other methods in mitigat-\ning the effects induced by layer expansion when compared\nto SGD. Results in Table 3 reveal that existing regulariza-\ntion and rehearsal-based methods exhibit some effectiveness\nin addressing GIFt, but their performance across different\ndatasets lacks consistency. In contrast, SparseGrow consis-\ntently minimizes the impact of growth-induced forgetting\nacross these applications.\nClass-Incremental Learning Setting Results\nTo assess the performance of our method in diverse task-\nagnostic settings, the methods were also evaluated in chal-\nlenging class-incremental learning scenarios, as depicted in\nTable 4. In this setting, similar inputs lead to distinct classes\nassigned to different output layers, often resulting in rapid\nforgetting within a single epoch. Given the heightened in-\ntensity of forgetting and the scale of the datasets, techniques\nsuch as rehearsal and regularization methods struggle to ef-\nfectively maintain accuracy, exhibiting only marginal supe-\nriority over SGD. In contrast, SparseGrow excels in preserv-\ning accuracy significantly, highlighting its potential in miti-\ngating forgetting in task-agnostic environments."}, {"title": "Conclusion", "content": "Our study focuses on the crucial issue of growth-induced\nforgetting caused by improper model growth in task-\nagnostic continual learning. We have identified layer expan-\nsion as a promising fundamental model growth technique.\nOur proposed SparseGrow approach is specifically designed\nto boost adaptability and knowledge retention of layer ex-\npansion. Experimental validations have demonstrated the ef-\nfectiveness of our method in mitigating growth-induced for-\ngetting and improving knowledge retention for incremental\ntasks. Future research could aim to optimize the timing of\nmodel growth and leverage techniques such as neural archi-\ntecture search to further enhance the model's adaptability\nand overall performance with new data."}]}