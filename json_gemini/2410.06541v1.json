{"title": "Chip-Tuning: Classify Before Language Models Say", "authors": ["Fangwei Zhu", "Dian Li", "Jiajun Huang", "Gang Liu", "Hui Wang", "Zhifang Suil"], "abstract": "The rapid development in the performance of large language models (LLMs) is accompanied by the escalation of model size, leading to the increasing cost of model training and inference. Previous research has discovered that certain layers in LLMs exhibit redundancy, and removing these layers brings only marginal loss in model performance. In this paper, we adopt the probing technique to explain the layer redundancy in LLMs and demonstrate that language models can be effectively pruned with probing classifiers. We propose chip-tuning, a simple and effective structured pruning framework specialized for classification problems. Chip-tuning attaches tiny probing classifiers named chips to different layers of LLMs, and trains chips with the backbone model frozen. After selecting a chip for classification, all layers subsequent to the attached layer could be removed with marginal performance loss. Experimental results on various LLMs and datasets demonstrate that chip-tuning significantly outperforms previous state-of-the-art baselines in both accuracy and pruning ratio, achieving a pruning ratio of up to 50%. We also find that chip-tuning could be applied on multimodal models, and could be combined with model finetuning, proving its excellent compatibility.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have experienced rapid development in recent years, achieving surprising success in various domains. Researchers have been scaling up the size of language models to pursue better performance, just as the scaling law (Kaplan et al., 2020) suggests. However, the increasing size of models leads to massive computational costs, posing a challenge to practical deployment and usage.\nModel compression techniques have since been proposed as a solution to relieving computational stress, which would assist in the deployment of large models. Different approaches have been explored to compress language models into more compact versions, including quantization (Liu et al., 2021; Dettmers et al., 2022, 2024), knowledge distillation (Gou et al., 2021; Gu et al., 2023; Ko et al., 2024) and pruning (Ma et al., 2023; Yang et al., 2024; Ashkboos et al., 2024; Men et al., 2024).\nRelevant research (Men et al., 2024) reveals that a fair portion of parameters in large language models are redundant, and removing these parameters would not bring severe damage to the performance of models. Based on the observation, different methods have been designed to identify and remove redundant parameters from LLMs, like layer merging (Yang et al., 2024), width compression (Ashkboos et al., 2024), layer removal (Men et al., 2024) and component removal (Ma et al., 2023). These methods maintain the majority of performance, proving the feasibility of model pruning.\nResearch on model interpretability has shown evidence that language models may develop internal representations for various features like color (Patel and Pavlick, 2022), truthfulness (Burns et al., 2022), chessboard states (Nanda et al., 2023), numbers (Zhu et al., 2024) or even abstract concepts like code errors (Templeton, 2024). These features typically start to form on middle layers and will be carried to subsequent layers (Stolfo et al., 2023). More interestingly, many of these features can be read out by probing techniques (Belinkov, 2022), in the way of training simple classifiers.\nInspired by the discovery that removing late layers of LLMs does not heavily impair network functionality (Men et al., 2024), we hypothesize that the critical features for solving certain problems may begin to form on intermediate layers of LLMs. By probing these necessary features on intermediate layers, we can safely prune subsequent layers with marginal performance loss.\nWe observe that previous research mainly aimed to build a general pruned model that can be directly applied to various downstream tasks. Based on the intuition that different tasks require different subsets of features, we hypothesize that pruning on specific tasks instead of pruning for a general model would yield better results, as the model could better focus on the related features.\nIn this paper, we introduce chip-tuning, a simple and effective structured pruning framework specialized for classification tasks. For a given classification task, we attach probing classifiers named chips to each layer of the language model, and train these classifiers to probe the final classification results from intermediate hidden states. After training, we can then select a chip with satisfactory accuracy, and prune all layers subsequent to the chip to obtain a more compact model for the task. The parameters of the backbone model are frozen throughout the whole process and will not introduce any additional computation cost.\nWe apply chip-tuning to language models with different sizes and families and observe their performance on various classification tasks. Compared with previous pruning methods, chip-tuning demonstrates better performance on classification tasks, and enables more radical pruning that reduces the parameters of models by up to 50% with marginal loss in performance. Additional experiments show that chip-tuning is also compatible with multimodal large language models (MLLMs) and other finetuning methods.\nThe main contributions of our paper can be summarized as:\n\u2022 We propose chip-tuning, a pruning framework for classification tasks that trains probing classifiers attached to certain layers of language models. By removing layers subsequent to the selected classifier, we can effectively reduce the size of the models.\n\u2022 We conduct experiments on different benchmarks, experimental results show that Chip-tuning is able to maintain the performance while reducing the size of models by up to 50%, much outperforming previous state-of-the-art baselines.\n\u2022 We evaluate chip-tuning on multimodal models and finetuned models, whose results prove the excellent compatibility of chip-tuning."}, {"title": "Related Work", "content": "Network Pruning. With the growth in the size of language models, the pruning technique has been proposed to eliminate unnecessary weights or structures in language models, thus accelerating language models. The pruning methods can be generally categorized into two types: unstructured pruning and structured pruning.\nUnstructured pruning methods focus on the level of individual weights, which try to speed up models by increasing the sparsity level of model weights. SparseGPT (Frantar and Alistarh, 2023) reduces the pruning problem to layer-wise sparse regression and incrementally prunes each column in the weight matrix with a sequence of Hessian inverses. Wanda (Sun et al., 2023) enhances the magnitude pruning approach with input activation norms, effectively reducing the complexity of pruning algorithms. RIA (Zhang et al., 2024a) notices that previous methods tend to prune away entire channels of network weights, and mitigates the issue by jointly considering input and output channels.\nStructured pruning methods operate at the level of network structures instead, which compress language models by removing redundant model components. LLMPruner (Ma et al., 2023) employs gradient information as a reference to remove non-critical structures. SliceGPT (Ashkboos et al., 2024) removes rows or columns corresponding to small principal components in the weight matrix to achieve smaller weight matrices. LaCo (Yang et al., 2024) proposes the layer collapse algorithm, which merges adjacent layers while ensuring the representation similarity on few-shot calibration examples. ShortGPT (Men et al., 2024) finds that deep layers of language models are not as effective as expected, and proposes the block importance metric to identify and remove redundant layers. BlockPruner (Zhong et al., 2024) decomposes each Transformer layer into two minimal residual blocks and performs fine-grained block pruning to avoid significant performance loss.\nProbing Language Models. The impressive capability of language models raises the hypothesis that language models have gone beyond mere memorization of surface correlations. Instead, they may learn the principles behind the training data and develop internal representations for features (Belinkov, 2022). A wide variety of features have been detected in the hidden state of language models like color (Patel and Pavlick, 2022) and truthful-"}, {"title": "Methodology", "content": "We illustrate the structure of the chip-tuning framework in Figure 1. The framework first inserts simple probing classifiers named chips to different layers of language models, and then solely trains the chips on task-specific training data.\nFinally, we can select the chip on a fixed layer or with other strategies (see Section 5.2), and layers subsequent to the attached layer will be removed.\n3.1 Chips\nA language model with the decoder-only structure consists of L transformer layers. At every token position t, each transformer layer l takes previous partial sequence $x_{<t}$ as input and outputs new hidden states $x_t^l$.\nAs discovered by previous research on probing, the hidden states of intermediate layers may contain rich features that can be read out by probing classifiers. Chips are simple probing classifiers that try to predict the classification label y from certain hidden states $x_t^l$.\nWe use two types of chips in our experiments: linear chips $p_L$ and two-layer perceptron (2xMLP) chips $p_M$, whose function could be notated as:\n$p_L(x) = \\text{softmax}(Wx + b)$\n$p_M(x) = \\text{softmax}(W_1 \\text{ReLU}(W_2x + b_2) + b_1)$\nwhere $W, W_1, W_2, b, b_1$ and $b_2$ are trainable parameters.\nFor simplicity, we take the hidden state at the last token position (i.e. $t = -1$) as the input vector of chips.\n3.2 Training\nAs the optimal layer $l^*$ for classification chips is initially unknown, we attach a chip $p^l$ to every layer $l$ of the language model, and train these chips simultaneously with standard cross-entropy loss:\n$L^l = y \\log p^l(x_t^l) + (1 - y) \\log(1 - p^l(x_t^l))$\n$L = \\sum_{l=0}^L L^l$\nNote that the parameters in the backbone language model are frozen in the training process, and only the weights of chips would be updated.\n3.3 Layer Removal and Inference\nWe use the straightforward layer removal method to reduce the size of language models. After selecting chip $p^{l^*}$ at layer $l$ as the classification chip, we simply remove all layers after layer $l$ to obtain a smaller model.\nNamely, with chip $p^l$ at layer $l$ finally selected, the pruned model would function as follows:\nChip-tuning also supports inference with multiple chips $p_0, p_1, ..., p_{n-1}$ for different tasks. Assuming that the deepest layer with chips attached is $l_m = max(l_0, l_1, . . ., l_{n-1})$, layers subsequent to $l_m$ will be pruned."}, {"title": "Experiments", "content": "4.1 Experimental Setup\nBenchmarks. We select 4 distinct benchmarks on natural language processing with the form of multi-choice for evaluation: MMLU (Hendrycks et al., 2020), Race (Lai et al., 2017), BoolQ (Clark et al., 2019) and C3 (Sun et al., 2020).\nFurthermore, we introduce three image classification datasets to test the effectiveness of chip-tuning on multimodal large language models (MLLMs): Flowers102 (Nilsback and Zisserman, 2008), StanfordCars (Krause et al., 2013), and Caltech101 (Fei-Fei et al., 2004), each containing 102, 196, and 101 classes respectively.\nModels. Following previous work (Men et al., 2024), we choose 2 model series to evaluate the effectiveness of chip-tuning: Llama2 (Touvron et al., 2023), Baichuan2 (Yang et al., 2023), which share similar decoder-only transformer structure. We use the 7B and 13B versions of Llama2 and Baichuan2 for experiments. For multimodal large language models, we use the 7B and 13B versions of LLaVA-1.5 (Liu et al., 2023) as the backbone model.\nDue to memory constraints, we run 13B models under the precision of 16-bit (fp16) instead of 32-bit (fp32).\nBaselines. We compare our method with several structured pruning methods: LLMPruner (Ma et al., 2023) removes non-critical coupled structures on the basis of gradient information. SliceGPT (Ashkboos et al., 2024) replaces weight matrices with smaller matrices by retaining principal components. LaCo (Yang et al., 2024) merges the layer in language models from deep to shallow, and sets a threshold to prevent excessive merging. ShortGPT (Men et al., 2024) removes redundant layers according to their proposed Block Influence metric, a variant of cosine similarity.\nSettings. For each benchmark, we use at most 20, 000 training data in the corresponding training split of the benchmark to train our chips. The chips are trained with a batch size of 1 for 1 epoch. We use a learning rate of 1 \u00d7 10-5 for our experiments, and set the hidden dimension of MLP chips to 256. All experiments are conducted on a single NVIDIA A100 40GB GPU.\nFor 7B models, we select chips at layer 20 as classification chips; for 13B models, we select chips at layer 25 as classification chips. These settings are equal to the prune ratio of 34.4% and 35.0%, respectively.\n4.2 Main Experiment Results\nTo evaluate the effectiveness of chip-tuning, we conduct experiments on multi-choice style benchmarks commonly used for large language model evaluation."}, {"title": "Analysis", "content": "5.1 Number of Pruned Layers\nChoosing different chips would change the number of pruned layers, and thus affect the classification performance. We conduct experiments on the MMLU dataset with Llama-2 models, and Figure 2 demonstrates the correlation between number of pruned layers and classification accuracy.\nIt can be clearly observed that the classification accuracy exhibits a drastic change on both datasets, increasing from random guess to a decent level, and then fluctuating within a relatively small range. The change happens at around layer 18 for Llama-2-7B and happens at layer 20 for Llama-2-13B, which\n5.2 Chip Selection Strategy\nAside from choosing chips on a fixed layer, there exist other strategies to achieve better performance. We adopt three distinct strategies and evaluate them on Llama-2-7B:\nFixed selects a fixed layer l for all tasks (l = 20 for 7B models and l = 25 for 13B models).\nValidate constructs a small validation set consisting of 200 examples, and chooses the chip which performs best on the validation set.\nOptimal evaluates the performance of all chips, and selects the chip with the highest accuracy. This strategy reflects the upper bound of chip-tuning.\n5.3 Impact of Training Dataset Scale\nTraining data is a crucial component in model training. Considering the scenario where training data is scarce, we test the performance of chip-tuning under different scales of the training dataset.\nFigure 4 shows the classification accuracy under different training dataset scales. The accuracy rapidly increases before 6,000 training examples and reaches a plateau afterward. Although the accuracy may drop at a certain time step, the figure still displays a pattern of slow increase after 6,000 training examples. We draw the conclusion that a"}, {"title": "Conclusion", "content": "In this paper, we propose chip-tuning, a structured pruning framework specialized for classification tasks. Chip-tuning adopts probing classifiers to extract relevant features from intermediate layers of language models, and safely removes subsequent layers without affecting the selected classifier. Experimental results on a variety of models and datasets demonstrate that chip-tuning surpasses previous baseline models on both performance and pruning ratio. Chip-tuning performs well by selecting chips on a fixed layer, and could further achieve a pruning ratio of up to 50% by selecting the optimal chip.\nMeanwhile, we find that chip-tuning is also compatible with multimodal models and finetuned models. Considering the simplicity of layer removal, chip-tuning shows its potential in deploying LLMs under practical scenarios. We hope our work could inspire further research on efficient model pruning."}, {"title": "Limitations and Risks", "content": "Based on the technique of probing, chip-tuning requires the backbone models to contain relevant features in their internal representations. On tasks that the backbone models perform poorly, chip-tuning would not yield satisfactory results either.\nMeanwhile, chip-tuning is designed mainly for classification tasks, which is the reason why we don't evaluate chip-tuning on datasets like HellaSwag that use perplexity-based evaluation methods. Directly applying chip-tuning to generation tasks may lead to unexpected results, and generation-oriented chips remain to be explored in the future."}, {"title": "Datasets", "content": "The properties of datasets we used are shown in Table 7."}, {"title": "Details for Main Experiments", "content": "Figure 5 shows how the performance changes with different number of layers pruned. We can see that the optimal chip varies as the dataset changes. However, pruning around layer 18 of the 7B model (about 40%) and layer 20 of the 13B model (about 50%) is generally acceptable.\nWe also notice that probing late layers of Llama-2-7B leads to worse results, which leaves the question of whether the 7B model \"forgets\" certain information on late layers. The question remains to be explored in the future.\nWe record the layer on which chips show the best performance or highest pruning ratio in Table 6. Notice that we define layer with the highest pruning ratio as the first layer after the drastic change in accuracy, which could be subjective."}, {"title": "Details for Multimodal Experiments", "content": "Figure 6 shows how the performance changes by pruning LLaVA1.5-7B. Different from text datasets, the optimal chip for image classification typically appears on late layers, while chips on early layers also exhibit decent accuracy. Surprisingly, 2-layer MLP chips fail to predict the class of images on StanfordCars. This may be a result of the larger class label set size (196) compared with Flowers102 (102) and Caltech101 (101)."}, {"title": "Details for LoRA Experiments", "content": "Table 5 shows the experimental settings for LORA experiments."}, {"title": "Experiments on Llama3", "content": "We evaluate chip-tuning on Llama3-8B-Instruct (AI@Meta, 2024), one of the up-to-date LLMs. We prune the model to layer 22 in experiments.\nThe experimental results in Table 8 are similar to those in Table 3: applying chip-tuning on Llama3 has minimal impact on classification accuracy, proving that chip-tuning is compatible with Llama3. The optimal performance of chips even outperforms the finetuned LoRA models."}, {"title": "Finetuning Pruned Baseline Models", "content": "For fair comparisions, we finetune the pruned baseline models on the training set of each benchmark to see how they perform with the same data provided. We use Llama2-7B as the backbone model, and finetune LLMPruner (Ma et al., 2023) and SliceGPT (Ashkboos et al., 2024) under their default LORA settings. We do not train LaCo and ShortGPT as we cannot find their official code.\nBoth baseline models are trained with at most 20,000 training data same to these chip-tuning used on each benchmark. The accuracy of finetuned baseline models are obtained by selecting the choice token (for example, \"A\", \"B\", \"C\", \"D\" for 4-choice problems) with the highest generation probability, as free-form generation would yield unexpected results.\nTable 9 shows the result of finetuning LLM-Pruner and SliceGPT with the same data used by chip-tuning. While the finetuned versions achieve higher accuracy than the original version, we can clearly see that chip-tuning greatly outperforms both baselines, further proving the effectiveness of chip-tuning."}]}