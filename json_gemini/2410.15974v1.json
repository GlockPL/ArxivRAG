{"title": "Cross-lingual Emotion Detection through Large Language Models", "authors": ["Ram Mohan Rao Kadiyala"], "abstract": "This paper presents a detailed system description of our entry which finished 1st with a large lead at WASSA 2024 Task 2, focused on cross-lingual emotion detection. We utilized a combination of large language models (LLMs) and their ensembles to effectively understand and categorize emotions across different languages. Our approach not only outperformed other submissions with a large margin, but also demonstrated the strength of integrating multiple models to enhance performance. Additionally, We conducted a thorough comparison of the benefits and limitations of each model used. An error analysis is included along with suggested areas for future improvement. This paper aims to offer a clear and comprehensive understanding of advanced techniques in emotion detection, making it accessible even to those new to the field.", "sections": [{"title": "1 Introduction", "content": "Emotion detection in texts across different languages is a challenging yet crucial task, especially in the context of global digital communication. The ability to accurately identify emotions in text, regardless of the language, can significantly enhance interactions in various domains such as customer service, social media monitoring, and mental health assessments. This paper introduces our approach to cross-lingual emotion detection, which was recently recognized as the top submission in WASSA 2024 Task 2 (Maladry et al., 2024). Our system leveraged the capabilities of several open source and proprietary Large Language Models (LLMs), including GPT-4 (OpenAI, 2024) and Claude-Opus (Anthropic, 2024) in a zero-shot configuration, as well as LLAMA-3-8B (Touvron et al., 2023), Gemma-7B (GemmaTeam, 2024), and Mistral-v2-7B (Jiang et al., 2023), which were fine-tuned. To assess the robustness and efficiency of these models, we conducted tests in both 4-bit and 16-bit precision. This varied precision testing helps in understanding the trade-offs between computational efficiency and model performance. Additionally, we compared the performance of our models against the top submission's (Patkar et al., 2023) approach on a similar monolingual task from the previous years' shared task. Furthermore, we experimented with enhancing model performance by incorporating additional training data from previous editions of the shared task, specifically WASSA 2023 (Barriere et al., 2023) and WASSA 2022 (Barriere et al., 2022) emotion classification task datasets."}, {"title": "2 Dataset", "content": "The dataset consisted of texts belonging to one the 5 languages - Dutch, English, French, Russian and Spanish annotated as one of the 6 classes - Anger, Fear, Love, Joy, Neutral and Sadness. The distribution of languages and each class in each of the datasets can be seen in Table 1 and Table 2."}, {"title": "3 System Description", "content": "The non-proprietary LLMs were fine-tuned over just the training dataset over 5 epochs with a learning rate of 0.0002 and weight decay of 0.01. The proprietary systems were tested with various prompt over the development set and the best performing prompt was used to make predictions over the test set. Additionally the previous year's benchmark was also tested alongside by replacing ROBERTa (Liu et al., 2019) with XLM-ROBERTa (Conneau et al., 2020). Additionally other ensembles like majority vote, model selection based on features were also tested. The Code and Models are available over the GitHub repository\u00b9 and Huggingface2 3 4. The primary metric was weighted F1 score, additionally Precision and Recall have also been observed."}, {"title": "3.1 Results Comparison", "content": "The results using each of the models on the development set by fine-tuning over 3 epochs on the training set can be seen in Table 3. Other approaches"}, {"title": "3.2 Error Analysis", "content": "Each of the models had its own advantages and drawbacks likely due to the differences in the pre-training data used by each of the models. The performance of each of the models was observed separately on each of the languages over the development set, this can be seen in Table 5. It can be seen that certain models performed better on some of the languages. This led to the conclusion that selecting an appropriate model based on language of the text to be classified might yield better results."}, {"title": "3.3 Our System", "content": "Several approaches of using ensembles based on majority voting, model selection based on macro F1, micro F1 and the weighted F1 scores were tested. The best performing system uses a majority voting criteria from the 5 models used. In cases where consensus is not achieved i.e no clear majority, the output of the model with highest weighted F1 score was chosen as the final label."}, {"title": "3.4 Possible Extensions", "content": "As seen in Table 5, each of the models had their own advantages and disadvantages with varying performances on each language. It is likely that adding more models into the system and features like text length or utilizing different models for binary classification of whether the given text belongs to a class. This can be seen in Table 6 displaying varying effectiveness of each model in predicting each emotion. A viable approach would be predicting each emotion as a binary task and then using other methods in cases where none or more than one class ends up as true. The fine-tuned LLMs were loaded in 4bit precision and later fine tuned using LORA (Hu et al., 2021) and tested in both 4bit precision and 16bit precision versions. The drop in performance in 4bit overall was minimal, however in many cases the predictions in 4bit ended up as correct while 16bit were incorrect. Another viable approach is to pick the top 2 likely class labels for each of the texts' predictions and using other methods to classify more effectively."}, {"title": "4 Conclusion", "content": "It can be seen from Table 4 that ensemble models have achieved a significantly better result over di-"}, {"title": "A Text Translation", "content": "Several translation models and approaches have been tested, with google-translate and utilizing LLMs for translating being the better suited approaches. However the texts were returned without translation in code-mixed text cases is some instances. Despite the higher cost using LLMs worked perfectly in detecting the main language and also to test by translating all texts to English."}, {"title": "B Prompts Used", "content": "The prompts used in the system and other analysis tasks were as follows :\n\u2022 Language Detection of texts : \"Classify given texts as English,Dutch,French,Spanish,Russian. Respond only with one word based on which language the text is in.\"\n\u2022 Translation completely to English : \"Translate the text to English. Respond with the same text if already in English completely.\"\n\u2022 Classification of Emotion (Proprietary):\n\"Classify given texts as Neutral, Joy, Anger, Love, Sadness, Fear. Respond only with one word based on which would be closest classification of user emotion from the text.\"\n\u2022 Classification of Emotion (fine-tuned) : \"Given the input text classify it based on what emotion is being exibited among the following : Joy/Neutral/Anger/Love/Sadness/Fear. Respond with only one emotion only among the options given. Respond with only one word and nothing else.\""}, {"title": "C Hyperparameters Used", "content": "Among the hyperparameter space explored for each approach, the best results were obtained with the following values. Rest of the parameters were un-specified during training and hence the default values have been used."}, {"title": "D System Replication Instructions", "content": "The system can be replicated using the hyperparameters mentioned in Table 7 with seed value of 1024. The models used are available on huggingface in various configurations i.e LoRA adapters, 16bit and 4bit precision models.\nLLAMA-3-8B model trained with additional data from previous years workshop datasets :\n\u2022 1024m/EXALT-1A-LLAMA3-5C-Lora\n\u2022 1024m/EXALT-1A-LLAMA3-5C-16bit\n\u2022 1024m/EXALT-1A-LLAMA3-5C-4bit\nLLAMA-3-8B model trained with datasets translated to English using GPT-4 :\n\u2022 1024m/EXALT-1A-LLAMA3-5B-Lora\n\u2022 1024m/EXALT-1A-LLAMA3-5B-16bit\n\u2022 1024m/EXALT-1A-LLAMA3-5B-4bit\nLLAMA-3-8B model used in the system (main) :\n\u2022 1024m/EXALT-1A-LLAMA3-5A-Lora\n\u2022 1024m/EXALT-1A-LLAMA3-5A-16bit\n\u2022 1024m/EXALT-1A-LLAMA3-5A-4bit\nGEMMA-8B model used in the system (main) :\n\u2022 1024m/EXALT-1A-GEMMA-5A-Lora\n\u2022 1024m/EXALT-1A-GEMMA-5A-16bit\n\u2022 1024m/EXALT-1A-GEMMA-5A-4bit\nMistral-7B model used in the system (main) :\n\u2022 1024m/EXALT-1A-MISTRAL-5A-Lora\n\u2022 1024m/EXALT-1A-MISTRAL-5A-16bit\n\u2022 1024m/EXALT-1A-MISTRAL-5A-4bit"}]}