{"title": "Synthetic Feature Augmentation Improves Generalization Performance of Language Models", "authors": ["Ashok Choudhary", "Cornelius Thiels", "Hojjat Salehinejad"], "abstract": "Training and fine-tuning deep learning models, especially large language models (LLMs), on limited and imbalanced datasets poses substantial challenges. These issues often result in poor generalization, where models overfit to dominant classes and underperform on minority classes, leading to biased predictions and reduced robustness in real-world applications. To overcome these challenges, we propose augmenting features in the embedding space by generating synthetic samples using a range of techniques. By upsampling underrepresented classes, this method improves model performance and alleviates data imbalance. We validate the effectiveness of this approach across multiple open-source text classification benchmarks, demonstrating its potential to enhance model robustness and generalization in imbalanced data scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "Language models are computational frameworks designed to understand and generate human language by analyzing textual data to capture patterns and structures. Advanced models, such as bidirectional encoder representations from transformers (BERT) [1] and generative pre-trained transformer 4 (GPT-4), leverage deep learning and large datasets to excel at natural language processing tasks like text generation, translation, and sentiment analysis.\nFine-tuning large language models (LLMs) on limited-imbalanced datasets [2] is challenging, often leading to overfitting and biased predictions [3]. For instance, in healthcare, models trained on datasets with underrepresented rare diseases may overlook these cases, reducing diagnostic accuracy. This issue is prevalent across domains where class imbalance skews model performance toward majority classes.\nTo overcome above challenges, methods like data augmentation [4], transfer learning [5], synthetic minority over-sampling technique (SMOTE) [6], variational autoencoders (VAEs) [7], and data synthesizing [8] can be used. Transfer learning allows pre-trained models to adapt to specific domains, while ensemble approaches enhance prediction stability. These techniques enable more robust, equitable models capable of handling real-world data complexities.\nIn this study, we explore the use of synthetic data generation within the embedding spaces of pre-trained models, such as BERT, to address model bias caused by class imbalance. By leveraging these embedding spaces, we employ advanced data augmentation techniques, including SMOTE and VAEs, to generate synthetic samples that closely represent the original data distributions. Our results show that incorporating these synthetic samples into imbalanced training datasets significantly enhances classification performance compared to training without synthetic augmentation. We assess the effectiveness of these methods on various benchmark datasets, demonstrating their potential to improve model robustness and fairness in real-world applications."}, {"title": "II. EMBEDDING SPACE AUGMENTATION MODELS", "content": "Figure 1 shows different steps of the proposed method. Let D = {(x1, y1), ..., (xn, yn)} be a dataset of text samples xi and corresponding labels yi. A nonlinear embedding function maps xi to a d-dimensional vector fi as\nfi = \\phi(xi), (1)\nwhere it captures intricate semantic and syntactic relationships within the text by mapping words or tokens into continuous numerical spaces. The generated embeddings are then passed to a synthetic embedding generator \\phi(fi), which synthesizes embedding vectors for the minority data class. The synthesized embedding vectors are then combined with the real embedding vectors to form a balanced training dataset. This balanced dataset is then used to train a classifier [8], [9]."}, {"title": "A. Synthetic Minority Over-sampling Technique", "content": "SMOTE is a popular method for addressing class imbalance by generating synthetic minority class samples. In embedding space, it captures semantic and syntactic relationships encoded in the embeddings. The technique identifies the k-nearest neighbors of each minority class sample in the embedding space. For a query vector fi \\in F, the goal is to find the nearest neighbor vector fnn \\in F s.t. fnn \\neq fi as\nfnn = arg min d(fi, fj), (2)\nfi\u2208F, fifi\nwhere d(fi, fj) is the distance between fi and fj. For a given minority class embedding fi \u2208 Rd, a synthetic sample fnew is generated along the line segment connecting fi and one of its randomly selected k-nearest neighbors finn \u2208 Rd as\nfnew = fi + \\lambda (fi,nn - fi), (3)"}, {"title": "B. Borderline Synthetic Minority Over-sampling Technique", "content": "This variation of SMOTE generates synthetic samples near the decision boundary between the majority and minority classes [10]. For each minority class embedding fi \u2208 Fmin, where Fmin is the set of minority embeddings, its k-nearest neighbors from both the minority and majority classes are identified in the embedding space. The sample fi is considered a borderline example if most of its neighbors belong to the majority class. If fi is a borderline sample, new synthetic samples are generated between fi and one of its minority class neighbors finn using Eq. (3). This ensures that the synthetic samples are generated close to the borderline minority embeddings, improving the classifier's ability to correctly identify the decision boundary. By focusing on borderline examples, it aims to increase the classification accuracy for the minority class while reducing the risk of generating noisy or redundant samples from the minority class core."}, {"title": "C. Adaptive Synthetic Sampling", "content": "The adaptive synthetic sampling (ADASYN) [11] generates synthetic samples for the minority class based on the difficulty of learning those samples. Given a dataset F with majority class Fmaj and minority class Fmin, where |Fmaj| = Nmaj and |Fmin| = Nmin such that Nmaj > Nmin, the goal is to generate synthetic samples to balance the class distribution."}, {"title": "D. Random Oversampling", "content": "Random Oversampling (ROS) [12] is a technique used to address class imbalance by replicating samples from the minority class. The goal is to increase the size of the minority class by sampling with replacement to Noversample = Nmaj \u2014 Nmin. Assuming fi represent a randomly chosen embedding from Fmin, where i = 1, 2, ..., Noversample, the new dataset after oversampling becomes\nF' = Fmaj \\cup (Fmin \\cup {f1, f2, ..., fNoversample }), (6)\nresulting in a balanced dataset where |Fmaj| = |Fmin| = Nmaj."}, {"title": "E. Variational Autoencoders", "content": "The VAEs are generative models that can be utilized to generate synthetic data by learning a probabilistic latent space representation of the input data. In this work, VAEs can be used as the embeddings generator \\phi(\u00b7), Figure 2. Given the set of embedding vectors fi \u2208 F, VAEs aim to learn a low-dimensional latent representation z by modeling the joint distribution pa (f, z), where a represents the parameters of the model. The joint distribution is defined through the likelihood of the observed data given the latent variables, pa (f|z), and a prior distribution over the latent variables, p(z). Training VAEs involves maximizing the Variational Evidence Lower Bound (ELBO) on the marginal likelihood pa (f), which serves as a computationally feasible surrogate for the true marginal likelihood. The ELBO is given by\nELBO = Ez~qp(z|f) [log pa(f|z)] \u2013 KL (q\u00df(z|f)||p(z)), (7)\nwhere q\u00df(z f) is the variational posterior (encoder) parameterized by \u00df, approximating the true posterior distribution of the latent variables given the data. The term KL (qp(z|f)||p(z)) represents the Kullback-Leibler divergence between the variational posterior and the prior distribution over the latent variables.\nBy maximizing the ELBO, the VAE effectively balances the reconstruction accuracy of the input embeddings f; and the regularization imposed by the latent space. This results in a compact and meaningful representation of the embeddings. Once the VAE is trained, we can generate new synthetic embeddings by sampling from the latent space and passing these samples through the decoder network pa(f|z). These synthetic embeddings are then used to augment the minority class in the embedding space, addressing class imbalance in the dataset."}, {"title": "III. EXPERIMENTS", "content": "We use publicly available datasets with binary and multi-class labels, deliberately down-sampled to create imbalanced versions. Synthetic data is generated using the discussed techniques to balance the training set. Performance is evaluated on a balanced test set using 10-fold cross-validation, with datasets split into 80% training, 10% validation, and 10% test. We compare model performance on the original imbalanced datasets and the balanced datasets with synthetic data. For the classification task, we employed an MLP with a single hidden layer comprising 128 hidden units. This choice of model architecture was intended to balance complexity and performance, providing a robust framework for our comparisons."}, {"title": "A. Datasets", "content": "IMDB Dataset: The IMDB dataset [13] contains 50,000 movie reviews evenly split into 25,000 positive and 25,000 negative reviews, making it ideal for binary sentiment classification. The reviews are preprocessed and substantial in length, supporting detailed sentiment analysis and modeling.\nSST-2 Dataset: The SST-2 dataset [14], a subset of the Stanford Sentiment Treebank, is designed for binary sentiment classification at the sentence level using movie reviews from Rotten Tomatoes.\nAG News Dataset The AG News dataset [15] includes over 1 million news articles categorized into four classes: World, Sports, Business, and Science/Technology, with 120,000 training and 7,600 test samples."}, {"title": "B. Results", "content": "Performance of the models were compared on imbalanced datasets versus those augmented with synthetic data. For minority classes, we systematically evaluated sample sizes in powers of two (2m, where 2 < m < 10) to assess their impact on performance. Using techniques like SMOTE and VAE, models trained with SMOTE-augmented data consistently achieved higher accuracy on balanced test datasets. Figures 3 and 4 illustrate the performance of different embedding augmentation methods in classification of the text samples in the IMDB, SST-2, and AG News datasets.\nThe IMDB and SST-2 datasets, being binary classification tasks, mitigate class imbalance to some extent. However, on the multiclass AG News dataset with four classes, SMOTE outperformed VAE in accuracy, despite less pronounced overall improvement. This demonstrates SMOTE's versatility and effectiveness across classification tasks. The results highlight SMOTE's ability to enhance model performance across sample sizes, emphasizing the importance of appropriate augmentation techniques for imbalanced datasets."}, {"title": "C. Embedding Space Analysis", "content": "Figure 5 presents the 2D projection of embeddings using uniform manifold approximation and projection (UMAP) [16] and t-distributed stochastic neighbor embedding (t-SNE) [17] from the AG News dataset, comparing results with and without SMOTE oversampling. Similar visualizations are provided for the IMDB and SST-2 datasets in Figures 6 and 7, which shows performance of the proposed method in synthesizing samples in the embedding space."}, {"title": "IV. CONCLUSION", "content": "This study evaluated SMOTE and VAE for handling class imbalance in binary (IMDB, SST-2) and multiclass (AG News) text classification tasks. By generating synthetic data for minority classes and comparing performance, we found that SMOTE consistently outperformed VAE in binary tasks and showed better results in multiclass scenarios, though improvements were less pronounced. These findings highlight SMOTE as a reliable method for addressing class imbalance and underscore the importance of effective data augmentation in improving model performance on imbalanced datasets."}]}