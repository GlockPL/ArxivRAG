{"title": "Tonguescape: Exploring Language Models Understanding of Vowel Articulation", "authors": ["Haruki Sakajo", "Yusuke Sakai", "Hidetaka Kamigaito", "Taro Watanabe"], "abstract": "Vowels are primarily characterized by tongue position. Humans have discovered these features of vowel articulation through their own experience and explicit objective observation such as using MRI. With this knowledge and our experience, we can explain and understand the relationship between tongue positions and vowels, and this knowledge is helpful for language learners to learn pronunciation. Since language models (LMs) are trained on a large amount of data that includes linguistic and medical fields, our preliminary studies indicate that an LM is able to explain the pronunciation mechanisms of vowels. However, it is unclear whether multi-modal LMs, such as vision LMs, align textual information with visual information. One question arises: do LMs associate real tongue positions with vowel articulation? In this study, we created video and image datasets from the existing real-time MRI dataset and investigated whether LMs can understand vowel articulation based on tongue positions using vision-based information. Our findings suggest that LMs exhibit potential for understanding vowels and tongue positions when reference examples are provided while they have difficulties without them. Our code for dataset building is available on GitHub 1.", "sections": [{"title": "Introduction", "content": "In phonetics, vowels are distinguished and described by focusing on tongue positions and lip shape. Beginning with Jones (1917), humans have explained vowels based on tongue positions during articulation. Human speakers are aware of speech mechanisms with training through introspection of experience and observations of visual information (e.g., MRI). For example, when pronouncing the English word \"image,\" speakers can perceive and explain that the initial sound is produced by positioning the tongue forward and high in the mouth.\nMoreover, speakers can relatively recognize vowels, using a certain vowel as a reference point. This is the method Jones (1917) employed to introduce the cardinal vowels, demonstrating their grasp of tongue positions through introspection and objective observations, and to link this understanding to the concept of vowel articulation. This knowledge helps explain to language learners how to pronounce and describe linguistic phenomena.\nLanguage Models (LMs) are trained on a large amount of data that includes linguistic and medical fields. Our preliminary studies showed that LMs know vowel pronunciation and the correlation between vowels and tongue positions as textual knowledge (see Section 2). To determine if LMs comprehend articulation relative to the articulatory organs, multi-modal information is essential. Multi-modal LMs capture not only textual information, but also images, videos, and audio (Zhou et al., 2023; Li et al., 2024; Gemini Team, 2024). Moreover, their application is expanding to more specialized fields, such as clinical tasks involving the detection and explanation of diseases from clinical images such as CT and MRI (Yan et al., 2024; Pal and Sankarasubbu, 2024). However, it is known that the alignment among modalities, such as images and text, is often weak in multi-modal LMs, and it remains unclear whether these models truly understand such interactions (Cao et al., 2022; Kawaharazuka et al., 2024; Hayashi et al., 2024)."}, {"title": "2 Backgound and Related Works", "content": ""}, {"title": "2.1 Tongue and Vowels in Linguistics", "content": "In phonetics, vowels are described by the height and blackness of the body of the tongue in the oral cavity. The tongue positions are measured by using relative positions in the vowel categories rather than absolute positions (Jones, 1917; Knight and Setter, 2021). These characteristics describe several linguistic phenomena (Knight and Setter, 2021) and are also adopted in the International Phonetic Alphabet (IPA) (International PhoneticAssociation, 1999). For example, when pronouncing the vowel /i/ in Figure 1, the tongue is moved forward and raised higher, creating a narrow space in the anterior region of the oral cavity.\nThe five-vowel system (/a/, /i/, /u/, /e/, /o/) is one of the most common vowel inventories in world languages (Kubozono, 2015), and this size of a vowel inventory accounts for more than half of languages (Maddieson, 2013). These vowels are characterized by tongue height and backness: high-mid-low and front-back 2. The tongue positions of each vowel are /a/ (low-back), /i/ (high-front), /u/ (high-back), /e/ (mid-front), and /o/ (mid-back), as shown in Figure 3. The tongue position for the vowel /o/ is intermediate between /a/ and /u/.\nSpeakers perceive and understand tongue positions during speech, allowing them to pronounce a vowel when instructed to say it between /a/ and /e/ with training. This also enables us to explain linguistic phenomena related to tongue positions, such as vowel harmony (Bybee, 2015). However, studying these phenomena sometimes requires introducing new but consistent principles, as introduced by Ko (2012), Joseph (2018), and Wang (2020). If LMs can understand the articulation, they can provide researchers with valuable insights into these underlying principles."}, {"title": "2.2 Tongue and Vowels in Language Models", "content": "Some studies focus on vowels and tongue-related information. Steuer et al. (2023) found that phoneme-level LMs capture vowel harmony, a linguistic phenomenon related to tongue position. Cotterell and Eisner (2017) predicted vowel inventories by introducing models based on stochastic point processes. They used formant information to identify vowels influenced by tongue position. Cotterell and Eisner (2018) also used formant information to"}, {"title": "2.3 Application of Understanding Tongue Positions", "content": "Tongue positions are also used to improve speech synthesis systems. Abeysinghe et al. (2022) found that the vowel space of a speech synthesis model changes during fine-tuning and that it can be visualized. Wu et al. (2023) introduced a speech synthesis method that uses MRI-based features and demonstrates that MRI provides useful features for synthesizing speech. If LMs possess the capability to comprehend the relationship between tongue position and articulation, it could further advance these studies, contributing to the synthesis of fluent or intentionally disfluent speech as human speech."}, {"title": "2.4 Preliminary Studies", "content": "We investigated whether LMs know the relationship between vowels and tongue positions using GPT-40 (OpenAI, 2024). demonstrates that they should be able to coherently explain the relationship between tongue position and vowel articulation like human speakers. Furthermore, Table 2 shows that they are also capable of predicting a vowel from the given tongue position. We used each tongue position of the five vowels as a query, and GPT-40 answered the correct vowel. Therefore, these preliminary studies show that LMs know how to pronounce vowels."}, {"title": "3 Dataset: Tonguescape", "content": "When predicting vowels from a real-time MRI, speakers will (1) detect the moment of vowel articulation in the video, (2) observe the tongue and determine its position, and then (3) select a vowel. We propose a QA dataset for vowel prediction from real-time MRI recordings of tongue movements during vowel articulation comprising three steps, with each step corresponding to these stages of human perception as shown in Figure 2. We curated and annotated videos and images of tongue positions from the Real-time MRI Articulatory Movement Database (rtMRIDB) (Maekawa, 2022), which comprises real-time MRI recordings of ar-"}, {"title": "3.1 Real-time MRI Articulatory Movement Datasets", "content": "The Real-time MRI Articulatory Movement Database (rtMRIDB) (Maekawa, 2022) is the dataset that contains videos recording the articulations of Japanese phonemes. The dataset consists of utterances by 22 Japanese speakers with physiological variation. It captures the lateral view of the speech production process including tongue and pharyngeal movements. Each video consists of sequential MRIs and aligned audio files. The data were recorded with 14 frame-per-second (FPS) or 27 FPS. Each video starts with the resting position, which is in non-speaking states, and ends with the resting position.\nJapanese dataset is suitable for our study because it is distinguished primarily based on tongue position and one of the most common vowel inventories (Kubozono, 2015)."}, {"title": "3.2 Tonguescape", "content": "VowelVideo We extracted 120 silent real-time MRI videos from the rtMRIDB where the five basic isolated vowels (/a/, /i/, /u/, /e/, /o/) were pronounced. We split 5 samples (one for each vowel) as training data, another 5 samples as validation data, and the remaining 110 samples as test data. Since the tongue position during vowel articulation is not significantly affected by the pronunciation of preceding consonants, we also curated 1,653 videos where a vowel follows a consonant, such as in /ka/ and /na/, as additional training data, totaling 1,658 videos. The test and validation data consist of videos for isolated vowel pronunciations. The videos capture the entire articulation process, starting from a non-speaking state, progressing through the articulation, and returning to a non-speaking state. Each video is around 1 or 2 seconds long and consists of about 14 or 27 frames per second.\nVowelImage As shown in Figure 2, we manually selected one of the most characteristic and representative frames from the video of the five basic isolated vowels in the VowelVideo dataset. This allows us to separate the process of selecting representative moments from the video and estimating vowels from the characteristics of vowel images.\nVowelImageWithGuide Some studies show that adding markers as guides in images improves the performance for some tasks (Shtedritski et al., 2023). Inspired by this, we added guide markers to all images in the VowelImage dataset to facilitate the identification of tongue position within the oral cavity, similar to human perception, as shown in Figure 2. We drew ellipses as simple guides within the oral cavity in the images to encompass the palate, the body of the tongue during low vowel articulation, the tip of the tongue during front vowel articulation, and the root of the tongue during back vowel articulation. These guides were automatically drawn at similar coordinates across all images in the dataset and were manually checked."}, {"title": "3.3 Dataset Difficulty", "content": "Comparing the difficulty levels of VowelVideo and VowelImage is challenging. While VowelImage allows for vowel prediction by observing and interpreting tongue position at a moment of articulation, it lacks the relative tongue movement information available in VowelVideo."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Experimental Settings", "content": "We investigate the ability of LMs to predict vowels from input video/images using our dataset. Figure 4 shows the prompt and example of our experi-"}, {"title": "4.2 Few-shot Examples as Relative Positions of the Tongue: Bridging Linguistics", "content": "We divide our research question into those concerning absolute position and those concerning relative position. The differences between these two positions are shown in Figure 5. This is based on the fact that vowels can be distinguished by considering the relative position of the tongue (Jones, 1917; Knight and Setter, 2021). \u201cAbsolute position\u201d of Figure 5(a) means an actual tongue position in an oral cavity when pronouncing a vowel. \u201cRelative position\" of Figure 5(b, c) is a tongue position in an oral cavity when pronouncing a vowel regarding a position when pronouncing another vowel. There are two types of \u201crelative position\u201d in this context: relative position in terms of comparison with another vowel and relative position arising from physiological variations between speakers. Dividing it in such a way allows us to gain a more detailed understanding of the capabilities of LMs. Understanding absolute positions means they know the relationship between tongue positions and vowels and can read tongue positions in an image and associate that with a specific vowel. Understanding relative positions means they understand the relationship between each vowel and can apply the knowledge of phonetics to predict vowels. We can evaluate the capability of recognizing relative posi-"}, {"title": "4.3 Results", "content": "VowelVideo Table 3 shows that the models struggle to predict vowels from videos correctly. VideoLLaMA2 (FT) performs better than the original model, but the accuracy is still close to the chance rate. This suggests that they are not well aligned for vowel information with video or sequential frames.\nVowelImage As shown in Table 3, the accuracy of each model is approximately 20% in the zero-shot setting. The accuracy improves in the one-shot and five-shot settings, particularly with Qwen2-VL-72B-Instruct. This suggests that while some LMs can infer vowels by considering tongue positions relatively, they lack an understanding of absolute positions similar to that of linguists.\nVowelImageWithGuide Table 3 shows that GPT-4o and Qwen2-VL-72B-Instruct performed much better in five-shot settings compared to VowelImage datasets. However, other LMs still face challenges in predicting vowels from images with guides. This suggests that LMs struggle to infer vowels when grounding in vision information."}, {"title": "5 Discussions and Analysis", "content": "We will focus on important aspects in the following sections and defer more discussions to Appendix C."}, {"title": "5.1 Comparison of each Dataset", "content": "VowelVideo and VowelImage Table 3 shows that it is easier for some models and a human to predict vowels from videos than from images in the zero-shot setting. The video data contains not only the moment of articulation but also states before and after the articulation. For example, we can look at relative positions during pronunciations. This is a similar situation when using VowelImage in one-shot or five-shot settings. Considering that some"}, {"title": "5.2 CLIP, LM and Tongue Positions", "content": "VowelImage and VowelImageWithGuide in Table 3, it is evident that most models in the zero-shot setting perform worse than CLIP. However, in the five-shot setting, some models outperform the fine-tuned CLIP. The underperformance of LMs compared to CLIP in the zero-shot setting suggests that LMs have difficulty predicting vowels based on absolute positions and understanding the association between vowels and tongue positioning in their training techniques. In contrast, the five-shot enhancement implies that they can consider relative positions. An analysis of the performance relative to CLIP reveals the ability of LMs to understand absolute positions and relative positions."}, {"title": "5.3 Case Study: Analysis of the Results", "content": "We have analyzed the results of all LMs and highlighted two: a proprietary LM (GPT-40) and an open LM (Qwen2-VL-72B-Instruct). The results of the other models can be found in Appendix C."}, {"title": "5.3.1 Results of GPT-40", "content": "Zero-shot Setting The zero-shot setting results suggest that it is challenging for the model to read absolute tongue positions and predict vowels.\nOne-shot Setting As shown in Figure 7, the one-shot setting results show that the given vowel was not reproduced in the output regardless of which vowel (/a/, /i/, /u/, /e/, or /o/) was provided as an example. This suggests two key points: understanding of tongue position variability and interpretation of one-shot setting accuracy. The model does not seem to comprehend that there is some degree of freedom in tongue positioning during articulation. If it did, it would likely estimate the one-shot vowel to be the one with a tongue position closest to the provided image. This effectively reduces the task from a five-choice problem to a four-choice problem. Thus, the improvement in accuracy can be attributed to the reduction in choices rather than the model's understanding of relative tongue positions. This result suggests that it struggles to recognize relative tongue positions given only one image.\nFive-shot Setting Figure 8 illustrates that the five-shot setting results show that the output is mainly /a / or /i /. The model tended to output the high vowel /i/ when input images that indicate higher vowels (/i/, /u/) were provided, while it frequently outputs the low vowel /a/ for others. The model seems to be able to recognize tongue height."}, {"title": "5.3.2 Results of Qwen2-VL-72B-Instruct", "content": "Zero-shot Setting Table 3 indicates that this model performs worse than the baseline and random choice. One of the reasons is that it did not predict any vowel for some images. Although this leads to poor performance, it means that the model predicted vowels considering the given information. However, even given these facts, the model struggles to predict vowels.\nOne-shot Setting In contrast to the results of GPT-40 shown in Figure 7, in most cases, this model predicted the vowel given as a one-shot example as shown in Figure 9. However, the model provided a vowel that has the same backness property when given /e/ and /o/ as a one-shot example. This reveals that the model considers the positions of the tongue.\nFive-shot Setting Figure 10 illustrates that most of the predictions are vowels /a/, /i/, and /u/. In numerous instances, the model identified the vowel as /e/ and /o/ as /a/. This misclassification could be attributed to the fact that both /e/ and /o/, like /a/, are categorized as non-high vowels."}, {"title": "5.4 Analysis of Fine-tuning Failuer", "content": "We have fine-tuned VideoLLaMA2 and Qwen-VL-Chat, but they show limited improvement or decreased performance. The limited improvement is caused because the training dataset is small, although the loss decreased for each model. The decreased performance could be caused because this model outputs one of two vowels after fine-tuning while outputting the same vowel before fine-tuning. The fine-tuning adds variation to the output of this model, which may have resulted in lower performance for VowelImageWithGuide."}, {"title": "5.5 Do LMs Consider Tongue Position?", "content": "In the one-shot setting, GPT-40 tended to infer mid or low vowels for most images of mid vowels while Qwen2-VL-72b-Instruct inferred high vowels as illustrated in Figures 7 and 9. Figures 8 and 10 illustrate that, in the five-shot setting, both GPT-40 and Qwen2-VL-72b-Instruct inferred /a/ for most images of mid vowels. These findings clearly demonstrate that the models consistently predict vowels when analyzing images of mid vowels /e/ and /o/ in each setting."}, {"title": "5.6 Do LMs Detect Tongue Positions?", "content": "Table 4 shows the results of predicting tongue height and backness both directly from images and by converting the estimated vowels to tongue positions, using GPT-40 and Qwen2-VL-72B-Instruct in the five-shot setting. These results show that while the model is capable of predicting vowels from some images, it still struggles to predict tongue positions accurately. These results indicate that LMs can consider tongue positions relatively."}, {"title": "6 Conclusion: Do LMs Associate Tongue Positions with Vowel Articulation?", "content": "We investigated the capability of LMs to understand the relationship between tongue positions and vowels to address our research question.\nAs shown in Table 2, LMs appear to understand the relationship between the positions of the tongue and the vowels. However, the results of the zero-shot setting reveal that they have difficulty determining the tongue positions from a given image, as mentioned in Section 5.2. The results of fine-tuned models also demonstrate the challenge of learning the relationship through objective observation. It can be posited that LMs appear to lack comprehension of tongue positions, as can be deduced from empirical knowledge and objective observation.\nWe have also discussed in Sections 5.1, 5.2, and 5.5 that some LMs can consider relative positions to predict vowels. These findings reveal that the LMs predict vowels with respect to other pairs of tongue positions and vowels while it is challenging to predict unseen vowels as shown in the results of the one-shot setting. We can conclude that LMs, to a certain extent, associate tongue positions with the concept of vowel articulation.\nIn conclusion, our findings indicate whether LMs can comprehend the vowel articulation that linguists have long sought to decode. While LMs faced challenges in our experiments, the performance of LMs can be improved like that of linguists when given examples. We not only hope these findings will apply to large-scale linguistic analysis, speech synthesis, and educational fields but also wish for further research of languages."}, {"title": "7 Limitations", "content": "Multi-modal Language Models While our experiments were conducted using a limited set of LMs, there are few models capable of processing videos or multiple images simultaneously. Given this context, our research can be considered comprehensive and a reasonable selection within the current state."}, {"title": "8 Ethical Considerations", "content": "License of Source Dataset In this study, we have used the Real-time MRI Articulatory Movement Database (rtMRIDB) (Maekawa, 2022) to create our dataset. This dataset is licensed only for research purposes. Since we have been permitted to use this dataset by the providing institution, there are no licensing issues.\nIdentifying Infomation and Offensive Content\nOur datasets are created from the Real-time MRI Articulatory Movement Database. We have confirmed that the original dataset does not contain any personally identifying information or offensive content, thus our dataset also does not contain them. We have also confirmed that no inappropriate content is included in our dataset.\nUse of AI Assistants In this study, we have used GitHub Copilot as an AI assistant for coding support."}, {"title": "9 Acknowledgement", "content": "In this study, we used \"Real-time MRI Articulatory Movement Database - Version 1 (rtMRIDB)\" developed by National Institute for Japanese Language and Linguistics and provided by Speech Resources Consortium, National Institute of Informatics. We thank Chihiro Taguchi and the anonymous reviewers for their valuable comments and suggestions. This work was supported by JSPS KAKENHI Grant Number JP23H03458."}]}