{"title": "PassionNet: An Innovative Framework for Duplicate and Conflicting Requirements Identification", "authors": ["Summra Saleem", "Muhammad Nabeel Asim", "Andreas Dengel"], "abstract": "Early detection and resolution of duplicate and conflicting requirements can significantly enhance project efficiency and overall software qual-ity. Researchers have developed various computational predictors by leveraging Artificial Intelligence (AI) potential to detect duplicate and conflicting requirements. However, these predictors lack in performance and requires more effective approaches to empower software development processes. Following the need of a unique predictor that can accurately identify duplicate and conflicting requirements, this research offers a comprehensive framework that facilitate development of 3 different types of predictive pipelines: language models based, multi-model similarity knowledge-driven and large language models (LLMs) context + multi-model similarity knowledge-driven. Within first type predictive pipelines landscape, framework facilitates conflicting/duplicate requirements iden-tification by leveraging 8 distinct types of LLMs. In second type, frame-work supports development of predictive pipelines that leverage multi-scale and multi-model similarity knowledge, ranging from traditional similarity computation methods to advanced similarity vectors gener-ated by LLMs. In the third type, the framework synthesizes predictive pipelines by integrating contextual insights from LLMs with multi-model similarity knowledge. Across 6 public benchmark datasets, extensive testing of 760 distinct predictive pipelines including 8 standalone LLM-based, 112 similarity knowledge-Driven, and 640 hybrid LLM contextual and similarity Knowledge-Driven demonstrates that hybrid predictive pipelines consistently outperforms other two types predictive pipelines in accurately identifying duplicate and conflicting requirements. This pre-dictive pipeline outperformed existing state-of-the-art predictors perfor-mance with an overall performance margin of 13% in terms of F1-score.", "sections": [{"title": "1 Introduction", "content": "Requirements engineering (RE) is a key pillar of software development life cycle (SDLC) models, as it guides development teams through the critical stages of planning, implementation, testing and deployment [1, 2]. It ensures that software solutions effectively meet stakeholder needs and contributes to the overall quality, efficiency, and success of software projects [3, 4]. Despite its importance, RE is inherently complex due to the involvement of diverse stake-holders with varying levels of technical expertise [5]. This Interdisciplinary nature of stakeholders often results in significant communication gaps between technical experts (such as developers and system architects) and non-technical participants (including end-users, business analysts, and management) [6]. The communication gap often leads to duplicate requirements, where the same functionality is described differently [7]. For instance, one requirement might state, \"The system shall be able to process payments\" while another require-ment may contain same information but in different words as \u201cThe software must support online payment transactions [8].\" Duplicate requirements lead to unintentional redundancy in implementation efforts such as development team may allocate valuable time and resources to independently implement-ing the same functionality in different parts of the project. it leads towards inefficiencies, increased costs, and delayed project timelines [4, 9]. Within Requirement engineering landscape another challenge is conflicting requirements where two or more requirements introduce incompatible or con-tradictory software functionalities [10]. For instance, one requirement might contain payment processing information of one type such as, \u201cThe system shall be able to process payments within 24 hours [11].\" While another requirement may contain payment processing information of another type as \u201cAll payment transactions must be processed instantly.\" Such conflicting requirements can lead to project delays, increased costs and potential system failure by creating ambiguity and misalignment among stakeholders [12]. These conflicts can lead to re-iterations of project phases, extra effort expended, and ultimately com-promise the successful development and implementation of software systems [13]. Conventionally, both types duplicate and conflicting requirements are iden-tified manually [14]. As software complexity increase, requirements size also grow exponentially which makes manual identification inefficient and error-prone [15]. To overcome this challenge, and expedite the identification of conflicting and duplicate requirements, researchers are developing AI-driven applications by leveraging the potential of Artificial Intelligence approaches [7, 16-19]. AI-driven applications are developed by making all possible pairs of requirements [20]. Requirement pairs containing duplicate content are labeled as \"duplicate,\" while those without duplicate content are labeled as \u201cneu-tral\" [21]. Similarly, conflicting requirements identification task, requirement pairs are labeled with two distinct classes: \u201cconflict\" for pairs containing con-tradictory content and \u201cneutral\" for pairs that do not have contradictory information. During the training phase, AI models analyze requirement pairs and their associated class labels to learn patterns and relationships between requirement pairs and labels. In the inference stage, the trained models makes use learned patterns to predict new or unseen requirement pairs correspond-ing class labels. AI field has witnessed remarkable advancements in strategies designed to extract and learn meaningful patterns and relationships between textual data and their corresponding labels. The potential of various AI strategies, including Convolutional Neural Networks (CNNs) [22, 23], Long Short-Term Memory networks (LSTMs) [23], and language models [13, 24], has been explored for development of duplicate and conflicting requirements iden-tification predictors. However, these predictors could not manage to produce optimal performance, primarily due to their inability to effectively extract and learn meaningful patterns within requirement pairs. This limitation hinders their ability to accurately classify pairs as duplicate or neutral and conflicting or neutral requirements. To address these limitations, this study proposes to integrate requirement pairs similarity information within AI-predictive pipelines. This integration enables predictive pipelines to harnesses the deep semantic understanding of LLMs with invaluable insights derived from similarity knowledge and discrim-inative capabilities of ML classifiers. To thoroughly evaluate the effectiveness of similarity knowledge integration into AI-predictive pipelines, this study introduces a comprehensive framework. Specifically, proposed framework offers development of 3 distinct types of predictive pipelines including; 1) LLMs con-text based, 2) multimodel similarity knowledge driven and 3) LLMS context + multimodel similarity knowledge-driven. The primary contributions of this research are summarized as follows:"}, {"title": "2 Related Work", "content": "This section offers valuable insights into 32 predictors [7, 12-24, 33-50] devel-oped over past 10 years for conflict/duplicate detection in requirements. Among these, specifically 12 predictors [7, 12, 16, 17, 33-40] focus on dupli-cate detection, while 17 predictors [13, 14, 18, 19, 22-24, 41-50] target conflict identification. However, only 3 [15, 20, 21] predictors have been proposed to simultaneously address both duplicate and conflict detection among require-ments. Moreover, based on working paradigms, these predictors fall into 7 different categorizes: 1) Rule-based [14], 2) Statistical [13, 33-36, 41, 42], 3) Ontology based [43, 44] 4) Machine learning based [7, 16], 5) Deep learning based [24, 45], 6) Language models based [15, 17-23, 46-48], 7) Tools based [12, 37-40, 49, 50] predictors. Guo et al. [14] designed a rule-based predictor by using finer semantics and heuristic rules for requirements conflict detection. However, it [14] lacks flexibility to adopt for unseen data, which makes it unsuitable for frequently evolving nature of requirements. Among 8 statistical algorithm based predic-tors [13, 33-36, 41, 42], 4 predictors [13, 36, 41, 42] utilized clustering algorithm including, mean shift and correlative clustering. On the other hand, remaining 4 predictors [33-36] used similarity based approaches. Specifically one pre-dictor [33] explored combined potential of term frequency inverse document frequency (TFIDF) and cosine similarity algorithm, while one predictor [34] leveraged fast heuristic similarity, and two predictors [13, 35] made use of dice and cosine similarity. In existing literature, two predictors [43, 44] utilized ontological model and predefined rule to address conflict requirement detection. Specifically, these predictors are designed using domain knowledge to offer more formal and explicit representation of shared conceptualization. Besides ontology-based predictors, two machine learning based predictors [7, 16] utilized TFIDF with LogR [16] and SVM [7] classifiers. Although these predictors [7, 16] offer data driven approach for duplicate requirements detection and lack to capture intri-cate relationships among redundant features. Following the success of deep learning techniques in various natural language processing (NLP) task, two deep learning predictors [24, 45] are developed. Specifically, one predictor [45] explored combined potential of document frequency and bi-gram embedding with feed-forward neural network (FNN). Contrarily, other predictor [24] uti-lized Word2Vec embedding methods to generate statistical vector along with Bi-LSTM classifier. The advent of language models has revolutionized NLP, leading to the development of LM-based predictors. These models can be adapted for downstream tasks through two primary approaches: 1) fine-tuning with a self-classifier, where the model's own output layer is adjusted for the specific task, and 2) fine-tuning using an external classifier, where the model's out-puts are fed to separate classifier. In the field of requirements engineering, 4 predictors have employed self-classifier approach, while 7 predictors have leveraged external classifiers for conflict and duplicate detection. 4 LM based predictors [15, 20, 21] utilized language models (BERT [15], DistilBERT [21], ROBERTA [46], DeBERTa [20]) with self classifier, to simultaneously address both duplicate and conflict detection. The remaining 7 LM based predictors [17-19, 22, 23, 47, 48] explored diverse strategies. Specifically, 2 predictors [18, 19] integrated formal and lexical logics with RoBERTa [18] and GPT-3 [19], one made use of BERT with cosine similarity [48], one reaped benefits of both BERT and GPT-3.5 with cosine similarity [48], and two incorporated ROBERTa [22] and GPT-3 [23] with CNN classifiers for more comprehen-sive feature extraction. Lastly, one predictor [17] explored potential of BERT with LSTM classifier for duplicate requirements detection. This versatility of language models to address conflict or duplicate requirements detection to, highlight rapid adaptation of advanced techniques in requirement engineering filed. Lastly among 7 tools based predictors, 5 predictors [12, 37-40] namely; NLP4RE, KNIME, PMD/CPD, CloneDR, DECKARD, Mulan Toolkit, Clone-Miner, focus on duplicate requirement detection. On the other hand, two predictors [49, 50] are developed for conflict requirement detection predictors including CDNFRE prototype [49] and CDADE [50] tools."}, {"title": "3 Architectural Design of PassionNet Framework", "content": "Figure 1 illustrates high-level overview of 3 distinct types of predictive pipelines that framework offers. In Figure 1, bottom left corner module with doted lines represents a pool of 8 distinct LLMs that offers development of first LLMs context aware predictive pipelines. The remaining modules of Figure 1 illustrate a comprehensive overview of 752 distinct predictive pipelines, including 112 multimodel similarity knowledge driven pipelines (second type) and 640 LLM context + multimodel similarity knowledge driven predictive pipelines (third type). Following subsections illustrates details of all 3 types of predictive pipelines."}, {"title": "3.1 Large Language Models Context Aware Predictive Pipelines", "content": "The proposed framework offers diverse array of 8 different LLMs for conflic-t/duplicate requirements identification. Table 2 provides overview of these LLMs in terms of architectures, layer configurations, masking technique, tokenization and attention mechanisms. The proposed framework leverages pre-trained language models that are subsequently refined through supervised fine-tuning to capture deep linguistic and semantic relations for conflict/du-plicate requirements identification. During fine-tuning, the requirements pairs are encapsulated into a single unit, beginning with special [CLS] tokens while two requirements are separated with [SEP] token. This format allows mod-els to effectively process and capture semantic relations between requirement pairs."}, {"title": "3.2 MultiModel Similarity Knowledge Driven Predictive Pipelines", "content": "This section illustrates the details of multimodel similarity knowledge driven predictive pipelines that comprises of 3 main modules: 1) traditional rep-resentation learning and similarity computation techniques, 2) LLMs based similarity computation and 3) classifiers."}, {"title": "3.2.1 Traditional Similarity Techniques based Predictive Pipelines", "content": "This section offers insights of traditional representation learning and similar-ity computation based predictive pipelines. These predictive pipelines working paradigm comprises of three distinct stages including statistical representa-tion learning, similarity computation, and classification. First stage involves transformation of requirements text into statistical vectors using TFIDF [4] or OkapiBM25 [4] (in section 1 of supplementary file). The second stage uti-lizes 10 distinct traditional similarity methods to compute similarity scores between transformed vectors of requirement pairs. The five main algorithms of similarity computation include Vector Space Model (VSM) [51], Latent Seman-tic Indexing (LSI) [52], Jensen-Shannon Divergence (JS) [53], Non-negative Matrix Factorization (NMF) [54], Latent Dirichlet Allocation (LDA) [55]. These methods are combined to develop 5 additioal hybird methods namely NMF+LDA, JS+LDA, VSM+NMF, JS+NMF and VSM+JS. Pusudo code of VSM, LSI, JS, NMF and LDA technqiues is shown in Figures 2 (a), 2 (b), 2 (c), 2 da) and 3, respectively. Each method offers unique strengths to capture semantic relationships and similarities between requirements. For instance, VSM represents projects requirements in a high-dimensional space, allowing for efficient similarity computation. LSI extends VSM by applying Singular Value Decomposition to reduce dimensionality and capture latent semantic relationships between words. JSI provides a symmetric measure of similarity between probability distributions, while NMF focuses on decomposing non-negative matrices to extract meaningful features. LDA, on the other hand, is particularly useful for uncovering latent entities in requirements. To describe the concept of similarity computation through traditional methods, lets consider a sample dataset $D = \\{(R_i, R_j) | i, j \\in \\{1,2,..., n\\}\\}$. In this dataset each sample represents a requirement pair Ri, Rj and each requirement contains a set of words i.e Ri \u2208 W1,W2, W3,......, Wn. In the first stage, TFIDF and OkapiBM25 generates statistical vectors of require-ments text. Subsequently, theses vectors are passed to 10 distinct similarity methods which produce statistical vector shown in Equation 1.\n(SR_{1,1}: S^{TFIDF}_{VSM}, ..., S^{TFIDF}_{VSM+JS}, S^{Okapi}_{VSM}, ...,S^{Okapi}_{VSM+JS})\n:\nSR_{i,j}: (S^{TFIDF}_{VSM},...,S^{TFIDF}_{VSM+JS}, S^{Okapi}_{VSM},...,S^{Okapi}_{VSM+JS}),\n:\nSR_{n,n}: (S^{TFIDF}_{VSM}, ..., S^{TFIDF}_{VSM+JS}, S^{Okapi}_{VSM},...,S^{Okapi}_{VSM+JS})\n(1)\nIn Equation 1, $S^{TFIDF}_{VSM}$ represents the similarity score computed using TFIDF representation and VSM similarity method and subsequent dimen-sions correspond to other unique combinations of representation and similarity methods. The generated similarity vectors are passed to 16 different machine learning classifiers namely Gaussian Process (GP) [56], Quadratic Discrim-inant (QD) [57], K-Nearest Neighbors (KNN) [58], Gaussian Naive Bayes (GNB) [59], Bernoulli Naive Bayes (BNB) [59], Support Vector Machines (SVM) [60], Logistic Regression (LogR) [61], Multi-Layer Perceptron (MLP) [62], Decision Trees (DT) [63], Random Forests (RF) [64], AdaBoost [65], Gra-dient Boosting (Gboost) [66], XGBoost [67], CatBoost [68], Histogram-based Gradient Boosting (Histgboost) [69], and LightGBM [70]. To evaluate whether TFIDF representation based computed similarity scores and classifiers outperform those based on OKAPI-BM25 representation, or if a combination of both yields superior results, the framework supports the development of three distinct types of predictive pipelines:\n1. TFIDF Representation based predictive Pipelines: Utilizes TFIDF repre-sentation with 10 similarity methods and 16 classifiers.\n2. \u039f\u039a\u0391\u03a1\u0399-BM25 Representation based predictive Pipelines: Employs \u039f\u039a\u0391\u03a1\u0399-BM25 representation with 10 similarity methods and 16 classifiers.\n3. TFIDF and OKAPI-BM25 Representations based predictive Pipelines: Concatenates similarity vectors computed through TFIDF and \u039f\u039a\u0391\u03a1\u0399-BM25 representations and passed to 16 distinct classifiers. This structured approach enables a comprehensive comparison of perfor-mance across the three strategies predictive pipelines."}, {"title": "3.2.2 Large Language Models and Traditional Similarity Techniques based Predictive Pipelines", "content": "In addition to traditional methods for similarity computation of require-ment pairs, PassionNet framework also offers language model powered similarity computation using 10 different LLMs. These LLMs include: ALBERT [25], BERT [26], BART [71], DeBERTa [27], Electra [28] GPT [29], Longformer [30], ROBERTa [31], XLM [72] and XLNet [32]. Each requirement pair Ri and Rj is concatenated to form a unified input with a separator token [SEP] inserted between them. Afterwards, unified input is fed to 10 distinct LLMs and each LLM generates a similar-ity score SLLM that reflects its unique interpretation of similarity in a requirement pair. These scores collectively make a 10-dimensional similar-ity vector of requirement pair Rij, which can be denoted as Si,jLLM = (SALBERT, SBERT, SBART, SDeBERTa, SElectra, SLongformer, SROBERTa,SXLM,SXLNet. Here, $S_{ROBERTa}$ represents similarity score computed using pretrained RoBERTa language model. The generated 10 dimensional similarity vectors are passed to 16 distinct classifiers. Within these predictive pipelines language models extracts text contextual information to compute similarity between require-ments pairs and classifiers extracts discriminative and informative patterns from similarity vectors to discriminate requirements pairs into duplicate, and confilicting, neutral classes. In addition, framework leverages traditional rep-resentation learning methods and similarity methods based similarity vectors and LLMs similarity vectors combine potential to develop predictive pipelines as follows:\n1. LLM similarity based predictive pipelines: Utilized 10-dimensional similar-ity vector with 16 ML classifiers.\n2. TFIDF + LLMs similarity based predictive pipelines: 16 ML classifiers takes Concatenated similarity vectors computed through LLMS and TFIDF representation with tradiational similarity computation methods\n3. OkapiBM25 + LLMs similarity based predictive pipelines: 16 ML clas-sifiers takes Concatenated similarity vectors computed through LLMS and OkapiBM25 representation with tradiational similarity computation methods\n4. Multimodel predictive Pipelines: Similarity vectors concatenation of all three methods (TFIDF + OkapiBM25 + LLMs) and 16 distinct classifiers A large-scale experimental evaluation of these predictive pipelines offers valuable insights into whether predictive pipelines based solely on similarity vectors derived from Large Language Models (LLMs) deliver superior perfor-mance, or combination of traditional similarity vectors and LLM-based vectors yields better results."}, {"title": "3.3 MultiModel Similarity Knowledge Driven and Language Model Context Aware Predictive Pipelines", "content": "Third type of predictive pipelines utilized diverse types of multimodel simi-larity knowledge with LLM based context. To extract LLM-based context for requirement pairs, the framework concatenates each pair into a single input with a [SEP] token between requirements. This unified representation enables the LLM to capture contextual and semantic relations of requirement pairs. Specifically, it extracts [CLS] token representation from 8 different pre-trained LLMs including; ALBERT [25], BERT [26], DeBERTa [27], Electra [28] GPT [29], Longformer [30], RoBERTa [31] and XLNet [32]. The extracted high-dimensional [CLS] token representations is subjected to Principal Component Analysis (PCA) [73] for dimensionality reduction to 5 lower-dimensions (8, 16, 32, 64, and 128). The reduced CLS representations is concatenated with the previously generated multimodel similarity knowledge. For example, a 8-dimensional CLS representation of GPT2 is combined with a 30-dimensional multimodel similarity knowledge vector to produce a 38-dimensional vector for each requirement pair. This process creates enriched similarity vectors that incorporate both LLM-based semantic understanding as well as tradi-tional and advanced similarity assessment which are fed to 16 ML classifiers. By leveraging multiple LLM architectures and varying dimensionality reduc-tions, the framework aims to capture a comprehensive range of semantic and structural similarities between requirement pairs, potentially improving the accuracy and robustness of conflict/duplicate requirement identification."}, {"title": "4 Benchmark Datasets", "content": "To demonstrate proposed PassionNet framework robustness and broad appli-cability, we conduct a comprehensive evaluation on six publicly available benchmark datasets. Specifically, 2 duplicate requirements datasets (Stack Overflow [20, 21], Bugzilla [20, 21]) and 4 conflicting requirements datasets namely (UAV [21], WorldVista [21], PURE [21] and OPENCOSS [21]). Figure 4 briefly describes these datasets statistics in terms of total number of sam-ples, samples distribution across each class, vocabulary size, and maximum, minimum and average lengths of requirements. A brief description of these datasets is provided below.\n* Unmanned Aerial Vehicle (UAV) The University of Notre Dame released UAV dataset in 2021, which contains requirements related to UAV control system operations [14, 74]. Typically, requirements of UAV control system operations precede main clause, which are represented by verbs such as \"send,\" \"receive,\" or \"provide\" [14]. In some scenarios, these requirements also contain additional elements, such as \u201cto do\" or \"to make\". Moreover, requirements in UAV dataset adhere to EARS (Easy Approach to Require-ments Syntax) format [75]. This dataset consists of 6,670 requirements pairs labeled as either conflict or neutral.\n* WorldVista WorldVista https://worldvista.org/Documentat /Documentation dataset contains software requirements related to health management system. This system tracks patient information throughout entire process, from hospital admission to discharge. The requirements are written in natural language by incorporating healthcare terminologies and have a simple sentence struc-ture. The WorldVista dataset comprises of 10,878 requirement pairs, each labeled as either conflict or neutral.\n* PURE In 2023, Malik et al. [21] developed pure dataset to identify con-flict and neutral software requirements pairs by utilizing PURE: publicly available repository of SRS document. This repository contains SRS docu-ments of 79 different projects. The authors only selected two SRS documents namely Thermodynamic System (THEMAS) and web interface for social networks (Mashbot). Raw text of these SRSS is arranged in paragraphs and contains sentences with varying length and structure. To address these issues, authors pre-processed text by breaking down each paragraph into individual sentences. Furthermore, the authors treated each sentence as a single requirement and combined list of requirements into single sentence with comma separated elements. This process ensured that each require-ment was represented as a single, clear sentence, with simplifies structure. The dataset consists of 2,211 requirement pairs, with 20 as conflicts and 2,191 as neutral pairs.\n* OPENCOSS OPENCOSS http://www.opencoss-project.eu reflects Open Platform for Evolutionary Certification Of Safety-critical Systems for rail-way, avionics, and automotive industries. Existing studies [15, 21, 47] has identified OPENCOSS as a challenging dataset to discriminate conflict and neutral requirements pairs. This challenge arises due to presence of numer-ous common words in both conflict and neutral pairs. This dataset contains 6786 requirement pairs and is highly imbalanced with 10 pairs classified conflict class and 6776 pairs belonging to neutral class.\n* StackOverflow StackOverflow website is dedicated to address software and programming related problems. Due to large number of users, similar questions are frequently posted on website, often focusing on same issue. Despite ongoing efforts to reduce redundant questions that have already been answered, duplicate inquiries still appear on website. To address this challenge, Malik et al. [21] specifically collected neutral and duplicate ques-tions pairs across various programming languages. To maintain consistency with other datasets, authors considered only 5,000 neutral pairs and 90 duplicate pairs.\n* Bugzilla https://bugzilla.mozilla.org/index.cgi is a widely used, open-source bug tracking system developed by Mozilla Foundation, originally launched in 1998. It is designed to facilitate users in managing and tracking software bugs. However, like other bug tracking systems, Bugzilla also face challenge of duplicate bug reports. Primarily this issue occur due to lack of coordination between users and their unawareness about existing reports. To facilitate duplicate detection of bug reports, Malik et al. [21] scrapped open bug reports from Mozilla's Bugzilla using the REST API. They categorized 4000 bug reports pairs as neutral and 90 bug reports pairs as duplicate."}, {"title": "5 Evaluation Measures", "content": "To assess performance of proposed framework, we leveraged 6 distinct eval-uation measures that are used in existing conflict/duplicate requirements detection predictors [15, 21, 47]. Due to highly imbalanced nature of the data, existing predictors have been evaluated over a diverse array of evaluation mea-sures including standard and macro versions of precision (Pr), recall (R), and F1 score (F1). Primarily standard version of these measures are derived from a confusion matrix, which consists of four components: 1) True Positive (TP), 2) True Negative (TN), 3) False Positive (FP), and 4) False Negative (FN). TP and TN refer to number of correctly predicted conflict/duplicate and neu-tral requirements pairs, respectively. Contrarily, FP and FN represent number of requirements pairs that are incorrectly classified as conflict/duplicate and neutral pairs, respectively. Figure 5 graphically represents confusion matrix in terms of TP, TN, FP, and FN. Equation 2 illustrates the mathematical expres-sion to compute standard version of precision recall and F1 score evaluation measures.\n$f(x)_{standard}$\n$Pr=\\frac{TP}{TP+FP}$\n$R=\\frac{TP}{TP+FN}$\n$F1=\\frac{2*Pr*R}{Pr+R}$\n(2)\nMacro variant independently calculates each metrics across all classes and then computes weighted average of matrix with respect to total classes. Specif-ically for imbalanced datasets macro variants of metrics address different aspects of class importance. Equation 3 depicts mathematical expression of these metrics in terms of marco variants.\n$f(x)_{imbalance}$\n$Pr_{macro}=\\frac{1}{m}\\Sigma_{i=1}^m Pri$\n$R_{macro}=\\frac{1}{m}\\Sigma_{i=1}^m Ri$\n$F1_{macro}=\\frac{2*Pr_{macro}*R_{macro}}{Pr_{macro}+R_{macro}}$\n(3)\nIn Equation 3, TPi, FPi, TNi and FNi denote the true positive, false positive, true negative, and false negative counts for class i and m represents total number of classes."}, {"title": "6 Experimental Setup and Results", "content": "The proposed framework is developed on top of 8 distinct APIs namely; scikit-learn https://scikit-learn.org/, numpy https://numpy.org/, math https:// docs.python.org/3/library/math.html, scipy https://scipy.org/, pandas https: //pandas.pydata.org/, matplotlib https://matplotlib.org/, FastAI https:// www.fast.ai/ and pytorch https://pytorch.org/. Following experimental cri-teria of existing studies [20, 21], we performed experimentation using 3-fold cross-validation setting. In this setting iteratively, one fold is taken as test set and other 2 folds are used for model training. In addition, for each iteration 5 percent of training set is used as validation set for LLM based predictive pipelines. To ensure a fair performance comparison among distinct predic-tive pipelines, we utilized Optuna https://optuna.org/ to identify the optimal hyperparameters for each pipeline by exploring an extensive hyperparameter search space. In supplementary file, Tables 1 and 2 illustrate ML classifiers and LLMS predictive pipelines hyperparameters and their selected search spaces. optimize the performance of both ML classifiers and LLMs, we conducted an extensive hyper-parameter search. To obtain representations from LLMs, requirement pairs are passed to the model separated with a [SEP] token. Afterwards, model's [CLS] token is extracted, which contains semantic rela-tionship between requirements pair. This high-dimensional representation is subsequently reduced to five dimensions using Principal Component Analy-sis (PCA), creating a compact yet informative embedding that captures the essential features of the requirement pair's relationship. The following subsections provides a comprehensive performance analysis of 3 distinct predictive pipelines offered by PassionNet Framework: language model-based, multimodel similarity knowledge driven and a hybrid approach combining LLMs context with multimodel similarity knowledge. Furthermore, it presents a detailed performance comparison of the top-performing predictors of each type of predictive pipelines. Lastly, it provides in-depth performance analysis of existing conflict/duplicate requirements identification predictors with proposed PassionNet framework."}, {"title": "6.1 Performance Benchmarking of Large Language Models based Predictive Pipelines", "content": "This section investigates effectiveness of end-to-end LLM-based predictive pipeline for conflict/duplicate requirements identification. This analysis uti-lizes 8 diverse LLMs, including ALBERT, BERT, DeBERTa, ELECTRA, GPT, Longformer, RoBERTa and XLNet, across 6 public benchmark datasets. This comprehensive analysis highlight adaptability and strength of different LLMs for conflict/duplicate requirements classification tasks across varying dataset characteristics. It is evident from Figure 6 over pure dataset most LLMS perform exc\u0435\u0440-tionally well, with five LLMS (Roberta, BERT, ALbert, Xlnet, Deberta) scoring f1 score above 0.89. On the other hand, 4 LLMs (Roberta, Longformer, DeBERTa and XLNet) excel on UAV dataset by achieving f1 score above 0.87. World dataset demonstrates more consistent performance across majority of LLMs, with 5 LLMs (longformer, deberta, RoBerta, BERT, XLNet) scoring f1 score above 0.80. However, OPEN dataset stands out as significant challenge, with almost all LLMs scoring very low or nearly zero f1 score that indicates potential difficulties to extracting meaningful patterns from fewer instances of conflict class. Overall, over conflict detection datasets, Roberta remains top-performing over 3 datasets (PURE, UAV, Open) and on WORLD, Roberta falls closely behind the top-performing Longformer with a slight margin. On duplicate detection datasets (BUG, Stack) DeBERTa stands out as top per-forming predictor. Particularly, on bug dataset Albert roberta performed quite well attaining f1 score above 0.64. However, over stack dataset all LLMs remain fail and secure f1 score below 0.55. Notably, ELECTRA and GPT failed completely across all datasets and continually achieved very low F1 score or zero across conflict/duplicate iden-tification datasets. Both ELECTRA and GPT are sensitive to data imbalance due to their respective training methods and struggle to capture discriminative features of data. Hence, due to highly imbalanced cases, both models tends to overfit on the neutral class and struggle to accurately identify conflict/du-plicate class. DeBERTa and RoBERTa excel in conflict/duplicate detection on imbalanced datasets due to their advanced architectures and fine-tuned training methods. DeBERTa's disentangled attention mechanism facilitates to capture minute differences in similar text while RoBERTa's dynamic masking enhances its ability to generalize semantic discriminations. Hence, sensitiv-ity to fine-grained semantic and syntactic details along with rich contextual embeddings of these LLMs prevent overfitting to the majority class."}, {"title": "6.2 A Comprehensive Performance Analysis of MultiModel Similarity Knowledge Driven Predictive Pipelines", "content": "This section provides comprehensive performance analysis of multimodel sim-ilarity knowledge driven predictive pipelines designed using 10 distinct LLMS, 2 traditional representation learning methods along with 10 traditional simi-larity methods as described in section 3.2. These predictive pipelines fall into 3 distinct groups: 1) standalone predictors (TFIDF, OkapiBM25, LLM), 2) hybrid predictors ((TFIDF+OkapiBM25, OkapiBM25+LLM, TFIDF+LLM)) 3) multimodel preidtcor (TFIDF+OkapiBM25+LLM). Table 3 provides per-formance values of 112 unique multi model similarity knowledge driven predictive pipelines in terms of conflict/duplicate class f1 score across 6 public benchmark datasets. It is evident from Table 3 that among standalone predictors, LLM based predictors underperform other standalone traditional predictors ( TFIDF,OkapiBM25). Conversely, TFIDF-based predictors excel on pure, uav and stackoverflow datasets and also achieved peak performance values of 0.882, 0.854 and 0.557, respectively. In contrast, OkapiBM25-based predictors attain superior performance on world (0.837), opencoss (0.609) and bug (0.731) datasets. This analysis reveals that standalone traditional predictors (TFIDF, OkapiBM25) are more effective than advanced LLMs to capture similarity patterns between requirements but are sensitive to dataset characteristics. Among hybrid predictors (TFIDF+OkapiBM25, OkapiBM25+LLM, TFIDF+LLM), combination of traditional methods with LLM enhances its performance which showcases their capability to strengthen advanced tech-niques. On the other hand, TFIDF+LLM based hybrid predictors outperform OkapiBM25+LLM across all datasets except opencoss and stackoverflow datasets where TFIDF+OkapiBM25 attains higher performance. This indi-cates that TFIDF provide a more stable foundation for LLM as compared to OkapiBM25. Moreover, hybrid predictors surpasses performance of stan-dalone predictors with slight performance margin except opencoss where OkapiBM25 standouts in terms of fl score. This suggest that integra-tion of diverse similarity computation techniques yields more comprehensive spectrum of requirements similarities. A through performance analysis of stan-dalone, hybrid and multimodel predictors reveals that mutltimodel predictors (TFIDF+OkapiBM25+LLMs) outperform other predictors on all datasets. This suggests that integrating different similarity approaches captures a more comprehensive requirements relations, leading to better conflict/duplicate identification. Furthermore, the effectiveness of classifiers is influenced by both predictor's nature and dataset's variability. For instance, SVM, RF and MLP consistently achieve good performance, while classifiers like GNB and BNB often lag behind, with BNB often yielding zero f1scores."}, {"title": "6.3 A Comprehensive Performance Analysis of MultiModel Similarity Knowledge Driven and Language Model Context Aware Predictive Pipelines", "content": "This section presents a comprehensive analysis of 640 unique multimodel similarity knowledge driven and language model context aware predictive pipelines. These pipelines utilize CLS token of eight distinct language mod-els five dimensions (8, 16, 32, 64, or 128) concatenated with a 30dimensional multimodel similarity vector (TFIDF+OkapiBM25+LLM) and fed to 16 ML classifiers. The performance metrics for these 640 pipelines are detailed in the supplementary file. Table 4 showcases the performance values of the most effective combinations, highlighting the ML classifier and language model CLS token with the optimal reduced dimension for each dataset (World, Pure"}]}