{"title": "Incremental and Data-Efficient Concept Formation to Support Masked Word Prediction", "authors": ["Xin Lian", "Nishant Baglodi", "Christopher J. MacLellan"], "abstract": "This paper introduces Cobweb/4L, a novel approach for efficient language model learning that supports masked word prediction. The approach builds on Cobweb, an incremental system that learns a hierarchy of probabilistic concepts. Each concept stores the frequencies of words that appear in instances tagged with that concept label. The system utilizes an attribute-value representation to encode words and their surrounding context into instances. Cobweb/4L uses the information-theoretic variant of category utility and a new performance mechanism that leverages multiple concepts to generate predictions. We demonstrate that with these extensions it significantly outperforms prior Cobweb performance mechanisms that use only a single node to generate predictions. Further, we demonstrate that Cobweb/4L learns rapidly and achieves performance comparable to and even superior to Word2Vec. Next, we show that Cobweb/4L and Word2Vec outperform BERT in the same task with less training data. Finally, we discuss future work to make our conclusions more robust and inclusive.", "sections": [{"title": "1. Introduction", "content": "Over the past decade, there has been significant advancement in language models that utilize neural network architectures, particularly in tasks such as word prediction and text completion. The concept of the cloze procedure, introduced by Taylor (1953), measures communication effectiveness, and in particular, it is a method of \u201cintercepting a message from a 'transmitter', mutilating its language patterns by deleting parts, and so administering it to \u2018receivers' that their attempts to make the patterns whole again potentially yield a considerable number of cloze units\" (Taylor, 1953, pp. 416). This idea inspired Devlin et al. (2019) to create the MLM (masked language modeling) paradigm, which is the task used during the training of Bidirectional Encoder Representations from Transformers (BERT). In this paradigm, certain input tokens are randomly masked, and the model predicts them solely based on the words in their surrounding contexts. Specifically, the architecture employs a bidirectional transformer encoder to predict masked tokens. This approach is widely adopted to train large language models, particularly in derivatives of BERT (Liu et al., 2019; Baevski et al., 2020). Other approaches share a similar rationale. For example, autoregressive language modeling uses it in their training process; they predict the next token based on previously predicted results (Yang et al., 2019; Brown et al., 2020)."}, {"title": "2. Cobweb as a Language Learning System", "content": "Apart from MLM, the exploration into how neural-network-based architectures predict words within their contexts dates back much earlier. For example, the Word2Vec (Mikolov et al., 2013) variant known as Continuous Bag-of-Words (CBOW) learns word embeddings by training a neural language model to predict a masked word given the context words around it.\nDespite the rapid development and superior performance of such neural language models for word-filling tasks, they have several shortcomings. The prevalent approach for utilizing large language models involves training followed by fine-tuning. The training phase demands substantial data and computational resources. For example, BERT's training corpus comprised the BookCorpus (800M words) (Zhu et al., 2015) and English Wikipedia (2.5B words) (Devlin et al., 2019). Additionally, it is questionable if large language models, like BERT, can learn continually. While various approaches to continual or online learning have been proposed in recent years (Loureiro et al., 2022; Jin et al., 2022; Hu et al., 2023), with either online training or fine-tuning, more work is needed to develop approaches that let large language models learn continuously.\nSome studies diverge from the mainstream focus on large language models and instead explore more human-like learning systems to address their limitations. For instance, Mitchell et al. (2018)'s NELL (Never-Ending Language Learner) employs an infinite loop akin to an Expectation Maximization (EM) algorithm for continual learning. Additionally, MacLellan et al. (2022) proposes a novel approach utilizing the Cobweb algorithm (Fisher, 1987), an approach for learning probabilistic concept hierarchies that was inspired by human cognition, for language induction. This approach incrementally builds modular structures based on prior knowledge, enabling rapid expertise acquisition from limited training data, aligning with the human-like learning constraints suggested by Langley (2022). Although the use of Cobweb in language induction is still in its nascent stages and its practicality and performance on certain language tasks remain uncertain, it represents an initial exploration into leveraging human-like learning mechanisms into language tasks.\nThis paper makes the following contributions:\n\u2022 We introduce Cobweb/4L, which inherits the Cobweb structure and learns language from instances that describe words and their surrounding context, as well as a new performance mechanism for categorizing test items to generate predictions;\n\u2022 We show that Cobweb/4L's performance mechanism yields substantially better predictive performance on a masked word prediction task than prior Cobweb performance mechanisms;\n\u2022 We present a preliminary evaluation comparing Cobweb/4L to Word2Vec, showing that it achieves better masked word prediction performance and is more data efficient;\n\u2022 We present a preliminary evaluation showing Cobweb/4L can achieve better masked work prediction performance than BERT, even when BERT is trained on three times as much data.\nWhile these results are still preliminary, they demonstrate the potential for Cobweb-based language modeling approaches to be competitive with mainstream neural language models."}, {"title": "2.1 Background", "content": "Cobweb (Fisher, 1987) was first proposed as an incremental approach for learning hierarchical concepts. Given a sequence of instances represented in nominal attribute-value pairs (e.g. {color: blue; form: triangle; number: 2}), Cobweb forms a probabilistic concept hierarchy, where each concept stores the attribute value count frequencies for the features of instances it has incorporated.\nThe learning process of Cobweb is illustrated in Figure 1(a). To guide the concept formation process, Cobweb uses Category Utility (Corter & Gluck, 1992), which evaluates the resulting increase in the predictive power of a concept c:\n$CU(c) = P(c)[\\sum_{ij}\\sum P(A_{i} = V_{ij}|c)^{2} \u2013 \\sum_{ij}\\sum P(A_{i} = V_{ij})^{2}]$ (1)\n$P(c)$ is the overall probability of the concept c, $P(A_{i} = V_{ij})$ is the probability of some instance having attribute-value $A_{i} = V_{ij}$, and $P(A_{i} = V_{ij}|c)$ is the conditional probability of some instance having attribute-value $A_{i} = V_{ij}$ given its membership in c.\nWhen a new instance is introduced to be learned, Cobweb sorts it down its current categorization tree, and at each node c, it considers a best way to incorporate the instance into the current node's children from four available operations: add, create, merge, split. It selects the option that produces the highest averaged category utility among the children of c. This recursive process continues until the instance is located at some leaf node.\nAfter constructing the Cobweb tree, it can apply its structure to classify another instance having some unobserved attribute(s) and then predict them, which is illustrated in Figure 1(b). In particular, Cobweb first sorts the instance down from its root to some leaf node just like during learning. During categorization, none of the concept frequencies are updated and only the add operation is considered. Consequently, the tree will not be updated to reflect the instance. After classification, Cobweb predicts the unobserved attribute(s) of the instance using one of the nodes along the categorization path (Lian et al., 2024). One option is to predict with the basic-level node, suggested by Corter & Gluck (1992) because the node holds the highest category utility value. Another option is to directly predict with the leaf node, which has been shown to yield better performance in prior work (MacLellan et al., 2016; MacLellan & Thakur, 2022; MacLellan et al., 2022).\nCobweb was not explored as a language-learning system until MacLellan et al. (2022) proposed three extensions to process words and their surrounding contexts\u2014the word, leaf, and path variants. The first combines ideas from both Word2Vec techniques within the framework of probabilistic concept hierarchies, and the other variants build on this initial scheme and apply more sophisticated ways to incorporate information about word contexts during both the predicting and learning process. However, there is not a direct comparison between it and Word2Vec, or other language models, regarding performance or data efficiency."}, {"title": "2.2 Cobweb/4L: A New Representation and Performance Mechanism for Improved Language Modeling", "content": "Building on Cobweb, we propose Cobweb/4L. It utilizes an instance representation that is similar to the one used by the Word variant proposed by MacLellan et al. (2022). As shown in Figure 2, given a sentence, a chosen anchor word $w_{a}$, and the window size, we generate an instance with attributes anchor, context-before and context-after, so the values are mapped from the words in the sentence. Specifically, anchor denotes the anchor word considered, and context-before and context-after attributes contain the context words before and after the anchor word in the sentence. For each context word w in either attribute considering the context in the instance, instead of assigning it with the actual count in the context, we assign it with a weighted count 1/(d + 1) where d is the number of words (or tokens) between w and $w_{a}$, so the closer w gets to $w_{a}$, the more weight it gets, and consequently its weighted count reflects the relative position within the instance.\nAfter parsing and preprocessing the raw texts into the instance format represented in Figure 2, Cobweb/4L learns using an approach similar to Cobweb, which is illustrated in Figure 3. Unlike prior Cobweb work, we make use of the information-theoretic category utility (Corter & Gluck, 1992) when deciding the operation to proceed, so the category utility of a concept node e becomes\n$CU(c) = P(c)[U(c_{p}) \u2013 U(c)]$ (2)\nwhere $c_{p}$ is the parent of c and $U(c)$ is the uncertainty (or entropy) of c:\n$U(c) = \\sum_{i} P(W_{i}|c)U(W_{i}|c)$ (3)\nwhere\n$U(W_{i}|c) = - \\sum_{j}P(W_{ij}|c) log P(W_{ij}|c)$ (4)\nis the uncertainty of the attribute (anchor, context-before,context-after) $W_{i}$ given c, and $P(w_{ij}|c)$ is the probability that $W_{i}$ has word $w_{ij}$ given c."}, {"title": "3. Experiments", "content": "Jones, 1983):\n$s(c) = P(c|x)P(x|c)$ (5)\nwhich is the product of cue and category validity. Once a new $c^{*}$ has been identified, Cobweb/4L adds it to the expanded node list $C^{*}$, adds its children to the search frontier, and repeats. This process starts at the root $c^{*} = C_{root}$ and repeats recursively by evaluating the collocation of the children of $c^{*}$ and adding these to the search frontier to be considered for expansion (see Figure 4). The process continues until Cobweb/4L has expanded |$C^{*}$| = $N_{max}$ nodes. After that, Cobweb/4L predicts the word $w_{i}$ for the attribute $W_{i}$ with probability:\n$P(W_{i} = w_{i}|C^{*}) = \\sum_{CEC^{*}}P(w_{i}|c) \\frac{exp{-s(c)}}{\\sum_{CEC^{*}} exp{-s(c)}}$ (6)\nwhich represents the combination of predictions from all expanded nodes, weighted by their collocation (the softmax ensures the probabilities sum to one). This approach is a kind of Bayesian model averaging (Hinne et al., 2020).\nIn our experiments, we conducted a comprehensive performance comparison between Cobweb/4L and two other baseline architectures, CBOW and BERT, with the datasets used in the Microsoft Research (MSR) Sentence Completion Challenge (Zweig & Burges, 2011)."}, {"title": "3.1 Compared Baseline Architectures", "content": "3.1.1 Word2Vec and CBOW\nWord2Vec (Mikolov et al., 2013) was initially proposed as two feedforward neural network architectures for computing continuous vector representations from a given corpus. One is CBOW, which is trained to predict the anchor word given the context words around it. The order of words does not influence the projection from words to vectors (i.e., they are a bag of words). The other is Continuous Skip-gram, which is similar to CBOW, but predicts surrounding words given the anchor word. Mikolov et al. (2013) compared the Skip-gram model with other language models, such as 4-gram and average LSA (latent semantic analysis) on the MSR sentence completion challenge task. They found that the combination of Skip-gram and recurrent neural network language model works the best among all objectives being compared. Our experiment here compared Cobweb/4L with CBOW in the word completion task given context. In particular, we trained CBOW with 3 epochs, batch size 64, and vector length 100.\n3.1.2 Transformers and BERT\nThe BERT architecture (Devlin et al., 2019) is a multi-layer bidirectional Transformer encoder Vaswani et al. (2017). In training the two initial versions of BERT (BERT-base and BERT-large), Devlin et al. (2019) used two tasks: one is the MLM, where they randomly substituted [MASK] for"}, {"title": "3.2 Methodology", "content": "the training data or not. In our work, we only train BERT with the MLM task using the architecture available at Hugging Face with 1 epoch and batch size of 64.\nTo evaluate an approach to the MSR sentence completion challenge task, each architecture is trained with the collection of 522 Conan Doyle's Sherlock Holmes novels. We first preprocessed the stories with lemmatization and tokenization to generate the training set for each evaluated architecture from these novels. We filtered out tokens that appeared less than three times. Next, we generated the corresponding shuffled (with fixed random seed 123) input representations from these tokens for each evaluated architecture. In particular, for Cobweb/4L, we generated attribute-value instances with a window size of 10\u2014each instance comprises one anchor word, 10 context words before, and 10 context words after. For CBOW, each trained datum comprises one anchor word and its corresponding set of context words (also with a window size of 10). For BERT, the training data is just the collection of tokenized texts.\nTo compare the performance and the data efficiency between Cobweb/4L and CBOW, after generating the respective preprocessed training dataset for each evaluated architecture, we trained each with approximately one-third of the data. This corresponds to the 5 million training training examples for Cobweb/4L and CBOW (each of which has the same anchor and context words). We saved a checkpoint for each model at 12 points throughout training-Cobweb/4L and CBOW were incrementally trained on 416667 examples for each checkpoint. The training data provided between each checkpoint was approximately 1/36th of all the Sherlock Holmes stories (1/3\u00d71/12 = 1/36).\nTo evaluate each checkpoint, we applied it to predict a single masked word in each of the 1040 test sentences. Each sentence comes with a list of five possible words for the masked slot. The words were chosen to be reasonable options that have similar frequencies in the overall corpus. Here is one example:\nI have it from the same source that you are both an orphan and a bachelor and are\nalone in London.\nA. crying B. instantaneously C. residing D. matched E. walking\nEach baseline generated probabilities for all possible masked words and we selected the option that has the highest probability. We then compared the selected word with the ground-truth answers to calculate accuracy.\nWe evaluated the architectures under this paradigm because this task, as a sentence completion (or masked word predicting) task, has a prevalent use in evaluating Word2Vec (Mikolov et al., 2013) and other similar language models that consider context (Melamud et al., 2016). For each approach, we tested each checkpoint to observe their performance throughout training and to derive their learning curves, so we can figure out which approach is more data efficient and reaches asymptotic performance faster."}, {"title": "3.3 Results and Discussion", "content": "is typically used and evaluated in more complex tasks. Since smaller models trained on domain-specific data may exhibit better performance within their respective domains compared to larger models trained on a diverse range of data, it would not be surprising if Word2Vec and/or Cobweb/4L outperform BERT on this task.\nThe test accuracy results on the challenge data across the 12 checkpoints for each evaluated architecture are shown in Figure 6. The single-node prediction approaches (leaf and basic-level nodes) used in prior Cobweb work do not perform well on this language modeling task. They display relatively unstable and inferior test accuracy across the checkpoints. Despite the increased number of training instances processed at each checkpoint, the learning curves for both single-node prediction methods fail to demonstrate improvement. Specifically, the basic-level prediction maintains an accuracy of approximately 0.225, while the leaf prediction hovers around 0.2 with notable variance (essentially chance performance). Conversely, Cobweb/4L's new multi-node prediction method consistently outperforms both single-node prediction approaches. In particular, all three learning curves with varying expanded nodes start with a test accuracy of about 0.28, surpassing the best performance for the single-node approaches at the outset. They exhibit a general trend of improvement at each successive checkpoint. Additionally, we observe that when more nodes are expanded, we observe more rapid accuracy improvements.\nCompared to CBOW, Cobweb/4L (with its multi-node prediction) shows more rapid learning, with higher performance during the earlier checkpoints. At later checkpoints, the 1000-node version of Cobweb/4L achieves similar performance to CBOW, while the 2000- and 3000-node versions exceed CBOW's performance.\nFurther, Table 1 shows the test accuracy for all evaluated approaches at their respective last checkpoint and their peaks. Although we trained BERT with all the available training data, its test accuracy still cannot surpass the accuracy achieved by Cobweb/4L with multi-node prediction and CBOW."}, {"title": "4. Related Work", "content": "While few systems operate under a rationale similar to Cobweb, we here mention several notable alternatives. A decision tree (DT) employs a top-down induction process, beginning with a root and extending down to its leaves, with a similar flowchart- (or tree-) like structure of Cobweb. However, in a DT, each internal node represents a logical test (split) and each leaf constitutes a prediction (Costa & Pedreira, 2023). One DT approach, CART (Classification and Regression Tree) (Breiman et al., 1984), learns from instances consisting of attributes and a target attribute in a greedy manner, iteratively selecting the most \u201cinformative\u201d splits. Its procedure continues until all leaves contain pure conditions, so it potentially results in overfitting, which can be mitigated by tree pruning. Later studies introduced non-greedy trees in general, such as lookahead trees (Norton, 1989; Ragavan & Rendell, 1993), which selects the optimal splits relating to the next k iterations with a k-lookahead. Another non-greedy DT method involves building multiple trees, randomly modifying and combining them using \u201cgenetic operators\u201d until a satisfactory solution is achieved, known as Evolutionary Algorithms (EA) (Mitchell, 1998). Various alternative DT induction approaches have been extensively explored, including gradient-based optimization (Jordan & Jacobs, 1994; Su\u00e1rez & Lutsko, 1999), recently more popularized by the artificial neural network community (Costa & Pedreira, 2023), and optimal trees which search for the tree that maximizes some measure under size constraints (e.g. maximum depth) Costa & Pedreira (2023). Most initial DT approaches are non-incremental, requiring the entire dataset upfront, such as CART and ID3 (Quinlan, 1986). Later, alternative approaches supporting incremental learning with a tree emerged, including ID4 Schlimmer & Fisher (1986), EA, Cobweb, ID5R (Utgoff, 1989; Kalles & Morris, 1996), etc.\nBeyond DT approaches, other human-like learning systems operate similarly, such as SAGE (Sequential Analogical Generalization Engine) proposed by McLure et al. (2010). SAGE maintains a generalization context for each concept, featuring a \"trigger\u201d to assess whether an incoming example should be adopted. With each new example, it utilizes MAC/FAC to retrieve up to three examples or generalizations based on similarity to the new example. It also learns concepts in-"}, {"title": "5. Conclusion and Future Work", "content": "crementally, and its later variations, equipped with the Nearest-Merge algorithm (Liang & Forbus, 2014), organize concepts hierarchically. We mention these approaches as we believe they should also be explored as potential alternatives to neural language modeling, similar to how we have been exploring extensions to Cobweb for this purpose. For example, we are interested in the possibility of employing decision tree learning or other systems like SAGE for language model learning.\nIn this paper, we introduced Cobweb/4L, a novel language modeling system based on Cobweb. It employs a new representation and a new multi-node performance mechanism that leverages several concepts in its hierarchy to generate predictions. We show this new approach significantly outperforms prior Cobweb performance mechanisms that only use single-node to generate a prediction. Further, we show the new approach learns more rapidly and achieves better accuracy than CBOW or BERT. While our results are preliminary, they suggest that a Cobweb-based language modeling approach has the potential to be both data efficient and accurate.\nMoving forward, our future work entails devoting more time and computational resources to evaluating Cobweb/4L and CBOW checkpoints that are trained on the remaining available instances generated from all the Holmes stories. Additionally, to ensure robust experimental results and draw more conclusive findings, we plan to conduct the same experiment with multiple random seeds. Furthermore, we will explore variations of the multi-node prediction method and compare the information-theoretic category utility used by Cobweb/4L with the probabilistic category utility measure employed in prior Cobweb research. In more extensive studies, we envision evaluating Cobweb/4L across a broader range of language modeling tasks, including other masked word prediction and autoregressive text generation."}]}