{"title": "Reliable Reasoning Beyond Natural Language", "authors": ["Nasim Borazjanizadeh", "Steven T. Piantadosi"], "abstract": "Despite their linguistic competence, Large Language models (LLMs) often exhibit limitations in their ability to reason reliably and flexibly. To address this, we propose a neurosymbolic approach that prompts LLMs to extract and encode all relevant information from a problem statement as logical code statements, and then use a logic programming language (Prolog) to conduct the iterative computations of explicit deductive reasoning. Our approach significantly enhances the performance of LLMs on the standard mathematical reasoning benchmark, GSM8k, and the Navigate dataset from the BIG-bench dataset. Additionally, we introduce a novel dataset, the Non-Linear Reasoning (NLR) dataset, consisting of 55 unique word problems that target the shortcomings of the next token prediction paradigm of LLMs and require complex non-linear reasoning but only basic arithmetic skills to solve. Our findings demonstrate that the integration of Prolog enables LLMs to achieve high performance on the NLR dataset, which even the most advanced language models (including GPT4) fail to solve using text only.", "sections": [{"title": "1 Introduction", "content": "The recent emergence of large language models (LLMs) [3, 25, 24, 8, 7, 29, 34, 35] has revolutionized the field of Natural Language Processing (NLP), with LLMs demonstrating human-level performance across various professional and academic benchmarks [26] and exhibiting an excellent understanding of linguistic rules and patterns [19].\n\nHowever, despite their linguistic competence, LLMs often demonstrate significant limitations in their capacity to reason reliably and flexibly [19, 12, 39]. These limitations likely stem from the autoregressive architecture of transformers, which enforces the solution to the problems sequentially: the models' reliance on a greedy process for predicting the next word constrains their backtracking and error recovery capability [12]. Models are expected to generate an answer in a single pass of their feedforward architecture, which cannot implement conditional loops [4]. Moreover, the statistical nature of LLMs' training and representation means they often fail in generalizing appropriately to problems outside their training distribution, especially in settings requiring reasoning and discrete processes [40]. Furthermore, even the most advanced LLMs, including GPT4, have an incredibly short working memory[4], while reliable reasoning requires accurate and robust retrieval and integration of all relevant information.\n\nAdditionally, the linear and sequential nature of natural language contrasts with the complex and non-linear computations often involved in deductive reasoning. Even humans struggle with reasoning tasks when the brainstorming medium is confined to text. This is well illustrated by the history of logic. Aristotle's writing on syllogistic reasoning, for example, lacked the tools of symbolic logic later developed for this kind of argumentation. The result is clunky and difficult to follow, even when correct:\n\nIf A has been proved to all or to some B, then B must belong to some A: and if\nA has been proved to belong to no B, then B belongs to no A. This is a different"}, {"title": "2 Our Approach", "content": "To enable LLMs to perform deductive reasoning robustly, we propose integrating a reliable, deductive reasoning module into their inference pipeline. Specifically, in this study, we prompt the model to encode the constraints and relationships among variables, as described in the problem statement, as a set of Prolog code statements. The generated code is then evaluated by Prolog, which uses deductive approach, to derive a deterministic answer to the problem (Figure 1). This not only has the advantage of mirroring the likley human architecture of separate linguistic and reasoning systems [13, 19], but as we show, significantly improves the performance of LLMs in mathematical reasoning.\n\nIndeed, this approach draws on the strengths of both symbolic and neural systems. Though systems like Prolog support reliable deduction, they have no mechanism to deal with the complexities and intricacies of natural language descriptions of problems. Moreover, they are unable to perform implicit reasoning, which involves extracting information that is not explicitly stated in the text but is"}, {"title": "3 Comparison to Other Approaches", "content": "Several studies have explored the integration of LLMs with external tools and symbolic reasoning modules [23, 22, 10, 30]. For instance, training LLMs to make API calls to tools like calculators, interpreters, or external datasets has been shown to improve their performance across a variety of reasoning tasks [10, 30, 28]. While these methods have successfully reduce the arithmetic errors of LLMs, they do not sufficiently address the reasoning limitations inherent to the next-token prediction paradigm of LLMs and the linear nature of text, which can restrict the ability to perform comprehensive searches over the space of possibilities, explore multiple pathways to a solution, or backtrack.\n\nOur approach builds upon and extends the work of LINC [23] and Nye et al. [22]. LINC uses a neurosymbolic process to convert natural language into first-order logic expressions with LLMs to determine the truth value of conclusions via a symbolic theorem prover. This method has shown significant performance gains on the FOLIO [15] and ProofWriter [33] datasets compared to CoT"}, {"title": "4 Experiments", "content": "We first present results of our approach on two existing datasets: the standard mathematical reasoning dataset, GSM8k, and the Navigate task, extracted from the BIG-bench benchmark. In our experiments, we compare our approach against the standard prompting method used for reasoning tasks: examples solved using Chain of Thought (CoT) reasoning in text.\n\nGSM8k [10] is a widely used benchmark for mathematical reasoning tasks, comprising of elementary school math problems. We tested the performance of our approach using GPT4 and GPT3.5 Turbo (hereafter GPT3.5), and text-davinci-003 models on the GSM8k dataset. To construct the Prolog prompt, we selected eight problems from the first 25 problems in the shuffled test split of the dataset. This selection was made to ensure that the prompt examples covered a variety of difficulty levels\nprompt, we used two examples of each of the two types of problems in this dataset. Since\nthere is no reported text-based baseline for the GPT models for the Navigate task, we prompted the\nmodels with CoT in text to establish the baseline performance for this task. For comparability, we\nconstructed a four-shot CoT in text prompt with the same four problems that were used to build the\nProlog prompt.\nFigure 2.b demonstrates a performance exceeding 98% for all three models integrated with Prolog\non the Navigate task, which is a significant improvement compared to the performance of models\nprompted with CoT in text. This data suggests that the integration of a symbolic reasoning module\ncan help prevent arithmetic errors in LLMs and assist with the models' limited working memory\nwhen tracking and updating variables of the world model states. Notably, the davinci-text-003 and\nGPT3.5 models, which have a more restricted working memory relative to GPT4, showed the most\nsignificant improvements."}, {"title": "5 NLR Dataset", "content": "We next introduce a new dataset, the Non-Linear Reasoning dataset (NLR), a collection of 55 problems hand-designed to evaluate the generality of LLMs' mathematical reasoning capabilities to out-of-distribution problems requiring complex, non-linear reasoning. The NLR dataset contains three categories of problems: (i) Math word problems (21 problems), (ii) Constraint satisfactions problems (17 problems), and (iii) Algorithmic instructions for updating a game model (17 problems).\n\nUnlike other reasoning datasets that are syntactically generated, where the problem statements lack diversity in the wording, and premises and conclusions are expressed in short, direct statements [15, 9], our problems necessitate richer natural language understanding and both implicit and explicit deductive reasoning for resolution. The problems are also designed to disrupt the statistical co-occurrence patterns of words, by drawing inspiration from popular logic problems, but uniquely modifying the rules/constraints of the problems.\n\nIn contrast to the MATH dataset [16], which requires advanced mathematical skills such as calculating integrals and eigenvalues, the NLR problems only require basic arithmetic. However, despite the simplicity of the required math operations, LLMs struggle to solve NLR problems end to end (as shown in Figure 3). This is likely due to the high interrelationship between variables of the problem and the requirement to store and backtrack to intermediate states and trace multiple possible paths to the solution, which is challenging to perform using LLM's next word prediction paradigm.\n\nTo better understand the performance of the models on the NLR dataset, we introduce a notion of \u201centanglement\u201d between variables in a problem. Variables are entangled when the value of a variable must be inferred through its relationship with other variables and modifying the value of the variable affects the value of the variables entangled with it. For instance, in the math word problem presented in Table 1, each person's age is defined in terms of other people's age, rather than by a specific numerical value. Consequently, the system of equations encoding the relationships of the variables require a nonlinear computational process for resolution, which involves an iterative process of simplifying and substituting more complex linear equations. The algorithmic instruction problem in Table 1 also exhibit variable entanglement. Seating a person in the cinema affects the availability of the 4 adjacent seats. Hence, the seat value is entangled with the values of the 4 neighboring seats. Consequently, all variables in the game model's current state must be updated based on the problem's conditionals and update rules after each action. This significantly challenges the model's working memory."}, {"title": "6 Limitations and Broader Impact", "content": "Our work, which is on the path to creating models that can reason generally, reliably, and correctly like humans, can lead to several positive societal impacts. Such models can significantly increase the reliability and usefulness of current AI. In turn, improving the reasoning capabilities of language models could lead to job displacement as they can more reliably automate complex tasks traditionally performed by humans.\n\nThe main limitation of the NLR dataset is scalability. Designing tasks that require unique reasoning patterns for resolution, math word problems with high variable entanglement, constraint satisfaction problems with constraints encoding multiple possibilities, and game algorithms with new rules is complex and time consuming. Moreover, LLMs may produce coherent but incorrect logical code solutions, making error detection challanging. Additionally, Prolog's limited infrastructure support"}, {"title": "7 Conclusion", "content": "This work highlights inherent limitations of LLMs in performing reliable and generalizable reasoning, and suggests these problems can be mitigated by integrating a symbolic reasoning system into their inference pipeline. Our neurosymbolic approach prompts the LLM to map the information encoded by problem statements to logical code statements, thereby delegating the iterative computations of explicit deductive reasoning to a reliable reasoning engine. This division of labor significantly enhances the performance of LLMs on mathematical reasoning tasks. Additionally, our novel NLR dataset provides a robust benchmark for evaluating the generality of LLMs' mathematical reasoning capabilities to problems that require unique nonlinear reasoning and challenge the limitations of the linear next word prediction paradigm of LLMs."}]}