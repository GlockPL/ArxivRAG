{"title": "Enhancing Robot Navigation Policies with Task-Specific Uncertainty Management", "authors": ["Gokul Puthumanaillam", "Paulo Padrao", "Jose Fuentes", "Leonardo Bobadilla", "Melkior Ornik"], "abstract": "Robots performing navigation tasks in complex environments face significant challenges due to uncertainty in state estimation arising from sensor noise, environmental changes, and incomplete information. Effectively managing this uncertainty is crucial, but the optimal approach varies depending on the specific details of the task: different tasks may require varying levels of precision in different regions of the environment. For instance, a robot navigating a crowded space might need precise localization near obstacles but can operate effectively with less precise state estimates in open areas. This varying need for certainty in different parts of the environment, depending on the task, calls for navigation policies that can adapt their uncertainty management strategies based on task-specific requirements.\nIn this paper, we present a framework for integrating task-specific uncertainty requirements directly into navigation policies. We introduce the concept of a Task-Specific Uncertainty Map (TSUM), which represents acceptable levels of state estimation uncertainty across different regions of the operating environment for a given task. Using TSUM, we propose Generalized Uncertainty Integration for Decision-Making and Execution (GUIDE), a policy conditioning framework that incorporates these uncertainty requirements into the robot's decision-making process.\nWe find that conditioning policies on TSUMs not only provides an effective way to express task-specific uncertainty requirements but also enables the robot to reason about the context-dependent value of certainty and adapt its behavior accordingly. We show how integrating GUIDE into reinforcement learning frameworks allows the agent to learn navigation policies that effectively balance task completion and uncertainty management without the need for explicit reward engineering. We evaluate GUIDE at scale on a variety of real-world robotic navigation tasks and find that it demonstrates significant improvements in task completion rates compared to baseline methods that do not explicitly consider task-specific uncertainty.", "sections": [{"title": "1 Introduction", "content": "Consider an autonomous surface vehicle (ASV) tasked with mapping underwater topography near a busy harbor. Performing such a task in a marine environment is often complicated by factors like waves and environmental changes. These elements can introduce significant uncertainty in the ASV's state estimation. To effectively carry out its mission, the ASV needs to manage this uncertainty. Importantly, constantly striving to reduce uncertainty is not always necessary for task completion. When navigating through areas with heavy boat traffic or obstacles, precise localization is crucial to avoid collisions and collect accurate data. However, while traversing open waters during this mission, the ASV can tolerate higher positional uncertainty, allowing it to focus on efficient task execution rather than achieving pinpoint accuracy. This scenario underscores a fundamental challenge in robotic navigation: the acceptable level of uncertainty at any location in the operation space is inherently task-specific and context-dependent.\nTraditional approaches to uncertainty management in robotic navigation often fall into two categories: minimizing uncertainty universally [12, 13, 39, 49] or enforcing fixed uncertainty thresholds [7, 19, 53]. Such one-size-fits-all strategy may be effective in controlled settings, they lack the flexibility to adapt to tasks that demand varying degrees of uncertainty management. For instance, in the ASV's mission introduced earlier, strictly enforcing precise localization may be unnecessary when navigating open waters, where higher uncertainty does not hinder task execution.\nThese existing methods often neglect the varying uncertainty requirements dictated by different contexts and tasks. While some approaches attempt to balance uncertainty reduction with task performance [63, 65], they typically apply this trade-off uniformly across all situations, failing to account for task-specific needs that may vary within a single mission. Furthermore, developing effective uncertainty management strategies often require extensive reward engineering to capture the intricacies of different tasks [5, 79].\nThe fundamental issue lies in the disconnect between task specifications and uncertainty management. Current frameworks often treat these elements separately, resulting in sub-optimal performance when effective task completion demands varying uncertainty levels across different regions in the operating environment."}, {"title": "2 Related Works", "content": "Uncertainty Modeling in Robotic Navigation: Uncertainty management is a fundamental aspect of robotic navigation, where robots must make decisions based on imperfect information about their state and the environment [49, 57]. Probabilistic techniques such as Bayesian filters [15, 25, 41] and particle filters [11, 16, 66] have been widely used to enable robust localization and mapping in stochastic environments. These methods allow robots to estimate their position and navigate effectively despite sensor noise and environmental uncertainties. However, traditional approaches often treat uncertainty uniformly across the environment, without considering how different regions may require varying levels of certainty depending on the navigation task.\nReinforcement Learning for Navigation: Reinforcement Learning (RL) has been successfully applied to robotic navigation tasks, enabling agents to learn navigation policies through interaction with the environment [69, 70, 76]. Standard RL algorithms have been used to train robots to navigate in various settings, from simulated environments to real-world scenarios [32, 67, 75]. While these methods can learn effective policies in controlled environments [27, 48, 80], they often perform suboptimally in real-world scenarios due to factors like partial observability, dynamic changes, and varying levels of uncertainty [8, 40]. Additionally, standard RL typically does not account for uncertainty explicitly, which can hinder performance in complex environments where uncertainty plays a significant role.\nTask-Specific Navigation Policies: Recent advances have focused on developing navigation policies that are conditioned on specific tasks or goals [10, 38, 42, 52]. Task-conditioned RL allows agents to adapt their behavior based on high-level task specifications, improving flexibility and generalization. In navigation, this has been explored by training policies capable of handling objectives, adjusting the robot's behavior according to the task [43, 68, 78]. These approaches often neglect the varying importance of uncertainty management [9, 26] across different tasks and environments.\nUncertainty-Aware Reinforcement Learning in Navigation: Incorporating uncertainty into RL for navigation has been investigated to enhance exploration, robustness, and safety [62, 76]. Some approaches introduce uncertainty penalization directly into the reward function, encouraging agents to avoid high-uncertainty regions [14, 34, 77]. Others utilize Bayesian RL methods to model uncertainty in value estimation, improving decision-making under uncertainty [1, 3]. Bootstrapped ensembles maintain multiple value function estimates to capture epistemic uncertainty, leading to more informed exploration strategies [2, 29, 45, 72]. While these methods consider uncertainty, they often do so globally or uniformly [31, 46, 50] across the environment and do not tailor the uncertainty management to specific navigation tasks or spatial regions.\nRisk-Aware Planning and Navigation: Risk-aware planning introduces measures to balance performance and safety by considering the potential risks associated with different actions [18, 33]. These techniques [44, 74] enable robots to make decisions that account for uncertainty in the environment [22]. In the context of RL, risk-sensitive approaches adjust the policy to avoid high-risk actions, often through modified reward functions or policy constraints [6, 37, 64]. Although effective in managing risk, these methods typically apply uniform risk thresholds and do not adapt to the task-specific uncertainty requirements that may vary across different navigation scenarios.\nTask-Specific Uncertainty Management in Navigation: Integrating high-level task specifications into uncertainty management for navigation remains an open challenge. Some recent works propose large pre-trained models to interpret complex instructions and generate corresponding navigation behaviors [4, 21, 47, 58-61]. While these approaches enable robots to understand and execute a wider range of tasks, they often lack a systematic method for representing and integrating task-specific uncertainty requirements into the navigation policy. This limitation reduces their effectiveness in complex environments where the importance of uncertainty can vary significantly across different regions.\nIn contrast to these prior works, our proposed GUIDE framework systematically and explicitly incorporates task-specific uncertainty requirements into both the planning and learning processes for navigation. By addressing the limitations of existing methods by treating uncertainty in a task-specific manner, our approach enables robots to dynamically adjust their navigation policies according to the varying importance of uncertainty."}, {"title": "3 The GUIDE Framework", "content": "Consider a robot operating in a continuous state space S, where each state $s \\in S$ represents its pose. It can execute actions $a \\in A$ from a continuous action space A. Assigned a navigation task t specified by a natural language description, the robot must achieve objectives and adhere to constraints related to features of the environment. Our goal is to develop a navigation policy $\\pi(a|s)$ that efficiently accomplishes the task while managing the robot's state estimation uncertainty $u(s)$ in a task-aware manner. We aim to address the following problems using GUIDE:\n(i) Interpret the task t to determine acceptable levels of uncertainty across different regions of the environment, reflecting the relative importance of uncertainty reduction as dictated by the task requirements.\n(ii) Synthesize a navigation policy that integrates this task-specific uncertainty information into the decision-making process. The policy should guide the robot to manage its uncertainty appropriately, prioritizing uncertainty reduction where necessary to ensure successful task execution, while efficiently pursuing task objectives in other areas."}, {"title": "3.1 Task-Specific Uncertainty Map (TSUM)", "content": "To effectively integrate task-specific uncertainty considerations into robotic navigation, we introduce the concept of Task-Specific Uncertainty Maps (TSUMs). A TSUM is a function $U: L \\rightarrow \\mathbb{R}^+$ that assigns an acceptable level of state estimation uncertainty to each location $l \\in L$ for a given navigation task $\\tau$.\nFormally, the TSUM $U^\\tau(l)$ is defined as:\n$U^\\tau(l) = \\psi (\\Phi^\\tau (l), C^\\tau (l), E(l)),$   (1)\nwhere:\nTask Relevance Function $\\Phi^\\tau(l)$: This function quantifies the importance of location l for the successful completion of task $\\tau$. It measures how critical precise navigation and state estimation are at l to fulfill the task objectives.\nTask-Specific Constraints $C^\\tau(l)$: These constraints represent conditions that must be satisfied at location l for task $\\tau$. They may include requirements such as avoiding restricted zones, operating within designated perimeters, or adhering to specific operational parameters unique to the task.\nEnvironmental Factors $E(l)$: This component captures static properties of the environment at location l, such as the presence of obstacles, terrain difficulty, or regions known to be unsafe or infeasible for navigation.\nThe function $\\psi$ combines these components into a scalar value $U^\\tau(l)$, representing the acceptable level of uncertainty at location l for task $\\tau$. We define $\\psi$ as a weighted sum:\n$U^\\tau(l) = w_\\Phi \\Phi^\\tau (l) + w_C C^\\tau (l) + w_E E(l),$   (2)\nwhere $w_\\Phi$, $w_C$, and $w_E$ are weighting coefficients that determine the relative influence of each component.\nWe now present a method to generate TSUM from any given task description."}, {"title": "3.1.1 Implementation Details", "content": "To generate the Task-Specific Uncertainty Map (TSUM) $U^\\tau(l)$, we combine task-specific semantics with spatial context.\nTask Description Processing. Similar to the work in [23], we extract m subtasks ${t_i}_{i=1}^m$ and c constraints ${c_k}_{k=1}^c$ from the natural language task description t using dependency parsing and Named Entity Recognition (NER).\nSemantic and Spatial Embeddings. Subtasks and constraints are mapped into an n-dimensional semantic embedding space using the function $E_c: C \\rightarrow \\mathbb{R}^n$, implemented with a fine-tuned RoBERTa model [36] trained on navigation tasks:\n$E_c(t_i), E_c(c_k) \\in \\mathbb{R}^n.$   (3)\nSpatial locations $l \\in L$ are characterized by their coordinates and environmental features. These features are processed by a neural network $f_{spatial}$ to produce spatial embeddings $E_l(l) \\in \\mathbb{R}^n$:\n$E_l(l) = f_{spatial} (features (l)).$   (4)\nThe network $f_{spatial}$ consists of two hidden layers with 128 and 64 units and ReLU activations.\nComputing $\\Phi^\\tau(l)$ and $C^\\tau (l)$. Attention weights $\\alpha_i$ and $\\beta_k$ are assigned to subtasks and constraints using hierarchical attention networks [71]:\n$\\alpha_i = \\frac{exp(u_i)}{\\sum_{j=1}^{m} exp(u_j)} \\quad u_i = v^T tanh(WE_c(t_i) + b),$   (5)\n$\\beta_k = \\frac{exp(u_k)}{\\sum_{l=1}^{c} exp(u'_l)} \\quad u'_k = v'^T tanh(W'E_c(c_k) + b'),$   (6)\nwhere $W \\in \\mathbb{R}^{h \\times n}$, $v \\in \\mathbb{R}^h$, $b \\in \\mathbb{R}^h$, h is the hidden layer size and $W' \\in \\mathbb{R}^{h' \\times n}$, $v' \\in \\mathbb{R}^{h'}$, $b' \\in \\mathbb{R}^{h'}$. The task relevance function $\\Phi^\\tau(l)$ and the"}, {"title": "3.1.2 Training the TSUM Generation Model", "content": "The embedding functions and attention networks are trained using a dataset of paired semantic concepts and spatial locations with known relationships\u00b9.\nTriplet Loss for Embedding Alignment. To align the semantic and spatial embeddings, we employ a triplet loss function [55]:\n$L_{triplet} = \\sum_{i=1}^{N} [||E_c (t_i) - E_l(l^+_i)||_2 - ||E_c (t_i) \u2013 E_l (l^-_i) ||_2 + \\delta]_+,$   (10)\nwhere N is the number of training samples, $l^+_i$ is a location relevant to subtask $t_i$, $l^-_i$ is an irrelevant location, $\\delta$ is a margin parameter, and $[x]_+ = max(0, x)$.\nThis loss encourages the model to bring embeddings of related subtasks and locations closer while separating unrelated pairs.\nAttention Network Training. The attention networks are trained by minimizing the Kullback-Leibler divergence [28] between the predicted attention distributions and the ground truth distributions:\n$L_{attention} = KL (\\alpha || \\alpha^{gt}) + KL (\\beta || \\beta^{gt}),$   (11)\nwhere $\\alpha^{gt}$ and $\\beta^{gt}$ are the ground truth attention weights.\nTotal Loss Function. The total loss for training is:\n$L = L_{triplet} + \\lambda L_{attention},$   (12)\nwith $\\lambda$ being a hyperparameter balancing the two loss components."}, {"title": "3.2 Conditioning Navigation Policies Using Task-Specific Uncertainty Maps", "content": "To enable robots to manage uncertainty in a task-aware manner, we propose conditioning the navigation policy on the Task-Specific Uncertainty Map (TSUM). Our objective is to learn a policy $\\pi$ that allows the robot to efficiently accomplish the task t while adhering to the uncertainty requirements specified by the TSUM.\nTo condition the policy, we augment the state representation to include both the acceptable uncertainty levels from the TSUM and the robot's current estimation of its state uncertainty. For a given task t, the augmented state $\\tilde{s}$ is defined as:\n$\\tilde{s} = [s, U^t (s), u(s)],$   (13)\nwhere s is the original state representing the robot's observation of the environment, $U^t(s)$ is the acceptable uncertainty level at state s derived from the TSUM, and u(s) is the robot's current state estimation uncertainty."}, {"title": "3.2.1 GUIDEd Soft Actor-Critic Algorithm", "content": "While any reinforcement learning algorithm can be used to learn the optimal policy $\\pi^*(a|s)$, we adopt the Soft Actor-Critic (SAC) algorithm [17] due to its sample efficiency and robustness. Our adapted version, referred to as GUIDEd SAC, integrates TSUMs by conditioning the policy and value functions on the augmented state $\\tilde{s}$.\nIn the SAC framework, the objective is to maximize the expected cumulative reward augmented by an entropy term, which encourages exploration:\n$J(\\pi) = E_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^t (R(s_t, a_t, \\tau) + \\alpha H(\\pi(\\cdot | \\tilde{s}_t)))) | \\tilde{s}_0],$   (15)\nwhere $H(\\pi(\\cdot | \\tilde{s}_t)) = -E_{a_t \\sim \\pi(\\cdot | \\tilde{s}_t)} [log \\pi(a_t | \\tilde{s}_t)]$ is the entropy of the policy $a_t$ at state $\\tilde{s}_t$, and $\\alpha$ is the temperature parameter balancing exploration and exploitation.\nGUIDEd SAC maintains parameterized function approximators for the policy $\\pi_\\theta(a|s)$ and the soft Q-value functions $Q_{\\phi_1} (\\tilde{s}, a)$ and $Q_{\\phi_2} (\\tilde{s}, a)$, where $\\theta$ and $\\phi_i$ denote the parameters of the policy and value networks, respectively.\nThe soft Q-value networks $Q_i (\\tilde{s}, a)$ are updated by minimizing the soft Bellman residual:\n$L_Q(\\phi_i) = E_{(\\tilde{s}_t, a_t, r_t, \\tilde{s}_{t+1})\\sim D} [(Q_{\\phi_i} (\\tilde{s}_t, a_t) - y_t)^2],$   (16)"}, {"title": "4 Experiments", "content": "To evaluate the effectiveness of GUIDE, we conduct experiments in a real-world, real-time scenario using an autonomous surface vehicle (ASV). The ASV is tasked with navigating through a GPS-constrained environment, where precise localization comes at a cost. The agent operates with noisy position estimates by default and can request GPS readings when needed, reducing the uncertainty but at the cost of a higher penalty for each request. The experimental setup is designed to mimic realistic operational constraints [51, 73], highlighting the core challenge: balancing task success with effective uncertainty management. Excessive reliance on noisy localization risks failure in critical areas, while frequent use of high-precision GPS leads to increased mission costs."}, {"title": "4.1 Experimental Setup", "content": "4.1.1 Environment. The ASV operates in an open lake characterized by environmental variability and human-induced disturbances. The environment features fountains that serve as obstacles and introduce additional layers of uncertainty by generating water disturbances that create both physical challenges for navigation and contribute to the overall uncertainty in state estimation."}, {"title": "4.1.2 Autonomous Surface Vehicle Setup", "content": "Our experiments utilize the SeaRobotics Surveyor Autonomous Surface Vehicle (ASV), which is equipped with a Global Positioning System (GPS), an Inertial Measurement Unit (IMU), and a YSI EXO2 multiparameter water quality sonde. The GPS provides precise positioning, while the IMU assists with motion tracking and orientation. The YSI EXO2 sonde, primarily used for environmental monitoring, contributes to the overall state estimation by providing additional context.\nThe action space of the ASV is defined as a = (\u03bb, \u03b1, \u03b7), where \u03bb\u2208 [0, \u03bbmax] represents the propulsion torque (with \u03bbmax being the maximum allowed torque), \u03b1 \u2208 [0, 2\u03c0) denotes the steering angle, and \u03b7 is a discrete variable indicating the mode of position estimation. The ASV has a maximum speed of 2 knots."}, {"title": "4.1.3 Position Estimation Modes", "content": "To represent the operational challenges of managing uncertainty during navigation, we model two modes of position estimation for the ASV [51]:\n(i) Noisy Position Estimation: By default, the ASV estimates its position using the IMU and YSI EXO2 sensors, which results in a noisy and less accurate position estimate [54]. This mode represents the low-cost but high-uncertainty estimate.\n(ii) Exact Position Estimation: The ASV can request exact position data from GPS. This incurs a higher resource cost, leading to a reduced overall mission reward."}, {"title": "4.1.4 Evaluation Protocol", "content": "The agent receives a natural language task specification, describing a navigation task using key features of the environment\u00b2.\nTask completion is evaluated based on specific criteria for each task type. For goal-reaching tasks, the agent must reach within a 1.5-meter radius of the target. For perimeter or exploration tasks, it must remain within a 3.5-meter corridor of the path, and for avoid tasks, a 3-meter minimum distance from designated areas is required. Task completion rate (TCR) is determined by the proportion of the task completed, and rewards are based on the total reward earned at task completion. All baselines interpret instructions through the same semantic parser, converting them into structured representations suitable for each algorithm."}, {"title": "4.2 Baselines and Ablations", "content": "We compare GUIDE against several baselines and conduct ablation studies.\n4.2.1 Ablation Studies. We perform two ablations:\n(i) Standard RL without TSUMs (SAC, Ablation 1): We train SAC agents using only the original state s, without TSUMs, to assess the importance of TSUMs in managing uncertainty.\n(ii) GUIDEd PPO (G-PPO, Ablation 2): We implement GUIDED PPO [56], incorporating TSUMs and agent uncertainty into the state representation, to examine the effect of the underlying RL algorithm on GUIDE's performance.\n4.2.2 Baselines. We compare GUIDE against several baselines: SAC-Based Methods. We include SAC variants that handle uncertainty differently:\n(i) RL with Uncertainty Penalization (SAC-P): We modify the reward function in SAC to penalize high uncertainty: RSAC-P = Rbase - \\zeta u(s), where Rbase includes task rewards and localization costs, \\zeta is a weighting factor, and u(s) is the agent's current uncertainty. This tests traditional reward shaping versus GUIDE's approach."}, {"title": "4.3 Results", "content": "The performance of GUIDE compared to various baseline methods is summarized in Table 1. Our results demonstrate that GUIDE consistently outperforms all baselines across all task categories. Notably, both GUIDEd Actor-Critic (G-SAC) and GUIDED PPO (G-PPO) outperform the baselines; however, G-SAC generally achieves higher task completion rates and rewards."}, {"title": "4.3.1 Analysis of Ablation Studies", "content": "Impact of TSUMs (Ablation 1). To assess the significance of incorporating Task-Specific Uncertainty Maps (TSUMs), we compare the standard Soft Actor-Critic (SAC) without TSUMs to GUIDEd Actor-Critic (G-SAC). As shown in Table 1, conditioning on TSUMs significantly enhances performance across all tasks. Without TSUMS, SAC cannot manage positional uncertainty in a task-specific manner, leading to suboptimal navigation decisions and lower rewards. To further illustrate the effect of TSUM integration, we present in Figure 4 the trajectories of agents with and without TSUMs on a representative task. We observe that SAC often overuses high-precision localization in areas where it is unnecessary, incurring additional costs without significant benefits. Conversely, in critical regions requiring precise navigation, SAC fails to reduce uncertainty appropriately, leading to collisions with the fountains. In contrast, the G-SAC agent effectively uses TSUMs to adapt its uncertainty management, switching to high-precision localization in areas where the TSUM indicates low acceptable uncertainty. This enables the agent to navigate safely around obstacles and complete tasks more efficiently.\nEffect of RL Algorithm Choice (Ablation 2). We also investigate the impact of the underlying reinforcement learning algorithm by comparing GUIDEd PPO (G-PPO) to G-SAC, both of which integrate TSUMs but differ in their RL methodologies. This difference in the performance of G-SAC and G-PPO can be attributed to several factors inherent to the algorithms. G-SAC, based on the SAC framework, is an off-policy method that leverages entropy regularization to encourage exploration while maintaining stability. Its off-policy nature allows for more efficient sample utilization, which is particularly beneficial in continuous action spaces and when data collection is costly or limited. In contrast, PPO is an on-policy algorithm that relies on proximal updates to prevent large policy shifts, using a clipped objective function. While PPO is stable, it can be less sample-efficient [35], as it requires new data for each policy update and may not explore as effectively in complex environments.\nOur empirical results and the trajectories in Figure 4 suggest that the off-policy efficiency and exploration capabilities of G-SAC make it better suited for navigation tasks. Furthermore, the entropy term in G-SAC encourages the agent to consider a wider range of actions, enabling it to discover more optimal strategies for managing uncertainty in a task-specific context.\n4.3.2 Comparison with Baselines. GUIDE consistently outperforms all baselines across task types in both task completion rate and average reward (see Table 1). Standard RL methods like SAC and SAC-P lack task-specific uncertainty management. SAC-P penalizes high uncertainty uniformly, resulting in overly conservative behavior where precision is unnecessary and insufficient caution in critical regions, ultimately leading to lower performance. B-SAC estimates epistemic uncertainty but fails to adapt to task-specific requirements, leading to inefficient exploration. Risk-Aware RL methods like CVaR are uniformly risk-averse, missing opportunities for calculated risk-taking that could improve task success. RAA aims to minimize overall uncertainty without considering task context, often generating inefficient paths. The Heuristic Policy (HEU) switches to exact localization near obstacles, but lacks GUIDE's adaptability, failing to adjust to sudden changes in uncertainty requirements."}, {"title": "4.3.3 Behavior of GUIDEd Agents", "content": "We observed distinct behaviors in GUIDEd agents across various tasks. Analyzing the specific task shown in Figure 5, GUIDEd agents strategically adjust their reliance on precise position estimation versus noisier estimates. In areas where the TSUMs indicate high precision is necessary - such as navigating near obstacles or close to the perimeters (highlighted in yellow) - the agents opt for exact positioning despite the higher operational cost. Conversely, in less critical regions (shown in purple), they rely on less precise, noisy estimates. This adaptability allows GUIDEd agents to manage uncertainty more efficiently than baselines, resulting in faster completion times and smoother trajectories. Although not perfect - occasionally missing sections of the perimeter (indicated by black shaded regions in Figure 5) - GUIDEd agents significantly outperform baselines like SAC-P with engineered rewards. Baseline methods, lacking adaptive uncertainty management, frequently fail to complete tasks safely. They often collide with obstacles (Figure 5 pink diamond) or take inefficient paths (Figure 5 black region)."}, {"title": "5 Conclusion", "content": "In this paper, we introduced GUIDE, a framework that integrates task-specific uncertainty requirements into robotic navigation policies. Central to our approach is the concept of Task-Specific Uncertainty Maps (TSUMs), which represent acceptable levels of state estimation uncertainty across different regions of the environment based on the given task. By conditioning navigation policies on TSUMs, we enable robots to reason about the context-dependent importance of certainty and adapt their behavior accordingly.\nWe demonstrated how GUIDE can be incorporated into reinforcement learning frameworks by augmenting the state representation to include both the acceptable uncertainty levels from TSUMs and the robot's current state estimation uncertainty. Specifically, we adapted the Soft Actor-Critic (SAC) algorithm to operate in this augmented state space, resulting in the GUIDEd SAC algorithm. Our extensive experiments on a variety of real-world robotic"}, {"title": "B Implementation Details", "content": "B.1 Training the TSUM Generation Model\nIn this section, we provide a detailed description of the dataset used to train the embedding functions and attention networks for the Task-Specific Uncertainty Map (TSUM) generation model. We explain the data collection process, the nature of the tasks included, and how the data was annotated and processed to facilitate effective learning.\nB.1.1 Dataset Description and Collection Process. The dataset consists of paired semantic concepts and spatial locations with known relationships, collected using an ASV in a controlled aquatic environment. We used this data alongside the open-sourced Virtual Ocean Robotics Challenge simulator\u00b3. The ASV was equipped with a GPS, an IMU, and a YSI EXO2 sonde for measuring water quality parameters. The environment featured a variety of navigational elements such as docks, fountains, and virtual designated zones to provide contextual information.\nTo construct the dataset, we designed and executed a series of navigation tasks that encompassed diverse operational scenarios. During each task, both in simulation and on hardware, sensor data, positional information, and timestamped actions were recorded. Approximately 50 natural language task descriptions were hand-crafted with placeholders to include specific semantic elements that could be directly associated with spatial locations within the environment. The dataset includes over 8,000 task instances, ensuring comprehensive coverage of different task types and constraints.\nB.1.2 Task Categories and Examples. The tasks in the dataset are grouped into six categories, each designed to test specific aspects of navigation and uncertainty management. Below, we describe each category in detail and provide representative examples.\nGoal Reaching: Waypoint. This category involves tasks where the ASV is instructed to navigate to specific coordinates. The focus is on reaching designated points in the environment.\nExamples:\n\u2022 Navigate to waypoint (12.0, -7.5).\n\u2022 Proceed to the coordinates (8.5, 15.0).\n\u2022 Go to the location at (5.0, -10.0).\nGoal Reaching: Contextual Landmarks. This category includes tasks where the ASV is instructed to navigate to locations identified by contextual landmarks rather than explicit coordinates. This tests the ability to interpret semantic information and associate it with spatial positions.\nExamples:\n\u2022 Go to the dock.\n\u2022 Proceed to the central fountain.\n\u2022 Navigate to the area in front of the left fountain.\nAvoidance Tasks. These tasks instruct the ASV to avoid certain points or areas, emphasizing obstacle detection and path planning to circumvent specified locations.\nExamples:\n\u2022 Avoid the coordinates (10.0, -5.0).\n\u2022 Steer clear of the submerged rock at (3.5, 4.0).\nPerimeter Navigation Tasks. In this category, the ASV is tasked with navigating around the perimeter of a specified area. This requires maintaining a certain distance from boundaries.\nExamples:\n\u2022 Navigate around the perimeter of the bottom-right quadrant.\n\u2022 Circumnavigate the central fountain.\n\u2022 Traverse the boundary of the entire lake.\nExploration Tasks. These tasks involve exploring a specified area for a fixed duration of 5 minutes, testing the ASV's ability to stay withing an area and cover parts.\nExamples:\n\u2022 Explore the top-half of the lake.\n\u2022 Conduct an exploration of the top-right quadrant.\nRestricted Area Navigation. Tasks in this category require the ASV to navigate while avoiding specified regions.\nExamples:\n\u2022 Go to waypoint (6.0, -3.0) while avoiding the right half of the lake.\n\u2022 Navigate to the right fountain, avoiding the exclusion zone.\n\u2022 Proceed to the dock without passing through the left half of the lake.\nB.1.3 Natural Language Processing and Embedding Generation. To process the natural language task descriptions, we utilized a fine-tuned RoBERTa language model, which captures contextual nuances and effectively handles synonyms and varied phrasings. This enables the model to interpret different expressions of similar tasks, ensuring robustness to linguistic variations. For example, phrases like \"proceed to\", \"navigate to\", and \"go to\" are recognized as equivalent in intent."}, {"title": "B.1.4 Data Annotation and Processing", "content": "For each task, the natural language descriptions were parsed to extract semantic concepts such as goals, landmarks, actions, and constraints. These concepts were mapped to spatial coordinates using the ASV's GPS data and predefined environmental maps. The pairing of semantic elements with precise spatial locations enabled the creation of meaningful training examples for the embedding functions.\nPositive pairs (related semantic concepts and spatial locations) and negative pairs (unrelated concepts and locations) were included to optimize the contrastive loss function during training. This ensures that the model learns to distinguish between relevant and irrelevant semantic-spatial relationships, enhancing its ability to generate accurate and contextually appropriate uncertainty maps."}, {"title": "B.2 Hyperparameters", "content": ""}]}