{"title": "Can LLMs Replace Human Evaluators? An Empirical Study of LLM-as-a-Judge in Software Engineering", "authors": ["RUIQI WANG", "JIYU GUO", "CUIYUN GAO", "GUODONG FAN", "CHUN YONG CHONG", "XIN XIA"], "abstract": "Recently, large language models (LLMs) have been deployed to tackle various software engineering (SE) tasks like code generation, significantly advancing the automation of SE tasks. However, assessing the quality of these LLM-generated code and text remains challenging. The commonly used Pass@k metric necessitates extensive unit tests and configured environments, demands a high labor cost, and is not suitable for evaluating LLM-generated text. Conventional metrics like BLEU, which measure only lexical rather than semantic similarity, have also come under scrutiny. In response, a new trend has emerged to employ LLMs for automated evaluation, known as LLM-as-a-judge. These LLM-as-a-judge methods are claimed to better mimic human assessment than conventional metrics without relying on high-quality reference answers. Nevertheless, their exact human alignment in SE tasks remains unexplored.\nIn this paper, we empirically explore LLM-as-a-judge methods for evaluating SE tasks, focusing on their alignment with human judgments. We select seven LLM-as-a-judge methods that utilize general-purpose LLMs, alongside two LLMs specifically fine-tuned for evaluation. After generating and manually scoring LLM responses on three recent SE datasets of code translation, code generation, and code summarization, we then prompt these methods to evaluate each response. Finally, we compare the scores generated by these methods with human evaluation. The results indicate that output-based methods reach the highest Pearson correlation of 81.32 and 68.51 with human scores in code translation and generation, achieving near-human evaluation, noticeably outperforming ChrF++, one of the best conventional metrics, at 34.23 and 64.92. Such output-based methods prompt LLMs to output judgments directly, and exhibit more balanced score distributions that resemble human score patterns. Finally, we provide insights and implications, concluding that current state-of-the-art LLM-as-a-judge methods can potentially replace human evaluations in certain SE tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Since BERT [6] and GPT [36], pre-trained language models (PLMs) have been widely used in various natural language processing (NLP) tasks, such as machine translation and text summarization. With the scaling of PLM parameters, the concept of large language models (LLMs) has been proposed. Featuring up to hundreds of billions of parameters, LLMs emerge new capabilities absent on smaller models [49], beyond solving simple linguistic tasks. These capabilities include, but not limited to, instruction following and multi-step reasoning, enabling LLMs to simulate human experts and achieve state-of-the-art performance in certain domains. Software engineering (SE) is one of the specialized domains that benefits from this trend. Many researchers and companies either emphasize their LLMs' strong coding performance [33, 37], or develop specialized code LLMs. For instance, DeepSeek-Coder-V2 [5] correctly generates code for 75.3% instructions in HumanEval [4] and MBPP [1] with 236B parameters, second only to GPT-40. Qwen2.5-Coder [18] achieves 88.4% Pass@1 on HumanEval with merely 7B parameters.\nHowever, there has been limited progress in evaluating LLM-generated content for SE. The commonly used Pass@k metric executes the first k generated code snippets on human-curated unit tests. While Pass@k evaluates the code's functional correctness accurately, it has several limitations, such as requiring comprehensive unit tests and manual configuration of test environments. What is more, Pass@k is unable to evaluate code from non-functional aspects, such as readability and adherence to good practice, nor can it be used to judge text-generating SE tasks like code summarization and code review [17]. Therefore, some SE datasets [9, 55] resort to use conventional metrics such as BLEU [34] and CodeBLEU [38], which also have downsides like inability to perform multi-aspect evaluation and requiring human-annotated reference answers. These metrics also focus on lexical rather than semantic similarity, making the evaluation results questionable.\nMeanwhile, NLP researchers attempt to apply LLMs to evaluate the quality of LLM-generated content, known as LLM-as-a-judge [60]. While human effort remains reliable for evaluation and for curating the reference answers in datasets, it is both slow and expensive, defeating the purpose of automatic evaluation. Therefore, researchers prompt or train LLMs to align with human preference, as an attempt to replace human evaluators. Since both code and text can be viewed as sequences of tokens, LLM-as-a-judge methods can be potentially adopted on SE tasks. Unfortunately, current meta-evaluation benchmarks feature a limited number of simple coding tasks as they mostly target NLP tasks. The lack of test samples and the insufficient task difficulty create a gap between benchmarking on existing datasets and real-world SE scenarios, where the instructions, code, and responses are usually more complex and varied.\nTo bridge the gap, we conduct an empirical study to apply a range of LLM-as-a-judge methods on realistic SE datasets. Specifically, we select a task for each of the three input-output type combinations, and a recent representative dataset for each task: CodeTransOcean [55] for Code Translation (Code-Code), ComplexCodeEval [9] for Code Generation (Text-Code), and CodeXGLUE [32] for Code Summarization (Code-Text). Their corresponding papers only adopt conventional metrics like Exact Match (EM), BLEU, and CodeBLEU. We randomly sample 50 instructions from each dataset, and three out of 12 code LLMs to generate responses for each instruction. For each response, we manually assign a score indicating its quality, resulting in a dataset of 450 samples of (instruction, response, score) triplets in total. Then we perform meta-evaluation of different types of LLM-as-a-judge methods by calculating their score alignment with human scores, to validate whether their judgments match human preference in real-world scenarios.\nWe design the following three research questions (RQs):\n\u2022 RQ1: Which LLM-as-a-judge method aligns with human preference better, and do they outperform conventional metrics?"}, {"title": "2 RELATED WORK", "content": "LLMs are large-scale PLMs. Some of them are instruction-tuned to follow instructions in human language. In this paper, we do not distinguish between PLMs and LLMs, and use LLMs to refer to pre-trained Transformers [44] in general.\nWhile many general-purpose LLMs demonstrate satisfying performance on SE tasks, especially code generation, there are many LLMs pre-trained specifically for code-related tasks. CodeBERT [10] is one of the earliest attempts to pre-train a Transformer on both code and text data. It is an encoder-only model with 125M parameters, pre-trained on over 8M datapoints from CodeSearchNet [19]. CodeT5 [47] is an encoder-decoder Transformer with up to 770M parameters, pre-trained with a denoising sequence-to-sequence objective on the same dataset. UniXcoder [14] supports encoder-only, decoder-only, and encoder-decoder modes, allowing abstract syntax trees (ASTs) as input after transforming ASTs into sequences.\nRecently, larger decoder-only LLMs have been increasingly popular in generation tasks. Codex [4] is a series of GPT-based LLMs with up to 12B parameters, achieving a Pass@1 score of 28.81% on HumanEval. CodeLlama [39] is another LLM family with up to 70B parameters from Meta AI, trained from Llama 2 [43] to follow human instructions. DeepSeek-Coder [15] is a family of LLMs with up to 33B parameters, supporting both normal generation and fill-in-the-middle (FIM). Its successor, DeepSeek-Coder-V2 [5], is a mixture-of-experts (MoE) LLM with 16B or 236B parameters, claiming to have GPT-4 [33] level performance at a Pass@1 score of 90.2% on HumanEval."}, {"title": "2.2 SE Benchmarks and Metrics", "content": "Many SE benchmarks focus solely on code generation, where LLMs generate code for the given requirements and function signatures. HumanEval [4] is one of the most adopted code generation benchmarks, featuring 164 human-curated Python problems. It uses Pass@k as the evaluation metric. MBPP [1] is another popular benchmark with 974 Python problems, aiming at entry-level developers. APPS [16] is a much larger Python benchmark with 10000 problems, ranging from being solvable in one-line to presenting substantial challenges in algorithms. ClassEval [7] challenges LLMs with 100 class-level code generation problems in Python, and measure class-level and method-level Pass@k.\nSome benchmarks target other SE tasks. CodeReviewer [27] aims at three tasks in the code review process: commit quality estimation, reviewer comment generation, and code editing. CodeXGLUE [32] supports 10 SE tasks such as code summarization and code search. ComplexCodeEval [9] collects code from influential GitHub repositories for 4 tasks such as code generation and unit test generation. These benchmarks all evaluate responses with conventional metrics including Exact Match, Edit Similarity, BLEU, and CodeBLEU instead of Pass@k, even for code-generating tasks. CRUXEval [13] evaluates LLMs from other aspects such as code understanding and execution with 800 short Python functions for input or output predictions. It requires LLMs to output assert statements to obtain Pass@k scores.\nHowever, limited efforts are made to curate meta-evaluation benchmarks to test evaluation metrics, as most datasets only contain instructions and reference answers, without responses of different quality or human-annotated scores. NoFunEval [41] designs six evaluation aspects, including functional correctness and non-functional aspects like latency and maintainability. It tests whether LLMs can improve code based on a specific aspect or select the better of two code snippets from that perspective. CodeUltraFeedback [52] evaluates LLMs' alignment with human evaluation from five non-functional code aspects like instruction following and coding style."}, {"title": "2.3 LLM-as-a-Judge in NLP", "content": "Some researchers obtain contextual token representations of the response and reference answer using encoder-only LLMs, and compute pairwise similarity to obtain the score. BERTScore [58] calculates Recall, Precision and F\u2081 score based on token representations obtained from BERT [6]. It also applies inverse document frequencies (IDFs) to reduce the weight of overly common and thus less essential tokens. MoverScore [59] constructs a transportation cost matrix based on token representations and computes Word Mover's Distance [23]. CodeBERTScore [62] is a code-specific adaptation of BERTScore with CodeBERT, approximating functional correctness and human scores with F3 and F\u2081 scores respectively. While these methods match contextual embeddings instead of n-grams, unlike many conventional metrics, they still measure how a response resembles the reference answer."}, {"title": "2.3.2 Probability-based Methods.", "content": "Since more LLMs come with decoders, it becomes possible to use generating probabilities for evaluation. BARTScore [57] assumes that BART [25] is more likely to generate a higher-quality response. It uses the probability of BART generating a given response as the score. GPTScore [11] applies a similar approach with 19 LLMs of sizes from 80M to 175B, supporting both reference-free and reference-based evaluation from multiple aspects. FFLM [20] is a reference-free method designed to evaluate the faithfulness of summaries. It calculates the probabilities of generating the summary with and without the original text as posterior and prior probabilities respectively. FFLM assumes that a faithful summary has higher posterior than prior probability, and calculates their difference as the score."}, {"title": "2.3.3 Output-based Methods.", "content": "While the above methods usually align with human evaluation better than conventional metrics, they do not explain their scores or support certain closed-source LLMs that do not provide probabilities or representations. Output-based methods prompt LLMs to output the judgments, and do not require access to their internal implementations. G-Eval [31] utilizes Chain-of-Thought (CoT) [50] to request evaluation steps, samples multiple scores and then averages them as the final score. ChatEval [3] assigns different personas to several LLM agents, asking them to discuss and select a better response from two.\nSome researchers construct training sets to fine-tune LLMs instead of designing prompting or inference strategies. InstructScore [54] is fine-tuned on GPT-4-synthesized data to generate error reports of text from various domains. PandaLM [48] is fine-tuned on pairwise comparison results and reference answers generated by GPT-3.5, aiming at addressing subjective aspects including conciseness and clarity. X-Eval [30] has an extra training stage to learn the connections between fine-grained evaluation aspects, allowing evaluating from aspects not seen during training.\nHowever, these methods have not been tested on a sufficient number of challenging SE samples, leaving it unclear whether they achieve reliable human alignment for SE applications."}, {"title": "3 LLM-AS-A-JUDGE FRAMEWORK OVERVIEW", "content": "In this section, we offer an overview of existing LLM-as-a-judge methods. As seen in Fig. 1, we categorize these methods based on the types of LLM features used\u00b9, including embedding-based, probability-based, and output-based methods. We denote the instruction (source) as src = $s_1...s_{|src|}$, the response (target) as tgt = $t_1...t_{|tgt|}$, and the reference answer as ref = $r_1...r_{|ref|}$.\n\u2022 Embedding-based: These methods first obtain token representations of the response and reference answer f(tgt) = f($t_1$)...f($t_{|tgt|}$) and f(ref) = f($r_1$)...f($r_{|ref|}$) from the LLM encoder f. We then evaluate tgt via fusing token-wise cosine similarities $s_{ij} = \\frac{f(t_i) \\cdot f(r_j)}{||f(t_i)|| ||f(r_j)||} $."}, {"title": "Probability-based", "content": "The LLM receives an input-output pair (in, out), and returns the condi-tional log-probability of generating out, i.e. log p(out|in) = $ \\sum_{k=1}^{|out|} log p(out_k|in, out_{<k})$. Typical (in, out) combinations include (src, tgt), (ref, tgt), (tgt, ref), and (none, tgt), where none means no input is provided. We then score tgt with these log-probabilities. Additional content may be present in the prompt, such as evaluation aspects like clarity."}, {"title": "Output-based", "content": "These methods first craft a prompt prompt with src and tgt. Depending on the design, prompt may also feature ref, evaluation aspects and criteria, and evaluation steps. After obtaining the judgment jud = LLM(prompt), we extract the final score from jud. Many prompting and inference strategies can also be applied, such as multi-agent and repeated sampling, where multiple scores are often combined using methods like a majority vote or averaging.\nLLMs can also be fine-tuned as specialized judges, usually applied with a single inference pass and no additional strategies. State-of-the-art LLMs like GPT-4 are often used to generate the reference judgment for training. In this paper, we discuss the performance of these LLMs, instead of focusing on the detailed training process.\nUnlike embedding-based and probability-based methods, which usually have scoring ranges of [0, 1] and (\u2212\u221e, 0) (or (-\u221e, \u221e)) respectively without rescaling, most output-based methods require LLMs to score on a scale of 1 to 5 or 1 to 10. They can also compare two responses and decide the better one or declare a tie. In our study, we investigate individual scoring in RQ1 and RQ2, and pairwise comparison in RQ3."}, {"title": "4 STUDY DESIGN", "content": "In this section, we elaborate on the details of our study design. In our study, we focus on leveraging different types of LLM-as-a-judge methods to evaluate the responses of three SE tasks. We collect instructions and generate responses from representative datasets, and then perform human and LLM evaluation on these responses, and analyze their correlations."}, {"title": "4.1 Datasets and Preprocessing", "content": "To ensure the difficulty of instructions and to approximate real-world development scenarios, we collect instructions from a recent dataset for each of the three widely-studied SE tasks for our empirical evaluation:"}, {"title": "Code Translation", "content": "is a code-to-code task demanding translating code between two languages while preserving the functionality. It challenges LLMs' skills to understand syntax and library usages in both languages, and to choose replacements when certain functionalities are unavailable in the target language."}, {"title": "Code Summarization", "content": "is a code-to-text task involving generating a concise and fluent description of a given code snippet. It challenges LLMs' abilities to abstract the code, leaving only critical information about core functionality rather than explaining step-by-step."}, {"title": "Code Generation", "content": "is a text-to-code task requiring generating a function based on a natural language description and the signature. It tests LLMs' capabilities to breakdown the functional requirement into steps, and to utilize provided dependencies."}, {"title": "4.1.2 Response Generation.", "content": "We deploy 12 recent code LLMs with different sizes from seven families shown in Table 1 from Hugging Face [53]. We generate responses using these LLMs with vLLM [24] on a Ubuntu 20.04 server with two Intel Xeon Platinum 8276L CPUs, four NVIDIA A100-40GB GPUs, and 256 GB RAM. For each instruction, we randomly select three LLMs to respond, yielding three responses A, B, C. For pairwise comparisons, we create three response pairs (A, B), (A, C), (B, C), and another three pairs (B, A), (C, A), (C, B) in order to check if LLM-as-a-judge methods yield consistent judgment after reversing the order within a response pair. Thus, we obtain 150 responses and 300 response pairs per task, resulting in 450 responses and 900 pairs in total.\nAs part of the prompt, contextual information in Table 2 is provided for LLMs. The full prompts are available in our repository [46]. LLMs are permitted to generate at most 3072 tokens, two times the maximum reference answer length, to minimize the need for truncation.\nAfter preliminary experiments, we discover that many reference summaries in CodeXGLUE are in fact incorrect. Therefore, we require the reference summary to at least have 15 tokens, reselect the instructions, and manually examine each instruction. We also find that in code generation, selected LLMs struggle to generate interpretable code because they cannot use dependencies effectively, as the only available dependency information in ComplexCodeEval is their names, which makes human evaluation almost impossible to yield meaningful scores. Consequently, we reselect the instructions with at most 5 dependencies to reduce difficulty, and augment the dependency information with GPT-408, prompting it to extract the signature from the reference answer10 and generate a short description for each dependency. We manually examine the descriptions to ensure that no other information about the reference answers is included. Responses are generated with the updated dependency information as well as other contextual information."}, {"title": "4.1.3 Manual Evaluation.", "content": "For manual evaluation, we design two evaluation aspects per task to guide human evaluators, enabling more fine-grained assessment without overwhelming evaluators with too many aspects and complicated criteria. The first aspect assesses the response's alignment with the instruction, e.g. Consistency with Code for summarization, requiring the summary to capture the code's core functionality. The second aspect judges the response's intrinsic quality, e.g. Readability & Idiomatic Usage for translation, demanding the responded code to be both readable and follow common coding styles in the target language. We also curate the criteria for each integer score ranging from 1 to 5 for both aspects. In general, a 5-point response is near perfect, a 4- or 3-point response contains minor or major issues but still makes sense, and a 2- or 1-point response is practically useless. Below, we show an example of Readability aspect and its corresponding criteria for code summarization as an example, while the remaining aspects and criteria can be found in our repository:\nTwo human evaluators with expertise in the chosen programming languages are involved in judging each of the 450 responses. During manual evaluation, we provide the corresponding instruction and the reference answer along with the response to be evaluated. Each evaluator is required to score both aspects before assigning an overall score, which is not necessarily the average of the former. The final human score for each response is the average of overall scores from two evaluators\u00b9\u00b9. For pairwise comparison, we calculate the absolute difference between the final human scores of two responses in a pair, declaring a tie when the difference is smaller than 0.5\u00b9\u00b2, or deciding the higher-scored response is better otherwise."}, {"title": "4.2 Selected Methods", "content": "We choose five popular conventional metrics, each requiring the response tgt and the reference answer ref but not the instruction. We verify if these metrics align better or worse with human evaluation compared to LLM-as-a-judge methods. For details about Recall, Precision, and Fn scores, please refer to their original papers."}, {"title": "4.2.1 Conventional Metrics.", "content": "BLEU [34] calculates modified n-gram precision (n = 1, 2, 3, 4) for tgt and ref, and applies a brevity penalty to penalize overly short responses.\nROUGE-L [29] measures the length of the longest common subsequence LCS between tgt and ref. It computes the F\u2081 score based on LCS.\nMETEOR [2] matches tokens in tgt and ref, and computes the F3 score based on the number of matched tokens. It also penalizes fragmented alignment by counting the number of contiguous match chunks in tgt."}, {"title": "4.2.2 Embedding-based Methods.", "content": "We choose two methods based on embedding, i.e. token representations of the response tgt and the reference answer ref. We use UniXcoder [14] in place of BERT or other non-code LLMs as our encoder, due to its ability to process both code and text.\nBERTScore [58] calculates pairwise token similarity between tgt and ref with token representations, and obtains the average Recall and Precision, which are combined into the F\u2081 score as the final score. BERTScore also applies inverse document frequencies (IDFs) as token weights.\nMoverScore [59] proposes to use Word Mover's Distance [23], measuring semantic dissimilarity as the minimum cost flow between n-gram representations, which is the IDF-weighted average of token representations. For each n, it constructs a cost matrix for each n-gram in tgt, and flow requirements based on IDF. The final score is the minimum cost to establish such a flow."}, {"title": "4.2.3 Probability-based Methods.", "content": "We select two probability-based methods. These methods may take at least two of the following as input: instruction src, response tgt, and reference ref, plus supplementary information like evaluation aspects13. We use davinci-002 here, since later OpenAI models only return probabilities of newly generated tokens instead of provided tokens.\nGPTScore [11] simply uses the sequence log probability log p(tgt|src, a) as the score according to their paper, which is the average of all token log probabilities. However, their code instead uses the harmonic mean of logp(tgt|ref, a) and log p(ref|tgt, a). To mitigate this difference, we additionally include src in both conditions, i.e. using log p(tgt|ref, src, a) and log p(ref|tgt, src, a).\nFFLM [20] is a reference-free metric that obtains both the prior probability P(tgt) and the posterior probability P(tgt|src). It claims that high-loss (low-probability) tokens contribute more to low-quality content, thus assigning a higher weight to them. FFLM also introduces the prefix prob-ability P(tgt|tgt : src) by prepending tgt to src, assuming that the prefix increases the generating probability if tgt is inconsistent with src. These three probabilities are fused into the final score."}, {"title": "4.2.4 Output-based Methods.", "content": "We select two methods: G-Eval and BatchEval, which apply different inference strategies, in addition to a control group (Vanilla) with no strategies applied, to assess if these strategies improve alignment with human evaluation for general-purpose LLMs. Unless otherwise stated, we use GPT-40 for these methods.\nWe also include a supervised fine-tuning (SFT) group, with two LLMs fine-tuned for NLP evaluation, along with their base LLMs without fine-tuning, to determine if fine-tuning for NLP evaluation also enhances human alignment in SE evaluation.\nWe provide only the instruction src, response tgt, and evaluation aspects14 in the prompt. For the detailed prompts, please refer to our repository. Note that these methods can also perform pairwise comparison, where we include both responses in the prompt."}, {"title": "4.3 Meta-Evaluation", "content": "Meta-evaluation refers to the process of evaluating different evaluation metrics. For the default method of individual scoring, we meta-evaluate the metrics via their correlation with human scores, including Spearman's p, Pearson correlation coefficient R, and Kendall's \u03c4. For pairwise comparison in RQ3, we compute the Accuracy of LLM-generated labels, in addition to the Agreement which checks if an LLM makes the same judgment when two responses in the prompt swap their positions. For the ease of reading, all correlation coefficients, Accuracies, and Agreements in this paper are multiplied by 100. We also check if the p-value of each correlation coefficient in RQ1 is smaller than 0.05 to ensure a 95% confidence interval."}, {"title": "5 STUDY RESULTS", "content": "In this section, we present experimental results and our analysis to answer the research questions."}, {"title": "5.1 RQ1: Alignment with Human Scores", "content": "We use LLM-as-a-judge methods to score individual responses and evaluate their correlation with human scores. We notice that the three types of correlation coefficients display similar trends, and make the following discoveries:\nCurrent LLM-as-a-judge methods lack generalizability, as they demonstrate drastically different performance in different tasks and scenarios. In Code Translation, BatchEval reaches the highest human alignment, offering near-human performance at p = 73.67, R = 81.32, and \u03c4 = 59.80, while G-Eval, DeepSeek-V2.5, and GPT-40 also reach a high correlation of R > 70 or p > 60, greatly outperforming conventional metrics capped at R = 34.23, p = 31.30. We attribute this to the characteristic of responses and reference answers: LLMs often copy statements from the original code with subtle language-specific modifications as the response. Meanwhile, although the reference answer maintain unchanged core functionality, its exact implementation and behavior might noticeably differ. This presents a disadvantage for reference-based methods including most non-output-based methods and conventional metrics. Output-based methods, however, are designed to work without reference and can utilize LLMs' knowledge of programming languages in evaluation.\nOn the contrary, LLM-as-a-judge methods struggle to outperform conventional metrics in evalu-ating code generation outputs and are completely surpassed in evaluating code summarization. For code generation, conventional metrics can reach a mid-high correlation of p = 67.11, R = 65.55, and \u03c4 = 49.66, while DeepSeek-V2.5 is the only LLM outperforming them at p = 66.39, R = 68.51, and \u03c4 = 54.74 without any additional inference strategies. This can be attributed to the characteristics of the ComplexCodeEval dataset, which emphasizes the usage of complicated dependencies by filling out the correct arguments and calling them at the right time instead of designing sophisticated algorithms. Therefore, a response-reference comparison at the lexical-level can offer an insight of the response's quality, while the LLMs' limited understanding of the dependencies fail to provide benefits in evaluation. With that said, for code generation, LLM-as-a-judge methods with large LLMs like GPT-40 are still applicable, since they display similar performance as conventional metrics but provide the benefits of not requiring reference answers. For code summarization, LLM-as-a-judge techniques are completely defeated by conventional metrics, hardly reaching a score of 30 in any correlation coefficient or even demonstrating a negative correlation with human evaluation. Nonetheless, conventional metrics also fail to deliver satisfying alignment with human evaluation, with p, R < 50 and \u03c4 < 40. This is potentially due to the fact that many LLMs try to explain the code step-by-step instead of summarizing the core functionality, which is difficult for these LLM-as-a-judge methods to detect. While conventional metrics can assign low scores to these responses, they have trouble handling paraphrasing, which is common in summaries. It is an interesting future direction to explore new metrics that align with humans for code summarization."}, {"title": "Inference using large LLMs yields the best human alignment across all tasks, while infer-ence strategies only provide marginal improvement.", "content": "Embedding-based and probability-based methods underperform output-based methods in most scenarios, capped at R = 34.77, 47.35, 29.44 versus the top performance of the latter at R = 81.32, 68.51, 26.19, and the top performance of conventional metrics at R = 34.23, 65.55, 47.01 in code translation, code generation, and code summarization respectively. Furthermore, embedding-based and probability-based methods require access to internal states, while the API services of many state-of-the-art LLMs only allow access to the final output. Therefore, these methods cannot be applied with such LLMs, limiting their applicability. Based on the low human alignment and limited applicable LLMs, we conclude that embedding-based and probability-based methods are impractical for evaluating SE tasks.\nAmong the output-based methods, we find that DeepSeek-V2.5 and GPT-40 outperform other LLMs without further training. Although Auto-J and Prometheus 2, trained to match human preference, provide better performance than their base model, with a 5.18% to 16.03% increase in Pearson's R, achieving R = 38.92 and R = 40.33 in evaluating code generation respectively, the overall performance is still inferior. This is likely due to the limited number of parameters, as Auto-J and Prometheus 2 only have 13B and 47B parameters. Another possible reason is the misalignment between evaluating NLP tasks during training, and evaluating SE tasks during inference. Though many NLP training datasets contain programming tasks, they may only present common tasks like code generation and fail to present sufficiently challenging instructions. Unfortunately, to the best of our knowledge, no multi-task human preference training sets for SE task evaluation have been curated so far. Hence, we are unable to investigate LLMs fine-tuned on such SE-specific datasets.\nSimilarly, current inference strategies, when employed to GPT-40, produce an inadequate perfor-mance boost of AR = 2.21, 6.03, 3.04 at maximum. Despite recent work claiming the effectiveness of scaling inference [42], we found that existing inference strategies for SE evaluation only bring marginal improvement in human alignment. Moreover, they have different downsides: G-Eval forces LLMs to generate the overall score, restricting the efficacy of the Chain-of-Thought procedure, while greatly increasing inference cost if the full explanations are needed; BatchEval increases the token count, leading to more expensive inference due to multi-round evaluation. Therefore, greedy decoding remains a viable LLM-as-a-judge solution with satisfactory performance and lower requirements of token count, when equipped with colossal state-of-the-art LLMs."}, {"title": "5.2 RQ2: Score Characteristics", "content": "We investigate the score characteristics of various LLM-as-a-judge methods. We make the following discoveries:\nMost non-SFT LLM-as-a-judge methods have low correlations with those from other categories and high correlations with those from the same category. In Table 4, we observe that pother < 50 for all categories, meaning that each category demonstrates a unique distribution of scores instead of resembling others. Conversely, pinner > 60 under most non-SFT circumstances, exhibiting a medium to high level of agreement among similar methods. This phenomenon suggests that the mechanics governing each category may significantly influence their score distributions. In contrast, scores from SFT methods correlate poorly even within the same category, likely due to variations in their base LLMs and fine-tuning datasets. Given the high level of disagreement among current fine-tuned LLMs, we argue that selecting an appropriate fine-tuned LLM is crucial for evaluating under specific SE contexts. Otherwise, it may produce entirely unexpected scores."}, {"title": "Output-based methods using large LLMs tend to align well with each other, whereas those using smaller LLMs exhibit low correlations with other methods.", "content": "Since output-based methods offer the best human alignment, we further investigate whether LLM size influences correlations between methods by grouping these LLM-as-a-judge methods into those using large LLMs (>230B) and those using small LLMs (<50B). These methods use DeepSeek-V2.5 and GPT-40, and maintain strong alignment despite the difference in LLMs and inference strategies. In contrast, methods using small LLMs yield p < 50 when compared to methods in the \"large\" group, and p < 30 within the \"small\" group. This pattern reflects a performance gap, as the \"large\" group align substantially better with human evaluations than the \"small\" group."}, {"title": "Only embedding-based methods resemble conventional metrics.", "content": "We discover in that pconv = 81.45, 79.78, 57.07 for the 3 tasks with embedding-based methods, indicating a high correlation with conventional metrics. As shown in Fig. 2, MoverScore from this category exhibits a distribution similar to that of ChrF++, one of the most human-aligning conventional metrics, as both tend to assign low to medium scores to responses. This similarity is anticipated, given that both metrics are designed to assess the similarity between the response and the reference. While MoverScore leverages contextual token representations beyond simple lexical matching, the underlying principles remain fundamentally aligned. On the other hand, distributions from other categories differ markedly from those of ChrF++ as shown by their low pconv values, further underscoring their limited resemblance to conventional metrics."}, {"title": "The best human-aligning methods closely replicate the distribution of human scores.", "content": ""}]}