{"title": "A Novel Access Control and Privacy-Enhancing Approach for Models in Edge Computing", "authors": ["Peihao Li"], "abstract": "With the widespread adoption of edge computing technologies and the increasing prevalence of deep learning models in these environments, the security risks and privacy threats to models and data have grown more acute. Attackers can exploit various techniques to illegally obtain models or misuse data, leading to serious issues such as intellectual property infringement and privacy breaches. Existing model access control technologies primarily rely on traditional encryption and authentication methods; however, these approaches exhibit significant limitations in terms of flexibility and adaptability in dynamic environments. Although there have been advancements in model watermarking techniques for marking model ownership, they remain limited in their ability to proactively protect intellectual property and prevent unauthorized access. To address these challenges, we propose a novel model access control method tailored for edge computing environments. This method leverages image style as a licensing mechanism, embedding style recognition into the model's operational framework to enable intrinsic access control. Consequently, models deployed on edge platforms are designed to correctly infer only on license data with specific style, rendering them ineffective on any other data. By restricting the input data to the edge model, this approach not only prevents attackers from gaining unauthorized access to the model but also enhances the privacy of data on terminal devices. We conducted extensive experiments on benchmark datasets, including MNIST, CIFAR-10, and FACESCRUB, and the results demonstrate that our method effectively prevents unauthorized access to the model while maintaining accuracy. Additionally, the model shows strong resistance against attacks such as forged licenses and fine-tuning. These results underscore the method's usability, security, and robustness.", "sections": [{"title": "1 Introduction", "content": "With the rapid development and widespread adoption of edge computing, it has demonstrated significant advantages in distributed computing, real-time data processing, and smart device interconnectivity. However, the security and privacy protection of data and models in edge computing environments have become increasingly prominent issues. The distributed nature of deep learning models and data in edge computing scenarios makes them more vulnerable to exploitation by attackers, potentially leading to model theft [1] and data breaches [2]. Relevant study[3] have shown that over 80% of machine learning models deployed in edge applications can be extracted by attackers either directly or through straightforward dynamic analysis techniques, posing severe threats to user privacy and potentially resulting in significant intellectual property losses. These threats not only compromise the security of individual edge devices but may also extend across the entire edge computing network, thereby affecting the overall system's reliability and trustworthiness.\nIn the context of edge computing, addressing the security of models and data predominantly relies on traditional encryption and authentication methods, such as Public Key Infrastructure (PKI)-based authentication[4, 5] and symmetric encryption for transmission protection [6, 7]. However, these approaches exhibit significant limitations when confronted with the dynamic and heterogeneous nature of edge computing environments. Trabelsi et al. [8] highlight in their research that conventional access control methods struggle with the frequent connectivity changes of edge devices and the adaptability required in heterogeneous network environments. Moreover, model watermarking [9, 10], while effective in marking ownership, is a passive protection technique that fails to prevent unauthorized access and is inadequate for actively safeguarding intellectual property. On the other hand, existing privacy protection techniques face substantial challenges in edge computing scenarios. Traditional methods, such as homomorphic encryption [11, 12] and differential privacy [13, 14], are difficult to deploy in edge environments due to limited computational resources and the high demand for real-time processing. Therefore, there is an urgent need to develop innovative solutions to enhance the security and privacy protection of models and data in edge computing environments.\nTo address these challenges, we propose a novel intrinsic access control scheme tailored for edge computing, utilizing image style as a licensing mechanism. This approach innovatively integrates image style recognition into the model's internal operational framework, enabling the model to only perform valid inferences on inputs that match specific styles, while invalidating forged styles and arbitrary images. By restricting the model's input data, our method achieves proactive protection of intellectual property, and since the original data must undergo style transfer during use, it also enhances data privacy to a certain extent. Compared to traditional methods, our approach offers greater flexibility and adaptability, making it better suited to address the complex threats in edge computing environments. Extensive experiments conducted on benchmark datasets such as MNIST, CIFAR-10, and FaceScrub demonstrate that our scheme excels in usability, security, and robustness, showcasing its broad application prospects in edge computing scenarios.\nThe main contributions of this paper are summarized as follows:\n- We introduce, for the first time, a novel model access control method that leverages style as licensing mechanism, and we implement a lightweight"}, {"title": "2 Proposed Method", "content": "In this section, we provide a detailed description of the novel access control method proposed in this paper. As illustrated in Fig.1, we use an image classification task to exemplify the specific workflow of this approach. First, we train a lightweight style license generation network, which is designed to convert original data collected by terminal devices into stylized license data with a specific style. Next, the license data is transmitted to the edge platform for inference by the model, which has been trained using a combination of style data, original data, and license data based on our proposed style license loss function. This model is characterized by its ability to perform effective inference only on data that matches the authorized style.\nThrough this method, we achieve two key objectives. First, we embed the access control mechanism directly within the deep learning model deployed on the edge platform. This integration ensures that even if an attacker gains access to the model's parameters and architecture, they would still be unable to use the model without the correct style license, thereby proactively protecting the intellectual property of the model from unauthorized use. Second, in edge computing scenarios, terminal devices are often vulnerable to threats such as man-in-the-middle attacks [15], side-channel attacks[16], and network traffic monitoring[17]. Our approach mitigates these risks by allowing the original data to be converted"}, {"title": "2.2 Lightweight Style Transfer", "content": "Given the limited computational resources of edge terminal devices, it is essential to train a lightweight style transfer model to serve as the style license generator for deployment on these devices. After experimentally comparing existing style transfer methods, we selected the Fast Neural Style approach [18] for its lightweight nature and real-time processing capabilities. This method leverages a pre-trained convolutional neural network (CNN) to directly transform input images into a specific style. During training, the generator model is optimized using content and style features extracted from a VGG network. Since the generator model is applied directly in the forward pass of the input image, once training is complete, the model can apply the desired style to any given input image within milliseconds, enabling real-time style transfer."}, {"title": "2.3 Style License Loss", "content": "To ensure that the model is effective only on style license data while remaining ineffective on any other data, we designed a specialized training scheme and loss function for the style license model. As illustrated in Fig.2, the training process for the style license model involves three datasets:\nStyle Dataset: Images with the licensed style.\nOriginal Dataset: Original, non-stylized images.\nStyled-Original Dataset: Images obtained by converting the original images into the licensed style.\nTo achieve our desired objective, we developed a custom loss function specifically designed for the convergence of the style license model during training, formulated as follows:\n\n$L= \\alpha L_{Cross-Entropy} + \\beta L_{Contrastive} + \\gamma L_{Style}$   (1)\n\nThe loss function is composed of a weighted combination of cross-entropy loss, contrastive loss, and style loss [19], each serving a distinct purpose in training the style license model. The cross-entropy loss can be represented as follows:\n\n$L_{Cross-Entropy} = -y_i log (f(x^{gen}_i))$   (2)\n\nwhere $x^{gen}$ denotes a set of styled-original data, also known as style license data, and $y_i$ represents the corresponding label. This part of the loss function primarily ensures that the model correctly classifies images with the licensed style.\nThe second part, the contrastive loss, can be expressed as:\n\n$L_{Contrastive} = max \\{margin - d\\{ f(x^{origin}_i), f(x^{gen}_i)\\}, 0 \\}$    (3)\n\nwhere $x^{origin}$ represents any set of original data, $x^{gen}$ represents the corresponding styled-original data, and $f(x)$ is the feature output of x in the model. The parameter $margin$ is used to set the minimum distance between the feature outputs of $x^{origin}$ and $x^{gen}$ in the model. The distance metric d(x, y) is a combination of Euclidean distance and cosine similarity, represented as:\n\n$d(x, y) = \\sum_{i=1}^{n} (x_i - y_i)^2 + \\varphi \\cdot \\left(1 - \\frac{\\sum_{i=1}^{n} x_i y_i}{\\|x\\| \\|y\\|} \\right)$   (4\n\nwhere n is the dimension of the vectors x and y, $\\sum_{i=1}^{n} (x_i - y_i)^2$ represents the Euclidean distance, $x \\cdot y$ denotes the dot product of the vectors x and y, and $\\|x\\|$ and $\\|y\\|$ are their L2 norms, respectively. The parameter $\\varphi$ balances the Euclidean distance and cosine similarity within the distance metric. The contrastive loss $L_{Contrastive}$ is crucial for calculating the feature distance between the original and style license images, ensuring that the model can distinguish between stylized and non-stylized images.\nThe third part, the style loss, is represented as:\n\n$L_{Style} = \\frac{1}{L} \\sum_{l=1}^{L} \\|G_{\\phi}(x^{style}) - G_{\\phi}(x^{gen})\\|_F^2$   (5)"}, {"title": "3 Evaluation", "content": "In this section, we comprehensively evaluate the proposed style license model. First, we validate the usability of our approach by comparing the accuracy of the style license model with that of traditional models. Then, we assess the privacy protection offered by stylized images by testing them against pretrained traditional image classification models. All experimental results in this paper are derived based on the MindSpore framework."}, {"title": "3.1 Experimental Setup", "content": "Our experiments were conducted using the MindSpore framework. We trained baseline models on the MNIST, CIFAR-10, and FaceScrub datasets using VGG16, ResNet18, and MobileNetV2 to evaluate the performance of the style license model. The MNIST dataset contains grayscale images of handwritten digits ranging from 0 to 9, each with a resolution of 28x28 pixels. CIFAR-10 consists of 60,000 color images at a resolution of 32x32 pixels, categorized into 10 classes representing various objects or scenes. Additionally, the FaceScrub dataset comprises 107,818 facial images of 530 celebrities, with all images resized to 3x224x224 pixels for our experiments. VGG16, characterized by its depth and simplicity, contains 16 convolutional layers and 3 fully connected layers, achieving depth through the stacking of 3x3 kernels. ResNet18, which features residual connections, includes 16 convolutional layers and 2 fully connected layers, allowing for the training of deeper networks. MobileNetV2 employs depth-wise separable convolutions combined with linear bottleneck layers to enhance computational efficiency and accuracy, making it well-suited for mobile and embedded systems. In our experiments, we first trained a specific style transfer model using the Fast Neural Style method to serve as the style license generator. This generator was then used to transform the data from the aforementioned benchmark datasets into the corresponding styled datasets, resulting in the creation of the Styled-Original Dataset."}, {"title": "3.2 Usability of Style License Model", "content": "We evaluated the usability of our proposed approach by assessing the accuracy of the style license model. First, we trained traditional classification models on"}, {"title": "3.3 Privacy Assessment of Stylized Image", "content": "To evaluate the effectiveness of our proposed method in protecting data privacy, we compared the accuracy of stylized images versus original images in traditional classification tasks. We trained baseline models using VGG16, ResNet18, and MobileNet V2 on the MNIST, CIFAR-10, and FaceScrub benchmark datasets. The degree of accuracy degradation for stylized images compared to original images in these baseline models was used to assess the extent of privacy protection achieved by the stylization process. The experimental results are shown in Fig.4.\nAs illustrated in Fig.4, the accuracy of stylized images dropped significantly across all models trained on different datasets compared to the baseline accuracy. Notably, the accuracy for models trained on the CIFAR-10 dataset fell below 40% for most cases. Although models trained on the MNIST dataset maintained over 50% accuracy on stylized data, there was still nearly a 50% decrease compared to the baseline accuracy. This result demonstrates that transmitting stylized images can provide substantial privacy protection for the original data."}, {"title": "4 Conclusion", "content": "This paper proposes an intrinsic access control method for deep learning models in edge computing environments, based on data style. The approach customizes a license generator for the model and designs a unique training loss function for the style license model, ensuring that the fully trained model can only effectively infer specific styled license data while being ineffective for any other data. By deploying the license generator on the terminal device and the model on the edge platform, the method proactively prevents unauthorized access to the model's intellectual property and enhances data privacy protection on terminal devices. The results demonstrate that the method maintains high accuracy for license data while significantly reducing the model's accuracy on original data, effectively preventing unauthorized access."}]}