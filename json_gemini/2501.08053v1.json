{"title": "Exploring Narrative Clustering in Large Language Models: A Layerwise Analysis of BERT", "authors": ["Awritrojit Banerjee", "Achim Schilling", "Patrick Krauss"], "abstract": "This study investigates the internal mechanisms of BERT, a transformer-based large language model, with a focus on its ability to cluster narrative content and authorial style across its layers. Using a dataset of narratives developed via GPT-4, featuring diverse semantic content and stylistic variations, we analyze BERT's layerwise activations to uncover patterns of localized neural processing. Through dimensionality reduction techniques such as Principal Component Analysis (PCA) and Multidimensional Scaling (MDS), we reveal that BERT exhibits strong clustering based on narrative content in its later layers, with progressively compact and distinct clusters. While strong stylistic clustering might occur when narratives are rephrased into different text types (e.g., fables, sci-fi, kids' stories), minimal clustering is observed for authorial style specific to individ-ual writers. These findings highlight BERT's prioritization of semantic content over stylistic features, offering insights into its representational capabilities and processing hierarchy. This study contributes to understanding how transformer models like BERT encode linguistic information, paving the way for future interdisciplinary research in artificial intelligence and cognitive neuroscience.", "sections": [{"title": "I. INTRODUCTION", "content": "Artificial neural networks (ANNs), especially deep learning models, have achieved significant success in tasks involving sequential data, notably in natural language processing (NLP) and speech recognition [1]. However, the extent to which these models replicate the localized processing observed in biological neural networks remains a subject of debate [2].\nThe transformer architecture, introduced by Vaswani et al. [3], has emerged as a promising model for bridging this gap. Transformers, particularly large language models (LLMs), are capable of performing complex cognitive tasks while offering the advantage of complete accessibility to their internal states and parameters, a feature unattainable in biological systems. This has led to their adoption as model systems in Cognitive Computational Neuroscience (CCN) [4], a field dedicated to understanding cognitive processes by integrating artificial and biological neural systems [5]\u2013[9]. Through progressive refine-ments, these models can be made increasingly biologically plausible [10]-[14], providing insights into both the human brain and the principles underlying artificial intelligence [15].\nAmong transformers, the Bidirectional Encoder Representa-tions from Transformers (BERT) [16], an encoder-only archi-tecture, is particularly notable for its ability to process input sequences holistically. Its self-attention mechanism dynami-cally weights input tokens based on contextual importance, resembling the brain's ability to allocate resources to salient sensory inputs. This makes BERT a compelling framework for exploring the hypothesis that localized processing of sequential data can emerge in artificial neural networks.\nExplainable AI (XAI) methods [17] provide the tools to interpret and visualize the internal workings of such models. Techniques like Principal Component Analysis (PCA) and Multidimensional Scaling (MDS) enable dimensionality re-duction, facilitating the identification of patterns and clusters in high-dimensional activation spaces. By applying these meth-ods to BERT, we aim to investigate how narrative content and stylistic features manifest within its internal representations.\nWe constructed a dataset of texts featuring diverse authorial styles and narrative themes, using GPT-4 to ensure variability and consistency. By analyzing BERT's layerwise activations and projecting them into two-dimensional spaces using PCA and MDS, we reveal distinct clustering patterns based on narrative content, with progressively compact clusters in later layers. In contrast, clustering by authorial style is minimal, highlighting BERT's prioritization of semantic content over stylistic features.\nThese findings support the hypothesis that encoder-only transformers like BERT exhibit localized processing of sequential data, aligning with principles of hierarchical abstraction observed in the brain. The study underscores the potential of transformers as tools for cognitive modeling and highlights their implications for both neuroscience and artificial intelligence. In the following sections, we detail our methodology, present key results, and discuss the broader implications of our findings."}, {"title": "II. METHODOLOGY", "content": "In this section, we detail the construction of our dataset, the application of a pre-trained language model to encode the data, and the techniques used to reduce dimensionality for visualization. Our objective is to generate a corpus that integrates both authorial style and narrative content, and then examine how these factors manifest within BERT's internal activation patterns.", "A. Dataset Construction": "1) Selection of Authors and Texts: We began by selecting a set of authors and texts that collectively cover a wide spectrum of English literary styles and thematic domains. The chosen authors are, in order: William Shakespeare, Charles Dickens, Charlotte Bront\u00eb, Sir Arthur Conan Doyle, Edgar Allan Poe, George R. R. Martin, Hector Hugh Munro (Saki), William Sydney Porter (O. Henry), Dan Brown, and Jerome K. Jerome. Each author is represented by one text:\n1) Twelfth Night, or What You Will (Shakespeare; a play that narrates a story)\n2) A Tale of Two Cities (Dickens; realistic themes)\n3) Jane Eyre (Bront\u00eb; passive and emotional)\n4) The Hound of the Baskervilles (Doyle; thriller, realistic yet larger than life, analytical)\n5) The Tell-Tale Heart (Poe; psychological, gritty)\n6) A Song of Ice and Fire: A Dance with Dragons (Martin; fantasy with mature themes)\n7) The Wolves of Cernogratz (Saki; seemingly straightfor-ward yet enigmatic)\n8) The Last Leaf (O. Henry; positively emotional)\n9) Angels and Demons (Brown; analytical)\n10) Three Men in a Boat (Jerome; humorous)\nThis carefully curated selection ensures a balanced represen-tation of major literary modes, encompassing diverse narrative styles, thematic complexities, emotional tones, and authorial voices.\n2) Neural Style Transfer: From each of the 10 texts, we extracted the first few lines, yielding 10 base narratives. To introduce stylistic variability, we leveraged ChatGPT-4 to perform \u201cneural style transfer\u201d in the literary domain. Before initiating this process, ChatGPT-4 was instructed to act as an expert in Comparative Literature, specializing in pre-Victorian, Victorian, and modern English Literature, with an in-depth understanding of the selected authors.\nFor each of the 10 base narratives, we prompted ChatGPT-4 to rewrite it in the style of each of the other 9 authors. This procedure generated 10 differently stylized versions of each narrative (the original plus 9 new stylizations), resulting in 100 narratives in total. To ensure a suitable ratio of data points to embedding dimensions for subsequent analysis, we repeated this style-transfer step 10 times, ultimately producing a dataset of 1000 narratives. This expanded dataset is essential for enabling our chosen projection methods to operate on the column space (the activation space) rather than the row space."}, {"title": "B. BERT: Representation Extraction", "content": "1) BERT as an Encoder-Only Transformer: We employed the BERT-base-uncased model, a pre-trained encoder-only transformer architecture. BERT's strength lies in its ability to capture contextual relationships between tokens in a bidi-rectional manner, without relying on recurrence or convo-lution. Its self-attention mechanism enables each token to attend directly to others, facilitating nuanced representations of semantic and syntactic features. By examining the internal activations of BERT, we aim to determine if and how it encodes the dual semantics of style and content present in our dataset.\n2) Tokenization and Embedding Retrieval: Each of the 1000 narratives was tokenized using the BERT-base-uncased tokenizer, converting the text into a sequence of WordPiece tokens. We then passed each tokenized narrative through BERT, extracting the [CLS] token embedding from every layer. As BERT-base-uncased produces a 768-dimensional embedding per layer, and we consider all 13 layers (including the embedding layer), we obtained a data structure of shape (13, 1000, 768). This encapsulates the layered representation of each narrative's [CLS] token, capturing a progression of representations that BERT forms as information flows through its layers.\nOur dataset now encodes two distinct kinds of semantics: narrative content (inherited from the original excerpts) and au-thorial style (imposed by the neural style transfer). As per our central hypothesis, we are expecting to see some localization of neural activations in BERT's internal representation space, but which of the semantics will influence the localization will be interesting to examine."}, {"title": "C. Dimensionality Reduction and Projection", "content": "To visualize and analyze the resulting representations, we employed two complementary dimensionality reduction meth-ods: Principal Component Analysis (PCA) and Multidimen-sional Scaling (MDS).\n1) Principal Component Analysis (PCA): PCA identi-fies orthogonal directions in the data that capture maximal variance, projecting high-dimensional points into a lower-dimensional space. This approach reveals dominant axes of variation that may correspond to salient semantic distinctions. By ensuring that the number of narratives (1000) exceeds the embedding dimension (768), PCA can operate from the column space\u2014our desired \u201cactivation space\"\u2014rather than being constrained by row-wise limitations.\n2) Multidimensional Scaling (MDS): This technique was used to reduce the dimensionality of the hidden layer acti-vations, preserving the pairwise distances between points as much as possible in the lower-dimensional space. In partic-ular, MDS is an efficient embedding technique to visualize high-dimensional point clouds by projecting them onto a 2-dimensional plane. Furthermore, MDS has the decisive ad-vantage that it is parameter-free and all mutual distances of the points are preserved, thereby conserving both the global and local structure of the underlying data [18]\u2013[24]."}, {"title": "D. Degree of Clustering", "content": "To quantify the degree of clustering, we used the GDV as published and explained in detail in [26]. The GDV provides an objective measure of how well the hidden layer activations cluster according to the ASC types, offering insights into the model's internal representations. Briefly, we consider N points $X_{n=1..N} = (X_{n,1},\u2026, X_{n,D})$, distributed within D-dimensional space. A label $I_n$ assigns each point to one of L distinct classes $C_{l=1..L}$. In order to become invariant against scaling and translation, each dimension is separately z-scored and, for later convenience, multiplied with:\n$s_{n,d} = \\frac{1}{\\sigma_d}(x_{n,d} - \\mu_d)$\nHere, $\\mu_d = \\frac{1}{N} \\sum_{n=1}^{N} x_{n,d}$ denotes the mean,\nand $\\sigma_d = \\sqrt{\\frac{1}{N} \\sum_{n=1}^{N}(x_{n,d} \u2013 \\mu_d)^2}$ the standard deviation of dimension d.\nBased on the re-scaled data points $s_n = (s_{n,1},\u2026\u2026\u2026, s_{n,D})$, we calculate the mean intra-class distances for each class $C_l$\n$d(C_l) = \\frac{2}{N_l(N_l-1)} \\sum_{i=1}^{N_l-1} \\sum_{j=i+1}^{N_l} d(s^{(l)}_i, s^{(l)}_j),$\nand the mean inter-class distances for each pair of classes $C_l$ and $C_m$\n$d(C_l, C_m) = \\frac{1}{N_l N_m} \\sum_{i=1}^{N_l} \\sum_{j=1}^{N_m} d(s^{(l)}_i, s^{(m)}_j)$.\nHere, $N_k$ is the number of points in class k, and $s^{(k)}_i$ is the $i^{th}$ point of class k. The quantity d(a, b) is the euclidean distance between a and b. Finally, the Generalized Discrimination"}, {"title": "Value (GDV)", "content": "is calculated from the mean intra class and inter-class distances as follows:\n$GDV = \\frac{1}{\\sqrt{D}} [\\frac{1}{{L \\choose 2}} \\sum_{l < m} \\tilde{d}(C_l, C_m) - \\sum_{l=1}^{L} \\tilde{d}(C_l)]$\nwhereas the factor $\\frac{1}{\\sqrt{D}}$ is introduced for dimensionality invari-ance of the GDV with D as the number of dimensions.\nNote that the GDV is invariant with respect to a global scaling or shifting of the data (due to the z-scoring), and also invariant with respect to a permutation of the components in the N-dimensional data vectors (because the euclidean distance measure has this symmetry). The GDV is zero for completely overlapping, non-separated clusters, and it becomes more negative as the separation increases. A GDV of -1 signifies already a very strong separation and perfect clustering."}, {"title": "E. Summary", "content": "Our methodology involves creating a dataset of 1000 nar-ratives that combine varying authorial styles and content, encoding these texts through BERT's layered representations, and projecting them into two-dimensional spaces for inter-pretability. By applying PCA, MDS, and GDV to these em- beddings, we gain insight into how and where within BERT'S representation space different semantics manifest, offering a window into localized processing in deep language models."}, {"title": "III. RESULTS", "content": "From Fig.1 and Fig.2, it is clear that no significant clusters emerge when the data is clustered based on authorial style. This is further supported by Fig.5, where layerwise GDV trends for authorial style remain close to zero or slightly positive, indicating minimal clustering.\nIn contrast, when clustering is based on narrative content, Fig.3 and Fig.4 demonstrate clear and compact clusters, with almost no outliers. As shown in Fig.5, the layerwise GDV"}, {"title": "IV. CONCLUSION", "content": "This study explored the hypothesis that localized neural processing, a hallmark of sensory information processing in the human brain, could extend to sequential data and be modeled using artificial neural networks. By analyzing BERT's layerwise activations in response to a dataset containing texts with distinct authorial styles and narrative content, we uncov-ered compelling evidence that supports this hypothesis within the capabilities of transformers.\nOur results reveal that BERT's activations exhibit clear clustering based on narrative content across all layers, with progressively compact and distinct clusters in later layers. This finding indicates that BERT prioritizes content as the principal distinguishing feature in its representations, aligning with the hierarchical abstraction of semantic information. Conversely, no meaningful clustering was observed for authorial styles, as reflected in both the projected and raw activation spaces, demonstrating that stylistic attributes are not prominently encoded by BERT.\nInterestingly, the observed lack of clustering for authorial styles contrasts with the potential for strong clustering when narratives are rephrased as different text types, such as fables,"}]}