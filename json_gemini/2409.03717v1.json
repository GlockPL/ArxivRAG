{"title": "Sample-Efficient Diffusion for Text-To-Speech Synthesis", "authors": ["Justin Lovelace", "Soham Ray", "Kwangyoun Kim", "Kilian Q. Weinberger", "Felix Wu"], "abstract": "This work introduces Sample-Efficient Speech Diffusion (SESD), an algorithm for effective speech synthesis in modest data regimes through latent diffusion. It is based on a novel diffusion architecture, that we call U-Audio Transformer (U-AT), that efficiently scales to long sequences and operates in the latent space of a pre-trained audio autoencoder. Conditioned on character-aware language model representations, SESD achieves impressive results despite training on less than 1k hours of speech - far less than current state-of-the-art systems. In fact, it synthesizes more intelligible speech than the state-of-the-art auto-regressive model, VALL-E, while using less than 2% the training data. Our implementation is available at https://github.com/justinlovelace/SESD.", "sections": [{"title": "1. Introduction", "content": "Neural approaches have revolutionized generative speech modeling, with recent advances driven by auto-regressive and diffusion-based systems [1, 2]. These improvements, however, come with a cost. Generative models are data hungry, and state-of-the-art systems have used increasingly large volumes of annotated data. This poses challenges for the application of these methods to low-resource domains and languages. Learning effective generative models with limited data has so far remained an open challenge.\nTo address this data bottleneck, we develop a latent diffusion model that can exploit abundant unlabeled speech data and therefore requires only a fraction of the labeled data [3]. We utilize a pre-trained autoencoder to map high-dimensional speech waveforms to compact latent representations. By training a diffusion model to generate samples in the lower-dimensional latent space, we offload modeling of fine-grained data characteristics to the unsupervised autoencoder. This allows the diffusion model to focus on the more tractable latent space, thereby improving data efficiency.\nIn speech synthesis, the generated audio must align with the text transcript. This makes diffusion models a proper fit, because they can incorporate complex conditioning information into the generative process. However, with limited training data it is challenging to generalize across diverse transcripts. To address this issue, we condition our model on representations from a pre-trained language model. These representations, learned through self-supervised pre-training, contain the rich linguistic information necessary for natural speech synthesis and help our model generalize effectively to diverse text inputs."}, {"title": "2. Related Work", "content": "Most related are the diffusion TTS models, NaturalSpeech2 (NS2) [5] and VoiceBox [1]. They depend on phonemizers and aligners for frame level phonetic transcripts, which can introduce errors [6]. Both need phoneme duration annotations for generation, necessitating an external model for phoneme duration prediction. Our system, however, can synthesize varied speech with just the utterance duration and transcript. NS2 also requires pitch annotations and a speech prompt, unlike our system which supports text-only generation. Importantly, our method is more data-efficient, requiring far less annotated data than NS2 and VoiceBox by 45.8x and 62.6x, respectively."}, {"title": "3. Background", "content": "Diffusion models [7, 8, 9] are latent variable models with latents $z = {z_t|t \\in [0,1]}$ given by a forward diffusion process $q(z|x)$, which defines a gradual transition from the data distribution, $x \\sim p(x)$, to a Gaussian distribution. The Markovian forward process iteratively adds Gaussian noise to the data over time and satisfies\n$q(z_t|z_s) = N(z_t; a_{t|s}z_s, (1 - a_{t|s})I)$,\n$q(z_t|x) = N(z_t; a_tx, (1 - a_t)I)$"}, {"title": null, "content": "where $a_{ts} = a_t/a_s$ and $0 < s < t < 1$. The noise schedule, determined by $a_t \\in [0,1]$, monotonically decreases the signal-to-noise ratio (SNR), $A_t = \\frac{a_t}{1-a_t}$ as a function of the time, t, such that the final latent becomes approximately Gaussian, $q(z_1) \\approx N(0, I)$. The forward process therefore defines a transition from the data distribution to a Gaussian distribution.\nDiffusion models define a generative process to invert the forward process. This specifies a transition from Gaussian noise, which can be sampled analytically, to the unknown data distribution. Inverting this process can be reduced to learning a denoising network, $x_\\theta (z_t, t, c) \\approx x$, that reconstructs the clean data given some noisy latent, the time, and (optionally) some conditioning information, c, about the data. The conditioning information could be a textual description of an image [10] or, in our case, a textual transcription of some speech.\nIn practice, the denoising network is often parameterized as a noise prediction network [8] or a velocity prediction network [11], where the velocity is given as $v = \\frac{a_t}{\\sqrt{1 - a_t}}x$, to improve training stability and performance [11]. We adopt the v-parameterization throughout this work and therefore train the denoising network with the regression objective\n$L(\\theta) = E_{t,x,c}[w(A_t)||\\hat{v_\\theta}(z_t, t, c) - v||^2]$\nwith some time-dependent weighting, $w(A_t)$, that is set empirically to emphasize noise levels that are important for downstream perceptual quality [8, 12]. This loss function is the weighted variational lower bound of the log likelihood of the data under the forward diffusion process [7, 8, 9]."}, {"title": "4. Sample-Efficient Speech Diffusion", "content": "This continuous latent diffusion approach significantly reduces the effective sequence length compared to modeling tokens - a 10 second clip consists of just 750 latent vectors rather than 24,000 tokens after quantization (a 32x reduction). The continuous latents generated during inference can then be quantized and decoded by EnCodec to produce the waveform.\nU-Audio Transformer (U-AT). For our diffusion network, we propose the U-Audio Transformer (U-AT), a hybrid architecture that combines the strengths of U-Nets and transformers (see Figure 1). U-Nets are well-suited for high-resolution data, while transformers excel at capturing long-range dependencies and incorporating conditioning information. In the U-AT, we first use a 1D U-Net to downsample the lengthy audio features from a maximum length of 1504 frames to 188 frames. This downsampling step allows us to apply a deep transformer backbone to the compressed sequence, incorporating information from the transcript [14]. Processing the full-resolution input with a transformer would be computationally prohibitive.\nTo enhance the transformer's capacity for modeling global information, we incorporate a recent advance from vision transformers [15] and prepend 8 learnable register tokens to the downsampled features. These tokens act as global memory slots, enabling the transformer to better process global information. After applying the transformer, the register tokens are discarded, and the U-Net decoder upsamples only the corresponding audio features back to the original sequence length for the final prediction. Hybrid U-Net/transformer architectures have shown promise for high-resolution image diffusion [16], motivating our adaptation to the audio domain.\nPosition-Aware Cross-Attention. Properly aligning the generated speech with the input transcript is a critical challenge in text-to-speech synthesis. To improve alignment, we introduce position-aware cross-attention layers in the transformer model that attend to transcript representations from a frozen ByT5-base encoder [4]. To explicitly incorporate positional information about the tokens in the transcript, we introduce a neural Position Encoder that maps the relative positions of the transcript tokens to key vectors. We sum these positional key vectors with the corresponding key vectors from the ByT5 embedding in the cross-attention mechanism. This allows the model to directly search for and attend to the relevant positions within the transcript when generating each audio frame.\nSpecifically, we compute the cross-attention logits as:\n$A_{ij} = q_i^T(k_j + f_\\theta(j/m))$\nwhere $q_i \\in R^d$ is the query vector for audio frame i, $k_j \\in R^d$ is the key vector for the ByT5 text embedding j in the sequence of m bytes, and $f_\\theta(j/m) \\in R^d$ is a relative position embedding computed with a lightweight MLP. This positional encoding is critical for generating speech aligned with the transcript.\nDiffusion Loss Weighting. Properly emphasizing the diffusion noise levels that are most important for perceptual quality is critical [17]. Previous work has utilized symmetric weightings like the V-Weighting or unimodal distributions centered around moderate noise levels [17, 1]. However, in text-to-speech synthesis, the input transcript and speech prompt provide valuable signal even at high noise levels where the signal in the corrupted latent itself is limited. These high noise levels are precisely where the conditioning information is most beneficial for resolving the global speech structure and aligning it with the provided transcript."}, {"title": null, "content": "We therefore propose an asymmetric diffusion loss weighting that emphasizes performance at high noise levels where the transcript and prompt are relied upon to estimate the original speech. We visualize our proposed asymmetric weighting, a symmetric weighting baseline, and the V-weighting in Figure 2. Our proposed weighting dedicates more model capacity to resolving aspects like word placement and positioning compared to symmetric weightings. Specifically, we parameterize the weighting $w(A_t)$ with a heavy-tailed Cauchy distribution for high noise levels $A_t < -1$, combined with a unimodal normal for lower noise levels:\n$w(A_t) = \\begin{cases} \\frac{1}{Z_c} \\text{Cauchy}(A_t; -1, 4.8) & \\text{if } A_t < -1 \\\\ \\frac{1}{Z_n} N(A_t; -1, 2.4) & \\text{if } A_t \\geq -1 \\end{cases}$\nwhere $Z_c$ and $Z_n$ normalize the densities to 1 at $\\mu=-1$. This asymmetric weighting improves transcript alignment in our generations compared to symmetric alternatives. For training efficiency, we utilize the adaptive noise scheduler [17] to reduce loss estimate variance.\nDuration Prediction. Our approach avoids the need for predicting phoneme durations as an intermediate step, which can introduce errors. During training, we provide the diffusion network with noisy latents of the correct sequence length corresponding to the full utterance duration. At inference time, we only specify the overall duration, not individual phoneme durations. In contrast, models like NaturalSpeech2 and VoiceBox require an external phoneme duration prediction model.\nOur model instead learns to resolve the phoneme durations in an end-to-end manner from only the text transcript during diffusion training. For duration prediction at inference time, we simply fine-tune the ByT5-base model as a stochastic duration predictor in a sequence-to-sequence manner. Conditioned on the transcript, it generates utterance durations (e.g. \"4.51\") auto-regressively with nucleus sampling (p=0.95) [18], achieving a 1.4 second RMSE. Importantly, our approach is agnostic to the duration selection method, avoiding cascaded errors from explicit phoneme duration modeling.\nSpeaker-Prompted Generation. The ability to perform speaker-prompted generation, where a short sample of reference audio conditions the generation on the desired speaker's voice characteristics, is a valuable capability for TTS systems. Diffusion models can perform speaker-prompted TTS through audio inpainting [1]. We train our denoising network for both text-only and speaker-prompted TTS synthesis in a multi-task fashion. With probability p = 0.5, we train the network to perform audio inpainting by concatenating a clean audio latent with a noisy latent vector. We sample a duration d and concatenate the start of the latent audio representation x[:d] with the end of the noisy latent $z_t [d:]$ to construct the input. We also introduce a binary embedding to identify corrupted frames, which we sum with the input after the initial projection. When calculating the loss, we mask out frames corresponding to the clean audio.\nFor the prompt duration, we sample the proportion of the input, d \u2208 [0, 1], to hold out as the clean prompt. For instance, if we sample d = 0.1 for a 10 second clip of audio, then we use the frames corresponding to the first second of audio as the clean prompt. For sampling the duration, we use a Beta distribution with a mode of .01 and a concentration of 5 to emphasize challenging cases with very short prompts. During inference, we prepend a speaker's reference audio and the associated text to perform speaker-prompted TTS synthesis.\nClassifier-Free Guidance. To enable classifier-free guidance"}, {"title": null, "content": "[19], we drop the text with probability p = 0.1 and jointly train a conditional and unconditional diffusion model. During inference, we introduce a sampling parameter w, and compute\n$v_\\theta(z_t, t, c) = v_\\theta(z_t, t) + w * (v_\\theta(z_t, t, c) - v_\\theta(z_t,t))$.\nWhen w = 1.0, this reduces to the conditional diffusion model, and setting w > 1.0 increases the influence of the conditioning information. For the cross-attention layers, we concatenate a learnable null embedding with the text features along the sequence dimension. We mask out the text features to drop conditioning information.\nImplementation Details. We begin with the 2D U-Net design used by iDDPM [12] for image diffusion and replace its 2D convolutions with corresponding 1D convolutions to adapt the U-Net to 1D sequences. For instance, we substitute each 2D convolution of size 3x3 with a 1D convolution of size 3. We make similar substitutions for the downsampling and upsampling operations. Our U-Net has 4 stages which downsample the input from 1504 frames to 188 frames. We utilize a feature dimensionality of 512 throughout the network.\nWe use a transformer backbone [14, 20] with 8 layers and a dimension of 512. We encode positional information with a 1D Dynamic Position Bias (DPB) [21]. This introduces a lightweight MLP that maps relative offsets between locations, $\\Delta x_{i,j} \\in {..., -1, 0, 1, 2, ...}$, to head-specific bias terms that are added to the self-attention logits before the softmax. To condition the diffusion network on the level of noise, we utilize $\\alpha$-conditioning [22, 23]. We map $a_t$ to a sinusoidal position embedding [14] and pass it through an MLP to obtain a time embedding. We condition the U-Net residual blocks and transformer feedforward layers on the time embedding following standard practice from prior image diffusion work [20]. We pad the audio with silence up to 20 seconds (i.e., 1504 latents), and mask out the silence from the network.\nOur final model has 137M trainable parameters. We train SESD for 250k steps with a batch size of 64 utterances on one Nvidia A6000 GPU. We use the AdamW optimizer [24] with a 1000-step linear warmup, a peak learning rate of 2e-4, a cosine learning rate decay, and independent weight decay of 2e-4. We apply dropout of 0.1 to the feedfoward, self-attention, and cross-attention layers in the transformer. We compute an exponential moving average (momentum of 0.9999) of the training model. For our ablation study, we train all models for 100k steps without dropout or weight decay. For generation, we use 250 sampling steps with the scaled cosine noise schedule with a scale factor of 0.5 [23, 16]. We use the DDPM sampler [8] with w = 5.0 for text-only synthesis and the DDIM sampler [25]"}, {"title": "5. Experiments", "content": "We utilize the clean and other training splits of the LibriSpeech (LS) dataset [26], totaling 960 hours of speech, to train SESD. For evaluation, we follow prior work [2, 1] and consider a filtered subset of LS test-clean consisting of clips between four and ten seconds in length. For speaker-prompted TTS, we utilize a 3 second clip from another sample of the same speaker.\nBaselines. For text-only synthesis, we compare against VITS [27], a variational autoencoder with adversarial training. We consider both VITS variants: the single-speaker VITS-LJ trained on LJ Speech, and the multi-speaker VITS-VCTK trained on VCTK. We also compare against English MMS-TTS [28], a recent single-speaker model. For speaker-prompted TTS, we compare against YourTTS [29], a VITS model conditioned on a speech prompt. We follow the evaluation protocol of the recent state-of-the-art generative models such as VALL-E [2] and VoiceBox [1] and compare against their reported metrics. We also compare against the speech-to-speech baselines GSLM and AudioLM [30, 31]."}, {"title": "6. Results", "content": "Our results in Table 1 demonstrate that our method can generate intelligible speech in a text-only setting, nearly matching the word error rate of the ground truth audio. Our text-only WER surpasses that of the single-speaker models while providing the additional capability of multi-speaker synthesis. In the speaker-prompted setting, our model generates speech that maintains the characteristics of the prompt. Notably, SESD outperforms the SoTA auto-regressive system, VALL-E, in terms of both the WER and the neural speaker similarity metric, with less than 2% the training data. We also match the performance of the latent diffusion NS2 system using 2.2% of the training data. We demonstrate the importance of our various design decisions in Figure 4. Our position-aware cross attention mechanism, model architecture, text encoder, and diffusion loss weighting are critical for generating intelligible speech."}, {"title": "7. Conclusion", "content": "We present SESD, a highly sample-efficient latent diffusion framework for text-to-speech synthesis that achieves strong results in a modest data regime. The key ingredients in the success of SESD are: a novel diffusion architecture that efficiently models long audio sequences, incorporating representations from a byte-level language model that capture linguistic properties critical for natural speech synthesis, and modifying the diffusion loss weighting to improve text-speech alignment. Together, these innovations enable SESD to perform speech synthesis directly from text without explicit phoneme alignment. SESD generates intelligible speech near human-level word error rates with less than 1k hours of training data."}]}