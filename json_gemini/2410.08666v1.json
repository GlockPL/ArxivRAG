{"title": "DeltaDQ: Ultra-High Delta Compression for Fine-Tuned LLMs via\nGroup-wise Dropout and Separate Quantization", "authors": ["Yanfeng Jiang", "Zelan Yang", "Bohua Chen", "Shen Li", "Yong Li", "Tao Li"], "abstract": "Large language models achieve exceptional per-\nformance on various downstream tasks through\nsupervised fine-tuning. However, the diver-\nsity of downstream tasks and practical require-\nments makes deploying multiple full-parameter\nfine-tuned models challenging. Current meth-\nods that compress the delta weight struggle to\nachieve ultra-high compression, failing to mini-\nmize the deployment overhead. To address the\nabove issue, we propose a novel distribution-\ndriven delta compression framework DeltaDQ,\nwhich utilizes Group-wise Dropout and Sepa-\nrate Quantization to achieve ultra-high com-\npression for the delta weight. We have ob-\nserved that the matrix-computed intermediate\nresults for the delta weight exhibit extremely\nsmall variance and min-max range characteris-\ntics, referred to as Balanced Intermediate Re-\nsults. Exploiting this phenomenon, we intro-\nduce Group-wise Dropout to perform dropout\non the delta weight using an optimal group\nsize. Furthermore, using Separate Quantization,\nsparse weights are quantized and decomposed\nto achieve a lower bit. Experimental results\nshow that DeltaDQ achieves 16\u00d7 compres-\nsion with improved accuracy compared to base-\nlines for WizardMath and WizardCoder models\nacross different parameter scales. Moreover,\nDeltaDQ demonstrates the ability for ultra-high\ncompression ratio, achieving 128\u00d7 compres-\nsion for the WizardMath-7B model and 512\u00d7\ncompression for the WizardMath-70B model.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (Brown et al.,\n2020; Zhang et al., 2022; Touvron et al., 2023;\nAchiam et al., 2023) have achieved unprecedented\nadvances in recent years, and most researchers and\nusers have adopted the Supervised Fine-Tuning\n(SFT) (Ouyang et al., 2022) to emerge the capabili-\nties of LLMs for a variety of different downstream\ntasks. SFT enables LLMs to achieve better qual-\nity in tasks such as mathematical reasoning and"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Model Compression for LLMs", "content": "Model compression can effectively reduce the de-\nployment costs of LLMs through techniques such\nas sparsification, quantization, knowledge distil-\nlation, and low-rank approximation (Tang et al.,\n2024; Zhou et al., 2024). In this paper, we focus\non sparsification and quantization. Current quanti-\nzation methods can primarily be categorized into\nweight-only and weight-activation quantization.\nWeight-only quantization, exemplified by methods\nlike GPTQ (Frantar et al., 2022) and AWQ (Lin\net al., 2023), focuses on reducing model weight pre-\ncision to lower bits, effectively decreasing memory\nusage and loading time. Weight-activation quanti-\nzation, such as LLM.int8() (Dettmers et al., 2022)\nand SmoothQuant (Xiao et al., 2023), quantize both\nweights and activations, further reducing inference\nmemory and latency. Sparsification, represented by\npruning methods such as SparseGPT (Frantar and\nAlistarh, 2023), Wanda (Sun et al., 2023), LLM-\nPruner (Ma et al., 2023), and ShearedLLaMA (Xia\net al., 2023), optimizes model inference by remov-\ning unimportant weights. Although these methods\nwork well for LLMs, they do not account for the\ncharacteristics of delta weight, which limits their\nability to achieve an optimal compression ratio."}, {"title": "2.2 Multi Fine-Tuned Model Deployment", "content": "The diversity of downstream tasks and user require-\nments makes deploying multiple fine-tuned mod-\nels challenging. Current SFT methods are mainly\ndivided into PEFT (Han et al., 2024) like LORA\n(Hu et al., 2021) and QLoRA (Dettmers et al.,\n2023), and full-parameter fine-tuning. For LoRA-\nbased fine-tuned models, frameworks like S-LORA\n(Sheng et al., 2023) and Punica (Chen et al., 2023)\nenable the deployment of thousands of LoRA mod-\nels using efficient parallel strategies and highly op-\ntimized CUDA kernels. For full-parameter fine-\ntuned models, optimization is primarily achieved\nthrough delta compression. Although DARE (Yu\net al., 2023) is designed for model merging, it effec-\ntively compresses the delta weight using Dropout\n(Srivastava et al., 2014). BitDelta (Liu et al., 2024)\nreduces memory requirements by quantizing the"}, {"title": "3 Methodology", "content": "We begin by introducing the foundational concepts\nof delta compression in Section 3.1. In Section\n3.2, we describe the Balanced Intermediate Re-\nsults, highlighting that the delta weight is more eas-\nily compressed compared to the fine-tuned weight.\nBuilding on this, we propose DeltaDQ, which is\nprimarily composed of the Group-wise Dropout\npresented in Section 3.3 and the Separate Quanti-\nzation in Section 3.4, as shown in Figure 2."}, {"title": "3.1 Preliminaries: Delta Compression", "content": "Algorithm. Assume there are n full-parameter\nfine-tuned models, with their weights denoted as\n{W1, W2, ..., Wn}, all derived from a homolo-\ngous base model such as Llama (Touvron et al.,\n2023). We can split the weight of each model into\ntwo parts, the weight of base model Wb and the\ndelta weight AW; of each fine-tuned model, in the\nfollowing way:\n$\\Delta W_i = W_i - W_b$.                                                (1)\nThe delta compression algorithm is to compress\nthe delta weight $W_i$, with the compression ratio\nspecifically applied to the delta weight.\nDeployment. For practical inference deployment,\nthe delta compression framework strategically sep-\narates the storage of the base weight $W_b$ and the\ncompressed delta weight \u2206$W_i$, as shown in Figure\n2. In computation, the request $R_i$ for each model\nfollows a separate computation scheme, similar to\nDELTAZIP (Yao and Klimovic, 2023). Specifically,\n$R_i$ is processed independently by $W_b$ and $W_i$,\nwith synchronization to generate the final outputs,\nas illustrated in Figure 3. Since our method is com-\npatible with other delta compression frameworks\nat the deployment level, in this paper, we primarily\nfocus on the algorithmic aspects."}, {"title": "3.2 Balanced Intermediate Results", "content": "In model compression, the whole optimization\nproblem is commonly reformulated as a layer-by-\nlayer subproblem. Let W represent the weight of\nthe l-th layer of model i, and \u0174 denote the corre-\nsponding compressed weights. The optimization\nobjective of model compression is to minimize the\nlayer-wise l2-loss, defined as Llayer:\n$L_{layer} = ||XW_i - X\\widehat{W_i}||_2$                                              (2)\nwhere X is the input of the l-th layer.\nAssuming X \u2208 Rtxhin, W, W\u2208 Rhout hin\nand the outputs A, A\u2208 Rthout, we can further\ntransform Llayer:\n$L_{layer} = ||XW_i - X\\widehat{W_i}||_2 = ||A_i - \\widehat{A_i}||_2 = \\sum_{p=1}^{t} \\sum_{q=1}^{hout} (a_{p,q|i} - \\widehat{a_{p,q|i}})^2$   (3)\nwhere ap,q|i, \u00e2p,q| are the elements in the p-th row\nand q-th column of outputs A and A before and\nafter compression; for instance, ap,q| represents:\n$a_{p,q|i} = x_{p,0}W_{q,0} + ... + x_{p,hin} W_{q,hin}$                                                     (4)\nAs shown in Figure 4, we find that each element\n\u2206ap,q of the delta weight outputs A has the\nfollowing two properties compared to the original\noutputs A:\n\u2022 Small Variance: The intermediate results\n\u03a7\u03c1,0\u2206Wq,0, ..., Xp,hin Wq,hin have a small\nvariance between them.\n\u2022 Narrow Min-Max Range: The intermediate re-\nsults xp,0\u2206wq,0, ..., Xp,hin Wq,hin of Aap,ql\nhave a very narrow range between the maxi-\nmum and minimum values.\nThese two properties make delta weight more suit-\nable for compression techniques such as sparsifi-\ncation compared to the original fine-tuned weight.\nWe refer to this phenomenon as Balanced Interme-\ndiate Results."}, {"title": "3.3 Group-wise Dropout", "content": "Inspired by the phenomenon of Balanced Interme-\ndiate Results, we observe that random dropping\nmethods, such as Dropout, provide a simple yet\neffective approach to compressing delta weight. In\ncontrast to DARE (Yu et al., 2023), which primar-\nily focuses on model merging, our method lever-\nages the Balanced Intermediate Results by stochas-\nticly dropping weights along matrix computation\ndimensions (the row dimensions) and subsequently\nscaling the remaining weights. The procedure for\napplying Row-wise Dropout to the delta weight\n\u0394 \u2206W \u2208 Rhout hin at the l-th layer is as follows:\n\u2022 Row-wise Drop: For a given compression\nratio a, generate hout random mask vectors\nm\u00a1 \u2208 R1\u00d7hin, where 1 \u2013 1/a of the elements\nin each vector are set to 0, and the remaining\nelements are set to 1. Form a mask matrix\nM using these hout mask vectors mi, and ap-\nply it to obtain the masked delta weight as\nAW A\u0174 = AWM.\n\u2022 Rescaling: Scale the remaining elements of\nAW by multiplying a, yielding \u2206 = \u03b1\u00b7\n\u0394 AW.\nThrough the aforementioned steps, we obtain the\nsparse A\u0174. For any given element \u2206ap,q, em-\nploying Row-wise Dropout, as opposed to global\nDropout like DARE, more effectively minimizes\nthe compression-induced error \u2206ap,qli \u2013 D\u00e2p,gli,\nthereby approximating Llayer = 0.\nGrouping techniques are widely employed in\nmodel compression, often significantly improv-\ning the accuracy. As illustrated in Figure 5, un-\nder a fixed compression ratio a, subdividing mi\ninto hin/hg finer-grained mask groups mi,j \u0404\nR1xhg (hg \u2264 hin) where ha represents the group\nsize, affects model accuracy. Unlike group-wise\nquantization, a smaller hg does not necessarily\nresult in higher accuracy and the optimal group\nsize ha varies across different models. Nonethe-\nless, compared to Row-wise Dropout, the group-\ning method often achieves superior results at the\noptimal group size h. To further mitigate accu-\nracy loss, we extend our approach to Group-wise\nDropout.\nTo efficiently determine the optimal group size\nh, we first constrain the weights across all lay-\ners and each row within these weights to use the\nsame group size. We define the range of hg as\n{\u03b1, \u03b1 * 21, \u03b1 * 22, ..., hin}. Directly selecting h"}, {"title": "3.4 Separate Quantization", "content": "Despite Group-wise Dropout achieving over 10\u00d7\ncompression on the delta weight in most fine-tuned\nmodels, further increasing the compression ratio a\noften leads to significant accuracy loss. As shown\nin Figure 6, the distribution of delta weight demon-\nstrates strong compatibility with uniform quantiza-\ntion. However, the primary limitation of quantiza-\ntion is the substantial accuracy degradation at ultra-\nlow bits, which also occurs with the delta weight.\nThe challenge remains in reducing the quantization\nbit further to enhance compression ratio without\nincurring accuracy loss.\nSince we are quantizing weight AW, follow-\ning a weight-only scheme, offline quantization is\nfeasible. Additionally, when splitting a fine-tuned\nmodel into base weight and delta weight, as dis-\ncussed in Section 3.1, there is an inherent sep-\narate computation during deployment. And the\nweight AW is a sparse matrix stored in Com-\npressed Sparse Row (CSR) format, where row\noffsets, column indices, and non-zero values are\nrecorded. When a sparse matrix is decomposed into\nm parts, the storage overhead for column indices\nand non-zero values remains unchanged, while the\nrow offsets increase by m 1 times, which is a neg-\nligible increase. Furthermore, each decomposed\nweight becomes even sparser, which is beneficial\nfor computations using sparse libraries (Naumov\net al., 2010). These characteristics allow us to apply\nseparate quantization on the sparsified delta weight.\nWe can directly decompose the delta weight based\non its magnitude, thereby reducing the quantization\nbit without compromising accuracy.\nAs illustrated in Figure 2, given a compression\nratio of a1 from the previous step, we first quantize\nA\u0174 using the following per-tensor granularity\nuniform quantizer:\n$Q = clip(\\frac{\\Delta \\widehat{W}}{s} + z, 0, 2^k -1)$                                              (6)\n$s = \\frac{max(\\Delta \\widehat{W}) \u2013 min(\\Delta \\widehat{W})}{2^{k-1}}$                                              (7)\n$z = \\frac{-min(\\Delta \\widehat{W})}{s}$ , (8)\nwhere Q represents the quantized weights, k de-\nnotes the quantization bit, s and z represent the\nscale factor and zero point, respectively. After\nquantization, the elements in Q belong to the set\n{0, 1, ..., 2k - 1}. We then decompose Q into m\nnew quantized weights Q based on the value:\n$Q_{i,j} = Q_I(r^{min} < Q_i < r^{max}) + o_j$                      (9)\n$r^{min} = \\frac{2^k}{m} * (j \u2212 1), r^{max} = \\frac{2^k}{m} * j \u2212 1$,                            (10)\n$o_j = \\frac{2^k}{m} * (j \u2013 1) (j = 1, ..., m)$                                       (11)\nHere, \u2161(\u00b7) represents the elements that are filtered\nwithin the range [rmin, rmax], and oj represents\nthe offset coefficient. The corresponding dequanti-\nzation process is:\n$DQ_{i,j} = s. (Q_{i,j} - z \u2013 o_j)$.                                        (12)\nIn this way, we can reduce the storage bit of each\nnew weight Q to k \u2013 log m bits, allowing us to\nachieve 2-bit or even 1-bit quantization, thereby\nincreasing the compression ratio to a1 * 16/(k \u2013\nlog m)."}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Setup", "content": "Baseline. We compare DeltaDQ with three repre-\nsentative compression methods. Magnitude (Han\net al., 2015) is a classical pruning approach based\non weight magnitudes and serves as a strong base-\nline. Additionally, we include two delta compres-\nsion techniques: DELTAZIP (Yao and Klimovic,\n2023), which employs both sparsification and quan-\ntization similar to our method, and DARE (Yu et al.,\n2023), which relies solely on sparsification.\nModels, Datasets and Evaluation. We evaluate\nthree types of fine-tuned models at different pa-\nrameter scales: WizardMath (Luo et al., 2023a),\nWizardLM (Xu et al., 2023), and WizardCoder\n(Luo et al., 2023b), where the first two are fine-\ntuned from Llama2, and the latter is fine-tuned\nfrom CodeLlama (Roziere et al., 2023). Our evalu-\nation primarily uses two datasets: GSM8k (Cobbe\net al., 2021) for assessing WizardMath, and Hu-\nmanEval (Chen et al., 2021) for WizardCoder.\nImplementation Details. DeltaDQ is built with\nPyTorch (Paszke et al., 2017) and utilizes models\nand datasets from Huggingface Transformers (Wolf"}, {"title": "4.2 Main Results", "content": "Basic Compression. We first compare the accu-\nracy against baselines at 2x, 4x, 8x, and 16\u00d7\nbasic compression ratio. As shown in Table 1, our\nframework achieves better accuracy for Wizard-\nMath and WizardCoder models compared to the\nremaining three methods in most cases. At 2x,\n4\u00d7, and 8\u00d7 compression ratio, DeltaDQ only uti-\nlizes Group-wise Dropout, yet it still outperforms\nthe baselines, particularly at a higher compression\nratio. And we observe that for a lower compres-\nsion ratio, simpler methods can also achieve strong\nresults. For example, at 2\u00d7 compression on the\nWizardMath-13B model, the Magnitude method de-\nlivers the best performance. At a 16\u00d7 compression\nratio, our framework surpasses the state-of-the-art\naccuracy for the WizardMath-7B and 13B models\nby 4.40 and 2.20, respectively, and outperforms the\noriginal WizardCoder-7B and 13B models by 3.05\nand 1.22. Additionally, we find that models with"}, {"title": "4.3 Analysis", "content": "Evaluation of Group-wise Dropout. In Group-\nwise Dropout, achieving optimal accuracy requires\nsearching for the optimal group size h. As shown\nin Table 4, our proposed proxy metric significantly\noptimizes the search speed compared to directly\ntesting accuracy. Under three different a settings,\nthe search time is reduced to about 30% of the Di-\nrect method while still achieving the same results.\nEvaluation of Separate Quantization. For Sepa-\nrate Quantization, as shown in Figure 7, we analyze\nthe impact of decomposing on GPU memory and\naccuracy. When quantized to 8-bit and 4-bit, the\nmemory footprint of the model parameters remains\nnearly unchanged as the number of parts increases.\nThis is because, after decomposing, the sparse ma-\ntrix storage only adds the row offsets and quantiza-\ntion parameters offset coefficient, both of which re-"}, {"title": "5 Conclusion", "content": "In this paper, we introduce DeltaDQ, a novel delta\ncompression framework primarily composed of\nGroup-wise Dropout and Separate Quantization.\nGroup-wise Dropout leverages the inherent prop-\nerties of delta weight to dropout in a group-wise\nmanner, while Separate Quantization applies fur-\nther compression to the sparse delta weight. Ex-\nperimental results demonstrate that DeltaDQ can\neffectively achieve ultra-high delta compression."}, {"title": "Limitations", "content": "While our framework shows significant potential,\nits deployment performance is currently hindered\nby the absence of optimized libraries for accelerat-\ning operations with low-bit sparse weights. Such\nspecialized libraries are essential to fully realize\nthe efficiency gains of our approach, as they would\nenable faster computation and more efficient mem-\nory access patterns tailored to the weight structure.\nThis is also part of our future work."}, {"title": "Ethical Considerations", "content": "Our framework may introduce slight variations in\nmodel outputs, as the process alters the precise val-\nues of the weights, potentially affecting inference\nresponses. However, it is designed to minimize\nthese effects, prioritizing the preservation of per-\nformance quality and output consistency."}]}