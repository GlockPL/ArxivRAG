{"title": "A Robotics-Inspired Scanpath Model Reveals the Importance of Uncertainty and Semantic Object Cues for Gaze Guidance in Dynamic Scenes", "authors": ["Vito Mengers", "Nicolas Roth", "Oliver Brock", "Klaus Obermayer", "Martin Rolfs"], "abstract": "How we perceive objects around us depends on what we actively attend to, yet our eye movements depend on the perceived objects. Still, object segmentation and gaze behavior are typically treated as two independent processes. Drawing on an information processing pattern from robotics, we present a mechanistic model that simulates these processes for dynamic real-world scenes. Our image-computable model uses the current scene segmentation for object-based saccadic decision-making while using the foveated object to refine its scene segmentation recursively. To model this refinement, we use a Bayesian filter, which also provides an uncertainty estimate for the segmentation that we use to guide active scene exploration. We demonstrate that this model closely resembles observers' free viewing behavior, measured by scanpath statistics, including foveation duration and saccade amplitude distributions used for parameter fitting and higher-level statistics not used for fitting. These include how object detections, inspections, and returns are balanced and a delay of returning saccades without an explicit implementation of such temporal inhibition of return. Extensive simulations and ablation studies show that uncertainty promotes balanced exploration and that semantic object cues are crucial to form the perceptual units used in object-based attention. Moreover, we show how our model's modular design allows for extensions, such as incorporating saccadic momentum or pre-saccadic attention, to further align its output with human scanpaths.", "sections": [{"title": "1. Introduction", "content": "Humans actively move their eyes to pay attention to individual parts of their environment. Historically, models characterized visual attention as a spotlight, enhancing processing in attended spatial regions (Posner, 1980; Treisman and Gelade, 1980) and selecting locations based on a saliency map (Koch and Ullman, 1985; Itti and Koch, 2001). There is, however, mounting evidence of psychophysical studies suggesting that attention operates on discrete visual objects (for reviews, see Cavanagh et al. 2023; Scholl 2001; Peters and Kriegeskorte 2021). The hypothesis that objects guide gaze behavior in dynamic real-world scenes has recently gained additional traction through a computational modeling approach (Roth et al., 2023) in which we demonstrated that object-based attention and selection leads to more human-like exploration behavior than space-based models. But how do these object representations arise before being actively attended? In this work, we present an object-based scanpath model that reproduces human viewing behavior in dynamic scenes and in which the segmentation of the scene in perceptual units and the simulation of saccadic decisions are treated as interdependent processes.\nPrevious models of human exploration behavior were either restricted to static scenes (Itti et al., 1998; K\u00fcmmerer et al., 2022; Schwetlick et al., 2022), only captured the average fixation density instead of simulating individual scanpaths (Droste et al., 2020; Molin et al., 2015), or required explicitly provided object segmentations (Roth et al., 2023). To describe what object-based attention can act on, \"proto-objects\u201d were introduced as pre-attentive volatile units that can be accessed and further shaped by selective attention (Rensink, 2000). Walther and Koch (2006) proposed a model that generates such proto-objects for static scenes based on salient regions in terms of color, edges, and luminance. On the other hand, evidence from unconscious object representations and unconscious object-based attention (cf. Chou and Yeh 2012; Tanaka et al. 1991) indicates pre-attentive targets of object-based selection do not need to be refined when attended, but are fully formed (Ca-"}, {"title": "2. Materials and methods", "content": "2.1. A model for interdependent saccadic decisions and object segmentation\nWe propose a model for the two processes of saccade target selection and object segmentation in natural scenes. \u03a4o establish an active interconnection"}, {"title": "2.1.1. Estimating object segmentation and its uncertainty", "content": "The object segmentation of a scene is inherently uncertain because the visual input is ambiguous, and what is considered to be an object might change over time, depending on the scanpath. Therefore, we aim not only to estimate the object segmentation but also to explicitly estimate the current uncertainty over it. To do so, we combine multiple cues for object segmentation as measurements in a recursive Bayesian filter (S\u00e4rkk\u00e4, 2013). This filter updates the object segmentation with each new measurement while also estimating its uncertainty, similar to the use of the segmentation filter in previous work on object segmentation for robotics (Mengers et al., 2023). As shown in Fig. 2 on the left, we consider three measurements of pre-attentive global segmentation based on motion, appearance, and semantics, as well"}, {"title": "2.1.2. Scanpath simulation", "content": "We model the saccadic decision-making process by adapting the object-based Scanpath simulation in Dynamic scenes (ScanDy) framework (Roth et al., 2023). The scanpath simulation updates its internal state, which includes a decision variable for all potential target objects in the scene, as segmented through the particle filter (Fig. 2). We model the target selection process of where and when to move the gaze position with a drift-diffusion model (DDM), in which each object represents a potential saccade target. The decision variable for each object depends on its eccentricity given the current gaze position, how relevant the visual scene features are as measured by salience, and the uncertainty of the local object boundaries as provided by the segmentation particle filter. Notably, the model does not rely on an explicit implementation of the \"inhibition of return\u201d (IOR) mechanism.\nScene relevance based on salient features. We quantify the relevance of the scene content for gaze behavior by computing frame-wise feature maps F."}, {"title": "2.2. Dataset", "content": "We compared the simulated scanpaths with human eye-tracking data recorded under free-viewing conditions on videos of natural scenes. We recorded eye-tracking data from 10 participants on 43 video clips from the Unidentified Video Objects (UVO, Wang et al., 2021) dataset (10 used for parameter tuning, 33 used for testing the model; randomly split). The videos were selected to show diverse content and to have temporally consistent, densely annotated object masks for the first 90 frames (cf. Wang et al., 2021). We recorded eye-tracking data for the here-used videos with an Eyelink 1000+ tabletop system (SR Research, Osgoode, ON, Canada) with a 1000 Hz sampling rate, as part of an ongoing collaborative large-scale eye movement database (publication of full dataset in preparation). We presented the"}, {"title": "2.3. Metrics and parameter fitting", "content": "We determined the model parameters by comparing the foveation duration and saccade amplitude distributions of the simulated scanpath with the human ground truth. We measured the similarity between a simulated distribution N to the ground truth M using the two-sample Kolmogorov-Smirnov (KS) statistic $D = sup_{x}|N(x) \u2013 M(x)|$. We systematically varied the DDM noise level s, the decision threshold \u03b8, and the relative importance of the feature map F' and uncertainty map U', quantified by the re-scaling parameters $f_{min}$ and $U_{min}$. We performed a coarse grid search in this four-dimensional parameter space on the 10 videos in the training set. We simulated five different scanpaths (different random seeds) for each parameter configuration and compared them. Since we are particularly interested in the effect of uncertainty on the simulated scanpaths, we refined the grid search for each $U_{min}$ value around the parameter sets leading to the lowest mean of the KS statistics for the foveation duration $D_{FD}$ and the saccade amplitude $D_{SA}$.\nWith the model parameters chosen such that the basic scanpath summary statistics of foveation durations and saccade amplitudes matched the human data, we evaluated the simulated scanpaths out-of-domain on the test set, i.e., on 33 previously unseen videos and on different metrics than what the parameters on the training set were selected for. For each parameter set, we simulated 10 scanpaths and compared them with the data from the 10 human observers. We focused on the analysis of how the gaze behavior balances the exploration of the background of a scene (Background), uncover an object for the first time for foveal processing (Detection), explore further details of the currently foveated object by making a within-object saccade (Inspection), or return to a previously uncovered object (Return) (Linka and de Haas, 2021; Roth et al., 2023). Comparing the foveation durations in each category provides an insightful metric of the exploration behavior, which is particularly suited for dynamic scenes (see Roth et al., 2023). We used this metric in the training set to choose the later described base model among different uncertainty values umin (see Appendix A for more details).\nSince our model does not have an explicit IOR mechanism, we were par-ticularly interested in whether it could reproduce typical IOR effects. IOR describes the inhibition of recently attended stimuli and the resulting delayed response to them (Posner et al., 1984; Klein, 2000). In a free-viewing con-dition, as in the data used for this study, we therefore expect that saccades"}, {"title": "3. Results", "content": "Our aim was to build an image-computable and mechanistic computational model that closely resembles human gaze behavior in dynamic real-world scenes. In this section, we compare our model with human scanpaths, first qualitatively in Sec. 3.1 and then quantitatively by reviewing aggregated statistics in Sec. 3.2. We systematically explore the influence of uncertainty on visual exploration behavior in Sec. 3.3. In an additional ablation study, we probed the impact of individual object representations as input sources and the importance of the interaction between object perception and saccadic decision-making for the simulated scanpaths, as described in Sec. 3.4. Lastly, we show how our model can be easily extended with additional modules, such as saccadic momentum or pre-saccadic attention, leading to more human-like saccade angle statistics and improvements in early object detections (Sec. 3.5)."}, {"title": "3.1. Qualitative scanpath analysis", "content": "Our model produces scanpaths that closely resemble human visual exploration behavior, as shown in Fig. 4. The mechanistic nature of our model makes the individual saccadic decisions transparent and interpretable: Initially, the model has high uncertainty over the whole scene, which is reduced through large saccades towards unexplored objects (Fig. 4a). Objects with particularly high saliency are likely to be revisited (Fig. 4b) or are further inspected (Fig. 4c-d). Return saccades to previously foveated objects also become more likely with time, as uncertainty over object boundaries can rise again, for example, through object motion. This qualitatively similar behavior of our model can also be seen in Appendix C, where we show the exact"}, {"title": "3.2. Aggregated scanpath statistics", "content": "We compare our base model predictions to human scanpaths on a set of videos not used for parameter search. As described before, we selected the model parameters to resemble the statistics of human foveation duration and saccade amplitude on ten videos. The model generalizes well to the previously unseen set of 33 videos, as shown based on the aggregated scanpath statistics in Fig. 5. Similar to human eye-tracking data, the foveation durations (Fig. 5a) of the simulated scanpaths follow a log-normal distribution with a mean of 390 ms and a median of 332 ms (humans: mean of 433 ms and median of 316 ms). The distribution of the model is more narrow compared to humans, which if other metrics would not be considered could be fixed by increasing both the decision threshold and the noise level in the drift-diffusion model, as described in Sec. 2.1.2. The saccade amplitudes in the simulated scanpaths (Fig. 5b) follow the gamma distribution of the human data with a mean of 3.70 dva and a median of 2.81 dva (humans: mean of 3.40 dva and median of 2.90 dva). These well-described statistics are not explicitly implemented in the model, but emerge from model con-"}, {"title": "3.3. Model ablation 1: Uncertainty drives exploration", "content": "Our model uses the uncertainty measure of the object segmentation module as an estimate for the perceptual uncertainty that influences the gaze behavior depending on the scanpath history. Here, we evaluate the effect of this uncertainty mechanism, comparing the simulated model scanpaths with a varying influence of the uncertainty on the saccadic decision-making process. Specifically, we vary the umin parameter of the model where a higher value decreases the importance of uncertainty. We also compare results from"}, {"title": "3.4. Model ablation 2: Semantic object cues and component interconnections form suitable perceptual units", "content": "Our model updates both the current object segmentation and perceptual uncertainty from the current image of the scene using different object cues. The segmentation then defines the perceptual units in saccadic decision-making, and the uncertainty influences the likelihood of selecting these perceptual units. In this ablation, we investigated to what extent different object"}, {"title": "3.5. Model extensions: Saccadic momentum improves saccade angle statistic and pre-saccadic attention benefits early object detections", "content": "We have shown so far that our model reproduces important hallmarks of scanpaths in dynamic real-world scenes. One instructive metric we have not yet investigated is the distribution of relative saccade angles. Importantly,"}, {"title": "4. Discussion", "content": "We presented a model for object-based attention and gaze behavior in complex dynamic scenes that builds on a previous model for saccadic decision-making (Roth et al., 2023) and an object segmentation model for interactive perception in robotics (Mengers et al., 2023). The active interconnection between the two model components resembles an algorithmic information processing pattern from robotics, Active InterCONnect (AICON; see Battaje et al., 2024). We examine this resemblance further in Sec. 4.4, but first discuss the results of our study and the limitations of our approach (Sec. 4.1), the conclusions we can draw about uncertainty as a driving factor for visual exploration (Sec. 4.2), and what we can learn from the model about the perceptual units of attention (Sec. 4.3)."}, {"title": "4.1. Advantages and limitations of our model", "content": "Our model still has many of the simplifications of our previous framework for Scanpath simulation in Dynamic scenes (ScanDy) (Roth et al., 2023). Importantly, we assume that attention spreads instantaneously and uniformly across objects and that saccades are always precisely executed without attempting to model the saccade programming and oculomotor control. Due to the modular implementation of the model, we expect adding such com-ponents to be a straightforward task. The current extensions of saccadic momentum and pre-saccadic attention both only required the addition of a few lines of code.\nSo far, we only model scanpaths during free-viewing, that is, where the observers had no task instructions. In the future, we plan to apply the same modeling approach to simulate scanpaths in complex dynamic scenes dur-ing visual search and scene memorization tasks. We expect that additional top-down attentional control during these tasks can be incorporated into the modeling by adapting the feature map F (see Sec. 2.1.2, F currently represents only visual saliency) and tuning the model parameters. For ex-ample, we would expect already reasonably simulated scanpaths for scene exploration through a down-scaling of the importance of F through fmin and"}, {"title": "4.2. Uncertainty drives exploration", "content": "The connection between active exploration behavior and the reduction of perceived uncertainty of the environment is well established in the literature (Sullivan et al., 2012; Friston et al., 2012; Renninger et al., 2007). Gottlieb et al. (2013) summarized that \"information-seeking obeys the imperative to reduce uncertainty and can be extrinsically or intrinsically motivated\u201d (p. 586) and that \"the key questions we have to address when studying exploration and information-seeking pertain to the ways in which observers handle their own epistemic states, and specifically, how observers estimate their own uncertainty and find strategies that reduce that uncertainty\" (p. 586). It is, however, not obvious how uncertainty should be measured and quantified in an image-computable model of visual attention.\nIn this context, it is important to clarify what we mean by uncertainty since the term can refer to many things. Our model specifically considers"}, {"title": "4.3. Perceptual units for object-based attention", "content": "Object-based attention is a well-established concept that has been thoroughly investigated in a large variety of experimental paradigms (Scholl, 2001; Peters and Kriegeskorte, 2021; Cavanagh et al., 2023). even after decades of debate, however, it remains an option question what constitutes a visual object (Spelke, 1990; Feldman, 2003; Palmeri and Gauthier, 2004; Scholl et al., 2001; Cavanagh et al., 2023). Our model allows us to systemat-ically vary the object cues used for the formation of the scene segmentation, which defines the perceptual units on which our object-based attentional se-lection process operates. Under the assumption that our implementation of saccadic decision-making mechanisms is similar to the human visual system, we expect that the object cues that lead to more human-like scanpaths are also the cues primarily used for saccadic decision-making in humans.\nOur results suggest that attentional guidance primarily relies on semantic object cues. Only models that used the semantic cues both for the global and the prompted scene segmentation showed the IOR effect and could reproduce the balance between foveation categories seen in humans (cf. Fig. 7). This result is consistent with evidence for global semantic understanding of natural scenes (Neri, 2017) and the literature on high-level unconscious processing (reviewed in Cavanagh et al., 2023). As expected, the model scanpaths also became less human-like if we replaced the prompted segmentation at the gaze position with an appearance-based, low-level object cue (all-g & ll-p). This model corresponds to the assumption, that a foveated object would get more finely segmented (e.g., a t-shirt, which was previously part of a person, becomes its own object when foveated). However, we did not see evidence for this, and removing low-level object cues did not impact the simulated scanpath statistics in any major way. There is, of course, ample evidence for the brain using appearance- and motion-based object cues to segment complex dynamic scenes (Schyns and Oliva, 1994; Reppas et al., 1997; Von der Heydt, 2015). Based on our results, however, we argue that"}, {"title": "4.4. Employing an information processing pattern from robotics", "content": "Although originally developed for robotic perception systems (Mart\u00edn-Mart\u00edn and Brock, 2022), the information processing pattern AICON has already been successfully applied to model parts of human visual informa-tion processing, reproducing certain visual illusions (Battaje et al., 2024). Our results show further evidence that AICON serves as a useful pattern in the study of information processing, be it in humans or robots. This sug-gests that there are similarities between the problems faced in both and that AICON provides a structure simplifying their solution. We believe our model showcases two such ways in which the problems are similar: subproblems that depend on one another, as well as ambiguity and noise in the sensory input.\nThe interdependent subproblems here are saccadic decision-making and ob-ject segmentation, while in robots, different sensor streams such as force and visual measurements often depend on each other and also on the currently executed action. AICON's focus on rich, bidirectional interactions (active interconnections) between different components provides a direct response to this interdependence. The ambiguity and noise of sensory input, be it of sensory neurons in the eye or the pixels of a camera, is inherent to the real world as opposed to simulated agents. The answer for robotic systems is usually modeling the resulting uncertainty to inform action selection, and indeed here we also find modeling uncertainty useful to drive the exploration towards more uncertain areas.\nSince the AICON pattern comes from robotics, it should also be possible to leverage the results of this study to build capable robotic systems. What we use here as a model for active human vision can easily be transformed"}, {"title": "5. Conclusion", "content": "We developed and evaluated a model for object-based attention and gaze behavior in real-world dynamic scenes. By integrating saccadic decision-making mechanisms with an object segmentation framework, our model suc-cessfully simulates human-like scanpaths. This integration, an implementa-tion of the Active InterCONnect (AICON) information processing pattern from robotics, enables the model to progressively refine its object segmen-tation through active exploration, while uncertainty over that segmentation guides the scanpath."}, {"title": "Appendix A. Parameter exploration", "content": "We find appropriate parameter values through extensive grid searches in a four-dimensional parameter space, as described in Sec. 2.3. To make the computational cost of the grid search feasible, we fix all parameters except for the decision threshold 0 for the drift-diffusion model (DDM), the DDM noise level s, and the scaling parameters for the importance of the uncertainty Umin and salient scene features fmin. All free and fixed model parameters are described in Tab. A.1.\nWe first ran a coarse parameter grid exploration for the parameters \u03b8\u03b5 [2, 3, 4, 5, 6], s \u2208 [0.1, 0.2, 0.3, 0.4], Umin \u2208 [0,1/10,1/5,1/3,1/2,1], and fmin \u2208 [0,1/10,1/5,1/3,1/2,1]. Around the best-performing parameters, we performed a finer grid search in 0 and s, as shown in Figs. A.9 and A.10. We did not consider parameter sets with s > 0.4 since previous model explorations have shown that the simulated scanpaths for higher noise levels are more likely to explore the background or objects that are not often foveated by human observers. As the main indication for noise-driven scanpaths, we took a lower correlation of the object dwell time between simulated and human scanpaths, as it is shown in Fig. 8b, which in fact decreases for models with s > 0.4.\nTo ensure a fair comparison between models in our ablation studies, we run additional parameter explorations for the models where the foveation duration and saccade amplitude change considerably compared to the base model with the parameters in Tab. A.1. For the model without uncertainty contribution (no uncert. in Fig. 6) we set U' = Umin, resulting in a model with 0 = 3.0, s = 0.3, fmin = 0, Umin = 1/3 having the lowest mean of KS statistics DFD and Dsa across the 4-dimensional grid of free parameters. When investigating the influence of different object cues, we explored a fine parameter grid fmin = 0, Umin = 1/5 for better comparability with the other models. This resulted in parameter values of 0 = 4.0, s = 0.4 for the model using ground truth objects (gt-obj in Fig. 7), 0 = 5.5, s = 0.4 for the model with all global object cues but without a prompted object (all-g & no-p), and 0 = 5.5, s = 0.4 for the model with global appearance and motion-based segmentation only (ll-g & no-p)."}, {"title": "Appendix B. Further details on the particle filter implementation", "content": "We track a belief over scene segmentation by combining different measurements over time within a particle filter, as we describe in Sec. 2.1.1."}, {"title": "Appendix B.1. Particle weighting", "content": "When computing the weight of each particle (Eq. (1)), we weigh it according to each segmentation cue. To do so, we first compute the unnormalized weights $w_t^b(z_t)$ according to each cue zt, using a distance function between two segmentations (Eq. (B.1)). We can determine this distance between two segmentations as the sum of the distances of each boundary pixel in one segmentation to the closest boundary pixel in the other, which is easily computable using the distance transform disttransform(s) of the boundary image s of a segmentation. Since this distance is non-symmetric, we, however, need to use it in both directions (Eq. (B.2) where W and H are the width and height of the image frame).\n$w_t^b(z_t) = \\frac{1}{d(s_t^b, z_t)}$          (B.1)\n$d(s_1, s_2) = \\sum_{x=1}^{W} \\sum_{y=1}^{H} ((s_1)_{xy} \\cdot (disttransform(s_2))_{xy} + (s_2)_{xy} \\cdot (disttransform(s_1))_{xy})$          (B.2)\nTo now determine the overall weight of a particle according to the set of all cues Zt at time t, we combine the unnormalized weights $w_t^b(z_t)$ for each cue zt as in a product, as shown in Eq. (B.3) where \u03b7 is a normalizing factor between particles. However, as the cues have different amounts of information and thus confidence, we combine with an additional exponential importance factor \u03b1z. These importance factors were set during some initial explorations on the dataset to produce good segmentations as shown in Tab. A.1.\nw(Z_t) = \\frac{1}{\\eta} \\prod_{z_t \\in Z_t}((w_t^b(z_t))^{\\alpha_z})      (B.3)"}, {"title": "Appendix B.2. Matching segmentation IDs for consistency over time", "content": "We obtain a single segmentation from the particle set during each iteration to inform saccadic decision-making, as we have described in Sec. 2.1.1."}, {"title": "To keep the IDs of this segmentation consistent, we use a variation of the Hungarian algorithm (Hopcroft and Karp, 1973) to match object IDs between object segmentations. To do so, we must determine the matching weights $w_m(m_1, m_2)$ between the mask m1 of an object in one segmentation and the mask m2 in another. We use the well-established Intersection over Union IOU(m1, m2) metric to measure their overlap:", "content": "$IOU(m_1, m_2) = \\frac{m_1 \\cap m_2}{m_1 \\cup m_2}$      (B.4)\nBut if we only consider these overlaps between the current and last segmentation, some object IDs will get lost due to perceptual uncertainty. Hence, we consider the last ten segmentations, but discount them with the factor \u03b2. We compute resulting matching weights wm(m1, m2) that the mask m1 in the current segmentation should have the same ID as the mask m2 in each of the last T = 10 segmentations following\n$w_m(m_1, m_2) = \\sum_{t=0}^{T} (IOU(m_1, m_{2,(T-t)}) \\cdot \\beta^{(T-t)}$        (B.5)\nand then match the IDs using maximum weight full matching in bipartite graphs (Jonker and Volgenant, 1987), allowing for new IDs if no existing ID can be matched."}, {"title": "Appendix C. Videos of human and model scanpaths", "content": "The visualizations of our model parts shown in Figs. 1 to 4 can be seen as downloadable videos for 10 different simulated scanpaths on that input sequence. For comparison, we also show the scanpaths of 10 human partici-pants on the same input sequence."}, {"title": "Appendix D. Extended models: Details and statistics", "content": null}]}