{"title": "A Robotics-Inspired Scanpath Model Reveals the Importance of Uncertainty and Semantic Object Cues for Gaze Guidance in Dynamic Scenes", "authors": ["Vito Mengersa,c,*", "Nicolas Rotha,c,*", "Oliver Brocka,c,**", "Klaus Obermayera,c,**", "Martin Rolfsb,c,**"], "abstract": "How we perceive objects around us depends on what we actively attend to, yet our eye movements depend on the perceived objects. Still, object segmentation and gaze behavior are typically treated as two independent processes. Drawing on an information processing pattern from robotics, we present a mechanistic model that simulates these processes for dynamic real-world scenes. Our image-computable model uses the current scene segmentation for object-based saccadic decision-making while using the foveated object to refine its scene segmentation recursively. To model this refinement, we use a Bayesian filter, which also provides an uncertainty estimate for the segmentation that we use to guide active scene exploration. We demonstrate that this model closely resembles observers' free viewing behavior, measured by scanpath statistics, including foveation duration and saccade amplitude distributions used for parameter fitting and higher-level statistics not used for fitting. These include how object detections, inspections, and returns are balanced and a delay of returning saccades without an explicit implementation of such temporal inhibition of return. Extensive simulations and ablation studies show that uncertainty promotes balanced exploration and that semantic object cues are crucial to form the perceptual units used in object-based attention. Moreover, we show how our model's modular design allows for extensions, such as incorporating saccadic momentum or pre-saccadic attention, to further align its output with human scanpaths.", "sections": [{"title": "1. Introduction", "content": "Humans actively move their eyes to pay attention to individual parts of their environment. Historically, models characterized visual attention as a spotlight, enhancing processing in attended spatial regions (Posner, 1980; Treisman and Gelade, 1980) and selecting locations based on a saliency map (Koch and Ullman, 1985; Itti and Koch, 2001). There is, however, mounting evidence of psychophysical studies suggesting that attention operates on discrete visual objects (for reviews, see Cavanagh et al. 2023; Scholl 2001; Peters and Kriegeskorte 2021). The hypothesis that objects guide gaze behavior in dynamic real-world scenes has recently gained additional traction through a computational modeling approach (Roth et al., 2023) in which we demonstrated that object-based attention and selection leads to more human-like exploration behavior than space-based models. But how do these object representations arise before being actively attended? In this work, we present an object-based scanpath model that reproduces human viewing behavior in dynamic scenes and in which the segmentation of the scene in perceptual units and the simulation of saccadic decisions are treated as interdependent processes.\nPrevious models of human exploration behavior were either restricted to static scenes (Itti et al., 1998; K\u00fcmmerer et al., 2022; Schwetlick et al., 2022), only captured the average fixation density instead of simulating individual scanpaths (Droste et al., 2020; Molin et al., 2015), or required explicitly provided object segmentations (Roth et al., 2023). To describe what object-based attention can act on, \"proto-objects\u201d were introduced as pre-attentive volatile units that can be accessed and further shaped by selective attention (Rensink, 2000). Walther and Koch (2006) proposed a model that generates such proto-objects for static scenes based on salient regions in terms of color, edges, and luminance. On the other hand, evidence from unconscious object representations and unconscious object-based attention (cf. Chou and Yeh 2012; Tanaka et al. 1991) indicates pre-attentive targets of object-based selection do not need to be refined when attended, but are fully formed (Ca-"}, {"title": "2. Materials and methods", "content": "We propose a model for the two processes of saccade target selection and object segmentation in natural scenes. \u03a4o establish an active interconnection"}, {"title": "2.1. A model for interdependent saccadic decisions and object segmentation", "content": "between them, we employ a design principle from robotics (Mart\u00edn-Mart\u00edn and Brock, 2022) that focuses on bidirectional interactions between components. For our model, this means that we implement both saccade target selection and object segmentation as components that require the other's current state as input, as shown in Fig. 1. Critically, we consider the uncertainty of current segmentation to weigh different segmentation measurements. This segmentation uncertainty is also an input to our saccade target selection, as studies of eye movements in natural environments have shown that uncertainty about the state of the visual environment is important to understand and predict gaze behavior (Hayhoe and Matthis, 2018; Gottlieb et al., 2013). In the following, we explain how each component models the respective process based on the visual input and the other's current state. We start with the component for object segmentation, which we adapted from our previous work in robotic perception (Mengers et al., 2023) to account for object information at the current gaze position and top-down semantic information. Then we explain how we modified our previous model for the"}, {"title": "2.1.1. Estimating object segmentation and its uncertainty", "content": "The object segmentation of a scene is inherently uncertain because the visual input is ambiguous, and what is considered to be an object might change over time, depending on the scanpath. Therefore, we aim not only to estimate the object segmentation but also to explicitly estimate the current uncertainty over it. To do so, we combine multiple cues for object segmentation as measurements in a recursive Bayesian filter (S\u00e4rkk\u00e4, 2013). This filter updates the object segmentation with each new measurement while also estimating its uncertainty, similar to the use of the segmentation filter in previous work on object segmentation for robotics (Mengers et al., 2023). As shown in Fig. 2 on the left, we consider three measurements of pre-attentive global segmentation based on motion, appearance, and semantics, as well as segmentation of only the locally attended object. This attentive segmentation is particularly important because it has higher confidence, thereby reducing segmentation uncertainty dependent on the current gaze. This is one direction of the strong interaction between object segmentation and saccadic decision-making in our model. We now first describe how we obtain the different measurements of object segmentation, before explaining how we combine them in a Bayesian way using a particle filter to estimate both segmentation and its uncertainty.\nCues for the current object segmentation. We aim to design a directly image-computable model and thus rely only on the RGB video as input for pre-attentive global segmentation. We extract three object segmentation cues from it: low-level appearance, higher-level semantic features, and common motion. For the appearance segmentation, we use the simple graph-based method by Felzenszwalb and Huttenlocher (2004) because it already provides reliable regions of common appearance. For the semantic segmentation, we face a more complex problem, for which we leverage recent advances in large, data-driven segmentation models (Kirillov et al., 2023). Concretely, we obtain a semantic segmentation using the method by Ke et al. (2023). To find common motion in the scene, we first quantify motion as optical flow between subsequent frames using a state-of-the-art, data-driven method (Shi et al., 2023). In that flow, we then find parts that move together by applying the same graph-based method (Felzenszwalb and Huttenlocher, 2004) as for appearance since it proves to be sufficiently reliable.\nMoreover, we use the current gaze location to inform object segmentation because gazing at an object should afford higher-confidence measurements of its boundaries (Henderson, 2003). To model such precise measurements around the currently attended object, we use a large data-driven segmentation model (Zhao et al., 2023) that can develop a prompted segmentation around a provided point. If we provide it with the current gaze location, we obtain its highest confidence object that contains this point. To further increase the quality of this prompted high-confidence segmentation, we perform it on the highest available resolution of the input image, which we downsample for other cues (see Tab. A.1). We use the prompted segmentation as an additional input to our filter for object segmentation. Since the current gaze location is a result of the previous saccadic decision process, this represents the connection of the two components in one direction. We explain the other, richer direction in Sec. 2.1.2 Scanpath simulation, but now"}, {"title": "2.1.2. Scanpath simulation", "content": "We model the saccadic decision-making process by adapting the object-based Scanpath simulation in Dynamic scenes (ScanDy) framework (Roth et al., 2023). The scanpath simulation updates its internal state, which includes a decision variable for all potential target objects in the scene, as segmented through the particle filter (Fig. 2). We model the target selection process of where and when to move the gaze position with a drift-diffusion model (DDM), in which each object represents a potential saccade target. The decision variable for each object depends on its eccentricity given the current gaze position, how relevant the visual scene features are as measured by salience, and the uncertainty of the local object boundaries as provided by the segmentation particle filter. Notably, the model does not rely on an explicit implementation of the \"inhibition of return\u201d (IOR) mechanism.\nScene relevance based on salient features. We quantify the relevance of the scene content for gaze behavior by computing frame-wise feature maps F. Since we model free-viewing gaze behavior, where the observers have no explicit task, we approximate the relevance of different parts of the scene through visual saliency. We used the video saliency model UNISAL (Droste et al., 2020), which was jointly trained on both image and video visual saliency datasets, since it is lightweight and produces state-of-the-art results on the DHF1K Benchmark (Wang et al., 2018). We inferred the video saliency maps using the model with the domain adaptation for the DHF1K video dataset, which is most similar to the videos used in this study. The resulting video saliency predictions used as frame-wise feature maps F(x, y) are normalized to [0,1]. Fis typically strongly localized around the most salient object (cf. Droste et al., 2020). To allow the model to rely less on this strongly focused map, we introduce a model parameter fmin that linearly scales F to F' \u2208 [fmin, 1]. By reducing the effective value range, a higher fmin parameter decreases the influence of the salience on the saccadic decision-making process.\nGaze-dependent visual sensitivity. The foveation of the human visual system leads to a decrease in visual sensitivity with eccentricity from the current gaze position. As in Roth et al. (2023), we model the visual sensitivity S across the scene with an isotropic Gaussian Gs = $\\frac{1}{2\\pi\\sigma^2} exp(-\\frac{(x-xo)^2+(y-yo)^2}{2\\sigma^3})$, where the standard deviation os = 7 dva is set according to similar models (cf. Roth et al., 2023; Schwetlick et al., 2020) and based on preliminary model explorations. We account for the well-documented object-based attentional benefit (Egly et al., 1994; Scholl, 2001; Malcolm and Shomstein, 2015) by approximating the covert spread of attention across the currently foveated object with a uniform sensitivity, replacing the part of Gs that falls within the currently foveated object.\nIn addition, we implemented two possible model extensions, which are not part of the base model but can be incorporated into the visual sensitivity S, namely saccadic momentum and pre-saccadic attention. In our explicit implementation of saccadic momentum, we increase the visual sensitivity in the direction of the previous saccade by generating an angle preference map based on the current gaze position and the angle of the previous saccade. We set a maximal value, which will be the sensitivity value in the direction of the previous saccade angle, that decreases linearly with the angle within a specified angle range to a minimum value. The resulting map (see Fig. D.11a) is then multiplied with S. In our implementation of pre-saccadic attention, we not only assume a uniform spread of visual sensitivity across the currently foveated object but also objects that are likely to be the next saccade target (see Fig. D.11b).\nUncertainty over object segmentation. The visual system integrates different sources of information into a coherent visual representation of the environment (Milner and Goodale, 2006). If an object moves or different input sources, for example, the appearance and the motion-based segmentation find different object boundaries, this leads to a disagreement between instances in the segmentation particle filter. We include the resulting uncertainty over the object segmentation as the third contributing factor for the saccadic decision-making process, in addition to the relevance of the scene features and the gaze-dependent visual sensitivity. The uncertainty measure is directly obtained from the entropy H(x, y) of the previously thresholded boundary likelihood in the object segmentation particle filter (see Eq. (4)). We smooth the resulting map with a Gaussian blur, so uncertainties at the object boundaries are attributed to both objects. The values in the uncer-"}, {"title": "2.2. Dataset", "content": "We compared the simulated scanpaths with human eye-tracking data recorded under free-viewing conditions on videos of natural scenes. We recorded eye-tracking data from 10 participants on 43 video clips from the Unidentified Video Objects (UVO, Wang et al., 2021) dataset (10 used for parameter tuning, 33 used for testing the model; randomly split). The videos were selected to show diverse content and to have temporally consistent, densely annotated object masks for the first 90 frames (cf. Wang et al., 2021). We recorded eye-tracking data for the here-used videos with an Eyelink 1000+ tabletop system (SR Research, Osgoode, ON, Canada) with a 1000 Hz sampling rate, as part of an ongoing collaborative large-scale eye movement database (publication of full dataset in preparation). We presented the"}, {"title": "2.3. Metrics and parameter fitting", "content": "We determined the model parameters by comparing the foveation duration and saccade amplitude distributions of the simulated scanpath with the human ground truth. We measured the similarity between a simulated distribution N to the ground truth M using the two-sample Kolmogorov-Smirnov (KS) statistic D = $sup_{n}|N(x) \u2013 M(x)|$. We systematically varied the DDM noise level s, the decision threshold 0, and the relative importance of the feature map F' and uncertainty map U', quantified by the re-scaling parameters fmin and Umin. We performed a coarse grid search in this four-dimensional parameter space on the 10 videos in the training set. We simulated five different scanpaths (different random seeds) for each parameter configuration and compared them. Since we are particularly interested in the effect of uncertainty on the simulated scanpaths, we refined the grid search for each Umin value around the parameter sets leading to the lowest mean of the KS statistics for the foveation duration DFD and the saccade amplitude DsA. We present an overview of all fixed and fitted model parameters and the parameter grid in Appendix A.\nWith the model parameters chosen such that the basic scanpath summary statistics of foveation durations and saccade amplitudes matched the human data, we evaluated the simulated scanpaths out-of-domain on the test set, i.e., on 33 previously unseen videos and on different metrics than what the parameters on the training set were selected for. For each parameter set, we simulated 10 scanpaths and compared them with the data from the 10 human observers. We focused on the analysis of how the gaze behavior balances the exploration of the background of a scene (Background), uncover an object for the first time for foveal processing (Detection), explore further details of the currently foveated object by making a within-object saccade (Inspection), or return to a previously uncovered object (Return) (Linka and de Haas, 2021; Roth et al., 2023). Comparing the foveation durations in each category provides an insightful metric of the exploration behavior, which is particularly suited for dynamic scenes (see Roth et al., 2023). We used this metric in the training set to choose the later described base model among different uncertainty values umin (see Appendix A for more details).\nSince our model does not have an explicit IOR mechanism, we were particularly interested in whether it could reproduce typical IOR effects. IOR describes the inhibition of recently attended stimuli and the resulting delayed response to them (Posner et al., 1984; Klein, 2000). In a free-viewing condition, as in the data used for this study, we therefore expect that saccades"}, {"title": "3. Results", "content": "Our aim was to build an image-computable and mechanistic computational model that closely resembles human gaze behavior in dynamic real-world scenes. In this section, we compare our model with human scanpaths, first qualitatively in Sec. 3.1 and then quantitatively by reviewing aggregated statistics in Sec. 3.2. We systematically explore the influence of uncertainty on visual exploration behavior in Sec. 3.3. In an additional ablation study, we probed the impact of individual object representations as input sources and the importance of the interaction between object perception and saccadic decision-making for the simulated scanpaths, as described in Sec. 3.4. Lastly, we show how our model can be easily extended with additional modules, such as saccadic momentum or pre-saccadic attention, leading to more human-like saccade angle statistics and improvements in early object detections (Sec. 3.5)."}, {"title": "3.1. Qualitative scanpath analysis", "content": "Our model produces scanpaths that closely resemble human visual exploration behavior, as shown in Fig. 4. The mechanistic nature of our model makes the individual saccadic decisions transparent and interpretable: Initially, the model has high uncertainty over the whole scene, which is reduced through large saccades towards unexplored objects (Fig. 4a). Objects with particularly high saliency are likely to be revisited (Fig. 4b) or are further inspected (Fig. 4c-d). Return saccades to previously foveated objects also become more likely with time, as uncertainty over object boundaries can rise again, for example, through object motion. This qualitatively similar behavior of our model can also be seen in Appendix C, where we show the exact"}, {"title": "3.2. Aggregated scanpath statistics", "content": "We compare our base model predictions to human scanpaths on a set of videos not used for parameter search. As described before, we selected the model parameters to resemble the statistics of human foveation duration and saccade amplitude on ten videos. The model generalizes well to the previously unseen set of 33 videos, as shown based on the aggregated scanpath statistics in Fig. 5. Similar to human eye-tracking data, the foveation durations (Fig. 5a) of the simulated scanpaths follow a log-normal distribution with a mean of 390 ms and a median of 332 ms (humans: mean of 433 ms and median of 316 ms). The distribution of the model is more narrow compared to humans, which if other metrics would not be considered could be fixed by increasing both the decision threshold and the noise level in the drift-diffusion model, as described in Sec. 2.1.2. The saccade amplitudes in the simulated scanpaths (Fig. 5b) follow the gamma distribution of the human data with a mean of 3.70 dva and a median of 2.81 dva (humans: mean of 3.40 dva and median of 2.90 dva). These well-described statistics are not explicitly implemented in the model, but emerge from model con-"}, {"title": "3.3. Model ablation 1: Uncertainty drives exploration", "content": "Our model uses the uncertainty measure of the object segmentation module as an estimate for the perceptual uncertainty that influences the gaze behavior depending on the scanpath history. Here, we evaluate the effect of this uncertainty mechanism, comparing the simulated model scanpaths with a varying influence of the uncertainty on the saccadic decision-making process. Specifically, we vary the umin parameter of the model where a higher value decreases the importance of uncertainty. We also compare results from"}, {"title": "3.4. Model ablation 2: Semantic object cues and component interconnections form suitable perceptual units", "content": "Our model updates both the current object segmentation and perceptual uncertainty from the current image of the scene using different object cues. The segmentation then defines the perceptual units in saccadic decision-making, and the uncertainty influences the likelihood of selecting these perceptual units. In this ablation, we investigated to what extent different object"}, {"title": "3.5. Model extensions: Saccadic momentum improves saccade angle statistic and pre-saccadic attention benefits early object detections", "content": "We have shown so far that our model reproduces important hallmarks of scanpaths in dynamic real-world scenes. One instructive metric we have not yet investigated is the distribution of relative saccade angles. Importantly,"}, {"title": "4. Discussion", "content": "We presented a model for object-based attention and gaze behavior in complex dynamic scenes that builds on a previous model for saccadic decision-making (Roth et al., 2023) and an object segmentation model for interactive perception in robotics (Mengers et al., 2023). The active interconnection between the two model components resembles an algorithmic information processing pattern from robotics, Active InterCONnect (AICON; see Battaje et al., 2024). We examine this resemblance further in Sec. 4.4, but first discuss the results of our study and the limitations of our approach (Sec. 4.1), the conclusions we can draw about uncertainty as a driving factor for visual exploration (Sec. 4.2), and what we can learn from the model about the perceptual units of attention (Sec. 4.3)."}, {"title": "4.1. Advantages and limitations of our model", "content": "Our model still has many of the simplifications of our previous framework for Scanpath simulation in Dynamic scenes (ScanDy) (Roth et al., 2023). Importantly, we assume that attention spreads instantaneously and uniformly across objects and that saccades are always precisely executed without attempting to model the saccade programming and oculomotor control. Due to the modular implementation of the model, we expect adding such components to be a straightforward task. The current extensions of saccadic momentum and pre-saccadic attention both only required the addition of a few lines of code.\nSo far, we only model scanpaths during free-viewing, that is, where the observers had no task instructions. In the future, we plan to apply the same modeling approach to simulate scanpaths in complex dynamic scenes during visual search and scene memorization tasks. We expect that additional top-down attentional control during these tasks can be incorporated into the modeling by adapting the feature map F (see Sec. 2.1.2, F currently represents only visual saliency) and tuning the model parameters. For example, we would expect already reasonably simulated scanpaths for scene exploration through a down-scaling of the importance of F through fmin and visual search through the inclusion of a target similarity map in F'. In both cases, the threshold of the drift-diffusion model (DDM) \u03b8 should be lowered to account for shorter foveation durations under the task conditions.\nThe important improvement over the ScanDy framework is the active interconnection with object segmentation. Through this interaction, the model becomes image-computable, that is, we do not have to define what constitutes an object a priori, but the object representations change based on the scanpath. The implementation of the object segmentation as a recursive Bayesian filter leads to a serial dependence of the segmentation, using both prior and present object information to represent the scene (Fischer and Whitney, 2014). Furthermore, the segmentation module automatically provides us with an uncertainty map, which depends on the prior and present gaze position. We show that through the automatic reduction of uncertainty as a consequence of saccadic decisions, this uncertainty map is well suited to drive saccadic exploration behavior.\nImportantly, when we say we have a mechanistic model, we refer to attentional mechanisms and explicitly not to how they are implemented in the brain on a neural level. Although there is evidence for Bayesian updating in the brain (Knill and Pouget, 2004; Ma et al., 2006), even in the form of a neural particle filter (Kutschireiter et al., 2017), we want to argue more conceptually for principled ways of information processing, independently of their implementation in the brain. For example, there is evidence for bidirectional information exchange between different components of perceptual processing, similar to the exchanges between our components for object segmentation and saccadic decision-making. Such exchanges have both been observed between different hierarchical levels of processing (Ahissar and Hochstein, 2004), but also laterally between the processing of different cues (Livingstone and Hubel, 1988) or even separate sensory modalities (McGurk and MacDonald, 1976). At the same time, we recursively update the segmentation in the object component and the evidence in the saccadic decision component, leveraging temporal coherence of the perceptual problem similar to what can be found in visual search (Niemi and N\u00e4\u00e4t\u00e4nen, 1981; Kristj\u00e1nsson et al., 2010) and object perception (Liu, 2008; Blake and Yang, 1997).\nThe modular and mechanistic design of the model allows us to explore essential hypotheses about attention and gaze behavior in dynamic scenes areas that are challenging to test experimentally. By studying the model's behavior, we can generate hypotheses that can later be tested in eye-tracking"}, {"title": "4.2. Uncertainty drives exploration", "content": "The connection between active exploration behavior and the reduction of perceived uncertainty of the environment is well established in the literature (Sullivan et al., 2012; Friston et al., 2012; Renninger et al., 2007). Gottlieb et al. (2013) summarized that \"information-seeking obeys the imperative to reduce uncertainty and can be extrinsically or intrinsically motivated\u201d (p. 586) and that \"the key questions we have to address when studying exploration and information-seeking pertain to the ways in which observers handle their own epistemic states, and specifically, how observers estimate their own uncertainty and find strategies that reduce that uncertainty\" (p. 586). It is, however, not obvious how uncertainty should be measured and quantified in an image-computable model of visual attention.\nIn this context, it is important to clarify what we mean by uncertainty since the term can refer to many things. Our model specifically considers the uncertainty of the boundary between potential objects, both about their existence and exact location, but not about the object's identity or other possible features. For example, if an object in the periphery moves, this typically would increase the uncertainty estimate in our model. One could argue that the additional motion cue should reduce the uncertainty about the shape of the object. Indeed, this intuition is reflected in our model since the input from the motion segmentation will clearly show the object. However, the overall uncertainty of the object might still increase because the exact position, shape, or state of the moving object might change, which would be reflected in conflicting object measurements from different sources or in a strong deviation from the prior belief. This prior is calculated as the segmentation of the previous frame, shifted by the optical flow.\nOur results show that this uncertainty map of the object segmentation module leads to human-like simulated scanpaths. The weight of the uncertainty map for the decision-making process, parameterized through Umin, strongly influences the ratio between foveation categories, in particular, the frequency with which objects are inspected. The prompted high-confidence object segmentation typically leads to a low uncertainty at the current position, encouraging further exploration of the scene for a strong influence of uncertainty (low Umin). If the influence is weak (high Umin), the gaze-dependent spread of attention leads to a strong tendency to further inspect objects with high salience. Interestingly, the umin parameter also influences the strength of the temporal IOR effect.\nMost mechanistic scanpath models require an explicit implementation of IOR (cf. Itti et al., 1998; Zelinsky, 2008; Schwetlick et al., 2020; Roth et al., 2023) to avoid being bound to the objects or locations with the highest salience (Itti and Koch, 2001). Our model takes a different approach, similar to previous computational models that have incorporated uncertainty-based strategies, where exploration is driven by high variance or entropy (Cohn et al., 1996; Rothkopf and Ballard, 2010). It is closely related to the principle of information maximization, which has been applied before to simulate eye movements in static scenes (Renninger et al., 2004; Lee and Yu, 1999; Wang et al., 2011). Where our model is uncertain is also closely related to \"Bayesian surprise\", which was introduced by Itti and Baldi (2009) in the context of scanpaths as a measure for how the data obtained from eye movements affects differences between posterior and prior beliefs of an observer about the world. These models also do not require an explicit IOR implementation, since there is little information to be gained by revisiting already foveated parts of the scene. However, when observing dynamic real-world scenes, further inspections and returns are frequent, and defining an information maximization or uncertainty-driven approach that can account for this behavior is not trivial. In our model, we do not need a separate estimation of the uncertainty, since it is a natural by-product of the \"AICON\u201d -ic way in which we obtain the object segmentation."}, {"title": "4.3. Perceptual units for object-based attention", "content": "Object-based attention is a well-established concept that has been thoroughly investigated in a large variety of experimental paradigms (Scholl, 2001; Peters and Kriegeskorte, 2021; Cavanagh et al., 2023). even after decades of debate, however, it remains an option question what constitutes a visual object (Spelke, 1990; Feldman, 2003; Palmeri and Gauthier, 2004; Scholl et al., 2001; Cavanagh et al., 2023). Our model allows us to systematically vary the object cues used for the formation of the scene segmentation, which defines the perceptual units on which our object-based attentional selection process operates. Under the assumption that our implementation of saccadic decision-making mechanisms is similar to the human visual system, we expect that the object cues that lead to more human-like scanpaths are also the cues primarily used for saccadic decision-making in humans.\nOur results suggest that attentional guidance primarily relies on semantic object cues. Only models that used the semantic cues both for the global and the prompted scene segmentation showed the IOR effect and could reproduce the balance between foveation categories seen in humans (cf. Fig. 7). This result is consistent with evidence for global semantic understanding of natural scenes (Neri, 2017) and the literature on high-level unconscious processing (reviewed in Cavanagh et al., 2023). As expected, the model scanpaths also became less human-like if we replaced the prompted segmentation at the gaze position with an appearance-based, low-level object cue (all-g & ll-p). This model corresponds to the assumption, that a foveated object would get more finely segmented (e.g., a t-shirt, which was previously part of a person, becomes its own object when foveated). However, we did not see evidence for this, and removing low-level object cues did not impact the simulated scanpath statistics in any major way. There is, of course, ample evidence for the brain using appearance- and motion-based object cues to segment complex dynamic scenes (Schyns and Oliva, 1994; Reppas et al., 1997; Von der Heydt, 2015). Based on our results, however, we argue that"}, {"title": "4.4. Employing an information processing pattern from robotics", "content": "Although originally developed for robotic perception systems (Mart\u00edn-Mart\u00edn and Brock, 2022), the information processing pattern AICON has already been successfully applied to model parts of human visual information processing, reproducing certain visual illusions (Battaje et al., 2024). Our results show further evidence that AICON serves as a useful pattern in the study of information processing, be it in humans or robots. This suggests that there are similarities between the problems faced in both and that AICON provides a structure simplifying their solution. We believe our model showcases two such ways in which the problems are similar: subproblems that depend on one another, as well as ambiguity and noise in the sensory input.\nThe interdependent subproblems here are saccadic decision-making and object segmentation, while in robots, different sensor streams such as force and visual measurements often depend on each other and also on the currently executed action. AICON's focus on rich, bidirectional interactions (active interconnections) between different components provides a direct response to this interdependence. The ambiguity and noise of sensory input, be it of sensory neurons in the eye or the pixels of a camera, is inherent to the real world as opposed to simulated agents. The answer for robotic systems is usually modeling the resulting uncertainty to inform action selection, and indeed here we also find modeling uncertainty useful to drive the exploration towards more uncertain areas.\nSince the AICON pattern comes from robotics, it should also be possible to leverage the results of this study to build capable robotic systems. What we use here as a model for active human vision can easily be transformed into a controller for active robotic vision. Such (inter-)active perception systems are a current research interest in robotics (Bohg et al., 2017; Bajcsy et al., 2018), and further integration with other robotic processing components might drive further insight given the bidirectional interaction between components following the AICON pattern. A robot that interacts with its environment might have very different relevant uncertainties compared to our uncertainty over object segmentation. These could drive exploration in interesting ways, while other ways of resolving uncertainty become available: If the segmentation of two objects is unclear, the easiest way to determine it is to physically interact and separate them. Through such insights, further integration might be beneficial for both building better robots and understanding more about the exploration behavior in humans.\nSimilarly to the application of AICON in this work, it has already been a useful pattern when modeling information processing in the completely different domain of collectives (Mengers et al., 2024). This further suggests that maybe information processing indeed faces similar challenges across different domains when biological organisms or synthetic systems behave in the real world: High-dimensional, ambiguous sensory input needs to be translated into suitable actions in a dynamically changing environment. We believe that the algorithmic pattern of AICON fits these challenges, allowing us to further study the information processing that underlies different behaviors. To achieve this, interdisciplinary exchanges similar to the exchanges between vision science and robotics here will be necessary and might lead to adjustments of the pattern. Currently, however, we consider it an early prototype of what might be a more unified understanding of the driving factors behind behavior."}, {"title": "5. Conclusion", "content": "We developed and evaluated a model for object-based attention and gaze behavior in real-world dynamic scenes. By integrating saccadic decision-making mechanisms with an object segmentation framework, our model successfully simulates human-like scanpaths. This integration, an implementation of the Active InterCONnect (AICON) information processing pattern from robotics, enables the model to progressively refine its object segmentation through active exploration, while uncertainty over that segmentation guides the scanpath."}, {"title": "Appendix A. Parameter exploration", "content": "We find appropriate parameter values through extensive grid searches in a four-dimensional parameter space, as described in Sec. 2.3. To make the computational cost of the grid search feasible, we fix all parameters except for the decision threshold 0 for the drift-diffusion model (DDM), the DDM"}]}