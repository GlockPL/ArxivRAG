{"title": "Enhancing and Accelerating Diffusion-Based Inverse Problem Solving through Measurements Optimization", "authors": ["Tianyu Chen", "Zhendong Wang", "Mingyuan Zhou"], "abstract": "Diffusion models have recently demonstrated notable success in solving inverse problems. However, current diffusion model-based solutions typically require a large number of function evaluations (NFEs) to generate high-quality images conditioned on measurements, as they incorporate only limited information at each step. To accelerate the diffusion-based inverse problem-solving process, we introduce Measurements Optimization (MO), a more efficient plug-and-play module for integrating measurement information at each step of the inverse problem-solving process. This method is comprehensively evaluated across eight diverse linear and nonlinear tasks on the FFHQ and ImageNet datasets. By using MO, we establish state-of-the-art (SOTA) performance across multiple tasks, with key advantages: (1) it operates with no more than 100 NFEs, with phase retrieval on ImageNet being the sole exception; (2) it achieves SOTA or near-SOTA results even at low NFE counts; and (3) it can be seamlessly integrated into existing diffusion model-based solutions for inverse problems, such as DPS [5] and Red-diff [15]. For example, DPS-MO attains a peak signal-to-noise ratio (PSNR) of 28.71 dB on the FFHQ 256 dataset for high dynamic range imaging, setting a new SOTA benchmark with only 100 NFEs, whereas current methods require between 1000 and 4000 NFEs for comparable performance.", "sections": [{"title": "1. Introduction", "content": "Inverse problems, which aim to recover the true signal xo from a corrupted and noisy measurement y, have broad applications across various fields, including image restoration [5, 12, 15, 23, 28, 30] and physics [1, 19]. The measurement process often functions as a many-to-one mapping from xo to y, resulting in non-unique solutions. One approach to addressing inverse problems is through a Bayesian framework, where we aim to sample xo from the posterior p(xoly) \u03b1 p(y xo)p(xo), with p(yxo) representing the known mea-"}, {"title": "2. Background", "content": "Since this paper is focused on diffusion-based solutions for inverse problems, we begin by introducing some preliminary knowledge on diffusion-based generative models and inverse problems in this section. We also briefly discuss why previous solutions require high NFEs and face difficulties in solving nonlinear tasks."}, {"title": "2.1. Diffusion Models", "content": "Diffusion models generate images by initially sampling from Gaussian noise and then applying a trained score function to solve a reverse stochastic differential equation (SDE) or ordinary differential equation (ODE) to produce images. Using the notation from EDM [11], we define a forward process that adds Gaussian noise to a clean image xo via a perturbation kernel: xt ~ N(s(t)xo, s(t)2o(t)2I), where s(t) and o(t) are predefined time-dependent scaling and variance functions. This forward process corresponds to a reverse ODE with the same marginal distribution p(xt) for all t as the forward process, given by"}, {"title": "3. Inverse Problem under the Bayesian Frame-work", "content": "The inverse problem aims to recover the true signal xo from a corrupted and noisy measurement y. Formally, we assume the measurement y is generated by the process"}, {"title": "4. Measurements Optimization", "content": "Existing methods for solving inverse problems typically take a single gradient step in ||y-A(x)||2 alongside one diffusion NFE. However, unlike text-conditional image generation, the measurement y contains more detailed information. Thus, for each NFE in the diffusion model, it is advantageous to consider how to extract more information from y and incorporate it into the diffusion sampling steps.\nWe now address two key aspects: (1) how to solve the optimization problem ||y \u2013 A(xo)|| when A can be highly non-convex, and (2) even if we obtain a good solution 20 by solving this optimization, how to effectively integrate it into the diffusion-based image generation framework. We will answer these questions in the following two paragraphs and then present the detailed algorithm based on this approach.\nStochastic Gradient Langevin Dynamics (SGLD) as an Optimization Method. To estimate xo from ||y - A(xo)||2, we choose Stochastic Gradient Langevin Dynamics (SGLD) [29] as the optimization method. Since the forward operator can be highly non-convex and ill-posed, SGLD is well-suited for finding solutions in such non-convex problems [18, 29]. In summary, we update x using"}, {"title": "5. Experiments", "content": "In this section, we compare our proposed method against recent SOTA methods for solving inverse problems across various tasks, including 5 linear and 3 non-linear tasks."}, {"title": "5.1. Experiment Setup", "content": "Datasets, Checkpoints, and Metrics. We use pretrained diffusion models for the FFHQ dataset [10] at 256 \u00d7 256 res-"}, {"title": "5.3. Performace of Red-diff-MO", "content": "In the main tables, we report the performance of DPS-MO as described in Algorithm 2. Additionally, we present the empirical results of our MO module integrated into the Red-diff framework, as outlined in Algorithm 3. Although Red-diff-MO does not perform as well as DPS-MO, it still achieves a substantial performance increase in nonlinear tasks with fewer NFEs. We show the performance of Red-diff-MO on phase retrieval and HDR tasks on the FFHQ dataset in Table 5. With only 100 NFEs, Red-diff-MO not only significantly boosts performance on nonlinear tasks but also requires 10 times fewer NFEs compared to Red-diff."}, {"title": "5.4. Ablation Study", "content": "Sampling Schedule Choice. Since our method is orthogonal to the choice of sampling schedule, we can choose"}, {"title": "Memory Requirement and Time Efficiency", "content": "To demonstrate the memory efficiency of our algorithm, we use the command nvidia-smi to monitor memory consumption while solving an inverse problem, in comparison to our baselines. We present the memory usage for the Inpainting task on the FFHQ dataset in Table 7. Although DPS-MO"}, {"title": "6. Conclusion", "content": "We propose the Measurement Optimization module, which offers a powerful and efficient approach to solve inverse problems with diffusion models. By iteratively combining SGLD with querying the diffusion prior, MO effectively incorporates measurement information while ensuring solutions remain on the valid data manifold. We integrated MO into both DPS and Red-diff, producing new algorithms termed DPS-MO and Red-diff-MO. Our comprehensive evaluations demonstrate that DPS-MO establishes new SOTA results across various tasks, requiring only 50-100 NFEs for most tasks. Although Red-diff-MO does not achieve SOTA, the MO module significantly boosts Red-diff's performance on nonlinear tasks, with only 100 NFEs. Our method not only minimizes memory usage but also reduces wall time for solving inverse problems.\nOne potential limitation of MO is that if the forward operator A is non-differentiable, such as in the case of a black-box operator, or if A requires a long computation time for each gradient step, our method could become time-consuming. Addressing how to handle non-differentiable, black-box A will be an interesting direction for future investigation."}, {"title": "7. Complete Proof of Theorem 1", "content": "The proof is adopted from Section B.3 of EDM [11]. We include the proof here for the completeness."}, {"title": "8. Experiment Details", "content": "We use the exactly the same task configuration as in [30] where you can find in Section D.1 in [30]."}, {"title": "8.2. Baseline Implementation", "content": "The baselines results are reported by [30]. Since we using the same configuration and dataset as [30] for each task, we report the performance of baseline methods directly from [30]."}, {"title": "8.3. Hyperparameters", "content": "In this section, we present the hyperparameters used to achieve the results in the main tables: Table 1, Table 2, Table 3, and Table 4. The hyperparameters are divided into two groups: one for the diffusion sampling schedule and the other for SGLD.\nFor the diffusion sampling schedule, we use the EDM schedule. More details can be found in Appendix 8.4. For"}, {"title": "8.4. DPS-MO Implementation Details", "content": "In this section, we introduce the EDM schedule, where s(t) = 1 Vt and o(t) = t, with t discretized as follows:"}, {"title": "9. SGLD Initialization", "content": "As described in Algorithm 2 and Algorithm 3, for each MO, we initialize with 20 from the previous diffusion time step. A natural question arises: what if we solve ||y \u2013 A(xo)||2 only once and reuse the solution to query the diffusion prior, including the noise addition and denoising process, for all time steps? In Section 5.4, we discuss the intuition behind solving ||y - A(x0)|| with different initializations. Here, we provide empirical results to validate this intuition."}, {"title": "10. Additional Results", "content": "We also include the empirical results for our method under 1000 and 4000 NFEs, as shown in Tables 10, 11, 12, and 13. These results indicate that increasing NFEs provides only marginal benefits for PSNR and LPIPS, and may even lead to overfitting. This suggests that our method saturates at an early stage for most tasks."}, {"title": "11. More Qualitative Samples", "content": "Here, we present qualitative samples from the FFHQ and ImageNet datasets across various tasks."}]}