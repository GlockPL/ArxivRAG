{"title": "HyQE: Ranking Contexts with Hypothetical Query Embeddings", "authors": ["Weichao Zhou", "Jiaxin Zhang", "Hilaf Hasson", "Anu Singh", "Wenchao Li"], "abstract": "In retrieval-augmented systems, context ranking techniques are commonly employed to re-order the retrieved contexts based on their relevance to a user query. A standard approach is to measure this relevance through the similarity between contexts and queries in the embedding space. However, such similarity often fails to capture the relevance. Alternatively, large language models (LLMs) have been used for ranking contexts. However, they can encounter scalability issues when the number of candidate contexts grows and the context window sizes of the LLMs remain constrained. Additionally, these approaches require fine-tuning LLMs with domain-specific data. In this work, we introduce a scalable ranking framework that combines embedding similarity and LLM capabilities without requiring LLM fine-tuning. Our framework uses a pre-trained LLM to hypothesize the user query based on the retrieved contexts and ranks the context based on the similarity between the hypothesized queries and the user query. Our framework is efficient at inference time and is compatible with many other retrieval and ranking techniques. Experimental results show that our method improves the ranking performance across multiple benchmarks. The complete code and data are available at https://github.com/zwc662/hyqe", "sections": [{"title": "Introduction", "content": "Context retrieval plays a crucial role in natural language processing (NLP). Standard techniques can efficiently extract relevant information from dedicated databases to address user queries. These techniques have been driving advancements in search engines, virtual assistants, and other retrieval-augmented systems by enabling precise, real-time responses in real time, and reducing the risk of hallucination (Ram et al., 2023; Asai et al., 2023a). \nAccurately ranking the relevance of the contexts to the user query is a crucial factor in the performance of retrieval-augmented systems (Shi et al., 2023). Classical retrieval methods such as TF-IDF and BM25 (Robertson and Zaragoza, 2009) rely on lexical similarities to rank contexts. Recent advancements in embedding models such as BERT (Kenton and Toutanova, 2019; Reimers and Gurevych, 2019) have enabled the capture of the semantic similarity between texts through dense vector representations. To improve the zero-shot performance in unseen contexts, Contriever (Izacard et al., 2021) and other successive embedding models are trained via contrastive learning techniques. However, retrieval with these embedding models focuses on similarity, but similarity alone does not always ensure that the context effectively addresses the query.\nLLMs have been incorporated to address this issue. For instance, LLM-based re-rankers (Sun et al., 2023; Pradeep et al., 2023) can determine whether a context addresses a query better than others. However, those re-rankers require fine-tuning, which demands extensive dedicated datasets and significant computational resources. Other methods include using LLM to expand the query before retrieval. HyDE (Gao et al., 2023a), for instance, utilizes an LLM to generate hypothetical contexts based on the query, subsequently retrieving concrete contexts that are close to these hypothetical contexts in the embedding space. However, the LLM must have sufficient background knowledge about the context to be retrieved so that it can generate semantically similar contexts. Otherwise, the hypothesis space of the generated contexts would be indefinitely large, and the LLM can generate outdated, irrelevant, hallucinated, and even counter-factual contexts (Brown et al., 2020; Mallen et al., 2022). We provide an example later in Fig.3(b), where GPT-3.5-turbo generates outdated information that fails to reflect recent developments on a specific topic.\nIn this paper, we propose a novel context ranking framework. Our approach uses an LLM to generate hypothetical queries based on the existing contexts. It then measures the relevance between the context and a user-given query based on the similarity between the hypothetical queries and the user-given query. While our method does not require the LLM to have prior knowledge about the query or the context, the hallucination of the LLM is restrained since a context has limited information and can only provide answers to a certain range of queries. Furthermore, while HyDE has to use an LLM to generate hypothetical contexts online for every input query, our approach allows retrieving previously generated hypothetical queries for future input queries. Our method also differs from the LLM-based re-ranker in two-fold. First, our method does not require fine-tuning an LLM. Second, our approach uses text embedding for ranking, while an LLM-based re-ranker has to call an LLM to answer the relevance between every input query and context. However, our method can be used in conjunction with other ranking methods to iteratively refine the ranking of the retrieved contexts.\nIn addition to introducing our approach, we compare our approach with existing approaches from the theoretical lens. We analyze the causality relationship between the queries and contexts within a class of context ranking approaches, identifying their potential issues, such as their susceptibility to spurious causality relationships. We then show that our approach mitigates these issues by following a variational inference approach. Our experimental results demonstrate improvements in ranking the retrieved contexts across multiple information retrieval benchmarks while maintaining efficiency and scalability. Our major contribution is listed as follows.\n\u2022 We propose to use LLMs to generate hypothetical queries and rank contexts by comparing the similarity between input queries and hypothetical queries.\n\u2022 We examine the causal relationships between queries and contexts in existing context ranking methods and develop a variational inference framework for context ranking.\n\u2022 We evaluate our method in multiple information retrieval benchmarks by combining different embedding models with different LLMs. The results show that our method can improve the ranking accuracy in most of the benchmarks."}, {"title": "Related Work", "content": "Retrieval-Augmented Systems have become a focal point in NLP research, enhancing LLMs by accessing broader knowledge bases beyond LLM context windows (Lewis et al., 2020; Gao et al., 2023b). These systems use information retrieval techniques to fetch relevant contexts from dedicated databases based on user queries, improving performance in tasks requiring extensive context (Mialon et al., 2023).\nInformation Retrieval Methods, such as TF-IDF and BM25, rely on lexical similarities to rank contexts (Robertson and Zaragoza, 2009). Recent advancements in embedding models such as BERT (Kenton and Toutanova, 2019; Reimers and Gurevych, 2019) allow capturing text semantics through dense vector representations (Asai et al., 2021). Contrastive learning techniques have further improved the zero-shot performance of embedding models such as Contriever (Izacard et al., 2021) in unseen contexts by training the models to differentiate between similar and dissimilar contexts (Gao et al., 2021).\nDocument Expansion and Query Expansion are classical techniques to improve retrieval quality and have been widely adopted in RAG systems (Wang et al., 2023). Query expansion, which dates back to (Carpineto and Romano, 2012), typically involves rewriting the query based on labels (Lavrenko and Croft, 2001). When labels are not available, the query can be expanded with generated contexts (Liu et al., 2022). For instance, HyDE (Gao et al., 2023a) uses LLMs to generate hypothetical contexts based on the input query and uses the embeddings of the query and the hypothetical contexts for retrieval. However, when the LLM lacks knowledge about the query, query expansion can be susceptible to hallucinated or counterfactual content (Brown et al., 2020).\nDocument expansion (Nogueira et al., 2019) involves appending each context with a generated query and creating indexes for the expanded context in the database. Our framework also generates queries based on the contexts but does not expand the contexts. Studies on generating high-quality queries to build synthetic datasets (Almeida and Matos, 2024) can be helpful for our framework, but that is not the focus of this paper.\nLarge Language Models (LLMs), from the small-size open source models such as Mistral-7b (Jiang et al., 2023) to the large-size proprietary mod-"}, {"title": "Background", "content": "A RAG system retrieves information from a document corpus C = {C1, C2, . . ., Ci, . . . } where each ci is a context. Assuming that Q is the whole set of user input queries, given an input query q \u2208 Q, a retriever returns a ranked list of relevant contexts from C. The ranking of those contexts can be evaluated by using Normalized Discounted Cumulative Gain (NDCG) (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002) which measures the ranking with graded relevance. After the retrieval step, one or multiple ranking procedures can be adopted to iteratively refine the quality of the ranking. We assume that each ranking procedure, including that during retrieval, uses a scoring function rq : C \u2192 R to quantify the relevance between any context c\u2208 C and the query q. We can rank the contexts with this rq, i.e., \u2200C1, C2 \u2208 C, C1 \u227a C2 \u21d4 r(q, C1) \u2264 rq(C2).\nA ranker can just target the first K contexts if the contexts have already been ordered in some previous ranking procedure. We denote the set that includes the first K contexts as Cq,K \u2286 C such that |Cq,K| = K. After ranking those contexts, a new scoring function rq over Cq,K is generated.\nWhen using an embedding model for ranking, we use the cosine similarity between query embedding and context embedding to determine the relevance of the query and context. We use E to denote the embedding model. The cosine similarity between a query q and a context c is\n$\\displaystyle sim(q, c) = \\frac{(E(q),E(c))}{||E(q)||^2||E(c)||^2}$ (1)\nAs a result, given any query q, an embedding model-based ranker's scoring function is defined as rq(c) = sim(q, c)."}, {"title": "Method", "content": "In this section, we introduce our framework for ranking contexts with hypothetical queries. We first illustrate our context ranking procedure, explain how to obtain those hypothetical queries, and then discuss its complexity.\nFor each context c\u2208 C, we hypothesize the probable queries that the context c can address or the topics it discusses. We refer to these queries as hypothetical queries, denoted as \u011d. For each c\u2208 C, we let H(c) denote the set of hypothetical queries associated with c. Our ranking method determines the relevance of a given query q and a context c based on the similarity between the embedding of q and the embedding of c, as well as the similarity between those of q and the hypothetical queries H(c) as in Eq.2 where we introduce a hyperparameter \u03bb to balance the two similarities.\nrq(c) := sim(q, c) + X\u00b7 max_sim(\u011d,q) (2)\nAlgorithm 1 outlines our context ranking procedure. We start with a set Cq,K of K candidate contexts, which are typically the top-K results from a prior ranking step. For each context c \u2208 Cq,K, we generate a set of hypothetical queries H(c) by using an LLM, compute the embedding of c and each q \u2208 H(c) with an embedding model E, and then calculate the relevance score rq(c) using Eq.1. Hypothetical Query Generation. Our framework allows utilizing various LLMs ranging from Mistral 7b to GPT-3.5 and GPT-4 to generate hypothetical"}, {"title": "A Variational Inference Perspective", "content": "In this section, we explain how to use variational inference to derive Eq.2 by establishing the causal relationship between queries and contexts.\n5.1 Causal Relationship between Queries and Contexts\nIn Fig.3, we treat context c and query q as two random variables. We can think of calculating the ranking score rq(c) as measuring the probability p(c|q) of context c answering question q in the causality model. Different ranking methods model the causal relationship between c and q in different ways, resulting in different p(c|q) and rq(c). For instance, the standalone cosine similarity sim(q, c) can produce a spurious p(c|q) since c and q being similar does not necessarily imply that c provides answers to q, as shown by the example in Fig.3(a). Query expansion methods such as HyDE (Gao et al., 2023a) introduce a hypothetical context \u0109 as a latent variable and employ a generative model to simulate p(c|q). However, this causality modeling inevitably involves LLM's prior knowledge as an intervention (Wachter et al., 2017), which can lead to spurious causality. The external knowledge from LLM is represented as an additional variable D from another context space that is indefinitely larger than that of c. As shown in Fig.3(b), it can influence the generation of \u0109 by introducing outdated, irrelevant, or even counterfactual information (Brown et al., 2020).\nIn contrast, HyQE, as shown in Fig.3(c), introduces a hypothetical query \u011d as a latent variable and employs a generative model to simulate p(\u011d|c) without involving the prior knowledge of the LLM. This confines the generation of hypothetical query \u011d strictly within the scope of the context c, avoiding the pitfalls of spurious causality and ensuring that the causal relationships remain accurate and relevant. This allows us to use cosine similarity to simulate p(qq) where \u011d and q are both queries.\n5.2 Ranking Contexts from a Variational Inference Perspective\nNow we show how we derive Eq.2 based on Fig.3(c). Given a user query q and a context set Cq,k, we define p(c) as some prior confidence over the context set Cq,K that satisfies p(c) \u03b1 exp(sim(q, c)). We let p(q|c) be the probability of context c providing answers to the query q, and let p(q) be some prior probability of accepting an input query q, which can be seen as a constant when q is already given. Based on p(c), p(q|c), and p(q), we aim to learn p(c|q) := p(c)p(q|c)/p(q), which can be seen as the confidence of the context c addressing the given query q. Then, we learn p(c|q) by finding a distribution pq(c) that matches p(c|q) so that we can establish a scoring function based on pq(c), i.e., rq(c) x log pq(c). This learning objective can be formulated as minimizing the KL-divergence DKL(pq(c)||p(c|q)) which can be achieved by maximizing the evidence lower-bound (ELBO) of DKL(pq(c)||p(c|q)) as shown in Eq.3.\nELBO(rq) :=\nDKL(Pq(c)||p(c)) - Ec~pq(c) [log p(q|c)] (3)\nEq.3 uses a regularization term DKL(pq(c)||p(c)) to penalize pq if pq(c) deviates from p(c). Therefore, we include sim(q, c) as a part of rq such that the greater p(c) x exp(sim(q,c)) is, the greater pq(c) x exp(rq(c)) becomes. Meanwhile, the second term in Eq.3 indicates that pq should also align with p(q|c), the probability of c providing answers to q. To estimate p(q|c), we factorize log p(q|c) = log E\u011d~p(\u011d\\c)[P(q|q)] where p(\u011d|c) is the probability of c addressing a hypothetical query \u011d and p(qq) is the probability of obtaining an input query q given that the semantics of \u011d is equivalent to a given hypothetical query \u011d. We can safely use semantic similarity to approximate relevance between queries, i.e., p(q|\u011d) x exp(sim(\u011d, q)). We estimate the expectation w.r.t p(\u011d|c) by uniformly sampling from the set H(c) of hypothetical queries such that log p(q|c) = log Eh~p(q\\c)[P(q|q)] \u2248 log [H(c)] \u03a3\u2208H(c) P(qq). We then have the following two options for further approximation:\nOption 1. Based on the soft-max approximation, log [H(c)] \u03a3\u2208H(c) P(qq) \u2248 max log p(q|q) = A. max sim(h, q) + const where X is a hyper-QEH(c)\nparameter. Then we recover Eq.2 by ignoring the constant and adding sim(\u011d, q) mentioned earlier.\nOption 2. Based on Jensen's inequality (Jensen, 1906), we derive a lower bound of the estimated log p(q c) as shown in Eq 4, This allows us to maximize ELBO in Eq.3 by maximizing Eq.4, resulting in an alternative of Eq.2 as shown in Eq.5.\n[HC)] \u011d\u2208H(c) log p(q|\u011d)\n= \u03bb. [HC)] \u03a3\u2208(c) sim(q, q) + const (4)\nrq(c) := sim(q, c) +\nIn our HyQE framework, we mainly focus on Option 1. We will compare Option 1 with Option 2 in our evaluation."}, {"title": "Experiments", "content": "We test our method on multiple benchmarks to investigate the main question: whether HyQE improves the nDCG@10 in the benchmarks? In addition, we also investigate the following questions.\nA. Does changing the LLMs influence the results?\nB. How many hypothetical queries does an LLM need to generate for each context?\nC. Does changing the A in Eq.2 influence the results?\nD. Is HyQE compatible with different retrieval methods such as HyDE (Gao et al., 2023a)?\nE. How well does Eq.5 perform in comparison with Eq.2?\nDatasets. We test our methods on the following datasets: COVID (Thakur et al., 2021), NEWS (Thakur et al., 2021), Touche2020 (Thakur et al., 2021), DL19 (Craswell et al., 2020), and DL20 (Craswell et al., 2020). We use the same prompt for all the datasets except for the touche2020 dataset, in which the queries represent topics of arguments while the contexts consist of dialogues in those arguments. The prompt designed for this dataset can be found in Appendix B.\nBaselines. We use two kinds of retrievers: one is embedding model-based retrievers, including contriever and bge-base-en-v1.5; the other is SPLADE++_EnsembleDistil (Formal et al., 2022), which is a sparse retrieval model that does not generate text embeddings. We use the pre-built Lucene indexes in Pyserini (Lin et al., 2021) for retrieval. We use five embedding models as the baselines for ranking: contriever (Izacard et al., 2021), bge-base-en-v1.5 (Xiao et al., 2023), E5-large-v2 (Wang et al., 2022), text-embedding-3-large, and nomic-embed-text-v1.5 (Nussbaum et al., 2024). We also use those embedding models as the backbones of HyQE and compare the results produced by HyQE with those produced by the baseline embedding"}, {"title": "Conclusion", "content": "In this paper, we introduce a novel framework for context ranking using hypothetical queries generated by LLMs. Our method is grounded in variational inference, aiming to preserve the causal relationship between queries and the contexts. The experimental results demonstrate that our approach not only outperforms baselines but also can be integrated seamlessly with existing techniques, allowing for iterative refinement and continuous improvement. Furthermore, our method can amortize the overhead in text generation with LLM as the input queries increase, offering a scalable and efficient solution for context retrieval and ranking."}, {"title": "Limitations", "content": "While our proposed framework demonstrates significant improvements in context ranking and is scalable, there are several limitations to consider:\n1. Overhead of Query Generation and Storage. The effectiveness of our method relies on using an LLM to generate the queries. The computational complexity for the query generation is amortized as the input queries grow. However, this amortization is built on the premise that the generated queries are stored for future retrieval. And such storage will raise the memory complexity of this framework. As a result, extremely large datasets could still pose challenges.\n2. Dependency on the Type of Query. The input query can have different types, e.g., questions asking for specific information, a sequence of keywords, etc. However, in the prompt we only ask the LLM to generate the questions that can be addressed by the context, which may not have different structures than the input query.\n3. Adaptability to Context Chunk Sizes. Our framework has been validated on well-known TREC and MS-MARCO datasets, where the contexts are provided. However, when dealing with document retrieval, the contexts are created by segmenting the documents into chunks. The documents may be segmented with different chunk sizes depending on the requirement. Each time the document is segmented, the hypothetical queries have to be regenerated from the contexts. This issue could potentially be mitigated by generating hypothetical queries from smaller, fixed-sized chunks of contexts and composing those queries for larger chunks of contexts. However, the specifics of this approach require further investigation to ensure its effectiveness and efficiency.\nAddressing these limitations in future work will be essential for enhancing the robustness, efficiency, and applicability of our proposed context ranking framework across a broader range of scenarios."}, {"title": "Additional Implementation Details", "content": "In our implementation, we have used Mistral-7b-instruct-v0.2, GPT-3.5-turbo, and GPT-40 to generate hypothetical queries.\nFor Mistral-7b-instruct-v0.2, we use the pre-trained model. We set the context window size as 3900, and the maximum number of outputs as 1024. We also use an instruction prompt as shown in Fig.8 to wrap the prompt in Fig.2.\n<s>[INST]\nYou are an AI assistant. Here are some rules you always follow:\nGenerate human readable output, avoid creating output with gibberish text.\nDon't plainly replicate the given instruction.\nGenerate only the requested output, don't include any other language before or after the requested output.\nNever say thank you, that you are happy to help, that you are an AI agent, etc. Just answer directly.\nGenerate professional language typically used in business documents in North America.\nNever generate offensive or foul language.\nThe user prompt is as follows:\n\n{prompt}[/INST]</s>\nWe show examples of the hypothetical queries generated by Mistral-7b-instruct-v0.2 in Fig.9.\nWe mentioned in Section 6 that we use a different prompt from that in Fig.2 for the Touche dataset. The prompt is shown in Fig.12. We designed this prompt because each query in this dataset is about the topic of an argument, and the contexts record the dialogues in the argument, which may deviate from the topic. An example is provided in Fig.1."}]}