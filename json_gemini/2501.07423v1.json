{"title": "An Investigation into Seasonal Variations in Energy Forecasting for Student Residences", "authors": ["Muhammad Umair Danish", "Mathumitha Sureshkumar", "Thanuri Fonseka", "Umeshika Uthayakumar", "Vinura Galwaduge"], "abstract": "This research provides an in-depth evaluation of various machine learning models for energy forecasting, focusing on the unique challenges of seasonal variations in student residential settings. The study assesses the performance of baseline models, such as LSTM and GRU, alongside state-of-the-art forecasting methods, including Autoregressive Feedforward Neural Networks, Transformers, and hybrid approaches. Special attention is given to predicting energy consumption amidst challenges like seasonal patterns, vacations, meteorological changes, and irregular human activities that cause sudden fluctuations in usage. The findings reveal that no single model consistently outperforms others across all seasons, emphasizing the need for season-specific model selection or tailored designs. Notably, the proposed Hyper Network based LSTM and MiniAutoEncXGBoost models exhibit strong adaptability to seasonal variations, effectively capturing abrupt changes in energy consumption during summer months. This study advances the energy forecasting field by emphasizing the critical role of seasonal dynamics and model-specific behavior in achieving accurate predictions.", "sections": [{"title": "I. INTRODUCTION", "content": "The Independent Electricity System Operator (IESO) projects that Ontario\u2019s energy demand will grow exponentially over the next decade [1]. Furthermore, the World Energy Council (WEC) emphasizes that energy efficiency will be critical in preventing scenarios where demand surpasses supply [2]. Accurate energy forecasting is a pivotal component of energy planning and procurement processes; however, it has historically been a weakness in Ontario [3]. The primary objective of forecasting in the energy sector is to predict electricity demand and consumption to ensure an optimal supply plan. Inaccurate forecasts can lead to an over-supply or a shortage, resulting in financial and energy-related consequences.\nImproving energy forecasting aligns closely with sustainability goals and yields significant financial savings. It also facilitates efficient energy management, optimized deployment and operational strategies, informed infrastructure planning, reduced carbon footprints, and minimized costs. The challenges of accurate energy forecasting arise from the inability of some models to capture energy patterns or achieve convergence effectively. Furthermore, energy consumption predictions are profoundly influenced by the unpredictable nature of human behavior and natural events. A notable example is the COVID-19 pandemic, which significantly disrupted global energy consumption patterns due to its unprecedented impact on human behavior [4].\nThis paper investigates 13 Machine Learning (ML) models, including introducing two novel models to predict day-ahead energy consumption using two real-world datasets: Residence 1 and Residence 2, located at Western University. The analysis aims to uncover specific energy usage patterns, anomalies, and limitations of predictive models influenced by unpredictable human behavior and external factors. The primary objective is to develop a robust short-term or day-ahead load forecasting model capable of significantly reducing forecasting errors in energy applications.\nInitially, five baseline models were evaluated: Multi-Layer Perceptron (MLP), Temporal Convolution Neural Network (TCN), and three sequential neural networks\u2014Recurrent Neural Network (RNN), Gated Recurrent Unit (GRU), and Long Short-Term Memory (LSTM). Subsequently, the study explored state-of-the-art and ensemble models to assess their potential for improved accuracy and adaptability across seasons. These models included Neural Basis Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS) [5], Auto-Regressive Feedforward Neural Network (AR-Net) [6], and a hybrid of MiniRocket, AutoEncoder, and XGBoost. Additionally, the performance of Transformer models and an Integration of Attention Mechanisms with LSTM was examined. A Hypernetwork model was designed and integrated with an LSTM network to enhance forecasting robustness further.\nThe remainder of this report is organized as follows. Section II provides the background on the employed algorithms and accuracy measures. Section III reviews related works in this domain. Section IV details the methodology, including data preprocessing, feature engineering, and the validation process. Section V presents the evaluation metrics, results, and analysis. Finally, Section VI concludes the report and discusses potential future work."}, {"title": "II. BACKGROUND", "content": "This section briefly describes the background information of the project including the model architectures used for training, and the metrics used to evaluate the performance.\nModel Architectures\nWe began the project by exploring five baseline model architectures: Multi-Layer Perceptron (MLP), Temporal Convolution, Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU). These models served as foundational benchmarks for our energy forecasting task. MLP, as one of the most straightforward deep learning architectures, is often employed for time series prediction tasks. Temporal Convolutions, on the other hand, leverage 1-D convolutional layers to efficiently capture temporal dependencies in sequential data [7].\nThe remaining three baseline models\u2014RNN, LSTM, and GRU\u2014were chosen for their specialization in sequence modeling. Additional feature extraction techniques were integrated additional feature extraction techniques were integrated to enhance the performance of these baseline models. These included temporal convolution-based methods such as MiniRocket [8], a highly efficient approach for feature extraction, and a temporal convolution-based autoencoder architecture. Furthermore, an LSTM-based architecture incorporating a self-attention layer was implemented to better capture long-range dependencies in time series data.\nIn addition to these baseline models, we experimented with two advanced architectures: Neural Basis Expansion Analysis for Interpretable Time Series Forecasting (N-BEATS) and Auto-Regressive Feedforward Neural Network (AR-Net), both introduced in recent literature. The N-BEATS model utilizes fully connected layers organized into multiple building blocks with forward and backward residual links, enabling interpretable and robust forecasting [5]. AR-Net combines autoregressive statistical modeling with feedforward neural networks to bridge traditional statistical methods and modern deep learning approaches [6].\nEvaluation Metrics\nTo assess the performance of the models on the energy forecasting task, we employed three metrics: Symmetric Mean Absolute Percentage Error (SMAPE), Mean Absolute Error (MAE), and Root Mean Square Error (RMSE). While Mean Absolute Percentage Error (MAPE) is commonly used due to its interpretability, we opted for SMAPE as it is scale independent and less sensitive to the magnitude of forecasted values. This characteristic is particularly advantageous for time series data with varying scales. The formula for SMAPE is:\nSMAPE = 100% \u00d7 \\frac{1}{n} \\sum_{i=1}^{n} \\frac{2|y_i \u2212 \\hat{y}_i|}{|y_i| + |\\hat{y}_i|} , (1)\nMAE, which measures the average magnitude of errors without considering their direction, is calculated as follows:\nMAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i \u2212 \\hat{y}_i| (2)\nRMSE, which penalizes larger errors more heavily, is computed as:\nRMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i \u2212 \\hat{y}_i)^2} (3)\nIn these equations, yi represents the actual energy value, \\hat{y}_i denotes the predicted energy value, and n is the total number of observations. Together, these metrics provide a comprehensive evaluation of the forecasting performance."}, {"title": "III. RELATED WORK", "content": "The field of energy forecasting encompasses a variety of approaches, each with its challenges. Traditional statistical models, such as ARIMA and State Space Models, often struggle to capture energy data\u2019s complex and nonlinear characteristics [9]. Recurrent Neural Networks (RNNs) offer an improvement by capturing temporal sequences in energy consumption patterns. However, they are hindered by vanishing and exploding gradient problems, limiting their ability to model long-term dependencies crucial for accurate energy predictions. Extended Short-Term Memory Networks (LSTMs) address these issues through a sophisticated gating mechanism; however, this comes at the cost of increased computational complexity [10].\nRecently, Transformer models, initially proposed for natural language processing tasks, have gained popularity in energy forecasting. Introduced by Vaswani et al. [11], Transformers revolutionized sequential data processing by eliminating the reliance on sequential input order. Instead, they leverage self-attention mechanisms to weigh the significance of different parts of the input data, making them particularly effective in handling complex energy consumption patterns. A study by L\u2019Heureux et al. [12] highlights the effectiveness of Transformer-based models in electrical load forecasting, demonstrating their potential for more accurate and computationally efficient predictions than traditional models.\nTree-based models, such as Gradient Boosted Decision Trees (GBDTs) and XGBoost, have also been widely used in load forecasting tasks. While these models excel in general predictive tasks, they face challenges in capturing complex transient patterns, such as sudden spikes in energy consumption [13].\nFeature extraction techniques are critical in improving model performance by creating robust representations of time-series data. Tools like Time Series Feature Extraction Library (TSFEL) [14] and methods such as Discrete Wavelet Transformation (DWT) [15] have been applied to extract meaningful features from energy data. Additionally, convolutional autoencoders combined with LSTMs have been explored for tasks like unsupervised clustering in electricity consumption, further enhancing the representation of temporal data [16].\nRecent advancements such as MiniRocket, a deterministic and efficient feature extraction method, have demonstrated competitive performance with fast feature computation, making them promising for energy forecasting applications [17]."}, {"title": "IV. METHODOLOGY", "content": "This section provides an overview of the datasets, preprocessing techniques, and implemented models. It includes descriptions of the baseline models, state-of-the-art approaches, hybrid methodologies, and the proposed models.\nDataset\nWe utilized two real-world datasets capturing electricity consumption (in kWh) of two residence buildings (Residence 1 and Residence-2) at Western University in London, Ontario, spanning January 2019 to June 2023. Figures 1 and 2 illustrate these datasets\u2019 energy consumption trends and seasonality. Residence 1 exhibited more erratic consumption patterns than Residence 2, but neither dataset displayed distinct cyclic irregularities; instead, they contained a mix of seasonal and trend components.\nFeature Enhancement: The original dataset consisted of three features: date-time, temperature, and energy (in kilowatt hours). Since neural networks cannot interpret date-time information in text format, additional features were derived from the date-time attribute. These included the day of the year, the day of the month, the day of the week, and the hour.\nSelected Features: Six features were used for all models: day of the year, day of the month, day of the week, hour, temperature, and energy.\nData Preprocessing Techniques:\na) MinMax Scaling: MinMax scaling was applied to normalize the dataset. This technique is widely used in energy forecasting due to its efficiency and minimal sensitivity to outliers. The formula for MinMax scaling is:\nX_{scaled} = \\frac{X \u2212 X_{min}}{X_{max} \u2212 X_{min}} (4)\nwhere X is the original data, Xmin and Xmax are the minimum and maximum values in the dataset, and Xscaled is the normalized data.\nb) Sliding Window Technique: A sliding window technique was used to create temporal data windows. Each input window, X24, represents the preceding 24 hours of data, including energy, temperature, day of the year, day of the month, day of the week, and hour. The target window, \\hat{y}_{24}, corresponds to the next 24 hours of energy consumption. A stride of one was employed to capture patterns effectively.\nThe sliding window operation can be expressed as:\nX_{24}, \\hat{y}_{24} = SW(data, 24, 1) (5)\nwhere data represents the complete dataset, \u201824\u2018 is the window size, and \u20181\u2018 is the stride. SW denotes the sliding window operation.\nData Splitting Strategy: The dataset was partitioned into training, validation, and test sets, with 70% allocated for training, 10% for validation, and the remaining 20% for testing.\nEnergy Forecasting with Basic Models\nThe initial phase of this study involved training baseline models to forecast energy consumption and using their results as a benchmark for comparison with advanced approaches. To optimize the performance of these models, hyperparameters, such as the number of neurons in hidden layers and dropout rates were tuned. Additional strategies like dropout layers and early stopping were employed during training to mitigate overfitting.\nThe MLP model consisted of two fully connected layers, with a dropout layer preceding the output layer. The Temporal Convolution model included three 1D-convolution layers interspersed with two dropout layers. The sequential models (RNN, LSTM, and GRU) shared a similar architecture: a single sequential module followed by a dropout layer and a fully connected output layer."}, {"title": "C. Energy Forecasting with State-of-the-Art Models", "content": "We employed two state-of-the-art models: N-BEATS and AR-Net.\nThe N-BEATS model is based on an ensemble of stacked blocks, where each block comprises feed-forward neural networks with ReLU activation functions. These blocks predict basis expansion coefficients aggregated to form the final forecast. This architecture effectively captures time dependencies and patterns in time-series data [5]. In our implementation, six N-BEATS blocks with four hidden layers were utilized.\nThe process can be mathematically described as:\n\\hat{y}_{24} = FC(M(X_{24})) (6)\nWhere \\hat{y}_{24} represents predictions for the next 24 hours, M(X24) denotes the processing of input data by the baseline model M, and FC is a fully connected layer that generates the final prediction."}, {"title": "D. Transformer", "content": "Transformers, introduced by Vaswani et al. [11], have revolutionized sequential data modeling. Their architecture incorporates positional encoding, multi-head self-attention, and feed-forward layers. For this study, only the encoder part of the Transformer was utilized due to data constraints.\nThe process is mathematically expressed as:\n\\hat{y}_{24} = FC(T_E(X_{24})) (7)\nWhere \\hat{y}_{24} denotes predictions for the next 24 hours, T_E(X_{24}) represents the processing of input data by the Transformer encoder T E, followed by a fully connected layer FC for the final prediction."}, {"title": "E. Integration of Attention and LSTM", "content": "A self-attention layer was integrated with an LSTM layer to enhance long-range dependency modeling. The architecture consists of an LSTM layer, a self-attention layer, and a fully connected layer for predictions. An optional dropout layer was included for regularization.\nThe process is mathematically represented as follows:\n\\hat{y}_{24} = FC(SA(LSTM(X_{24}))) (8)\nwhere SA represents the self-attention layer applied after the LSTM."}, {"title": "F. Ensemble Models for Energy Forecasting", "content": "Ensemble models were developed by combining feature extraction techniques. Two baseline ensemble models were built:\n1) MiniRocket and Stochastic Gradient Descent (SGD) Regressor: Features extracted by MiniRocket were fed into an SGD regressor with a multi-output wrapper [8].\n2) MiniRocket and XGBoost: Features from MiniRocket were used as input to an XGBoost model.\nFurther, a novel ensemble was designed, combining MiniRocket and a convolutional autoencoder for feature extraction, with XGBoost as the final regression layer. This approach leverages the strengths of both feature extractors and tree-based prediction techniques."}, {"title": "G. Proposed Models", "content": "a) Hypernetwork and LSTM: We propose an energy forecasting architecture combining a primary LSTM network with a Hypernetwork. The Hypernetwork dynamically generates weights for the LSTM based on input features, enabling adaptive behavior. The Hypernetwork consists of three fully connected layers with ReLU activation functions for non-linearity and Xavier for weight initialization. The primary network comprises a standard LSTM layer and a fully connected layer for final predictions.\nThe Hypernetwork process is mathematically represented as:\n\\theta_{HN} = FC_{HN}(X_{24}) (9)\nwhere \\theta_{HN} are the dynamically generated weights for the LSTM, and FC_{HN} represents the fully connected layers of the Hypernetwork.\nThe primary LSTM network\u2019s process is given by:\n\\hat{y}_{24} = FC(LSTM(X_{24}; \\theta_{HN})) (10)\nwhere LSTM(X24; \u03b8HN ) processes the input data X24 with the weights \u03b8HN generated by the Hypernetwork, and FC is a fully connected layer producing the final output \\hat{y}_{24}.\nb) Ensemble of MiniRocket, Convolutional Autoencoder, and XGBoost: This ensemble model leverages two feature extractors, MiniRocket, and a Convolutional Autoencoder, combined with XGBoost for energy forecasting.\nThe convolutional autoencoder uses a simple encoder architecture with two 1D convolutional layers interspersed with max-pooling layers. The decoder mirrors the encoder with deconvolution and up-sampling layers. The input data X24 is reshaped (from (n, 24, 6) to (n, 6, 24), where n is the batch size) before being fed into the MiniRocket model to perform convolution operations along a different dimension, extracting diverse features. The autoencoder encoder architecture is mathematically described as follows:\ny = ReLU(Conv1D(X_{24})) (11)"}, {"title": null, "content": "where num kernels = 16, kernel size = 2.\ny = M axP ool1D(y) (12)\ny = ReLU(Conv1D(y)) (13)\nwhere num kernels = 8, kernel size = 3.\ny_1 = Flatten(M axP ool1D(y)) (14)\nThe final ensemble architecture combines the outputs of the autoencoder AE, MiniRocket MR, and XGBoost X GB as follows:\ny_1 = AE(X_{24}) (15)\ny_2 = MR(reshape(X_{24})) (16)\n\\hat{y}_{24} = X GB(concat(y_1, y_2)) (17)\nThis ensemble model effectively balances computational efficiency with predictive accuracy. MiniRocket, for instance, can extract features from the training dataset ((55077, 24, 6)) in approximately 47 seconds using a single Intel\u00ae Xeon\u00ae CPU, making it a practical solution for large-scale energy forecasting tasks."}, {"title": "V. RESULTS AND EVALUATION", "content": "This section outlines the hyperparameter optimization methodology, defines the objective functions, discusses regularization techniques, and presents analyses of overall results and seasonal performance evaluations.\nHyperparameter Optimization\nAll models were trained on the same dataset, maintaining input and output dimensions consistency. A Grid Search approach was employed to identify the optimal configurations of units, layers, optimizers, and learning rates for each model.\nObjective Function\nThe primary objective for all models was to minimize the difference between actual and predicted values. Two loss functions were employed:\na) Mean Squared Error (MSE)::\nMSE = \\frac{1}{24} \\sum_{i=1}^{24} (y_i \u2212 \\hat{y}_i)^2 (18)\nb) Mean Absolute Error (MAE)::\nMAE = \\frac{1}{24} \\sum_{i=1}^{24} |y_i \u2212 \\hat{y}_i| (19)\nwhere yi is the actual energy value, \\hat{y}_i is the predicted energy value, and 24 represents the number of observations. MSE was employed for most models, while MAE was specifically used for the Temporal Convolutional Network due to its superior performance with this metric.\nRegularization\nWeight decay was implemented as a regularization technique across all models to prevent overfitting. This penalizes large weight values, effectively limiting model complexity.\nOptimized Hyperparameters for Base Models\nThe optimized hyperparameters for the baseline models were obtained via Grid Search:\n\u2022 GRU: 64 units, the dropout rate of 0.1, the learning rate of 0.004.\n\u2022 LSTM: 64 units, the dropout rate of 0.5, the learning rate of 0.001.\n\u2022 RNN: 64 units, the dropout rate of 0.5, the learning rate of 0.01.\n\u2022 MLP: 32 units, the learning rate of 0.004.\n\u2022 Temporal Convolutional Model: 128 units, the dropout rate of 0.5, the learning rate of 0.01.\nThe Stochastic Gradient Descent (SGD) optimizer was employed for all baseline models.\nOptimized Hyperparameters for SOTA Models\n\u2022 N-BEATS: 6 blocks, each with four hidden layers and 512 units.\n\u2022 ARFNN: 64 units, two layers, learning rate of 0.001.\nFor both models, the SGD optimizer was the most effective.\nOptimized Hyperparameters for the Transformer Model\nThe Transformer model configuration included the following:\n\u2022 2 attention heads.\n\u2022 Fully connected layers with 64 units before and 24 units after the encoder.\n\u2022 Dropout rate of 0.1, learning rate of 0.001, and SGD optimizer.\nOptimized Hyperparameters for the Integration of Attention and LSTM Model\nThe model was configured with:\n\u2022 LSTM layer: 64 units, dropout rate of 0.5.\n\u2022 Learning rate: 0.01.\n\u2022 Optimizer: Adam."}, {"title": null, "content": "Optimized Hyperparameters for the Hypernetwork and LSTM Model\nThe optimal configuration was:\n\u2022 LSTM layer: 64 units, dropout rate of 0.1, learning rate of 0.01.\n\u2022 Hypernetwork: 128 units in the first layer and 64 in the second layer.\n\u2022 Optimizer: SGD.\nOptimized Hyperparameters for the Ensemble Model\nThe ensemble model was optimized as follows:\n\u2022 MiniRocket: 512 convolution kernels, resulting in feature vectors of length 512.\n\u2022 Autoencoder: AdamW optimizer, learning rate 0.0001, Mean Squared Error Loss.\n\u2022 XGBoost: Learning rate of 0.05, colsample bytree of 0.5, reg lambda of 1.2, a subsample of 0.8, booster set to \u2019gbtree.\u2019\nEarly Stopping Implementation\nAn early stopping mechanism was integrated into the training process to enhance efficiency and avoid overfitting. This mechanism monitored validation performance (10% of the dataset) and halted training if no improvement in validation loss was observed over five consecutive epochs. This patience threshold balanced adequate time for model convergence with preventing unnecessary computations.\nResidence 1 Performance Analysis\nAs shown in Figures 10 and Table II, our proposed HyperNetLSTM model outperforms the competing approaches with the lowest Symmetric Mean Absolute Percentage Error (SMAPE) of 8.87. Notably, it achieves the best Mean Absolute Error (MAE) of 21.03 and a competitive Root Mean Square Error (RMSE) of 29.58. These lower error metrics suggest that the HyperNetLSTM predictions have fewer large deviations, a crucial characteristic in energy forecasting.\nIn contrast, the MiniWSGD model exhibits the highest error metrics, with an MAE of 39.33, RMSE of 55.92, and\nSeasonal Performance Analysis\nWe evaluated all models across different seasons for Residence 1 to investigate temporal variations in forecasting accuracy. As shown in Tables III, IV, and V, no single model dominates across all seasons, highlighting the importance of season-specific factors in energy usage.\nThe Transformer model achieves the best performance in the fall. Its self-attention mechanism helps capture multi-scale temporal features, making it robust to the seasonal fluctuations typical of fall weather.\nDuring Winter, HyperNetLSTM delivers the best results, underscoring its ability to learn complex consumption patterns exacerbated by heating demands in colder months. MiniAu toEncXGBoost also performs remarkably well, suggesting that autoencoder-based feature extraction combined with gradient boosting is beneficial for winter-specific demand fluctuations.\nIn the Summer season, HyperNetLSTM again stands out by accurately modeling abrupt decreases in energy usage during vacation periods. These findings reinforce that specific architectural designs (e.g., HyperNetworks combined with LSTMs) can effectively learn season-specific nuances, while others (such as MiniWSGD) struggle to capture intricate temporal dependencies.\nThese seasonal insights highlight the need for model selection that aligns with domain-specific characteristics. Depending on the season, distinct models may be more adept at forecasting consumption trends, emphasizing the importance of flexible, context-aware forecasting strategies.\nResidence 2 Performance Analysis\nAs shown in Table VI and Figure 12, the MiniAutoEncXG Boost model achieves the strongest performance for Residence 2, posting the lowest SMAPE of 7.37, MAE of 18.12, and RMSE of 25.08. These metrics underscore its effectiveness in capturing energy usage patterns for this particular household.\nThe AttentionLSTM model also performs competitively, highlighting the utility of attention mechanisms in handling temporal dependencies. By contrast, Transformer-based models show relatively weaker performance for Residence 2, suggesting the need to refine their architectural hyperparameters or training strategies for this dataset. MiniAutoEncXGBoost\u2019s superior performance likely stems from the synergy between autoencoder-based feature extraction and gradient boosting, enabling it to uncover intricate consumption patterns.\nSeasonal Performance Analysis for Residence 2\nWe further evaluated all models across different seasons for Residence 2. As shown in Tables VII through X, each season presents unique challenges:\n- Fall (Table VII): LSTM achieves the lowest SMAPE of 6.60, indicating its effectiveness at modeling autumn-specific consumption patterns. - Spring (Table IX): AttentionLSTM outperforms other methods (SMAPE = 6.80), demonstrating the benefit of integrating attention with LSTM architectures to capture transitions in energy usage. - Summer (Table X): MiniAutoEncXGBoost stands out again with a SMAPE of 12.24, effectively modeling sporadic consumption changes often seen during summer vacations. - Winter (Table VIII): ARFFNN achieves a SMAPE of 5.60, underscoring its capacity to learn the more pronounced consumption spikes typical in cold weather.\nThese findings emphasize the importance of selecting models that align well with seasonal demand fluctuations. Adapting to each season\u2019s unique consumption characteristics, forecasters can significantly improve prediction accuracy and better manage energy resources."}, {"title": "VI. CONCLUSION", "content": "This study\u2019s detailed analysis across various seasons and techniques confirms that no single model performs best in all scenarios. Seasonal variations influence energy forecasting accuracy and require seasonally contextualized model selection to handle the unique characteristics of energy consumption patterns effectively. Additionally, the variability introduced by human behavior poses significant challenges for forecasting models. Current standalone AI models may not fully capture these complexities. We propose developing innovative human-AI hybrid models integrating human-centric considerations alongside seasonal factors. Such models would account for the unpredictable nature of human activity while improving forecasts\u2019 responsiveness to fluctuating energy demand patterns. This approach represents a promising direction for future research, offering the potential to significantly enhance the precision and reliability of energy forecasting models in residential settings."}]}