{"title": "HSVLT: Hierarchical Scale-Aware Vision-Language Transformer for Multi-Label Image Classification", "authors": ["Shuyi Ouyang", "Hongyi Wang", "Ziwei Niu", "Zhenjia Bai", "Shiao Xie", "Yingying Xu", "Ruofeng Tong", "Yen-Wei Chen", "Lanfen Lin"], "abstract": "The task of multi-label image classification involves recognizing multiple objects within a single image. Considering both valuable semantic information contained in the labels and essential visual features presented in the image, tight visual-linguistic interactions play a vital role in improving classification performance. Moreover, given the potential variance in object size and appearance within a single image, attention to features of different scales can help to discover possible objects in the image. Recently, Transformer-based methods have achieved great success in multi-label image classification by leveraging the advantage of modeling long-range dependencies, but they have several limitations. Firstly, existing methods treat visual feature extraction and cross-modal fusion as separate steps, resulting in insufficient visual-linguistic alignment in the joint semantic space. Additionally, they only extract visual features and perform cross-modal fusion at a single scale, neglecting objects with different characteristics. To address these issues, we propose a Hierarchical Scale-Aware Vision-Language Transformer (HSVLT) with two appealing designs: (1) A hierarchical multi-scale architecture that involves a Cross-Scale Aggregation module, which leverages joint multi-modal features extracted from multiple scales to recognize objects of varying sizes and appearances in images. (2) Interactive Visual-Linguistic Attention, a novel attention mechanism module that tightly integrates cross-modal interaction, enabling the joint updating of visual, linguistic and multi-modal features. We have evaluated our method on three benchmark datasets. The experimental results demonstrate that HSVLT surpasses state-of-the-art methods with lower computational cost.", "sections": [{"title": "1 INTRODUCTION", "content": "Multi-label image classification refers to the task of recognizing multiple objects within a single image. It yields great value for various applications such as image retrieval [35], human attribute recognition [21] and scene understanding [29]. Unlike single-label classification, the multi-label classification task presents a substantial challenge as it involves identifying multiple objects within a single image with imbalanced label distribution and varying categories, requiring abundant local visual information. The semantic information contained in the labels can help model the potential objects in the image, underscoring the importance of modeling global cross-modal relationships. As objects within the given image may differ significantly in size and appearance, extracting visual features and modeling visual-linguistic relationships at multiple scales can enhance the ability to capture different objects. In addition, performing cross-scale aggregation help to leverage complementary information from different scales for decision-making purposes."}, {"title": "2 RELATED WORKS", "content": "Multi-Label Image Classification. For multi-label image classification, early approaches [36, 41] were based on object detection, where objects in the image were detected and located before being individually classified. Subsequently, researchers explored how to leverage label correlations to enhance classification accuracy. Gong et al. [15] employed specialized loss functions to optimize label correlations. Chen et al. [6] utilized a directed graph to model label dependencies. In addition, the effectiveness of exploring spatial dependencies with label semantics has also been demonstrated. Wang et al. [34] proposed the utilization of spatial transformer layers to emphasize the image regions relevant to the labels, while Chen et al. [4] devised the recurrent attention reinforcement module for the same purpose. Wu et al. [38] reformulated the multi-label image classification problem as a graph matching structure, which incorporates instance space relations, label semantic relevance, and instance-label assignment probability into the framework. Recently, Transformer-based methods have shown improved ability in modeling long-range cross-modal dependencies and made significant progress in multi-label classification. In these methods, M3TR [47] separately learns ternary relationships inter- and intra- modalities and performs semantic cross-attention, while Zhu et al. designed Two-Stream Transformer [51] to extract global features and correlations of label semantics separately.\nVision Transformer. Transformer [31] models have been widely used for several computer vision tasks. The ViT model applies self-attention in shallow layers enhancing performance for vision tasks. Recent Transformer-based models have achieved impressive results across a range of vision tasks, including image classification [11, 25], object detection [2], and semantic segmentation [30]. The self-attention mechanism of the Transformer allows the models to effectively capture long-range dependencies in images, establishing global contextual information. In addition, the multi-scale design of the Transformer [16, 48] allows the model to aggregate features of different scales in the image, improving its overall performance in vision tasks. For multi-label image classification task, various Transformer-based methods have been presented using advantages of long-range dependencies. Lanchantin et al. [20] introduced a general framework based on a Transformer, with the use of a ternary coding scheme for training. Cheng et al. [7] proposed a Multi-Label Transformer architecture comprising of window partitioning, pixel attention within windows, and cross-window attention. Zhao et al. [47] presented a multi-modal multi-label recognition transformer framework that includes ternary relationship learning for both inter- and intra-modal information. Zhu et al. [51] introduced the Two-Stream Transformer, which utilizes spatial and semantic streams to respectively learn the visual perception and label semantics and their correlation. However, in current Transformer-based approaches, visual feature extraction and cross-modal fusion are limited to a single scale, failing to capture information at diverse scales within an image, which negatively impact the classification performance."}, {"title": "3 HIERARCHICAL SCALE-AWARE\nVISION-LANGUAGE TRANSFORMER", "content": null}, {"title": "3.1 Overview", "content": "The proposed HSVLT simultaneously updates visual, linguistic features and global visual-linguistic relationships within scales as well as performing cross-scale aggregation to aid in multi-label classification prediction. The overall architecture of HSVLT is presented in Figure 2.\nGiven an input pair of an image and labels in the dataset, our model generates predictions of object labels that are present within the image. Our HSVLT adopts a hierarchical multi-scale architecture that follows the workflow of [joint vision-language encoder] - [cross-scale aggregation] - classification. HSVLT consists of four stages, each with different numbers of interaction blocks and different feature map resolutions. The encoder (Sec.3.2) includes a novel lightweight attention module (Sec.3.3) that uses interactive cross-modal cues to simultaneously update visual, linguistic features and model global visual-linguistic relationships. We also propose a global cross-scale aggregation module (Sec.3.4) to effectively evaluate multi-modal information across scales and enhance the classification accuracy. In the following subsections, we describe each components of HSVLT in detail."}, {"title": "3.2 Joint Vision-Language Encoder", "content": "To enhance the alignment of visual-linguistic features in semantic space, we introduce an joint vision-language encoder that can effectively capture joint visual, linguistic and multi-modal features considering interactive cross-modal cues. Figure 3 illustrates the interaction block structure of our encoder, which incorporates a novel attention mechanism (Sec. 3.3) that replaces the conventional self-attention mechanism.\nAs shown in Figure 2(a), our encoder has a pyramid structure, which contains 4 stages with decreasing spatial resolutions. There are \\(N_i\\) interaction blocks in our encoder for the i-th stage. The visual and linguistic inputs provided into the encoder are denoted as \\(V_0 \\in \\mathbb{R}^{C_{00}\\times H \\times W}\\) and \\(L_0 \\in \\mathbb{R}^{C_I\\times T}\\), where H and W are height and width of the input image, T is the number of labels, \\(C_{00}\\) and \\(C_I\\) represent the number of channels for the visual and linguistic inputs, respectively.\nEncoder Workflow. Each stage contains a scale transformation block, a channel unification block and a stack of interaction blocks. Specially in 1-st stage, we use a word embedding block to extract the linguistic feature \\(L^0 \\in \\mathbb{R}^{C_{01}\\times T}\\) via a language encoder BERT [10], and a patch embedding block to extract the visual feature \\(V^0 \\in \\mathbb{R}^{C_{01}\\times H_1\\times W_1}\\). At the beginning of 2, 3, 4-th stage, the visual feature and the linguistic feature pass the scale transformation block and the channel unification block respectively to down-sample the visual feature map and unify the channel dimensions, getting visual feature \\(V^0 \\in \\mathbb{R}^{C_{vi}\\times H_i\\times W_i}\\) and linguistic feature \\(L \\in \\mathbb{R}^{C_{vi}\\times T}\\), which are sent to interaction blocks. For the sake of clarity, in this section, we assume that \\(N_i = 1\\) for all stages. This assumption implies that we are utilizing only one interaction block per stage to illustrate the network. For each stage, output of joint visual feature \\(V_i \\in \\mathbb{R}^{C_{vi}\\times H_i\\times W_i}\\), joint linguistic feature \\(L_i \\in \\mathbb{R}^{C_{vi}\\times T}\\) and joint multi-modal feature \\(S_i \\in \\mathbb{R}^{C_{vi}\\times T}\\) can be obtained as follows:\n\\[V^1, L^1 =\\begin{cases}PE(V_0), WE(L_0), & i=1\\Down(V_{i-1}), Unify(L_{i-1}), & i=2,3,4\\end{cases}\\]\\[V^i, S^i, L^i = Interact(V^0, L), \\tag{2}\\]\nwhere i indexes the stage, function PE(.) indicates the patch embedding block, function WE(.) indicates the word embedding block, function Down(.) indicates the scale transformation block, function Unify(.) indicates the channel unification block, function Interact(.) indicates the interaction block. Both the patch embedding block and the scale transformation block consist of a convolution with stride of 2 and kernel size of 3 \u00d7 3, followed by a batch normalization layer. The channel unification block employs a 1\u00d71 convolution for linear transformation.\nInteraction Block. As shown in Figure 3(a), we perform cross-modal interaction in the interaction block as follows:\n\\[V^2, L^2 = IVLA(Norm(V), Norm(L)), \\tag{3}\\]\nwhere function IVLA(.) indicates the Interactive Visual-Linguistic Attention module (Sec.3.3), function Norm(.) indicates the normalization operation. Then we obtain \\(V^i, L^i\\) and \\(S^i\\) by \\(V_i = Norm(V + V^2)\\), \\(L_i = Norm(L + L^2)\\), and \\(S_i = Norm(L)\\)."}, {"title": "3.3 Interactive Visual-Linguistic Attention", "content": "As depicted in Figure 3(b), our proposed attention mechanism, namely Interactive Visual-Linguistic Attention (IVLA), holistically captures local visual details and global visual-linguistic relationships using interactive cross-modal cues. IVLA comprises four components: a cross-modal interaction to model global visual-linguistic relationships, an interactive linguistic fusion to update the linguistic feature with cross-modal cues, an interactive visual fusion to capture local visual details and incorporates cross-modal cues, and a gate regulation block to controls the flow of cross-modal knowledge. In i-th stage, given the visual input \\(V^i\\) and the linguistic input \\(L^i\\), we obtain visual output \\(V^2 \\in \\mathbb{R}^{C_{vi}\\times H_i\\times W_i}\\) and linguistic output \\(L^2 \\in \\mathbb{R}^{C_{vi}\\times T}\\) as following steps.\nCross-Modal Interaction. We utilize a cross-modal interaction to model global visual-linguistic relationships for visual and linguistic features. The steps to get interactive cross-modal activation \\(Att_{cross} \\in \\mathbb{R}^{H_iW_i \\times T}\\) are described as the following:\n\\[Att_{cross} = \\frac{flatten(\\omega_{v1}(V^1))\\omega_{l1}(L)}{\\sqrt{C_{vi}}}, \\tag{4}\\]\nwhere \\(\\omega_{v1}, \\omega_{l1}\\) are projection functions, and \\(flatten(\\cdot)\\) means unrolling the two spatial dimensions into one dimension in row-major. Here, \\(Att_{cross}\\) is the attention scores between \\(V^i\\) and \\(L\\), which represents the degree of correlation between the two modalities. \\(\\omega_{v1}\\) is implemented as a 1\u00d71 convolution followed by instance normalization. \\(\\omega_{l1}\\) is implemented as a 1\u00d71 convolution. Both \\(\\omega_{v1}\\) and \\(\\omega_{l1}\\) generate \\(C_{vi}\\) number of output channels.\nInteractive Linguistic Fusion. We activate the linguistic features using interactive cross-modal activation \\(Att_{cross}\\), and combine it with the linearly transformed language input through element-wise multiplication, resulting in the cross linguistic feature \\(L_{cross} \\in \\mathbb{R}^{C_{vi}\\times T}\\). \\(L_{cross}\\) can be obtained using the following equation:\n\\[L_{cross} = \\omega_{l3}(L) \\frac{flatten(\\omega_{v2}(V^1))softmax(Att_{cross})}{\\sqrt{C_{vi}}}, \\tag{5}\\]\nwhere \\(\\omega_{v2}\\) and \\(\\omega_{l3}\\) are projection functions same as \\(\\omega_{v1}\\) and \\(\\omega_{l1}\\).\nInteractive Visual Fusion. There is a G-Conv operation to capture local features, which has spatial inductive-bias in modeling rich local visual information. Cross visual feature \\(V_{cross} \\in \\mathbb{R}^{C_{vi}\\times H_iXW_i}\\) can be obtained using the following equation:\n\\[V_{cross} = G-Conv(V) + unflatten((softmax(Att_{cross})\\omega_{l2}(L)^T)^T), \\tag{6}\\]\nwhere \\(\\omega_{l2}\\) indicates projection function same as \\(\\omega_{l1}\\), unflatten(.) indicates the opposite operation of flatten(.), and G-Conv(.) denotes the application of a 7\u00d77 convolution operation, followed by a GELU activation function.\nGate Regulation. To merge cross-modal knowledge into original visual features \\(V^i\\) and linguistic features \\(L^i\\), we introduce Gate Regulation, a gating mechanism. Its core function is to balance the influence of \\(V_{cross}\\) and \\(L_{cross}\\) on the original information in \\(V^i\\) and \\(L^i\\), ensuring controlled passage of cross-modal knowledge to the next stage. A gate unit learns weight mappings from \\(V_{cross}\\) and \\(L_{cross}\\) to adaptively re-scale each element. The mathematical formulations of the Gate Regulation is provided below:\n\\[V^2 = V^1 + V_{cross} \\odot Gate(V_{cross}), \\tag{7}\\]\n\\[L = L + L_{cross} \\odot Gate(L_{cross}), \\tag{8}\\]\nwhere \\(Gate(\\cdot)\\) is composed of two 1\u00d71 convolutions for linear transformation, a ReLU function and a Tanh function."}, {"title": "3.4 Cross-Scale Aggregation and Classification", "content": "To effectively leverage the visual-linguistic knowledge from different scales, the joint multi-modal features \\(S_1, S_2, S_3, S_4\\) from scales need to be aggregated for final classification prediction.\nWe investigate three concise structures shown in Figures 4(a), 4(b), and 4(c), where Head refers to a transformation applied to the channel dimension. Two typical existing multi-scale integration structures used in other vision tasks are shown in Figures 4(a), 4(b). The structure adopted by our HSVLT is shown in Figure 4(c). Figure 4(a) is mostly adopted by CNN-based models [3, 46], lacking of holistic consideration to multiple scales. Figures 4(b) is a purely MLP-based structure [39] with a high computational cost.\nAs shown in Figure 4(c), we propose a Cross-Scale Aggregation (CSA) module to aggregate features from the four stages and use a lightweight Hamburger [14] to further model the global cross-scale context, resulting in improved performance without compromising computational efficiency. We obtain the final prediction results by the following equation:\n\\[Out = Class(Ham(Concat[S_1, S_2, S_3, S_4])), \\tag{9}\\]\nwhere \\(S_i\\) is the joint multi-modal feature maps from i-th stage, Ham(.) indicates a Hamburger function, and Class(.) indicates a 1 \u00d7 1 convolution for final prediction."}, {"title": "4 EXPERIMENTS", "content": null}, {"title": "4.1 Dataset and Evaluation", "content": "We perform experiments on three widely used benchmark datasets for multi-label image classification, including Pascal VOC 2007 [12], Microsoft COCO [22], and NUS-WIDE [8]. They have 9,963, 82,081, and 269,648 images respectively, containing 20, 80, and 81 classes. Following previous works [47, 51], we evaluate our proposed method with mean average precision (mAP), a commonly employed metric in the multi-label image classification task. To provide a comprehensive comparison, we introduce six additional multi-label metrics, namely overall precision, recall, F1-measure (OP, OR, OF1) and per-class precision, recall, F1-measure (CP, CR, CF1) [5]. These metrics are reported under the condition that a label is considered positive if its predicted probability exceeds 0.5. For a systematic comparison with competitors, the top three labels are also reported based on their confidence scores in descending order."}, {"title": "4.2 Implementation Details", "content": "We conduct experiments using PyTorch library and use BERT implementation from HuggingFace's Transformer library [37]. Following the settings in previous works [50, 51], we initialize convolutions in IVLA with weights pre-trained on ImageNet-21K from the ConvNeXt [26]. Language encoder of our model is initialized using official pre-trained weights of BERT with 12 layers and hidden size 768. Table 1 presents the detailed network settings, where \\(N_i\\) denotes the number of interaction blocks in the i-th stage. In the interactive visual fusion of IVLA, we use the kernel size of 7\u00d77 for our convolutions. The rest of weights in our model are randomly initialized.\nFollowing, we use AdamW optimizer with weight decay 0.01 and batch size 8. The learning rate is initialed as 1e-5 and scheduled by polynomial learning rate decay with a power of 0.9. To ensure a fair comparison with other models, all input images are adjusted to 448 x 448. The learning rate is decayed by a factor of 10 when the loss plateaus. Following previous works [28, 51], we perform random horizontal flip, random resized crop and RandAugment [9] for data augmentation during the training stage."}, {"title": "4.3 Comparison with the State-of-the-Arts", "content": "We compare the performance of our proposed HSVLT with state-of-the-art (SOTA) methods on three widely-used benchmarks, namely Pascal VOC 2007 [12], Microsoft COCO [22], and NUS-WIDE [8]. The table below highlights the best scores in red, and the second-best scores in blue, facilitating a straightforward comparison. The symbol\u2020 in Tables 2, 3 indicates the utilization of a higher input image resolution (576 \u00d7 576).\nPascal VOC 2007. For equitable assessments, both our model and competitors train on train-val set and test on test set. Table 2 displays comparative experimental results, including class-wise AP and overall mAP. Our HSVLT achieves SOTA performance in terms of mAP, surpassing other methods. Notably, category AP improvements are evident, e.g., chair, bottle, plant, with HSVLT surpassing 2nd best by 4.2%, 3.0%, and 2.5%. Overall, HSVLT achieved the highest AP for more than two-thirds of the label categories.\nMicrosoft COCO. Following previous studies [5, 23, 47], we report precision, recall, and F1-measure with and without Top-3 scores for the Microsoft COCO dataset, as shown in Table 3. Notably, our HSVLT achieves remarkable mAP of 91.6%, surpassing all. HSVLT also excels in CF1 and OF1 metrics, in both All and Top-3 contexts. Furthermore, our HSVLT achieves a higher mAP score while using significantly fewer GFLOPs and parameters compared to previous Transformer-based approaches, as shown in Figure 5.\nNUS-WIDE. Test outcomes on NUS-WIDE dataset are in Table 4. HSVLT excels in critical mAP, surpassing TSFormer by 2.8%. HSVLT also leads in CF1, OF1 for All, and CF1 for Top-3, showing superiority over SOTA. These findings underscore HSVLT's capability in precise object capture due to its multi-scale structure and joint cross-modal interaction."}, {"title": "4.4 Ablation Study", "content": "Ablation on IVLA design. We have conducted an ablation study on IVLA design on the Microsoft COCO test set. Table 5(a) presents a comparison of the performance of different convolution kernels in IVLA's interactive visual fusion. The results demonstrate that the 7x7 convolution kernel exhibits superior performance. Moreover, Table 5(b) details the structure of IVLA and shows the effects of various components, including G-Conv, L-Act, V-Gate, and L-Gate. G-Conv refers to a 7x7 convolution with a GELU activation function, L-Act indicates the dot product activation operation of interactive cross-modal activation \\(Att_{cross}\\) on linguistic features, and V-Gate and L-Gate represent gated units responsible for controlling the flow of the cross-modal knowledge for visual and linguistic features, respectively. The experimental results indicate that the proposed structure achieves the best performance. It follows that each part contributes to the final performance.\nEffectiveness of CSA. In order to evaluate the effectiveness of the CSA module, we have conducted an ablation study on three benchmark datasets, namely, VOC 2007, Microsoft COCO, and NUS-WIDE. Specifically, we compare the performance of the complete HSVLT with that of HSVLT (w/o CSA) and report the results in Tables 2, 3, 4, respectively. The experimental results indicate that the removal of the CSA module leads to a degradation in performance, as evidenced by a decrease of 0.2%, 0.8%, and 0.7% in mAP across the three datasets, respectively. As shown in Table 5(c), the performance of our proposed CSA outperforms past multi-scale integration structures. In addition, Ours and Ours (w/o CSA) in Figure 5 also show that CSA improves performance with a slight increase in parameters and GFLOPs, demonstrating the superiority of CSA in interacting multi-modal features among scales.\nAblation of CSA on various stages and features. Given joint multi-modal features from different stages, CSA forms features from different scales into a sequence for joint refinement in single forward pass. \\(S_i, i \\in {1, 2, 3, 4}\\) represents multi-modal feature from i-th stage inputted to CSA. Table 5(d) compares multiple input sequences, confirming the value of multi-scale interaction for global reasoning. We also evaluated CSA impact using varied sequences, including \\(L_i, i \\in {1, 2, 3, 4}\\) for joint linguistic feature from i-th stage. In Table 5(e), \\(S_1, S_2, S_3, S_4\\) perform optimally for CSA.\nAblation of different language models. In prior experiments, we used BERT [10] for label semantic embeddings. To assess diverse embedding methods' impact on HSVLT, we contrast it with one-hot encoding [1] and another pre-trained embedding method, namely Glove [27]. The one-hot method encodes labels as one-hot vectors and learns a parameter matrix to map them to the desired embedded space dimension. Table 5(f) indicates BERT excels, and other methods outperform prior SOTA with same model, confirming HSVLT's adaptability to different language models."}, {"title": "4.5 Interpretation of HSLVT", "content": "In Figures 6(a) and 6(b), we compare HSVLT's performance with Transformer-based methods in extreme sizes and confusing appearance predictions. In Figure 6(a), \"person\" is much smaller than \"bus\", and \"sofa\" largely occupies the image, highlighting HSVLT's object recognition advantage for extreme sizes. In Figure 6(b), \"sofa\"-\"chair\" similarity and \"cat\"-\"dog\" confusion are addressed by HSVLT's local visual info and global cross-modal interactions.\nFigure 6(c) contrasts TSFormer and HSVLT in classification results and feature maps. Examining Ground Truth (GT), we observe \"person\" and \"dog\" with significant size difference. HSVLT's comprehensive consideration of multi-scale information and tighter cross-modal correlations yields precise multi-label results. \\(V_T\\) and \\(V_i\\) represent features from TSFormer and HSVLT's i-th stage. \\(V_1, V_2, V_3, V_4\\) capture diverse aspects of image information. In the example, \\(V_2\\) recognizes the \"person\", while \\(V_4\\) focuses on the \"dog\". Thus, the comprehensive aggregation of multi-modal features obtained from different stages helps to enhance multi-label classification performance."}, {"title": "5 CONCLUSION", "content": "In this paper, we propose a novel Transformer-based framework named HSVLT for multi-label image classification. HSVLT jointly captures local visual features and models global visual-linguistic relationships considering interactive cross-modal cues at each scale. The proposed network design interacts multi-modal information between different scales with cross-scale aggregation. Experiments show that HSVLT outperforms existing methods on three benchmark datasets with lower computational cost."}, {"title": "ACKNOWLEDGMENTS", "content": "This work was supported in part by the Major Technological Innovation Project of Hangzhou (No. 2022AIZD0147), National Key Research and Development Project (No. 2022YFC2504605), Zhejiang Provincial Natural Science Foundation of China (No. LZ22F020012), Major Scientific Research Project of Zhejiang Lab (No. 2020ND8AD01)."}]}