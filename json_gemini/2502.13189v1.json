{"title": "MOBA: MIXTURE OF BLOCK ATTENTION FOR LONG-CONTEXT LLMS", "authors": ["Enzhe Lu", "Zhejun Jiang", "Jingyuan Liu", "Yulun Du", "Tao Jiang", "Chao Hong", "Shaowei Liu", "Weiran He", "Enming Yuan", "Yuzhi Wang", "Zhiqi Huang", "Huan Yuan", "Suting Xu", "Xinran Xu", "Guokun Lai", "Yanru Chen", "Huabin Zheng", "Junjie Yan", "Jianlin Su", "Xinyu Zhou", "Yuxin Wu", "Neo Y. Zhang", "Zhilin Yang", "Mingxing Zhang", "Jiezhong Qiu"], "abstract": "Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored.\nIn this work, we propose a solution that adheres to the \u201cless structure\" principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MOBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to support Kimi's long-context requests and demonstrates significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA.", "sections": [{"title": "Introduction", "content": "The pursuit of artificial general intelligence (AGI) has driven the development of large language models (LLMs) to unprecedented scales, with the promise of handling complex tasks that mimic human cognition. A pivotal capability for achieving AGI is the ability to process, understand, and generate long sequences, which is essential for a wide range of applications, from historical data analysis to complex reasoning and decision-making processes. This growing demand for extended context processing can be seen not only in the popularity of long input prompt understanding, as showcased by models like Kimi (MoonshotAI 2023), Claude (Anthropic 2023) and Gemini (Reid et al. 2024), but also in recent explorations of long chain-of-thought (CoT) output capabilities in Kimi k1.5 (Team et al. 2025), DeepSeek-R1 (D. Guo et al. 2025), and OpenAI 01/03 (Guan et al. 2024).\nHowever, extending the sequence length in LLMs is non-trivial due to the quadratic growth in computational complexity associated with the vanilla attention mechanism (Waswani et al. 2017). This challenge has spurred a wave of research aimed at improving efficiency without sacrificing performance. One prominent direction capitalizes on the inherent sparsity of attention scores. This sparsity arises both mathematically from the softmax operation, where various sparse attention patterns have be studied (H. Jiang et al. 2024) and biologically (Watson et al. 2025), where sparse connectivity is observed in brain regions related to memory storage."}, {"title": "Method", "content": "In this work, we introduce a novel architecture, termed Mixture of Block Attention (MOBA), which extends the capabilities of the Transformer model by dynamically selecting historical segments (blocks) for attention computation. MOBA is inspired by techniques of Mixture of Experts (MoE) and sparse attention. The former technique has been predominantly applied to the feedforward network (FFN) layers within the Transformer architecture, while the latter has been widely adopted in scaling Transformers to handle long contexts. Our method is innovative in applying the MoE principle to the attention mechanism itself, allowing for more efficient and effective processing of long sequences."}, {"title": "Preliminaries: Standard Attention in Transformer", "content": "We first revisit the standard Attention in Transformers. For simplicity, we revisit the case where a single query token $q \\in R^{1\\times d}$ attends to the N key and value tokens, denoting $K, V \\in R^{N\\times d}$, respectively. The standard attention is computed as:\n$Attn(q, K, V) = Softmax(qK^T)V,$\nwhere d denotes the dimension of a single attention head. We focus on the single-head scenario for clarity. The extension to multi-head attention involves concatenating the outputs from multiple such single-head attention operations."}, {"title": "MOBA Architecture", "content": "Different from standard attention where each query tokens attend to the entire context, MoBA enables each query token to only attend to a subset of keys and values:\n$MOBA(q, K, V) = Softmax(qK[I]^T)V[I],$\nwhere $I \\subset [N]$ is the set of selected keys and values.\nThe key innovation in MOBA is the block partitioning and selection strategy. We divide the full context of length N into n blocks, where each block represents a subset of subsequent tokens. Without loss of generality, we assume that the context length N is divisible by the number of blocks n. We further denote $B = \\frac{N}{n}$ to be the block size and\n$I_i = [(i - 1) \\times B + 1, i \\times B]$\nto be the range of the i-th block. By applying the top-k gating mechanism from MoE, we enable each query to selectively focus on a subset of tokens from different blocks, rather than the entire context:\n$I = \\bigcup_{g_i>0} I_i$.\nThe model employs a gating mechanism, as $g_i$ in Equation 4, to select the most relevant blocks for each query token. The MOBA gate first computes the affinity score $s_i$ measuring the relevance between query q and the i-th block, and applies a top-k gating among all blocks. More formally, the gate value for the i-th block $g_i$ is computed by\n$g_i =\\begin{cases}1 & s_i \\in Topk({s_j|j \\in [n]},k) \\\\0 & otherwise\\end{cases},$\nwhere $Topk(., k)$ denotes the set containing k highest scores among the affinity scores calculated for each block. In this work, the score $s_i$ is computed by the inner product between q and the mean pooling of $K [l_i]$ along the sequence dimension:\n$s_i = (q, mean_pool(K[I]))$\nA Running Example. We provide a running example of MoBA at Figure 1a, where we have two query tokens and four KV blocks. The router (gating network) dynamically selects the top two blocks for each query to attend. As shown in Figure 1a, the first query is assigned to the first and second blocks, while the second query is assigned to the third and fourth blocks."}, {"title": "", "content": "It is important to maintain causality in autoregressive language models, as they generate text by next-token prediction based on previous tokens. This sequential generation process ensures that a token cannot influence tokens that come before it, thus preserving the causal relationship. MoBA preserves causality through two specific designs:"}, {"title": "Causality: No Attention to Future Blocks.", "content": "MoBA ensures that a query token cannot be routed to any future blocks. By limiting the attention scope to current and past blocks, MoBA adheres to the autoregressive nature of language modeling. More formally, denoting pos(q) as the position index of the query q, we set $s_i = -\\infty$ and $g_i = 0$ for any blocks i such that $pos(q) < i \\times B$."}, {"title": "Current Block Attention and Causal Masking.", "content": "We define the \"current block\" as the block that contains the query token itself. The routing to the current block could also violate causality, since mean pooling across the entire block can inadvertently include information from future tokens. To address this, we enforce that each token must be routed to its respective current block and apply a causal mask during the current block attention. This strategy not only avoids any leakage of information from subsequent tokens but also encourages attention to the local context. More formally, we set $g_i = 1$ for the block i where the position of the query token pos(q) is within the interval $I_i$. From the perspective of Mixture-of-Experts (MoE), the current block attention in MoBA is akin to the role of shared experts in modern MoE architectures (Dai et al. 2024; A. Yang et al. 2024), where static routing rules are added when expert selection.\nNext, we discuss some additional key design choices of MoBA, such as its block segmentation strategy and the hybrid of MOBA and full attention."}, {"title": "Fine-Grained Block Segmentation.", "content": "The positive impact of fine-grained expert segmentation in improving mode performance has been well-documented in the Mixture-of-Experts (MoE) literature (Dai et al. 2024; A. Yang et al. 2024). In this work, we explore the potential advantage of applying a similar fine-grained segmentation technique to MOBA. MOBA, inspired by MoE, operates segmentation along the context-length dimension rather than the FFN intermediate hidden dimension. Therefore our investigation aims to determine if MoBA can also benefit when we partition the context into blocks with a finer grain. More experimental results can be found in Section 3.1."}, {"title": "Hybrid of MoBA and Full Attention.", "content": "MoBA is designed to be a substitute for full attention, maintaining the same number of parameters without any addition or subtraction. This feature inspires us to conduct smooth transitions between full attention and MoBA. Specifically, at the initialization stage, each attention layer has the option to select full attention or MoBA, and this choice can be dynamically altered during training if necessary. A similar idea of transitioning full attention to sliding window attention has been studied in previous work (X. Zhang et al. 2024). More experimental results can be found in Section 3.2."}, {"title": "Comparing to Sliding Window Attention and Attention Sink.", "content": "Sliding window attention (SWA) and attention sink are two popular sparse attention architectures. We demonstrate that both can be viewed as special cases of MOBA. For sliding window attention (Beltagy et al. 2020), each query token only attends to its neighboring tokens. This can be interpreted as a variant of MoBA with a gating network that keeps selecting the most recent blocks. Similarly, attention sink (G. Xiao et al. 2023), where each query token attends to a combination of initial tokens and the most recent tokens, can be seen as a variant of MOBA with a gating network that always selects both the initial and the recent blocks. The above discussion shows that MoBA has stronger expressive power than sliding window attention and attention sink. Moreover, it shows that MoBA can flexibly approximate many static sparse attention architectures by incorporating specific gating networks.\nOverall, MoBA's attention mechanism allows the model to adaptively and dynamically focus on the most informative blocks of the context. This is particularly beneficial for tasks involving long documents or sequences, where attending to the entire context may be unnecessary and computationally expensive. MoBA's ability to selectively attend to relevant blocks enables more nuanced and efficient processing of information."}, {"title": "Implementation", "content": "We provide a high-performance implementation of MoBA, by incorporating optimization techniques from FlashAttention (Dao, D. Fu, et al. 2022) and MoE (Rajbhandari et al. 2022). Figure 2 demonstrates the high efficiency of MOBA, while we defer the detailed experiments on efficiency and scalability to Section 3.4. Our implementation consists of five major steps:\n\u2022 Determine the assignment of query tokens to KV blocks according to the gating network and causal mask.\n\u2022 Arrange the ordering of query tokens based on their assigned KV blocks.\n\u2022 Compute attention outputs for each KV block and the query tokens assigned to it. This step can be optimized by FlashAttention with varying lengths."}, {"title": "Efficiency and Scalability", "content": "The above experimental results show that MoBA achieves comparable performance not only regarding language model losses but also in real-world tasks. To further investigate its efficiency, we compare the forward pass time of the attention layer in two models trained in Section 3.3 - Llama-8B-1M-MOBA and Llama-8B-1M-Full. We focus solely on the attention layer, as all other layers (e.g., FFN) have identical FLOPs in both models. As shown in Figure 2a, MOBA is more efficient than full attention across all context lengths, demonstrating a sub-quadratic computational complexity. In particular, it achieves a speedup ratio of up to 6.5x when prefilling 1M tokens.\nWe also explore the length scalability of MoBA by gradually increasing the context length to 10 million tokens. To maintain a constant attention sparsity, we keep the top-k value and number of MoBA Block fixed while proportionally increasing the block size. To reach the 10M context length, we expanded tensor parallelism (Shoeybi et al. 2019) toward the query head level, Specifically, we broadcast key and value tensors across distributed query heads, effec-tively addressing GPU memory limitations while preserving computational efficiency. As shown in Figure 2b, MOBA demonstrates superior efficiency compared to standard Flash Attention when scaling to longer sequences. Specifically, at 10M tokens moba achieves a speedup ratio of 16x reduction in attention computation time. The inset graph in the top figure, focusing on shorter sequences (32K to 512K), shows that even though both methods perform comparably at smaller scales, MoBA's computational advantage becomes increasingly evident as sequences grow longer, highlighting its particular strength in processing extremely long sequences.\nOverall, the high efficiency of MoBA can be attributed to two key innovations: (1) the block sparse attention mech-anism, and (2) the optimized implementation combining Mixture-of-Experts (MoE) and FlashAttention, as described in Section 2.3. These techniques effectively address the quadratic complexity limitation of full attention, reducing the computational complexity to a more economical sub-quadratic scale."}, {"title": "Related Work", "content": "The development of efficient attention (Tay, Dehghani, et al. 2020) mechanisms has been a critical area of research in the field of natural language processing, particularly with the rise of Large Language Models (LLMs). As the demand for handling longer sequences and reducing computational costs grows, efficeint attention techniques have emerged as a promising solution to reduce the quadratic complexity of self-attention mechanisms while maintaining model performance.\nStatic Sparse Patterns: Significant efforts, such as Sparse Transformer (Child et al. 2019), Star-Transformer (Q. Guo et al. 2019), BlockBERT (Qiu et al. 2019), Longformer (Beltagy et al. 2020), GMAT (Gupta et al. 2020), ETC (Ainslie, Ontanon, et al. 2020), BigBird (Zaheer et al. 2020), LongT5 (M. Guo et al. 2021) and LongNet (J. Ding et al. 2023), have been dedicated to the design of static attention patterns in LLMs. Their choices of static attention patterns can encompass strided and fixed attention, window attention, global token attention, random attention, dilated attention, block sparse attention, or any combinations of them. In the realm of multimodal models, static sparse attention mechanisms have also been developed, such as axial attention (Ho et al. 2019) for 2D images and spatial-temporal attention (Z. Zheng et al. 2024) for 3D videos.\nDynamic Sparse Patterns: Different from static patterns, dynamic sparse attention techniques adaptively determine which tokens to attend. Reformer (Kitaev et al. 2020) and Routing Transformer (Roy et al. 2021) respectively employ locality-sensitive hashing (LSH) and K-means to cluster tokens, and attend to clusters rather than the full context. Memorizing Transformers (Yuhuai Wu et al. 2022) and Unlimiformer (Bertsch et al. 2024) dynamically attend to tokens selected by the k-nearest-neighbor (kNN) algorithms. CoLT5 (Ainslie, Lei, et al. 2023) designs a routing modules to select the most important queries and keys. Sparse Sinkhorn Attention (Tay, Bahri, et al. 2020) learns to permute blocks from the input sequence, allowing dynamic block sparse attention computation.\nTraining-free Sparse Attention: In addition to the previously discussed approaches that study training sparse atten-tion models, there are also strategies designed to incorporate sparse attention mechanisms to enhance the efficiency of the two primary stages of model inference either the prefill stage or the decode stage, or both of them. During the prefill optimization phase, the complete prompt can be utilized for attention profiling, which allows for the exploration of more intricate sparse attention patterns. For instance, MoA (T. Fu et al. 2024), Minference (H. Jiang et al. 2024), and SeerAttention (Y. Gao et al. 2024) have investigated sparse attention configurations such as A-shape, vertical-slash, and dynamic block sparsity. In the context of decode optimization, considerable work has been dedicated to compressing and pruning the KV-cache to achieve a balance between the quality and speed of text generation. Notable efforts in this area include H2O (Z. Zhang et al. 2024), StreamingLLM (G. Xiao et al. 2023), TOVA (Oren et al. 2024), FastGen (Ge et al. 2023) and Quest (Tang et al. 2024). Quest, in particular, can be viewed as MoBA with a smaller block size and a specialized block representation function which combines both min and max pooling. Another work"}, {"title": "", "content": "closely related to MoBA is Longheads (Y. Lu et al. 2024) which can be viewed as MoBA with a top-1 gating network, meaning that each query selects the most relevant KV blocks for attention.\nBeyond Traditional Attention Architecture: Another line of research investigates novel model architectures that deviate from the conventional attention mechanism. As architectures change, these methods require training models from scratch and are unable to reuse pre-trained Transformer-based models. Studies in this domain have explored architectures that are inspired by Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), State Space Models (SSMs), or Linear Attention (Katharopoulos et al. 2020), Examples of such models include Hyena (Poli et al. 2023), Performer (Choromanski et al. 2020), Linformer (S. Wang et al. 2020), RWKV Peng, Alcaide, et al. 2023, Mamba (Gu et al. 2023), RetNet (Sun et al. 2023), etc.\nIn summary, the landscape of efficient attention techniques is diverse, encompassing sparse patterns that range from static to dynamic, optimization objectives that span from training to inference, and architectures that extend from traditional attention mechanisms to innovative alternatives. Each method presents unique advantages and trade-offs, and the choice of technique often depends on the specific requirements of the application, such as the maximum sequence length, computational resources, and the desired balance between efficiency and performance. As research in this area continues to evolve, it is expected that these methods will play a crucial role in enabling LLMs to tackle increasingly complex tasks while maintaining efficiency and scalability."}, {"title": "Conclusion", "content": "In this paper, we introduce Mixture of Block Attention (MoBA), a novel attention architecture inspired by the princi-ples of Mixture of Experts (MoE) that aims to enhance the efficiency and scalability of large language models (LLMs) for long-context tasks. MoBA addresses the computational challenges associated with traditional attention mecha-nisms by partitioning the context into blocks and employing a dynamic gating mechanism to selectively route query tokens to the most relevant KV blocks. This approach not only reduces computational complexity but also maintains model performance. Moreover, it allows for seamless transitions between full and sparse attention. Through extensive experiments, we demonstrated that MoBA achieves performance comparable to full attention while significantly im-proving computational efficiency. Our results show that MoBA can scale effectively to long contexts, maintaining low LM losses and high performance on various benchmarks. Additionally, MoBA's flexibility allows it to be integrated with existing models without substantial training cost, making it a practical continual pre-training solution for enhanc-ing long-context capabilities in LLMs. In summary, MoBA represents a significant advancement in efficient attention, offering a balanced approach between performance and efficiency. Future work may explore further optimizations of MOBA's block-selection strategies, investigate its application to other modalities, and study its potential for improving generalization in complex reasoning tasks."}, {"title": "Appendix", "content": "To address the bias in natural data distribution that favors short contexts, we strategically segmented the overall se-quences into discrete segments based on their actual positions. For example, the segment spanning positions 30K-32K exclusively reflects losses associated with documents exceeding 30K context lengths and also masks the positions from 30K to 32K. This approach ensures a more balanced and representative evaluation across different context lengths. In our exploration of long-context scalability, we made a pivotal discovery: the trailing tokens account for the majority of the performance discrepancy between the full context baseline and the newly proposed sparse attention architectures. Consequently, we streamlined the long-context scaling process by focusing on trailing token scaling. This not only simplifies the computational requirements but also significantly enhances the efficiency and effectiveness of investi-gating long-context scenarios. This finding holds substantial implications for the development of more efficient and scalable attention mechanisms in the future."}]}