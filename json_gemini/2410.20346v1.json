{"title": "Historical Test-time Prompt Tuning for Vision Foundation Models", "authors": ["Jingyi Zhang", "Jiaxing Huang", "Xiaoqin Zhang", "Ling Shao", "Shijian Lu"], "abstract": "Test-time prompt tuning, which learns prompts online with unlabelled test samples during the inference stage, has demonstrated great potential by learning effective prompts on-the-fly without requiring any task-specific annotations. However, its performance often degrades clearly along the tuning process when the prompts are continuously updated with the test data flow, and the degradation becomes more severe when the domain of test samples changes continuously. We propose HisTPT, a Historical Test-time Prompt Tuning technique that memorizes the useful knowledge of the learnt test samples and enables robust test-time prompt tuning with the memorized knowledge. HisTPT introduces three types of knowledge banks, namely, local knowledge bank, hard-sample knowledge bank, and global knowledge bank, each of which works with different mechanisms for effective knowledge memorization and test-time prompt optimization. In addition, HisTPT features an adaptive knowledge retrieval mechanism that regularizes the prediction of each test sample by adaptively retrieving the memorized knowledge. Extensive experiments show that HisTPT achieves superior prompt tuning performance consistently while handling different visual recognition tasks (e.g., image classification, semantic segmentation, and object detection) and test samples from continuously changing domains.", "sections": [{"title": "1 Introduction", "content": "Vision Foundation Models (VFMs) [1, 2, 3] have demonstrated impressive zero shot generalization capabilities over various downstream tasks at the cost of domain expertise for crafting appropriate task-specific prompts [4, 5, 6]. To circumvent this limitation, prompt learning [4], which aims to adapt VFMs to fit downstream tasks by optimizing prompts as learnable vectors with few-shot task training samples, has been extensively explored recently. However, existing prompt tuning methods generally suffer from two constraints: 1) they require labelled training data for each downstream task which can be tedious and laborious to collect [7, 8], and 2) the learnt prompts tend to overfit to the few-shot training samples, leading to degraded generalization toward downstream tasks [9, 10, 11]. Test-time prompt tuning [7] instead learns prompts with a online flow of unlabelled test samples during the inference stage. It has attracted increasing attention recently as it allows learning effective prompts on-the-fly without requiring any task-specific annotations as illustrated in Fig. 1 (a).\nExisting test-time prompt tuning methods usually start with an initial template prompt like \u201ca photo of a [class]\u201d and optimize it with a self-supervised objective over test images together with their model predictions [7, 8]. However, these methods often experience a clear performance degradation along the tuning process when the prompts are continuously updated with the test data flow, largely due to the lack of test-sample annotations as illustrated in Fig. 1 (b). Specifically, these methods"}, {"title": "2 Related Work", "content": "Test-time Adaptation, which is a type of domain adaptation technique [18, 19, 20, 21], aims for designing the technique to improve model generalization over test samples [22, 23, 24]. Early studies such as test-time training (TTT) and its variants [22, 23], introduce auxiliary tasks (e.g., rotation prediction task [25]) into the supervised training objective to improve the model generalization at the training stage, and then adapt the pre-trained model to test samples via self-supervised objectives at the inference stage. Differently, recent studies [24, 20, 26, 27, 28, 29, 30, 31] generally focuses on fully test-time adaptation, where the model is adapted to test samples only during the inference stage, without introducing any auxiliary task into the training phase. For example, TENT [24] minimizes the batch-wise prediction entropy for test images while MEMO [27] enforces the prediction consistency between different augmentations of each test sample. With the advent of vision foundation models (VFMs), test-time prompt tuning [7, 8] has recently been explored for adapting pre-trained VFMs toward downstream tasks via prompt tuning at the inference stage.\nPrompt Learning of Vision Foundation Models (VFMs) [1, 2, 3] has been studied extensively as VFMs despite their impressive zero-shot generalization capabilities over various downstream tasks often require to design appropriate task-specific prompts for optimal adaptation. Inspired by the \u201cprompt learning\u201d in NLP [32], one typical prompt learning approach for VFMs [4, 9, 33, 34, 35, 36, 37, 38, 39, 40, 41] learns to optimize prompts as learnable vectors with few-shot labelled samples of downstream tasks. Despite its effectiveness, it requires to label task-specific training data which is often laborious with poor scalability [7]. In addition, the learnt prompts tend to overfit to few-shot task samples, and this often degrades the generalization of VFMs while adapting toward various downstream tasks [7]. Different from prompt learning, test-time prompt tuning [7, 8] explores a new prompt learning setup that learns prompts on-the-fly with an online flow of unlabelled test images during the inference stage.\nTest-time Prompt Tuning (TPT) aims to learn prompts on-the-fly using the test samples at inference. It has attracted increasing attention recently [7, 8, 42, 43, 44, 45] as it can learn effective prompts online with unlabelled test samples flow continuously. Most existing test-time prompt tuning studies focus on image classification tasks [7, 8, 42, 43, 44, 45]. For example, TPT [7] optimizes prompts by minimizing the prediction entropy between each test sample and its augmented views. DiffTPT [8] improves the TPT by introducing the pre-trained diffusion model [46] to produce multiple diverse and informative augmented views. Different from these studies [7, 8, 42, 43, 44, 45], HisTPT aims to mitigate the knowledge 'forgetting' problem in test-time prompt tuning when the text tokens are continuously updated with the test data flow. HisTPT achieves it by constructing comprehensive memorization capturing useful historical knowledge. In addition, HisTPT achieves superior performance across various visual recognition tasks consistently, and it can effectively handle the challenging scenario where the domain of test samples changes continuously.\nMemory-based Learning has been studied extensively in computer vision [12, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57], such as semi-supervised learning [51, 58], long-term video understanding [15, 59] and domain adaptation [60, 61, 14]. For the adaptation of vision foundation models (VFMs), several studies employ memory for improving the performance on downstream tasks [62, 63, 64, 65, 66]. For instance, [66] tackles image captioning challenge by memorizing visual-related sentences which helps VFMs to generate high-quality captions with fewer hallucinations. [65] replaces text features by identity-specific sequence features extracted by CLIP, which effectively facilitates video-based person re-identification. [64] and [62] enable efficient training-free VFMs adaptation by caching category-specific data features. Different from these studies, HisTPT designs three types of knowledge banks for memorizing useful knowledge learnt from previously test samples and introduces an adaptive knowledge retrieval mechanism that retrieves memorized knowledge for each test sample adaptively, aiming for mitigating the knowledge 'forgetting' problem in test-time prompt tuning."}, {"title": "3 Method", "content": null}, {"title": "3.1 Preliminaries and Task Definition", "content": "Preliminaries of Vision Foundation Models (VFMs). We denote a pre-trained VFM by $\\mathcal{F} = {F_I, F_T}$, where $F_I$ and $F_T$ are image encoder and text encoder respectively. Given a test image $x \\in \\mathcal{X}_{test}$ and the names of its possible belonged classes $y \\in \\mathcal{Y}_{test} = {y_c}_{c=1}^C$, the VFM image"}, {"title": "3.2 Historical Test-time Prompt Tuning", "content": "We design three types of knowledge banks to help memorize the useful knowledge learnt from the previous test samples and adaptively exploit the memorized knowledge for regularizing the prediction of the current test samples. As illustrated in Fig. 2, local knowledge bank buffers features of the recent test images, capturing up-to-date distribution changes along the tuning process. Hard-sample knowledge bank actively identifies and stores hard samples from the local knowledge bank, which helps to capture difficult and corner features. Global knowledge bank maintains global and representative information along the whole prompt tuning process by accumulating all the features from the local knowledge bank and hard-sample knowledge bank. In addition, HisTPT introduces an adaptive knowledge retrieval mechanism that adaptively retrieves relevant memorized knowledge for prediction regularization and prompt optimization for each test image.\nGiven a continuous flow of N test samples $X_{test} = {x_n}_{n=1}^N$, we take the time step n as an example to describe the knowledge bank construction with the previous test sample $x_{n-1}$ and the prompt optimization of the current sample $x_n$ with the memorized knowledge.\nKnowledge Bank Construction. HisTPT comes with three types of knowledge banks for capturing fresh and representative knowledge during the test-time prompt tuning with previous test samples.\nLocal Knowledge Bank captures and stores fresh and up-to-date knowledge by buffering the features of the recent test samples. It works as a FIFO queue with a fixed size of L, where the features of the oldest test sample will be dequeued and the features of the most recent test sample will be enqueued to update the local knowledge bank, i.e, $M_{local} = {u_{local}, P_{local}}_l=1^L$ on the flow. Specifically, for the latest test sample $x_{n-1}$ and its learnt text tokens $t_{n-1}$, local knowledge bank enqueues its text feature $u_{n-1}$ and prediction probability $p_{n-1}$, i.e., $u_{n-1} = {u_{n-1}^c}_{c=1}^C$ where $u_{n-1}^c = F_T ((t_{n-1}; y_c))$, and $p_{n-1} = {p_{n-1}^c}_{c=1}^C$ where $p_{n-1}^c$ is calculated via Eq. 1. Note that the size of local knowledge bank L is much smaller than the total number of test samples N since local knowledge bank aims to capture fresh information and up-to-date distribution changes of test samples along the test-time prompt tuning process.\nHard-sample Knowledge Bank identifies hard samples from local knowledge bank for capturing difficult and corner information. We identify hard samples by those having high classification"}, {"title": "4 Experiments", "content": "This section presents experiments including datasets, implementation details, benchmarking with the state-of-the-art, as well as discussion of our designs."}, {"title": "4.1 Datasets", "content": "We evaluate HisTPT over multiple datasets across three widely studied visual recognition tasks:\nSemantic Segmentation: We benchmark HisTPT over 6 image segmentation datasets with pixel-wise annotations, including Cityscapes [16], BDD100K [67], Mapillary [68], ADE20K [69], Pascal Content [70] and ACDC [17].\nImage Classification: We benchmark HisTPT over 10 classification datasets, including Flowers102 [71], DTD [72], Oxford-Pets [73], StanfordCars [74], UCF101 [75], Caltech101 [76], Food101 [77], SUN397 [78], Aircraft [79] and EuroSAT [80].\nObject Detection: We benchmark HisTPT over 4 object detection datasets, including Cityscapes [16], BDD100K [67], ADE20K [69] and ACDC [17]."}, {"title": "4.2 Implementation Details", "content": "Semantic Segmentation: Following [81], we adopt SEEM [3] with two vision backbones including Focal-Tiny [82] and Davit-Large [83] as the segmentation foundation models. In training, we employ AdamW optimizer [84] with a weight decay of 0.05, and set the initial learning rate as 0.0001.\nImage Classification: Following [7, 8], we use CLIP [1] with two backbones, i.e., ResNet-50 [85] and ViT-B/16 [86], as the classification foundation models. In training, we adopt AdamW optimizer [84] with a weight decay of 0.01, and set the initial learning rate as 0.005.\nObject Detection: For object detection task, we adopt SEEM [3] with two vision backbones including Focal-Tiny [82] and Davit-Large [83] as the detection foundation models. In training, we employ AdamW optimizer [84] with a weight decay of 0.05, and set the initial learning rate as 0.0001.\nFor all experiments, the prompt is initialized as \u201ca photo of a\u201d and the corresponding 4 tokens (i.e., M = 4) of dimension D = 512 are optimized as in [7, 8]. Unless otherwise specified, we set the size of the local knowledge bank and hard-sample knowledge bank at L = H = 32 and the number of the selected hard-sample features K at 16. We set the update coefficient \u03b3 of the global knowledge bank at 0.99. Following [7], we set the optimization step in test-time prompt tuning at 1 by default. All the experiments are conducted on one NVIDIA Tesla V100 GPU with batch size 1."}, {"title": "4.3 Comparisons with State of the Arts", "content": "Semantic Segmentation. We evaluate and benchmark HisTPT over 6 semantic segmentation datasets. Since there is little prior study on test-time prompt tuning on semantic segmentation, we benchmark HisTPT by reproducing methods [7, 8], which are designed for image classification task, on semantic segmentation task. Table 1 shows experimental results. We can observe that HisTPT achieves superior segmentation performance, largely due to its comprehensive memorization that helps to regularize the predictions of test samples and mitigates the knowledge forgetting problem in test-time prompt tuning. In addition, HisTPT is complementary to existing methods and produces clear and consistent performance boosts. This is attributed to the proposed HisTPT which can effectively mitigate the knowledge forgetting existing methods.\nImage Classification. Following [7, 8], we evaluate HisTPT over 10 image classification tasks. To suit the setup in this work, we reproduce methods [7, 8] by keeping their prompts continuously updated during the test-time adaptation. As shown in Table 2, HisTPT outperforms state-of-the-art methods consistently over different classification tasks such as classic classification on Flowers102 [71], texture classification on DTD [72] and human action recognition on UCF101 [75]. This demonstrates the superior generalization ability while HisTPT faces diverse downstream data.\nObject Detection. We evaluate and benchmark HisTPT over 4 object detection datasets. Similar to semantic segmentation benchmarking, we benchmark HisTPT by reproducing methods [7, 8] (designed for image classification task) on the object detection task. As shown in Table 3, HisTPT achieves superior detection performance and can well handle a wide range of detection tasks including detection under various weather conditions [17] across different scenes [16, 69]. The superior detection performance is largely attributed to the knowledge banks in HisTPT which effectively help generate more accurate predictions and learn better prompts for test samples."}, {"title": "4.4 Ablation Studies", "content": "We examine the proposed HisTPT by performing ablation study over Cityscapes semantic segmentation task. As shown in Table 4, the three types of knowledge banks can work well alone and improve"}, {"title": "4.5 Discussion", "content": "Complementarity to Prompt Learning Methods. As a test-time tuning technique, the proposed HisTPT is complementary to prompt learning methods that learn prompts at the training stage. We examine this feature by setting the learnt prompts by prompt learning [4, 9] as the initial prompts of HisTPT. As Table 5 shows, equipping HisTPT with the learnt prompts improves the performance clearly, indicating that HisTPT as a plug-in can greatly enhance existing prompt learning methods.\nOptimization Steps. We examined how the optimization step\naffects HisTPT by increasing it from 1 to 10. Figure 3 shows the\nmean mIoU over 6 semantic segmentation datasets with SEEM-\nTiny. We can observe that increasing the optimization step im-\nproves segmentation consistently. Nevertheless, the performance\ngain becomes marginal after 6-8 optimization steps. The actual\noptimization step can be set by balancing the inference efficiency\nand the inference accuracy.\nContinuously Changing Test Domains. As discussed in Sec-\ntion 1, HisTPT can handle challenging scenarios when the domain of test samples changes continu-\nously. We examine this feature over semantic segmentation data that were collected under normal\nweather [16] and various adverse weathers [17, 87] (fog, night, rain and snow). As Table 6(a) shows,\nthe performance of existing test-time prompt tuning methods TPT [7] and DiffTPT [8] degrades\ngradually along the tuning process when the weather changes from normal to adverse, largely due\nto increasing error accumulation and \u2018forgetting\u2019 while the test domain changes continuously. As a"}, {"title": "5 Conclusion", "content": "This paper introduces Historical Test-time Prompt Tuning (HisTPT), a general test-time prompt tuning framework that aims to mitigate the 'knowledge forgetting' problem across various visual recognition tasks. HisTPT introduces three types of knowledge banks, including local knowledge bank, hard-sample knowledge bank and global knowledge bank, each of which works with different mechanisms for memorizing useful knowledge. With the three knowledge banks, HisTPT builds up comprehensive memorization that preserves useful knowledge from previous test samples, mitigating the knowledge forgetting and enabling robust test-time prompt tuning. In addition, HisTPT comes with an adaptive knowledge retrieval mechanism that regularizes the prediction of the current test sample by adaptively retrieving the memorized knowledge. Extensive experiments show that HisTPT achieves superior performance consistently across various vision tasks. In addition, HisTPT can effectively handle the challenging scenario where the domain of test samples changes continuously. Moving forwards, we will further investigate memory-based learning for adaptation of vision foundation models."}]}