{"title": "GSpect: Spectral Filtering for Cross-Scale Graph Classification", "authors": ["Xiaoyu Zhang", "Wenchuan Yang", "Jiawei Feng", "Bitao Dai", "Tianci Bu", "Xin Lu"], "abstract": "Abstract-Identifying structures in common forms the basis\nfor networked systems design and optimization. However, real\nstructures represented by graphs are often of varying sizes,\nleading to the low accuracy of traditional graph classification\nmethods. These graphs are called cross-scale graphs. To overcome\nthis limitation, in this study, we propose GSpect, an advanced\nspectral graph filtering model for cross-scale graph classification\ntasks. Compared with other methods, we use graph wavelet\nneural networks for the convolution layer of the model, which\naggregates multi-scale messages to generate graph representa-\ntions. We design a spectral-pooling layer which aggregates nodes\nto one node to reduce the cross-scale graphs to the same size. We\ncollect and construct the cross-scale benchmark data set, MSG\n(Multi Scale Graphs). Experiments reveal that, on open data sets,\nGSpect improves the performance of classification accuracy by\n1.62% on average, and for a maximum of 3.33% on PROTEINS.\nOn MSG, GSpect improves the performance of classification\naccuracy by 15.55% on average. GSpect fills the gap in cross-\nscale graph classification studies and has potential to provide\nassistance in application research like diagnosis of brain disease\nby predicting the brain network's label and developing new drugs\nwith molecular structures learned from their counterparts in\nother systems.\nIndex Terms-complex networks, graph neural networks,\ngraph classification, cross-scale, spectral graph theory", "sections": [{"title": "I. INTRODUCTION", "content": "DATA that have a non-Euclid structure such as pro-tein structures [1], social networks [2] and compounds\n[3] are often represented by graphs with nodes and edges.\nAs structure determines function in many networked systems,\ngraph classification (for exact definition, please refer to III-A)\nis a fundamental research problem in numerous fields. For\nexample, in computer vision, graph classification methods are\nused to measure the similarity of human action recognition\namong graphs [4]. In neuroscience, researchers use graph\nclassification methods to study the similarity of brain networks\n[5]. In chemistry, graph classification methods are used to\nlearn the similarity of chemical compounds in terms of their\neffect on reaction partners [6]. Fields such as bioinformatics\nand molecular chemistry often encounter a problem named\ngraph classification: graphs with different structures possess\ntotally different functions. Researchers must separate graphs\nwith different structures to select appropriate graphs in a short\ntime. For example, Alzheimer's disease (AD) is known to be\ncaused by structural changes in the brain. Researchers take\nsamples of brain networks and determine whether the sample\nis likely to develop AD [7].\nResearchers have proposed numerous methods to accom-\nplish the graph classification problem, like graph kernels [8].\nGraph kernels can be used for graph classification. However,\nthese methods often define graphs in a heuristic manner,\nthereby resulting in low explainability and flexibility of these\nmethods. It is for this reason that graph neural networks\n(GNNs) have become a popular method for graph classification\ntasks in recent years due to their ability to learn node and edge\nrepresentations and capture messages from complex graph\nstructures. Researchers usually design a GNN convolution\nlayer to obtain the graph representation and design a GNN-\nbased pooling layer to reduce the size of the graphs. One\nof the most classic definition of GNN convolution layers\nis spectral-based GNN. Spectral-based GNN use diagonal\nspectral filters to capture messages on the spectrum. This\nmethod has been wildly used for node classification and edge\nprediction tasks [9] [10]. However, spectral-based methods\nhave a few limitations [11]: First, any perturbation to the\ngraph results in the change of the graph's eigenvalues. Second,\nthe learned filters are size-dependent. One graph determines a\nunique network structure, which implies that it is difficult to\nbe applied to graphs with different sizes. So it is difficult to\nbe used in the cross-scale graph classification tasks.\nTraditional graph classification methods only work on com-\nparing structure of similar sizes [12] [13] [14]. However, in\npractice, structures of an order-of-magnitude difference in size\nmay have the same function. For example, in biology, the\nstructure of proteins, which possesses critical functions-such\nas immune signaling [15], targeted therapeutics [16], sense-\nresponse systems [17] and self-assembly materials [18]\u2014can\nbe represented as graphs whose nodes represent the atoms\nand edges represent the chemical bonds. The protein structure\ndetermines its function. Certain proteins which have the same\nfunctions usually have similar graph structures. However,\nthese protein-graphs occasionally have an order-of-magnitude\ndifference in the number of nodes [19]. This set of graphs\nis called cross-scale graphs. Cross-scale graph classification\ntasks refers to dividing the graphs with an order-of-magnitude\ndifference in the number of nodes into sets. Research on cross-\nscale graphs is an important research direction in the field\nof complex networks. Cross-scale graphs plays an important"}, {"title": "II. RELATED WORKS", "content": "role in the practical applications such as network clustering\n[20], hierarchical reduction [21], and state partition [22].\nResearchers require cross-scale graph classification algorithms\nto select structure-similar but cross-scale proteins from a\nhuge selection space. [23] have proposed methods tailored to\ndatasets of varying graph sizes, but these studies have exclu-\nsively designed methods for small-scale, sparse graphs (don't\nconsider large-scale graphs) and conducted experiments only\non publicly available datasets with similar graph sizes. There\nis no available method for cross-scale graphs' classification\ntask and there are no open data sets with graphs which have\nenough difference (up to 103) in the number of nodes.\nGraph Wavelet Transform (GWT) is a powerful tool for\ncapturing multi-scale graph representations [24] [25] due to its\nunique properties, making it becomes a powerful tool to solve\ncross-scale graph classification problems. The advantages of\nGWT is listed as follows. GWT offers multi-scale analysis\ncapabilities, effectively representing both local and global\nfeatures of graph structures [24]. Besides, its localization prop-\nerties in both spatial and frequency domains enable efficient\ncapture of local structural information [26]. In addition, GWT\ntypically produces sparse representations of graph signals,\nfacilitating key feature extraction [27]. Compared to global\nspectral methods, GWT often demonstrates higher computa-\ntional efficiency, especially for large-scale graphs [28]. Apart\nfrom that, it naturally adapts to irregular graph structures, a\nchallenge for traditional wavelet transforms [29]. Recently,\nit is proved that GWT allows for cross-scale information\nintegration, helping to capture hierarchical structures in graphs\n[30]. Moreover, it exhibits robustness to minor structural\nchanges, which is valuable when dealing with noisy data [25].\nThese characteristics make GWT a versatile and effective tool\nfor multi-scale graph representation, with wide applications\nin graph classification, node classification, and graph signal\nprocessing.\nIn this article, we modified the spectral-based GNN using\nthe graph wavelet theory and design a novel framework\n(GSpect) to accomplish cross-scale graph classification tasks.\nSpecifically, considering the characteristic that the wavelet\nfunction can accurately capture the signal information in\ndifferent frequency bands, we first use a graph wavelet neural\nnetwork as the convolution layer for graph classification tasks.\nSecond, we design a graph pooling layer. Compared with\nother spectral clustering methods [31] [26], we directly per-\nform Fourier transformation on the graph's adjacency matrix\nand node attributes directly to obtain the frequency domain\nrepresentation. We use spectral filters to filter high-frequency\ninformation and resize the graph on the principle of the\nsave-most message. Third, considering the fact that there are\nno appropriate cross-scale graph classification data sets, we\ncollect three classes of empirical networks covering the set\nof protein structure data, macromolecular compound structure\ndata, and social networks in combination with the three typical\nmodeled networks of ER [32], WS [33] and BA [34], to create\na synthesis cross-scale graph classification benchmark data set\nMSG. We verify the performance of GSpect both on the open\ndata sets and on MSG.\nThis article makes the following contributions:"}, {"title": "A. Graph Kernel Models for Graph Classification", "content": "Graph kernels capture the similarity between graphs for\ngraph classification tasks. Given a set of graphs, the graph ker-\nnel methods aim to learn the kernel function that captures the\nsimilarity between any two graphs. Traditional graph kernels,\nsuch as random walk kernel, subtree kernel, and shortest-path\nkernels are widely used in graph classification tasks [8] [35].\nThe WL algorithm [36] maps the original graph to a sequence\nof graphs whose node attributes are generated from graph\ntopology and label information. A kernel family, including an\nefficient kernel family of comparison subtree patterns, can be\ndefined from this WL sequence. This algorithm has became\none of the most widely used graph kernel methods for graph\nclassification. Al-Rfou et al. [37] proposed deep divergence\ngraph kernels (DDGK). DDGK learn kernel functions for a\npair of graphs. Given two graphs G1 and G2, this method\nlearns a kernel function K(.) as a similarity metric function\nfor graphs. The function is defined in the following manner:\n$k (G_1, G_2) = ||\\Psi (G_1) \u2013 \\Psi (G_2)||^2$,                                                             (1)$\nwhere \u03a8 (G1) is the graph representation of G1. This method\nlearn the graph representation by computing the divergence of\nthe target graph. Given a set of source graphs G1, G2, ..., GN,\na graph encoder is the representation of each graph in the set.\nThen, for the target graph Gi, the divergence between Gi and\nthe source graph is computed to measure the similarity. The\nequation of divergence between Ga and Gb is as given bellow:\n$D' (G_a||G_b) = \\sum_{V_i \\in V_a} \\sum_{j,e_{ij} \\in E_a} - log P_r (V_j | V_i, H_b)$,                                                                    (2)$\nwhere a is the encoder trained on graph Ga. D'(Ga ||\nGb represents the divergence from graph Ga to graph Gb.\n$P_r(v_j|v_i, H_b)$ represents the probability of node $v_j$ occurring\ngiven node $v_i$ under the encoder $H_b$ of graph $G_b$.\nHowever, graph kernel models have a few limitations: Most\nof them have low computational efficiency, and graph kernel\nmethods use kernel functions(like Equation 1) to measure the\nsimilarity between two graphs, which implies that graph kernel\nmethods can't be used to handle graph classification problems\nwith a lot of graphs."}, {"title": "B. Classic GNN Models for Graph Classification", "content": "In recent years, researchers have become increasingly inter-\nested in the extension of the deep learning method to graphs.\nDriven by the success of deep neural networks, the researchers\ndrew on the ideas of convolutional neural networks, recurrent\nneural networks, and auto encoder to define and design a\nneural network structure for processing graph data. Conse-\nquently, a new method called GNNs emerged. Researchers\nhave designed a few GNN-based graph classification meth-\nods. For example, the graph convolutional network (GCN)\n[9] is one of the earliest methods in this discipline. GCNs\nlearn node representations and propagate them to other nodes\nusing a spectral graph convolution technique. In many graph\nclassification tasks, GCN have demonstrated state-of-the-art\nperformance. However, GCNs have limitations in capturing\nlong-range relationships and higher-order graph structures. To\nsolve these problems, MPNN [38] applies a message-passing\nalgorithm to learn node representations on the local graph\nstructure. It has been demonstrated that MPNN are efficient\nin capturing higher-order graph topology and long-range rela-\ntionships. With the development of the attention mechanism,\ngraph attention networks (GATs) [39] have become a popular\nmethod for graph classification. GATs use self-attention to\nlearn node representations, thereby enabling the model to\nfocus on only the key nodes in the graph. GATs have been\nshown to achieve state-of-the-art performance on many graph\nclassification tasks. In addition to these methods, several other\nGNN variants have been proposed, such as graph isomorphism\nnetworks (GINs) [40]. These techniques have improved the\nclassification accuracy for a variety of graph classification\nproblems.\nAs the size of graphs to be classified are usually different\nand cannot be directly compared, many methods apply graph\npooling to resize the graphs to a unique size before the clas-\nsification. A number of intuitive methods are used for graph\npooling. For example, max-pooling and mean-pooling use the\nmaximum or average value of a group of nodes to represent\nthem [41]. However, these methods lack flexibility, which\nreduces the competitiveness of these methods. To overcome\nthese limitations, Ma et al. [42] introduced EigenPooling, an\ninnovative approach rooted in the graph Fourier transform.\nThis method leverages the spectral domain to effectively pool\nnodes in a graph. However, the pooling process is heuristic and\ncannot be optimised by machine learning algorithms. For this\nproblem, currently available spectral clustering (SC) methods\n[31] [43] were proposed to identify clusters, which are subsets\nof nodes that are more densely connected to each other than to\nthe rest of the graph. However, it leads to more computational\ncomplexity. However, these methods only execute pooling\nonce, which occasionally leads to the loss of key nodes. To\nsolve this problem, Ying et al. [44] developed the hierarchical\npooling approach (DiffPool). They created the concept assign\nmatrix that maps a set of nodes to a single node using GNN\nmodels. The function of the assignment matrix is given below:\n$S^{(k)} = softmax[GNN_{k,pooling}(A^{(k)}, X^{(k)})]$,                                                                                                                                (3)\nwhere $A^{(k)}$ and $X^{(k)}$ are the graph's adjacency matrix and\ngraph representation matrix. $GNN_{k,pooling}$ is a learnable\nfunction. In practice, DiffPool combines its pooling method\nwith the differentiable graph encoder to make the architecture\ntop-to-end trainable."}, {"title": "C. Wavelet Transform-Based Research", "content": "As a part of the spectral theory, the wavelet theory has\nbeen widely used in the field of image processing and signal\nanalysis. For example, Yahia et al. [45] use wavelet neural\nnetworks for image classification and attain high accuracy.\nSome researchers applied wavelet transform to the spectral\ngraph theory. For example, Hammond et al. [24] defined a\nwavelet function to project the graph to the wavelet domain,\nthe equation is defined in the following manner:\n$\\psi_{f,i}(j) = \\sum_{t=1}^{N} g (f \\lambda_t) u_t (i)u_t(j)$,                                                                                                                               (4)\nwhere N is the number of vertices, \u03bbt is the t-th eigenvalue\nof the graph Laplacian matrix, ut is the eigenvector of the\nLaplacian matrix. The symbol  denotes the complex conju-\ngate operator, and g is the spectral graph wavelet generating\nkernel. This research is the first to propose the concept of the\ngraph wavelet transform. However, it does not combine graph\nwavelet theory and deep learning.\nGraph wavelet transform is becoming more frequently used\nin the design of GNNs. Xu et al. [28] designed the graph\nwavelet neural network(GWNN) using spectral graph theory\nfor node classification tasks and obtained satisfactory results.\nThey reported that using graph wavelet transform can circum-\nvent the short-comings of previous spectral CNN methods,\ndepending on the graph Fourier transform. Similarly to the\ngraph Fourier transform, the wavelet base is designed in the\nfollowing manner:\n$\\Psi_s = U_s G_s U_s^T$,                                                                                                                                            (5)\nwhere Us represents the Laplacian eigenvectors and Gs =\ndiag(exis, ..., ens) is the scaling matrix. Substituting the\ngraph Fourier transform with wavelet transform, GWNN uses\ndiagonal masks to generate the representation of each node.\nThe structure of the m-th layer is defined as:\n$X^{m+1} = h ( \\sum_{i=1}^{p} F_i^m X_i^m) j = 1,\u2026, q$.                                                                                                                                               (6)\nNote that $F_i^m$ is a diagonal matrix, which is effective for\nnode-level classification tasks, as the features of different\nnodes cannot be mixed. GWNN is highly competitive at node-\nlevel tasks. However, GWNNs don't have a mechanism to"}, {"title": "III. GSPECT", "content": "handle graphs of different sizes, which is crucial for graph\nclassification tasks. Besides, GWNNs lack a standard pooling\nmechanism to aggregate node-level features into a fixed-size\ngraph-level representation, which is necessary for classifying\ngraphs of varying sizes.\nAs the application in graph multi-modal learning,\nBehmanesh et al. [46] proposed a graph wavelet convolution\nnetwork (GWCN) for multi-modal learning. GWCN gener-\nates single-modal representations by applying the multi-scale\ngraph wavelet transform and learning permutations that encode\ncorrelations among various modalities. GWCN have the best\nperformance on node classification tasks.\nWavelet-based methods are a powerful tool for capturing\nmulti-scale graph representations. However, currently, few\nmethods use graph wavelet transform for cross-scale graph\nclassification."}, {"title": "A. Problem Description", "content": "Let G = {V, E} represent a graph, with V and E being the\nset of nodes and edges, respectively. A \u2208 {0,1}n\u00d7n represents\nthe adjacency matrix and X \u2208 Rnxl represent the node\nattribute matrix. I represents the length of the attribute vector.\nThere is a set of labeled graphs ({G}, {y}), where yi \u2208 Z\nrepresents the label of Gi, and max[size({G})]/min[size({G})]\n> 103. The target of the cross-scale graph classification task\nis to learn a mapping f: G \u2192 y. Compared with other\nmachine learning methods applied in computer vision and\nnatural language processing, we need to convert graphs with\ndifferent topologies into vector v \u2208 Rq, where q \u2264 min(n).\nThen, the mature approach of machine learning methods can\nbe used."}, {"title": "B. Model Framework", "content": "In this section, we introduce the framework of our model\nGSpect. GSpect consists of four parts(Fig. 2). The first part is\nthe convolution layer. We use graph wavelet transform for the\nconvolution layer to generate the graph-level representation. In\nthe second part, we design the spectral-pooling layer to filter\nthe useless information and obtain the low-order representation\nfor classification. The spectral-pooling layer aggregates the\nnodes with similar representations in the spectrum and obtain a\nlow-order graph. The third part is a fully connected layer for\nclassification. Because the convolution and pooling process\nneed to be repeated many times, we use simple GCN in\nconvolution after pooling. Finally we design an optimising\nfunction to optimise the model. Furthermore, we proved the\nstability of the model (see Appendix A)."}, {"title": "C. Graph Wavelet Convolution Layer", "content": "As the first step of GSpect, we design a convolution layer\nto generate the graph presentations. For traditional convolu-\ntion methods, cross-scale graphs has big difference in size,\nwhich leads to the difficulty of getting graph presentations.\nIn this section, we propose the graph wavelet convolution\nlayer (GWC). Taking advantage of the fact that the wavelet\nfunction can capture multi-scale messages, we use the wavelet\ntransform to project the graph into the wavelet domain and use\na learnable filter to aggregate messages from every entry and\nobtain the graph representation.\nIn earlier research, wavelet bases are defined in the follow-\ning manner: If = [Vf,1,...,Vf,N], where \u03c8f,i represent the\nthe transform matrix at node i and scale f. Different studies\nhave various definitions of Vf,i. A few traditional functions of\nwavelet bases need to compute the eigenvalues of the graph,\nwhich leads to a large amount of computation. To escape this,\nwe use the definition of [24] to approximate the wavelet bases,\nwhich is defined in the following manner:\n$\\Psi_f = C_{0, f} + \\frac{1}{2} \\sum_{i=1}^M C_{i,f} T_i(L)$,                                                                                                                            (7)\n$C_{i,f} = 2e^{-f} J_i(-f)$,                                                                                                                                                                                                (8)\nwhere L is the Chebyshev polynomial [14] of order i which\nis used to approximate If, M is the number of Chebyshev\npolynomials and Ji (-f) is the Bessel function of the first\ncategory [47] and f is the wavelet scale.\nsAccording to prior research [46], we use the wavelet base\n$\\Psi_f$ to project the graph's embedding matrix to the wavelet\ndomain.\nSince the formula Equation 8 is in an approximate form,\nthe inverse of the matrix may not exist. Therefore, this article\nuses the pseudoinverse of the matrix instead. We first perform\nsingular value decomposition on If, that is:\n$\\Psi^f = VEU^T$.                                                                                                                                                                                                             (9)\nWhere V and U are left and right singular vector matrix. Then\nthe inverse $\\Psi_f^{-1}$ is defined as follows:\n$\\Psi_f^{-1} = VE^{-1}U^T$                                                                                                                                                                                         (10)\nThen, in the wavelet domain we use a learnable filter to\naggregate messages from every entry. Thereafter, we use If\nto convert the representation back. Finally, we use the bias\nand activation functions to formalise the convolution layer. The"}, {"title": "D. Spectral-pooling Layer", "content": "Since the graphs have different sizes even after convolution,\nthey cannot be directly classified. To solve this problem, we\ndesign a pooling layer to process the graph presentation and\ngenerate graphs in the same size for classification.\nMotivated by the research [42] [44], we continue to use the\nconcept assignment matrix:\n$S^k = GNN_{k,pooling} (A^k, X^k)$,                                                                                                                                                                                   (13)\nwhich implies learning a project matrix that projects the\nadjacency matrix to a low-order adjacency matrix. In essence,\nit converges a group of nodes to a single node. Rather than\nusing a normal GNN structure to learn St directly, we propose\na new method in this article. We use Fourier transform to\nconvert the adjacency matrix A and graph embedding X into a\nfrequency domain and use a spectral filter to filter out useless\ninformation and reduce the size of matrix through spectral\nconvolution. The assign matrix is defined in the following\nmanner:\n$S_{(n-m) \\times n}^{k} = \\xi_{(n-m) \\times (n-m)} \\Theta_{(n-m) \\times n} \\xi_{n \\times l}^{-1,k}$,                                                                                                        (14)\nwhere \u03be(u, v) = \u03a3\u03b1 \u03a3y f(x,y)e-j2\u03c0(+) is the Fourier\ntransform matrix. The function f(x,y) can be any arbitrary\nfunction. n and m is the node number before pooling and after.\n\u0398(n\u2212m)\u00d7n is the learnable parameter. Thus, the total equation\nof adjacency matrix A and graph embedding X is:\n$X_{(n-m) \\times l}^{k+1} = S_{(n-m) \\times n}^k X_{h \\times 1}^k$,                                                                                                                                                                         (15)\n$A_{(n-m) \\times (n-m)}^{k+1} = S_{(n-m) \\times n}^k A_{n \\times n}^k (S_{(n-m) \\times n}^k)^T$.                                                                                                                                          (16)"}, {"title": "E. The Optimization Method", "content": "The parameters in the model need to be optimized. In this\nsection, we introduce the optimization function of GSpect.\nAccording to existing research [48], it is difficult to optimize\nthe model using gradient descent only during the graph clas-"}, {"title": "IV. EXPERIMENT", "content": "In this section, we test the model's effectiveness on graph\nclassification tasks. We aim to answer the following questions:\nQ1 How does our model compared to other advanced\nmodels in open data sets?"}, {"title": "A. Experiment Settings", "content": "1) Data Sets: We use the following five open data sets to\nverify the effectiveness of the model:\nD&D [12] (Biological macromolecules). D&D is a protein\ndata set. It extracted 1178 high-resolution proteins from a non-\nredundant subset of the protein database using simple features,\nsuch as secondary structure content, amino acid propensity,\nsurface properties, and ligands. The nodes are amino acids,\nand if the distance between the two nodes is less than six\nangstroms, an edge is used to represent this relationship. Nodes\nin DD data set are unlabeled and nodes only have features. The\ncriterion for classification is whether a protein is an enzyme.\nPTC [49] (Small molecules). PTC is a collection of 344\ncompounds that report carcinogenicity to rats. Researchers\nneed to classify these compounds to the criterion of carcino-\ngenicity. Nodes represent atoms and edges between nodes\nrepresent bonds between corresponding atoms. Each node has\n19 node labels.\nPROTEINS [50] (Biological macromolecules). PROTEINS\nis another network of proteins. The task is to determine\nwhether such molecules are enzymes. The nodes are amino\nacids.\nIMDB-B [13] (Social network). IMDB-B is a movie col-\nlaboration data set consisting of a self-network of 1,000\nactors who play movie roles in IMDB. In each network, the\nnodes represent the actors/actresses. Researchers use an edge\nto link them if they act in the same movie. The criterion\nfor classification is the type of movies. These networks are\ncollected from the action movies and romantic movies.\nMUTAG [14] (Small molecules). MUTAG is a data set of\nnitroaromatic compounds designed to predict their mutagenic-\nity against salmonella typhimurium. The graphs are used to\nrepresent compounds, where nodes represent atoms and are\nlabeled by atomic type (represented by single encoding), while\nedges between nodes represent bonds between corresponding\natoms. It includes 188 compound samples and 7 discrete node\nlabels."}, {"title": "B. Comparison between GSpect and Other Models", "content": "1) Classifying Graphs in Open Data Sets: The performance\nof GSpect and baseline models on the classification of open\ndata sets are presented in Table II. GSpect achieves four of\nthe best performance out of five data sets with the average\nimprovements of 1.62% in classification accuracy. In particu-\nlar, our model highly improves the performance (by 3.33%)\nfor the biological macromolecules data set (PROTEINS). The\nreaon for this is that GWC captures multi-scale messages from\na complex structure. Moreover, because every graph should\nbe pooled into the same size, the spectral-pooling method\nsaves most messages during the process of pooling on a\nlarger scale. It must be noted that at the data set IMDB-\nB, GSpect lags behind G-mixup. This because IMDB-B is\na social network data set and has a small number of nodes\nthat have an enormous number of neighbor nodes. GSpect\ndoes not consider the effect of these key nodes in graph\nclassification. In addition, G-mixup employs random graph\nmixing to generate new graphs, ensuring that the mixed graphs\nretain the fundamental structure of the original graphs, such\nas connectivity, which may lead to its superior performance\nin social networks."}, {"title": "C. Ablation Study", "content": "To answer Q2, we design an ablation study to verify which\npart of GSpect is significant and why GSpect has better\nperformance.\nTable III reports the results of ablation study. It is evident\nthat GWC and the spectral-pooling layer improves the per-\nformance partly, which proves the effectiveness of GWC and\nspectral-pooling.\nNote that using spectral pooling with the data sets MUTAG\nand PROTEINS, using spectral-pooling only leads to better\nperformance than GSpect. The reason for this is that the GWC\naggregates the multi-scale spectral messages as its output and\nthe spectral-pooling layer filters the redundant messages and\ngenerates the principal component representation. However, in\nthis study, we retain the first F-scale wavelet and loss portion\nof the high-scale messages. For MUTAG and PROTEINS,\nthis method affects the accuracy of classification. Another\nreason is, unlike most other methods, GWC trains a non-\nsparse parameter matrix, which may lead to overfitting on\nthese datasets. After removing GWC, the model complexity\nis reduced, which in turn improves its generalization ability.\nWith regard to stability, GWC and spectral-pooling partially\nincrease the standard deviation of classification accuracy,\nwhich reduces the stability of the model. This is because these\ntwo methods have more learnable parameters which increase\nthe difficulty of optimization and increase the probability of\nfalling into local optimum."}, {"title": "D. Sensitivity Analysis", "content": "To answer Q3, we change the value of hyperparameters\nand observe the performance of GSpect in the classification\naccuracy. The experiment is based on MUTAG. Fig. 6 presents\nthe results of the sensitivity analysis. According to Fig. 6, we\nfind that GSpect undergoes small changes when the number of\nChebyshev polynomials M and the number of wavelet scale F\nchanges. This result implies that on the basis of maintaining\nhigh classification accuracy, researchers can select small F\nand M to reach lower code execution time.\nHowever, the classification accuracy reduces sharply when\nthe equilibrium coefficient \u1e9e increases. As Equation 19 shows,\nwhen \u1e9e is close to 1, Lp plays a leading role in the opti-\nmization function. The results reveal that using Lp alone will\nreduce the performance of GSpect. Thus, researchers need to\nadjust \u1e9e to ensure that the two optimization function have the\nsame order of magnitude."}, {"title": "V. CONCLUSION", "content": "Structure determines function in many systems. As a key\nmethod for identifying common structures for functional de-\nsign and system optimization, cross-scale graph classification\nis crucial in many aspects such as bioinformatics, drug design,\nand complex networks. Considering there is few methods for\ncross-scale graph classification tasks, we proposed GSpect, an\nadvanced cross-scale graph classification model in this study.\nWe use the graph wavelet neural network as the convolution\nlayer which improved the performance of obtaining graph-\nlevel representations. In addition, we designed the spectral-\npooling layer which filters useless messages directly on the\nspectrum and aggregates the nodes to resize the graph by\nspectral pooling. Based on the fact that there is few cross-\nscale graph data sets, we collect data and create the cross-\nscale data set MSG. We compared this data set with the\nstate-of-the-art ones to prove the superiority of the classi-\nfication accuracy of GSpect using both open data sets and\nMSG. Experiments reveal that, on open data sets, GSpect\nimproves the performance of classification accuracy by 1.62%\non average, and for a maximum improvement of 3.33% on\nPROTEINS. On MSG, GSpect improves the performance of\nclassification accuracy by 15.55% on average. Further, we\nemployed an ablation study to observe the improve of accuracy\nby GWC and spectral-pooling. The results reveal that when\nwe employed them simultaneously, we obtain the best results\nwith regard to graph classification, which proves that it is\nnecessary to use them simultaneously. Further, we conducted\nthe sensitivity analysis to verify the stability of GSpect when\nthere is a change in the hyperparameters. The results reveal"}, {"title": "APPENDIX A", "content": "STABILITY OF GSPECT\nIn this section, we establish the stability of GSpect. Given\nthat the model comprises two serially connected modules\n(GWC layer and spectral pooling layer), we independently\ndemonstrate the stability of each module. The proof strategy is\nas follows: first, we prove that the model satisfies the Lipschitz\ncontinuity condition, and then we prove the model's stability."}, {"title": "A. Lipschitz Continuity of GSpect", "content": "Lipschitz continuity is a prerequisite for stability. The\ndefinition of Lipschitz continuity is given below.\nDefinition 1: A function f : Rn \u2192 Rm is said to be\nLipschitz continuous if there exists a constant K \u2265 0, known\nas the Lipschitz constant, such that for all x, y \u2208 R, the\nfollowing inequality holds:\n$|| f(x) - f(y) || \\leq K||x - y||$.                                                                                                                                                               (20)\nLipschitz continuity implies that the function f does not\noscillate too wildly; small changes in the input x result in small\nchanges in the output f(x). Therefore, Lipschitz continuity is\ncrucial for ensuring predictable behavior of dynamical systems\nand for the convergence of numerical methods.\nIn this section, we first establish the Lipschitz continuity of\nGWC and then establish the Lipschitz continuity of spectral-\npooling."}, {"title": "1) Lipschitz Continuity of GWC", "content": "We first prove that the\ngraph wavelet transform If is Lipschitz continuous. This\nproof is based on the definition of the graph wavelet transform\ngiven in equation 8. Then we prove the Lipschitz continuity of\nGWC. The proof of If's Lipschitz continuity is given below.\nProof A.1: We first discuss the continuity"}]}