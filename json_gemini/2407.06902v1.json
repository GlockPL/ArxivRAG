{"title": "Learning From Crowdsourced Noisy Labels: A Signal Processing Perspective", "authors": ["Shahana Ibrahim", "Panagiotis A. Traganitis", "Xiao Fu", "Georgios B. Giannakis"], "abstract": "One of the primary catalysts fueling advances in artificial intelligence (AI) and machine learning (ML) is\nthe availability of massive, curated datasets. A commonly used technique to curate such massive datasets\nis crowdsourcing, where data are dispatched to multiple annotators. The annotator-produced labels are\nthen fused to serve downstream learning and inference tasks. This annotation process often creates noisy\nlabels due to various reasons, such as the limited expertise, or unreliability of annotators, among others.\nTherefore, a core objective in crowdsourcing is to develop methods that effectively mitigate the negative\nimpact of such label noise on learning tasks. This feature article introduces advances in learning from\nnoisy crowdsourced labels. The focus is on key crowdsourcing models and their methodological treat-\nments, from classical statistical models to recent deep learning-based approaches, emphasizing analytical\ninsights and algorithmic developments. In particular, this article reviews the connections between signal\nprocessing (SP) theory and methods, such as identifiability of tensor and nonnegative matrix factorization,\nand novel, principled solutions of longstanding challenges in crowdsourcing-showing how SP perspec-\ntives drive the advancements of this field. Furthermore, this article touches upon emerging topics that are\ncritical for developing cutting-edge AI/ML systems, such as crowdsourcing in reinforcement learning with\nhuman feedback (RLHF) and direct preference optimization (DPO) that are key techniques for fine-tuning\nlarge language models (LLMs).", "sections": [{"title": "Introduction", "content": "Artificial Intelligence (AI) and machine learning (ML) have made significant strides over the past few\ndecades, expanding the potential of models and learning algorithms. These advancements have revolu-\ntionized natural language processing by enhancing language generation and understanding, transformed\ncomputer vision with superior image recognition and analysis, and enabled complex decision-making ca-\npabilities. A key driving factor behind such successes of AI/ML is the availability of large-scale, accurately\nlabeled training data. In fact, data annotation has become an indispensable integrating part of the AI in-\ndustry. In a recent market report from Grand View Research, it was stated that \u201cthe global data collection and\nlabeling market size was valued at $2.22 billion in 2022, and it is expected to expand at a compound annual\ngrowth rate of 28.9% from 2023 to 2030", "wisdom of crowd\" in data labeling makes sense, as the label accuracy\nprovided by individual annotators is sensitive to many factors. If the annotators are human workers, their\nannotation accuracy is limited by their expertise level, background knowledge, and personal experience;\nwhen the annotations are given by automated machine annotators, the accuracy is affected by their model\nexpressiveness and the quality/amount of their training data. The idea of systematic crowdsourcing can\nbe dated back to more than a century ago; see the inserted box \u201cCrowdsourcing in History": "Over the past\ntwo decades, many commercial crowdsourcing platforms have become widely used, including notable\nones such as Amazon Mechanical Turk (AMT) and CrowdFlower [2]. These platforms were instrumental in\ncreating many highly influential datasets in modern AI history. For example, the ImageNet dataset, that has\ndriven significant research advancements in AI, contains approximately 14 million annotations provided\nby approximately 25,000 AMT annotators. More recently, large language models (LLMs), such as ChatGPT,\nGemini, and Llama, used substantial amounts of crowdsourced labels for fine-tuning [3].\nBeyond AI, crowdsourcing has also left significant footprints in a gamut of real-world applications in sci-\nence and engineering. Popular citizen science projects like GalaxyZoo and eBird are byproducts of crowd-\nsourcing, which provided effective and economical solutions to the ever-lingering data scarcity problem in\nlife and physical sciences. Crowdsourcing techniques have also shown great promise in medical imaging-\nbased diagnosis, producing reliable results with minimal expert supervision [4]. In addition, crowdsourcing\nis widely utilized in sensing applications through crowdsensing [5], where a large group of mobile users or\ndevices collectively share real-time data. An example of crowdsensing is the crowd-contributed real-time\ninformation on road conditions, which is now widely available in applications such as Google Maps and\nWaze. Various data fusion applications in remote sensing and healthcare that aggregate information from\nmultiple sources of varying reliability also have strong connections to crowdsourcing techniques [6]. Fur-\nthermore, crowdsourcing is conceptually similar to ensemble learning methods, such as bagging, boosting,\nand stacking, that combine multiple models to improve overall predictive performance.\nDespite the remarkable achievements of crowdsourcing, the fundamental challenge is that annotator-\nprovided labels can be substantially noisy, and such noisy labels are detrimental to downstream tasks'\nperformance. For example, in deep learning systems, noisy labels cause overfitting to noise and poor gen-\neralization. Early approaches for alleviating the negative impacts of crowdsourced label noise focused on\nimproving the quality of annotators' responses during label collection. These approaches include various\nproject management-based strategies, such as designing proper labeling workflow, querying proper ex-\nperts, enhancing supervision mechanisms, and using effective incentive methods-see [7] and references\ntherein. However, implementing such complex quality control mechanisms has become increasingly diffi-\ncult and less cost-effective as data volume increases. For massive data annotation tasks faced by modern\ncrowdsourcing systems, automated annotation integration and label correction algorithms advocated by\nthe machine learning community are much more relevant. In this article, the term \u201ccrowdsourcing"}, {"title": "Problem Settings", "content": "In this section, we formally define the problem statement of crowdsourcing. In addition, we introduce two\nmajor paradigms of the crowdsourcing system design.\n2.1 Problem Statement\nAs mentioned, crowdsourcing can be used to integrate various types of annotations (e.g., class labels [8],\ntraffic information [9], and bird counts); that is, in principle, the annotations in crowdsourcing can be both\ndiscrete and continuous values. Nonetheless, for simplicity, we use categorical class label annotations as the\nprimary working example to introduce crowdsourcing ideas and algorithms. Consider a dataset consisting\nof N data items, i.e., $\\mathcal{X} := {\\mathbf{x}_n}_{n=1}^N$, where $\\mathbf{x}_n \\in \\mathbb{R}^D$ is the feature vector representing the nth data item.\nAssume that each data item belongs to one of K classes. Let $\\{y_n\\}_{n=1}^N$ denote the set of ground-truth labels,\nwhere $y_n \\in [K], \\forall n$; i.e., $y_n = k$ if the data item $\\mathbf{x}_n$ belongs to class k. Note that the ground-truth labels\n$\\{y_n\\}_{n=1}^N$ are unknown, and M crowdsourced workers (annotators) are employed to label the dataset $\\mathcal{X}$, i.e.,\nto give their estimates of $\\{y_n\\}$. We use $y_n^{(m)} \\in [K]$ to denote the label assigned to the nth data item by the\nmth annotator. Note that the annotators may be human, pre-trained ML algorithms, or predefined decision\nrules. Crowdsourced labels are often noisy, which means that we have $y_n^{(m)} \\neq y_n$ for many (m, n) pairs.\n2.2 Crowdsourcing Systems: Two Paradigms\nThe developments of crowdsourcing methods can be roughly classified into two categories, namely,\nlabel integration approaches (see, e.g., [8, 10\u201312]) and end-to-end (E2E) learning approaches (e.g., [13-16]).\nThe former isolates noisy label correction from downstream tasks, yet the latter directly uses the noisy\nlabels to train systems serving for specific tasks.\nLabel Integration. Fig. 1 shows the schematics of the first type of crowdsourcing system, where label\nintegration is the key module. These systems consist of two stages, where the label integration stage is not\naffected by the subsequent downstream task stage. In the label integration stage, the labels $y_n^{(m)}$'s are fused\nto produce an estimated label $\\hat{y}_n$ in the hope that $\\hat{y}_n = y_n$. Oftentimes, the characteristics of the annotators,\ni.e. \u201cannotator confusions\u201d in the figure, are also estimated in the process. Then, the estimated labels $\\{\\tilde{y}_n\\}$\nare used for training the downstream machine learning system, e.g., training a neural network function to\npredict ground-truth labels. The training stage does not need specific design to handle multiple annotators\nor noisy labels.\nEnd-to-End (E2E) Learning. Fig. 2 illustrates a typical E2E crowdsourcing system. This type of systems\nconsiders both data and noisy labels as input and directly trains the learning systems on the target down-\nstream tasks. As there is no stage breakdown, these systems are called end-to-end (E2E) systems. When the\ntarget is to train a classifier $f_{\\theta} : \\mathbb{R}^D \\rightarrow \\mathbb{R}^K$, E2E systems design a loss function $l$ such that\n$\\mathbf{f}_{\\theta} \\leftarrow \\arg \\min_{\\theta, \\eta} l(\\{\\mathbf{x}_n\\}, \\{y_n^{(m)}\\}, \\theta, \\eta)$,\nwhere $\\eta$ represents additional model parameters according to specific loss designs. The learned $\\mathbf{f}_{\\theta}$ is ex-\npected to be a good ground-truth label predictor. For example, denote the ground-truth label posterior\ndistribution as a function $f^* : \\mathbb{R}^D \\rightarrow \\mathbb{R}^K$ where\n$[f^*(\\mathbf{x}_n)]_k = \\Pr(y_n = k | \\mathbf{x}_n), \\forall (\\mathbf{x}_n, y_n)$.\n(1)\nThe goal of many E2E approaches (see, e.g., [13\u201315]) is to learn $\\mathbf{f}_{\\theta}$ such that $\\mathbf{f}_{\\theta} \\approx f^*$.\nGenerally speaking, label integration approaches can feature more lightweight and tractable algorithms,\nas they do not require training a learning system $f_{\\theta}$ that is usually parameterized by a complex function\nclass, such as kernels or neural networks. On the other hand, E2E methods often exhibit more appealing\nperformance, possibly because they naturally exploit information from data features and the underlying\nstructure of the specific tasks in a joint way. Both crowdsourcing approaches can be considered unsupervised,"}, {"title": "Label Integration Approaches", "content": "In this section, we focus on approaches designed for the label integration module in Fig. 1.\n3.1 Majority Voting and Challenges\nThe arguably simplest approach for label integration is majority voting (MV), where the estimated label for\neach data item is the one voted by most annotators, i.e.,\n$\\tilde{y}_n = \\arg \\max_k \\sum_{m=1}^M \\mathbb{I}[y_n^{(m)} = k]$.\n(2)\nHowever, MV is not always effective, as it implicitly assumes equal reliability among all annotators. Fur-\nthermore, due to the significant time and effort required, it is not economical to ask all annotators to label\nevery data item. This means that many items may not have sufficient annotations that merit a voting pro-\ncess. Under such circumstances, MV can be far from optimal for label integration [17]. To take unequal\nreliability of the annotators into consideration, weighted majority voting (WMV) schemes were proposed.\nFor example, a non-negative scalar weight $w^{(m)}$ representing the labeling accuracy of annotator m was\nconsidered in [18]:\n$\\tilde{y}_n = \\arg \\max_k \\sum_{m=1}^M w^{(m)} \\mathbb{I}[y_n^{(m)} = k]$.\n(3)\nNonetheless, accurate weight estimation per se is a challenging task. Due to these reasons, MV schemes are\noften only used as the basic baselines in label integration.\n3.2 Label Noise Models in Label Integration\nA notable milestone in label integration research occurred when the MV approaches were superseded by\nprobabilistic modeling approaches. By explicitly modeling noisy labels using probabilistic generative mod-\nels, these approaches often provide more reliable label integration performance. In this section, we intro-\nduce a number of representative models in this genre.\nDawid-Skene (DS) Model. The seminal work of Dawid and Skene introduced one of the most influen-\ntial probabilistic models for crowdsourcing in the late 1970s [8]. Under the Dawid-Skene (DS) model, the\nground-truth label is a latent discrete random variable, denoted as Y, and each label $y_n \\in [K]$ is drawn\nindependently from the random variable Y. The annotator's responses are the observed random variables,"}, {"title": null, "content": "denoted as $\\hat{Y}^{(1)}, ..., \\hat{Y}^{(M)}$ with $y_n^{(m)}$ representing the nth realization of $\\hat{Y}^{(m)}$. The key assumption of the DS\nmodel is that, given the ground-truth label, annotators' responses are conditionally independent; see Fig. 3.\nIn other words, the DS model is a naive Bayes model.\nUnder the DS model, the joint probability of all the annotator responses can be expressed as follows:\n$\\Pr(\\hat{Y}^{(1)} = k_1,...,\\hat{Y}^{(M)} = k_M) = \\sum_{k=1}^K \\left( \\Pr(Y = k) \\prod_{m=1}^M \\Pr(\\hat{Y}^{(m)} = k_m | Y = k) \\right)$,\n(4)\nwhere $k_1,..., k_M \\in [K]$ are the annotator responses, and we have used the law of total probability, Bayes\nrule, and the conditional independence of annotator responses given the ground-truth label. From the\nrelation in (4), the following terms are defined:\n$A_m(k', k) \\equiv \\Pr(\\hat{Y}^{(m)} = k' | Y = k), d(k) \\equiv \\Pr(Y = k), \\forall k,k' \\in [K]$,\n(5)\nwhere $A_m \\in \\mathbb{R}^{K \\times K}$ is called the confusion matrix of the annotator m and $\\mathbf{d} \\in \\mathbb{R}^K$ is the PMF of the ground-\ntruth label distribution. As the name suggests, the off-diagonal elements of $A_m$ characterize the probabil-\nities of annotator m making mistakes. An ideal annotator would have $A_m = I$. Also note that, as each\ncolumn of a confusion matrix is a PMF, it holds that $A_m \\geq 0, \\mathbf{1}^T A_m = \\mathbf{1}^T$, with $\\mathbf{1}$ denoting the all ones vec-\ntor of appropriate dimension. Thus, both $\\mathbf{d}$ and columns of $A_m$ belong to the so-called (K \u2212 1) dimensional\nprobability simplex, $\\Delta_K = \\{ \\mathbf{x} \\in \\mathbb{R}^K : \\mathbf{x} \\geq 0, \\mathbf{1}^T \\mathbf{x} = 1 \\}$.\nUnder the DS model, if the $A_m$'s and $\\mathbf{d}$ are known, one can construct optimal ground-truth label es-\ntimators using the maximum a posteriori (MAP) principle, i.e. maximizing the posterior distribution of the\nunknown label, given annotator responses:\n$\\begin{aligned}\n\\tilde{y}_n &= \\arg \\max_{k \\in [K]} \\Pr(y_n = k | y_n^{(1)} = k_1,..., y_n^{(M)} = k_M) \\\\\n&= \\arg \\max_{k \\in [K]} \\frac{\\Pr(y_n^{(1)} = k_1,..., y_n^{(M)} = k_M | y_n = k) \\Pr(y_n = k)}{\\Pr(y_n^{(1)} = k_1,..., y_n^{(M)} = k_M)} \\\\\n&= \\arg \\max_{k \\in [K]} \\log d(k) + \\sum_{k'=1}^K \\sum_{m=1}^M \\mathbb{I}[y_n^{(m)} = k'] \\log A_m(k', k),\n\\end{aligned}$\n(6)\nwhere we have successively used Bayes rule, the properties of the logarithm and the definitions in (5).\nHence, the label integration problem under the DS model amounts to estimating $\\{A_m\\}$ and $\\mathbf{d}$ accurately.\nConnections to Communications and Information Theory. The DS model can also be interpreted as\nsingle-input multiple-output (SIMO) channel from information theory. The common input is the single\nground-truth label per data item $y_n$, the multiple outputs are the M annotator responses $\\{y_n^{(m)}\\}_{m=1}^M$,\nand the annotator confusion matrices $A_m$'s can be associated with the unknown \"channel\" character-\nistics. A similar problem to crowdsourcing is decentralized detection [19], where a distributed network\nof sensors (annotators) observe the same environmental phenomenon, and send their observations\n(labels) to a fusion center, and the fusion center has to recover the underlying hypothesis of the en-\nvironment. The key difference between decentralized detection and crowdsourcing is that in the for-\nmer case, the fusion center can control the characteristics of both the annotators and the fusion rule.\nThe crowdsourcing setting with real-valued annotations also bears resemblance to classical problem\nof blind multichannel deconvolution [20], where the unobserved input signal and the channel charac-\nteristics are inferred from multiple noisy observed signals. Furthermore, the crowdsourcing problem\nexhibits similarities to the chief executive officer (CEO) problem in information theory [21], where M\nagents observe noisy sequences and the CEO aims to recover the ground truth within a communica-\ntion constraint, similar to the labeling cost budget in crowdsourcing.\nSpecial Cases of the DS Model. The DS model serves as a foundation for different models that are fre-\nquently used in the literature. The so-called \u201cone-coin\u201d model [22] is a simplified DS model that encodes"}, {"title": null, "content": "each annotator's reliabilities using only one parameter such that\n$\\Pr(\\hat{Y}^{(m)} = k' | Y = k) = \\begin{cases}\np_m, & k' = k,\\\\\n\\frac{1 - p_m}{K-1}, & k' \\neq k,\n\\end{cases}$\ni.e., annotator m determines their response with a single biased coin flip. For each data item, the annotator\nassigns the correct label with probability $p_m$, and gives a wrong label with equal probabilities across the\nremaining K - 1 classes. Other popular DS model variants include the spammer-hammer model [23] and the\nconfusion vector model [24]. In the spammer-hammer model, each annotator is a \u201chammer\u201d with probability\nq, providing correct labels, or a \"spammer\" with probability 1 q, providing random labels. The confusion\nvector model employs a parameter vector $\\mathbf{a}_m \\in \\mathbb{R}^K$, per annotator m, to characterize their reliabilities:\n$\\Pr(\\hat{Y}^{(m)} = k' | Y = k) = \\begin{cases}\na_m(k), & k' = k,\\\\\n\\frac{1 - a_m(k)}{K-1}, & k' \\neq k,\n\\end{cases}$\ni.e., unlike the one-coin model, the probability that the annotator m chooses the correct answer or a wrong\nanswer varies across different classes.\nThese special cases of the DS model offer succinct characterizations of annotator confusions. Yet, they\nare less general than the DS model. Nevertheless, these models often admit more tractable algorithms and\nreasonable performance guarantees; see, e.g., [22, 23, 25].\nExtended DS Model: Incorporating Item Difficulties. The DS model introduced in [8] assumes that an-\nnotator behavior is the same across all data items, i.e., $A_m$ is the same for all n. Nonetheless, it may be\nmore realistic to assume that the difficulty of labeling varies across data items. To capture both annotator\nbehavior and item difficulty [26] introduced the following model:\n$\\Pr(\\hat{Y}^{(m)} = k' | Y = k) = \\frac{\\exp(A_m(k', k) + B_n(k', k))}{\\sum_{k'} \\exp(A_m(k', k) + B_n(k', k))}$,\nwhere $A_m \\in \\mathbb{R}^{K \\times K}$ is defined as before, and $B_n \\in \\mathbb{R}^{K \\times K}$ is an item-specific confusion matrix that reflects\nitem difficulty. Following the same spirit, the one-coin model was also generalized to incorporate item\ndifficulty; see the generative model of labels, abilities, and difficulties (GLAD) model in [27].\nBayesian Models. To incorporate prior information, reduce the number of parameters, and enhance model\ninterpretability, Bayesian models were also introduced on top of the DS model (see, e.g., [16, 28]) Using\nthe fact that both $\\mathbf{d}$ and $A_m(:, k)$ are PMFs, [28] imposes Dirichlet priors on to $\\mathbf{d}$ and $A_m(:, k)$'s, while [16]\nassumes a Beta prior for the confusion parameters, in the binary classification case.\n3.3 Methods for Noise Model Learning\nUnder the aforementioned noise generation models, label integration boils down to learning the key model\nparameters, e.g., the confusion matrices and the prior probability vector in the DS model. In this subsection,\nwe briefly review some representative methods for learning these models.\nExpectation Maximization (EM). The seminal work by Dawid and Skene [8] sought a maximum-likelihood\nestimator (MLE) of annotator confusion matrices and class priors of the DS model. Collect all ground-\ntruth labels and annotator responses in $\\mathcal{Y} = \\{y_n\\}$ and $\\hat{\\mathcal{Y}} = \\{y_n^{(m)}\\}$, respectively, and all unknown model\nparameters in $\\psi = \\{A_1, ..., A_M, \\mathbf{d}\\}$. The MLE of $\\psi$ is given by\n$\\begin{aligned}\n\\hat{\\psi} &= \\arg \\max_{\\psi} \\log \\Pr(\\hat{\\mathcal{Y}}; \\psi)\\\\\n&= \\arg \\max_{\\psi} \\sum_{n=1}^N \\log \\Pr(y_n^{(1)}, ..., y_n^{(M)}; \\psi) \\\\\n&= \\arg \\max_{\\psi} \\sum_{n=1}^N \\log \\sum_{k=1}^K \\mathbf{d}(k) \\prod_{m=1}^M \\sum_{k'=1}^K \\mathbb{I}[y_n^{(m)} = k'] A_m(k', k),\n\\end{aligned}$\n(7)"}, {"title": null, "content": "where $\\log \\Pr(\\hat{\\mathcal{Y}}; \\psi)$ is the log-likelihood function of observed annotator labels, parametrized by $\\psi$. As di-\nrectly optimizing the log-likelihood is often intractable, Dawid and Skene introduced an EM-based algo-\nrithm to learn the DS model parameters. The EM algorithm is the workhorse for learning Naive Bayes and\nmixture models. Using the EM algorithm, the log-likelihood function is maximized iteratively, with the\nfollowing steps performed at each iteration:\n(i) The \u201cExpectation\u201d (E) step: The E-step in iteration t is performed as follows:\n$Q(\\psi; \\psi^t) = \\mathbb{E}_{\\mathcal{Y} \\sim \\Pr(\\mathcal{Y}; \\hat{\\mathcal{Y}}, \\psi^t)} [\\log \\Pr(\\mathcal{Y}, \\hat{\\mathcal{Y}}; \\psi)] = \\sum_{n=1}^N \\sum_{k=1}^K q(y_n = k; \\psi^t) \\log \\Pr(y_n^{(1)}, ..., y_n^{(M)}; \\psi)$,\nwhere the superscript t denotes iteration index, and the expectation is taken with respect to (w.r.t.) the pos-\nterior probability $\\Pr(\\mathcal{Y}; \\hat{\\mathcal{Y}}, \\psi^t)$ based on the current estimates $\\psi^t = \\{A_1^t, ..., A_M^t, \\mathbf{d}^t \\}$. Note that, $Q(\\psi; \\psi^t)$\nis essentially a lower bound of the log-likelihood. The E-step boils down to estimating $q(y_n = k; \\psi^t) =$\n$\\Pr(y_n = k | y_n^{(1)}, ..., y_n^{(M)}, \\psi^t)$ given $A_m$'s and $\\mathbf{d}^t$. Under the DS model and using Bayes rule, $q(y_n = k; \\psi^t)$\nadmits a simple closed form, i.e.,\n$q(y_n = k; \\psi^t) = \\frac{\\exp \\left( \\log \\mathbf{d}^t(k) + \\sum_{m=1}^M \\sum_{k'=1}^K \\log A_m^t(k', k) \\mathbb{I}[y_n^{(m)} = k'] \\right)}{\\sum_{k''=1}^K \\exp \\left( \\log \\mathbf{d}^t(k) + \\sum_{m=1}^M \\sum_{k'=1}^K \\log A_m^t(k', k'') \\mathbb{I}[y_n^{(m)} = k'] \\right)}$.\n(8)\nNote that this posterior has a similar form to the MAP estimator of (6).\n(ii) The \u201cMaximization", "step": "The M-step refines model parameters by maximizing $Q(\\psi; \\psi^t)$, which\nalso admits analytical updates:\n$\\begin{aligned}\nA_m^{t+1}(k', k) &= \\frac{\\sum_{n=1}^N q(y_n = k; \\psi^t) \\mathbb{I}[y_n^{(m)} = k']}{\\sum_{k''=1}^K \\sum_{n=1}^N q(y_n = k; \\psi^t) \\mathbb{I}[y_n^{(m)} = k'']},\\\\\n\\mathbf{d}^{t+1}(k) &= \\frac{\\sum_{n=1}^N q(y_n = k; \\psi^t)}{\\sum_{k'=1}^K \\sum_{n=1}^N q(y_n = k'; \\psi^t)}.\n\\end{aligned}$\nThe EM scheme can also be readily adapted to learn the special cases of the DS model, i.e., one-coin, con-\nfusion vector, spammer-hammer, and GLAD models as discussed in 3.2. A similar EM strategy is also\nemployed to learn the Bayesian model proposed in [16].\nOne salient feature of the EM algorithm is its scalability\u2014it enjoys a computational complexity that\nis linear in N and M, which is appealing for large-scale crowdsourcing problems. However, it was also\nobserved that the EM algorithm does not converge well, if the initialization is not carefully chosen [11,29],\nperhaps due to the nonconvexity of the MLE loss.\nSpectral Methods. One of the notable spectral methods for label integration is the eigendecomposition-\nbased approach proposed in [22] under the one-coin binary model. Consider the binary classification prob-\nlem with unobserved ground-truth labels $y_n \\in \\{-1,+1\\}$ and the one-coin model parameters $p_m$'s, where\n$p_m$ is the probability that annotator m provides the correct label. Let $z_n^{(m)}$ be the correctness indicator of\nannotator m on item n; i.e., $z_n^{(m)} = 1$ if annotator m provides the correct label and $z_n^{(m)} = -1$ otherwise.\nThen, it can be shown that\n$\\mathbb{E} \\left[ z_n^{(m)} z_n^{(m)} \\right] = \\begin{cases}\np_m^2 + (1 - p_m)^2, & w.p \\: p_m,\\\\\n-1, & w.p \\: 1 - p_m - (1 - p_m)^2.\n\\end{cases}$\nSince $y_n^{(m)} = z_n^{(m)} y_n$ holds, we further obtain:\n$\\mathbb{E} \\left[ \\sum_{m=1}^M y_n^{(m)} y_{n'}^{(m)} \\right] = \\begin{cases}\n\\sum_{m=1}^M \\mathbb{E} \\left[ z_n^{(m)} z_{n'}^{(m)} \\right] y_n y_{n'}, & n \\neq n',\\\\\nM, & n = n',\n\\end{cases}$\n(9)"}, {"title": null, "content": "where $\\kappa = \\sum_{m=1}^M (2 p_m - 1)^2$. By defining the annotator response matrix $\\mathbf{\\hat{U}}$ with entries $\\mathbf{\\hat{U}}(n, m) = y_n^{(m)}$, the\nrelation in (9) can also be expressed as\n$\\mathbb{E}[\\mathbf{\\hat{U} \\hat{U}}^T] = \\kappa \\mathbf{y} \\mathbf{y}^T + (M - \\kappa) I_N$,\n(10)\nwhere $\\mathbf{y} = [y_1, ..., y_N]^T$. Based on the above, [22] proposed an intuitively simple spectral algorithm where\nthe ground-truth labels $\\mathbf{y}$ are extracted from the top eigenvector of the empirical estimate of $\\mathbb{E}[\\mathbf{\\hat{U} \\hat{U}}^T]$. Con-\nsequently, the annotator confusions $\\mathbf{p}$ can be inferred from the estimated $\\mathbf{y}$. Nonetheless, the approach\nrequires that the annotator response matrix is fully observed, meaning that all annotators provide labels\nfor all data items. To accommodate the incomplete labeling scenarios, [25] extended this strategy by con-\nsidering an annotator-item binary matrix alongside the annotator response matrix and performing a joint\nsingular value decomposition (SVD) operation, still under the one-coin model.\nMoment-Based Approaches: From Tensor Decomposition to Nonnegative Matrix Factorization. The\nrank-one plus diagonal structure in (10) is only applicable under the one-coin model. To identify the param-\neters of the general DS model, [10] proposed a moment matching approach that considered the third-order\nmoments of the annotator responses as follows:\n$\\mathbb{E}[\\hat{Y}^{(m)} \\circ \\hat{Y}^{(i)} \\circ \\hat{Y}^{(j)}] = \\sum_{k=1}^K \\mathbf{d}(k) A_m(:,k) \\circ A_i(:,k) \\circ A_j(:,k), \\forall m \\neq i \\neq j$,\n(11)\nwhere $\\circ$ denotes the outer product (i.e., $X = \\mathbf{a} \\circ \\mathbf{b} \\circ \\mathbf{c} \\Leftrightarrow X(m,i,j) = a(m) b(i) c(j)$), and $\\hat{Y}^{(m)}$ denotes\nthe K-dimensional one-hot encoding of the annotator response random variable $\\hat{Y}^{(m)}$, i.e., if $\\hat{Y}^{(m)} = k$,\nthen $\\hat{Y}^{(m)} = \\mathbf{e}_k$, where $\\mathbf{e}_k \\in \\mathbb{R}^K$ is a unit vector with $[\\mathbf{e}_k]_k = 1$ and zeros elsewhere. In general, the\nconditional independence assumption of the DS model induces the outer product expression of the higher-\norder moments. To see why, consider the second-order moment\n$\\begin{aligned}\n[\\mathbb{E}[\\hat{Y}^{(m)} \\hat{Y}^{(i)}]]_{k_1, k_2} &= \\Pr(\\hat{Y}^{(m)} = k_1, \\hat{Y}^{(i)} = k_2)\\\\\n&= \\sum_{k=1}^K \\Pr(Y = k) \\Pr(\\hat{Y}^{(m)} = k_1 | Y = k) \\Pr(\\hat{Y}^{(i)} = k_2 | Y = k),\n\\end{aligned}$\nwhich implies $\\mathbb{E}[\\hat{Y}^{(m)} \\hat{Y}^{(i)}] = \\sum_{k=1}^K \\mathbf{d}(k) A_m(:,k) \\circ A_i(:,k) = A_m \\text{diag}(\\mathbf{d}) A_i^T$; a similar derivation holds\nfor the third-order moments.\nUsing third-order moments, a coupled tensor factorization (CTD) criterion can be used to identify $\\mathbf{d}$ and\n$\\left\\{A_m\\right\\}$ [10]:\n$\\begin{aligned}\n&\\underset{\\{A_m\\},\\mathbf{d}}{\\text{minimize}} \\sum_{m=1}^M \\sum_{\\substack{i > m\\\\ j > i}} ||\\mathcal{T}_{m,i,j} - [\\mathbf{d}, A_m, A_i, A_j]||^2\\\\n&\\text{subject to } A_m \\geq 0, \\mathbf{1}^T A_m = \\mathbf{1}^```json\n^T, \\mathbf{d} > 0, \\mathbf{1}^T \\mathbf{d} = 1,\n\\end{aligned}$\n(12)\nwhere $[\\mathbf{d}, A_m, A_i, A_j]$ is the shorthand notation for $\\sum_{k=1}^K \\mathbf{d}(k) A_m(:,k) \\circ A_i(:,k) \\circ A_j(:,k)$, and $\\mathcal{T}_{m,i,j}$ is\nthe empirical version of $\\mathbb{E}[\\hat{Y}^{(m)} \\circ \\hat{Y}^{(i)} \\circ \\hat{Y}^{(j)}]$. The term \u201ccoupled\u201d is due to the fact that the tensors $\\mathcal{T}_{m,i,j}$\nshare one or two latent factors. To further regularize the objective function first- and second-order moments\nwere used in [10]. The problem in (12) is nontrivial to solve. An alternating direction method of multipliers\n(ADMM)-based optimization algorithm was employed in [10] to handle this criterion. Another third-order\nmoment-based DS learning approach was introduced in [11] that employs an orthogonal tensor decom-\nposition via a robust power method. Nonetheless, higher-order moments like $\\mathcal{T}_{m,i,j}$ in general require a\nsignificant amount of samples (annotator labels) to be accurately estimated."}, {"title": null, "content": "To avoid the sample complexity required for estimating third-order moments, [12,30] proposed using\nonly second-order statistics, leading to a coupled nonnegative matrix factorization (CNMF) criterion:\n$\\begin{aligned}\n&\\underset{\\{A_m\\},\\mathbf{d}}{\\text{minimize}} \\sum_{m=1}^M \\sum_{i=1}^N KL (S_{m,i} || A_m \\text{diag}(\\mathbf{d}) A_i^T)\\\\\n&\\text{subject to } A_m \\geq 0, \\mathbf{1}^T A_m = \\mathbf{1}^T, \\mathbf{d} > 0, \\mathbf{1}^T \\mathbf{d} = 1,\n\\end{aligned}$\n(13)\nwhere $S_{m,i}$ is the empirical estimate of $\\mathbb{E}[\\hat{Y}^{(m)} \\hat{Y}^{(i)}]$, and the KL divergence was used in the loss due to\nthe PMF estimation nature.\nThe moment-based methods in [10\u201312,30] come with interesting theoretical support, reminiscent of\nsignal processing research on tensor and nonnegative matrix factorization; see also Sec. 3.5.\nBayesian Methods. Under the Bayesian paradigm, [28] considered the following joint probability under\npresumed priors of $A_m$ and $\\mathbf{d}$:\n$\\Pr(\\{A_m\\}, \\mathbf{d}, \\mathcal{Y} | \\hat{\\mathcal{Y}}) \\propto \\left[ \\prod_{n=1}^N \\mathbf{d}(y_n) \\prod_{m=1}^M A_m(y_n^{(m)}, y_n) \\right] \\left[ \\Pr(\\mathbf{d} | \\mathbf{\\nu}) \\prod_{m=1}^M \\sum_{k=1}^K \\Pr(A_m(:,k) | \\mathbf{\\pi}^{(m)}) \\right]$\n(14)\nwhere $\\mathbf{\\nu}, \\mathbf{\\pi}^{(m)} \\in \\mathbb{R}^K, \\forall m, k$ are Dirichlet priors for $\\mathbf{d}$ and $A_m(:,k)$'s, respectively. Compared to the plain-\nvanilla DS model learning approaches, Bayesian approaches enjoy more succinct models by treating $A_m$\nand $\\mathbf{d}$ as random quantities. However, this comes at the cost of computational intractability; the posterior\nin (14) is often not easy to evaluate, as it involves marginalization of this joint probability distribution over\nall parameters. To circumvent this issue, sampling techniques are often adopted. From the posterior in (14),\ninference of unknown parameters, i.e, $\\{A_m\\}, \\mathbf{d}, \\mathcal{Y}$, is performed using the Gibbs sampling technique by\niteratively sampling each parameter from its conditional density function [28]. Other Bayesian approaches\ninclude variational inference techniques by approximating the conditional probability densities using belief\npropagation and mean field assumptions [31].\nOther Methods. Beyond methods based on the DS model and its extensions, there are also numerous\nalternative label integration methods that provide interesting insights and simple implementations. Some\nexamples are introduced below:\nLabel integration can be formulated as an optimization problem as follows [32]:\n$\\begin{aligned}\n&\\underset{\\mathbf{w},\\mathbf{y}}{\\text{minimize}} \\sum_{m=1}^M w_m \\sum_{n=1}^N \\text{dist}(y_n, y_n^{(m)})\n&\\text{subject to } R(\\mathbf{w}) = 1,\n\\end{aligned}$\n(15a)\n(15b)\nwhere $\\mathbf{w} = [w_1,...,w_M]^T$ and $\\mathbf{y} = [y_1, ..., y_N]^T$ are the vector of annotator reliabiliities (similar to the\nconfusion parameter in the one-coin model or the weight values in weighted majority voting) and the\nvector of ground-truth labels, respectively. The distance measure $\\text{dist}(y_n, y_n^{(m)})$ captures the deviation\nfrom the ground-truth and the annotator responses, e.g., 0 1 loss for the classification case. $R(\\mathbf{w})$ is\na regularization term, which ensures that the annotator reliabilities $w_m$ remain bounded. For example,\n$R(\\mathbf{w}) = \\sum_{m=1}^M \\exp(-w_m)$ is employed in [32], rendering the overall optimization problem convex and\nallowing it to be effectively handled using the method of Lagrange multipliers.\nA minimax conditional entropy-based label integration was introduced in [24], which considered the\nsum of the entropies of the observed crowdsourced labels. Let $a_n^{(m)}$ denote the PMF of the noisy label $y_n^{(m)}$,\ni.e., $[a_n^{(m)}]_k = \\Pr(y_n^{(m)} = k)$. Note that this PMF is dependent on the data item index n. Then, following"}, {"title": null, "content": "minimize-maximize formulation is considered in [24]:\n$\\begin{aligned}\n&\\underset{\\mathcal{Y}} {\\min} \\underset{\\{\\mathbf{a}_n^{(m)} \\in \\Delta_K\\}} {\\max} \\sum_{m=1}^M \\sum_{n=1}^N \\sum_{k=1}^K [\\mathbf{a}_n^{(m)}]_k \\log [\\mathbf{a}_n^{(m)}]_k \\\\\n&\\text{subject to } [\\mathbf{a}_n^{(m)}]_k = \\sum_{m=1}^M \\mathbb{I}[y_n^{(m)} = k], \\forall n, k,\\\\\n&\\sum_{n=1}^N \\mathbb{I}[y_n = k'] [\\mathbf{a}_n^{(m)}]_k = \\sum_{n=1}^N \\mathbb{I}[y_n = k'] [y_n^{(m)} = k] \\forall m,k,k'.\n\\end{aligned}$\nHere, the parameters $\\{\\mathbf{a}_n^{(m)}\\}$ are learned by maximizing the entropy of the observations. Simultaneously, the\nground-truth labels y are inferred by minimizing the entropy, ensuring that the observed labels are the\n\"least random\" choice given the ground-truth label. The constraints enforce that the learned PMFs $\\{\\mathbf{a}_n^{(m)}\\}$\nalign with the empirical observations of the annotator responses. Specifically, the first constraint means\nthat the entries of $\\mathbf{a}_n^{(m)}$ match to the numbers of votes obtained per class per data item collectively from all\nannotators; the second constraint ensures that the learned parameters of each annotator m agree with the\nempirical estimate of the confusions derived from all the responses of that annotator.\nAnother notable approach was proposed in [33] that adopted a geometric interpretation to design a\nlabel integration algorithm. To illustrate their idea, consider an M-dimensional indicator vector $g_{n,k}$ for\neach $n \\in [N]$ and $k \\in [K]$, with entries $g_{n,k}(m) = \\mathbb{I}[y_n^{(m)} = k]$. Then, $\\mathbf{w}^T g_{n,k}$ corresponds to the aggregated\nscore of weighted majority voting with $\\mathbf{w} = [w_1,...,w_M]^T$ denoting the vector of weight values [c.f. (3)].\nInspired by the notion of maximum margin in multiclass support vector machines (SVMs), the approach\nin [33] seeks a hyperplane that separates the point $g_{n, y_n}$ from other points $g_{n,k}$, $k \\neq y_n$ by the maximum\nmargin. Consequently, the annotator-specific weights and the ground-truth labels are learned using the\nfollowing constrained optimization problem:\n$\\begin{aligned}\n&\\underset{\\mathbf{w}, \\{y_n\\}}{\\text{minimize}} \\frac{1}{2} ||\\mathbf{w}||^2 \\\\\n&\\text{subject to } \\mathbf{w}^T (g_{n,y_n} - g_{n,k}) \\geq \\beta \\mathbb{I}[y_n \\neq k], \\forall n, k,\n\\end{aligned}$\nwhere $\\beta$ represents the maximum margin hyperparameter.\nOther interesting developments in crowdsourced label integration include graph-based approaches,\ne.g., the message passing-based algorithm proposed in [17] and its multi-class extension in [34].\n3.4 Label Integration in More Complex Scenarios\nDependent Data. The models discussed so far consider that the data samples are drawn i.i.d. from some\nunknown distribution. However, structured data often arise, e.g., the words in a text and the frames in"}, {"title": null, "content": "a video. In these cases, the data samples show temporal dependencies, i.e., $\\mathbf{x}_n$ is dependent on $\\mathbf{x}_{n-1}$\nand $\\mathbf{x}_{n+1}$. Modeling these dependencies can be useful especially in natural language processing (NLP)-\nrelated crowdsourcing tasks, such as part-of-speech tagging and named-entity recognition, that have re-\ncently gained more popularity with the advent of large language models.\nWhile dealing with sequential data, the DS model can be extended using a hidden Markov model\n(HMM) [35]. A simple extension involves a one-step, time-homogeneous Markov structure is employed\nto model the sequence of labels $y_1, y_2, ..., y_N$ such that nth label depends only on its immediate predeces-\nsor, i.e. $\\Pr(y_n | y_{n-1}, ..., y_1) = \\Pr(y_n | y_{n-1})$\u2014also see Fig. 4. In addition to annotator confusion matrices\n$A_m$'s and the prior probability vector $\\mathbf{d}$, the model is also characterized by a $K \\times K$ transition matrix T that\ndescribes the transitions between labels, i.e., $T(k, k') = \\Pr(y_n = k | y_{n-1} = k')$. Under this DS-HMM model,\nthe joint probability of the observed crowdsourced labels $\\hat{\\mathcal{Y}} = \\{y_n^{(m)}\\}$ is given by\n$\\Pr(\\hat{\\mathcal{Y}}) = \\sum_{\\mathbf{k}} \\mathbf{d}(k_1) \\left[ \\prod_{n=2}^N T(k_n, k_{n-1}) \\right] \\left[ \\prod_{A=1}^M A_m(y_n^{(m)}, k_n) \\right]$,\n(16)\nwhere $\\mathbf{k} = [k_1,...,k_N]^T \\in [K]^N$.\nTo estimate model parameters, an EM algorithm, similar to the one outlined in Sec. 3.3, can be derived\n[35]. The key difference is that the algorithm incorporates a forward-backward algorithm in the E-step due\nto the causal nature of the ground-truth labels. The EM algorithm can also be initialized with the solutions\nobtained from the moment matching strategy, which is reminiscent of the method described in (12). Here,\nthe moments of annotator responses are characterized including the transition probabilities as well, e.g.,\nthe second-order moments are given by\n$\\mathbb{E}[\\hat{Y}^{(m)} \\hat{Y}^{(i)}] = A_m T \\text{diag}(\\mathbf{d}) A_i^T, \\forall m \\neq i$.\n(17)\nOnce the parameters are estimated, a MAP estimate of the ground-truth labels $\\mathcal{Y}$ can be obtained via the\nViterbi algorithm.\nA Bayesian alternative is introduced in [36] that characterizes annotators as follows:\n$C_m(k, k', k'') = \\Pr(y_n = k | y_{n-1} = k', y_n = k'')$,\nwhere $C_m$ is a $K \\times K \\times K$-sized tensor that incorporates the label dependencies as well for the annotator\nconfusions. Under Dirichlet prior assumptions on the model parameters, the approach maximizes the\nposterior probability and adopts a variational Bayes-based algorithm for inference.\nEmpirical studies of these approaches [35,36] show that considering the dependencies of the data is\nbeneficial and the proposed algorithms are much promising than those designed for i.i.d. data\u2014see Tab."}, {"title": "Performance Characterizations of Label Integration", "content": "A key metric that quantifies the deviation of the corrected labels $\\tilde{y}_n$'s from the ground-truth ones $y_n$'s is\ngiven by the probability of error (error rate) $P_e$, defined as follows:\n$P_e = \\Pr(\\tilde{Y} \\neq Y)$,\n(18)\nwith $\\tilde{Y}$ denoting the random variable corresponding to the corrected label $\\tilde{y}_n$. It was shown in [10,39] that\nunder the DS model, the error rate $P_e$ of the MAP rule in (6) decreases exponentially as the number of\nannotators M increases, i.e.,\n$P_e \\leq \\alpha \\exp(-\\beta M)$,\n(19)\nfor some constants $\\alpha > 0, \\beta > 0$. Similar exponential decrease w.r.t. increasing M was reported for the\none-coin, confusion vector models as well as for the majority voting rule [39]. The theoretical results reveal\nthe importance of \u201ccrowd wisdom", "A_1(": "k) \\circ ... \\circ A_M(:,k)$,\n(20)\nwhere $\\mathcal{P}(i_1, ..., i_k) = \\Pr(\\hat{Y}^{(1)} = i_1, ..., \\hat{Y}^{(M)} = i_k)$, which is an Mth-order rank-K tensor. The essential\nuniqueness of $\\mathbf{d}$ and $\\{A_m\\}$ in this tensor model holds under mild conditions; see [40] and the inserted\nbox", "curse of dimensionality\", the identifiability of the DS model using joint distributions\nof three annotators was established in [10, 11]. These smaller joint distributions are much more realistic to\nestimate in practice. In particular, [10] showed that the third-order moment term in (17) is also a rank-K\"\n    },\n    {\n      \"title\": null,\n      \"content\": \"tensor under the CPD model. Therefore, the optimal solutions to (12) reveal the ground-truth $A_m, \\forall m$ up to\na unified permutation ambiguity under mild conditions, e.g., $\\text{rank}(A_m) = K$ for all $m \\in [M]$ as $\\text{rank}(A_m) =$\n$\\text{krank}(A_m)$ in this case (see [41] and the inserted box \u201cIdentfiiability of CPD and NMF": ".", "S_{1,i_T}\\\\": "...", "begin{bmatrix}\nA_{m_1}\\\\": "nA_{m_Q}\n\\end{bmatrix} \\text{diag}(\\mathbf{d}) [A_{i_1}, ..., A_{i_T}]$,\nwhere Q + T = M. Fitting the above model using a KL divergence loss leads to a CNMF formulation like\nthat of (13). As both W and H are nonnegative, the identifiability of these factors holds if both W and H\nsatisfy the separability condition from the NMF literature [42] (see the inserted box \u201cIdentifiability of CPD\nand NMF", "k,": "approx e_k$;\ni.e., annotator $m_k$ is an expert of recognizing items from class k, leading to the existence of a unit vector in\nW. In other words, if there are K annotators, who are experts of class k = 1,..., K respectively, then W\nsatisfies separability\u2014and the same argument applies to H; see [12,30] for more relaxed conditions and\nmore advanced settings (e.g., where not all $S_{m,i}$'s are observed).\nBeyond the DS model, identifiability was also studied for other noise models. As discussed, for the one-\ncoin model, model identifiability was established by using the uniqueness (up to a scaling ambiguity) of\nthe principle eigenvector of $\\mathbb{E}[\\mathbf{\\hat{U} \\hat{U}}^T]$ [22]\u2500see \u201cSpectral Methods\u201d and Eq. (10) in Sec. 3.3. When dependent\ndata are present, the identifiability of both the confusion matrices and the HMM were established in [35],\nalso using tensor-based arguments."}, {"title": "End-to-End (E2E) Learning from Crowdsourced Labels", "content": "Compared to the label integration paradigm, the E2E approaches of Fig. 2 have shown more appealing\nperformance over various datasets\u2014see some numerical evidence in Fig. 7. This may be due to the fact the\nE2E approaches directly work with data features. They are also often a one-stage approach that can avoid\nerror accumulation and propagation among stages.\n4.1 E2E Learning via Maximum Likelihood and EM\nLet us denote a dataset with crowdsourced labels as $\\mathcal{D} = \\{\\mathbf{x}_n, \\{y_n^{(m)}\\}_{m=1}^M \\}_{n=1}^N$. Under similar assumptions\nto the DS model, i.e. data items are sampled independently, and annotator responses are conditionally\nindependent, given the ground-truth label, the joint likelihood of the data can be expressed as follows:\n$\\begin{aligned}\n\\Pr(\\mathcal{D}) &= \\prod_{i=1}^N \\Pr(\\mathbf{x}_n, y_n^{(1)},...,y_n^{(M)}) \\\\\n&= \\prod_{i=1}^N \\sum_{y_n=1}^K \\Pr(y_n|\\mathbf{x}_n) \\prod_{i=1}^M \\Pr(y_n^{(m)} | y_n, \\mathbf{x}_n),\n\\end{aligned}$\n(21)\n$\\begin{aligned}\n&= \\prod_{i=1}^N \\sum_{y_n=1}^K \\Pr(y_n|\\mathbf{x}_n) \\prod_{i=1}^M \\Pr(y_n^{(m)} | y_n).\n\\end{aligned}$\n(22)\nHere, (21) used the conditional independence of annotators' outputs given the ground-truth label, and (22)\nused the assumption that annotator confusion is independent from data items. In the above, $f^*$ and $A_m$\nare defined as before [cf. (1) and (5)]. Again, the confusion matrix $A_m$ is assumed to be the same across all\nn, as $y_n^{(m)}$ and $y_n$ for all n are i.i.d. samples.\nThe goal of E2E learning is to find $\\Pr(y_n|\\mathbf{x}_n)$. Under classification settings, the posterior distribution\nmaps any data item to a PMF over the class labels 1,..., K. Therefore, let a function $\\mathbf{f}_{\\theta} : \\mathbb{R}^D \\rightarrow \\mathbb{R}^K$\nparametrized by $\\theta$ to represent the ground-truth label posterior $f^*$. Collecting all model parameters in\n$\\psi = (A_1, ..., A_M, \\theta)$, we have\n$\\Pr(\\mathcal{D}; \\psi) = \\prod_{i=1}^N \\sum_{y_n=1}^K [f_{\\theta}(\\mathbf{x}_n)]_{y_n} \\prod_{i=1}^M A_m (y_n^{(m)}, y_n)$.\nSimilar to the label integration setting in Sec. 3.3, the MLE problem is formulated as\n$\\hat{\\psi} = \\arg \\max_{\\psi} \\log (\\Pr(\\mathcal{D}; \\psi))$,\n(23)\nand as with the label integration case, optimization w.r.t. $\\psi$ is not trivial. To tackle the optimization problem\nin (23), [14, 16] adopted the EM strategy. Specifically, by considering the unobserved ground-truth labels\n$\\mathcal{Y} = \\{y_n\\}_{n=1}^N$ as the latent variables, the expected value of the complete log-likelihood $\\log(\\Pr(\\mathcal{D}, \\mathcal{Y}; \\psi))$\nunder the current estimate of $\\psi$ is computed in the E-step:\n$Q(\\psi; \\psi^t) = \\mathbb{E}_{\\mathcal{Y} \\sim \\Pr(\\mathcal{Y}; \\mathcal{D}, \\psi^t)} [\\log \\Pr(\\mathcal{D}, \\mathcal{Y}; \\psi)] = \\sum_{n=1}^N \\sum_{k=1}^K q(y_n = k; \\psi^t) \\log \\Pr(\\mathbf{x}_n, Y^{(1)}, ..., Y^{(M)}; \\psi)$\nwith $q(y_n = k; \\psi^t) = [f_{\\theta}^t(\\mathbf{x}_n)]_k \\prod_{i=1}^M A_m^t (y_n^{(m)}, k)$ and Z being the normalization constant [cf. (8)].\nThe M-step estimates $\\psi$ by maximizing $Q(\\psi; \\psi^t)$. This can be done by alternating between the following\nupdates:\n$\\begin{aligned}\nA_m^{t+1}(k', k) &= \\frac{\\sum_{n=1}^N q(y_n = k; \\psi^t) \\mathbb{I}[y_n^{(m)} = k']}{\\sum_{k=1}^K \\sum_{n=1}^N q(y_n = k; \\psi^t) \\mathbb{I}[y_n^{(m)} = k"}, {"title": null, "content": "This EM formulation is very similar to the one advocated for the DS model in Sec. 3.3, albeit with the\nclassifier $\\mathbf{f}_{\\theta}$ taking the role of the prior class probabilities $\\mathbf{d}$. The EM framework is flexible in terms of\nincorporating various $\\mathbf{f}_{\\theta}$ function classes. For binary classification, [16] advocated for a logistic regression\nmodel, where $[\\mathbf{f}_{\\theta}]_1 = \\sigma(\\theta^T \\mathbf{x}_n)$ and $[\\mathbf{f}_{\\theta}]_2 = 1 - \\sigma(\\theta^T \\mathbf{x}_n)$, and $\\sigma$ denotes the sigmoid function. As a result,\na Newton-Raphson algorithm can be implemented for (24b). Neural networks were used in [14] to serve\nas $\\mathbf{f}_{\\theta}$, where (24b) was updated by back-propagation based stochastic gradient. To deal with sequence-\ntype data, [44] used a condition random field (CRF) function as the classifier, which uses of the Viterbi\nalgorithm and the limited-memory Broyden-Fletcher-Goldfarb\u2013Shannon (BFGS) algorithm for the M-step.\nA Bayesian method was adopted in [45] that used a Gaussian process to model a binary classifier and\nadopted an expectation propagation (EP)-based algorithm for the inference that involves EM-like iterative\nsteps.\n4.2 Deep Learning with \u201cCrowd Layer\u201d\nAmong all the functions that can be used as $\\mathbf{f}_{\\theta}$, deep neural networks (DNNs) naturally attract a lot of\nattention, due to their remarkable empirical success in various domains. While [14] showed that EM can\nbe used together with DNNs, the EM framework has some limitations. First, the EM framework is based\non multi-class classification, yet it is not straightforward to extend it to cover other problem settings, e.g.,\nwhen sequence data is involved\u2014the E-step could quickly become intractable. Second, the derivation of\nthe EM framework relies on the conditional independence of the annotators, which may not be always a\nvalid assumption, as discussed in Sec. 3.4. Third, the function $\\mathbf{f}_{\\theta}$ needs to be trained in each M-step, which\nmay be computationally demanding.\nAn alternative approach to incorporate DNNs in crowdsourcing was advocated in [14]. Consider the\nprobability of m-th annotators' response to the data item $\\mathbf{x}_n$ as follows:\n$\\Pr(y_n^{(m)} = k | \\mathbf{x}_n) = \\sum_{k'=1}^K \\Pr(y_n^{(m)} = k | y_n = k') \\Pr(y_n = k' | \\mathbf{x}_n), k \\in [K]$,\n(25)\nwhere we used the law of total probability and the assumption that annotator responses are instance-\nindependent, given the label $y_n$. Upon defining a K-dimensional vector $\\mathbf{p}_n^{(m)}$ such that $[\\mathbf{p}_n^{(m)}]_k = \\Pr(y_n^{(m)} =$\n$k | \\mathbf{x}_n)$, Eq. (25) can be expressed as follows:\n$\\mathbf{p}_n^{(m)} = A_m f^*(\\mathbf{x}_n), \\forall m, n$,\n(26)\nwhere $[f^*(\\mathbf{x}_n)]_k = \\Pr(y_n = k | \\mathbf{x}_n)$ is as defined in (1). Under this model, observations can be understood\nas realizations of a categorical random variable, i.e. $y_n^{(m)} \\sim \\text{categorical}(\\mathbf{p}_n^{(m)})$. To estimate $A_m$ and $f^*$, a\ncommonly used criterion in machine learning is cross entropy (CE), i.e.,\n$CE(\\mathbf{p}_n^{(m)}, A_m f_{\\theta}(\\mathbf{x}_n)) = -\\sum_{k=1}^K [\\mathbf{p}_n^{(m)}]_k \\log[A_m f_{\\theta}(\\mathbf{x}_n)]_k,$\nwhere $\\mathbf{p}_n(k) = 1$ if $y_n^{(m)} = k$ and $p_n(k') = 0$ for $k' \\neq k$, and $\\mathbf{f}_{\\theta}$ is the learning function for approximating\n$f^*$ as before. In a nutshell, CE seeks a model $\\{A_m, f_{\\theta}(\\mathbf{x}_n)\\}$ that matches the \u201cempirical PMF\u201d $\\mathbf{p}_n^{(m)}$. It can\nbe shown that when $N \\rightarrow \\infty$, the minimum of CE is attained at $A_m f_{\\theta}(\\mathbf{x}_n) = \\mathbf{p}_n^{(m)}$. Collecting all annotator\nresponses, [14] used the following coupled cross-entropy minimization criterion (CCEM) [14]:\n$\\begin{aligned}\n&\\underset{\\mathbf{f}_{\\theta} \\in \\mathcal{F}, \\{A_m \\in A\\}}{\\text{minimize}} \\frac{1}{|S|} \\sum_{(\\mathbf{m},n) \\in S} \\sum_{k=1}^K \\mathbb{I}[y_n^{(m)} = k] \\log[A_m f_{\\theta}(\\mathbf{x}_n)]_k\n\\end{aligned}$\n(27)\nwhere $S \\subset [M] \\times [N]$ is the index set of annotator-labeled samples, $\\mathcal{F} \\subset \\{\\mathbf{f}(\\mathbf{x}) \\in \\mathbb{R}^K | \\mathbf{f}(\\mathbf{x}) \\in \\Delta_K, \\forall \\mathbf{x}\\}$ is a\nfunction class parameterized by $\\theta$, $\\Delta_K$ represents the (K \u2212 1)-probability simplex, and A is the constrained"}, {"title": null, "content": "set of confusion matrices $\\{A \\in \\mathbb{R}^{K \\times K} | A \\geq 0, \\mathbf{1}^T A = \\mathbf{1}^T \\}$. In practice, $\\mathbf{f}_{\\theta} \\in \\mathcal{F}$ can be approximately\nenforced by using a softmax layer as its output. The constraints ensure that the output of $\\mathbf{f}_{\\theta}$ and columns\nof $A_m$'s are PMFs. The term \u201ccoupled", "14": "hence this approach is termed crowdlayer. The CCEM type for-\nmulation is arguably more versatile in terms of modeling and computation relative to the EM-type E2E\nalgorithms. First, regularization terms on $A_m$ and $\\theta$ can be easily added for various purposes, e.g., in-\ncorporating prior knowledge and enhancing identifiability [13, 15", "13": "."}]}