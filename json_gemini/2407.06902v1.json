{"title": "Learning From Crowdsourced Noisy Labels: A Signal Processing Perspective", "authors": ["Shahana Ibrahim", "Panagiotis A. Traganitis", "Xiao Fu", "Georgios B. Giannakis"], "abstract": "One of the primary catalysts fueling advances in artificial intelligence (AI) and machine learning (ML) is\nthe availability of massive, curated datasets. A commonly used technique to curate such massive datasets\nis crowdsourcing, where data are dispatched to multiple annotators. The annotator-produced labels are\nthen fused to serve downstream learning and inference tasks. This annotation process often creates noisy\nlabels due to various reasons, such as the limited expertise, or unreliability of annotators, among others.\nTherefore, a core objective in crowdsourcing is to develop methods that effectively mitigate the negative\nimpact of such label noise on learning tasks. This feature article introduces advances in learning from\nnoisy crowdsourced labels. The focus is on key crowdsourcing models and their methodological treat-\nments, from classical statistical models to recent deep learning-based approaches, emphasizing analytical\ninsights and algorithmic developments. In particular, this article reviews the connections between signal\nprocessing (SP) theory and methods, such as identifiability of tensor and nonnegative matrix factorization,\nand novel, principled solutions of longstanding challenges in crowdsourcing-showing how SP perspec-\ntives drive the advancements of this field. Furthermore, this article touches upon emerging topics that are\ncritical for developing cutting-edge AI/ML systems, such as crowdsourcing in reinforcement learning with\nhuman feedback (RLHF) and direct preference optimization (DPO) that are key techniques for fine-tuning\nlarge language models (LLMs).", "sections": [{"title": "Introduction", "content": "Artificial Intelligence (AI) and machine learning (ML) have made significant strides over the past few\ndecades, expanding the potential of models and learning algorithms. These advancements have revolu-\ntionized natural language processing by enhancing language generation and understanding, transformed\ncomputer vision with superior image recognition and analysis, and enabled complex decision-making ca-\npabilities. A key driving factor behind such successes of AI/ML is the availability of large-scale, accurately\nlabeled training data. In fact, data annotation has become an indispensable integrating part of the AI in-\ndustry. In a recent market report from Grand View Research, it was stated that \u201cthe global data collection and\nlabeling market size was valued at $2.22 billion in 2022, and it is expected to expand at a compound annual\ngrowth rate of 28.9% from 2023 to 2030", "wisdom of crowd\" in data labeling makes sense, as the label accuracy\nprovided by individual annotators is sensitive to many factors. If the annotators are human workers, their\nannotation accuracy is limited by their expertise level, background knowledge, and personal experience;\nwhen the annotations are given by automated machine annotators, the accuracy is affected by their model\nexpressiveness and the quality/amount of their training data. The idea of systematic crowdsourcing can\nbe dated back to more than a century ago; see the inserted box \u201cCrowdsourcing in History": "Over the past\ntwo decades, many commercial crowdsourcing platforms have become widely used, including notable\nones such as Amazon Mechanical Turk (AMT) and CrowdFlower [2]. These platforms were instrumental in\ncreating many highly influential datasets in modern AI history. For example, the ImageNet dataset, that has\ndriven significant research advancements in AI, contains approximately 14 million annotations provided\nby approximately 25,000 AMT annotators. More recently, large language models (LLMs), such as ChatGPT,\nGemini, and Llama, used substantial amounts of crowdsourced labels for fine-tuning [3].\nBeyond AI, crowdsourcing has also left significant footprints in a gamut of real-world applications in sci-\nence and engineering. Popular citizen science projects like GalaxyZoo and eBird are byproducts of crowd-\nsourcing, which provided effective and economical solutions to the ever-lingering data scarcity problem in\nlife and physical sciences. Crowdsourcing techniques have also shown great promise in medical imaging-\nbased diagnosis, producing reliable results with minimal expert supervision [4]. In addition, crowdsourcing\nis widely utilized in sensing applications through crowdsensing [5], where a large group of mobile users or\ndevices collectively share real-time data. An example of crowdsensing is the crowd-contributed real-time\ninformation on road conditions, which is now widely available in applications such as Google Maps and\nWaze. Various data fusion applications in remote sensing and healthcare that aggregate information from\nmultiple sources of varying reliability also have strong connections to crowdsourcing techniques [6]. Fur-\nthermore, crowdsourcing is conceptually similar to ensemble learning methods, such as bagging, boosting,\nand stacking, that combine multiple models to improve overall predictive performance.\nDespite the remarkable achievements of crowdsourcing, the fundamental challenge is that annotator-\nprovided labels can be substantially noisy, and such noisy labels are detrimental to downstream tasks'\nperformance. For example, in deep learning systems, noisy labels cause overfitting to noise and poor gen-\neralization. Early approaches for alleviating the negative impacts of crowdsourced label noise focused on\nimproving the quality of annotators' responses during label collection. These approaches include various\nproject management-based strategies, such as designing proper labeling workflow, querying proper ex-\nperts, enhancing supervision mechanisms, and using effective incentive methods-see [7] and references\ntherein. However, implementing such complex quality control mechanisms has become increasingly diffi-\ncult and less cost-effective as data volume increases. For massive data annotation tasks faced by modern\ncrowdsourcing systems, automated annotation integration and label correction algorithms advocated by\nthe machine learning community are much more relevant. In this article, the term \u201ccrowdsourcing"}, {"title": "Problem Settings", "content": "In this section, we formally define the problem statement of crowdsourcing. In addition, we introduce two\nmajor paradigms of the crowdsourcing system design.\n2.1 Problem Statement\nAs mentioned, crowdsourcing can be used to integrate various types of annotations (e.g., class labels [8],\ntraffic information [9], and bird counts); that is, in principle, the annotations in crowdsourcing can be both\ndiscrete and continuous values. Nonetheless, for simplicity, we use categorical class label annotations as the\nprimary working example to introduce crowdsourcing ideas and algorithms. Consider a dataset consisting\nof $N$ data items, i.e., $\\mathcal{X} := {\\mathbf{x}_n}_{n=1}^N$, where $\\mathbf{x}_n \\in \\mathbb{R}^D$ is the feature vector representing the $n$th data item.\nAssume that each data item belongs to one of $K$ classes. Let ${\\mathbf{y}_n}_{n=1}^N$ denote the set of ground-truth labels,\nwhere $y_n \\in [K], \\forall n$; i.e., $y_n = k$ if the data item $\\mathbf{x}_n$ belongs to class $k$. Note that the ground-truth labels\n${\\mathbf{y}_n}_{n=1}^N$ are unknown, and $M$ crowdsourced workers (annotators) are employed to label the dataset $\\mathcal{X}$, i.e.,\nto give their estimates of ${\\mathbf{y}_n}$. We use $\\hat{y}_n^{(m)} \\in [K]$ to denote the label assigned to the $n$th data item by the\n$m$th annotator. Note that the annotators may be human, pre-trained ML algorithms, or predefined decision\nrules. Crowdsourced labels are often noisy, which means that we have $\\hat{y}_n^{(m)} \\neq y_n$ for many $(m, n)$ pairs.\n2.2 Crowdsourcing Systems: Two Paradigms\nThe developments of crowdsourcing methods can be roughly classified into two categories, namely,\nlabel integration approaches (see, e.g., [8, 10\u201312]) and end-to-end (E2E) learning approaches (e.g., [13-16]).\nThe former isolates noisy label correction from downstream tasks, yet the latter directly uses the noisy\nlabels to train systems serving for specific tasks.\nLabel Integration. Fig. 1 shows the schematics of the first type of crowdsourcing system, where label\nintegration is the key module. These systems consist of two stages, where the label integration stage is not\naffected by the subsequent downstream task stage. In the label integration stage, the labels $\\hat{y}_n^{(m)}$'s are fused\nto produce an estimated label $\\hat{y}_n$ in the hope that $\\hat{y}_n = y_n$. Oftentimes, the characteristics of the annotators,\ni.e. \u201cannotator confusions\u201d in the figure, are also estimated in the process. Then, the estimated labels {$\\tilde{y}_n$}\nare used for training the downstream machine learning system, e.g., training a neural network function to\npredict ground-truth labels. The training stage does not need specific design to handle multiple annotators\nor noisy labels.\nEnd-to-End (E2E) Learning. Fig. 2 illustrates a typical E2E crowdsourcing system. This type of systems\nconsiders both data and noisy labels as input and directly trains the learning systems on the target down-\nstream tasks. As there is no stage breakdown, these systems are called end-to-end (E2E) systems. When the\ntarget is to train a classifier $f_{\\theta} : \\mathbb{R}^D \\rightarrow \\mathbb{R}^K$, E2E systems design a loss function $\\ell$ such that\n$\\begin{aligned}\nf_{\\hat{\\theta}} = \\mathop{\\arg \\min}_{\\theta, \\eta} \\ell({\\{{\\mathbf{x}_n}\\}, {\\{\\hat{y}_n^{(m)}\\}}}, {\\theta}, {\\eta}),\n\\end{aligned}$\nwhere $\\eta$ represents additional model parameters according to specific loss designs. The learned $f_{\\hat{\\theta}}$ is ex-\npected to be a good ground-truth label predictor. For example, denote the ground-truth label posterior\ndistribution as a function $f^* : \\mathbb{R}^D \\rightarrow \\mathbb{R}^K$ where\n$\\begin{aligned}\n    [f^*(\\mathbf{x}_n)]_k = \\Pr(y_n = k | \\mathbf{x}_n), \\quad \\forall (\\mathbf{x}_n, y_n).\n    \\tag{1}\n\\end{aligned}$\nThe goal of many E2E approaches (see, e.g., [13\u201315]) is to learn $f_{\\theta}$ such that $f_{\\theta} \\approx f^*$.\nGenerally speaking, label integration approaches can feature more lightweight and tractable algorithms,\nas they do not require training a learning system $f_{\\theta}$ that is usually parameterized by a complex function\nclass, such as kernels or neural networks. On the other hand, E2E methods often exhibit more appealing\nperformance, possibly because they naturally exploit information from data features and the underlying\nstructure of the specific tasks in a joint way. Both crowdsourcing approaches can be considered unsupervised,"}, {"title": "Label Integration Approaches", "content": "In this section, we focus on approaches designed for the label integration module in Fig. 1.\n3.1 Majority Voting and Challenges\nThe arguably simplest approach for label integration is majority voting (MV), where the estimated label for\neach data item is the one voted by most annotators, i.e.,\n$\\begin{aligned}\n\\hat{y}_n = \\mathop{\\arg\\max}_{k} \\sum_{m=1}^M \\mathbb{I}[\\hat{y}_n^{(m)} = k].\n\\tag{2}\n\\end{aligned}$\nHowever, MV is not always effective, as it implicitly assumes equal reliability among all annotators. Fur-\nthermore, due to the significant time and effort required, it is not economical to ask all annotators to label\nevery data item. This means that many items may not have sufficient annotations that merit a voting pro-\ncess. Under such circumstances, MV can be far from optimal for label integration [17]. To take unequal\nreliability of the annotators into consideration, weighted majority voting (WMV) schemes were proposed.\nFor example, a non-negative scalar weight $w^{(m)}$ representing the labeling accuracy of annotator $m$ was\nconsidered in [18]:\n$\\begin{aligned}\n\\hat{y}_n = \\mathop{\\arg\\max}_{k} \\sum_{m=1}^M w^{(m)} \\mathbb{I}[\\hat{y}_n^{(m)} = k].\n\\tag{3}\n\\end{aligned}$\nNonetheless, accurate weight estimation per se is a challenging task. Due to these reasons, MV schemes are\noften only used as the basic baselines in label integration.\n3.2 Label Noise Models in Label Integration\nA notable milestone in label integration research occurred when the MV approaches were superseded by\nprobabilistic modeling approaches. By explicitly modeling noisy labels using probabilistic generative mod-\nels, these approaches often provide more reliable label integration performance. In this section, we intro-\nduce a number of representative models in this genre.\nDawid-Skene (DS) Model. The seminal work of Dawid and Skene introduced one of the most influen-\ntial probabilistic models for crowdsourcing in the late 1970s [8]. Under the Dawid-Skene (DS) model, the\nground-truth label is a latent discrete random variable, denoted as $Y$, and each label $y_n \\in [K]$ is drawn\nindependently from the random variable $Y$. The annotator's responses are the observed random variables,"}, {"title": "Methods for Noise Model Learning", "content": "Under the aforementioned noise generation models, label integration boils down to learning the key model\nparameters, e.g., the confusion matrices and the prior probability vector in the DS model. In this subsection,\nwe briefly review some representative methods for learning these models.\nExpectation Maximization (EM). The seminal work by Dawid and Skene [8] sought a maximum-likelihood\nestimator (MLE) of annotator confusion matrices and class priors of the DS model. Collect all ground-\ntruth labels and annotator responses in $\\mathbf{Y} = {y_n}$ and $\\hat{\\mathbf{Y}} = {\\hat{y}_n^{(m)}}$, respectively, and all unknown model\nparameters in $\\psi = {A_1, . . . , A_M, \\mathbf{d}}$. The MLE of $\\psi$ is given by\n$\\begin{aligned}\n    \\hat{\\psi} = \\mathop{\\arg \\max}_{\\psi} \\log \\Pr(\\hat{\\mathbf{Y}}; \\psi) = \\mathop{\\arg \\max}_{\\psi} \\sum_{n=1}^N \\log \\Pr(\\hat{y}_n^{(1)}, ..., \\hat{y}_n^{(M)}; \\psi)\n    = \\mathop{\\arg \\max}_{\\psi} \\sum_{n=1}^N \\log \\sum_{k=1}^K {\\mathbf{d}(k) \\prod_{m=1}^M \\sum_{k'=1}^K \\mathbb{I}[\\hat{y}_n^{(m)} = k'] A_m(k', k)},   \\tag{7}\n\\end{aligned}$\nwhere $\\log \\Pr(\\hat{\\mathbf{Y}}; \\psi)$ is the log-likelihood function of observed annotator labels, parametrized by $\\psi$. As di-\nrectly optimizing the log-likelihood is often intractable, Dawid and Skene introduced an EM-based algo-\nrithm to learn the DS model parameters. The EM algorithm is the workhorse for learning Naive Bayes and\nmixture models. Using the EM algorithm, the log-likelihood function is maximized iteratively, with the\nfollowing steps performed at each iteration:\n(i) The \u201cExpectation\u201d (E) step: The E-step in iteration $t$ is performed as follows:\n$\\begin{aligned}\nQ(\\psi; \\psi^t) &= \\mathbb{E}_{y \\sim \\Pr(\\mathbf{Y}; \\hat{\\mathbf{Y}}, \\psi^t)}[\\log \\Pr(\\mathbf{Y}, \\hat{\\mathbf{Y}}; \\psi)] = \\sum_{n=1}^N\\sum_{k=1}^K q(y_n = k; \\psi^t) \\log \\Pr(\\hat{y}_n^{(1)}, ..., \\hat{y}_n^{(M)}; \\psi),\n\\end{aligned}$\nwhere the superscript $t$ denotes iteration index, and the expectation is taken with respect to (w.r.t.) the pos-\nterior probability $\\Pr(\\mathbf{Y}; \\hat{\\mathbf{Y}}, \\psi^t)$ based on the current estimates $\\psi^t = {A_1, . . . , A_M, \\mathbf{d}^t}$. Note that, $Q(\\psi; \\psi^t)$\nis essentially a lower bound of the log-likelihood. The E-step boils down to estimating $q(y_n = k;\\psi^t) =$\n$\\Pr(y_n = k|\\hat{y}_n^{(1)}, ..., \\hat{y}_n^{(M)}, \\psi^t)$ given $A_m$'s and $\\mathbf{d}^t$. Under the DS model and using Bayes rule, $q(y_n = k; \\psi^t)$\nadmits a simple closed form, i.e.,\n$\\begin{aligned}\nq(y_n = k; \\psi^t) = \\frac{\\exp \\left( \\log {\\mathbf{d}^t(k)} + \\sum_{m=1}^M \\sum_{k'=1}^K \\log A_m^t(k', k) \\mathbb{I}[\\hat{y}_n^{(m)} = k'] \\right)}{\\sum_{k''=1}^K \\exp \\left( \\log {\\mathbf{d}^t(k'')} + \\sum_{m=1}^M \\sum_{k'=1}^K \\log A_m^t(k'', k') \\mathbb{I}[\\hat{y}_n^{(m)} = k'] \\right)}. \\tag{8}\n\\end{aligned}$\nNote that this posterior has a similar form to the MAP estimator of (6).\n(ii) The \u201cMaximization\u201d (M) step: The M-step refines model parameters by maximizing $Q(\\psi; \\psi^t)$, which\nalso admits analytical updates:\n$\\begin{aligned}\nA_m^{t+1}(k', k) &= \\frac{\\sum_{n=1}^N q(y_n = k; \\psi^t) \\mathbb{I}[\\hat{y}_n^{(m)} = k']}{\\sum_{k''=1}^K \\sum_{n=1}^N q(y_n = k; \\psi^t) \\mathbb{I}[\\hat{y}_n^{(m)} = k'']},\\\\\n{\\mathbf{d}}^{t+1}(k) &= \\frac{\\sum_{n=1}^N q(y_n = k; \\psi^t)}{\\sum_{k'=1}^K \\sum_{n=1}^N q(y_n = k'; \\psi^t)}.\n\\end{aligned}$\nThe EM scheme can also be readily adapted to learn the special cases of the DS model, i.e., one-coin, con-\nfusion vector, spammer-hammer, and GLAD models as discussed in 3.2. A similar EM strategy is also\nemployed to learn the Bayesian model proposed in [16].\nOne salient feature of the EM algorithm is its scalability\u2014it enjoys a computational complexity that\nis linear in $N$ and $M$, which is appealing for large-scale crowdsourcing problems. However, it was also\nobserved that the EM algorithm does not converge well, if the initialization is not carefully chosen [11,29],\nperhaps due to the nonconvexity of the MLE loss.\nSpectral Methods. One of the notable spectral methods for label integration is the eigendecomposition-\nbased approach proposed in [22] under the one-coin binary model. Consider the binary classification prob-\nlem with unobserved ground-truth labels $y_n \\in \\{-1, +1\\}$ and the one-coin model parameters $p_m$'s, where\n$p_m$ is the probability that annotator $m$ provides the correct label. Let $z_n^{(m)}$ be the correctness indicator of\nannotator $m$ on item $n$; i.e., $z_n^{(m)}= 1$ if annotator $m$ provides the correct label and $z_n^{(m)}= -1$ otherwise.\nThen, it can be shown that\n$\\begin{aligned}\n\\mathbb{E}[z_n^{(m)} y_n^{(m)}] = \\begin{cases}\n    \\rho_m + (1-\\rho_m)^2, & \\text{w.p } p_m \\\\\n    -1, &  \\text{w.p } 1 - p_m  \\\\\n\\end{cases}\n\\end{aligned}$\nSince $\\hat{y}_n^{(m)} = z_n^{(m)} y_n$ holds, we further obtain:\n$\\begin{aligned}\n \\mathbb{E}[\\sum_{m=1}^M\\sum_{m' = 1}^M \\hat{y}_n^{(m)} \\hat{y}_{n'}^{(m)}] = \\begin{cases} \n\\hat{y}_n \\hat{y}_{n'} \\kappa & n \\neq n'\\\\n\\hat{y}_n M  & n = n'\\\\n\\end{cases} \\tag{9}\n\\end{aligned}"}, {"title": "Performance Characterizations of Label Integration", "content": "A key metric that quantifies the deviation of the corrected labels $\\tilde{y}_n$'s from the ground-truth ones $y_n$'s is\ngiven by the probability of error (error rate) $P_e$, defined as follows:\n$\\begin{aligned}\n    P_e = \\Pr(\\tilde{Y} \\neq Y),\n    \\tag{18}\n\\end{aligned}$\nwith $\\tilde{Y}$ denoting the random variable corresponding to the corrected label $\\hat{y}_n$. It was shown in [10,39] that\nunder the DS model, the error rate $P_e$ of the MAP rule in (6) decreases exponentially as the number of\nannotators $M$ increases, i.e.,\n$\\begin{aligned}\n    P_e \\leq a \\exp(-\\beta M),    \\tag{19}\n\\end{aligned}$\nfor some constants $a > 0$, $\\beta > 0$. Similar exponential decrease w.r.t. increasing $M$ was reported for the\none-coin, confusion vector models as well as for the majority voting rule [39]. The theoretical results reveal\nthe importance of \u201ccrowd wisdom", "A_1(": "k) \\circ ... \\circ A_M(:,k)}, \\tag{20}\n\\end{aligned}$\nwhere $\\mathbb{P}(i_1, ..., i_M) = \\Pr(\\hat{Y}^{(1)} = i_1, ..., \\hat{Y}^{(M)} = i_M)$, which is an $M$th-order rank-$K$ tensor. The essential\nuniqueness of $\\mathbf{d}$ and ${A_m}$ in this tensor model holds under mild conditions; see [40] and the inserted\nbox", "curse of dimensionality\", the identifiability of the DS model using joint distributions\nof three annotators was established in [10, 11]. These smaller joint distributions are much more realistic to\nestimate in practice. In particular, [10] showed that the third-order moment term in (17) is also a rank-K\"\n    },\n    {\n      \"title\": \"E2E Learning from Crowdsourced Labels\",\n      \"content\": \"Compared to the label integration paradigm, the E2E approaches of Fig. 2 have shown more appealing\nperformance over various datasets\u2014see some numerical evidence in Fig. 7. This may be due to the fact the\nE2E approaches directly work with data features. They are also often a one-stage approach that can avoid\nerror accumulation and propagation among stages.\n4.1 E2E Learning via Maximum Likelihood and EM\nLet us denote a dataset with crowdsourced labels as $\\mathcal{D} = {(\\mathbf{x}_n, {\\{\\hat{y}_n^{(m)}\\}_{m=1}^M)}_{n=1}^N$. Under similar assumptions\nto the DS model, i.e. data items are sampled independently, and annotator responses are conditionally\nindependent, given the ground-truth label, the joint likelihood of the data can be expressed as follows:\n$\\begin{aligned}\n\\Pr(\\mathcal{D}) &= \\prod_{i=1}^N Pr(\\mathbf{x}_n, {\\hat{y}_n^{(1)},...,\\hat{y}_n^{(M)}}) \\nonumber \\\\\n&= \\prod_{i=1}^N \\sum_{y_n=1}^K \\left[ Pr(y_n | \\mathbf{x}_n) \\prod_{i=1}^M Pr(\\hat{y}_n^{(m)} | y_n, \\mathbf{x}_n) \\right],    \\tag{21} \\\\\n&= \\prod_{i=1}^N \\sum_{y_n=1}^K \\left[ Pr(y_n | \\mathbf{x}_n) \\prod_{i=1}^M Pr(\\hat{y}_n^{(m)} | y_n) \\right]. \\tag{22}\n\\end{aligned}$\nHere, (21) used the conditional independence of annotators' outputs given the ground-truth label, and (22)\nused the assumption that annotator confusion is independent from data items. In the above, $f^*$ and $A_m$\nare defined as before [cf. (1) and (5)]. Again, the confusion matrix $A_m$ is assumed to be the same across all\n$n$, as $\\hat{y}_n^{(m)}$ and $y_n$ for all $n$ are i.i.d. samples.\nThe goal of E2E learning is to find $\\Pr(y_n|\\mathbf{x}_n)$. Under classification settings, the posterior distribution\nmaps any data item to a PMF over the class labels 1,..., $K$. Therefore, let a function $f_{\\theta} : \\mathbb{R}^D \\rightarrow \\mathbb{R}^K$\nparametrized by $\\theta$ to represent the ground-truth label posterior $f^*$. Collecting all model parameters in\n$\\psi = (A_1, ..., A_M, \\theta)$, we have\n$\\begin{aligned}\nPr(\\mathcal{D}; \\psi) = \\prod_{i=1}^N \\sum_{y_n=1}^K \\left[ [f_{\\theta}(\\mathbf{x}_n)]_{y_n} \\prod_{i=1}^M A_m(\\hat{y}_n^{(m)}, y_n) \\right].\n\\end{aligned}$\nSimilar to the label integration setting in Sec. 3.3, the MLE problem is formulated as\n$\\begin{aligned}\n    \\hat{\\psi} = \\mathop{\\arg \\max}_{\\psi} \\log (\\Pr(\\mathcal{D}; \\psi)), \\tag{23}\n\\end{aligned}$\nand as with the label integration case, optimization w.r.t. $\\psi$ is not trivial. To tackle the optimization problem\nin (23), [14, 16] adopted the EM strategy. Specifically, by considering the unobserved ground-truth labels\n$\\mathbf{Y} = {y_n}_{n=1}^N$ as the latent variables, the expected value of the complete log-likelihood $\\log(\\Pr(\\mathcal{D}, \\mathbf{Y}; \\psi))$\nunder the current estimate of $\\psi$ is computed in the E-step:\n$\\begin{aligned}\nQ(\\psi; \\psi^t) = \\mathbb{E}_{y \\sim \\Pr(\\mathbf{Y}; \\mathcal{D}, \\psi^t)}[\\log \\Pr(\\mathcal{D}, \\mathbf{Y}; \\psi)] = \\sum_{n=1}^N\\sum_{k=1}^K q(y_n = k; \\psi^t) \\log Pr(\\mathbf{x}_n, {\\hat{y}_n^{(1)}, ..., \\hat{y}_n^{(M)}}; \\psi)\n\\end{aligned}$\nwith $q(y_n = k; \\psi^t) = [f_{\\theta^t}(\\mathbf{x}_n)]_k \\prod_{i=1}^M A_m^t(\\hat{y}_n^{(m)}, k)$ and $Z$ being the normalization constant [cf. (8)].\nThe M-step estimates $\\psi$ by maximizing $Q(\\psi; \\psi^t)$. This can be done by alternating between the following\nupdates:\n$\\begin{aligned}\nA_m^{t+1}(k', k) &= \\frac{\\sum_{n=1}^N q(y_n = k; \\psi^t) \\mathbb{I}[\\hat{y}_n^{(m)} = k']}{\\sum_{k''=1}^K \\sum_{n=1}^N q(y_n = k; \\psi^t) \\mathbb{I}[\\hat{y}_n^{(m)} = k'']},     \\tag{24a} \\\\\n\\theta^{t+1} &= \\mathop{\\arg \\max}_{\\theta} Q(\\theta; (\\theta^t, {A_m^{t+1}})).    \\tag{24b}\n\\end{aligned}\"\n    },\n    {\n      \"title\": \"Deep Learning with \u201cCrowd Layer": "content", "Among all the functions that can be used as $f_{\\theta}$, deep neural networks (DNNs) naturally attract a lot of\nattention, due to their remarkable empirical success in various domains. While [14": "showed that EM can\nbe used together with DNNs", "14": ".", "follows": "n$\\begin{aligned"}, "K"], "follows": "n$\\begin{aligned}\n{\\mathbf{p}_n^{(m)}} = A_m f^*(\\mathbf{x}_n), \\quad \\forall m, n,       \\tag{26}\n\\end{aligned}$\nwhere $[f^*(\\mathbf{x}_n)]_k = \\Pr(y_n = k | \\mathbf{x}_n)$ is as defined in (1). Under this model, observations can be understood\nas realizations of a categorical random variable, i.e. $\\hat{y}_n^{(m)} \\sim categorical({\\mathbf{p}_n^{(m)}})$. To estimate $A_m$ and $f^*$, a\ncommonly used criterion in machine learning is cross entropy (CE), i.e.,\n$\\begin{aligned}\n    CE(\\hat{\\mathbb{P}}^{(m)}, A_m f_{\\theta}(\\mathbf{x}_n)) = - \\sum_{k=1}^K {\\hat{\\mathbb{P}}_n^{(m)}(k) \\log[A_m f_{\\theta}(\\mathbf{x}_n)]_k},  \\\\\\\\n\\end{aligned}$\nwhere $\\hat{\\mathbb{P}}_n^{(m)}(k) = 1$ if $\\hat{y}_n^{(m)} = k$ and $\\hat{\\mathbb{P}}_n^{(m)}(k') = 0$ for $k' \\neq k$, and $f_{\\theta}$ is the learning function for approximating\n$f^*$ as before. In a nutshell, CE seeks a model ${A_m, f_{\\theta}(\\mathbf{x}_n)}$ that matches the \u201cempirical PMF\u201d ${\\mathbf{p}_n^{(m)}}$. It can\nbe shown that when $N \\rightarrow \\infty$, the minimum of CE is attained at"}