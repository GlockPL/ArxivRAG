{"title": "More Expressive Attention with Negative Weights", "authors": ["Ang Lv", "Ruobing Xie", "Shuaipeng Li", "Jiayi Liao", "Xingwu Sun", "Zhanhui Kang", "Rui Yan"], "abstract": "We propose a novel attention mechanism, named Cog Attention, that enables attention weights to be negative for enhanced expressiveness, which stems from two key factors: (1) Cog Attention can shift the token deletion and copying function from a static OV matrix to dynamic QK inner products, with the OV matrix now focusing more on refinement or modification. The attention head can simultaneously delete, copy, or retain tokens by assigning them negative, positive, or minimal attention weights, respectively. As a result, a single attention head becomes more flexible and expressive. (2) Cog Attention improves the model's robustness against representational collapse, which can occur when earlier tokens are over-squashed into later positions, leading to homogeneous representations. Negative weights reduce effective information paths from earlier to later tokens, helping to mitigate this issue. We develop Transformer-like models which use Cog Attention as attention modules, including decoder-only models for language modeling and U-ViT diffusion models for image generation. Experiments show that models using Cog Attention exhibit superior performance compared to those employing traditional softmax attention modules. Our approach suggests a promising research direction for rethinking and breaking the entrenched constraints of traditional softmax attention, such as the requirement for non-negative weights.", "sections": [{"title": "1. Introduction", "content": "The Transformer architecture (Vaswani et al., 2017) has achieved success across numerous applications, such as language modeling (Brown et al., 2020) and image generation (Dosovitskiy et al., 2021). A crucial factor contributing to its success is the softmax attention mechanism (Bahdanau et al., 2015).\nSoftmax ensures non-negative attention weights, but we argue that it limits the expressiveness of the attention mechanism. Suppose a softmax attention head has an OV matrix capable of deleting tokens that the QK matrix attends to; since attention weights must be non-positive, a useful token is also somewhat deleted. By allowing negative attention weights, however, deletion or copying can be expressed through the sign of the attention weight and accomplished during the weighted summation of value vectors. This functional shift also allows the OV matrix to focus more on higher-level tasks, such as refinement or modification, rather than solely handling contextual deletions or copies. Consequently, negative attention weights eliminate irrelevant tokens while preserving useful ones, mitigating the risk of \"friendly fire\" on useful tokens.\nDespite the potential benefits of incorporating negative weights in attention mechanisms, this question has been rarely explored. Apart from the common belief that attention weights should naturally be non-negative, introducing negative weights can lead to challenges such as training instability, numerical overflow, and difficulties in attention normalization due to issues like division by zero, etc.\nIn this paper, we propose a novel attention mechanism named Cog Attention that enables negative weights. Cog Attention exhibits superior properties from a mechanistic interpretability perspective and surpasses softmax attention in various applications without introducing any additional parameters. In Section 3, we provide mechanistic evidence for the expressiveness of Cog Attention:\n(1) We identify attention heads that share the same work-"}, {"title": "2. Method", "content": "We begin by presenting the formulation of our proposed Cog Attention. Then, we discuss the design motivation and underlying principles."}, {"title": "2.1. Formulation", "content": "Let $q, k, v \\in \\mathbb{R}^{n\\times d}$ represent the query, key, and value vectors in an attention head. $n$ is the number of input tokens and $d$ is the hidden states dimension. The general attention computation can be expressed as follows:\n$P_{i} = q_{i}k,\\newline a_{i} = \\phi(p_{i}),\\newline o = \\sum_{j=0}^{i} a_{ij}v_{j}, $  (1)\nwhere $p_{i} \\in \\mathbb{R}^{1\\times n}$ is the $i$-th row of the inner-product matrix, $a \\in \\mathbb{R}^{1\\times n}$ is the $i$-th row of the attention weights. $o \\in \\mathbb{R}^{1\\times d}$ is the weighted summation of vectors attended."}, {"title": "2.2. Design Principle", "content": "(1) The way to introduce negative weights. Although the inner product of query and key vectors naturally contains both positive and negative values, we apply an exponential function to this inner product and subsequently recover the sign of each term. This method is driven by our observation that an effective attention pattern for convergence must demonstrate sufficient kurtosis; i.e., it should be sparse and sharp enough. Without the exponential function, the attention pattern tends to be too flat, which can impede training convergence. We also tried using a cubic function as an alternative, which would eliminate the sign recovery process while still offering attention weights with adequate kurtosis. However, we ultimately chose the exponential function because of its convenience in gradient computation.\n(2) The way to avoid numerical overflow. In Eq.3, we avoid numerical overflow in exponential functions by subtracting the maximum absolute value from the index. This approach differs from the softmax, which subtracts the maximum value, as seen in Eq.2. Our approach ensures that the maximum input to SignExp(\u00b7) remains 0, effectively avoiding overflow caused by a large $s_{i,j} \\cdot p_{i,j}$, as illustrated in Figure 2.\n(3) The way to normalization. In softmax functions, the denominator in Eq.2 is the summation of the numerator, serving to normalize the outputs. This process ensures that the resulting attention weights sum to 1, with each term constrained within the range of [0, 1]. Previous studies suggested that, for better convergence, the sum of attention"}, {"title": "3. Enhanced Expressiveness of Cog Attention: A Mechanistic Interpretability Perspective", "content": "In this section, we provide mechanistic interpretability evidence to demonstrate that negative attention weights can enhance neural networks' expressiveness."}, {"title": "3.1. Cog Attention Enhances the Flexibility of Attention Heads", "content": "Due to its unconstrained attention weights, Cog Attention enables processes such as deletion or copying, transitioning from a static OV matrix to dynamic query-key products. This capability stands as a key advantage of Cog Attention, facilitating concurrent processes within a single head, thereby enhancing the models' flexibility and expressiveness.\nWe trained a Transformer language model with 141 million parameters and a Transformer-like language model using Cog Attention of the same size, respectively. Details regarding the model training can be found in Section 4. We studied attention heads' working mechanisms on the indirect object identification (IOI) task (Wang et al., 2023), where the model is given with a context that includes the names of two people. For instance, given the input \"Christopher and David had a lot of fun at school. David gave a ring to,\" \"Christopher\" is the indirect object (IO), while \"David\" is the subject (S). The correct answer in IOI task is always the IO, which in this case is \"Christopher.\" There are 100 samples in the dataset.\nTo identify the most influential attention heads contributing to correct predictions in each model, we employed the path patching algorithm (Wang et al., 2023). We identified two significant heads: the 4th Cog Attention head in Layer 9 (CH9.4) and the 11th softmax head in Layer 9 (SH9.11). These heads accomplish the IOI task through a process of elimination. Additionally, we computed the attention weights from the final token to both the IO and S tokens, plotted against the inner product of the heads' outputs and IO and S's individual embedding, as shown in Figure 1(c) and (d).\nThe figures demonstrate that the two heads employ different mechanisms for the elimination process:\nSH9.11 assigns a large weight to the S tokens, with its OV matrix generating a vector that opposes the representation of S's embedding. Given that the function of the OV matrix is determined by the trained parameters, SH9.11 also eliminates IOs due to its non-negative attention weights toward them. This is supported by Pearson correlation coefficient of -0.21 between the attention weights and the inner product, suggesting a weak to medium correlation. As a result, the non-negative nature of the softmax attention weights limits the performance on the IOI task.\nIn contrast, CH9.4 assigns negative weights to S, effectively eliminating it while assigning minimal weights to IOs. Even if an IO is assigned with a negative weight, we contend that the OV matrix in CH9.4 is relieved from deletion to perform post-processing such as refinement or modification, and thus retains IOs. Evidence for this claim comes from the eigenvalue positivity for the OV matrices"}, {"title": "3.2. Cog Attention is More Robust to Representational Collapse", "content": "Representational collapse (Liu et al., 2020; Xie et al., 2023) refers to that the representations at many positions become similar as the Transformer model goes deeper. Barbero et al. (2024) proposed an explanation for this issue, suggesting that earlier input tokens have more information paths to the final position compared to later tokens. As a result, the information from earlier tokens \u201cover-squashed\" into the representation at the final position, which is used for next-token prediction. This over-squashing causes representational collapse, leading Transformer models to struggle with distinguishing contexts that differ only slightly.\nWe demonstrate that Transformer models using Cog Attention are more robust to representational collapse than those using softmax attention. This improvement may be attributed to the negative weights reducing the number of effective information paths from earlier tokens to later positions, thereby mitigating the degree of over-squashing.\nTo assess the extent of representational collapse in a Transformer-based language model, we evaluate two tasks. In Task 1, \"Finding a Zero,\" the model processes two input sequences: the first consists of n + 1 ones, and the second consists of n ones with a leading zero. Following Barbero"}, {"title": "4. Experiments", "content": "We construct Transformer-like models using Cog Attention as the attention module, named Cogformer. We implement Cogformer in various tasks, including language modeling and image generation, to serve as a decoder-only model and a diffusion model, respectively. To showcase the effectiveness of Cog Attention we evaluate and compare the performance of Cogformer with the Transformer across"}, {"title": "4.1. Decoder-Only Models for Language Modeling", "content": "Architecture Details and Hyper-parameters. We train decoder-only Cogformer and Transformer-based language models on the RedPajama dataset (Computer, 2023). Apart from variations in the attention modules, the overall architecture and training hyper-parameters for Cogformer are identical to those of the vanilla Transformer. Importantly, we preserve the softmax attention in both the first and last layers of a deep Cogformer; we will elaborate on this design choice in Section 5.1. We use rotary position embedding (Su et al., 2023) and SwiGLU (Shazeer, 2020) activation in the feed-forward networks (FFN) following Llama (Touvron et al., 2023). RMSNorms are applied before both the attention and FFN modules. We adopt the Llama tokenizer, having a vocabulary of 32,000 tokens.\nThe model, with 141M parameters, consists of 12 layers, each with 12 attention heads. Its hidden state dimension is 768, while the intermediate dimension of the MLP layers is 3,072, with a context length of 2,048 tokens.\nThe models are trained on 100 billion tokens, with batch size of 131,072 tokens, an initial learning rate of 2e-4, and linear warmup for the first 2,000 steps followed by a cosine decay schedule to 4% of the peak learning rate. The AdamW optimizer (Loshchilov & Hutter, 2019) is configured with $(\u03b2\u03b9, \u03b22) = (0.9, 0.95)$, a norm clipping value of 1, and a weight decay of 0.1. Our training is conducted on 8\u00d7A800-80G GPUs for approximately one week.\nTest Tasks. We evaluate language models on a variety of widely used tasks that comprehensively assess their lan-guage modeling capabilities. These tasks include:\n(1) ARC-easy (Clark et al., 2018), a multiple-choice question-answering task, and ARC-challenge, which presents greater challenges than ARC-easy.\n(2) PIQA (Bisk et al., 2020), a task on physical common-sense reasoning.\n(3) SST-2 (Socher et al., 2013), a binary sentiment classification task.\n(4) MNLI (Williams et al., 2018), where the model predicts whether a given pair of sentences entail, contradict, or are unrelated to each other."}, {"title": "4.2. Diffusion Models for Image Generation", "content": "For our image generation tasks, we use U-ViT (Bao et al., 2023) as our baseline, which employs the Vision Transformer (Dosovitskiy et al., 2021) architecture as a diffusion model. We replace the softmax attention in U-ViT with Cog Attention (excluding the first and the last layers), resulting in our diffusion model, U-ViC. Both the U-ViC model and the U-ViT baseline are trained at a scale of 44 million parameters. The training is performed on the CIFAR-10 dataset (Krizhevsky, 2009) for unconditional image generation and the MS-COCO dataset (Lin et al., 2015) for text-conditioned image generation. For detailed information on the model architecture and training hyperparameters, please refer to Table 2 and Table 5 in (Bao et al., 2023).\nWe evaluate performance using the FID score (Heusel et al., 2017), where lower values indicate better performance. Our experiments used 8 A800 GPUs, whereas Bao et al. (2023) used 4 \u00d7 2080 or 4 x A100 GPUs. Furthermore, we did not employ xFormer for acceleration. These differences in environments may contribute to the differences between the results reported in (Bao et al., 2023) and our reproduced ones."}, {"title": "5. Discussions", "content": "We discuss some important characteristics of Cog Attention, which might help the future research."}, {"title": "5.1. Convergence of Cogformer", "content": "As briefly mentioned before, in a deep Cogformer, we preserve the softmax attention in both the first and last layers. This aims to maintain the same convergence rate as a vanilla Transformer.\nWhile we initially attempted to apply Cog Attention across all layers, we found that it resulted in slower convergence. Our observations of attention dynamics revealed that, at the beginning of training, the signs of attention weights do not accurately represent meaningful semantics due to the query-key inner product not being fully learned, leading to diverted optimization direction. The model appears to learn a shortcut at first, achieving lower training loss compared to the vanilla Transformer but then becoming stuck, allowing the vanilla Transformer to surpass it as training progresses. In contrast, softmax attention offers a near-uniform distribution that facilitates smoother initial training. Through empirical trials, we found that preserving softmax attention in the first layer of Cogformer effectively resolves the convergence issue.\nRegarding the preservation of softmax attention in the last layer, we observed that a Cogformer with softmax attention only in the first layer resulted in last-layer attention weights' sign highly consistent either entirely positive or entirely negative. This suggests that the last layer may not require the expressiveness of negative weights."}, {"title": "5.2. Cog Attention Produces More Diverse Attention Patterns and Less Sink", "content": "Our analysis reveals that Cog Attention generates more diverse attention patterns compared to softmax attention. To investigate this, we examined the attention patterns across"}, {"title": "5.3. Time Cost Overhead", "content": "We compare the average time cost per training step on a single A800-80G GPU for our trained Transformer language model and Cogformer, using inputs of varying lengths. The results are shown in Figure 8. Due to the additional absolute operation and an extra multiplication, Cogformer incurs extra time overhead compared to the Transformer. We recognize this as a current limitation of our method and are actively exploring strategies to enhance its efficiency."}, {"title": "6. Related Works", "content": "Numerous efforts modified softmax function in the attention mechanism mainly for efficiency purposes, yet these attempts have not removed the constraints for non-negative weights. Some studies have proposed softmax alternatives such as ReLU (Shen et al., 2023; Wortsman et al., 2023), sigmoid (Ramapuram et al., 2024), cosine-based distance re-weighting (Qin et al., 2022), and learnable activations (Liu et al., 2024).\nRecent studies showed the potential of an attention mechanism tailored to incorporate negative weights. Gong et al. (2024) demonstrate that removing the softmax function from the Mixture of Experts (MoE) router does not negatively impact performance. Ye et al. (2024) introduced differential attention, which calculates final attention weights by subtracting the outputs of two attention heads, thereby aiming to reduce noise. It might produce a small amount of negative attention weights as a by-product of the subtraction. It requires careful tuning of the scaling factor between the two heads and the inclusion of an additional normalization layer to ensure convergence. In this paper, we proposed Cog Attention, which introduces negative weights without requiring extra parameters or meticulous hyperparameter tuning. We demonstrated that Cog Attention increases the expressiveness of the attention mechanism."}, {"title": "7. Conclusions", "content": "In this paper, we introduced Cog Attention, a novel attention mechanism that incorporates negative weights. Our analysis, grounded in mechanistic interpretability, reveal that negative weights in attention computation enhance the expressiveness of neural networks and increase robustness against representational collapse. We subsequently implemented Cogformer, a new variant of the Transformer that integrates Cog Attention as its attention layer, to train language models and image generation diffusion models. The results indicate that Cog Attention can enhance Transformer performance across these tasks. We also discussed several properties of Cog Attention that could be beneficial for various applications, which we will explore in future works."}]}