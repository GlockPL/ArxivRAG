{"title": "SOFAR: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation", "authors": ["Zekun Qi", "Wenyao Zhang", "Yufei Ding", "Runpei Dong", "Xinqiang Yu", "Jingwen Li", "Lingyun Xu", "Baoyu Li", "Xialin He", "Guofan Fan", "Jiazhao Zhang", "Jiawei He", "Jiayuan Gu", "Xin Jin", "Kaisheng Ma", "Zhizheng Zhang", "He Wang", "Li Yi"], "abstract": "Spatial intelligence is a critical component of embodied AI, promoting robots to understand and interact with their environments. While recent advances have enhanced the ability of VLMs to perceive object locations and positional relationships, they still lack the capability to precisely understand object orientations a key requirement for tasks involving fine-grained manipulations. Addressing this limitation not only requires geometric reasoning but also an expressive and intuitive way to represent orientation. In this context, we propose that natural language offers a more flexible representation space than canonical frames, making it particularly suitable for instruction-following robotic systems. In this paper, we introduce the concept of semantic orientation, which defines object orientations using natural language in a reference-frame-free manner (e.g., the \u201cplug-in\u201d direction of a USB or the \u201chandle\u201d direction of a knife). To support this, we construct OrienText300K, a large-scale dataset of 3D models annotated with semantic orientations that link geometric understanding to functional semantics. By integrating semantic orientation into a VLM system, we enable robots to generate manipulation actions with both positional and orientational constraints. Extensive experiments in simulation and real world demonstrate that our approach significantly enhances robotic manipulation capabilities, e.g., 48.7% accuracy on Open6DOR and 74.9% accuracy on SIMPLER.", "sections": [{"title": "I. INTRODUCTION", "content": "Open-world spatial intelligence is crucial for embodied AI, as a robot must understand not only \"what\" an object is but also its precise \"where\" for effective interaction. To this end, vision-language-models (VLMs)  with spatial understanding  that comprehend spatial concepts  and relationships  have been built. These models incorporate spatial knowledge into their architecture design or training data, enabling them to perform tasks such as distinguishing left from right , counting objects , and even planning for position-only manipulations . Despite the remarkable achievements, we ask: What is the missing cornerstone of such spatial understanding? Given the original intent of \u201cseeing is for doing\u201d , how can we push spatial understanding further?\nWe observe that current VLMs struggle with understanding object orientation, making them insufficient for generic robot manipulation planning. Consider some everyday scenarios: inserting a pen into a pen holder, righting a tilted wine glass, or plugging a cord into a power strip. Previous approaches  primarily focused on understanding \"where is the pen\u201d or \u201cwhere is the wine glass\" while ignoring their orientations, making them insufficient for accomplishing these seemingly simple object manipulation tasks.\nMore importantly, different orientations of an object hold varying semantic significance. The capability of connecting specific orientations to their semantic meanings is essential for language-guided robot manipulations. For example, inserting a pen into a pen holder requires aligning the pen tip with the direction of the pen holder's opening; righting a wine glass necessitates aligning the glass's top with the z-axis in the world coordinate frame; and plugging into a power strip involves understanding the \u201cinsertion\" direction, which is perpendicular to the power strip's surface. However, translating a specific language description into a desired object orientation is challenging for existing VLMs.\nTo move forward, we introduce language-grounded orienta- tion that bridges spatial reasoning and object manipulation, characterized by the following:\n\u2022 From Position Awareness to Orientation Awareness. While prior works  emphasize position re- lationship, orientation understanding is equally critical for defining the full six degrees of freedom (6-DoF) of object or end-effector poses . Orientation awareness involves understanding object orien- tations and their relationships in the open world, enabling robots to complete tasks requiring precise alignment and rearrangement together with position awareness.\n\u2022 From Orientation to Semantic Orientation. Traditional orientation, defined relative to a base frame or template model , is insufficient for open- world manipulation guided by language instructions . We introduce semantic orientation, linking orientational vectors of an object to open-vocabulary prompts (e.g., the \"handle\" direction of a knife or \"plug-in\" direction of a USB). This bridges geometric reasoning with functional semantics, enabling robots to interpret task-specific orientation changes.\nAchieving such spatial awareness requires addressing two key challenges: acquiring semantic orientation knowledge in the open world and integrating it with VLMs. To tackle the first, we propose PointSO, a generalizable cross-modal 3D Transformer , which serves as a robust and versatile framework for open-world spatial orientation understanding. To train PointSO effectively, we construct OrienText300K, a large-scale orientation-text paired dataset curated from internet sources. This dataset, devoid of expensive robot data, is automatically labeled by prompting GPT-40  with extensive and diverse language-grounded semantic orien- tation queries. These queries encompass intra-object spatial understanding and inter-object interaction-related semantics, such as manipulation orientations. OrienText300K comprises over 350K 3D models of diverse everyday objects. Powered by OrienText300K, PointSO can reliably infer semantic orientation for an arbitrary object without being restricted to a known category or instance.\nWe further develop an integrated reasoning system, SOFAR, to coordinate our proposed PointSO and existing position foundation models for achieving more comprehensive spatial understanding, where Florence-2  and SAM  handle the object positions, while our PointSO focuses on understand- ing and outputting orientations complimentary. Specifically, we parse an input RGB-D observation as an orientation-aware 3D scene graph using SAM-segmented object point clouds and our PointSO. The RGB-D observation, together with the scene graph, is then input to the VLM, which outputs a chain-of- thought  spatial reasoning for both position and orientation commands. These commands can then serve as visual planning outcomes to support robotic manipulation tasks.\nTo assess our system, we introduce Open6DOR V2, a large- scale robot manipulation benchmark designed for 6-DoF object rearrangement in simulation. This benchmark demands robust positional and orientational reasoning in open-world settings and supports both open-loop and closed-loop robotic control. Our experiments demonstrate that our system considerably out- performs state-of-the-art vision-language models and popular vision-language-action (VLA) models\u2014even those trained with extensive and costly robot trajectories. These performance gains are also observed in real-world experiments. Additionally, we establish a new spatial visual-question-answering benchmark, that confirms the system's exceptional open-world spatial reasoning capabilities.\nIn summary, our contributions are fourfold. First, we introduce PointSO, an orientation base model that infers the semantic directions of novel objects in an open-world context. Second, we curate OrienText300K, a large-scale 3D model dataset annotated with semantic directions to support the training of orientation models. Third, we develop an integrated system that enhances powerful VLMs with advanced spatial understanding, facilitating robot manipulations that require both positional and orientational spatial knowledge."}, {"title": "II. SEMANTIC ORIENTATION: CONNECTING LANGUAGE AND OBJECT ORIENTATION", "content": "Traditionally, the orientation of an object is defined within a reference frame, using quaternions or Euler angles to represent relative rotations. Intuitively, object orientations commonly correspond to some specific semantics in most interactive behaviors. This aligns with the fact that humans typically understand an object's orientation in a more semantic, reference- free way. For instance, when plugging a plug into a charger, we accomplish the action of \u201cplugging in\u201d by matching the metal prongs' direction with the outward direction of the charger's socket. Drawing on this observation, we define an object's Semantic Orientation as follows. Given an object X and a description l, the corresponding semantic orientation \\(s_l^X \\in S(2)\\) is an object-centric direction represented as a unit vector semantically matching the description l.\n\\[s_l^X = F(X, l).  \\qquad(1)\\]\nl is an open-vocabulary language description that should have a clear semantic correspondence to a general orientation (e.g., front, top), an object part (e.g., handle, cap), or a specific manipulation goal (e.g., pour out, plug-in).\nFor an object X, it may have multiple semantic orientations corresponding to different functions or attributes via changing the language description l, forming a semantic orientation set \\(S_X = \\{s_1, s_2,...,s_n\\}\\). Based on this set, the rotation of X can be characterized by transforming its semantic orientations.\nSemantic orientations are powerful representations that help characterize various orientation-related knowledge. What relates"}, {"title": "III. SOFAR: SEMANTIC ORIENTATION BRIDGES SPATIAL REASONING AND OBJECT MANIPULATION", "content": "Our proposed PointSO model now paves the off-the-shelf for object-centric spatial orientation understanding. However, it is still unclear how to leverage such object-centric spatial understanding for scene-level spatial reasoning both in the digital world (e.g., orientation-aware visual question answering, VQA) and in the physical world (e.g., robot manipulations). To enable such applications, we build an integrated reasoning system where a powerful VLM acts as an agent and reasons about the scene while communicating with off-the-shelf models including PointSO and SAM [65]. Fig. 5 illustrates an overview of our proposed framework, aiming at Semantic Orientation For Autonomous Robotic manipulation (SOFAR). SOFAR con- sumes an RGB-D image and a language query as input and first leverages off-the-shelf models including SAM and PointSO to convert the image into an orientation-aware 3D scene graph. Then SOFAR leverages a VLM agent to produce planning outcomes based upon the scene graph and the input language query, which can be later used for robot manipulation. We will introduce the construction of the orientation-aware 3D scene graph in Section III-A and how to perform spatial-aware task reasoning and plan for robot manipulation in Section III-B.\nTo convert the input RGB-D image into an orientation-aware 3D scene graph, we first segment the RGB image to obtain object-level 3D point clouds using SAM  and then construct a scene graph with object-attribute nodes.\nGiven a language query Q, we first prompt a VLM model \\(F_{VLM}\\) to abstract the task-oriented object phrase set. Thus, a set \\(P = \\{p_i|i = 1,2,..., M\\}\\) with M object phrases in language will be generated from Q. With set P, we use language-conditioned object segmentation with SAM to obtain an object set \\(X = \\{X_i|i = 1,2,...,M\\}\\), where \\(X_i\\) is the 3D point cloud of the i-th object. Besides, we assign individual IDs to objects which are used for Set-of-Mark (SoM) prompting  on VLM's image input. Next, we prompt the VLM to generate every object's corresponding task-oriented language description set \\(L_i\\). We predict the semantic orientation using pretrained PointSO for each description in the description set \\(L_i\\), forming the semantic orientation set \\(S_i\\) for the i-th object."}, {"title": "IV. EXPERIMENTS", "content": "We propose two benchmarks to demonstrate the effectiveness of our SOFAR in spatial reasoning and robotic manipulation.\n1) Open6DOR V2: For simulation experiments, we choose the Open6DOR[28] Benchmark to comprehensively evaluate our spatial understanding abilities. Beyond the perception tasks that it originally proposes, we further construct an execution track to enable comparison with close-loop policies. We name the new combined benchmark Open6DOR V2.\n\u2022 Perception tasks. In line with Open6DOR's definition, the model takes an RGB-D scene image along with a language instruction as input and directly outputs the translation and orientation of the target object.\n\u2022 Execution tasks. We replicate Open6DOR scenes and ground them into a robosuite simulation environment for execution, excluding single-object scenes to better assess the understanding of spatial relationships. The model takes the RGB-D image with a language instruction as input and completes the entire execution process. We build it based on robosuite  and adopt the format established by LIBERO [76]. Evaluation is conducted according to the final position and orientation of the target object.\n2) 6-DoF SpatialBench: To further evaluate spatial under-"}, {"title": "V. RELATED WORKS", "content": "Vision-Language Models(VLMs) are rapidly being developed in research community, driven by the storming lead in extending GPT-style  Large Language Models (LLMs) like LLaMA  to VLMs . SpatialVLM  pioneers this direction by constructing VQA data in spatial understanding from RGB-D, which is used for training an RGB-only VLM. Following SpatialVLM, SpatialRGPT  extends RGB-based spatial understanding to RGB-D by constructing spatial understanding data using 3D scene graphs. SpatialBot  explores RGB-D spatial reasoning through hierarchical depth-based reasoning. Some other works propose visual prompting for improving GPT-4V's spatial understanding . Meanwhile, another line of works explores VLMs using 3D representations such as point clouds for 3D scene  and object- centric  understanding. Despite the remarkable progress, these works are limited in 3-DoF understanding which is not actionable. In contrast, we explore spatial understanding in 6-DoFs from RGB-D via VLMs. Unlike vanilla 3D scene graphs used by SpatialRGPT for data construction, we propose orientation-aware 3D scene graphs realized by our proposed PointSO. In addition, we formulate spatial understanding as graph learning, where the scene graph nodes are directly input during inference.\nLanguage-Grounded Robot Manipulation adopts the human language as a general instruction interface. Existing works can be categorized into two groups: i) End-to-end models like RT-series  built upon unified cross-modal Transformers with tokenized actions , large vision- language-action (VLA) models built from VLMs , or 3D representations . Training on robot data such as Open X-Embodiment  and DROID , a remarkable process has been made. However, the data scale is still limited compared to in-the-wild data for training VLMs. ii) Decoupled high-level reasoning and low-level actions in large VLMs and small off-the-shelf policy models, primitives , or articulated priors . Our SOFAR lies in this group, where an open-world generaliza- tion property emerges from VLMs and our proposed PointSO empowered by orientation-aware spatial understanding."}, {"title": "VI. LIMITATIONS", "content": "One notable limitation for decoupled systems like SOFAR is that the execution may fail due to a sub-module error, i.e., robots may place target objects with an error transformation because of unstable grasping or inaccurate visual perception. For example, the pen will be placed in an unexpected pose due to the rotation during execution. Future works include integrating scalable data and more advanced models and exploring the potential of combining end-to-end and such decoupled methods, and expanding SOFAR to more applications."}]}