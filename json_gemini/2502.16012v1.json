{"title": "Cross-Model Transferability of Adversarial Patches in Real-time Segmentation for Autonomous Driving", "authors": ["Prashant Shekhar", "Bidur Devkota", "Dumindu Samaraweera", "Laxima Niure Kandel", "Manoj Babu"], "abstract": "Adversarial attacks pose a significant threat to deep learning models, particularly in safety-critical applications like healthcare and autonomous driving. Recently, patch based attacks have demonstrated effectiveness in real-time inference scenarios owing to their 'drag and drop' nature. Following this idea for Semantic Segmentation (SS), here we propose a novel Expectation Over Transformation (EOT) based adversarial patch attack that is more realistic for autonomous vehicles. To effectively train this attack we also propose a 'simplified' loss function that is easy to analyze and implement. Using this attack as our basis, we investigate whether adversarial patches once optimized on a specific SS model, can fool other models/architectures. We conduct a comprehensive cross-model transferability analysis of adversarial patches trained on state-of-the-art (SOTA) Convolutional Neural Network (CNN) models such PIDNet-S, PIDNet-M and PIDNet-L, among others. Additionally, we also include the Segformer model to study transferability to Vision Transformers (ViTs). All of our analysis is conducted on the widely used Cityscapes dataset. Our study reveals key insights into how model architectures (CNN vs CNN or CNN vs. Transformer-based) influence attack susceptibility. In particular, we conclude that although the transferability (effectiveness) of attacks on unseen images of any dimension is really high, the attacks trained against one particular model are minimally effective on other models. And this was found to be true for both ViT and CNN based models. Additionally our results also indicate that for CNN-based models, the repercussions of patch attacks are local, unlike ViTs. Per-class analysis reveals that simple-classes like 'sky' suffer less misclassification than others. The code for the project is available at: https://github.com/p-shekhar/adversarial-patch-transferability", "sections": [{"title": "I. INTRODUCTION", "content": "Deep learning has transformed computer vision, enabling significant advancements in tasks such as image classification, object detection, and semantic segmentation [1], [2]. Among these, real-time semantic segmentation plays a crucial role in various applications, including autonomous driving [3], robotics [4], medical imaging [5], and surveillance [6]. The ability to accurately label each pixel in an image allows au-tonomous systems to understand their surroundings, make in-formed decisions, and operate safely in dynamic environments. However, as these systems become increasingly reliant on deep learning models, their vulnerability to adversarial attacks has emerged as a critical security concern [7]\u2013[9]. Adversarial attacks involve carefully crafted perturbations designed to deceive neural networks into making incorrect predictions. While traditional pixel-wise perturbation-based attacks have been extensively studied [10], [11], they often require noise spread across the entire image, making them difficult to apply in real-world scenarios. In this regard, attacks such as Fast-Gradient Sign Method (FGSM) [12] add imperceptible noise to the entire image learnt using a particular model, thereby forcing the model to make prediction errors on that image. However in autonomous driving scenarios, such carefully crafted, image-specific noise filters do not generalize to other images captured at different angles and lightening, rendering them in-effective. In contrast, patch-based adversarial attacks introduce highly visible yet localized perturbations, which can be physically printed and placed within a scene. These adversarial patches have been shown to effectively fool clas-sification and detection models, raising concerns about their impact on semantic segmentation models [13], particularly in safety-critical applications. As a demonstration, here Fig. 1(a) presents the application of an adversarial patch trained on images from the Cityscapes dataset [14] using the PIDNet-L [3] SS model for evaluation. More details about the patch training are provided in the following sections. To notice the effectiveness of the patch attack, readers are advised to look at the segmentation mask predicted from the attacked image (shown in Fig. 1(d)). Here, based on the legends we can see the patch is forcing the pretrained model (that inherently predict well as shown in Fig. 1(c)) to predict incorrectly on pixels that are not even covered by the patch. In-fact such attacks are particularly effective since the model is forced to predict classes like 'fence' or 'sidewalk' in the middle of the road leading to events such as emergency braking or abrupt turning, causing accidents."}, {"title": "A. Adversarial Patch Attacks in Segmentation Models", "content": "Typically, adversarial patch attacks in classification models focus on changing the output label of an image, hence the attack is responsible for just changing a scalar prediction [15]. However for segmentation models where the requirement is spatially consistent predictions across an entire scene, pro-ducing effective and efficient adversarial attacks is difficult. The reasoning behind this is the requirement for simultane-ously fooling the model on multiple pixels across the scene. This makes adversarial attacks on segmentation models more challenging but also more impactful in real-world applications [16]. For instance, an attack could manipulate an autonomous vehicle's perception system by making pedestrians or road signs disappear, or as shown in Fig 1 can lead to appearance of a fence on the road, leading to catastrophic consequences. Adversarial patches designed for segmentation models can disrupt the spatial coherence of segmentation masks, causing severe misclassification of critical regions [17].\nOne of the key open questions in adversarial robustness is the transferability of adversarial patches across models. Transferability refers to the ability of an adversarial patch trained on one model to successfully deceive other models without additional optimization. If adversarial patches are highly transferable, an attacker could design a single patch that simultaneously degrades the performance of multiple seg-mentation architectures, reducing the effectiveness of model diversity as a defense strategy. Understanding how different architectures react to adversarial patches is crucial for design-ing more resilient segmentation models and robust defense mechanisms."}, {"title": "B. Real-Time Semantic Segmentation Models and Their Vul-nerabilities", "content": "Real-time semantic segmentation models are designed to balance accuracy and computational efficiency, enabling their deployment in resource-constrained environments such as em-bedded vision systems and autonomous vehicles. Modern SS architectures can be broadly categorized into two types:\nCNN-based models: These include architectures such as PIDNet [3], BiSeNet [18], [19], and ICNet [20], which leverage convolutional layers for hierarchical feature ex-traction. CNN-based models are highly optimized for efficiency, making them well-suited for real-time appli-cations.\nTransformer-based models: Recent advances in Vision Transformers (ViTs) [21] have introduced architectures such as SegFormer [22], which utilize self-attention mechanisms to capture long-range dependencies. Trans-formers have demonstrated superior performance in many computer vision tasks, but their robustness to adversarial attacks remains an area of active research.\nDespite their efficiency and high performance, real-time segmentation models remain susceptible to adversarial pertur-bations. CNN-based models, while optimized for speed, often rely on spatially localized feature extraction, making them po-tentially vulnerable to small, well-placed adversarial patches. On the other hand, Transformer-based models process global context and may exhibit different adversarial characteristics, either being more robust due to self-attention mechanisms or more vulnerable to specific structured attacks.\nThis study investigates the cross-model transferability of adversarial patches across a diverse set of real-time segmen-tation architectures. In this research we aim to answer the following key research questions (i): Can an adversarial patch trained on one segmentation model successfully fool other models ? (ii): Are CNN-based models and Transformer-based models equally susceptible to adversarial patches ? (iii): Which semantic classes (e.g., roads, pedestrians, vehicles) are most affected by black-box adversarial perturbations ? (iv): How does model architecture influence adversarial robustness and transferability?"}, {"title": "C. Contributions of This Work", "content": "To address these questions, we conduct a systematic eval-uation of adversarial patch attacks on multiple semantic segmentation models, covering both CNN-based architec-tures (PIDNet-S, PIDNet-M, PIDNet-L, ICNet, BiSeNetV1, BiSeNetV2) and a ViT-based architecture (SegFormer). Here all CNN based architectures are strictly real-time SS Models. However Segformer on the other does not belong to this class. Still we have included Segformer in this research to evaluate the CNN vs ViT attack scenarios. Overall, our contributions are as follows:"}, {"title": "1) Novel Expectation Over Transformation (EOT) based attack formulation:", "content": "We propose a variant of the original EOT formulation [23] for learning blackbox adversarial patch attacks, thus making it more realistic for a Real-time SS model in autonomous driving scenarios."}, {"title": "2) Adaptive adversarial training loss:", "content": "For learning the patch itself, we proposed an adaptive loss function that simplifies the one introduced in [13]. In particular, we reduce the number of hyper-parameters in the loss metric to make the attack more robust."}, {"title": "3) Comprehensive Transferability Study:", "content": "We analyze how well adversarial patches transfer across different seg-mentation models, identifying key architectural weak-nesses. Additionally, by comparing CNN-based and ViT-based models, we provide insights into their relative robustness against patch-based attacks. To add to this, we also evaluate the per-class performance degradation to determine which object categories (e.g., pedestrians, vehicles, buildings) are most affected by adversarial patches. To the best of our knowledge, this is the first research to compare the performance of ViTs and CNN based SS models against patch based adversarial attacks at this level of detail in autonomous driving domain.\nOur findings (sections III-B and III-C) highlight the security risks posed by adversarial patches in real-time segmentation models and emphasize the need for robust defenses against such attacks. The insights gained from this study will help guide future research in adversarial robustness, defense mech-anisms, and secure deployment of deep learning models in real-world applications."}, {"title": "II. ATTACK MODEL", "content": "As mentioned in the previous section, here we build on the original formulation of EOT based attacks proposed in [23]. For classification problems, such attacks can be formulated as:\n$x' = arg max_{x'} E_{t\\sim T} [log P(y_t|t(x'))]$ subject to:\n$E_{t\\sim T} [d(t(x'), t(x))] < \\epsilon, \\;\\;\\; x, x' \\in [0,1]^d$\nwhere:\nx is the image under consideration having d pixels\nx' = x + \\delta is the adversarial image produced after optimizing \u03b4.\nT is a distribution over transformations of interest.\nt(x') = t(x + \u03b4) is a transformation function (t ~ T) applied to the adversarial example.\nP(yt| t(x')) denotes the probability of the target class yt given the transformed adversarial input.\nd(\u00b7,\u00b7) is a distance metric quantifying the discrepancy between original and attacked transformed image.\nThe optimization process in (1) ensures that the adversarial perturbation is robust to a set of transformations (for example t ~ T), making it effective in real-world conditions. In other words, when the image x is optimally attacked to form x',\nthen irrespective of the transformations applied over it (from a fixed family of transformations), on average the attacked model will predict the wrong class yt instead of the true class label y for x. In this research however, we move away from the targeted setting and focus on untargeted attacks in SS, where the objective is to force the model to predict incorrectly without having a target class of interest. To implement this, we minimize the probability of the true class associated with a pixel of the image. But we do this under the EOT framework. Assuming (x, y) ~ Pxy(X, Y), where x is an image and y is its corresponding segmentation mask. Additionally, we assume a joint data distribution Pxy. Using these notations, we solve the following optimization problem:\n$\\delta^* = arg min E_{(x,y)\\sim P_{xy}} E_{t\\sim T} [log P(y|t(x'))]$ subject to:\n$E_{t\\sim T} [d(t(x'), t(x))] < \\epsilon, \\;\\;\\; x \\in [0,1]^d$\n$t(x') = t(x) \\circ I + P\\delta$\nIt should be noted that (2) is a minimization problem using the true label y, as opposed to the maximization problem involving target classification label yt in (1). One additional key difference is that in formulation (2) we assume x'= x \u25e6 I + P\u03b4, where \u25e6 is an elementwise multiplication operator and x\u25e6I zeros out the pixel values in the location where an adversarial patch is to be added. Also, \u03b4 is the patch of adversarial pixels, and the operator P pads 0's to this patch to match the dimensions of x. Furthermore, during model training we incorporate transformations as: t(x') = t(x) \u25e6 I + P\u03b4. Hence computationally, we apply transformation on the clean image and then paste the patch on it to create a transformed adversarial example. This is different than t(x') = t(x + \u03b4) in (1) where transformation was applied on an already attacked image. t(x') = t(x) \u25e6 I + P\u03b4 is more suitable for training a generalizable patch attack since in real driving scenarios the images captured by the car camera would be at different angles, distorted, and in different lighting etc. But the attack should be independent of those transformations and should work in its standard form.\nTo understand the adversarial patch learning process con-sider a pretrained SS model m. For an image pixel xi if the model correctly predicts the class, then we will have yi = m(xi). Additionally let \u03a9 be the set of correctly classified pixels whereas \u03a9 is the set of pixels incorrectly classified. Hence the set of all pixels of an image x would be denoted by the set: \u039b = \u03a9 \u222a \u03a9 \u222a \u03b4. Assuming these notations, for training an adversarial patch (using a batch of n images) we exclusively focus on the cross-entropy loss of all pixels that are correctly classified, i.e.\n$L = (1/n) \\sum_{x} L_{x}, \\;\\;\\; where L_{x} = -(1/|\\Omega_x|) \\sum_{i \\in \\Omega_x} y_{i}log(m(x_{i}))$\nThis is a simplification of the previous works such as [13] that considered both the losses of correctly and incorrectly"}, {"title": null, "content": "classified pixels and weighted them according to a hyper-parameter \u03b3. However to keep the loss function simple, in this research we only use loss of correctly classified pixels (\u03a9) to drive the optimization of patch \u03b4. During optimization we expect as the number of correctly classified pixels reduce, the total loss L wouldn't go up considerably as (3) is the mean quantity over correctly classified pixels. However, still the Mean Intersection over Union (MIoU) on pixels in the set \u03a9 \u222a \u03a9 would reduce. This is achieved by the following gradient ascent step over the pixels in the patch (\u03b4):\n$\\delta \\leftarrow \\delta + \\epsilon \\cdot sign(\\nabla_{\\delta}L)$\nMotivated from the FGSM algorithm [12], here we use the sign function to ensure uniformity in gradient updates. To keep the attack realistic, we clip the \u03b4 pixels that go out of the range [0,1]. Additionally, it should be noted that algorithm (4) coupled with pixel clipping ensures $t(x') = t(x) \\circ I + P_{\\delta}$ satisfies the requirement $E_{t\\sim T} [d(t(x'), t(x))] < \\epsilon$."}, {"title": "III. EXPERIMENTAL RESULTS", "content": "In this section we present the results from our proposed approach, analyzing the robustness and transferability of the proposed patch attack."}, {"title": "A. Patch training setup", "content": "We begin by obtaining the SS models of interest (PIDNet-S,-M,-L, ICNet, BiSeNetV1, BiSeNetV2 and Segformer) from [24]-[27]. Among these models PIDNet-S, M, L, BiSeNetV1, V2 and Segformer are already pre-trained on Cityscapes dataset. For ICNet, pretraining on Cityscapes is performed using the training scripts from PIDNet repository [24]. In order to keep things simple we assume all patches are to be placed at the center of the images. During patch training we work with Cityscapes-train dataset that has 2,975 images with 1024\u00d72048 resolution. Regarding transformations t ~ T, we perform scaling of the images in the range [0.5,2], and then crop it to 1024 \u00d7 1024. Additionally, we also flip the image horizontally to simulate different views. During training, after each image (in a batch) goes through these transformations we implement t(x') = t(x) \u25e6 I + P\u03b4 by pasting the patch at the center. Using the output of SS model, we compute the loss using (3) and use the gradient ascent step (4) with \u03f5 = 0.005 to optimize \u03b4. For all CNN based model we use a batch size of 6 and 30 epochs for training the patch. For Segformer, to fit the model in the memory we do a batch size of 1 and just 15 epochs (as there are more frequent gradient updates). The stabilization of Intersection over Union (IoU) decay curves in Fig. 2 clearly demonstrates negligible benefit of doing additional epochs of patch training. At the end of training we obtain 7 patches (corresponding to 7 models), each with a resolution of 200 \u00d7 200 (patch size is prefixed to 200). Throughout this study we use MIoU [28] as a metric to quantify segmentation performance."}, {"title": "B. Segmentation performance decay during patch training", "content": "Since there are only 6 real-time SS models under consider-ation (excluding Segformer), we study the performance decay of these models as the adversarial patches are optimized over epochs in Fig. 2(a). Depending on the structure of each model, every curve presents a unique decay structure. It is interesting to note that MIoU decay curve of even models with same architecture (PIDNet family) can be widely different based on model complexity. Around epoch 15, PIDNet-L shows accelerated decay in performance presenting an interesting behavior to study further. Additionally, BiSeNetV2 shows a rapid decay in performance demonstrating its susceptibility to adversarial attacks of this nature. In Fig 2(b,c,d), we present the class wise decay in performance of PIDNet class of models (-L,-M,-S), quantified using class specific Intersection over Union (IoU). A few interesting things to note here: (i) The performance on 'sky' pixels practically remains unaffected. This might have to do with simplicity of the class. Model -L however does achieve degradation in performance on 'sky' class after epoch 17. (ii) The classes 'road' or 'sidewalk' shows significant degradation for both -L and -S model. However for -M, its not as drastic. This possibly demonstrates the dependence on model structure. (iii) For -L, class 'road' even shows a little improvement after epoch 15. However since most other classes show degradation, overall performance goes down (as seen in panel (a)). (iv) Besides \u2018sky\u2019, 'vegetation\u2019 also shows very limited decay in performance for all 3 models, possibly due to its unique spatial structure."}, {"title": "C. Patch attack transferability and generalization", "content": "Table I and Fig. 3 analyze the capability of the adversarial patches to generalize. In Table I, rows correspond to patch attack trained with respect to a specific model (shown in col 1). Additionally row 0 in-particular shows the case where Cityscapes-val images are attacked by a random noise patch. This random attack provides us the baseline performance for comparing actual adversarial attacks. Columns show the performance of each of these models under each of the attacks. Here we study 3 types of transferability:\nTransferability across models and architectures (CNNs and ViTs): From Table I we conclude, if a patch is trained with respect to a particular model it is practically only useful for that model. For example, a patch trained on PIDNet-L (row 1), only degrades the performance of PIDNet-L (0.8996 to 0.7914) but the performance of the rest of the models practically remain unaffected (comparing random patch row to PIDNet-L patch row). The same is true for patches trained across all models.\nFor better explanation, in Fig. 3 we also show the bar plots of drop in MIOU for all 7 models when individual patch attacks are applied to the Cityscapes-val images. Here green bars show positive reduction in MIoU and tiny red bars (visible in some cases) shows negative reduction (increment) in MIOU. Hence for example, if a patch is trained on Segformer (last subplot in Fig. 3), only Segformer sees considerable drop in performance and rest of the models are practically unaffected. Additionally, atleast based on the attack discussed, the attacks don't translate well between CNNs and ViTs.\nTransferability across unseen images: This is demon-strated by training patches on Cityscapes-train and eval-uating on Cityscapes-val. Thus from Table I we can conclude that even on unseen images, the intra-model attacks are effective.\nTransferability across image sizes: Since the attacks were trained using a crop size of 1024 \u00d7 1024, but successfully implemented on 1024 \u00d7 2048 Cityscapes-Val dataset in Table I, the attacks can be considered effective irrespective of the image size.\nTo provide the readers with more visual intuition of the effect of adversarial patches, Fig. 4 shows the performance of 4 models across patches trained with respect to these models. Here the comparison involves CNNs vs CNNs (PIDNet-L vs ICNet vs BiSeNetV1) and CNNs vs ViTs (all CNN models against Segformer). As seen in column 1, with a PIDNet-L patch, only PIDNet-L's prediction is being affected. Again like Fig. 1, there is a big patch of sidewalk being predicted in the middle of the road. Similarly for ICNet patch (col 2), the effect is most prominent on ICNet prediction (row 3 col 2). The other two models and patches show similar behavior. One very interesting aspect of CNNs vs ViTs we notice here is that CNN based patch attacks mostly influence the prediction in the neighborhood of the patch as seen in subplots (2,1), (3,2) and (4,3). However ViTs due to their token and self-attention structure, propagate the effect to far-off regions in the image. For example in subplot (5,4), we can see the magenta 'building' class pixels basically taking over the 'sky'/'vegetation' pixels on the left and top."}, {"title": "IV. DISCUSSION AND CONCLUSION", "content": "The current research provided a comprehensive overview of the effectiveness of patch based adversarial attacks on real-time semantic segmentation systems from the transferability point of view. For this, we firstly introduced a EOT based formulation of a black-box SS attack that simulates real-time attacks more closely as compared to previous studies [23]. Following this, we considered an experiment suite of 7 different SS models (6 CNNs-based and 1 ViT-based) and studied the Intra-model and Inter-model attack scenarios in a variety of settings. Based on the results obtained, we conclude that patches trained on a particular model don't translate well to other models. In-fact, the model being attacked might stay totally unaffected if the attack was not particularly trained for it. However, due to EOT, the attacks do translate well to unseen images in different cities, under different lighting and angles. Moving forward, our next immediate goal is to make these patches more robust with respect to models, so that they are simultaneously effective across multiple architectures and thus can be used in development of more robust and secure segmentation piplelines in autonomous vehicles."}]}