{"title": "VISION HGNN: AN ELECTRON-MICROGRAPH IS WORTH HYPERGRAPH OF HYPERNODES", "authors": ["Sakhinana Sagar Srinivas", "Sreeja Gangasani", "Rajat Kumar Sarkar", "Venkataramana Runkana"], "abstract": "Material characterization using electron micrographs is a crucial but challenging task with applications in various fields, such as semiconductors, quantum materials, batteries, etc. The challenges in categorizing electron micrographs include but are not limited to the complexity of patterns, high level of detail, and imbalanced data distribution(long-tail distribution). Existing methods have difficulty in modeling the complex relational structure in electron micrographs, hindering their ability to effectively capture the complex relationships between different spatial regions of micrographs. We propose a hypergraph neural network(HgNN) backbone architecture, a conceptually alternative approach, to better model the complex relationships in electron micrographs and improve material characterization accuracy. By utilizing cost-effective GPU hardware, our proposed framework outperforms popular baselines. The results of the ablation studies demonstrate that the proposed framework is effective in achieving state-of-the-art performance on benchmark datasets and efficient in terms of computational and memory requirements for handling large-scale electron micrograph-based datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Accurate development, characterization, and testing of miniaturized semiconductor devices are essential in leading-edge chip design to ensure their proper functioning and performance. State-of-the-art imaging and analysis techniques(Holt & Joy (2013)) play a critical role in the fabrication, inspection, and testing of the next-generation miniaturized semiconductor devices, such as those with a feature size of 7nm or smaller, as they help ensure their quality and reliability. The advanced imaging of miniature devices in the semiconductor industry typically utilizes a broad spectrum of electron beam tools, including Scanning Electron Microscopy(SEM), Transmission Electron Microscopy(TEM), Reflective Electron Microscopy(REM), and others. Electron microscopes, as ultra-modern imaging tools, produce high-magnification and high-resolution images of material specimens, known as electron micrographs, to perform microstructural characterization or identification of materials, which is essential for the accurate design fabrication, and evaluation of miniaturized semiconductor devices. However, classifying electron micrographs is challenging due to high intra-class variance, low inter-class dissimilarity, and multiple spatial scales of visual patterns. Figure 1 illustrates the various challenges in the automatic nanomaterial identification task. The de facto standard neural-network architectures for vision tasks such as ConvNets(Iandola et al. (2016), He et al. (2016)), Vision transformers(ViTs, Dosovitskiy et al. (2020), Liu et al. (2021), d'Ascoli et al. (2021), Chen et al. (2021b)), hybrid architectures(Wu et al. (2021), Graham et al. (2021), Wang et al. (2022)) and MLP-based vision models(Tolstikhin et al. (2021), Touvron et al. (2021a)) do not explicitly model the higher-order dependencies between the multiple grid-like patches(also referred to as tokens) of the electron micrographs. Nevertheless, this work aims to explore an alternative effective, efficient neural-network architecture beyond traditional methods for modeling the fine-grained interrelations among the spatially and semantically dependent regions(patches) of the electron micrographs for automatic nanomaterial identification tasks via the hypergraph framework. We utilize the hypergraphs as a mathematical model(Ouvrard (2020); Feng et al. (2019); Gao et al. (2022); Yadati et al. (2019)) for a structured representation of the electron micrographs to learn the hierarchical relations among the spatial regions(patches) unconstrained by their spatial location in the micrograph. We begin"}, {"title": "2 PROBLEM STATEMENT", "content": "Consider a dataset consisting of visual hypergraph-label pairs (Gi, Yi) where the ground-truth label of Gi is denoted by yi. The objective of the classification task is to learn a novel mapping neural network function f: Gi \u2192 yi that maps the discrete visual hypergraphs to the set of predefined categories."}, {"title": "3 PROPOSED APPROACH", "content": "As illustrated in Figure 3, our framework consists of the following modules. (a) hypergraph structure learning module, for brevity, HgSL backbone, to learn the discrete visual hypergraph representations of the electron micrographs through pairwise proximity function. (b) a local and global neighborhood connectivity-driven hypergraph attention network, for brevity, the HgAT backbone is designed to capture short, and moderate-range dependencies, i.e., encapsulates the hypergraph's structural and feature information in the hypernode-level embeddings. (c) a self-attention mechanism-based-hypergraph transformer network, for brevity, an HgT backbone with no hypergraph spatial priors to learn all pairwise hypernode interactions for better learning of the long-range pairwise dependencies. (d) the hypergraph read-out module, for brevity, the HgRo backbone, performs the global average pooling that collapses hypernode-level embeddings to obtain the single hypergraph-level embedding. (e) a linear projection layer and a normalized exponential function transform the hypergraph-level embedding to a multinomial probability distribution over the predefined electron micrograph categories to predict the visual hypergraph category."}, {"title": "3.1 HYPERGRAPH STRUCTURE LEARNING(HGSL)", "content": "The HgSL operates in two phases. At first, it performs tokenization of electron micrographs. Next, it optimizes the discrete visual hypergraph structure through a differentiable approach to learn the more robust and optimal representation through the nearest neighbor search technique and formulate the posterior classification task as message-passing schemes with hypergraph neural networks."}, {"title": "3.1.1 ELECTRON MICROGRAPH TOKENIZER", "content": "We split an electron micrograph with the size h \u00d7 w \u00d7 c, where (h, w) is the resolution of the RGB image, and c is the number of channels into non-overlapping n uniform patches, where the size of each patch is p \u00d7 p \u00d7 c and p is patch size. We reshape the patches to obtain feature matrix X' \u2208 Rnxp2c. We linearly transform the feature matrix, X', through a trainable embedding layer E to compute a refined feature matrix X \u2208 Rn\u00d7d as described below,\nX = XE; E \u2208 Rp2cxd                                                                    (1)\nThe row i of the feature matrix X = [x; . . . ; x2] represents the (low-dimensional) feature representa-tion for patch x \u2208 Rd, i = 1, 2, . . ., n, where d is the predefined feature dimension."}, {"title": "3.1.2 HYPERGRAPH REPRESENTATION", "content": "We represent the patches as the unordered hypernodes of an undirected visual hypergraph denoted as V = {V1, V2, ..., Un}. For each hypernode vi, we form an undirected hyperedge ep from the hypernode vi to vj; if vj is among the top-K visual-semantic-nearest neighbors of vi. Thus we obtain n hyperedges incident with K + 1 non-repeating hypernodes. The hyperedges describe the relations and capture more complex relationships and interdependencies among hypernodes in the visual hypergraphs. We then obtain a hypergraph G = (V, E, X) where E = {1, 2, ..., en} denotes the set of hyperedges and X \u2208 Rn\u00d7d is the hypernode feature matrix. Each row i in X represents the hypernode feature vector, xv\u2081 \u2208 Rd. Note: Xv\u2081 is the patch feature representation, x. The incidence matrix, \u0397 \u2208 Rn\u00d7n, describes the hypergraph structure. Hi, p = 1 if the hyperedge p incident with hypernode i and otherwise 0. The hyperparameter K < n determines the sparsity of the visual hypergraph. Let Np,i = {vi|Hi,p = 1} represent the subset of hypernodes vi incident with any hyperedge p. The intra-edge neighborhood of the hypernode i is given by Np,i\\i. It is a localized group of perceptually similar patches and captures higher-order relationships. The inter-edge neighborhood of hypernode i, Ni,p = {ep|Hi,p = 1}, spans the spectrum of the set of hyperedges ep incident with hypernode i. There is no natural ordering of the hypernodes in the hypergraph. To preserve patch-locality information in the main hypergraph, we linearly add the trainable position embeddings(Epos) to the hypernode feature vectors to enable position awareness. The HgSL module computes the hypernode's positional embeddings based on the intra- and inter-edge neighborhood in the hypergraph. Equation 2 shows the transformed feature vector of the hypernodes.\n[\u03a7\u03c5\u03b9;...; \u03a7\u03c5\u03c0] = [X01,..., Xun] + Epos; Epos \u2208 Rnxd                                                     (2)"}, {"title": "3.2 MODEL ARCHITECTURE", "content": "Figure 4 depicts the Vision-HgNN framework. The Hypergraph attention network(HgAT) performs the message-passing schemes on the visual hypergraphs to obtain (low-dimensional) hypernode embeddings. The Hypergraph transformer(HgT) utilizes the self-attention mechanism for transforming the hypernode embeddings determined by the HgAT operator to compute refined hypernode embeddings(zHgT , z\u016bgt). We discussed the HgAT and HgT operators in the appendix. The HgRo module performs the average pooling of hypernode embeddings(zHg,. .., zug) to obtain hypergraph-level embedding zLHgt \u2208 Rd. We apply a linear projection and softmax to transform zLHgt for determining the model predictions y = softmax(WoutzLHgr), where Wout \u2208 Rd\u00d7d."}, {"title": "3.3 ALGORITHMIC ARCHITECTURE", "content": null}, {"title": "4 EXPERIMENTS AND RESULTS", "content": null}, {"title": "4.1 DATASETS", "content": "We conduct experiments on the SEM dataset(Aversa et al. (2018)) for automatic nanomaterials identification. The human-annotated dataset contains a set of 10 categories belonging to a wide range of nanomaterials spanning a broad range of particles, nanowires, patterned surfaces, etc., for a total of \u224821,283 electron micrographs. The initial experimental results are reported by Modarres et al. (2017) on the subset of the complete dataset. Due to the unavailability of the subset dataset publicly, we conducted experiments on the original dataset(Aversa et al. (2018)), which contains 12% more samples. The dataset curators(Aversa et al. (2018)) had not shared the predefined train/validation/test splits, so we leveraged the k-fold cross-validation technique to evaluate our model performance for competitive benchmarking with the varied baseline models. Additionally, we utilized several open-source material benchmark datasets to demonstrate the effectiveness of our proposed method."}, {"title": "4.2 BENCHMARKING ALGORITHMS", "content": "We train the Vision-HgNN framework through a supervised learning approach for joint visual hypergraph inference and category prediction of micrographs. Table 2 provides a performance comparison of the Vision-HgNN framework with other baseline models, which include ConvNets, GNNs(Rozemberczki et al. (2021); Fey & Lenssen (2019)), and ViTs(al. (2022b;a)) architectures. In addition, we utilize the different self-supervised learning algorithms: Vision Contrastive Learning(VCL, et al. (2020))) and Graph Contrastive Learning(GCL, Zhu et al. (2021))) for comparison with our proposed method. We ensure a fair and rigorous comparison between the Vision-HgNN framework and the baseline algorithms by generating the results under identical experimental settings. The evaluation metric is the Top-N accuracy, where N \u2208 {1,2,3, 5}. The standard deviation values are less than at most 4% of the mean value. The proposed method demonstrates the best"}, {"title": "5 CONCLUSION", "content": "The challenge associated with the design of chips smaller than 7 nanometers is the increased complexity of the manufacturing process. As feature sizes shrink, the tolerance for errors in the manufacturing process decreases, making it more difficult to produce high-quality chips with consistent performance. In addition, the smaller feature sizes can lead to increased variability in the performance of the transistors, which can negatively impact the overall performance and reliability of the chip. Indeed, state-of-the-art imaging and analysis techniques are crucial in the development of next-generation semiconductor devices with feature sizes of 7nm or smaller. These techniques play a prominent role in the fabrication, inspection, and testing processes and are essential for driving the development of advanced microelectronics technologies. The high-resolution imaging of the device structures and materials allows for the identification of potential defects and deviations from design specifications, which can then be addressed through process optimization or design modification. In addition, these challenges present significant opportunities for innovation and the development of automatic material characterization methods for electron micrographs, which is essential for ensuring the quality and reliability of semiconductor devices. We conduct the first comprehensive study of the hypergraph-neural networks for electron micrograph classification tasks to improve the accuracy and efficiency of material characterization in various applications. We learn the optimal visual hypergraph structure to capture complex relationships and interactions between different spatial regions in an electron micrograph, allowing for a more robust and accurate representation of the micrograph. Hypergraph Neural Networks(HgNNs) operate on visual hypergraphs, where the patches in the micrograph are represented as hypernodes. The hyperedges in the hypergraph represent relationships between multiple patches, allowing for the capture of higher-order relationships between the patches in the micrograph. The experimental results corroborate our approach of augmenting HgAT layer stacks with a subsequent fully-connected transformer module(HgT) to achieve better performance compared to the state-of-art methods on automatic electron micrographs classification tasks. For future work, we would endeavor to generalize our framework on other electron micrograph datasets like REM, TEM, FE-SEM, STEM, etc., for anomaly detection, segmentation, etc."}, {"title": "A APPENDIX", "content": null}, {"title": "A.1 MODULES DESCRIPTION", "content": null}, {"title": "A.1.1 HYPERGRAPH ATTENTION NETWORK(HGAT)", "content": "The HgAT(Veli\u010dkovi\u0107 et al. (2017); Brody et al. (2021)) architecture extends the traditional convolution operation on the visual hypergraphs. It combines both local and global attention mechanisms to learn robust and expressive representations of visual hypergraphs. The hypergraph encoder(HgAT) is designed to utilize the hypergraph structure(H) and feature matrix(X) to compute the hypernode embeddings hv \u2208 Rd, Vi \u2208 V. We learn the optimal embeddings h\u2082, to preserve the high-level visual content embedded in the structural characteristics and feature attributes of the hypergraph. The layer-wise HgAT operator performs local and global-neighborhood aggregation for utilizing the powerful relational inductive bias of spatial equivariance encoded by the hypergraph's connectivity to model the fine-grained correlations explicitly between visual patches. We perform the attention-based intra-edge neighborhood aggregation for learning the latent hyperedge embeddings as,\nh(e)\n\u041f\u0435\u0440\nZ\n= (\u03a3\n(\u03a3\u03b1Wh(-1,2)), l = 1 ... LHgAT                                                                         (3)\nz=1\ni \u2208 Np,i\n0\nVi\nwhere the superscript l denotes the layer and for scenario l = 1, h00,2) = xv\u2081. hep \u2208 Rd denotes the hyperedge embeddings. We compute multiple-independent embeddings in each layer with different trainable parameters and output summed-up embeddings. o is the sigmoid function. The attention coefficient ap,i determines the relative importance of the hypernode i incident with the hyperedge, p and is computed by,\na(l,z)\np,i\n=                                 exp (e())\n\u03a3\u03b5 exp())\n-; e (liz) = ReLU (W()h(l-1,2))                                                    (4)\nwhere ep,i denote the attention score. We then model the complex relations between hyperedges and hypernodes by performing the attention-based inter-edge neighborhood aggregation for learning the expressive hypernode embeddings as described by,\nh) = \u2211 ReLU (W) h(-1,2) + \u03a3\u03b2(2) W(2) h(l,z)), l = 1... LHgAT                      (5)\nz=1\nPENi,p\nwhere w(*), W(*) \u2208 Rd\u00d7d are learnable weight matrices. We utilize the ReLU function to introduce non-linearity for updating the hypernode-level embeddings. The normalized attention scores Bi,p specifies the importance of hyperedge p incident with hypernode i and are computed by,\nB(l,z)\npip\n=                              exp())                                                   (6)\n\u03a3\u0395\u039d. exp()); \u03a6\u03c1\n-; (2) = ReLU (W). (W2)h(l-1,2) + Wh(l,z)))\nwhere Wz) \u2208 Rdxd and W\u00a7z) \u2208 R2d are weight matrix and vector. \u2295 denotes the concatenation operator. Oi,p is the unnormalized attention score. Stacking multiple layers broadens the receptive field, but performance degrades due to over-squashing(Alon & Yahav (2020)) and over-smoothing issues(Li et al. (2018)). In between each layer, we apply batch norm and dropout for regularization. We concatenate the embeddings of each HgAT layer, transform it through a linear projection, and serve as input to the HgT network."}, {"title": "A.1.2 HYPERGRAPH TRANSFORMER(HGT)", "content": "The HgT operator generalizes the transformer neural networks(Vaswani et al. (2017)) to arbitrary sparse hypergraph structures with full attention as a desired inductive bias for generalization. The permutation-invariant HgT module models the pairwise relations between all hypernodes and updates the hypernode-level embeddings by exploiting global contextual information in the visual hypergraphs. The HgT module with no structural priors acts as a drop-in replacement for various other methods of stacking multiple HgNN layers with residual connections(Fey (2019), Xu et al. (2018)), virtual hypernode mechanisms(Gilmer et al. (2017); Pham et al. (2017)), or hierarchical pooling schemes(Ramp\u00e1\u0161ek & Wolf (2021), Lee et al. (2019)) to model the long-range correlations in the visual hypergraph. The HgT operator incentivizes learning the fine-grained relations to facilitate learning of expressive embeddings by spanning large receptive fields to effectively capture the high-level semantic information embedded in the hypergraph structure. The transformer encoder(Vaswani et al. (2017)) consists of alternating layers of multiheaded self-attention(MSA) and MLP blocks. We"}, {"title": "3.2 MODEL ARCHITECTURE", "content": null}, {"title": "apply Layernorm(LN(Ba et al. (2016))) for regularization and residual connections after every block. We skip the details for conciseness and to avoid notion clutter. Inspired by ResNets(He et al. (2016)), we add skip-connections through an initial connection strategy to relieve the vanishing gradients and over-smoothing issues.\nh' = MSA (LN (h(-1)+xE)) + h(-1),       l = 1... LHgT                                                  (7)\nVi\nVi\nz = MLP (LN (h)) + h'),              l = 1... LHgT                                                      (8)\nVi\nWe do not add position embeddings in skip-connections as HgAT operator had encoded the structural information into the hypernode embeddings. HgT overcomes the inherent information bottleneck of HgAT representational capacity for effective hypergraph summarization. It does so by learning hypernode-to-hypernode relations beyond the original sparse structure and distills the long-range information in the downstream layers to learn task-specific expressive hypergraph embeddings."}, {"title": "A.2 ALGORITHMIC ARCHITECTURE", "content": "Algorithm 1 summarizes the hypergraph structure learning(HgSL) module. While Algorithm 2 gives an overview of the Vision-HgNN framework. Our implementation utilizes the HgAT module to encode discrete visual hypergraphs to compute the hypernode embeddings, and the HgT module refines the embeddings through the self-attention mechanism. The HgRo module computes the hypergraph-level embedding to facilitate the classification task"}, {"title": "A.3 EXPERIMENTAL SETUP", "content": "The data pre-processing involves per electron micrograph standard intensity normalization with a mean and covariance of 0.5 across all the channels to obtain normalized electron micrographs to the [-1, 1] range. The size of each electron micrograph in the SEM dataset (Aversa et al. (2018)) is 1024 x 768 x 3 pixels. We resize electron micrographs to obtain a relatively lower spatial resolution, 256 \u00d7 256 \u00d7 3 pixels, and split the downscaled electron micrographs into non-overlapping uniform patches of size 32 \u00d7 32 \u00d7 3 pixels. The total number of patches(n) for each electron micrograph is 64. The position embedding(Epos) and patch embedding(E) have a dimensionality size(d) of 128. We obtain the visual hypergraph representations of the electron micrographs through the Top-K nearest neighbor search algorithm with the optimal K value is 20. We utilize the k-fold cross-validation technique to evaluate the performance of our proposed method with k = 10. The training dataset comprises consecutive \u201ck-2\" of the folds. The validation and test dataset contains each with \"1\" in the remaining \"2\" folds. We implement an early stopping technique on the validation set to prevent the model from over-fitting and for model selection. The model is trained for 100 epochs to learn from the training dataset. We set the initial learning rate as 1e-3. The recommended batch size is 48. The optimal number of layers of HgAT and HgT operators, i.e., LHgAT and LHgT, are 2. The number of multiple-independent replicas(Z) is 4 for the HgAT operator. We utilize a learning rate scheduler to drop the learning rate by half if the Top-1 accuracy shows no improvement on the validation dataset for a waiting number of 10 epochs. We run the adam optimization algorithm to minimize the cross-entropy loss between the ground-truth labels and the model predictions. We train our model and baseline methods on multiple NVIDIA Tesla T4, Nvidia Tesla v100, and GeForce RTX 2080 GPUs built upon the PyTorch framework. We evaluate the model performance and report the evaluation metrics on the test dataset. Each computational experiment runs for a unique random seed. In this work, we report the ensemble average of the results obtained from five computational experiments. The experimental results reported are the average value of the different random seeds-based experimental run outputs."}, {"title": "A.4 BASELINE SETTINGS", "content": "We construct the visual graphs representation of the electron micrographs through the Top-K nearest neighbor search technique, where the patches had viewed as nodes and the edges model the pairwise associations between the semantic nearest-neighbor nodes. The baseline GNNs (Rozemberczki"}, {"title": "A.5 STUDY OF MODULES", "content": "We conduct detailed ablation studies to shed light on the relative contribution of modules for the improved overall performance of our framework. We gradually exclude the modules to design several variants of our framework and then investigate the variant model's performance compared to the Vision-HgNN model on the SEM dataset to demonstrate the efficacy and support the rationale of our modules. We refer to w/o HgAT and w/o HgT as Vision-HgNN models without HgAT and HgT modules, respectively. Table 3 shows the results of the ablation studies."}, {"title": "A.6 ABLATION STUDIES", "content": "Our proposed Vision-HgNN framework consists of HgSL, HgAT, HgT, and HgRo modules. We study the impact of each module in great detail that is responsible for the enhanced performance of our framework by substituting the modules with well-known algorithms of similar functionality to design replaced models. We compare the performance of the replaced models with our proposed framework to support the efficacy of our modules.\nStudy of HgSL Module:- We study the effectiveness of the HgSL module in modeling the complex relations among the spatially and semantically dependent visual patches for a structured representation of electron micrographs. The HgSL module learns the optimal K-uniform bi-directed hypergraph structure of the electron micrographs through the top-K nearest neighbor search algorithm. The most popular graph-based approaches for constructing the optimal graph structure are classified into two categories, (a) constructing a K-regular bi-directed graph or (b) the K-irregular bi-directed graph. The K-irregular graph construction techniques overcome the limitations of K-uniform graph structure learning techniques, which are not continuously differentiable. The former algorithmic approach in the literature includes (a) the top-K nearest neighbor search strategy using cosine similarity(CS,Deng & Hooi (2021)) and (b) structure learning through implicit correlations of node embeddings(MTGNN, Wu et al. (2020)). The later algorithmic techniques include (a) parametrization of the adjacency matrix and sample through the Gumbel reparameterization trick (Jang et al. (2016); Kool et al. (2019)) to output the link probability between nodes(GPT, Shang et al. (2021)), (b) regularized graph generation(RGG, Yu et al. (2022)) to learn sparse implicit graph by dropping redundant connections between nodes, and (c) graph relational learning using the self-attention mechanism(GRL, Zhang et al.) to construct a graph from observed data. We refer to the Vision-HgNN model for which the HgSL module is modeled with the different operators as follows,\n\u2022 w/ CS: Vision-HgNN model with the CS operator.\n\u2022 w/ MTGNN: Vision-HgNN model with the MTGNN operator.\n\u2022 w/ GPT: Vision-HgNN model with the GPT operator.\n\u2022 w/ RGG: Vision-HgNN model with the RGG operator.\n\u2022 w/ GRL: Vision-HgNN model with the GRL operator.\nTable 5 reports the performance of the replaced models compared to the Vision-HgNN framework. The Top-1 scores of the substituted models, w/ MTGNN, w/ GPT, w/ RGG, w/ GRL declined by 10.74%, 12.94%, 15.63%, 12.69% on SEM dataset compared to the Vision-HgNN model. The impact of w/ CS is marginal and achieves on-par performance compared to the Vision-HgNN model. The results show the advantages of utilizing the HgSL module modeled with the top-K nearest neighbor search technique, a simple yet effective similarity learning technique based on the Euclidean distance between patches for capturing the underlying higher-order relational information in visual hypergraphs.\nStudy of HgAT Module:- (a) We study the importance of the attention mechanism in the hypernode-level hypergraph encoder(HgAT) to compute the expressive hypernode embeddings(hv\u2081) of visual hypergraphs. We disable the attention mechanism of the layerwise HgAT operator. We perform the unweighted sum-pooling operation on the neural messages in the intra- and inter-neighborhood aggregation scheme for computing hypergraph embeddings. We refer to the Vision-HgNN model in the absence of the attention-mechanism for determining the hyperedge(he) and hypernode(hv\u2081) embeddings as follows,\n\u2022 w/o ap,i, Bi,p:Vision-HgNN model without the attention mechanism."}]}