{"title": "Grade Inflation in Generative Models", "authors": ["Phuc Nguyen", "Miao Li", "Alexandra Morgan", "Rima Arnaout", "Ramy Arnaout"], "abstract": "Generative models hold great potential, but only if one can trust the evaluation of the data they generate. We show that many commonly used quality scores for comparing two-dimensional distributions of synthetic vs. ground-truth data give better results than they should, a phenomenon we call the \"grade inflation problem.\" We show that the correlation score, Jaccard score, earth-mover's score, and Kullback-Leibler (relative-entropy) score all suffer grade inflation. We propose that any score that values all datapoints equally, as these do, will also exhibit grade inflation; we refer to such scores as \"equipoint\" scores. We introduce the concept of \u201cequidensity\u201d scores, and present the Eden score, to our knowledge the first example of such a score. We found that Eden avoids grade inflation and agrees better with human perception of goodness-of-fit than the equipoint scores above. We propose that any reasonable equidensity score will avoid grade inflation. We identify a connection between equidensity scores and R\u00e9nyi entropy of negative order. We conclude that equidensity scores are likely to outperform equipoint scores for generative models, and for comparing low-dimensional distributions more generally.", "sections": [{"title": "I. INTRODUCTION", "content": "The ability to compare pairs of two-dimensional distributions robustly and accurately is critical in machine learning, and in data analysis more generally. This ability is especially valuable in the context of generative models [1] to measure model quality by assessing how well synthetic data fits the training data. For example, generative modeling of tabular data (where each row is a datapoint and each column is a feature) has important uses across many fields [2], [3], [4], [5]. Generative modeling is of great interest for imaging and multi-modal data as well [1], [6], [7]. Although datasets for the latter applications are generally high-dimensional, two-dimensional analysis plays a critical role in quality assessment, either for testing the fidelity of pairwise relationships or via dimensionality reduction techniques such as PCA, SNE, or UMAP [8], [9], [10].\nDespite its ubiquity, the task of comparing two-dimensional distributions is non-trivial and has led to much work in developing quality scores. At a high level, quality scores can be classified as statistical vs. functional. Statistical scores are designed to demonstrate that some statistic, for example the mean of one of the features, has the same value in the synthetic dataset as in the real dataset (up to sampling error). In contrast, functional scores are meant to show that the outcome of some procedure, for example a classifier's inference or human inspection, is indistinguishable regardless of whether the input data is real or synthetic (again, up to sampling error) [11]. Statistical scores have the advantage of being easier to calculate and of being generalizable from dataset to dataset; functional scores often take more human and/or compute time and are more likely to be idiosyncratic to a particular dataset. For these reasons, statistical scores are of general interest.\nStatistical scores appear often in the literature on generative modeling, especially of synthetic tabular data. For example, two widely-used and easy-to-compute statistics are the cor- relation coefficient R and its square $R^2$ (the coefficient of determination), which measure the joint distribution between pairs of features (columns). In turn, two common types of correlation coefficient are Pearson's, which measures linear relationships, and Spearman's, which is a generalization for any monotonic relationship. For a given pair of columns, one can calculate Pearson's or Spearman's R for the real data ($R_p$), do the same for the synthetic data ($R_q$), and use them to"}, {"title": "II. METHODS", "content": "A. Datasets\nWe generated synthetic data from both in-house two- dimensional toy datasets (\"Dart',\u201d \u201cTrimodal,\" and \"Stripes\") and the following high-dimensional machine-learning datasets obtained from the University of California Irvine machine- learning repository (UCIMLR): \u201cCovertype,\u201d \u201cCommunities and Crime,\u201d \u201cEnergy Efficiency,\u201d and \u201cRice (Cammeo and Osmancik)\u201d (UCIMLR ID nos. 31, 183, 242, and 545, respectively). The latter were retrieved using the lucie Python package [2]. Anscombe's Quartet was obtained via the seaborn Python package. The Datasaurus Dozen were obtained via\nB. Generative models and KDES\nThe following models/model architectures were used to generate synthetic data: an in-house implementation of Gaus- sian Copula [17], the CTGAN implementation in the SDV package [12], [18], [19], a flow-based model from the Python package nflow [20], as well as in-house energy-based models [21]. KDEs were generated using the kdeplot function of Python's seaborn package with levels = 5 but otherwise default parameters [22]. Note in seaborn the bandwidth of the KDEs is given by Scott's method [23], and the lowest 5% of the probability mass is ignored when finding the likelihoods of the contours.\nC. Quality scores\nWe consider five different quality scores. The correlation score, the earth mover's score, the Jaccard score, and the Kullback-Leibler (KL) divergence score are based on previous work [24]; the Eden score is newly described below. Correlation score was defined using Pearson's R as 1 \u2013 |Rp - Rq|/2, with p and q the two distributions (e.g. real vs. synthetic data), reflecting the most common definition we found the literature.\nThe earth-mover's score was defined as $e^{-k \\cdot EMD}$, where k is a scaling factor that can be chosen to scale the score as desired (here, k = 1), and EMD is the normalized earth-mover's distance from the pyemd python package [25]. Details for the remaining scores follow.\nD. Jaccard score implementation\nThe Jaccard score, also known as the intersection-over-union (IoU), is defined as\n$J = \\frac{p \\cap q}{p \\cup q}$"}, {"title": "F. Eden score", "content": "In the Eden score, the difference between two KDEs is calculated by calculating the difference for each successive ring (or \"annulus\") defined by the contours of the KDES, and averaging these differences. For each annulus in the real/synthetic data, we define a per-annulus similarity score. Without loss of generality, label the outermost annulus 0, the second-outermost annulus 1, and so on. (The unbounded region outside of all contours is excluded.) Define the ith per-annulus score by a variation of the Jaccard formula (Eq. 1), with the area playing the role of size:\n$S_i = \\frac{Area(i^{th} annulus of p \\cap i^{th} annulus of q)}{Area(i^{th} annulus of p \\cup i^{th} annulus of q)}$\nThe Eden score is obtained by averaging scores over all annuli:\n$Eden = \\frac{1}{N_{annuli}} \\sum_{i=0}^{N_{annuli}-1} S_i$\nHere Nannuli is the number of annuli (note: for simplicity we refer to the innermost contour an annulus, even though technically it encloses a disk [possibly more than one]). Nannuli = 5 was used.\nTo estimate the areas in the numerator and denominator of Eq. 9, a Monte-Carlo method is used: a bounding rectangle centered at the data is sprinkled with a large number of uniformly distributed points; the number of points that lie inside the union/intersection of a real and a synthetic annulus i are counted. To determine whether a point lies inside a union/intersection of two annuli, we compute the likelihood of the point under fp and fq and check whether the likelihood falls within the likelihood range that defines two adjacent contours.\nNote the (minor) stochasticity due to KDE sampling and the Monte-Carlo procedure.\nG. Validation\nWe used human visual inspection as a gold standard for comparison of two-dimensional distributions. 39 pairs of plots were shown to 20 human raters. Each plot consisted of two superimposed KDEs in different colors, corresponding to training data and synthetic data output by a generative model. Plots were chosen so that the plot with the higher Eden score received a lower score according to at least one of the other score. These results were not shared with the human raters, making this a blind test. Each human rater was asked to choose the plot in which the contours matched better, considering all contours; the interpretation of \"better\" was otherwise left up to the rater. The 39 pairs corresponded to 3 repeats of each of 13 unique pairs of columns. For each repeat, the pair was subjected to rotations or color swaps, and the left- right order of the plots was randomized; this allowed for a per-person test of consistency (to be able to assess whether the same plot picked all three times regardless of position, orientation, and color, or not). Raters all had a background in data science, science, and/or engineering, to increase the likelihood of exposure to/familiarity with the general practice of data presented as KDEs. Separately (i.e. without the rater), the fit in each plot was scored according to each scoring"}, {"title": "III. RESULTS", "content": "A. Correlation score\nTo illustrate how the correlation score can lead to grade inflation, we first measured this score between pairs of distributions from Anscombe's quartet. The four distributions in this set were devised to show how different distributions can have identical Pearson's R (to several digits). Fig. 1a illustrates the outcome: a perfect correlation score, despite the two distributions differing materially from each other. This is grade inflation. The \"dino\" dataset from the Datasaurus Dozen illustrates the compounding problem that arises as a result of low-correlation distributions being more common than high-correlation ones (Fig. 1b). Specifically, treating dino as the training set, we randomly intialized a generative model with the same x and y means and standard deviations, and sampled from that model-without any training. The dino dataset has a Pearson's R of close to zero. Random data also has a Pearson's R of close to zero. Because these values are similar, the correlation score is nearly perfect-0.97 in the sample in Fig. 1b-despite the model having learned literally nothing beyond the location and scale of the data. Again, this is grade inflation.\nTo illustrate the phenomenon on datasets and generative models used in real-world data science, we applied a variety of models to a selection of datasets from the UCIMLR and measured the correlation score between the synthetic/generated data and the real/training data, for representative pairs of columns (Fig. 2a-d). Even when fits were low quality by eye, the correlation score was universally high, ranging from 0.903 to 0.994 (Table I), demonstrating grade inflation. As a positive control, we also scored a high-quality model fit of an in-house dataset called Dart (Fig. 2e). Not surprisingly, the correlation score was also excellent, at 0.981, but notably this score was actually lower than the 0.994 achieved by the low-quality fit in Fig. 2a (Table I). Thus, correlation score has difficulty differentiating between high- and low-quality fits, leading to grade inflation for some poor fits.\nB. Earth-mover's score\nA more sophisticated score derives from the earth-mover's distance (EMD; a.k.a. the Mallow, Wasserstein, or Kantorovich- Rubenstein distance) [27], [28], [29], [30], which is the basis of the Frechet inception distance that is commonly used in machine learning [31]. If each two-dimensional distribution is a pile of sand, the EMD is the minimum amount of work required to transform one distribution into the other by moving sand around. EMD is easily converted to a score with range (0, 1) via exponentiation (Methods). To apply this score, the finite collections of datapoints in each distribution binned into histograms (the algorithm in [25] does not require smoothing"}, {"title": "IV. DISCUSSION", "content": "All modeling benefits from reliable quality scores, including two-dimensional distributions resulting from generative models. Scores can mislead in several ways, including overfitting and in selecting the wrong model [35]. Here we add the grade- inflation problem, named for a perenially-decried phenomenon in U.S. higher education [36], in which a statistic scores a model higher than it should. We describe why the commonly used correlation score should be particularly prone to grade inflation, and offer examples from real-world datasets where other commonly used scores-specifically, the earth-mover's, Jaccard, and KL scores-still have this problem, while at least one other score our newly proposed Eden score-appears not to. We also show the value of multiple sampling for measuring and reporting confidence in these scores, which appears to be a relatively uncommon practice in the literature. We suspect the grade-inflation problem is not new, but is made newly relevant by the explosive growth of data science, the need to select among high-performing models, the bottleneck human visual inspection presents in the setting of such growth in interest, and the computational resources available for data visualization (for example, pairwise scatter plots for large, high-dimensional datasets, which taxed previous generation of computers, sometimes prohibitively).\nIt is interesting that the Eden score agreed substantially better with human perception of goodness-of-fit than the other scores tested, in a blinded head-to-head comparison on data from generative models. This finding suggests that when human raters compare distributions for similarity, they, like the Eden score, might also be comparing distributions at multiple densities and subconsciously averaging the result. The caveats: raters were specifically asked to consider all contours, were shown the distributions as KDE plots (as opposed to, for example, as three-dimensional and/or interactive renderings), and were limited to people with scientific backgrounds. This is an interesting topic for further investigation.\nA. Equipoint vs. equidensity scores\nWhat explains the difference between the earth-mover's, Jaccard, and KL scores, which exhibited grade inflation, and Eden, which did not? One answer is that these two sets of scores differ qualitatively in how they weight different regions of the two distributions. The earth-mover's score weights each datapoint equally, regardless of where in a distribution that point lies. Because by definition there are more datapoints in areas of higher density, the earth-mover's score will tend to be high as long as the regions of highest density line up well between the two distributions, almost regardless of how mismatched the low-density regions are (in the absence of extreme outliers). The Jaccard and KL scores also weight each datapoint equally, with similar results. Note, these three scores are likely not the only ones with this property; for example, multi-dimensional extensions of the Kolmogorov-Smirnov [KS] statistic also seem to have this property [37], and the Frechet inception distance is likely to suffer grade inflation as well, since it is based on"}, {"title": "D. Connections to entropy and diversity", "content": "The correspondence between unequal weighting at the level of datapoints and equal weighting at the level of densities has an interesting connection to entropy and diversity, specifically to the R\u00e9nyi entropies $H^q$ and the corresponding Hill diversities $D_q = exp \\ H^q$. (Here a = q, a potentially confusing but purely notational difference reflecting the conventions in the respective literatures; here we distinguish q the viewpoint parameter from q the distribution by context.) Both H\u00ba and D\u00ba can be interpreted as sums of the frequencies of species in a system, with q as a frequency-weighting parameter such that rarer species contribute less as q rises. D\u00ba is used to calculate the effective number of species in a population, taking frequencies into account to a degree q. It has been observed [39], [40], [33] that many commonly used statistics correspond to positive integer values of q. For example, q = 0 corresponds to a simple count of the number of unique species, q = 1 corresponds to the Shannon entropy, q = 2 corresponds to Simpson's index, and so on up to q = \u221e, which corresponds to the Berger-Parker index [41]. (These correspondences generally take the form of simple mathematical transformations of D\u00ba.)\nq (= a) can also take negative values; however, en- tropies/diversities with negative a have received little if any attention in the literature, perhaps owing to a dearth of real- world examples for a < 0 [42]. We broach the possibility that equidensity scores might be interpreted as an example of negative a if one considers the species in the entropy calculation to be datapoints, and that there is a duality with a = 0 if instead one considers the species to be equidensity regions (i.e., annuli/rings/topological contours). Note this is qualitatively different from how negative a operates in the R\u00e9nyi divergences, which is to swap distributions p and q (to a reasonable approximation, increasingly true the further from a = 0). Insofar as equidensity regions are groupings or \"communities\" of datapoints, there may also be connections with the subcommunity/metacommunity formulation in the diversity literature [43], which correspond to concepts such as relative/joint entropy and mutual information in the entropy literature [33]. We note that a Python package already exists that accepts negative a [44]. Exploring such connections could be an interesting direction for future work."}, {"title": "E. Limitations and conclusions", "content": "The primary limitations of this work are its focus on examples of two-dimensional distributions and a small number of scores; it does not investigate higher dimensions nor attempt to systematically discover and/or evaluate all possible equipoint or equidensity scores. We note that these scores do apply in higher dimensions; however as dimensionality rises, there is risk that the curse of dimensionality will affect equidensity scores as it does, for example, the Jaccard score, leading to artifactually lower scores in higher dimensions than one might consider reasonable (in the case of the Jaccard score, because the union grows much faster than the intersection as dimensionality increases). (How a one-dimensional version of the Eden score would compare to the KS statistic or a one-dimensional KL score, is an interesting question.) Studies of higher dimensions might be considered useful, since non-trivial real datasets are often high-dimensional (e.g., many columns), but our approach of using humans as the gold standard would be complicated by the inability to easily visualize higher dimensions and potentially important losses if dimensionality reduction is used (e.g. PCA, tSNE, UMAP) [10], [9], [8]. Fortunately, lower- order relationships often carry a large amount of information in complex systems [15], [16]. For this reason, we expect Eden and other equidensity scores to be useful additions to the generative-modeling toolkit."}]}