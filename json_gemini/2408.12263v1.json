{"title": "Toward the Evaluation of Large Language Models Considering Score Variance across Instruction Templates", "authors": ["Yusuke Sakai", "Adam Nohejl", "Jiangnan Hang", "Hidetaka Kamigaito", "Taro Watanabe"], "abstract": "The natural language understanding (NLU) per- formance of large language models (LLMs) has been evaluated across various tasks and datasets. The existing evaluation methods, how- ever, do not take into account the variance in scores due to differences in prompts, which leads to unfair evaluation and comparison of NLU performance. Moreover, evaluation de- signed for specific prompts is inappropriate for instruction tuning, which aims to perform well with any prompt. It is therefore necessary to find a way to measure NLU performance in a fair manner, considering score variance be- tween different instruction templates. In this study, we provide English and Japanese cross- lingual datasets for evaluating the NLU per- formance of LLMs, which include multiple in- struction templates for fair evaluation of each task, along with regular expressions to con- strain the output format. Furthermore, we pro- pose the Sharpe score as an evaluation metric that takes into account the variance in scores between templates. Comprehensive analysis of English and Japanese LLMs reveals that the high variance among templates has a significant impact on the fair evaluation of LLMs.", "sections": [{"title": "1 Introduction", "content": "Decoder-based large language models (LLMs) have become foundational resources in the field of natural language processing, demonstrating su- perior natural language understanding (NLU) abili- ties and high pre-trained knowledge capacity in a wide variety of downstream tasks. Recently, LLMs can produce more human-like responses through in- struction tuning (Wei et al., 2022a), which involves training the LLMs to respond appropriately to user instructions for various tasks.\nAlthough LLM performance has been evaluated across various NLU tasks, the evaluation processes lack standardization in terms of prompts and out- put formats. This lack of standardization leads to differences in evaluation outcomes that can- not be attributed solely to the differences among LLMs. Moreover, the differences in prompts used for evaluation affect the evaluation results in NLU tasks (Zheng et al., 2023; Lu et al., 2022; Pezeshkpour and Hruschka, 2024; Zhao et al., 2021; Hou et al., 2024; Li et al., 2024). In the specific case of instruction tuning, the goal is a prompt-independent generalization, though it is questionable to measure such generalization perfor- mance using prompts designed for specific targets.\nFor fair evaluation and comparison of the NLU performance of LLMs, we created benchmark datasets comprising multiple evaluation instruction templates for each NLU task based on the FLAN templates (Wei et al., 2022a), using five English NLU tasks and their corresponding Japanese tasks based on JGLUE (Kurihara et al., 2022). Addi- tionally, we proposed a new evaluation metric, the Sharpe score, which accounts for the variance in LLM outputs due to template differences, inspired by the Sharpe ratio (Sharpe, 1966) used in finance to assess investment efficiency.\nWe demonstrated its effectiveness for the evalua- tion of template-based NLU capability, as well as for analysis of the NLU performance of multiple LLMs in various experimental scenarios, such as zero-shot versus fine-tuning settings and English versus Japanese settings. We examined how fac- tors such as continuous training, instruction tuning, and language-specific knowledge affect knowledge- transfer capability. In order to enforce output gen- eration in line with the expected response format, we accompanied each instruction template with a regular expression of the expected output for each task. The regular expressions are employed in con- strained decoding methods as implemented in Out- lines (Willard and Louf, 2023). We experimented with both constrained decoding and greedy decod- ing, demonstrating that constrained decoding with regular expressions is effective for zero-shot evalu-"}, {"title": "2 Background and Related Work", "content": "The evaluation of the NLU capability of LLMs has mostly been based on benchmark datasets that combine several NLU tasks, such as GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019). Furthermore, NLU datasets that include domain- specific knowledge such as medical, economic, and mathematical knowledge (Jin et al., 2019; Baker et al., 2015; Pal et al., 2022; Shah et al., 2022; Chen et al., 2022; Amini et al., 2019; Hendrycks et al., 2021; Lin et al., 2022; Zhong et al., 2023b; bench authors, 2023; Suzgun et al., 2023; Liang et al., 2023) have been proposed for testing domain specific knowledge in LLMs. These benchmark datasets generally use automatic evaluation metrics, such as accuracy and F1 score.\nThese datasets are typically constructed in a con- cise format relevant to the particular task, providing only minimal information, such as questions and their answers. Therefore, the standard practices in evaluating LLMs employ instruction templates to make the datasets easy for LLMs to understand the instructions. The data instances are instantiated with instruction templates to yield natural language sentences, from which LLMs infer answers in an autoregressive manner.\nBenchmark datasets for several languages other than English are available as well. Datasets for Japanese, a language we focus on in this study, include llm-jp-eval\u00b9, JP Language Model Evaluation Harness\u00b2, and Nejumi\u00b3, all of which employ Japanese NLU datasets centered around JGLUE (Kurihara et al., 2022). The JP Language Model Evaluation Harness uses LLMs as classifiers by combining each question with corresponding an- swer choices and selecting the one with the lowest perplexity when all choices are ranked. Llm-jp-eval and Nejumi perform automatic evaluation by post- processing the generated text. In the evaluation used by Nejumi, if an answer cannot be obtained from the generated text, it assigns an arbitrary label, whereas the Ilm-jp-eval treats it as incorrect4.\nHowever, benchmarks for evaluating LLMs re- port results using only specific prompts, completely ignoring the performance variance of LLMs caused by different prompts. To mitigate the performance variance of LLMs due to different prompts, some LLMs such as FLAN (Wei et al., 2022a; Chung et al., 2024; Longpre et al., 2023), WizardLM (Xu et al., 2024), OpenAssistant (K\u00f6pf et al., 2023), and T0 (Sanh et al., 2022) enhance their generalization capabilities by instruction tuning with diverse tem- plates, enabling robust responses to diverse inputs. Prompt engineering (Wei et al., 2022b; Kojima et al., 2022; Zhong et al., 2023a; Yang et al., 2024; Zhou et al., 2023; Chen et al., 2024; Yao et al., 2023; Chen et al., 2023) has improved downstream task performance by converting input sentences into optimal prompts for LLMs. It focuses, how- ever, on finding the best prompts for particular LLMs, making the engineered prompts unsuitable for evaluating the LLMs' NLU performance con- sidering generalization capability.\nWhile these approaches ensure the robustness of inputs, existing evaluation frameworks typically examine only a single template and ignore perfor- mance variance across multiple instruction tem- plates. Consequently, to evaluate models' perfor- mance while taking into account their generaliza- tion ability, we need to find an evaluation method that incorporates variance across multiple instruc- tion templates."}, {"title": "3 Evaluation Method", "content": "In our evaluation, we focus on the variance in re- sults caused by differences in templates. To this end, we propose datasets and methods for evaluat- ing the NLU performance of LLMs using multiple instruction templates. We evaluate performance in zero-shot and fine-tuning settings, but omit in-context learning, i.e., few-shot learning, settings. The prior studies (Mosbach et al., 2023; Zhang et al., 2024) have shown that the few-shot setting merely represents the exploration for optimal input prompts, capped by the performance of fine-tuning under the same number of examples."}, {"title": "3.1 Creation of Benchmark Datasets", "content": "As shown in Table 1, we employ five English NLU tasks and their corresponding Japanese tasks to evaluate cross-lingual transfer capability and per- formance of multilingual LLMs. Appendix A pro- vides details of each task and dataset.\nWe created the instruction templates for eval- uation based on the FLAN templates (Wei et al., 2022a) by modifying them for the English tasks and then manually translating them into Japanese for the Japanese tasks. These instruction templates consist of structured prompts designed to guide the LLMs in performing specific tasks. Figure 1 shows examples of the dataset creation process for MNLI tasks. For each data instance, MNLI provides pairs of sentences, a premise, and a hypothesis. We then apply each instruction template to these sentence pairs to create natural language sentences to be used as input sequences. The expected output for- mat for answers follows FLAN. We convert the answer labels to conversational text and instruct the LLMs to generate only the corresponding num- ber or letter. We apply this procedure to other tasks to construct the entire benchmark dataset. Table 1 shows the number of templates and instances in the dataset. Furthermore, regular expressions for the expected answer format accompany each template, e.g., [0-2] in template 0-0 in Figure 1. By us- ing regular expression-based constrained decoding methods, such as Guidance or Outlines7 (Willard and Louf, 2023), it is possible to ensure generation in the expected format without any post-processing. This allows the outputs to be used directly for eval- uation, making the evaluation and comparison be- tween LLMs fairer and simpler."}, {"title": "3.2 Experimental Settings", "content": "Table 2 shows the LLMs evaluated in our exper- iments. We report the results for both zero-shot and fine-tuning settings. For the fine-tuning setting, we use QLORA (Dettmers et al., 2023)8 to train the LLMs on each dataset. The detailed experi- mental settings of the parameters are described in Appendix B. We conduct greedy decoding and con- strained decoding using regular expressions with Outlines (Willard and Louf, 2023). In greedy de- coding, since the generated text may not follow the expected answer format, we referred to the post- processing method used by Nejumi. We evaluate JCOLA and CoLA using accuracy (Acc) and the Matthews correlation coefficient (MCC); JSTS and STS-B using the Pearson and Spearman correla-"}, {"title": "4 Experimental Results and Discussions", "content": "Results on the Japanese benchmark dataset are shown in Tables 3 and 4 for the zero-shot and fine-tuning setting, respectively. Similarly, results on the English benchmark dataset are shown in Tables 5 and 6 for the zero-shot and fine-tuning setting, respectively. Note that the results for the English benchmark dataset exclude the Japanese LLMs listed in Table 2. We will focus on important aspects in the following sections and defer more discussions to Appendix D."}, {"title": "4.1 Zero-Shot Setting", "content": "Linguistic acceptability In the JCoLA task in Table 3, even the best-performing LLM has accu- racy equal to the chance rate, and MCC score is close to zero, indicating that none of the LLMs can perform the task successfully in the zero-shot set- ting. Table 5 shows the same tendency in the CoLA task, suggesting that linguistic acceptability judg- ment is a challenging task in the zero-shot setting. The low performance could be explained by the fact that JCOLA and CoLA employ answer labels annotated by linguists, in which their judgement might differ from non-experts in terms of accept- ability since linguists prioritize grammaticality (Hu et al., 2023). Since LLMs are usually trained on general-domain corpora collected from the web, this difference may have an impact.\nSemantic textual similarity In terms of zero- shot performance, shown in Table 5, Llama-2-13B- inst achieves high performance on the STS-B task in the English dataset. Furthermore, Table 3 shows that it also achieves high performance on the JSTS task in the Japanese dataset. This suggests that the LLM has a sufficient cross-lingual transfer capabil- ity for semantic textual similarity.\nReading comprehension From the JSQUAD task results shown in Table 3, the exact match rate improves after instruction tuning for Weblab-10B, LLM-jp-13B, ELYZA-Llama-2-7B, Llama-2-7B, and Llama-2-13B. However, no improvements are observed for StableLM-ja-7B after instruction tun- ing. This suggests that the quality of the instruction tuning data is important in the zero-shot setting.\nCommonsense reasoning In CommonsenseQA and JCommonsenseQA results shown in Table 3 and Table 5, the Llama-2-7B-inst and Llama-2- 13B-inst demonstrate a degree of language-transfer capability, although we would expect certain cul- tural differences embedded in commonsense knowl- edge of English and Japanese. However, if we focus at ELYZA-Llama-2-7B-inst10, we observe a decrease in zero-shot performance compared to Llama-2-7B-inst. Nevertheless, in the results of the fine-tuning setting shown in Table 4, ELYZA- Llama-2-7B-inst scores improved compared to Llama-2-7B-inst. This suggests that while the model has acquired knowledge through continuous training on Japanese data, it may have forgotten how to utilize it, leading to drop in accuracy in the zero-shot setting. At the same time, as shown in Table 5, ELYZA-Llama-2-7B and ELYZA-Llama-"}, {"title": "4.2 Fine-Tuning Settings", "content": "In the fine-tuning setting shown in Table 6, Llama2- 13B is either the best or second-best model in most cases on the English dataset. Moreover, the pre- trained-only model achieves better results than its instruction-tuned version of Llama2-13B-inst. This demonstrates that instruction tuning does not guar- antee better evaluation performance on the bench- mark datasets, likely because instruction tuning aims to generalize the model for diverse queries.\nAs shown in Table 4, Llama2-13B achieves the highest or nearly the highest evaluation scores in JSTS, JNLI, and JSQuAD. In JCOLA, Weblab-10B achieves a particularly high score, and in JCom- monsenseQA, StableLM-ja-7B-inst stands out with high scores. Comparison of these results with the results on English datasets suggests that LLMs can handle tasks such as JSTS, JNLI, and JSQuAD by leveraging their cross-lingual transfer capabilities. However, in the case of natural language inference (NLI, represented by MNLI and JNLI in our data), it has been pointed out that models might make predictions based solely on superficial features due to overfitting (Kavumba et al., 2022; McCoy et al., 2019; Wang et al., 2022; Tang et al., 2023; Du et al., 2023). Thus, further investigation is necessary to justify whether these results are truly due to cross- lingual transfer, or not."}, {"title": "4.3 Decoding Methods", "content": "In the zero-shot setting shown in Table 3 and Ta- ble 5, constrained decoding with regular expres- sions generally achieves higher performance than"}, {"title": "5 Analysis Considering Variance Among Templates", "content": "Evaluation by Multiple Templates"}, {"title": "5.1 Evaluation by Multiple Templates", "content": "Figure 2 shows the evaluation results in the fine- tuning setting with only a single template on the Japanese dataset. The accuracy of each template varies greatly for JNLI and JCommonsenseQA, de- pending on whether the template's answer format uses letters or numbers. Moreover, in JSTS and JCOLA, certain templates result in lower scores. On the other hand, when constrained decoding is ap- plied, some models and tasks produce more stable outputs. This suggests that while the models can re- spond to the input sentences, they fail to faithfully"}, {"title": "5.2 Evaluation Metrics Considering Performance Variance Among Templates", "content": "LLMs are expected to provide cor- rect answers to diverse prompts, rather than only responding to specific prompts. Therefore, we pro- pose the Sharpe score, an evaluation metric de- signed to evaluate both the robustness and accuracy of outputs by considering different instruction tem- plates. The Sharpe score is based on the Sharpe ratio (Sharpe, 1966), which is used in finance to assess investment efficiency. Please refer to Ap- pendix E for details on the Sharpe ratio and the derivation process of the Sharpe score from the Sharpe ratio, as well as their similarities.\nWe define the Sharpe score as follows:\nSharpe score = $\\frac{\\mu_{score}}{\\alpha \\sigma_{score} + 1},$ (1)\nwhere \u03b1 is a parameter that controls the impact of variance in scores among templates. We add 1 to the denominator as a smoothing term to avoid the zero-division issue. When \u03b1 is 0, the score is reduced to an average of performance evaluation metrics. When \u03b1 is 1, the Sharpe score is com- puted analogously to the Sharpe ratio. For values greater than 1, the variance in results across tem- plates leads to a proportionally larger penalty. The default parameter of \u03b1 is set to 1.0. The more de- tail experimental results with the Sharpe score are discussed in Appendix C."}, {"title": "Ranking", "content": "Figure 3 shows the changes in the rank- ings among the models, using the Sharpe score by incrementing the hyperparameter \u03b1 from 0 to 2 by steps of 0.1 in the Japanese dataset. Appendix C shows the results for the English dataset sharing a similar tendency. While the mean and variance values are constant for each model, the change in the hyperparameter \u03b1 reflects the degree of impact of variance, resulting in the final score being under- estimated. Moreover, when there are fluctuations in the rankings between models, a model that has moved down in rank might perform well in the over- all score but exhibit large variations in scores for each template. This indicates that a model that has moved up in rank can produce more stable outputs. In Figure 3, we observe that the rankings of the models in JSQUAD and JCommonsenseQA show little change when the parameter \u03b1 is varied. How- ever, for other datasets such as JNLI, the rankings frequently change with the variation of \u03b1, indicat- ing a larger variance in evaluation scores among the templates. These results suggest that, while there is generally a correlation between low variance in evaluation scores among templates and high per- formance when considering only the average of instruction templates, the trend of improvement in performance and variance does not necessarily align for all tasks. Therefore, it was found that the Sharpe score, which considers variance, is an effective performance evaluation metric."}, {"title": "6 Conclusion", "content": "In this paper, we focused on the variance in the eval- uation results of LLMs caused by the variations in instruction templates. We proposed a cross-lingual benchmark dataset based on multiple instruction templates, and reported the evaluation results of models trained on varied data. We also proposed the Sharpe score, which considers the variance in evaluation scores among templates, and demon- strated that it is necessary to consider variance when evaluating LLM performance.\nBased on a comparison of diverse LLMs using our dataset and an analysis of the results, we fo- cused on the tasks where cross-lingual knowledge is effective and the effectiveness of LLMs created for specific languages such as Japanese. An issue closely related to what we touched upon in Sec- tion 4.1, i.e., the catastrophic forgetting due to con- tinuous training and instruction tuning, is already being studied (Wang et al., 2023; Luo et al., 2023; Kotha et al., 2024), and our dataset may help in an- alyzing the knowledge and cross-lingual capability of LLMs in more detail. Future LLM development would also benefit from a study verifying the extent of knowledge acquisition and the effects of instruc- tion tuning after different sequences of pre-training, instruction tuning, and continuous training. As a future work, we intend to conduct further analyses and to create a comprehensive evaluation frame- work for analyzing the NLU capabilities of LLMs by expanding the proposed dataset."}, {"title": "7 Limitations", "content": "Coverage of tasks, templates, and languages This study covered a limited number of tasks, tem- plates, and languages. We conducted a comprehen- sive validation to demonstrate that evaluation re- sults diverge depending on the variations in instruc- tion templates, highlighting the necessity of evalu- ations using multiple templates. For the instruction templates used in the evaluation, we utilized the prompt templates from the FLAN dataset, modify- ing them to create the English evaluation templates and then translating those into the Japanese eval- uation templates. In terms of tasks, our study is comprehensive as it covers all the currently acces- sible tasks in JGLUE, the Japanese standard NLU benchmark dataset, as well as data from compara- ble English tasks. Although increasing the number of tasks and languages is a direction for future research, obtaining completely aligned data is chal- lenging. Therefore, creating such aligned multi- lingual datasets and developing evaluation prompt templates for other tasks to increase the number of corresponding tasks will also be future challenges.\nNumber of LLMs used for evaluation In this study, we evaluated a total of 15 types of LLMs, cat- egorized into four types of language models. Due to the rapid development of LLMs, the number of models continues to increase dramatically, making it impractical to include all results in this study. Therefore, we focused our evaluation on selected language models that cover various training proce- dures and training data. As discussed in Section 4, we conducted a comprehensive investigation into factors such as transfer performance, the impact of instruction-tuning, continuous training for each language, and the number of parameters. Further- more, the Sharpe score revealed that the stability of outputs varied across models when considering the variance. Consequently, we believe that the number and quality of language models used in this study are sufficient to demonstrate the necessity of con- sidering output stability in the evaluation of LLMs. To accommodate various future language models, one of the directions we are considering is to create leaderboards and other tools.\nEvaluation of LLMs trained on FLAN templates Zero-shot evaluation of language models trained on similar data, such as FLAN-T5 (Chung et al., 2024) and FLACUNA (Ghosal et al., 2023), would lead to unfair evaluations as discussed in Section 4.1. Therefore, it would not be appropriate to evaluate such models trained on FLAN data using the eval- uation instruction templates created in this study. In contrast, in the fine-tuning setting we used, it is possible to conduct a fair evaluation without con- sidering the effects of pre-training or instruction- tuning data sources, assuming there was no leakage of test data. While we recommend evaluation after fine-tuning, this approach incurs a high computa- tional cost, and therefore developing a mechanism to evaluate zero-shot performance in such models is also desirable and remains a future challenge due to the higher cost of fine-tuning compared to inference."}, {"title": "8 Ethical Considerations", "content": "Our evaluation templates are based on the FLAN templates, which are released under the Apache License 2.0, allowing modification and redistri- bution. We have made modifications, including translations, to these templates. While the original templates were created by the authors of FLAN, we have adapted and extended them for our purposes. The extended templates will be released under the same Apache License 2.0. Moreover, we will only be distributing our modified templates and will not distribute any datasets such as JGLUE, ensuring that there are no licensing issues."}, {"title": "A Detailed Explanation of Each Task", "content": "As shown in Figure 4, we employ five Japanese NLU tasks included in JGLUE (Kuri- hara et al., 2022)11 and the corresponding English tasks to evaluate cross-lingual trans- fer capability and performance of multilingual LLMs: (1) JCOLA (Someya et al., 2024) and COLA (Warstadt et al., 2019) are linguistic ac- ceptability tasks, where the given sentences are assigned binary labels based on whether they are linguistically acceptable or not. (2) JSTS (Kurihara et al., 2022) and STS-B (Cer et al., 2017) are tasks of judging semantic textual similarity, where simi- larity scores are assigned to pairs of sentences. (3) JNLI (Kurihara et al., 2022) and MNLI (Williams et al., 2018) are natural language inference tasks, where pairs of sentences are classified as having one of three relationships: entailment, contradic- tion, or neutrality. (4) JSQuAD (Kurihara et al., 2022) and SQuAD (Rajpurkar et al., 2016) are reading comprehension tasks that require extract- ing the answer to a question from a given paragraph. (5) JCommonsenseQA (Kurihara et al., 2022) and CommonsenseQA (Talmor et al., 2019) are com- monsense reasoning tasks, where the most plausi- ble answer to a question is selected from a set of options. JGLUE was created from scratch based on the methodology used for the corresponding English datasets, ensuring dataset alignment."}, {"title": "B Detailed Experimental Settings", "content": "Hyper-parameters Table 7 shows the exper- imental settings of the parameters. We use"}, {"title": "C Experimental Results Using Sharpe Score", "content": "Results Table 8 and 9 show the results consider- ing the variance among templates using the Sharpe score for the fine-tuning setting on Japanese and English datasets, respectively. Note that \u03b1 is set to 1, and the corresponding raw results in the same settings are shown in Tables 4 and 6, respectively. Compared to the raw results in Table 4, the evalua- tion results adjusted by the Sharpe score in Table 8 result in changes in the model ranking. For exam- ple, in JNLI with greedy decoding, ELYZA-Llama- 2-7B achieves the best evaluation result after ad- justing by the Sharpe score in Table 8. Similar change in the model rankings occurs in other cases"}, {"title": "D Discussions (Details)", "content": "Model Size"}, {"title": "D.1 Model Size", "content": "Based on the comparison of the 13B model group with the 7B model group, it cannot be concluded that an increase in parameters necessarily affects NLU performance. However, if we compare mod- els based solely on the number of parameters within the Llama-2 series, the increase in evaluation scores relative to the increase in parameters is minimal. On the other hand, when comparing PLaMO-13B with StableLM-ja-7B, despite the difference in the number of parameters, StableLM-ja-7B achieves higher performance. This suggests that improve- ments in NLU performance are more significantly influenced by the training data than by the number of parameters. These results are in line with recent studies (Hoffmann et al., 2024; Xue et al., 2023) that indicate that the quantity of training data is more effective than the number of parameters."}, {"title": "D.2 Language Transfer Capability", "content": "When discussing the cross-lingual transfer capa- bility in Sections 4.1 and 4.2, we noted that LLM- jp-13B-inst (results in Table 5), trained with the instruction-tuning dataset Jaster, which is based on JGLUE, can make certain inferences even in the zero-shot setting through cross-lingual transfer, despite not being trained on the corresponding En- glish data for STS-B and CommonsenseQA. For STS-B, the results are comparable to those dis- cussed for Llama2-13B in Section 4.1, demonstrat- ing similar transfer performance from Japanese to English. For CommonsenseQA, the model could likely make correct inferences because some com- monsense knowledge is shared between Japanese and English. This indicates that when NLU tasks are explicitly learned for a specific language, the performance can be transferred to some extent to other languages. It remains a future challenge, how- ever, to identify the domains where cross-lingual transfer is possible."}, {"title": "D.3 Decoding Methods (Details)", "content": "In the zero-shot setting shown in Table 3 and Ta- ble 5, constrained decoding with regular expres- sions generally achieves higher performance than greedy decoding. However, in the fine-tuning set- ting shown in Table 4 and Table 6, greedy decod- ing generally achieves higher performance than constrained decoding. Therefore, especially when evaluating the zero-shot setting, it is reasonable to use constrained decoding to eliminate errors due to differences in output formats.\nAdditionally, in Table 3, we can see that LLM- jp-13B-inst shows a significant difference in scores between greedy and constrained decoding. One possible reason for this is the influence of the instruction data, specifically the Jaster12 dataset created, which is based on the JGLUE datasets. We hypothesize that due to instruction tuning with Jaster, higher generation probabilities are assigned to certain words, which may have worked well with greedy decoding but not with constrained decoding (Jain et al., 2024 makes a similar observation about instruction tuning)."}, {"title": "D.4 How Many Examples Are Required for Adequate Evaluation in the Fine-Tuning Setting?", "content": "We investigated the number of sentences required for the fine-tuning setting to evaluate the NLU per- formance of LLMs. Figure 6 shows the evalua- tion scores when fine-tuning StableLM-ja-7B-inst and ELYZA-Llama-2-7B-inst with 1, 10, 100, 500, 1000, and 10000 sentences13. Thin lines represent the results of fine-tuning with each number of sen- tences only once, while thick lines represent the results of repeated fine-tuning with the respective number of sentences to achieve a total of 1000 sen- tences, e.g., training with 10 sentences 100 times"}, {"title": "E Sharpe Ratio (Details)", "content": "The Sharpe ratio is used as a measure of the risk- adjusted return of an investment. The Sharpe ratio can be expressed as follows:\nSharpe ratio = $\\frac{R_p - R_f}{\\sigma_p},$ (2)\nwhere $R_p$ is the return of the portfolio, $R_f$ is the risk-free rate, and $\\sigma_p$ is the standard deviation of the portfolio return.\nWhen applying this concept to our evaluation, the return of the portfolio $R_p$ corresponds to the average of the evaluation scores $\\mu_{score}$, the risk- free rate $R_f$ corresponds to the chance rate, and the standard deviation of the portfolio return $\\sigma_p$ corre- sponds to the standard deviation of the evaluation scores for each template $\\sigma_{score}$. Since the chance rate is constant for each task, we can ignore it. We therefore define the Sharpe score as Equation (1). The default parameter of \u03b1 is set to 1.0 in Sharpe"}, {"title": "F Example of Japanese Instruction Template", "content": "Figure 7 shows examples of the dataset creation process for JNLI tasks. We created Japanese JNLI templates by manually translating the MNLI tem- plates corresponding to the English tasks, as shown in Figure 1. For instance, JNLI provides pairs of sentences, a premise, and a hypothesis. We then ap- ply each instruction template to these sentence pairs to create natural language sentences to be used as input sequences. The expected output format for answers follows FLAN. We convert the answer la- bels to conversational text and instruct the LLMs to generate only the corresponding number or letter."}]}