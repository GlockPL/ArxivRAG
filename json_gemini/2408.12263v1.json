{"title": "Toward the Evaluation of Large Language Models Considering Score\nVariance across Instruction Templates", "authors": ["Yusuke Sakai", "Adam Nohejl", "Jiangnan Hang", "Hidetaka Kamigaito", "Taro Watanabe"], "abstract": "The natural language understanding (NLU) per-\nformance of large language models (LLMs)\nhas been evaluated across various tasks and\ndatasets. The existing evaluation methods, how-\never, do not take into account the variance in\nscores due to differences in prompts, which\nleads to unfair evaluation and comparison of\nNLU performance. Moreover, evaluation de-\nsigned for specific prompts is inappropriate for\ninstruction tuning, which aims to perform well\nwith any prompt. It is therefore necessary to\nfind a way to measure NLU performance in\na fair manner, considering score variance be-\ntween different instruction templates. In this\nstudy, we provide English and Japanese cross-\nlingual datasets for evaluating the NLU per-\nformance of LLMs, which include multiple in-\nstruction templates for fair evaluation of each\ntask, along with regular expressions to con-\nstrain the output format. Furthermore, we pro-\npose the Sharpe score as an evaluation metric\nthat takes into account the variance in scores\nbetween templates. Comprehensive analysis\nof English and Japanese LLMs reveals that the\nhigh variance among templates has a significant\nimpact on the fair evaluation of LLMs.", "sections": [{"title": "1 Introduction", "content": "Decoder-based large language models (LLMs)\nhave become foundational resources in the field\nof natural language processing, demonstrating su-\nperior natural language understanding (NLU) abili-\nties and high pre-trained knowledge capacity in a\nwide variety of downstream tasks. Recently, LLMs\ncan produce more human-like responses through in-\nstruction tuning (Wei et al., 2022a), which involves\ntraining the LLMs to respond appropriately to user\ninstructions for various tasks.\nAlthough LLM performance has been evaluated\nacross various NLU tasks, the evaluation processes\nlack standardization in terms of prompts and out-\nput formats. This lack of standardization leads\nto differences in evaluation outcomes that can-\nnot be attributed solely to the differences among\nLLMs. Moreover, the differences in prompts\nused for evaluation affect the evaluation results in\nNLU tasks (Zheng et al., 2023; Lu et al., 2022;\nPezeshkpour and Hruschka, 2024; Zhao et al.,\n2021; Hou et al., 2024; Li et al., 2024). In the\nspecific case of instruction tuning, the goal is a\nprompt-independent generalization, though it is\nquestionable to measure such generalization perfor-\nmance using prompts designed for specific targets.\nFor fair evaluation and comparison of the NLU\nperformance of LLMs, we created benchmark\ndatasets comprising multiple evaluation instruction\ntemplates for each NLU task based on the FLAN\ntemplates (Wei et al., 2022a), using five English\nNLU tasks and their corresponding Japanese tasks\nbased on JGLUE (Kurihara et al., 2022). Addi-\ntionally, we proposed a new evaluation metric, the\nSharpe score, which accounts for the variance in\nLLM outputs due to template differences, inspired\nby the Sharpe ratio (Sharpe, 1966) used in finance\nto assess investment efficiency.\nWe demonstrated its effectiveness for the evalua-\ntion of template-based NLU capability, as well as\nfor analysis of the NLU performance of multiple\nLLMs in various experimental scenarios, such as\nzero-shot versus fine-tuning settings and English\nversus Japanese settings. We examined how fac-\ntors such as continuous training, instruction tuning,\nand language-specific knowledge affect knowledge-\ntransfer capability. In order to enforce output gen-\neration in line with the expected response format,\nwe accompanied each instruction template with a\nregular expression of the expected output for each\ntask. The regular expressions are employed in con-\nstrained decoding methods as implemented in Out-\nlines (Willard and Louf, 2023). We experimented\nwith both constrained decoding and greedy decod-\ning, demonstrating that constrained decoding with\nregular expressions is effective for zero-shot evalu-"}, {"title": "2 Background and Related Work", "content": "The evaluation of the NLU capability of LLMs\nhas mostly been based on benchmark datasets that\ncombine several NLU tasks, such as GLUE (Wang\net al., 2018) and SuperGLUE (Wang et al., 2019).\nFurthermore, NLU datasets that include domain-\nspecific knowledge such as medical, economic, and\nmathematical knowledge (Jin et al., 2019; Baker\net al., 2015; Pal et al., 2022; Shah et al., 2022;\nChen et al., 2022; Amini et al., 2019; Hendrycks\net al., 2021; Lin et al., 2022; Zhong et al., 2023b;\nbench authors, 2023; Suzgun et al., 2023; Liang\net al., 2023) have been proposed for testing domain\nspecific knowledge in LLMs. These benchmark\ndatasets generally use automatic evaluation metrics,\nsuch as accuracy and F1 score.\nThese datasets are typically constructed in a con-\ncise format relevant to the particular task, providing\nonly minimal information, such as questions and\ntheir answers. Therefore, the standard practices\nin evaluating LLMs employ instruction templates\nto make the datasets easy for LLMs to understand\nthe instructions. The data instances are instantiated\nwith instruction templates to yield natural language\nsentences, from which LLMs infer answers in an\nautoregressive manner.\nBenchmark datasets for several languages other\nthan English are available as well. Datasets\nfor Japanese, a language we focus on in this\nstudy, include llm-jp-eval\u00b9, JP Language Model\nEvaluation Harness2, and Nejumi\u00b3, all of which\nemploy Japanese NLU datasets centered around\nJGLUE (Kurihara et al., 2022). The JP Language\nModel Evaluation Harness uses LLMs as classifiers\nby combining each question with corresponding an-\nswer choices and selecting the one with the lowest\nperplexity when all choices are ranked. Llm-jp-eval\nand Nejumi perform automatic evaluation by post-\nprocessing the generated text. In the evaluation\nused by Nejumi, if an answer cannot be obtained\nfrom the generated text, it assigns an arbitrary label,\nwhereas the Ilm-jp-eval treats it as incorrect4.\nHowever, benchmarks for evaluating LLMs re-\nport results using only specific prompts, completely"}, {"title": "3 Evaluation Method", "content": "In our evaluation, we focus on the variance in re-\nsults caused by differences in templates. To this\nend, we propose datasets and methods for evaluat-\ning the NLU performance of LLMs using multiple\ninstruction templates. We evaluate performance\nin zero-shot and fine-tuning settings, but omit in-\ncontext learning, i.e., few-shot learning, settings.\nThe prior studies (Mosbach et al., 2023; Zhang\net al., 2024) have shown that the few-shot setting\nmerely represents the exploration for optimal input\nprompts, capped by the performance of fine-tuning\nunder the same number of examples."}, {"title": "3.1 Creation of Benchmark Datasets", "content": "As shown in Table 1, we employ five English NLU\ntasks and their corresponding Japanese tasks to"}, {"title": "3.2 Experimental Settings", "content": "Table 2 shows the LLMs evaluated in our exper-\niments. We report the results for both zero-shot\nand fine-tuning settings. For the fine-tuning setting,\nwe use QLORA (Dettmers et al., 2023)8 to train\nthe LLMs on each dataset. The detailed experi-\nmental settings of the parameters are described in\nAppendix B. We conduct greedy decoding and con-\nstrained decoding using regular expressions with\nOutlines (Willard and Louf, 2023). In greedy de-\ncoding, since the generated text may not follow the\nexpected answer format, we referred to the post-\nprocessing method used by Nejumi. We evaluate\nJCOLA and CoLA using accuracy (Acc) and the\nMatthews correlation coefficient (MCC); JSTS and\nSTS-B using the Pearson and Spearman correla-"}, {"title": "4 Experimental Results and Discussions", "content": "Results on the Japanese benchmark dataset are\nshown in Tables 3 and 4 for the zero-shot and\nfine-tuning setting, respectively. Similarly, results\non the English benchmark dataset are shown in\nTables 5 and 6 for the zero-shot and fine-tuning\nsetting, respectively. Note that the results for the\nEnglish benchmark dataset exclude the Japanese\nLLMs listed in Table 2. We will focus on important\\aspects in the following sections and defer more\ndiscussions to Appendix D."}, {"title": "4.1 Zero-Shot Setting", "content": "Linguistic acceptability In the JCoLA task in\nTable 3, even the best-performing LLM has accu-\nracy equal to the chance rate, and MCC score is"}, {"title": "4.2 Fine-Tuning Settings", "content": "In the fine-tuning setting shown in Table 6, Llama2-\n13B is either the best or second-best model in most\ncases on the English dataset. Moreover, the pre-\ntrained-only model achieves better results than its\ninstruction-tuned version of Llama2-13B-inst. This\ndemonstrates that instruction tuning does not guar-\nantee better evaluation performance on the bench-\nmark datasets, likely because instruction tuning\naims to generalize the model for diverse queries.\nAs shown in Table 4, Llama2-13B achieves the"}, {"title": "4.3 Decoding Methods", "content": "In the zero-shot setting shown in Table 3 and Ta-\nble 5, constrained decoding with regular expres-\nsions generally achieves higher performance than"}, {"title": "5 Analysis Considering Variance Among\nTemplates", "content": ""}, {"title": "5.1 Evaluation by Multiple Templates", "content": "Figure 2 shows the evaluation results in the fine-\ntuning setting with only a single template on the\nJapanese dataset. The accuracy of each template\nvaries greatly for JNLI and JCommonsenseQA, de-\npending on whether the template's answer format\nuses letters or numbers. Moreover, in JSTS and\nJCOLA, certain templates result in lower scores. On\nthe other hand, when constrained decoding is ap-\nplied, some models and tasks produce more stable\noutputs. This suggests that while the models can re-"}, {"title": "5.2 Evaluation Metrics Considering\nPerformance Variance Among Templates", "content": "Sharpe score LLMs are expected to provide cor-\nrect answers to diverse prompts, rather than only\nresponding to specific prompts. Therefore, we pro-\npose the Sharpe score, an evaluation metric de-\nsigned to evaluate both the robustness and accuracy\nof outputs by considering different instruction tem-\nplates. The Sharpe score is based on the Sharpe\nratio (Sharpe, 1966), which is used in finance to\nassess investment efficiency. Please refer to Ap-\npendix E for details on the Sharpe ratio and the\nderivation process of the Sharpe score from the\nSharpe ratio, as well as their similarities.\nWe define the Sharpe score as follows:\nSharpe score = \\frac{\\mu_{score}}{\\alpha \\sigma_{score} + 1}',\nwhere a is a parameter that controls the impact\nof variance in scores among templates. We add 1\nto the denominator as a smoothing term to avoid\nthe zero-division issue. When a is 0, the score is\nreduced to an average of performance evaluation\nmetrics. When a is 1, the Sharpe score is com-\nputed analogously to the Sharpe ratio. For values\ngreater than 1, the variance in results across tem-\nplates leads to a proportionally larger penalty. The\ndefault parameter of a is set to 1.0. The more de-\ntail experimental results with the Sharpe score are\ndiscussed in Appendix C.\nRanking Figure 3 shows the changes in the rank-\nings among the models, using the Sharpe score by\nincrementing the hyperparameter a from 0 to 2 by\nsteps of 0.1 in the Japanese dataset. Appendix C\nshows the results for the English dataset sharing\na similar tendency. While the mean and variance\nvalues are constant for each model, the change in\nthe hyperparameter a reflects the degree of impact"}, {"title": "6 Conclusion", "content": "In this paper, we focused on the variance in the eval-\nuation results of LLMs caused by the variations in\ninstruction templates. We proposed a cross-lingual\nbenchmark dataset based on multiple instruction\ntemplates, and reported the evaluation results of\nmodels trained on varied data. We also proposed\nthe Sharpe score, which considers the variance in\nevaluation scores among templates, and demon-\nstrated that it is necessary to consider variance\nwhen evaluating LLM performance.\nBased on a comparison of diverse LLMs using\nour dataset and an analysis of the results, we fo-\ncused on the tasks where cross-lingual knowledge\nis effective and the effectiveness of LLMs created\nfor specific languages such as Japanese. An issue\nclosely related to what we touched upon in Sec-\ntion 4.1, i.e., the catastrophic forgetting due to con-\ntinuous training and instruction tuning, is already\nbeing studied (Wang et al., 2023; Luo et al., 2023;\nKotha et al., 2024), and our dataset may help in an-\nalyzing the knowledge and cross-lingual capability\nof LLMs in more detail. Future LLM development\nwould also benefit from a study verifying the extent\nof knowledge acquisition and the effects of instruc-\ntion tuning after different sequences of pre-training,\ninstruction tuning, and continuous training. As a\nfuture work, we intend to conduct further analyses\nand to create a comprehensive evaluation frame-\nwork for analyzing the NLU capabilities of LLMs\nby expanding the proposed dataset."}, {"title": "7 Limitations", "content": "Coverage of tasks, templates, and languages\nThis study covered a limited number of tasks, tem-\nplates, and languages. We conducted a comprehen-\nsive validation to demonstrate that evaluation re-\nsults diverge depending on the variations in instruc-\ntion templates, highlighting the necessity of evalu-\nations using multiple templates. For the instruction\ntemplates used in the evaluation, we utilized the\nprompt templates from the FLAN dataset, modify-\ning them to create the English evaluation templates\nand then translating those into the Japanese eval-\nuation templates. In terms of tasks, our study is\ncomprehensive as it covers all the currently acces-\nsible tasks in JGLUE, the Japanese standard NLU\nbenchmark dataset, as well as data from compara-\nble English tasks. Although increasing the number\nof tasks and languages is a direction for future\nresearch, obtaining completely aligned data is chal-\nlenging. Therefore, creating such aligned multi-\nlingual datasets and developing evaluation prompt\ntemplates for other tasks to increase the number of\ncorresponding tasks will also be future challenges.\nNumber of LLMs used for evaluation In this\nstudy, we evaluated a total of 15 types of LLMs, cat-\negorized into four types of language models. Due\nto the rapid development of LLMs, the number of\nmodels continues to increase dramatically, making\nit impractical to include all results in this study.\nTherefore, we focused our evaluation on selected\nlanguage models that cover various training proce-\ndures and training data. As discussed in Section 4,\nwe conducted a comprehensive investigation into\nfactors such as transfer performance, the impact\nof instruction-tuning, continuous training for each\nlanguage, and the number of parameters. Further-\nmore, the Sharpe score revealed that the stability of\noutputs varied across models when considering the\nvariance. Consequently, we believe that the number\nand quality of language models used in this study\nare sufficient to demonstrate the necessity of con-\nsidering output stability in the evaluation of LLMs.\nTo accommodate various future language models,\none of the directions we are considering is to create\nleaderboards and other tools.\nEvaluation of LLMs trained on FLAN templates\nZero-shot evaluation of language models trained on\nsimilar data, such as FLAN-T5 (Chung et al., 2024)\nand FLACUNA (Ghosal et al., 2023), would lead\nto unfair evaluations as discussed in Section 4.1."}, {"title": "8 Ethical Considerations", "content": "Our evaluation templates are based on the FLAN\ntemplates, which are released under the Apache\nLicense 2.0, allowing modification and redistri-\nbution. We have made modifications, including\ntranslations, to these templates. While the original\ntemplates were created by the authors of FLAN, we\nhave adapted and extended them for our purposes.\nThe extended templates will be released under the\nsame Apache License 2.0. Moreover, we will only\nbe distributing our modified templates and will not\ndistribute any datasets such as JGLUE, ensuring\nthat there are no licensing issues."}, {"title": "A Detailed Explanation of Each Task", "content": "As shown in Figure 4, we employ five\nJapanese NLU tasks included in JGLUE (Kuri-\nhara et al., 2022)11 and the corresponding\nEnglish tasks to evaluate cross-lingual trans-\nfer capability and performance of multilingual\nLLMs: (1) JCOLA (Someya et al., 2024) and\nCOLA (Warstadt et al., 2019) are linguistic ac-\nceptability tasks, where the given sentences are\nassigned binary labels based on whether they are\nlinguistically acceptable or not. (2) JSTS (Kurihara\net al., 2022) and STS-B (Cer et al., 2017) are tasks\nof judging semantic textual similarity, where simi-\nlarity scores are assigned to pairs of sentences. (3)\nJNLI (Kurihara et al., 2022) and MNLI (Williams\net al., 2018) are natural language inference tasks,\nwhere pairs of sentences are classified as having\none of three relationships: entailment, contradic-\ntion, or neutrality. (4) JSQuAD (Kurihara et al.,\n2022) and SQuAD (Rajpurkar et al., 2016) are\nreading comprehension tasks that require extract-\ning the answer to a question from a given paragraph.\n(5) JCommonsenseQA (Kurihara et al., 2022) and\nCommonsenseQA (Talmor et al., 2019) are com-\nmonsense reasoning tasks, where the most plausi-\nble answer to a question is selected from a set of\noptions. JGLUE was created from scratch based\non the methodology used for the corresponding\nEnglish datasets, ensuring dataset alignment."}, {"title": "B Detailed Experimental Settings", "content": "Hyper-parameters Table 7 shows the exper-\nimental settings of the parameters. We use"}, {"title": "C Experimental Results Using Sharpe\nScore", "content": "Results Table 8 and 9 show the results consider-\ning the variance among templates using the Sharpe\nscore for the fine-tuning setting on Japanese and\nEnglish datasets, respectively. Note that a is set to\n1, and the corresponding raw results in the same\nsettings are shown in Tables 4 and 6, respectively.\nCompared to the raw results in Table 4, the evalua-\ntion results adjusted by the Sharpe score in Table 8\nresult in changes in the model ranking. For exam-\nple, in JNLI with greedy decoding, ELYZA-Llama-\n2-7B achieves the best evaluation result after ad-"}, {"title": "D Discussions (Details)", "content": ""}, {"title": "D.1 Model Size", "content": "Based on the comparison of the 13B model group\nwith the 7B model group, it cannot be concluded\nthat an increase in parameters necessarily affects\nNLU performance. However, if we compare mod-\nels based solely on the number of parameters within\nthe Llama-2 series, the increase in evaluation scores\nrelative to the increase in parameters is minimal.\nOn the other hand, when comparing PLaMO-13B\nwith StableLM-ja-7B, despite the difference in the\nnumber of parameters, StableLM-ja-7B achieves\nhigher performance. This suggests that improve-\nments in NLU performance are more significantly\ninfluenced by the training data than by the number\nof parameters. These results are in line with recent\nstudies (Hoffmann et al., 2024; Xue et al., 2023)\nthat indicate that the quantity of training data is\nmore effective than the number of parameters."}, {"title": "D.2 Language Transfer Capability", "content": "When discussing the cross-lingual transfer capa-\nbility in Sections 4.1 and 4.2, we noted that LLM-\njp-13B-inst (results in Table 5), trained with the\ninstruction-tuning dataset Jaster, which is based\non JGLUE, can make certain inferences even in\nthe zero-shot setting through cross-lingual transfer,\ndespite not being trained on the corresponding En-\nglish data for STS-B and CommonsenseQA. For\nSTS-B, the results are comparable to those dis-\ncussed for Llama2-13B in Section 4.1, demonstrat-\ning similar transfer performance from Japanese to"}, {"title": "D.3\nDecoding Methods (Details)", "content": "In the zero-shot setting shown in Table 3 and Ta-\nble 5, constrained decoding with regular expres-\nsions generally achieves higher performance than\ngreedy decoding. However, in the fine-tuning set-\nting shown in Table 4 and Table 6, greedy decod-\ning generally achieves higher performance than\nconstrained decoding. Therefore, especially when\nevaluating the zero-shot setting, it is reasonable to\nuse constrained decoding to eliminate errors due to\ndifferences in output formats."}, {"title": "D.4 How Many Examples Are Required for\nAdequate Evaluation in the Fine-Tuning\nSetting?", "content": "We investigated the number of sentences required\nfor the fine-tuning setting to evaluate the NLU per-\nformance of LLMs. Figure 6 shows the evalua-\ntion scores when fine-tuning StableLM-ja-7B-inst\nand ELYZA-Llama-2-7B-inst with 1, 10, 100, 500,\n1000, and 10000 sentences13. Thin lines represent\nthe results of fine-tuning with each number of sen-\ntences only once, while thick lines represent the\nresults of repeated fine-tuning with the respective\nnumber of sentences to achieve a total of 1000 sen-"}, {"title": "E Sharpe Ratio (Details)", "content": "The Sharpe ratio is used as a measure of the risk-\nadjusted return of an investment. The Sharpe ratio\ncan be expressed as follows:\nSharpe ratio = \\frac{R_p - R_f}{\\sigma_p},\nwhere Rp is the return of the portfolio, Rf is the\nrisk-free rate, and op is the standard deviation of\nthe portfolio return.\nWhen applying this concept to our evaluation,\nthe return of the portfolio Rp corresponds to the\naverage of the evaluation scores \u00b5score, the risk-\nfree rate Rf corresponds to the chance rate, and the\nstandard deviation of the portfolio return op corre-\nsponds to the standard deviation of the evaluation\nscores for each template score. Since the chance\nrate is constant for each task, we can ignore it. We\ntherefore define the Sharpe score as Equation (1).\nThe default parameter of a is set to 1.0 in Sharpe"}, {"title": "F Example of Japanese Instruction\nTemplate", "content": "Figure 7 shows examples of the dataset creation\nprocess for JNLI tasks. We created Japanese JNLI\ntemplates by manually translating the MNLI tem-\nplates corresponding to the English tasks, as shown\nin Figure 1. For instance, JNLI provides pairs of\nsentences, a premise, and a hypothesis. We then ap-\nply each instruction template to these sentence pairs\nto create natural language sentences to be used as\ninput sequences. The expected output format for\nanswers follows FLAN. We convert the answer la-\nbels to conversational text and instruct the LLMs to\ngenerate only the corresponding number or letter."}]}