{"title": "PyTerrier-GenRank: The PyTerrier Plugin for Reranking with Large Language Models", "authors": ["Kaustubh D. Dhole"], "abstract": "Using LLMs as rerankers requires experimenting with various hyperparameters, such as prompt formats, model choice, and reformulation strategies. We introduce PyTerrier-GenRank, a PyTerrier plugin to facilitate seamless reranking experiments with LLMs, supporting popular ranking strategies like pointwise and listwise prompting. We validate our plugin through HuggingFace and OpenAI hosted endpoints.", "sections": [{"title": "Introduction", "content": "Generative reranking methods have recently gained popularity for their ability to produce a more optimal ranking of documents by prompting large language models. These methods are also employed in Retrieval Augmented Generation (Dhole, 2024) paradigms as they are effective for enhancing generation through the incorporation of retrieved information. Typically, a sparse retriever such as BM25 (Robertson et al., 1992) or a dense retriever like ColBERT (Khattab and Zaharia, 2020) is utilized to retrieve a small subset (ranging from 20 to 1000) from among millions or billions of text pieces. This subset is then forwarded to a second-stage generative reranker for reranking.\nThe increasing adoption of such reranking methods necessitates rigorous testing of various hyperparameters, including employing different prompts, evaluating performance with different model weights, and using it in conjunction with various retrieval, reranking, and reformulation (Dhole et al., 2024a,b) pipelines.\nIn that regard, we introduce PyTerrier-GenRank to encourage rapid experimentation of retrieval pipelines employing recently popular LLM-based reranking methods. The code includes PyTerrier (Macdonald and Tonellotto, 2020) wrappers over many helper functions of the RankLLM repository (Pradeep et al., 2023a,b). Our plugin can be quickly installed in any Python environment and is available at this link\u00b9."}, {"title": "Generative Reranking", "content": "Large Language Models (LLMs) have been widely used for reranking tasks in various ways involving novel hyperparameters. Among the most popular ones have been ranking documents in a pointwise, pairwise and listwise fashion. MonoBERT (Nogueira et al., 2019) and MonoT5 (Nogueira et al., 2020) are among the earliest pointwise ranking methods that were trained to emit a single scalar relevance score for a query-document pair. In contrast, pairwise approaches (Qin et al., 2024) involved prompting models with a query and two documents to compare and rank them. RankVicuna (Pradeep et al., 2023a), RankZephyr (Pradeep et al., 2023b), and RankGPT (Sun et al., 2023; OpenAI, 2023) have explored listwise ranking strategies by prompting their respective models with a list of documents and generating a ranked list of document IDs. These paradigms have also been investigated in the context of recommender systems (Yue et al., 2023).\nThe number of hyperparameters involved in creating an optimal retriever, the choice of prompts,"}, {"title": "Python IR Toolkits", "content": "We choose PyTerrier (Macdonald and Tonellotto, 2020) due to its modularity and Python-friendly interface. It is built on the Terrier search engine and excels in modular, rapid experimentation for IR research, allowing users to easily integrate and evaluate various retrieval and re-ranking models. It has been employed in various studies to evaluate retrieval effectiveness (Dhole and Agichtein, 2024b; Datta et al., 2024; Parry et al., 2024; Datta et al., 2022; Dhole and Agichtein, 2024a)."}, {"title": "Generative Reranking Wrappers", "content": "Our plugin employs PyTerrier wrappers over reranking class functions modified from the Pyserini (Lin et al., 2021) based RankGPT and RankLLM repositories. We allow additional hyperparameters to be expressed from function calls. We provide sample code to execute HuggingFace (Wolf et al., 2020) models and OpenAI-based endpoints."}, {"title": "Sanity Check", "content": "As a sanity check, we re-evaluate some previously published rerankers like RankVicuna, RankGPT, using this framework. Those marked with superscript have already been previously tested with the Pyserini framework in their respective papers.\nWe also evaluate the zero-shot retrieval effectiveness of newly published instruction-tuned models like LLama-Spark.\nWe find that among the 8B zero-shot approaches, LLama-Spark is the most effective as a reranker.\nThe complete results are shown in Table 1."}, {"title": "Ethics Statement", "content": "Large Language Models are embedded within a broader socio-technical framework (Dhole, 2023; Dhole et al., 2023). When deploying them for tasks such as ranking and recommendations, it's crucial to assess their long-term impacts, including the potential for creating echo chambers and the risk of generating biased suggestions."}]}