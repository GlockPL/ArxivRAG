{"title": "SLM Meets LLM: Balancing Latency, Interpretability and Consistency in Hallucination Detection", "authors": ["Mengya (Mia) Hu", "Rui Xu", "Deren Lei", "Yaxi Li", "Mingyu Wang", "Emily Ching", "Eslam Kamal", "Alex Deng"], "abstract": "Large language models (LLMs) are highly capable but face latency challenges in real-time applications, such as conducting online hallucination detection. To overcome this issue, we propose a novel framework that leverages a small language model (SLM) classifier for initial detection, followed by a LLM as constrained reasoner to generate detailed explanations for detected hallucinated content. This study optimizes the real-time interpretable hallucination detection by introducing effective prompting techniques that align LLM-generated explanations with SLM decisions. Empirical experiment results demonstrate its effectiveness, thereby enhancing the overall user experience.", "sections": [{"title": "1 Introduction", "content": "Despite Large Language Models (LLMs) having impressive capabilities (Zhou et al., 2020; Wang et al., 2021, 2020; Pagnoni et al., 2021; Dziri et al., 2021), they are prone to hallucinations\u2014responses that are ungrounded from the source (Rashkin et al., 2021; Maynez et al., 2020; Nan et al., 2021; Liu et al., 2023; Shi et al., 2023; Wei et al., 2022)- undermining their reliability and making hallucination detection critical (Kaddour et al., 2023; Pal et al., 2023).\nConventional hallucination detection methods, such as classification (Kry\u015bci\u0144ski et al., 2019; Zhou et al., 2020; Zha et al., 2023) or ranking (Falke et al., 2019) models, have been effective in their domains but often lack interpretability, which is an essential for user trust and mitigation (Rudin et al., 2022). Given the recent widespread adoption of LLMs, researchers have explored using LLMs for hallucination detection (Lei et al., 2023; Lin et al., 2021; Min et al., 2023; M\u00fcndler et al., 2023), utilizing techniques like chain-of-thought reasoning (Marasovi\u0107 et al., 2021; Kunz and Kuhlmann, 2024; Turpin et al., 2024; Shen et al., 2023), or fine-tuning an autonomous detection agent at Billion-parameter size (Cheng et al., 2024), or checking consistency of different LLM responses per question (Manakul et al., 2023). While LLM-based methods provide interpretability, they introduce latency challenges, due to their enormous size and the computational overhead of processing long source texts (Becker et al., 2024; Jiang et al., 2024). This creates a major challenge for latency-sensitive real-time applications.\nWe propose a novel workflow to address this challenge by balancing latency and interpretability. Our approach combines a small classification model, which in our case is a small language model (SLM), for initial hallucination detection. A downstream LLM module, termed a \"constrained reasoner,\" then explains the detected hallucinations. This process is illustrated in Figure 1. Considering the relatively infrequent occurrence of hallucinations in practical use (Cao et al., 2021; Wu et al., 2023; Gu et al., 2020), the average time cost of using LLMs solely for reasoning on hallucinated texts is manageable. Additionally, this approach leverages the pre-existing reasoning and explanation capabilities of LLMs(McCoy et al., 2023), obviating the need for substantial domain-specific data and significant computational cost on fine-tuning.\nConventional studies have employed LLMs as end-to-end solutions (Sobania et al., 2022; Goyal et al., 2022). More recently, Shi et al. explored the ability of LLMs to explain small classifiers through their latent features, showing promising results on non-reasoning tasks. In this study, we propose a novel framework to effectively apply this approach"}, {"title": "2 Problem Definition", "content": "We denote the Grounding Source as X and the model generated hypotheses Y = (Y1, Y2, \u2026, Yn). The generation process can be expressed as a function F : X \u2192 Y, where F is the text generation model (e.g., summarization model). yi, where i \u2208 [1, n], is hallucinated if conflicts with or cannot be verified against X.\nTo balance latency and interpretability in hallucination detection, we propose a novel two-stage framework: a SLM for hallucination detection followed by a LLM-based reasoning module, termed \"constrained reasoner\". The upstream detection can be formulated as: D : (X,Y) \u2192 J where J = (j1, j2, ..., jn) represents the binary labels decided by the detector D. The subset of response sentences Y detected as hallucinations by D is denoted as H = {yk \u2208 Y | jk = hallucination} = (h1,..., hm), where m \u2264 n. Only detected potential hallucinations H are passed to downstream reasoning module. The constrained reasoner R provides explanations for hallucinations flagged by upstream, R : (X, H) \u2192 E, where E = (\u20ac1,...,em) contains m explanations, each ek, where k \u2208 [1,m] corresponding to a hallucinated sentence he detected by D. R is called constrained reasoner because it operates under the given constraint that hi is hallucinated, as determined by D.\nHowever, even in self-rationalization models, reasoning results E may not align with detection results J even they are generated together (Wiegreffe et al., 2021; Ye and Durrett, 2022). The inconsistency can be more pronounced in the two-stage frame, where explanations are provided post hoc. We define the real intention in explanation E as S = ($1,..., $m). Reasons inconsistent with the upstream decision is thus {ek \u2208 E, where Sk = non-hallucination} (as our framework only passes R the detected hallucinations to explan due to the latency concern). There are three aspects we want to study regarding the consistency of the constrained reasoner R:"}, {"title": "Inconsistency Identification", "content": "We design a flagging mechanism to ask LLM-based R to signal when it judges the hypothesis as non-hallucination and thus unable to provide explanation why the hypothesis is hallucinated. Therefore, ek is semi-structured consisting of a free-text reason tk and a flag sk indicating whether R thinks the text is hallucination. Formally, ek = (tk, \u015dk). We conduct human evaluation, by asking annotators to careful read tk and mark sk whether the reason is explaining the hypothesis is hallucination. Then, we measure effectiveness of the flagging mechanism."}, {"title": "Inconsistency Filtering", "content": "The simplest mitigation for inconsistent reasonings is to filter them out. We assess the reduction of inconsistencies after filtering flagged explanations, i.e. ones with Sk = non-hallucination. We compare the remaining true inconsistency rates, i.e. the rate of Sk = non-hallucination as baseline."}, {"title": "Reasoning Feedback", "content": "The ground truth label for each yi is gi, but in practice, ji may differ from gi due to SLM imperfections. We explore the potential of R as a feedback mechanism to improve D. We compare the flagged inconsistencies, \u015dk, against the ground truth gk to assess R's performance in identifying non-hallucinations."}, {"title": "3 Experiment", "content": "Our experiment is designed to study the consistency of reasoning within the proposed hallucination detection framework and effective approaches to filter inconsistencies. Additionally, we explore the potential of LLMs as feedback mechanisms for refining the detection process. We employ GPT4-turbo as R to elucidate the rationale behind hallucination determinations, using the temperature of 0 and top-p of 0.6. The experiments are conducted across four datasets: NHNET (Shen et al., 2023), FEVER (Thorne et al., 2018), HaluQA and HaluSum (Li et al., 2023). We use complete test set of NHNet. Due to the size of rest three datasets and GPT resource limitations, we sample 3000 data per dataset for experimentation.\nTo simulate an imperfect SLM classifier, we sample both hallucinated and non-hallucinated responses from the datasets, assuming the upstream label as hallucination. Thus the groundtruth hallucinated text are the simulated true positive cases, and the groundtruth non-hallucinated texts are the the simulated false positive cases. The specific ratio of true and false positives from the SLM is"}, {"title": "3.1 Methodology", "content": "The experiment focuses on three primary approaches, with their key distinctions summarized in Table 1 (The full prompts are provided in Appendix A.2).\nVanilla approach simply instructs R to explain why the text was detected as hallucination by D. It does not address how to handle inconsistency, i.e. disagreements with the upstream decision. As the reasonings are free-text, there is no straightforward mechanism to identify when inconsistencies arise. If contradictory explanations are generated, they will be presented to the user, which can undermine user trust and experience. It is served as a baseline for Inconsistency Filtering comparison.\nFallback approach introduces a flagging mechanism whereby R can respond with \"UNKNOWN\" to indicate $k = non-hallucination thus it cannot provide a suitable explanation. This flag helps signal potential inconsistencies, enabling developers to address them effectively.\nCategorized approach refines the flagging mechanism by incorporating more granular hallucination categories. These categories are derived from the analysis of real hallucination data. Among those, a specific category hallu12 is used to signal inconsistencies where \u015dk = non-hallucination. By exposing the reasoner to these detailed categories, the goal is to enhance R's understanding of hallucinations and improve its ability to correctly identify true hallucinations."}, {"title": "4 Result and Discussion", "content": "Inconsistency Identification Table 2 illustrates the performance of identifying real inconsistent reasonings using the designed flags. Both methods demonstrate strong precision. However, the Fallback approach exhibits poor recall, i.e. often failing to signal inconsistent reasons with the designed \"UNKNOWN\" flag. In contrast, Categorized approach effectively categorized the majority of inconsistent reasonings under the hallu12 flag, making it easier to filter or mitigate them for downstream usage.\nInconsistency Filtering Filtering reasonings with the designed flag effectively reduced inconsistencies between the upstream detection and constrained reasoner R, as illustrated in Figure 2. The Vanilla approach, as expected, showed a high inconsistency rate. While the introduction of the \"UNKNOWN\" category in the Fallback approach reduced inconsistencies, its effectiveness was limited by low recall as mentioned above. In contrast, the Categorized approach achieved a dramatic reduction across all datasets, with a post-filtering rate as low as ~ 0.1 \u2013 1%, effectively enhancing the workflow's consistency.\nReasoning Feedback As results shown in Table 3, the Categorized approach demonstrated strong potential as feedback mechanism, outperforming the Fallback method with high recall. It achieves a macro-average F1 score of 0.781. This indicates its capability to accurately identify false positives from the SLM, making it a promising feedback mechanism for improving the upstream model\u2014an area worth further exploration. The high inconsistency rate observed in the Categorized approach before filtering, as shown in Figure 2, highlights the ability of LLMs like GPT to accurately identify true hallucinations when refined hallucination categories are provided, as indicated by the high F1 in Table 3. This suggests that LLM can maintain correct judgments without being easily influenced or swayed by specific instructions."}, {"title": "5 Conclusion", "content": "In this study, we introduce a practical framework for efficient and interpretable hallucination detection by combining SLM for detection with LLM for constrained reasoning. Our Categorized prompting strategy with filtering effectively aligns LLM explanations with SLM decisions, empirically proven effective on 4 hallucination and factual consistency datasets. Furthermore, this strategy shows promise as a feedback mechanism for refining SLMs, offering a path toward more robust and adaptive systems. While our experiments focus on real-time interpretable hallucination detection, the insights gained are broadly applicable, shades lights in improving classification decision systems and enhancing SLM capabilities through LLM-based constrained interpretation."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Data distribution", "content": "The distribution of hallucinated and non-hallucinated examples in each dataset is shown in Table 4."}, {"title": "A.2 Constrained Reasoner Approaches", "content": ""}, {"title": "A.2.1 Vanilla prompt", "content": "Vanilla prompt shown in 3 only gives the instruction and few-shot examples to do the downstream reasoning task. However, it does not specify how LLM should deal with the situation where LLM does not follow the upstream decision."}, {"title": "A.2.2 Fallback Prompt", "content": "Fallback prompt shown in 4 gives LLM an alternative route when it does not agree with the upstream decision and will give inconsistent downstream explanations."}, {"title": "A.2.3 Categorized Prompt", "content": "Categorized prompt shown in 5 gives LLM an alternative route when it does not agree with the upstream decision and will give inconsistent downstream explanations. Moreover, this prompt asks the LLM to categorize the reasons when LLM agrees with the upstream decision as an extra confirmation."}]}