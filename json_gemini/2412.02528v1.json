{"title": "Bias Analysis of AI Models for Undergraduate Student Admissions", "authors": ["Kelly Van Busum", "Shiaofen Fang"], "abstract": "Bias detection and mitigation is an active area of research in machine learning. This work extends previous research done by\nthe authors [1] to provide a rigorous and more complete analysis of the bias found in AI predictive models. Admissions data\nspanning six years was used to create an Al model to determine whether a given student would be directly admitted into the\nSchool of Science under various scenarios at a large urban research university. During this time, submission of standardized\ntest scores as part of a student's application became optional which led to interesting questions about the impact of\nstandardized test scores on admission decisions. We developed and analyzed Al models to understand which variables are\nimportant in admissions decisions, and how the decision to exclude test scores affects the demographics of the students who\nare admitted. We then evaluated the predictive models to detect and analyze biases these models may carry with respect to\nthree variables chosen to represent sensitive populations: gender, race, and whether a student was the first in his/her family to\nattend college. We also extended our analysis to show that the biases detected were persistent. Finally, we included several\nfairness metrics in our analysis and discussed the uses and limitations of these metrics.", "sections": [{"title": "1 INTRODUCTION", "content": "Artificial Intelligence (AI) has started to play an increasingly important role in higher education. AI-based predictive models\nbuilt from existing datasets such as admissions data provide insight into the policies, practices, and biases in U.S. higher\neducation system. At the same time, AI models have their own limitations and potential for harm such as biases towards\ncertain populations. It is therefore important to understand these limitations and biases to truly integrate AI tools in future\nhigher education applications. This paper describes an attempt to build, apply, and analyze Al models using an existing\nstudent admission dataset. An initial analysis of the Al models showed that the models contain bias, so a more rigorous and\ncomplete analysis of the bias was conducted.\nAdmissions data from the School of Science at a large urban research university was used to create machine learning-\nbased Al models. These models predict whether a student would be directly admitted into the School of Science, or not,\nunder a variety of scenarios. The dataset spans six years, and over this time, the admissions policy of the university changed\nfrom requiring students to submit standardized test scores as part of their application, to making test scores optional. Such\nfundamental changes in admission policies can significantly impact our higher education system for different populations, so\nit is important to have a strong understanding of the implications of such policy changes.\nWhen existing data is used to create predictive models, the behaviors and features of these models provide a powerful\nopportunity to better understand both the potential impact of policies on different populations, and the limitations of the\nmodels themselves. This paper attempts to address both issues using a dataset collected from past admission data from the\nSchool of Science. Our analysis will first try to better understand what variables are important in admissions decisions, and\nhow the decision to exclude test scores may affect the demographics of the students who are admitted. The predictive model\ncontains a variety of demographic variables, and three: gender, race, and whether a student was the first in his/her family to\nattend college (\u201cFirst-Generation\u201d), were chosen to represent sensitive populations (\u201csensitive variables\u201d). We then carefully\nevaluate the Al models for presence of potential biases with respect to performance relative to these three variables. The\nresult of this analysis provides some initial evidence that AI algorithms can be harmful when used as part of the admission\ndecision-making process if bias is not effectively mitigated.\nThis paper extends our earlier work [1] in this area in four ways. First, we used a larger dataset. This dataset contains an\nextra year of data, as well as some students who were excluded from the previous dataset. Next, in our previous work we\npresented an analysis for students admitted under the test-required policy and a separate analysis for those admitted under the\ntest-optional policy. In this paper, we redid that analysis using the updated dataset, and added a third analysis where we\nanalyzed the entire dataset with test scores removed. This was to simulate a larger test-optional dataset. Third, we conducted\na more in-depth analysis of the bias found in the Al models to ensure that it persists. Finally, we include a discussion of\nadditional metrics for fairness so that our bias detection is more rigorous and complete.\nStudents who attend large urban universities have some unique challenges. They may be more likely than students who\nattend other types of universities to face the competing demands of work, family, and school leading to poorer academic\nexperiences. Attending a university in a city with a high cost-of-living and the continuing effects of COVID-19 are both\nfactors that can increase the financial demands on students, also leading to increased stress and poorer academic outcomes.\nAlthough urban universities tend to be more diverse, minority students still report struggling with a sense of belonging, which\ncan negatively affect academic performance. To best support all students, care must be taken to understand the impact of\npolicy on equity.\nTest-optional policies were designed to address the concern that standardized test scores are biased metrics for predicting\nstudent success and to increase equity in admissions procedures. But more research needs to be done to fully understand how\nthese policies might change the demographics of the students admitted to the university and other impacts. Identifying the\nimportant factors in admissions decisions and how test-optional policies might change admitted student demographics is the\nfirst goal of this project.\nAs an additional step toward understanding and mitigating bias in admissions policies, college admissions officers are\nincreasingly turning to the use of machine learning-based Al algorithms. However, when Al models are trained using\nexisting datasets, the models can introduce new bias and fairness issues with the potential for harm. This shows a need to\nbetter understand how bias might manifest as Al algorithms become more widely used in higher education. Identifying and\nmeasuring bias in the models built to predict admission decisions is another important goal of this project.\nThis work makes the following contributions:"}, {"title": "2 RELATED WORK", "content": "Students at urban universities face some unique challenges. For example, they are likely to experience the conflicting\ndemands of work, family, and school, which can negatively affect their satisfaction with educational experiences 0. Students,\nespecially minority students, may face barriers that undermine academic achievement, reduce their sense of belonging, and\ninterfere with degree completion 0. Those who attend a university located in a city with a lack of affordable housing may\nstruggle to find appropriate housing which can negatively affect their academic lives, health, and well-being 0. Data\ncollected about the impact of COVID-19 on urban college students showed that decreases in student earnings and household\nincomes led to significant disruptions in students' lives, and that these disruptions had an especially negative impact on first-\ngeneration students 0.\nRecently, many universities have started to experiment with an admissions policy that allows students to apply without\nsubmitting standardized test scores. The hope is that these policies will address concerns of bias in the use of test scores as a\nmetric to predict student success. One survey suggests that test-optional policies are changing enrollment demographics,\nespecially with respect to underrepresented minorities, as Black and Hispanic students are 24% and 21% respectively more\nlikely to apply to a school with a test-optional policy [6]. A second study found that test-optional admission increased the\nfirst-time enrollment by 10-12% of underrepresented minorities, and 6-8% by women 0. Another study of liberal arts schools\nfound that although test-optional policies enhance the perceived selectivity of a school, they did not increase the diversity 0.\nResearchers are also exploring nuances of the policy. For example, what are the implications of giving students the option to\nsubmit test scores, when they are not required? Does it matter that some students choose not to submit scores because they\nare too low, while others choose not to take the test at all 0?\nArtificial intelligence tools, particularly machine learning algorithms, are increasingly used in higher education\napplications. Using AI to assist college admission process can be more objective and efficient 0. Machine learning algorithms\ncan also help universities better understand admission criteria and their impact in the admissions process 0 as well as\nadmissions yield 0. AI can also be used to predict the likelihood of admission for individual students [13-14] in some\nsituations and can sometimes provide evidence for bias in human-centered admissions processes 0.\nWhile AI tools bring many benefits to higher education applications, they can also be biased towards sensitive populations\ndue to the intrinsic bias in existing datasets and in the algorithms themselves 0. There are a variety of frameworks and\ntaxonomies for classifying bias 0 and although some bias can be neutral/unobjectionable, many biases are problematic and\nrequire a response 0. Identifying and categorizing bias in AI is a first step toward creating methods for mitigating bias, an\narea of active research [19-20].\nThe social implications of the use of AI in education are nuanced, providing further motivation for careful understanding\nof the effects of algorithmic bias. For example, student perceptions of fairness with respect to the use of Al in college\nadmissions can affect organizational reputation and the likelihood of students leaving the university [21]. Algorithmic bias\ncan also affect the degree to which people are comfortable accepting AI-based recommendations or adopting AI systems\n[22]. Complicating this understanding is the gap in the literature between technical studies (which include jargon) and\ndescriptive studies (aimed at the lay-person) on algorithmic fairness in higher education [23-24], showing a need for\nscholarly work which bridges these disciplines."}, {"title": "3 METHODS and DATASET", "content": "This research was conducted using admissions data from the School of Science at a large urban research university. Students\nin the dataset were applying for admission from Fall 2017- Spring 2023. The admissions process became test-optional during\nthis period; first-year students applying for admission from Spring 2021 forward could choose not to submit standardized test\nscores. We built various machine learning-based models using different training sets and feature sets to predict which\nstudents were admitted directly into the School of Science (\u201cDirect Admits\u201d). Students who are categorized as \u201cNot Direct\nAdmits\" are those who were admitted into the university but not into the School of Science, and those who were not admitted\nto the university at all.\nPrior to test-optional admissions, Direct Admit decisions were based on grade point average (GPA) and standardized test\nscores. When the admissions process became test-optional, Direct Admit decisions for students who opted to exclude\nstandardized test scores from their application were based on GPA and an assessment of \u201cmath readiness\" based on\nperformance in high-school math courses. The dataset used in this research contains approximately 11,600 students who\nwere required to submit test scores, and approximately 7,900 students who were applying under the new test-optional policy.\nThe features used in the predictive model were: whether the student is a beginning student, the number of campuses to\nwhich the student applied, age, gender, race/ethnicity, whether the student is a first-generation student, whether the student is\nan in-state resident, GPA, and standardized test scores. Standardized test scores include scores from the two prevalent\nAmerican college admissions tests, the ACT (American College Test) and SAT (formerly known as the Scholastic\nAssessment Test, but now the acronym does not stand for anything). ACT scores were normalized to SAT scores. In this\ndataset, ethnicity (Hispanic/Latino) was part of the race variable. Through exploratory analysis, three variables were chosen\nas variables to represent sensitive populations for further analysis: gender, race/ethnicity, and first-generation students.\nThis study consisted of an analysis of three groups of data:\n\u2022\tGroup 1: Data over all six years.\n\u2022\tGroup 2: Test-required cohort (Fall 2017 \u2013 Fall 2020).\n\u2022\tGroup 3: Test-optional cohort (Spring 2021 \u2013 Spring 2023).\nFor each of these groups, we conducted three analyses:\n\u2022\tGPA was included, but standardized test scores were excluded.\n\u2022\tStandardized test scores were included, but GPA was excluded.\n\u2022\tBoth GPA and test scores were included.\nFinally, each analysis was repeated three times: once where the sensitive variable was gender, once where the sensitive\nvariable was race, and once where the sensitive variable was first-generation students. The goal was to understand how\nchanging the value of a single sensitive variable affected the accuracy of the models. A summary of the findings is illustrated\nin Figure 1. In addition, we note that the test-optional cohort contains only two-and-a-half years of data, and that recent data\nis confounded by the pandemic."}, {"title": "4 RESULTS AND DISCUSSION", "content": "The results presented here are based on the accuracy of the Al predictive models in various scenarios. An exploratory\nanalysis of the data and corresponding AI models found that:\n1. Standardized test scores play a major role in admissions decisions.\n2. Gender and First-Generation are two other variables that play prominent roles in admissions decisions.\n3. Many students who would not be admitted under test-required policies would be admitted under test-optional policies,\nincluding more students from sensitive populations.\nWe also evaluated the models for bias with respect to sensitive variables. The results were:\n1. Gender: The model for the larger test-optional dataset predicts women will be incorrectly admitted more than men\n(Specificity bias).\n2. First-Generation: The model for the test-required cohort and the larger test-optional dataset predicts that non-first-\ngeneration students will be incorrectly admitted more than first-generation students (Specificity bias), and that first-\ngeneration students will be incorrectly rejected more than non-first-generation students (Sensitivity bias). In addition,\nfor the larger test-optional dataset, there is a discrepancy in the average predicted probability for admission (Balance for\nthe Positive Class).\n3. Race/Ethnicity: All three models predict that white students will be incorrectly admitted more than non-white students\n(Specificity bias). The model for the test-required cohort and the model for the larger test-optional both predict that non-\nwhite students will be incorrectly rejected more than white students (Sensitivity bias). There is also a discrepancy\nbetween the average predicted probability for rejection (Balance for the Negative class)."}, {"title": "4.1 General Analysis", "content": "The general analysis focused on three scenarios:\n1.\tThe test-required cohort (includes both GPA and standardized test scores; 11,600 students)\n2.\tThe test-optional cohort (includes GPA but excludes test scores; 7,900 students)\n3.\tThe full dataset (includes GPA, but excludes test scores; 19,500 students)\nScenarios 1 and 2 were chosen because they are real-life scenarios. Scenario 3 was chosen to simulate a more robust test-\noptional dataset. The predictive accuracy for Scenario 3 is 80%, lower than for the other two scenarios (as expected), but still\nvery good."}, {"title": "4.2 Bias in Predictive Models", "content": "In the process of identifying bias with respect to three sensitive variables in the predictive models, the focus was initially on\ntwo scenarios: the test-required cohort where the predictive model includes both GPA and test scores and the test-optional\ncohort where the predictive model excludes test-scores. Because the test-optional dataset only includes two-and-a-half years'\nworth of data, we added a third scenario where we create a predictive model of the entire dataset with test-scores excluded, to\nsimulate a larger test-optional dataset. Note that this larger test-optional dataset may potentially have some demographic\ndifferences when compared to the true test-optional cohort of students, as simply removing test scores from the entire dataset\ndoes not account for the effects of a change in policy on the type of applicants it attracts. This dataset is intended to be used\nonly to evaluate predictive models with a larger dataset and should not be used to infer policy implications.\nTwo of the sensitive variables are Gender and First-Generation, two variables with high permutation importance in both\nmodels. The third sensitive variable chosen was Race/Ethnicity, which was categorized as White and Non-White. The bias\nwe tried to detect was a difference of 5% or more in overall prediction accuracy, or in Specificity or Sensitivity, between\nvalues of the sensitive variable. We did not find any significant differences in overall prediction accuracies between the\nsensitive variables for any of the three models. Differences in Specificity and/or Sensitivity were present in all models. In\nthis application, Specificity is a measure of students who were incorrectly admitted, and sensitivity is a measure of students\nwho were incorrectly denied admission."}, {"title": "4.3 Aggregate Bias", "content": "It is important to note that the results presented above in Section 4.2 are a snapshot. For example, to examine bias in the test-\nrequired cohort when Gender is the sensitive variable, the test-required cohort was divided into training and testing datasets\nonce and a model was created once. However, the overall accuracy of the predictive model generated for the test-required\nand test-optional cohorts is approximately 88%, and the overall accuracy of the predictive model generated for the test-\noptional cohort is approximately 80%. This is very high, which means that the number of outliers is small. This means that\nthe distribution of outliers in the training and testing datasets may have a meaningful impact on the bias observed in the\nresults.\nTo better understand the effects of outliers on the observed bias, we conducted an analysis of the stability of the bias for\neach model to ensure that any bias observed would persist regardless of how the dataset under examination was divided into\ntraining and testing datasets. For each model, we ran repeated trials. For each trial, two-thirds of the data was randomly\nselected for training, using proportionate stratified sampling with respect to the sensitive variable. We recorded the mean\nvalues for Specificity and Sensitivity, as well as the mean values for the Brier Score, Balance for the Negative Class and\nBalance for the Positive Class.\nThe results for Gender are shown in Table 7. The model for the larger test-optional dataset predicts women will be\nincorrectly admitted more than men (Specificity bias)."}, {"title": "5 CONCLUSIONS", "content": "This research analyzed admissions data at a large urban research university using machine learning-based AI models to\npredict whether a given student was directly admitted into the School of Science. The dataset is interesting because the\nuniversity's admissions policies changed from test-required to test-optional during this period. This research contributes to a\nbetter understanding of the variables that play an important role in admissions decisions and shows that there is a significant\nchange in admitted student body under a test-optional policy. In addition, the predictive models create new bias with respect\nto different populations under sensitive variables of gender, race, and first-generation. The conclusions we can draw from\nthis study include:\n\u2022\tTest-optional policies may significantly alter the demographics of the students who are admitted. The impacts of this\nneed additional study.\n\u2022\tAl models are increasingly used to create and understand policies in higher education, so it is critical to carefully analyze\nthese models for errors and biases. Further, the bias may not be apparent when examining the overall accuracy of the\nmodel, so more thorough evaluations must be done.\n\u2022\tAlthough policies may reflect social bias, separate metrics for bias and fairness must be developed when evaluating AI\nmodels themselves. Additionally, while useful, there are limitations to bias and fairness metrics that need to be better\nunderstood.\nOne avenue for future work in this area is to analyze a more expansive dataset. This research focused on School of Science\ndata, but are the results the same when applied to a university-wide dataset? In addition, the test-optional policy is recent,\nand this dataset only includes about two years of test-optional data. Continuing this analysis over the next few years could\nprovide some interesting insight and more robust results. Similarly, test scores for the test-optional cohort were excluded\nentirely in this analysis. The overall accuracy of these models was high, so this was sufficient for the models, though not\nentirely reflective of reality. Under a test-optional policy, test-scores can be included in admissions decisions, if this is what a\nstudent prefers. A model that includes this nuance may lead to additional insights about the effects of test-optional policies.\nA second area for future work is in bias mitigation. When bias in Al models is detected, how can it be corrected or\nmitigated? For example, can carefully designed adjustments to the training set improve bias and fairness metrics in the\nmodel? Additional questions include: Are there scenarios under which it should not be corrected? What is the interplay\nbetween social bias reflected in admissions policies, and bias in Al models? etc.\nAs AI algorithms become more widely used, they can lead to greater accuracy, consistency, time-savings, and\nunderstanding in many domains. Al is a powerful tool and must be used with the understanding that as it is being used to\nsolve problems, it can also introduce new problems. Techniques for identifying, measuring, and mitigating these problems\nare critical."}]}