{"title": "LABITS: Layered BIDIRECTIONAL Time SURFACES\nREPRESENTATION FOR EVENT CAMERA-BASED CON-\nTinUOUS DENSE TRAJECTORY ESTIMATION", "authors": ["Zhongyang Zhang*", "Jiacheng Qiu*", "Shuyang Cui", "Yijun Luo", "Tauhidur Rahman"], "abstract": "Event cameras provide a compelling alternative to traditional frame-based sen-\nsors, capturing dynamic scenes with high temporal resolution and low latency.\nMoving objects trigger events with precise timestamps along their trajectory, en-\nabling smooth continuous-time estimation. However, few works have attempted\nto optimize the information loss during event representation construction, impos-\ning a ceiling on this task. Fully exploiting event cameras requires representations\nthat simultaneously preserve fine-grained temporal information, stable and char-\nacteristic 2D visual features, and temporally consistent information density-an\nunmet challenge in existing representations. We introduce Labits: Layered Bidi-\nrectional Time Surfaces, a simple yet elegant representation designed to retain all\nthese features. Additionally, we propose a dedicated module for extracting active\npixel local optical flow (APLOF), significantly boosting the performance. Our ap-\nproach achieves an impressive 49% reduction in trajectory end-point error (TEPE)\ncompared to the previous state-of-the-art on the MultiFlow dataset. The code will\nbe released upon acceptance.", "sections": [{"title": "1 INTRODUCTION", "content": "As an emerging visual modality, event cameras offer unique and practical advantages. Compared to\nconventional frame-based cameras, they provide higher temporal resolution, greater dynamic range,\nhigher efficiency, and lower latency (Gallego et al. (2020)). Furthermore, under stable lighting, event\ncameras are primarily sensitive to the edges of moving objects, naturally filtering out stationary\nobjects while tracking moving ones. Their ultra-high temporal resolution also enables smoother and\nmore continuous target tracking. In recent years, numerous papers leveraging this feature of event\ncameras have addressed topics such as feature tracking (Messikommer et al. (2023)), optical flow\ngeneration (Wan et al. (2024)), and video interpolation (He et al. (2022)) based on events.\nFrom an event camera's perspective, each moving point generates a discrete trajectory in the xyt\nspace, with each triggered event representing a sampled point on this trajectory, along with its times-\ntamp. The instantaneous velocity at any point on the trajectory can be calculated from the relative\npositions and times of these events using straightforward mathematical formula v = \u2206x/\u2206t, align-\ning with intuitive understanding. While real-world factors like rotations, depth movements, and\nmultiple moving objects complicate pixel-wise speed, the trajectory information persists embedded\nwithin the event streams. Our target is to unearth these hidden treasures.\nCurrently, there are two main approaches to utilizing events. One is to directly construct events into a\ngraph in the xyt space and input it into a GNN (Graph Neural Network) (Li et al. (2021)) or treat each\nevent as a spike to be input into an SNN (Spiking Neural Network) (Kosta & Roy (2023)). The other\nis to first convert events into a dense representation and then input them into an Artificial Neural\nNetwork (ANN). Although GNNs appear efficient, constructing the graph is computationally and\nmemory-intensive with increasing events, and GNNs suffer from over-smoothing as network depth\nincreases (Chen et al. (2020)). Additionally, the graph conversion discards fine-grained spatial and\ntemporal information, making position-sensitive scenarios problematic. These limitations restrict\nGNN applications in event-based vision, especially for low-level tasks. On the other hand, while"}, {"title": "2 RELATED WORKS", "content": "Event Camera: Event cameras contain a bio-inspired dynamic vision sensor, where each pixel\nunit works asynchronously and triggers an event instantly when it detects a log-intensity change\nover a predefined threshold. Each event e = (t, x, y, p) records the spatial coordinate (x, y) of the\ncorresponding pixel position on the image sensor plane, the microsecond-level shooting timestamp\nt, and a binary polarity value p that indicates the direction of brightness change (Zhang et al. (2024)).\nEvent cameras are widely used in various computer vision and robotics tasks. They excel in motion-\ncentric tasks like optical flow estimation and object or human pose tracking, as demonstrated in\nnumerous studies (Hu et al. (2022); Wu et al. (2024); Chamorro Hern\u00e1ndez et al. (2020); Zhang\net al. (2023)). Their high temporal resolution and event-driven operation also enable innovative\nvideo processing techniques, including frame interpolation (Tulyakov et al. (2022); Sun et al. (2023);\nLiu et al. (2024)) and motion deblurring (Chen et al. (2024); Yang et al. (2024); Kim et al. (2024)).\nThese applications leverage the unique characteristics of event cameras for detailed, dynamic scene\nanalysis without the high data demands of traditional high-speed video recording.\nEvent Representations: In event-based vision, mainstream methods convert the event stream into\na dense representation, then pass it to ANNs for various tasks (Bardow et al. (2016)). However,\nexisting representations often fail to fully leverage the unique capabilities of event cameras.\nEarly representations attempt to convert event streams into intensity frames, highlighting moving\nedges and mimicking 2D features of traditional cameras. The event count representation (Maqueda\net al. (2018); Zhu et al. (2018b)) sums the number of events per pixel within a time window, while\nthe event frame (Rebecq et al. (2017)) sums event polarities. Both discard temporal information, ob-\nscuring events over time. The voxel grid representation (Zhu et al. (2019)) quantizes time and maps\nevents to temporal grids using bilinear sampling. While better than event count, time information\nretention is still limited, and temporal obscuring persists.\nTime surface-style representations form another key branch. The original time surface, or Surface of\nActive Events (SAE, Benosman et al. (2013)), encodes only the most recent event's timestamp per\npolarity at each pixel, disregarding prior events, no matter the scene's complexity. This leads to poor\n2D pattern capture and temporal occlusion, where newer events overwrite earlier ones. Variants like\nthe averaged time surface (Sironi et al. (2018)) reduce noise and mitigate occlusion, but reintroduce\ntemporal obscuring and ambiguity. TORE volume (Baldwin et al. (2022)) preserves the most recent\nK events per pixel, creating multi-layer time surfaces. However, redundancy arises when the same\nobject repeatedly triggers recent events at the same pixel, perpetuating temporal occlusion. This is\npartly due to the complex textures of real-world objects, which introduce numerous small edges that\ntrigger events. All aforementioned representations, and others, are summarized in Table 2.\nTrajectory Estimation: Continuous-time trajectory estimation was initially proposed for rolling\nshutter compensation Kerl et al. (2015). More closely related to our work is the regression of pixel\ntrajectories, aligned with high-speed feature tracking in event cameras Gehrig et al. (2020); Alzu-\ngaray & Chli (2020). This work connects to methodologies described in Gehrig et al. (2024), where\nfeatures are continuously tracked via the integration of B\u00e9zier curves, correlation map sequences,\nand image data. However, unlike our approach, their solution emphasizes visual pattern-based cor-\nrelation while neglecting the fine-grained temporal information inherent in raw events, often leading\nto erroneous trajectory estimations."}, {"title": "3 LABITS: LAYERED BIDIRECTIONAL TIME SURFACE", "content": "Event cameras have distinct advantages over traditional frame-based cameras due to their asyn-\nchronous nature. This characteristic enables event cameras to provide highly precise timestamps at\nthe microsecond level, making them ideal for capturing instantaneous motion velocity. However, ex-\nisting event representations fail to fully exploit these advantages. Most approaches convert the sparse\nand discrete event streams into frame-like structures that highlight 2D visual features such as moving\nedges and patterns, thus making the event modality compatible with conventional computer vision\nmodels. The transformation essentially forces event cameras to conform, fitting themselves into the\nframework dominated by frame-based cameras, rather than fully leveraging their own strengths and\nshowcasing the unique advantages that conventional cameras cannot replicate.\nBuilding event representations inevitably involves information loss. It's almost a trilemma to faith-\nfully preserve the original fine-grained timestamp information, compile meaningful 2D visual pat-\nterns, and maintain historical movement information at intermediate times simultaneously. How-\never, all three aspects are essential for accurate, dense, continuous-time trajectory estimation. Fine-\ngrained event-level timestamp information is the unique strength of the event modality in movement\nprediction. 2D visual patterns form the basis for correlation-based tracking mechanisms, while inter-\nmediate movement information ensures stable estimation accuracy throughout the entire trajectory.\nTherefore, we designed Labits: Layered Bidirectional Time Surfaces to meet these demands. The\nlogic behind this representation is straightforward: a time-surface-like structure is essential for re-\ntaining the microsecond-level timestamp features. The accumulation time range of events should\nbe strictly controlled to avoid issues with visual feature blurriness, so splitting the target time range\ninto a series of smaller time bins is essential. Stopping at this stage creates a layered time surface.\nHowever, under this scheme, only the last events' information is utilized within each time bin at\neach pixel, which is unpreferable. Moreover, when estimating the local speed at each time bin inter-\nsection, only past movement information is considered, so the prediction here is based on backward\ndifference, where the error estimation is known to be O(dt). Further reducing this error estimation\nis nothing difficult: simply consider future event if no event is observed in the past dt search range.\nThis change in representation is simple yet powerful: First, pixels ahead of the moving edges' direc-\ntion often don't have triggered events in the near past; by incorporating future events, these originally\nempty pixels are assigned a meaningful value, thereby increasing the information density of the re-\nsulting representation. Second, the first and last event within each time bin at each pixel are both\nfully utilized. Especially when the time range is small, this sampling strategy is highly representa-\ntive. Third, considering near past and future events simultaneously changes the basic representation\nunit from time bin to intermediate probe times. Local speed estimations at these probe times use\ncentral difference, reducing the estimation error to O(dt\u00b2) (detailed in the Supplementary Material).\nLet $E = \\{(t_n, x_n, y_n, p_n)\\}_{n=1}^{N}$ represent the event stream, where $t_n$ denotes the timestamp, $(x_n, y_n)$\nare the spatial coordinates, and $p_n$ is the polarity of the n-th event. The variables $T_{start}$ and $T_{end}$ refer\nto the first and last event timestamps, respectively, and $T_{total}$ is the total time duration of the events.\nThe time interval between two adjacent probe times is denoted as $T_{range}$, and $T_i$ denotes the i-th probe\ntimestamp. The output tensor $L \\in \\mathbb{R}^{B \\times H \\times W}$ is the Labits representation, where B is the number\nof probe time points, H and W is the sensor height and width. The Labits generation algorithm is\nelaborated in Algorithm 1.\nAs shown in Algorithm 1, the backward and forward temporal search range at each probe time point\nis $t_{range}$, which corresponds to the time interval of each time bin. Under this scheme, the timestamp\nof the earliest and latest events within each time bin are recorded by the Labits layers at the two\nadjacent probe points surrounding the bin, further minimizing information loss.\nMoreover, the design of Labits ensures that it maintains stability and consistency. The values in the\neach layer of Labits always lie within a fixed range, making Labits inherently robust against outliers,\nsuch as hot pixel noise, which can otherwise distort the normalization scale in other representations.\nWith Labits, dense instantaneous optical flows at active pixels can be generated across evenly-spaced\nprobe times using straightforward neural networks, due to the clear correlation between Labits layer\nvalues and their corresponding local speeds. The resulting layered instantaneous optical flows could\nserve as a basis for future research into tasks requiring high temporal resolution and advanced motion\nunderstanding, such as event-based object or human pose tracking."}, {"title": "4 METHOD", "content": "4.1 LABITS-TO-APLOF NET\nEvent cameras are highly sensitive to motion, particularly instantaneous or local motion. In this\ncontext, \"local\" refers to both spatial and temporal locality, aspects not captured by conventional\noptical flow techniques. For instance, in the MultiFlow dataset, the optical flow ground truth at each\nintermediate probe time is relative to the initial reference time point, representing cumulative motion\nrather than instantaneous speed. In contrast, events triggered by the same moving object within a\nshort time frame are closely tied to the object's instantaneous speed, based on the variation in their"}, {"title": "5 RESULTS", "content": "Datasets and Evaluation Metrics. Our model is trained and evaluated on the MultiFlow dataset\n(Gehrig et al. (2024)), which comprises 10,100 training and 2,000 test sequences, each including\npaired RGB images, events, and optical flows relative to a reference time. With optical flow ground\ntruths recorded every 10 ms, we can generate intermediate local optical flows for training the Labits-"}]}