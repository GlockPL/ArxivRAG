{"title": "A Review of Fairness and A Practical Guide to Selecting Context-Appropriate Fairness Metrics in Machine Learning", "authors": ["Caleb J. S. Barr", "Olivia Erdelyi", "Paul D. Docherty", "Randolph C. Grace"], "abstract": "Recent regulatory proposals for artificial intelligence emphasize fairness requirements for machine learning models. However, precisely defining the appropriate measure of fairness is challenging due to philosophical, cultural and political contexts. Biases can infiltrate machine learning models in complex ways depending on the model's context, rendering a single common metric of fairness insufficient. This ambiguity highlights the need for criteria to guide the selection of context-aware measures-an issue of increasing importance given the proliferation of ever tighter regulatory requirements. To address this, we developed a flowchart to guide the selection of contextually appropriate fairness measures. Twelve criteria were used to formulate the flowchart. This included consideration of model assessment criteria, model selection criteria, and data bias. We also review fairness literature in the context of machine learning and link it to core regulatory instruments to assist policymakers, Al developers, researchers, and other stakeholders in appropriately addressing fairness concerns and complying with relevant regulatory requirements.", "sections": [{"title": "I. INTRODUCTION", "content": "ARTIFICIAL intelligence (AI) has a growing influence on our daily lives. In the past 20 years, it has been used in high-stakes decision-making, such as predicting prisoner recidivism [1], [2], evaluating creditworthiness [3], and predicting financial fraud [4]. Recently, the progression of AI and improved societal trust have enabled its use in more high-risk scenarios, such as autonomous driving [5], piloting fighter jets [6], and identifying cancer cells [7], [8]. Industrial and political demand for low-cost decision support has led to the development of novel AI approaches and implementations. However, there has also been a growing level of concern regarding the fairness of machine learning (ML) assisted decisions. For example, ProPublica's report identified racial bias in the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) software [9], a recidivism prediction tool used in parts of the United States. They found that African American offenders were twice as likely as white offenders to be labelled high risk for reoffending but not reoffend. Conversely, white offenders were more frequently labelled low risk but reoffended. Several papers refuted ProPublica's claim that COMPAS was biased against African Americans [10], [11]. Nevertheless, the ProPublica report, as well as other prominent cases of bias-like Google's ad-targeting system that showed ads for high-paying executive jobs more often to male job seekers than equivalent female job seekers [12] and Amazon's Al recruitment tool that favored male candidates [13]-instigated a proliferation of ML fairness research.\nProminent research domains include definitions of fairness in ML [14], [15], incompatibilities between fairness measures [16], [17], [18], fairness-enhancing augmentations to ML approaches [19], [20], [21], and choosing appropriate fairness measures for given contexts [22], to name a few. This quickly developing field is hard to navigate for experts and does not seem to be well-understood by most practitioners. However, the situation is currently radically changing with the emergence of increasingly onerous ous Al regulations worldwide. These regulations compel any stakeholder in the Al domain to learn about bias and fairness issues as part of their effort to comply with regulatory requirements, be it to keep their competitive edge, maintain their customers' trust, or avoid fines. Existing Al tools may need to be audited to confirm compliance with emerging legislation. For example, AI-assisted recruitment tools, recidivism tools, or advertisement targeting tools may yield reputational and litigation risk if they are found to be unfair across race or gender. The common element in all these settings is that the relevant teams within those organizations need to familiarize themselves with regulatory requirements on bias and fairness to carry out their respective tasks and ensure that the AI systems they develop, purchase, or use are fair. Organizations need to familiarize themselves with regulatory requirements on bias and fairness to ensure that the Al systems they develop, purchase, or use are fair. Fundamental issues that must be considered include: How are bias and fairness defined and measured? Are they the same thing, or, if not, how do they differ and what is their relationship to each"}, {"title": "II. PRELIMINARIES", "content": "The lack of definitions or inconsistent usage of terms in policies, regulations and sometimes in scientific literature is adding an extra layer of complexity to understanding and complying with fairness requirements. One source of confusion is that even scientific literature tends to use bias and unfairness interchangeably [22], [36]. Therefore, we start by clearly defining the related but not interchangeable notions of bias and fairness based on ISO/IEC 22989 [37], ISO/IEC TR 24027 [28], and relevant scientific literature."}, {"title": "A. Bias", "content": "ISO/IEC 24027 [28] defines bias as \u201csystematic difference in the treatment of certain objects, people, or groups in comparison to others.\u201d In scientific literature, similar definitions are used [22]. It is important to distinguish between the meaning of bias in the Al and social contexts, as well as between desired and unwanted bias.\nIn the AI context, certain biases are essential for the operation of Al systems, as they are necessary for ML algorithms to differentiate between particular situations and carry out their tasks, be it classification, clustering or something else. Take the example of a classification task, where the aim is to group offenders in two distinct groups, \"recidivists\" and \"non-recidivists\". No ML model can solve this task unless it is allowed to distinguish between offenders in a systematic manner. Such desired biases are different from unwanted biases, which can be introduced into AI systems in various ways and yield unfair system outcomes that negatively affect individuals or groups of people.\nIn the social context, the main focus of attention is less on the technical workings of AI systems than on how they impact society. Consequently, bias usually refers to Al system outcomes (i.e., how the use of an Al system may affect society rather than the actual system output) that cause injustice by"}, {"title": "B. Fairness", "content": "ISO/IEC 22989 [37] and ISO/IEC TR 24027 [28] conceive of fairness as a \u201ctreatment, a behavior or an outcome that respects established facts, beliefs and norms and is not determined by favoritism or unjust discrimination.\u201d Note that neither standardization nor the scientific world offers a universal fairness definition. This is because the notion of fairness-both within the Al field and beyond is notoriously complex, context-dependent, and differs widely due to philosophical, religious, cultural, social, historical, political, legal, and ethical factors [22]. Therefore, attempts to define a single, universally-applicable definition of fairness have been unsuccessful [38] and numerous fairness definitions exist."}, {"title": "C. The Relationship Between Bias and Fairness", "content": "Bias and fairness are related, in that fairness notions (also referred to as fairness measures or fairness metrics) can be used to identify unwanted biases in Al systems. Yet whether an AI system is fair by some metric depends on much more than whether unwanted bias is present in the system. Unwanted biases do not necessarily result in unfair outcomes, nor are unfair outcomes necessarily caused by unwanted biases. In general, achieving fairness\u2014especially simultaneously satisfying more than one fairness notion is not always possible and typically involves trade-offs. Thus, explicitly defining fairness objectives and selecting the most appropriate fairness notion(s) given a particular context in a transparent manner is of paramount importance."}, {"title": "III. BIAS IN AI SYSTEMS", "content": "ML systems can be susceptible to the same biases as humans because their development and implementation require human decisions [39], [40]. Recently, researchers have started developing standardized bias identification methods to aid regulators in assessing model fairness. Agarwal and Agarwal (2023) developed a seven-layer model to standardize bias assessment in ML, identifying where biases enter the system and outlining the role of the developers and users in minimizing them [41].\nIn scientific literature, ML bias is often divided into three categories [21], [35], [42], [43] based on the temporal location of bias occurrence, these include: data bias (during data collection), algorithm bias (during model development), and user interaction bias (during implementation). These biases form a closed loop [35], [42], [43] (Fig. 1). Biases start at any of these sources and propagate through the Al system.\nConversely, the ISO/IEC TR 24027:2022 [28] describes bias propagation as a unidirectional interaction (Fig. 2). While scientific literature defines bias temporally, the ISO/IEC TR 24027:2022 defines bias spatially (i.e., with respect to where in the AI development these biases occur). Human cognitive"}, {"title": "A. Data to Model Bias", "content": "Scientific literature describes data bias as biases in the dataset used to train a ML model which often lead to similar algorithm biases [35]. Among the various sources of bias, dataset bias appears to be the most frequently observed form of bias as most model decisions are decisions about data [39].\nThe concept of 'data bias' described in the ISO/IEC TR 24027:2022 [28] is different to that described in literature. Both scientific literature and the ISO/IEC TR 24027:2022 describe data bias as arising from technical design decisions and constraints. However, most scientific literature also considers unfair data characteristics as data biases.\n1) Measurement Selection Bias: Measurement selection bias occurs when predictive features are chosen and measured, leading to distortions or inaccuracies in results between protected and unprotected groups [21], [45], [46]. For example, when choosing predictive features, certain features may correlate more with protected groups (e.g., race and gender) than others [39]. This inequity can arise from disparate measurement accuracy across groups [45]. For example, on average, men typically underreport their pain in comparison to women in healthcare settings [47]. Measurement selection bias is considered a data characteristic that leads to data bias in the ISO/IEC TR 24027:2022 [28].\n2) Omitted Variable Bias: Omitted variable bias occurs when an important variable is excluded from the model [35], [39], [48]. Variables may be omitted because they could be proxies for protected groups or were overlooked by the modeler. For example, Mustard (2003) found that omitting conviction rates"}, {"title": "B. Algorithm to User Interaction Bias", "content": "Biases in algorithm design and construction can affect user behavior [35]. Algorithm biases fall mostly under engineering decision biases in the ISO/IEC TR 24027:2022 [28]. Engineering decision biases encompass all biases involved with model specification decisions, parameter selection and feature design.\n1) Algorithm Bias: Algorithmic bias (defined as model bias in the ISO/IEC TR 24027:2022 [28]) occurs when a model minimizes average error, thus fitting the model to the most typical members of a population [17], [21], [46]. Hence, the model may not be well calibrated to outlier cases. This occurs when there is sample/representation bias that is not specifically treated by the model."}, {"title": "C. User Interaction to Data Bias", "content": "Many data sources for model training are generated by the developer through either feature engineering, data manipulation (such as translation and rotation of images for image recognition models), or data selection. Evidence shows that AI bias can reinforce human cognitive biases [55]. Therefore, biases generated by user interaction with model outcomes can lead to cognitive manipulation during data development, allowing the formation of data biases. User interaction biases fall under human cognitive biases in the ISO/IEC TR 24027:2022 [28].\n1) Historical Bias: Historical bias (labelled societal bias in the ISO/IEC TR 24027:2022 [28]) occurs when historically biased human decisions are used to generate data [17], [21], [45], [46]. For instance, historically disadvantaged groups can be arrested at higher rates [56]. In recidivism prediction models, this could lead to an overestimation of offending risk for the disadvantaged population [39], and therefore increased surveillance and further reconviction of the disadvantaged group. Thereby propagating historical biases.\n2) Temporal Bias: Temporal biases arise from variations in behavior over time [57]. These changes in behavior may be due to policy changes, cultural shifts, or changes in reporting. These variations can cause differences between training and implementation data, negatively affecting model performance. For example, recidivism rates noticeably changed after implementation of Washington State's Offender Accountability Act [58], which introduced a new recidivism prediction tool. The prediction tool was trained on retrospective data that was not highly indicative of future recidivism.\n3) Population Bias: Population bias (termed group attribution"}, {"title": "D. The Impacts of Context in Bias Identification", "content": "Each AI development has its own specific model requirements and faces different framings of the AI problem. Model requirements include the specific input parameters, classification or scoring goals, and definitions of accuracy or fairness. Model framing includes consideration of the consequences of model use; how the model is used; and where the training data comes from. These considerations form the context of the approach. The outcomes of the Al model can be sensitive to that context. Thus, careful consideration of the context of an Al implementation can help with the identification of biases in an AI model.\nFor example, Obermeyer et al. (2019) found that a US healthcare algorithm that evaluated patient sickness for a given risk score predicted black patients to be equally as ill as white patients, despite being considerably sicker. This bias arose because the algorithm predicted healthcare costs rather than illness, assuming healthcare cost to be a proxy for illness [59]. Since there was lower access to care for black patients, their healthcare costs were lower than white patients. Hence, the framing of the scenario in terms of health costs led to inappropriate bias. Several authors have discussed the relationship between model context and bias identification [39], [45], but this has yet to be linked to the bias interaction loop. Below, we describe examples of how context interacts at each stage of the bias interaction loop.\nData to model bias can occur when the declared goal of an AI approach cannot be effectively determined by the data. For instance, using data-driven approaches to predict recidivism cannot accurately capture actual recidivism rates, but rather reports, arrests, or conviction rates [39], [60]. This is a significant issue for offences with low reporting levels [61]. Using reported recidivism introduces a natural bias, as different groups are arrested at different rates [39], [60], [62]. Hence, while framing recidivism as a simply observable quantity rather than attempting to capture the true value is convenient, and at times necessary, this can lead to bias that could be mitigated if the true incidence could be estimated or measured.\nModel to user interaction bias can occur when users misinterpret or misuse the predictions of a model. For example, a user may incorrectly infer that a positive model prediction implies a higher chance of positive outcomes than indicated by the prediction threshold. In hiring processes, a manager may overvalue the scoring of a CV evaluation tool, which typically"}, {"title": "E. Efforts to Reduce Bias Entering an AI System", "content": "While numerous fairness-enhancing methods have been proposed [21], [70], these methods rarely consider the causes of biases. Below we summarize the various studies on bias mitigation in Al models. Pessach and Shmueli (2022) [21], and Caton and Haas (2023) [70] provide comprehensive overviews of many of the fairness-enhancing methods.\nTo address incomplete datasets, Mart\u00ednez-Plumed et al. (2019) suggested that mean imputation for quantitative datasets and mode for qualitative datasets would reduce bias. They argued that imputation is preferable to deletion, as rates of incompleteness in datasets can be correlated with protected groups (as explained in III. A. 4.). The authors noted that mechanisms to handle missing data were not incorporated in"}, {"title": "IV. DEFINING FAIRNESS IN MACHINE LEARNING", "content": "Definitions of 'fair' differ widely due to philosophical, religious, cultural, social, historical, political, legal, and ethical factors [22]. These factors, along with personal history, can significantly influence how a system architect incorporates fairness into a ML approach [35]. The wide range of influences contributes to the numerous fairness definitions proposed. Mehrabi et al. (2021) highlighted that most fairness definitions (in addition to datasets and problems) proposed in the literature have been developed by Western researchers [35], leading to a relatively Euro-centric perspective in fairness research.\nVarious software toolkits have been specifically designed to aid in the assessment of fairness in ML models: AIF360 [75]; FairLearn [76]; TensorFlow Responsible AI [77]; Aequitas [78]; and Themis-ML [79]. Among these, Aequitas is the most cited toolkit in the literature [22]. It allows users to test models for several bias and fairness metrics [78]. AIF360 provides users with bias-mitigation algorithms and offers guidance for selecting the most appropriate tool [75]. FairLearn, TensorFlow Responsible AI, and Themis-ML also provide these functionalities to varying degrees."}, {"title": "A. Legal Notions of Fairness", "content": "The recent emergence of fairness research has largely been driven by studies from the USA. The US legal framework is governed by two notions of fairness: disparate treatment and disparate impact (Civil Rights Act 1964). Disparate treatment refers to the differential treatment of individuals based on their group membership [80]. Disparate impact occurs when a neutral policy negatively affects members of one group more than another [80]. Satisfying both of these notions simultaneously is not always possible. For example, considering gender in a ML model would be classified as disparate treatment, but not considering gender could lead to disparate impact."}, {"title": "B. Academic Notions of Fairness", "content": "To identify the most popular fairness definitions currently in use, a systematic review was conducted (Fig. 4). Fairness notions implemented in each of the examined research articles were recorded, and the results presented in Table I.\n1) Search Strategy: Google scholar, Scopus and PubMed databases were searched for relevant articles. Article titles were searched for the following terms: (\"machine learning\" OR \"artificial intelligence\") AND (\"predict\" OR \"assess\" OR \"model\" OR \"predicting\" OR \" assessing\" OR \"modeling\") AND (\"fairness\" OR \"bias\" OR \"fair\u201d). The search was limited to articles published between January 1st, 2023, and March 3rd, 2024. This search yielded 47 documents from Google scholar, 53 from SCOPUS, and 10 from PubMed. After removing duplicates, 76 unique publications remained.\n2) Eligibility Criteria: The inclusion criteria required the publication to be a journal article or full conference paper written in English. The abstracts were reviewed to ensure they indicated that measurements of fairness against a ML model were conducted. The study was ultimately included if fairness of a ML model was measured and presented. In total, 21 documents were found that met our criteria. Fig. 4 presents a flowchart of the study identification results. The results of the systematic review are presented in Table I.\n3) Fairness Notions: Historically, fairness in ML has been primarily defined through observational measures [16], [19]. Recently, literature has proposed causal-based approaches to measuring fairness [93], [94]. Observational measures of fairness consider only the data, assessing statistical relationships between variables. In contrast, causal approaches consider a broader context, including how data is generated and how variable selection affects model behavior [33], [95], [96]. Causal fairness notions typically consider either interventions that simulate randomized experiments, or hypothetical idealized worlds (counterfactuals) that are compared to the actual world [33]. Some researchers argue that causal measures of fairness are necessary to address the problem of fairness comprehensively [33], [97], [98]."}, {"title": "V. CHALLENGES WITH MEASURING FAIRNESS", "content": "Assessment of fairness in an Al model is made difficult by several factors. Firstly, to assess group-level fairness, groups that may be unfairly treated must be identified. However, unfairly treated groups may be represented by the intersections of commonly recognized marginalized groups or entirely new groups altogether. These can be hard to identify. Secondly, the application of fairness metrics in certain contexts is constrained by inherent limitations, such as base rate differences and fairness metric incompatibilities. Unequal base rates can skew the results of fairness assessments for specific metrics, undermining their validity. Furthermore, particular characteristics in data across groups can make it impossible to satisfy two fairness notions simultaneously, while avoiding trivial solutions [16], [17], [18], [22]. Finally, the optimization of models for improved fairness can lead to a reduction in model accuracy due to the fairness-accuracy trade-off."}, {"title": "A. Problems with Marginalized Group Identification", "content": "In a review of bias identification studies in AI, Bucchi and Fonseca (2023) found that most studies identifying biases in AI used an a priori approach. This method identifies biases in marginalized groups where known social biases exist [109]. Studies employing an a priori approach (see [40], [105]) can effectively identify ML model biases among these recognized marginalized groups. However, this preconceived notion of biases targeting predefined groups limits the scope of bias identification. Bucchi and Fonseca (2023) identified only one study that used an a posteriori approach to identifying biases (i.e., without assuming predefined marginalized groups [109]). Using this method, Watkins (2023) discovered a previously unidentified marginalized group (an unguided cluster of subjects later described as 'workers') [110]. Such discovery would have been impossible using an a priori approach. Therefore, Al researchers should be careful not to limit their perspective on what groups may be marginalized in certain AI implementations. Selbst et al. (2019) described this problem as the 'solutionism' trap that designers and researchers fall into"}, {"title": "B. Base Rates", "content": "A group's base rate is their percentage of positive outcomes. Fairness measures defined by the confusion matrix require base rates to be uniform across groups [18] due to the relationship between outcome predictions and thresholding. Grant (2023) provides an example illustrating the effect base rates have on fairness metrics in a recidivism prediction scenario. In this example, professional thieves had a 90% chance of reoffending compared to 10% for amateur thieves. Suppose Group 1 contained 100 professionals and 10 amateurs, and Group 2 contained 10 professionals and 100 amateurs. A straightforward method of predicting recidivism would be to assign a high-risk score to professionals and a low-risk score to amateurs. This would result in Group 1 (base rate = 0.83) having a FPR of 53% and a FNR of 1%, while Group 2 (base rate = 0.17) would have a FPR of 1% and a FNR of 53% [107]. To achieve fairness across Group 1 and Group 2 using EO (3), the FPR and FNR rates should be equal across groups. This is not possible while maintaining any level of model accuracy. This shows that even with a valid prediction system, fairness measures defined by the confusion matrix are skewed in the direction of the base rate (e.g., lower base rates lead to more false negatives). Therefore, under unequal base rates, it is impossible to determine whether unfairness is due to biased data or the base rate itself. Many authors claim equal base rates are a requirement for fairness measures defined by the confusion matrix [60], [108], [112]. As a result, the use of certain fairness metrics is limited when there is disparity in group base rates."}, {"title": "C. Incompatibility Results", "content": "The disparity in base rates across groups can lead to incompatibilities between certain fairness metrics. This was the focus of much of the earlier work on fair ML. Chouldechova (2017) mathematically proved that PPV is incompatible with EO when compared groups have unequal base rates [60]. Barocas et al. (2019) [96] and Wasserman (2004) [113] further demonstrated the incompatibility between NPV and EO unless a model is perfectly predictive. Kleinberg et al. (2016) and Pleiss et al. (2017) showed that CAL cannot be simultaneously satisfied with EO unless predictions are perfect or base rates are equal [108], [114]. SP and BG-ACC were found to be incompatible when base rates are unequal [30]. SP and EO are also incompatible unless predictions are perfect [30]. Finally, Garg et al. (2020) mathematically proved that SP, EO, and PP are jointly incompatible unless base rates are uniform [115]. Furthermore, it is worth noting that equivalences exist between certain fairness metrics. For example, when EO criteria are met, the TPR is equal between groups, satisfying EOP requirements.\nA detailed investigation of these incompatibilities across"}, {"title": "D. The Fairness-Accuracy Trade-off", "content": "Fairness in ML is often considered alongside accuracy due to the widely observed fairness-accuracy trade-off [16], [19], [116], [117], [118], [119], [120], [121], [122]. For ML models with optimized accuracy, increasing fairness necessitates a degradation in accuracy. Excluding trivial cases, it is impossible to maximize both fairness and accuracy simultaneously [16]. This is because fairness requirements act as additional constraints when training a model [21]. Menon and Williamson (2018) note that the magnitude of the fairness-accuracy trade-off is proportional to the correlation between the sensitive attribute (the group classifier, e.g. gender) and the target variable [120].\nConversely, recent research has challenged the assumption that a fairness-accuracy trade-off always exists. For instance, Langenberg et al. (2023) demonstrated that for balanced datasets (i.e., containing equal numbers of positive and negative outcomes), increasing accuracy improved fairness [123] due to the minimization of errors [124]. There are other instances where the fairness-accuracy trade-off is negligible [125], [126]. Despite these isolated cases, a trade-off between fairness and accuracy is generally accepted as ubiquitous in ML [16], [120], with the strength of the trade-off dependent on the context of application. Hence, the level of influence of the fairness metric on model accuracy should be carefully controlled. Typical measures of model accuracy include AUC [127], Fl-score [128], [129], and accuracy (ACC) [130], [131]."}, {"title": "VI. SELECTING APPROPRIATE FAIRNESS MEASURES BASED ON CONTEXT", "content": "The importance of defining context-appropriate fairness metrics is presented in [20], [34], and [179]. The impacts of different fairness requirements on accuracy is inconsistent [16], highlighting the need for appropriate fairness measures to balance this trade-off. Furthermore, unequal base rates skew certain fairness measures, making it impossible to determine whether unfairness was the result of biased data or the base rate itself. This limits the set of context-appropriate fairness metrics under unequal base rate conditions. Lastly, the incompatibility between fairness measures can lead to contradictions. Hence, a framework for selecting the most appropriate fairness metric for the given context is required.\nMakhlouf et al. (2021 & 2022) developed novel methods for selecting context-appropriate observational [34] and causal [33] fairness notions using carefully constructed flowcharts. However, fairness in ML is a rapidly evolving research field and the outcomes of the papers reviewed suggest that different decision metrics may be appropriate. These include consideration of the type of fairness assessment [106], type of model [133], type of output [65], base rate [18], and data biases (Section III.). Fig. 5 provides a flowchart for selecting context-appropriate observational fairness notions based on the reviewed contextual fairness literature."}, {"title": "A. A Context-Based Method for Selecting Observational Fairness", "content": "Twelve criteria were used to formulate the flowchart in Fig. 5. These criteria include consideration of the model assessment criteria, such as whether fairness of data or fairness of outcome is being assessed (VI. A. 1.); whether equity requirements are in place (VI. A. 5.); whether thresholds are fixed or floating (VI. A. 7.); whether there is an emphasis on precision or recall (VI. A. 9.); whether there is an emphasis on FP or FN misclassification (VI. A. 10.); whether there is an emphasis on the positive or negative class (VI. A. 11.); and whether the dataset is balanced (VI. A. 12.). The selection criteria also include consideration of model design, such as whether the model is used for classification tasks, predicts continuous outcomes, or is a generative model (VI. A. 2.); whether a distance metric is available in the model inputs (VI. A. 4.); whether the model outputs binary or regressive values (VI. A. 6.); and whether base rates are equal (VI. A. 8.). Additionally, the fairness selection criteria also consider whether the data is biased (VI. A. 3.). Each of the fairness selection criteria are identified by a node in Fig. 5.\n1) Assessing Data vs Assessing Outcome (Node 1): Tang et al. (2023) proposed that unbiased models achieve fairness requirements in data generation, predicted outcome, and induced impact. They argued that fairness in data generation be measured using causal definitions. Assessment of fairness in data generation requires consideration of the relationship between measured variables and the sensitive attribute only, avoiding any consideration of model formation. This is achievable by causal fairness measures as they account for the relationships between variables. They further proposed that fairness with respect to predicted outcome be measured using observational definitions, under the assumption that biases in the data are understood. This is because model predictions (Y) are based on actual outcomes (Y), requiring Y to be unbiased themselves. Observational fairness measures the relationship between Y and Y directly [106].\n2) Continuous Prediction, Classification, or Generative Modeling (Node 2): The type of ML model has important implications on the context-appropriateness of certain fairness definitions. Modern supervised ML has three broad categories: classification; continuous prediction; and generative modeling. When predicting on a continuous domain (e.g., predicting house prices), fairness through awareness is the only measure that can handle this data type as all other measures require group comparisons. Group fairness measures are more suited for classification tasks because binary outcomes are easily comparable between groups. Generative AI assesses fairness differently to common observational and causal measures. These measures primarily evaluate outputs based on specific prompts [163]. Assessment of fairness in generative Al is beyond the scope of the current paper and not considered here.\n3) Biased Data (Node 3): Section III. highlighted the complex ways bias can affect an AI model, with one of the main sources being the dataset itself. For Al models predicting binary outcomes with suspected bias in the dataset, individual fairness measures should be avoided. Individual fairness measures"}, {"title": "B. Using the Observational Fairness Selection Method", "content": "The creation of Fig. 5 encapsulates the reviewed material on context-based fairness. The framework outlined in Fig. 5 is designed to recommend appropriate observational fairness measures based on the specific context, while discouraging the use of less suitable measures. (Context appropriate selection of causal fairness measures is beyond the scope of this paper; for more information on this topic, refer to [33], [101], [166].)\nAnahideh et al. (2022) proposed a framework that estimates the correlation among fairness notions for a given context using a Monte Carlo approach. These correlations were used to identify a set of diverse and distinct fairness metrics that best represent the given context [167], creating a subset of metrics that approximately concur and thus provide a \u201cworkable balance in the reduction of unfairness\" [167]. The idea that fairness metrics may be correlated in a specific context is supported by Friedler et al. (2019), who also demonstrated that fairness metrics are correlated with each other [147]. However, this approach could fall into the 'circular argument' fallacy, where two correlated metrics could be equally inappropriate. Moreover, the implementation of certain fairness metrics in real-world applications is ultimately justified by a specific philosophical framework. Therefore, selecting fairness notions based on their proximity may not always be appropriate.\n1) Limitations of the Observational Fairness Selection Method: Fig. 5 suggests context-appropriate fairness notions for measuring fairness of predicted outcome only (VI. A. 1.). These observational measures only assess the relationship between model predictions, generated from the available dataset, and actual outcomes. However, the available dataset may contain unwanted biases. This issue cannot necessarily be captured by the observational framework of Fig. 5 and requires consideration of causal fairness. Causal fairness notions have been proposed for assessing fairness in data generation [106]. Tang et al. (2023) asserted that a fair model requires fairness in data generation, predicted outcome and induced impact [106]. Since the recommendations of Fig. 5 did not consider fairness in data generation, Fig. 5 could be augmented by the concepts provided by Makhlouf et al. (2022) [33] to select an appropriate causal measure of fairness. Fairness with respect to induced impact could then be observed by examining how users interact with the ML outputs, and the consequences of the model prediction.\n2) Examples of Use: Three examples are used to show how Fig. 5 can be used to select an appropriate fairness measure.\n2a) Prisoner recidivism: Let us assume we are assessing fairness in predicted outcome (node 1). Classification models are used for predicting prisoner recidivism (node 2). Due to historically disadvantaged groups being arrested at higher rates [56], biases in the dataset would likely be present (node 3). Equity is not required for recidivism models (node 5). Let us also assume that binary outputs are being used (node 6) and that the threshold for recidivism is fixed (node 10). However, differing recidivism rates between races [39], [60], [62] would result in unequal base rates (node 11). Because of this, Fig. 5 recommends the use of regressive outputs opposed to binary outputs. Therefore, instead of predicting whether an offender would reoffend, the model would now predict the likelihood of reoffending. Recidivism prediction models have an emphasis"}, {"title": "C. Caveats of the Observational Fairness Selection Method", "content": "It is worth noting some caveats about Fig. 5. Selbst et al. (2019) identified 'portability' as one of the failure modes fair ML researchers encounter. Researchers fall into a portability trap by failing to \"understand how repurposing algorithmic solutions designed for one social context may be misleading, inaccurate, or otherwise do harm when applied to a different context\" [69]. In effect, the prized goal of achieving a one-size-fits-all solution, highly valued in ML communities, often does more harm than good in fairness research due to the inability to apply a common measure of fairness across the broad range of ML problems. Whereas Fig. 5 aims to mitigate the portability trap by using context to select an appropriate fairness measure, it may be too prescriptive in certain cases and ultimately ignorant of specific cases being considered. All fairness metrics must be carefully considered within the appropriate philosophical and cultural framework in which they are applied [22], [35]. Nonetheless, this potential for ambiguity should not inhibit the implementation of notions of fairness in ML. Ultimately, there is a moral, and often legal or commercial, impetus to introduce such notions when appropriate. Nor should the potential for ambiguity inhibit the generation of guidelines for selecting appropriate fairness metrics for specific contexts. Fairness has a basis in philosophy, and thus definitions of fairness may change across cultures. Hence, the criteria introduced in Fig. 5 should be carefully considered against the cultural framework where the metric is to be applied. This paper considered published literature predominantly from Western countries. Thus, care must be taken to avoid the portability trap"}, {}]}