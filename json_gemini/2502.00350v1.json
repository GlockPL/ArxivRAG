{"title": "OrcaLoca: An LLM Agent Framework for Software Issue Localization", "authors": ["Zhongming Yu", "Hejia Zhang", "Yujie Zhao", "Hanxian Huang", "Matrix Yao", "Ke Ding", "Jishen Zhao"], "abstract": "Recent developments in Large Language Model (LLM) agents are revolutionizing Autonomous Software Engineering (ASE), enabling automated coding, problem fixes, and feature improvements. However, localization - precisely identifying software problems by navigating to relevant code sections - remains a significant challenge. Current approaches often yield suboptimal results due to a lack of effective integration between LLM agents and precise code search mechanisms. This paper introduces ORCALOCA, an LLM agent framework that improves accuracy for software issue localization by integrating priority-based scheduling for LLM-guided action, action decomposition with relevance scoring, and distance-aware context pruning. Experimental results demonstrate that ORCALOCA becomes the new open-source state-of-the-art (SOTA) in function match rate (65.33%) on SWE-bench Lite. It also improves the final resolved rate of an open-source framework by 6.33 percentage points through its patch generation integration. ORCALOCA is available at https://anonymous.4open.science/r/OrcaLoca-84B1/.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have advanced rapidly, driving intelligent agents across diverse domains. In Autonomous Software Engineering (ASE) (Devin, 2024), LLM-driven agents enable automatic code generation, program repair, and feature enhancement. Incorporating LLMs into software development processes has been demonstrated promising by tools such as GitHub Copilot (Microsoft, 2023) and LLM-based agents like AutoCodeRover (Zhang et al., 2024b) and SWE-agent (Yang et al., 2024b). To navigate repositories, create patches, and fix problems, these agents leverage capabilities such as fault localization, action planning, and program-building unit tests. Among these abilities, localization - the ability to precisely identify and navigate to relevant code for resolving software engineering problems remains a crucial yet underexplored challenge in ASE.\nLocalization is well-recognized as a critical yet challenging step (Yang et al., 2024b; Xia et al., 2024) in ASE. As shown in Figure 1, on average, only 53.5% of issues achieve a correct function match across all submitted agents solutions (Jimenez et al., 2025). Localization is challenging due to an inherent complexity of software repositories. For instance, the average codebase of SWE-bench (Jimenez et al., 2024) consists of 3,010 files with around 438K lines of code. Worse yet, user requirements are often expressed in imprecise natural language, making it even more challenging to extract relevant code from a large repository based on the user's issue input. In particular, we identify three key challenges of LLM agent-based localization:\n1) How to explore the codebase with strategic action planning and precise navigation? Prior works on agent-based software localization encounter two key limitations: (i) action planning inefficiencies arise as certain methods rely solely on LLMs for guidance (Zhang et al., 2024a), resulting in unstable and redundant search behaviors; (ii) graph-based scheduling (Ma et al., 2024b) limits flexibility by enforcing preprocessed traversal routes that confine searches to neighboring nodes.\n2) How to achieve both context conciseness and search space completeness? Concise context, such as code skeletons, reduces noise and keeps the context manageable but risks omitting critical details for precise localization. Conversely,"}, {"title": "2. Related Work", "content": "full detailed search space ensures completeness but introduces overwhelming noise, redundancy, and irrelevant exploration paths. Achieving both conciseness and completeness simultaneously is challenging, as existing methods often optimize for one at the expense of the other, leaving an open gap in effective localization.\n3) How to effectively manage context during exploration? Large repositories often introduce noise due to ambiguities, such as function overrides and inherited classes. As the exploration process progresses, irrelevant information can accumulate, misleading the LLM and resulting in incorrect identification of bug locations. Existing frameworks (Zhang et al., 2024a; Wang et al., 2024b), merely concatenate all search results into the context, which is insufficient to manage the expanding complexity of large-scale exploration.\nTo address these challenges, we propose an agent system consisting of three key components:\n\u2022 Priority-Based Scheduling for LLM-Guided Actions: To address challenge 1), we design a dynamic action scheduling system that incorporates priority queues and LLM-guided action generation for codebase exploration. The priority queue dynamically reorders actions based on their contextual relevance and urgency, solving the shortcomings of previous systems that lacked effective action management.\n\u2022 Action Decomposition with Relevance Scoring: To resolve challenge 2), we introduce a method that decomposes high-level actions, such as class skeletons or file skeletons, into finer-grained sub-actions. These sub-actions are evaluated and ranked according to their relevance to the issue using a multi-agent workflow, ensuring comprehensive exploration while avoiding noise and redundancy.\n\u2022 Distance-Aware Searched Context Pruning: To address challenge 3), we design a context manager that dynamically prunes the searched context. The pruning algorithm leverages a node distance heuristic within the graph-oriented codebase. By filtering out irrelevant data, the context manager ensures that exploration stays focused and aligned with the bug localization."}, {"title": "2.1. Fault Localization Algorithms and Systems", "content": "Fault localization (FL) aims to identify suspicious locations (e.g., statements or methods) in source code that are associated with bugs. Prior to the advent of LLMs, fault localization had been extensively studied, with techniques such as spectrum-based fault localization (SBFL) (Jones & Harrold, 2005), mutation-based fault localization (MBFL) (Papadakis & Le Traon, 2015), and learning-based approaches like FLUCCS (Sohn & Yoo, 2017), DeepFL (Li et al., 2019), and TRANSFER (Meng et al., 2022). However, effective fault localization in large-scale software systems remains challenging due to the vast size of codebases and the overwhelming volume of error messages, which often exceed the capabilities of standalone learning models.\nSince the advanced code and natural language understanding capabilities of LLMs, Recent studies (Yang et al., 2024a; Wu et al., 2023; Li et al., 2024; Hossain et al., 2024; Kang et al., 2023; Qin et al., 2024; Wang et al., 2024c) have proposed LLM-based FL methods. These methods incorporate agents and tools to address the challenges of large-scale systems. AUTOFL (Kang et al., 2023) enhances standalone LLMs with tool invocations, such as repository retrieval tools, for more effective exploration of code repositories. RCAgent (Wang et al., 2024c) integrates four tools (code analysis, log analysis, memory retrieval, and information collection) to support decision-making. AgentFL (Qin et al., 2024) scales LLM-based fault localization to project-level contexts by combining multiple agents with static analysis tools like Tree-sitter.\nHowever, effectively and robustly exploring the codebase while balancing the trade-off between context granularity and search space remains a significant challenge. In contrast to existing techniques, ORCALOCA introduces a dynamic action scheduling exploration system and mechanisms to score decomposed actions, addressing these limitations effectively."}, {"title": "2.2. LLM-Agent for Software Engineering", "content": "LLMs have recently demonstrated remarkable capabilities in achieving human-level performance across a wide range of tasks, significantly advancing the field of ASE. Unlike traditional function-level or file-level coding tasks like Humaneval(Chen et al., 2021), ASE requires not only basic coding proficiency but also advanced skills in managing and interacting with code repositories. To solve such more complex tasks, LLM-based agents enhance project-level software engineering tasks by iteratively and autonomously performing actions, observing feedback, and planning future steps (Hong et al., 2023; Kong et al., 2024; Wang et al., 2024a; Yang et al., 2024b; Xia et al., 2024; Ouyang et al., 2024; Zhang et al., 2024b).\nOpenHands (Wang et al., 2024b) is a community-driven platform integrating widely used agent systems to explore end-to-end LLM-based agent solutions for handling complex SE tasks. AutoCodeRover (Zhang et al., 2024b) introduces LLM agents with specialized code search methods to iteratively retrieve code context and locate bugs using test cases. Agentless (Xia et al., 2024) proposes a two-stage bug-fixing system based on a streamlined workflow approach. Repounderstander (Ma et al., 2024a) empowers agents to comprehensively understand the whole repositories by a code knowledge graph for repositories and a Monte Carlo tree search-based repository exploration strategy."}, {"title": "3. Methodology", "content": "achieving a.3.1. Search System Setup and Agent Workflow\nOur search system is inspired by prior works such as (Ma et al., 2024a; Ouyang et al., 2024), which employ graph databases for indexing code repositories. Similarly, we construct a CodeGraph, a graph-based representation of the codebase G = (V,E), to facilitate indexing and searching code entities. As illustrated in Figure 2. (b), the CodeGraph G contains two primary edge types e1,e2 \u2208 E. e1 is containment, which represents hierarchical relationships, such as methods within classes or classes within files. e2 is the reference that represents relationships such as function calls between entities. The entities include functions, classes, methods, and files. Each code entity v \u2208 V in the CodeGraph is assigned with a unique identifier (UID) using the format file_path(::cls)(::method).\nFor example, in standalone functions, the UID is simply file_path::method. These identifiers encode the containment hierarchy directly, with :: representing the \"containment\" relationship. To enhance compatibility with the CodeGraph, we redeveloped the API from AutoCodeRover (Zhang et al., 2024a) to provide better support for CodeGraph-based searches (See Appendix A).\nBuilding upon the ideas of Chain of Thought (CoT) (Wei et al., 2022) and ReACT (Yao et al., 2022), ORCALOCA follows a reason-and-act workflow with a constrained action space. We design a custom-designed LLM prompt, which will generate Observation (O), Potential Bug Locations (PB), and Search Actions (SA) in each step. Here, we formulate PB as a set of entities VPB: PB={VPB|VPB\u2208V}.\nTo better illustrate the agent workflow, we formulate it as a tuple M, where M = (S,C,A,P,po). Here, S means the state space, including previous observations, potential bug locations, and retrieved search results. A stands for action space, which is restricted by our search APIs. In A, each action ak \u2208 A represents a query for retrieving relevant code snippets, generating a feedback as Search Result (SR). VSR with UID, SR=VSR\u2208 V. The context space C means"}, {"title": "3.2. Priority-Based Scheduling for LLM-Guided Actions", "content": "To solve challenge 1) we discussed in Section 1, ORCALOCA provides a more robust framework, which leverages a priority queue to manage the LLM-generated actions, offering a more comprehensive and effective method for action planning.\nTo achieve a thorough reasoning COT, our agent limits each step to only processing one action. However, for SA generated by LLM, it may have multiple action candidates based on the given context. To address this, we design a policy \u03c0 that uses a dynamic action scheduler queue (ASQ) on top of LLM-generated actions. The ASQ has priority management which is implemented on top of a heap data structure.\nIn ORCALOCA, action priorities are dynamically adaptable across different levels. The default priority for action ak \u2208 SA is 1. However, this priority can be elevated based\non contextual relevance and strong relationships. For instance, in Figure 2. (c), the step from 7 to 8 shows how the action involving the file serializer.py is assigned a higher priority due to its strong connection with serializer_factory. The same principle is set for action decomposition, which is discussed in Section 3.3.\nTo account for urgency, we also keep a counter Car for each unique action ak. When the LLM generates the same action repeatedly, the counter Cak grows, indicating the LLM's focus on checking the content. The counter Car replaces the original priority value and adjusts the position of ak' in the queue. This system ensures that the most important actions are carried out first. For example in Figure 1. (c), the step from 6 to 7 shows that serializer_factory would come to the next step due to its counter has accumulated to 3, which even surpasses the file related action models.py corresponding to CreateModel.\nAdditionally, to address the unpredictability and hallucinations of LLMs, we set up a redundancy elimination mechanism to improve action scheduling. This mechanism ensures that redundant actions are avoided, enhancing efficiency and preventing unnecessary exploration.\nConsider the previous agent API used by systems like (Zhang et al., 2024b; Ma et al., 2024a). When it comes to search class content, it has two different APIs search_class(cls) and search_class_in_file(cls, f) which will target at class searching. Initially, the LLM may lack precise information about the location of the target class, which leads to the use of the general method search_class (ModelChoiceField). However, after analyzing the returned content, the LLM will learn the file path and generate a subsequent, more specific action, such as search_class_in_file (ModelChoiceField, django/forms/models.py). Without careful handling of API ambiguities in scheduling, even a unique class like ModelChoiceField could result in duplicate"}, {"title": "3.3. Action Decomposition with Relevance Scoring", "content": "actions and redundant content searches.\nTo mitigate this, as illustrated in Figure 3 (a), we maintain an action search database. Before an action is passed to the agent's chain-of-thought (COT) reasoning, we prefetch its UID from CodeGraph and register its unique identifier (UID) in this database. This prefetching process ensures that each action is checked against previously executed actions, preventing duplicates and enabling more efficient scheduling.\nAchieving both conciseness and completeness simultaneously is challenging. Previous solutions (Xia et al., 2024; Zhang et al., 2024a) frequently employed skeletal techniques for huge classes or files, returning solely the class and methods signature. However, brutal traversal over all the methods could lead to noisy context and redundant actions. To overcome this challenge, we propose action decomposition with relevance scoring.\nSpecifically, if the search result SR of an action ak corresponds to a class USR \u2208 Vclass, we employ a score and rank sub-agent to evaluate the relevance of each method in the class Nclass = {v | v \u2192 vclass \u2208 e\u2081 } to the problem statement. The sub-agent (implemented by another LLM agent) will select the top-k most relevant methods, which are recomposed as new search actions, denoted as a. These decomposed actions at are assigned a higher priority (e.g. 2), and pushed to the ASQ for execution. In this way, the main agent could work with the scoring sub-agent in a multi-agent workflow. Moreover, we extend this decomposition principle to handle large files. For a file that triggers skeleton mode, we collect code entities within the file, like functions and classes, and treat them as individual units for the sub-agent.\nWe have shown the illustrated example in Figure 2. (c).\nIn addition to enhancing granularity, our method addresses ambiguities, which commonly appear in large software repositories such as function overrides, and inherited classes. To resolve these issues, we implement a robust disambiguation mechanism within our decomposition strategy. We first constructed an inverted index that stores only the callable indices that exhibit ambiguities. The value of the index encloses the exact location, including the file, path, and relevant class, if applicable. As shown in Figure 3. (b), when our API finds a query with ambiguities, it will locate itself in the inverted index, enabling us to gather all the possible locations to form a disambiguation message for the LLM agent. Additionally, we will split the potential locations and fine-grainedly push back the related search actions in the action queue."}, {"title": "3.4. Distance-Aware Searched Context Pruning", "content": "To prune the irrelevant context and keep LLM focusing on useful information, we developed a distance-aware context pruning method, which we call as the Context Manager (CM). The CM is designed to maintain a concise and relevant set of search results (SR) by evaluating their relationship to the potential bug locations (PB).\nFirst of all, to enhance relevance, the CM retains only SR entries linked to valid search query UIDs. Disambiguation messages (See Figure 3. (b)) and skeleton UIDs, typically used for large files and classes, are explicitly excluded to prevent irrelevant data from polluting the context.\nThe pruning process is guided by CodeGraph G, where each search result SR is mapped to a unique graph node USR \u2208 V. The CM evaluates each SR based on its distance to the potential bug locations PB, which are also represented as nodes in the graph. Specifically, the CM computes the average shortest path distance between each node USR and the candidate nodes in PB: d(SR, PB) = 1/|PB| \u2211v\u2208PB min (d(USR,v),d(v,vSR)), where d(USR,v) represents the shortest path from USR to v in the directed CodeGraph, and d(v,vSR) represents the reverse shortest path. The final distance metric for pruning is defined as the minimum of these two values.\nOnce distances are calculated, the CM prioritizes the most relevant results. It selects the top-k candidates based on the calculated average distance, ensuring that LLM bypass those irrelevant code blocks. As shown in Figure 2. (d), in the last step, the context will filter out the irrelevant info like OperationWriter, CreateModel, which will make the conclusion step have a stable and correct bug location output. Importantly, the CM is applied to every step during the exploration phase.\nBy aligning SR entries with the structural relationships within the CodeGraph, the CM helps the system focus on areas most likely to contain the bug. This approach not only streamlines the input context but also improves the accuracy and efficiency of the search process."}, {"title": "4. Evaluation", "content": "To achieve 4.1. Setup\n4.1.1. DATASETS\nSWE-bench (Jimenez et al., 2023) is a widely used dataset for evaluating the ability of LLM systems to address real-world software engineering challenges. It comprises 2,294 task instances derived from 12 popular Python repositories, where each task requires a patch to resolve the issue described in its corresponding GitHub issue.\nTo reduce evaluation costs and complexity, the SWE-bench team introduced two refined subsets:\n\u2022 SWE-bench Lite contains 300 instances filtered using heuristics, such as removing tasks with images, external"}, {"title": "4.1.2. BASELINES", "content": "hyperlinks, or short descriptions. Each task includes functional tests to validate the correctness of submitted patches.\n\u2022 SWE-bench Verified, developed in collaboration with OpenAI, includes 500 instances manually validated by professional annotators, providing greater reliability.\nTo further optimize costs for repeated experiments, we defined a smaller subset, SWE-bench Common, consisting of 93 instances that form the intersection of SWE-bench Lite and SWE-bench Verified. Its compact size and high reliability make it ideal for tasks such as ablation studies.\nIn our experiments, we evaluate the performance of ORCALOCA using SWE-bench Lite and conduct ablation studies using SWE-bench Common."}, {"title": "4.1.3. IMPLEMENTATION", "content": "We compare ORCALOCA against 17 different approaches listed on the public leaderboard (Jimenez et al., 2025) of SWE-bench Lite. These approaches are categorized into 2 groups: (1) closed-source solutions, such as Alibaba Lingma (Ma et al., 2024b); (2) open-source solutions, including OpenHands (Wang et al., 2024b), AutoCodeRover (Zhang et al., 2024b), Agentless (Xia et al., 2024), RepoGraph (Ouyang et al., 2024), HyperAgent (Phan et al., 2024), and SWE-Agent (Yang et al., 2024b).\nThe SWE-bench Lite leaderboard mandates that each submission include the generated patches for addressing the given issues. This requirement enables the computation and comparison of a broader range of metrics beyond the resolved rate. In addition to analyzing the leaderboard data, we reproduced the Agentless-1.5 model for a direct comparison with ORCALOCA, as its editor component is integrated into our system."}, {"title": "4.1.4. METRICS", "content": "ORCALOCA is built on the LlamaIndex framework (Liu, 2022), which supports various foundation models. For our experiments, we used Claude-3.5-Sonnet-20241022 (Anthropic, 2024) as the underlying model, with a sampling temperature set to 0.1 to prioritize deterministic results.\nFor the top-k values used in action decomposition (Section 3.3), we set k = 3 for class decomposition and k = 2 for file decomposition. In the context pruning (Section 3.4), the context window size is configured to retain 12 entries (top-k). Our framework also supports a wide range of customizable configurations, enabling users to fine-tune their agent workflows. These settings include parameters such as class decomposition, file decomposition, disambiguation decomposition, priority adjustment, and the ability to enable or customize priority levels. This flexibility allows users to tailor their agent's behavior to specific use cases, enhancing both exploration and fine-tuning capabilities. The cost of searching is about $0.87 per instance.\nTo evaluate the contribution of ORCALOCA to the final Resolved Rate on SWE-bench Lite, we integrated the Repair, Patch Validation, and Patch Selection components of Agentless-1.5 (Xia et al., 2024) by converting the output of ORCALOCA into Agentless format. Inspired by Repograph (Ouyang et al., 2024), the dependencies of the output code are also added. We largely adhered to the experimental setup outlined in the Agentless public repository, using the same LLM model, Claude-3.5-Sonnet-20241022. For the repair process, we generated 40 patches (1 at a temperature of 0 and the rest at 0.8) with the str_replace_format argument set. During patch validation, we employed both regression and reproduction tests. Regression tests were filtered with a temperature of 0, while reproduction tests were generated using 40 samples (1 at a temperature of 0 and the rest at 0.8). Finally, the results of selected regression and reproduction tests were used to identify the most effective patch among the 40 candidates. The cost of editing is about $0.90 per instance.\nTo evaluate the performance of ORCALOCA, we utilized four metrics: Resolved Rate, Function Match Rate, File Match Rate, and Function Match Precision. Each metric is designed to provide unique insights into the effectiveness and quality of the agent.\n\u2022 Resolved Rate is a metric originally proposed by the SWE-bench benchmark (Jimenez et al., 2024), which we adopted for our evaluation. The benchmark assesses whether an issue is resolved by constructing a Docker container for each instance, applying the user-submitted patch, running regression tests within the container, and analyzing the test results. The final metric is the percentage of the instances that are resolved.\n\u2022 Function Match Rate and File Match Rate assess the localization accuracy of ORCALOCA by calculating the percentage of Match in instances. These metrics, inspired by prior works such as Agentless (Xia et al., 2024) and Repograph (Ouyang et al., 2024), evaluate how well the agent's outputs align with the golden patch. (To align with these works, we use the term function as a general term that includes functions and methods).\nTo determine Function Match, we define the golden and agent-generated localization function results for each instance i as sets: Bi,goldenB agent CV, following definitions in Section 3.1. A match is registered if Bugolden \u2286 B funcagent. For File Match, we consider the subset of file nodes in the graph G, denoted as: Bfuncfile.\nAccording the definition of our graph, every node v \u2208 V is either a file node or has an ancestor by containment edge that is a file node. Thus, we define a mapping function:"}, {"title": "4.2. Results", "content": "fileOf: V\u2192 file, which returns the file containing node v. The File Match is then determined as: BfileBfile golden \u2286 Bfileagent, where Bfile agent= {fileOf(v) | v\u2208 Biagent }.\n\u2022 Function Match Precision is a metric proposed by us to assess the quality of localization results. For instance, a localization output that includes every function in the repository would always ensure a function match but would be practically useless. To solve this problem, the Function Match Precision is computed for each instance as FMPi = |Bfulgolden \u2229 Bruncentagen/ |||Befundent, and the final metric is the average of FMP per instances.\n4.2.1. PERFORMANCE ON LEADERBOARD\nAs shown in Table 1, our ORCALOCA sets a new open-source State-Of-The-Art (SOTA) with a Function Match Rate of 65.33% (196 out of 300) and a File Match Rate of 83.33% (250 out of 300). These results demonstrate the effectiveness of our proposed localization methodology.\nMoreover, ORCALOCA demonstrates strong performance on the Resolved Rate metric, successfully resolving 41.00% (123 out of 300) issues in the SWE-bench Lite dataset. By integrating the editing capabilities of Agentless-1.5, we achieved 6.67 percentage points improvement in function match rate and 6.33 percentage points increase in the final resolved rate over its performance. These results establish ORCALOCA as a significant milestone in the research community's efforts toward developing more robust autonomous software engineering solutions."}, {"title": "4.2.2. IMPACT OF LOCALIZATION ON RESOLVED RATE", "content": "4.2.3. UNIQUE LOCALIZATIONS AND SOLUTIONS\nWe analyze the unique issues localized and resolved by ORCALOCA compared to other open-source agents including Agentless (Xia et al., 2024), AutoCodeRover (Zhang et al., 2024b) and OpenHands (Wang et al., 2024b). As shown in Figure 4, ORCALOCA uniquely localized 6 issues, demonstrating the effectiveness of our approach. Additionally, it resolved 8 unique issues, emphasizing the impact of accurate localization in ASE. These results highlight ORCALOCA 's capability as a strong complement to other systems, even if they are developed with significantly larger resources (like OpenHands).\n4.2.4. ABLATION STUDIES\nWe conducted our ablation study on SWE-bench Common, a smaller subset of SWE-bench Lite, to evaluate the contributions of each proposed method. As shown in Table 3, removing any of these methods caused a noticeable performance drop of approximately 3\u20135 percentage points. Specifically:\n\u2022 Priority Scheduling (Section 3.2): Eliminating scheduler priority weakened ORCALOCA 's heuristic planning ability, making it more susceptible to distractions from less important content.\n\u2022 File & Class / Disambiguation Decomposition (Section 3.3): Removing the decomposition approach restricted ORCALOCA 's ability to explore a broader search space, thereby reducing overall performance. Notice here through the experiment we prove the LLM is hard to locate with correct info by only getting the disambiguation info (See Figure 3. (b)).\n\u2022 Distance-Aware Context Pruning (Section 3.4): Without distance-aware context pruning, ORCALOCA was forced to handle a larger and noisier context, making it significantly more difficult to focus on the most relevant code snippet. Thus the noise will degrade the final bug localization accuracy."}, {"title": "5. Conclusion", "content": "To evaluate how ORCALOCA 's improved localization enhances the final patch resolved rate, we fully reproduced Agentless-1.5 (Xia et al., 2024) on SWE-bench Lite as a baseline. As shown in Table 2, ORCALOCA outperforms Agentless-1.5 across all three key metrics: Resolved Rate, Function Match Rate and Function Match Precision.\nAgentless-1.5 reports two sets of localization metrics due to its multi-sampling approach (four localization attempts per instance in the official reproduction). Patch generation then evenly distributes these samples, producing 10 patches per localization result (40 in total, as per Section 4.1.3). To fairly evaluate localization performance under this setting, we compute metrics using two aggregation methods:"}, {"title": "6. Other Competing Methods", "content": "We presented ORCALOCA, a framework designed to enhance software issue localization by incorporating innovative methodologies such as priority-based scheduling for LLM-generated actions, action decomposition with relevance scoring, and distance-aware context pruning to streamline the search process and improve localization accuracy. On the SWE-bench Lite benchmark, ORCALOCA achieved a 65.33% function match rate, establishing a new open-source state-of-the-art (SOTA) for software issue localization. Furthermore, by integrating the patch generation component from another open-source framework, ORCALOCA attained a final resolution rate of 41.00%, achieving a 6.33 percentage points improvement over the original framework. These contributions not only advance the field of ASE but also provide a modular framework that may inspire future research in integrating LLMs with automated debugging systems."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."}, {"title": "A. Code Graph Details", "content": "\u2022 Blackbox AI Agent (Blackbox, 2024) is building coding agent to transform the way we build software.\n\u2022 Gru(2024-12-08) (Gru, 2024) builds different agents to solve different software engineering problems. But all Grus are built with the same principles: Clear Problem Domain, Dedicated Tools and Direct Value Delivery.\n\u2022 Globant Code Fixer Agent (Globant, 2024) is an independent and intelligent software entities designed to transform business operations.\n\u2022 devlo (devlo, 2024) boosts user's productivity by handling development tasks, freeing user to focus on innovation and ship products faster.\n\u2022 OpenCSG Starship Agentic Coder (OpenCSG, 2024) is a multi-agent collaborative and scalable environment to empower user in building the next generation of intelligent applications.\n\u2022 Bytedance MarsCode Agent (Liu et al., 2024) is a novel framework that leverages LLMs to automatically identify and repair bugs in software code.\n\u2022 Alibaba Lingma Agent (Ma et al., 2024b) understands the whole software repository to achieving automatic software engineering.\n\u2022 Kodu-v1 (Kodu-AI, 2024) implements a VS Code extension that adapts to user's skill level, helping user bring ideas to life faster than ever before.\n\u2022 OpenHands + CodeAct v2.1 (Wang et al., 2024b) is a platform for the development of powerful and flexible AI agents that interact with the world in similar ways to those of a human developer: by writing code, interacting with a command line, and browsing the web.\n\u2022 PatchKitty-0.9: It may have been developed concurrently with our work and is reportedly designed by researchers from UC Santa Barbara. While it was claimed to be open-source in its SWE-bench Lite submission, no repository or related links have been released yet.\n\u2022 Composio SWE-Kit (2024-10-30) (Composio, 2024) helps user connect AI agents to external tools like Gmail, GitHub, Salesforce, etc. It's like a bridge between user's AI and the tools it needs to get work done.\n\u2022 Moatless Tools (Moatless, 2024) is a hobby project where the authors experiment with some ideas they have about how LLMs can be used to edit code in large existing codebases. They believe that rather than relying on an agent to reason its way to a solution, it is crucial to build good tools to insert the right context into the prompt and handle the response.\n\u2022 AutoCodeRover-v2.0 (Zhang et al., 2024b) is an automated approach for solving Github issues to autonomously achieve program improvement, where LLMs are combined with sophisticated code search capabilities, ultimately leading to a program modification or patch.\n\u2022 Agentless-1.5 (Xia et al., 2024) is an agentless approach to automatically resolve software development issues. Compared to the verbose and complex setup of agent-based approaches, it employs a simplistic three-phase process of localization, repair, and patch validation, without letting the LLM decide future actions or operate with complex tools.\n\u2022 RepoGraph (Ouyang et al., 2024) is a plug-in module that manages a repository-level structure for modern AI software engineering solutions.\n\u2022 HyperAgent (Phan et al., 2024) is a novel generalist multi-agent system that addresses a broad spectrum of SE tasks across multiple programming languages by emulating the workflows of human developers.\n\u2022 SWE-agent (Yang et al., 2024b): is a system that facilitates LM agents to autonomously use computers to solve software engineering tasks. SWE-agent's custom agent-computer interface (ACI) significantly enhances an agent's ability to create and edit code files, navigate entire repositories, and execute tests and other programs."}, {"title": "A.1. Graph Construction Process", "content": "A. Code Graph DetailsThe CodeGraph represents the structural and semantic relationships within a codebase by integrating containment and reference relationships. It is constructed using Abstract Syntax Tree (AST) analysis and additional directory-based hierarchical relationships."}, {"title": "A.2. Containment Graph Construction", "content": "The containment graph models the lexical and structural hierarchy of the codebase. We extract entities by analyzing each file in the repository using AST, identifying: Classes: vclass, Functions: vfunction, Methods: vmethod, files: vfile\nA containment edge e\u2081 is added to represent hierarchical relationships: vmethod \u2192 vclass \u2208e1, vfunction file\u2208e1\nAlthough directories are not code entities, we explicitly include them in the CodeGraph to preserve structural context. The directory structure is modeled as follows:\n\u2022 Files within the same directory are connected via containment edges.\n\u2022 A directory node is linked to its subdirectories.\n\u2022 The root directory (\".\") connects to all 1-depth subdirectories and files, forming the top-level hierarchy:"}, {"title": "A.3. Reference Graph Construction", "content": "directory \u2192 vroot \u2208 e1", "edges": "vcaller \u2192 v"}]}