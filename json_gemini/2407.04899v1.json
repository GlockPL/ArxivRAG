{"title": "Algorithmic Language Models\nwith Neurally Compiled Libraries", "authors": ["Lucas Saldyt", "Subbarao Kambhampati"], "abstract": "Important tasks such as reasoning and planning are fundamentally algorithmic,\nmeaning that solving them robustly requires acquiring true reasoning or planning\nalgorithms, rather than shortcuts. Large Language Models lack true algorith-\nmic ability primarily because of the limitations of neural network optimization\nalgorithms, their optimization data and optimization objective, but also due to archi-\ntectural inexpressivity. To solve this, our paper proposes augmenting LLMs with\na library of fundamental operations and sophisticated differentiable programs, so\nthat common algorithms do not need to be learned from scratch. We add memory,\nregisters, basic operations, and adaptive recurrence to a transformer architecture\nbuilt on LLaMA3. Then, we define a method for directly compiling algorithms\ninto a differentiable starting library, which is used natively and propagates gra-\ndients for optimization. In this preliminary study, we explore the feasability of\naugmenting LLaMA3 with a differentiable computer, for instance by fine-tuning\nsmall transformers on simple algorithmic tasks with variable computational depth.", "sections": [{"title": "Introduction", "content": "Machine learning is relaxed program induction, where, implicitly or explicitly, the goal is to find\nprograms that accomplish a given task [1, 2, 3, 4]. For example, a large language model trained\non math problems must implicitly learn a calculator program internally. Furthermore, models may\naim to induce more complex internal programs, such as sorting algorithms, planning algorithms, or\ncombinatorial solvers. However, gradient descent has no guarantee of recovering such programs,\nand often approximates them via statistical features and other shortcuts [5, 6]. To address the issue\nof inducing already-known programs from data, we define a scalable adaptive neural compilation\nalgorithm, which compiles code into neural network parameters [7, 8, 9, 10], and use this to augment\na large language model with a library of differentiable programs. Also, we enable the model to learn\ncompositions of subprograms, providing a strong inductive bias for general algorithmic ability [11].\nLanguage models are optimized to model natural language through objectives like masked-token or\nnext-token prediction. However, these objectives are often insufficient for the emergence of authentic\nreasoning ability, even when it may appear superficially [12, 5, 6, 13, 14]. In general, this lack of\nreasoning ability is a fundamental flaw that is not easily mitigated via prompting or fine-tuning [15,\n12]. First, algorithmic reasoning, by definition, requires an architecture to be universally expressive.\nHowever, transformer expressivity is upper-bounded by TC\u00ba [16]. Furthermore, there is ample\nempirical evidence that optimization does not recover even programs within TC\u00ba [12, 6].\nAugmentation aims to address the limitations of large language models. For instance, a language\nmodeling objective is often insufficient to induce a robust calculator sub-program, so it is common to\naugment a language model with a calculator. Even when appropriate tools are available, a model must\nuse them correctly, by providing the right inputs to the right tool in the right context. We call this the"}, {"title": "1.1 Neural Compilation", "content": "Neural compilation is a technique for deterministically transforming code into neural network parame-\nters that express the exact same program in a given architecture. Precursors to neural compilation were\nfirst discussed in Siegelmann and Sontag, and then implemented in Gruau et al. [18, 7]. However, the\nfirst adaptive (trainable) neural compilation technique was first defined in Bunel et al. [8]. Similarly,\nthere are modern approaches to neural compilation, based on the transformer architecture, but these\neither focus on interpretability, are not universal, or are not adaptive [19, 9, 20, 16, 10]."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Previous Neural Compilation Techniques", "content": "Adaptive Neural Compilation augments a recurrent neural network with memory, registers, and\na differentiable interpreter for a minimal assembly language [8]. Then, [8] compiles algorithms by\nsolving for weights as linear equations. This model relied on a lookup-table based ALU, unit vector\nnumeric encodings, dot-product memory/register lookups, and probability mixtures for control flow.\nThis work focused on learning contextual programs, but in contrast we focus on compilation as a\nmeans to specify algorithms to Large Language Models.\nRASP/Tracr/CoNN describe a neural compilation technique for unaugmented transformers, aimed\nat interpretability. Specifically, RASP defines a minimal language [19], Tracr defines a working\ncompiler [9], and CoNN exploits the Tracr compiler to augment a transformer. While CoNN compiled\naddition and subtraction, their mixture-of-experts approach has a basic calculator directly output the\nanswer as a series of tokens, which is limited only to very simple problems and does not support\ncompositionality or training for new tasks [21]. In comparison, our work is the first to experiment\nwith end-to-end trained large language models augmented with universal programs.\nLooped Transformer constructs a universal machine within a recurrent transformer architecture.\nHowever, it is not intended to be adaptive, nor is it explicitly constructed for library learning or\nintegration with pretrained LLMs [10]."}, {"title": "2.2 Differentiable Computing and Program Synthesis", "content": "Differentiable computing is the idea that programs can be approximated by neural networks by\ndefining differentiable primitives that support universal computation, for instance by using softmax for\narray access. Recurrent neural networks and LSTMs are early instances of differentiable computers,\nand generally performed well for several decades, but in the limit cannot learn and generalize\narbitrary programs from data [22]. One potential reason for these failures is a lack of inductive bias\nvia expressive primitives, but the critical reason is optimization difficulty [23].\nNeural Turing Machines [24, 25] construct a sophisticated differentiable computer, and demonstrate\nits application to complex tasks, such as inducing sorting, planning, or graph algorithms. NTMs\nare foundational to differentiable computing, however, they are exceptionally hard to train, even in\ncomparison to RNNs and LSTMs [26]. This raises possibility of architectures which achieve both the\nexpressiveness of NTMS and the trainability, parallelism, and capacity of transformers [27]."}, {"title": "3 Model", "content": "Our model augments the LLaMA 3 transformer architecture with a differentiable computer, \u2206, and\nassociated program library A. Fundamentally, an intermediate layer of the transformer provides\ninputs to the differentiable computer and selects programs to run. The differentiable computer A is\nbased on the register machine introduced in Bunel et al. [8]. This machine interprets a set of assembly\ninstructions A. A program p is a sequence of these instructions, and the library A is a collection of\nprograms. The computer has state S in the form of memory M and registers R, and tracks execution\nwith an instruction counter c and halting probability h.\n\\(S = (M, R, c, h)\\)"}, {"title": "3.1 Library Structure", "content": "The fundamental contribution of this paper is a differentiable standard library of programs. The\noverall model uses the program library by classifying (selecting) programs and inputs to run.\nCall Instruction Creating a differentiable library fundamentally relies on introducing a method\nfor calling functions arbitrarily. To achieve this, we add a call primitive, supported by a store\ninstruction, which stores the current program counter in a given register. Doing this allows returning"}, {"title": "3.2 Model Background", "content": "Differentiable Memory Bunel et al. defines differentiable memory as a matrix Mij, where the\ndimension i is an address and the dimension j is an encoding. An address a is a unit vector, produced\nvia softmax output. Reading from memory at an address is done with the dot product:\n\\(r_j = M_{ij}a_i\\)\nWriting a vector to memory requires updating all of memory using probability mixtures. First, for an\naddress a, vector c being written to a, and overall probability of writing, p, a memory update is:\n\\(M_{write} = (1 - a) \\odot M_{old} + a \\odot c\\)\n\\(M_{new} = (1 - p) \\odot M_{old} + p \\odot M_{write}\\)\n(1-a) \\(M\\) represents kept (unaltered) memory content, and a \\(c\\) represents new, written content.\nDifferentiable Registers Registers are defined as a matrix Rij. To write an output c to address a:\n\\(R_{ij} = (1 - a) R_{ij} + a c\\)\nReading from an address a to a value v is done with a dot product: \\(v_j = R_{ij}a_i\\). The distinction\nbetween memory and registers is that instruction inputs/outputs use registers, not memory. Also,\nregisters are always written at every timestep by any instruction, while memory is only written from\nread or write instructions when they have non-zero probability.\nDifferentiable Register Machine The computer executes a set of assembly instructions, A, repre-\nsenting the computer's language. A differentiable program o is structured as a list of these instructions\nand their arguments, where each instruction can be accessed at its address. These addresses are\ntracked via a special instruction counter, c. Then, a differentiable interpreter I runs instructions A in\norder to execute the program. See Listing 3.2 for examples of control flow."}, {"title": "Probabilistic Execution", "content": "Instructions, the program counter, and addresses are represented as\nmultinomial probability distributions output by softmax. Accordingly, the program and interpreter\nis always in superposition. Instead of running a single instruction at a time, the interpreter runs\neverything, everywhere, all at once, but with execution and results weighted by the distributions for\ninstructions, program counters, and addresses. In the case that every distribution is dirac-delta (100%\nprobability of one possibility), then execution is fully deterministic. See Figure 1.\nProgram execution is tracked as a probability mixture between incrementing the instruction counter\nand jumping to a new location l in the program, based on the condition probability p:\n\\(C_{t+1} = (1 - j) \\cdot inc(c_t) + j \\cdot ((1 - p) \\cdot inc(c_t) + p \\cdot l)\\)"}, {"title": "Differentiable Interpreter", "content": "An interpreter \\(I : S_t, 1 \\rightarrow S_{t+1}\\) runs each instruction by querying a\n4D lookup table. This model is based on one-hot encodings of size n, so this lookup table T has\ndimensions |A| \u00d7 n \u00d7 n \u00d7 n. This table is filled according to each instruction, with special cases\nfor reading or writing to registers/memory. For instance To, :, :, : is the addition table. To run a\ninstruction, first the register values are resolved to \\(u_j, v_j = R_{ij}r_i \\forall r\\). The final lookup is:\n\\(\\iota_l = T_{ijkl} f_i u_j v_k\\)\nIntuitively, this corresponds to first looking up a particular operation (e.g. add), then the first argument\n(e.g. 2), and then the second argument (2) to get the answer 4. Each instruction specifies a register to\nstore the output in, so finally o\u03b9 is written to R using equation 5. Since each operation f and argument\nr are independent multinomial distributions, this entire operation is probabilistic, so the output o is\npotentially a mixture of running different instructions with different registers."}, {"title": "4 Experimental Results", "content": ""}, {"title": "4.1 Preliminary Studies", "content": "Before scaling to billion-parameter models, we explore behaviors of components of our differentiable\ncomputer, namely lookup tables, circuits, and small programs. These experiments use a minimal\nneural network with one layer before the computer and one layer after, on the premise this will inform\nbehavior at the LLM scale. These networks simply need to route inputs/outputs correctly to/from\nthe computer, which is replaced by parsing in case of LLMs. We find that lookup tables are more\nlearnable than circuits, and that we can learn recursive algorithm routing to a certain depth."}, {"title": "Impact of Recursion Depth on Trainability via Fibonacci", "content": "We use fibonacci as a method for exploring the effect of computation depth on trainability. While the\nfibonacci function has a closed form, for our study we treat it as an inherently sequential algorithm,\nso a recursion depth of 1 entails 8 interpreter steps, and a depth of 2 entails 16 interpreter steps.\nThis degradation in performance is expected, and is meant to establish practical limits of using the\ndifferentiable computer. We observe similar results in the context of transformer networks."}, {"title": "4.2 Small Transformers", "content": "Next we study the behavior of small transformers with natural language inputs. First, we establish\nthat it is possible to train these models to use calculators perfectly, and then we study the effect of\ncomputational-depth on trainability. These experiments use small transformers with between 30\nmillion and 100 million parameters. This architecture is identical to LLaMA 3, but with much smaller\nscale parameters (e.g. hidden size of 128 or 256 and only two transformer layers). Natural language\ninputs are tokenized with the 128K vocabulary LLaMA3 tokenizer."}, {"title": "Modular Arithmetic", "content": "First, we provide the model with a lookup table for mod-128 arithmetic\noperations and train it on natural language versions of one-step arithmetic, e.g. \"Add 3 and 4\".\nSolving this problem is a matter of parsing the sentence to extract the operation and operands, and\nthen providing these to the differentiable calculator. Supervision is given only on answers, via\ncross-entropy. By 62 epochs, the model can use the calculator with 99.2% accuracy on the test set."}, {"title": "Recursive Fibonacci", "content": "Next, we experiment with giving the model a fibonacci program. Again the\nmodel must extract inputs from a natural language sentence, e.g. \"Calculate the fibonacci function,\nstarting from inputs 6 and 2, to a recursion depth of 3\", and provide them to a library function.\nHowever, because this function call has a longer gradient path, it is less trainable than modular\narithmetic."}, {"title": "5 Conclusion", "content": "In this preliminary study, we investigated the feasibility of augmenting large language models with\nlibraries of differentiable programs. To some extent, differentiability is effective in assisting fine-\ntuning. However, there are empirical limits to differentiability, especially as computational depth\nincreases. Our experimental results establish an initial threshold for computational depths that remain\ntrainable. Even within this limit, interesting augmentations are still possible. For example, we\nestablish that a small language model can be fine-tuned to use a calculator effectively.\nNeural compilation reveals interesting connections between programs and neural networks. First is\nthe direct transformation (compilation) from symbolic programs into neural network weights. Second,\nweights can be decompiled back into abstract symbolic programs. This suggests the possibility of\ntightly integrating program synthesis and neural networks in a joint optimization algorithm, of neural\nnetworks with non-trivial formal guarantees, and general interpretability [34, 35, 36, 37, 38]."}, {"title": "A Appendix", "content": ""}, {"title": "B Lookup Tables", "content": "Differentiable Lookup Tables Tables trade memory for\ncomputation by pre-calculating the answers to input combi-\nnations. Intuitively, these take similar form to grade-school\narithmetic tables (right). To access a lookup table differen-\ntiably, one-hot encoded unit vectors are used as indices for\nlookup via sequential dot products. For instance the number\n2 encodes to [00100] for encoding n = 5. A dot product in\none axis is equivalent to selecting the row or column contain-\ning 2, e.g. [02468]. If the other operand is 4 ([00001]), then\na second dot product selects the final element, 8, which is the\nanswer to 2 x 4, the two index vectors. In practice, answers\nin a lookup table such as this are encoded using unit vectors,\nmaking a 3D tensor Mijk where the axes i and j correspond\nto the first operands, and k is the encoding dimension of the\nanswer. Then, a lookup is the einstein summation:"}, {"title": "C Numerical Representation", "content": "One-Hot Number Encodings Algorithmic ability is closely tied to numeracy. However, neural\nnetworks are not natively good at representing numbers. Often, numbers are represented using unit\nencodings, e.g. [00100]. This can be desirable for doing dot-product based lookup, or when viewing\nnumbers as features, but it is undesirable for scaling properties. The sparsity of these representations\ncan be advantageous in some ways (number representations are not entangled), but disadvantageous\nin others (a single number provides only a single bit of supervision, and geometric distance does not\ncorrespond to number distance). Probabilistic unit encodings are calculated using a softmax layer to\nnormalize a dense vector into a multinomial probability distribution. This can represent a wide range\nof other distributions.\nBinary Number Encodings Binary representations are highly advantageous in classical computer\nscience, as they allow encoding an exponential amount of numbers in a linear space, e.g. for a bit\nvector of length n, we can represent numbers up to 2n - 1. However, binary representations may\nbe too entangled to be used as features in neural networks, e.g. the binary encodings for 2 and 3\n([10] and [11]) overlap in the most significant bit. A unit vector has a native interpretation as a direct\nprobability distribution over numbers, while a binary vector has a probabilistic interpretation for each\nbit. However, a probabilistic unit vector encodes more possible distributions than a binary one.\nBinary to Unit Conversions Ideally, numbers are always represented in binary, except for when\nthey are needed as features or for differentiable indexing for lookup tables or memory. Accordingly,\nwe wish to define bidirectional conversions between binary and unit vectors. In particular, we want\nto preserve the probabilistic representations of both encodings. To lookup binary vectors from unit\nencodings, we simply do a dot-product lookup with a n \u00d7 b table of binary encodings. The reverse\ndirection is less obvious, as we are going from Rlog(n) back to Rn. However, it admits a closed form\nsimilar to the binomial distribution, but for independent trials. This represents, intuitively, flipping a\ncoin at each bit to produce different numbers. For instance, for a 2-bit vector b, with bits bo and b\u2081, the one-hot conversion is \\([(1 \u2013 b\u2081)(1 \u2013 b\u2080)  (1 \u2013 b\u2081)b\u2080  b\u2081 (1 \u2013 b\u2080)  b\u2081b\u2080]\\), which generalizes to\nhigher dimensions.\nFreezing Library Programs A typical gradient update modifies every parameter 0 at once:\n\\(\\theta_t = \\eta\\nabla L(\\theta, x) + \\theta_{t-1}\\)\nHowever, simply masking parameters element-wise prevents overwriting neural network sections we\nwould like to keep, such as programs in the library. The mask M is simply a tensor the same shape\nas 0, with 1 for every parameter we'd like to update, and 0 otherwise.\n\\(\\theta_t = \\eta\\nabla L(\\theta, x) \\odot M + \\theta_{t-1}\\)"}, {"title": "Neural Compilation by Memorization", "content": "In many contexts, exact algorithmic behavior is desirable,\nbecause of the robustness and generalization that it provides. However, there are contexts where\nlearning approximate algorithmic behavior is sufficient or even desirable. Also, we may wish to\nspecialize programs to a known, static input distribution. When either of these goals are true,\nanalytical neural compilation is not scalable. In general, contextual programs, require far more\nexpressive program controllers. In this context, we hope to find neural network initializations\nrather than full solutions. The architecture of the differentiable computer allows us to provide\nintermediate supervision in the form of programs, and train 8 directly on programs. Accordingly, we\ncan approximately compile a program by memorizing it. In other words, we advocate for intentionally\noverfitting a sub-network to output a particular program. We call this neural compilation by\nmemorization, and explore the trade offs of using this approach.\n\\(\\theta = \\underset{\\Theta\\in\\Theta}{\\text{min}} L(d(\\theta, e(x)))\\)"}, {"title": "D Technical Background", "content": "First, we provide background on common building blocks used in differentiable computing, and\ndefine our full model in Section E. Lookup tables and memory are primarily derived from the math\npresented in [8] and [24, 25, 2].\nDifferentiable Lookup Tables trade memory for computation by pre-calculating the answers to\ninput combinations. For binary operations like addition, a lookup table is a 3D tensor Mijk where the\naxes i and j correspond to the first operands, and k is the encoding dimension of the answer. Then,\na lookup is the summation \\(C_k = M_{ijk}a_i b_j\\). Now we define a lookup table for multiple operations,\ne.g. the four basic arithmetic operators, or common fundamental programming operations such as\nmax/min/inc/dec. To do so, a new leading dimension is added for the operator, so a lookup table\nbecomes a 4D tensor Thijk, which is indexed via three dot products, corresponding to looking up an\noperator, and then operands, sequentially. This is written with the Einstein summation:\n\\(C_k = T_{hijk} f_h a_i b_j\\)\nWhen memory is abundant, lookup tables are extremely favorable, as they have shallow and stable\ngradient paths (since they are only tensor contractions). An issue with lookup tables, and more broadly\nwith unit vector encodings, is that they scale poorly with respect to the maximum representable\nnumber. If the maximum number is n 1, then a unit vector is length n (assuming zero is included).\nA binary-arity lookup table will be n \u00d7 n \u00d7 n, and a composite lookup table will be o xnxnxn\nfor o operations. Fundamentally, this is not scalable enough to enable arbitrary multiplication beyond\nvery small scales, e.g. even representing a n = 1024 lookup table requires 32Gb of memory. This\nlimitation is a byproduct of unit encodings. Alternatives include using binary number encodings or\ncircuit-like representations for basic operations.\nDifferentiable Circuits An alternative to lookup tables is to encode basic operations via circuits.\nThis is done by defining differentiable relaxations of common logic gates, and then building conven-\ntional circuits, such as the ripple-carry addition circuit, from them. This has been explored several\ntimes in previous literature, and there are multiple options for defining differentiable logic gates with\ndifferent trade-offs [39]. We opt for probabilistic interpretations of and, or, not and xor:\nand(a, b) = a.b\nor(a, b) = a. b + (1 \u2212 a) \u00b7 b + a. (1 \u2212b)\nxor(a, b) = (1 \u2212 a) \u00b7 b + a \u00b7 (1 \u2013 b)\nnot(a) = 1-a\nMaking larger differentiable circuits is a simply a matter of re-defining classical circuits with these\ndifferentiable gates. Accordingly, we define differentiable circuits for addition, multiplication,\nsubtraction, and long division. Compared to lookup tables, circuits require minimal storage space and\ngeneralize indefinitely. However, they require binary representations and have long gradient paths."}, {"title": "E Model", "content": ""}, {"title": "E.1 Differentiable Computer", "content": "We introduce a differentiable computer A as program controllers & with parameters 0, assembly\ninstruction set A, interpreter I, and state tuple S. Each state consists of memory M, registers R,\ninstruction counter c, and a halting probability h. Furthermore, we introduce a program library L,\nwhich augments the low-level language instructions in A with multi-instruction programs written\nin terms of A. This entails that & produces high-level programs, which can call both low-level\ninstructions and programs in the standard library.\n\\(\u2206 = (\u03b4, 0, A, I, L, S)\\)\n\\(S = (M, R, c, h)\\)\nA controller 8 is a parameterized function which produces probabilistic programs, from inputs z. By\nintroducing z, it becomes possible to have contextual programs which vary with the overall input.\n\\(\u03b4: \u03b8, \u03b6 \\rightarrow \u03c1\\)\nProbabilistic programs Qij are represented as matrices, where axis i corresponds to the program\ncounter, and axis j is an instruction encoding. Accordingly, an individual instruction is obtained by\ntaking a dot-product with the program counter, producing a probabilistic mixture over instructions:\n\\(L_i = Q_{ij} C_i\\)"}]}