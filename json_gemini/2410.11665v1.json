{"title": "VisualRWKV-HD and UHD: Advancing High-Resolution Processing for Visual Language Models", "authors": ["Zihang Li", "Haowen Hou"], "abstract": "Accurately understanding complex visual information is crucial for visual language models (VLMs). Enhancing image resolution can improve visual perception capabilities, not only reducing hallucinations but also boosting performance in tasks that demand high resolution, such as text-rich or document analysis. In this paper, we present VisualRWKV-HD and VisualRWKV-UHD, two advancements in the VisualRWKV model family, specifically designed to process high-resolution visual inputs. For VisualRWKV-HD, we developed a lossless downsampling method to effectively integrate a high-resolution vision encoder with low-resolution encoders, without extending the input sequence length. For the VisualRWKV-UHD model, we enhanced image representation by dividing the image into four segments, which are then recombined with the original image. This technique allows the model to incorporate both high-resolution and low-resolution features, effectively balancing coarse and fine-grained information. As a result, the model supports resolutions up to 4096 x 4096 pixels, offering a more detailed and comprehensive visual processing capability. Both VisualRWKV-HD and VisualRWKV-UHD not only achieve strong results on VLM benchmarks but also show marked improvements in performance for text-rich tasks.", "sections": [{"title": "1 Introduction", "content": "With the recent significant advancements in large language models(Aky\u00fcrek et al., 2024)(LLMs)(Achiam et al., 2023), visual language models (VLMs) have also rapidly progressed. The efforts(Peng et al., 2023) to extend LLMs to handle visual inputs through visual instruction tuning are growing steadily(Liu et al., 2024). Concurrently, visual language models with linear time complexity, such as VisualRWKV(Hou et al., 2024) and VL-Mamba(Qiao et al., 2024), have also been proposed. However, research on efficiently processing high-resolution visual inputs within linear visual language models remains lacking. Enhancing image resolution can improve visual perception capabilities, not only reducing hallucinations but also boosting performance in tasks that demand high resolution, such as text-rich or document analysis. However, the challenge with high-resolution images is that they often lead to increased computational demands and longer input sequences, which can hinder model efficiency and performance(Hou et al., 2024).To address these challenges, in this paper, we introduce two novel advancements in the VisualRWKV model family: VisualRWKV-HD and VisualRWKV-UHD.  These models are specifically designed to capitalize on the benefits of high-resolution visual inputs while maintaining computational efficiency and performance, marking the first advancement of linear RNN models to solve high-resolution tasks. There are four main contributions in this paper:\n1. VisualRWKV-HD with Ensemble of Encoders: We introduce an ensemble of encoders, where the input image size is fixed at 1024 during the pre-training of SigLip, DINOv2, and Segment Anything Model (SAM). As a base model, SAM is expected to exhibit generalization capabilities across various downstream tasks with different image sizes. This is particularly important for high-definition (HD) datasets, which have larger sizes and more detail. SAM performs well when the image resolution aligns with its training resolution of 1024. Therefore, we used SAM to support resolutions up to 1024 x 1024, achieving significant performance improvements on several benchmarks, such as TextVQA.\n2. VisualRWKV-UHD: In VisualRWKV-UHD, the image is divided into four segments and then recombined, allowing the image features to contain both high-resolution and low-resolution information. This approach balances coarse and fine-grained features, allowing the model to support resolutions up to 4096 x 4096 while ensuring that the number of image tokens does not exceed 1024.\n3. MLP with Context Gating: We found that excessive feature information led to internal competition within the model. To address this, we introduced MLP with Context Gating to replace the linear projection layer, stabilizing the training process and improving performance.\n4. High-Resolution and Low-Resolution Alignment: To align high-resolution vision encoders with low-resolution modules, we propose an alternative approach that combines every 2x2 block (each containing four adjacent vectors) into a new channel dimension. This method preserves information without the need for additional training.\nIn summary, this study presents the VisualRWKV-HD and VisualRWKV-UHD models. VisualRWKV-HD introduces a pre-trained ensemble of vision encoders, increasing the supported resolution to 1024 x 1024. On the other hand, VisualRWKV-UHD is designed for efficiency, capable of handling even higher resolutions with support for inputs up to 4096 x 4096 pixels. Importantly, it maintains a maximum"}, {"title": "2 Related Works", "content": "In the rapidly evolving field of visual language models (VLMs)(Bai et al., 2023), linear models have introduced innovative strategies for integrating image and text data. These models, characterized by their straightforward architectures, have proven effective in tasks such as image captioning and visual question answering. Linear Visual Language Models have made significant progress in efficiently merging visual and textual information. VL-Mamba(Qiao et al., 2024) stands out for its emphasis on robust alignment between visual and textual modalities, leveraging a modular design that enables flexibility across various applications. This adaptability allows the model to cater to a wide range of tasks while maintaining high performance.On the other hand, VisualRWKV(Hou et al., 2024), including its high-definition variants, tackles the complexities of high-resolution image processing by employing techniques such as lossless down-sampling and image segmentation. These methods facilitate the effective integration of both high and low-resolution features, ensuring that essential details are preserved. Together, these models not only signify important advancements in the VLM landscape but also underscore innovations in efficiency, detail retention, and multi-modal alignment. As they continue to evolve, these linear visual language models are set to propel further research, particularly in the domain of handling complex visual information."}, {"title": "2.2 High-Resolution Visual Language Models", "content": "LLaVA-UHD(Xu et al., 2024) is a model specifically designed for high-resolution visual tasks, integrating advanced multi-modal capabilities that enhance its applications in image captioning and visual question answering. One of its key strengths lies in its ability to seamlessly integrate visual and textual data, allowing it to generate contextually relevant outputs based on complex visual inputs. This integration is achieved through optimized model architecture, improving efficiency and adaptability across various tasks. The model has been trained on extensive datasets, enhancing its generalization capabilities. This training enables LLaVA-UHD to effectively comprehend fine visual details and nuances, making it particularly suitable for applications that require high precision in visual understanding."}, {"title": "2.3 Comparative Analysis", "content": "To elucidate the innovations in VisualRWKV-HD and VisualRWKV-UHD, we compare them with existing models such as LLAVA-UHD and Visual-RWKV. VisualRWKV combines visual encoders and language models for cross-modal understanding but struggles with high-resolution images due to computational bottlenecks. LLAVA-UHD addresses high-resolution processing by employing multi-scale feature fusion and adaptive resolution mechanisms, though this can lead to high computational demands. In contrast, VisualRWKV-HD uses a pre-trained SAM visual encoder to handle high-resolution inputs efficiently without increasing input sequence length, and it optimizes model architecture to reduce complexity. VisualRWKV-UHD further integrates high-resolution and low-resolution features through image segmentation and introduces the MLPWithContextGating mechanism for better detail retention and training stability. Overall, VisualRWKV-HD and VisualRWKV-UHD offer improved efficiency and detail preservation in high-resolution image processing compared to LLAVA-UHD and VisualRWKV."}, {"title": "3 Method", "content": ""}, {"title": "3.1 VisualRWKV-HD", "content": ""}, {"title": "3.1.1 Ensemble of Encoders", "content": "In previous versions of VisualRWKV, we utilized SIGLIP and DINO encoders focused on processing low-resolution images, achieving good results. In VisualRWKV-HD, we introduced a pre-trained high-resolution SAM vision encoder, enhancing the model's supported resolution to 1024 x 1024. This improvement significantly boosted the model's performance across multiple benchmark tests, making it more efficient and accurate in handling tasks that require rich details and visual complexity. The inclusion of the SAM encoder allows the model to better capture critical features in images, enhancing its overall visual understanding capabilities."}, {"title": "3.1.2 Lossless DownSampler", "content": "SigLip and DINOv2, as encoders from the previous generation of VisualRWKV, have shown effectiveness in low-resolution image tasks. In this generation, VisualRWKV-HD introduces the SAM encoder. To address the alignment issue with the SAM encoder, we designed a lossless downsampler that combines 2x2 blocks (each containing four adjacent vectors) into a new channel dimension. This method allows the high-resolution vision encoder to effectively align with lower-resolution modules without losing information during training. Experimental results indicate that this approach effectively preserves image details and enhances model performance.\nYou could use a formula to represent the process of combining the 2x2 blocks into a new channel dimension. Here's a suggested formula:\n$C_{new} = Concat(C_1, C_2, C_3, C_4)$"}, {"title": "3.2 VisualRWKV-UHD", "content": "In this model, we further split the image into four parts and then aggregate them, ensuring that the image representation contains both high-resolution and low-resolution features, as shown in Figure 2. This strategy balances coarse features and fine-grained features, increasing the supported resolution to 4096 x 4096. Through this innovative method, the model can more accurately understand and analyze different details of input images when dealing with complex visual information."}, {"title": "3.3 Projection Layer", "content": "During training, we discovered that the excessive amount of feature information led to adversarial effects within the model. To address this issue, we introduced MLP With Context Gating to replace the traditional linear projection layer.\nContext Gating(Miech et al., 2017), a mechanism used to improve neural networks by dynamically adjusting feature representations.. MLP with Context Gating enhances the performance of a multi-layer perceptron (MLP) by adding a gating mechanism that modulates the input features. The Context Gating layer applies a non-linear transformation to the input using a learned weight matrix and a bias term, which are passed through a sigmoid activation. The resulting gate output is multiplied element-wise with the original input, effectively controlling which features are passed through based on the context.\n$y = \\sigma(W_g x + b_g) \\otimes x$"}, {"title": "4 Experiments", "content": "In the Experiment section, we evaluate the performance of VisualRWKV-HD and VisualRWKV-UHD models across a variety of tasks, focusing on their ability to handle high-resolution visual inputs effectively. We conducted experiments on several widely-used visual language model (VLM) benchmarks, with a particular emphasis on text-rich and document analysis tasks that benefit from high-resolution image processing."}, {"title": "4.1 Baselines", "content": "In the Baselines section, we compare the performance of the proposed VisualRWKV-HD and VisualRWKV-UHD models with the standard VisualRWKV model to understand the enhancement in model capabilities due to high-resolution processing. The standard VisualRWKV(Hou et al., 2024) serves as a baseline, representing the foundational architecture of our model family but lacking the high-resolution enhancements. By comparing performance metrics across different versions, we aim to clarify the specific impact of high-resolution processing on text-rich tasks and document analysis."}, {"title": "4.2 Benchmarks", "content": "We conducted extensive evaluations of VisualRWKV-HD and VisualRWKV-UHD using eight diverse benchmark datasets: SQA(Lu et al., 2022), TextVQA(Singh et al., 2019), GQA(Hudson and Manning, 2019) (accuracy), VizWiz(Bigham et al., 2010), MME(Fu et al., 2023), POPE(Li et al., 2023), MMB(Liu et al., 2023), and MMB-CN, as shown in Table 1. These datasets encompass a wide range of visual language understanding tasks, each demanding detailed visual and textual interpretation. SQA focuses on scenario-based question answering, requiring the model to comprehend both complex visual and linguistic contexts to provide accurate responses. TextVQA features images embedded with text, challenging the models to accurately extract and interpret textual information from visual inputs. GQA (accuracy) evaluates the models' precision in reasoning over image content, posing questions that demand an in-depth understanding of the image. VizWiz tests the models in a real-world context with noisy and incomplete visual data, a particularly challenging task designed to assist visually impaired individuals.In addition, we incorporated MME for multimodal emotion recognition, where the model is tasked with identifying emotions from both visual and textual inputs. POPE evaluates the model's ability to infer personality traits based on visual content. Lastly, MMB and MMB-CN benchmarks were included to assess the models' performance in multimodal translation and question answering tasks, with MMB-CN specifically designed for Chinese visual language understanding. We also conducted tests on document benchmarks, including DocVQA(Mathew et al., 2021), InfographicVQA(Mathew et al., 2022), and ChartQA(Masry et al., 2022), to evaluate the performance of our proposed method, as shown in Table 2. DocVQA focuses on document understanding, where our model demonstrated strong performance in interpreting mixed text and image content and effectively answering queries. In InfographicVQA, our method successfully analyzes visually complex infographics, accurately identifying key elements and relationships. Lastly, ChartQA assesses our model's ability to interpret various chart types, revealing excellent performance in understanding chart data and providing insights. Overall, these benchmarks highlight the strengths of our approach in enhancing high-resolution image understanding and its applicability across diverse vision-language tasks. These datasets serve as robust benchmarks to evaluate the models' capabilities in handling high-resolution visual inputs across different"}, {"title": "4.3 Quantitative Evaluation", "content": "In the Quantitative Evaluation, we assess the performance of our proposed VisualRWKV-HD and VisualRWKV-UHD models against several benchmarks to highlight their advancements in handling high-resolution visual inputs, as outlined in the abstract. Table 3 presents a comparative analysis between our models and other leading visual language models (VLMs). In the SQA benchmark, VisualRWKV-UHD demonstrates its superior ability to process high-resolution images, scoring 56.97, which surpasses Mobile1.7B's 54.7, underscoring the advantage of our model's resolution enhancements. Similarly, in the GQA benchmark, Mobile1.7B scores 56.1, while VisualRWKV-HD and UHD achieve 60.84 and 59.52, respectively, showing a clear improvement due to our innovative lossless downSAMpling and segmented image representation techniques. For tasks like POPE, where precision is crucial, VisualRWKV-HD outperforms Mobile1.7B with a score of 86.0 versus 84.5, and UHD also exceeds with 85.3. In the MMB benchmark, our models significantly outperform Mobile1.7B, with HD achieving 61.31 and UHD 58.42, compared to Mobile1.7B's 53.2. On the MME Perception task, VisualRWKV-HD excels with a score of 1378.62, noticeably higher than Mobile1.7B's 1196.2, while UHD also performs well at 1321.33. Interestingly, even in comparison to models with larger parameters, such as Mini-Gemini (2B parameters), VisualRWKV-HD (1.6B parameters) delivers superior results, notably in the MME Perception task (HD: 1378.62, Mini-Gemini: 1341) and MMB (HD: 60.31, Mini-Gemini: 59.8). This underscores the efficiency of our models in visual processing. Lastly, our models outperform TinyLLaVa-v1 in the GQA benchmark, with VisualRWKV-HD scoring 60.84 and UHD 59.52, while TinyLLaVa-v1 achieves only 57.5. Overall, the results confirm that the enhancements in visual resolution processing, through lossless downSampling and segmented image techniques, allow VisualRWKV-HD and UHD to excel in high-resolution tasks, providing a clear edge over both lower-resolution models and larger"}, {"title": "4.4 Ablation Study", "content": "In the Experiment section, we assess the performance of the VisualRWKV-HD and VisualRWKV-UHD models across various tasks, emphasizing their effectiveness in handling high-resolution visual inputs. Our experiments target several well-established benchmarks in the realm of visual language models (VLMs), particularly focusing on tasks that are text-rich and involve document analysis, both of which greatly benefit from advanced high-resolution image processing capabilities. This evaluation aims to illustrate how these models perform in real-world applications where detail and clarity are paramount."}, {"title": "4.4.1 Ablation on Vision Decoder", "content": "In this section, we compared visual encoders, specifically Siglip and Siglip + DINOv2, based on a resolution of 384, as shown in Table 4. The results showed a comprehensive performance improvement. We further enhanced the model by integrating SAM into the Siglip + DINOv2 framework, leading to additional performance gains on the SQA, TQA, and MMB/MMBCN datasets.We assessed the impact of using DINOv2 and SAM on training stability and computational efficiency. The key metrics evaluated included training stability and overall computational cost, as well as the performance results across various datasets. After introducing DINOv2 and SAM, the model exhibited enhanced stability during training and improved performance across all datasets. This highlights the significant role that the SAM and DINOv2 visual encoders play in effectively processing high-resolution inputs."}, {"title": "4.4.2 Ablation on Resolution", "content": "In this experiment, we conducted further research based on the introduction of siglip, DINOv2, and SAM by increasing the resolution from 384 to 448, as shown in Table 5. This adjustment led to improved performance of the model on datasets such as SQA, GQA, and VizWiz.We compared the performance of VisualRWKV-HD and VisualRWKV-UHD under different resolution settings, exploring the impact of increased resolution on accuracy and processing time. Experimental Results show that: Higher resolutions significantly enhanced accuracy in text-dense tasks. Although inference"}, {"title": "4.4.3 Ablation on Projection", "content": "In this experiment, we explored the impact of replacing the linear projection method with MLP-WithContextGating at a resolution of 448, as shown in Table 6. The goal was to compare models with and without MLPWithContextGating to assess its effect on training stability and computational efficiency. After incorporating MLPWithContextGating, the model demonstrated significant improvements across various tasks, including TQA, GQA, and POPE. While the degree of enhancement varied, overall performance surpassed that of the model using the linear projection approach. When MLPWithContextGating was removed, the model's training stability decreased, and memory usage increased, highlighting the importance of this technique in handling high-resolution inputs effectively.These findings suggest that MLPWithContextGating plays a crucial role in enhancing computational efficiency and model stability, particularly when processing high-resolution images."}, {"title": "4.4.4 Ablation on Data Scaling up", "content": "In the experiment, we compared the performance of different datasets (mix665k, HD559k, and HD667k) on the VisualRWKV-HD and VisualRWKV-UHD models, focusing on tasks like DocVQA, InfographicVQA, ChartQA, TQA, MME, and VizWiz, as shown in Table 7. The results show that as the dataset size increased, the models' performance improved significantly. By comparing the use of the mix665k, HD559k, and HD667k datasets, we evaluated their impact on the training stability and computational efficiency of the VisualRWKV-HD and UHD models:VisualRWKV-HD: After introducing the HD559k dataset, compared to mix665k, the model demonstrated notable improvements in tasks such as DocVQA, InfographicVQA, and ChartQA. The performance in VizWiz and TQA also increased substantially. This highlights how increasing data size and quality enhances the model's ability to handle complex visual language tasks.\nVisualRWKV-UHD: With the introduction of the HD559k dataset, the performance in TQA and SQA tasks further improved, confirming the effectiveness of the downSAMpling method in preserving data details while enhancing the model's generalization capabilities. By using larger high-resolution datasets, the model can better understand text-rich visual scenes and demonstrate more robust performance across multiple tasks. Additionally, in VisualRWKV-UHD, when processing the HD559k and HD667k datasets, dividing the image into four parts and then aggregating them allows the model to integrate both high-resolution and low-resolution features. This approach strengthens image representation, enabling the model to perform better on tasks requiring detailed visual analysis."}, {"title": "4.5 Efficiency Analysis", "content": "The method proposed in this paper primarily concatenates features along the channel dimension, maintaining a cap of 1024 image tokens without increasing their number. Unlike other methods that inflate the input image tokens and consequently impact the inference speed of vision-language models (VLMs), our approach preserves efficiency.\nDuring the prefill stage, VisualRWKV-UHD processes four times more images than VisualRWKV-HD, which results in slightly lower efficiency but enhances the understanding of high-resolution images."}, {"title": "5 Conclusion", "content": "In this paper, we presented VisualRWKV-HD and VisualRWKV-UHD, two advanced models in the VisualRWKV family, designed to excel in processing high-resolution visual inputs. Through comprehensive evaluations across a range of benchmarks, including SQA, GQA, and VizWiz, we demonstrated that these models significantly outperform traditional approaches, particularly in text-rich and document analysis tasks.\nThe introduction of techniques such as lossless downsampling, segmented image representation, and the integration of robust visual encoders (DINOv2 and SAM) has not only enhanced the accuracy and efficiency of our models but also stabilized training processes. Comparative analysis with LLaVA-UHD revealed that VisualRWKV models achieve a better balance of computational cost, memory efficiency, and processing speed, making them suitable for real-time applications that demand high precision.\nThe findings highlight the importance of high-resolution processing in complex visual language tasks, underscoring the potential of VisualRWKV-HD and UHD to serve as valuable tools in real-world applications requiring detailed visual interpretation. Our results advocate for continued research into optimizing model architectures and techniques to further enhance visual processing"}, {"title": "Limitations", "content": "Despite the significant performance improvements of VisualRWKV-HD and VisualRWKV-UHD models, several limitations persist. Firstly, their high computational and memory requirements may restrict accessibility for users with less powerful hardware, particularly affecting real-time applications. Additionally, the models rely heavily on the quality and quantity of training data; limited availability of high-quality labeled data could compromise their effectiveness. The lack of interpretability in decision-making processes remains a challenge, especially in critical fields like healthcare and finance. Furthermore, while the models excel in benchmark tasks, their generalization capabilities in diverse and unfamiliar scenarios need further validation. Finally, the integration of these models with other modalities, such as audio or tactile data, is an area yet to be fully explored, which could limit their applicability in multi-modal learning contexts. Addressing these limitations is essential for enhancing the practical usability of VisualRWKV models in real-world applications."}, {"title": "A Model Architecture and Computing", "content": "Model Architecture: The VisualRWKV models used in our experiments are visual extensions of the Recurrent Weighted Key-Value (RWKV) architecture, designed to handle both visual and textual data. We experimented with the following configurations:\n\u2022 VisualRWKV 1.6B: A baseline model using 1.6 billion parameters.\n\u2022 VisualRWKV 1.6B + MLP: Enhanced with a Multi-Layer Perceptron (MLP) to improve feature extraction.\n\u2022 VisualRWKV 1.6B + MLP (HD/UHD): Models utilizing High Definition (HD) and Ultra High Definition (UHD) strategies for fine-grained feature extraction.\nComputing Infrastructure : Infrastructure A range of computational resources were employed in the study. The standard training and benchmark evaluation were conducted using 8 NVIDIA A100-80GB GPUs. The VisualRWKV 7B model is trained with 6 A100 GPUs due to insufficient memory capacity with 8 GPUs. For the efficiency analysis, we employed an NVIDIA RTX 3090 GPU.\nComputing Budget: Training an epoch of VisualRWKV 1.6B with 8 A100 GPUs takes 6.7 hours, equivalent to 53.6 GPU hours; Training an epoch of VisualRWKV 3B with 8 A100 GPUs takes 11.3 hours, equivalent to 90.4 GPU hours; Training an epoch of VisualRWKV 7B with 6 A100 GPUs takes 26.5 hours, equivalent to 159 GPU hours\nIn all cases, the RWKV backbone was adapted for visual tasks by incorporating Vision Encoders and using Context Gating. These models were fine-tuned for visual question-answering tasks on various datasets."}, {"title": "B Datasets", "content": "We trained and evaluated the models on the following datasets:\n\u2022 mix665k: This is the dataset used by LLaVA for instruction tuning, comprising 665,000 diverse images aimed at enhancing the model's adaptability to various visual tasks and instructions, thereby improving its overall performance and usability.\n\u2022 HD559k: This dataset is our custom high-resolution dataset consisting of 559,000 high-quality images. It focuses on testing the model's performance when processing high-quality visual content, particularly in terms of detail, color, and clarity, ensuring that the model can accurately capture complex visual information.  provide an overview of the data proportions in the HD559k dataset.\n\u2022 HD667k: As another significant contribution from our team, HD667k is a larger high-resolution dataset containing 667,000 images. This dataset not only enriches the training data for the model but also provides additional support for its performance in diverse and complex visual scenarios, helping to improve the model's generalization ability and robustness in practical applications.  provide an overview of the data proportions in the HD667k dataset."}, {"title": "C Experimental Setup", "content": "Preprocessing: Input images were divided into four sections, each encoded by three vision encoders (SigLIP, DINOv2, SAM). Features were merged and passed through an MLP with Context Gating.\nTraining: The models were trained using the AdamW optimizer with a learning rate of X, using NVIDIA GPUs and mixed precision. Training continued for 100 epochs, with early stopping applied after 10 epochs of no improvement.\nEvaluation: Models were evaluated on the DocVQA, InfographicVQA, and ChartQA datasets. These datasets represent different challenges, from document understanding to infographics and chart analysis."}, {"title": "D Data and Hyperparameters", "content": "We used a two-phase training process for VisualRWKV. In the Feature Alignment Phase, 558K images from LAION-CC-SBU were utilized to connect a frozen vision encoder with a frozen LLM. This phase establishes the foundation for robust image-text alignment. In the Visual Instruction Tuning Phase, an expanded dataset of 150K multimodal examples generated by GPT and 515K VQA datasets were used to enhance the model's capacity for multimodal tasks.All the data used in this paper are consistent with their intended use.\nEthical guidelines were strictly followed in data preparation, focusing on identifying and handling PII and sensitive content via automated tools and manual reviews. Anonymization techniques, such as data masking, were applied to ensure data integrity and privacy."}]}