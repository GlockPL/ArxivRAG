{"title": "Dynamic Spectrum Access for Ambient Backscatter Communication-assisted D2D Systems with Quantum Reinforcement Learning", "authors": ["Nguyen Van Huynh", "Bolun Zhang", "Dinh-Hieu Tran", "Dinh Thai Hoang", "Diep N. Nguyen", "Gan Zheng", "Dusit Niyato", "Quoc-Viet Pham"], "abstract": "Spectrum access is an essential problem in device-to-device (D2D) communications. However, with the recent growth in the number of mobile devices, the wireless spectrum is becoming scarce, resulting in low spectral efficiency for D2D communications. To address this problem, this paper aims to integrate the ambient backscatter communication technology into D2D devices to allow them to backscatter ambient RF signals to transmit their data when the shared spectrum is occupied by mobile users. To obtain the optimal spectrum access policy, i.e., stay idle or access the shared spectrum and perform active transmissions or backscattering ambient RF signals for transmissions, to maximize the average throughput for D2D users, deep reinforcement learning (DRL) can be adopted. However, DRL-based solutions may require long training time due to the curse of dimensionality issue as well as complex deep neural network architectures. For that, we develop a novel quantum reinforcement learning (RL) algorithm that can achieve a faster convergence rate with fewer training parameters compared to DRL thanks to the quantum superposition and quantum entanglement principles. Specifically, instead of using conventional deep neural networks, the proposed quantum RL algorithm uses a parametrized quantum circuit to approximate an optimal policy. Extensive simulations then demonstrate that the proposed solution not only can significantly improve the average throughput of D2D devices when the shared spectrum is busy but also can achieve much better performance in terms of convergence rate and learning complexity compared to existing DRL-based methods.", "sections": [{"title": "I. INTRODUCTION", "content": "With the rapid development of wireless technologies, it is expected that future communication systems, e.g., 5G Advanced/6G, will need to support an enormous number of heterogeneous wireless devices. Many of these wireless devices require short-range, high-rate, and low-latency communications. To accommodate these requirements and increase spectrum efficiency, device-to-device (D2D) communication has been proposed. In particular, D2D communication enables communication between adjacent wireless devices without the use of network infrastructures like base stations (BSs) [1]. As such, D2D communication can enable direct communications between devices and reduce end-to-end latency. A key advantage of the D2D communication technology is that it can reuse the spectrum of cellular systems for direct communications between D2D devices, resulting in better spectrum and energy efficiency [2]. With these features, D2D communication is expected to be an essential part of 5G Advanced and 6G, especially in IoT and vehicular communications [3]. However, D2D communication may introduce interference to cellular users (CUs) if it reuses the same spectrum which the CUs are currently occupying. This is particularly challenging in future wireless networks where a large number of wireless devices operate in a dense area. For that, there is a demand for effective spectrum access strategies for D2D devices when opportunistically reusing the cellular spectrum."}, {"title": "A. Related Work", "content": "Numerous methods have been proposed in the literature to enable dynamic/opportunistic spectrum access to D2D communications. For example, the authors in [4] considered the spectrum sharing problem for D2D communications in cellular networks. By considering both the overlay and under-lay modes, the authors aimed to obtain an optimal spectrum sharing strategy that enables D2D devices to orthogonally share the spectrum with CUs or opportunistically use the frequency/time resources occupied by CUs. Then, analytical rate expressions were derived to optimize the two spectrum sharing modes based on a weighted proportional fair utility function. Similarly, the authors in [5] optimized the spectrum sharing for D2D-based ultra-reliable low latency communications. A rate optimization problem for the considered D2D network was first formulated, and then the successive convex approximation based iterative algorithm was adopted to solve this non-convex optimization problem. Differently, the authors in [6] proposed a spectrum sharing approach based on a contract-based cooperative technique to obtain an opportunistic spectrum access policy for D2D users while maximizing the cellular network's profit. Specifically, the cooperative spectrum trading process between CUs and D2D devices was modeled based on a pre-defined principal-agent approach. Under this approach, the cellular network acts as a principal and offers a power-payment contract to the D2D pair. After that, the D2D pair, which acts as an agent, tries to choose a contract that can maximize its utility function. Based on this design, the authors can derive optimal contracts that the cellular system can offer and obtain the optimal contract-based spectrum sharing policy.\nAlthough demonstrating good performance for spectrum sharing in D2D communication, the applications of these studies may be limited. This is because the existing solutions are mainly based on static optimization techniques which require all information of the system in advance to formulate the problem and obtain the optimal solution. Unfortunately, it is difficult, if not impossible, to obtain complete prior knowledge of the system due to the dynamics and uncertainty of wireless communications and the mobility of users. To deal with this problem, deep reinforcement learning (DRL) has emerged as a promising tool recently [7]\u2013[11]. By observing the system states and outcomes after performing actions, DRL can efficiently learn the dynamics and uncertainty of the system to converge to the optimal spectrum access policy without requiring prior knowledge about CUs, BSs, and D2D users. For example, the authors in [7] and [8] developed DRL-based algorithms that help D2D users dynamically access the shared spectrum to maximize the system's sum throughput without using any prior knowledge about the system. With the proposed DRL-based solutions, D2D transmitters can learn the characteristics of CUs and the BS to decide when they can access the shared spectrum. Simulation results then showed that the proposed DRL-based approaches can achieve a better sum throughput compared to a baseline based on BS cooperation."}, {"title": "B. Motivations and Main Contributions", "content": "Unfortunately, the aforementioned studies and others in the literature may not work well when there are a large number of CUs operating in the shared spectrum. Under such scenarios, the shared spectrum can be always heavily occupied for transmissions of CUs, resulting in low communication efficiency for D2D users and/or frequent collisions for CUs. To address this essential issue, in this paper, we propose to use the ambient backscatter communication (AmBC) technology [13]-[15] to help D2D users transmit data even when the shared spectrum is occupied by CUs. The key fundamental of the AmBC technology is that it can transmit information by just backscattering ambient RF signals without generating any active signals. For that, when the shared spectrum is occupied, D2D users can switch to the ambient backscatter mode to backscatter RF signals generated from the BS to transmit data to their receivers. The backscattered signals from D2D users do not introduce any noticeable interference to CUs since there is no active signal as demonstrated in [13]. It is worth noting that the AmBC technology has been integrated into D2D communication in a few studies [12], [16]. However, dynamic spectrum access for D2D communication is not considered in these works. In particular, the authors in [16] proposed a new self-sustainable communication paradigm for D2D communi-cation by using ambient backscatter communications and then theoretically analyze the average throughput, energy outage probability, and coverage probability of the proposed hybrid D2D communication system. The authors in [12] instead focused on designing passive relays with ambient backscatter communications in wireless-powered D2D systems.\nIn addition, current DRL-based spectrum access approaches may take a very long time to converge to the optimal policy, especially in complex problems with high-dimensional state spaces. Moreover, DRL-based approaches usually require large deep neural network architectures to efficiently learn highly dynamic and complex environments, resulting in long training time and thus may not be applicable for applications with low latency requirements [25]. Inspired by the quantum super-position and quantum entanglement principles, we develop a quantum reinforcement learning (RL)-based solution to obtain the optimal dynamic spectrum access policy for D2D communications. Instead of using conventional deep neural networks, our proposed solution employs a parametrized quantum circuit to approximate the optimal dynamic spectrum access policy for D2D devices. Through extensive simulations, we demon-strate that the proposed quantum RL approach can achieve a faster convergence rate with much fewer trainable parameters compared to existing DRL methods. It is worth noting that the proposed quantum RL approach can efficiently run on classical computers by using the TensorFlow Quantum and Cirq libraries [36]. As far as we know, this is the first study to take into account quantum RL and the AmBC technology for dynamic spectrum access in D2D communications. Our main contributions can be summarized in the following.\n\u2022 We propose to integrate the AmBC technology into D2D devices for data transmissions by backscattering ambient RF signals, e.g., signals generated by the BS, when the shared spectrum is occupied by CUs. With this new design, D2D devices still can maintain their communications and avoid collisions with CUs even when the shared spectrum is always busy.\n\u2022 We use the Markov decision process to model the dy-namics and uncertainty of the system caused by user mobility and wireless environments. Then, we develop a DRL-based approach, namely deep Q-learning, to find the optimal spectrum access policy, i.e., stay idle, access the shared spectrum and perform active transmissions, or backscatter the BS's signal for transmissions, to maximize the long-term average throughput of D2D users.\n\u2022 To further improve the convergence rate and reduce the training complexity of our proposed solution, we develop a novel quantum RL approach to learn the environment's properties more efficiently and quickly. In particular, a parametrized quantum circuit is used to approximate"}, {"title": "II. SYSTEM MODEL", "content": "In this paper, we consider a D2D-enabled cellular network which consists of multiple CUs and a BS, as illustrated in Fig. 1. In the considered network, wireless devices can communicate with each other without routing packets through the cellular network by using D2D communications. Without the loss of generality, we assume that D2D nodes and CUs can share the spectrum resources in a time-splitting manner [8], [19]. Similar to [8], the time slot allocation among CUs is orthogonal. We then define $p_{access}$ as the probability that a CU accesses the shared spectrum in each time slot. As mentioned, D2D nodes can also access this shared spectrum using the same time slots\u00b9. However, D2D transmitters may generate interference and disrupt the transmissions of CUs if they access the shared spectrum at the same time. As such, D2D nodes must find appropriate time slots to communicate to avoid introducing interference to CUs. In addition, as studied in [8], a CU suffers from less interference from D2D nodes if it is near the BS. Thus, we define $d_{protected}$ as the maximum distance between CUs and the BS that allows CUs to communicate without being affected by interference from D2D nodes. We define $P_{protected}$ as the probability that a CU is in the secure area, i.e., its distance to the BS is less than $d_{protected}$.\nIn the future wireless networks, a large number of diverse wireless devices can share the same communication spectrum in a dense area. As such, it is very challenging for D2D devices to opportunistically access the shared spectrum to transmit their data. To further improve the spectrum usage efficiency and improve the system throughput, we propose to use the AmBC technology [14], [15] for D2D nodes. In particular, with the ambient backscatter circuit, D2D nodes can communicate with each other by simply reflecting or absorbing ambient RF signals, i.e., cellular signals in our considered system. To do that, each D2D node (e.g., mobile phones or IoT devices) is equipped with an RF switch that can switch between two different loads to reflect or absorb cellular signals. In the reflecting state, D2D nodes can transmit bits \"1\". In contrast, D2D nodes can transmit bits \"0\" in the absorbing state. In this way, the AmBC technology can help D2D nodes transmit information even when the current time slot is occupied by CUs without introducing any transmission disruptions to CUs. We then denote $C \\equiv \\{c : c \\in \\{0,1,2,3\\}\\}$"}, {"title": "A. D2D Communication Channel Model", "content": "Practically, D2D links can be modeled by using a probabilistic path-loss model, including a non-line-of-sight (NLOS) link and a line-of-sight (LOS) link [8]. As described in [21], the LOS path loss can be expressed as follows:\n$L_{LOS} = 16.9 log_{10}(d_{r}) + 32.8 + 20.0 log_{10}(f)$,\nwhere $d_{r}$ is the distance between the D2D transmitter (D2D-Tx) and the D2D receiver (D2D-Rx), and $f$ is the center frequency. Similarly, the NLOS path loss can be expressed as follows:\n$L_{NLOS} = 40.0 log_{10}(d_{tr}) +79.0 + 30.0 log_{10}(f)$.\nThe average path loss of the D2D link in dB then can be calculated as follows:\n$L = P_{LOS} L_{LOS} + (1 - P_{LOS}) L_{NLOS}$,\nwhere $P_{LOS}$ and $(1-P_{LOS})$ represent the probability of having an LOS link and an NLOS link, respectively. Based on the distance between the D2D-Tx and the D2D-Rx, $P_{LOS}$ can be calculated as follows [21]:\n$P_{LOS} = \\begin{cases}\n1 & d_{tr} \\le 4, \\\\\nexp(-(d_{tr} -4)/3) & 4 < d_{tr} \\le 60, \\\\\n0 & d_{tr} \\ge 60.\\end{cases}$\nLet $P_a$ be the transmit power of the D2D-Tx. The average signal power received by the D2D-Rx can be expressed as follows:\n$P_{r} [mW] = 10^{(P_a[dBm]-L[dB])/10}$.\nThe D2D connection's achievable transmission rate can be calculated as follows:\n$C_d = W log_2(1 + P_r/P_n)$,\nwhere $W$ is the bandwidth of the channel and $P_n$ is the noise power."}, {"title": "B. Ambient Backscatter Communication Channel Model", "content": "In this paper, we adopt the AmBC technology to allow D2D nodes to communicate with each other even when the spectrum is occupied by CUs. In particular, when the BS is communicating with a CU in the share spectrum, D2D nodes can reflect or absorb the cellular signals to transmit their information without generating active RF signals, and thus do not interrupt the transmissions of the CU. Interested readers can refer to [14] and [15] for more information about the design, principles, and circuits of the AmBC technology. In the following, we provide the channel model and formulate the achievable rate of ambient backscatter communications.\nSpecifically, the RF signals sent from the BS can be expressed as [22]:\n$x(n) = \\sqrt{P_t}s(n)$,\nwhere $P_t$ is the transmit power of the BS, and $s(n)$ with $E(|s(n)|^2) = 1$ is the transmitted signal at the $n$-th symbol interval of the BS. Then, the ambient RF signal received at the D2D-Tx can be formulated as follows:\n$y_{dt}(n) = h_{st}x(n) = \\sqrt{P_t}h_{st}s(n)$,\nwhere $h_{st}$ is the channel coefficient between the BS and the D2D-Tx. Let $a$ denote the reflection coefficient at the D2D-Tx and $b(n)$ denote the D2D-Tx's signal at the $n$-th symbol interval, we then can express the backscatter signal from the D2D-Tx as follows:\n$x_{dt}(n) = \\sqrt{a} b(n)y_{dt} = \\sqrt{aP_t}h_{st}b(n)s(n)$.\nThe backscattered signal received at the D2D-Rx then can be expressed as follows:\n$y_{tr}(n) = h_{tr}x_{dt}(n) = \\sqrt{aP_t}h_{st}h_{tr}b(n)s(n)$,\nwhere $h_{tr}$ is the channel coefficient between the D2D-Tx and the D2D-Rx. The BS's signal received at the D2D-Rx can be also expressed as follows:\n$y_{dr}(n) = h_{dr}x(n) = \\sqrt{P_t}h_{dr}s(n)$,\nwhere $h_{dr}$ is the channel coefficient between the BS and the D2D-Rx. Then, the total received signal at the D2D-Rx can be expressed as:\n$y(n) = y_{tr}(n) + y_{dr} + N(n)$\n$= \\sqrt{aP_t}h_{st}h_{tr}b(n)s(n) + \\sqrt{P_t}h_{dr}s(n) + N(n)$,\nwhere $N(n)$ is the Gaussian noise at the D2D-Rx with zero mean and variance $\\sigma^2$, i.e., $N(n) \\sim CN(0, \\sigma^2)$.\nSimilar to [23], [24], we assume that the D2D-Rx is equipped with the successive interference cancellation (SIC) technique to decode the received signals. In particular, the SIC technique, which is a common physical-layer approach, is usu-ally used to handle two or more signals received at a receiver. With SIC, the D2D-Rx can decode the stronger signals, i.e., the BS signal, first, subtract it from the received signals, and then extract the weaker signal, i.e., the backscattered signal, from the residue [23]. In this way, the signal-to-noise (SNR) ratio at the D2D-Rx $\\gamma$ can be expressed as:"}, {"title": "III. PROBLEM FORMULATION", "content": "We use the Markov decision process (MDP) to formulate the optimization problem in order to address the dynamic and uncertain nature of the system under consideration. In particular, a tuple < S, A,r >, where S denotes the state space, A represents the action space, and r denotes the immediate reward function, is theoretically used to define an MDP."}, {"title": "A. Safe Space", "content": "Practically, the channel state can only be accurately ob- served at the end of a time slot after the agent makes an action at the beginning of the time slot. As such, our system state space includes the channel state at the previous time slot. In addition, to help the agent learn the CU's behaviors and the system's properties more efficiently, we also consider the chosen action and the location of the CU in the previous time slot. As discussed in Section II, the achievable rates of D2D communications and ambient backscatter communications de- pend greatly on the distance between D2D nodes as well as the distance from the D2D-Tx to the BS. For this reason, these two factors are also included in our system state space. Given the above, S can be formally defined as follows:\n$S = \\{s : s \\in \\{a, c, p, d_{dt}, d_{tr}\\}, \\forall a \\in A, \\forall c \\in C, \\forall p \\in \\{0, 1\\}\\}$,\nwhere a denotes the previous action taken by the agent, c denotes the previous channel state, p denotes if the CU in the previous time slot is in the BS's secure area (i.e., p = 1) or otherwise (i.e., p = 0), $d_{dt}$ is the distance between the D2D-Tx to the BS, and $d_{tr}$ is the distance between the D2D-Tx and the D2D-Rx. In this way, the system state at time slot t can be formally expressed as $s_t = (a_{t-1}, c_{t-1}, p_{t-1}, d_{dt}, d_{tr}) \\in S$."}, {"title": "B. Action Space", "content": "As mentioned in Section II, the D2D-Tx can choose to perform D2D communications or ambient backscatter commu- nications to transmit data to the D2D-Rx. In addition, it can choose to stay idle. For that, the action space can be formally defined as follows:\n$A = \\{a : a \\in \\{0,1,2\\}\\}$,\nwhere $a = \\begin{cases}\n0 & \\text{if the D2D-Tx stays idle}, \\\\\n1 & \\text{if the D2D-Tx performs D2D communications}, \\\\\n2 & \\text{if the D2D-Tx chooses to leverage the AmBC technology for its transmissions}.\n\\end{cases}$"}, {"title": "C. Immediate Reward", "content": "In this work, our aim is to maximize the average throughput of D2D devices while minimizing the interference introduced to CUs. For that, the immediate reward after the D2D-Tx takes action $a_t$ at state $s_t$ can be defined as follows:\n$r_t(s_t, a_t) = \\begin{cases}\n0, & \\text{if } a_t = 0, \\\\\nC_d, & \\text{if } a_t = 1 \\text{ and there is no active CU}, \\\\\nC_d, & \\text{if } a_t = 1 \\text{ and there is an active CU} \\\\\n& \\text{in the secured area}, \\\\\nP, & \\text{if } a_t = 1 \\text{ and there is an active CU} \\\\\n& \\text{outside the secured area}, \\\\\nC_b, & \\text{if } a_t = 2.\n\\end{cases}$\nSpecifically, when the D2D-Tx chooses to stay idle (i.e., $a_t$ = 0) the immediate reward will be 0. If the D2D-TX transmits its information by D2D communications (i.e., $a_t = 1$) and there is no CU actively using the shared spectrum, the immediate reward will be $C_d$ as defined in (6). In addition, if there is an active CU but this CU is in the secured area, then the immediate reward is also $C_d$. In contrast, if this CU is outside the secured area, the immediate reward will be $P$, which is a very low value to ensure that the agent will avoid actions that introduce interference to CUs. Finally, if the D2D-Tx chooses to use the ambient backscatter to transmit data (i.e., $a_t = 2$), the immediate reward will be $C_b$ as defined in (14)."}, {"title": "D. Long-term Average Throughput Optimization Formulation", "content": "The long-term average throughput maximization problem for D2D devices given the formulated MDP can be expressed as follows:\n$\\max_{\\pi} R(\\pi) = \\lim_{T \\to \\infty} \\frac{1}{T} \\sum_{t=1}^{T} E\\left[r_t(s_t, \\pi(s_t))\\right]$,\nwhere $\\pi$ is a spectrum access policy which is a mapping from state $s_t$ to action $a_t$, $r_t(s_t, \\pi(s_t))$ represents the immediate reward received after taking an action following policy $\\pi$ at state $s_t$, and $R(\\pi)$ is the average long-term throughput under policy $\\pi$. It is worth noting that with our formulated MDP, $R(\\pi)$ is independent from the initial state $s_0$ of the system as the underlying Markov chain is irreducible [25]. Thus, the optimal dynamic spectrum access policy exists and can be obtained."}, {"title": "IV. QUANTUM REINFORCEMENT LEARNING", "content": "To efficiently solve the optimization problem in Section III, RL algorithms can be adopted. Among RL algorithms, Q- learning and deep Q-learning (DQL) are the most common algorithms adopted in the field of communications and net- working [25]. Fundamentally, Q-learning and DQL algorithms are designed based on the Bellman equation to perform simple value iteration updates to approximate the Q-values for all state-action pairs."}, {"title": "V. PERFORMANCE EVALUATION", "content": "This section provides an extensive performance evaluation of the quantum RL approach compared with other baselines in various scenarios. In particular, we first present the parameter setting for the considered system and also the proposed quantum RL approach. Then, we analyze the simulation results in terms of convergence rate, running time, and average throughput in different scenarios."}, {"title": "VI. CONCLUSION", "content": "In this paper, we have proposed a dynamic spectrum access approach for D2D communications by leveraging the AmBC technology and quantum RL. In particular, by using the AmBC technology, D2D devices can transmit their information even when CUs are accessing the shared spectrum by simply backscattering the RF signals sent from the base station. This approach is particularly effective when the shared spectrum is usually busy which will be a common situation in future dense heterogeneous wireless networks. In addition, it is challenging to obtain the optimal spectrum access policy for D2D devices given the dynamics and uncertainty of the system due to the nature of wireless communication as well as the mobility and behaviors of mobile users. To address this problem, we have developed a quantum RL that can efficiently and quickly learn the environment to approximate the optimal policy by leverag- ing the quantum superposition principle. Extensive simulations have demonstrated that our proposed solution not only can improve the average throughput of D2D communications but also can quickly learn the environment with significantly fewer training parameters compared to the state-of-the-art methods."}]}