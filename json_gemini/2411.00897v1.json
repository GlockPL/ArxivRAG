{"title": "Enhancing the Traditional Chinese Medicine Capabilities of Large Language Model through Reinforcement Learning from AI Feedback", "authors": ["Song Yu", "Xiaofei Xu", "Fangfei Xu", "Li Li"], "abstract": "Although large language models perform well in understanding and responding to user intent, their performance in specialized domains such as Traditional Chinese Medicine (TCM) remains limited due to lack of expertise. In addition, high-quality data related to TCM is scarce and difficult to obtain, making large language models ineffective in handling TCM tasks. In this work, we propose a framework to improve the performance of large language models for TCM tasks using only a small amount of data. First, we use medical case data for supervised fine-tuning of the large model, making it initially capable of performing TCM tasks. Subsequently, we further optimize the model's performance using reinforcement learning from AI feedback (RLAIF) to align it with the preference data. The ablation study also demonstrated the performance gain is attributed to both supervised fine-tuning and the direct policy optimization. The experimental results show that the model trained with a small amount of data achieves a significant performance improvement on a representative TCM task.", "sections": [{"title": "I. INTRODUCTION", "content": "Language modeling, as an important approach to language understanding and generation, has been extensively studied over the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models drown a lot of attention, and by pre-training Transformer models on large-scale corpora, these models have demonstrated their power in solving various natural language processing tasks [1]. Interestingly, when the parameter size exceeds a certain level, these language models not only improve performance, but also show capabilities that are not available in small-scale models. In general, large language models (LLMs) are Transformer-based language models containing billions or more parameters trained on large amounts of textual data, such as OpenAI's ChatGPT [2] and GPT-4 [3], which are capable of understanding and answering a wide range of questions, and their performance on certain tasks even meets or exceeds that of humans. In addition, the open source community has rapidly introduced a series of LLMs, such as ChatGLM [4], LLaMA [5], Qwen [6], which have demonstrated impressive performance.\nHowever, the training data for LLMs mainly come from the Internet and books and are mostly common-sense data, thus limiting them in domains such as Traditional Chinese Medicine (TCM). Additionally, high quality TCM data is scarce, and existing large language models perform poorly on TCM tasks. Despite the challenges, TCM LLM has great potential to provide value in answering questions, assisting doctors in diagnosis and prescribing.\nIn the medical domain, several medical language models have been proposed, such as Med-Palm [7], DoctorGLM [8], etc., which show great promise in a variety of medical applications, such as medical question and answer, dialog systems, and text generation. However, almost all of these works focus on modern medicine, with very few addressing TCM. Developing a large model for the TCM domain is challenging. First, most TCM works are written in ancient Chinese, and many terms do not have corresponding explanations in modern Chinese, with large grammatical differences [9]. Additionally, many theories and diagnostic and therapeutic methods in TCM lack uniform quantitative and objective standards and cannot be easily verified. Finally, TCM is not a widely applied discipline, and there is less information on related works, and high-quality data are even more difficult to obtain [10].\nTo address these limitations, we propose a framework for enhancing the performance of large-model TCM tasks with only a small amount of data. Specifically, we focus on two types of tasks: initial visit and follow-up visit, as shown in Table 1. Our framework consists of three stages. First, we collect a corpus of real medical cases and perform supervised fine-tuning on a open-source large language model, this step will steer the LLM into solving TCM tasks. Second, for each input, we instruct the model to generate multiple outputs to build a preference dataset. Considering the inefficiency and high cost of manual annotation, we introduced a reinforcement learning method based on AI feedback (RLAIF) to train language models using AI-generated feedback instead of human feedback. Finally, we use preference data to instruct the model's learning, enabling it to generate outputs that better"}, {"title": "II. RELATED WORK", "content": "In 2022, ChatGPT went live and attracted a lot of attention, triggering a heated discussion about LLM across the community. With the release of GPT-4 (2023) and GPT- 4o (2024), the multimodal capability of large models was further improved, which set off a new wave of AI, however, OpenAI did not announce its training strategies and weights due to various factors. As a result, open source LLMs such as LLaMA and BLOOM [11] quickly attracted a lot of attention from the research community once they were released. These models made public their training methods and weighting files, enabling researchers and developers to further train and improve the models on this basis. A large number of open source LLMs have also emerged in China, such as ChatGLM, DeepSeek [12], Qwen, Baichuan [13], and ERNIE [14]. These open-source LLMs use rich Chinese corpus for training and optimization.\nWith the proliferation of these models, various optimization techniques have been explored to enhance their performance. Optimizing the performance of large language models involves various techniques, with RLHF (reinforcement learning from human feedback) [15] and RLAIF (reinforcement learning from AI feedback) [16] being two prominent methods. In RLHF, a reward model is trained to learn alignment based"}, {"title": "III. METHOD", "content": "This section describes the process of building the framework, which is divided into data construction, supervised fine- tuning, reinforcement learning from AI feedback. Each step is discussed sequentially to reflect the research workflow. The\nIntegrated methodology flowchart is shown in Figure 1."}, {"title": "A. Data Construction", "content": "One of the challenges in training high-performance LLM models for TCM lies in obtaining high-quality data. A high quality corpus can greatly improve the performance of LLMs and even break the scaling law to some extent [21]. The model needs not only theoretical data from TCM textbooks, but also professional data from real doctor-patient scenarios, which can reflect the specific conditions of patients and guide the addition, subtraction and proportion of medicines. In order to ensure the diversity of the medical corpus, we collect a variety of real medical text data from multiple sources, including open source data, proprietary data, and real medical consultations. These data cover most of the domains and symptoms of TCM, providing rich and detailed medical knowledge for the model.\nIn this paper, we will focus on the classical types of prescription tasks. Specifically, there are two types of prescription tasks: first visit and follow-up visit. The first visit task needs to issue a prescription based on the user's symptoms and examination results, while the follow-up visit task needs to synthesize all the previous follow-ups, prescriptions, and feedbacks from the user's medication to give subsequent medication suggestions."}, {"title": "B. Supervised fine-tuning", "content": "Supervised fine-tuning (SFT) is a key stage in empowering the model with dialog capabilities. With the help of high- quality doctor-patient dialog data, the model can effectively invoke the medical knowledge accumulated during the pre- training process to understand and answer the user's query. The goal of SFT is to teach the model how to understand and generate appropriate replies based on TCM diagnostic principles and treatment methods.\nThe SFT process consists of optimizing the model parame- ters to minimize the cross-entropy loss between the predicted output and the actual output for a given input set. In the SFT process, the model learns to generate prescriptions based on the initial diagnosis and subsequent diagnoses. This process involves providing the model with detailed case information including symptoms, diagnoses, and patient feedback to guide its learning process.\nThe cross-entropy loss used for training the model is defined as follows:\n$L = \\frac{1}{NT} \\sum_{i=1}^{N} \\sum_{t=1}^{T} Y_{it} \\log(Y_{it})$        (1)\nwhere N is the total number of samples, T is the sequence length, $Y_{it}$ is the true token at position t for the i-th sample, and $Y_{it}$ is the predicted probability of the true token at position t for the i-th sample. This loss function measures the difference between the predicted token distribution and the actual token distribution, and the goal of training is to minimize this loss. The pseudo-code for this phase is shown in Algorithm 1.\nNote a good prompt helps to stimulate the model's capabil- ity, so we designed a prompt to guide the model to generate the specified response, as shown in Table 2. We decide whether to use English or Chinese prompts based on the corpus used in the pre-training stage of the base model, but the output is uniformly specified to be in Chinese."}, {"title": "C. Reinforcement Learning from AI Feedback", "content": "Despite SFT accumulating medical knowledge and guiding conversational abilities, the model can still produce inaccurate, harmful, or unfriendly responses, which can have serious consequences in medical dialogues. We use RLAIF to improve the conversation process. Specifically, for each question, we guide the supervised fine-tuned model to generate diverse outputs, score them using a specific annotation model, and then rank them using Borda rank. Finally, we align the training using the DPO algorithm.\nConsidering the specificity of medical conversations, we obtain feedback in three ways, make metrics from three different aspects of the output, and develop a sorted annotation rule.\nLexical overlap: This refers to cases where both the ground truth and the model output contain the same or similar terms. We use BM25, a well-established ranking function used by search engines [22]. The advantage of BM25 lies in its ability to weigh query terms according to their TF-IDF importance, thus providing a reliable measure of term overlap.\nSemantic overlap: This refers to cases where the ground truth and the model output contain semantically related"}, {"title": "IV. EXPERIMENTS AND EVALUATION", "content": "We use four widely used open-source models as our base models, GLM-4-9B-Chat, Llama-3-8B-instruct [24], Qwen2- 7B-chat, DeepseekMOE-16B [25]. GLM-4-9B-Chat is the open-source version of the latest generation of LLms in the GLM-4 family from Smart Spectrum AI, and has been evaluated on a variety of benchmarks in semantics, math, rea- soning, code and knowledge extraction, demonstrated superior performance against previous generation of models. LLaMA3 is a LLM developed by Meta, optimized for a wide range of conversational use cases, outperforming many open-source models on common industry benchmarks. Qwen2 is a series of open source large models of Tongyi Qianqian developed by Aliyun. The series provides multiple versions and scales of open source models, such as Base and Instruct, so as to meet different computing needs. DeepSeekMoE 16B is a Mixture- of-Experts (MoE) language model with 16.4B parameters. It employs an innovative MoE architecture, which involves two principal strategies: fine-grained expert segmentation and shared experts isolation.\nTraining was performed on 1 NVIDIA Tesla A40(48GB) using a low-rank adaptive (lora) parameter efficient tuning"}, {"title": "B. Baseline", "content": "In order to fully evaluate our method, we chose a series of LLMs with different training path and training data as baselines for comparison.\nZero-shot [27]: The model is given a task without any prior examples. This approach tests the model's ability to generalize from its training data to new, unseen scenarios. For each model, the zero-shot prompt is carefully crafted to be clear and concise, providing only the necessary context and the query.\nFew-shot [28]: The model is provided with a few ex- amples to learn from before it is asked to perform the task. This method helps the model understand the task better by seeing similar instances. The few-shot prompts are designed to include a small number of examples (2 in our experiment) before presenting the actual query.\nSFT [29]: Supervised fine-tuning will provide medical cases with their corresponding standard diagnoses for the model to learn.\nSFT+DPO: After warming up using supervised fine- tuning, the model is guided to generate multiple outputs, and the data is labeled using an automated labeling system which in turn generates preference data for the dpo training process.\nZero-shot learning is an important method for evaluating the generalization ability of language models without any task- specific data. For the field of TCM, zero-shot testing can demonstrate the adaptability of the model to unseen TCM tasks, especially when data is scarce. This method can high- light the basic language understanding and reasoning ability of the model, and provide a lower bound on the performance of tasks in this field.\nFew-shot learning provides the ability of the model to learn quickly from a small number of examples. Through training with a few samples, we can evaluate whether the model can effectively reason and generate accurate results under the condition of limited labeled data.\nSFT is a standard method for model fine-tuning. For the field of TCM, supervised fine-tuning of the model using"}, {"title": "C. Evaluation Metrics", "content": "The assessment of the quality of medical dialog is a multi- faceted task. In order to comprehensively assess the quality of medical dialog, we used Bleu [30], Rough [31], and bert-score [32] as evaluation metrics. For the same problem, we sampled three outputs from the model and calculated the average metric of these three outputs for error reduction and their standard deviation."}, {"title": "D. Results", "content": "As shown in Tables 4 and 5, the experimental results comprehensively evaluate the performance metrics of various models and methods applied to the proposed TCM task. The primary performance metrics include ROUGE-1, ROUGE-2, ROUGE-L, BLEU-4, precision, recall, and BERT-Score F1. The zero-shot results demonstrate the initial capabilities of the models, generating responses based solely on pre-trained knowledge without any prior examples. In this scenario, all models performed poorly as they had not learned how to pre- scribe correctly based on medical cases during the pre-training phase. Notably, LLaMA3 performed significantly worse than other models, possibly due to LLaMA being more extensively trained on English corpora, while TCM tasks likely require a higher level of proficiency in Chinese.\nAfter adopting the few-shot method, the performance of all models improved to varying degrees, highlighting their ability to learn and adapt from a small number of instances, thereby enhancing their response quality. It is worth noting that GPT-3.5-turbo showed the most significant improvement, indicating its strong ability to follow instructions. The SFT stage embedded more specific TCM knowledge and prescrip- tion tasks into the models, enabling them to generate more accurate and contextually appropriate responses, significantly improving performance across all models at this stage. The DPO stage refined the model outputs using preference data, aligning them more closely with human preferences and ex- pectations, achieving superior overall performance. For certain models, such as LLaMA3, the bias in outputs was reduced post-DPO, indicating the effectiveness of aligning the work with preference data, leading to more stable and intention-aligned outputs.\nDespite GPT-3.5-turbo's excellent performance, models en- hanced through SFT and DPO exhibited superior results, underscoring the effectiveness of our proposed framework. Considering the difficulty of obtaining high-quality TCM data,"}, {"title": "E. Case Study", "content": "To illustrate the effectiveness of our proposed framework, we conducted a detailed case study focusing on its applica- tion in Traditional Chinese Medicine (TCM) diagnosis and prescription tasks. This case study demonstrates the model's capability to handle both initial and follow-up diagnoses ef- fectively, showcasing its practical utility in real-world medical scenarios. We selected a patient case from our corpus, which includes both initial and follow-up visits. The case details are as follows:\nPatient: Liu, Female, 71 years old.\nChief Complaint: Dizziness for over a year.\nPresent Illness History: The patient started experiencing dizziness a year ago, occurring when lying flat and turning her head to the left at night, as well as when looking upward dur- ing the day. This dizziness is accompanied by visual rotation, occasional nausea and vomiting, a feeling of heaviness and fatigue in the limbs, irritability, normal appetite and sleep, a slightly bitter taste in the mouth, and regular bowel movements and urination."}, {"title": "V. CONCLUSION AND LIMITATIONS", "content": "In this paper, we propose a framework that combines super- vised fine-tuning and direct preference optimization to improve the performance of large language models for Traditional Chinese Medicine tasks. Our proposed approach addresses the unique challenges faced by TCM, such as the scarcity of high-quality data and the expertise required for accurate medical applications. By utilizing a small but high-quality corpus of TCM and incorporating an automated annotation process, we are able to significantly improve the model's ability to generate accurate and relevant medical prescriptions. Experimental results show that our framework outperforms existing models, including widely used LLMs such as GPT- 3.5-turbo, on various evaluation metrics such as ROUGE, BLEU, and BERT-Score.\nOur case study further illustrates the practical applicability of the framework in real TCM consultation scenarios, demon- strating the model's ability to effectively handle both initial and follow-up consultations. Automatic annotation proved to be efficient, reducing the need for manual annotation while maintaining high accuracy of the model output.\nDespite the promising results, there are limitations to our approach. The reliance on small datasets, while demonstrating the efficiency of the framework, also highlights the potential advantages of larger and more diverse corpora. In addition, our task is limited to the TCM prescription task only, and the expert annotation is not as high quality as the manual"}]}