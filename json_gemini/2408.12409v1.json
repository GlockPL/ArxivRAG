{"title": "Multi-Source Knowledge-Based Hybrid Neural Framework for Time Series Representation Learning", "authors": ["Sagar Srinivas Sakhinana", "Krishna Sai Sudhir Aripirala", "Shivam Gupta", "Venkataramana Runkana"], "abstract": "Accurately predicting the behavior of complex dynamical systems, characterized by high-dimensional multivariate time series(MTS) in interconnected sensor networks, is crucial for informed decision-making in various applications to minimize risk. While graph forecasting networks(GFNs) are ideal for forecasting MTS data that exhibit spatio-temporal dependencies, prior works rely solely on the domain-specific knowledge of time-series variables inter-relationships to model the nonlinear dynamics, neglecting inherent relational structural dependencies among the variables within the MTS data. In contrast, contemporary works infer relational structures from MTS data but neglect domain-specific knowledge. The proposed hybrid architecture addresses these limitations by combining both domain-specific knowledge and implicit knowledge of the relational structure underlying the MTS data using Knowledge-Based Compositional Generalization. The hybrid architecture shows promising results on multiple benchmark datasets, outperforming state-of-the-art forecasting methods. Additionally, the architecture models the time varying uncertainty of multi-horizon forecasts.", "sections": [{"title": "INTRODUCTION", "content": "Multivariate time series forecasting(MTSF) is a crucial task with diverse applications in various sectors, including finance, healthcare, energy, and others. MTSF facilitates strategic decision-making by predicting interrelated variables that change over time. In retail and e-commerce, it is used to forecast product demand, optimize supply chains, and manage inventory levels. Cloud providers utilize MTSF to accurately predict web traffic and efficiently scale server fleets to meet anticipated demand. Forecasting MTS data is challenging due to the complex interrelationships among multiple variables and the unique characteristics of MTS data, such as non-linearity, high-dimensionality and non-stationarity. Over the past few years, Spatial-temporal graph neural networks(STGNNs) have gained popularity for modeling intricate dependencies in MTS data, improving forecast accuracy. While explicit relationships among variables are provided by human experts through a predefined or explicit graph, implicit relationships are obtained using data-driven neural relational inference methods([Kipf et al., 2018]). Implicit relationships, characterized by their high complexity and non-linearity, evolve over time and reveal hidden interrelations among variables unknown to human experts which are not trivial, whereas explicit relations stemming from domain expertise remain static. Existing \u201chuman-in-the-loop\u201d STGNNs([Yu et al., 2017], [Li et al., 2017], [Guo et al., 2020]) integrate domain-specific knowledge and learn MTS data dynamics, but real-world situations often present unknown or incomplete graph structures, resulting in suboptimal forecasting. These predefined structures may also inadequately capture non-static spatio-temporal dependencies, impeding the accurate inference of latent time-conditioned relations that influence variable co-movements within the MTS data. Moreover, STGNNs neglect the significance of edges in explicit graph structure, hindering modeling of complex systems. Incorporating effective methods to represent edge information within STGNNs is necessary for accurately modeling complex dynamical systems. Additionally, STGNNs have limitations in capturing the importance of subgraphs within the larger explicit graph structure, which can be addressed by developing new methods to incorporate subgraph dynamics to model complex systems. Conversely, recent \u201chuman out-of-the-loop\u201d STGNNs([Shang et al., 2021], [Deng and Hooi, 2021], [Wu et al., 2020]) simultaneously learn the discrete dependency graph structures and the MTS data dynamics, but neglect predefined inter-relationships from domain expertise, leading to subpar performance in graph time series forecasting. Moreover, implicitly learning the latent graph structure from MTS data is constrained by the limitations of pairwise connections among the variables. However, the interconnected networks in complex dynamical systems could have higher-order structural relations beyond pairwise associations. Hypergraphs, a generalization of graphs, can effectively model these relations in high-dimensional MTS data. Additionally, while conventional STGNNs emphasize pointwise forecasting, they do not offer any estimates of uncertainty for the multi-horizon forecasts. We propose the Multi-Source Knowledge-Based Hybrid Neural Framework(for brevity, MKH-Net) to address these challenges. This framework integrates the domain-specific knowledge and data-driven knowledge using a joint-learning approach to model the complex spatio-temporal dynamics underlying the MTS data, resulting in better forecast accuracy and reliable uncertainty estimates. The proposed framework has two main components: spatial and temporal inference components. Using a space-then-time(STT, [Gao and Ribeiro, 2022]) approach, the framework performs the spatial message-passing schemes prior to the temporal-encoding step. The spatial inference component combines \u201cimplicit hypergraph\u201d, \u201cexplicit subgraph\u201d, and \u201cdual-hypergraph\u201d representation learning methods to learn the various aspects of the underlying structure of interrelationships among variables in MTS data, characterizing the complex sensor network-based dynamical systems. The \u201cimplicit hypergraph\u201d method learns the hierarchical interdependencies between variables in MTS data by modeling the discrete hypergraph relational structure and performs the hypergraph representation learning schemes to obtain latent hypernode-level representations, which accurately capture the spatio-temporal dynamics of the hypergraph-structured MTS data. The \u201cexplicit subgraph\u201d method extracts overlapping subgraph patches and uses subgraph message-passing schemes to learn spatio-temporal dynamics within the explicit graph-structured MTS data. The \u2018dual-hypergraph\u201d method captures the latent information of edges in explicit graph-structured MTS data by utilizing the Dual Hypergraph Transformation(DHT) method. This is achieved through a powerful message-passing scheme that is tailored specifically to the edges in the structured MTS data, resulting in a more accurate modeling of the underlying spatio-temporal dynamics. The proposed MKH-Net framework uses a gating mechanism to perform a convex combination of the multi-knowledge representations computed by the different methods, resulting in more accurate latent representations of the complex non-linear dynamics in MTS data. The MKH-Net framework is capable of capturing various types of dependencies that exist across different observation scales, as correlations among variables may vary in short-term versus long-term views of the MTS data. The temporal learning component models the time-evolving dynamics of interdependencies among variables in MTS data, enabling the framework to provide accurate multi-horizon forecasts and precise predictive uncertainty estimates. To put it briefly, the proposed framework offers an end-to-end methodology for learning spatio-temporal dynamics in MTS data with both explicit graph and implicit hypergraph structures. This approach utilizes multiple representation learning methods, including \u201cexplicit subgraph\u201d, \u201cimplicit hypergraph\u201d, and \u201cdual-hypergraph\u201d, to capture evolutionary and multi-scale interactions among variables in the latent representations to achieve better modeling accuracy. The framework also models time-varying uncertainty in forecasts and utilizes the learned latent representations for downstream MTSF tasks, resulting in accurate multi-horizon forecasts and reliable predictive uncertainty estimates. Additionally, the framework is designed to offer better generalization and scalability for large-scale spatio-temporal MTS forecasting tasks found in real-world applications."}, {"title": "PROBLEM DEFINITION", "content": "Let us consider a historical time series dataset with n correlated variables, observed over T time steps, represented by the notation $X=[x_1, . . . , x_T]$. Here, the subscript indicates the time step, while the observations for all the n variables at time step t are denoted by $x_t=[x_t^{(1)}, x_t^{(2)}, . . . , x_t^{(n)}]^T \\in R^{(n)}$, where the superscript refers to the variables. In the context of MTSF, we employ the rolling-window method for multi horizon forecasting, where a look-back window is predefined at the current time step t to include the prior \u03c4 -steps of MTS data, to predict the next \u03c5-steps. Specifically, we aim to utilize a historical window of n-correlated variables, represented by the notation $X_{(t\u2212\u03c4:t\u22121)} \\in R^{n\u00d7\u03c4}$, which have been observed over the previous \u03c4 -steps prior to the current time step t, to make predictions about the future values of n variables for the next \u03c5-steps, denoted as $X_{(t:t+\u03c5\u22121)} \\in R^{n\u00d7\u03c5}$. To capture the spatio-temporal correlations among a multitude of correlated time series variables, the MTSF problem is formulated on graph and hypergraph structures. The historical inputs are represented as continuous-time spatial-temporal graphs denoted by $G_t=[V, E, X_{(t\u2212\u03c4:t\u22121)}, A^{(0)}]$, where Gt is composed of a set of nodes(V) which represent the variables, edges(E) that describe the connections among the variables, and a node feature matrix $X_{(t\u2212\u03c4:t\u22121)}$ that changes over time. The explicit static-graph structure based on prior knowledge of time-series variables relationships is described by the adjacency matrix $A^{(0)} \\in {0, 1}^{|V|\u00d7|V|}$. To further capture the complex relationships among MTS data, we consider the historical inputs as a sequence of dynamic hypergraphs, denoted by $HG_t=[HV, HE, X_{(t\u2212\u03c4:t\u22121)}, I]$. Here, the hypergraph is represented by a fixed set of hypernodes(HV) and hyperedges(HE), where the hypernodes denote the variables, and the hyperedges capture the latent higher-order relationships between the hypernodes. The time-varying hypernode feature matrix is given by $X_{(t\u2212\u03c4:t\u22121)}$. The implicit hypergraph structure is learned through an embedding-based similarity metric learning approach. The incidence matrix $I \\in R^{n\u00d7m}$ describes the hypergraph structure, where $I_{p,q}$=1 if the hyperedge q is incident with hypernode p, and otherwise 0. The sparsity of the hypergraph is determined by the number of hyperedges(m) in the hypergraph. The proposed framework aims to learn a function $F(\u03b8)$ that can map MTS data, $X_{(t\u2212\u03c4:t\u22121)}$, to their respective future values, $X_{(t:t+\u03c5\u22121)}$, given a $G_t$ and $HG_t$.\n\n$[x_{(t\u2212\u03c4)}, \u00b7 \u00b7 \u00b7 , x_{(t\u22121)}; G_t, HG_t]  \\xrightarrow{F(\u03b8)} x_{(t+1)}, \u00b7 \u00b7 \u00b7 , x_{(t+\u03c5\u22121)}]$\n\nThe MTSF task formulated on the explicit graph (Gt) and implicit hypergraph (HGt) can be expressed as follows:\n\n$\\min_\u03b8 L[X_{(t:t+\u03c5\u22121)}, \\hat{X}_{(t:t+\u03c5\u22121)}; X_{(t\u2212\u03c4:t\u22121)}, G_t, HG_t]$\n\nHere, \u03b8 represents all the learnable parameters of the trainable function F(\u03b8). The model predictions are denoted by $\\hat{X}_{(t:t+\u03c5\u22121)}$, and L represents the loss function. We train our learning algorithm using the mean absolute error(MAE) function, which is defined as follows:\n\n$L_{MAE} (\u03b8)=\\frac{1}{\u03c5} \\sum_{i=1}^{\u03c5} ||X_{(t:t+\u03c5\u22121)} \u2212 \\hat{X}_{(t:t+\u03c5\u22121)}||$"}, {"title": "OUR APPROACH", "content": "Our framework presents a neural forecasting architecture composed of three main components: the projection layer, spatial inference, and temporal inference components, which are illustrated in Figure 1. The spatial inference component includes three methods: \u201cimplicit hypergraph\u201d, \u201cexplicit subgraph\u201d, and \u201cdual-hypergraph\u201d representation learning methods. The \u201cimplicit hypergraph\u201d method computes the dependency hypergraph structure of multiple time series variables and uses higher-order message-passing schemes to model the hypergraph-structured MTS data. This approach computes time-conditioned, optimal hypernode-level representations, capturing complex relationships between variables over time. The \u201cexplicit subgraph\u201d method consists of two modules, the patch extraction and the subgraph encoder. The patch extraction module extracts overlapping subgraph patches from a predefined graph, while the subgraph encoder module uses spatial graph-filtering techniques to compute time-evolving, optimal node-level representations, which effectively capture the underlying spatio-temporal dynamics of the graph-structured MTS data. The \u201cdual-hypergraph\u201d method transforms edges(nodes) in an explicit graph into hypernodes(hyperedges) in a dual hypergraph, allowing hypernode-level message-passing schemes to be applied for edge representation learning of the explicit graphs. The temporal inference component of the framework combines multiple latent representations from different methods and learns their temporal dynamics. By jointly optimizing the different learning components, the framework provides accurate multi-horizon forecasts and reliable uncertainty estimates for various time series forecasting tasks."}, {"title": "PROJECTION LAYER", "content": "The proposed framework includes a projection layer that utilizes gated linear networks(GLUs, [Dauphin et al., 2017]) to learn non-linear representations of the input data. The input data is represented by $X_{(t\u2212\u03c4:t\u22121)} \\in R^{n\u00d7\u03c4}$, and the projection layer uses GLUs to selectively pass information through a gating mechanism and transform the input data into a new feature matrix, $\\bar{X}_{(t:t+\u03c5\u22121)} \\in R^{n\u00d7d}$, computed as follows,\n\n$\\bar{X}_{(t:t+\u03c5\u22121)}=[\u03c3(W_0X_{(t\u2212\u03c4:t\u22121)}) \u2297 W_1X_{(t\u2212\u03c4:t\u22121)}]W_2$\n\nwhere $W_0, W_1, W_2 \\in R^{\u03c4\u00d7d}$ denotes the trainable weight matrices, and an element-wise multiplication operation denoted by \u2297. The non-linear activation function, \u03c3, is applied to enhance the representation learning process."}, {"title": "SPATIAL-INFERENCE", "content": "Figure 5 shows the spatial inference component of the framework, which comprises of three distinct methods. Further details are discussed in the following sections.\n\nHypergraph inference and representation learning\n\nThe \u201cimplicit hypergraph\u201d method is composed of two modules: hypergraph inference(HgI) and hypergraph representation learning(HgRL). HgI module uses a similarity metric learning method to capture the hierarchical interdependence relations among time-series variables and compute a discrete hypergraph topology for a hypergraph-structured representation of MTS data. The differentiable embeddings, $z_i, z_j \\in R^{(d)}$, where $1\u2264i\u2264n$ and $1\u2264j\u2264m, represent the hypernodes and hyperedges of the hypergraph, respectively, and capture their global-contextual behavioral patterns in a d-dimensional vector space. These embeddings enable the HgI module to effectively model the dynamic relationships among the variables over time, making it a powerful tool for learning task-relevant relational hypergraph structures from complex MTS data. We compute the pairwise similarity between any pair $z_i$ and $z_j$ as follows,\n\n$P_{i,j}=\u03c3([S_{i,j} ||1 \u2212 S_{i,j}]); S_{i,j}=\\frac{z_i^T z_j + 1}{2 ||z_i|| \u00b7 ||z_j||}$ \n\nwhere \u2225 denotes vector concatenation. The sigmoid activation function maps the pairwise scores to the range [0,1]. The hyperedge probability over hypernodes of the hypergraph is denoted by $P^{(k)}_{i,j} \\in R^{nm\u00d72}$, where k\u2208{0, 1}. The scalar value of $P^{(k)}_{i,j} \\in [0, 1]$ encodes the relation between an arbitrary pair of hypernodes and hyperedges (i, j). $P^{(0)}_{i,j}$ represents the probability of a hypernode i connected to hyperedge j, while $P^{(1)}_{i,j}$ represents the probability that the hypernode i is not connected to the hyperedge j. We utilize the Gumbel-softmax trick, as presented in [Jang et al., 2016], which allows for accurate and efficient sampling of discrete hypergraph structures from the hyperedge probability distribution Pi,j . This technique enables the HgI module to effectively capture intricate relationships among variables in MTS data. The connectivity pattern of the sampled hypergraph structure is represented by an incidence matrix $I \\in R^{n\u00d7m}$, which encapsulates the relationships between hypernodes and hyperedges in the hypergraph. The Gumbel-softmax trick enables learning of the hypergraph structure in an end-to-end differentiable manner, facilitating the application of gradient-based optimization methods during model training within an inductive learning approach. The incidence matrix is computed as,\n\n$I_{i,j}=exp(\\frac{g^{(k)}_{i,j} + P^{(k)}_{i,j}}{\u03b3})exp(\\frac{g^{(k)}_{i,j} + P^{(k)}_{i,j}}{\u03b3})$\n\nWhere, the temperature parameter(\u03b3) of the Gumbel Softmax distribution is set to 0.05. The random noise sampled from the Gumbel distribution is denoted by $g^{(k)}_{ij}\u223cGumbel(0, 1)=log(\u2212 log(U(0, 1))$, where U denotes the uniform distribution with a range of 0 to 1. The learned hypergraph is then regularized to be sparse by optimizing the probabilistic hypergraph distribution parameters, which drops the redundant hyperedges over hypernodes. The forecasting task provides indirect supervisory information, which helps to reveal the higher-order structure or hypergraph relation structure in the observed MTS data. We utilize a sequence of dynamic hypergraphs to represent the MTS data, where each hypergraph is denoted by $HG_t=[HV, HE, X_{(t\u2212\u03c4:t\u22121)}, I]$. A hypergraph representation learning(HgRL) module is employed to compute optimal hypernode-level representations that capture the spatio-temporal dynamics within the hypergraph-structured MTS data. These representations are then used for performing inference on the downstream multi-horizon forecasting task. The HgRL module is a neural network architecture that utilizes both Hypergraph Attention Network(HgAT) and Hypergraph Transformer(HgT) to accomplish this task. HgT employs multi-head self-attention mechanisms to learn latent hypergraph representations, $h\u2032_i(t)$, without prior knowledge about the hypergraph structure. On the other hand, HgAT performs higher-order message-passing schemes on the hypergraph topology to compute the latent hypernode representations, $h_i(t)$. The combination of HgT and HgAT provides HgRL with a powerful backbone for capturing complex relationships and dependencies among variables within the hypergraph-structured MTS data in the differentiable latent hypergraph representations. Further implementation details and an in-depth explanation are available in the appendix. A gating mechanism is implemented to regulate the information flow from $h\u2032_i(t)$ and $h_i(t)$, which produces a weighted combination of representations $h_{i,IMP}(t)$. The gating mechanism is described by,\n\n$g\u2032=\u03c3(f\u2032_s (h\u2032_i(t)) + f\u2032_g (h_i(t)))$\n\n$h_{i,IMP}(t)=\u03c3(g\u2032(h\u2032_i(t))) + (1 \u2212 g\u2032)(h_i(t))$\n\nwhere $f\u2032_s$ and $f\u2032_g$ are linear projections. Fusing representations can be useful for modeling the multi-scale interactions underlying spatio-temporal hypergraph data and can help mitigate overfitting. By incorporating the most relevant information, the proposed framework captures time-evolving underlying patterns in MTS data, resulting in more accurate and robust forecasts. In brief, the hypergraph learning module optimizes the discrete hypergraph structure using a similarity metric learning technique and formulates the posterior forecasting task as message-passing schemes with hypergraph neural networks to learn optimal hypergraph representations, resulting in accurate and expressive representations of MTS data, thereby improving forecast accuracy.\n\nSubgraph Representation Learning\n\nWe represent the MTS data as continuous-time spatio temporal graphs, utilizing domain-specific knowledge, where each graph is denoted by $G_t=[V, E, X_{(t\u2212\u03c4:t\u22121)}, A^{(0)}]$. Subgraphs, which are substructures within a larger graph exhibit higher-order connectivity patterns, provide a powerful mechanism for capturing the complex interactions among time series variables in the graph-structured MTS data. As a result, subgraphs are more relevant for learning the complex spatio-temporal dependencies in the MTS data. By extracting higher-order connectivity patterns, subgraphs enable the learning of more accurate and interpretable latent representations, leading to improved performance on downstream MTSF task. The subgraph representation learning(SgRL) method involves two sequential modules: patch extraction and subgraph encoder, which collaboratively extract and encode subgraphs, resulting in expressive node-level representations that capture both local neighborhood and larger-scale structural information from the original explicit graph. Figure 3 depicts the SgRL method. The patch extraction module partitions the explicit graph at each time step into overlapping patches, also known as subgraph patches. Let\u2019s consider an explicit graph $G_t$=(V, E), an integer k and a positive integer p. We aim to partition an explicit graph $G_t$ with node set V and edge set E into k subgraph patches, denoted by $G_t^{(1)}, G_t^{(2)}, . . . , G_t^{(k)}$, where each subgraph patch consists of $\\frac{|V|}{k}$ nodes in chronological order and their p-hop neighbors in the original graph $G_t$. The task involves selecting the nodes and edges of each subgraph to ensure that they are mutually exclusive and form a connected subgraph with their p-hop neighbors. In short, to achieve the partition of explicit graph $G_t$=(V, E) into k subgraphs with p-hop neighbors, we can perform the following steps:\n\n\u2022 Divide the set of nodes V into non-overlapping k partitions $V^{(1)}, V^{(2)}, . . . , V^{(k)}$, each containing $\\frac{|V|}{k}$ nodes of the graph in chronological order. $V=V^{(1)} \u222a . . . \u222a V(k)$ and $V^{(i)} \u2229 V^{(j)}=\u2205, \u2200i\u0338=j$.\n\n\u2022 For each partition $V^{(i)}$, find the set of p-hop neighbors $N_p(u), u\u2208V(i)$ in the original graph $G_t$, where $N_p(u)$ defines the p-hop neighborhood of node u. Basically, we expand each partition to their p-hop neighbourhood in order to preserve the structural information between multiple partitions and utilize pair-wise graph connections: $V^{(i)}\u2190V(i) \u222a [N_p(u)|u\u2208V(i)]$.\n\n\u2022 Define the subgraph patch $G_t^{(i)}=(V^{(i)} \u222a N_p(u), E^{(i)})$, where $u\u2208V(i), E^{(i)}$ is the set of edges in E that have both endpoints in $V^{(i)} \u222a N_p(u), u\u2208V(i)$.\n\nRepeat steps 2-3 for all partitions $V^{(i)}$ to obtain the set of subgraph patches $G_t^{(1)}, G_t^{(2)}, . . . , G_t^{(k)}$. The subgraphs, $G_t^{(i)}$ exhibit a diverse range of topological structures with varying numbers of nodes, edges, and connectivity, rendering them non-uniform in size. The subgraph encoder, which is applicable to arbitrary subgraph patches, captures the structural relationships between multiple-time series variables and generates fixed-length node-level vector representations of the subgraph patches. Subgraph encoding is particularly useful for large spatio-temporal graphs that are impractical to process as a whole, as it enables computation of node-level representations for subsets of the graph, rather than the entire graph. This enables capturing the essential structural information of spatio-temporal graphs while maintaining low computational complexity and memory requirements. The subgraph encoder uses a p-subtree GNN extractor to generate node-level representations for each subgraph patch $G_t^{(i)}$. The encoder extracts local structural and feature information by applying a GNN model([Kipf and Welling, 2016]) to the subgraph with node feature vector, $\\bar{x}_{u, t}^{(i)}$ for a given node $u\u2208V(i)$. The output node representation, denoted by $h^{(i,t)}_u$ at u, serves as the subgraph representation at u, where the superscript i denotes the subgraph patch. The p-subtree GNN extractor is a technique used in graph neural networks(GNNs) for feature extraction. For a node u in a patch, the p-subtree GNN extractor recursively constructs all possible subtrees of radius p around it. Each subtree is treated as a separate subgraph, and a GNN is applied to each subgraph to aggregate features from the nodes within the subtree. The resulting feature vector represents the p-hop neighborhood of node u. The connectivity pattern for a given subgraph patch $G_t^{(i)}$ is described by the patch adjacency matrix $A^{(i)}\u2208{0, 1}^{|Vi|\u00d7|Vi|}$. To extract features from the subgraph, a GNN model with p layers, denoted as $GNN_t^{(p)}$ is applied to the subgraph patch $G_t^{(i)}$ with node feature vector $\\bar{x}_{u, t}^{(i)}$ and patch adjacency matrix $A^{(i)}$, resulting in the output node representation, $h^{(i, t)}_u$ at node u. In concise form, we can express the subgraph representation learning function as:\n\n$h^{(i,t)}_u =GNN_t^{(p)}(u)$\n\nThe p-subtree GNN extractor can represent the p-subtree structure rooted at node u. Since node u may appear in multiple subgraph patches, we calculate the mean of its node representations across all subgraph patches $G_t^{(i)}, i\u2208k$, to produce a fixed-size vector representation $h^{SUB}_u(t)$ of each node u in the original input graph $G_t$.\n\nDual Hypergraph Representation Learning\n\nSpatio-temporal Graph Neural Networks(STGNNs) have shown to be effective in modeling graph-structured data by integrating both node and edge features. Nevertheless, STGNNs have primarily focused on nodes and their connectivity, neglecting the significant role of edges in graph structure. Even with explicit edge representation, STGNNs face challenges in capturing critical edge information, resulting in limitations to their success. Furthermore, STGNNs utilize edge features as auxiliary information to enhance node-level representations, leading to suboptimal edge information capture. Edges capture the interaction, dependency, or similarity between nodes, which is essential in modeling the complex sensor network-based dynamical systems. By incorporating the latent edge information, STGNNs can more accurately represent the structure and dynamics of these complex systems, leading to better predictions and decision-making. Thus, more effective methods are necessary to represent edge information within STGNNs to achieve comprehensive and accurate spatio-temporal graph modeling. To overcome this challenge, a simple yet powerful message-passing scheme tailored specifically to edges has been proposed. This approach provides optimal edge representation, effectively addressing the limitations of previous STGNN approaches. The proposed solution to address the limited edge representation in spatio-temporal graph modeling approaches involves a Dual Hypergraph Transformation(DHT) method that transforms edges into hypernodes and nodes into hyperedges, resulting in dual hypergraphs that can capture higher-order interactions among hypernodes. This graph-to-hypergraph transformation is influenced by hypergraph duality([Berge, 1973], [Scheinerman and Ullman, 2011]). By using the DHT method to represent edges as hypernodes in a hypergraph, any existing hypergraph message-passing schemes designed for hypernode-level representation learning can be applied for learning the representation of the edges in the spatio-temporal graphs. This hypergraph-based approach is particularly effective, as it enables more comprehensive and accurate graph modeling of spatio-temporal graphs by better representing the crucial edges information.\n\nLet $G_t=[X_{(t:t+\u03c5\u22121)}, I^{(0)}, E_{(t:t+\u03c5\u22121)}]$ denote the spatio temporal graphs. The connections between the nodes and edges are obtained from the prior knowledge of time-series relationships, and they are described by the incidence matrix, $I^{(0)}\u2208{0, 1}^{|V|\u00d7|E|}$. $X_{(t:t+\u03c5\u22121)}\u2208R^{|V|\u00d7d}, E_{(t:t+\u03c5\u22121)}\u2208R^{|E|\u00d7d}$ denote the node features and empty edge features, respectively. The Dual Hypergraph Transformation(DHT) is a method of transforming spatio-temporal graphs to obtain new dual hypergraphs. This is achieved by interchanging the roles of nodes and edges in spatio-temporal graphs, allowing for the use of hypernode-based message-passing methods to learn the edges representations while maintaining the original spatio-temporal graphs information. To achieve this transformation, the incidence matrix of the original spatiotemporal graph Gt is transposed to obtain the incidence matrix for the new dual spatio-temporal hypergraph $G_t^*$. In addition to the structural transformation through the incidence matrix, the DHT interchanges node and edge features across Gt and $G_t^*$. In brief, the transformation maps a spatio-temporal graph triplet representation to its corresponding dual spatio-temporal hypergraph representation while preserving the information in the original graph features, as follows:\n\n$DHT : G_t=[X_{(t\u2212\u03c4:t\u22121)}, I^{(0)}, E_{(t\u2212\u03c4:t\u22121)}] \\to G_t^* =[E_{(t\u2212\u03c4:t\u22121)}, I^{(0)T}, X_{(t\u2212\u03c4:t\u22121)}]$\n\nHere, $E_{(t\u2212\u03c4:t\u22121)}, I^{(0)T}$, and $X_{(t\u2212\u03c4:t\u22121)}$ represent the hypernode feature matrix, the incidence matrix, and the hyperedge feature matrix of the dual hypergraph, $G_t^*$, respectively. This precise and efficient approach to spatio-temporal graph transformation can capture different aspects of the underlying relational structure of interconnected dynamical systems and improves downstream forecasting accuracy. We represent the MTS data as the hyperedge-attributed dual hypergraphs($G_t^*$), and the dual hypergraph-structured MTS data are processed by a hypergraph representation learning(HgRL) module. The HgRL computes optimal hyperedge-level representations $h^{DHT}_u(t)$ that capture the spatio-temporal dynamics within the dual hypergraph-structured MTS data. These representations are further utilized for downstream forecasting tasks, allowing for accurate and reliable multi horizon forecasts."}, {"title": "TEMPORAL-INFERENCE", "content": "The mixture-of-experts(MOE) mechanism in deep learning combines the predictions of multiple subnetworks or experts, such as \u201cimplicit hypergraph\u201d, \u201cexplicit subgraph\u201d, and \u201cdual-hypergraph\u201d representation learning methods, through a gating mechanism that calculates a weighted sum of their predictions based on the input. The objectives of training are to identify the optimal distribution of weights for the gating function and to train the experts using the specified weights. In the context of cooperative game theory, the mixture of experts can be viewed as a cooperative game where the experts work together to optimize the system\u2019s performance by accurately predicting the output given an input, rather than maximizing their individual payoffs. The gating mechanism can be trained to optimize the weights or probabilities assigned to each agent\u2019s expertise, based on their individual performance and the overall performance of the system, resulting in a globally optimal solution. To obtain fused representations in the MOE mechanism, the multiple experts predictions are combined using the weights calculated by the gating mechanism as follows,\n\n$g\u2032\u2032=\u03c3(f\u2032\u2032_s (h_{i,IMP}(t) + h_{i,SUB}(t)) + f\u2032\u2032_g (h_{i,DHT}(t)))$\n\n$h_i(t)=\u03c3(g\u2032\u2032(h_{i,IMP}(t) + h_{i,SUB}(t))) + (1 \u2212 g\u2032\u2032)(h_{i,DHT}(t))$\n\nThe \u201cimplicit hypergraph\u201d, \u201cexplicit subgraph\u201d, and \u201cdual-hypergraph\u201d methods compute the representations $h_{i,IMP}(t), h_{i,SUB}(t), h_{i,DHT}(t)$, respectively, where $f\u2032\u2032_s$ and $f\u2032\u2032_g$ are linear projections. The fused representations are input to a subsequent temporal feature extractor that models the non-linear temporal dynamics of inter-series dependencies among variables in the spatio-temporal MTS data using a stack of 1 \u00d7 1 convolutions. Finally, the extractor predicts the pointwise forecasts, $\\hat{X}_{(t:t+\u03c5\u22121)}$. Our proposed framework uses a spatial-then-time modeling approach to learn the multiple structure representations and dynamics in MTS data. By first encoding the spatial information of the relational structure, including explicit graph, implicit hypergraph, and dual hypergraph our approach captures complex dependencies among the variables. By incorporating the temporal inference component, the framework analyzes the evolution of these dependencies over time to improve interpretability and generalization. This approach is beneficial for real-world applications involving complex spatial-temporal dependencies that are challenging to model using traditional methods. Additionally, our framework variant(w/Unc- MKH-Net) provides accurate and reliable uncertainty estimates of multi-horizon forecasts by minimizing the negative Gaussian log likelihood. Our proposed methods(MKH-Net, w/Unc- MKH-Net) enable simultaneous modeling of latent interdependencies and analyzing their evolution over time in sensor network-based dynamical systems in an end-to-end manner."}, {"title": "EXPERIMENTAL RESULTS", "content": "Table 1 presents a thorough comparison of the proposed models(MKH-Net and w/Unc-MKH-Net), against various baseline models on the MTSF task across five distinct benchmark datasets(PeMSD3, PeMSD4, PeMSD7, PeMSD7M, and PeMSD8). We evaluated the models performance using forecast errors for a widely recognized benchmark of 12(\u03c4 )step-prior to 12(\u03c5)-step-ahead forecasting task. Our evaluation employed a multi-metric approach in multi-horizon prediction tasks for a comprehensive evaluation of the models performance compared to baseline models. To ensure a comprehensive and robust evaluation of the model\u2019s performance, various performance metrics, such as mean absolute error(MAE), root mean squared error(RMSE), and mean absolute percentage error(MAPE), were employed in the assessment. The results of the baseline models from [Choi et al., 2022] are reported in this current study. Our experimental results demonstrate that the proposed models(MKH-Net, w/Unc-MKH-Net) consistently outperform baseline models with lower forecast errors across the various benchmark datasets. The proposed model(MKH-Net) achieved a significant reduction of 28.39%, 8.50%, 0.24%, 15.41%, and 6.86% in the RMSE metric compared to the next-best baseline models on the PeMSD3, PeMSD4, PeMSD7, PeMSD8, and PeMSD7(M) datasets, respectively. In addition to pointwise forecasts, the w/Unc-MKH-Net model(MKH-Net integrated with local uncertainty estimation) predicts time-varying estimates of uncertainty in model predictions. Despite its slightly inferior performance to the MKH-Net model, it still outperforms several robust baselines in the literature, as evidenced by the reduced prediction error. The empirical results highlight the effectiveness of the proposed neural forecasting architecture in capturing the complex and nonlinear spatio-temporal dynamics present in MTS data, resulting in improved forecasts. Further information on the experimental methodology, ablation studies, and additional experimental results can be found in the appendix. The appendix also includes a detailed analysis of the MKH-Net capability to handle missing data, and provides a more in-depth analysis of the w/Unc-MKH-Net ability to estimate uncertainty. Moreover, the appendix provides comprehensive visualizations of model predictions with uncertainty estimates compared to the ground truth and offers additional information on existing works and a brief overview of baselines. Lastly, the appendix addresses the subject of compositional generalization in the context of graph time series forecasting."}, {"title": "CONCLUSION", "content": "Our proposed framework integrates implicit hypergraph, explicit subgraph, and dual-hypergraph representation learning methods to provide a thorough understanding of the spatio-temporal dynamics in MTS data, enabling accurate multi-horizon forecasting. Our approach has been validated by experimental results on real-world datasets, demonstrating its effectiveness through improved multi-horizon forecasts and reliable uncertainty estimations."}, {"title": "APPENDIX", "content": "HYPERGRAPH ATTENTION NETWORK(HgAT)\nThe Hypergraph Attention Network(HgAT) operator extends attention-based convolution operations to spatio-temporal hypergraphs, allowing for more flexibility and expressiveness in modeling complex relationships among time series variables in large MTS datasets. By incorporating both local and global attention-based convolution operations, the HgAT operator efficiently learns hypergraph representations that captures the complex spatio-temporal dynamics within the hypergraph-structured MTS data, making it a powerful and flexible tool for spatio-temporal data analysis and modeling. This hypergraph encoder performs inference on hypergraph-structured MTS data, denoted by $HG_"}]}