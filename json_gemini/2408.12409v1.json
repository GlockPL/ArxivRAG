{"title": "Multi-Source Knowledge-Based Hybrid Neural Framework for Time Series Representation Learning", "authors": ["Sagar Srinivas Sakhinana", "Krishna Sai Sudhir Aripirala", "Shivam Gupta", "Venkataramana Runkana"], "abstract": "Accurately predicting the behavior of complex dynamical systems, characterized by high-dimensional multivariate time series(MTS) in interconnected sensor networks, is crucial for informed decision-making in various applications to minimize risk. While graph forecasting networks(GFNs) are ideal for forecasting MTS data that exhibit spatio-temporal dependencies, prior works rely solely on the domain-specific knowledge of time-series variables inter-relationships to model the nonlinear dynamics, neglecting inherent relational-structural dependencies among the variables within the MTS data. In contrast, contemporary works infer relational structures from MTS data but neglect domain-specific knowledge. The proposed hybrid architecture addresses these limitations by combining both domain-specific knowledge and implicit knowledge of the relational structure underlying the MTS data using Knowledge-Based Compositional Generalization. The hybrid architecture shows promising results on multiple benchmark datasets, outperforming state-of-the-art forecasting methods. Additionally, the architecture models the time-varying uncertainty of multi-horizon forecasts.", "sections": [{"title": "INTRODUCTION", "content": "Multivariate time series forecasting(MTSF) is a crucial task with diverse applications in various sectors, including finance, healthcare, energy, and others. MTSF facilitates strategic decision-making by predicting interrelated variables that change over time. In retail and e-commerce, it is used to forecast product demand, optimize supply chains, and manage inventory levels. Cloud providers utilize MTSF to accurately predict web traffic and efficiently scale server fleets to meet anticipated demand. Forecasting MTS data is challenging due to the complex interrelationships among multiple variables and the unique characteristics of MTS data, such as non-linearity, high-dimensionality and non-stationarity. Over the past few years, Spatial-temporal graph neural networks(STGNNs) have gained popularity for modeling intricate dependencies in MTS data, improving forecast accuracy. While explicit relationships among variables are provided by human experts through a predefined or explicit graph, implicit relationships are obtained using data-driven neural relational inference methods([Kipf et al., 2018]). Implicit relationships, characterized by their high complexity and non-linearity, evolve over time and reveal hidden interrelations among variables unknown to human experts which are not trivial, whereas explicit relations stemming from domain expertise remain static. Existing \u201chuman-in-the-loop\" STGNNs([Yu et al., 2017], [Li et al., 2017], [Guo et al., 2020]) integrate domain-specific knowledge and learn MTS data dynamics, but real-world situations often present unknown or incomplete graph structures, resulting in suboptimal forecasting. These predefined structures may also inadequately capture non-static spatio-temporal dependencies, impeding the accurate inference of latent time-conditioned relations that influence variable co-movements within the MTS data. Moreover, STGNNs neglect the significance of edges in explicit graph structure, hindering modeling of complex systems. Incorporating effective methods to represent edge information within STGNNs is necessary for accurately modeling complex dynamical systems. Additionally, STGNNs have limitations in capturing the importance of subgraphs within the larger explicit graph structure, which can be addressed by developing new methods to incorporate subgraph dynamics to model complex systems. Conversely, recent \u201chuman-out-of-the-loop\" STGNNs([Shang et al., 2021], [Deng and Hooi, 2021], [Wu et al., 2020]) simultaneously learn the discrete dependency graph structures and the MTS data dynamics, but neglect predefined inter-relationships from domain expertise, leading to subpar performance in graph time-series forecasting. Moreover, implicitly learning the latent graph structure from MTS data is constrained by the limitations of pairwise connections among the variables. However, the interconnected networks in complex dynamical systems could have higher-order structural relations beyond pairwise associations. Hypergraphs, a generalization of graphs, can effectively model these relations in high-dimensional MTS data. Additionally, while conventional STGNNs emphasize pointwise forecasting, they do not offer any estimates of uncertainty for the multi-horizon forecasts. We propose the Multi-Source Knowledge-Based Hybrid Neural Framework(for brevity, MKH-Net) to address these challenges. This framework integrates the domain-specific knowledge and data-driven knowledge using a joint-learning approach to model the complex spatio-temporal dynamics underlying the MTS data, resulting in better forecast accuracy and reliable uncertainty estimates. The proposed framework has two main components: spatial and temporal inference components. Using a space-then-time(STT, [Gao and Ribeiro, 2022]) approach, the framework performs the spatial message-passing schemes prior to the temporal-encoding step. The spatial inference component combines \u201cimplicit hypergraph\u201d, \u201cexplicit subgraph\u201d, and \u201cdual-hypergraph\u201d representation learning methods to learn the various aspects of the underlying structure of interrelationships among variables in MTS data, characterizing the complex sensor network-based dynamical systems. The \"implicit hypergraph\" method learns the hierarchical interdependencies between variables in MTS data by modeling the discrete hypergraph relational structure and performs the hypergraph representation learning schemes to obtain latent hypernode-level representations, which accurately capture the spatio-temporal dynamics of the hypergraph-structured MTS data. The \u201cexplicit subgraph\" method extracts overlapping subgraph patches and uses subgraph message-passing schemes to learn spatio-temporal dynamics within the explicit graph-structured MTS data. The 'dual-hypergraph\u201d method captures the latent information of edges in explicit graph-structured MTS data by utilizing the Dual Hypergraph Transformation(DHT) method. This is achieved through a powerful message-passing scheme that is tailored specifically to the edges in the structured MTS data, resulting in a more accurate modeling of the underlying spatio-temporal dynamics. The proposed MKH-Net framework uses a gating mechanism to perform a convex combination of the multi-knowledge representations computed by the different methods, resulting in more accurate latent representations of the complex non-linear dynamics in MTS data. The MKH-Net framework is capable of capturing various types of dependencies that exist across different observation scales, as correlations among variables may vary in short-term versus long-term views of the MTS data. The temporal learning component models the time-evolving dynamics of interdependencies among variables in MTS data, enabling the framework to provide accurate multi-horizon forecasts and precise predictive uncertainty estimates. To put it briefly, the proposed framework offers an end-to-end methodology for learning spatio-temporal dynamics in MTS data with both explicit graph and implicit hypergraph structures. This approach utilizes multiple representation learning methods, including \u201cexplicit subgraph\u201d, \u201cimplicit hypergraph\", and \"dual-hypergraph\", to capture evolutionary and multi-scale interactions among variables in the latent representations to achieve better modeling accuracy. The framework also models time-varying uncertainty in forecasts and utilizes the learned latent representations for downstream MTSF tasks, resulting in accurate multi-horizon forecasts and reliable predictive uncertainty estimates. Additionally, the framework is designed to offer better generalization and scalability for large-scale spatio-temporal MTS forecasting tasks found in real-world applications."}, {"title": "PROBLEM DEFINITION", "content": "Let us consider a historical time series dataset with n correlated variables, observed over T time steps, represented by the notation X=(x_1,...,x_T). Here, the subscript indicates the time step, while the observations for all the n variables at time step t are denoted by x_t= (x_t^{(1)}, x_t^{(2)},...,x_t^{(n)})\\in\\mathbb{R}^{(n)}, where the superscript refers to the variables. In the context of MTSF, we employ the rolling-window method for multi-horizon forecasting, where a look-back window is predefined at the current time step t to include the prior \\tau-steps of MTS data, to predict the next \\nu-steps. Specifically, we aim to utilize a historical window of n-correlated variables, represented by the notation X_{(t-\\tau: t-1)}\\in\\mathbb{R}^{n\\times\\tau}, which have been observed over the previous \\tau-steps prior to the current time step t, to make predictions about the future values of n variables for the next \\nu-steps, denoted as X_{(t:t+\\nu-1)}\\in\\mathbb{R}^{n\\times\\nu}. To capture the spatio-temporal correlations among a multitude of correlated time series variables, the MTSF problem is formulated on graph and hypergraph structures. The historical inputs are represented as continuous-time spatial-temporal graphs denoted by G_t=(V, E, X_{(t-\\tau: t-1)}, A^{(0)}), where G_t is composed of a set of nodes(V) which represent the variables, edges(E) that describe the connections among the variables, and a node feature matrix X_{(t-\\tau: t-1)} that changes over time. The explicit static-graph structure based on prior knowledge of time-series variables relationships is described by the adjacency matrix A\\in\\{0,1\\}^{|V|\\times|V|}. To further capture the complex relationships among MTS data, we consider the historical inputs as a sequence of dynamic hypergraphs, denoted by HG_t=(H_V,H_E, X_{(t-\\tau: t-1)}, I). Here, the hypergraph is represented by a fixed set of hypernodes(H_V) and hyperedges(H_E), where the hypernodes denote the variables, and the hyperedges capture the latent higher-order relationships between the hypernodes. The time-varying hypernode feature matrix is given by X_{(t-\\tau: t-1)}. The implicit hypergraph structure is learned through an embedding-based similarity metric learning approach. The incidence matrix I\\in\\mathbb{R}^{n\\times m} describes the hypergraph structure, where I_{p,q}=1 if the hyperedge q is incident with hypernode p, and otherwise 0. The sparsity of the hypergraph is determined by the number of hyperedges(m) in the hypergraph. The proposed framework aims to learn a function F(\\theta) that can map MTS data, X_{(t-\\tau: t-1)}, to their respective future values, X_{(t:t+\\nu-1)}, given a G_t and HG_t."}, {"title": "OUR APPROACH", "content": "Our framework presents a neural forecasting architecture composed of three main components: the projection layer, spatial inference, and temporal inference components, which are illustrated in Figure 1. The spatial inference component includes three methods: \u201cimplicit hypergraph\u201d, \u201cexplicit subgraph\u201d, and \u201cdual-hypergraph\u201d representation learning methods. The \"implicit hypergraph\" method computes the dependency hypergraph structure of multiple time series variables and uses higher-order message-passing schemes to model the hypergraph-structured MTS data. This approach computes time-conditioned, optimal hypernode-level representations, capturing complex relationships between variables over time. The \u201cexplicit subgraph\" method consists of two modules, the patch extraction and the subgraph encoder. The patch extraction module extracts overlapping subgraph patches from a predefined graph, while the subgraph encoder module uses spatial graph-filtering techniques to compute time-evolving, optimal node-level representations, which effectively capture the underlying spatio-temporal dynamics of the graph-structured MTS data. The \"dual-hypergraph\u201d method transforms edges(nodes) in an explicit graph into hypernodes(hyperedges) in a dual hypergraph, allowing hypernode-level message-passing schemes to be applied for edge representation learning of the explicit graphs. The temporal inference component of the framework combines multiple latent representations from different methods and learns their temporal dynamics. By jointly optimizing the different learning components, the framework provides accurate multi-horizon forecasts and reliable uncertainty estimates for various time-series forecasting tasks."}, {"title": "PROJECTION LAYER", "content": "The proposed framework includes a projection layer that utilizes gated linear networks(GLUs, [Dauphin et al., 2017]) to learn non-linear representations of the input data. The input data is represented by X_{(t-\\tau: t-1)}\\in\\mathbb{R}^{n\\times\\tau}, and the projection layer uses GLUs to selectively pass information through a gating mechanism and transform the input data into a new feature matrix, X_{(t:t+\\nu-1)}\\in\\mathbb{R}^{n\\times d}, computed as follows,\nX_{(t:t+\\nu-1)}=({\\sigma}(W_0X_{(t-\\tau: t-1)}) \\odot W_1X_{(t-\\tau: t-1)})W_2\nwhere W_0, W_1,W_2\\in\\mathbb{R}^{\\tau\\times d} denotes the trainable weight matrices, and an element-wise multiplication operation denoted by \\odot. The non-linear activation function, \\sigma, is applied to enhance the representation learning process."}, {"title": "SPATIAL-INFERENCE", "content": "Figure 5 shows the spatial inference component of the framework, which comprises of three distinct methods. Further details are discussed in the following sections.\nHypergraph inference and representation learning\nThe \"implicit hypergraph\" method is composed of two modules: hypergraph inference(HgI) and hypergraph representation learning(HgRL). HgI module uses a similarity metric learning method to capture the hierarchical interdependence relations among time-series variables and compute a discrete hypergraph topology for a hypergraph-structured representation of MTS data. The differentiable embeddings, z_i, z_j\\in\\mathbb{R}^{(d)}, where 1<i<n and 1<j<m, represent the hypernodes and hyperedges of the hypergraph, respectively, and capture their global-contextual behavioral patterns in a d-dimensional vector space. These embeddings enable the HgI module to effectively model the dynamic relationships among the variables over time, making it a powerful tool for learning task-relevant relational hypergraph structures from complex MTS data. We compute the pairwise similarity between any pair z_i and z_j as follows,\nP_{i,j}=([S_{i,j}||1 - S_{i,j}]); S_{i,j}=\\sigma(\\frac{z_i^Tz_j}{||z_i||||z_j||}+1)\nwhere || denotes vector concatenation. The sigmoid activation function maps the pairwise scores to the range [0,1]. The hyperedge probability over hypernodes of the hypergraph is denoted by P_k \\in\\mathbb{R}^{nm\\times 2}, where k\\in\\{0,1\\}. The scalar value of P_{(i, j)}\\in[0, 1] encodes the relation between an arbitrary pair of hypernodes and hyperedges (i, j). P_0 represents the probability of a hypernode i connected to hyperedge j, while P_1 represents the probability that the hypernode i is not connected to the hyperedge j. We utilize the Gumbel-softmax trick, as presented in [Jang et al., 2016], which allows for accurate and efficient sampling of discrete hypergraph structures from the hyperedge probability distribution P_{i,j}. This technique enables the HgI module to effectively capture intricate relationships among variables in MTS data. The connectivity pattern of the sampled hypergraph structure is represented by an incidence matrix I\\in\\mathbb{R}^{n\\times m}, which encapsulates the relationships between hypernodes and hyperedges in the hypergraph. The Gumbel-softmax trick enables learning of the hypergraph structure in an end-to-end differentiable manner, facilitating the application of gradient-based optimization methods during model training within an inductive-learning approach. The incidence matrix is computed as,\nL_{i,j}=exp ((\\frac{P_{i,j}^{(k)}+g_{i,j}^{(k)}}{\\tau})) /\\sum_k exp((\\frac{P_{i,j}^{(k)}+g_{i,j}^{(k)}}{\\tau}))\nWhere, the temperature parameter(\\tau) of the Gumbel-Softmax distribution is set to 0.05. The random noise sampled from the Gumbel distribution is denoted by g^{(k)}\\sim Gumbel(0, 1)=log(-log(U(0, 1)), where U denotes the uniform distribution with a range of 0 to 1. The learned hypergraph is then regularized to be sparse by optimizing the probabilistic hypergraph distribution parameters, which drops the redundant hyperedges over hypernodes. The forecasting task provides indirect supervisory information, which helps to reveal the higher-order structure or hypergraph relation structure in the observed MTS data. We utilize a sequence of dynamic hypergraphs to represent the MTS data, where each hypergraph is denoted by HG_t=(H_V, H_E, X_{(t-\\tau: t-1)}, I). A hypergraph representation learning(HgRL) module is employed to compute optimal hypernode-level representations that capture the spatio-temporal dynamics within the hypergraph-structured MTS data. These representations are then used for performing inference on the downstream multi-horizon forecasting task. The HgRL module is a neural network architecture that utilizes both Hypergraph Attention Network(HgAT) and Hypergraph Transformer(HgT) to accomplish this task. HgT employs multi-head self-attention mechanisms to learn latent hypergraph representations, h_i^{(t)}, without prior knowledge about the hypergraph structure. On the other hand, HgAT performs higher-order message-passing schemes on the hypergraph topology to compute the latent hypernode representations, h_j^{(t)}. The combination of HgT and HgAT provides HgRL with a powerful backbone for capturing complex relationships and dependencies among variables within the hypergraph-structured MTS data in the differentiable latent hypergraph representations. Further implementation details and an in-depth explanation are available in the appendix. A gating mechanism is implemented to regulate the information flow from h_i^{(t)} and h_j^{(t)}, which produces a weighted combination of representations h_i^{IMP}. The gating mechanism is described by,\ng'={\\sigma}(f_i'(h_i^{(t)})+f_j'(h_j^{(t)}))\nh_i^{IMP}=(g'(h_i^{(t)})) + (1 - g')(h_j^{(t)})\nwhere f_i' and f_j' are linear projections. Fusing representations can be useful for modeling the multi-scale interactions underlying spatio-temporal hypergraph data and can help mitigate overfitting. By incorporating the most relevant information, the proposed framework captures time-evolving underlying patterns in MTS data, resulting in more accurate and robust forecasts. In brief, the hypergraph learning module optimizes the discrete hypergraph structure using a similarity metric learning technique and formulates the posterior forecasting task as message-passing schemes with hypergraph neural networks to learn optimal hypergraph representations, resulting in accurate and expressive representations of MTS data, thereby improving forecast accuracy."}, {"title": "Subgraph Representation Learning", "content": "We represent the MTS data as continuous-time spatio-temporal graphs, utilizing domain-specific knowledge, where each graph is denoted by G_t=(V,E,X_{(t-\\tau: t-1)}, A^{(0)}). Subgraphs, which are substructures within a larger graph exhibit higher-order connectivity patterns, provide a powerful mechanism for capturing the complex interactions among time series variables in the graph-structured MTS data. As a result, subgraphs are more relevant for learning the complex spatio-temporal dependencies in the MTS data. By extracting higher-order connectivity patterns, subgraphs enable the learning of more accurate and interpretable latent representations, leading to improved performance on downstream MTSF task. The subgraph representation learning(SgRL) method involves two sequential modules: patch extraction and subgraph encoder, which collaboratively extract and encode subgraphs, resulting in expressive node-level representations that capture both local neighborhood and larger-scale structural information from the original explicit graph. Figure 3 depicts the SgRL method. The patch extraction module partitions the explicit graph at each time step into overlapping patches, also known as subgraph patches. Let's consider an explicit graph G_t=(V,E), an integer k and a positive integer p. We aim to partition an explicit graph G_t with node set V and edge set E into k subgraph patches, denoted by G^{(1)}, G^{(2)},...,G^{(k)}, where each subgraph patch consists of \\frac{|V|}{k} nodes in chronological order and their p-hop neighbors in the original graph G_t. The task involves selecting the nodes and edges of each subgraph to ensure that they are mutually exclusive and form a connected subgraph with their p-hop neighbors. In short, to achieve the partition of explicit graph G_t=(V,E) into k subgraphs with p-hop neighbors, we can perform the following steps:\n\u2022 Divide the set of nodes V into non-overlapping k partitions V^{(1)}, V^{(2)}, . . ., V^{(k)}, each containing \\frac{|V|}{k} nodes of the graph in chronological order. V=V^{(1)} \\cup . . . \\cup V^{(k)} and V^{(i)} \\cap V^{(j)}=\\emptyset, \\forall i\\neq j.\n\u2022 For each partition V^{(i)}, find the set of p-hop neighbors N_p(u), u\\in V^{(i)} in the original graph G_t, where N_p(u) defines the p-hop neighborhood of node u. Basically, we expand each partition to their p-hop neighbourhood in order to preserve the structural information between multiple partitions and utilize pair-wise graph connections: V^{(i)} \\leftarrow V^{(i)} \\cup \\cup\\{N_p(u)|u\\in V^{(i)}\\}.\n\u2022 Define the subgraph patch G^{(i)}=(V^{(i)} \\cup N_p(u), E^{(i)}), where u\\in V^{(i)}, E^{(i)} is the set of edges in E that have both endpoints in V^{(i)} \\cup N_p(u), u\\in V^{(i)}.\nRepeat steps 2-3 for all partitions V^{(i)} to obtain the set of subgraph patches G^{(1)}, G^{(2)},..., G^{(k)}. The subgraphs, G^{(i)} exhibit a diverse range of topological structures with varying numbers of nodes, edges, and connectivity, rendering them non-uniform in size. The subgraph encoder, which is applicable to arbitrary subgraph patches, captures the structural relationships between multiple-time series variables and generates fixed-length node-level vector representations of the subgraph patches. Subgraph encoding is particularly useful for large spatio-temporal graphs that are impractical to process as a whole, as it enables computation of node-level representations for subsets of the graph, rather than the entire graph. This enables capturing the essential structural information of spatio-temporal graphs while maintaining low computational complexity and memory requirements. The subgraph encoder uses a p-subtree GNN extractor to generate node-level representations for each subgraph patch G^{(i)}. The encoder extracts local structural and feature information by applying a GNN model([Kipf and Welling, 2016]) to the subgraph with node feature vector, x_{ut}^{(i)}, for a given node u\\in V^{(i)}. The output node representation, denoted by h_{u, t}^{(i)} at u, serves as the subgraph representation at u, where the superscript i denotes the subgraph patch. The p-subtree GNN extractor is a technique used in graph neural networks(GNNs) for feature extraction. For a node u in a patch, the p-subtree GNN extractor recursively constructs all possible subtrees of radius p around it. Each subtree is treated as a separate subgraph, and a GNN is applied to each subgraph to aggregate features from the nodes within the subtree. The resulting feature vector represents the p-hop neighborhood of node u. The connectivity pattern for a given subgraph patch G^{(i)} is described by the patch adjacency matrix A^{(i)}\\in\\{0,1\\}^{|V_i|\\times|V_i|}. To extract features from the subgraph, a GNN model with p layers, denoted as GNN^{(P)}, is applied to the subgraph patch G^{(i)} with node feature vector x_{u,t}^{(i)} and patch adjacency matrix A^{(i)}, resulting in the output node representation, h_{u, t}^{(i)} at node u. In concise form, we can express the subgraph representation learning function as:\nh_{u,t}^{(i)}=GNN^{(P)}(x_{ut}^{(i)}, A_i)\nThe p-subtree GNN extractor can represent the p-subtree structure rooted at node u. Since node u may appear in multiple subgraph patches, we calculate the mean of its node representations across all subgraph patches G^{(i)}, i\\in k, to produce a fixed-size vector representation h_u^{(t)} of each node u in the original input graph G_t."}, {"title": "Dual Hypergraph Representation Learning", "content": "Spatio-temporal Graph Neural Networks(STGNNs) have shown to be effective in modeling graph-structured data by integrating both node and edge features. Nevertheless, STGNNs have primarily focused on nodes and their connectivity, neglecting the significant role of edges in graph structure. Even with explicit edge representation, STGNNs face challenges in capturing critical edge information, resulting in limitations to their success. Furthermore, STGNNs utilize edge features as auxiliary information to enhance node-level representations, leading to suboptimal edge information capture. Edges capture the interaction, dependency, or similarity between nodes, which is essential in modeling the complex sensor network-based dynamical systems. By incorporating the latent edge information, STGNNs can more accurately represent the structure and dynamics of these complex systems, leading to better predictions and decision-making. Thus, more effective methods are necessary to represent edge information within STGNNs to achieve comprehensive and accurate spatio-temporal graph modeling. To overcome this challenge, a simple yet powerful message-passing scheme tailored specifically to edges has been proposed. This approach provides optimal edge representation, effectively addressing the limitations of previous STGNN approaches. The proposed solution to address the limited edge representation in spatio-temporal graph modeling approaches involves a Dual Hypergraph Transformation(DHT) method that transforms edges into hypernodes and nodes into hyperedges, resulting in dual hypergraphs that can capture higher-order interactions among hypernodes. This graph-to-hypergraph transformation is influenced by hypergraph duality([Berge, 1973], [Scheinerman and Ullman, 2011]). By using the DHT method to represent edges as hypernodes in a hypergraph, any existing hypergraph message-passing schemes designed for hypernode-level representation learning can be applied for learning the representation of the edges in the spatio-temporal graphs. This hypergraph-based approach is particularly effective, as it enables more comprehensive and accurate graph modeling of spatio-temporal graphs by better representing the crucial edges information.\nLet G_t=(X_{(t:t+\\nu-1)}, I^{(0)}, E_{(t:t+\\nu-1)}) denote the spatial-temporal graphs. The connections between the nodes and edges are obtained from the prior knowledge of time-series relationships, and they are described by the incidence matrix, I^{(0)}\\in\\{0,1\\}^{|V|\\times|E|}. X_{(t:t+\\nu-1)}\\in\\mathbb{R}^{|V|\\times d}, E_{(t:t+\\nu-1)}\\in\\mathbb{R}^{|E|\\times d} denote the node features and empty edge features, respectively. The Dual Hypergraph Transformation(DHT) is a method of transforming spatio-temporal graphs to obtain new dual hypergraphs. This is achieved by interchanging the roles of nodes and edges in spatio-temporal graphs, allowing for the use of hypernode-based message-passing methods to learn the edges representations while maintaining the original spatio-temporal graphs information. To achieve this transformation, the incidence matrix of the original spatio-temporal graph G_t is transposed to obtain the incidence matrix for the new dual spatio-temporal hypergraph G'_t. In addition to the structural transformation through the incidence matrix, the DHT interchanges node and edge features across G_t and G'_t. In brief, the transformation maps a spatio-temporal graph triplet representation to its corresponding dual spatio-temporal hypergraph representation while preserving the information in the original graph features, as follows:\nDHT:G_t=(X_{(t-\\tau: t-1)}, I^{(0)}, E_{(t-\\tau: t-1)}) \\leftrightarrow\nG'_t=(E_{(t-\\tau: t-1)}, I^{(0)^T}, X_{(t-\\tau: t-1)})\nHere, E_{(t-\\tau: t-1)}, I^{(0)^T}, and X_{(t-\\tau: t-1)} represent the hypernode feature matrix, the incidence matrix, and the hyperedge feature matrix of the dual hypergraph, G'_t, respectively. This precise and efficient approach to spatio-temporal graph transformation can capture different aspects of the underlying relational structure of interconnected dynamical systems and improves downstream forecasting accuracy. We represent the MTS data as the hyperedge-attributed dual hypergraphs(G'_t), and the dual hypergraph-structured MTS data are processed by a hypergraph representation learning(HgRL) module. The HgRL computes optimal hyperedge-level representations h_i^{DHT} that capture the spatio-temporal dynamics within the dual hypergraph-structured MTS data. These representations are further utilized for downstream forecasting tasks, allowing for accurate and reliable multi-horizon forecasts."}, {"title": "TEMPORAL-INFERENCE", "content": "The mixture-of-experts(MOE) mechanism in deep learning combines the predictions of multiple subnetworks or experts, such as \u201cimplicit hypergraph\u201d, \u201cexplicit subgraph\", and \u201cdual-hypergraph\u201d representation learning methods, through a gating mechanism that calculates a weighted sum of their predictions based on the input. The objectives of training are to identify the optimal distribution of weights for the gating function and to train the experts using the specified weights. In the context of cooperative game theory, the mixture of experts can be viewed as a cooperative game where the experts work together to optimize the system's performance by accurately predicting the output given an input, rather than maximizing their individual payoffs. The gating mechanism can be trained to optimize the weights or probabilities assigned to each agent's expertise, based on their individual performance and the overall performance of the system, resulting in a globally optimal solution. To obtain fused representations in the MOE mechanism, the multiple experts predictions are combined using the weights calculated by the gating mechanism as follows,\ng\"={\\sigma}(f_i\"(h_i^{IMP} +h_j^{(t)})+ f_j\"(h_j^{(t)}))\nh_i^{(t)}=(g\"(h_i^{IMP} +h_j^{(t)})) + (1 - g\") (h_j^{DHT})\nThe \"implicit hypergraph\u201d, \u201cexplicit subgraph\", and \u201cdual-hypergraph\u201d methods compute the representations h_i^{IMP}, h_j^{SUB}, h_i^{DHT}, respectively, where f_i\" and f_j\" are linear projections. The fused representations are input to a subsequent temporal feature extractor that models the nonlinear temporal dynamics of inter-series dependencies among variables in the spatio-temporal MTS data using a stack of 1 x 1 convolutions. Finally, the extractor predicts the pointwise forecasts, X_{(t:t+\\nu-1)}. Our proposed framework uses a spatial-then-time modeling approach to learn the multiple structure representations and dynamics in MTS data. By first encoding the spatial information of the relational structure, including explicit graph, implicit hypergraph, and dual hypergraph our approach captures complex dependencies among the variables. By incorporating the temporal inference component, the framework analyzes the evolution of these dependencies over time to improve interpretability and generalization. This approach is beneficial for real-world applications involving complex spatial-temporal dependencies that are challenging to model using traditional methods. Additionally, our framework variant(w/Unc- MKH-Net) provides accurate and reliable uncertainty estimates of multi-horizon forecasts by minimizing the negative Gaussian log likelihood. Our proposed methods(MKH-Net, w/Unc- MKH-Net) enable simultaneous modeling of latent interdependencies and analyzing their evolution over time in sensor network-based dynamical systems in an end-to-end manner.\""}, {"title": "APPENDIX", "content": "HYPERGRAPH ATTENTION NETWORK(HgAT)\nThe Hypergraph Attention Network(HgAT) operator extends attention-based convolution operations to spatio-temporal hypergraphs", "X_{(t": "tau: t+\\nu-1)"}, "I), to compute the hypernode representation matrix, H_{(t:\\tau: t+\\nu-1)}\\in\\mathbb{R}^{n\\times d}, wherein each row denotes the hypernode representations, h_{i}^{(t)} \\in \\mathbb{R}^{(d)} encapsulating the intricate dynamics of the hypergraph-structured MTS data. Here, HG_t at time step t, is characterized by the incidence matrix, I\\in\\mathbb{R}^{n\\times m}, and feature matrix, X_{(t:\\tau: t+\\nu-1)}\\in\\mathbb{R}^{n\\times d}. The HgAT operator consistently captures the time-evolving dependencies of multiple variables in hypernode representations by encoding both structural and feature attributes of spatio-temporal hypergraphs, thereby modeling the intricate interdependencies and relationships among the variables. Let N_{j,i} represents a subset of hypernodes i associated with a specific hyperedge j. The intra-edge neighborhood of a hypernode i, described as N_{j,i}\\setminus\\{i\\}, comprises a localized group of semantically-correlated time series variables, where the hyperedge j captures the higher-order relationships among the incident hypernodes. Meanwhile, the inter-edge neighborhood of a hypernode i, denoted as N_{i,j}, encompasses the set of hyperedges j connected with hypernode i, further enriching the relationships between the hypernodes and hyperedges. The HgAT operator utilizes the relational inductive bias encoded within the hypergraph's connectivity, and performs the intra-edge and inter-edge neighborhood aggregation schemes to explicitly model the spatio-temporal correlations among time series variables. The intra-edge neighborhood aggregation delves into the interrelations between a particular hypernode i and its neighboring hypernodes N_{j,i}\\setminus\\{i\\} connected by a specific hyperedge j, while the inter-edge neighborhood aggregation considers the relationships between a specific hypernode i and all other hyperedges N_{i,j} incident with it. We perform the attention-based intra-edge neighborhood aggregation to learn the latent hyperedge representations, which helps us better understand the hypergraph-structured MTS data, described as follows.\nh_j^{(t)}={\\Sigma}_z({\\Sigma}_{i \\in N_{j,i}}a_{j,i}^{(t, l, z)})W_0^{(z)}h_i^{(t,l-1,z)}\nwhere the hyperedge representations are denoted by h_j\\in \\mathbb{R}^{(d)}, with the layer denoted by l. Each hypernode is initially represented by its corresponding feature vector, h_{i,0,z}^{(t)} = x_i^{(t)}, where x_i^{(t)} \\in \\mathbb{R}^{(d)} denotes the ith row of the feature matrix X_{(t:\\tau: t+\\nu-1)}\\in\\mathbb{R}^{n\\times d}. The symbol \\sigma represents the sigmoid function. The HgAT operator generates multiple representations of the input data, h_{i,l-1,z}^{(t)} represented by superscript z each with its own set of parameters. These representations are then combined by summation, akin to the multi-head self-attention mechanism([Vaswani et al., 2017"]}