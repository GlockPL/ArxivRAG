{"title": "Explainable CTR Prediction via LLM Reasoning", "authors": ["Xiaohan Yu", "Li Zhang", "Chong Chen"], "abstract": "Recommendation Systems have become integral to modern user experiences, but lack transparency in their decision-making processes. Existing explainable recommendation methods are hindered by reliance on a post-hoc paradigm, wherein explanation generators are trained independently of the underlying recommender models. This paradigm necessitates substantial human effort in data construction and raises concerns about explanation reliability. In this paper, we present ExpCTR, a novel framework that integrates large language model based explanation generation directly into the CTR prediction process. Inspired by recent advances in reinforcement learning, we employ two carefully designed reward mechanisms, LC alignment, which ensures explanations reflect user intentions, and IC alignment, which maintains consistency with traditional ID-based CTR models. Our approach incorporates an efficient training paradigm with LoRA and a three-stage iterative process. ExpCTR circumvents the need for extensive explanation datasets while fostering synergy between CTR prediction and explanation generation. Experimental results demonstrate that ExpCTR significantly enhances both recommendation accuracy and interpretability across three real-world datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Recommendation Systems (RS) have become a cornerstone of modern user experiences, empowering users to discover relevant and personalized items or contents [14]. Collaborative methods [9, 25, 29] have been dominant in this field, leveraging user-item interaction data for future predictions. While these methods, ranging from simple collaborative approaches to deep neural networks, have demonstrated remarkable efficacy in predicting user engagement, particularly in tasks such as click-through rate (CTR) prediction, they often operate as \"black boxes\", offering recommendations without explaining the underlying rationale [44]. The imperative for transparency and accountability has given rise to the burgeoning of explainable recommendation, which moves beyond mere suggestions by providing justifications. Such explanations provide numerous benefits: building user trust and satisfaction, enhancing persuasiveness, and enabling effective debugging and refinement [34]. Currently, the prevailing approach to explainable recommendation relies on a post-hoc paradigm, where explanations are generated independently of the recommendation model after its predictions are made. These methods necessitate substantial human effort to curate external training datasets through customer review processing or handcrafted rules to produce human-readable explanations.\nRecently, Large Language Models (LLMs) have emerged as a powerful tool in natural language processing, demonstrating exceptional reasoning capabilities. Their potential to generate human-readable explanations for complex tasks is particularly promising for explainable recommendation. Studies such as PETER [18] and RecExplainer [16] have explored integrating item and user latent representations into pre-trained language models, harnessing collaborative information to enhance explanation generation. Other researchers probe the innate reasoning capabilities of LLMs for recommendation tasks using in-context learning techniques [24]. Chat-Rec [7] has showcased the potential of LLMs for improving explainability in multi-round conversational contexts. Despite these promising developments, these approaches still rely on post-hoc explanations. They either over-rely on enhancing existing methods by substituting traditional language models with transformer-based LLMs or utilize basic zero-shot generation capabilities. Consequently, research on explainable recommendation with LLMs remains in its infancy. As illustrated in Figure 1, several critical challenges persist:\n\u2022 Resource intensity. Developing high-quality training datasets for explanation generators is resource-intensive, demanding substantial human effort. While customer reviews present a potential source of pseudo-explanations, they necessitate meticulous curation, extraction, and reformulation to yield training samples.\n\u2022 Explanation quality unreliability: The post-hoc paradigm introduces potential discrepancies between the generated explanations and the underlying operations of recommender systems. Current methodologies typically employ a unidirectional information flow, where latent representations or prediction results are passed from the recommender model to a separate explanation generator 1. This unidirectional process lacks mechanisms for quality assessment or feedback from the generated explanations to the existing recommender system. Consequently, there is no assurance that the produced explanations accurately reflect the recommender's internal decision-making process.\nIn light of the aforementioned challenges, we propose ExpCTR, a novel approach that aims to operate in a data-free manner, while fostering synergy between CTR prediction and LLM-based explanation generation. Our method seamlessly integrates LLM-driven explanation generation with the CTR prediction process. Drawing inspiration from recent advancements in reinforcement learning [26], we employ real-world feedback signals to refine the LLM's reasoning capabilities to better align with the objectives of CTR prediction.\nExpCTR involves a carefully crafted prompt template, tailored to fully elicit the LLM's reasoning capabilities through a chain-of-thought prompting strategy. Subsequently, we utilize a proximal policy optimization (PPO) algorithm that incorporates two distinct reward mechanisms: (1) LC alignment reward, which ensures that the produced explanations accurately reflect user intentions and preferences, as assessed by an LLM-based CTR predictor. (2) IC alignment reward, which treats the explanations as a textual input feature for a traditional ID-based CTR model, ensuring that the explanations are consistent with the model's internal mechanisms and predicted outcomes. These two rewards collectively incentivize the LLMs to generate explanations that are both human-centric and recommender-aligned. To accommodate the reward designs, we devise a specific training paradigm that leverages LoRA for LLM lightweight fine-tuning. This paradigm is based on a three-stage iterative process, consisting of aligning with user interactions with LC alignment reward, training a CTR model with textual features, and aligning with the recommender system's internal mechanisms with IC alignment reward. These stages are iteratively repeated to progressively improve the ExpCTR's performance. Our approach effectively circumvents the need for extensive explanation data construction and fosters collaboration between LLM-driven explainability and accurate CTR prediction. By deepening the understanding of user preferences and the recommendation mechanism, ExpCTR shows the potential to significantly enhance both interpretability and recommendation effectiveness. Our key contributions can be summarized as follows:\n\u2022 We introduce ExpCTR, an innovative framework that enhances the reasoning capabilities of LLMs to generate precise explanations that are closely aligned with CTR models. This approach simultaneously improves CTR prediction performance and RS interpretability. To the best of our knowledge, this represents the first attempt to leverage LLMs for this dual purpose without dependence on extensive data resources.\n\u2022 We develop a reinforcement learning based approach to efficiently fine-tune LLMs using LoRA. Our approach integrates two meticulously designed reward mechanisms within a tailored three-stage training paradigm.\n\u2022 We conduct a comparative analysis of ExpCTR against several state-of-the-art CTR prediction methods and evaluate the quality of the generated explanations, demonstrating the effectiveness of our method."}, {"title": "2 RELATED WORK", "content": "Explainable recommendation (ER) extends traditional recommendation systems by addressing the \"why\" behind suggested items. ER provides not only item recommendations but also justifications clarifying the rationale for those suggestions [45]. Current methods can be broadly classified into two categories, model-intrinsic and post-hoc. Model-intrinsic methods aim for inherent explainability by leveraging interpretable algorithms [44]. Conversely, post-hoc approaches leverage black-box models for recommendation, followed by a separate explanation model that deciphers the reasoning behind the recommendations. The rise of deep neural networks has propelled post-hoc methods to the forefront, transforming explainable recommendation into a natural language generation task. Early works rely on pre-defined templates or association rules [6, 35]. Later advancements adopt Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) architectures for generating textual explanations [19, 43]. With the advent of the Transformer architecture, researchers have explored their potential for explanation generation [18]. [40] incorporates reinforcement learning techniques to address potential issues like hallucinations. Despite these advancements, these approaches rely on generators trained independently with carefully curated explanation datasets. Given the scarcity of user-item-explanation triplets in real-world RS, substantial efforts have been dedicated to constructing high-quality explanation datasets. Techniques such as word overlap analysis [18], LSH-based near-duplicate detection [17] and a combination of manual and automatic reformulation on dialogue datasets [8] have been employed to this end.\nRecently, the burgeoning field of LLMs has spurred research on LLM-based explainable recommendation, which still predominantly employs post-hoc approaches. For instance, [7] generates explanations in a zero-shot manner within a conversational scenario. [24] probes the innate reasoning capabilities of LLMs for recommendation tasks using in-context learning techniques. However, these approaches heavily rely on LLM's intrinsic reasoning capabilities, with the recommender system remaining unaware of the generated explanations, let alone assessing their quality. This raises concerns about their effectiveness and the accuracy of the produced justifications in reflecting the true reasoning behind recommendations. This paper aims to address these limitations by proposing a novel approach that ensures coherent and reliable explanations directly integrated within the recommendation process."}, {"title": "3 PRELIMINARY", "content": ""}, {"title": "3.1 Problem Definition", "content": "Let U = {U1, U2, ..., un} denote a set of n users and I = {i1, i2, ..., im} a set of m items. The user-item interaction data D is represented by a binary interaction matrix R\u2208 {0,1}n\u00d7m, where Rui indicates whether user u has interacted with item i. A value of 1 signifies explicit feedback (e.g., watching videos, clicking) and 0 otherwise. Each interaction is associated with a textual review eu,i. The objective of explainable recommendation is to jointly predict future user interactions and generate explanations for these predictions. We formulate this as a probabilistic model:\nP(Z, \u0177 D)\nwhere Z represents the set of explanations for all user-item pairs and \u0177 denotes the predicted interaction scores."}, {"title": "3.2 Theoretical Basis of ExpCTR", "content": "To generate post-hoc explanations, we decompose the joint probability as follows:\nP(Z, \u0177 D) = P(\u0177 D) P(Z|\u0177, D).\nWe first train a CTR model f: UXI \u2192 R. This model learns latent representations hi,j from user-item interaction and side information (e.g., user demographics, item features). The optimization process is formulated as:\nmin\u2211 LCTR(\u0177, y),\n(u,i) e D\nwhere y denotes the ground truth interactions for user-item pairs. We define a generator g: U \u00d7 I \u00d7 R \u2192 V that explains why user u might interact positively or negatively with item i. This model generates explanations conditioned on the predicted result \u0177. The generator is optimized as:\nmin \u03a3\u03a3 - log p(tk|t<k, \u0177),\n(u,i)\u2208D k=1\nwhere \u0113u,i denotes the processed customer reviews used as explanation samples [2, 17, 18]. Post-hoc methodologies exhibit a critical dependency on curated training datasets, which fundamentally shapes the conditional distribution P(Z|\u0177, D). Another limitation lies in that the generated explanations exert no influence on the CTR model, thereby failing to guarantee that the explanations faithfully reflect the underlying mechanisms of the CTR model. To address this, we integrate CTR prediction and explanation generation within a unified framework, leveraging LLMs, which can be mathematically expressed as:\nP(Z, \u0177 D) = P(Z|D) \u00b7 P(\u0177|Z, D) .\nWe employ a LLM to generate explanations, circumventing the need for constructing a high-quality explanation dataset a laborious and costly task. The CTR model, P(\u0177 | Z, D), depends on the generated explanations Z, thus establishing a direct link between the explanations and their impact on CTR predictions. This approach represents a significant departure from traditional post-hoc methods, as the generated explanations are not simply after-the-fact rationalizations but integral components of the recommendation decision-making process.\nConcretely, we adapt the CTR model to incorporate the generated explanations as features, denoted by \u0192 : U \u00d7 I \u00d7 V \u2192 R. The prediction is then computed as follows:\n\u0177 = f(R, Z|OCTR)."}, {"title": "4 METHODOLOGY", "content": "Figure 2 depicts the overall architecture of ExpCTR. It consists of three primary components: (1) Explanation Generation leverages a LLM to produce textual explanations for recommendations. (2) Reward Design utilizes CTR prediction processes to provide quality assessments for the generated explanations that serve as reward signals. (3) Training Paradigm introduces Lora lightweight fine-tuning techniques along with an iterative training process."}, {"title": "4.1 Explanation Generation", "content": "Traditional recommendation systems rely on implicit representations of users and items and suffer from a lack of interpretability [5]. However, recent advancements in LLMs have demonstrated extensive world knowledge and advanced reasoning capabilities [23, 27, 38]. These capabilities offer a promising avenue for human-interpretable explanation generation. To harness this potential, we design a prompt template to guide the LLM to generate effective explanations. The prompt leverages user historical interaction data and frames the LLM as a helpful recommendation assistant:"}, {"title": "4.2 Reward Design", "content": "We optimize the LLM through quality assessment of the generated explanations. Inspired by InstructGPT [26], we leverage a reinforcement learning paradigm to achieve this objective with a well-designed reward function that incentivizes the LLM to generate informative explanations for CTR prediction and accurately represent the underlying user motivations behind their interactions."}, {"title": "4.2.1 Proximal Policy Optimization", "content": "Following [26], we adopt the proximal policy optimization (PPO) [30] algorithm for the reinforcement learning process. Given a prompt and response (explanation), the LLM produces a reward determined by a reward function, concluding the episode. The objective function for PPO training is formulated as:\nobjective =E(x,Z)~DRL R(Z) - \u1e9elog \u03c0\u03c6(\u0396/x) init (Z|x)"}, {"title": "4.2.2 Explanation and LLM-CTR Alignment (LC Alignment)", "content": "This component evaluates the effectiveness of the LLM's generated explanation towards accurately inferring the intended user behavior. A high LC alignment reward signifies that the explanation successfully conveys the underlying factors influencing user interaction. We operationalize LC alignment reward by leveraging recent advancements in CTR prediction with LLMs. We frame the task as a binary classification problem, where the LLM predicts whether a user will like a given item (e.g., book) based on their rationales (the generated explanation by LLM in Section 4.1).\nSpecifically, we design a prompt template to guide the LLM towards predicting CTR. This template provides context for the LLM, including the user's thoughts about the item and a binary response option (\"Yes\" or \"No\") indicating their decision:"}, {"title": "4.2.3 Explanation and ID-CTR Alignment (IC Alignment)", "content": "The congruence between generated explanations and the CTR model is quantitatively assessed by evaluating their contribution to CTR predictions. A positive reward value potentially signifies that the explanation provides substantial insights into the underlying patterns driving CTR. To rigorously evaluate this alignment, we integrate the generated explanations directly into the existing CTR prediction architecture. This integration serves a dual purpose: evaluating explanatory quality and potentially enhancing predictive accuracy by leveraging latent information within the explanations.\nOur approach first obtains a dense textual representation for each generated explanation Zu,i by employing a pre-trained language model (PLM), fencoder : V \u2192 Rd, to map the explanation text into a unified semantic space. This enables the capture of underlying meaning and relationships within the explanation. fencoder can be any frozen pre-trained language model, such as BERT [4], BGE [39] and we derive the dense representation for the explanation as follows:\nZu,i = MeanPooling (fencoder(Zu,i)),\nwhere zu,i denotes the mean pooling hidden representations from the last layer in PLM. Subsequently, we integrate these textual representations with an original ID-based CTR model architecture. This integration facilitates the learning of a joint representation that combines user and item information with the insights provided by the explanation. We propose a simple yet effective concatenation operation to achieve this integration. Specifically, the textual representation Zu,i is concatenated with the hidden representation hu,i (defined in Section 3.2) and fed into the existing CTR model to predict the CTR score as follows:\nsui = f(Concate(hu,i, zu,i)),\nwhere f (as in Equation 6) can be any original ID-based model architecture, such as DeepFM [22]. To evaluate the impact of semantic representations of LLM's explanations, we compare the performance of the CTR model with and without these explanations and quantify the differences in CTR predictions. A notable performance improvement when explanations are incorporated indicates that the introduced semantic features contribute positively. This implies a better-aligned explanation, justifying a higher reward. This evaluation is formalized as follows:\nRIC(Zu,i) = 1 \u2212 |Yu,i \u2013 su,i| + |su,i \u2013 \u0161u,il,\nwhere si indicates the CTR prediction score obtained without using explanations as input features, by setting the representations Zu,i to zero vectors. The IC alignment reward is normalized and clipped, as in Equation 10, resulting in Rhorm (Zu,i).\nBy synergizing the effects of these two reward components during the LLM training phase, we aim to ensure that the generated explanations not only accurately capture user rationales behind their behavior but also contribute meaningfully to the performance of CTR models. This approach promotes explanations that are both faithful and informative, ultimately leading to a more robust and interpretable recommendation system."}, {"title": "4.3 Training Paradigm", "content": ""}, {"title": "4.3.1 Light Weight Tuning", "content": "To mitigate the computational training burden associated with three independent LLMs - the initial LLM rinit, explanation generator and the LC alignment reward model, we adopt a lightweight tuning approach. Recent findings [11, 12, 20] suggest that LLMs can be effectively compressed without significant performance degradation, owing to the inherently lower-dimensional nature of the information they encode. Leveraging this insight, we employ Low-Rank Adapters (Lora) [12] to optimize our training process which introduces trainable low-rank matrices into each transformer layer, allowing for efficient parameterization while preserving model performance. Specifically, we employ a base LLM as both the initial model and the frozen LC alignment reward model. The explanation generator is instantiated as the base LLM with Lora. Crucially, this strategy drastically reduces the parameters. By consolidating computations into a single LLM with a minimal number of trainable parameters in Lora, we achieve substantial computational efficiency without compromising model quality."}, {"title": "4.3.2 Iterative Training", "content": "This section outlines the iterative training methodology employed to optimize the explanation generation model, RL, considering both LC alignment and IC alignment re- \u03c0\u03bf wards. Our approach involves a three-stage iterative training process, alternating between component-specific optimization phases. Stage 1. LC Alignment. We commence by utilizing a frozen language model rinit to compute LC rewards. The explanation generation model RL is then optimized using the LC alignment reward:\nR(Z) = Rorm (Zui).\nThis phase establishes a foundational understanding of \"correct\" explanations, aligning the model with user preferences and intentions, and producing factually sound explanations. This stage persists for a predetermined number of iterations, during which we continuously accumulate fresh explanations for each user-item pair.\nStage 2. CTR Model Training with Textual Features. Following Stage 1, we accumulate a corpus of generated explanations. These explanations are integrated as textual features with the original ID dataset for training the CTR model f:\nmin\u2211 LCTR(Sui Yu,i).\n(u,i, Zu,i) \u2208 {D,Z}\nStage 3. IC Alignment. In the final stage, we further refine the explanation generation model by incorporating the IC alignment reward:\nR(Z) = Rorm (Zui).\nStages 2 and 3 are then repeated for a predefined number of iterations, allowing for continuous model refinement and performance improvement. Upon the completion of the training process, we obtain a robustly trained explanation generation model RL and a CTR model f that effectively leverages textual features for prediction. This unified training approach prioritizes that generated explanations are informative and likely to resonate with both users and the CTR model, avoiding the pitfall of producing generic or uninformative content."}, {"title": "5 EXPERIMENT", "content": "In this section, we detail the experimental setup to evaluate the performance of ExpCTR. We aim to address the following research questions through a series of rigorous experiments and analyses:\n\u2022 RQ1: How does ExpCTR compare to existing state-of-the-art approaches in terms of generating explanations for recommendation decisions and improving CTR prediction?\n\u2022 RQ2: How effective is the integration of the PPO algorithm in ExpCTR?\n\u2022 RQ3: How does the quality of the explanations produced by our framework measure up?"}, {"title": "5.1 Experimental Setting", "content": ""}, {"title": "5.1.1 Datasets", "content": "To comprehensively evaluate the effectiveness and generalizability of our proposed framework, we leverage three publicly available, large-scale datasets: BookCrossing 1, MovieLens-20M 2, and Amazon Books 3. Following [1], we employ a stratified random sampling approach for each user within each dataset. Specifically, we randomly select one item a user interacted with as the target item for prediction. The remaining interacted items, up to a maximum of 10 items chronologically preceding the target item, are considered the user's historical interactions. Then, we partition the constructed data samples into training, validation, and testing sets with a ratio of 8:1:1. For datasets containing rating scores, we binarize the ratings using a threshold where ratings above the threshold are considered positive interactions (items the user liked), while ratings below are considered negative interactions. Specifically, the threshold for the ML-20M and Amazon Books datasets is 4, and 5 for the BookCrossing dataset [31, 48]. Finally, Amazon Books and ML-20M comprise 16,000/2,000/2,000 while BookCrossing comprises 32,000/4,000/4,000 data samples."}, {"title": "5.1.2 Compared Methods", "content": "For CTR evaluations of ExpCTR, we leverage two distinct scoring mechanisms: LLM scores derived from the LC alignment module, designated as \"ExpCTR-LLM\", and CTR scores with explanations as textual features from the IC alignment module, referred to as \"ExpCTR-Aug\". ExpCTR-LLM reflects the effectiveness of the generated explanations in capturing and articulating user preferences and rationales for future interactions, which results in better outcomes under an LLM scorer. Conversely, a superior ExpCTR-Aug score suggests that the explanation aligns well with the internal workings of ID-based CTR models and provides supplementary information that enhances performance. This dual evaluation approach provides an indirect yet effective method for assessing explanation quality. We compare ExpCTR against diverse established baseline models, encompassing both ID-based and LLM-based recommendation methods:\n\u2022 ID-based methods: Factorization Machines (FM) [29] captures pairwise feature interactions for recommendation tasks. Deep learning models, including DSSM [13], DeepFM [22], AutoInt [31], PNN [28], Fi-GNN [21], DCN [36], DCNV2 [37], utilize multi-layer perceptrons, self-attention mechanisms, and graph neural networks to effectively capture both low-order and high-order feature interactions to enhancing recommendation accuracy. DIN [49] and DIEN [48] leverage attention mechanisms to extract user dynamic interests from their historical behavior sequences. Caser [33], GRU4Rec [10], SASRec [15] and BERT4Rec [32] are sequential-based recommendation models that employ Convolutional Neural Networks (CNNs), Gated Recurrent Units (GRUs), and transformer-encoder architectures for robust user behavior modeling, respectively, leading to more accurate recommendations.\n\u2022 LLM-based methods: In-Context Learning (ICL) for Recommendation [3] leverages an LLM for recommendations by directly posing queries to the LLM. TALLRec [1] adapts LLMs to recommendation scenarios through instruction tuning."}, {"title": "5.1.3 Metrics", "content": "To assess the effectiveness of ExpCTR, we utilize multiple regular CTR prediction metrics [22, 49]. Specifically, we evaluate performance using the Area Under the ROC Curve (AUC), binary cross-entropy loss (Log Loss), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE)."}, {"title": "5.1.4 Implementation Details", "content": "In our experimental setup, we employ LLaMA-3-7b as the foundational model for both explanation generation and LC alignment reward computation. For explanations encoding, we employ BGE-small [39]. The IC alignment reward is built upon the DeepFM and implemented through the open-source project Recbole [46]. Our optimization incorporates a learning rate of 1 \u00d7 10-5, with a KL penalty of 0.05. The reward clip threshold 8 is set to 1.0. The iterative training paradigm consists of two epochs per iteration. For TALLRec, we leverage the entire training dataset and apply a learning rate of 1\u00d710\u22124. All experiments are conducted on a single machine equipped with NVIDIA A800 GPUs."}, {"title": "5.2 Performance Comparison (RQ1)", "content": "Table 1 presents a comparative analysis of our proposed method with existing ID-based CTR methods and LLM-based methods. The results yield several noteworthy observations:\n\u2022 Baseline models such as ICL and TALLRec demonstrate strong performance across all datasets, particularly when compared to ID-based methods. This suggests that the LLMs possess a robust foundational capability for reasoning and comprehension. Nevertheless, ExpCTR-LLM consistently surpasses these two LLM-based CTR models on all metrics and datasets. This empirical evidence indicates that our generated explanations accurately reflect and describe user behavior patterns, leading to significant performance improvements over ICL, which uses the same frozen LLM scorer, and TALLRec, which is finetuned directly under the CTR prediction task. These findings highlight ExpCTR's capability to leverage the intrinsic reasoning capabilities of LLMs effectively. Additionally, the consistent performance gains underscore the efficacy of our proposed framework, with the integration of reinforcement learning and the LC alignment reward function further extending and motivating the potential of LLMs in recommendation scenarios.\n\u2022 ExpCTR-Aug emerges as a substantial advancement over ExpCTR-LLM, demonstrating superior performance across all evaluated datasets. This result highlights the pivotal role of the IC alignment reward in augmenting model efficacy. The explanations generated by ExpCTR-Aug offer profound insights into ID-based CTR models, leading to considerable performance improvements compared to the DeepFM baseline, with observed gains of 18.2%, 11.9%, and 17.8% in AUC across the respective datasets. These results underscore the dual advantages of ExpCTR that it not only enhances the interpretability of the recommendation system but also delivers substantial improvements in recommender system accuracy."}, {"title": "5.3 In-depth Analysis of PPO (RQ2)", "content": ""}, {"title": "5.3.1 PPO Reward Analysis", "content": "To investigate the efficacy and learning dynamics of the training paradigm in ExpCTR, we conduct a comprehensive analysis of the reward trajectories during the iterative training process. Specifically, we examined the evolution of LC alignment and IC alignment rewards on ML-20M and BookCrossing datasets. The results of this analysis are presented in Figure 3. Our analysis reveals a consistent upward trend in LC alignment rewards across both datasets, which stabilizes during the final stages of training and coincides with the performance enhancement of ExpCTR-LLM. Notably, the ML-20M dataset exhibits a more pronounced increase, ultimately reaching a higher plateau. This observation aligns with the superior performance obtained on the ML-20M dataset (9.1% improvement in AUC over ICL). These observations suggest that our LC alignment reward mechanism effectively steers LLMs towards generating explanations that are increasingly congruent with user behavior patterns and exhibit a strong correlation with subsequent user interactions. In contrast, the IC alignment stage is characterized by a more fluctuating curve across both datasets, with the alignment stabilizing more rapidly compared to LC alignment. This corresponds to the relatively modest improvement observed in ExpLLM-Aug over ExpCTR-LLM. The BookCrossing dataset experiences a slight decline followed by steady growth, reflecting the refinement process of IC alignment for LLM-based explanation generation. This empirical evidence underscores the effectiveness of our training paradigm in fostering the development of a more user-centric and contextually relevant recommendation system."}, {"title": "5.3.2 Hyperparameter Sensitivity Analysis", "content": "We assess the sensitivity of hyperparameters in PPO training, specifically focusing on the KL penalty \u1e9e and the reward normalization bound 8, both of which are crucial for effective PPO training [30]. Figure 4 shows the performance variations across different hyperparameter settings, with \u1e9e ranging from [0.01, 0.05, 0.1, 0.5] and 8 ranging from [0.5, 1.0, 2.0, 5.0], evaluated on the BookCrossing and Amazon Books datasets. For the KL penalty \u1e9e, its effect on ExpCTR's performance is notable across both datasets. Specifically, extreme values of \u1e9e-either too high or too low-detract from the model's capabilities, with a setting of 0.05 typically yielding the most competitive results. In contrast, the reward normalization bound 8 shows significant performance variability on the BookCrossing dataset, while remaining stable on the Amazon Books dataset and we choose 8 = 1.0 for both datasets."}, {"title": "5.4 Case Study (RQ3)", "content": "To elucidate the efficacy of ExpCTR in generating improved explanations, we present a comparative analysis of explanations produced by ICL and our proposed approach. Table 2 illustrates representative examples, accompanied by actual user reviews to provide real-world context for our analysis.\nIn the first case, we observe that the user's attitude towards the targeted book is fundamentally positive. The user's comment, \"The right book\", indicates approval, while the phrase \"class have already started\" represents an extraneous factor beyond the scope of the recommender system. This positive sentiment aligns with the high CTR prediction of 0.8843. ICL gives a negative attitude (\"neutral or indifferent opinion\"). However, ExpCTR successfully captures this alignment towards the CTR model, generating a recommendation explanation that accurately reflects the user's probable affinity for the book (\"similar to the themes of history and mythology in their preferred books\"). The second case demonstrates a more nuanced improvement. The ICL approach erroneously infers a negative attitude (\"likely dislike\"), contradicting the user's actual 5.0 rating. In contrast, ExpCTR, enhanced by LC alignment reward training, correctly identifies the positive interaction potential (\"likely enjoy\"). Furthermore, the explanation generated by our model corresponds closely to the user's actual thoughts, accurately identifying the book's themes of \"business, self-improvement, marketing\"."}, {"title": "6 CONCLUSION", "content": "In the paper, we present ExpCTR to address the limitations of current post-hoc explainable recommendation methods. By integrating LLM-based explanation generation into the CTR prediction process, ExpCTR eliminates the need for extensive data preparation and mitigates reliability issues. Our approach leverages reinforcement learning to align LLM reasoning with both user preferences and the recommender system's internal workings. We believe that ExpCTR represents a significant step forward in the field of explainable recommendations and opens up new avenues for future research."}]}