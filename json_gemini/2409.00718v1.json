{"title": "Multiscale Color Guided Attention Ensemble Classifier for Age-Related Macular Degeneration using Concurrent Fundus and Optical Coherence Tomography Images", "authors": ["Pragya Gupta", "Subhamoy Mandal", "Debashree Guha", "Debjani Chakraborty"], "abstract": "Automatic diagnosis techniques have evolved to identify age-related macular degeneration (AMD) by employing single modality Fundus images or optical coherence tomography (OCT). To classify ocular diseases, fundus and OCT images are the most crucial imaging modalities used in the clinical setting. Most deep learning-based techniques are established on a single imaging modality, which contemplates the ocular disorders to a specific extent and disregards other modality that comprises exhaustive information among distinct imaging modalities. This paper proposes a modality-specific multiscale color space embedding integrated with the attention mechanism based on transfer learning for classification (MCGAEc), which can efficiently extract the distinct modality information at various scales using the distinct color spaces. In this work, we first introduce the modality-specific multiscale color space encoder model, which includes diverse feature representations by integrating distinct characteristic color spaces on a multiscale into a unified framework. The extracted features from the prior encoder module are incorporated with the attention mechanism to extract the global features representation, which is integrated with the prior extracted features and transferred to the random forest classifier for the classification of AMD. To analyze the performance of the proposed MCGAEc method, a publicly available multi-modality dataset from Project Macula for AMD is utilized and compared with the existing models.", "sections": [{"title": "1 Introduction", "content": "Diagnosing retinal disorders plays a vital role in guiding treatment decisions and improving outcomes for individuals with retinal conditions. Age-related maculopathy is a degenerative condition of the central region of the retina that is correlated with the cause of visual impairment that is recurring after 65 years of age [3]. The diagnosis of AMD was first described in [30]. In the earlier phases of AMD, the patients have drusen and RPE abnormalities, whereas geographic atrophy and neovascularization of the retina may be interconnected with vision loss during the progression of the disorder. AMD can be characterized as dry or wet according to the pathogenesis. Choroidal neovascularization (CNV) [14] is a manifestation of wet AMD and is diagnosed by analyzing the uncharacteristic expansion of blood vessels from the choroid into the retina. DME [16] is a serious condition that can be attributed to hyperglycemia, which is a form of diabetic retinopathy (DR). It occurs due to prolonged exposure to high blood sugar, particularly in diabetic patients, which causes fluid leakage into the macula region, swelling, and thickening. Drusen [2] is a condition of dry AMD where tiny yellow or white deposits accumulate under the retina. If not characterized immediately, these ocular disorders can impair the retinal layer, especially the macular area, and perhaps end up with vision loss. Traditional diagnostic and grading systems for AMD are conducted by analyzing the color Fundus images [12]. Over the years, significant advancements have been made in medical diagnostics, offering new tools and techniques that enable precise and early detection of retinal disorders, especially the evolution of imaging techniques using Fundus images. OCT is one of the widely exploited diagnostic tools that will provide 3D structural information associated with the demonstration of cross-sectional images. OCT provides constructive information in investigating retinal disorders in challenging diagnostic cases and acquiring cross-sectional lesions of neovascularization associated with neighborhood tissue information. Clinical practitioners utilize OCT to examine the activity of AMD nowadays [41]. These techniques are effective; however, they suffer from intrinsic constraints, including specialized clinical practitioners and time-consuming, which induce variation in the ocular diagnosis, and hindered intervention can emerge. Recent advancements in deep learning techniques have presented enormous possibilities for automated diagnosis tasks of retinal disorders at the expert level, decreasing the dependency on human experts in diverse fields [44]. There is a rapidly growing interest in"}, {"title": "2 Proposed Model", "content": "This section introduces the proposed MCGAEc framework illustrated in Fig. 2. MCGAEC comprises a modality-specific multiscale color space encoder module in which a fundus image is transformed into distinct color spaces, say, YCbCr and HSV, which are transferred to two encoder paths. For OCT images, one encoder path named gray-scale is constituted. In each path, the transformed Fundus images are forwarded to multiscale space conversion, followed by input for the pre-trained VGG16 model to acquire vital features from each considered color space at various scales. The extracted features from each path from the Fundus images are transferred to the self-attention module. Simultaneously, we extract the features from the gray-scale path for the OCT images. Then, features extracted from the pre-trained VGG16 and self-attention module are concatenated. Finally, we fused all the extracted features from each path and fed them to the RFC for the classification of AMD. Modality-Specific Multiscale Color Space Encoder Model: The color variations of fundus images enclose exhaustive ranges, and heterogeneity in color casts restricts the classical models [6]. Motivated by traditional enhancement techniques that function over different color spaces [15,25,28], we extract distinct characteristic features from two color spaces (HSV, YCbCr) where the identical fundus image has distinct pictorial representation in diverse color spaces demonstrated in Fig. 3. The fundus image is explicitly to visualize in RGB color space because of its intense physical significance in color. However, the color segments R, G, and B are positively associated and are easy to be influenced by the variation of luminance, occlusion, and other factors. On the other hand, YCbCr color space can intuitively reminisce the luminance (Y) and two chroma components (Cb and Cr). YCbCr is crucial in digital images and video to separate luminance from its chrominance. This fragmentation is beneficial because the human eye is more susceptible to luminance than chrominance, and it entitles more efficiency in compression that aligns with the visual perception of humans. HSV color space characterizes the hue, saturation, contrast, and brightness of the Fundus image. The considered color space has diverse characteristics and benefits. To integrate their properties in the fundus image feature enhancement, we assimilate the characteristics of distinct color spaces into a unified deep characteristic model. Furthermore, the color variations of two considered points with a diminutive variation in one color space can be enormous in another color space. Thus, the distinct color space integration can facilitate the measure of the color divergence of fundus images. To extract the necessary features from distinct color spaces, which is required for the classification of AMD, multi-scaling is incorporated. When the scale is augmented, most of the noise is eliminated. If the features are available in more than one coarse scale, which indicates it should be available at different scales as well [36]. Therefore, this strategy is implemented over each color space by considering different scales for fundus and OCT images, respectively. Afterward, we executed the pre-trained VGG16 [35], which includes 16 deep layers for extracting the features from considered multiscale color space for fundus and OCT images. VGG16 is an adequate model for the image classification task, and the pre-trained model enables to extraction of features from an extensive corpus of images when utilizing a small dataset. VGG16 model provides the balance between the performance and computational efficiency compared to other complex models. Additionally, it allows the experiments without requiring comprehensive computational resources, which provides an advancement to integrate the attention layer with that. A transfer learning mechanism is utilized, and extracted features from each are fed to the distinct attention module.\nSelf-Attention: The self-attention mechanism was first introduced in the domain of image processing [33], which is integrated into the attention layer. It enables to concentrate on salient or global features of the datasets. It provides an adequate correlation towards global feature information within each single image. The primary notion behind the self-attention mechanism is to associate weighted average values evaluated from the prior layers, and the attention weights are assessed as follows:\n$Attenttion(Q, K,V) = softmax (\\frac{QKT}{\\sqrt dhead})V$ (1)\nwhere Qis a query, K is the key, and V is the value. In the assertion of the precise characterization of each multiscale color space feature, the extracted features from the pre-trained model through each path have dissimilar con-tributions. Consequently, we utilize a self-attention mechanism to exploit the inter-variability between the extracted features obtained through each path. In the proposed framework, the extracted features from each path through the pre-trained VGG16 model are used as input for the self-attention module. The extracted features through the pre-trained VGG16 model are confined by structure to concentrate primarily on local features of fundus and OCT images while incompetent to acquire the global feature information. Self-attention focuses on specific global features of the images. Therefore, the extracted features from the pre-trained deep network model are fed into the self-attention model corresponding to each path. The attention features and extracted prior features from the pre-trained model from each path are fused together by concatenation for a better representation of each feature acquired through the Fundus and OCT images, which are used as input for the classification model.\nClassification Model: The computed features through the modality-specific multiscale color space encoder model with self-attention are transferred into the supervised machine learning RFC [19]. The RFC comprises an amalgamation of tree classifiers where a particular classifier is acquired through a random vector, which is sampled individually from the input data, and the respective tree provides a unit vote for the most prevalent class to categorize input data. The RFC is utilized for the classification of the input data, which relies on the utilization of randomly chosen features at each node to expand a tree. In RFC, features are illustrated on the inner nodes, which are called decision nodes, and enable to generate the predictions from a sequence of feature-based fragmentation. RFC employs a collection of random decision trees and integrates them concurrently to construct a robust model that is less acute to the training data. The RFC algorithm is capable of handling higher dimensional data and utilizing an enormous number of trees in the combination. The output of the RFC is computed by a majority counting of votes obtained through trees. Here, RFC is used, which includes considerably lower computational complexity as each particular tree exclusively utilizes a part of the input vector in a Random Forest."}, {"title": "3 Experimental Framework", "content": "In this section, the description of the multi-modality OCT and fundus image datasets for diagnosis of AMD is provided, followed by evaluation measures to check the performance of the proposed MCGAEc model. Finally, a comprehensive empirical study, including the ablation analysis, is given to show the significance of the proposed MCGAEc model for the classification using the multi-modality dataset."}, {"title": "3.1 Dataset and Evaluation Measures", "content": "The experimental analysis of the MCGAEc model is assessed over the publicly open multi-modality OCT and fundus images dataset at the Project Macula [32] for AMD classification (https://projectmacula.cs.uab.edu). The publicly available dataset aimed to investigate AMD in patients and their severity level. The diagnosis of AMD over the provided dataset is endorsed by pathohisto-logical examination. The dataset is categorized into three classes: normal, non-neovascular, and neovascular. The normal class includes 50 OCT and Fundus images each, followed by 19 and 40 in non-neovascular and neovascular, respectively.\nThe spatial resolution of Fundus and OCT images in each class is of different variation, and data is not enough to train the model. Therefore, data augmentation is applied with rotations, translations, and contrast changes for increments in the number of images. Data augmentation [34] is extensively applied to enhance the generalization of the proposed method. We erratically retrieved 500 fundus images for each class and 500 OCT images for each class that matched with the Fundus images. We have performed the rotation in a range [-25\u00b0, +25\u00b0], with translation [-10%, +10%] of the width of the image, and contrast change with ranges of [-50%, +50%]. All the generated OCT and fundus images are resized 224\u00d7224 for the input of the pre-trained model. The experiments are performed in the selection of an optimal number of trees from the set {100, 300, 500, 700, 1000} and estimators from the range 3-25 for the RFC classifier. The optimal number of trees for the proposed framework for the RFC is to obtain 1000 trees and 10 estimators for each node for the AMD dataset. The five-fold cross-validation is employed for multi-classification, which is illustrated in section 3.2\nTo examine the performance of the MCGAEc method on the above-considered dataset, commonly used performance evaluation measures, including AUC (area under the receiver operating characteristic), Accuracy, Sensitivity, Specificity, F1, and Matthews Correlation Coefficient (MCC) score is considered, which are described as follows:\n$Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}$ (2)\n$Sensitivity = \\frac{TP}{TP + FN}$ (2)\n$Specificity = \\frac{TN}{TN + FP}$ (3)\n$F\u2081 = \\frac{2TP}{2TP + FP+ FN}$ (3)\n$MCC = \\frac{TNTP - FN \u00d7 FP}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$ (4)\nWhere TP denotes true positive counts, FN denotes false negative counts, TN denotes true negative counts, and FP denotes false positive counts. For all the evaluation measures, higher values indicate better classification performance."}, {"title": "3.2 Effectiveness of the Proposed MCGAEC Model", "content": "We first illustrate the comparison between the proposed MCGAEc model over the single modality by considering different color spaces with various scales. We have performed 5-fold cross-validation, and the experimental results are reported in Table 1. When considering a single modality fundus image, we have performed the experiments on YCbCr and HSV color spaces by taking the lower and higher regularization levels to capture the features at different scales, as shown in Table 1. For OCT images, we have considered different scales for comparison with the proposed framework and other considered cases. The pre-trained VGG16 model is the backbone for the feature extraction from single modality and multi-modality with respect to different multi-scale color spaces.\nFig. 4 demonstrates ROC curves for each considered case over the proposed framework for the AMD dataset. It can be observed from the ROC curve the proposed modality-specific multiscale color space embedding based on attention mechanism (MCGAEc model) for AMD classification is more adequate compared to other single modalities based on single color spaces. The proposed method achieves a higher AUC value of 0.994 compared to other single modalities with single color spaces, as demonstrated in Fig. 4. Table 1 signifies that the proposed MCGAEc model can improve the capability of AMD classification compared to the utilization of a single modality model. In the single modality, the fundus image, when transformed to YCbCr color space with regularization level o = 4, has achieved a higher AUC of 0.990 compared to other single modality cases. Further, it achieves a higher F1-score of 0.930 among the other single modality for classification. However, when the fundus image is transformed into HSV color space with multiscale, it achieves the lowest F1 score compared with others. The proposed model has higher Accuracy, Sensitivity, Specificity, F1, and MCC score compared to others demonstrated in Table 1. It indicates that when different color spaces are considered in multiscale and incorporated with the multi-modality Fundus and OCT images, the performance of the classifier is enhanced significantly, and it enables to capture of essential local and global discriminative features at different scales using attention mechanism."}, {"title": "3.3 Quantitative Comparison with Existing Approaches", "content": "In this section, we have compared the proposed model with the existing method over the considered AMD dataset, and the empirical study is demonstrated in Table 2-3. For comparison purposes with the AMD dataset, the implementation of the SOTA methods is conducted. The training results over the AMD dataset of the proposed framework are presented in Table 2 and compared with the Yoo et al. [43], which indicates that the proposed model is competent to extract the most discriminative features, which is essential for the classification of AMD in comparison to [43]. The proposed MCGAEc model enables to distinguish between false positive and true positive at distinct threshold levels indicated by the higher AUC value acquired by the proposed framework. Table 3 rep-resents the test result over the multi-modality fundus and OCT image dataset and is compared with the SOTA methods. The proposed model attained a higher AUC of 0.994 compared to other methods, which indicates the consistency of the proposed classification framework at different threshold levels except [11], and it can be observed from the ROC curve illustrated in Fig. 4. The presented method [11] acquires lower Specificity compared to our proposed method, which indicates the diagnosis of AMD in the false positive category. MCGAEc model performance is adequate in terms of quantitative comparison for the considered AMD dataset. The empirical analysis of the proposed MCGAEc model is adequate compared to the SOTA approaches illustrated in Table 3 over the test set."}, {"title": "3.4 Discussion", "content": "This is the first experimental study, best to the knowledge of the authors to consider a multiscale color space for fusing distinct imaging modalities for the classification of AMD disorder. In this study, we proposed an MCGAEc model that considers the fundus and OCT images at multiscale color spaces simultaneously for the diagnosis of AMD. Fundus imaging modality characterizes information on the region of the drusen (AMD). OCT provides subsurface cross-section imaging, providing information about the different layers of the retina. Such information is complimentary to the structural information of the vasculature on the surface as obtained from fundus images. OCT imaging modality is correlated with the subsurface of retinal layers and intra-retinal fluid lesions. The thickness of retinal layers, which is influenced by choroidal neovasculariza-tion, is investigated through the OCT, and the fundus image is competent to apprehend the evolution in the size of the drusen. However, the fundus image is insufficient in identifying choroidal neovascularization rigorously [26]. Whereas OCT is not able to identify the transitions in the drusen and retinal pigment epithelium [5]. The fundus and OCT imaging modalities provide complementary information on the retina. Early stages-based techniques utilized fundus or OCT for diagnosis of AMD based on deep learning models [8,13,18,21, 22]. This work focuses on investigating the crucial feature information extracted from Fundus and OCT images, which are integrated with the proposed MCGAEc method to capture the surface and subsurface retinal information for the diagnosis of AMD and glaucoma. We have assimilated multiscale color space to diagnose complex retinal diseases, which helps to capture dissimilar features from OCT and Fun-dus simultaneously at distinct color spaces with multiple scales. In the proposed approach, we attempt to combine distinct feature properties preserved in the various color spaces and if it is captured at a finer scale so there is a possibility of the presence of that particular feature at multiple scales. Therefore, we have considered YCbCr and HSV color spaces at different scales and integrated them with the pre-trained VGG16 model to extract the crucial feature for diagnosis purposes. On the other hand, OCT images are considered on different scales to capture the subsurface retinal layer information for the diagnosis of choroidal neovascularization. The attention mechanism is incorporated to extract global feature representation and integrated with the local feature information, followed by the ensembling of each classifier at the feature fusion module, and a random forest classifier is utilized for the classification of various stages of AMD. To show the significance of the modality-specific multiscale color space embedding, the experiments are also performed over a single modality based on different scales of color spaces, and results are illustrated in Table 1. It can be observed from Ta-ble 1 that when the fundus image is utilized and transformed into YCbCr color space at multiscale, it achieves 0.910 Accuracy for the AMD dataset compared to other single modalities with one color space transformation. However, the exper-imental performance is similar for single modality fundus images when distinct color spaces are fused together. On the other hand, when modality-specific mul-tiscale color space embedding strategy is assessed, then the performance of the MCGAEc model is elevated and achieved 0.947 Accuracy for the AMD dataset. Further, we have performed a comparative study demonstrated in Table 2 and 3, which shows the efficacy of the proposed MCGAEc method over the SOTA methods. The proposed study can be integrated into the clinical setting to help ophthalmologists with the diagnosis of the retinal disorder and, based on the observation, can predict the retinal disease."}, {"title": "4 Conclusion", "content": "We have presented a multi-modality MCGAEc deep learning model that assimi-lates the feature representations in various color spaces and emphasizes the vital discriminative features by multiscale mechanism. Besides, the global feature rep-resentation is incorporated into the proposed model by employing the attention mechanism at each path of the ensemble classifier. To analyze the behavior of the proposed model, extensive experiments were accomplished over the publicly available multi-modality AMD dataset and compared with the existing approach, which indicates the effectiveness of the proposed model. Additionally, the signif-icance of the proposed method has been verified by performing experiments on a single modality with distinct color spaces, and the proposed MCGAEc model achieves higher evaluation measures compared to the considered cases. Moreover, we successfully incorporated ROI-specific mechanisms to learn essential features from the multi-modality imaging techniques used to diagnose retinal disorders and localize the affected region."}, {"title": "Declarations", "content": "Conflict of interest The authors declare that they have no conflict of interests."}]}