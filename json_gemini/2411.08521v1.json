{"title": "SAD-TIME: a Spatiotemporal-fused network for depression detection with Automated multi-scale Depth-wise and TIME-interval-related common feature extractor", "authors": ["Han-Guang Wang", "Hui-Rang Hou", "Li-Cheng Jin", "Chen-Yang Xu", "Zhong-Yi Zhang", "Qing-Hao Meng"], "abstract": "Depression is a severe mental disorder, and accurate diagnosis is pivotal to the cure and rehabilitation of people with depression. However, the current questionnaire-based diagnostic methods could bring subjective biases and may be denied by subjects. In search of a more objective means of diagnosis, researchers have begun to experiment with deep learning-based methods for identifying depressive disorders in recent years. In this study, a novel Spatiotemporal-fused network with Automated multi-scale Depth-wise and TIME-interval-related common feature extractor (SAD-TIME) is proposed. SAD-TIME incorporates an automated nodes' common features extractor (CFE), a spatial sector (SpS), a modified temporal sector (TeS), and a domain adversarial learner (DAL). The CFE includes a multi-scale depth-wise 1D-convolutional neural network and a time-interval embedding generator, where the unique information of each channel is preserved. The SpS fuses the functional connectivity with the distance-based connectivity containing spatial position of EEG electrodes. A multi-head-attention graph convolutional network is also applied in the SpS to fuse the features from different EEG channels. The TeS is based on long short-term memory and graph transformer networks, where the temporal information of different time-windows is fused. Moreover, the DAL is used after the SpS to obtain the domain-invariant feature. Experimental results performed on two datasets under tenfold cross-validation show that the proposed SAD-TIME method achieves remarkable results in cross-subject mode.", "sections": [{"title": "1. Introduction", "content": "Depression is a common but severe mental disorder, which is characterized by persistent sadness and a lack of interest or pleasure in previously rewarding or enjoyable activities, according to the World Health Organization. In 2019, 280 million people were living with depression, including 23 million children and adolescents [1]. As a consequence, the diagnosis of depression is indispensable. Several questionnaire-based diagnostic methods have been used, including Beck depression inventor (BDI) [2], the patient health questionnaire [3], diagnostic and statistical manual of mental disorders [4] and the Hamilton depression rating scale [5]. However, questionnaire-based diagnoses will bring subjective biases and may be denied by subjects [6]. Therefore, the alternate, objective and accurate approaches are pivotal.\nElectroencephalogram (EEG) data, as one production of sensor-technology development, have been widely used in the depression-detection field. It can be combined with machine learning or deep learning methods, which is considered to be an appropriate alternate approach to fulfill the depression diagnosis task [7].\nMultiple manual features, obtained from prior knowledge, are required when integrating EEG data with traditional machine learning algorithms, such as support vector machine (SVM), K-nearest neighbor (KNN), decision tree, and radial basis function network. Mohammadi et al. [8] selected features from fuzzy entropy, Katz fractal dimension, and fuzzy fractal dimension using a Gustafson-Kessel clustering fuzzy algorithm and combined the selected features with a multilayer perceptron neural network to distinguish depressed subjects and healthy control group. Zhao et al. [9] used four EEG microstates with features including corresponding temporal parameters (i.e., duration, occurrence, and coverage) and complexity (i.e., sample entropy and Lempel-Ziv complexity) and incorporated these features with an SVM classifier to distinguish depressed, first-episode, drug-na\u00efve adolescent from non-depressed controls. Cai et al. [10] utilized the combination of linear and nonlinear features extracted from the EEG signal under different emotional modalities as manual features to assist with the detection of depressive subjects. However, firstly, the manual features depend more on prior knowledge. Secondly, the spatial and temporal features are not further extracted when utilizing the traditional machine learning methods just with the time-domain, frequency-domain, or time-frequency-domain features, considering the multiple channels and the sequence-signal attributes of EEG data.\nApplying the spatial information of the electrodes or the brain connectivity (including functional and effective"}, {"title": "2. Methodologies", "content": "To automatically obtain common features without using prior knowledge, while exploiting spatial information and functional connectivity, and to soften the rigid connection from SpS (Spatial Sector) to TeS (Temporal Sector), we propose SAD-TIME, a Spatiotemporal-fused network for depression detection with Automated multi-scale Depth-wise and TIME-interval-related common feature extractor, as outlined in Fig. 1. SAD-TIME consists of four parts, i.e., (1) CFE (Common Feature Extractor), which processes T time windows obtained from original EEG data of one subject and can be further divided into a DwCS and a TiS. (2) SpS, where the functional connectivity matrix is obtained and then fused with the distance-based connectivity matrix to conduct graph convolution. (3) TeS, where the multiple time windows are fused together by using a layernorm-LSTM and a modified GTN (Graph Transformer Network), and the class prediction is conducted. (4) DAL, where GRL (gradient reversal layer) [26] is used, and the domain classification result is obtained.\nThe data input of this framework is $X \\in R^{B \\times T \\times V \\times len}$, where B is the batch size, T indicates the number of time windows divided from the original EEG data, V represents the number of EEG channels that correspond to EEG electrodes, and len denotes the data length of one time-window after division."}, {"title": "2.1 Common Feature Extractor (CFE)", "content": "The DwCS of CFE is applied to each time window, considering that it does not destroy the underlying relation among the multiple channels of X and explores the hidden information of each EEG channel, rather than using a normal 1D-CNN or 2D-CNN with the EEG channels as the input channels. The depth-wise 1D-CNN kernels on a single scale are summarized as $CK \\in R^{(V*kd) \\times 1 \\times ks}$, where the number of input channels is V, and ks and kd indicate the size of the kernel and the output dimension of the kernel, respectively. Multiple 1D-CNN layers are used, and in the first layer of the depth-wise convolutional network, for each time window $X^{(t)} \\in R^{B \\times V \\times len}$ (t = 1, 2, ..., T), a specific 1D-CNN kernel with the channel dimension of kd is used at one EEG channel to control the channel dimension of output, which is similar to using multiple kernels in 2D-CNN to control the channel dimension of the output. However, when using the depth-wise method, the results are different from the normal 1D-CNN or 2D-CNN because each channel is processed separately and specifically. For the kd-dimension kernels of each input channel, the results are not added together but concatenated instead. At one EEG channel, the feature dimension is l'*kd, where l' is the length of the signal following convolution with a kernel of size ks. Furthermore, a multi-scale method is adopted, which means 1D-CNN kernels with different ks are used for each layer. In this study, three-types 1D-CNN kernels are exploited and the detailed information of different depth-wise convolutional layers is shown in Table 1. The final feature from this sector is represented as $f_{depth} \\in R^{B \\times T \\times v \\times (kd*l)}$, where $f l_{i}(i=1,2,3)$ represents the feature length after the processing of three different-scales kernels.\nApart from the DwCS, TiS is used to automatedly generate another part of common features by using the embedding of the time interval between two time-windows, where the time intervals work as one kind of time patterns containing the temporal information along different time-windows. As shown in Fig.2, the time interval is derived by combing the time-slice cutting at the end of the former time-window and the cutting at the start of the latter time-window. For example, time-inteval, is obtained by concatenating End\u2081 and Start2. The process of generating time-intervals is similar to using an overlapping area between two time-windows. T-1 time-intervals Ti can be obtained from X ($T_i \\in R^{B \\times (T-1) \\times V \\times (2*ts)}$, where ts is the time-slice length of one start/end slice). However, the start cutting of the first time-window and the end of the T-th time-window have no pair. As a consequence, the embedding of these two special slices is processed separately.\nThe process of TiS is summarized in Fig. 2, where TiS is further divided into two fractions. The first fraction generates the embeddings of the separate first start and last end slices by using two different linear layers, whose outputs have twice"}, {"title": "2.2 The Spatial Sector (SpS) of The Spatiotemporal Network", "content": "In the SpS, the graph theory is utilized, where a notion of the node is introduced, and each node corresponds to an EEG channel. When applying the graph-theory network on the $f_{common}$, the feature of each node $f_i \\in R^{B \\times T \\times FE}$ (i = 1, 2, ..., V) on multiple time-windows is obtained from it. Furthermore, the statistical relationships between nodes establish a connectivity map, which is also the adjacency matrix $A \\in R^{B \\times T \\times V \\times V}$.\nIn the proposed model, a distance-based connectivity map $A_{Db} \\in R^{B \\times V \\times V}$ containing the spatial-position information of electrodes is used, as shown in Fig. 3, where Fig. 3(a) is provided by [27]. The connected state of $A_{Db}$ is either connected or disconnected, which is respectively represented by 0 or 1. The \u03bb-mask matrix $A_{\\lambda} \\in R^{B \\times V \\times V}$ is obtained by substituting the elements in $A_{Db}$ whose values are 1 with 1+\u03bb and 0 with 1- \u03bb . The $A_{\\lambda}$ is introduced to fuse the $A_{Db}"}, {"title": "2.3 The Temporal Sector (TeS) of The Spatiotemporal Network", "content": "The TeS is mainly used to further extract the innately temporal features from the SpS $f_{sps}$ and fuse those time-windows processed after the SpS along time-window axis, which may benefit obtaining the smooth connection from the SpS to TeS. The domain-invariant features derived from the source-domain and the target-domain subjects are input into the TeS separately at the training and testing stage, and the TeS can be further divided into three fractions.\nThe first fraction of the TeS is a layernorm-LSTM, where a layernormalization-layer is used between two LSTM units. The final output is the hidden state at the last timestep, and each time-window corresponds to each timestep. The last hidden state contains the information of the former time-windows, which helps to fuse those time-windows together into one feature map $f_{LL} \\in R^{B \\times V \\times FL}$ ('FL' is the output-feature dimension). The process is shown in Eq. (7).\n$f_{LL} = Relu(\\Psi(\\Phi(f_{sps})))$\nwhere $\\Psi(\\cdot)$ is the sub-network function of layernorm-LSTM and the Relu represents the relu-activation function.\nThe second fraction is a modified GTN. The GTN [29] is used to construct a new connectivity matrix with the meta-paths extracted from the heterogeneous graph and then obtain the new node feature based on the newly generated connectivity matrix. The heterogeneous graph can be represented by an adjacency-matrix set $A = {A_1, A_2,...,A_T}$, where the adjacency matrix of each time-window $A_i$, (i = 1, 2, ..., T) is utilized and is homogeneous.\nGTN consists of multiple graph transformer (GT) layers. Each GT layer generates a two-order meta-path, representing that a one-more-hop-connected connectivity matrix is generated from the A. In the process of GTN, an intermediate adjacency matrix Q is introduced and is derived using Eq. (8).\n$Q_l = \\phi(A,softmax(W_l)).$\nwhere $M_{PGL} = M_{PGL} +I$ and D is its degree matrix. $W \\in R^{FS \\times FG}$ indicates a weighted matrix, and the negative slope of the leakyrelu-activation function used here has a default value of 0.01.\nThe final feature map $f_{res} \\in R^{B \\times V \\times FT}$, containing innate temporal information after fusing time-windows, is derived by concatenating $f_{LL}$ and$f_{GTN}$ along the last feature dimension. (FT = FL + FG)"}, {"title": "2.4 Domain Adversarial Learner (DAL)", "content": "The DAL is composed of the domain adversarial neural network (DANN) [30] and assists in obtaining the domain-invariant features $f_{sps} \\in R^{B \\times T \\times V \\times FS}$, which benefits in aligning the domain distribution [31]. Moreover, the DAL is just used in the training stage, which subsumes a GRL inserted between the SpS and the DAL. The FCN and softmax layer contained in the DAL consists of one flatten layer, one linear layer projecting the flattened dimension from V*FS to 128, one relu-activation layer, one batchnormalization layer, and one softmax layer. After the FCN and softmax, the predicted domain labels of T time windows are obtained. A cross-entropy loss is used to update the domain classifier, as shown in Eq. (11).\n$Loss_d = -\\frac{1}{L \\times T} \\sum_{l=1}^{L} \\sum_{t=1}^{T} \\sum_{r=1}^{R} D_{i,t,r}log D_{i,t,r}^\\prime$\nwhere L denotes the number of subjects in the source domain and target domain while training in one batch, T indicates that T time-windows are separately given the predicted labels, R represents the number of classes and $D_i$ and $D_i^\\prime$ are the true domain-label and the predicted domain-label (source-domain or target-domain), respectively.\nIn the training and updating process, both the depression classifier and domain classifier want to obtain an accurate classification result. However, when using the GRL, the gradient at the input of DAL is reversed while backpropagation and the whole loss function for the network is equivalent to Eq. (12).\n$Loss = Loss_c - Loss_d,$\nwhere an expected result is obtained that the domain classifier is unable to discriminate the domain of one subject from the $f_{sps}$, but the depression classifier is able to find out the right depression classification result. Thus, the $f_{sps}$ is the domain-"}, {"title": "3. Experimental Settings", "content": "The results for the experiments are shown in Fig. 4. In Fig. 4(a), regarding the results obtained on the PRED+CT dataset, it is observed that the outcomes under DANN are generally superior, and the best performance is achieved with DANN+SpS-way, where the Accuracy reaches 92.00%. However, when SpS-way is replaced with CF-way, a decline in all performance metrics is observed. For instance, Recall decreases from 90.49% to 84.21%, and PAM from 78.53% to 65.86%. From this perspective, considering the spatial features further extracted from common features as domain-invariant features is a better choice. Yet, comparing the results of MDAN+CF-way and MDAN+SpS-way, it is noted that MDAN+CF-way achieves higher metrics with 88.00% Acc, 86.21% F1 score, and 65.86% PAM, surpassing the indicators obtained with MDAN+SpS-way, where Acc, F1 score, and PAM are 84.00%, 80.95%, and 55.18%, respectively. Given this scenario, utilizing MDAN appears to have a negative impact on using as domain-invariant features. Hence, adopting the DANN approach yields better results.\nAt this point, in the results of the PRED+CT dataset, an interesting phenomenon can be observed: the performance metrics for DANN+CF-way are identical to those of MDAN+CF-way. However, on the one hand, when analyzing DANN+SpS-way and MDAN+SpS-way, a significant difference is evident through their F1 score and PAM outcomes, with the former achieving 91.32% and 78.53%, and the latter 80.95% and 55.18%, respectively. Thus, employing the DANN method still yields better results. On the other hand, upon examining the performance outcomes of DANN+CF-way versus MDAN+CF-way on the MODMA dataset in Fig. 4(b), a considerable discrepancy is noticed, with the DANN method again proving superior. The reason for the identical results between DANN+CF-way and MDAN+CF-way in Fig.4 (a) and the significant differences in Fig. 4(b) may be attributed to the differing signal lengths contained within each time window of the two datasets. The dimensions of the common features generated vary when automatic feature generation is utilized, such as obtaining a common feature dimension of 1004 in PRED+CT compared to 447 using MODMA. This variation is a natural consequence of employing convolution kernels of the same size and the employment of the time intervals, highlighting an area for future improvement. Given the higher common feature dimension in PRED+CT, both the DANN and MDAN domain adversarial methods might produce favorable outcomes. In contrast, a greater variation in results between DANN+CF-way and MDAN+CF-way is likely to occur in the MODMA dataset."}, {"title": "5. Conclusion", "content": "In this work, a Spatiotemporal-fused network with Automated multi-scale Depth-wise and TIME-interval-related common feature extractor (SAD-TIME) is proposed to automatedly extract EEG channel feature and properly fuse the multiple time-windows, where a novel automated and independent-of-prior-knowledge common-feature extractor (CFE) with multi-scale depth-wise 1D-CNN sector (DwCS) and time-interval sector (TiS), a spatial sector with a \u03bb-mask matrix and a multi-head graph convolutional network, a temporal sector with layernorm long short-term memory (LSTM) and modified graph transformer networks (GTN) and a domain adversarial learner (DAL) are contained. Extensive experiments have been conducted in the individual-based tenfold cross-validation for depression detection with the cross-subject mode. SAD-TIME has achieved an accuracy of 92.00% and 94.00% on PRED+CT and MODMA datasets for the depression detection task, respectively. Furthermore, from the results of those experiments, it would help to fuse the information from multiple time-windows of the depressed and healthy subjects properly by both the combination of the modified LSTM and GTN and the usage of the novel CFE that includes a depth-wise convolution fraction remaining the innate relations among multiple electroencephalogram (EEG) channels and time-interval-feature generation fraction revealing one kind of time pattern. Furthermore, the A-mask matrix is demonstrated to assist with the fusion of spatial information from the common feature and improve the performance of the model.\nHowever, firstly, in the DwCS and TiS, the generated feature dimension is different for two datasets as a consequence of the bound of the kernel size and the different signal time lengths of two datasets. A more plug-and-play and generalized automated feature extraction module should be further studied. Secondly, the proposed SAD-TIME model works on the small-size samples. Therefore, further research will be carried out on the frame-wise tragedy with larger-sized segmented samples. The leave-one-cross-subject mode may be better suited when using larger samples and the domain adaption method. Finally, in the DAL, a proper domain adaption method for a better feature-fusion effect of multiple time windows may be studied further."}]}