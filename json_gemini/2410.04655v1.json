{"title": "GRAPH FOURIER NEURAL KERNELS (G-FUNK): LEARNING SolutioNS OF NONLINEAR DIFFUSIVE PARAMETRIC PDES ON MULTIPLE DOMAINS", "authors": ["Shane E. Loeffler", "Zan Ahmad", "Syed Yusuf Ali", "Carolyna Yamamoto", "Dan M. Popescu", "Alana Yee", "Yash Lal", "Natalia Trayanova", "Mauro Maggioni"], "abstract": "Predicting the time-dependent dynamics of complex systems governed by nonlinear partial differential equations (PDEs), with varying parameters and domains, is a difficult problem that is motivated by applications in many fields. We introduce a novel family of neural operators based on a Graph Fourier Neural Kernel (G-FuNK), for learning solution generators of nonlinear PDEs with varying coefficients, across multiple domains, for which the highest-order term in the PDE is diffusive. G-FuNKs are constructed by combining components that are parameter- and domain-adapted, with others that are not. The latter components are learned from training data, using a variation of Fourier Neural Operators, and are transferred directly across parameters and domains. The former, parameter- and domain-adapted components are constructed as soon as a parameter and a domain on which the PDE needs to be solved are given. They are obtained by constructing a weighted graph on the (discretized) domain, with weights chosen so that the Laplacian on that weighted graph approximates the highest order, diffusive term in the generator of the PDE, which is parameter- and domain-specific, and satisfies the boundary conditions. This approach proves to be a natural way to embed geometric and directionally-dependent information about the domains, allowing for improved generalization to new test domains without need for retraining. Finally, we equip G-FuNK with an integrated ordinary differential equation (ODE) solver to enable the temporal evolution of the system's state. Our experiments demonstrate G-FuNK's ability to accurately approximate heat, reaction diffusion, and cardiac electrophysiology equations on multiple geometries and varying anisotropic diffusivity fields. We achieve low relative errors on unseen domains and fiber fields, significantly speeding up prediction capabilities compared to traditional finite-element solvers.", "sections": [{"title": "1 INTRODUCTION", "content": "Neural Operators for PDEs In scientific machine learning, data-driven deep learning methods aim at predicting the solutions of partial differential equations (PDEs), avoiding the computationally expensive numerical integration methods needed for large-scale simulations. This is especially beneficial for applications like domain optimization and precision medicine, where multiple PDEs need to be solved for varying parameters or domains, requiring significant computational resources. Neural operators learn mappings between high-dimensional function spaces, allowing them to generalize across a family of PDEs without retraining for varying parameters or conditions Kovachki et al. (2023). Mathematically, a neural operator N\u0259 is defined as a mapping from an input domain function space A(Na; Rda) to an output target function space U(Na; Rdu), represented as\n\u039d\u03bf: \u0391(\u03a9\u03b1; Rda) \u2192 U(Na; Rdu),\n(1)"}, {"title": "Problem Setup", "content": "In this context, we focus on the family of second-order nonlinear differential operators, governing the evolution of a function u : (x, t) \u2192 R, x \u2208 \u03a9\u03b1, t \u2265 0, in the form,\n[\ndu(x,t) = V. (K(x)\u2207xu(x, t)) + S(u(x, t), x, Vxu(x, t)) = N(u(x, t), K(x), x, t),\n(\u03b8\u03b7(x)u(x) = 0, \u03bd\u03b1 \u03b5\u03c0\u03b1\n(2)\nwhere K is a diffusion tensor field on the domain satisfying uniform ellipticity, N : R3d \u00d7 RdK \u2192 R is a vector function that can encompass source terms, sinks, or other linear and nonlinear interactions within the term S, in this work we use no-flux or no-diffusive-flux Neumann boundary conditions, where n(x) is the normal to da at x, and da is the boundary of the spatial domain La (in Rd or a d-dimensional manifold M\u00b2) within a family of domains {a}a\u2208A. The dependence of the solution to (2) on the domain a is often complex. Note from 2 that we are focusing here on the semilinear case, i.e. N is linear in its highest-order term (the diffusive component), and in general nonlinear in the lower-order terms.\nThe training data consists of trajectories {u(m) (xi, te)}mM,na(m), L , where xi is sampled on a graph\nm=1,i=1,l=1 Ga(m) (a mesh discretizing \u03a9\u03b1(m)), and time points 0 = to < t\u2081 < \u2026 < t\u2081 = T are sampled at a fine timescale, together with the parameter K(m) and the graph Go(m). In particular, note that the observation at t = 0 says that we are given the initial condition, and that different trajectories in the training set may correspond to different (given) values of both the parameter K and the domain Ga.\nThe objective (in contrast to equation (1)) is to learn from the training data a neural operator Ne, with denoting the neural network parameters, that approximates the generator of the solutions of (2) by learning a mapping which takes any given state u(x, te), parameters K and domain Ga, and outputs an approximation of the left-hand side of (2). Solutions of the PDE may be approximated by integrating\ndt u(x,t) \u2248 No(u(x, t); Ga, K),\n(3)\nwith x \u2208 Ga, on the whole interval [0, T], given any initial condition u(., 0) even with changes in parameters and spatial domain. This solver has the chance of being more efficient than one for equation 2, for example by employing larger time steps, by efficiently incorporating the shape of a and the boundary conditions, or by reducing the dimensionality of the problem when x is high-dimensional."}, {"title": "Reduced Models and Homogenization", "content": "In fact, we are very much interested in situations where the \"true\" system is driven by equations more general than equation 2, with many more variables, possibly evolving at multiple time scales; this will be the case in examples 2 and 3, where besides the spatial variables and the unknown action potential u, there are many other quantities v evolving in space and time (e.g. modeling ionic concentrations that affect the evolution of u, and these are driven by a large system of ODEs). In these situations, given observations from such systems, our estimated equations equation 3 can be viewed as performing dimension reduction on the variables (e.g. by keeping only the spatial variables) and homogenization, obtaining an effective equation for u, without tracking the evolution of the other quantities v."}, {"title": "Related Works", "content": "Several studies have demonstrated the ability of different types of neural networks to approximate nonlinear operators with strong theoretical guarantees (e.g., PDE solution operator) Chen & Chen (1995); Lu et al. (2021); Kovachki et al. (2021). Fourier Neural Operators (FNOs) and Deep Operator Networks (DeepONets) and their variants are among the most popular computational frameworks under the umbrella of neural operators Lu et al. (2021); Li et al. (2020a). Vanilla DeepONets, however, are restricted to a fixed grid and resolution, and thus do not generalize well on unseen domains. Yin et al. (2024) propose a work-around spatial transformations of multiple domains to a universal domain where DeepONets can be used to learn a latent operator, the evaluation of which can be mapped back to the target domains via the inverse for evaluation. FNOs, while robust to variations in grid resolution, require regularly spaced square grids to perform FFT Li et al. (2020a). As an improvement, Li et al. (2023) propose a Geometry-Aware Fourier Neural Operator (Geo-FNO), which learns an additional deformation step to transform irregular grids, achieving greater accuracy than interpolating the solution on a uniform grid. However, they explore only a relatively small subset of possible transformations and do not investigate generalization performance on differently shaped grids. For large-scale 3D PDEs, Li et al. (2024) also propose a more general way of adapting to arbitrary domains and irregular grids with Geometry Informed Neural Operators (GINO) which incorporates information about the geometry into the learning process, however, they mention that the trained model is limited to certain family of shapes and to generalize well, abundant, geometrically varying training samples are necessary, a luxury that is not available in real world settings such as computational medicine. Several works have also shown promise with regards to predicting full solution trajectories over a time interval, but typically do not carry the same generalizability over multiple domains as the aforementioned works Chen et al. (2023); Zhang et al. (2024); He et al. (2024); Regazzoni et al. (2024).\nGraph Neural Networks (GNNs) are a versatile class of neural network architectures designed to work directly with graph-structured data. These networks can make predictions at the level of nodes, edges, or entire graphs, typically utilizing a method known as message passing Gilmer et al. (2020). In this method, neighboring nodes exchange messages, and the aggregated information from these messages is used to update the states of subsequent nodes. GNNs are particularly well-suited for irregular grid data and can be effectively applied to training data generated by finite-element method simulations (considered ground truth). Li et al. (2020b) proposed a message-passing operator (using edge-conditioned graph convolutions Simonovsky & Komodakis (2017)) to learn solution operators for PDEs. Iakolev et al. also utilized a message-passing scheme to learn isotropic PDEs Iakovlev et al. (2020). Other studies have considered learning time-dependent PDEs using graph-based approaches Li et al. (2020c); Behmanesh et al. (2023); Pilva & Zareei (2022), but none of these works combine learning with both varying domains and varying parameters in the PDE. Furthermore, the message-passing frameworks they employed in these works involve edge aggregation schemes that limit the learning of direction-dependent information, which is crucial in the case, for example, of anisotropic diffusions. Finally, several studies have shown that spectral GNNs possess superior expressiveness and interpretability, capturing global information more effectively than spatial GNNs Bo et al. (2023); Defferrard et al. (2016); Wang & Zhang (2022); Yang et al. (2022)."}, {"title": "Contributions", "content": "We propose a new family of neural operators consisting of novel Graph Fourier Neural Kernel (G-FuNK) layers for learning generators of solutions to time-dependent PDEs with varying coefficients across multiple domains as in equation (2). This method combines ideas from both GNNs and FNOs: it leverages the spectral domain of graphs, unlike GNNs that rely on localized message passing, and is suitable on general graphs rather than relying on grids, as FNOs. G-FuNK employs the Graph Fourier Transform (GFT) to convert functions on graphs into functions on the graph spectral domain. This facilitates global information integration across the entire graph, allowing G-FuNK to capture both local and global dependencies effectively, as FNOs do in the classical Euclidean grid setting.\nG-FuNK integrates components that are both parameter- and domain-adapted with others that are not. The non-adapted components are learned from training data, and can be directly transferred across different parameters and domains, independent of the mesh resolution or the time step size. The adapted components are constructed for specific parameters and domains by forming a parameter-dependent, tailored weighted graph on the discretized domain, such that the corresponding graph Laplacian approximates the highest-order diffusive term in the PDE with those specific values of the parameter, on that domain.\nWe remark that here we do wish to predict entire trajectories of temporal dynamics on [0, T], which is difficult as inaccuracies can lead to dramatic accumulation of errors, depending on the PDE. Many existing methods Li et al. (2020a;b;c) are either restricted by design on learning the map from initial conditions (or several observation points) to a solution at a fixed time t, or while they could in principle generate solutions at all times, experiments in this setting are not reported in the corresponding papers. With this goal in mind, our framework is equipped with an integrated ODE solver to enable generating the temporal evolution of the dynamics of the system. This allows for efficient, mesh-independent prediction of the system's solutions from new initial conditions, with new values of the parameter K, on a new domain \u03a9\u03b1. To our knowledge, there is no current neural operator framework which naturally handles directionally dependent information of anisotropic domains while predicting time-evolving trajectories of the solution operator on multiple geometries.\nWe demonstrate G-FuNK's capabilities by first considering the basic example of the anisotropic heat equation on the square, where the tensor field K(x) reflects the directional dependence of diffusivity. We then move to nonlinear, more complex models of reaction-diffusion equations for cardiac electrophysiology (EP), first on families of rectangular domains, then on 3D geometries representing the outer surfaces of real human left atrial (LA) chambers from computed tomography (CT) data. The PDE operators on the patient-specific LA chambers involve effects from high-dimensional coupled ODEs. Our method accurately captures the complex dynamics of the transmembrane potential on varying geometries and anisotropic diffusivity (fiber) fields. Our experiments show that G-FuNK can significantly speed up patient-specific cardiac electrophysiology simulations, which are used for making real-time quantitatively informed clinical decisions Boyle et al. (2019); Trayanova et al. (2024), achieving low relative errors on unseen domains."}, {"title": "2 G-FUNK: GRAPH FOURIER NEURAL KERNEL", "content": ""}, {"title": "2.1 GRAPHS, LAPLACIANS, AND THE GRAPH FOURIER TRANSFORM", "content": "Let Ga(V,E) be an undirected graph where V = {x}_1 is the set of na nodes (or vertices)\nconstructed from a finite element mesh discretization or down-sampled point cloud representation\nof Na and E \u2286 {(Xi, X j)|Xi, X j\u2208 V} is the set of edges that connects pairs of nodes which are defined as the k-nearest neighbors of each xi. Let W \u2208 Rna\u00d7na be the weighted adjacency matrix, with Wij = Wji \u2265 0, with strict inequality if and only if (xi, xj) \u2208 E, and deg is the na \u00d7 na diagonal degree matrix of the graph, defined by degii := \u2211; Wij. The undirected graph Laplacian is a positive semi-definite matrix with a spectral decomposition given by\nLg := deg - W = \u03a8\u039b\u03a8,\n(4)\nwhere \u03a8 = [1,...,\u03c8n] \u2208 Rnaxna is the matrix of orthonormal eigenvectors of LG, and A = diag(1, 2, ..., Ana) is the diagonal matrix of the corresponding non-negative eigenvalues. Note that {i}\u2081 is an orthonormal basis for functions on the graph, since Lg is symmetric. When the graph is constructed on points randomly sampled on a domain or a manifold, this discrete Laplacian is an approximation (in a suitable sense) to a continuous Laplacian in the limit as the number of samples goes to infinity, and its eigenvectors are approximations to the eigenfunctions of the continuous Laplacian, which are the natural generalization of Fourier modes from a torus or rectangular box to general domains and manifolds. Furthermore, the geometry of the domain (graph, in the discrete case, Riemannian manifold in the continuous case) is completely determined by the Laplacian, and therefore by its eigenvalues and eigenvectors.\nWe define the Graph Fourier Transform (GFT) as follows: for a function a \u2208 Rna on the vertices of Ga, the Graph Fourier Transform (GFT), denoted by F, rewrites a in the basis of eigenvectors of the graph Laplacian L\u00e7a, yielding a function a of the eigenvalues \u51651,..., Ana, defined as\n\u00e2 := Fa := ((\u03c8i, a))i=1,...,na = \u03a8\u03a4\u03b1,\n(5)\nIn the reverse process, the inverse Graph Fourier Transform (IGFT), represented by F\u22121, reconstructs the function a from its spectral representation a, through the equation\n\u039d\u03b1\na = F-1a := \u2211(\u03bb\u03b9)\u03c8\u2081 = \u03a8\u1fb6.\n(6)\ni=1\nIt should be noted that computing the kmax lowest eigenvalues for an undirected graph is generally inexpensive with a complexity of O(kmaxnaj) for a graph with na vertices, j neighbors per ver- tex. Eigenpairs are of course computed once per domain/diffusivity parameter, rather than being recomputed at every instance during network training."}, {"title": "2.2 THE G-FUNK LAYERS AND NETWORK", "content": "In this section, we describe the neural operator framework, a variation of the FNO of Li et al. (2020a), equipped with our novel Graph Fourier Neural Kernel (G-FuNK) layers to learn solution generators to time-dependent PDEs across multiple domains and multiple anisotropic diffusion tensor fields.\nRecall that the domain or, rather, a discretization thereof is given, and so is the diffusion field K. We exploit the knowledge of the former to construct a graph Ga which discretizes \u03a9\u03b1 into \u03c0\u03b1 nodes V = {xi}\u2081, and knowledge of the latter to weight the edges in order to approximate the second-order term V. (K(x)) in the PDE. By choosing, on each edge Eij,\n-1\nwij := (xj \u2212 x\u2081). (K(xi)\u22121 + K(xj)-1) \u00b7 (Xj - Xi),\n(8)\nwe weight the edges at xi proportionally to the direction of the diffusion coefficient K(xi) at xi, and to the squared inverse of the distance. The average of K(xi)\u00af\u00b9 and K(xj)\u00af\u00b9 is used to obtain a symmetric graph.\nThe neural operator No, equipped with several G-FuNK layers, approximates the solution generator of the PDE by applying a series of transformations, which can be formally expressed as the composition of different functions representing the network layers. The network begins with the following function transformation:\nko(x) = P(a(x); \u03b8\u03c1),\n(9)\nwhere P: Rnaxda \u2192 Rna\u00d7dp is the initial lifting operation to a higher dimensional feature space. For each G-FuNK layer n = 1, . . ., N, we have, following the structure diagram in Figure 1 from left to right:\nkn(x) = F(kn(x); \u03a8\u03a4) (\u03bb),\nln(x) = Ln(kn (1); \u0392)(\u03bb),\n\u00cen (1) = Rn (ln(1); 0rn)(1),\nfn(x) = F-1(rn(\u03bb); \u03a8)(x)\nWn(x) = Wn (kn(x); 0wn)(x),\nzn(x) = \u03c3(fn(x) +wn(x))(x) = kn+1(x),\n(10)\nwhere F : Rnaxdp \u2192 Rkmaxxdp represents the Graph Fourier Transform that projects the function into the kmax lowest frequencies in the spectral domain, Ln : Rkmaxxdp \u2192 RkmaxxdpXp for n = 1, . . ., N is a linear transformation of kn together with a set of the eigenvalues raised to p powers (i.e. B is a matrix Rkmax \u00d7P with columns of (x1,...,), (1,...,),, (x-1,-1)),\nRn:Rkmaxxdp\u00d7p \u2192 Rkmaxxdp' for n = 1, . . ., N are the parameterized linear transformations in the spectral domain corresponding to the n-th G-FuNK layer which can be diagional, tri-diagional, full matrices, etc., F-1 : Rkmaxxdp' \u2192 Rna\u00d7dp' is the inverse Graph Fourier Transform that maps the transformed spectral function back to the graph domain. Wn : R\u00f1oxdp \u2192 Rna\u00d7dp' for n = 1, . . ., N are additional linear mappings applied after transforming back to the graph domain in each G-FuNK layer, o : Rna\u00d7dp' \u2192 Rnaxdp' an activation function applied in each G-FuNK layer.\nAfter processing through all N G-FuNK layers, the final output is obtained by the projection layer:\ny(x) = Q(zn(x); 02)\n(11)\nwhere Q : R&\u00d7dp' \u2192 Rnaxdu projects from the last layer's output onto the target space. All together, the neural operator with N G-FuNK layers can be expressed as the following composition of the afforementioned transformations to approximate d\u0142u(x,t):\n-1\n\u039d\u03bf(\u03b1(x)) := Q\u03bf\u03c3(WN + F\u00af\u00b9 \u0970RN \u25cb L\u00d1 \u00b0F)\u0970\u22ef\u22ef\u03bf\u03c3(W\u2081 + F\u22121 \u25e6 R1 \u00b0 L1 \u00b0 F) \u0970P(a(x); 0),\nG-FuNK Layer N\nG-FuNK Layer 1\n(12)\nWe denote 0 = {0pn,0Rn,0Wn,0Qn}n=1 as a collection of all learnable parameters in the network that are optimized during the training phase to minimize the discrepancy between the neural operator's output and the known numerically computed trajectories. \nGiven discrete observations {u(xi,te)}{=1 of the state variable at a location xi \u2208 Ga at different time steps 0 = t1 <\uff65\uff65\uff65 <t\u2081, we can integrate the approximation over time using an ODE solver, I:\nu(xi, te + \u2206t) = I (No(u(xi, te, \u03a8, \u039b)),\n(13)\nwhere At is the time step size, and I is a neural ODE numerical algorithm (we use the one proposed by Chen et al. (2018)). This integrator uses the adjoint method during the gradient computation in backpropagation (since the loss function involves predicting the solution at multiple future time points), which makes training computationally demanding.\nIn the G-FuNK framework, the eigenvectors of the Laplacian perform the same action as the Fourier transform in the FNO framework, with the important difference that these eigenvectors are adapted to the diffusion coefficient, capturing the parametric dependence of the highest order differential term in the PDE. We suggest using the true eigenfuctions of the shape which embed global structural and physical properties as corroborated by numerical and statistical methods which represent PDE solutions in terms of known basis functions that contain information about the solution structure Bhattacharya et al. (2021); Nagy (1979); Almroth et al. (1978). Furthermore, in FNOs, the eigenvalues are not needed as there is no domain change; here by incorporating the eigenvalues we capture both the dependency on K and on the domain Ga.\nFinally, we comment that Ln and Rn allow G-FuNK to approximate spectral multipliers, while Wn allows for the approximation of spatial pointwise multipliers. At a high level, this is particularly"}, {"title": "Mesh-Independence of G-FuNK", "content": "Although the proposed framework involves mesh-dependent steps, such as forming a weighted graph on the discretized domain, the method maintains a high degree of mesh-independence. More precisely, as the number of points increases, the Laplacian we construct approaches its limit, allowing our framework to effectively handle finer discretizations. Moreover, there are established methods for interpolating eigenfunctions to an underlying continuous domain Coifman et al. (2005), which further supports the transition from discrete to continuous representations. This characteristic is crucial, as it sheds light on the ability of our method to transition from discrete approximations to continuous domains as the mesh size approaches zero. Our framework's capability to interpolate and generalize across varying mesh sizes ensures that it remains robust and accurate in capturing the underlying dynamics of PDEs in the limit of fine discretizations."}, {"title": "3 NUMERICAL EXPERIMENTS", "content": "In this section, we present several numerical experiments demonstrating the ability of the G-FuNK operator learning framework to accurately predict the solution generator for PDEs of the form in (2), towards the goal of accelerating precision medicine in cardiac electrophysiology. Results for all examples and comparisons to baseline models are summarized in Table 1.\nHeat Equation. The introduction of anisotropic diffusion is critical in simulating phenomena such as the propagation of heat in materials with fibrous structures, where the diffusive properties vary along different directions. We consider the classical heat equation with very strong anisotropic diffusion on a 2D unit square domain with no-flux Neumann boundary conditions, with K as a diffusion tensor representing the anisotropy:\nd\u0142u = \u2207 \u00b7 (K(x)\u2207u), x\u2208 Na,t > 0, On(x)u(x) = 0 \u2200x \u2208 \u03b8\u03b1\n(14)\nIn our experiments, we consider a diffusion tensor with an anisotropy ratio of 9:1, given by:\n[\n9 0\n0 1\nK(x) = [F(1)(x) (2) (x)] [0 1] [(1)(x) (2) (x)],\n(15)\nwhere \u0124(1)(x) and (2)(x) are orthonormal vectors fields that define the longitudinal and transverse directions of diffusion and are described in Appendix section A.1.1, equation (17). The longitudinal direction of the anisotropic diffusion were defined as a superposition of geometric and linear functions, with random parameters sampled independently for each trajectory in both training and test data."}, {"title": "2D Nonlinear Reaction Diffusion", "content": "We consider a reaction diffusion system in a 2D domain with no-diffusive-flux Neumann boundary conditions. This system of equations is very stiff and involve multiple non-linear equations to describe the propagation of action potentials in the form of a bioelectrical wave Courtemanche et al. (1998). The solutions of these equations typically have the range between -85 and 20 mV, with the wavefront having a rate of change > 100 mv, making them steep and close to discontinuous. For these systems, an external stimulus current is applied to start the wave propagation from a random point within the domain for each trajectory.\nIn terms of the transmembrane potential u, the system is of the form\n\u0434\u0438 = V. (K(x)\u2207u) + \u2211 Js (u, v), dv = Y(u, v), \u039a(x)\u2207un(x) = 0\u2200x \u2208 \u03b8\u03a9\u03b1, (16)\ndt\nS\nwhere x \u2208 \u03a9\u03b1, t > 0, \u2211, Js is the sum of 12 ionic and external stimulus currents described by non-linear functions Courtemanche et al. (1998). In this case, no diffusive flux Neumann boundary conditions were used.\nIn this example, each trajectory had a unique rectangular domain, \u03a9\u03b1(m), with edge length drawn independently from 2 uniform distributions on [15, 30]. The diffusive tensor K(x) was set at 5 to 1 in the x direction across each domain."}, {"title": "Cardiac Electrophysiology", "content": "In the field of personalized cardiac electrophysiology (EP), the ability to describe the electrical propagation in the Left Atria (LA) is of the utmost importance to provide better healthcare to patients inflicted by electrical abnormalities. Using cardiac computed tomography (CT) scans from 25 atrial fibrillation (AF) patients, triangulated meshes of the LA surface were constructed from image segmentations. The mitral valve and four pulmonary veins were removed via visual inspection resulting in a topology with five holes. The fiber orientation for each mesh was mapped using an atlas-based procedure Roney et al. (2021); Ali et al. (2021); Roney et al. (2019) and produced fibers similar to the examples shown in Figure 4. The process of manually annotating the geometry to define the fiber fields adds inherent noise from one geometry to another.\nFinite element simulations were computed using the openCARP software Plank et al. (2021) in which the Courtemanche electrophysiological ionic model Courtemanche et al. (1998) was solved with no-flux Neumann boundary conditions to describe the electrical wave propagation in each domain. The Courtemanche equations follow the same form as (16). The diffusive ratio was set as 5 to 1 in the fiber direction with a base diffusive value of 0.625 mm/ms. Each simulation incorporated a stimulus current, represented as a delta spike, chosen at a random location within the domain and the initial condition was chosen at 10 ms after this stimulus was applied. For this work, the aim was to start from an initial condition uo and compute the next time step repeatedly. This was done using 24 domains, G-FuNK was trained to infer the solution from 0ms to 90ms which focused on the steep wavefront, and an additional, unseen geometry was used as an out-of-training test set as shown in Figure 5."}, {"title": "Discussion", "content": "Altogether, G-FuNK was adaptable to all the examples discussed above with low relative error. The results for the various tasks are listed in Table 1. G-FuNK performed well on all examples with a fairly small amount of learnable parameters compared to FNOs and GeoFNOs Li et al. (2020a; 2024).\nMessage-passing GNNs aggregate information at the nodes, and as expected are unable to accurately handle directionally dependent information (varying diffusive fields). We notice that when we incorporate edge weights, as defined in equation (8), into the GNN model for the Heat equation example, the results improve and the model requires much fewer parameters. Therefore, for all comparisons following, we incorporated the edge weights as an input into the different networks.\nPrior works such as FNO and GeoFNO, with the code provided, do not present results or compatibility for learning full trajectories of time-evolving processes, where errors can accumulate due to spatial misalignment of predictions Li et al. (2020a; 2023). To compare with FNO and GeoFNO, we modified the approaches to be compatible with the same neural ODE solver we use for time-dependent predictions with G-FuNK. For the heat example, it should be noted that G-FuNK was able to learn the effects of anisotropic diffusion from only the eigenvectors. The other networks were given the primary diffusive vector as inputs at each sample point which has less meaning outside of planar geometries. In the 2D reaction-diffusion problem with random rectangles, both GeoFNO and G-FuNK perform similarly on the test set, as expected. In regular rectangular domains, the graph Fourier and fast Fourier bases converge, implying that FNO and GeoFNO are subsets of G-FuNK equipped neural operators, corroborated by the similar performance. For training in this example, we still considered anisotropy, but with all the fiber fields strictly pointing in the horizontal axis. We show that G-FuNK and GeoFNO perform quite similarly on test domains with the same directionality presented in the fiber fields as the training samples. However, when the test set domains and fiber fields were rotated by 90 degrees, GeoFNO's performance significantly degraded, with a relative l2 error of 0.5681, as shown in Table 1 and Figure 6. In contrast, G-FuNK, trained using only the initial condition and graph eigenpairs, remained rotation-invariant to the unseen fiber orientation.\nFor the cardiac EP example, we do not show results with GeoFNO since the topologically complex 3D geometries of the human left atrium with five holes cannot be mapped diffeomorphically to a cube or torus where the Fourier transform is available. For this example, our G-FuNK equipped operator learning framework is able to provide respectable predictions, where a majority of the relative l2 error is due to a temporal misalignment of the prediction and the ground truth which can be specifically attributed to a small lag of about 1.62 ms in the wavefront. This shift resulted in a higher l2 error but the cross-correlation is 0.941 implying that the prediction is correctly shaped with a small lag.\nOur method predicts entire trajectories in under 1 second, significantly outperforming traditional numerical methods. For example, cardiac EP simulations typically take at least 15 minutes on 12 CPU cores for one given set of initial conditions. To make quantitatively informed clinical predictions, one must perform comprehensive parameter sweeps to identify optimal treatment strategies for a given patient, which could take hours to days with the finite element approach. These trajectories are predicted by the G-FuNK equipped operator learning framework in less than 1 second. The inclusion of more geometries in the 3D cardiac EP examples is of course expected to lead to lower error. Additionally, small changes in the eigenvalues across domains can lead to mismatches in the order of the eigenvalues between geometries which could be a source in the reported error: this can be avoided with an eigenvector-matching procedure, which will be the subject of future investigations."}, {"title": "4 CONCLUSION", "content": "Data-driven methods for solving PDEs rely heavily on the quality and quantity of the data provided. In this work, we introduce a new family of neural operators based on our novel Graph Fourier Neural Kernel (G-FuNK) layers, which integrate GNNs with a variant of FNOs. We show that our framework achieves high accuracy on pedagogical examples, including the anisotropic 2D heat and reaction- diffusion equations. Even in the relatively data-sparse context of cardiac electrophysiology explored here, G-FuNK demonstrates respectable accuracy. Overall, we highlight the method's expressiveness and versatility by learning PDE solution generators in the spectral domain of graphs, providing a natural way to embed geometric and directionally dependent information about anisotropic domains. Our G-FuNK-equipped operator learning framework effectively predicts the temporal dynamics of complex systems from a single initial condition, handling unstructured data, varying anisotropic diffusion tensor fields, highly nonlinear reaction terms, and different domains, including topologically complex 3D left atrial geometries."}, {"title": "A.1 HEAT EQUATION", "content": ""}, {"title": "A.1.1 HEAT DATA GENERATION", "content": "For this section of work, the heat equation was solved using the NDSolve function from the Mathematica software on the 2D unit square where the initial condition, uo, was unique for each trajectory and chosen as a superposition of three binomial distributions where each center, deviation, and covariance were picked from uniform distributions with ranges of [0, 1], [0.1, 0.2] and [-0.1, 0.1], respectively. The primary direction of diffusion was defined as F\u00b9 in the equation below.\nF(1) (x,y) = $\n[\nsin(a\u2081(+2)) (x+a2)) + cos(a3(4+4)) + a5x + a6 + a7y + a8\nsin(b\u2081(+62)) + cos(b3 (4+64))+b5x+66 + by + bs\n]$\n(17)\nThe parameters a\u2081 to as and b\u2081 to b8 were derived from the following uniform distributions and were unique for each trajectory in the training and test data sets:\nThis allowed for vector fields like the ones shown in the Figure 7 below.\nAfter all simulations were completed, a perturbed Delaunay triangulation grid was constructed and linear interpolation was used to generate the trajectories used for training at discretized points within the domain with a max cell measure of 0.0002 and from 0 to 20ms at every 1 ms intervals. In addition, all other data, like diffusion and the vector denoting the primary direction of diffusion orientation,"}, {"title": "\u0391.1.2 NETWORK DESCRIPTION FOR THE ANISOTROPIC HEAT EXAMPLE", "content": "For each trajectory, an undirected Laplacian matrix was created such that each point was connected to its 30 nearest neighbors and weighted using the heat kernel method described in the main text. The graph Laplacian was constructed using the NetworkX software and spectral decomposition was done using the Scipy software's eigsh function.\nThe network was trained and optimized using the PyTorch software with an l2 loss. The network is described as so, a. Linear mapping from three inputs (u, x, y) to 200 width. b. three G-FuNK layers each with a width of 200 and using 50 modes and using a GELU activation function. c. Two Linear layers going from a width of 200 to 32, with a GELU activation, and then 32 to 1. The network was integrated using the NeuralODE software with a forward Euler integrator with a time step of 0.25 ms. G-FuNK was optimized with an l2 loss function and an Adam optimizer with an initial learning rate of 5\u00b71"}]}