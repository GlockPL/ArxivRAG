{"title": "AT-SNN: Adaptive Tokens for Vision Transformer on Spiking Neural Network", "authors": ["Donghwa Kang", "Youngmoon Lee", "Eun-Kyu Lee", "Brent Byunghoon Kang", "Jinkyu Lee", "Hyeongboo Baek"], "abstract": "In the training and inference of spiking neural networks (SNNs), direct training and lightweight computation methods have been orthogonally developed, aimed at reducing power consumption. However, only a limited number of approaches have applied these two mechanisms simultaneously and failed to fully leverage the advantages of SNN-based vision transformers (ViTs) since they were originally designed for convolutional neural networks (CNNs). In this paper, we propose AT-SNN designed to dynamically adjust the number of tokens processed during inference in SNN-based ViTs with direct training by considering power consumption proportional to the number of tokens. We first demonstrate the applicability of adaptive computation time (ACT), previously limited to RNNs and ViTs, to SNN-based ViTs, enhancing it to discard less informative spatial tokens selectively. Also, we propose a new token-merge mechanism that relies on the similarity of tokens, which further reduces the number of tokens while enhancing accuracy. We implement AT-SNN to Spikformer and show the effectiveness of AT-SNN in achieving high energy efficiency and accuracy compared to state-of-the-art approaches on the image classification tasks, CIFAR-10, CIFAR-100, and TinyImageNet. Notably, our approach uses up to 42.4% fewer tokens than the existing best-performing method on CIFAR-100, while conserving higher accuracy.", "sections": [{"title": "1 Introduction", "content": "As an alternative to the high energy consumption of artificial neural networks (ANNs), spiking neural networks (SNNs) have recently received considerable attention (Stone 2018; Nunes et al. 2022; Lobo et al. 2020). SNNs, regarded as the next generation of neural networks, emulate the synaptic functioning of the human brain. While ANNs perform a floating point operation on a single timestep, SNNs operate by repeatedly computing binary-formatted spikes over multiple timesteps, allowing them to achieve accuracy comparable to ANNs with lower power consumption.\nState-of-the-art research aimed at reducing power consumption in SNNs has led to significant advancements in both training and inference. Initially, training methods have progressed from the ANN-to-SNN (Sengupta et al. 2019), which necessitates hundreds of timesteps, to direct training (Wu et al. 2018) that requires substantially fewer timesteps with a marginal accuracy drop (Tang et al. 2023; Wu et al. 2022). Regarding inference, lightweight computation methods have been proposed to provide the trade-off between timesteps and accuracy. However, only a few approaches apply direct training and lightweight computation methods simultaneously, and these were initially designed for convolutional neural network (CNN) models, e.g., the circled 'C' of Diet-SNN (Rathi and Roy 2020), tdBN (Zheng et al. 2021), TET (Deng et al. 2022), and DT-SNN (Li et al. 2023) in Fig. 1(a). Consequently, they fail to fully harness the potential of the vision transformer (ViT) models, which typically provide higher accuracy than CNNs, e.g., the circled 'T' of Spikformer (Zhou et al. 2022) with 77.42% compared to the circled 'C' of other methods in Fig. 1(a).\nUnlike CNNs, ViTs segment the input image into small patches, treating each as an independent token and executing spiking self-attention (SSA) to assess token relationships. This operation requires substantial computational resources and, consequently, accounts for the majority of energy consumption in ViT (Zhou et al. 2022). Our strategy is to employ adaptive computation time (ACT) (Graves 2016) to improve its implementation in SNN-based ViTs, enabling the selective omission of spatial tokens that are less informative with direct training. ACT, initially applied to recurrent neural networks (RNNs), to model neural outputs with a halting distribution, converts the discrete halting issue into a continuous optimization problem, thereby minimizing total computation (Graves 2016). Subsequently, ACT was applied to ViTs, by variably masking tokens based on their halting"}, {"title": "2 Related Work", "content": "Methods like DT-SNN and SEENN (Li et al. 2023, 2024) dynamically adjust timesteps of SNN during inference based on accuracy needs, using entropy and confidence metrics. SEENN employs reinforcement learning to optimize timesteps for each image, while TET (Deng et al. 2022) introduces a loss function to address gradient loss in spiking neurons, achieving higher accuracy with fewer timesteps. However, these methods are less suitable for deeper models requiring fewer timesteps, where efficiency gains are limited. MST (Wang et al. 2023) proposes an ANN-to-SNN conversion method for SNN-based ViTs, using token masking within model blocks to reduce energy consumption by decreasing spiking tokens during inference. Despite its effectiveness, MST still requires hundreds of timesteps by relying on ANN-to-SNN.\nACT (Graves 2016) dynamically allocates inference time for RNN models based on the difficulty of the input, enhancing accuracy in natural language processing (NLP) tasks. SACT (Figurnov et al. 2017) adapts ACT for ResNet architectures, allowing the model to halt inference early depending on the input data, thus maintaining classification accuracy while reducing FLOPs. Similarly, A-ViT (Yin et al. 2022) dynamically adjusts token computation within transformers to optimize efficiency. However, these studies, which are based on ANN, do not account for the timestep characteristics of SNN and typically perform a single inference per input. LFACT (Zhang, Ebrahimi, and Klabjan 2021) expands ACT to enable repeated inferences across input sequences, though it remains limited to RNNs. In contrast, AT-SNN must consider multiple timesteps and blocks, making it uniquely suited for SNN-based vision transformers."}, {"title": "3 Method", "content": "3.1 Can ACT be applied to SNN-based ViT?\nACT was initially proposed for RNNs to provide a condition for accuracy-effective halting during inference. As illustrated in Fig. 2(a), an encoder block B\u00b9 of an RNN repeatedly processes the same input x\u00b9 while the hidden state evolves. During training, ACT employs a unique loss function to learn the impact of execution halting for the combination of B\u00b9 and x\u00b9 on the accuracy, thereby determining the halting probability, referred to as the halting score h. During inference, h is repeatedly added, and the halting condition is met when the accumulated halting scores reach one.\nOn the other hand, ViT has two distinct properties: (i) ViT has multiple identical encoder blocks as shown in Fig. 2(b); and (ii) the input vectors between consecutive blocks have low estimation errors and high cosine similarity as shown in Fig. 3(a), mainly because the input x\u00b2 of B\u00b2 is the sum of the output of B\u00b9 and x\u00b9 in Fig. 2(a). Due to (i) and (ii), the inputs of each block, similar to the initial input x\u00b9, accumulate halting scores (e.g., h\u00b9 + h\u00b2 + \u00b7\u00b7\u00b7) as they pass through the identical encoder blocks. Similar to ACT in RNNs, the process halts when the accumulated halting scores reach one.\nAs illustrated in Fig. 2(c), the SNN-based ViT possesses an architectural structure that is fundamentally analogous to that of the standard ViT, yet it performs iterative computations across multiple timesteps. The primary distinction lies in the storage of information as membrane potential within each block, which, upon surpassing a specific threshold, triggers a binary activation function that outputs a value of one. Consequently, property (i) is satisfied, but it is nec-"}, {"title": "3.2 Adaptive Tokens for SNN-based ViT", "content": "We formulate the SNN-based ViT as follows (Zhou et al. 2023):\n$f_T(x) = FC( \\frac{1}{T} \\sum_{t=1}^T B_t^L \u25cb B_t^{L-1} \u25cb ... \u25cb B_t^1 \uff61S(x))$, (1)\nwhere $x \u2208 R^{T\u00d7C\u00d7H\u00d7W}$ is the input of which T, C, H, and W denote the timesteps, channels, height, and width. The function S() represents the spike patch splitting (SPS) module, which divides the input image into $K_0$ tokens. The function B(.) denotes a single encoder block, consisting of spike"}, {"title": "4 Experiments", "content": "We first analyze the qualitative and quantitative results to assess how efficiently AT-SNN reduces tokens for the input images (Sec. 4.1). Then, we conduct a comparative analysis to evaluate how effectively AT-SNN reduces tokens in terms of accuracy, comparing it with existing methods, and analyze how the reduced tokens by AT-SNN impact energy consumption (Sec. 4.2). Finally, we discuss the properties required for AT-SNN's ACT and merge to efficiently process tokens through an ablation study (Sec. 4.3).\nImplementation details. We implement the simulation on Pytorch and SpikingJelly (Fang et al. 2023). All experiments in this section are conducted on SNN-based vision transformer following Spikformer or Spikingformer architectures. This section covers only the results of Spikformer; the results for Spikingformer are provided in the supplementary material. We first train the Spikformer during 310 epochs and retrain during 310 epochs based on the pre-trained model. We train the model on NVIDIA A6000 GPUs and use automatic-mixed precision (AMP) (Micikevicius et al. 2017) for training acceleration. We set T = 4, \u03b4\u03c1 = 10-3, \u03b1 = 5,\u03b2 = -10, and e = 0.01. For a fair comparison, we implemented several existing methods (e.g., TET and DT-SNN intially designed for CNN) on our target model, and these models are marked with an asterisk (*) in Tables 1.Additionally, to compare different cases, some methods were trained with a different number of timesteps than those used during inference; we marked these cases with a dagger (\u2020) in Table 1 to indicate the number of timesteps used during training. For example, even if the model is trained using only up to two timesteps during the training phase, it is possible to extend execution beyond two timesteps during inference. The accuracy is dependent on the timestep targeted during training. We evaluate our"}, {"title": "4.1 Analysis", "content": "Qualitative results. For visualization of AT-SNN, we use Spikformer-8-384 with eight blocks per timestep and y = 3, trained on TinyImageNet. Each input image contains 64 tokens (8 \u00d7 8). Fig. 5 displays the input image (odd-numbered columns) and a heatmap (even-numbered columns) showing the number of blocks each token passes through across all"}, {"title": "4.2 Comparison to prior art", "content": "We consider SNN methods based on both CNNs (e.g., VGG, CIFARNet, and ResNet) and Transformers (e.g., Spikformer and Spikingformer). Among the methods considered, except for TET, DT-SNN, STATIC (i.e., a Transformer model without any lightweight techniques), AT-SNN, and AT-SNN\u00ae, all are based on CNN models. For the methods based on CNN models, we used the performance data reported in the original papers without modification (Rathi et al. 2020; Wu et al. 2018; Zhang and Li 2020). We further explored AT-SNN, an extension of AT-SNN. AT-SNN\u00ae (AT-SNN with entropy) follows the DT-SNN approach, halting computation when the entropy of confidence scores exceeds a threshold."}, {"title": "4.3 Ablation Study", "content": "Two- vs one-dimensional halting. Fig. 9 compares the halting score accumulation methods on CIFAR-100: one that accumulates scores across two dimensions (both timestep and block-levels as per Eq. (4)) and another that accumulates only across one dimension (block-level only). As shown in Fig. 9, the two-dimensional halting mechanism achieves higher accuracy while removing more tokens compared to the one-dimensional halting. This is because, by definition, the LHS of Eq. (9) becomes larger under two-dimensional halting than under one-dimensional halting, which in turn increases the LHS of Eq. (10), leading to more tokens being halted. Moreover, two-dimensional halting achieves even higher accuracy than one-dimensional halting, a result that is related to the characteristics of spiking neurons discussed in the following paragraph.\nTemporal-awareness of merge and halting. In SNNs, spiking neurons accumulate inputs over multiple timesteps and fire when their value exceeds a certain threshold. We observed that temporally-aware masking (TAM), where the same neurons are masked across multiple timesteps, reduces accuracy less than random masking (RM). For instance, as illustrated in Fig. 10(a) and (b), consider i-th and j-th neurons receiving inputs of (0.3, 0.8) and (0.5, 0.6) over two timesteps. If we randomly mask two out of the four inputs, no neuron may fire, as in Fig. 10(a). However, by consistently masking only the j-th neuron, the i-th neuron can fire and propagate information, as shown in Fig. 10(b). To ex-"}, {"title": "5 Conclusion", "content": "In this paper, we introduced AT-SNN, a framework designed to dynamically adjust the number of tokens processed during inference in directly trained SNN-based ViTs, with the aim of optimizing power consumption. We extended ACT mechanism, traditionally applied to RNNs and ViTs, to selectively discard less informative spatial tokens in SNN-based ViTs. Furthermore, we proposed a token-merge mechanism based on token similarity, which effectively reduced the token count while enhancing accuracy. We implemented AT-SNN on Spikformer and demonstrated its effectiveness in achieving superior energy efficiency and accuracy on image classification tasks, including CIFAR-10, CIFAR-100, and TinyImageNet, compared to state-of-the-art methods.\nLimitation. As discussed in Fig. 8, AT-SNN aims to reduce the number of tokens involved in inference, and thus does not reduce energy consumption in the SPS. Reducing energy consumption in the SPS is a subject for future work."}]}