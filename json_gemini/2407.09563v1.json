{"title": "Psychology of Artificial Intelligence:\nEpistemological Markers of the Cognitive\nAnalysis of Neural Networks", "authors": ["Michael Pichat"], "abstract": "What is the \"nature\" of the cognitive processes and contents of an arti-\nficial neural network? In other words, how does an artificial intelligence\nfundamentally \"think,\" and in what form does its knowledge reside? The\npsychology of artificial intelligence, as predicted by Asimov (1950), aims\nto study this AI probing and explainability-sensitive matter. This study\nrequires a neuronal level of cognitive granularity, so as not to be limited\nsolely to the secondary macro-cognitive results (such as cognitive and cul-\ntural biases) of synthetic neural cognition. A prerequisite for examining\nthe latter is to clarify some epistemological milestones regarding the cog-\nnitive status we can attribute to its phenomenology.", "sections": [{"title": "What are the elementary cognitive building blocks\nof a neural network?", "content": "A layer of a neural network operates in a vector space whose dimensions can be\nassociated with epistemological characteristics specific to that layer.\nFrom a qualitative point of view, these dimensional characteristics can be\nlinguistic (phonemic, phonetic, morphological, syntactic, lexical, semantic, prag-\nmatic, etc.), visual (hue, saturation, luminosity, contrast, dimensional depth,\nshape, resolution, color depth, color spectrum, sharpness, noise, texture, con-\ntour, composition, scale, proportion, etc.), auditory (frequency, intensity, dura-\ntion, timbre, pitch, melody, rhythm, harmony, noise, etc.), logical, contextual\n(relative position, etc.), qualitative, quantitative, etc. But these characteristics\ncan also be of any other imaginable nature, whether \"human-like\" (i.e., charac-\nteristics referring to categories of thought and terms that humans possess) or\n\"alien-like\" (i.e., characteristics referring to categories of thought that humans\ndo not currently possess) (Bills, 2023).\nFrom a typological point of view, there can be no stable and exhaustive\nclassification of these dimensional characteristics. They are a direct function of\nthe nature of the data with which it is decided to feed a given neural network\nand the decreed modalities of its learning system, including the specific type of\nfeedback that will be administered to it in the case of (totally or partially) super-\nvised learning. Moreover, these characteristics are the immediate result of the\nspecific architecture allocated to the neural network, its number of parameters\nand their nature, and the nature of its constitutive mathematical operators.\nIn the sense of formal logic, these dimensional characteristics can be asso-\nciated with arguments and predicates (property, relation, transformation) or\ncombinations thereof. These synthetic categories of thought are contingent cog-\nnitive constructions and not ontological elements. They pertain to an infinity\nof different possible cognitive modalities for segmenting the world.\nWe will return to these last two points after further clarifying some episte-\nmological characteristics of synthetic neural cognition."}, {"title": "What is the \"nature\" of the cognitive activity\noperated by a neural layer?", "content": "The input vector space of a neural layer is expressed in dimensional character-\nistics specific to that layer. The embedding of an input to this layer is thus\nformatted in this singular vector space; just as the weight matrix constitutive\nof this layer is calibrated in this same vector space.\nLet us immediately avoid an elementary trap of cognitive anthropomorphism\nin the psychology of artificial intelligence by taking the case of a language model.\nA neural layer processing an incoming token does not reason about this token\nas such (like our human cognitive system, or at least our impression of it), does\nnot \"see\" this token, but performs mathematical processing on the embedding\nin which this token is encoded. The epistemological analysis of this mathemat-\nical processing teaches us a series of lessons about the \"nature\" of the neural\ncognitive activity that is de facto carried out by these mathematical operations.\nIn a given layer, the aggregation function of each formal neuron performs\na well-determined mathematical processing on the embedding of an incoming\ntoken. For a given neuron i, this processing is generated by the (horizontal)\nvector associated with it, which contains the weights $W_{i,j}$ specific to it. Each\nof these weights $W_{i,j}$ acts on one of the categorical dimensions j of the vector\nspace in which the incoming tokens are expressed (see figure 1).\nMore precisely, each of these weights $W_{i,j}$ will multiply a categorical dimen-\nsion j of the incoming vector space. This multiplication of the weight $W_{i,j}$\nthus performs a cognitive activity of determining the intensity of attentional\nfocus that neuron i will operate on this dimension j. In other words, a weight\nis an epistemological selector that decides the level of importance (none, low,\nmedium, high, total) to be given to a given dimensional characteristic. The re-\nsulting value of the performed multiplication indicates the (multiplicative) com-\nbination of two intensities: (i) the intensity of possession (X) by the incoming\ntoken of the categorical dimension j, weighted by the attentional intensity $W_{i,j}$\nto be given to this categorical possession intensity. This value thus expresses\nthe following information: what is the importance of the level of possession (by\nthe token) of the categorical dimension? As such, we can cognitively qualify\nit as an epistemological residue expressing a weighted level of possession of an\nepistemological category of world segmentation, for a given step vector space\n(of progressive categorical segmentation) (see Figure 2).\nThen, in the second step, all the products (attentional weight x categori-\ncal dimension) are added. This addition operationalizes de facto a weighted\nepistemological fusion activity of the levels of possession, by the input token, of\nthe categorical dimensions of the input vector space of the neural layer involved.\nThis epistemological fusion thus constructs a new dimension j' of representation\nof the input token, a new categorical segmentation, and a new, more abstract\ncategorical dimension. This cognitive fusion proceeds by a linear combination\nof all the epistemological residues of this initially formatted token in the dimen-\nsions specific to the input vector space j: each new token expression abstraction\nis thus elaborated by selective concatenation of all the initial categorical dimen-\nsional segments.\nSince the mentioned composition is additive, for a given neuron i, the more\nintensely a token possesses the different starting dimensions j on which this\nneuron attentively focuses (i.e., for which the weights $W_{i,j}$ of this neuron have\nhigh values), the more intensely the resulting new abstraction (the dimension\nj') will be possessed by the output embedding of this token. In other words,\nlike the union operator in fuzzy logic, artificial neural abstraction proceeds by\nselective epistemological concatenation: a new output abstraction being the\nchosen union of certain characteristics distributed in the starting feature space\n(of characteristics), the \"meaning\" of a strong possession of this new abstraction\nis thus that of the simultaneous intense possession of all the initial superimposed\ndimensions of which it is the result of selective composition.\nThus, in part, the progressive abstraction cognitive process operated by a\nneural network works this way: at the end of each neural processing layer, a\ntoken is re-expressed in a new output vector space, each new dimension being\nthe result of a \"selective reduction\" constitutive of this new dimension, of all the\ninput dimensions. Each of these new, more abstract dimensions is characterized\nby its own mode of selectively combining (i.e., in a weighted manner) all the\ninitial categorical dimensional segments of the starting vector space, that is, by\ngiving more or less importance to each of these dimensional categories of world\n(of tokens) segmentation.\nSuch is the cognitive activity carried by the matrix products performed by\nthe successive layers of a neural network: progressively projecting the embed-\ndings of incoming tokens into increasingly abstract vector spaces, each new level\nof output abstraction achieved by weighted epistemological fusion of the input\nabstractions. From layer to layer, the embeddings of tokens are thus reformat-\nted to express increasingly ethereal and fine-grained dimensions of abstraction\n(abstraction of abstraction, etc.)."}, {"title": "What is the cognitive function of successive neu-\nral layers?", "content": "The theory of conceptualization developed by Vergnaud (2016) offers us a valu-\nable framework for thinking about the cognitive function of formal neurons in\na neural network. According to the author, the primary form of knowledge is\nits operational one. The latter consists of knowledge-in-action, progressively\nfabricated through learning. This knowledge is constructed in the contingent\nexperience of having to act in response to environmental data. They are called\n\"in-action\" because their function is not theorization, formalization, or even\nexplanation. Their purpose is indeed primarily pragmatic: they are attached\nto classes of situations (in response to which they were constructed) and allow\nextracting, or rather associating, functional, operational characteristics whose\nconsideration is crucial for the efficiency of the (cognitive and behavioral) ac-\ntivity to be performed on these classes of situations.\nKnowledge-in-action first allows segmenting the continuous and formless flow\nof world information into elementary thought categories; thus selecting, or rather\ndecreeing, the (only) characteristics (taken from the world or rather imposed\non it) on which it is deemed pertinent to focus one's attention in order to con-\nsider \"what is important from a pragmatic point of view,\" \"what is a source\nof efficiency.\" These are what Vergnaud calls \"concepts-in-action\", which al-\nlow \"reading\" \"retaining\" or rather deciding to direct one's gaze on particular\ndimensions of the world (objects, properties, relations, etc.) that are central\nto achieving a given objective. As such, each characteristic dimension of the\nvector space on which a neuron operates, at the input of a neural layer, has\na cognitive function of concept-in-action. These dimensions, specific to each\nlayer, allow identifying the characteristics, analytical concepts, categorical ab-\nstractions, and thought categories in which it is functional to project, analyze,\nand categorize the incoming information from this layer, to retain functional\naspects relative to the task's purpose.\nKnowledge-in-action, Vergnaud tells us, also allows knowing how to coor-\ndinate the identified thought categories. And thus, combining them effectively\nwithin a coherent modeling of all the retained analytical criteria. This pro-\ncess is carried out by what the researcher calls theorems-in-action, which are\nlocal micro-theories, held to be true, relative to the rule by which concepts-in-\naction interact. As such, a neuron, or more precisely the weight vector defining\na neuron, has a cognitive function of theorem-in-action. This is because this\nweight-vector neuron is a propositional function (f(x,y,z,...)) allowing additive\nand weighted composition of the dimensions of the input vector space of a given\nneural layer; that is, selectively fusing some of these dimensions to cognitively\nact in a particular way on these specific dimensions. Each of these specific cog-\nnitive fusions (i.e., each of these neurons) has the effect, in terms of cognitive\nactivity, of generating a new dimension of a new vector space, the output vector\nspace of the layer involved.\nThe new categorical dimensions of the output vector space of a layer, which\nare de facto those of the input vector space of its successor layer, are in turn,\naimed to constitute even more functional concepts-in-action that will themselves\nthen be combined by the neural theorems-in-action of this successor layer into\neven newer, more abstract, and efficient concepts-in-action. This is the central\ncognitive function of a neural network within its consecutive layers: progres-\nsively achieving, through a series of steps of increasing conceptual abstractions,\na level of segmentation in a categorical vector space whose dimensions have\nbeen sufficiently selectively recombined, refined, and made relevant to opti-\nmally analyze, within the ultimate neural layer, the information received by\nthe synthetic neural system. The learning phase of a neural network aims to\nfabricate these successive analytical conceptual stages (i.e., this knowledge-in-\naction); and its functioning phase then aims to exploit, and apply this iterative\nconceptual knowledge to ultimately calculate the output (i.e., the output em-\nbedding) conceptual values (i.e., dimensional values) that will (ideally) produce\nthe expected cognitive response of this neural network."}, {"title": "Formal neurons do not decipher the properties\nof the world, they make them emerge by acting\ncognitively on it", "content": "Neural concepts-in-action do not epistemologically have an ontological status.\nThese categorical dimensions are not the result of an illusory \"decoding\" by a\nformal neural network of pseudo-intrinsic and pre-existing characteristics of a\nworld of properties that would be pre-given and that such a synthetic cognitive\nsystem would have the skill to discover, to reveal; and there is indeed cognitive\nskill, but one of enactive construction in the sense of Varela (1988) and not of\nrevelation in the sense of empirical realism.\nThe dimensional characteristics mobilized by a neural network are, as we\nhave already mentioned, a function of the architecture and nature of the pa-\nrameters assigned to it, the singular data with which it was decided to train it, as\nwell as the mathematical operators attributed to it for its learning and function-\ning. Neural concepts-in-action are thus embodied in the choices of mathematical\noperations and the structure of these operations that singularly presided over\nthe fabrication of each neural network. As Maturana (1978) and Von Foerster\n(2003) would say, the vector axes of a given neural layer are not the result of\n\"information transmission\"; they pertain not to an \"analog copy\" but to an\nauthentic \"digital reconstruction.\"\nThe strength of a neural network is thus not to highlight the per se true\nor good properties of the world's objects; that is, to fabricate representations\nthat would be faithful, adequate mirrors of inherent predicates of the world.\nBut, in a logic of systemic and constructivist circularity dear to Varela, to\nmake functional regularities (in this case, model parameters) emerge during the\n(cognitive) action of the latter, with its own cognitive-mathematical attributes,\non the world's objects. These regularities translate into a bilateral coupling, a\nmutual shaping, a structural articulation between the world and the synthetic\ncognitive system that operates its specific thought operations on it."}, {"title": "Extensions", "content": "We have here limited ourselves to an epistemological reflection concerning the\naggregation functions of neural networks. Neglecting for the moment the equally\ncentral subjects of (i) activation functions and their topological effects, (ii) at-\ntention heads and their contextual impacts, and (iii) different types of neural\narchitectures. These subjects also need to be questioned in an epistemological\ndynamic.\nMore broadly, from a perspective of neuro-symbolic hybridization (Alshm-\nrany, 2024; Sun, 2024) of our artificial intelligence technologies, it seems neces-\nsary to question the status of a possible direct articulation, at low granularity, of\nsynthetically generated neural knowledge-in-action and formal human symbolic\nknowledge. Coordination of formal and empirical knowledge, allowing them to\nmutually regulate and enrich each other from a developmental perspective as\nmentioned by Vygotsky (1934). Coordination appearing as one of the prereq-\nuisites, among others from neuroscience (Minsky, 1988), for the next, radically\nmore advanced generation of artificial intelligence systems."}]}