{"title": "Learning Dynamics of LLM Finetuning", "authors": ["Yi Ren", "Danica J. Sutherland"], "abstract": "Learning dynamics, which describes how the learning of specific training examples influences the model's prediction of other examples, give us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during finetuning, by analyzing the step-wise decomposition and accumulated influence among different responses. Our framework allows a uniform interpretation of many interesting observations about the training of popular algorithms for both instruction tuning and preference tuning. The analysis not only explains where the benefits of these methods come from but also inspires a simple, effective method to further improve the alignment performance. Code for experiments is available at https://github.com/Joshua-Ren/Learning_dynamics_LLM.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks usually acquire new knowledge by updating their parameters via gradient descent (GD). This procedure can be described by learning dynamics, which links changes in the model's predictions to the gradients generated by learning from specific examples. With the help of learning dynamics, researchers have not only explained many interesting phenomena during training, but used these insights to propose novel, improved algorithms (Pruthi et al. 2020; Ren, Guo, et al. 2023; Xia et al. 2024).\nThe study of large language models (LLM) is gaining popularity due to their surprising capabilities on various tasks. To ensure the LLMs follow human instructions and align well with human values, finetuning has attracted much recent attention. Practitioners often start with instruction tuning, where the model learns extra knowledge necessary for the downstream task (such as instruction-following), and then preference tuning, where the model aligns its outputs to human preference (Ouyang et al. 2022). Various finetuning algorithms have been proposed to fit into this pipeline, with differing explanations as to why they improve the model's performance.\nDifferent from most existing analyses of LLM finetuning, which use the perspective of their training targets, their status at the end of training, or their relationships to reinforcement learning (e.g. Ji et al. 2024; Rafailov et al. 2024; Tajwar et al. 2024), this paper tries to understand LLMs' evolution from a dynamical perspective. Specifically, we formalize the learning dynamics of LLMs' finetuning using a similar decomposition of gradient updates to that of Ren et al. (2022). Surprisingly, this framework can be easily adapted to various finetuning algorithms with different goals, including supervised finetuning (SFT, Wei et al. 2022), self-play finetuning (SPIN, Z. Chen et al. 2024), direct preference optimization (DPO, Rafailov et al. 2023), and their variants. This framework has the potential to explain several interesting and counter-intuitive observations during training including the \"repeater\" phenomenon after preference tuning (Holtzman et al. 2020), hallucination\u00b9 (Huang et al. 2023), the decay in confidence of all responses during off-policy DPO (Rafailov et al. 2024), and more.\nMoreover, we also provide a new perspective on understanding why off-policy DPO and other variants underperform their on-policy counterpart (Guo, B. Zhang, et al. 2024). Our explanation starts by observing an interesting \"squeezing effect,\" which we demonstrate is a consequence of gradient ascent (as in DPO and similar algorithms) on models with cross-entropy loss following a softmax layer. In short, for each token's prediction, the negative gradient will push down the model's predictions on (almost) all possible output labels, moving this"}, {"title": "2 Background and Related Works", "content": ""}, {"title": "2.1 Learning dynamics of deep learning", "content": "When studying a practical machine learning algorithm, we usually care about how the learned model $f_\\theta$ is influenced by different factors, e.g., the model's parameter $\\theta$, the training dataset, the learning hyperparameters, etc. Broadly speaking, \u201clearning dynamics\u201d is usually used as an umbrella term to describe how the change of a specific factor influences the model's prediction. In this paper, we narrow down this term to describe \"how the change in $\\theta$ influences the corresponding change in $f_\\theta$\", i.e., the relationship between $\\Delta \\theta$ and $\\Delta f_\\theta$.\nFor a simple model like $f_{a,b}(x) = ax^2 + b$, it is straightforward to directly manipulate $\\Delta \\theta$ and observe the resulting $\\Delta f_\\theta$ accordingly. However, for a deep neural network, such an observation is impractical for many reasons. First, a deep model usually contains an enormous number of parameters, individual parameters are usually very difficult to interpret. Second, the input and output spaces of the model $f_\\theta(x)$ are usually high-dimensional, making it hard to observe $\\Delta f_\\theta$ directly. Combining the fact that modern deep models are usually trained via different variants of stochastic gradient descent (SGD), where all parameters change simultaneously in each update, we usually use the following definition to study learning dynamics:\n$$\\Delta \\theta \\triangleq \\theta_{t+1} - \\theta_t = -\\eta \\cdot \\nabla L(f_{\\theta_t}(x_u), y_u);$$ \n$$\\Delta f_\\theta(x) \\triangleq f_{\\theta_{t+1}}(x_o)- f_{\\theta_t}(x_o).$$\nHere the update of $\\theta$ during step $t\\rightarrow t + 1$ is given by learning the sample $x_u$ using SGD with learning rate $\\eta$. In short, the learning dynamics in this paper address the question:\nAfter an SGD update on $x_u$, how does the model's prediction on $x$ change?"}, {"title": "2.2 Challenges of Analyzing the Learning Dynamics of LLM's Finetuning", "content": "The first conundrum for analyzing the learning dynamics of LLM's finetuning is the high dimensionality and the sequence nature of both the input and output signals. The high-dimensional property makes it hard to observe the model's output, and the sequence nature makes the distributions on different tokens mutually dependent,"}, {"title": "3 Learning Dynamics under Per-step and Accumulated Perspectives", "content": "We first consider a standard supervised learning problem, where the model takes a high-dimensional input x and generates a sequence response $y = {Y_1,\\dots, Y_L } \\in V^L$, where V is the vocabulary of size V. We assume we actually define a distribution over y, i.e., $\\pi_\\theta(y | x)$. To get this probabilistic distribution, the model first generates a logits matrix $z = h_\\theta(x) \\in \\mathbb{R}^{V\\times L}$ and then takes the Softmax operation on each column. To make the analysis for the LLM case easier, we track the change of $\\log \\pi_\\theta$ instead of $\\pi_{\\theta_t}$, slightly different from the results of Ren et al. (2022).\nPer-step influence decomposition. The learning dynamics of (1) become\n$$\\Delta \\theta \\triangleq \\theta_{t+1} - \\theta_t = -\\eta \\cdot \\nabla L (\\pi_{\\theta_t}(y | x_u), y_u)$$\n$$\\triangle \\log \\pi^\\theta(y | x_o) \\triangleq \\log \\pi_{\\theta_{t+1}} (y | x_o) - \\log \\pi_{\\theta_t} (y | x_o),$$\nwhere $y_u \\in \\mathbb{R}^V$ (likely one-hot) is the supervisory signal. To get a better intuition, we start from the L = 1 scenario, where the $\\Delta \\theta$ and $\\Delta \\pi$ can be linked by the following proposition.\nProposition 1 (Similar to Proposition 1 of Ren et al. 2022). Let $\\pi = Softmax(z)$ and $z = h_\\theta(x)$. The one-step learning dynamics decompose as\n$$\\Delta \\log \\pi^\\theta (y | x) = -\\eta A^t (x_o) K^t (x_o, x_u) G^t (x_u, y_u) + O(\\eta^2 ||\\nabla_\\theta z(x)||_p),$$\nwhere $A^t (x) = \\nabla_z \\log \\pi_{\\theta_t} (x) = I - \\mathbb{1} \\pi^\\theta_t (x_o), K^t (x_o, x_u) = (\\nabla_\\theta z(x_o)|_{\\theta_t}) (\\nabla_\\theta z(x_u)|_{\\theta_t})^\\top$ is the empirical neural tangent kernel of the logit network and $G^t (x, y) = \\nabla_z \\mathcal{L}(x, y)|_{z_t}$, which for cross-entropy loss is $\\pi_{\\theta_t} (y | x_u)-y_u$\nThe proof and more discussion can be found in Appendix A.\nIn this decomposition, $A^t (x) = I - \\mathbb{1} \\pi^\\theta_t (x)$, which only depends on the model's predicted probability at time t. Intuitively, for any length-V vector b, left-multiplying this $V \\times V$ matrix $A^t$ centers the vector with respect to the probability distribution $\\pi_{\\theta_t} (x)$, because $A^t b = (I - \\mathbb{1} \\pi^\\top_t)b = b - \\mathbb{1}(\\pi^\\top_t b)$, and hence the mean of $A^t b$ under $\\pi$ is $b = 0$.\nThe matrix Kt is the empirical neural tangent kernel (eNTK, Jacot et al. 2018; J. Lee et al. 2019) of the model, i.e., the product of the model's gradients with respect to x, and xu. Since the eNTK usually changes slowly (or is even nearly invariant) when finetuning with a small learning rate (Jacot et al. 2018; Arora et al. 2019; Fort et al. 2020; Ren, Guo, et al. 2023), we can treat Kt as a relatively stable and model-specific similarity measurement between different input samples: larger $||K^t||$ means the update of xu can influence model's prediction on x more. The term Gt is determined by the loss function L, which usually provides the energy and direction for the model's adaptation. For example, consider a cross-entropy loss $\\mathcal{L}_{CE} = -y_u \\log \\pi (y | x_u)$. Then we have $G^{CE} = \\pi_{\\theta_t} (y | x_u) - y_u$, a length-V vector that points from the model's current predictive distribution to the desired supervisory distribution. When considering one-hot labels, the above term can also be written as $G^{CE} = \\pi_{\\theta_t} (y | x_u) - e_{y_u}$, which is more common in practice.\nFor a multi-label classification problem where L > 1, to this first-order approximation, we only need to calculate L different $\\Delta log \\pi^\\theta$ and stack them together."}, {"title": "4 Learning Dynamics of LLM Supervised Finetuning", "content": "Per-step decomposition of the SFT loss. We are now ready to tackle the supervised finetuning (SFT) of an LLM. The typical loss function used in this stage is the negative log-likelihood (NLL) of a given completion $y = (y_1,\\dots,y_L) \\in V^L$, conditioned on the prompt $x_u$:\n$$\\mathcal{L}_{SFT}(x, y) \\triangleq - \\sum_{l=1}^{L} \\log \\pi^\\theta (y_l = y_l | y_{1:l-1}, x_u) = - \\sum_{l=1}^{L} y_l \\log (y | x_u, y_{1:l-1}).$$\nNote that compared with the multi-label classification problem discussed before, where the joint distribution of all labels can be factorized as $\\pi(y | x) = \\Pi_l \\pi(y_l | x)$, the sequential nature of language modeling makes the analysis more complicated, because we must have $\\pi(y | x) = \\Pi_l \\pi(y_l | x, Y_{1:l-1})$. Note that this is the correct form, and not one conditioning on the previous model predictions, due to the \"teacher forcing\" scheme typically used in training sequence models. To solve this problem, we can merge this factorization into the definition of the backbone $h_\\theta$ while keeping the decomposition format of Proposition 1. Specifically, define $x = [x; y]$. Then"}, {"title": "4.1 Response Space and the Probing Dataset", "content": "Besides the sequential nature of the loss function, another conundrum in analyzing LLM learning dynamics is the huge response space Y: the number of possible $y \\in Y$ is $V^L$, but the vast majority of possible sequences look nothing like natural language, and we expect the model to generate only a subset of natural language-like responses. These properties prevent us from observing the changes of all possible y like what we did for MNIST. Instead, we define several interesting regions of Y, and select corresponding typical responses to observe. Intuitively, we can use the semantic relevance between y and xu as a heuristic. Such a measurement can be understood as \u201chow suitable this y is as a response to xu, compared to y.\u201d Then, starting from the structure of common preference optimization datasets such as Antropic-HH (Y. Bai et al. 2022) and UltraFeedback (Cui et al. 2023), we can roughly divide Y into three sub-spaces and propose seven typical responses to evaluate (as in Figure 2):\n*   $\\mathcal{Y}_{IF}$: reasonable responses following the instruction xu:\n    1.  $y_t$, the chosen (i.e., the preferred) response to xu.\n    2.  $y_\\bar{t}$, the rejected (i.e., the less preferred, but still reasonable) response to xu.\n*   $\\mathcal{Y}_{non-IF}$: irrelevant responses to xu that are still recognizably human language (in these datasets, roughly \"internet-standard\" English):\n    3.  $y^\\shortmid_{j\\neq u}$, the chosen response for a different question xj!=u selected from the training set.\n    4.  $y^\\shortmid_{test}$, the chosen response of a question $x_{test}$ selected from the test set.\n    5.  $y_{hum}$, a \"random\" English sentence generated by GPT4 with as many words as $y_t$.\n*   $\\mathcal{Y}_{non-hum}$: token sequences that do not form meaningful human language:\n    6.  $y_{rnd}$, a random permutation of the words (space-separated strings) of $y_t$\n    7.  $y'_{rnd}$, a random permutation of the words of a generated sentence as in $y_{hum}$"}, {"title": "4.2 Experimental verification", "content": "We now verify our analysis of the learning dynamics using experiments. We first create an SFT training set by randomly selecting 5000 examples from the training split of Antropic-HH; we give similar results for UltraFeedback in Appendix B.2. The model will be finetuned using these $[x; y^+] \\in D_{train}$ for several epochs. To observe the learning dynamics, we create a probing dataset by first selecting 500 prompts from Dtrain and then generate seven different responses for each x following Figure 2. We evaluate the log-likelihood of these responses every 25 updates (with a training batch size of 4, the probing occurs every 100 examples). The model is finetuned for 8 epochs, where we call the first several epochs from $\u03c0_\\theta_0$ to $\u03c0_\\theta_{fit}$ fitting stage and the last several epochs from $\u03c0_\\theta_{fit}$ to $\u03c0_\\theta_{OT}$ overfitting stage. We verify the generalizability of our findings by using six models: pythia-410M/1B/1.4B/2.8B (Biderman et al. 2023) and Qwen1.5-0.5B/1.8B (J. Bai et al. 2023).\nWe emphasize beforehand that while the behaviors seen in this section are perhaps not particularly surprising, they validate that our learning dynamics explanations can explain practical behavior in LLM finetuning, and that these techniques will yield a surprising \"squeezing\" effect when applying them to preference finetuning in the next section.\nBehaviors of y \u2208 YIF. As illustrated in the first panel in Figure 3, in the fitting stage, we see SFT \"pulls up\" the curve of this region. That is because their $K^t([x, y], [x_u, y_t])$ are usually large enough, which is similar to the pairing effect of 4 and 9 in the MNIST experiment. As the training goes on, the model enters into the overfitting stage, where $\u03c0_\\theta_{OT}(y_\\bar{t})$ starts to plateau and then decrease while $\u03c0_\\theta_{OT}(y^+_t)$ keeps increasing. That is because the energy provided by residual term G_SFT always points to $y_t$ while deviating from other y. As the training goes on and the energy in the main direction decreases (because the model's confidence in the target increases, which makes the gap between the current prediction and the one-hot supervisions becomes smaller), the deviating energy will gradually dominate and \"push down\" those regions.\nBehaviors of y \u2208 Ynon-hum. The learning dynamics for this region is simpler: the predictions start from a very low log-likelihood and keep decreasing throughout training, as illustrated in the last two panels in Figure 2 and the second panel in Figure 3. The responses in this region play a similar role to the 0 examples in the MNIST experiment: as they usually look dissimilar to 4, hence the norms of the corresponding $K^t$ are small, which leads to only tiny pressure \"upwards\" when the model learns. On the other hand, as the responses of all possible y given xu must sum to one, increasing of $\u03c0_\\theta_t(y)$ on other groups must naturally \u201cpush down\u201d the curve in this region. Another interesting finding is that $\u03c0_\\theta_t(y_rnd) $ is always bigger than $\u03c0_\\theta_t(y'_{rnd})$, where the latter contains all the words from yt. That is because learning the chosen response increases the likelihood of specific phrases, e.g., $[y_1, y_2]$. Then, in $\u03c0_\\theta_t(y'_{rnd})$, if $y_1$ is followed by $y_3 \u2260 y_2$, the resulting prediction would be very small.\nBehaviors of y \u2208 Ynon-IF. This group is the most complex one because first, we must consider the influence coming from the update of another training example, e.g., $[x_{j\\neq u};y^\\shortmid_{j\\neq u}]$. Furthermore, although the responses in this group are irrelevant to xu, they are reasonable human language and the model will assign non-negligible probability mass to them. That means if we want to sample a response from the model given xu (e.g., when deploying the model or doing on-policy training), the probability that the model provides a response from (or"}, {"title": "5 Learning Dynamics of LLM Preference Finetuning", "content": "Instruction tuning as in the SFT stage above improves the model's instruction-following ability. In order to better align LLM's output with human preferences, typically instruction tuning is followed by preference tuning (Ouyang et al. 2022). The first widespread preference tuning method comes from RLHF (reinforcement learning with human feedback (Christiano et al. 2017)), which requires substantial expert annotation of LLM responses. RLAIF (H. Lee et al. 2023) offload this hard work by collecting preference signals from AI, but a large reward model is still required. To avoid this and mitigate the high-variance nature of the RL-based training, Rafailov et al. (2023) propose DPO (direct preference optimization, an RL-free method), and show it has the same optimization target as RLHF under the Bradley-Terry reward model (R. A. Bradley and Terry 1952). RL-free preference-tuning methods like DPO and variants such as IPO (Azar et al. 2024), SLiC (Zhao et al. 2023), KTO (Ethayarajh et al. 2024), and so on have gained popularity due to their good performance and robustness during training. Significant effort (e.g. Ji et al. 2024; Pal et al. 2024; Rafailov et al. 2024; Tajwar et al. 2024) has gone to theoretical explanations of the equivalence and differences between them and the RL-based methods. This"}, {"title": "5.1 Learning Dynamics of RL-free Preference Tuning: DPO as an Example", "content": "We start from the off-policy DPO with the following loss function:\n$$\\mathcal{L}_{DPO}(\\theta) = - \\sum_{(x_u, y^\\shortmid_t, y_\\bar{t})} \\log \\sigma (\\beta \\log \\frac{\\pi_\\theta (y^\\shortmid_t | x_u)}{\\pi_{ref}(y^\\shortmid_t | x_u)} - \\beta \\log \\frac{\\pi_\\theta (y_\\bar{t} | x_u)}{\\pi_{ref}(y_\\bar{t} | x_u)}),$$\nwhere $y^\\shortmid_t$ and $y_\\bar{t}$ are pre-generated responses (both in YIF), and $\u03c0_{ref}$ is the reference model, typically the result of SFT. In the loss function, the $\\pi_{\\theta_t}$ terms are also calculated using the \"teacher forcing\" mechanism, which is identical to the SFT case. Hence we decompose the learning dynamics for DPO similarly to Equation (6):\n$$\\[\\triangle \\log \\pi^\\theta (y | x_o)]_m = \\sum_{l=1}^{L} \\eta [A^t (x_o)]_m [K^t (x_o, x_u)]_l [G^t_{DPO} (x_u, y^\\shortmid_t, y_\\bar{t})]_l + O(\\eta^2)$$\n$$G_{DPO} \\approx \\beta (1 - \\alpha) (y_\\bar{t} \u2013 y_\\shortmid_t); \\alpha = \\sigma (\\beta \\log \\frac{\\pi_\\theta (y^\\shortmid_t | x_u)}{\\pi_{ref}(y^\\shortmid_t | x_u)} - \\beta \\log \\frac{\\pi_\\theta (y_\\bar{t} | x_u)}{\\pi_{ref}(y_\\bar{t} | x_u)}) \\in R.$$\nwhere $K^t (x_o, x_u)$ is the equivalent eNTK between the updating sequence $[x_u; y^\\shortmid_t, y_\\bar{t}]$ and the observing sequence $[x_o; y_o]$. The derivation, which is similar to before, and the $G^t$ functions for other RL-free methods are given in Appendix A.2.2."}, {"title": "5.2 Comparison of SFT, DPO, and other Variants", "content": "By comparing the learning dynamics of SFT and DPO, we find they have identical $A^t$ and similar $K^t$. The main difference lies in $G^t$, which determines the direction and strength of the evolution of $\u03c0_\\theta_t$. Recall the learning dynamics of SFT in the first panel of Figure 4, the pressures controlled by G^t contain a big positive vector on $y_t$ and many small negative vectors on other y, where the positive vector will gradually diminish with the increase of $\u03c0_\\theta_t (y_t | x_u)$ during training.\nThe DPO loss, on the other hand, consistently generates a pair of vectors (a positive $y_t$ and a negative $y_\\bar{t}$) regardless of the predictions of any policy network\u00b2. However, as illustrated in Equation (10), the norm of $G_{DPO}$ is controlled by a scalar $\u03b2(1 \u2013 \u03b1)$, which is influenced by both the current policy $\u03c0_\\theta_t$ and the reference policy $\u03c0_{ref}$. We first analyze the role of \u03b1, which is controlled by the margin (i.e., the value inside the parentheses of \u03c3(\u00b7)) that represents how well the current policy separates $y_t$ and $y_\\bar{t}$ compared with the reference policy. Due to the monotonicity of \u03c3(\u00b7), a larger margin leads to larger \u03b1, which in turn restrains the strength of $G_{DPO}$. In other words, $G_{DPO}$ automatically provides less energy on the examples that are already well separated. We then check the role of \u03b2, which controls the regularizing effect on the KL distance between $\u03c0_\\theta_t$ and $\u03c0_{ref}$ in the original RL loss (Rafailov et al. 2023). When the margin is smaller than zero, larger \u03b2 leads to a smaller \u03b1 and hence provides stronger $G_{DPO}$ for the model to \u201ccatch up\u201d the separating ability of the reference model faster. But when the model is good enough (the margin is positive), increasing \u03b2 will increase \u03b1 and hence create a negative influence on $\u03b2(1 \u2212 \u03b1)$, which makes the model update less. This behavior aligns well with the claims of Rafailov et al. (2023): the stronger regularizing effect tends to \u201cdrag $\u03c0_\\theta$ back towards $\u03c0_{ref}$\u201d when it deviates from $\u03c0_{ref}$ too much in terms of the separating capability.\nNote that the analyses above don't make any assumptions on where $y_t$ and $y_\\bar{t}$ come from. Hence our framework can also be extended to on-policy RL-free algorithms, which often perform better than their off-policy counterparts (Guo, B. Zhang, et al. 2024; Tajwar et al. 2024). By definition, the main difference between off-policy and on-policy algorithms is how the supervisory responses are generated. Off-policy methods typically use a fixed pre-collected dataset, where $y_t$ and $y_\\bar{t}$ are usually generated by another LLM or humans. In other words, it is likely that both the chosen and rejected responses come from the \"less likely\" region of the model's"}, {"title": "5.3 The Squeezing Effect of the Negative Gradient", "content": "The existence of negative gradients (the $y_\\bar{t}$ term in Equation (10)) is the key to understanding the difference of learning dynamics between SFT and DPO. We find this negative gradient will impose a non-trivial \"squeezing effect\" on any models outputting the probabilistic distribution using Softmax output heads, even in a simple multi-class logistic regression task. Specifically, consider the L = 1 case and assume our algorithm imposes a negative gradient on label \u1ef9, the model's predictive distribution $\u03c0_{\\theta_{t+1}}$ will change as follows:\n*   The negative gradient will reliably decrease the confidence of \u1ef9, i.e., $\u03c0_{\\theta_{t+1}} (y = \\bar{y})$ is guaranteed to decrease.\n*   The decreased probability mass is \"squeezed\" into the most confident dimension before the update, i.e., $\u03c0_{\\theta_{t+1}} (y = y^*)$, where $y^* = argmax_{i\\in [V]\\backslash{\\{\\bar{y}\\}}} \u03c0_{\\theta_{t}} (y = i)$ is guaranteed to increase.\n*   The rich get richer and the poor get poorer: generally, dimensions with high $\u03c0_{\\theta_t}$ tend to increase, and those with low $\u03c0_{\\theta_t}$ tend to decrease.\n*   Peakier $\u03c0_{\\theta_t}$ suffer a more serious squeezing effect. If the probability mass concentrates on few dimensions in $\u03c0_{\\theta_t}$, which is common for a pretrained model, all $\u03c0_{\\theta_{t+1}}(y \\neq y^*)$ decrease (only $y^*$ is considered \u201crich\u201d).\n*   Smaller $\u03c0_{\\theta_t} (\\bar{y})$ make the squeezing effect stronger. If \u1ef9 is unlikely under $\u03c0_{\\theta_t}$, the probability mass of all other $\u03c0_{\\theta_{t+1}}(y \\neq y^*)$ will be more seriously decreased, and the $\u03c0_{\\theta_{t+1}} (y = y^*)$ increases more. That is what we observed in the off-policy DPO scenario.\nAppendix C proves these observations analytically for linear models, by directly computing $\u03c0_{\\theta_{t+1}}/\u03c0_{\\theta_t}$ in different situations. Intuitively, recall that the cross-entropy loss for a one-hot label $e_i$ is just $\u2013 \\log \u03c0_i = \u2013 z_i + \\log \\sum_j \\exp(z_j)$, which has gradient $-e_i + \u03c0$. Suppose there is one $k \\neq i$ such that $\u03c0_k$ is rather large, say 0.8. Then gradient ascent will decrease $z_i$, but increase $z_k$ almost as much. Since $\u03c0_k$ is dominant, increasing its logit results in a large absolute increase to $\u03c0_k$, which necessarily reduces all other probabilities to account for it. In the next subsection, we will verify these claims by experiments on real LLM finetuning problems. With the help of these analyses, we figure out why the model's confidence in all responses decreases during off-policy DPO training and successfully find where the decreased probability mass has gone."}, {"title": "5.4 Experimental Analysis of DPO's Learning Dynamics", "content": "To verify our framework also explains the learning dynamics of preference tuning well, we conduct similar experiments for DPO as we do for SFT. Recall the residual term $G_{DPO}$ introduces a positive arrow on $y_t$, which means the learning tries to \u201cpull up\u201d the model's prediction on $y_t$. To verify this, we create two types of rephrases of y using GPT4 and call them $y_{gpts}$ and $y_{gptf}$ (the first tend to keep the semantics while the latter tends to keep the format of y; details in Figure 6). See the three curves in the first panel in Figure 5,"}, {"title": "6 Inspirations from Learning Dynamics", "content": "Explaining the success of various related algorithms. The proposed learning dynamics explain many interesting observations during SFT and DPO; this perspective also helps explain the success of many practical algorithms. For example, the literature mostly credits on-policy RL-free methods with better aligning the current policy's distribution to the reward model (Tajwar et al. 2024), but we can extend this explanation from the learning dynamics perspective. As illustrated in the third panel in Figures 4 and 18, we see $y_t$ is sampled from the current $\u03c0_\\theta_t$, which is less likely to be in a valley. Hence the squeezing effect introduced via the big negative gradient is reduced. The benefits of SPIN (Z. Chen et al. 2024), which is inspired by treating the LLM itself as a GAN-style system (Goodfellow et al. 2014), can also be explained similarly: as the negative response y always comes from the model's policy in the previous generation, its $\u03c0_\\theta_t$ is less likely to be very"}, {"title": "7 Conclusion", "content": "Learning dynamics, which depicts how the model's prediction changes when it learns new examples, provide a powerful tool to analyze the behavior of models trained with gradient descent. To better use this tool in the context of LLM finetuning, we first derive the step-wise decomposition of LLM finetuning for various common algorithms. Then, we propose a unified framework for understanding LLM predictions' behaviors across different finetuning methods. The proposed analysis successfully explains various phenomena during LLM's instruction tuning and preference tuning, some of them are quite counter-intuitive. We also shed light on how specific hallucinations are introduced in the SFT stage, as previously observed (Gekhman et al. 2024), and where the improvements of some new RL-free algorithms come from compared with the default off-policy DPO. Finally, inspired by this analysis, we propose a simple (but counter-intuitive) method that is effective in improving the alignment of models."}, {"title": "A Proof of Propositions and Residual Term for Different Losses", "content": ""}, {"title": "A.1 Proof of Proposition 1", "content": "Proposition 1 (Similar to Proposition 1 of Ren et al. 2022). Let $\\pi = Softmax(z)$ and $z = h_\\theta(x)$. The one-step learning dynamics decompose as\n$$\\Delta \\log \\pi^\\theta (y | x) = -\\eta A^t (x_o) K^t (x_o", "observing example\" xo. Starting from Equation (3), we first approximate log $\\pi_{\\theta_{t+1}} (y | x_o)$ using first-order Taylor expansion (we use it to represent \u03c0\u03b8\u03b9 here for notation conciseness)": "n$$\\log \\pi^{t+1} (y | x) = \\log \\pi^t (y | x_o) + (\\nabla \\log \\pi^t (y | x_o), \\theta_{t+1} \u2013 \\theta_t) + O(||\\theta_{t+1} \u2013 \\theta_t||^2).$$\nThen, assuming the model updates its parameters"}]}