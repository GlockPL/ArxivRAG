{"title": "Learning Dynamics of LLM Finetuning", "authors": ["Yi Ren", "Danica J. Sutherland"], "abstract": "Learning dynamics, which describes how the learning of specific training examples influences the model's prediction of other examples, give us a powerful tool for understanding the behavior of deep learning systems. We study the learning dynamics of large language models during finetuning, by analyzing the step-wise decomposition and accumulated influence among different responses. Our framework allows a uniform interpretation of many interesting observations about the training of popular algorithms for both instruction tuning and preference tuning. The analysis not only explains where the benefits of these methods come from but also inspires a simple, effective method to further improve the alignment performance. Code for experiments is available at https://github.com/Joshua-Ren/Learning_dynamics_LLM.", "sections": [{"title": "1 Introduction", "content": "Deep neural networks usually acquire new knowledge by updating their parameters via gradient descent (GD). This procedure can be described by learning dynamics, which links changes in the model's predictions to the gradients generated by learning from specific examples. With the help of learning dynamics, researchers have not only explained many interesting phenomena during training, but used these insights to propose novel, improved algorithms (Pruthi et al. 2020; Ren, Guo, et al. 2023; Xia et al. 2024).\nThe study of large language models (LLM) is gaining popularity due to their surprising capabilities on various tasks. To ensure the LLMs follow human instructions and align well with human values, finetuning has attracted much recent attention. Practitioners often start with instruction tuning, where the model learns extra knowledge necessary for the downstream task (such as instruction-following), and then preference tuning, where the model aligns its outputs to human preference (Ouyang et al. 2022). Various finetuning algorithms have been proposed to fit into this pipeline, with differing explanations as to why they improve the model's performance.\nDifferent from most existing analyses of LLM finetuning, which use the perspective of their training targets, their status at the end of training, or their relationships to reinforcement learning (e.g. Ji et al. 2024; Rafailov et al. 2024; Tajwar et al. 2024), this paper tries to understand LLMs' evolution from a dynamical perspective. Specifically, we formalize the learning dynamics of LLMs' finetuning using a similar decomposition of gradient updates to that of Ren et al. (2022). Surprisingly, this framework can be easily adapted to various finetuning algorithms with different goals, including supervised finetuning (SFT, Wei et al. 2022), self-play finetuning (SPIN, Z. Chen et al. 2024), direct preference optimization (DPO, Rafailov et al. 2023), and their variants. This framework has the potential to explain several interesting and counter-intuitive observations during training including the \"repeater\" phenomenon after preference tuning (Holtzman et al. 2020), hallucination\u00b9 (Huang et al. 2023), the decay in confidence of all responses during off-policy DPO (Rafailov et al. 2024), and more.\nMoreover, we also provide a new perspective on understanding why off-policy DPO and other variants underperform their on-policy counterpart (Guo, B. Zhang, et al. 2024). Our explanation starts by observing an interesting \"squeezing effect,\" which we demonstrate is a consequence of gradient ascent (as in DPO and similar algorithms) on models with cross-entropy loss following a softmax layer. In short, for each token's prediction, the negative gradient will push down the model's predictions on (almost) all possible output labels, moving this"}, {"title": "2 Background and Related Works", "content": ""}, {"title": "2.1 Learning dynamics of deep learning", "content": "When studying a practical machine learning algorithm, we usually care about how the learned model $f_\\theta$ is influenced by different factors, e.g., the model's parameter $\\theta$, the training dataset, the learning hyperparameters, etc. Broadly speaking, \u201clearning dynamics\u201d is usually used as an umbrella term to describe how the change of a specific factor influences the model's prediction. In this paper, we narrow down this term to describe \"how the change in $\\theta$ influences the corresponding change in $f_\\theta$\", i.e., the relationship between $\\Delta \\theta$ and $\\Delta f_\\theta$.\nFor a simple model like $f_{a,b}(x) = ax^2 + b$, it is straightforward to directly manipulate $\\Delta \\theta$ and observe the resulting $\\Delta f_\\theta$ accordingly. However, for a deep neural network, such an observation is impractical for many reasons. First, a deep model usually contains an enormous number of parameters, individual parameters are usually very difficult to interpret. Second, the input and output spaces of the model $f_\\theta(x)$ are usually high-dimensional, making it hard to observe $\\Delta f_\\theta$ directly. Combining the fact that modern deep models are usually trained via different variants of stochastic gradient descent (SGD), where all parameters change simultaneously in each update, we usually use the following definition to study learning dynamics:\n$$\\Delta \\theta \\triangleq \\theta_{t+1} - \\theta_t = -\\eta \\cdot \\nabla L(f_{\\theta_t}(x_u), y_u);\\qquad \\Delta f_\\theta(x) \\triangleq f_{\\theta_{t+1}}(x_o) - f_{\\theta_t}(x_o).$$\nHere the update of $\\theta$ during step $t \\rightarrow t+1$ is given by learning the sample $x_u$ using SGD with learning rate $\\eta$. In short, the learning dynamics in this paper address the question:\nAfter an SGD update on $x_u$, how does the model's prediction on $x$ change?\nStudying the learning dynamics defined above can shed light on many important problems in deep learning and also help to understand various counter-intuitive phenomena. Here are a few.\nUnderstanding generalization. If we consider $x_u$ from the training set, and $x$ from the test set, this form of learning dynamics addresses generalization: the model generalizes better if the loss of $f_\\theta(x)$ keeps decreasing when it learns from $x_u$. By studying the influence of different $x_u$ at different stages during supervised learning, Ren et al. (2022) explain a \"zigzag\" pattern of the learning path, which sheds light on why the model can spontaneously pursue better supervisory signals and correct noisy labels in the early stage of training (see also S. Liu et al. 2020). Kumar et al. (2022) and Ren, Guo, et al. (2023) apply learning dynamics to explain why directly finetuning a well-trained backbone with a randomly initialized task head might harm the out-of-distribution generalization ability. Ren et al. (2020) and Ren, Lavoie, et al. (2023) also explains where the simplicity bias favoring compositional representations comes from during knowledge distillation (Hinton et al. 2015), providing a new perspective of understanding why successive knowledge transferring can improve the model's systematic generalization ability.\nMeasuring the \"quality\" or \"influence\" of training samples. Besides explaining the model's behavior, learning dynamics is also helpful for evaluating the quality or the effectiveness of different training samples. For example, Pruthi et al. (2020) propose a quantitative metric called TracIn to compute the influence of a training example on the predictions made by the model. This metric is then applied by Xia et al. (2024) to search for the most influential examples in LLM instruction finetuning. By expanding Equation (1) in the neural tangent kernel (NTK) regime, Guo, Ren, et al. (2024) propose a metric called 1pNTK to measure the relative difficulty among different training samples. These metrics and analyses inspired by learning dynamics are expected to be helpful in many related fields, like coreset selection (Feldman 2020), active learning (Settles 2009) (see, e.g., Mohamadi et al. 2022), and dataset distillation (T. Wang et al. 2018)."}, {"title": "2.2 Challenges of Analyzing the Learning Dynamics of LLM's Finetuning", "content": "The first conundrum for analyzing the learning dynamics of LLM's finetuning is the high dimensionality and the sequence nature of both the input and output signals. The high-dimensional property makes it hard to observe the model's output, and the sequence nature makes the distributions on different tokens mutually dependent,"}, {"title": "3 Learning Dynamics under Per-step and Accumulated Perspectives", "content": "We first consider a standard supervised learning problem, where the model takes a high-dimensional input $x$ and generates a sequence response $y = {Y_1,\\dots, Y_L } \\in V^L$, where $V$ is the vocabulary of size $V$. We assume we actually define a distribution over $y$, i.e., $\\pi_\\theta(y | x)$. To get this probabilistic distribution, the model first generates a logits matrix $z = h_\\theta(x) \\in \\mathbb{R}^{V \\times L}$ and then takes the Softmax operation on each column. To make the analysis for the LLM case easier, we track the change of $log \\pi_\\theta$ instead of $\\pi_\\theta$, slightly different from the results of Ren et al. (2022).\nPer-step influence decomposition. The learning dynamics of (1) become\n$$\\Delta \\theta \\triangleq \\theta_{t+1} - \\theta_t = -\\eta \\cdot \\nabla L (\\pi_{\\theta_t}(y | x_u), y_{u}),$$$$\\Delta \\log \\pi^t (y | x_o) \\triangleq \\log \\pi_{\\theta_{t+1}} (y | x_o) - \\log \\pi_{\\theta_t} (y | x_o),$$\nwhere $y_u \\in \\mathbb{R}^V$ (likely one-hot) is the supervisory signal. To get a better intuition, we start from the $L = 1$ scenario, where the $\\Delta \\theta$ and $\\Delta \\pi$ can be linked by the following proposition.\nProposition 1 (Similar to Proposition 1 of Ren et al. 2022). Let $\\pi$ = Softmax(z) and z = $h_\\theta(x)$. The one-step learning dynamics decompose as\n$$\\Delta \\log \\pi^t (y | x) = -\\eta A^t (x_o) K^t (x_o, x_u) G^t (x_u, y_u) + O(\\eta^2 ||\\nabla_\\theta z(x)||_F^2),$$\nwhere $A^t (x) = \\nabla_z \\log \\pi_{\\theta_t} (x) = I - 1\\pi_{\\theta_t} (x)^\\top$, $K^t (x_o, x_u) = (\\nabla_\\theta z(x_o)|_\\theta^t) (\\nabla_\\theta z(x_u)|_\\theta^t)^\\top$ is the empirical neural tangent kernel of the logit network and $G^t (x_u, y_u) = \\nabla_z L(x_u, y_u)|_z^t$, which for cross-entropy loss is $\\pi_{\\theta_t} (y | x_u) - y_u$.\nThe proof and more discussion can be found in Appendix A.\nIn this decomposition, $A^t (x) = I - 1\\pi_{\\theta_t} (x)^\\top$, which only depends on the model's predicted probability at time t. Intuitively, for any length-V vector b, left-multiplying this $V \\times V$ matrix $A^t$ centers the vector with respect to the probability distribution $\\pi_{\\theta_t} (x)$, because $A^tb = (I - 1\\pi^\\top)b = b - 1(\\pi^\\top b)$, and hence the mean of $A^tb$ under $\\pi$ is $\\mathbb{E}(A^tb) = 0$.\nThe matrix $K^t$ is the empirical neural tangent kernel (eNTK, Jacot et al. 2018; J. Lee et al. 2019) of the model, i.e., the product of the model's gradients with respect to x, and $x_u$. Since the eNTK usually changes slowly (or is even nearly invariant) when finetuning with a small learning rate (Jacot et al. 2018; Arora et al. 2019; Fort et al. 2020; Ren, Guo, et al. 2023), we can treat $K^t$ as a relatively stable and model-specific similarity measurement between different input samples: larger $||K^t||$ means the update of $x_u$ can influence model's prediction on x more. The term $G^t$ is determined by the loss function L, which usually provides the energy and direction for the model's adaptation. For example, consider a cross-entropy loss $L_{CE} = - y_u^\\top \\log \\pi (y | x_u)$. Then we have $G_{CE} = \\pi_{\\theta_t} (y | x_u) - y_u$, a length-V vector that points from the model's current predictive distribution to the desired supervisory distribution. When considering one-hot labels, the above term can also be written as $G_{CE} = \\pi_{\\theta_t} (y | x_u) - e_{y_u}$, which is more common in practice.\nFor a multi-label classification problem where $L > 1$, to this first-order approximation, we only need to calculate L different $\\Delta \\log \\pi^t$ and stack them together."}, {"title": "4 Learning Dynamics of LLM Supervised Finetuning", "content": "Per-step decomposition of the SFT loss. We are now ready to tackle the supervised finetuning (SFT) of an LLM. The typical loss function used in this stage is the negative log-likelihood (NLL) of a given completion $y = (y_1,\\dots, y_L) \\in V^L$, conditioned on the prompt $x_u$:\n$$L_{SFT}(x_u, y) = - \\sum_{l=1}^L \\log \\pi(y= y_l | y_{1:l-1}, x_u) = \\sum_{l=1}^L y_l^\\top \\log \\pi(y | x_u, y_{1:l-1}).$$\nNote that compared with the multi-label classification problem discussed before, where the joint distribution of all labels can be factorized as $\\pi(y | x) = \\prod_l \\pi(y_l | x)$, the sequential nature of language modeling makes the analysis more complicated, because we must have $\\pi(y | x) = \\prod_l \\pi(y_l | x, y_{1:l-1})$. Note that this is the correct form, and not one conditioning on the previous model predictions, due to the \"teacher forcing\" scheme typically used in training sequence models. To solve this problem, we can merge this factorization into the definition of the backbone $h_\\theta$ while keeping the decomposition format of Proposition 1. Specifically, define $\\chi \\triangleq [x; y]$. Then"}, {"title": "4.1 Response Space and the Probing Dataset", "content": "Besides the sequential nature of the loss function, another conundrum in analyzing LLM learning dynamics is the huge response space Y: the number of possible $y \\in Y$ is $V^L$, but the vast majority of possible sequences look nothing like natural language, and we expect the model to generate only a subset of natural language-like responses. These properties prevent us from observing the changes of all possible y like what we did for MNIST. Instead, we define several interesting regions of Y, and select corresponding typical responses to observe. Intuitively, we can use the semantic relevance between y and $x_u$ as a heuristic. Such a measurement can be understood as \u201chow suitable this y is as a response to $x_u$, compared to $y^\\star$.\u201d Then, starting from the structure of common preference optimization datasets such as Antropic-HH (Y. Bai et al. 2022) and UltraFeedback (Cui et al. 2023), we can roughly divide Y into three sub-spaces and propose seven typical responses to evaluate (as in Figure 2):\n*   VIF: reasonable responses following the instruction $x_u$:\n    1.  $y^\\star$, the chosen (i.e., the preferred) response to $x_u$.\n    2.  $y^\\lnot$, the rejected (i.e., the less preferred, but still reasonable) response to $x_u$.\n*   Ynon-IF: irrelevant responses to $x_u$ that are still recognizably human language (in these datasets, roughly \"internet-standard\" English):\n    3.  $y_{j \\neq u}^\\star$, the chosen response for a different question $x_{j \\neq u}$ selected from the training set.\n    4.  $y_{test}^\\star$, the chosen response of a question $x_{test}$ selected from the test set.\n    5.  $y_{hum}$, a \"random\" English sentence generated by GPT4 with as many words as $y^\\star$.\n*   Ynon-hum: token sequences that do not form meaningful human language:\n    6.  $y_{rnd}^u$, a random permutation of the words (space-separated strings) of $y^\\star$.\n    7.  $y_{rnd}$, a random permutation of the words of a generated sentence as in $y_{hum}^\\star$"}, {"title": "4.2 Experimental verification", "content": "We now verify our analysis of the learning dynamics using experiments. We first create an SFT training set by randomly selecting 5000 examples from the training split of Antropic-HH; we give similar results for UltraFeedback in Appendix B.2. The model will be finetuned using these $[x; y^\\star] \\in \\mathcal{D}_{train}$ for several epochs. To observe the learning dynamics, we create a probing dataset by first selecting 500 prompts from $\\mathcal{D}_{train}$ and then generate seven different responses for each x following Figure 2. We evaluate the log-likelihood of these responses every 25 updates (with a training batch size of 4, the probing occurs every 100 examples). The model is finetuned for 8 epochs, where we call the first several epochs from $\\pi_\\Theta$ to $\\pi_{res}$ fitting stage and the last several epochs from $\\pi_{res}$ to $\\pi_{ovf}$ overfitting stage. We verify the generalizability of our findings by using six models: pythia-410M/1B/1.4B/2.8B (Biderman et al. 2023) and Qwen1.5-0.5B/1.8B (J. Bai et al. 2023).\nWe emphasize beforehand that while the behaviors seen in this section are perhaps not particularly surprising, they validate that our learning dynamics explanations can explain practical behavior in LLM finetuning, and that these techniques will yield a surprising \"squeezing\" effect when applying them to preference finetuning in the next section.\nBehaviors of $y \\in VIF$. As illustrated in the first panel in Figure 3, in the fitting stage, we see SFT \"pulls up\" the curve of this region. That is because their $K_t([x, y], [x_u, y_t^\\star])$ are usually large enough, which is similar to the pairing effect of 4 and 9 in the MNIST experiment. As the training goes on, the model enters into the overfitting stage, where $\\pi_{\\theta_t} (y_i^\\star)$ starts to plateau and then decrease while $\\pi_{\\theta_t} (y_t^\\star)$ keeps increasing. That is because the energy provided by residual term $G_{SFT}$ always points to $y_t^\\star$ while deviating from other y. As the training goes on and the energy in the main direction decreases (because the model's confidence in the target increases, which makes the gap between the current prediction and the one-hot supervisions becomes smaller), the deviating energy will gradually dominate and \"push down\" those regions.\nBehaviors of $y \\in V_{non-hum}$. The learning dynamics for this region is simpler: the predictions start from a very low log-likelihood and keep decreasing throughout training, as illustrated in the last two panels in Figure 2 and the second panel in Figure 3. The responses in this region play a similar role to the 0 examples in the MNIST experiment: as they usually look dissimilar to 4, hence the norms of the corresponding $K_t$ are small, which leads to only tiny pressure \"upwards\" when the model learns. On the other hand, as the responses of all possible y given $x_u$ must sum to one, increasing of $\\pi_{\\theta_t} (y)$ on other groups must naturally \u201cpush down\u201d the curve in this region. Another interesting finding is that $\\pi_{\\theta_t} (y_{rnd})^*$ is always bigger than $\\pi_{\\theta_t} (y_{rnd}^u)$, where the latter contains all the words from $y_t^\\star$. That is because learning the chosen response increases the likelihood of specific phrases, e.g., $[y_1, y_2]$. Then, in $\\pi_{\\theta_t} (y_{rnd}^u)$, if $y_1$ is followed by $y_3 \\neq y_2$, the resulting prediction would be very small.\nBehaviors of $y \\in V_{non-IF}$. This group is the most complex one because first, we must consider the influence coming from the update of another training example, e.g., $[x_{j\\neq u}; y_{j \\neq u}^\\star]$. Furthermore, although the responses in this group are irrelevant to $x_u$, they are reasonable human language and the model will assign non-negligible probability mass to them. That means if we want to sample a response from the model given $x_u$ (e.g., when deploying the model or doing on-policy training), the probability that the model provides a response from (or"}, {"title": "5 Learning Dynamics of LLM Preference Finetuning", "content": "Instruction tuning as in the SFT stage above improves the model's instruction-following ability. In order to better align LLM's output with human preferences, typically instruction tuning is followed by preference tuning (Ouyang et al. 2022). The first widespread preference tuning method comes from RLHF (reinforcement learning with human feedback (Christiano et al. 2017)), which requires substantial expert annotation of LLM responses. RLAIF (H. Lee et al. 2023) offload this hard work by collecting preference signals from AI, but a large reward model is still required. To avoid this and mitigate the high-variance nature of the RL-based training, Rafailov et al. (2023) propose DPO (direct preference optimization, an RL-free method), and show it has the same optimization target as RLHF under the Bradley-Terry reward model (R. A. Bradley and Terry 1952). RL-free preference-tuning methods like DPO and variants such as IPO (Azar et al. 2024), SLiC (Zhao et al. 2023), KTO (Ethayarajh et al. 2024), and so on have gained popularity due to their good performance and robustness during training. Significant effort (e.g. Ji et al. 2024; Pal et al. 2024; Rafailov et al. 2024; Tajwar et al. 2024) has gone to theoretical explanations of the equivalence and differences between them and the RL-based methods. This"}, {"title": "5.1 Learning Dynamics of RL-free Preference Tuning: DPO as an Example", "content": "We start from the off-policy DPO with the following loss function:\n$$L_{DPO} (\\theta) \\triangleq - \\sum_{(x_u, y_u, y_u)} \\log \\sigma (\\beta \\log \\frac{\\pi_{\\theta} (y^\\star | x_u)}{\\pi_{ref}(y^\\star | x_u)} - \\beta \\log \\frac{\\pi_{\\theta} (y^\\lnot | x_u)}{\\pi_{ref}(y^\\lnot | x_u)} ),$$\nwhere $y_t^\\star$ and $y_u^\\lnot$ are pre-generated responses (both in VIF), and $\\pi_{ref}$ is the reference model, typically the result of SFT. In the loss function, the $\\pi_{\\theta}$ terms are also calculated using the \"teacher forcing\" mechanism, which is identical to the SFT case. Hence we decompose the learning dynamics for DPO similarly to Equation (6):\n$$\\[\\Delta \\log \\pi^t (y | x_o)\\]_m = \\sum_{l=1}^L \\eta \\[A^t(x_o)\\]_m [K^t (x_o, x_u)]_l [G_{DPO}^t(x_u, y_u^\\star, y_u^\\lnot)]_l + O(\\eta^2)$$\n$$G_{DPO} \\approx \\beta(1 - \\alpha) (y_u^\\star - y_u^\\lnot); \\qquad \\alpha = \\sigma (\\beta \\log \\frac{\\pi_{\\theta} (y^\\star | x_u)}{\\pi_{ref}(y^\\star | x_u)} - \\beta \\log \\frac{\\pi_{\\theta} (y^\\lnot | x_u)}{\\pi_{ref}(y^\\lnot | x_u)}) \\in \\mathbb{R},$$\nwhere $K_t (X_o, X_u)$ is the equivalent eNTK between the updating sequence $[x_u; y_t^\\star, y_u^\\lnot]$ and the observing sequence $[x_o; y_o]$. The derivation, which is similar to before, and the $G^t$ functions for other RL-free methods are given in Appendix A.2.2."}, {"title": "5.2 Comparison of SFT, DPO, and other Variants", "content": "By comparing the learning dynamics of SFT and DPO, we find they have identical $A^t$ and similar $K^t$. The main difference lies in $G^t$, which determines the direction and strength of the evolution of $\\pi_{\\theta_t}$. Recall the learning dynamics of SFT in the first panel of Figure 4, the pressures controlled by $G^t$ contain a big positive vector on $y_t^\\star$ and many small negative vectors on other y, where the positive vector will gradually diminish with the increase of $\\pi_{\\theta_t} (y^\\star | x_u)$ during training.\nThe DPO loss, on the other hand, consistently generates a pair of vectors (a positive $y_t^\\star$ and a negative $y_u^\\lnot$) regardless of the predictions of any policy network\u00b2. However, as illustrated in Equation (10), the norm of $G_{DPO}$ is controlled by a scalar $\\beta(1 - \\alpha)$, which is influenced by both the current policy $\\pi_{\\theta_t}$ and the reference policy $\\pi_{ref}$. We first analyze the role of $\\alpha$, which is controlled by the margin (i.e., the value inside the parentheses of $\\sigma(\\cdot)$) that represents how well the current policy separates $y_t^\\star$ and $y_u^\\lnot$ compared with the reference policy. Due to the monotonicity of $\\sigma(\\cdot)$, a larger margin leads to larger $\\alpha$, which in turn restrains the strength of $G_{DPO}$. In other words, $G_{DPO}$ automatically provides less energy on the examples that are already well separated. We then check the role of $\\beta$, which controls the regularizing effect on the KL distance between $\\pi_{\\theta_t}$ and $\\pi_{ref}$ in the original RL loss (Rafailov et al. 2023). When the margin is smaller than zero, larger $\\beta$ leads to a smaller $\\alpha$ and hence provides stronger $G_{DPO}$ for the model to \u201ccatch up\u201d the separating ability of the reference model faster. But when the model is good enough (the margin is positive), increasing $\\beta$ will increase $\\alpha$ and hence create a negative influence on $\\beta(1 - \\alpha)$, which makes the model update less. This behavior aligns well with the claims of Rafailov et al. (2023): the stronger regularizing effect tends to \u201cdrag $\\pi_{\\theta}$ back towards $\\pi_{ref}$\u201d when it deviates from $\\pi_{ref}$ too much in terms of the separating capability.\nNote that the analyses above don't make any assumptions on where $y_t^\\star$ and $y_u^\\lnot$ come from. Hence our framework can also be extended to on-policy RL-free algorithms, which often perform better than their off-policy counterparts (Guo, B. Zhang, et al. 2024; Tajwar et al. 2024). By definition, the main difference between off-policy and on-policy algorithms is how the supervisory responses are generated. Off-policy methods typically use a fixed pre-collected dataset, where $y_t^\\star$ and $y_u^\\lnot$ are usually generated by another LLM or humans. In other words, it is likely that both the chosen and rejected responses come from the \"less likely\" region of the model's"}, {"title": "5.3 The Squeezing Effect of the Negative Gradient", "content": "The existence of negative gradients (the $y_u^\\lnot$ term in Equation (10)) is the key to understanding the difference of learning dynamics between SFT and DPO. We find this negative gradient will impose a non-trivial \"squeezing effect\" on any models outputting the probabilistic distribution using Softmax output heads, even in a simple multi-class logistic regression task. Specifically, consider the $L = 1$ case and assume our algorithm imposes a negative gradient on label $\\tilde{y}$, the model's predictive distribution $\\pi_{\\theta_{t+1}}$ will change as follows:\n*   The negative gradient will reliably decrease the confidence of $\\tilde{y}$, i.e., $\\pi_{\\theta_{t+1}} (y = \\tilde{y})$ is guaranteed to decrease.\n*   The decreased probability mass is \"squeezed\" into the most confident dimension before the update, i.e., $\\pi_{\\theta_{t+1}} (y = y^*)$, where $y^* = \\mathop{\\text{argmax}}_{i \\in [V] \\backslash {\\tilde{y}}} \\pi_{\\theta_t} (y = i)$ is guaranteed to increase.\n*   The rich get richer and the poor get poorer: generally, dimensions with high $\\pi_{\\theta_t}$ tend to increase, and those with low $\\pi_{\\theta_t}$ tend to decrease.\n*   Peakier $\\pi_{\\theta_t}$ suffer a more serious squeezing effect. If the probability mass concentrates on few dimensions in $\\pi_{\\theta_t}$, which is common for a pretrained model, all $\\pi_{\\theta_{t+1}} (y \\neq y^*)$ decrease (only $y^*$ is considered \u201crich\u201d).\n*   Smaller $\\pi_{\\theta_t} (\\tilde{y})$ make the squeezing effect stronger. If $\\tilde{y}$ is unlikely under $\\pi_{\\theta_t}$, the probability mass of all other $\\pi_{\\theta_{t+1}} (y \\neq y^*)$ will be more seriously decreased, and the $\\pi_{\\theta_{t+1}} (y = y^*)$ increases more. That is what we observed in the off-policy DPO scenario.\nAppendix C proves these observations analytically for linear models, by directly computing $\\pi_{\\theta_{t+1}} / \\pi_{\\theta_t}$ in different situations. Intuitively, recall that the cross-entropy loss for a one-hot label $e_i$ is just $- \\log \\pi_i = -z_i + \\log \\sum_j \\exp(z_j)$, which has gradient $-e_i + \\pi$. Suppose there is one $k \\neq i$ such that $\\pi_k$ is rather large, say 0.8. Then gradient ascent will decrease $z_i$, but increase $z_k$ almost as much. Since $\\pi_k$ is dominant, increasing its logit results in a large absolute increase to $\\pi_k$, which necessarily reduces all other probabilities to account for it.\nIn the next subsection, we will verify these claims by experiments on real LLM finetuning problems. With the help of these analyses, we figure out why the model's confidence in all responses decreases during off-policy DPO training and successfully find where the decreased probability mass has gone."}, {"title": "5.4 Experimental Analysis of DPO's Learning Dynamics", "content": "To verify our framework also explains the learning dynamics of preference tuning well, we conduct similar experiments for DPO as we do for SFT. Recall the residual term $G_{DPO}$ introduces a positive arrow on $y_t^\\star$, which means the learning tries to \u201cpull up\u201d the model's prediction on $y_t^\\star$. To verify this, we create two types of rephrases of $y_t^\\star$ using GPT4 and call them $y_{gpts}^\\star$ and $y_{gptf}^\\star$ (the first tend to keep the semantics while the latter tends to keep the format of $y_t^\\star$; details in Figure 6). See the three curves in the first panel in Figure 5,"}, {"title": "6 Inspirations from Learning Dynamics", "content": "Explaining the success of various related algorithms. The proposed learning dynamics explain many interesting observations during SFT and DPO; this perspective also helps explain the success of many practical algorithms. For example, the literature mostly credits on-policy RL-free methods with better aligning the current policy's distribution to the reward model (Tajwar et al. 2024), but we can extend this explanation from the learning dynamics perspective. As illustrated in the third panel in Figures 4 and 18, we see $y_u^\\star$ is sampled from the current $\\pi_\\Theta$, which is less likely to be in a valley. Hence the squeezing effect introduced via the big negative gradient is reduced. The benefits of SPIN (Z. Chen et al. 2024), which is inspired by treating the LLM itself as a GAN-style system (Goodfellow et al. 2014), can also be explained similarly: as the negative response $y_u^\\star$ always comes from the model's policy in the previous generation, its $\\pi_\\Theta$ is less likely to be very"}, {"title": "DA Simple Method to Improve Alignment", "content": "D.1 Pinpointing the drawback of off-policy DPO\nBased on our observations and analysis above, we speculate that \"imposing big negative gradients on the valley region\" is one of the bottlenecks of off-policy RL-free methods. Starting from this hypothesis, we believe introducing on-policy sampling has the potential to mitigate this problem, as demonstrated in SPIN (Z. Chen et al. 2024) and other online algorithms (Guo, B. Zhang, et al. 2024). However, we also speculate that these methods improve the model's performance not only by mitigating the squeezing effect. Hence to figure out to what extent the squeezing effect can harm the model's performance, we propose a simple yet effective method to isolate its influence. As this method can directly mitigate this effect, it can also be considered as an ablation study of this interesting phenomenon.\nD.2 A simple method inspired by learning dynamics\nAs illustrated in Figure 20, where the baseline method is a standard SFT-then-DPO pipeline. The proposed method is very simple. We only need to augment the dataset used in SFT by adding (x, \u0233) pairs for each sample into it. All other settings are unchanged. The motivation for this method is also quite simple: as SFT can pull up the region of supervised \u0177 and we don\u2019t want the model to impose big negative gradients on a valley region, we can just pull up those \u0233 before DPO. Furthermore, as demonstrated in the third panel in Figure 18 and Equation (19), the negative gradient in DPO would be strong enough to push down \ud835\udf0b\ud835\udf03(y), because the gradient will be large if the model cannot separate \u0177 and \u0233 well. In other words, under DPO\u2019s loss, there is no need to worry about the model overfitting those \u0233 during SFT.\nD.3 Experimental verification\nTo verify our analysis, we conduct experiments by fine-tuning a pretrained Qwen1.5-1.8B (J. Bai et al. 2023) model using Antropic-HH dataset (Y. Bai et al. 2022) (we use a subset containing 5000 random examples from the training split). The pipelines of different methods are demonstrated in Figure 20. In this experiment, we call the pretrained model B0 (and E0, which is identical to B0), which is an official checkpoint pretrained by J. Bai et al. (2023). Model B1 and E1 are the ones after SFT, which are different for these two methods. Model B2\u22122/4/6 and E2\u22122/4/6 are the models finetuned using DPO for 2/4/6 epochs. All the settings (except the starting model) of the DPO stage are the same for these two methods.\nWe first observe the learning dynamics of these two methods in Figure 21, where all the trends support our analysis quite well. See the first two panels that compare \ud835\udf0b\ud835\udf03(y) and \ud835\udf0b\ud835\udf03(\u0233) respectively. It is clear that these two methods have an almost identical curve on \ud835\udf0b\ud835\udf03(y) in the SFT stage but behave quite differently on \ud835\udf0b\ud835\udf03(\u0233): because we directly train the model using (x, \u0233) in the proposed method. Then, after the SFT stage, we conduct DPO using identical settings for these two methods. From the first three panels, we can observe the decay speed of all curves of the proposed method is smaller than its counterpart in the baseline. That is the benefit introduced by \u201cpulling up\u201d the \ud835\udf0b\ud835\udf03(y) region before conducting DPO. With this specific design, the big negative gradients in DPO are imposed on the peaky region (the behavior is like the third panel in Figure 18) rather than the valley region (see the fourth panel), hence the squeezing effect is successfully restrained. The results in the last panel of Figure 21 are also a strong verification of the whole picture. During the SFT stage, the observed \u201cargmax-probability\u201d of the proposed method is higher than the baseline, because we impose twice \u201cpull up\u201d pressure, i.e., those for (x,y), compared with the baseline. However, at the beginning of DPO, we observe a clear drop in the orange curve. That is because the negative gradients are exactly imposed on those \u0233 (in the second panel of Figure 21, \ud835\udf0b\ud835\udf03(y) is already very high). Furthermore, at the end of DPO, we see the \u201cargmax-probability\u201d of the proposed method is significantly lower than the baseline setting, which implies that the squeezing effect is restrained in our setting.\nIn order to figure out whether the model trained using the proposed flow, which successfully restrains the squeezing effect, indeed does alignment better, we conduct pair-wise comparisons of these models\u2019 responses and report their win rate as in (Rafailov et al. 2023). Specifically, we first randomly select 1000 test questions from the test split of Antropic-HH and generate 1000 responses by feeding the prompts to each of these models (we use the default sampling setting provided in (Rafailov et al. 2023)). Then, with the prompt template provided in Figure 23, we evaluate the win rate of the responses pairs using GPT3.5-turbo and Claude3-Haiku. Here we report the average win rate of different comparisons (the degenerated responses are not compared, so the number of compared examples is slightly smaller than 1000). Note that a win rate greater than 0.5 means the method that comes first is preferred by the evaluator."}, {"title": "7 Conclusion", "content": "Learning dynamics, which depicts how the model's prediction changes when it learns new examples, provide a powerful tool to analyze the behavior of models trained with gradient descent. To better use this tool in the context of LLM finetuning, we first derive the step-wise decomposition of LLM finetuning for various common algorithms. Then, we propose a unified framework for understanding LLM predictions' behaviors across different finetuning methods. The proposed analysis successfully explains various phenomena during LLM's instruction tuning and preference tuning, some of them are quite counter-intuitive. We also shed light on how specific hallucinations are introduced in the SFT stage, as previously observed (Gekhman et al. 2024), and where the improvements of some new RL-free algorithms come from compared with the default off-policy DPO. Finally, inspired by this analysis, we propose a simple (but counter-intuitive) method that is effective in improving the alignment of models."}]}