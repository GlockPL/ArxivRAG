{"title": "BAMBA: A Bimodal Adversarial Multi-Round Black-Box Jailbreak Attacker for LVLMs", "authors": ["Ruoxi Cheng", "Yizhong Ding", "Shuirong Cao", "Shaowei Yuan", "Zhiqiang Wang", "Xiaojun Jia"], "abstract": "LVLMs are widely used but vulnerable to illegal or unethical responses under jailbreak attacks. To ensure their responsible deployment in real-world applications, it is essential to understand their vulnerabilities. There are four main issues in current work: single-round attack limitation, insufficient dual-modal synergy, poor transferability to black-box models, and reliance on prompt engineering. To address these limitations, we propose BAMBA, a bimodal adversarial multi-round black-box jailbreak attacker for LVLMs. We first use an image optimizer to learn malicious features from a harmful corpus, then deepen these features through a bimodal optimizer through text-image interaction, generating adversarial text and image for jailbreak. Experiments on various LVLMs and datasets demonstrate that BAMBA outperforms other baselines.", "sections": [{"title": "1 Introduction", "content": "Large Visual Language Models (LVLMs) [8], such as GPT-40, are being increasingly applied in various domains. They possess an extensive knowledge base, which also includes harmful or sensitive content. Attackers try to extract harmful content from these models to serve their malicious intent [12]. Red-teaming [18] plays a critical role in assessing the safety of LVLMs, aiming to identify flaws and mitigate potential harm.\nHowever, there exist four key limitations in current literature. (1) Limitation of single-round attack. Most red-teaming research focuses on single-round prompt optimization [33, 11], while multi-round attacks(see an example in Figure 1), where each prompt is modified based on prior responses,"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Large Vision-Language Models", "content": "Large Vision-Language Models (VLMs) combine vision and language processing, taking text and image inputs to generate free-form text output for multimodal tasks [30]. They typically use pre-trained LLMs and image encoders, connected by a feature alignment module.\nFor example, LLaVA [13] connects open-source visual encoder CLIP [20] with language decoder LLaMA [23], performing end-to-end fine-tuning on generated visual-language instruction data."}, {"title": "2.2 Adversarial Jailbreak against LVLMS", "content": "Most jailbreak attacks only focus on unimodal prompt optimization. Zou et al. [33], Liao and Sun [11], Ma et al. [16] generate adversarial suffixes to bypass safety measures in aligned LLMs. Qi et al. [19] discover that a single visual adversarial sample can conduct jailbreak and Niu et al. [17] propose a maximum likelihood-based algorithm to find the image jailbreaking prompt.\nAttempts have been made to extend attack to both text and image modalities. Ying et al. [29] target both, but optimize the modalities separately. Wang et al. [25] perform joint text-image optimization to maximize affirmative response probability, but limited to white box and single-round. Yin et al. [28] effectively targets bimodal interactions but doesn't extend to jailbreak attacks."}, {"title": "2.3 Multi-Round Jailbreak Attacks", "content": "Jailbreak attacks typically rely on single-round optimization [25, 19], while multi-round attacks have demonstrated greater effectiveness [26].\nWang et al. [24] distributes risks across multiple rounds of queries and utilizes psychological strategies to surpass the safeguard. Dong et al. [4] reuses prompts from previous attacks to generate NSFW images. Wang et al. [26] breaks down malicious queries into sub-queries and iteratively refining the generated image. Yang et al. [27] uses reinforcement learning to iteratively perturb prompt tokens and generate NSFW images. Liu et al. [15] generate text prompts using reinforcement learning and a universal template. However, all these methods rely on prompt engineering and templates, limiting their adaptability and generalization."}, {"title": "3 Problem Setup and Threat Model", "content": "Consider an LVLM that processes dual-modal inputs (image and text), the attacker's objective is to maximize the toxicity of its output up to a certain threshold with adversarial inputs."}, {"title": "3.1 Attacker's Goal", "content": "The attack starts with a benign image $x_{\\text{benign}}$, an initial text prompt $y_{\\text{init}}$ and a harmful content corpus $Y = \\{y_i\\}_{i=1}^m$, where each $y_i$ represents a harmful text sequence and $m$ is their total number. The attacker aims to generate an adversarial image $x_{\\text{adv}}$ and an adversarial text $y_{\\text{adv}}$, such that when fed into the LVLM, they trigger a jailbreak. Importantly, the adversarial text $y_{\\text{adv}}$ should maintain semantic similarity to the original prompt $y_{\\text{init}}$. For convenience, the characters appearing later can represent their embeddings extracted using feature extractor from a white-box model."}, {"title": "3.2 Threat Model", "content": "The attacker has only black-box access to the target LVLM, meaning they cannot access internal model parameters, training data, or the model's state. However, they can observe input-output pairs, which helps them to generate adversarial inputs. Additionally, image and text embeddings are extracted using feature extractors from a white-box LVLM, such as MiniGPT-4 [32] and BLIP [9]."}, {"title": "4 Methodology", "content": "We introduce BAMBA, a bimodal adversarial multi-round black-box jailbreak attack for LVLMs, as shown in Algorithm 1. Our approach begins by extracting malicious features from a harmful corpus $Y = \\{y_i\\}_{i=1}^m$, which are then injected into a benign image $x_{\\text{benign}}$. Using Projected Gradient Descent (PGD) [6], we generate an adversarial image $x_{\\text{adv}}$ designed to maximize the sum of toxicity scores when paired with each text prompt $y_i$. These malicious features are further enhanced by a bimodal adversarial sample optimizer. In this stage, both the image and an initial text prompt are iteratively refined through a dual-modal optimization process, in the aim of increasing the output toxicity of the target model in each round. The optimization continues until the toxicity score surpasses a predefined threshold, signaling a successful jailbreak, or until the maximum number of iterations is reached, indicating failure."}, {"title": "4.1 Adversarial Image Generation.", "content": "We first extract malicious features from a harmful content corpus $Y = \\{y_i\\}_{i=1}^m$, and inject them into a benign image $x_{\\text{benign}}$, resulting in an adversarial image $x_{\\text{adv}}$.\nFirst, we aim to make $x_{\\text{adv}}$ as close as possible to each $y_i$ in order to induce adversarial perturbations that capture harmful features. This objective can be formulated as:\n$x_{\\text{adv}} = \\underset{x_{\\text{adv}} \\in B}{\\text{argmin}} \\sum_{i=1}^m ||x_{\\text{adv}} - y_i||$,\nwhere the goal is to minimize the absolute sum of the differences between $x_{\\text{adv}}$ and $y_i$, and $B$ is the constraint space on the image.\nSecond, $x_{\\text{benign}}$ should trigger a high toxicity response from the target model when paired with each text $y_i$ in $Y$. This can be formulated as:\n$x_{\\text{adv}} = \\underset{x_{\\text{adv}} \\in B}{\\text{argmax}} \\sum_{i=1}^m T(x_{\\text{adv}}, y_i)$,\nwhere $T(x_{\\text{adv}}, y_i)$ measures the toxicity of target model's response to $x_{\\text{adv}}$ and $y_i$.\nThe loss function $\\mathcal{L}(x_{\\text{adv}})$ is defined as:\n$\\mathcal{L}(x_{\\text{adv}}) = \\sum_{i=1}^m -T(x_{\\text{adv}}, y_i) + \\lambda ||x_{\\text{adv}} - y_i||$,"}, {"title": "4.2 Bimodal Adversarial Optimization Loop.", "content": "The malicious features extracted from the harmful corpus are further enhanced through a bimodal adversarial optimization process, where both the adversarial image $x_{\\text{adv}}$ and the initial text prompt $y_{\\text{init}}$ are iteratively optimized. Each update step refines one modality based on the other, amplifying the adversarial effect and generating more potent adversarial inputs.\nIn this approach, the image and text evolve in a cyclical manner. Specifically, the adversarial text $y_{\\text{adv}}$ is first optimized based on the current image $x_{\\text{adv}}$, and then the adversarial image is updated based on the modified text $y_{\\text{adv}}$. This back-and-forth process continues, progressively amplifying the toxicity of the output until the system is successfully bypassed, i.e., the jailbreak is achieved.\nTo guide the updates of image and text, two distinct loss functions are employed, each targeting the same goal: maximizing toxicity of target model response.\nAdversarial Text Optimization. We use Greedy Coordinate Gradient (GCG) to update the adversarial text $y_{\\text{init}}$ [33]. It uses gradients of the one-hot token indicators to identify potential replacements for each token, selecting the one that most reduces the loss.\nGiven adversarial image $x_{\\text{adv}}$ generated in previous optimization, a text prompt $y_{\\text{init}}$ (denoted as $y_{\\text{adv}}$ in later bimodal optimization loop) is updated using the following loss function $L_{\\text{text}}(y_{\\text{adv}})$:\n$L_{\\text{text}}(y_{\\text{adv}}) = -T(x_{\\text{adv}}, y_{\\text{adv}})$.\nThe adversarial text $y_{\\text{adv}}$ is then updated using a set of candidate suffixes with a single-token substitution $\\{y_{\\text{adv}}^{(i)}\\}_{i=1}^S$.\nAdversarial Image Optimization. After updating the adversarial text $y_{\\text{adv}}$, we optimize the adversarial image $x_{\\text{adv}}$ based on the updated text prompt. The corresponding loss function $L_{\\text{image}}(x_{\\text{adv}})$ is also given by:\n$L_{\\text{img}}(x_{\\text{adv}}) = -T(x_{\\text{adv}}, y_{\\text{adv}})$.\nThe adversarial image $x_{\\text{adv}}$ is then updated using the gradient descent method as follows:\n$x_{\\text{adv}} \\leftarrow x_{\\text{adv}} - \\alpha \\cdot \\nabla_{x_{\\text{adv}}} L_{\\text{image}}(x_{\\text{adv}}, y_{\\text{adv}})$, where $\\alpha$ denotes the learning rate.\nAfter each optimization, the adversarial image-text pair ($x_{\\text{adv}}, y_{\\text{adv}}$) is fed as input to the target model. If the response does not successfully jailbreak, the bimodal optimization loop continues."}, {"title": "5 Evaluation", "content": ""}, {"title": "5.1 Experimental Setup", "content": "Datasets. We use the same harmful content corpus to optimize the benign image following previ-"}, {"title": "5.2 Results", "content": "We test BAMBA alongside nine baseline methods, as well as the scenario without any attack, using MiniGPT-4 as the attacker and Perspective API in optimization on both open-source and closed-source VLVMs. As shown in Table 2, BAMBA"}, {"title": "5.3 Ablation Study", "content": "We further investigate the impact of different system parameters on the experimental results.\nNumber of queries for each input pair. As shown in Figure 7, the loss fluctuations stabilized after 10 queries. Further queries do not improve the results, making 10 the optimal choice.\nThreshold of toxicity score. As shown in Figure 8, when $T_{\\text{toxicity}}$ exceeds 1.5, all labelers classify the response as a jailbreak.\nParameter in adversarial image generation. As shown in Figure 9, the most appropriate value for $\\lambda$ to balance the toxicity of adversarial examples with embedding differences is 1.0."}, {"title": "5.4 Case Study", "content": "When the initial bimodal prompt was input, MiniGPT-4 refused to answer. However, after optimizing the prompt with BAMBA, MiniGPT-4 provided advice for a malicious intent, as shown in Figure 10."}, {"title": "6 Conclusion", "content": "In this paper, we propose BAMBA, a bimodal adversarial multi-round black-box jailbreak attacker for LVLMs, which leverages target model's response as feedback for optimization. Initially, we extract harmful features from a corpus and inject them into a benign image. Then, we design a bimodal optimization loop to further enhance them and generate adversarial images and texts for jailbreak. Experiments show that BAMBA outperforms existing baselines, highlighting its potential to defend AI systems against adversarial attacks."}, {"title": "Limitations", "content": "BAMBA is the first adversarial jailbreak attack method that performs iterative optimization based on the target model's response. While the method has been proven effective, there exists a challenge: each response generated by the model takes several seconds, and thousands of iterations in a single loop require a considerable amount of time."}, {"title": "Ethics and Social Impact", "content": "The BAMBA red team test exposes critical vulnerabilities in current VLVMs, highlighting the urgent need for improved model security and defense mechanisms. Although these models show considerable promise for real-world applications, they also present significant risks when exploited by malicious actors, potentially generating harmful or biased outputs. Attacks like BAMBA can manipulate model behavior, bypass content moderation systems, and amplify issues such as misinformation and toxicity. To mitigate these risks, it is essential to prioritize transparency, accountability, and the implementation of robust safeguards, ensuring the responsible and ethical deployment of advanced AI technologies."}, {"title": "Potential Risks", "content": "While BAMBA can effectively bypass LVLM defenses, it also poses a threat of misuse by malicious actors to generate harmful or unethical content, such as misinformation or offensive material. To mitigate these risks, robust monitoring systems must be implemented to detect and filter harmful outputs. Furthermore, measures ensuring transparency and accountability must be enforced to guarantee responsible usage and prevent it from being misused in sensitive contexts."}, {"title": "A Perspective API Details", "content": "The Perspective API is a robust tool that utilizes machine learning to evaluate the \"toxicity\" of text inputs. By analyzing user-generated content, it provides a toxicity score that helps identify potentially harmful or offensive language. The API assesses comments across various emotional attributes, which are specific categories of language characteristics. The output is a numerical score between 0 and 1, where a higher score indicates a greater likelihood of toxicity. In our experiment, we selected the following eight attributes for evaluation as shown in Appendix A."}, {"title": "B Human Labeler Details", "content": "Demographic Data In our evaluation process, demographic data of the human labelers offers crucial insights into their backgrounds and potential influences on their assessments:\nAge: The participants in our study ranged in age from 18 to 60 years. This demographic was chosen to reflect a cohort familiar with contemporary technologies, social trends, and linguistic cultures, all of which are relevant to the evaluation criteria, such as harmful content and linguistic nuances.\nGender: We ensured a balanced gender representation among our labelers, with 10 male and 10 female participants. This diversity was aimed at minimizing potential biases and enriching perspectives on sensitive issues, particularly those related to gender bias.\nEducational Background: All labelers were university students familiar with English-speaking cultures and social volunteers, offering their services on an unpaid basis. This educational background provided them with the technical expertise necessary to assess text toxicity and identify jailbreaking, ensuring a deep understanding and accurate interpretation of the evaluation criteria.\nCultural Background: Although our labelers shared a common level of proficiency in English, they came from diverse cultural backgrounds and age groups. This diversity enriched the evaluation process, offering different perspectives that influenced the accuracy and quality assessments of the language.\nBlindness to Method Enforcement Enforcing blindness to method is critical to maintaining impartiality and reliability throughout our evaluation process:\nRandomized Presentation: Responses presented to labelers were randomized to prevent order bias, ensuring each response was assessed objectively based on its content rather than its position in the sequence.\nMasking of Identifying Information: Any identifying information that could reveal the methods used to generate responses was carefully concealed. This includes removing prefixes and suffixes associated with affirmative or negative answers, as well as other factors that might introduce bias into the evaluation.\nClear Instructions: Labelers received detailed instructions emphasizing the importance of focus-ing exclusively on the content of the responses. They were instructed to assess the responses based on predefined criteria, such as text toxicity and identify jailbreaking, without considering how the responses were generated.\nTraining and Monitoring: Prior to the evaluation, labelers underwent training sessions to familiarize themselves with the evaluation criteria and reinforce adherence to blindness to method. Regular monitoring throughout the evaluation process ensured consistency and adherence to protocol."}]}