{"title": "BOOTSTRAPPING LANGUAGE-GUIDED NAVIGATION\nLEARNING WITH SELF-REFINING DATA FLYWHEEL", "authors": ["Zun Wang", "Jialu Li", "Yicong Hong", "Songze Li", "Kunchang Li", "Shoubin Yu", "Yi Wang", "Yu Qiao", "Yali Wang", "Mohit Bansal", "Limin Wang"], "abstract": "Creating high-quality data for training robust language-instructed agents is a long-\nlasting challenge in embodied AI. In this paper, we introduce a Self-Refining\nData Flywheel (SRDF) that generates high-quality and large-scale navigational\ninstruction-trajectory pairs by iteratively refining the data pool through the collab-\noration between two models, the instruction generator and the navigator, without\nany human-in-the-loop annotation. Specifically, SRDF starts with using a base\ngenerator to create an initial data pool for training a base navigator, followed by\napplying the trained navigator to filter the data pool. This leads to higher-fidelity\ndata to train a better generator, which can, in turn, produce higher-quality data\nfor training the next-round navigator. Such a flywheel establishes a data self-\nrefining process, yielding a continuously improved and highly effective dataset\nfor large-scale language-guided navigation learning. Our experiments demon-\nstrate that after several flywheel rounds, the navigator elevates the performance\nboundary from 70% to 78% SPL on the classic R2R test set, surpassing human\nperformance (76%) for the first time. Meanwhile, this process results in a superior\ngenerator, evidenced by a SPICE increase from 23.5 to 26.2, better than all previ-\nous VLN instruction generation methods. Finally, we demonstrate the scalability\nof our method through increasing environment and instruction diversity, and the\ngeneralization ability of our pre-trained navigator across various downstream nav-\nigation tasks, surpassing state-of-the-art methods by a large margin in all cases.", "sections": [{"title": "1 INTRODUCTION", "content": "The lack of high-quality data is one of the main bottlenecks in training embodied agents to complete\nreal-world human activities. Unlike many other discriminative or generative learning problems,\nwhere the data itself naturally formulates a self-supervised learning objective (Devlin, 2018; He\net al., 2022) or the data labeling can be facilitated by existing models (Ros et al., 2016; Tian et al.,\n2024), training embodied agents usually requires expensive human annotation on complex vision-\nlinguistic contents and physical interactions. In the essential embodied AI problem of Vision-and-\nLanguage Navigation (VLN) (Zhang et al., 2024b), one widely considered solution is to first train\na trajectory-to-instruction generator (Fried et al., 2018; Tan et al., 2019; Chen et al., 2022c) using\na small amount of human-annotated data and then use the generator to caption large-scale trajec-\ntories sampled from interactive environments (Hao et al., 2020; Chen et al., 2022c; Wang et al.,\n2023e). Among them, Hao et al. (2020) demonstrates the effectiveness of scaling the synthetic\ninstruction-trajectory pairs in the existing training environments, while Chen et al. (2022c); Wang\net al. (2023e); Li & Bansal (2023) emphasizes the importance of scaling training environments for\nbetter generalization ability of agents. However, the quality of the synthetic data, especially the lan-\nguage fidelity, is highly under-investigated. We find that training solely with the large-scale synthetic\ndata (352xlarger in instruction-trajectory pairs, and 13\u00d7more diverse in environments) yields worse\nresults compared to training with a small human-annotated dataset (see Table 1), which indicates\nthe need for high-quality instructions beside simply scaling the data and environments.\nThough many methods have been proposed for improving data quality in multi-modal understand-\ning and generation tasks (Fang et al., 2023; Li et al., 2024a; Betker et al.; Fang et al., 2024; Li\net al., 2024b; Nguyen et al., 2024b), improving synthetic instruction-trajectory pairs for language-\nguided navigation has several distinct challenges. First, the most straightforward approach is to\nbuild a strong instruction generator, but the limited amount of high-quality training data (e.g.,\nR2R (Anderson et al., 2018b) train split has only 14K human-labeled data) makes it challeng-\ning to train a robust generator capable of producing high-fidelity instructions for diverse trajec-\ntories. Additionally, manually correcting instructions by humans is resource-intensive and costly.\nMoreover, the alignment of instruction-trajectory pairs\nin VLN is hard to evaluate, as the instructions not only\ncontain semantic information (e.g., 'Walk past the ta-\nble') but also have rich directional information (e.g.,\n\u201cTurn left') to match with the corresponding trajectory.\nBesides, the visual elements mentioned in the instruc-\ntions are also temporally aligned to panoramas in mul-\ntiple scenes. As a result, traditional metrics like CLIP\nscore (Radford et al., 2021) struggle to evaluate such\nmulti-scene directional and semantic alignment, as they\nonly capture single-scene semantic relationships.\nIn this work, we propose a Self-Refining Data Flywheel, SRDF, that automatically evaluates and\nimproves the quality of the generated instructions at scale, through an iterative collaboration be-\ntween the navigator and the instruction generator. As shown in Figure 1 (a), our first step is similar\nto ScaleVLN's process (Wang et al., 2023e), which trains an instruction generator using the original\nhuman-labeled data, then generates instructions for unlabeled paths sampled from 800 HM3D train-\ning environments and trains a strong navigator using the generated data. Then, for evaluating the\ngenerated instructions, we propose to use the trained navigator's path-fidelity score (nDTW (Ilharco\net al., 2019) and SPL (Anderson et al., 2018a), measuring how closely the navigator's followed path\nmatches the original trajectory) as the similarity score. Since the trained navigator is strong enough\n(achieves human-level performance in Wang et al. (2023e)) and has already learned to connect vi-\nsual landmarks and directional cues to actions effectively, its fidelity in following the instructions\ncan naturally reflect how well the instruction-trajectory pairs are aligned. It also avoids manually\nsetting vague thresholds when using CLIP-score-like metrics for filtering. In our case, SPL=1 yields\na perfect trajectory match. After using the navigator as a filter to obtain a high-quality subset of\nthe generated data, this subset of data will be used to train a better instruction generator in the next\niteration. The improved instruction generator re-generates instructions for bad samples to produce\nbetter datasets, which is used to train a stronger navigator. The process iterates, with the navigator\nimproving the instruction generator via data filtering, and the instruction generator improving the"}, {"title": "2 RELATED WORK", "content": "Vision-and-Language Navigation. VLN requires an agent to navigate in unseen environments\nbased on natural language instructions. Numerous datasets have been proposed for this task (An-\nderson et al., 2018b; Ku et al., 2020; Qi et al., 2020; Shridhar et al., 2020; Thomason et al., 2020;\nPadmakumar et al., 2022; Nguyen & Daum\u00e9 III, 2019; Chen et al., 2019; Kim et al., 2021), span-\nning both indoor and outdoor environments with varied levels of instruction detail. The limited\navailability of human-annotated data for training generalizable VLN agents to achieve near-human\nperformance is a key challenge due to the high cost of collecting instruction-trajectory pairs. To\naddress this, various data augmentation approaches have been explored. Some focus on scaling\nenvironments by editing existing ones (Li et al., 2022; Liu et al., 2021b) or generating new ones\nwith text-to-image models (Li & Bansal, 2023). Others scale instruction data by training instruction\ngenerators to generate instructions for unannotated paths (Hao et al., 2020; Zhang & Kordjamshidi,\n2023; Zhang et al., 2024a), or by leveraging large sets of rendered environments from simulators\n(e.g., HM3D (Ramakrishnan et al., 2021), Gibson (Xia et al., 2018)) (Wang et al., 2023e; Kamath\net al., 2022; Chen et al., 2022c). While data scaling has been effective for VLN, the quality of\nthe data, particularly the alignment between instructions and trajectories, remains under-explored.\nIn this paper, we investigate the impact of data quality on VLN and propose a data flywheel that\niteratively refines itself through collaboration between the navigator and the generator.\nHigh-Quality Multimodal Dataset Curation. Many multimodal studies show that improving\ndata quality can significantly enhance model performance, either through advanced dataset filter-\ning (Fang et al., 2023; Li et al., 2024a; Gadre et al., 2024; Sun et al., 2023) or refining captions\nwith strong models (Betker et al.; Fang et al., 2024; Li et al., 2024b; Nguyen et al., 2024b; Wang\net al., 2024c). Recently, Segment Anything (SAM) (Kirillov et al., 2023) demonstrated how data and\nmodels can improve each other through a data flywheel with a human-in-the-loop process, evolving\nfrom model-assisted to fully automated annotation. Our data flywheel similarly integrates filtering\nand re-captioning data via navigator verification and generator refinement, operating in a data-model\nloop like SAM, but without any human intervention.\nSelf-Improving Language Models. Studies show that Large Language Models (LLMs) can im-\nprove themselves by training on their own generated outputs across tasks like programming (Halupt-\nzok et al., 2022), summarization (Patil et al., 2024), question-answering (Lee et al., 2024; Yu et al.,\n2024), reasoning (Prasad et al., 2024), and others (Li et al., 2023a; Madaan et al., 2024; Zhou et al.,\n2024c), where the quality of the self-generated data is ensured via human (Ouyang et al., 2022; Bai\net al., 2022a), off-the-shelf verifiers/reward models (Ni et al., 2023; Wang et al., 2019; Dou et al.,\n2024; Bai et al., 2022b; Lee et al., 2023; Nguyen et al., 2024a) or model's self feedback (Yuan\net al., 2024; Wu et al., 2024; Wang et al., 2024d). In our pipeline, the instruction generator iter-\natively self-improves using its own generated data. Unlike prior work, our approach establishes a\nfully automated, multi-round, two-model mutual improvement process, enabling the navigator, the\ngenerator, and the dataset to evolve concurrently through continuous model-driven feedback."}, {"title": "3 METHODOLOGY", "content": "3.1 BACKGROUND\nVLN data typically consists of instruction-trajectory pairs, where a trajectory represents a path in a\n3D environment, and the corresponding instruction guides the agent to follow it. This data can be\nused for training either instruction-to-action navigators or trajectory-to-instruction generators. Since\nmanually annotating trajectories is costly, a common approach is to first train a generator on limited\nhuman-annotated data and then use it to generate instructions for paths in unlabeled environments,\nwhich are subsequently used to augment navigator training.\nWhile this method helps increase the data amount, it poses challenges regarding the quality and\nfidelity of the generated instructions. These challenges imply two essential problems how to\ngenerate better data and evaluate the data quality, which we aim to address in the following sections.\n3.2 SRDF: SELF-REFINING DATA FLYWHEEL\nIn this section, we introduce the Self-Refining Data Flywheel (SRDF) to tackle the challenge of\nevaluating and improving VLN data to bootstrap its learning. Broadly, our system comprises a\nnavigator, N, and an instruction generator, G, shown in Figure 1 (a). We use N to assist G by\noptimizing G using the data filtered by N, while G enhances N by refining the low-quality data. This\niterative process is repeated multiple times, consistently enhancing both G and N's performance.\nMain Components. Our SRDF requires the following resources at the beginning: (1) seed data\nDSeed, typically human-annotated, for training a base G and N (2) unlabeled trajectory pool, DTraj,\nusually collected from large-scale environment datasets for generating new training data.\nTraining Base Instruction Generator. Most previous works (Fried et al., 2018; Tan et al., 2019;\nDou & Peng, 2022) train instruction generators from scratch, which limits their ability to generalize\ntext effectively. Some recent approaches leverage pretrained vision-and-language models but neglect\ncritical directional information during trajectory encoding (Li & Bansal, 2023; Kong et al., 2024),\nleading to instructions with inconsistent or incorrect directional cues.\nWe hypothesize that an effective instruction generator should be capable of understanding both\nmulti-image inputs and interleaved image-text inputs. Multi-image understanding is crucial for ac-\ncurately encoding multi-panorama trajectories and interleaved image-text comprehension helps en-\ncode directional images within the raw text space, enabling simple yet effective visual prompting of\ntrajectories. To achieve this, we utilize Mantis (Jiang et al., 2024), an interleaved multi-image mul-"}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETUP\nDatasets and Evaluation Metrics. We establish our data flywheel and perform ablation studies\nprimarily on the R2R dataset, while also assessing the transferability of our pre-trained navigator\non a range of downstream tasks. These include fine-grained VLN (R2R, RxR-English), VLN with\ndialog history (CVDN), high-level VLN (REVERIE, SOON), long-term VLN (R4R), and VLN in\ncontinuous environments (R2R-CE). Each dataset is split into training, val_seen, and val_unseen\nsets, with R2R, CVDN, REVERIE, SOON, and R2R-CE also containing test splits. The statistics\nfor the training splits are summarized in Table 2 (manually-labeled datasets), and further details can\nbe found in the Appendix.\nWe evaluate our navigator using the standard path-fidelity metrics including Success Rate (SR), Suc-\ncess Rate Weighted by Path Length (SPL) (Anderson et al., 2018a), Goal Progress (GP) (Thomason\net al., 2020), Navigation Error (NE), normalized Dynamic Time Warping (nDTW) (Ilharco et al.,\n2019) and Success Rate Weighted by Dynamic Time Warping (sDTW) (Ilharco et al., 2019). We\nleave the details of these metrics to Appendix.\nAlthough REVERIE and SOON include object grounding tasks, we focus on evaluating navigation\nperformance, as our generated dataset is specifically designed to enhance navigation tasks. We use\nSPL as the primary metric for R2R, REVERIE, SOON, and R2R-CE, nDTW for RxR-English,\nsDTW for R4R, and GP for CVDN."}, {"title": "4.2 FLYWHEEL RUNNING RESULTS", "content": "Table 3 presents the results for both the instruction generator and navigator across all rounds. We\nfollow ScaleVLN (Wang et al., 2023e) to train the navigator with ScaleVLN-HM3D and Prevalent\ndata for augmentation, while we use InternViT (Chen et al., 2024) features for fair comparison. The\ninstruction generator for ScaleVLN is EnvDrop (Tan et al., 2019). In round 1, our new instruc-\ntion generator, fine-tuned on R2R with LoRA using the pre-trained Mantis, significantly surpasses\nEnvDrop. This results in a substantial SR boost for the navigator from 78.6% to 82.4%.\nNavigator and Instruction Generator Improve Each Other. At each round, we use the naviga-\ntor to filter high-quality data FDF to re-train the instruction generator, and use the improved instruc-\ntion generator to refine low-quality instructions LDN to re-train the navigator. Despite the strong\nperformance of the round 1 baseline, the generator is further improved by incorporating navigator-\nfiltered data in round 2. The high-quality data filtered by the navigator leads to +1.5 SPICE and +4.2\nCIDEr. This trend continues in round 3, where SPICE reaches 25.7, while other metrics remain\nstable, demonstrating the crucial role of the navigator in enhancing the instruction generator via data\nfiltering. For the navigator, the data-refining process leads to continuous improvements in naviga-\ntion performance with +1.2% SR in round 2, and +0.8% SR in round 3, underscoring the importance\nof the generator data refinement in enhancing the navigator, as well as the effectiveness of iterative\nnavigator-generator collaboration to build an effective self-refining flywheel."}, {"title": "4.3 ANALYSIS", "content": "Comparison of Different Scoring Functions. We analyzed classical filtering methods will likely\nfail to capture complex path-instruction similarity in the previous discussion. In Table 4, we further\nverify the importance of our navigator-filtering compared to other filtering baselines. Using our\nround 1 instruction generator, we produce instructions for 783 trajectories from the validation unseen\nsplit, rank them based on various scoring functions, and filter the top 400 to assess their similarity to\nGT instructions. 'No filter' refers to the average score of all 783 instructions, and intuitively, more\nsimilar instructions should yield higher NLP metric scores.\nWe compare our navigator's nDTW-score with CLIP-Sim (Radford et al., 2021) and Mantis\nscore (Jiang et al., 2024). CLIP-Sim is computed by averaging image-instruction similarities across\nall observations in the trajectory, while Mantis-score is produced by inputting the path as interleaved\nimage-text pairs and asking Mantis to provide a similarity score. Results show that the Mantis score\nfails to improve over the baseline, likely due to the trajectories is too complex to understand for\nthe MLLM. CLIP-Sim provides a slight SPICE improvement, possibly because it can capture some\nlandmark-level similarities between the trajectory images and the instructions, but does not improve\nSPICE-D as it lacks directional understanding. In contrast, our Navigator-nDTW similarity filter-\ning method successfully identifies high-quality instructions, leading to a substantial improvement,\ndemonstrating our navigator-nDTW captures path-instruction similarities much better than others.\nEffect of Instruction Diversity in Navigator Training\nWe assess the impact of instruction diversity by training the\nnavigator with different numbers of instructions per path\n(#instr = 1, 3, 6, 12) on the MP3D environments, as\nshown in Table 5. We use CLIP-B/16 as the visual fea-\nture to establish a well-known baseline (Li & Bansal, 2023;\nWang et al., 2023e) with the Prevalent dataset, while \"Our\"\nuses instructions generated by our round 2 instruction gen-\nerator. Compared to Prevalent, our instructions consistently\nachieve stronger downstream results at each #instr level, emphasizing the importance of instruc-\ntion quality. Our navigator also benefits significantly when increasing #instr from 1 to 3 and 3 to\n6, while Prevalent's performance saturates after 3, demonstrating that scaling instruction diversity\nwill be more effective when instruction quality is higher. Increasing #instr to 12 yields similar\nresults to 6, suggesting that #instr = 6 is an optimal balance for training.\nEffect of Additional Data in Instruction Generator Training. In Table 6, we examine the im-\nportance of high-quality data in instruction generator training, and the potential scalability of our\npipelines by evaluating the influence of environment numbers. The round-1 generator without sup-\nplementary data serves as the baseline. Adding the ScaleVLN dataset does not improve performance,\nlikely due to its low diversity, which limits generalization in text generation tasks.\nWhen training with the Prevalent dataset, which has greater diversity, performance gains remain\nminimal, possibly because of the data noise, as it also shows low quality in Table 1. In contrast,\nadding data from ours with increased environments (we split FD by environments) consistently"}, {"title": "5 CONCLUSION", "content": "In this work, we introduce a fully automatic self-refining data flywheel to construct a substantially\nhigh-quality VLN dataset for augmentation. We propose to iteratively refine the data with the navi-\ngator and instruction generator working in tandem-the navigator filtering high-quality data to train\na better instruction generator and the instruction generator regenerating better instructions to train a\nbetter navigator, ultimately producing both a strong navigator, instruction generator and a high-\nquality VLN dataset. We thoroughly analyzed the impact of each component in the flywheel,\ndemonstrating that our approach significantly surpasses state-of-the-art methods across multiple\nVLN benchmarks, covering various instruction styles (R2R, CVDN, REVERIE, SOON), trajectory\nlengths (R4R, RxR-English), and control spaces (R2R-CE), as well as instruction generation task on\nR2R. Our self-refining flywheel provides a novel, scalable solution to the data bottleneck in VLN,\nhighlighting the crucial role of instruction quality and alignment in training embodied agents. This\nmethod has the potential to drive future advancements in embodied navigation models and paves the\nway for exploring more sophisticated tasks that rely on high-quality, dynamic, and scalable data."}, {"title": "A APPENDIX", "content": "We first present additional implementation details of our experiments in Section A.1, including\nspecifics of the generator, navigator model, and training procedures. Section A.2 illustrates the\nSRDF pipeline with pseudo code, while Section A.4 provides further details and experiments re-\ngarding our trajectory-encoding template design. Details of datasets and evaluation metrics are\nprovided in Section A.5, with more comprehensive downstream results shown in Section A.6. Fi-\nnally, Section A.7 presents detailed examples of our generated instructions in comparison with other\nbaselines.\nA.1 ADDITIONAL IMPLEMENTATION DETAILS\nNavigator. We use DUET model (Chen et al., 2022b) as our navigator, which integrates global and\nlocal information through a dual-scale graph transformer architecture. This architecture processes\nboth high-level scene representations as well as detailed local features simultaneously, enhancing the\nmodel's capability to interpret language instructions in complex visual contexts. By constructing a\ntopological map in the meanwhile, DUET extends the navigational action space from the current\nviewpoint to all navigable directions encountered, thus improving planning and error correction.\nThe model employs attention mechanisms to balance global scene contexts with local observations,\nsignificantly improving navigation accuracy towards targets based on natural language commands.\nGenerator. We use Mantis (Jiang et al., 2024) as our base model for instruction generation. Mantis\ncomprises a SigLIP vision encoder (Zhai et al., 2023), a multimodal projector, and a LLaMA-3-\n8B-Instruct language model backbone (Dubey et al., 2024). It is first pre-trained on multimodal\nprojector data using LLaVa pre-training, followed by fine-tuning on the multi-image interleaved\nMantis-Instruct dataset.For our task, we initialize the instruction-tuned model, Mantis-8B-siglip- \nllama3, and apply updates only to the added LoRA layers in the language backbone.\nTraining Details. The navigator is trained using similar objectives as in (Wang et al., 2023e),\nincluding Masked Language Modeling and Single Action Prediction. We initialize our model with\nLXMERT (Tan & Bansal, 2019) for the first and second rounds, and with our round-2 pre-trained\nmodel for the third round. During generator training, all parameters except the injected LoRA\nlayers are frozen, and only the LoRA layers are fine-tuned. For navigator training, we initialize the\ninstruction generator with Mantis in the first and second rounds and use the round-2 trained generator\ndirectly for round 3. Additionally, for downstream task supervisions, we encourage the model to go\nback to the viewpoint on the GT path yielding best nDTW to the current progress for R4R and RxR-\nEnglish as their trajectories are not shortest-path, while we use shortest-path supervision as teacher\naction for other tasks.\nA.2 PSEUDO CODE OF SRDF\nAlg. 1 provides detailed pseudo code for our Self-Refining Data Flywheel (SRDF), illustrating the\nprocess described in Section 3.2. Specifically, the pipeline starts with training an initial instruction\ngenerator using seed data. The generator creates training data for the navigator, which is then used to\ntrain a base navigator. The navigator, in turn, filters high-quality data to further improve both itself\nand the generator in subsequent rounds. This iterative process continues for T iterations, refining\ndata quality and improving model robustness with each cycle.\nA.3 SFT DATA TEMPLATE\nWe use the template shown in Figure 3 to construct our SFT data for fine-tuning the instruction\ngenerator, which encodes a trajectory into an interleave image-action sequence. At each viewpoint,\nwe include key views, such as the view when arriving at the viewpoint and the view when leaving\nit. For each view, we also append the corresponding action in raw text after the image tokens,\ncreating a multi-image interleaved format to effectively encode the trajectories. The output is the\ncorresponding instruction of the input trajectory."}, {"title": "A.4 ADDTIONAL EXPERIMENTS", "content": "Fourth-Round Generator-Training. Our method demonstrates the potential for continued per-\nformance improvement with additional refinement rounds. To illustrate this, we conducted a fourth-\nround generator training experiment (which is equivalent to the \u201cround 3 w/ FDG\u201d results in Ta-\nble 9) Following a similar procedure, we generated FDF, trained the generator using this data, and\nobserved consistent performance gains over the third round. Results in Table 10 show a new state-\nof-the-art (SoTA) SPICE score of 26.2, marking an improvement of +2.8 over the previous SoTA\n(as shown in Table 9). These results highlight the scalability and effectiveness of our approach,\nindicating that with sufficient computational resources and time, further rounds of improvement can\nbe sustained.\nEffect of Different Scoring Metrics in Generator Training. We conducted additional experi-\nments to demonstrate that classical methods for language model self-improvement face limitations\nin the VLN context without reliable feedback from the navigator. Specifically, we evaluated two ap-\nproaches: (1) self-score, a self-rewarding method where the language model scores its own outputs,\nand (2) CLIP-score, which uses an external tool (CLIP) to provide similarity scores. In these exper-"}, {"title": "A.7 QUALITATIVE CASE STUDY OF GENERATED INSTRUCTIONS", "content": "In Figure 4 and 5, we visualized some examples of our generated instructions, and compare them\nwith Prevalent (Hao et al., 2020) and ScaleVLN (Wang et al., 2023e) baselines. All the example\ntrajectories in Figure 4, and Figure 5 (a), (b) are collected using recovered environment images from\nScaleVLN (Wang et al., 2023e), while Figure 5 (c), (d) are from MP3D environments.\nRare-Room/Landmark Recognition Ability. Figure 4 demonstrates the strong image-text un-\nderstanding capability of our instruction generator. Specifically, our generator can recognize rare\nobjects, such as dentist chair/room or a grandfather clock, thanks to our interleaved image-action\ntrajectory-encoding design. This design preserves the original abilities of the pretrained MLLM\nwhile effectively encoding trajectories to generate instructions with rich and accurate landmarks. In\ncontrast, the baseline instruction generator fails to capture these rare concepts due to its from-scratch\ntraining paradigm. Instead, it only generates some general landmarks with weak clues.\nDetailed Object-Describing Ability. Figures 4 (c) and (d) illustrate that our instruction generator\ncan describe key objects along the path with greater detail. For instance, while the baseline mentions\nthe bar and the painting successfully, our generator provides more specifics, such as brown leather\nbar stool and large painting on the wall. Such detailed descriptions are crucial for helping the\nnavigator learn richer visual cues. Additionally, in Figure 4 (d), our generator performs slight spatial\nreasoning between objects, resulting in more precise stopping guidance \u2013 the blue and white throw\npillows on the right side of the couch.\nGeneralization to Outdoor Environments. In Figure 5 (a), (b), we demonstrate the ability of our\ngenerator to produce some useful instructions for outdoor environments, even though the model is\ntraining using instructions from indoor environments. For instance, in (a), our generated instruction\nidentifies the glass doors leading outside, which is more distinct the ScaleVLN's table \u2013 still a gen-\neral landmark without a strong viewpoint-specific clue. In (b), the generator successfully identifies\noutdoor landmarks including the car and the buches, while the baseline only knows walkway.\nOCR Ability. Surprisingly, our instruction generator demonstrates interesting OCR capabilities,\nas shown in Figures 5 (c) and (d). In example (c), the generator successfully identifies the words\ncape and plug on the wall, while in example (d), it even identifies a full sentence-Let's start to\nredefine how work gets done. This OCR ability is likely inherited from the pre-trained MLLM, and\nour fine-tuning approach effectively retains this capability, resulting in highly detailed and accurate\nguidance in the generated instructions.\nIdiomatic Expressions. Our generator sometimes uses idiomatic expressions in its instructions.\nAn example is shown in Figure 5 (c), Sample 2, where the generator says, go past the desk then stop\nat the end of the rope. The phrase at the end of the rope usually means that someone has reached the\nlimit of their patience or endurance. In this context, however, it refers to reaching the farthest point\nthat the navigator can proceed\u2014likely the wall. This ability adds diversity to the instructing style,\nmaking the generated instructions more varied and engaging."}]}