{"title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving", "authors": ["Yong Lin", "Shange Tang", "Bohan Lyu", "Jiayun Wu", "Hongzhou Lin", "Kaiyu Yang", "Jia Li", "Mengzhou Xia", "Danqi Chen", "Sanjeev Arora", "Chi Jin"], "abstract": "We introduce Goedel-Prover, an open-source large language model (LLM) that achieves the state-of-the-art (SOTA) performance in automated formal proof generation for mathematical problems. The key challenge in this field is the scarcity of formalized math statements and proofs, which we tackle in the following ways. We train statement formalizers to translate the natural language math problems from Numina into formal language (Lean 4), creating a dataset of 1.64 million formal statements. LLMs are used to check that the formal statements accurately preserve the content of the original natural language problems. We then iteratively build a large dataset of formal proofs by training a series of provers. Each prover succeeds in proving many statements that the previous ones could not, and these new proofs are added to the training set for the next prover. Despite using only supervised fine-tuning, our final prover significantly outperforms the previous best open-source model, DeepSeek-Prover-V1.5, which employs reinforcement learning. On the miniF2F benchmark, our model achieves a success rate of 57.6% (Pass@32), surpassing DeepSeek-Prover-V1.5 by 7.6%. On PutnamBench, Goedel-Prover successfully solves 7 problems (Pass@512), ranking first on the leaderboard. Furthermore, it generates 29.7K formal proofs for Lean Workbook problems, nearly doubling the 15.7K produced by earlier works.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) have demonstrated remarkable capabilities in reasoning tasks, especially in solving mathematical problems (Guo et al., 2025; Yang et al., 2024a). These models excel at reasoning through natural language, which we refer to informal reasoning. However, natural language-based reasoning is difficult to automatically verify by machines, which undermines the reliability of informal reasoning in practical applications. This also makes it more difficult to further improve the reasoning capabilities of language models. In contrast to informal reasoning, formal reasoning allows reasoning in a machine-verifiable format, opening up new possibilities for verification and automation. In particular, proof assistants such as Lean (De Moura et al., 2015; Moura & Ullrich, 2021), Isabelle (Paulson, 1994), and Coq (Barras et al., 1997) provide formal languages that can express reasoning in a way that can be mechanically verified. Thus, it is of great interest to train LLMs to write proofs in these formal languages.\nA significant challenge in training LLMs for theorem proving in formal languages is the scarcity of formalized math statements and proofs. Writing proofs for theorems expressed"}, {"title": "2 Related Work", "content": "Automated theorem proving. Automated theorem proving (ATP) is a long-standing problem in symbolic AI (Robinson & Voronkov, 2001). Traditional approaches represent theorems in first-order logic and prove them using decision procedures (De Moura & Bj\u00f8rner, 2008; Barbosa et al., 2022) and search (Kov\u00e1cs & Voronkov, 2013; Schulz et al., 2019). The proof search has been enhanced by replacing handcrafted heuristics with machine learning techniques (Urban et al., 2011; Kaliszyk et al., 2018). However, approaches based on first-order logic struggle to scale to complex theorems and often do not yield human-readable proofs.\nIn recent years, learning-based theorem proving has undergone a significant transformation. A notable approach, introduced by Polu & Sutskever (2020), involves leveraging large language models to assist in theorem proving with proof assistants such as Lean (De Moura et al., 2015; Moura & Ullrich, 2021) and Isabelle (Paulson, 1994). Follow-up research has explored various avenues, such as retrieving useful lemmas (Irving et al., 2016; Miku\u0142a et al., 2024; Yang et al., 2024b), utilizing Monte Carlo tree search for proof discovery (Lample et al., 2022), and harnessing the capabilities of large language models (LLMs) for natural language reasoning (Jiang et al., 2022; Lin et al., 2024). Notably, Polu et al. (2023) was the first to apply expert iteration (Anthony et al., 2017) to theorem proving. This method alternates between two phases: (1) attempting to prove unsolved theorems and (2) enhancing the prover by incorporating newly discovered proofs into its training data. Expert iteration has yielded significant improvements in several recent provers (Wu et al., 2024; Xin et al., 2024b), including our Goedel-Prover.\nMost automated theorem provers operate in a stepwise manner, generating individual proof steps that are then assembled into complete proofs using proof search algorithms. Recently, researchers have shown that generating entire proofs is feasible (First et al., 2023; Xin et al., 2024a; Wang et al., 2024). This approach avoids the costly search process, resulting in lower latency and potentially offering a more efficient use of computational resources during testing. While Goedel-Prover also generates whole proofs, our data and methodology can, in principle, be adapted to develop stepwise provers as well."}, {"title": "3 Method", "content": "We begin by translating informal statements (expressed in natural language) into formal statements (represented in Lean). Using these formal statements, we iteratively train our prover with proofs generated by the prover and verified by the Lean compiler. The details of each step are elaborated in the following parts."}, {"title": "3.1 Statement Formalization", "content": "We first train the statement formalizers to translate informal statements in Numina into formal statements as shown in Figure 2. To enhance the diversity of formalized statements, we train two models to formalize informal statements.\n\u2022 Formalizer A: We train the Formalizer A model using Formal and Informal (F-I) statement pairs sourced from Lean Workbook.\n\u2022 Formalizer B: We employ Claude-sonnet-3.5 to formalize 230K statements from Numina. From this set, we extract 170K statements that successfully passed Lean compilation. These 170K F-I statement pairs are then used to train Formalizer B.\nBoth Formalizer A and B are trained using supervised fine-tuning with Qwen2.5-Coder-32B. The training of these two formalizers takes less than 24 hours on 8 H100 GPUs. Table 1 presents two examples in which both Formalizer A and Formalizer B yield reasonable formalizations. However, our final prover exhibits varying performance on these formalized statements, highlighting the influence of formalization style on model effectiveness.\nQuality assessment. We employ two tests to assess the quality of the formalized statements. First, the formalized statement must conform to Lean syntax and can successfully compile, with the potential proof replaced by the placeholder \u201c:= by sorry\u201d. This syntax check is known as the Compiling Correctness (CC) Test in the literature (Ying et al., 2024a). Second, the formalized statement must accurately capture the original informal problem, incorporating all assumptions, conditions, and implicit definitions. We refer to this second test as the Faithfulness and Completeness (FC) Test. For the FC test, we use Qwen2.5-72B-Instruct with the prompts shown in Figure 4. For each formalized statement, we generate four independent judgments, and the FC score is calculated as #{\u201cAppropriate\u201d in four Judgments}/4. For example, if the four judgments produced by Qwen2.5-72B-Instruct"}, {"title": "3.2 Expert Iteration", "content": "After obtaining a large collection of formalized statements in Section 3.1, we employ expert iteration to train the prover (Liu et al., 2024; Wu et al., 2024; Li et al., 2024b), which is illustrated in Figure 3. Specifically, we first utilize DeepSeek-Prover-V1.5-RL to generate 16 proofs for each statement. We then verify these proofs with the Lean compiler. If at least one proof solves the statement, we retain one proof per statement. In cases where multiple proofs are available, we randomly sample one solution. These collected proofs are used for supervised fine-tuning (SFT) based on DeepSeek-Prover-V1.5-Base, resulting in the iter-1 prover. We continue this expert iteration process; each time, we use the iter-k prover to generate answers and cumulatively collect correct solutions to train DeepSeek-Prover-V1.5-Base for the next iteration, the iter-(k + 1) prover. Refer to Appendix A for more details on each iteration.\nWe experiment with learning rates of $1 \\times 10^{-4}$ and $5 \\times 10^{-5}$, training for either 1 or 2 epochs. We use the packing trick (Tunstall et al., 2022) with a small batch size of 8 to speed up the training. In each iteration, the training time for 1 epoch is approximately 12 hours using 4 H100 GPUs. The inference time for the 1.78M statements set by Pass@16 is 6 hours, utilizing 64 H100 GPUs. Additionally, the verification time for these proofs requires 10 hours with 8,000 CPUs."}, {"title": "4 Results", "content": "Benchmarks. Following the works of (Wang et al., 2024; Xin et al., 2024a; Wu et al., 2024; Li et al., 2024b), we primarily use miniF2F (Zheng et al., 2021) as our main evaluation benchmark. We also track the problems solved by our prover in Lean Workbook (Ying et al., 2024a) and investigate the performance on ProofNet (Azerbayev et al., 2023) and PutnamBench (Tsoukalas et al., 2024). Additionally, we uniformly sample a subset from our formalized dataset to create a held-out evaluation dataset. Below, we provide descriptions of each dataset.\n\u2022 miniF2F (Zheng et al., 2021) is a formal theorem proving benchmark, consisting of 488 problem statements (244 validation and 244 test problems) in Lean. The problems are drawn from high-school exercises, as well as high-school level competitions including the AIME, AMC, and the International Mathematical Olympiad (IMO). The original benchmark was released in Lean 3, and for our analysis, we use the version of miniF2F in Lean 4.9.0 provided by Xin et al. (2024a).\n\u2022 ProofNet (Azerbayev et al., 2023) is a formal theorem proving benchmark of undergraduate-level mathematics, consisting of 371 problem statements in Lean (185 validation and 186 test problems). The problems are primarily drawn from undergraduate pure mathematics textbooks, covering topics such as real and complex analysis, linear algebra, abstract algebra, and topology. The original benchmark was released in Lean 3, and for our analysis, we use the version of ProofNet in Lean 4.9.0 provided by Xin et al. (2024a).\n\u2022 Lean Workbook (Ying et al., 2024a) is a large-scale Lean 4 problem set formalized from natural language math problems (mainly from the forum AOPS), which consists of 140K statements in Lean 4. We also monitor the problems solved by our model during the expert iteration process. Notably, the problem set from Lean Workbook is included in this training, which is consistent with DeepSeek-Prover-V1.5 (Xin et al., 2024a) and InternLM2.5-StepProver (Wu et al., 2024).\n\u2022 PutnamBench (Tsoukalas et al., 2024) is a formal theorem proving benchmark on competition mathematics problems sourced from the William Lowell Putnam Mathematical Competition years 1962 - 2023. PutnamBenchcomprises 644 Lean 4 statements, covering algebra, analysis, number theory, geometry, combinatorics, probability and set theory."}, {"title": "5 Discussion", "content": "We delve into the characteristics of proofs generated by Goedel-Prover-SFT and discuss potential directions for improvement, particularly regarding the proof style adopted by the model, the role of search as well as online interaction in proof generation, and the integration of external symbolic computation tools such as SymPy.\nThe Proof Style. We observe that the proofs provided by Goedel-Prover-SFT often rely on high-level tactics such as nlinarith,simp_all,and norm_num, among others. These high-level tactics handle multiple reasoning steps internally, delegating the resolution of intermediate steps to their built-in automation. For example, the nlinarith tactic can automatically solve certain linear and non-linear equalities and inequalities. Figure 9 shows a typical proof generated by our prover. The first several steps involve only trivial transformations of the original problem, whereas the final line uses nlinarith to immediately achieve the goal. Whether this style of proof is sufficient for complex reasoning remains an important area for exploration.\nSearch and online interaction. Currently, Goedel-Prover-SFT generates the entire proof for the problem at once, without receiving further feedback. While our current approach is appealing in terms of computation, incorporating search and interaction in future work could enhance performance. For example, once a tactic is generated by our prover, it can interact with the Lean compiler to receive feedback on how the goal changes after the tactic is applied. This information can then be utilized in generating the next tactic, potentially improving the overall proof strategy (Wu et al., 2024).\nSymPy. Future work may aim to leverage other software packages to enhance Lean's capabilities. For instance, Lean's ring tactic can handle algebraic simplifications by applying axioms such as distributivity, associativity, and commutativity. However, a combination of tactics is required for non-algebraic transformations of transcendental functions, such as logarithmic and trigonometric functions, and other advanced simplifications beyond commutative rings. We explored using a Python-based computer algebra system, SymPy (Meurer et al., 2017), to simplify complex expressions in theorem statements and feed the simplified form into the prover. Specifically, we parse equations of the form $A = B$ within the goals of Lean theorem statements, construct the SymPy expression $A - B$, and then apply the simplify method in Lean. This procedure directly solves 9.4% of miniF2F by simplifying the statements to $0 = 0$. In addition, it solves 0.8% of the problems in miniF2F that were unsolved by Goedel-Prover-SFT with Pass@32, but did not improve Goedel-Prover-SFT with Pass@3200. Thus, SymPy simplification is not part of any of our reported results. However, we think such procedures need further exploration."}, {"title": "A Expert Iteration Details", "content": "The main training pipeline is illustrated in Section 3.2. When we implement the expert iteration algorithm, we gradually add the data. From iter-0 to iter-3, we gradually add the statements formalized by Claude-sonnet-3.5. At iter-3, we train the Formalizer B and add the formalized statements generated by Formalizer B for iter-4 to iter-6. At iter-7, we begin to add the statements generated by Formalizer A. We also add Mathlib4 data into the training dataset for better ProofNet performance when starting from iter-6."}, {"title": "B More Examples", "content": "B.1 Mathlib4 example\nFigure 7 and 8 show the statement and proof in mathlib4 and miniF2F respectively. It can be easily seen that both the statement and proof rely on pre-defined objects. Unlike miniF2F statements, the example in Figure 7 can not even pass the lean compilation, given that pre-defined objects are missing.\ntheorem ndrec_eq_ndrecC: @Acc.ndrec = @Acc.ndrecC := by\nfunext ar motive intro a t rw [Acc.ndrec, rec_eq_recC, Acc.ndrecC]"}, {"title": "B.2 Proof Style", "content": "The proofs from Goedel-Prover-SFT often rely on high-level tactics like nlinarith,simp_all, and norm_num, which handle complex equalities and inequalities while leaving intermediate steps to powerful methods. As shown in Figure 9, initial steps typically involve trivial transformations, with the final step quickly resolving the goal using nlinarith. The effectiveness of this proof style for complex reasoning is still under exploration."}]}