{"title": "Online Multi-Label Classification under Noisy and Changing Label Distribution", "authors": ["YizhangZou", "XuegangHu", "PeipeiLi", "JunHu", "YouWu"], "abstract": "Multi-label data stream usually contains noisy labels in the real-world applications, namely occuring in both relevant and irrelevant labels. However, existing online multi-label classification methods are mostly limited in terms of label quality and fail to deal with the case of noisy labels. On the other hand, the ground-truth label distribution may vary with the time changing, which is hidden in the observed noisy label distribution and difficult to track, posing a major challenge for concept drift adaptation. Motivated by this, we propose an online multi-label classification algorithm under Noisy and Changing Label Distribution (NCLD). The convex objective is designed to simultaneously model the label scoring and the label ranking for high accuracy, whose robustness to NCLD benefits from three novel works: 1) The local feature graph is used to reconstruct the label scores jointly with the observed labels, and an unbiased ranking loss is derived and applied to learn reliable ranking information. 2) By detecting the difference between two adjacent chunks with the unbiased label cardinality, we identify the change in the ground-truth label distribution and reset the ranking or all information learned from the past to match the new distribution. 3) Efficient and accurate updating is achieved based on the updating rule derived from the closed-form optimal model solution. Finally, empirical experimental results validate the effectiveness of our method in classifying instances under NCLD.", "sections": [{"title": "I. INTRODUCTION", "content": "Online multi-label classification (OMC) aims to instantly annotate multi-label streaming objects that arrive sequentially, which are associated with two or more labels [1], [2]. Multi-label data stream is very common in real-world applications. For example, the online image recognition system automatically annotates multiple objects in the streaming images [3]; in movie recommendation, the online customers accessing the movie website are recommended several movies of different types, e.g., tragedy, science fiction and horror movies [4]; in music emotion classification, different types of emotions are recognized from continuous pieces of music [5].\nPure and noiseless multi-label data is rarely available in the online environment due to labelling costs and human error [6], [7], noisy labels are often present in the label set and appear in relevant or irrelevant labels, leading to performance degradation. In particular, existing OMC works aim to instantly obtain accurate continuous label outputs, and partition the outputs to provide reliable predictions over a reasonable threshold. For example, the work of [8] integrates the label ranking information and the label scoring information into an online learning framework, where the former provides an accurate estimated ranking between labels and the latter ensures the rationality of using zero as a fixed threshold. However, in the case of noisy labels, incorrect labels and corresponding ranking information will mislead the model, and the error will propagate over time, causing inaccurate and unstable OMC performance. Though some new OMC works have been proposed recently to tackle the limited-supervision case such as semi-supervised classification [9] and classification with missing labels [10], the OMC with noisy labels is first formulated and addressed to the extent of our knowledge.\nFurthermore, as a widespread type of concept drift, the changes in the distribution of labels are formulated and ad- dressed by previous OMC works [11], [12]. Together with the presence of noisy labels, real-world applications raise the need to perform online classification under noisy and changing label distribution (NCLD). In line with [8], throughout the paper we also consider the two extreme cases of concept drift in the ground-truth label distribution: 1) from a single- label distribution to a multi-label distribution (i.e., concept growth); 2) from a multi-label distribution to a single-label distribution (i.e., concept reduction). Note that under NCLD, only the noisy label distribution is observed and available instead of the real distribution, resulting in the performance degradation of previous methods that require the clean labels for the detection and adaptation to distribution changes. For example of concept growth, when the single-label distribution drifts to the multi-label distribution, existing accuracy-based detection methods [13], [14] can not give an indication of drift because of the incalculable classification loss utilizing the unavailable ground-truth labels. Similarly, distribution- based detection methods [15], [16] also fail because the label cardinality increase [17] brought by concept growth cannot be measured based on the noisy label distribution, and it is also difficult to track and adapt to the ground-truth label distribution for the adaptive OMC methods without concept drift detection [8], [9].\nIn conclusion, there is a gap in considering streaming noisy labels for OMC, let alone performing accurate OMC under noisy and changing label distribution (NCLD). Motivated by this, we propose a novel NCLD-oriented OMC method. As stated in [18], [19], we also formulate the convex objective which incorporates both label scoring and ranking regular- ization component for the verified high performance. Since the noise-induced label scoring and ranking information will mislead the model as mentioned above, it is necessary to enhance the robustness of label scoring and ranking regulariza- tion terms to noisy labels respectively. Additionally, detecting and adapting to the ground-truth concept drift also matters in classification under NCLD. To reduce the time complexity of our method for high-dimensional data, the Extreme learning machine (ELM) framework is used due to its high efficiency. ELM is a single-layer feedforward neural network that reduces the high dimensionality via a non-linear mapping with the random input weights, which is widely used in online learning [20], [21]. In this paper, a novel ELM-based online multi- label classification method under NCLD is proposed, which utilizes the label scoring and ranking regularized framework. The contributions of this paper are summarized as follows:\nBased on the idea of label reconstruction, we develop the new label scoring term, which scores the label outputs of each instance jointly taking into account the observed labels and the label scores of its local nearest neighbours, ensuring that the scores of noiseless labels are more credible than noisy labels for better model fitting.\nUsing the unbiased estimator, we derive an unbiased label ranking term with respect to the ranking loss under the noiseless case, thus constructing the robust ranking order between the ground-truth relevant and irrelevant labels.\nUnder NCLD, we derive an unbiased statistic with respect to label cardinality [17] to detect ground-truth concept drift, and propose two adaptation strategies to maintain robust performance during concept drift.\nWe derive an efficient closed-form solution for the unified objective and the corresponding sequential update rule, thus preserving all information learned from the past without storing it for the online model.\nWith contributions 1) and 2), our work achieves that the scores of real relevant labels and irrelevant labels can follow the bipartition over the fixed threshold of 0. Thus, during the online classification process, we can choose 0 as a fixed threshold to obtain robust binary predictions under noisy label distributions. Contributions 1), 2) and 3) together provide robustness to both noisy labels and concept drift (i.e., NCLD). Finally, contribution 4) enables model efficiency and accuracy, where efficiency is guaranteed by the ELM framework and the closed-form solution ensures competitive accuracy compared to batch versions.\nThe rest of this paper is organized as follows: Section II reviews related work on OMC and online classification with limited supervision. Section III gives a detailed introduction to our method. Next, Section IV presents the experimental results and Section V concludes the paper by summarizing our findings and suggesting possible directions for future research. Finally, Section VI gives the detailed proofs of the propositions proposed in the paper."}, {"title": "II. RELATED WORK", "content": "The related arts of our work are about online multi-label classification and online classification with limited supervision areas.\nA. Online Multi-Label Classification\nOnline multi-label classification performs instant annotation on multi-label objects arriving in sequence, which can be broadly categorized into three types, namely replay-based methods, regularization-based methods and ensemble meth- ods. As a lazy learning-based method, replay-based methods maintain a data summary that approximates the latest data distribution and train the model based on it when a new instance comes, the problem of concept drift can be properly tackled since the data summary is dynamically updated. The work of [23] acts online classification by utilizing a weighted clustering model, which considers the change of data distribu- tion. OnSeMl [9] constructs two buffer pools storing the latest instances with or without label annotations respectively to perform semi-supervised online classification. Regularization- based methods continuely update the online model based on the model induced by the past data and the current incoming data. The work of [24] transforms the labels into a continuous value and applies a regression algorithms to perform online classification. Aiming at detecting the potential emerging labels, MuENL [25] sequentially updates the old classifier built for known labels and collaboratively builds the new classifier for each new label. Ensemble methods, different from single- model methods, maintain and update multiple models with high classification accuracy on classifying the latest data, and integrate the multiple model outputs as the final classification results. The work of [26] introduced dynamically-weighted stacked ensemble learning method to assign optimal weights to sub-classifiers.\nB. Online Classification with Limited Supervision\nSince the fully-supervised data is hard and expensive to collect in real-world applications, many works is proposed to study different settings of online classification with limited supervision. The state of the arts of limited-supervised online classification can be categorized into multi-class methods and multi-label methods according to the data type to be classified. For multi-class methods, Wang et.al [27] proposed and solved online partial label learning where each data example is associated with multiple candidate labels. To further handle the possibility that new classes emerge in open and dynamic environment, the work of [28] incorporates an ensemble-based detector to identify new classes and label disambiguation process to tackle candidate labels. The work of [29] considers a more general noisy case that the instances belonging to one class may be assigned to another class, and acts stream clas- sification based on the estimated noise transition matrix. For multi-label methods, Li et.al [9] addressed semi-supervised online multi-label classification via label compression and local smooth regression, while the work of [21] integrates the kernel function into the well-developed ELM framework for online semi-supervised classification. To adapt to the environ- ment with missing labels, Bakhshi et.al [10] uses a simple imputation strategy with a selective concept drift adaptation"}, {"title": "III. OUR METHOD", "content": "In this section, the newly developed objective function based on the label scoring and ranking framework is proposed to deal with the noisy label problem, whose closed-form solution indicates the efficient model update and high model performance. Next, the sequential update rules are derived in detail. Finally, to handle two cases of noisy and changing label distributions, we derive an unbiased statistic with respect to label cardinality to detect the potential concept drift, and propose two novel strategies to adapt to it.\nA. Problem Formulation\nFirstly we give the problem formulation of OMC with noisy labels. Given a multi-label data stream $D = [D_i]$, $i = 1, 2, ...$ with potentially infinite number of data chunks, each chunk $D_i = {X_i, Y_i}$ equally contain $N$ data instances. Within it, $X_i = [x_i^t], t \\in [N]$ indicates the feature space in $D_i$ ($[N] = {1, ..., N}$) and $Y_i = [y_i^t], t \\in [N]$ represents the observed label space in $D_i$ with noisy labels, where $x_i^t \\in R^d, y_i^t \\in R^q$ denote the $d$-dimensional feature vector and $q$-dimensional label vector of the $t$-th instance in $D_i$ respectively. In data chunk $D_i$, if the $j$-th label of the $t$-th instance is tagged as relevant, $y_{ij}^t = 1$, conversely $y_{ij}^t = -1$. Correspondingly, we denote the ground-truth label matrix as $G_i = [g_{ij}^t], t \\in [N]$, which is unavailable in the process of online classification. Instead, the noisy rates throughout the online data with respect to each class label are known:\n$p (Y_{ij} = -1 | g_{ij} = +1) = p_1^2$,\n$p (Y_{ij} = +1 | g_{ij} = -1) = p_2^2$,\n$\\forall i \\geq 1, j \\in [q], p_1^2 + p_2^2 < 1$.\nWhere $p_1^2, p_2^2$ denote the rates that relevant (irrelevant) $j$-th labels are flipped into irrelevant (relevant) labels.\nGiven the data stream $D$ with the noisy rate defined above, when $D_i$ arrives at time $i$, the task of OMC with noisy labels is to utilize the model $\\Phi_{i-1}$ learned from past data $[D_u], u \\in [i - 1]$ to score the labels of each instance in $D_i$ as $O_i$, thus, as mentioned in Section I, the relevant label set of the $t$-th instance is predicted as $\\hat{y}_i^t = {y_{ij}^t|o_{ij}^t > 0, j \\in [q]}$, then $Y_i$ is observed and the $\\Phi_i$ is obtained based on the sequential update rule. The details of performing robust OMC with noisy labels are given in the following section III-B.\nAdditionally, considering the concept drift in the ground- truth label distribution, we formulate two cases of NCLD as:"}, {"title": "B. OMC with Noisy Labels", "content": "Without considering the concept drift, we first formulate the objective function robust to noisy labels. As mentioned in Section I, two parts are incorporated in the objective including the label scoring and label ranking term. With the online model $\\Phi_{i-1}$ trained based on $[D_u], u \\in [i-1]$, the label scores of the $t$-th instance in $D_i$ is computed as $o_i^t = h_i^t \\Phi_{i-1} \\in R^q$ where $h_i^t$ is obtained with $x_i^t$ via ELM. The $j$-th component $o_{ij}^t$ of $o_i^t$ indicates the score with respect to the $j$-th label. Usually for noiseless data, the mean square error is formulated as:\n$\\min_{\\Phi} \\frac{1}{2}||O - Y||_F^2 + \\frac{\\alpha}{2}||\\Phi||_2^2$\ns.t. $O = H\\Phi$\nwhere $O \\in R^{N \\times q}$ denotes the scoring matrix and each element in the $t$-th row and the $j$-th column corresponds to each $o_{ij}^t$ with respect to the i-th chunk (for simplicity, below we abbreviate the symbol $i$ without confusion), the second term gives the $L_2$-regularization constraint on the model coefficient matrix $\\Phi \\in R^{L \\times q}$ and $\\alpha$ is the regularization factor. However, the first term indicates the optimal scores of each instance-label pair $o_{ij}^t = Y_{tj}$, which leads to the inferior solution due to the potential noisy label $Y_{tj}$.\nTo handle noisy labels, we utilize the relation between each instance regarding feature space to reconstruct the label scores. Firstly, we construct a weighted directed graph G, which is instantiated using K-Nearest-Neighbours error minimization based on the feature space $X^i = [x_i^t], t \\in [N]$ in the current chunk $i$:\n$\\min_{S \\in R^{N \\times N}} ||X^i - SX^i||_F^2$\ns.t. $S \\geq 0, S1_N = 1_N$,\n$S_{n,n} = 0, S_{n,m} = 0 \t (m \\notin N(n))$"}, {"title": "C. Sequential Update Rule", "content": "1) Initialization Phase: Before online classification, it is common to train a base model $\\Phi_0$ on the initialization chunk $D_0 = {X_0, Y_0}$. According to Eq. (13), the optimal solution meets:\n$\\Phi_0 = K_0^{-1} H_0^T M_0$\nwhere $K_0 = \\alpha I + H_0^T R_0 H_0 \\in R^{d \\times d}$, $M_0 = \\beta Y_0 - \\gamma A_0 \\in R^{N \\times q}$, $H_0$ is calculated by Eq. (2), and the definitions of $R_0$ and $A_0$ have been given in the Section-III-B."}, {"title": "D. OMC under NCLD", "content": "As mentioned in Section III-A, we consider two types of NCLD here: 1) the ground-truth concept growth $D_S^\\mathcal{G} \\rightarrow D_M^\\mathcal{G}$; 2) the ground-truth concept reduction $D_M^\\mathcal{G} \\rightarrow D_S^\\mathcal{G}$, where $D_S^\\mathcal{G}$ and $D_M^\\mathcal{G}$ denote the stream data with the single-label and multi- label distributions respectively. With the robustness of the model to noisy labels well-established, the remaining problem is about how to detect the ground-truth concept drift given the noisy data stream D. Based on the intuition that the ground-truth cardinality [17] of each data chunk can be used to track the concept drift ($LCard(D) = \\sum_{t=1}^{N} \\sum_{j=1}^{q} I{g_{tj}=1}$),\nthe following proposition is proposed to give an unbiased estimation of the ground-truth cardinality given the noisy observation $D_i = {X_i, Y_i}$:\nProposition 3. Given the noisy and the ground-truth stream data distribution denoted as D and $D_G$, the following equality w.r.t. the t-th instance holds:\n$E_{(x,y)~D_G} (LCard(x_t, y_t)) = E_{(X,Y)~D} (\\sum_{j=1}^{q} I{\\{y_{tj}=1\\}}W_{t,j})$\n$=\\sum_{j=1}^{q} E_{(X,Y_{tj})~D} {\\{y_{tj}=1\\}} \\frac{P_{D_G} (Y_{tj}|x_t)}{P_D(Y_{tj}|x_t)})$\n$= \\sum_{j=1}^{q} I_{\\{y_{tj}=1\\}} \\frac{P_D (Y_{tj}|x_t) - p^2_{-Y_{tj}}}{(1 - p^2_1 - p^2_2) P_D (Y_{tj}|x_t)}$$\nwhere $I(\u22c5)$ is the indicator function, $I(x) = 1$ if the predicate $x$ holds, conversely $I(x) = 0$.\nTherefore, the cardinality of the data chunk $D_i$ can be reliably estimated via the the empirical form of Eq. (21):\n$LCard(D) = \\frac{1}{N} \\sum_{t=1}^{N} \\sum_{j=1}^{q} I_{\\{y_{tj}=1\\}}W_{t,j}$. We use the car- dinality difference between two adjacent chunks to detect the potential concept drift, since not only concept growth but also concept reduction will lead to a significant cardinality differ- ence. In other words, if $|LCard(D_i) - LCard(D_{i-1})| > \\epsilon_i$, holds when the concept drift occurs on the data chunk $D_i$, where the following proposition computes $\\epsilon_i$ based on the Hoeffding inequality [33]:\nProposition 4. Given the label cardinality estimation $[LCard(x_t)]$, $t \\in [N]$ in the current data chunk $D_i$, the threshold with respect to the i-th round can be calculated as:\n$\\epsilon_i = R_i \\sqrt{\\frac{ln(2/\\delta)}{2N}}$\nwhere $R_i = \\max_t[LCard(x_t)] - \\min_t[LCard(x_t)], t \\in [N]$ and $\\delta \\in (0, 1)$ denotes the confidence value.\nTherefore, if we find that $|LCard(D_i) - LCard(D_{i-1})| > \\epsilon_i$, we can conclude that the ground truth concept drift is occurring on the i-th data chunk. Once the concept drift is detected, we need to update the model for drift adaption [34]. The following two adaption methods are provided:\n1) Retrain the model: Since the data distribution changes from one to the other, the most straightforward strategy is to abandon the past model established on the old data $[D_u], u \\in [i - 1]$ and retrain the model on the new data $D_i$. Thus, it can be achieved by simply setting $\\Phi_{i-1} = 0$ when the concept drift is detected on $D_i$, then initializing $\\Phi_i$ only with $D_i$ and retraining the new model $\\Phi_i$ based on $D_i$, $P_i$ and the reset $\\Phi_{i-1}$ using Eq. (20). However, this update strategy may be too aggressive to retain valuable information learned from the past."}, {"title": "V. LIMITATIONS AND FUTURE WORKS", "content": "Our work utilizes the given noisy rates to derive unbiased objective of online model, which is usually unavailable in real-world applications. It is more reasonable and practical to estimate the noisy rate as the data chunks arrive sequentially, rather than with the direct access. Therefore, our further work is to design an online framework that aims to classify the data with noisy label distributions without knowing noisy rates in advance, but by estimating the noisy rates during the process of online classification."}, {"title": "VI. THE PROOF", "content": "In this section, we provide a comprehensive set of proofs for the propositions presented in the previous sections.\nA. Proof of Proposition 1\nIn terms of Eq. (5), the optimization term w.r.t. each $o_{tj}$ can be formulated as:\n$\\min_{O_{tj}} \\frac{\\beta}{2} ||O_{tj} - Y_{tj} ||^2 + \\frac{1-\\beta}{2} ||O_{tj} - \\sum_{n}S_{t,n}O_{nj}||^2, n \\in N(t)$\nof which the gradient w.r.t. each $O_{tj}$ is: $\\nabla_{O_{tj}} = \\beta(O_{tj} - Y_{tj}) + (1 - \\beta) (O_{tj} - \\sum_{n} S_{t,n}O_{nj}), n \\in N(t), t \\in [N]$. Since the convexity of the objective in Eq. (5) indicates the Karush-Khun-Tucker condition [38], for each optimal label scores $O_{tj}$ must meet $\\nabla O_{tj} = 0$, hence the proof is completed."}]}