{"title": "Revolutionizing Encrypted Traffic Classification with MH-Net: A Multi-View Heterogeneous Graph Model", "authors": ["Haozhen Zhang", "Haodong Yue", "Xi Xiao", "Le Yu", "Qing Li", "Zhen Ling", "Ye Zhang"], "abstract": "With the growing significance of network security, the classification of encrypted traffic has emerged as an urgent challenge. Traditional byte-based traffic analysis methods are constrained by the rigid granularity of information and fail to fully exploit the diverse correlations between bytes. To address these limitations, this paper introduces MH-Net, a novel approach for classifying network traffic that leverages multi-view heterogeneous traffic graphs to model the intricate relationships between traffic bytes. The essence of MH-Net lies in aggregating varying numbers of traffic bits into multiple types of traffic units, thereby constructing multi-view traffic graphs with diverse information granularities. By accounting for different types of byte correlations, such as header-payload relationships, MH-Net further endows the traffic graph with heterogeneity, significantly enhancing model performance. Notably, we employ contrastive learning in a multi-task manner to strengthen the robustness of the learned traffic unit representations. Experiments conducted on the ISCX and CIC-IoT datasets for both the packet-level and flow-level traffic classification tasks demonstrate that MH-Net achieves the best overall performance compared to dozens of SOTA methods.", "sections": [{"title": "Introduction", "content": "As advancements in computer network technology continue and various devices connect to the Internet, user privacy becomes increasingly vulnerable to malicious attacks. While encryption technologies like VPNs and Tor (Ramadhani 2018) offer protection to users (Sharma, Dangi, and Mishra 2021; Xiao et al. 2024), they can paradoxically serve as tools for attackers to conceal their identities. Traditional data packet inspection (DPI) methods have lost effectiveness against encrypted traffic (Papadogiannaki and Ioannidis 2021). Designing a universally effective method to classify an attacker's network activities (e.g., website browsing or application usage) from encrypted traffic remains a formidable challenge.\nIn the past few years, many methods have been proposed to enhance the capability of encrypted traffic classification techniques. Among them, statistic-based methods (Taylor et al. 2016; Hayes and Danezis 2016; van Ede et al. 2020; Panchenko et al. 2016; Xu, Geng, and Jin 2022) generally rely on hand-crafted traffic statistical features and then leverage a traditional machine learning model for classification. However, they require heavy feature engineering and are susceptible to unreliable flows (Zhang et al. 2023). With the burgeoning of representation learning (Le-Khac, Healy, and Smeaton 2020), some methods also use deep learning models to conduct traffic classification, such as pre-trained language models (Lin et al. 2022; Meng et al. 2022), neural networks (Liu et al. 2019; Zhang et al. 2023; Zhao et al. 2023), etc. Although these methods demonstrate competitive performance, they fail to sufficiently uncover the fine-grained correlations between traffic bytes, which can be attributed to the following two disadvantages. (1) Rigid information granularity constrained by bytes. Most existing methods treat a byte as an indivisible unit by default, which ignores the diverse granularity of information contained in traffic data. To give a practical example, a Chinese character is represented by two bytes, while an English character is represented by only one byte, indicating that traffic data contains information of different granularities universally (note that the granularity is not necessarily bytes, it may also be bits). (2) Lack of consideration of multiple correlation types between bytes. Current methods mix the correlation of bytes at different positions in the byte sequence, which overlooks and fails to utilize the differences between diverse types of correlation (e.g., the correlation types between bytes in the header and bytes in the payload are different). Consequently, how to uncover and leverage the potential fine-grained correlations between traffic bytes to enhance traffic classification is a salient problem.\nTo address the above challenges, in this paper, we propose a novel model named MH-Net, which classifies network traffic using multi-view heterogeneous traffic graphs."}, {"title": "Related Work", "content": "Flow-level Traffic Classification Methods. Flow-level traffic classification methods aim to classify traffic flows, which can be summarized into three categories.\n\u2022 Statistical Feature Based Methods. Many methods use statistical features to represent packet properties and utilize traditional machine learning models for classification. App-Scanner (Taylor et al. 2016) extracts features from traffic flows based on bidirectional flow characteristics, while CUMUL (Panchenko et al. 2016) uses cumulative packet length as its feature. ETC-PS (Xu, Geng, and Jin 2022) strengthens packet length sequences by applying the path signature theory, and hierarchical clustering is also leveraged for feature extraction by Conti et al. (Conti et al. 2015).\n\u2022 Fingerprinting Matching Based Methods. Fingerprinting denotes traffic characteristics and is also used in traffic identification. FlowPrint (van Ede et al. 2020) generates traffic fingerprints by creating correlation graphs that compute activity values between destination IPs. K-FP (Hayes and Danezis 2016) uses the random forest to construct fingerprints and identifies unknown samples through k-nearest neighbor matching.\n\u2022 Deep Learning Based Methods. Deep learning has demonstrated powerful learning abilities, and many traffic classification methods are based on it. RBRN (Zheng et al. 2020), DF (Sirinam et al. 2018), and FS-Net (Liu et al. 2019) all use statistical feature sequences (e.g., packet length sequences) as inputs for convolutional neural networks (CNNs) or recurrent neural networks (RNNs). Additionally, there are some methods using raw bytes as features. EBSNN (Xiao et al. 2022) combines RNNs with the attention mechanism to process header and payload byte segments. ET-BERT (Lin et al. 2022) conducts pre-training tasks on large-scale traffic datasets to learn a powerful raw byte representation, which is time-consuming and expensive. Graph neural networks (GNNs) are another model that can be used for traffic classification tasks. GraphDApp (Shen et al. 2021) builds traffic interaction graphs from traffic bursts and uses GNNs for representation learning. TFE-GNN (Zhang et al. 2023) employs point-wise mutual information (Yao, Mao, and Luo 2019) to construct byte-level traffic graphs and designs a traffic graph encoder for feature extraction. YaTC (Zhao et al. 2023) features a masked autoencoder-based traffic transformer to enable efficient feature extraction while boosting performance.\nPacket-level Traffic Classification Methods. In contrast, packet-level traffic classification methods identify diverse categories of each network packet. Securitas (Yun et al. 2015) generates n-grams for raw bytes and utilizes Latent Dirichlet Allocation (LDA) to form protocol keywords as features, followed by SVM, C4.5 decision tree, or Bayes network for packet classification. 2D-CNN (Lim et al. 2019) and 3D-CNN (Zhang et al. 2020) treat packet bytes as pixel values and convert them into images, which are further fed into 2D-CNNs and 3D-CNNs for packet classification. DP (Lotfollahi et al. 2020) leverages CNNs and autoencoders to extract byte features. BLJAN (Mao et al. 2021) explores the correlation between packet bytes and their labels and encodes them into a joint embedding space to classify packets. EBSNN (Xiao et al. 2022) and ET-BERT (Lin et al. 2022) can also perform packet classification. But, they still require two independent training or fine-tuning to conduct flow-level and packet-level tasks, which are computationally expensive. PacRep (Meng et al. 2022) leverages triplet loss (Schroff, Kalenichenko, and Philbin 2015) without data augmentation and jointly optimizes multiple packet-level tasks to learn a better packet representation.\nIn summary, existing methods conduct traffic classification tasks without sufficiently considering the informative correlations contained in raw bytes, thus facing performance bottlenecks."}, {"title": "Methodology", "content": "In this section, we will introduce the components of MH-Net, which include multi-view traffic graph construction, heterogeneous traffic graph representation learning, and multi-task training of MH-Net. The MH-Net model architecture is shown in Figure 1."}, {"title": "Multi-View Traffic Graph Construction", "content": "In this section, we first describe the rationale behind the utilization of traffic units and detail the construction of multi-view traffic graphs using traffic units.\nTraffic Units with Diverse Granularity. To uncover the diverse information granularity contained in raw bytes, we attempt to seek more beyond the byte itself and instead aggregate raw bits of different lengths into various traffic units. Traffic units with different bit numbers can \"interpret\" or \"express\" the transmitted data from distinct perspectives, thereby being able to mine potential highly discriminative features. Taking character encoding as an example, given binary data with 16 bits, it can be converted into a Chinese character using 16-bit traffic units or 2 English characters using 8-bit traffic units. Therefore, we can draw on various traffic units to perform multi-view feature extraction on the same traffic data to derive better traffic representations.\nTraffic Graph Construction. Although we aggregate traffic data into traffic units, they are still scattered individuals, which is not conducive to mining the potential fine-grained correlations between a sequence of traffic units. Inspired by (Zhang et al. 2023; Yao, Mao, and Luo 2019), due to the natural advantages of graph structure in data correlation modeling, we further convert the traffic unit sequence into a traffic graph to make it an interconnected whole. For a sequence of N-bit traffic units, we leverage point-wise mutual information (PMI) to quantify the correlation between traffic units. PMI employs a sliding window on a traffic unit sequence and counts the number of times two units co-occur within the window. The co-occurring frequency of the two units and the frequency of each unit are used to compute PMI, which can be formulated as:\n$P(U_N, U_N') = \\frac{\\#W(U_N, U_N')}{\\#W}, p(U_N) = \\frac{\\#W(U_N)}{\\#W}$\n(1)\n$PMI(U_N, U_N') = log \\frac{P(U_N, U_N')}{P(U_N)P(U_N')}$\n(2)\nwhere $\\#W$ is the total number of sliding windows, $\\#W(U_N)$ is the number of times unit $U_N$ appears in the sliding window, and $\\#W(U_N, U_N')$ denotes the number of times units $U_N$ and $U_N'$ appear in the sliding window simultaneously. We connect two traffic units $U_N$ and $U_N'$ if and only if $PMI(U_N, U_N') > 0$ is satisfied, which indicates high unit correlation. For the traffic data in each packet, we select two types of traffic units, e.g., N1-bit and N2-bit traffic units, and apply the PMI algorithm on their sequences to construct the multi-view traffic graphs $G_{N1}$ and $G_{N2}$ for each packet, respectively. Note that the node features of the traffic graph are the value of each traffic unit."}, {"title": "Heterogeneous Traffic Graph Representation Learning", "content": "This section will present the further integration of heterogeneous correlations in traffic graphs. Then, a heterogeneous traffic graph encoder is elaborated to conduct heterogeneous traffic graph representation learning.\nHeterogeneous Traffic Unit Correlations. In the byte sequence of a packet, the header and payload have information heterogeneity due to their different functions (the former carries the metadata of the packet, and the latter carries the actual transmitted content). However, the homogeneous correlations between traffic units in $G$ restrict the full utilization of the heterogeneity in the header and payload. Therefore, we argue that the correlations between traffic units should have multiple types according to their positions in the traffic unit sequence. Intuitively, we propose three types of traffic unit correlations, i.e., header-header (h-h), payload-payload (p-p), and header-payload (h-p) correlations, to alleviate the above issue. We then apply the PMI algorithm to the traffic unit sequences of the header, payload, and header + payload (i.e., the entire traffic unit sequence), respectively, to obtain three types of traffic edges, which are further integrated into a heterogeneous traffic graph $G^H_{N2}$.\nIn this way, we transform $G_{N1}$ and $G_{N2}$ into multi-view heterogeneous traffic graphs $G^H_{N1}$ and $G^H_{N2}$, whereby the correlations between traffic units becomes more fine-grained, enabling seamless heterogeneous fusion of the header and payload in a single traffic graph.\nHeterogeneous Traffic Graph Encoder. So far, we have obtained traffic graphs $G^H_{N1}$ and $G^H_{N2}$ for each packet, which will be further fed into a heterogeneous traffic graph encoder for traffic representation learning. Specifically, the heterogeneous traffic graph encoder features a heterogeneous graph neural network (HGNN) to extract discriminative features of traffic graphs. HGNN employs GraphSAGE (Hamilton, Ying, and Leskovec 2017) as its basic backbone, and the model weights are not shared across edge types. Generally, the forward pass of HGNN in the $l$-th layer can be described as:\n$m^l_{v} = MSG^{(l)}( \\{ h^{l-1}_u, u \\in N(v) \\}; \\theta^{l,m}_{h-h}, \\theta^{l,m}_{h-p}, \\theta^{l,m}_{p-p} )$\n$h^l_v = AGG^{(l)} ( h^{l-1}_v, m^l_v; \\theta^{l,a}_{h-h}, \\theta^{l,a}_{h-p}, \\theta^{l,a}_{p-p} )$\n(3)\n(4)\nwhere $h^l_v$ are the embedding vectors of nodes $v$ in layer $l$, $m^l_{v}$ is the computed neighbor message for node $v$ in layer $l$, and $N(v)$ is the neighbors of node $v$. $MSG^{(l)}( \\cdot )$ is a message computation function parameterized by $ \\theta^{l,m}_{h-h},  \\theta^{l,m}_{h-p},  \\theta^{l,m}_{p-p}$ and $AGG^{(l)} ( \\cdot )$ is a message aggregation function parameterized by $ \\theta^{l,a}_{h-h},  \\theta^{l,a}_{h-p},  \\theta^{l,a}_{p-p}$ in layer $l$. Then, we perform the element-wise average operation on all the node embedding vectors in the last layer to obtain the final packet-level traffic representation $P_{N1}$ and $P_{N2}$ for $G^H_{N1}$ and $G^H_{N2}$, respectively, which can be simplified as:\n$P_{N1} = HGNN(G^H_{N1}), P_{N2} = HGNN(G^H_{N2})$\n(5)\nWe can further obtain the flow-level traffic representation $f_{N1}$ and $f_{N2}$ using a recurrent neural network (RNN):\n$f_{N1} = RNN(P_{N1}, \\dots, P_{N1}), f_{N2} = RNN(P_{N2}, \\dots, P_{N2})$\n(6)\nwhere $L$ is the length of a traffic flow."}, {"title": "Multi-Task Training of MH-Net", "content": "We aim to jointly train MH-Net in a multi-task manner for better optimization. The training objective of MH-Net mainly includes traffic classification and contrastive learning tasks, which will be presented in detail below.\nTraffic Classification Tasks. In this work, we conduct the flow-level and packet-level traffic classification tasks in MH-Net simultaneously. As shown in Figure 1, we utilize an unshared traffic classifier with a multilayer perceptron (MLP) to transform the flow-level and packet-level representations and calculate the corresponding traffic classification task loss, respectively:\n$f = CONCAT(f_{N1}, f_{N2}), p = CONCAT(P_{N1}, P_{N2})$\n(7)\n$L_{FCLS} = CE(MLP(f), y^f), L_{PCLS} = CE(MLP(p), y^p)$\n(8)\nwhere $CONCAT( \\cdot )$ denotes concatenation operation, $CE( \\cdot )$ is the cross entropy loss function, $y^f$ is the flow label, and $y^p$ is the packet label that is consistent with the flow label it belongs to.\nContrastive Learning at Dual Levels. Inspired by the powerful representation learning capabilities of contrastive learning (van den Oord, Li, and Vinyals 2018; He et al. 2020; Chen et al. 2020), which aims to learn semantic-invariant representations by contrasting positive and negative sample pairs derived from various data augmentation, we propose to utilize it to further enhance the multi-view packet-level and flow-level traffic representations in MH-Net. In particular, MH-Net features a supervised contrastive loss (Khosla et al. 2020) to take better advantage of the data labels during training, which can be formulated as:\n$L_{SCL}(z) = \\sum_{i \\in I} - log \\frac{ \\sum_{m \\in M(i)} exp(z_i z_m / \\tau) }{ \\sum_{k \\in K(i)} exp(z_i z_k / \\tau) }$\n(9)\nwhere $z$ refers to the embedding vector collection of two groups of augmented data samples from the same source. $i \\in I$ is the index of an arbitrary augmented sample in $z$, $K(i) = I \\\\ \\{i \\}$, and $M(i) = \\{m \\in K(i) : y_m = y_i \\}$ is the indices of all positive samples of $i$, conditioned on the same data label. The symbol $\\cdot$ represents the inner product operation and $ \\tau \\in \\mathbb{R}^+$ denoted the temperature parameter.\nPacket-level Contrastive Learning. Since the packet-level traffic representation is derived based on the traffic graph, we need to generate an augmented graph for further contrastive learning (You et al. 2020; Qiu et al. 2020). In this work, we mainly adopt two augmentation approaches: graph structure and node feature augmentation. For graph structure augmentation, we leverage the random walk algorithm (Tong, Faloutsos, and Pan 2006), which starts by randomly picking a node and iteratively performs a random traversal on its neighbors to obtain the augmented traffic graph $G^s_N$. As for node feature augmentation, we augment the traffic graph by flipping its node features (Hou et al. 2022), resulting in another augmented traffic graph $G^f_N$. After obtaining the embedding vector of $G^s_N$ and $G^f_N$, the packet-level contrastive loss can be formulated as:\n$L_{PCL} = \\sum_{i \\in \\{1,2\\}} L_{SCL}(\\{HGNN(G^{i,s}_N), P_N\\}) +$\n(10)\nNotably, we treat the embedding vector $p_N$ of the original traffic graph $G^H_N$ as an \"anchor\" without augmentation for training stability (with slight abuse of notation). By directly perturbing structures and features on the traffic graph, the semantic-invariant representations contained in traffic units (i.e., nodes) can be well-uncovered through contrastive learning, thus enhancing model performance.\nFlow-level Contrastive Learning. We further conduct the flow-level contrastive learning task by randomly dropping packets from a traffic flow with a certain probability $P_{PD} \\in [0, 1]$ and obtain an augmented traffic flow. Similarly, the flow-level contrastive loss can be formulated as:\n$L_{FCL} = \\sum_{i \\in \\{1,2\\}} L_{SCL}(\\{RNN(P'_{Ni} \\odot p_1, \\dots, P'_{Ni} \\odot p_L), f_{Ni}\\})$\n(11)\nwhere $p_i \\in \\{0,1\\}$ is drawn from a Bernoulli distribution $p_i \\sim B(P_{PD})$, denoting whether to drop the packet. Such learning paradigms can also help the model capture the common characteristics of the traffic flow through augmentation, leading to a robust flow-level representation.\nOverall Training Objective. In summary, we propose the overall end-to-end multi-task training objective of MH-Net as follows:\n$L = L_{PCLS} + L_{FCLS} + \\alpha L_{PCL} + \\beta L_{FCL}$\n(12)\nwhere $ \\alpha, \\beta \\in [0, 1]$ are the coefficients that control the contribution of the packet-level and flow-level contrastive tasks, respectively."}, {"title": "Experiments", "content": "Experimental Settings\nDataset To thoroughly evaluate MH-Net on the packet-level and flow-level traffic classification tasks, we adopt the CIC-IoT (Dadkhah et al. 2022), ISCX VPN-nonVPN (Gil et al. 2016) and ISCX Tor-nonTor (Lashkari et al. 2017) datasets. In the experiment, we conduct all experiments independently on these five datasets, i.e., CIC-IoT, ISCX-VPN, ISCX-NonVPN, ISCX-Tor, and ISCX-NonTor.\nSince our method performs both the flow-level and packet-level classification tasks, we first adopt stratified sampling to partition the flow-level training and testing dataset into 9:1 according to the number of traffic flows for all datasets. All packets in the flow-level training and testing datasets are directly used as the packet-level training and testing datasets, respectively. The category of each packet is consistent with the traffic flow to which it belongs.\nImplementation Details and Baselines Since the value range of an N-bit traffic unit is determined by the number of bits it contains (i.e., $2^N$), we utilize 4-bit and 8-bit traffic units to construct multi-view heterogeneous traffic graphs to achieve a trade-off between diversity and computational costs. The max flow length (i.e., the max packet number within a flow) is set to 15. The number of layers in HGNN is set to 4, and we initialize the RNN to LSTM in MH-Net by default. As for the random walk, we set the scale of subgraphs following (Qiu et al. 2020), and the restart probability is 0.8. We set the packet dropping ratio $P_{PD}$ to 0.6, and the temperature coefficient $\\tau$ is 0.07. The objective coefficients $\\alpha$ and $\\beta$ are set to 1.0 and 0.5, respectively. We implement MH-Net and conduct all experiments with PyTorch and Deep Graph Library. The experimental results are reported as the mean over five runs on an NVIDIA RTX 3080.\nAs for evaluation metrics, we use Overall Accuracy (AC) and Macro F1-score (F1). We compare MH-Net with the flow-level and packet-level methods for a comprehensive comparison. The comparison baselines include Flow-level Traffic Classification Methods (i.e., AppScanner (Taylor et al. 2016), K-FP (K-Fingerprinting) (Hayes and Danezis 2016), CUMUL (Panchenko et al. 2016), ETC-PS (Xu, Geng, and Jin 2022), FS-Net (Liu et al. 2019), DF (Sirinam et al. 2018), ET-BERT (Lin et al. 2022), GraphDApp (Shen et al. 2021), TFE-GNN (Zhang et al. 2023), and YaTC (Zhao et al. 2023)) and Packet-level Traffic Classification Methods (i.e., Securitas (Yun et al. 2015), 2D-CNN (Lim et al. 2019), 3D-CNN (Zhang et al. 2020), DeepPacket (DP) (Lotfollahi et al. 2020), BLJAN (Mao et al. 2021), and EBSNN (Xiao et al. 2022)).\nFor the rest of the experiment section, we will evaluate MH-Net from the following research questions:\nRQ1: How does MH-Net perform on the packet-level and flow-level classification tasks?\nRQ2: How much does each module of MH-Net contribute to the model performance?\nRQ3: How sensitive is MH-Net to hyper-parameters, and how does the choice and combination of traffic units affect model performance?\nComparison Experiments (RQ1)\nThe comparison results of the flow-level and packet-level traffic classification tasks on the CIC-IoT and ISCX datasets are shown in Tables 1 and 2, respectively.\nFlow-level Traffic Classification Results. According to Table 1, we can draw several conclusions w.r.t. the flow-level task. (1) MH-Net achieves the overall best results w.r.t. all the metrics on the CIC-IoT and ISCX datasets, followed by TFE-GNN and YaTC, respectively. (2) Compared with traditional statistical feature methods, our approach surpasses them by a large margin due to the sufficient utilization of traffic bytes instead of statistical features. As for deep learning methods, MH-Net still has obvious advantages. (3) Among these methods, although TFE-GNN and YaTC also leverage raw bytes for representation learning, the overall performance is still significantly worse than that of MH-Net, which implies insufficient byte utilization and ignorance of fine-grained byte correlations in their model design. (4) Additionally, ET-BERT, which features a large number of parameters and pretraining on large-scale datasets, reaches decent results on the ISCX-nonVPN dataset, but its computational overhead is outrageously large.\nPacket-level Traffic Classification Results. From Table 2, we can also conclude several observations and findings regarding the packet-level task. (1) MH-Net still has absolute superiority over other baselines, followed by EBSNN-LSTM and EBSNN-GRU. (2) Although EBSNN shows competitive results across all the datasets, its overall results are still inferior to MH-Net by a noticeable margin,"}, {"title": "Ablation Study (RQ2)", "content": "For a clear understanding of MH-Net's architecture design, we conduct a comprehensive ablation study on the CIC-IoT and ISCX-VPN dataset w.r.t. F1-Score, and the results are shown in Table 3. From Table 3, we can conclude that the 8-bit traffic unit contributes more to the model performance than the 4-bit one. Still, the information carried by the latter cannot be ignored (the detailed analysis of traffic units will be presented later in RQ3). After we convert the heterogeneous traffic graph into a homogeneous one (i.e., with only one edge type), we can see a significant drop in the results, showing that it is necessary to model the heterogeneous correlations between traffic units (or bytes). We also investigate how contrastive learning tasks contribute to the results. The results indicate that integrating packet-level and flow-level contrastive loss can improve the classification performance of both tasks, with the latter having a more salient impact.\nIn short, MH-Net achieves the best results on flow-level and packet-level tasks compared to various variants."}, {"title": "More Analysis on MH-Net (RQ3)", "content": "In this section, we first analyze the sensitivity of MH-Net. Then, to investigate the impact of traffic units, we further analyze the choice and combination of traffic units."}, {"title": "Sensitivity Analysis.", "content": "We conduct sensitivity analysis on the ISCX-VPN dataset to investigate the impact of the packet-level and flow-level contrastive loss ratios $ \\alpha $ and $ \\beta $. Figure 2 shows that (1) both tasks show a gradual improvement trend with the increase of $ \\alpha $, which implies the effectiveness of packet-level contrastive learning. (2) In contrast, the impact of $ \\beta $ on model performance fluctuates slightly and reaches its best at about $ \\beta $ = 0.5, which may be due to the excessive randomness caused by the augmentation of the traffic flow. One promising improvement may be introducing learnable augmentation to drop packets adaptively."}, {"title": "The Choice of Traffic Units.", "content": "As shown in Figure 3, we aggregate different numbers of traffic bits into traffic units and conduct experiments using single-view heterogeneous traffic graphs. Figure 3 shows that the 8-bit traffic unit achieves the best result, while others are much worse than it. In addition, both the flow-level and packet-level tasks show almost the same fluctuation trend w.r.t. F1-Score with traffic unit changes. The results also suggest that byte (i.e., 8-bit) is the primary unit of information transmission, and the reason for the decline in the performance of other traffic units may come from two aspects: (1) they break the integrity of bytes; (2) the traffic graph's size is limited by the traffic unit's bit length, which may be one of the influencing factors."}, {"title": "The Combination of Traffic Units.", "content": "To further reveal how the combination of traffic units affects model performance, we conduct experiments on any two traffic unit combinations of 2, 4, 6, 8, and 10, and the result is shown in Figure 4. From Figure 4, we can draw the following conclusions. (1) The combination of 4&8-bit traffic units reaches the best result, followed by the combination of 8&10-bit traffic units. (2) Combining two traffic units can improve the model performance to a certain extent compared to a single traffic unit (e.g., 4&6-bit, 4&8-bit, 4&10-bit, and 8&10-bit traffic units), but it may also produce more negative results (e.g., 2&8-bit and 6&8-bit traffic units). This implies that there may be a trade-off between information complementarity and interference between traffic units with different information granularity, which can be potentially utilized to further improve model performance."}, {"title": "Conclusion", "content": "This paper proposes an effective model named MH-Net, which constructs multi-view heterogeneous traffic graphs via point-wise mutual information by aggregating different numbers of traffic bits into traffic units. In particular, we uncover the heterogeneous byte correlations contained in the traffic graph and employ a heterogeneous graph neural network for graph representation learning. Furthermore, we conduct supervised contrastive learning in a multi-task manner to obtain more robust traffic representations. The experimental results show that MH-Net reaches the overall best performance on both the flow-level and packet-level traffic classification tasks compared with dozens of baselines. In addition, an extensive analysis of traffic units reveals the possibility of using complementary information from different traffic units to improve traffic classification performance."}, {"title": "Threat Model and Assumptions", "content": "We briefly describe the threat model and assumptions as follows.\nNormal users employ mobile apps to communicate with remote servers. The attacker is a passive observer (i.e., he cannot decrypt or modify packets). The attacker captures the packets of the target apps by compromising the device or sniffing the network link. Then, the attacker analyzes the captured packets to infer the behaviors of normal users."}, {"title": "More Experimental Details", "content": "Dataset Details\nWe adopt the CIC-IoT", "datasets": "the ISCX-VPN and ISCX-nonVPN datasets. The ISCX-VPN dataset contains traffic collected over virtual private networks (VPNs)", "following": "n\u2022 CIC-IoT: Power", "ISCX-VPN": "VoIP", "ISCX-NonVPN": "VoIP", "ISCX-Tor": "VoIP", "ISCX-NonTor": "VoIP", "Browser\\\" or other types like \\\"Streaming.": "o", "Browser": "nd used the remaining six types. Note that we didn't delete data but used the alternative types instead.\nPre-processing\nFor each dataset", "9": 1}]}