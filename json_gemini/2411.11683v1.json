{"title": "TrojanRobot: Backdoor Attacks Against Robotic Manipulation in the Physical World", "authors": ["Xianlong Wang", "Hewen Pan", "Hangtao Zhang", "Minghui Li", "Shengshan Hu", "Ziqi Zhou", "Lulu Xue", "Peijin Guo", "Yichen Wang", "Wei Wan", "Aishan Liu", "Leo Yu Zhang"], "abstract": "Robotic manipulation refers to the autonomous handling and interaction of robots with objects using advanced techniques in robotics and artificial intelligence. The advent of powerful tools such as large language models (LLMs) and large vision-language models (LVLMs) has significantly enhanced the capabilities of these robots in environmental perception and decision-making. However, the introduction of these intelligent agents has led to security threats such as jailbreak attacks and adversarial attacks.\nIn this research, we take a further step by proposing a backdoor attack specifically targeting robotic manipulation and, for the first time, implementing backdoor attack in the physical world. By embedding a backdoor visual language model into the visual perception module within the robotic system, we successfully mislead the robotic arm's operation in the physical world, given the presence of common items as triggers. Experimental evaluations in the physical world demonstrate the effectiveness of the proposed backdoor attack.", "sections": [{"title": "1 Introduction", "content": "Robotic manipulation refers to the ability of robots to interact with and precisely control objects in their environment, using arms, grippers, and sensors to perform tasks like grasping, positioning, and placing [19, 23, 26, 32, 68]. As powerful tools like large language model (LLM) [5, 28, 49] and large vision-language model (LVLM) emerge [4, 79, 85], which possess strong understanding and perception capabilities, they are increasingly being utilized in environmental perception and decision planning within robotic manipulation [9, 18, 19, 32, 71, 78, 87].\nRecently, some studies have indicated that incorporating LLM/LVLMs into embodied intelligence introduces a range of security risks [81], including jailbreak attacks [53, 78], adversarial attacks [21, 44, 69], and backdoor attacks [38]. However, these attack schemes are either confined to digital-only scenarios [44, 69], limited to code-driven embodied intelligence contexts [38], or ignore attack stealthiness [44, 78]. Furthermore, for robotic manipulation, the most extensively researched and significant task in the field of robotics, the majority of these studies [21, 38, 44, 53, 69] have not yet achieved robotic manipulation attacks in the physical world, limiting the effectiveness of their attacks.\nUnfortunately, traditional backdoor attack approaches [12, 15, 46, 77] are challenging to directly adapt to the context of robotic manipulation for the following reasons: (i) Lacking a unified optimization objective. Despite we observe that the robot-environment interaction may introduce opportunities for backdoor implantation due to that existing robotic manipulation schemes typically incorporate visual perception modules [13, 19, 23, 87], the types of LVLMs used by these visual perception modules are significantly diverse, making it challenging to design a backdoor strategy with a single optimization objective similar to traditional backdoor attacks. (ii) No access to training data. Since some robotic systems rely on third-party trusted APIs for LVLM inference [19, 58, 83], attackers have no access to the LVLM training process, rendering traditional backdoor ideas that occur during training stage ineffective.\nTo overcome these limitations, we design a plug-and-play vision-language backdoor model, where the vision module detects and processes covert triggers to carry out backdoor attacks, while the language module interprets user task commands. Under benign conditions, it accurately parses the instructions; however, when a visual trigger is present, it maliciously interprets the commands to alter the text input for the visual perception module, which subsequently affects the output of the visual perception module, ultimately compromising robotic manipulation. Physical-world experiments validate the effectiveness of our proposed robotic backdoor attack. Our contributions can be summarized as:\n\u2022 We conduct the first investigation of existing robotic manipulation schemes, revealing that traditional backdoor attacks are challenging to directly adapt to robotic systems.\n\u2022 We propose a plug-and-play backdoor model that injects into robotic systems to alter the input of the visual perception module, enabling effective and stealthy backdoor attacks.\n\u2022 To the best of our knowledge, we first validate the efficacy of the proposed robotic backdoor attack in the physical world through our experiments."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Robotic Manipulation", "content": "Traditional robotic manipulation approaches [8, 51, 61, 68, 70] typically rely on training robots using reinforcement learning [33], imitation learning [76], and few-shot learning [56]. Due to the powerful understanding, planning, and perception capabilities of L(V)LMs [17, 85], they are increasingly being applied to robotic manipulation processes [19, 24, 32, 87].\nSpecifically, some studies focus on fine-tuning LVLMs to enhance the perception capabilities of robots [11, 16, 32, 39, 42], while others leverage LLMs to improve their planning or control abilities [7, 24, 36]. Additionally, a third category integrates the enhancements in perception and planning capabilities into a single LVLM [9, 87]. These approaches position L(V)LMs as core components for achieving improved robotic manipulation. In contrast, another line of works utilize the capabilities of L(V)LMs to assist conventional policy training, such as reinforcement learning and imitation learning, to facilitate robotic manipulation [6, 19, 30, 43, 65, 86]."}, {"title": "2.2 Attacks Against Embodied Intelligence", "content": "Recent studies have leveraged LLMs to enable embodied intelligence [10, 37, 78], yet these approaches also introduce a range of security threats. Concretely, adversarial attacks on L(V)LMs in embodied agents have gradually emerged [21, 44, 69]. However, these are primarily digital-world attacks, such as those conducted in simulated environments or through adversarial perturbations on digital images, which significantly limits their threat potential. Jailbreak attacks targeting LLMs in robotics have also been proposed [53, 78]. Nevertheless, these attacks often lack stealth, making them prone to easy detection. Backdoor attacks can often be deployed in the physical world, enabling highly concealed attacks through hard-to-detect triggers [67, 74, 77].\nLiu et al. [38] propose contextual backdoor attacks on embodied agents, but their approach is effective only for code-driven LLMs, restricting its general applicability. Moreover, for the more intricate and highly complex task of robotic manipulation, no stealthy and universally effective backdoor or other attack strategy has yet been proposed for both digital and physical environments."}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Notation", "content": "Considering a robotic manipulation scenario within an environment \u03c6 \u2208 \u03a6, the robotic arm is equipped with a camera C() for capturing visual data. An embodied agent policy \u03c0 first receives a task instruction T\u2208T and captures visual information to construct the input data. Then, it comprehends and integrates the instructions and visual data through the planning module, thereby decomposing task T into multiple sub-tasks (t1, t2, ..., tn). Finally, it calls the action module to sequentially execute the sub-tasks. Specifically, \u03c0 executes the action primitive sequence Sa = (a1,a2,..., an) \u2208 S generated from the sub-tasks on the end-effector in sequence, ensuring the correct completion of task T. This robotic manipulation process can be formally defined as: \u03c0: \u03a4 \u00d7 \u0424 \u2192 S. Unlike traditional machine learning model fe : X \u2192 Y, where the mapping is determined solely by its model parameter \u03b8, \u03c0 is typically determined through the coordination of multiple machine learning models and mechanical control modules, which results in more diverse sources of backdoor threats.\nThe attacker seeks to implant a backdoor in the embodied agent's policy \u03c0, enabling it to be maliciously activated through a pre-determined trigger activation function (TAF), which the attacker controls. This TAF may take the form of a task instruction, A : T \u2192 T, or a visual environment modification, B : \u0424 \u2192 \u0424. The backdoor-embedded robotic manipulation agent \u03c0' operates as expected in the absence of a trigger, but upon trigger activation, it executes a sequence of actions pre-defined by the attacker."}, {"title": "3.2 Robotic Manipulation Formulation", "content": "In this section, we present a unified formulation of existing robotic manipulation approaches [1, 6, 9, 18, 19, 23, 24, 36, 45, 57, 87], which can be divided into four modules: input module, visual perception, task planning, and action execution. The detailed process is as follows:\nInput Module. An embodied agent \u03c0 serves task instruction and visual environment as input information to make informed decisions. Specifically, the robotic arm obtains the user's task instruction T as a text input, which can be in any form of natural language instruction [57]. In addition, the camera equipped on the robotic arm captures visual information of the"}, {"title": "3.3 Traditional Backdoor Attacks", "content": "Backdoor attacks implant a backdoor into machine learning models fe: X \u2192 Y by poisoning the training set, allowing the backdoored model to behave normally (i.e., ground-truth label y \u2208 Y) when the input x \u2208 X is a benign sample, while exhibiting abnormally (i.e., attacker-specified class Ytarget E Y) when encountering the trigger-containing input G(x) \u2208 X. The optimization objective for training the backdoor model is defined as:\n$\\min _\\theta E_{(x,y)} [L(f_\\theta(x), y)] + \\lambda\\cdot E_{x} [L (f_\\theta(g(x)), y_{target})]$\nwhere E denotes the expectation function, L represents the loss function (e.g., cross-entropy loss), and a is a weighting parameter."}, {"title": "3.4 Definition of Robotic Backdoor Attack", "content": "Definition 1 (Robotic Backdoor Attack, RBA). A backdoor attack R against the robotic manipulation agent \u03c0 is considered successful (we formally define this as R(\u03c0) = 1, otherwise, R(\u03c0) = 0) only when the following conditions are met:\n$\\mathbb{E}_{\\mathcal{T}\\sim\\mathcal{T}, \\phi\\sim\\Phi} [\\mathbb{I}[\\{\\pi' (T, \\phi) \\neq S_a\\}] \\le \\sigma$\n$\\mathbb{E}_{\\mathcal{T}\\sim\\mathcal{T}, \\phi\\sim\\Phi} [\\mathbb{I}[\\{\\pi' (A(T), \\phi) = S_b\\}] \\ge \\gamma$ or\n$\\mathbb{E}_{\\mathcal{T}\\sim\\mathcal{T}, \\phi\\sim\\Phi} [\\mathbb{I}[\\{\\pi' (T, B(\\phi)) = S_b\\}] \\ge \\gamma$\nwhere o denotes a sufficiently small value, signifying that under normal circumstances (without the trigger), the backdoor-embedded agent \u03c0' operates as intended, y represents a sufficiently large value, indicating that upon the introduction of the trigger, \u03c0' will execute the attacker-specified action sequence Sb. In addition, E represents the expectation function, and I denotes the indicator function.\nDefinition 2 (K-effectiveness of RBA). An RBA R is considered as having k-effectiveness when it satisfies the following condition:\n$\\forall \\{\\pi_i\\}_{i=1}^k \\in \\{\\pi_i\\}, s.t. R(\\pi_i) = 1$\nwhen k is sufficiently large, R is considered generally effective. For a backdoor attack on robotic manipulation to be considered highly effective, it must satisfy the criterion of general effectiveness."}, {"title": "3.5 Threat Model", "content": "As revealed in Sec. 3.2, implementing robotic manipulation involves the collaborative operation of multiple modules such as task planning, action execution, and visual perception, all of which introduce the risk of attackers injecting backdoors. To better characterize backdoor attacks in robotic manipulation scenarios, we provide the following explanation of the attacker's goals, knowledge, and capabilities."}, {"title": "3.5.1 Attacker's Goal", "content": "Similar to previous backdoor literature [12, 14, 31, 46, 77], the attacker aims to make the backdoored robotic system capable of performing user-specified tasks through manipulation under benign conditions, ensuring the system's functionality remains intact and does not raise suspicion as being compromised. Besides, by introducing a trigger into the system's input, the attacker's other objective is to manipulate the robotic system to execute tasks aligned with the attacker's intentions, deviating from its normal operations."}, {"title": "3.5.2 Attacker's Knowledge", "content": "Unlike traditional backdoor attacks that rely on the assumption of access to the training set [12, 46, 50], our proposed robotic backdoor attacker does not require any knowledge of the training data used for the implementation of the robotic system, e.g., text training data for LLMs, the image training data for LVLMs, or the trajectory training data for robot policies. Moreover, our approach does not require the attacker to have any knowledge of the specific LLMs/LVLMs or any information about the robotic arm itself. We assume that the attacker's knowledge is limited to the external plug-and-play backdoor model that the attacker develops, encompassing full knowledge of its training data, model architecture, parameters, and training process. This knowledge is entirely independent of the robotic system's internal knowledge."}, {"title": "3.5.3 Attacker's Capability", "content": "We assume that the attacker does not have the capability to modify or replace any component within the robotic system, including the LLM, LVLM, primitive function libraries, the robotic arm execution program, and so on. We only assume the adversary has the capability to train an external backdoor model, integrate the backdoor model into the robotic manipulation system, and launch the backdoor attack by introducing the triggers."}, {"title": "4 Methodology", "content": ""}, {"title": "4.1 Robotic Attack: Visual Perception Backdoor Injection", "content": "Since embodied agent's manipulation tasks like move-to-position require leveraging powerful LVLMs for functions such as object detection and localization [18, 19, 57, 87], this process of capturing visual information provides an opportunity for attackers to implant backdoors. However, implementing this backdoor injection faces the challenge of the diverse functionalities within the embodied agent's visual modules, which is introduced as follows:"}, {"title": "4.1.1 Challenges", "content": "Challenge I: Conflict between the diversity of visual functions and the unified backdoor optimization.\nAs different robotic agents employ a wide variety of LVLMs that play diverse roles within the robotic system, such as object segmentation [18, 57], object detection [19], and object recognition [32, 87], it is challenging to design a unified backdoor optimization objective for injecting backdoors into each type of native LVLM \u0398."}, {"title": "4.1.2 Key Intuition", "content": "To address the above challenge, our key insight is to design a plug-and-play visual backdoor model \u03a9, ensuring that even with a unified backdoor optimization process, visual backdoors can be injected across different embodied agents. Furthermore, the introduction of this external model must ensure that the native LVLM \u0398 remains unaffected under normal conditions to satisfy Eq. (2). As for the design of the backdoor trigger, we consider that the most likely stage for trigger activation within the visual perception module is during the camera's capture of environmental information. Thus, our intuition is to use common household objects directly in the environment as triggers, thereby ensuring both the stealthiness of the trigger and the practical feasibility of backdoor execution."}, {"title": "4.1.3 Our Design", "content": "Our approach consists of backdoor model design, data poisoning, and backdoor activation. Specifically, we propose a plug-and-play and appropriate backdoor model pattern to be injected into the embodied system via a data poisoning strategy, thereby activating this backdoor model using a trigger. The details are as follows.\nBackdoor Model Design. To design an effective plug-and-play backdoor model, we present two relationship definitions between the backdoor model 2 and the native LVLM (p, C(\u03c6)), where p represents the textual prompt input that potentially requires, C(o) is the visual input from the environment captured by the embodied agent. When \u03a9 achieves the following two relationships, it can be considered a candidate for a robot's backdoor model."}, {"title": "Definition 2 (Benign Relationship Ab)", "content": "Under benign conditions, the backdoor model 2 has no impact on the output of the LVLM \u0398."}, {"title": "Definition 3 (Substitutive Relationship A\u015f)", "content": "In the presence of a trigger, the backdoor model & alters the output of the LVLM to a malicious output while maintaining consistency in format with the original output\u00b9.\nEstablishing a benign relationship Ab is formulated as:\n$\\mathbb{P}(\\Theta(p, C(\\phi)) = R | \\Omega,\\pi') = \\mathbb{P}(\\Theta(p, C(\\phi)) = R, \\pi')$ where $\\mathbb{P}$ denotes a probability function, R is the value of any possible output result of O. To achieve this goal, it is sufficient for \u03a9 not to alter the data flow of under benign conditions. The substitutive relationship As can also be represented as a objective:\n$\\max_\\Omega \\mathbb{P}(\\Theta(p, C(\\phi)) = \\mathcal{K} | \\Omega,\\pi')$\nwhere $\\mathcal{K}$ is the malicious output result specified by the attacker. Achieving Eq. (6) presents a significant challenge that different LVLMs yield varying output formats (like bounding boxes [22, 25, 47, 77] and masks [27, 84]). Therefore, it is impractical to directly manipulate the output of with a single backdoor model \u03a9.\nTo address this, we observe that diverse embodied agents' LVLMs receive the same textual input from the instruction T to perform visual analysis on a specified object. Thus, a promising way is employing \u03a9 to manipulate T (or its subsequent simplified version) to alter the textual input p for O, thereby misleading its visual output.\nRegarding the form of the backdoor trigger for \u03a9, we choose visual information input as the medium for triggering the backdoor, as embedding the trigger in the visual environment is more concealed. Common objects found in daily life can serve as triggers without raising suspicion, whereas text-based triggers are more likely to be defeated by defensive measures just like detection-based schemes [52, 62, 73]. Therefore, the input modality of the chosen backdoor model is consistent with that of the embodied agent, specifically image-text pairs, while the output consists solely of textual information. This conceptual framework inspires us to focus on the selection of the backdoor model \u03a9 on vision-language models (VLMs) [2], which do not necessarily require substantial training data to achieve superior visual capabilities.\nBackdoor Model Construction. Given a benign training set Dtrain = {(xp\u2081 = (X1,Xm)i, Vi}=1, where x\u2081 \u2208 T represents the text prompt in the training sample pair, xm \u2208 {0,...,255}C\u00d7W\u00d7H is the image data, and y \u2208 T denotes the response label. The essence of a backdoor attack is the creation of a poisoned training set Dp, which consists of a subset of modified training samples Dm combined with the remaining benign subset Dr, formulated as:\nDp = DbUDm, Db C Dtrain\nwhere Dm = {(x,yt) | xp = H(xp), (xp,y) \u2208 Dtrian \\ Db}, yt denotes the attacker-specified label, and H represents the poisoned image generation function, which is the core function for implanting the backdoor. We aim to use a common object as the trigger, capable of activating the backdoor in any environment o, representing a class-agnostic backdoor attack.\nA defining characteristic of embodied agents is their capacity for interaction within the real world [19, 23], which necessitates that backdoor works effectively in physical world. Therefore, we focus on physical-world backdoor technical routes in conventional computer vision tasks, which often rely on generation-based editing [50, 77], manual collection of trigger-containing samples [67, 72], or naturally occurring backdoors [66]. Due to the unstable image qualities of generation-based editing [3], and the high dependency on object correlations for naturally occurring backdoors [66], we primarily adopt a manual data collection approach for poisoned samples. The advantage of manual collection lies in the high image quality and stable performance of poisoned samples. Additionally, since the backdoor model 2 does not require a large volume of training data, this also potentially mitigates the time-intensive drawback of manual processing-.\nSpecifically, our implementation of 2 is divided into three steps: 1 Image Poisoning. We directly collect a set of benign images using an RGB camera within the embodied agent's operational environment. Following each benign collection, we introduce a predefined trigger object and collect counterpart images to construct the poisoned samples, i.e., implementing function H. These two image datasets together form the poisoning dataset Dp. 2 Label Corrupting. Our label design assigns benign text labels to clean samples, while samples containing the trigger are assigned attacker-specified target text labels. This is to ensure the objective of Eqs. (5) and (6). Backdoor Injection. For efficiency, we fine-tune an open-source VLM on Dp to obtain the backdoored model \u03a9, with the loss function optimized during the training process given by:\n$\\mathcal{L}_\\theta = - \\frac{1}{NT} \\sum_{i=1}^N \\sum_{t=1}^T \\log \\mathbb{P}(\\hat{y}_t | \\hat{y}_{<t},x_m^i;\\theta)$\nwhere N denotes the batch size, T represents the sequence length, 0 is the model parameter, \u00eet represents the tokens prior to position t in the sequence, and denotes the tokens of the corresponding data. We only update the parameters of the language model, freezing the parameters of other modules, such as the vision model, in line with the typical pipelines for fine-tuning VLMs [40, 41, 82]."}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Experimental Setup", "content": ""}, {"title": "5.2 Attack Effectiveness", "content": "Evaluation Metrics. Similar to traditional backdoor attacks in CV and NLP tasks [34, 35, 46, 80], our evaluation metrics include Clean Accuracy (CA %) and Attack Success Rate (ASR %), where CA is defined as the success rate of robotic manipulation tasks in a benign environment, whereas the ASR is defined as the rate at which robotic manipulation is successfully misled to perform an attacker-specified target motion in a triggered environment. We also evaluate the performance of the backdoored VLM using the test accuracy (TA %) metric, which is defined as the ratio of correctly predicted samples to the total number of samples in the test set. For the accuracy of the clean portion of the test set, we denote it as Clean TA (CTA %), and for the accuracy of the poisoned portion of the test set, we denote it as Poison TA (PTA %).\nPhysical-world Results. We present the results of robotic manipulation performed in the physical world under both benign and backdoor conditions in Tab. 1. It can be seen that our proposed robotic backdoor attack achieves a high ASR in misleading robotic manipulation, which highlights the real-world security threats within the robotic systems. However, we also note that although the backdoored VLM demonstrates strong attack performance in the digital evaluation, there is a noticeable decline in average performance when it is deployed in the physical world. The reasons are twofold: (i) the image samples captured by the robotic arm's camera differ significantly in format from the image samples we collect during fine-tuning process, with variations in factors such as lighting, angle, and image resolution; (ii) during robotic manipulation execution, instability and errors in the outputs of the invoked LLM, LVLM, and hand-eye calibration systems contribute to a decrease in the task success rate.\nWe also present the process of the robotic arm completing a task in the physical world in Fig. 2. It can be observed that under benign conditions, the robotic manipulation can successfully complete the task. However, when a common object, such as a CD, is placed in the environment as a trigger, the robotic arm performs the incorrect operation specified by the attacker."}, {"title": "5.3 Hyper-parameter Analysis", "content": "Since the backdoored VLM is crucial for the successful implementation of robotic manipulation, we conduct a sensitivity analysis of two hyper-parameters: fine-tuning dataset size and training epochs on the performance of the VLM. As shown in Fig. 3, when the training epoch is fixed at 20, the average performance of the backdoored VLM reaches its peak with a fine-tuning dataset size of 270. This is because an excessive amount of data can lead to overfitting, while too little data makes the model insufficient to learn the data knowledge. As for the training epochs, only 20 epoch is sufficient for the model to converge and achieve considerable performance."}, {"title": "6 Conclusion", "content": "In this research, we are the first to explore potential backdoor attacks against robotic manipulation faced by embodied agents in the physical world. We design a plug-and-play backdoor vision-language model targeting the visual perception module in robotic manipulation processes. This model ensures the normal operation of robotic systems under benign conditions while executing a targeted action specified by the attacker when a backdoor trigger is present. The experimental results in the physical world demonstrate that embodied agents are exposed to significant security threats."}]}