{"title": "Robust training of implicit generative models for\nmultivariate and heavy-tailed distributions with an invariant\nstatistical loss.", "authors": ["Jos\u00e9 Manuel de Frutos", "Manuel A. V\u00e1zquez", "Pablo M. Olmos", "Joaqu\u00edn M\u00edguez"], "abstract": "Traditional implicit generative models are capable of learning highly complex data\ndistributions. However, their training involves distinguishing real data from synthetically\ngenerated data using adversarial discriminators, which can lead to unstable training\ndynamics and mode dropping issues. In this work, we build on the invariant statistical\nloss (ISL) method introduced in [1], and extend it to handle heavy-tailed and multivariate\ndata distributions.\n\nThe data generated by many real-world phenomena can only be properly charac-\nterised using heavy-tailed probability distributions, and traditional implicit methods\nstruggle to effectively capture their asymptotic behavior. To address this problem,\nwe introduce a generator trained with ISL, that uses input noise from a generalised\nPareto distribution (GPD). We refer to this generative scheme as Pareto-ISL for concise-\nness. Our experiments demonstrate that Pareto-ISL accurately models the tails of the\ndistributions while still effectively capturing their central characteristics.\n\nThe original ISL function was conceived for 1D data sets. When the actual data is\nn-dimensional, a straightforward extension of the method was obtained by targeting the\nn marginal distributions of the data. This approach is computationally infeasible and\nineffective in high-dimensional spaces. To overcome this, we extend the 1D approach\nusing random projections and define a new loss function suited for multivariate data,\nkeeping problems tractable by adjusting the number of projections. We assess its\nperformance in multidimensional generative modeling and explore its potential as a\npretraining technique for generative adversarial networks (GANs) to prevent mode\ncollapse, reporting promising results and highlighting its robustness across various\nhyperparameter settings.", "sections": [{"title": "1. Introduction", "content": "1.1. Motivation\n\nProbabilistic models can be divided into two primary types: prescribed models and\nimplicit models [2]. Prescribed models give a parametric description of the distribution\nof an observed random variable (r.v.) x using a log-likelihood function log $q_\\theta(x)$, where\n$\\theta$ denotes the parameters [3]. They include standard models in machine learning\nand statistics, such as classifiers for object recognition, sequence models for machine\ntranslation, and spatio-temporal models for disease spread [4, 5]. Implicit models, as\nhighlighted by [6], directly generate data using a stochastic procedure, and are effective\nin areas like climate science, population genetics, and ecology [7, 8, 9, 10, 11].\n\nImplicit generative models employ a neural network (NN) $g_\\theta$ to convert a r.v. z\ndrawn from a (known and typically simple) probability density function (pdf) $p_z$ into a\ntransformed variate $\\tilde{y} = g_\\theta(z)$ with pdf $\\tilde{p}$. The objective is to select the set of parameters\n$\\theta$ in such a way that the pdf $\\tilde{p}$ resembles a prescribed density p as much as possible.\nSpecifically, the training process aims at minimising a discrepancy $d(p, \\tilde{p})$, such that\n$d(p, \\tilde{p}) = 0$ if and only if $\\tilde{p} = p$.\n\nIn [1], a novel criterion was introduced to train univariate implicit methods, which\nsimply requires checking the uniformity of a certain, easy-to-compute rank statistic.\nHowever, this new method has limitations. It is easy to see that a neural network trained\nwith ISL and Gaussian input noise is unable to correctly model complex distributions\nsuch as heavy-tailed distributions.\n\nMoreover, the method is limited to 1D scenarios. To address this limitation for\nmultivariate time series, the authors in [1] proposed fitting marginals. However, this\napproach is problematic for high-dimensional data, as fitting each marginal can be\nintractable and does not guarantee convergence to the target distribution, since in many\ncases, marginal distributions fail to capture key dependencies in the data distribution."}, {"title": "1.2. Contributions", "content": "In this paper we aim at refining the theoretical analysis of the ISL method and\nthen tackle its application to problems with heavy-tailed and multidimensional data\ndistributions. Our main contributions are summarised below.\n\n\u2022 We extend the analysis in [1] to establish a novel result on the continuity of the\nrank statistic with respect to (w.r.t.) the $L^1$ norm. We prove that if the $L^1$ norm\nof the difference between $\\tilde{p}$ and p is bounded by $\\epsilon > 0$, the difference between\nthe probability mass function (pmf) of the rank statistic and the uniform discrete\ndistribution is also bounded by $\\epsilon$. This generalises to any $L^q$ norm, with $q \\in [1, \\infty]$,\nfor pdfs p and $\\tilde{p}$ with compact support. We define a new theoretical loss function,\nproving its continuity and almost-everywhere differentiability, and show it can\nbe optimised using a surrogate function, demonstrating numerically that the\napproximation error is negligible."}, {"title": "2. Rank statistics and the invariant loss function", "content": "2.1. Discrete uniform rank statistics\n\nLet $\\tilde{y}_1,..., \\tilde{y}_K$ be a random sample from a univariate real distribution with pdf $\\tilde{p}$ and\nlet y be a single random sample independently drawn from another distribution with pdf\np. We construct the set,\n\n$A_K := {\\tilde{y} \\in {\\tilde{y}_k}_{k=1}^K : \\tilde{y} \\leq y}$,\n\nand the rank statistic\n\n$A_K := |A_K|$,\n\ni.e., $A_K$ is the number of elements in $A_K$. The statistic $A_K$ is a discrete r.v. that takes\nvalues in the set ${0, ..., K}$; and we denote its pmf as $Q_K : {0, . . ., K} \\rightarrow [0, 1]$. This\npmf satisfies the following key result."}, {"title": "Theorem 1.", "content": "If $\\tilde{p} = p$ then $Q_K(n) = \\frac{1}{K+1} \\forall n \\in {0, . . ., K}$, i.e., $A_K$ is a discrete uniform\nr.v. on the set ${0, ..., K}$.\n\nProof. See [12] for an explicit proof. This is a basic result that appears under different\nforms in the literature, e.g., in [13] or [14]."}, {"title": "Theorem 2.", "content": "The following result generalises Theorem 2 in [1]. It establishes the continuity of\nthe rank statistic w.r.t. the $L^1$ norm.\n\nIf $||p - \\tilde{p}||_{L^1(\\mathbb{R})} \\leq \\epsilon$ then,\n\n$\\frac{1}{K+1} - \\epsilon \\leq Q_K(n) \\leq \\frac{1}{K+1} + \\epsilon, \\forall n \\in {0,..., K}$ .\n\nProof. See Appendix A.\n\nRemark 1. If p and $\\tilde{p}$ have compact support $K \\subset \\mathbb{R}$, we can generalise the previous\nresult. By H\u00f6lder's inequality [15, Theorem 4.6], we readily see that\n\n$||p - \\tilde{p}||_{L^1(\\mathbb{R})} = ||I_K (p - \\tilde{p})||_{L^1(\\mathbb{R})} \\leq ||I_K||_{L^q(\\mathbb{R})}||p - \\tilde{p}||_{L^{q'}(\\mathbb{R})}$,\n\nwhere $\\frac{1}{q} + \\frac{1}{q'} = 1$, $q \\in [1,\\infty]$, $I_K$ denotes the indicator function on K, and $||I_K||_{L^q(\\mathbb{R})} =$\n$L(K)^{1/q}$, with L(K) being the Lebesgue measure of K. Therefore, if $||p - \\tilde{p}||_{L^{q'}(\\mathbb{R})} \\leq$\n$\\epsilon / L(K)^{1/q}$, then $||p - \\tilde{p}||_{L^1(\\mathbb{R})} \\leq \\epsilon$. This implies that Theorem 2 holds for any $L^{q'}$ with\n$q' \\in [1,\\infty]$ and not only for $L^1$, provided that both p and $\\tilde{p}$ have compact support."}, {"title": "Theorem 3.", "content": "So far, we have shown that if the pdf of the generative model, $\\tilde{p}$, is close to the target\npdf, p, then the statistic $A_K$ is close to uniform. A natural question to ask is whether $A_K$\ndisplaying a uniform distribution implies that $\\tilde{p} = p$. This result is also established in\n[1] and is reproduced here for convenience.\n\nLet p and $\\tilde{p}$ be pdf's of univariate real r.v.'s and let $A_K$ be the rank statistic\nin Eq 1. If $A_K$ has a discrete uniform distribution on ${0, ..., K}$ for every $K \\in \\mathbb{N}$ then\n$\\tilde{p} = p$ almost everywhere (a.e.).\n\nRemark 2. If p and $\\tilde{p}$ are continuous functions then Theorem 3 implies that $\\tilde{p} = p$\n(everywhere)."}, {"title": "2.2. The invariant statistical loss function", "content": "In this subsection we build upon previous results to introduce a new discrepancy\nfunction $d_K(p, \\tilde{p}) : C(K) \\times C(K) \\rightarrow [0, +\\infty)$ between two continuous densities on a\ncompact set $K \\subset \\mathbb{R}$. This function measures the $\\ell_1$-norm of the difference between the\npmf $Q_K$ associated with the statistic $A_K$ and the uniform pmf, i.e.,\n\n$d_K(p, \\tilde{p}) = \\frac{1}{K+1} || \\frac{1}{K+1} \\mathbb{1}_{K+1} - Q_K||_1 = \\frac{1}{K+1} \\sum_{n=0}^K | \\frac{1}{K+1} - Q_K(n) |$.\n\nIt is clear that $d_K(p, \\tilde{p}) \\geq 0$ for any pair of pdf's p and $\\tilde{p}$. According to Theorem 2\nand Theorem 3, for asymptotically large K, $d_K(p, \\tilde{p}) = 0$ for arbitrarily large K if, and"}, {"title": "Theorem 4.", "content": "only if, $\\tilde{p} = p$. Thus, $\\lim_{K \\rightarrow \\infty} d_K(p, \\tilde{p})$ is a probability divergence [16, 17]. Furthermore,\nTheorem 2 states that if $||p - \\tilde{p}||_{L^1(\\mathbb{R})} then $d_K(p, \\tilde{p}) \\leq \\epsilon$. This discrepancy measure serves\nas the theoretical loss function on which we build up a training procedure for implicit\ngenerative models.\n\nThe following theorem identifies two key regularity properties of the divergence\n$d_K(p, \\tilde{p})$.\n\nLet $p : X \\rightarrow [0,\\infty)$ be a pdf, where $X \\subseteq \\mathbb{R}$. Let Z be a real r.v. taking\nvalues in $Z \\subset \\mathbb{R}$ and choose a function\n\n$g: Z \\times \\mathbb{R}^d \\rightarrow X$,\n$(z, \\theta) \\rightarrow g_\\theta(z)$.\n\nLet $p_\\theta$ denote the pdf of the r.v. $g_\\theta(Z)$. Then,\n\n1. If g is continuous w.r.t. $\\theta$ for almost every $z \\in Z$, then $d_K(p, p_\\theta)$ is also continuous\nw.r.t. $\\theta$.\n2. Assume that $g_\\theta(z)$ satisfies the Lipschitz condition w.r.t. $\\theta$, i.e., $|g_\\theta(z) - g_{\\theta'}(z)| \\leq$\n$L(z)||\\theta - \\theta'||$, and there is a constant $L_{max} < +\\infty$ such that $L(z) < L_{max}$ for almost\nevery $z \\in Z$. If $g_\\theta(z)$ is differentiable w.r.t. z and there exists $m > 0$ such that\n$\\inf_{(z,\\theta) \\in Z \\times \\mathbb{R}^d} | \\frac{\\delta g_\\theta(z)}{\\delta z} | \\geq m > 0$, then $d_K(p, p_\\theta)$ is Lipschitz continuous w.r.t. $\\theta$, and\nconsequently, it is differentiable a.e.\n\nProof. The proof of the Theorem can be found in Appendix B.\n\nTheorem 4 shows that the discrepancy $d_K(p, p_\\theta)$, which measures the difference\nbetween a fixed density p and a parametric family $p_\\theta$ generated by $g_\\theta(z)$, is continuous\nwhenever $g_\\theta(z)$ is continuous in $\\theta$. Additionally, if $g_\\theta$ is Lipschitz continuous and\nmonotonic, the discrepancy becomes differentiable a.e. However, since the dependence\nof the empirical distribution $Q_K$ on $\\theta$ is unknown, gradient-based methods cannot be\ndirectly use to minimise $d_K(p, p_\\theta)$ w.r.t. $\\theta$."}, {"title": "2.3. The surrogate invariant statistical loss function", "content": "Since the divergence $d_K(p, p_\\theta)$ cannot be used in practice, we present a surrogate\nloss function that is tractable, in the sense that it can be optimised w.r.t. the network\nparameters $\\theta$ using standard methods. The training dataset consists of a set of N i.i.d.\nsamples, $y_1,..., y_N$, from the true data distribution, p. For each $y_n$, we generate K i.i.d.\nsamples from the generative model, denoted $\\tilde{y} = {\\tilde{y}_1,..., \\tilde{y}_K}$, where $\\tilde{y}_i = g_\\theta(z_i)$. From\n$\\tilde{y}$, we obtain one sample of the r.v. $A_K$, that we denote as $a_{K,n}$.\n\nWe replace $d_K(p, p_\\theta)$ by a differentiable approximation and refer to this surrogate\nfunction as invariant statistical loss (ISL) [1]. The ISL mimics the construction of a\nhistogram from the statistics $a_{K,1}, a_{K,2}, ..., a_{K,N}$. Given a real data point $y_n$, we can\ntally how many of the K simulated samples in $\\tilde{y}$ are less than the n-th observation $y_n$.\nSpecifically, one computes\n\n$\\tilde{a}_{K,n}(y) = \\sum_{i=1}^K \\sigma_\\alpha(y_n - \\tilde{y}_i) = \\sum_{i=1}^K \\sigma_\\alpha(y_n - g_\\theta(z_i))$,"}, {"title": "3. Pareto-ISL", "content": "where $z_i \\sim p_Z$ is a sample from a univariate distribution, and $\\sigma_\\alpha(x) := \\sigma(\\alpha x)$, with\n$\\sigma(x) := 1/(1 + \\exp(-x))$ the sigmoid function. As we can see, $\\tilde{a}_{K,n}$ is a differentiable\n(w.r.t. $\\theta$) approximation of the actual statistic $A_K$ for the observation $y_n$. The parameter $\\alpha$\nenables us to adjust the slope of the sigmoid function to better approximate the (discrete)\n'counting' in the construction of $a_{K,n}$.\n\nA differentiable surrogate histogram is constructed from $\\tilde{a}_{K,1},..., \\tilde{a}_{K,N}$ by leveraging\na sequence of differentiable functions. These functions are designed to mimic the bins\naround $k \\in {0, ..., K}$, replacing sharp bin edges with functions that replicate bin values\nat k and smoothly decay outside the neighborhood of k. In our particular case, we\nconsider radial basis function (RBF) kernels {$\\psi_k$}$_{k=0}^K$ centered at $k \\in {0,..., K}$ with\nlength-scale $\\nu^2$, i.e., $\\psi_k(a) = \\exp(-(a - k)^2/2\\nu^2)$. Thus, the approximate normalized\nhistogram count at bin k is given by\n\n$q[k] = \\frac{1}{N} \\sum_{i=1}^N \\psi_k(\\tilde{a}_{K,i}(y))$,\n\nfor $k = 0,..., K$. The ISL is computed as the $\\ell_\\infty$-norm distance between the uniform\nvector $\\frac{1}{K+1} \\mathbb{1}_{K+1}$ and the vector of empirical probabilities $q = [q[0], q[1], . . ., q[K]]$,\nnamely,\n\n$L_{ISL}(\\theta, K) := || \\frac{1}{K+1} \\mathbb{1}_{K+1} - q||_2$\n\nRemark 3. The ISL is a sum and composition of $C^{\\infty}(\\mathbb{R})$ functions. It is smooth w.r.t.\nthe fictitious samples {$\\tilde{y}_i$}, and the data $y_n$. As a result, its regularity aligns with that\nof the neural network (as a function of both its parameters and the input noise).\n\nThe hyperparameters for the ISL method include the number of samples K, which is\ntunable, the activation function $\\sigma(\\alpha \\cdot x)$, and the set of basis functions {$\\psi_k$}$_{k=0}^K$, specified\nas radial basis function (RBF) kernels with a length scale of $\\nu^2$. These parameters\ncontrol the flexibility and behavior of the model during learning.\n\nIn Appendix C.1 we present a numerical study of the accuracy in the approximation\nof the divergence $d_K(p, p_\\theta)$ by its surrogate, ISL $L_{ISL}$."}, {"title": "2.4. Progressively training by increasing K", "content": "As shown in [1], ISL outperforms other generative methods in learning the central\nregions of typical 1D distributions. However, Figure 1 indicates that when standard\n\nThe training procedure can be made more efficient by performing it in a sequence of\nincreasing values of K (see [1]). Specifically, one can select $K^{(1)} < K^{(2)} < ... < K^{(l)}$,\nwhere l is the total number of stages and $K^{(l)} = K_{max}$, the maximum admissible value\nof K. The iterative training scheme is outlined in Algorithm 1. The gain in efficiency of\nthe progressive training procedure compared to a scheme with fixed K is illustrated in\nAppendix C.2."}, {"title": "3.1. Tail distributions and extreme value theory", "content": "As shown in [1], ISL outperforms other generative methods in learning the central\nregions of typical 1D distributions. However, Figure 1 indicates that when standard\nGaussian noise is used as an input, NNs struggle to capture the tails of Cauchy mixtures,\nsince compactly supported inputs cannot produce unbounded pdfs (which we will refer\nto as unbounded distributions). This issue can be addressed by using input noise from a\ngeneralized Pareto distribution (GPD). In this section, we introduce Pareto-ISL, which\nutilizes a GPD for input noise, and demonstrate its effectiveness in learning heavy-tailed\ndistributions.\n\nThe conditional excess distribution function $F_u(y)$ provides a key tool for analyzing\nthe tail of a distribution by focusing on exceedances over a specified threshold u. This\nfunction describes the probability distribution of the excess amount $X - u$, given X > u\nexceeds u. By conditioning on large values, it isolates the behavior of the distribution in\nits tail, where extreme or rare events are more likely to occur. In the context of extreme\nvalue theory, for sufficiently high thresholds u, the conditional excess distribution\nfunction converges to the GPD. The GPD, parameterized by the tail index $\\xi$ and scaling\nparameter $\\sigma$, provides a flexible model for the tail, allowing us to characterize its\nheaviness and the probability of extreme values.\n\nThe following definitions are taken from [18] and provided here for convenience."}, {"title": "Definition 1.", "content": "The conditional excess distribution function with threshold $u \\in \\mathbb{R}$ is\ndefined as\n\n$F_u(y) = P(X - u \\leq y|X > u) = \\frac{F(u + y) - F(u)}{1- F(u)}$"}, {"title": "Definition 2.", "content": "The GPD, parametrized by tail index $\\xi \\in \\mathbb{R}$ and scaling parameter\n$\\sigma \\in \\mathbb{R}_+$, has the following complementary cumulative distribution function (CCDF)\n\n$S(z; \\xi, \\sigma) = \\begin{cases}\n(1 + \\xi z/ \\sigma)^{-1/\\xi}, & \\text{for } \\xi \\neq 0, \\\\\ne^{-z/ \\sigma}, & \\text{for } \\xi = 0.\n\\end{cases}$"}, {"title": "3.2. Comparison of Pareto-ISL and standard ISL in learning a Cauchy mixture", "content": "Lipschitz continuous functions map bounded distributions to bounded ones [19,\nChapter 3], limiting the ability of NNs to model heavy-tailed distributions. To address\nthis, unbounded input distributions and NN generators are required.\n\nTo construct unbounded NN generators, recall that piecewise linear (PWL) functions,\n(which include operations like rectified linear unit (ReLU), leaky ReLU, linear layers,\naddition, and batch normalization), are closed under composition [20, Theorem 2.1]\nand are unbounded, making them ideal for constructing generators that approximate\nheavy-tailed distributions. We then define a Pareto-ISL generator, $g_{PWL}$, as a piecewise\nlinear generator driven by an input from a GPD with tail index $\\xi$, and trained using ISL.\nEstimators such as Hill's estimator [21] can be used to estimate $\\xi$, aiding in the accurate\nmodeling of heavy-tailed behavior.\n\nIn Figure 1, we compare Pareto-ISL against standard ISL where the data distribution\nis a two-component Cauchy mixture with locations at -1.0 and 1.0, and scales 0.7 and\n0.85. All generators use a uniform four-layer multilayer perceptron (MLP) with 35\nunits per layer and ReLU activation. Generators are trained with ISL using K = 20,\nN = 1000, and a learning rate of 10\u20133. For Pareto-ISL, the tail parameter is set to $\\xi$ = 1,\naligning GPD noise with the tail index of the Cauchy mixture. Introducing GPD noise\nimproves tail approximation and enhances the modeling of the central part of the target\ndistribution, as demonstrated in the logarithmic-scale (bottom row) and linear-scale (top\nrow) of Figure 1.\n\nIn Appendix C.3, we present a multidimensional heavy-tailed distribution and\ncompare Pareto-ISL to ISL with Gaussian noise (results shown in Figure C.9)."}, {"title": "3.3. Performance evaluation of Pareto-ISL compared to other implicit generative models", "content": "In a second experiment, we evaluate the performance of Pareto-ISL as compared\nto different GANs. For this comparison, we consider four data distributions, includ-\ning a Cauchy distribution with location parameter 1 and scale parameter 2 (labeled\nCauchy(1, 2)), a Pareto distribution with scale parameter 1 and shape parameter 1 (la-\nbeled Pareto(1, 1)) and two mixture distributions, labeled Model3 and Model4. Model3"}, {"title": "4. ISL-Slicing: A Random Projections-Based Approach", "content": "Machine learning datasets are often multi-dimensional. Building on [25, 26], we\nextend the one-dimensional loss function $L_{ISL}$ to a general metric for higher dimensions."}, {"title": "4.1. Random projections vs. marginals on high-dimensional data", "content": "We do this by randomly projecting high-dimensional data onto various 1D subspaces,\nspecifically in all possible directions $s \\in \\mathbb{S}^d$, where $\\mathbb{S}^d$ is the unit hypersphere in\nd + 1-dimensional space.\n\nLet x be a (d + 1)-dimensional r.v. and let $s \\in \\mathbb{R}^{d+1}$ be a deterministic vector. We\ndenote by $s#p$ the pdf of the real r.v. $y = s^T x$. Using this notation we define the sliced\nISL distance between distributions with pdfs p and $\\tilde{p}$ as\n\n$d_\\mathbb{S}^d(p, \\tilde{p}) := \\int_{\\mathbb{S}^d} d_K(s#p, s#\\tilde{p}) ds,$\n\nwhere $d_K(p, \\tilde{p}) = d_K(s#p, s#\\tilde{p})$.\n\nSince the expectation in the definition of the sliced ISL distance is computationally\nintractable, we approximate it using Monte Carlo sampling. Specifically, we choose\na pdf q on $\\mathbb{S}^d$ and sample directions $s_i \\sim q$, for i = 1,...,m. Then, the Monte Carlo\napproximation of the sliced ISL distance is\n\n$\\tilde{d}_\\mathbb{S}^d(p, \\tilde{p}) = \\frac{1}{m} \\sum_{i=1}^m d_K(s_i#p, s_i#\\tilde{p})$.\n\nIf $\\tilde{p} = p_\\theta$ is the pdf of the r.v. $y = g_\\theta(z), z \\sim p_z$, i.e., the output of a NN with random\ninput z and parameters $\\theta$, then one can use $\\tilde{d}_\\mathbb{S}^d(p, p_\\theta)$ as a loss function to train the NN."}, {"title": "Remark 4.", "content": "In practice, we use the surrogate loss (see Section 2.3) to approximate\n$d_K(s_i#p, s_i#p_\\theta)$ in Eq. 4, and sample random vectors from the unit sphere. Randomly\nchosen vectors from the unit sphere in a high-dimensional space are typically almost\northogonal. More precisely, ([27])\n\n$\\langle \\frac{v_1}{||v_1||_2}, \\frac{v_0}{||v_0||_2} \\rangle < \\epsilon \\rangle > 1 - 2e^{-\\frac{1}{2}d\\epsilon^2}$,\n\nwhere $v_0$ and $v_1$ are uniformly distributed random vectors, and $\\epsilon$ is a small positive\nconstant.\n\nThe pseudocode for training implicit models using the proposed random projection\nmethod, referred to as the ISL-slicing algorithm, is provided in Algorithm 2.\n\nFollowing [28], we conduct experiments on a multi-modal synthetic dataset. Specif-\nically, we generate D-dimensional synthetic data from the model\n\n$\\begin{cases}\nz \\sim \\mathcal{N}(0, 10 \\cdot I_d), A \\sim \\mathcal{N}(0, I_{D \\times d}), \\epsilon \\sim \\mathcal{N}(0, 0.01 \\cdot I_D), \\\\\nx = Az + \\epsilon, d \\ll D.\n\\end{cases}$\n\nIn these experiments, we fit a standard GAN to a dataset where D = 100 and d = 2.\nThe generator is a 3-layer fully connected neural network with 10, 1000, and 100 units,\nrespectively, using ReLU activations throughout."}, {"title": "4.2. Experiments on 2D distributions", "content": "Following [28], we conduct experiments on a multi-modal synthetic dataset. Specif-\nically, we generate D-dimensional synthetic data from the model\n\n$\\begin{cases}\nz \\sim \\mathcal{N}(0, 10 \\cdot I_d), A \\sim \\mathcal{N}(0, I_{D \\times d}), \\epsilon \\sim \\mathcal{N}(0, 0.01 \\cdot I_D), \\\\\nx = Az + \\epsilon, d \\ll D.\n\\end{cases}$\n\nWe begin by examining simple 2D distributions characterized by different topo-\nlogical structures: one distribution with two modes, another with eight modes, and"}, {"title": "4.3. ISL-pretrained GANs for robust mode coverage in 2D grids", "content": "We begin by examining simple 2D distributions characterized by different topo-\nlogical structures: one distribution with two modes, another with eight modes, and\na third featuring two rings. Our objective is to assess the ability of the ISL-slicing\nmethod to fully capture the support of these distributions. We compare our approach to\nnormalizing flows and GANs, using KL-divergence and visual assessment as metrics.\n\nFor GAN, WGAN, and ISL, we use a 4-layer MLP generator with a 2D input\nsampled from a standard normal distribution. Each layer has 32 units and uses the\nhyperbolic tangent activation. The discriminator is an MLP with 128 units per layer,\nusing ReLU activations except for the final sigmoid layer. We used a batch size of 1000\nand optimized the critic-to-generator update ratio over {1:1, 2:1, 3:1, 4:1, 5:1} for\nGAN and WGAN. The learning rate was chosen from {$10^{-2}$, $10^{-3}$, $10^{-4}$, $10^{-5}$}. For ISL,\nwe set K = 10, N = 1000, 10 random projections, and a learning rate of $10^{-3}$. For the\nnormalizing flow model, we used the RealNVP architecture from [29], with 4 layers of\naffine coupling blocks, parameterized by MLPs with two hidden layers of 32 units each.\nThe learning rate was set to $5 \\cdot 10^{-5}$, using the implementation from [30]. All methods\nwere trained for 1000 epochs, with optimization performed using the ADAM algorithm.\n\nFigure 3 highlights the challenges of training GANs, particularly their suscepti-\nbility to mode collapse. In contrast, normalizing flow methods preserve topology via\ninvertibility constraints but struggle to model complex structures, often forming a single\nconnected component due to density filaments. Our method overcomes this by cap-\nturing the full distribution support and distinguishing between connected components,\nas seen in the Dual Moon example. However, ISL can fill regions between modes, as\nseen in the Circle of Gaussians, an issue mitigated by increasing K. Alternatively,\ncombining a pretrained network with ISL (trained for 100 epochs) and a GAN yielded\nthe best results (method denoted as ISL+GAN), capturing full support while excluding\nzero-density regions. This approach is detailed further in Section 4.3. We also estimate\nthe KL-divergences between the target and model distributions, as listed in Table 3. In\nall cases, the ISL method, and particularly the ISL+GAN approach, outperform the\nrespective baselines.\n\nIn this experiment, we explore how ISL-pretrained GANs improve mode coverage\non the 2D-Ring and 2D-Grid datasets, benchmarks commonly used in generative model\nevaluation. The 2D-Ring dataset consists of eight Gaussian distributions arranged in a\ncircle, while the 2D-Grid dataset has twenty-five Gaussians on a grid. We first pretrain\nwith ISL-slicing to ensure comprehensive mode coverage, then fine-tune with a GAN.\nPerformance was compared to other GANs using two metrics: the number of covered\nmodes (#modes) and the percentage of high-quality samples (%HQ). A mode is covered"}, {"title": "4.4. Preventing mode collapse in DCGANs on MNIST", "content": "In this experiment", "metrics": "the number of covered\nmodes (#modes) and the percentage of high-quality samples (%HQ). A mode is covered\nWe evaluate the ability of our method, in combination with DCGAN [36"}]}