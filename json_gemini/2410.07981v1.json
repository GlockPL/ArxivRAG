{"title": "MOLMIX: A Simple Yet Effective Baseline for Multimodal Molecular Representation Learning", "authors": ["Andrei Manolache", "Drago\u0219 T\u00e2n\u0163aru", "Mathias Niepert"], "abstract": "In this work, we propose a simple transformer-based baseline for multimodal molecular representation learning, integrating three distinct modalities: SMILES strings, 2D graph representations, and 3D conformers of molecules. A key aspect of our approach is the aggregation of 3D conformers, allowing the model to account for the fact that molecules can adopt multiple conformations-an important factor for accurate molecular representation. The tokens for each modality are extracted using modality-specific encoders: a transformer for SMILES strings, a message-passing neural network for 2D graphs, and an equivariant neural network for 3D conformers. The flexibility and modularity of this framework enable easy adaptation and replacement of these encoders, making the model highly versatile for different molecular tasks. The extracted tokens are then combined into a unified multimodal sequence, which is processed by a downstream transformer for prediction tasks. To efficiently scale our model for large multimodal datasets, we utilize Flash Attention 2 and bfloat16 precision. Despite its simplicity, our approach achieves state-of-the-art results across multiple datasets, demonstrating its effectiveness as a strong baseline for multimodal molecular representation learning.", "sections": [{"title": "1 Introduction and Related Work", "content": "Accurately representing molecular structures is fundamental in computational chemistry and drug discovery [1, 2, 3]. Effective molecular representations enable machine learning models to predict molecular properties, understand chemical behaviors, and accelerate the development of new compounds. Traditional molecular representation methods typically focus on a single modality, such as SMILES strings [4, 5], chemical fingerprints [6], 2D molecular graphs [7], or the 3D geometry of molecules [8, 9]. While effective, these methods overlook important molecular characteristics that can be captured by other modalities [10, 11]. To address this, recent research has introduced multimodal approaches that integrate multiple molecular representations to provide richer representations for machine learning tasks in chemistry. St\u00e4rk et al. [12] proposed an information maximization approach to enhance the mutual information between 2D and 3D molecular embeddings. Similarly, Liu et al. [13] used contrastive pre-training to align 2D and 3D representations. Other approaches extract both 2D and 3D features, such as shortest path distances and 3D distance encodings, to build multimodal models [14, 15]. Zhu et al. [16] unified 2D and 3D molecular data in a pre-training framework by predicting either masked 2D structures or 3D conformations. Additionally, language-based models have been integrated with molecular data. Tang et al. [17], Xiao et al. [18], and Srinivas and Runkana [19] leveraged large-scale language models to incorporate textual descriptions of molecules, enhancing molecular property predictions.\nDespite these advances, there remains a need for simple yet effective models that can seamlessly integrate multiple modalities and handle multiple conformers without significant computational overhead. Moreover, recent observations [24, 25] suggest that some model design choices might be unnecessary for strong empirical performance, thereby making the added complexity superfluous and inefficient. To address this challenge, we propose MOLMIX, a simple yet effective baseline for multimodal molecular representation learning. We employ modality-specific encoders - a transformer for SMILES strings, a GNN for 2D graphs, and equivariant neural networks for 3D conformers - to extract text and node embeddings from each modality. These embeddings are concatenated into a multimodal sequence, separated by special tokens, and fed into a downstream transformer that predicts molecular properties. By leveraging efficient techniques like Flash Attention [26, 27] and bfloat16 precision, MOLMIX scales to handle large sequences of atom tokens with minimal computational overhead, enabling the direct incorporation of all conformers. Despite its straightforward design, MOLMIX achieves state-of-the-art results across multiple datasets, demonstrating that simplicity can be highly effective in multimodal molecular representation learning, while the modular design allows us to easily exchange the specific modality encoders.\nTo summarize, our main contributions are:\n1. Simple multimodal molecular framework: We introduce MOLMIX, which seamlessly combines SMILES strings, 2D molecular graphs, and multiple 3D conformers into a unified sequence for molecular representation learning.\n2. Conformer aggregation: By incorporating node embeddings from 3D conformers, MOLMIX effectively captures conformational variability.\n3. Scalability: We utilize Flash Attention and bfloat16 (bf16) precision to scale our model, enabling the processing of large multimodal datasets with minimal computational overhead.\n4. State-of-the-Art performance: MOLMIX achieves superior results on multiple benchmark datasets, establishing a strong baseline for future research in multimodal molecular representation learning.\n5. Transfer learning capabilities We show that MOLMIX could potentially be used for pre-training on large molecular datasets."}, {"title": "2 MOLMIX: A Multimodal Molecular Transformer", "content": "1D Encoder We represent molecules using SMILES strings, which encode chemical structures as sequences of characters. Let $S = [s_1, s_2,..., s_n]$ denote the input SMILES string, where each $s_i$ is a character. Each $s_i$ is mapped to an embedding vector $e_i = Embedding(s_i)$. To account for sequence order, positional encodings are added: $z_i = e_i + PE(i)$. A transformer encoder [28] then processes these vectors to obtain the hidden representations\n$h_i^{1D} = Transformer(z_i)$,\nfor all $i \\in \\{1, ..., n\\}$. Each hidden representation $h_i^{1D}$ corresponds to the respective input character $s_i$, effectively capturing the contextual information about the molecule, for each character.\n2D Encoder We represent molecules as graphs $G = (V, E)$, where $V$ is the set of atoms and $E$ is the set of covalent bonds. Each atom $v \\in V$ and bond $e_{uv} \\in E$ are associated with initial feature vectors $x_v$ and $e_{uv}$, respectively. We use a message-passing framework with GINE [29, 30] as the backbone to capture the molecular graph's structural information. At each message-passing step $j$, the hidden representation of atom $v$ is updated as\n$h_v^{2D} = GINE \\Big(h_v^{2D_{j-1}}, \\{ h_u^{2D_{j-1}} | u \\in \\mathcal{N}(v) \\}, \\{ e_{uv} \\}\\Big)$,\nwhere $\\mathcal{N}(v)$ denotes the neighbors of atom $v$. This iterative process aggregates information from neighboring atoms and bonds, enabling the model to learn graph representations. The final hidden embeddings $h_v^{2D}$ encode both local and global structural features of the molecule.\n3D Encoder To leverage the three-dimensional structural information of molecules, we utilize 3D conformations represented by the spatial coordinates of each atom. Let $V$ denote the set of atoms. Each atom $v \\in V$ is associated with a 3D coordinate $r_v \\in \\mathbb{R}^3$. To extract meaningful atom embeddings that respect the geometric properties of the molecule, we employ an neural network with 3D inductive biases, such as SchNet [8] or GemNet [9], as the backbone model. These networks process the 3D coordinates $\\{r_v\\}_{v\\in V}$ along with the initial atom features $\\{x_v\\}_{v\\in V}$ and apply a cutoff function to consider interactions within a specified distance range, generating hidden embeddings\n$h_v^{3D} = 3DNetwork(r_v, x_v)$,\nfor all $v \\in V$. These atom embeddings $h_v^{3D}$ capture both the local geometry and the global spatial arrangement of the molecule.\nDownstream Multimodal Transformer To integrate different molecular representations, we design a multimodal transformer that combines three distinct modalities. The SMILES encoder outputs token embeddings $h_i^{1D}$, where $h_i^{1D}$ corresponds to the $i$th character in the string. From the 2D MPNN encoder, we extract atom embeddings $h_v^{2D, j}$ for atom $v$ at layer $j$. By using embeddings from all layers, the model captures both local and distant atom interactions, mitigating the oversmoothing effect common in deep GNNs. The 3D encoder provides atom embeddings $h_{v, c}^{3D}$ for atom $v$ and conformer $c$, encapsulating spatial geometry. We use multiple conformers by simply adding all the atom embeddings to the multimodal sequence. Modality-specific learnable encodings are added to the embeddings from each modality. These modality-enhanced embeddings are concatenated into a unified sequence, with special tokens included: a classification token $h_{CLS}$ is added at the start, and separation tokens $h_{SEP}$ are placed between modalities. The resulting input sequence is structured as\n$H = [h_{CLS}, \\underbrace{h_1^{1D}, ..., h_n^{1D}}_{1D}, h_{SEP}, \\underbrace{h_1^{2D}, ..., h_n^{2D}}_{2D}, h_{SEP}, \\underbrace{h_1^{3D}, ..., h_n^{3D}}_{3D}, h_{SEP} ]$\nThis sequence is then processed by the downstream transformer, which utilizes the self-attention mechanism to integrate and contextualize information across all modalities. After the transformer layers, the embedding corresponding to the classification token $h_{CLS}^{out}$ is extracted and sent to a readout MLP to perform downstream tasks such as property prediction and molecular classification:\n$h_{CLS}^{out} = MultimodalTransformer(H)$,\n$\\hat{y} = MLP(h_{CLS}^{out})$\nWe reduce memory overhead in our multimodal transformer with bfloat16 precision and Flash Attention 2 [27]. See appendix C for details and a memory comparison with classical attention."}, {"title": "3 Experimental Setup and Results", "content": "In this section, we evaluate how MOLMIX improves predictive performance on real-world datasets by addressing the following questions: Q1) How does MOLMIX's performance compare to other sophisticated models?; Q2) Does incorporating multiple modalities enhance downstream performance?; Q3) Are pre-trained weights beneficial for transfer learning?\nTo address Question 1, we train MOLMIX on four MoleculeNet datasets [31]-Lipo, ESOL, FreeSolv, and BACE-covering various molecular properties, including physical chemistry and biophysics. Conformers are generated using the RDKit chemoinformatics package [32]. We follow the same train/validation/test splits as [22]. We also train models on the newly introduced MARCEL benchmark [23], specifically the Drugs-75k and Kraken datasets. Drugs-75K, a subset of the GEOM-Drugs dataset [33], contains 75,099 molecules with conformers generated by Auto3D [34], and labels for ionization potentials (IP), electron affinity (EA), and electronegativity ($\\chi$). Kraken [35] includes 1,552 monodentate organophosphorus ligands, with conformers generated via DFT, and labels for four 3D ligand descriptors: Sterimol B5 (B5), Sterimol L (L), buried Sterimol B5 (BurB5), and buried Sterimol L (BurL). We follow the same splits as in [23]."}, {"title": "4 Conclusions and Further Work", "content": "We propose MOLMIX, a simple yet effective multimodal molecular transformer supporting conformer aggregation. MOLMIX preserves inductive biases of modality encoders and achieves state-of-the-art results across multiple datasets. We hint towards MOLMIX being able to support transfer learning, suggesting that it could be used as a molecular foundation model. Finally, we use Flash Attention and bf16 precision to handle longer sequences and multiple modalities efficiently.\nWe leave three open questions. First, large self-supervised VLMs excel in 0-shot prediction and fine-tuning [36, 37, 38, 39]. Exploring self-supervised pre-training for MOLMIX using signals like masked language modeling [40] and noise-contrastive estimation [41] could be valuable. Second, multiple conformers without pooling may be suboptimal; token merging [42] could improve memory and runtime. Lastly, adding modalities like molecular fingerprints may enhance performance."}, {"title": "A Proof for Theorem 1", "content": "Theorem 1. Let $S$ be the SMILES string, $G$ be the 2D graph, and $\\{C_1, ..., C_k\\}$ be a set of $k$ 3D conformers for a molecule. Let $\\hat{y} = f_\\theta(S,G, \\{C_1, ..., C_k\\})$ be the output prediction obtained as described in eq. (1) - (5). Let our 3D encoder be invariant to the actions of some group $G$. Then $f_\\theta$ is also invariant to any $T_1, . . ., T_k \\in G$, i.e. $f_\\theta(S, G, \\{T_1C_1,...,T_kC_k\\}) = f_\\theta(S, G, \\{C_1,...,C_k\\})$.\nProof. Let $g_\\theta$ be our 3D encoder network, as described in eq. (3). Let $V$ be the set of atoms and $\\{X_v\\}_{v\\in \\nu}$, $\\{r_v\\}_{v\\in \\nu}$ the atom features and their 3D coordinates, such that a conformer can be described as the tuple $c = (\\{x_v\\}_{v\\in \\nu}, \\{r_v\\}_{v\\in \\nu})$. We assume that $g_\\theta$ is invariant to any action $T \\in G$, therefore we have that, for any conformer $c$, $g_\\theta (Tc) = g_\\theta(c) = h^{3D}$.\nLet $h_\\theta$ be the downstream Transformer together with the readout layer, as described in eq. (4) - (5). Since we add the same learnable modality encoding to each $h^{3D}_{v,c}$, we also have that for any permutation $\\pi\\in Sym(K)$, we have\n$h_\\theta(\\{g_\\theta(T_1C_1),...,g_\\theta(T_kC_k)\\}  = h_\\theta(\\{g_\\theta(C_1),...,g_\\theta(C_k)\\}) \\\\\n= h_\\theta(\\{h^{3D}_{v,1},...,h^{3D}_{v,k}\\}_{v\\in \\nu}) \\\\\n= h_\\theta(\\{h^{3D}_{v,\\pi(1)},..., h^{3D}_{v,\\pi(k)}\\}_{v\\in \\nu})  = \\hat{y}$,\ntherefore, if when we include the 2D graph $G$ and the SMILES string $S$, we obtain $f_\\theta(S, G, \\{T_1C_1,...,T_kC_k\\}) = f_\\theta(S, G, \\{c_1, ..., C_k \\}) = \\hat{y}$."}, {"title": "B Qualitative attention example", "content": "We present the attention scores for each head in a MOLMIX model trained on the Drugs-75k dataset, using a randomly sampled molecule from the dataset for visualization. As shown in Figure 1, distinct patterns emerge across the attention heads. While it remains challenging to assign a definitive interpretation to each individual head, certain sparse or dense patterns are evident in each cross-modality section. This suggests that the model is learning to extract meaningful and potentially useful features from all modalities."}, {"title": "C Attention implementation details", "content": "We employ Flash Attention 2 [27] for the self-attention mechanism in our models. Flash Attention 2 is a hardware-optimized implementation that significantly reduces both memory usage and runtime compared to the standard attention algorithm. It achieves these gains by leveraging GPU programming techniques, such as kernel fusion and tiling. Additionally, we utilize the varlen implementation, which prevents unnecessary memory and compute consumption on padding tokens."}]}