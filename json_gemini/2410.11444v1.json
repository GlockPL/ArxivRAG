{"title": "ON CHAMPIONING FOUNDATION MODELS: FROM EXPLAINABILITY TO INTERPRETABILITY", "authors": ["Shi Fu", "Yuzhu Chen", "Yingjie Wang", "Dacheng Tao"], "abstract": "Understanding the inner mechanisms of black-box foundation models (FMs) is essential yet chal-\nlenging in artificial intelligence and its applications. Over the last decade, the long-running focus has\nbeen on their explainability, leading to the development of post-hoc explainable methods to ratio-\nnalize the specific decisions already made by black-box FMs. However, these explainable methods\nhave certain limitations in terms of faithfulness, detail capture and resource requirement. Conse-\nquently, in response to these issues, a new class of interpretable methods should be considered to\nunveil the underlying mechanisms in an accurate, comprehensive, heuristic and resource-light way.\nThis survey aims to review interpretable methods that comply with the aforementioned principles\nand have been successfully applied to FMs. These methods are deeply rooted in machine learn-\ning theory, covering the analysis of generalization performance, expressive capability, and dynamic\nbehavior. They provide a thorough interpretation of the entire workflow of FMs, ranging from the\ninference capability and training dynamics to their ethical implications. Ultimately, drawing upon\nthese interpretations, this review identifies the next frontier research directions for FMs.", "sections": [{"title": "Introduction", "content": "Foundation Models (FMs), emerging as a new paradigm in deep learning, are fundamentally changing the landscape\nof artificial intelligence (AI). Unlike traditional models trained for specific tasks, FMs leverage massive and diverse\ndatasets for training, which allows them to handle various downstream tasks through techniques like supervised fine-\ntuning (SFT) [1], Reinforcement Learning from Human Feedback (RLHF) [2], Retrieval Augmented Generation\n(RAG) [3], prompting engineering [4, 5] or continual learning [6]. In natural language processing, most FMs un-\nder autoregressive structure (e.g., GPT-3 [7], PaLM [8], or Chinchilla [9]) are established to output the next token\ngiven a sequence. Most text-to-image models, for example DALL\u00b7E [10], are trained to capture the distribution of im-\nages given a text input. Video FMs, trained on video data, potentially combined with text or audio, can be categorized\nbased on their pretraining goals: generative models like VideoBERT [11] focus on creating new video content, while\ndiscriminative models like VideoCLIP [12] excel at recognizing and classifying video elements. Hybrid approaches\nlike TVLT [13] combine these strengths for more versatile tasks. FMs have the potential to revolutionize a vast array\nof industries. Consider ChatGPT, a versatile chatbot developed by OpenAI, which has already made strides in en-\nhancing customer support, virtual assistants, and voice-activated devices [14]. As FMs become more integrated into\nvarious products, their deployments will be scaled to accommodate a growing user base. This trend is evident in the\ngrowing list of companies, such as Microsoft with Bing Chat and Google with Bard, all planning to deploy similar\nFMs-powered products, further expanding the reach of this technology."}, {"title": "The Bottleneck of Explainability in Foundation Models", "content": "Despite of rapid development of FMs, there are also ongoing concerns that continue to arise. The underlying causes\nbehind certain emerging properties in FMs remain unclear. [15] firstly consider a focused definition of emergent abili-\nties of FMs: \"an ability is emergent if it is not present in smaller models but is present in larger models\". They conduct\nsufficient empirical studies to show emergent abilities can span a variety of large-scale language models, task types,\nand experimental scenarios. On the contrary, serveral works claim that emergent abilities only appear for specific\nmetrics, not for model families on particular tasks, and that changing the metric causes the emergence phenomenon to\ndisappear [16]. Moreover, prompting strategies can elicit chains of thought (CoT), suggesting a potential for reasoning\nabilities of FMs [5]. However, recent critiques by Arkoudas highlight limitations in current evaluation methods, argu-\ning that GPT-4's occasional displays of analytical prowess may not translate to true reasoning capabilities [17]. 'All\nthat glitters is not gold', as potential risks gradually emerge behind the prosperous phenomenon. The phenomenon of\n\u201challucination\" has garnered widespread public attention, referring to the tendency of FMs to generate text that sounds\nconvincing, even if it is fabricated or misleading, as discussed in recent research [18]. Additionally, studies have shown\nthat adding a series of specific meaningless tokens can lead to security failures, resulting in the unlimited leakage of\nsensitive content [19]. These issues serve as stark reminders that we still lack a comprehensive understanding of the\nintricate workings of FMs, hindering our ability to fully unleash their potential while mitigating potential risks.\nThe term \"explainability\" refers to the ability to understand the decision-making process of an AI model. Specifi-\ncally, most explainable methods concentrates on providing users with post-hoc explanations on the specific decisions\nalready made by the black-box models [20\u201322]. Recently, several survey studies review local and global explainable\nmethods for FMs [21-23]. Local methods, like feature attribution [24-26], attention-based explanation [27-29], and\nexample-based explanation [30, 31], focus on understanding the model's decisions for specific instances. Conversely,\nglobal methods, such as probing-based explanation [32, 33], neuron activation explanation [34, 35], and mechanistic\nexplanation [36, 37], aim to delve into the broader picture, revealing what input variables significantly influence the\nmodel as a whole. More specifically, these explanation techniques demonstrate excellent performance in exploring the\ndifferences in how humans and FMs work. For instance, [38] utilize perturbation based explainable approach, SHAP\n(SHapley Additive exPlanations) [25], to investigate the influence of code tokens on summary generation, and find\nno evidence of a statistically significant relationship between attention mechanisms in FMs and human programmers.\nEndeavors are also being made to propose explainable methods tailored specifically for FMs. [39] propose a novel\nframework to provide impact-aware explanations, which are robust to feature changes and influential to the model's\npredictions. Recently, a GPT-4 based explainable method is utilized to explain the behavior of neurons in GPT-2 [34].\nHowever, explanations garnered from these explanable methods for complex FMs are often not entirely reliable and\nmay even be misleading:\n\u2022 Inherent Unfaithfullness: [20] argue that if an explanation perfectly mirrored the model's computations, the\nexplanation itself would suffice, rendering the original model unnecessary. This highlights a key limitation\nof explainable methods for complex models: any explanation technique applied to a black-box model is\nlikely to be an inaccurate representation. For instance, adversarial examples are carefully crafted inputs that\nare slightly perturbed from the original data, but often lead to misclassification or incorrect predictions by"}, {"title": "Interpretability tailored for Foundation Models", "content": "Numerous researchers have endeavored to distinguish the term \u201cinterpretability\u201d from \u201cexplainability", "the ability to explain or present, in understandable terms to a\nhuman.\\\" [41": "highlight, interpretability and explainability seek to answer different questions: 'How does the model\nwork?' versus 'What else can the model tell me?'. [20] states that interpretable machine learning emphasizes creating\ninherently interpretable models, whereas explainable methods aim to understand existing black-box models through\npost-hoc explanations. Following this definition, recent efforts have focused on developing inherently interpretable\nFMs based on symbolic or causal representations [42-44]. However, a prevalent reality in the field of FMs is that the\nmost widely used and high-performing FMs remain black-box in nature."}, {"title": "Interpretable Methods", "content": "From above all, there is no universally agreed-upon definition of interpretability within the machine learning commu-\nnity. The concept often varies based on the application domain and the intended audience [20, 45]. This motivates\nus to define appropriate interpretability and develop effective interpretable methods to reveal the internal workings of\nFMs, as shown in Figure 1. In our perspective, the interpretability tailored for FMs refers to the ability to reveal the\nunderlying relationships between various related factors, such as model architecture, optimization process, data and\nmodel performance, in an accurate, legible, and heuristic way. This level of interpretability can significantly enhance\npublic confidence in model components, mitigate potential risks, and drive continuous improvement of these black-\nbox models. To achieve this, we will develop interpretable methods tailored for FMs, grounded in rigorous learning\ntheory. These methods will encompass analyses of generalization performance, expressivity, and dynamic behavior.\nGeneralization performance analysis, a form of uncertainty quantification, is the process of evaluating a foundation\nmodel's ability to accurately predict or classify new, unseen data. Expressivity analysis is the process of evaluating\na model's capacity to represent complex patterns and functions within the data. Dynamic behavior analysis involves\ncapturing the transformations and adaptations that occur within FMs as they learn from data, makes predictions, and\nundergoes training iterations. These interpretable methods align with the principles of interpretability as they:\n\u2022 Accurate. Compared to explainable methods based on post-hoc explanation, theoretical analysis, grounded\nin stringent mathematical or statistical principles, has the ability to predict the expected performance of\nalgorithms under various conditions, ensuring accuracy and faithfulness in their assessments.\n\u2022 Comprehensive. The insights drawn from theoretical constructs are typically more comprehensive, delving\ninto the multidimensional influences on model performance from a holistic viewpoint. For instance, within\nthe realm of model generalization analysis, we can discern the intricate ways in which factors like optimiza-\ntion algorithms and model architecture shape and impact the model's ability to generalize effectively.\n\u2022 Heuristic. Theoretical analyses can provide heuristic insights by distilling complex information into practical\nrules, guidelines, or approximations that facilitate decision-making, problem-solving, and understanding of\ncomplex systems. These insights are derived from a deep understanding of the underlying principles but are\npresented in a simplified and practical form for application in real-world scenarios.\n\u2022 Resource-light. Theoretical analyses in machine learning are regarded as resource-light because they empha-\nsize understanding fundamental concepts, algorithmic properties, and theoretical frameworks through mathe-\nmatical abstractions. This approach stands in contrast to explainable methods that entail heavy computational\ndemands for training additional models.\nThe structure of this paper is as follows (for a visual representation of the overall framework, please see Figure 2):\nSection 2 reviews key interpretable methods, including analyses of generalization performance, expressivity, and dy-\nnamic behavior. Section 3 delves into the inference capabilities of FMs, exploring in-context learning, CoT reasoning,\nand distribution adaptation. Section 4 provides detailed interpretations of FMs training dynamics. Section 5 focuses\non the ethical implications of FMs, including privacy preservation, fairness, and hallucinations. Finally, Section 6\ndiscusses future research directions based on the insights gained from the aforementioned interpretations."}, {"title": "Generalization Analysis", "content": "Generalization error measures the performance of a machine learning model on unseen data. Mathematically, for a\nhypothesis h and a dataset $S = \\{(X_1,Y_1), ..., (X_n, Y_n)\\}$, the generalization error can be expressed as:\n$R(A(S)) \u2013 R_n(A(S)) = \\mathbb{E}_{(x,y)\\sim D}[ l(h(x), y)] - \\frac{1}{n} \\sum_{i=1}^n l(h(x_i), Y_i)$,\nwhere D denotes the data distribution, l is the loss function, and $A:S \\rightarrow h$ represents the learning algorithm.\nLower generalization error bound indicates a model that better generalizes from the training data to new data, thereby\nenhancing its predictive performance on real-world tasks."}, {"title": "VC-dimensition", "content": "The Vapnik-Chervonenkis dimension (VC-dimension, [117-119]) is an expressive capacity\nmeasure for assessing the hypothesis complexity. The pertinent definitions are presented below.\nDefinition 2.1. For any non-negative integer n, the growth function of a hypothesis space H is defined as follows:\n$\\Pi_H(n) := \\max_{x_1,...,x_n \\in X} |\\{(h(x_1),..., h(x_n)) : h \\in H\\}|.$"}, {"title": "Rademacher complexity", "content": "Rademacher complexity captures the ability of a hypothesis space to fit random labels as\na measure of expressive capacity [120, 121]. An increased Rademacher complexity signifies higher hypothesis com-\nplexity and, correspondingly, an augmented capacity for model expressiveness. The common definition are presented\nbelow.\nDefinition 2.2 (Empirical Rademacher complexity). Given a real-valued function class H and a dataset S, the empir-\nical Rademacher complexity is defined as follows,\n$\\hat{R}(H) = E_{\\sigma} [\\sup_{h \\in H} \\frac{1}{n} \\sum_{i=1}^n \\sigma_i h(x_i)]$,\nwhere $\\sigma_1,...,\\sigma_n$ are i.i.d. Rademacher random variables with $P \\{\\sigma_i = 1\\} = P \\{\\sigma_i = -1\\} = \\frac{1}{2}$.\nOne can establish a worst-case generalization error bound via the Rademacher complexity. Specifically, for a loss\nfunction with an upper bound of B, for any $\\delta \\in (0,1)$, with probability at least 1 \u2013 $\\delta$, the following holds for all\n$h \\in H$,\n$R(h) \u2013 R_n(h) \\leq 2B \\hat{R}(H) + 3B\\sqrt{\\log(2/\\delta)/n}$."}, {"title": "Algorithmic Stability", "content": "Stability concepts address these limitations and offer more refined insights into the\noptimization-dependent generalization behavior. Early work by [124] introduced the notion of algorithmic stability\nand demonstrated how it could be used to derive generalization bounds. They showed that if an algorithm is uniformly\nstable, meaning the output hypothesis does not change significantly with small perturbations in the training data, then it\nis possible to establish tight generalization bounds. Their pioneering work laid the foundation for subsequent research\nin understanding the stability properties of various learning algorithms and their impact on generalization performance\n[125\u2013128]. In particular, uniform stability and error stability are key algorithmic properties that will be utilized in the\nanalyses presented in this section.\nDefinition 2.3 (Uniform stability [124]). Let S and S' be any two training samples that differ by a single point. A\nlearning algorithm A is $\\beta$-uniformly stable if\n$\\sup_{z \\in Z}[ |l(A(S), z) \u2013 l(A(S'), z)|] \\leq \\beta$,\nwhere $z = (x, y)$ and l is the loss function.\nDefinition 2.4 (Error stability [124]). A learning algorithm A has error stability $\\beta$ with respect to the loss function l\nif the following holds:\n$\\forall S \\in Z^n, \\forall i \\in \\{1, ..., n\\}, |E_z[l(A(S), z)] \u2013 E_z[l(A(S^{ \\setminus i}), z)]| \\leq \\beta$.\nConsequently, if algorithm A is applied to two closely similar training sets, the disparity in the losses associated\nwith the resulting hypotheses should be constrained to no more than $\\beta$. [127] have demonstrated that, utilizing mo-\nment bound and concentration inequality, generalization error bounds of stable learning algorithms can be derived as\nfollows:\nTheorem 2.5 (Exponential generalization bound in terms of uniform stability). Let algorithm Abe $\\beta$-uniformly stable,\nand let the loss function satisfy $l(h, z) \\leq M, \\forall h \\in H$ and $\\forall z \\in Z$. Given a training sample $S = \\{(x_i, y_i)\\}_{i=1}^n$, for\nany $\\delta\\in (0, 1)$, it holds that:\n$R(A(S)) \u2013 R_n(A(S)) \\leq \\beta\\log n\\log (1/\\delta) + M\\sqrt{\\log (1/\\delta) /n}$\nwith probability at least 1 \u2013 $\\delta$."}, {"title": "PAC-Bayesian approach", "content": "PAC-Bayesian theory was initially developed by McAllester [129] to explain Bayesian\nlearning from the perspective of learning theory. Following the exposition in [116], we assign a prior probability\nP(h), or a probability density if H is continuous, to each hypothesis h \u2208 H. By applying Bayesian reasoning, the\nlearning process establishes a posterior distribution over H, denoted as Q. In a supervised learning scenario where H\nconsists of functions mapping from X to Y, with $Z = X \\times Y$. Then, Q can be interpreted as defining a randomized\nprediction rule. Specifically, for a unseen example x, a hypothesis h \u2208 H is drawn according to Q, and the prediction\nh(x) is then made. The loss associated with Q is defined as:\n$l(Q, z) \\stackrel{def}{=} E_{h \\sim Q} [l(h, z)].$\nThen, both the generalization loss and the empirical training loss of Q can be expressed as follows:\n$R(Q) \\stackrel{def}{=} E_{h \\sim Q \\atop z \\sim D} [l(h, z)]   and   R_n(Q) \\stackrel{def}{=} E_{h \\sim Q} [\\frac{1}{n}\\sum_{i=1}^n l(h, z_i)].$\nIn accordance with [116], we proceed to present the following generalization bound:\nTheorem 2.6. Let P be a prior distribution over hypothesis space H, and D be any distribution over an input-output\npair space Z. For any $\\delta \\in (0, 1)$ and Q over H (including those that depend on the training data $S = \\{z_1,..., z_n \\}$),\nwe have:\n$R(Q) \\leq R_n(Q) + \\sqrt{\\frac{KL(Q||P)+\\ln(\u03b7/\\delta)}{2(n - 1)}}$"}, {"title": "Expressive Power Analysis", "content": "Expressive Power traditionally refers to the ability of a neural network to represent a wide and diverse range of\nfunctions. However, with the advent of FMs, the concept of expressive power has been further expanded. For instance,\nthe use of prompts such as chain of thought can also influence a model's expressive capabilities. This concept is\nessential for evaluating the potential of FMs to capture complex structures and relationships within data.\nUnderstanding the expressive power of FMs is particularly important for interpreting both their capabilities and limi-\ntations. As FMs are increasingly utilized in various applications, analyzing their expressive power helps us determine\nthe range of tasks they can handle. This insight is crucial for identifying scenarios where these models are likely to\nexcel or encounter difficulties, thereby guiding the development of more robust and versatile architectures. Moreover,\na thorough examination of expressive power ensures that a model's performance aligns with expectations, not only in\ntraditional applications but also in novel and unforeseen scenarios.\nMathematically, expressive power can be defined in terms of a model's ability to approximate functions within a certain\nclass. Let H denote the class of functions that a neural network can represent, parameterized by its architecture (e.g.,\ndepth, width, and activation functions). Consider a target function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ within a broader function space F\nsuch as the space of continuous functions C (X) over a compact domain $X \\subset \\mathbb{R}^d$. The hypothesis space H is\nsaid to have sufficient expressive power with respect to F if $\\forall f \\in F$ and $\\forall e > 0$, there exists a function h\u2208 H such that:\n$\\sup_{x \\in X} | f (x) - h (x)| \\leq \\epsilon$\nThis condition signifies that the model H can approximate any function f from the space F with arbitrary precision,\ngiven sufficient capacity. Therefore, expressive power serves as a critical metric for understanding the theoretical\ncapabilities of neural networks, particularly in terms of their potential to adapt, and perform complex tasks across\nvarious domains.\nThe commonly-used theoretical tools for analyzing the expressive power of models are as follows:\n\u2022 Universal Approximation Theorem: The Universal Approximation Theorem states that, with a single hid-\nden layer and finite number of neurons, a feedforward neural network can approximate any continuous func-\ntion on a compact input space to arbitrary accuracy, given a sufficiently large number of neurons. Specifically,\nfor a neural network with a single hidden layer containing m neurons, the output f (x) can be represented as\n$f(x) = \\sum_{i=1}^m w_i^{(2)} \\sigma( w_i^{(1)} x + b^{(1)})+b^{(2)}$,\nwhere $w_i^{(1)}$ is the weights connecting input to the i-th neuron in the hidden layer, $b^{(1)}$ is bias term of the\ni-th neuron in the hidden layer, and $\\sigma$ represents a nonlinear activation function. Thus, given a continuous\nfunction $g: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ and a compact set $K\\subset \\mathbb{R}^n$, for any $\\epsilon > 0$ and $\\delta > 0$, there exists a neural network\nwith a single hidden layer and a nonlinear activation function $\\sigma$ such that\n$|f(x) - g(x)| < \\epsilon$\nfor all x \u2208 K, where f(x) is the output of the neural network.\n\u2022 Expressivity in Higher Complexity Classes: Expressivity in Higher Complexity Classes examines the abil-\nity of neural networks, particularly transformers using Chain-of-Thought prompting, to solve problems within\nhigher computational complexity classes, such as NC1 (log-space parallel computation) and AC\u00ba (constant-\ndepth Boolean circuits). CoT enhances a network's ability to decompose and solve complex tasks, thereby\nincreasing its expressiveness:\n$NC^1 = \\{problems \\ solvable \\ in \\ log-space \\ parallel \\ computation\\}$\nThese theoretical tools play a critical role in deepening our understanding and evaluation of the expressive power of\nneural networks and transformer-based models."}, {"title": "Dynamic Behavior Analysis", "content": "Training dynamic behavior analysis is the study of how machine learning algorithms evolve over time during the\ntraining process, with a focus on understanding how parameter updates influence the convergence behavior, speed,\nand final quality of the solution. This analysis is crucial for optimizing the performance and reliability of machine\nlearning models, particularly in complex scenarios involving large-scale data and deep architectures.\nFollowing the work [114], dynamic behavior analysis can be systematically divided into three key steps. First, it\nensures that the algorithm begins running effectively and converges to a reasonable solution, such as a stationary\npoint, by examining conditions that prevent issues like oscillation or divergence. Next, it focuses on the speed of\nconvergence, aiming to accelerate the process and reduce the computational resources and time required for training.\nTechniques like adaptive learning rates or momentum may be used to enhance efficiency. Finally, the analysis ensures\nthat the algorithm converges to a solution with a low objective value, ideally achieving a global minimum rather than\nsettling for a local one. This step is crucial for securing a high-quality final solution that generalizes well to unseen\ndata. Additionally, several fundamental tools used for the theoretical analysis of training dynamics are as follows:\n\u2022 Neural Tangent Kernel (NTK): The NTK is a theoretical framework that models the training dynamics\nof deep neural networks in the infinite-width limit. NTK approximates the training process by treating the\nnetwork as a linear model with a fixed kernel, defined by\n$\\Theta (x, x') = \\lim_{n \\rightarrow \\infty} \\nabla_{\\theta}f_0(x)\u00b7\\nabla_{\\theta}f_0 (x')$,\nwhere $f_0(x)$ is the network output, $\\theta$ represents the network parameters, and x and x' are input samples. The\nNTK captures the evolution of the network's output during gradient descent and is instrumental in under-\nstanding the expressive capabilities of neural networks.\n\u2022 (Stochastic) Differential Equation: Gradient flow represents the continuous-time analog of gradient de-\nscent, described by the differential equation:\n$\\frac{d\\theta (t)}{dt} = -\\nabla R (\\theta (t))$\nThis mathematical approach allows for a smooth analysis of the training dynamics, modeling how parameters\nevolve continuously over time and providing deeper insights into the convergence behavior of the algorithm."}, {"title": "Interpreting Inference Capabilities of Foundation Models", "content": "In this section, we aim to leverage the interpretable methods discussed in Section 2 to interpret the fundamental causes\ndriving impressive inference capability of FMs, including ICL, CoT, and adaptability to distribution shifts."}, {"title": "In-Context Learning", "content": "As models and their training datasets have grown in complexity, FMs have demonstrated remarkable capabilities, no-\ntably in ICL [130]. ICL allows models to learn from a few demonstrations provided in the input context, enabling\nthem to handle complex tasks without parameter updates, as seen in mathematical reasoning problems [5]. Unlike su-\npervised learning, ICL leverages pretrained FMs for prediction without requiring a dedicated training phase. Formally,\nICL is defined as follows.\nDefinition 3.1. In a formal framework, the process takes a query input x and a set of candidate answers $Y = \\{Y_1,..., Y_n\\}$, where Y may represent class labels or a collection of free-text expressions. A pre-trained model $f_M$\ngenerates the predicted answer by conditioning on a demonstration set $C_n$ alongside the query input x. The demonstra-\ntion set $C_n$ encompasses n demonstration examples $C_n = \\{(X_1,Y_1), ..., (x_n, Y_n)\\}$, where each $(x_k, Y_k), k = 1,\u2026\u2026, n$,\nrepresents an in-context example aligned with the task at hand. The answer \u0177 is modeled by a pre-trained function\n$f_M$, defined as:\n$\\hat{y} = f_M (C_n, x; \\theta)$,\nwhere $\\theta$ is the parameter of the pre-trained model. The function $f_M$ outputs the current answer given the demonstration\nand the query input.\nBased upon the above definition, the generalization of FMs under ICL can be measured by the upper bound on popu-\nlation risk. In the most common practical setting, where predictions are made at every position, the population risk is\ndefined as follows:\n$R(f) = \\frac{1}{n} \\sum_{k=1}^n \\mathbb{E}_{C_k,(x_{k+1},Y_{k+1})}l(f_M(C_k, x_{k+1}; \\theta), Y_{k+1})$"}, {"title": "Interpreting Through Generalization Analysis", "content": "Recently, various theoretical perspectives on the generalization of FMs under ICL have been explored [92-\n94, 105, 108-111, 132]. Since FMs rely on the context provided during inference to perform tasks, these theoretical\nframeworks offer valuable insights into how models generalize to novel tasks without requiring additional training.\nThey highlight the pivotal importance of model architecture, pretraining methodologies, and input context structure\nin driving this generalization. In this section, we review and interpret relevant studies on FMs from the perspective\nof generalization error, focusing on several key approaches. One line of work explores the dynamics of gradient flow\nover the loss function. Another major strand of research applies statistical learning principles, including algorithmic\nstability, covering numbers, and PAC-Bayes theories. These methods offer different avenues for understanding the\nfundamental mechanisms behind in-context learning and model generalization.\nIn this line of research examining gradient flow behavior over the loss function, the primary focus remains on linear\nproblem settings. To rigorously analyze gradient flow dynamics, studies like [110] often simplify the transformer"}, {"title": "Task complexity for pertaining", "content": "Theorem 3.2. (Task complexity for pertaining [110]). Let n \u2265 0 represent the quantity of in-context examples utilized\nto generate the dataset. Let $\\theta_T$ denote the parameters output by SGD on the pretraining dataset with a geometrically\ndecaying step size. Assume that the initialization $\\theta_0$ commutes with H and $\\eta_0 \\leq 1/(c tr(H) tr(H_n))$, where c > 1 is\nan absolute constant and $H_n$ is defined as $\\psi^2 H((tr(H)/n + \\sigma^2/n\\psi^2)I + (n + 1)/nH)$. Then we have\n$E \\bigg[ R_{fixed}(f_M(C_n, x; \\theta_T) \u2013 \\min_{\\theta} R_{fixed}(f_M(C_n, x; \\theta))) \\bigg] \\\\ \\leq  \\frac{1}{T} \\sum_{t=1}^T \\eta^2  \\bigg[ \\|\\big(I - (\\gamma \\gamma^T)H_n\\big) \\frac{H}{n}\\big\\|_F^2 + (2tr(H) + \\sigma^2) \\frac{D_{eff}}{T_{eff}} \\bigg] $,\nwhere the effective number of tasks and effective dimension are given by\n$T_{eff}:=\\frac{T}{log(T)}, \\qquad D_{eff} := \\sum_j \\sum_i min\\{1, \\frac{\\lambda_{i}}{\\lambda_j + \\lambda_i}}\\}^2$,"}, {"title": "Excess risk of Pretraining", "content": "Theorem 3.3. (Excess risk of Pretraining [105]). With probability at least 1 \u2013 $\\delta$ over the pretraining instances, the\nERM solution \u2013 to satisfies\n$R_{fixed}(f_M (C_n, x; \\theta)) \u2013 \\min_{\\theta} R_{fixed} (f_M (C_n, x; \\theta)) \\leq \\sqrt{\\frac{L^2 (MD^2 + DD') \\log B}{T}}$"}, {"title": "Total Variation Distance", "content": "Theorem 3.4. (Total Variation Distance [108]) Let $P_{\\theta}$ represent the probability distribution induced by the trans-\nformer with parameter $\\theta$. Additionally, the model $\\hat{P}_{\\theta}$ is pretrained by the algorithm:\n$\\hat{\\theta} = \\argmin_{\\theta \\in \\Theta}  \\frac{1}{n} \\sum_{t=1}^{n-1} log P_{\\theta}(x_{t+1} | C_t) $.\nFurthermore, they consider the realizable setting, where ground truth probability distribution $P(\\cdot | C_n)$ and $P_{\\theta^*}(\\cdot |$\n$C_n)$ are consistent for some $\\theta^* \\in \\Theta$. Then, with probability at least 1 \u2013 $\\delta$, the following inequality holds:\n$TV (P(\\cdot | C_n), P_{\\hat{\\theta}} (\\cdot | C_n)) \\leq \\frac{1}{n^{1/2}}  \\sqrt{log(1 + n)} + \\frac{1}{n^{1/4}}  \\sqrt{log(1/\\delta)}$,"}, {"title": "Stability of Multilayer Transformer", "content": "Theorem 3.5 (Stability of Multilayer Transformer ([93])). The formal representation of the n-th query and n - 1\nin-context examples in the n-th prompt is $x_{prompt} = (C(n-1),x_n)$, where $C(n-1) = (s(X_1,Y_1), ..., s(X_{n-1}, Y_{n-1}))$.\nLet $C(n-1)$ and $C(n-1)'$ be two prompts that differ only at the inputs $(x_j, Y_j)$ and $(x'_j, Y'_j)$ where j < n. Assume that\ninputs and labels are contained inside the unit Euclidean ball in $\\mathbb{R}^d$. These prompts are shaped into matrices $X_{prompt}$\nand $X'_{prompt} \\in \\mathbb{R}^{(2n-1)\\times d}$, respectively. Consider the following definition of TF($\\cdot$) as a D-layer transformer: Starting\nwith $X^{(0)} := X_{prompt}$, the i-th layer applies MLPs and self-attention, with the softmax function being applied to each\nrow:\n$X^{(i)} = ParallelMLPs (ATTN (X^{(i-1)}))$ where $ATTN(X) := softmax (\\frac{X W_Q W_K^T X^T}{\\sqrt{d}}) X V$\nAssume the transformer TF is normalized such that $\\|V\\| \\leq 1$ and $\\|W\\| \\leq \\Gamma/2$, and the MLPs adhere to MLP(x) =\nReLU(Mx) with $\\|M\\| \\leq 1$. Let TF output the last token of the final layer $X^{(D)}$ corresponding to the query $x_n$.\nThen,\n$| TF (x_{prompt}) - TF (x'_{prompt}) | \\\\ \\leq  \\frac{2 \\Gamma \\sqrt{d}}{2^{D-1}} \\bigg( \\frac{((1+ \\Gamma) e \\Gamma)^D}{\\sqrt{(1+ \\Gamma)^D e}} \\bigg) \\Delta$.\nThus, assuming the loss function $l(y,\\cdot)$ is L-Lipschitz, the algorithm induced by TF($\\cdot$) exhibits error stability with\n$\\beta =  \\frac{2 L ((\\frac{(1+ \\Gamma) e}{2})^{D} - 1)}{\\sqrt{n}}$"}, {"title": "Excess Risk Bound in", "content": "Theorem 3.6 (Excess Risk Bound in [93", "1": ".", "2$\\\\$\\delta": "n$R(A) \u2013 \\min_A R(A) \\leq inf \\bigg\\{ inf"}]}