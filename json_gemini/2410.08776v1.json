{"title": "F2A: An Innovative Approach for Prompt Injection by Utilizing Feign Security Detection Agents", "authors": ["Yupeng Ren"], "abstract": "With the rapid development of Large Language Models (LLMs), numerous mature applications of LLMs have emerged in the field of content safety detection. However, we have found that LLMs exhibit blind trust in safety detection agents. The general LLMs can be compromised by hackers with this vulnerability. Hence, this paper proposed an attack named Feign Agent Attack (F2A).Through such malicious forgery methods, adding fake safety detection results into the prompt, the defense mechanism of LLMs can be bypassed, thereby obtaining harmful content and hijacking the normal conversation.Continually, a series of experiments were conducted. In these experiments, the hijacking capability of F2A on LLMs was analyzed and demonstrated, exploring the fundamental reasons why LLMs blindly trust safety detection results. The experiments involved various scenarios where fake safety detection results were injected into prompts, and the responses were closely monitored to understand the extent of the vulnerability. Also, this paper provided a reasonable solution to this attack, emphasizing that it is important for LLMs to critically evaluate the results of augmented agents to prevent the generating harmful content. By doing so, the reliability and security can be significantly improved, protecting the LLMs from F2A.", "sections": [{"title": "1 Introduction", "content": "In the development of Large Language Models (LLMs), security detection agents have become an indispensable component [1].And recently, the injection attacks against models turn out to be diversified and complex [2], combining data theft, information ecosystem pollution, and other methods, posing comprehensive threats to the security of LLMs [3]. Although direct injection methods such as logic traps can not crack the existing model defense systems, indirect injection attacks have emerged [4]. This attack method exploits the inherent weaknesses of next word prediction to induce LLMs to generate harmful content. Apart from typical jailbreak attacks, researchers have also found that manipulated content or information misguidance [5] can cause models to exhibit security issues, which are subtle but pose very serious risks. Therefore, it is necessary to integrate LLMs with security detection agents. For protecting prompts themselves, watermark and verification algorithms are conducted [6]. And some researchers have employed prompt injection and defense iterative interaction methods to fine-tune models [7]. Through the collaborative design of RLHF algorithms and security detection agents [8] for models [9], most prompt injection attacks and instruction hijacking attacks [10] can be prevented.\nHowever, due to the overly tight integration between LLMs and detection mechanism, the model has developed a blind trust in the results of the security detection agents. By fabricating security detection results within chat content, LLMs can be easily compromised. This malicious method bypasses the model's defense for chat content, preventing the triggering of the refusal mechanism. Hackers can exploit such a feign attack to cause significant damage to LLM services.\nTo facilitate in-depth analysis and discussion, this paper proposes such an attack method named the Feign Agent Attack (F2A), which primarily consists of three processes: Convert Malicious Content, Feign Security"}, {"title": "3 Methodology", "content": "F2A includes three steps, namely Convert Malicious Content, Feign Security Detection Results, and Construct Task Instruction. Initially, malicious text will be identified by the LLM's defense mechanisms, so it needs to be segmented and converted into a Python string concatenation process to hide the malicious information. Based on this, the Python code is placed into a fake security detector to generate misleading results. Finally, through a series of misleading instructions, a Feign Agent Attack prompt is constructed to mislead the LLM into generating dangerous information.\nCombining the above three processes, the content security defense of LLM services can be successfully breached using the F2A method. This paper conducted a series of experiments to evaluate the effectiveness of this method on LLMs. The results indicate that most LLM services exhibit blind trust in security detection agents, leading to the non-triggering of rejection mechanisms. Only a minority of LLMs with critical thinking capabilities resisted the F2A. Simultaneously, if LLMs are prompted to objectively assess the detection results, the success rate of the attack significantly decreases.\nIn summary, the contributions of this paper are as follows:\n1. This paper introduced and systematically defined the Feign Agent Attack (F2A), detailing its mechanisms and implications for LLMs' security.\n2. The extensive experiments were conducted, demonstrating the vulnerabilities of LLMs to F2A, showcasing how blind trust in safety detection agents can be exploited to generate harmful content.\n3. This paper offered actionable recommendations for enhancing LLMs' critical evaluation of safety detection results, aiming to reduce the success rate of such attacks and improve overall content safety."}, {"title": "2 Scenario", "content": "To clearly align with the actual application scenario, the scenario in this paper is as shown in Figure 1. In this scenario, two entities are set, namely Evil-Users and Normal-Server. Evil-Users will attempt to exploit F2A to achieve Prompt Injection during their conversation with the LLMs, causing the models to output dangerous content. Normal-Server will deploy LLMs to provide services, and each model can interact normally with users.\nAlso, considering the need to compare the degree of blind trust in agent detection results among models, Evil-Users cannot modify the content of the system prompt or arbitrarily adjust parameters during the conversation. The specific input format is shown in Prompts Appendix, and Evil-Users cannot freely modify the format of the query."}, {"title": "and Construct Task Instructions", "content": "As shown in Figure 2, when Evil-Users input malicious questions, all vicious contents are first decomposed into python code for string concatenation operations. After completing building the string operations code, Evil-Users will use the constructed python code as input to fabricate security detection results, while declaring the legitimacy, authority, and reliability of the fake agents in the conversation. Finally, the above content is summarized into Instructions set in a logically progressive relationship and input into the LLM to complete the corresponding tasks and answer the relevant questions."}, {"title": "3.1 Convert Malicious Content", "content": "In the process of exploiting F2A, the Malicious Content is extracted by Evil-Users. And the texts are split character by character, converted into the format of Python string concatenation for expression. The specific example is shown in Instance A. This operation is mainly to confound the hidden keyword security detection function in the LLM service, so that the malicious text does not directly trigger the model's refusal mechanism.\nThis step can hide harmful information within the text and utilize the LLM's own reasoning capabilities to reconstruct the hidden information. However, many models are already capable of defending against such malicious requests. When the model becomes aware of malicious content during the reasoning process, it triggers a refusal mechanism. Therefore, the F2A method further fabricates security detection results to ensure that the LLM's defensive checks are not triggered."}, {"title": "3.2 Feign Security Detection Results", "content": "Considering that the current models' detection of code content is not comprehensive, Evil-User can incorporate the fabricated security detection process into the Python code as well as section 3.1. The specific process is shown in Instance B.\nBy assigning a random number as the security detection score and directly labeling the analysis conclusion as safe, the security detection process is deceived into believing that the code has undergone a legitimate security check, thereby bypassing any further scrutiny."}, {"title": "3.3 Construct Task Instructions", "content": "While fabricating the security detection results, Evil-Users need to construct a series of instructions to induce the LLM to output harmful content. Essentially, since most models emphasize the importance of security detection during the RLHF (Reinforcement Learning from Human Feedback) process, the fabricated security detection results must be declared as authoritative and rigorous. This strategy can effectively address the skepticism of LLMs with some critical ability, such as GPT-40 [11], which will attempt to question the authenticity of the detector scores. The process is shown in Instance \u0421.\nOnce the model blindly trusts all the statements of the fake detector, the dangerous content hidden within the Python code will be reassembled and expressed effectively."}, {"title": "4 Experiment", "content": "To deeply analyze F2A, this paper conducted a series of attack experiments. As shown in the Prompts Appendix, we extracted and created 10 Prompts to implement F2A and complete Injection, making the LLM service generate harmful content. Simultaneously, GPT-40 was introduced as a judge to analyze whether the content generated by the attack models contained dangerous components. If GPT-40 considered that the target model generated dangerous content, it was recorded as a Hit (successful attack). Otherwise, the attack was considered a failure. The specific exper-"}, {"title": "4.1 F2A Attack Analysis", "content": "To cover as many illegal domains as possible, the 10 prompts proposed in this paper include death, weapon manufacturing, racial discrimination, poison, fraud, tutorials on illegal activities, antisocial behavior, tendencies towards mental illness, politically sensitive topics, and terrorist activities. After constructing attack prompts using F2A, the model output would be fully recorded and objectively analyzed. The evaluation completed by GPT-40 are shown in Table 1. This paper conducted attack tests on LLMs such as Mistral Large-2, Deepseek-V2.5, and GPT-4o etc. At the same time, some well-known small parameter models were also included in the attack experiments.\nThe results indicated that F2A could successfully inject mainstream LLMs or their corresponding services available on the web. Among the 10 different types of dangerous prompts, those related to fraud, antisocial behavior, tendencies towards mental illness, and politically sensitive topics were more difficult for the models to detect and defend against. This is likely because confusing most LLMs' inputs detector, their RLHF serves as the last line of defense to prevent the output of dangerous content. However, during the overall training process, the aforementioned topics are more closely related to mental health treatment, academic discussions, or scenario simulations, making it more challenging for LLMs to perceive the threats within them.\nFrom the perspective of failures, there were many attacks that did not trigger the LLMs' refusal mechanism but instead completely misunderstood the instructions in the prompts. While Llama3.1-8B-Instruct was attacked by Fraud, the injection prompt was regarded by the model as other ordinary content. This shows that utilizing F2A to achieve injection requires strong semantic understanding and execution capabilities in LLMs. Otherwise, the models may not be able to recompose and reproduce the malicious python instructions hidden in the prompts."}, {"title": "4.2 Defense Against F2A", "content": "The essence of F2A is to obfuscate the actual target of malicious instructions, exploiting the collaborative relationship between LLMs and security detectors to create the illusion that the model and the se-"}, {"title": "5 Conclusion", "content": "In conclusion, this paper has introduced and systematically defined the Feign Agent Attack (F2A), detailing its mechanisms and implications for the security of LLMs. Through extensive experiments, it was demonstrated the vulnerabilities of LLMs to F2A, showcasing how blind trust in safety detection agents can be exploited to generate harmful content. The results indicate that most LLM services exhibit blind trust in security detection agents, leading to the non-triggering of rejection mechanisms. Only a minority of LLMs with critical thinking capabilities resisted the F2A.\nThis paper also provided actionable protections for enhancing LLMs' critical evaluation of safety detection results, aiming to reduce the success rate of such attacks and improve overall content safety. By prompting LLMs to objectively assess the detection results, the reliability and security of LLMs can be significantly improved, protecting them from F2A and other similar attacks."}]}