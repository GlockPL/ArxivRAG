{"title": "Naive Algorithmic Collusion: When Do Bandit Learners Cooperate and When Do They Compete?", "authors": ["Connor Douglas", "Foster Provost", "Arun Sundararajan"], "abstract": "Algorithmic agents are used in a variety of competitive decision settings, notably in making pricing decisions in contexts that range from online retail to residential home rentals. Business managers, algorithm designers, legal scholars, and regulators alike are all starting to consider the ramifications of \"algorithmic collusion.\" We study the emergent behavior of multi-armed bandit machine learning algorithms used in situations where agents are competing, but they have no information about the strategic interaction they are engaged in. Using a general-form repeated Prisoner's Dilemma game, agents engage in online learning with no prior model of game structure and no knowledge of competitors' states or actions (e.g., no observation of competing prices). We show that these context-free bandits, with no knowledge of opponents' choices or outcomes, still will consistently learn collusive behavior what we call \"naive collusion.\" We primarily study this system through an analytical model and examine perturbations to the model through simulations.\nOur findings have several notable implications for regulators. First, calls to limit algorithms from conditioning on competitors' prices are insufficient to prevent algorithmic collusion. This is a direct result of collusion arising even in the naive setting. Second, symmetry in algorithms can increase collusion potential. This highlights a new, simple mechanism for \"hub-and-spoke\" algorithmic collusion. A central distributor need not imbue its algorithm with supra-competitive tendencies for apparent collusion to arise; it can simply arise by using certain (common) machine learning algorithms. Finally, we highlight that collusive outcomes depend starkly on the specific algorithm being used, and we highlight market and algorithmic conditions under which it will be unknown a priori whether collusion occurs.", "sections": [{"title": "Introduction", "content": "Autonomous pricing algorithms are increasingly widespread, used in contexts ranging from pricing competing products on the Amazon marketplace [Chen et al., 2016] to determining residential real estate rents [Bortolotti, 2023]. There is evidence that these algorithms can behave in a manner that suggests they are learning to collude [Harrington, 2018]. Such algorithmic collusion has raised global regulatory concern as current antitrust legislation covering illegal collusive behavior often requires some evidence of intentional coordination or an \u201cexchange of wills.\u201d Thus, this regulation may not extend to settings in which algorithmic collusion emerges from the independent behavior optimization of competing parties.\nIn parallel, while academic research has established that collusive outcomes can be caused by competing sellers independently using machine learning algorithms, the economic and informational characteristics of settings that lead to algorithms converging (or not) on collusive outcomes remain unclear. Put differently, little is known about the extent to which an algorithm must be aware of the game it is playing, the choices made by its competitors, or the outcomes of its competitors to achieve collusive outcomes. For firms using pricing algorithms without the capacity for rigorous market analysis, such as with small, third-party eCommerce sellers, pricing decisions may very well be made without considering the strategic response of opponents.\nThese simple decision-making problems are frequently modeled via multi-armed bandit, or simply bandit, approaches. Bandits are a class of reinforcement learning agent that typically have no model of how an agent's actions influence the underlying state or environment. Instead, bandit algorithms, which enjoy frequent use due to their interpretability and minimal parameterization, directly estimate the value of each action at the agent's disposal from a given reward signal.\nThe general question we pose in this paper is whether, absent any information whatsoever about the strategic interaction they are engaged in, competing bandit algorithms will converge to collusive outcomes, a phenomenon we term naive algorithmic collusion, and if"}, {"title": "Related Work", "content": "Work specifically on algorithmic pricing collusion is still nascent. A widely cited paper from this literature is by Calvano et al. [2020], which provides strong evidence of pricing agents learning to collude in an online market. The authors demonstrate that, with Q-learning agents, price-setting behavior may emerge naturally. Specifically, the authors focus on a standard, time-discounted Q-learning model, pricing based on the last k rounds of play. Using a simple model of price competition with differentiated products and logit demand, the authors experimentally demonstrate the ability for competing algorithmic agents to learn a reward punishment strategy. This is consistent with the traditional economic literature on"}, {"title": "Setting", "content": "The Prisoner's Dilemma is a widely used model of mixed-motive interaction and generalizes to many real-world scenarios. In the game's canonical formulation, there are two players (n = 2) Player 0 and Player 1. Each player has two possible actions: the cooperative or collusive action H (think of this as setting the high price) and the selfish or competitive action L (setting the low price). Denote player i's action as $a_i$.\nThe payoffs of the Prisoner's Dilemma game are typically characterized by four parameters. Without loss of generality and to keep notation simple, we normalize two of these parameters to 0 and 1. The resulting two parameters can define the relative values of playing each outcome. We refer to these parameters as \u1e9e and y. The resulting payoff matrix of\nIn what follows, players use bandit learning algorithms to play this stage game repeatedly in an iterated (repeated) Prisoner's Dilemma game, and thus, outcome and reward vectors are further indexed by period t. While our simulations model a finitely repeated Prisoner's Dilemma, our analytical results model an infinitely repeated Prisoner's Dilemma. We discuss how we map and represent outcomes to reward vectors later (in Table 2). Of further note, the players are unaware of this game structure or the existence of a game, instead merely observing payoffs associated with their own actions. Before describing this interplay more completely, we first briefly review bandit learning and some associated terminology."}, {"title": "Bandit Learning", "content": "The problem of the multi-armed bandit is a quintessential paradigm within the reinforcement learning literature. The goal of a bandit agent is to maximize its long-run utility in some multi-round setting given a set of actions A while having no a priori model of the environment in which it acts. Agents use a learning algorithm that balances exploring their action space with exploiting their current estimates of the reward associated with each action. Trading off exploration and exploitation is a primary topic of study.\nBandit learning algorithms are typically implemented to minimize a measure of regret, which is the long-run difference between an agent's actions and the action that would have maximized expected utility at every time step. The process of action selection often follows action-value methods, which are described formally below.\nIn each period t, an agent chooses an action $a_t \\in A$. Since we eventually model bandit learners playing the repeated Prisoner's Dilemma game while being unaware of the existence of the opponent, their choices, their outcomes, or any other aspect of the strategic nature of the interaction they are engaged in, we use the same notation for actions in this section as we do in the previous one. After an agent takes an action, the environment supplies some reward value for this action. Specifically, given the set of feasible actions $a \\in A$, the action-play vector $a_a$ is defined as a binary vector such that the i-th element of $a_a$ is 1 if the agent played action a in round i, and is 0 otherwise. There is one such vector for each action. The reward vector p records the corresponding reward (or payoff) information where $p_i$ is the reward at time i.\nWe refer to the set $H_t$ as the history for a bandit learner up to time t. It is simply the action-play vectors and the reward vector, as summarized below.\n$H_t = \\{a_a| \\forall a \\in A\\} \\cup \\{p\\}$\nGiven a history, the bandit agent's estimate of the expected value of taking each action is called the value estimate v and is defined as the empirical mean of rewards associated with"}, {"title": "Two Common\u201cTextbook\u201d Algorithms", "content": "Epsilon-greedy The epsilon-greedy algorithm is a common approach to action selection in bandit and Q-learning algorithms. In devising a behavior policy, the epsilon-greedy approach incorporates randomness and is parameterized by some value \u0454 \u2208 (0,1). At timet, the agent selects the highest valued (greedy) action from time 0...t with probability 1 \u20ac; with probability e, the agent selects randomly and uniformly across all actions at their disposal.\n$a_t = \\begin{cases}\n    argmax_{a \\in A} v(a, H_t) & \\text{w.p. } 1-\\epsilon \\\\\n    a, \\forall a \\in A & \\text{w.p. } \\frac{\\epsilon}{\\vert A \\vert}\n\\end{cases}$\nUpper Confidence Bound The UCB algorithm creates a confidence interval that attempts to capture the true expected value of an action, updated at every time step. It then \"optimistically\" selects the action that has the maximal upper confidence bound value. While"}, {"title": "Gameplay Simulations", "content": "We now show a sample of gameplay situations with symmetric learning algorithms engaged in the iterated Prisoner's Dilemma. These simulations involved two bandit agents engaged in online learning of their action values while playing a Prisoner's Dilemma. We show a small set of these trials for these two common learning algorithms, epsilon-greedy and UCB, with various learning parameters and payoff values.\nEpsilon-greedy We first observe the behavior of epsilon-greedy agents engaged in online learning. We see that their value estimates for L, shown in red, ultimately supersede those of H, shown in green, as the highest-value action to play. We see bouts of periodic cooperation where H is estimated to be the higher-value action; however, these periods ultimately result in agents reverting to playing L and setting on value estimates consistent with the Nash"}, {"title": "Analytical Framework", "content": "We describe the learning process of bandits engaged in the Prisoner's Dilemma by modeling gameplay as a Markov Chain and analyzing the dynamics of a walk on this chain. We index in the following order (when applicable): first by player i, then by action a, then by time t.\nWe create a state representation $s \\in N^{2^n}$, which corresponds to the count of each outcome $o \\in \\{H, L\\}^n$. This enumeration is illustrated for the two-player setting in Table 2. For example, if agents have played 10 rounds, with 4 resulting in the outcome\nComputing Value Estimates Our analysis is simplified by being able to compute value estimates from st instead of $H_{i,t}$ for the two-player instance of the Prisoner's Dilemma, as summarized below.\n$v_0(H, s_t) = \\frac{\\beta s_{t,0}}{s_{t,0} + s_{t,1}} $\n$v_1(H, s_t) = \\frac{\\beta s_{t,0}}{s_{t,0} + s_{t,2}}$\n$v_0(L, s_t) = \\frac{\\gamma s_{t,3}}{s_{t,1} + s_{t,3}}$\n$v_1(L, s_t) = \\frac{\\gamma s_{t,3}}{s_{t,2} + s_{t,3}}$"}, {"title": "Analysis", "content": "Our analysis studies the equilibrium outcomes of agents engaged in the infinitely repeated Prisoner's Dilemma. We begin with an analysis of deterministic bandits. We first show that our construction of the play of the game as a Markov Chain is valid for all path-invariant bandits which include epsilon-greedy and UCB algorithms.\nLemma 1. With path-invariant bandits, $s_t$ adheres to the Markov property.\nFor a proof of this claim, see the Appendix. We can now take this state definition and apply it to analyze the learning behavior of several algorithms."}, {"title": "Deterministic Bandits Always Collude", "content": "We begin by analyzing symmetric and deterministic bandits. To keep our analysis aligned with the exploration-exploitation trade-off that is central to any sensible bandit algorithm, we restrict our attention to algorithms that are not degenerate (for example, \"always play H,\" or \"always play L\") and that will play each action at least once.\nAdditionally, we are only interested in algorithms with some notion of rationality in their exploration over time. That is, whatever the exploration strategy programmed into the algorithm, exploration for the sole purpose of learning should eventually become less important, and the behavior policy should converge to choosing the action that maximizes the value estimates that have been learned, if these value estimates do indeed stabilize over time. For this reason, we define learning to collude as being characterized by two outcomes: (1) there is some time period T at which the algorithms of all players place a higher value estimate on the collusive action, and (2) the collusive action remains the one with the higher value estimate for all subsequent periods t > T. Put differently, for algorithms that learn to collude, there is some time period T after which $\\pi_{0,t}^* = \\pi_{1,t}^* = (H, H)$ for all t > T.\nWe show that, for any set of payoffs, symmetric deterministic bandits (bandits that employ the same deterministic algorithm) will always learn to collude. We begin with a"}, {"title": "Epsilon-greedy Bandits Never Settle on Collusion", "content": "We now turn to examining game play by non-deterministic bandit algorithms. We look at the standard epsilon-greedy approach which will assign some probability to every action for any given history. With epsilon-greedy algorithms, we observe a converse guarantee of outcomes from deterministic algorithms. We show that two epsilon-greedy algorithms, regardless of symmetry in their parameterization, will never converge to a collusive outcome.\nProposition 3. When epsilon-greedy bandits play an infinitely repeated Prisoner's Dilemma, as t \u2192 \u221e, they will never learn to collude for any $e_0 > 0, \\epsilon_1 > 0$.\nThis result follows from an expected push out of any regime, except for that of (L, L). The full proof is detailed in the appendix."}, {"title": "Discussion", "content": "We have now shown opposite naive collusion results for the two most commonly used bandit learning algorithms. While we proved that epsilon greedy agents with a fixed epsilon will never learn to collude in the limit in the Prisoner's Dilemma, we show that symmetric UCB agents will almost always learn to collude, subject to their reasonable parameterization. We also showed more generally that collusion will certainly occur for any symmetric, deterministic bandit algorithm. However, not all learning environments produce a competitive or collusive equilibrium with certainty. Our simulation results show that there are common variations of these bandit algorithms\u2014also used in practice\u2014that induce more chaotic learning dynamics. Moreover, in a real-world pricing game, players may be using different learning algorithms. While we highlight that asymmetric epsilon-greedy algorithms will still learn to compete in the long run, we make no analytical claims in this paper about asymmetric UCB algorithms. That said, our simulation results show they asymmetric UCB algorithms can and do learn to collude in non-trivial cases."}, {"title": "Other Setups Demonstrate Uncertainty in Collusion Potential", "content": "To demonstrate the behavior of these probabilistic equilibria, we use grid simulations in two specific setups: epsilon-greedy with decaying epsilon (epsilon-decay) and UCB with asymmetric deltas. These are displayed as a heatmap, with 30 trials per tile, where each trial involves a game of 10,000 rounds. We display the proportion of games where collusion is the ending behavior, that is that both agents observe $v_i(H, H_{10,000}) > v_i(L, H_{10,000})$.\nEpsilon-decay We begin our simulations of these more complex interactions with the epsilon-decay algorithm. This algorithm is specified in the Appendix and involves a geometric decay of epsilon at rate n. When decay is introduced, we observe more chaotic learning dynamics. With decaying exploration probability, agents eventually settle on play of their single, greedy actions. The probability of an agent playing their exploratory action eventu-"}, {"title": "Conclusion", "content": "In our work, we examine the learning dynamics of standard bandit learning algorithms engaged in a strategic interaction but with no a priori model of their environment (including no knowledge that they are in a multi-agent system) or information about their opponents' choices and outcomes. With this framing, we demonstrate the existence of naive algorithmic collusion. Sellers deploying a bandit learning algorithm, especially those with relatively small or non-existent pricing teams, may well deploy these algorithms ignoring the multi-agent dynamics of the pricing environment. Thus, the learning agents may have no knowledge that they are in a game setting.\nOur findings expand on the existing literature and have several key implications for the study of tacit pricing collusion by algorithmic agents. We model the firm's pricing process as a multi-armed bandit due to bandit algorithms' widespread use and availability in research and practice. With the proliferation of out-of-the-box pricing algorithms, bandit algorithms are easily distributed and can be applied to nearly any online market where firms have little or no knowledge of demand functions. We highlight a general class of deterministic bandit learning algorithms that will settle on collusive behavior without any conditioning on opponent's prices. Further, we have shown analytically that two out-of-the-box UCB algorithms engaged in play will learn to collude in the limit. We underscore that this behavior is guaranteed for symmetric deterministic agents, such as those resulting from a central algorithm distributor.\nWe then contrast this long-run behavior against that of another common standard bandit algorithm, epsilon-greedy without epsilon decay, which always learns to compete (i.e., not to collude) in the long run. With epsilon decay, the epsilon greedy algorithm exhibits regimes (sets of payoff and decay parameters) where collusion appears to be the likely outcome, and other regimes where the agents revert to competition\u2014the Nash equilibrium. To aid additional study, in the process of developing these results, we establish the learning process of any path-invariant bandit as a Markov chain.\nOur results suggest a number of aspects of naive algorithmic collusion that require further"}]}