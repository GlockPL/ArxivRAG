{"title": "Optimizing Instruction Synthesis: Effective Exploration of Evolutionary Space with Tree Search", "authors": ["Chenglin Li", "Qianglong Chen", "Zhi Li", "Feng Tao", "Yicheng Li", "Hao Chen", "Fei Yu", "Yin Zhang"], "abstract": "Instruction tuning is a crucial technique for aligning language models with humans' actual goals in the real world. Extensive research has highlighted the quality of instruction data is essential for the success of this alignment. However, creating high-quality data manually is labor-intensive and time-consuming, which leads researchers to explore using LLMs to synthesize data. Recent studies have focused on using a stronger LLM to iteratively enhance existing instruction data, showing promising results. Nevertheless, previous work often lacks control over the evolution direction, resulting in high uncertainty in the data synthesis process and low-quality instructions. In this paper, we introduce a general and scalable framework, IDEA-MCTS (Instruction Data Enhancement using Monte Carlo Tree Search), a scalable framework for efficiently synthesizing instructions. With tree search and evaluation models, it can efficiently guide each instruction to evolve into a high-quality form, aiding in instruction fine-tuning. Experimental results show that IDEA-MCTS significantly enhances the seed instruction data, raising the average evaluation scores of quality, diversity, and complexity from 2.19 to 3.81. Furthermore, in open-domain benchmarks, experimental results show that IDEA-MCTS improves the accuracy of real-world instruction-following skills in LLMS by an average of 5% in low-resource settings.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have exhibited remarkable capabilities across a wide range of tasks in the field of natural language processing (NLP) (Brown et al., 2020; Kojima et al., 2022; Wei et al., 2022; Ouyang et al., 2022; Touvron et al., 2023; Jiang et al., 2023; OpenAI, 2023). Notably, LLMs can be trained to enhance their instruction-following skills through various methods, including fine-tuning on human-annotated data (Ouyang et al., 2022; Zhou et al., 2023b; Touvron et al., 2023) or extracted knowledge from stronger LLMs (Wang et al., 2022; Xu et al., 2023a; Zhao et al., 2023; Xu et al., 2023a,b; Wang et al., 2024). Zhou et al. (2023b) have demonstrated that this alignment can be achieved with low-resource 1k data. However, acquiring such data through human annotation remains high-cost, thus limiting further progress.\nRecent work explores synthesizing instruction data with LLMs by prompting them with example data or prompts and iteratively enhancing the instruction data, offering an efficient and cost-effective alternative to human annotation (Xu et al., 2023a; Luo et al., 2023b,a; Liu et al., 2023). They introduced evolution prompts for LLMs, such as \u201cAdd constraints\u201d, \u201cIncrease reasoning\u201d and \u201cComplete input.\u201d, enabling LLMs to iteratively improve seed instructions. However, the process suffers high uncertainty due to the limited evolution prompts, random selection methods, and lack of control over the evolution direction. Specifically, failures occur when LLMs select inappropriate evolution prompts or fail to halt the instruction synthesis process appropriately. As shown in Figure 1, randomly selecting the prompts can turn a seed instruction like \"1+1=\" into a perplexing instruction. Language models will struggle to learn new skills from these low-value instructions, as humans also find them difficult to understand. Conversely, a few high-value instructions can significantly enhance the model's skills, enabling it to solve real-world problems.\nIntuitively, simple seed instructions can evolve into a wide variety of forms during the evolutionary process. To efficiently optimize and control this evolution, we introduce a novel framework, IDEA-MCTS, which expands the evolution prompts as the action space and incorporates a tree search algorithm to iteratively enhance seed instruction data. In MCTS, each seed instruction acts as the root node. High-value nodes are identified through selection and use evolution prompts for further expansion, followed by simulation and backtracking, to find an optimal evolution action space to enhance the instructions. In this process, we employ customizable evaluation models to assess the quality, diversity, and complexity of the nodes, effectively controlling the direction of instruction evolution. This framework enhances instruction data and provides a clearer understanding of the evolution process, as shown in the case analysis in Appendix D. Our experimental results show that IDEA-MCTS significantly enhances the seed instruction data and models fine-tuned on this enhanced data exhibit substantial improvements compared to previous methods. We believe this work provides clear guidance for instruction synthesis, aiding models in achieving data-efficient alignment and enhancing overall performance. The contributions of our work are as follows:\n\u2022 To synthesize high-value instructions for enhancing language model skills, we propose IDEA-MCTS, a scalable framework that controls the direction of instruction evolution by expanding the evolution space and integrating evaluation models in tree search.\n\u2022 To enhance the efficiency and accuracy of instruction evolution, we expand the existing limited evolutionary space in two ways: evolving general effective instructions from themselves, and evolving task-specific instructions by designing meta-prompts."}, {"title": "2 Related Work", "content": "Data Synthesis for Instruction Tuning Instruction tuning (IT) is a crucial technique for enhancing the performance and alignment of LLMs (Taori et al., 2023; Chiang et al., 2023; Wang et al., 2023). Recent efforts have extended into open-domain IT, characterized by a wide range of formats and task types, driven by crowdsourced human-generated instruction-response pairs (K\u00f6pf et al.; Conover et al., 2023; Zhang et al., 2023a; Peng et al., 2023; Zhou et al., 2023b). However, the high cost of human annotation poses significant challenges (Zhang et al., 2023a). One promising solution for this limitation is the synthesis of instruction data with the help of stronger LLMs (Bai et al., 2022; OpenAI, 2023; Anil et al., 2023; Team, 2023). Yet, using LLM-generated data increases the risk of low-quality examples, highlighting the need for more focus on dataset refinement and enhancement. Some works (Chen et al., 2023; Lu et al., 2023; Liu et al., 2023) address this by prompting stronger LLMs to filter instruction data based on its quality, diversity, and complexity, serving as a form of refinement. However, this approach lacks the synthesis of new instruction, limiting the model's instruction-following capabilities, especially in low-resource scenarios where only a small amount of data is available. Other works (Zhao et al., 2024; Xu et al., 2023a) enhance existing seed instructions by using LLMs with carefully designed prompt templates. Zhao et al. (2024) enhanced the original instructions using tree-structured prompts but focused only on the complexity and heavily relies on LLMs' intrinsic knowledge. Additionally, some work (Xu et al., 2023a; Luo et al., 2023b,a; Liu et al., 2023) design a series of evolution prompts to iteratively guide LLMs in enhancing the seed instructions. However, random selection during instruction evolution introduces high uncertainty and affects the quality of generated instructions. To effectively enhance the seed instruction data, we propose IDEA-MCTS, which expands the evolution action space, introduces evaluation models and iteratively improves instruction data with MCTS.\nTree Search for LLM Enhancement Tree search methods such as BFS, A* search (Hart et al., 1968), and MCTS (Kocsis and Szepesv\u00e1ri, 2006; Coulom, 2006; Ye et al., 2021; Silver et al., 2016), are widely used to find an optimal state in a tree structure. Integrating tree-search methods with LLMs presents a novel approach to find an effective sequence of actions that leads to a favorable outcome. Effective search strategy is crucial for reasoning and planning (Hao et al., 2023; Zhou et al., 2023a; Hu et al., 2023). Depth/breadth-first search in (Yao et al., 2023), A* search in (Zhuang et al., 2023) and MCTS in (Zhang et al., 2023b; Yu et al., 2023; Hao et al., 2023; Zhou et al., 2023a; Chen et al., 2024b). Feng et al. (2023); Tian et al. (2024); Chen et al. (2024a) have utilized tree search for LLM self-improvement. Unlike previous approaches, we leverage the powerful generative capabilities of LLMs and MCTS for instruction synthesis."}, {"title": "3 Approach", "content": "In this section, we introduce the novel framework IDEA-MCTS, which enhances the quality, diversity, and complexity of seed instructions with a stronger LLM, using MCTS. We first define the problem, including the state, action space, and reward function. Then, we discuss the expansion of evolution prompts from two key aspects and the use of MCTS with LLM to efficiently explore the action spaces. Finally, we fine-tune models based on the instruction data generated by the LLM, proving the effectiveness of the framework in low-resource settings."}, {"title": "3.1 Problem Setting", "content": "We begin with a seed instruction sample x as the root node and employ a stronger language model $p_{\\theta}$. Our goal is to improve the quality, diversity, and complexity of x. To achieve this, we use evolution prompts, such as 'add constraints', as our action space. During the tree search, intermediate instructions generated by the LLM, denoted as $z_t$, serve as new nodes.\n$z_{t+1} = P_{\\theta}(z_t, a)$\nBy applying an action a, which is an evolution prompt to wrap the state $z_t$, we obtain the next instruction $z_{t+1}$ via $p_{\\theta}$. We assess each intermediate instruction z\u0142 based on its quality, diversity, and complexity. The value v(zt) of an instruction is determined using the following equation:\n$v(z_t) = P_{\\theta q}(z_t) + P_{\\theta d}(z_t) + P_{\\theta c}(z_t)$\nIn this equation, $p_{\\theta q}(z_t)$, $p_{\\theta d}(z_t)$, and $p_{\\theta c}(z_t)$ represent the quality, diversity, and complexity scores of the instruction z\u0142, respectively. Notably, instruction diversity is measured by the number of distinct intents. Further details about these value scores will be discussed in the following sections. By integrating these elements, we aim to create a framework that robustly enhances seed instructions."}, {"title": "3.2 Instruction Evolution with MCTS", "content": "In our framework, we leverage a stronger language model $p_{\\theta}$ and value function v to evolve the seed instruction x using MCTS, as shown in Figure 2. Intuitively, more precise and diverse evolution prompts contribute to enhancing the quality of seed instructions. To achieve this, we first expand the evolution prompts from two ways, general effective and task-specific instructions. We explore the open-space evolution prompts, that contribute a general effective instructions such as goals, key constraints, and requirements (Xu et al., 2023a). On the other hand, we aim to ensure that the seed instructions can effectively transfer to task-specific contexts. With LLMs, we design the meta prompts, as shown in Figure 4, to extract task-related evolution prompts that contain the words \"such as.\" As shown in Table 1, the designed evolution prompts can enhance both the depth and breadth of the seed instruction.\nThen we construct a decision tree. MCTS proceeds for k episodes, starting from the root (initial state) and progressively expanding this tree through two primary steps: Selection and Expansion. During Selection, the child with the highest Upper Confidence bounds applied to Trees (UCT) value (Kocsis and Szepesv\u00e1ri, 2006; Coulom, 2006) is chosen for the next iteration. The UCT of a child state z is computed as follows:\n$UCT(z) = V(z) + C \\sqrt{\\frac{ln(N(p))}{N(z)}}$\nwhere N(z) represents the number of visits to node z, and V(z) is the value function (expected return). During Expansion, multiple child states z are explored from the current state p by sampling n actions. The child node with the highest UCT value is selected for expansion in the subsequent iteration. In Evaluation, we assess the quality, complexity, and diversity of the instruction data using the value function v, which serves as the node's reward. In Simulation, selection and expansion are performed repeatedly until a termination state is reached, constructing the rollout policy. The termination state occurs when the tree's depth or node value meets a specified threshold. Backpropagation is performed at the end of an episode: the return v is used to update every V(z) along the path using the formula:\n$V(z) = V_{old}(z) \\frac{1}{N(z)} + \\frac{v}{N(z)}$\nwhere Vold(z) denotes the old value function. MCTS relies on an environment model to reverse steps and build a search tree, imposing strict assumptions. This constraint does not apply to LLMs. Our method allows resetting to any step by copying historical text input, overcoming the limitation. By integrating MCTS with LLMs, we demonstrate how heuristic search algorithms can efficiently evolve instructions by leveraging the powerful generative capabilities of LLMs.\nFinally, after evolving the seed instructions, we obtain responses from the stronger LLM and fine-tune the open-source model. To ensure clarity and logic, we avoided complex templates from previous works (Wei et al., 2021; Longpre et al., 2023). Instead, our method follows a straightforward instruction template (Taori et al., 2023)."}, {"title": "4 Experiments", "content": "4.1 Experiments Setting\nBaselines We compare our method with manually annotated data and other techniques for enhancing instructional data using a more powerful LLM. Additionally, we present the baselines utilized in our experiments:\n\u2022 Seed: Serves as the baseline without any enhancement methods.\n\u2022 LIMA (Jha et al., 2023): Contains 1,000 high-quality, human-annotated instructional data points, demonstrating significant improvements for LLMs."}, {"title": "4.2 Statistical Analysis of the Data Evolved from MCTS", "content": "We conduct a comprehensive analysis of the evolved instruction data from three critical dimensions: quality, complexity, and diversity with the EVOL_QUALITY, EVOL_COMPLEXITY, and InsTagger. As shown in Table 2, the Seed contains 1,000 instructions selected from the Alpaca-52K. The Evol-Instruct contains 1,000 instructions obtained through random evolution, while the MCTS contains 1,000 instructions obtained through the MCTS evolution. MCTS+ method can achieve the highest scores across all evaluation metrics (Liu et al., 2023; Lu et al., 2023), demonstrating significant improvement in quality, diversity, and complexity. It outperforms the Seed, with average scores increasing from 2.19 to 3.81. The expansion of the instruction evolution space proves to be a highly effective strategy for enhancing the quality of instruction data."}, {"title": "4.3 Main results", "content": "The main results presented below are based on LLM evaluations and further human evaluations are provided in Appendix C. Table 3 demonstrates that models fine-tuned with data evolved from MCTS+ exhibit better performance compared to other fine-tuning methods. In particular, LLaMA2 and LLaMA3 can show significant gains with MCTS+, with improvements of 6.02% and 4.1%, respectively, over the Seed method. Furthermore, Phi-3 and Mistral fine-tuned with MCTS+ method outperform previous methods across various skills, including help_base, koala, self_instruct, oasst, and vicuna. Notably, the Mistral model achieves a win rate of 61.24% in help_base, surpassing the previous highest win rate by 3.88 obtained using the Evol-Instruct method. Overall, Mistral exhibits a 5.28% enhancement in performance compared to the Evol-Instruct method. These results show that MCTS effectively enhances models' instruction-following capabilities better than traditional methods. Additionally, fine-tuning with the LIMA method does not significantly improve the model's performance on Alpaca-eval, suggesting potential generalization limitations of manually annotated models."}, {"title": "4.4 Generalization", "content": "During the expanded evolution process, with a focus on task-specific instruction data features on Alpaca-Eval, we also evaluate the model's performance on the open-domain benchmark MT-Bench and assess its capabilities on the NLP benchmark OpenLLM. Additionally, we consider the effectiveness of using Dolly as a seed dataset.\nAs shown in Table 4, the MCTS+ method enhances both the model's single-turn and multi-turn dialogue capabilities. The single-turn score is improved from 6.25 (Seed) to 6.74 (MCTS+), while the multi-turn score is increased from 4.54 (Seed) to 7.15 (MCTS+). This results in an overall average score improvement from 5.90 (Seed) to 6.94 (MCTS+), highlighting the method's effectiveness in handling more complex, multi-turn dialogues. Using Dolly as the seed instruction dataset, Table 5 shows that the MCTS+ method can achieve the best performance, with a 3% improvement compared to the Evol-Instruct method. Specifically, the overall score is improved from 48.07% (Evol-Instruct) to 51.18% (MCTS+). In individual metrics, MCTS+ can improve the help_base from 44.19 to 49.61, koala from 49.36% to 51.92%, self_instruct from 60.11% to 63.30%, oasst from 42.46% to 44.84%, and vicuna from 41.25% to 42.50%.\nAs shown in Table 6, despite being fine-tuned on very different instruction-following prompts, the model's capabilities in NLP tasks show a slight improvement, with a 1.5% increase compared to the seed method."}, {"title": "4.5 Ablation Experiment", "content": "Our method can be proved effective in two key areas: expanding the action space and using MCTS evolution. As shown in Table 7, models with expanded action space (denoted as + methods) consistently outperform those without it, regardless of using random or MCTS evolution. For example, the Mistral using the MCTS+ method shows a 3.72% improvement over the Evol-Instruct method. Additionally, data evolved through MCTS maintains high quality, further improving the instruction-following abilities of the model. The Phi-3 model, using MCTS evolution, improves performance by 1.5% before action space expansion and by 2.92% after expansion."}, {"title": "5 Conclusion", "content": "In this paper, we introduce a novel framework that leverages the power of MCTS combined with heuristic evaluation to synthesis high-value instruc tion data. Our statistical analysis validates the framework's effectiveness in synthesizing high-value data. By fine-tuning open-source models with these evolved instructions, models achieve competitive competitive performance compared to previous methods."}, {"title": "Limitations", "content": "We need to acknowledge that the process of using LLMs for evolving instructions with MCTS is opaque and incurs API costs. Knowledge distillation might balance the trade-off between expenses and synthesizing high-quality instructions. On the other hand, we have demonstrated the effectiveness of MCTS-evolved instructions under low-resource conditions. Further exploration of scaling laws could enhance our understanding of the framework."}]}