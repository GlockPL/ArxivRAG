{"title": "Token2Wave", "authors": ["Xin Zhang", "Victor S. Sheng"], "abstract": "This paper provides an in-depth analysis of Token2Wave, a novel token representation method derived from\nthe Wave Network, designed to capture both global and local semantics of input text through wave-inspired\ncomplex vectors. In Token2Wave, each token is represented with a magnitude component, capturing the\nglobal semantics of the entire input text, and a phase component, encoding the relationships between indi-\nvidual tokens and the global semantics. Building on prior research that demonstrated the effectiveness of\nwave-like operations, such as interference and modulation, during forward propagation, this study investi-\ngates the convergence behavior, backpropagation characteristics, and embedding independence within the\nToken2Wave framework. A detailed computational complexity analysis shows that Token2Wave can signif-\nicantly reduce video memory usage and training time compared to BERT. Gradient comparisons for the\n[CLS] token, total input text, and classifier parameters further highlight Token2Wave's unique characteris-\ntics. This research offers new insights into wave-based token representations, demonstrating their potential\nto enable efficient and computationally friendly language model architectures.", "sections": [{"title": "Introduction", "content": "Currently, there are two types of token embedding methods. The fixed token embedding, such as Skip-gram\nand Continuous Bag of Words (CBOW) [1], assign the same embedding vector to each token, which cannot\nadapt to the dynamic meanings of tokens in varying contexts. The context-dependent embedding, on the other\nhand, generates different embeddings for the same token depending on its contexts. Many current Natural\nLanguage Processing (NLP) methods, such as the Transformer [2], use the attention mechanism to update\ntoken embeddings by measuring relationships between tokens with dot products. However, attention only infers\nglobal semantics indirectly through pairwise relationships rather than directly capturing the overall meaning\nof the text.\nIn our previous work [3], we introduced the Wave Network, a language model based on a new token\nrepresentation method called Token2Wave. Token2Wave uses complex vector token representations to\nrepresent both the global and local semantics of each token with two parts: a magnitude vector representing the\nglobal semantics of the input text, and a phase vector capturing the relationships between individual tokens\nand global semantics. The complex vector token representations enables wave-like operations, such as\ninterference and modulation for efficient updates.\nWhile the previous work focused on constructing token representations as waves and their forward prop-\nagation in text classification tasks, the current study delves into the architectural and functional aspects of\nthe Wave Network. Here, we present a thorough analysis of the convergence performance, gradient behaviors\nof the network components (e.g., [CLS] embedding, overall input embedding, classifier), and the independence\nlevel among embedding dimensions. By focusing on these aspects, we aim to provide deeper insights into the\ntheoretical details of the Wave Network and its potential effectiveness in various NLP tasks."}, {"title": "Representing Tokens as Waves", "content": "In this framework, we represent each token using a complex vector in the form of $Ge^{i\u03b1}$, which comprises\ntwo components: a magnitude vector G that represent the global semantics of the text, and a phase vector \u03b1"}, {"title": "1) Global Semantics Representation", "content": "The meaning of each token within a text frequently relies on the overall meaning of the entire context. Rep-\nresenting these global semantics can help disambiguate individual tokens, making this understanding critical\nfor downstream tasks that depend on a global view of the text.\nBased on principles from signal processing, where signals are often represented in polar coordinates, we\ntreat each token as a discrete signal in the frequency domain. Here, magnitude represents the signal's intensity,\nand phase specifies its relative position within a cycle [4]. As shown in the part (I) of Figure 1, given an input\ntext with n tokens input_text = [$W_1$,$W_2$,..., $W_j$,..., $W_n$]:"}, {"title": "2) Local Semantics Representation", "content": "Local semantics typically capture the specific meaning of each individual token, helping in the analysis of\ndependencies and fine-grained distinctions between tokens within a text. For instance, tasks such as sentiment\nanalysis, entity recognition, and keyword extraction often rely on an accurate understanding of each token's\nunique meaning.\nFrom a signal processing perspective, phase describes the relative relationships between signals. We will\nuse the phase of complex vector token representations to represent the relative relationships between\nindividual tokens and the global semantic vector. That is, the phase representation of a token is cou-\npled with its global semantic vector. For each token $w_j$ in the input text, its phase vector is \u03b1j\n= [$\\alpha_1$, $\\alpha_2$, ..., $\\alpha_k$,..., $\\alpha_{768}$] = [input_$\\alpha_1$, input_$\\alpha_2$,..., input_$\\alpha_k$,..., input_$\\alpha_{768}$], where input_$\\alpha_k$ is defined as\narctan2($\\frac{1-(W_{j,k}/input-G_k)^2}{W_{j,k}/input-G_k}$)\n based on the corresponding element input_Gk in the global semantic vector of the\ninput text input_G = [input_$G_1$, input_$G_2$, ..., input_$G_k$,..., input_$G_{768}$]. Note that we use the function arc-\ntan2 to ensure angles fall within the range of -\u03c0 to \u03c0, consistent with the standard phase angle in physics.\nUsing these definitions, we can derive the token phase matrix from the token embedding matrix, transform\nfrom part (I) to part (IV) in Figure 1.\nTo represent the complex vector token representations in Cartesian coordinates, we use the Euler's\nformula $e^{i\u03b8}$ = cos(\u03b8)+i\u00b7sin(\u03b8) [6] to convert complex vector token representations from polar to Cartesian\ncoordinates, as shown in part (VI) of Figure 1. For example, the complex vector token representations\ninput_G can be expressed in Cartesian coordinates as input_G\u00b7cos(input_$\\alpha_j$)+i \u00b7 input_G\u00b7 sin(input_$\\alpha_j$).\nThe inner product of sin(input_$\\alpha_j$) and cos(input_$\\alpha_j$) is zero over a full period, making them orthogonal\n[7]. Consequently, the real part input_G\u00b7 cos(input_$\\alpha_j$) and the imaginary part i\u00b7input_G\u00b7 sin(input_$\\alpha_j$)\nare also orthogonal, fulfilling the properties of wave representations as described in physics [8]. As Figure\n1 illustrates, the real part of the token embedding $w_j$ represents the token's contribution along the k-th\ndimension, capturing the local semantics of the input text. The imaginary part describes the global semantic\nelement apart from $w_j$, representing the context of token $w_j$ along the k-th dimension within the input text."}, {"title": "Complex Vector Token Representation Update", "content": "Complex vectors naturally align with the physical properties of waves [9, 10], enabling the use of wave-inspired\noperations for efficient updates to complex vector token representations. In our Wave network, we introduce\na linear layer designed to generate two distinct versions of each token's complex vector representation at the\ninput-text level. These versions facilitate the application of wave-based operations, such as interference and\nmodulation."}, {"title": "Wave Interference", "content": "From physical perspective, wave interference is a phenomenon where two coherent waves combined by adding\ntheir intensities or displacements, considering their phase difference. In the context of generating complex\nvector token representations from input-level global semantics, as discussed in the Section 2, we define\ntwo variant complex vector token representations for token $w_j$ as: input_$Z_j$ = input_G\u00b7$e^{i\u00b7input-\\alpha_j}$ and\ninput_$Z'_j$ = input_G' \u00b7 $e^{i\u00b7input-\\alpha'_j}$. We use complex vectors addition to simulate wave interference [11] and\nobtain the combined complex vector token representation interference_$Z_j$ for token $w_j$ as follows:\nInterference_$Z_j$ = input_$Z_j$ + input_$Z'_j$ = input_G \u00b7 $e^{i\u00b7input-\\alpha_j}$ + input_G'. $e^{i\u00b7input-\\alpha'_j}$\n= (input-G cos(input_$\\alpha_j$) + input_G'. cos(input_$\\alpha'_j$))\n+i(input_G.sin(input_$\\alpha_j$) + input_G'. sin(input_$\\alpha'_j$))\n= $w_{j,k} + w'_{j,k} + i. (\\sqrt{w_{1,k}^2 + w_{2,k}^2 + ... + w_{j-1,k}^2 + w_{j+1,k}^2 + ... + w_{n,k}^2} \\pm \\sqrt{w_{1,k}^2 + w_{2,k}^2 + ... + w_{j-1,k}^2 + w_{j+1,k}^2 + ... + w_{n,k}^2})$    (1)\nNext, we illustrate how the phase difference between two complex vector token representations, such as\ninput_$Z_j$ and input_$Z'_j$, affects the overall intensity of the resulting complex vector through their interference\nterm. As discussed in detail in our prior work [3], the interference term Re(input_$Z_j$.input_$Z'$) can be derived\nfrom the square of the magnitude of input_$Z_{j,k}$. This interference term indicates how the phase difference\nbetween two complex vector representations determines constructive or destructive interference. Briefly, we\nexpress the interference term as follows:\n2. Re(input_$Z_j$.input_$Z'$) = 2. Re (input_G\u00b7$e^{i\u00b7input-\\alpha_j}$.input_G'\u00b7$e^{-i\u00b7input-\\alpha}$)\n= 2 \u00b7 input_G \u00b7 input_G' \u00b7 cos(input_$\\alpha_j$ \u2013 input_$\\alpha'_j$)                                                                                (2)\nEquation 2 demonstrates that the cosine value of the phase difference directly determines the interference\nresult."}, {"title": "Wave Modulation", "content": "From a physical perspective, wave modulation involves varying one or more characteristics of a periodic wave-\nform, known as the carrier signal, in response to a separate input signal that contains the information to\nbe transmitted. In signal processing, this concept is applied in two main forms. First, amplitude modulation\nadjusts the amplitude of the carrier wave based on the input signal's amplitude, encoding information in the\nwave's strength [12]. Second, phase modulation varies the phase of the carrier wave in response to the input\nsignal's changes, encoding information through shifts in the wave's position [13]. Both amplitude and phase\nmodulation can be achieved by multiplying complex vectors representing waves [14\u201316].\nIn the context of generating complex vector token representations from input-level global semantics,\nas discussed in the Section 2, we consider two variant complex vector token representations for token $w_j$\nas: input_$Z_j$ = input_G\u00b7$e^{i\u00b7input-\\alpha_j}$ and input_$Z'_j$ = input_G'. $e^{i\u00b7input-\\alpha'_j}$. We use complex vectors multipli-\ncation to simulate wave modulation [11] and obtain the combined complex vector token representation\nmodulation_$Z_j$ for token $w_j$ as follows.\nModulation_$Z_j$ = input_$Z_j$. input_$Z'_j$ = input_G. $e^{i input \\alpha_j}$. input_G' . $e^{i\u00b7input-\\alpha'_j}$ = input_G. input_G' . $e^{i\u00b7input-\\alpha_j+input-\\alpha'_j}$\n= input_G\u00b7 input_G' \u00b7 cos(input_$\\alpha_j$ + input_$\\alpha'_j$)\n+ i \u00b7 input_G \u00b7 input_G' \u00b7 sin(input_$\\alpha_j$ + input_$\\alpha'_j$)\n= ($w_{j,k} \u2022 w'_{j,k} - \\sqrt{w_{1,k}^2 + w_{2,k}^2 + ... + w_{j-1,k}^2 + w_{j+1,k}^2 + ... + w_{n,k}^2}$\n$\\cdot\\sqrt{w_{1,k}^2 + w_{2,k}^2 + ... + w_{j-1,k}^2 + w_{j+1, k}^2 + ... + w_{n,k}^2} ) + i\\cdot(\\sqrt{w_{1,k}^2 + w_{2,k}^2 + ... + w_{j-1,k}^2 + w_{j+1,k}^2 + ... + w_{n,k}^2}$          (3)\n$\\cdot(\\sqrt{w_{1,k}^2 + w_{2,k}^2 + ... + w_{j-1,k}^2 + w_{j+1, k}^2 + ... + w_{n,k}^2})$\nFor detailed computation steps, please refer to our previous work [3]."}, {"title": "Restore Token Embedding from Complex Vector Token Representations", "content": "BERT utilizes a special classification embedding [CLS] at the beginning of each input text to represent the\noverall input, and then make predictions based on this [CLS] embedding when performing text classification\ntasks [17]. To facilitate an accurate comparison with the Transformer and BERT on token representation and\nrepresentation update, we also utilize a [CLS] token at the beginning of each input text to represent the overall\ninput. the [CLS] token is then represented as a complex vector token representation like other tokens\nin the input text. For text classification tasks, we convert the representation of [CLS] embedding back to the\ntoken embedding space along with other tokens.\nAs described in Section 2, we restore the token embeddings by performing a multiplication between the\nglobal semantic vector input_G and the cosine value of the phase vector input_\u03b1j."}, {"title": "Experiments Settings", "content": "To ensure consistency and enable direct comparison, the experimental settings in this paper follow the same\nparameters as those used in our previous study [3]. Specifically, the learning rate is set to 1e-3 for both\nthe Wave network and Transformer, and 2e-5 for BERT [18]. The batch size varies depending on the task:\nfor resource utilization comparison experiments, all models use a batch size of 64; for accuracy comparison\nexperiments, the batch size is 64 for the Wave network and Transformer, and 32 for BERT [18]. In the gradient\ncomparison and embedding independence experiments, all three models use a batch size of 32. All models\nare trained or fine-tuned for four epochs. For fast convergence experiments, we evaluate test accuracy every\n10 batches over a total of 500 batches. To maintain a consistent architecture, both the Wave network and\nTransformer use a single-layer structure. The Wave network generates initial token embeddings randomly using\ntorch.nn.embedding in Pytorch [19], whereas the Transformer uses pre-trained BERT token embeddings. The\nBERT model is fine-tuned using the pre-trained base version.\nIn the gradient and independence analysis experiments, we compare the Wave network and Transformer\nby focusing on three key components: (1) the back-propagation gradient of the cross-entropy loss of the [CLS]\ntoken embedding, (2) the back-propagation gradient of the cross-entropy loss of the overall token embeddings\nin the input text, and (3) the back-propagation gradient of the cross-entropy loss for the classifier parameters.\nBy excluding auxiliary components like linear and feed-forward layers, we ensure a controlled comparison of\nhow core components respond to changes in training data and influence learning dynamics. Additionally, we\nanalyze the feature independence across the dimensions of the [CLS] token embedding to better understand\nmodel behavior."}, {"title": "Result and Analysis", "content": ""}, {"title": "Accuracy Comparison", "content": "This section replicates the accuracy comparison from our previous work [3], verifying similar improvements in\nVRAM usage, training time, and convergence speed. As shown in Table 1 and Table 2 for the AG News dataset,\ncompared to the single-layer Transformer, the WNI and WNM in Table 2 significantly reduces the VRAM\nconsumption by 64.71% and 62.35%, respectively, and shorten training time by 15.21% and 15.14%. At the\nsame time, they improve classification accuracy by 18.68% and 19.61%, corresponding to wave interference and\nmodulation. When compared to the pre-training BERT base, the single-layer Wave network reduces VRAM\nconsumption by 76.56% and 75%, and cuts training time by 85.8%, while maintaining 96.96% and 96.85% of\nBERT's accuracy."}, {"title": "Convergence Performance", "content": "We hypothesize that effective representation and update methods should enable models to complete down-\nstream tasks with minimal training, even when starting from randomly initialized embeddings. To test this,\nwe collected 50 data points from the Wave network and Transformer, assessing test set accuracy after every\nten training batches."}, {"title": "Gradients of [CLS] Embedding:", "content": "For the Wave network's [CLS] token embedding gradient norm graph, the [CLS] token summarizes information\nfrom the entire input text. At the start of the first epoch, the gradient norm dropped sharply from 0.13\nto an oscillation range between 0.05 and 0.02. This sharp drop indicates that the [CLS] token embedding\ninitially struggled to capture the necessary information for classification, prompting significant adjustments.\nIn subsequent training, the minimum gradient value stabilized between 0.01 and 0.02, while the maximum\ngradually increased from 0.05 to 0.1. This suggests that while the of the [CLS] token embedding of most"}, {"title": "Gradients of Overall Embedding of Input Text:", "content": "For the Wave network's overall token embedding gradient norm of the input text, at the beginning of the\nfirst epoch, the gradient norm oscillates between 0.01 and 0.04. This suggests that, in the early stages of\ntraining, the model makes global adjustments to reduce classification loss. Since classification errors vary greatly\nacross samples, the overall gradient fluctuates accordingly. As training progresses, the maximum gradient norm\nincreases from 0.04 to 0.09, indicating that some harder-to-classify samples have larger cross-entropy errors.\nAs a result, the model applies larger gradients to adjust their token embeddings. Meanwhile, the minimum\ngradient norm remains at 0.01, showing that correctly classified samples consistently maintain small gradients.\nFor the Transformer's overall token embedding gradient norm, at around 2000 batches in the first epoch, the\ngradient norm peaks at 2000. Similar high peaks are observed in subsequent epochs, with values of 1200 in\nthe second and 2900 in the fourth epoch. These extreme values suggest that certain samples have abnormally\nlarge cross-entropy errors, leading to very large gradients. Additionally, the presence of lower gradient norms\nfor other samples indicates that errors persist across a wide range of samples, resulting in fluctuating token\nembeddings and inconsistent classification performance."}, {"title": "Gradients of Classifier:", "content": "For the gradient norm graph of the classifier parameters in the Wave network, the gradient norm initially\ndrops sharply from 28 to a range of 14 to 5 during the beginning of the first epoch. This indicates that the\nmodel made substantial adjustments to the classifier weights in the early stage of training, reflecting the need\nto reduce the initially high classification cross-entropy loss. Since this is a single-layer classifier, such large"}, {"title": "Independency Level Among the Dimensions of Embeddings:", "content": "The [CLS] embedding represents global semantics and forms the basis for classification tasks in both wave\nnetwork and Transformer. This experiment delves into its independence among dimensions to unravel the\ncomplex relationship between feature independence and classification accuracy. We analyze the correlations"}, {"title": "Comparison of Complexity and Parameters", "content": "Wave network is more efficient and effective in representing tokens and updating representations, significantly\nreduce the time and space complexity, and parameter quantity. Even its feed-forward and normalization layer\nmust deal with the real and imaginary parts separately in the current PyTorch and TensorFlow versions."}, {"title": "Time Complexity", "content": "Transformer:"}, {"title": "Space Complexity", "content": "Transformer:"}, {"title": "Discussion", "content": "We introduced Token2Wave, a novel representation method that captures both global and local text semantics.\nToken2Wave offers significant potential to transform fields that rely heavily on the NLP by delivering expected\nperformance with minimal computational hardware and reduced processing time. This efficiency suggests that\nmany devices with limited hardware resources could perform advanced NLP tasks, including personalized fine-\ntuning and reasoning, without relying on extensive infrastructure. These benefits could profoundly improve\nexisting business environments. For instance, Token2Wave could enable medical devices with limited computing\nresources to incorporate natural language understanding for supporting remote diagnosis and real-time health\nmonitoring or provide personalized psychological support and advice based on user sentiment on a mobile\ndevice."}]}