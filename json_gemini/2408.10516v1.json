{"title": "Data Augmentation Integrating Dialogue Flow and Style to Adapt Spoken Dialogue Systems to Low-Resource User Groups", "authors": ["Zhiyang Qi", "Michimasa Inaba"], "abstract": "This study addresses the interaction challenges encountered by spoken dialogue systems (SDSs) when engaging with users who exhibit distinct conversational behaviors, particularly minors, in scenarios where data are scarce. We propose a novel data augmentation framework to enhance SDS performance for user groups with limited resources. Our approach leverages a large language model (LLM) to extract speaker styles and a pre-trained language model (PLM) to simulate dialogue act history. This method generates enriched and personalized dialogue data, facilitating improved interactions with unique user demographics. Extensive experiments validate the efficacy of our methodology, highlighting its potential to foster the development of more adaptive and inclusive dialogue systems.", "sections": [{"title": "1 Introduction", "content": "As an innovative technology at the forefront of artificial intelligence and speech processing, spoken dialogue systems (SDSs) have attracted significant interest from both academia and industry (Kawahara, 2018; Si et al., 2023; Abdul-Kader and Woods, 2015; Kim et al., 2021). Despite the powerful capabilities of large language models (LLMs), traditional SDS remain a focal point of research due to their superior control and interpretability (Singh et al., 2024). These systems are predominantly trained using data from human-to-human interactions, which highlight varying speaking styles, such as clarity of intentions, as depicted in Figure 1. This variability necessitates that human speakers adjust their dialogue strategies when engaging with different users. For instance, compared to adults, minors often exhibit less clarity in their intentions and give ambiguous responses, requiring more confirmatory language or additional inquiries to better adapt to the unique speaking styles of younger users. This adaptive approach is crucial for enhancing the effectiveness and user-friendliness of SDS in real-world scenarios.\nHowever, adapting SDSs to these distinctive speaking styles typically requires a wealth of annotated dialogue data, which can be challenging to"}, {"title": "2 Related Work", "content": "The scarcity of annotated data and the challenge of data imbalance are persistent issues in various artificial intelligence domains (Shorten and Khoshgoftaar, 2019; Shi et al., 2020; Ahmad et al., 2021; Hedderich et al., 2021; Kim et al., 2023). To address these effectively, data augmentation techniques have been employed, as demonstrated in prior research across different tasks (Feng et al., 2021; Bayer et al., 2022). For instance, Schick and Sch\u00fctze (2021) generated text similarity datasets from scratch by instructing a large PLM. Similarly, Liu et al. (2022) and Chen and Yang (2021) enhanced data by manipulating individual utterances within dialogues\u2014such as adding, deleting, changing their order, or regenerating them\u2014while preserving the original meaning, which improved model performance in dialogue summarization tasks. While the abovementioned methods focus on generating individual sentences, our study aims to create coherent dialogues comprising multiple sentences tailored for specific target groups.\nMohapatra et al. (2021) utilized GPT-2 (Radford et al., 2019) to develop user and agent bots, generating comprehensive task-oriented dialogues through bot interactions, demonstrating notable enhancements in low-resource scenarios with datasets MultiWOZ (Budzianowski et al., 2018) and PersonaChat (Zhang et al., 2018). Recently, with the advanced text generation capabilities of LLMs, researchers have started using LLMs for data augmentation (Pan et al., 2023; Kim et al., 2023; Wang et al., 2023). For instance, Kim et al. (2023) guided LLMs to generate a broad spectrum of social dialogues using social commonsense knowledge from a knowledge graph. Pan et al. (2023) generated domain-specific, task-oriented dialogues by extracting dialogue paths from out-of-domain conversations. The concept of dialogue paths in their work aligns with the concept of DA history in our research. However, the key distinction is that while they extract DA paths from existing data, we generate tailored DA histories based on existing data, specifically optimized for target user groups."}, {"title": "3 The Proposed Framework", "content": "In this study, we aim to enhance the DA prediction performance of the system when dealing with low-resource user groups that exhibit unique dialogue strategies, by generating training data through the proposed data augmentation framework. In the construction of SDSs, accurate DA prediction is crucial as it facilitates dialogue state tracking and guides response generation, thereby reducing erroneous responses (Chen et al., 2017). The task depicted in the left portion of Figure 2 is defined as follows. Assuming the current turn of the dialogue is turn t, we utilize the dialogue history \\(H_d = (S_{t-n}, U_{t-n}, ..., S_{t-1}, U_{t-1})\\) from the previous n turns, along with the system's DA history \\(H_a = (a_{t-n},..., a_{t-1})\\) from these turns, as the input. The output is the system's DA \\(a_t\\) for the current turn.\nSince we predict the current turn's DA based on the dialogue history and the system's DA history, it becomes crucial to generate dialogue and system DA histories that closely align with the target user group. To achieve this, we control the generation of dialogue data by capturing the speaking style of dialogue participants and generating dialogue flows that mimic real human interactions with the target user group. The importance of this approach lies in the fact that the model can effectively understand and adapt to unique dialogue strategies only when the training data realistically simulates complex dialogue scenarios. In real human interactions, users with unique dialogue strategies are in the minority and exhibit considerable diversity. Due to the limitations in data scale, traditional training datasets often fail to cover this diversity, which limits the model's adaptability and accuracy when dealing with such users. By simulating the dialogue styles and processes of specific user groups, we can generate more diverse and precise training data, thereby enhancing the model's generalizability and adaptability to diverse users.\nAs illustrated in Figure 2, our data augmentation framework comprises three components: (1) employing ChatGPT\u00b9 to extract the speaker's styles S, (2) finetuning a pre-trained model to generate the system's DA history \\(H_{Aug} = (a_{t-n}^{Aug},..., a_{t-1}^{Aug})\\), and (3) inputting the extracted speaking styles S and the generated system's DA history \\(H_{Aug}\\) into ChatGPT to generate the training dialogue data \\(H_{Aug} = (S_{t-n}^{Aug}, U_{t-n}^{Aug},..., S_{t-1}^{Aug}, U_{t-1}^{Aug})\\)."}, {"title": "3.1 Speaker Styles Extraction", "content": "Since the unique speaking styles employed by the target user group significantly influence the content of conversations, it's crucial to capture the speaking styles of this group by comparing dialogues from the target user group with those from non-target groups. This helps guide the subsequent generation of dialogues specifically tailored to the target user group. To facilitate this, we employ ChatGPT to extract speaker styles from conversations involving target users.\nSpecifically, we input a set of m dialogues, half of which involve users from the target group and the other half from non-target user groups. This balanced approach allows for an effective comparison, helping to identify and differentiate prominent speaking characteristics unique to the target group. Subsequently, ChatGPT is utilized to generate out-"}, {"title": "3.2 DA History Generation", "content": "As depicted in Figure 1, the unique conversational strategies employed by the target group also significantly influence the DAs of those engaging with them. Our objective at this stage is to generate a diverse and realistic DA history \\(H_{Aug}\\) that is specifically optimized for groups with distinctive speaking strategies. As shown in Figure 3, we achieve this by finetuning a PLM using existing data to generate the system's DA history \\(H_{Aug}\\) for the previous n turns.\nIn particular, we utilize the DA at and utterance St from the current turn t as inputs, with the DA history Ha from the previous n turns as the desired output to establish training data. These data are then divided into two sets: one for training and the other for generation. Initially, we finetune the PLM using all available training data to capture DA histories that closely resemble real human conversations. Subsequently, we conduct a secondary finetuning utilizing training data exclusively from the target user group. This dual finetuning approach ensures that the model can generate DA histories that closely mimic real human dialogues and align with the unique speaking strategies of the target users. The first finetuning, which employs a relatively large dataset, enables the model to produce DA histories that mirror authentic human interactions. The second finetuning, focused on a smaller dataset specific to the target user group, allows the model to better tailor the DA histories to their unique characteristics.\nDuring the generation phase, we input the the DA at and utterance St from the current turn t and generate the DA history \\(H_{Aug}\\) from the previous n turns. To ensure diversity, we simultaneously generate multiple outputs, selecting only those (at, \\(H_{Aug}\\)) combinations that have not been previously observed."}, {"title": "3.3 Dialogue Generation", "content": "Having obtained speaker styles and DA history tailored to users employing unique dialogue strategies, our ultimate goal is to generate dialogues corresponding to these styles and histories to enrich the training data for DA prediction. At this stage, we leverage ChatGPT's powerful generation capabilities to create dialogue data for training purposes. Utilizing a few-shot prompt, we input the extracted speaking styles S and the DA histories \\(H_{Aug}\\) into ChatGPT to generate dialogues \\(H_{Aug}\\) that reflect the conversational style of the target users. Subsequently, we use the generated dialogues \\(H_{Aug}\\) and DA histories \\(H_{Aug}\\) as inputs, with at as the gold-standard answer, to construct the training data. The prompts used for generating these dialogues are detailed in Appendix D.\nThis approach aims to enhance the model's ability to predict DAs when interacting with target users who exhibit unique conversational strategies. It effectively addresses the challenge of data scarcity by employing data augmentation."}, {"title": "4 Experiment", "content": "To evaluate the effectiveness of the proposed data augmentation framework, we conducted experiments using data from minors who employed unique conversational styles and strategies in actual dialogues within the dataset. These experiments were carried out in a low-resource setting across multiple splits, each utilizing different subsets of data from minors. We trained multiple DA prediction models on datasets of varying sizes, including models trained with augmented data added to the existing datasets."}, {"title": "4.1 Dataset", "content": "This study utilized a multimodal dialogue Japanese dataset known as the \"Travel Agency Task Dialogue Corpus\u201d (Inaba et al., 2022, 2024), which features conversations from users of various age groups, with detailed annotations of DAs. This dataset contains 115 hours of dialogue, spanning 330 conversations, with each averaging about 20 minutes. The dialogues were facilitated via Zoom video calls, involving six operators and 55 customers, including 20 minors (ages 7-17), 25 adults (ages 20-60), and 10 seniors (ages 65-72). Each customer participated in six dialogues.\nThe dialogues revolve around recommending travel destinations to users across various age groups. The dataset authors employed a Hidden Markov Model (HMM) (Rabiner, 1989) to analyze the transitions in dialogue among different age groups using sequences of DAs. A notable observation was that minors often used unique dialogue strategies compared to other age groups, typically expressing fewer independent opinions. The annotation of DAs was performed by functional segment, a unit smaller than an utterance. Each operator's segment is annotated as one of the 28 predefined DAs related to travel destination recommendations, or as \"None\". Examples of these DAs include asking about the travel season (Season-Question) and summarizing the travel plan (Travel-Summary), all of which are detailed in Appendix A. Since segments labeled \"None\" primarily consist of non-informative responses such as \"Yeah\" or \"Uh-huh,\" and our objective is to guide the system to generate accurate and meaningful responses using DA tags, we selectively included only those training instances where the gold-standard responses were not labeled \"None\" in this study. Additionally, we employed text-based human transcriptions"}, {"title": "4.2 Low-Resource Setting", "content": "We trained five DA prediction models using datasets of varying scales: Minors-Only, Zero-Shot, Low-Resource, Full-Resource, and Low-Resource+Augmentation(Ours). To simulate low-resource conditions for specific user demographics, we used dialogue data from only 3 minors out of a group of 20, totaling 18 dialogues for training. For evaluation, we used 60 dialogues from 10 minors.\n\u2022 Minors-Only: Employed only 18 dialogues from 3 minors.\n\u2022 Zero-Shot: Utilized all data from adults and seniors, amounting to 210 dialogues.\n\u2022 Low-Resource: Combined the 18 dialogues from the Minors-Only with all 210 dialogues from adults and seniors, totaling 228 dialogues.\n\u2022 Full-Resource: Included dialogues from 10 minors (60 dialogues), encompassing those from the 3 minors in the low-resource setting, plus all 210 dialogues from adults and seniors, totaling 270 dialogues.\n\u2022 Low-Resource + Aug(mentation) (Ours): Used the 228 dialogues from the Low-Resource and supplemented them using our proposed augmentation framework. Additional data was generated until the dataset size matched that of the Full-Resource for a direct comparison."}, {"title": "4.3 Setup and Details", "content": "In the process of extracting speaker styles, we fed m = 6 dialogues into GPT-4-0125-preview, where three were from minors in a low-resource setting, and the other three involved different adults or seniors. For generating training dialogues, GPT-3.5-turbo-0125 was employed.\nDuring the DA history generation phase, we utilized Japanese T5-Large\u00b2 as the PLM. We conducted two rounds of finetuning to ensure the model is capable of generating DA histories that not only closely mimic real human conversations but also align with the unique conversational strategies of minors during interactions. During the first training phase, the learning rate was set at 1e-4, and"}, {"title": "5 Results and Analysis", "content": "Table 2 shows the mean and standard deviation after five runs using seeds ranging from 1 to 5 across four different splits. While the Minors-Only solely comprised data from minors, its performance was inferior to the Zero-Shot model trained only with adult and elderly dialogue data due to the limited amount of training data. Therefore, we also used all available adult and elderly dialogue data in other setups to enhance the model's generalization capabilities.\nAdditionally, since Zero-Shot does not use minor's dialogues, the training data remains consistent across the four different splits. The variation in Zero-Shot's performance across the splits further underscores the differences in the model's adaptability to different minors, with the third split proving most challenging.\nAcross the four splits, the performance of our proposed data augmentation framework, Low-Resource + Aug (Ours), almost all surpassed that of Low-Resource on both T5 and GPT-NeoX in terms of mean exact and partial match rates. This demonstrates that even in a low-resource setting, our method successfully captures the characteristics of minor speakers and generates dialogue flows that align with minor speaking behaviors, thereby guiding the generation of training dialogues.\nHowever, even though we augmented the data to match the quantity of the Full-Resource in each split, Full-Resource typically showed superior per-"}, {"title": "5.1 Ablation", "content": "To evaluate the individual effectiveness of components in our proposed framework, we conducted ablation experiments using Japanese GPT-NeoX across four splits:\n\u2022 w/o DA History Gen: In this model, we omitted the generation of new DA histories and instead randomly selected DA histories from the Low-Resource for data generation.\n\u2022 DA History Gen w/o Second Finetune: This variant involved finetuning the DA history generation model only once, without a second round of finetuning tailored specifically for minors.\n\u2022 w/o Speaker Style: This model utilized the same DA histories as our complete method but did not use extracted speaker styles during dialogue data generation.\nTable 3 shows the average results across the four splits, conducting five trainings for each model in every split with seed values set from 1 to 5. The findings indicate that both w/o DA History Gen and w/o Speaker Style variants achieved higher mean exact and partial match rates than the Low-Resource. This demonstrates that the training data generated through the independent use of style extraction and DA history generation components can also significantly improve performance.\nFurthermore, although DA History Gen w/o Second Finetune did not use data from the target user group for a second fine-tuning during the training of the DA history generation model, its performance still surpassed that of w/o DA History Gen. This indicates that in generating DA history, even without a second finetuning to optimize the PLM for minors, the new (at, \\(H_{Aug}\\)) combinations generated by a PLM trained with all available data can still enhance performance. Ultimately, Ours achieved the highest rates for both exact and partial matches, indicating that the combination of speaker styles extraction and DA history generation is most effective and underscores the necessity of targeted age-specific second finetuning when training the DA history generation model."}, {"title": "5.2 Why did the Speaker Style work?", "content": "Figure 4 displays dialogues generated by w/o Speaker Style and Ours, using the same DA history. The DA history consists of first asking the user a travel-related request (RequestQuestion), then confirming the request (RequestConfirm), and finally indicating the content to be searched (SearchConditionInform). We observed that without the speaker style, the user in the w/o Speaker Style provided specific travel requirements, and the dialogue progressed smoothly. In contrast, the user in the Ours did not exhibit a clear intent. This indicates that the speaker style is effective, resulting in dialogues that more closely match the speaking styles of minors and aligning more closely with real human conversations."}, {"title": "5.3 Why did the DA History Generation work?", "content": "We compared the performance in generating DA histories between DA History Gen w/o Second Finetune and Ours on split 1.\nFor a direct comparison, we used 9,999 data points (at, St) from dialogues involving 90 adults and seniors to generate DA histories \\(H_{Aug}\\), resulting in three DA histories per data point. This generation was conducted under the settings of top_k=50, top_p=0.9, and temperature=0.9. After removing duplicate (at, \\(H_{Aug}\\)), DA History Gen w/o Second Finetune produced 7,677 new (at, \\(H_{Aug}\\)), whereas Ours generated 10,412. We assessed how many of these combinations appeared in dialogues involving 17 minors (excluding those from the Low-Resource), finding 908 for DA History Gen w/o Second Finetune and 956 for Ours. Referencing Table 3, we can infer that compared to w/o DA History Gen which relied solely on existing DA histories, both DA History Gen w/o Second Finetune and Ours generated DAs that were present in the target user group, leading to improved performance. Notably, Ours, which underwent secondary finetuning for the target users,"}, {"title": "6 Conclusion", "content": "We introduced a data augmentation method designed to enhance the performance of the DA prediction model for users with limited data and unique conversational styles. Our experiments confirmed the reliability of the proposed method and the effectiveness of its components. While this study did not exhaustively explore the full potential for improvement of the proposed method, we plan to further evaluate this aspect in our future work."}]}