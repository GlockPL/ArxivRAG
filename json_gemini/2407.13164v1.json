{"title": "Translate-and-Revise: Boosting Large Language Models for Constrained Translation", "authors": ["Pengcheng Huang", "Yongyu Mu", "Yuzhang Wu", "Bei Li", "Chunyang Xiao", "Tong Xiao", "Jingbo Zhu"], "abstract": "Imposing constraints on machine translation systems presents a challenging issue because these systems are not trained to make use of constraints in generating adequate, fluent translations. In this paper, we leverage the capabilities of large language models (LLMs) for constrained translation, given that LLMs can easily adapt to this task by taking translation instructions and constraints as prompts. However, LLMs cannot always guarantee the adequacy of translation, and, in some cases, ignore the given constraints. This is in part because LLMs might be overly confident in their predictions, overriding the influence of the constraints. To overcome this overiding behaviour, we propose to add a revision process that encourages LLMs to correct the outputs by prompting them about the constraints that have not yet been met. We evaluate our approach on four constrained translation tasks, encompassing both lexical and structural constraints in multiple constraint domains. Experiments show 15% improvement in constraint-based translation accuracy over standard LLMs and the approach also significantly outperforms neural machine translation (NMT) state-of-the-art methods.", "sections": [{"title": "1 Introduction", "content": "Constrained translation seeks to generate translations that adhere to pre-specified constraints. To achieve this, conventional approaches impose constraints on machine translation systems and force them to follow the constraints during inference (Hokamp and Liu, 2017; Hasler et al., 2018; Dinu et al., 2019; Bergmanis and Pinnis, 2021b; Wang et al., 2022b; Ailem et al., 2022). More recently, large language models (LLMs) have been shown to be strong translation systems (Hendy et al., 2023; Moslem et al., 2023). They provide a general way to involve various instructions, demonstrations, and constraints into the translation process (Mu et al., 2023; Bogoychev and Chen, 2023), enabling us to perform constrained translation using off-the-shelf, well-trained LLMs.\nWhile applying LLMs to constrained translation is straightforward, we observe empirically that even strong LLMs (i.e. GPT-3.5) do not always follow the instructions to obey constraints: LLMs' predictions often override the guide of constraints, which result in missing constraints during translation. See Figure 1 for an example where we use an LLM to translate an English sentence to a Chinese sentence with a lexical constraint \u201cCOVID-19\u2192\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u201d. We note that, despite significant effort in developing clear and instructive prompts, we were not able to improve the LLM in a single run of the LLM through the use of these constraints. For instance, we observed that when using open-source LLM to translate COVID-19, it tends to translate it as \u201c\u65b0\u51a0\u201d more than 80% of the time, overlooking the constraint in the prompt to translate COVID-19 as \u201c\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2\u201d. The problem consists of a real use case for what describes as 'memo trap' in the LLM literature (McKenzie et al., 2023).\nTo alleviate this problem and thus improve the accuracy to meet constraints, we propose to construct prompts iteratively that enable better focus on the unsatisfied constraints. The idea behind our approach is to leverage the auto correction skills of LLMs by explicitly prompting them with which constraints are not satisfied (Madaan et al., 2023; Zhang et al., 2023c; Jiang et al., 2023). To do this, we introduce a revision step after the initial run of LLMs where we provide the LLMs with both the already-generated translation and the constraints that have not been covered. Then, we instruct the LLM to revise its output by taking these constraints into account.\nWe conduct experiments across four diverse constrained translation datasets, encompassing two distinct constraint types: lexical and structural. Our proposed \u201cTranslate-and-Revise\u201d (TAR) approach consistently elevates the performance of LLMs in constrained translation, achieving state-of-the-art (SoTA) results on multiple datasets.\nThe contributions of this work are as follows:\n\u2022 We introduce a novel TAR strategy that initially employs LLMs as constraint-aware translators and subsequently reproposes them as revisers to revise translations that do not meet given constraints. We show that TAR significantly reduces missing constraints during translations.\n\u2022 We rigorously evaluate our approach on four constrained translation datasets spanning multiple domains like news and electronics. Our results demonstrate a significant improvement in constraint fidelity and translation quality, outperforming existing methods and achieving SoTA results.\n\u2022 To the best of our knowledge, our study is the first to evaluate LLMs across four distinct constrained translation datasets, thereby providing a robust LLM baseline for future research in the area. We believe our findings serve a solid baseline towards establishing more comprehensive benchmarks in the field of constrained translation."}, {"title": "2 Methods", "content": "Given a source language input and bilingual constraints, TAR first employs LLMs as translators for an initial translation. While this step often yields high-quality outputs, when the LLMs' confidence during generation exceeds the guidance of the constraints, it results in suboptimal translation outputs. To mitigate the occurrence of missing constraints in LLMs-based translation, we introduce a reviser to enhance adherence to the constraints in the translation. The revision process is iterated multiple times until all constraints are satisfied, or the maximum allowable number of modifications is reached. The process of TAR is provided in Figure 1. Next, we describe TAR in more details.\n2.1 Translate\nLet $X = {X_1,X_2,..., X_n}$ be the source-language sentence with length $n$, and $Y = {Y_1, Y_2, ..., Y_m}$ be the target-language sentence with length $m$. The translation procedure can be written as:\n$Y = Trans(f(X))$                                            (1)\nwhere $Trans(\u00b7)$ symbolizes the translation model (either an NMT model or an LLM), and $f(\u00b7)$ denotes a template by which we process $X$ to make it suitable as the input of $Trans(.)$"}, {"title": "2.2 Revise", "content": "However, the LLM-based translation cannot always cover all original constraints. We randomly sampled 20 incorrect translation results and observed that, in datasets like WMT21 Terminology Translation (Alam et al., 2021), to 95%(19/20) of the cases, the tokens generated by the model were similar to the expected constraints meaning and exhibited high confidence levels. The confidence level of LLMs in generating these tokens remained virtually unchanged, whether or not constraints were included in the instructions, revealing overconfidence in generation while overlooking the constraints.\nWe notice a strong connection between our real use case and 'memo trap' (McKenzie et al., 2023) as unsatisfied constraints often pertain to non-mainstream translations resulting terms used with lower frequency and the incorrect translations usually refer to the mainstream translations. Compared 'memo trap', we show that the phenomenon extends to toy settings and is prominent even for real applications and for SOTA models like GPT-3.5.\nTo overcome these challenges, we initially employ a rule-based method to identify which constraints are not completed. Subsequently, these uncompleted constraints $(S,T)^{un}$, along with the source language input $X$, flawed translations output $Y^{flawed}$, and all other given constraints $(S,T)$, are passed to the LLM. At this juncture, the LLM assumes the role of a reviewer, tasked with revising flawed translation upon receipt of uncompleted constraints. The aforementioned process is defined by the following formula:\n$Y = Revise(f(X, (S,T), (S,T)^{un}, Y^{flawed}))$                        (3)"}, {"title": "3 Experiments", "content": "In this study, we evaluate the performance of the TAR in constrained translation. While most previous research has typically focused on just one or two constrained translation tasks (Dinu et al., 2019; Wang et al., 2022a; Hashimoto et al., 2019; Zeng et al., 2023), our evaluation expands to two types of constraints: lexical constraints and structural constraints, covering four practical scenarios: general lexically constrained translation, translation with terminology constraints, translation with named entity constraints, and structured document translation.\n3.1 Setup\nDatasets Detailed information about the datasets we used can be found in Table 1. Lexical constraints refer to sentences with predefined word or phrase constraints sourced from existing databases. Structural constraints, in contrast, encompass inline markup tag constraints like XML tags, for example, $\\langle ph\\rangle$ and $\\langle/ph\\rangle$. Here are more details about the datasets that we use in this work:\nGeneral Lexically Constrained Translation: This is based on a dataset provided by (Dinu et al., 2019). The dataset is derived from newstest2017 En \u2192 De. Lexical constraints are extracted with guidance from two general-domain term databases: IATE and Wik-tionary\u00b9.\nTerminology Translation: To benchmark against SOTA NMT systems, we employ the official test set from the terminology translation task in WMT212(WMT21 TT) (Alam et al., 2021).\nEntity Translation: We also endeavor to evaluate our method using the extensive Entity Translation Corpus (ETC) (Zeng et al., 2023), which comprises six test sets from the WMT News Translation Task spanning 2015-2021. For alignment, we employ spaCy NER models\u00b3 to extract source entities and use awesome-align (Dou and Neubig, 2021) for their correspondence.\nStructured Document Translation: Following recent works (Wang et al., 2022a), we conduct experiments on the LXM dataset\u2074 (Hashimoto et al., 2019), in which XML tags are hierarchically distributed"}, {"title": "3.2 Main results", "content": "Table 2, Table 3, Table 4, and Table 5 detail the performance of TAR on general lexically constrained translation, terminology translation, entity translation, and structured document translation, respectively. Here are our main results.\nComparison with base LLMS TAR consistently boosts the performance of LLMs in constrained translation. Two primary factors contribute to this improvement:\n(1) Our natural language-based prompts, as opposed to the conventional few-shot translation prompts (Hendy et al., 2023), are more effective for constrained translation. Specifically, in terminology translation (refer to Table 3), our prompts lead to an average BLEU score increase of 0.3 and a CCR rise of 7.7%.\nWe noticed significant gains over base LLMs in entity translation. Across four language directions, there is a consistent uplift in BLEU scores, averaging an increase of 0.9. Notably, the CCR experiences increases of 19.4%, 8.9%, 24.3%, and 19.9% respectively. This marked improvement can primarily be attributed to the superior instruction-following capabilities of LLMs. By incorporating constraints in instructions, we can guide the model more effectively to address these constraints, alleviating the issue of LLMs struggling to correctly translate named entities."}, {"title": "3.3 Impact of Inputs on Reviser Performance", "content": "The reviser receives inputs including the source language sentence, translation results, given constraints, and the uncompleted constraints. To evaluate the significance of each component, we conducted experiments wherein we omitted specific elements from the input. The variations include: 1) Excluding uncompleted constraints; 2) Excluding original constraints; and 3) Only indicating to the model that the translation is flawed without specifying the uncompleted constraints. All other settings remain unchanged. The comparative outcomes on IATE and Wiktionary are presented in Table 7.\nFrom our observations, the CCR scores of the variants show a decline compared to the default input of the TAR reviser on both datasets. Interestingly, the omission of the original constraints has a more pronounced impact on CCR. This could be attributed to the negative modification of completed constraints made by the reviser when it is inaccessible to the given constraints. Additionally, when both elements"}, {"title": "4 Analysis", "content": "4.1 TAR Augments NMT Translators\nIn our study, we initially depended on constraint-aware translators to produce preliminary translation results. However, in real-world scenarios, industry practitioners often possess powerful domain-agnostic NMT models. These models, due to their lack of training with specific constraints, frequently fall short in constrained translation tasks. In this section, we integrate TAR into these general-purpose NMT models. By iteratively optimizing the NMT translation results through TAR, we can significantly enhance the CCR of the translation while ensuring its quality.\nSpecifically, we first the WMT21 champion model (Tran et al., 2021) to obtain a preliminary translation result. Since this model is not specifically trained for constraints, the initial translation often exhibits a suboptimal CCR. Building on this, we apply TAR to revise this outcome, iteratively optimizing to form the final translation result.\nThe experimental results on WMT21 TT and ETC datasets are presented in Table 8, we can see that TAR bolsters both BLEU and CCR scores of NMT models. On average, we observed an uplift of 0.7 and 0.8 in BLEU scores, coupled with impressive gains of 10.7% and 14.3% in CCR across the two datasets and language pairs. Although there remains a gap in constraint-based translation accuracy compared to the standard TAR, it generally exhibits superior translation quality. This insight demonstrates that when equipped with TAR, even domain-agnostic NMT models can adeptly tackle constrained translation. This eliminates the need for forced decoding algorithms or additional training, greatly enhancing their usability in constrained translation applications.\n4.2 Scaling TAR to More LLMs\nTo evaluate the scalability and robustness of TAR across different models, we applied it to a variety of LLMs, including commercial models like GPT-3 and GPT-4, as well as the open-source Qwen\u00ba (Bai et al., 2023). We maintained consistency in all other settings. Experiments were conducted in the \"En-Zh\" and \"En-Ru\" language directions for terminology translation. As shown in Figure 3, TAR consistently improves the performance of various LLMs in constrained translation tasks. To be specific, the average CCR for GPT-3, GPT-4, and Qwen increases by 20%, 11.7%, and 14.4%, respectively. Comparing the results of ChatGPT with those of GPT-3 and GPT-4, it's evident that TAR enables more powerful models to fully harness their capabilities in constrained translation. Intriguingly, although the CCR score"}, {"title": "4.3 Revision Iterative Round and Prompt Ensemble", "content": "The reviser, in its function, takes the translation result and uncompleted constraints as input. Naturally, one might consider iteratively revising the output multiple times. The question arises: how many iterations strike the optimal balance? Here, we employed constraint-aware LLMs and NMT on the \u201cEn-Zh\u201d and \u201cEn-Ru\" directions of the terminology translation dataset. We assessed performance across different iterative rounds of the revision module, consistently using the same prompts for each iterative phase.\nAs presented in Figure 4a, there is a significant leap in performance predominantly during the initial revision. Although performance does enhance with increasing iterations, the rate of improvement starts to taper off, indicating diminishing returns.\nFurthermore, we also investigated whether the LLM reviser can benefit from varying prompts in multiple iterations. To experiment this, after designing several revision templates, we randomly select one for each iterative round. Thus, a complete multi-round revision process can utilize various templates. This design is in part similar to prompt ensemble methods (Zhang et al., 2023a; Pitis et al., 2023), combining benefits of various prompts, akin to ensemble learning (Dong et al., 2020). Experimental results are shown in Figure 4b. Compared to applying a single template in revision iterations, utilizing diverse templates achieves superior performance."}, {"title": "4.4 More Analysis", "content": "Due to space limitations, we provide a more detailed analysis of our method in the appendix, including the additional costs incurred by TAR, the impact on performance as the number of constraints increases, reasons for potential declines in BLEU scores during the revision phase for certain datasets, and the performance of traditional constrained translation data augmentation methods on LLMs."}, {"title": "5 Related Work", "content": "5.1 Constrained Translation\nMachine translation has made considerable progress in incorporating pre-specified constraint, which can be categorized into hard constrained translation and soft constrained translation.\nHard constrained translation: This line of research expanded the original search space via decoding algorithm modification to strictly incorporate constraints (Hokamp and Liu, 2017; Post and Vilar, 2018; Hu et al., 2019). However, while these methods achieve a high constraint-based translation accuracy, they tend to be computationally expensive and can sometimes compromise translation quality (Hasler et al., 2018; Zhang et al., 2021).\nSoft constrained translation: Here, research primarily centers on data augmentation strategies to train NMT models to integrate constraints. Several techniques have been proposed, including: replacing source language constraints with special token (Crego et al., 2016; Wang et al., 2017; Zhang et al., 2023b); substituting source language constraints with target language constraints (Song et al., 2019; Dinu et al., 2019); using inline annotations to individually mark source and target language constraints (Ailem"}, {"title": "6 Conclusion", "content": "In this work, we introduce the TAR prompting method, adeptly leverages LLMs for constrained translation. Our approach involves a two-step process: first using LLMs for constrained translation, and subsequently deploying them to revise translations with uncompleted constraints. Our approach mainly improves the constraint accuracy while maintaining translation quality by overcoming the \u2018memo trap' from the LLMs during translation using dedicated revision prompts in an iterative manner.\nWe further show that TAR can be applied to LLM based translation systems as well as traditional NMT systems, in both cases resulting in better constraint accuracy while maintaining translation quality and the technology is not limited to particular LLMs. More generally, our study sheds light on the importance of accurate feedback in general for LLM revision to work effectively."}, {"title": "7 Limitations", "content": "While we have demonstrated TAR's efficacy across four constrained translation datasets, real-world applications are considerably more varied, our prompts might not always yield optimal outcomes. In fact, the essence of TAR lies in its revision mechanism. However, as emphasized in Section 3.3, detecting constraint adherence using LLMs poses challenges. Rule-based methods, though effective in offering accurate feedback to the reviser, can falter in broader constraint scenarios, such as controlled text generation demanding specific stylistic alignment. In such contexts, devising a method to secure accurate and efficient feedback to guide model revisions remains a research imperative. We believe that overcoming these challenges will solidify TAR's standing as a universally effective framework across diverse constraint scenarios."}]}