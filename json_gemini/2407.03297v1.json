{"title": "IMPROVED NOISE SCHEDULE FOR DIFFUSION TRAINING", "authors": ["Tiankai Hang", "Shuyang Gu"], "abstract": "Diffusion models have emerged as the de facto choice for generating visual signals. However, training a single model to predict noise across various levels poses significant challenges, necessitating numerous iterations and incurring significant computational costs. Various approaches, such as loss weighting strategy design and architectural refinements, have been introduced to expedite convergence. In this study, we propose a novel approach to design the noise schedule for enhancing the training of diffusion models. Our key insight is that the importance sampling of the logarithm of the Signal-to-Noise ratio (log SNR), theoretically equivalent to a modified noise schedule, is particularly beneficial for training efficiency when increasing the sample frequency around log SNR = 0. We empirically demonstrate the superiority of our noise schedule over the standard cosine schedule. Furthermore, we highlight the advantages of our noise schedule design on the ImageNet benchmark, showing that the designed schedule consistently benefits different prediction targets.", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion models have emerged as a pivotal technique for generating visual signals across diverse domains, such as image synthesis (Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022) and video generation (Brooks et al., 2024). They are particularly adept at approximating complex distributions, where Generative Adversarial Networks (GANs) may encounter difficulties. Despite the substantial computational resources and numerous training iterations required for convergence, improving the training efficiency of diffusion models is essential for their application in large-scale scenarios, such as high-resolution image synthesis and video generation.\nArchitectural enhancements offer a promising path to improve both the training speed and performance of diffusion models. For instance, the use of Adaptive Layer Normalization (Gu et al., 2022), when combined with zero initialization in the Transformer architecture as demonstrated by Peebles & Xie (2023), represents such an improvement. Similarly, the adoption of U-shaped skip connections within Transformers, as outlined in previous works (Hoogeboom et al., 2023; Bao et al., 2022; Crowson et al., 2024), also boosts efficiency. In a parallel development, Karras et al. (2024) have contributed to this endeavor by reengineering the layers of ADM UNet (Dhariwal & Nichol, 2021) to preserve the magnitudes of activations, weights, and updates, ensuring a more efficient learning process.\nConcurrently, various loss weighting designs have been implemented to accelerate the convergence of training. Previous works, such as eDiff-I (Balaji et al., 2022) and Min-SNR (Hang et al., 2023), found that the training of diffusion models may encounter conflicts among various noise intensities. Choi et al. (2022) prioritize specific noise levels during training to enhance learning of visual concepts. Min-SNR (Hang et al., 2023) reduces weights of noisy tasks, pursuing the Pareto Optimality in different denoising tasks, validated its effectiveness on multiple datasets and architectures. A softer version of this approach, aiming to further enhance high-resolution image synthesis within hourglass diffusion models, was introduced by Crowson et al. (2024). SD3 (Esser et al., 2024) empirically found that it's crucial to increase the weight of the intermediate noise intensities, which has demonstrated the effectiveness during training the diffusion models."}, {"title": "2 METHOD", "content": "2.1 PRELIMINARIES\nDiffusion models (Ho et al., 2020; Yang et al., 2021) learn to generate data by iteratively reversing the diffusion process. We denote the distribution of data points as $x \\sim P_{data}(x)$. The diffusion process progressively adds noise to the data, which is defined as:\n$x_t = \\alpha_t x + \\sigma_t \\epsilon$, where $\\epsilon \\sim N(0, I)$,\nwhere $\\alpha_t$ and $\\sigma_t$ are the coefficients of the adding noise process, essentially representing the noise schedule. For the commonly used prediction target velocity: $v_t = \\alpha_t \\epsilon - \\sigma_t x$ (Salimans & Ho, 2022), the diffusion model $\\theta$ is trained through the Mean Squared Error (MSE) loss:\n$L(\\theta) = \\mathbb{E}_{x \\sim p_{data}(x)} \\mathbb{E}_{t \\sim p(t)} [w(t)||v_{\\theta}(\\alpha_t x + \\sigma_t \\epsilon, t, c) - v_t||^2]$,\nwhere $w(t)$ is the loss weight, c denotes the condition information. Common practices sample t from the uniform distribution U[0, 1]. Kingma et al. (2021) introduced the Signal-to-Noise ratio as\n$SNR(t) = \\frac{\\alpha_t^2}{\\sigma_t^2}$ to measure the noise level of different states. To simplify, we denote $\\lambda$ = log SNR to"}, {"title": "2.2 IMPROVED NOISE SCHEDULE DESIGN", "content": "Given that the timestep t is a random variable sampled from uniform distribution, the noise schedule implicitly defines the distribution of importance sampling on various noise levels. The sampling probability of noise intensity $\\lambda$ is:\n$p(\\lambda) = p(t) |\\frac{dt}{d\\lambda}|$\nConsidering that t satisfies uniform distribution, and $\\lambda$ is monotonically decreasing with t, we have:\n$p(\\lambda) = |\\frac{dt}{d\\lambda}|$\nWe take cosine noise schedule (Nichol & Dhariwal, 2021) as an example, where $\\alpha_t = cos(\\frac{\\pi t}{2})$, $\\sigma_t = sin(\\frac{\\pi t}{2})$. Then we can deduce that $\\lambda = -2logtan(\\pi t/2)$ and $t = \\frac{2}{\\pi}arctan e^{-\\lambda/2}$. Thus the distribution of $\\lambda$ is: $p(\\lambda) = -dt/d\\lambda = sech(\\lambda/2)/2\\pi$. This derivation illustrates the process of obtaining $p(\\lambda)$ from a noise schedule $\\lambda(t)$. On the other hand, we can derive the noise schedule from the sampling probability of different noise intensities $p(\\lambda)$. By integrating Equation 4, we have:\n$t=1-\\int_{-\\infty}^{\\lambda} p(\\lambda) d\\lambda = P(\\lambda)$,\n$\\lambda = P^{-1}(t)$,\nwhere P($\\lambda$) represents the cumulative distribution function of $\\lambda$. Thus we can obtain the noise schedule $\\lambda$ by applying the inverse function $P^{-1}$. In conclusion, during the training process, the importance sampling of varying noise intensities essentially equates to the modification of the noise schedules."}, {"title": "2.3 UNIFIED FORMULATION FOR DIFFUSION TRAINING", "content": "VDM++ (Kingma & Gao, 2023) proposes a unified formulation that encompasses recent prominent frameworks and loss weighting strategies for training diffusion models, as detailed below:\n$L_w(\\theta) = \\mathbb{E}_{x \\sim D,\\epsilon \\sim N(0,I),\\lambda \\sim p(\\lambda)} \\frac{1}{w(\\lambda)}||\\epsilon_{\\theta}(x_{\\lambda}; \\lambda) - \\epsilon||^2$,\nwhere D signifies the training dataset, noise $\\epsilon$ is drawn from a standard Gaussian distribution, and $p(\\lambda)$ is the distribution of noise intensities. Different predicting targets, such as $x_0$ and $v$, can also be re-parameterized to $\\epsilon$-prediction. $w(\\lambda)$ denotes the loss weighting strategy. Although adjusting $w(\\lambda)$ is theoretically equivalent to altering $p(\\lambda)$. In practical training, directly modifying $p(\\lambda)$ to concentrate computational resources on training specific noise levels is more effective than enlarging the loss weight on specific noise levels. Therefore, we focus on how to design $p(\\lambda)$."}, {"title": "2.4 PRACTICAL SETTINGS", "content": "Stable Diffusion 3 (Esser et al., 2024), EDM (Karras et al., 2022), and Min-SNR (Hang et al., 2023; Crowson et al., 2024) find that the denoising tasks with medium noise intensity is most critical to the overall performance of diffusion models. Therefore, we increase the probability of $p(\\lambda)$ when $\\lambda$ is of moderate size, and obtain a new noise schedule according to Section 2.2.\nSpecifically, we investigate four novel noise strategies, named Cosine Shifted, Cosine Scaled, Cauchy, and Laplace respectively. The detailed setting are listed in Table 1. Cosine Shifted use the hyperparameter $\\mu$ to explore where the maximum probability should be used. Cosine Scaled explores how much the noise probability should be increased under the use of Cosine strategy to achieve better results. The Cauchy distribution, provides another form of function that can adjust both amplitude and offset simultaneously. The Laplace distribution is characterized by its mean $\\mu$ and scale $b$, controls both the magnitude of the probability and the degree of concentration of the distribution. These strategies contain several hyperparameters, which we will explore in Section 3.5. Unless otherwise stated, we report the best hyperparameter results.\nBy re-allocating the computation resources at different noise intensities, we can train the complete denoising process. During sampling process, we standardize the sampled SNR to align with the cosine schedule, thereby focusing our exploration solely on the impact of different strategies during training. It is important to note that, from the perspective of the noise schedule, how to allocate the computation resource during inference is also worth reconsideration. We will not explore it in this paper and leave this as future work."}, {"title": "3 EXPERIMENTS", "content": "3.1 IMPLEMENTATION DETAILS\nDataset. We conduct experiments on ImageNet (Deng et al., 2009) with 256 \u00d7 256 and 512 \u00d7 512 resolution. For each image, we follow the preprocessing in Rombach et al. (2022) to center crop and encode images to latents. The shape of compressed latent feature is 32 \u00d7 32 \u00d7 4 for 2562 images and 64 \u00d7 64 \u00d7 4 for 5122 images.\nNetwork Architecture. We adopt DiT-B from Peebles & Xie (2023) as our backbone. We replace the last AdaLN Linear layer with vanilla linear. Others are kept the same as the original implementation.\nTraining Settings. We adopt the Adam optimizer with learning rate 1 \u00d7 10-4. We set the batch size to 256 as Peebles & Xie (2023); Hang et al. (2023). Each model is trained for 500K iterations if not specified. Our implementation is mainly built on OpenDiT (Zhao et al., 2024) and experiments are mainly conducted on 8\u00d716G V100 GPUs.\nBaselines and Metrics. We compare our proposed noise schedule with several baseline settings in Table 2. For each setting, we sample images using DDIM (Song et al., 2021) with 50 steps. Despite the noise strategy for different settings may be different, we ensure they are the same at each sampling step. This approach is adopted to exclusively investigate the impact of the noise strategy during the training phase. Moreover, we report results with different classifier-free guidance scales, and the FID is calculated using 10K generated images."}, {"title": "3.2 COMPARISON WITH BASELINES AND LOSS WEIGHT DESIGNS", "content": "This section details the principal findings from our experiments on the ImageNet-256 dataset, focusing on the comparative effectiveness of various noise schedules and loss weightings in the context of CFG values. Table 3 illustrates these comparisons, showcasing the performance of each method in terms of the FID-10K score.\nThe experiments reveal that our proposed noise schedules, particularly Laplace, achieve the most notable improvements over the traditional cosine schedule, as indicated by the bolded best scores and the blue numbers representing the reductions compared to baseline's best score of 10.85.\nWe also provide a comparison with methods that adjust the loss weight, including Min-SNR and Soft-Min-SNR. We find that although these methods can achieve better results than the baseline, they are still not as effective as our method of modifying the noise schedule. This indicates that deciding where to allocate more computational resources is more efficient than adjusting the loss weight. Compared with other noise schedules like EDM (Karras et al., 2022) and Flow (Lipman et al., 2022), we found that no matter which CFG value, our results significantly surpass theirs under the same training iterations."}, {"title": "3.3 ROBUSTNESS ON DIFFERENT PREDICTING TARGETS", "content": "We evaluate the effectiveness of our designed noise schedule across three commonly adopted prediction targets: $\\epsilon$, $x_0$ and v. The results are shown in Table 4.\nWe observed that regardless of the prediction target, our proposed Laplace strategy significantly outperforms the Cosine strategy. It's noteworthy that as the Laplace strategy focuses the computation on medium noise levels during training, the extensive noise levels are less trained, which could potentially affect the overall performance. Therefore, we have slightly modified the inference strategy of DDIM to start sampling from $t_{max}$ = 0.99."}, {"title": "3.4 ROBUSTNESS ON HIGH RESOLUTION IMAGES", "content": "To explore the robustness of the adjusted noise schedule to different resolutions, we also designed experiments on Imagenet-512. As pointed out by Chen (2023), the adding noise strategy will cause more severe signal leakage as the resolution increases. Therefore, we need to adjust the hyperparameters of the noise schedule according to the resolution."}, {"title": "3.5 ABLATION STUDY", "content": "We conduct an ablation study to analyze the impact of hyperparameters on various distributions of $p(x)$, which are enumerated below.\nLaplace distribution is easy to implement and we adjust the scale to make the peak at the middle timestep. We conduct experiments with different Laplace distribution scales $b \\epsilon$ {0.25, 0.5, 1.0, 2.0, 3.0}. The results are shown in Figure 3. The baseline with standard cosine schedule achieves FID score of 17.79 with CFG=1.5, 10.85 with CFG=2.0, and 11.06 with CFG=3.0"}, {"title": "4 RELATED WORK", "content": "EFFICIENT DIFFUSION TRAINING\nGenerally speaking, the diffusion model uses a network with shared parameters to denoise different noise intensities. However, the different noise levels may introduce conflicts during training, which makes the convergence slow. Min-SNR (Hang et al., 2023) seeks the Pareto optimal direction for different tasks, achieves better convergence on different predicting targets. HDiT (Crowson et al., 2024) propose a soft version of Min-SNR to further improve the efficiency on high resolution image synthesis. Stable Diffusion 3 (Esser et al., 2024) puts more weight on the middle timesteps by multiplying the distribution of logit normal distribution. On the other hand, architecture modification is also explored to improve diffusion training. DiT (Peebles & Xie, 2023) proposes adaptive Layer Normalization with zero initialization to improve the training of Transformer architectures. A more robust ADM UNet with better training dynamics is proposed in EDM2 (Karras et al., 2024) by preserving activation, weight, and update magnitudes.\nNOISE SCHEDULE DESIGN FOR DIFFUSION MODELS\nThe design of the noise schedule plays a critical role in training diffusion models. In DDPM, Ho et al. (2020) propose linear schedule for the noise level, which is later used in Stable Diffusion (Rombach et al., 2022) version 1.5 and 2.0. iDDPM (Nichol & Dhariwal, 2021) introduces a cosine schedule aimed at bringing the sample with the highest noise level closer to pure Gaussian noise. EDM (Karras et al., 2022) proposes a new continuous framework and make the logarithm of noise intensity sampled from a Gaussian distribution. Flow matching with optimal transport (Lipman et al., 2022; Liu et al., 2022) linearly interpolates the noise and data point as the input of flow-based models. Chen (2023) underscored the need for adapting the noise schedule according to the token length, and several other works (Lin et al., 2024; Tang et al., 2023) emphasize that it's important to prevent signal leakage in the final step."}, {"title": "5 CONCLUSION", "content": "In this technical report, we present a novel method for enhancing diffusion model training by redefining the noise schedule. We theoretically analyzed that this approach equates to performing importance sampling on the noise. Empirical results show that our proposed Laplace noise schedule, focusing computational resources on mid-range steps, yields superior performance compared to the adjustment of loss weights under constrained budgets. This study not only contributes significantly to developing efficient training techniques for diffusion models but also offers potential for future large-scale applications."}, {"title": "APPENDIX A: DETAILED IMPLEMENTATION FOR NOISE SCHEDULE", "content": "We provide a simple PyTorch implementation for the Laplace noise schedule and its application in training. This example can be adapted to other noise schedules, such as the Cauchy distribution, by replacing the laplace_noise_schedule function. The model accepts noisy samples $x_t$, timestep t, and an optional condition tensor c as inputs. This implementation supports prediction of {$X_0$, v, $\\epsilon$}."}, {"title": "APPENDIX B: DETAILS FOR SAMPLING PROCESS", "content": "As we mentioned before, choosing which noise schedule for sampling worth exploration. In this paper, we focus on exploring what kind of noise schedule is needed for training. Therefore, we adopted the same inference strategy as the cosine schedule to ensure a fair comparison. Specifically, first we sample {$t_0, t_1,...,t_s$} from uniform distribution U[0, 1], then get the corresponding SNRs from Cosine schedule: {$\\frac{\\alpha_{t_0}}{\\sigma_{t_0}}$,$\\frac{\\alpha_{t_1}}{\\sigma_{t_1}}$,...,$\\frac{\\alpha_{t_s}}{\\sigma_{t_s}}$}. According to Equation 6, we get the corresponding {$t'_0, t'_1,..., t'_s$} by inverting these SNR values through the respective noise schedules. Finally, we use DDIM (Song et al., 2021) to sample with these new calculated {$t'$}"}]}