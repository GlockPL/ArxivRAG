{"title": "Unlocking Real-Time Fluorescence Lifetime Imaging: Multi-Pixel Parallelism for FPGA-Accelerated Processing", "authors": ["Ismail Erbas", "Aporva Amarnath", "Vikas Pandey", "Karthik Swaminathan", "Naigang Wang", "Xavier Intes"], "abstract": "Fluorescence lifetime imaging (FLI) is a widely used technique in the biomedical field for measuring the decay times of fluorescent molecules, providing insights into metabolic states, protein interactions, and ligand-receptor bindings. However, its broader application in fast biological processes, such as dynamic activity monitoring, and clinical use, such as in guided surgery, is limited by long data acquisition times and computationally demanding data processing. While deep learning has reduced post-processing times, time-resolved data acquisition remains a bottleneck for real-time applications. To address this, we propose a method to achieve real-time FLI using an FPGA-based hardware accelerator. Specifically, we implemented a GRU-based sequence-to-sequence (Seq2Seq) model on an FPGA board compatible with time-resolved cameras. The GRU model balances accurate processing with the resource constraints of FPGAs, which have limited DSP units and BRAM. The limited memory and computational resources on the FPGA require efficient scheduling of operations and memory allocation to deploy deep learning models for low-latency applications. We address these challenges by using STOMP, a queue-based discrete-event simulator that automates and optimizes task scheduling and memory management on hardware. By integrating a GRU-based Seq2Seq model and its compressed version, called Seq2SeqLite, generated through knowledge distillation, we were able to process multiple pixels in parallel, reducing latency compared to sequential processing. We explore various levels of parallelism to achieve an optimal balance between performance and resource utilization. Our results indicate that the proposed techniques achieved a 17.7x and 52.0x speedup over manual scheduling for the Seq2Seq model and the Seq2SeqLite model, respectively.", "sections": [{"title": "1 Introduction", "content": "In the biomedical field, non-invasive imaging techniques that effectively capture dynamic cellular processes are crucial for advancing disease detection, monitoring treatment responses, and developing new therapies. Fluorescence lifetime imaging (FLI) has become a valuable tool in this context, providing detailed insights into cellular and molecular activities by measuring the decay times of fluorescent signals [25]. Unlike traditional intensity-based fluorescence imaging, FLI remains unaffected by variables such as fluorophore concentration and excitation intensity. This makes FLI highly suitable for investigating complex biological phenomena, including protein-protein interactions and ligand-receptor binding in intact organisms, especially in cancer imaging [11, 26, 29]. This work aims to bridge the gap between the computational demands of advanced FLI data analysis and the practical limitations of existing hardware platforms used for deployment. By leveraging efficient scheduling techniques and optimizing resource allocation, we strive to make real-time FLI more accessible for clinical and research applications. This work thus lays the foundation for implementing deep learning models on hardware-constrained devices for biomedical applications, moving us closer to the goal of real-time computational imaging solutions that can keep pace with rapid biological processes."}, {"title": "2 FLI time-series estimation", "content": "We implemented the GRU-based sequence-to-sequence (Seq2Seq) architecture from [21] for fast estimation of SDF sequences from TPSF in FLI (Fig. 1). This model processes the TPSF time-series input for each pixel and outputs the corresponding SDF sequence, enabling efficient real-time analysis without explicit deconvolution or prior knowledge of the IRF. This deep Seq2Seq model consists of an encoder and a decoder layers, each comprising two GRU cells with 128 hidden units. The initial hidden states of all GRU cells are set to zero vectors of size 128, for the encoder, while in the decoder, hidden states are initialized based on the final hidden states of the encoder GRU cells.\nIn the encoder, the first GRU cell processes the input TPSF sequence $(x_1, x_2,..., x_T)$, where T is the number of time gates (70 in this case). At each time step t, it updates its hidden state $h_t^{(1)}$ using the previous hidden state $h_{t-1}^{(1)}$ and the input $x_t$:\n$z_t^{(1)} = \\sigma (W_z^{(1)} x_t + U_z^{(1)} h_{t-1}^{(1)} + b_z^{(1)}),$ (1)\n$r_t^{(1)} = \\sigma (W_r^{(1)} x_t + U_r^{(1)} h_{t-1}^{(1)} + b_r^{(1)}),$ (2)\n$\\tilde{h}_t^{(1)} = \\tanh (W^{(1)} x_t + U^{(1)} (r_t^{(1)} \\odot h_{t-1}^{(1)}) + b^{(1)}),$ (3)\n$h_t^{(1)} = (1 - z_t^{(1)}) \\odot h_{t-1}^{(1)} + z_t^{(1)} \\odot \\tilde{h}_t^{(1)},$ (4)"}, {"title": "3 Description", "content": "In this section, we detail our FPGA implementation and our parallel execution method utilizing a scheduler adapted for FPGAs."}, {"title": "3.1 FPGA Implementation", "content": "For efficient memory utilization, the available BRAM was divided into three distinct memory types: constant memory, shared memory, and data memory.\nConstant Memory. Constant Memory stores the model weights. For the Seq2Seq model, the weights consist of $W\\in R^{1 \\times 128}$ for the first GRU cell and $W \\in R^{128 \\times 128}$ for the second GRU cell. The matrices $U \\in R^{128 \\times 128}$ represent the recurrent weights for all GRU cells, and the biases are represented by vectors $b \\in R^{1 \\times 128}$. In the Seq2SeqLite model, the weights are reduced: $W \\in R^{1 \\times 32}$ for both the encoder and decoder GRU layers, $U \\in R^{32 \\times 32}$ for all cells, and $b\\in R^{1x32}$.\nShared Memory. Shared Memory stores the hidden state vectors of the GRU cells. For the Seq2Seq model, the hidden states are $h\\in R^{1x128}$ for both the encoder and decoder's first and second GRU cells. In the Seq2SeqLite model, the hidden state vectors are reduced to $h\\in R^{1x32}$ for both encoder and decoder GRU cells.\nData Memory. Data Memory is used for temporary storage of intermediate results generated during the computations within the GRU cells. This memory facilitates the calculation of the hidden layers by caching intermediate operations, such as the candidate hidden states and gate activations during the GRU computations.\nThe segmentation of memory resources, as described, allows for efficient parallel processing and reduces memory access contention, thereby optimizing the overall system performance on the FPGA."}, {"title": "3.2 Exploiting Pixel-Level Parallelism through Scheduling", "content": "Time-series data, unlike traditional image processing, presents opposing data flow patterns. Each image taken per time step is to be processed sequentially and cannot be batched and executed in parallel. However, time-series calculation allows for pixel-level parallelism. On the top, we see an image is captured every 40 ns. To compute this time series data, we have to compute each particular pixel across images captured in time sequentially (the blue tower in the bottom left). Each time step of this pixel represents a temporal point in its TPSF, which is sequentially processed through the encoder and decoder GRU cells. This process is inherently sequential because different regions of the TPSF contain information about the various fluorophores present in the sample. However, across the x and y dimensions, we can process the data in parallel to exploit spatial independence. We exploit this parallelism to improve our FLI calculation time while significantly improving FPGA utilization.\nTo further demonstrate our motivation for exploiting pixel-level parallelism, we compare the utilization of one of the DSPs on the FPGA when we execute one pixel at a time vs. two pixels in parallel. As shown in Fig. 3 (top), when one pixel's data is computed through the encoder it utilizes 61.5% of DSP 0 and 1.8% of all the DSPs (not shown). As the number of pixels are processed in parallel we can improve the utilization of the DSPs and the BRAMs. An example of this can be seen in Fig. 3 (bottom) for the encoder simultaneously operating on 2 pixels, achieving a utilization of 69.6% for DSP 0 and 3.5% for all DSPs (not shown). We can see that some of the underutilized slots on the DSP are now utilized to compute for the second pixel.\nFor this, our methodology utilizes a modified static scheduling and mapping policy that is built to improve utilization of the FPGA for better performance. We adapt STOMP [24], a tool for discrete event simulations in multicore processors, to optimize scheduling and data processing in resource-constrained FPGAs. For this, we"}, {"title": "4 Methodology", "content": "Knowledge Distillation and Seq2SeqLite Model Training. We followed the same training process outlined in [13], the Seq2Seq model acts as the teacher, and the Seq2SeqLite model serves as the student. The student model is trained to minimize a combined loss function, incorporating both standard task loss and distillation loss, to ensure it learns the behaviors of the larger model while retaining efficiency. Furthermore, QAT was employed to simulate 8-bit precision during training. This allows the Seq2SeqLite model to adapt to reduced precision computations, ensuring minimal accuracy degradation after quantization. QAT prepares the model for the resource constraints of FPGA deployment, where low-precision arithmetic significantly reduces power consumption and memory usage. The model was trained using the Adam optimizer with a learning rate of 0.001 and Mean Squared Error (MSE) as a loss function. The model was implemented using Tensorflow [1] and trained on a system with an Intel i9-11900KF CPU and a Nvidia RTX 3090 GPU.\nFPGA Specifications for Deployment. The XEM7360-K410T FPGA, based on the Xilinx Kintex-7 XC7K410T, was selected due to its compatibility with the time-resolved SPAD array camera and its ability to meet the specific computational and memory requirements of the models. The FPGA includes 1540 DSP slices, which handle the arithmetic operations necessary for matrix multiplications and other calculations in the GRU layers. These slices allow for the efficient execution of the mathematical operations required for recurrent layers during inference. Additionally, the FPGA has 795 Block RAM (BRAM) tiles, used to store intermediate data such as activations, weights, and hidden states. Given the sequential nature of the GRU-based model, where hidden states need to be passed across time steps, the BRAM tiles were crucial for managing memory during the execution of the model, allowing for efficient data flow between the encoder and decoder. These specifications provided the necessary resources for deploying the quantized Seq2SeqLite model, which"}, {"title": "5 Empirical Evaluation", "content": "In this section, we evaluate the effectiveness of our scheduling methodology for parallel pixel processing on FPGAs, comparing it to a manually scheduled single pixel execution. We assess both the Seq2Seq model and its compressed and quantized variant, Seq2SeqLite, to demonstrate improvements in terms of execution time and hardware utilization. For our evaluation, we utilized 128 DSPs, 128 BRAMs for the constant memory, 128 BRAMs for the shared memory and 256 BRAMs for the data memory.\nAs shown in Fig. 4, the proposed parallel execution strategy significantly accelerates the processing of FLI tasks. For the Seq2Seq model, we are able to process 64 pixels in parallel for both the encoder and decoder stages. This allows us to achieve a maximum speedup of 17.1\u00d7 and 17.5\u00d7 over the manual schedule for the encoder and decoder layers, respectively. The Seq2SeqLite model showed even greater improvements. We are able to execute 256 pixels in parallel and achieve speedups of 42.8\u00d7 and 53.9\u00d7, respectively. Furthermore, when we schedule pixels in parallel for the post processing step (see Fig. 5(a)), we are able to achieve a speedup 160.6x for 512 parallel pixels over single pixel execution. Also note that processing any more pixels in parallel doesn't provide more speedup as the hardware utilization is saturated.\nAs shown in Fig. 5(b), our parallel pixel execution approach yields a speedup of 17.7\u00d7 for the Seq2Seq model, while the reduced and quantized Seq2SeqLite model alone provides a speedup of 4.5x. However, neither of these methods individually is sufficient to"}, {"title": "6 Related Work", "content": "Numerous approaches [2, 4, 7-10, 12, 13, 15, 18, 19, 21-23, 27] have been proposed for time-series estimation across different fields. Earlier methods focused on statistical models like ARIMA [17], followed by the adoption of RNNs, such as LSTMs and GRUs, due to their ability to capture long-term dependencies in sequential data [4, 8]. Recently, transformer-based architectures have gained popularity for time-series applications. With their self-attention mechanism, transformers excel in capturing relationships over long sequences [23]. However, they require a large number of parameters, often in the range of millions, making them less suitable for resource-constrained environments, such as FPGAs or edge devices [19, 27].\nGiven the challenges of deploying large transformer models in low-resource settings, GRUs have been used as a more practical alternative. GRUs offer a balance between model complexity and performance by reducing the number of parameters compared to LSTMs and transformers while still providing robust time-series estimation [7]. For instance, GRU-based encoder-decoder architectures have been applied to handle time-series data in various domains, showing their suitability for FPGA implementations [2, 9, 13, 18]. This makes GRU models particularly advantageous for applications requiring efficient use of limited computational and memory resources, such as real-time FLI. In a recent study [18], researchers designed a compact FLI system based on Piccolo SPAD sensors and implemented a GRU-based recurrent model on an FPGA platform for real-time lifetime parameter estimation. This approach achieved significant reductions in latency and data bandwidth, making real-time FLI feasible. Similarly, another study [13] focuses on compressing GRU neural networks for real-time FLI on FPGAs. To address the resource constraints of FPGAs, the authors employ compression techniques such as post-training quantization, quantization-aware training, and KD. These techniques result in a smaller model, called Seq2SeqLite, which reduces the model size significantly while maintaining accuracy. The compressed model is tested on experimental data, achieving real-time performance with 8-bit precision, demonstrating that GRU-based models can improve the performance of FLI for clinical and research applications. However, none of these prior work explore parallel pixel execution approach on time-series data while utilizing an optimized scheduling policy to perform FLI estimation on a resource-constrained FPGA to achieve real-time execution."}, {"title": "7 Conclusion", "content": "In this work, we have demonstrated the feasibility of achieving real-time FLI on resource-constrained FPGAs by leveraging efficient scheduling techniques and pixel-level parallelism. Our approach significantly improves DSP and BRAM utilization, allowing multiple pixels to be processed in parallel, thereby reducing latency and enabling real-time performance. By integrating a GRU-based Seq2Seq model and its compressed counterpart, Seq2SeqLite, we optimized the FPGA's limited resources, achieving significant speedups through automated scheduling on parallel pixels.\nOur results show a substantial improvement in the execution time of both the encoder and decoder stages, achieving 17.7\u00d7 and 52.0x speedup for real-time FLI estimation for a 299k and 7k parameter models, respectively. The combination of deep learning model compression, quantization-aware training, and parallel pixel execution has proven highly effective for this application. This research provides a critical step toward making real-time FLI more accessible for clinical and biomedical applications, such as guided surgery and dynamic biological process monitoring.\nFuture work will explore further optimizations in FPGA resource management, such as LUT-based execution, and alternative memory distribution strategies to enhance performance and allow for processing even more pixels in parallel. These enhancements will continue to push the boundaries of real-time computational imaging on constrained hardware platforms, ultimately achieving video-level performance, making it suitable for clinical implementation."}]}