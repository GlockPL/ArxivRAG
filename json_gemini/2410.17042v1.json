{"title": "DEEP MEMORY SEARCH: A METAHEURISTIC APPROACH FOR OPTIMIZING HEURISTIC SEARCH", "authors": ["Abdel-Rahman Hedar", "Alaa E. Abdel-Hakim", "Wael Deabes", "Youseef Alotaibi", "Kheir Eddine Bouazza"], "abstract": "Metaheuristic search methods have proven to be essential tools for tackling complex optimization challenges, but their full potential is often constrained by conventional algorithmic frameworks. In this paper, we introduce a novel approach called Deep Heuristic Search (DHS), which models metaheuristic search as a memory-driven process. DHS employs multiple search layers and memory- based exploration-exploitation mechanisms to navigate large, dynamic search spaces. By utilizing model-free memory representations, DHS enhances the ability to traverse temporal trajectories without relying on probabilistic transition models. The proposed method demonstrates significant improvements in search efficiency and performance across a range of heuristic optimization problems.", "sections": [{"title": "1 Introduction", "content": "Heuristics search is a method to find better optimal solutions at a reasonable computational cost without guaranteeing optimality or feasibility. Metaheuristics algorithms are the set of intelligent strategies used to improve the heuristic procedures efficiency. Metaheuristics algorithms can be considered as an iterative master process used to guide and modify the subordinate heuristics operations to produce high-quality solutions efficiently Rashedi et al. [2018], Huang"}, {"title": "2 Modeling Metaheuristics as a Memory Search", "content": "Metaheuristic search can be conceptualized as a memory-based optimization process. By treating the search procedure from a memory-centric perspective, it is possible to dramatically enhance the effectiveness of the search strategy, especially when applied to large and complex search spaces. These spaces are not only vast in spatial dimensions but also in temporal depth. Traditional MDPs typically model temporal trajectories using probabilistic transition systems; however, such models often fail to capture the dynamics of systems that are not governed by explicit state transitions.\nThis necessitates the development of \"model-free\" memory representations that can traverse temporal spaces without relying on predefined transition models. One prominent example of such an approach is Reinforcement Learning (RL),"}, {"title": "3 Framework: Deep Heuristic Search", "content": "The Deep Heuristic Search (DHS) is a generic search approach that can extend metaheuristics to perform advanced search. DHS comprises three main components, which are illustrated in Fig. 1. These components can be summarized as follows:\n\u2022 Integrated Search Strategies: invokes different and multiple search strategies including intensification, diversification and hybridization, as well as search restarting strategies.\n\u2022 Variant Depth Operations: define search operations to have multiple levels of implementation according to the search strategies. Each search operation has three implementation levels; expand-mode, normal-mode, and condense-mode. The first and last modes works on expanding or refining the search according to the search strategy used, whether it is diversification or intensification, respectively. Similar evolution strategies to this proposed pace-adjustable coarse-to-fine paradigm had succeeded in machine learning approaches, e.g. gradient descent for supervised learning Bottou [1998] and adaptive clustering for unsupervised learning Hedar et al. [2018a,b].\n\u2022 Multi-Depth Memory: constructs different memory elements that keep track of historical featured search data along the time dimension. This temporal depth enables the search process to implement the search operations with guidance of recent or historical featured search data according to the search strategies.\nDeep memory is the core element of DHS. The other components are highly dependent on the operation of the deep memory. Therefore, the operation of deep memory is explained before the other components, as shown in the following subsection."}, {"title": "3.1 Multi-Depth Memory", "content": "The memory on DHS spans over a two-dimensional space. Traversing along the first dimension provides different degrees of diversity to DHS. This dimension expresses various search features including elitism, visit frequency, characteristics, spatiality and/or recentness.\nThe second dimension gives the DHS memory its deepness property. Mainly, this dimension represents the temporal search history whose controllable depth that is application-dependent. Memory temporal search gives the flexibility to control the application of both local search and global search strategies. Particularly, the DHS memory comprises two levels in terms of depth. The first is a deep one, which is concerned with aged search results. Therefore, this deep memory turns out to serve a global search for featured solutions over the entire search history. The other memory level is a shallow structure, which remembers only recently-visited featured solutions or those which were visited during the current search stage. Fig. 2 shows how the deep and the shallow structures are constructed in the elite DHS memory. This memory elements collects predefined numbers of best obtained solutions as explained later. In the deep memory structure, the $N_d$ best solutions which are globally-reached by the overall search are stored with their objective function values. The shallow memory structure preserves the $N_s$ best recently-visited solutions. Practically, $N_d$ and $N_s$ are selected such that $N_d > N_s$.\nThe depth of memory elements consists of two dimensions, one dimension represents diversity and the other does for the time. Therefore, in order to maintain the diversity, DHS memorizes different search features including elitism, visit frequency, characteristics, spatiality and/or recentness. Moreover, in order to control the application of both local search and global search strategies, the DHS creates a time depth in memory. In other words, each memory element can have two structures, one deep and another shallow one. The deep structure memorizes the search featured solutions globally over the entire search history, while the shallow structure only remembers recently visited featured solutions or those which were visited in the current search stage. For example, in Fig. 2, assume $N_d = 10$ and $N_s = 5$. In this case, the construction of deep and shallow structures in the elite memory are illustrated. In the deep memory structure, the ten best solutions obtained overall the search are stored with their function values. On the other side, the shallow memory structure, the only five best recently-visited solution are stored in this memory. To control the temporal depth in the shallow memory structure, the depth index $(d_i, i = 1, . . ., 5)$ is used to track the number of iterations or generations that have passed since visiting the corresponding solutions. To prevent any sudden drop in the number of elements in shallow memory, the extended shallow memory is used to preserve the best next elements in the shallow memory.\nFor the diversity dimension, we adopt several diversity approaches. In the next subsections, we discuss the most important types of these diversity methodologies."}, {"title": "3.1.1 Elitism", "content": "In this type of memory, the elite solutions are stored based on their objective function values. Therefore, the best reached solutions at a moment are stored in the deep elite memory. The best-visited solutions in the last $(d_e)$ iterations/generations or in the current search stage are stored in the shallow elite memory, as well as its extended memory."}, {"title": "3.1.2 Visit Frequency", "content": "The accumulated number of solutions or region visits is stored in the deep frequent memory. On the other side, the shallow frequent memory and its extended peer store only the number of visits in the last $(d_f)$ iterations/generations or in the current search stage."}, {"title": "3.1.3 Characteristics", "content": "Some characteristics of featured solutions can be saved to guide the search process. For example, connected sub-graphs can help in search for connected dominating sets in network applications. Therefore, storing the best connected solutions can direct the research process towards finding better solutions. Therefore, the deep characteristic memory stores the best featured solutions obtained so far based on selected characteristics. The shallow characteristic memory and its extended memory only store the best featured solutions obtained in the last $(d_c)$ iterations/generations or in the current search stage."}, {"title": "3.1.4 Spatiality", "content": "The search space can be sampled or partitioned and some landmarks inside it or in its partitions can be stored. These spatial data of the search space helps introducing new and diverse solutions. Moreover, generating solutions in some applications needs composing solutions that covers different regions of the search space. The deep spatial memory stores the space landmark data and their visiting times. The shallow spatial memory and its extended memory only store the search data for the visited landmarks in the last $(d_s)$ iterations/generations or in the current search stage."}, {"title": "3.1.5 Recentness", "content": "A tabu list is constructed to contain the $(d_r)$ last visited solutions. This list keeps the DHS aware of the most recently visited solutions. Therefore, the unnecessary, yet exhausting irrelevant, return to those solutions is avoided."}, {"title": "3.2 Pace Adjuster", "content": "Within the operation of the proposed DHS, a traditional metaheuristics methodology is invoked and directed by the DHS components. DHS has three different modes of operations: normal, expand, and condense modes. In the normal mode, the search operations used in the invoked metaheuristic is applied as its original definitions. If some search strategies are called, then the definitions of the operations should be modified to fulfill the considered search strategies. The expand and condense implementation modes are defined to be used during the diversification and intensification, respectively. More details with illustrating examples about these search modes are given in the following."}, {"title": "3.2.1 Normal-Mode Operations", "content": "Metaheuristics have their own search operations and they are defined to fulfill the invoked optimal search strategies. In this work, three well-known search operations have been considered to illustrate the new concept of deep operations. However, the deep operation concept can be extended to adopt other search operations in different metaheuristics. The considered three search operations are shown in the following.\n\u2022 The arithmetic crossover in genetic algorithms. Given two parents; $x_1$ and $x_2$, then their children can be generated as:\n$y_1 = \\lambda x_1 + (1 - \\lambda)x_2,$\n$y_2 = (1 - \\lambda)x_1 + \\lambda x_2,$\nfor some $\\lambda \\in [0, 1]$.\n\u2022 The self-adaptation mutation in evolution strategies. A mutated child $(y, \\theta)$ can be obtained from a parent $(x, \\sigma)$, using the following equations, which assigns the i-th component of the mutated child as:\n$\\theta_i = \\sigma \\cdot e^{\\tau' N(0,1) + \\tau N_i(0,1)},$\n$y_i = x_i + \\theta_i N_i(0,1),$"}, {"title": "3.2.2 Expand-Mode Operations", "content": "In the DHS framework, Expand-Mode operations are defined to stretch the search regions of interest, and hence allow generating more diverse solutions in order to fulfil a wider diversification process. For instance, this mode can be applied for the aforementioned considered operations as follows:\n\u2022 Arithmetic crossover: set the parameter $\\sigma > \\sigma^*$, where $\\sigma^*$ is the value of the parameter if the normal-mode is implemented.\n\u2022 Self-Adaptation Mutation: set the parameter $\\lambda \\in [-1, 1]$ instead of setting it in [0, 1].\n\u2022 Neighborhood Zones: set the neighborhood radius $r > r^*$, where $r^*$ is the value of the radius in the normal-mode."}, {"title": "3.2.3 Condense-Mode Operations", "content": "Condense-Mode operations are used to enable the DHS search process to focus the search process at finer regions in order to fulfil an exhaustive intensification process. The condense-mode of operations can be applied on the aforementioned operations as follows:\n\u2022 Arithmetic crossover: set the parameter $\\sigma < \\sigma^*$, where $\\sigma^*$ is the value of the parameter if the normal-mode is implemented.\n\u2022 Self-Adaptation Mutation: set the parameter $\\lambda \\in [0.5, 1]$ instead of setting it in [0, 1].\n\u2022 Neighborhood Zones: set the neighborhood radius $r < r^*$, where $r^*$ is the value of the radius in the normal-mode."}, {"title": "3.3 Integrated Strategic Search", "content": "The deep search is the main component of the DHS framework which enables five level of search stages. Each search stage has its own strategies to accomplish the search goals. The main layout of the DHS framework is given in Fig. 6 which contains the five deep search stages, which are:\n\u2022 Initial Search\n\u2022 Exploratory Search\n\u2022 Mixed Search\n\u2022 Intensive Search\n\u2022 Final Search\nThe first and last stages are special ones, which seek initializing and finalizing the search process, respectively. The other search stages are extensive diversification and/or intensification search with restarting strategies. Since the DHS extends one or more of metaheuristic methods, then their search strategies are updated to fill up the five DHS search stages."}, {"title": "3.3.1 Initial Search", "content": "The main objective of the Initial Search is to generate an initial solution set and construct the initial memory structures. Therefore, in this search stage special search strategies are invoked to investigate the main features of the search space. Specifically, the Initial Search works on partitioning the search space and clustering solution features as well as initializing the search memory. Then, a large solution set called the gentry is generated to cover different and diverse space partitions and solution clusters. Therefore, the deep spatial memory can help in constructing such partitions and clusters. Finally, an initial solution/population can be selected among the best candidates in the gentry."}, {"title": "3.3.2 Exploratory Search", "content": "In the exploratory search stage, the search operations of the considered metaheuristics are modified to generate more diverse solutions. The main strategies of this search stage is how to perform a quick wide exploration process of the search space and to construct more concrete search memories about this space. Therefore, expand-mode operations are defined to fulfil this strategies. Moreover, a restarting mechanism can be implemented to adeptly configure such expand-mode operations. Some search memory elements can be used to examine the exploratory search progress in order to finish it and move to the next search stage."}, {"title": "3.3.3 Mixed Search", "content": "The search operations of the considered metaheuristics are applied with their normal-mode definitions in the mixed search stage. However, special intensification and diversification procedures can be inlaid whenever promising solutions are detected or the research process fails to find better solutions, respectively. The mixed search can be restarted with new adapted control parameters especially whenever the diversification process suggests new diverse solutions. The deep spatial and frequent memories can define a practical automatic termination criteria to stop this main search stage."}, {"title": "3.3.4 Intensive Search", "content": "In the intensive search stage, the best candidates stored in the deep elite and featured memories are revisited using refiner search process. Therefore, neighborhood regions of these best and featured solutions are searched using condense-mode operations. The search process maybe restarted from the same solution or from a new one with adeptive configuration of the search control parameters."}, {"title": "3.3.5 Final Search", "content": "This final search stage is a special search that collects the final forms of the best reached solutions. Then, the components of these solutions are analyzed in order to generate new better solutions. Such analysis process depends on the problem structure and objective. For example, the presence of similarities in some components of the best solutions can be used as a core for generating new solutions."}, {"title": "4 Conclusion", "content": "The Deep Heuristic Search (DHS) framework offers a novel memory-based approach to solving metaheuristic opti- mization problems. By leveraging multiple search layers and memory-guided exploration-exploitation strategies, DHS"}]}