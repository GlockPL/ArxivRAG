{"title": "ChatBCI: A P300 Speller BCI Leveraging Large Language Models for Improved Sentence Composition in Realistic Scenarios", "authors": ["Jiazhen Hong", "Weinan Wang", "Laleh Najafizadeh"], "abstract": "P300 speller brain computer interfaces (BCIs) allow users to compose sentences by selecting target keys on a graphical user interface (GUI) through the detection of P300 component in their electroencephalogram (EEG) signals following visual stimuli. Most existing P300 speller BCIs require users to spell all or the first few initial letters of the intended word, letter by letter. Consequently, a large number of keystrokes are required to write a sentence, which can be time consuming, increasing user's cognitive load and fatigue. Therefore, there is a need for more efficient and user-friendly methods for faster, and practical sentence composition.\nIn this work, we introduce ChatBCI, a P300 speller BCI that leverages the zero-shot learning capabilities of large language models (LLMs) to suggest words from user-spelled initial letters or predict the subsequent word(s), reducing keystrokes and accelerating sentence composition. ChatBCI retrieves word suggestions through remote queries to the GPT-3.5 API. A new GUI, displaying GPT-3.5 word suggestions as extra keys is designed. Stepwise linear discriminant analysis (SWLDA) is used for the P300 classification.\nSeven subjects completed two online spelling tasks: 1) copy-spelling a self-composed sentence using ChatBCI, and 2) improvising a sentence using ChatBCI's word suggestions. Results demonstrate that in Task 1, on average, ChatBCI outperforms letter-by-letter BCI spellers, reducing time and keystrokes by 62.14% and 53.22%, respectively, and increasing information transfer rate by 198.96%. In Task 2, ChatBCI achieves 80.68% keystroke savings and a record 8.53 characters/min for typing speed. Overall, ChatBCI by employing remote LLM queries enhances sentence composition in realistic scenarios, significantly outperforming traditional spellers without requiring local model training or storage. ChatBCI's (multi-) word predictions, combined with its new GUI, pave the way for developing next-generation speller BCIs that are efficient and effective for real-time communication, especially for users with communication and motor disabilities.", "sections": [{"title": "1. Introduction", "content": "Event-related potentials (ERPs), which appear in electroencephalogram (EEG) signals in response to an external stimulus, have been long utilized in developing P300 speller brain-computer interfaces (BCIs) [1, 2]. These speller BCIs particularly find potential applications in aiding individuals with communication and motor disabilities, such as those with amyotrophic lateral sclerosis (ALS), to express their thoughts and needs.\nThe P300 component, a positive peak in ERP that occurs approximately 300 ms after the onset of stimulus onset, has been considered a reliable signal in speller BCIs [3, 4, 5, 6]. In a classical P300 speller BCI, keys in the graphical user interface (GUI) are arranged in a 6\u00d76 matrix [3], with each row and column in the matrix flashing repeatedly in a random sequence to generate visual stimulations. When the user focuses on a particular key, the flashing of the row and the column containing the desired key is recognized as a deviant stimulus, evoking a P300 response similar to the oddball paradigm [7, 8]. Due to the low signal-to-noise ratio (SNR) of single-trial EEG signals, generally multiple repetitions are required, to confirm a selection.\nOver the past few decades, efforts have been made to improve BCI spellers by enhancing the speed and accuracy of making P300-based key selections. These include optimizing the GUI [9], enhancing flashing patterns [10, 11], and adjusting inter-stimulus intervals (ISIS) [12, 13, 14]. These efforts however, did not target improving the spelling efficiency, as selections were mostly made letter by letter, limiting the usability of speller BCIs for composing long messages due to the fatigue and cognitive load caused by the prolonged focus required to select numerous characters from the keyboard.\nIncorporating natural language processing (NLP) in P300 speller BCIs has been suggested recently to take advantage of NLP's ability to predict and suggest words based on partial input. By learning the linguistic properties of the natural language from a training corpus, statistical language models such as N-Gram or probabilistic automata have been used to estimate the probabilities for the upcoming letter or word, given the partially completed text composed by the user to enhance the spelling process."}, {"title": "2. Methods", "content": "Figure 1 provides an overview of the proposed ChatBCI, which consists of a stimulation computer, a recording computer, and an EEG recoding system. The stimulation computer displays the LLM-integrated GUI and transmits stimulus codes, indicating the sequence of row and column flashes on the GUI, to the recording computer. The recording computer records and processes the acquired EEG signals, and infers the user's selected key by matching the detected P300 ERP with the flashing pattern. The selected key is then sent back to the stimulation computer to update the GUI, and remote LLM queries are conducted to retrieve and update word suggestions displayed on the GUI based on the newly-selected key. In what follows, we describe the GUI, explain how LLM is integrated with the BCI speller, and discuss how key selections are determined via detection of P300 ERP.\n2.1. Graphical User Interface (GUI) of the ChatBCI\nFigure 2 illustrates the GUI developed for the proposed ChatBCI. The GUI has three panels. The main body of the GUI, the keyboard panel (shown within the red box), is a 5\u00d78 matrix of keys, and includes 26 alphabet keys for letters A to Z, four function keys, and two 1 \u00d7 5 columns on the left and right sides to display the 10 suggested candidates that are provided dynamically through GPT-3.5 queries. The function keys provide basic editing functions for the user when composing sentences:\u2018DW\u2019(delete word) removes the last word, whether complete or incomplete, from the current sentence; \u2018DC'(delete character) removes the last character; \u2018Sp' inserts a space; and 'En' indicates the completion of an entry. In the sentence panel (shown within the green box), the\n2.2. Integration with Large Language Models (LLMs)\nPredictive Spelling Through Remote GPT Query: In ChatBCI, predictive spelling suggestions are generated from the partial text spelled by the user, through remote queries to the GPT-3.5-turbo API [30]. Figure 3 illustrates an example of this process, where the user has composed \u201cI-WOULD\u201d as part of the target sentence of \u201cI-WOULD-LIKE-TO-HAVE-WATER", "I-WOULD": "ChatBCI first generates a list of messages for remote GPT query. The messages are formed by filling the current partial text into a manually-engineered prompt template (Figure 3-(b)). To ensure ChatBCI continuously provides helpful suggestions throughout the spelling process [31], we utilize the role-taking functionality of the GPT-3.5-turbo API. The prompt is split into two messages, each assuming a distinct role. Specifically, the first message, in the system role, instructs GPT on the task, the number of predictive typing keys to include, and the required response format. The second message, in the user role, contains the partial text entered by the user.\nAfter sending the query messages to GPT, ChatBCI receives a response string. Manual fine-tuning was employed to design the prompt template in Figure 3-(b), so that most response strings from various partial text inputs contain valid typing suggestions that are separable using a consistent rule, as shown in Figure 3-(c). The string is then split and reformatted into individual word suggestions (Figure 3-(d)), and the BCI is updated with the word suggestions acquired from the current partial text input (Figure 3-(e)). This allows the user to directly select \u201cLIKE\u201d, thereby, significantly saving time and keystrokes compared to typing each letter individually. The entire process (Figure 3) is repeated whenever a letter or word is added to or removed from the partial text, ensuring word suggestions remain updated based on the new partial text.\nWord Completion and Word Prediction: Using the prompt template shown in Figure 3, GPT-3.5 intelligently provides two types of predictive spelling suggestions: word completion when the last word is incomplete, and word prediction when the last word is fully typed. For example, in the text \u201cI-WANT-TO-B\" which ends with an incomplete word, GPT suggests predictions that complete the last word, such as \u201cBE\u201d or \u201cBUY\u201d. On the other hand, for a text such as \u201cI-WOULD\u201dwhere the last word is complete but the sentence is not, GPT suggests predictions for the next word, for example \u201cLIKE\u201d or \u201cWANT\u201d.\nTo integrate both scenarios of word completion and word prediction in ChatBCI, we designed 2 simple rules for updating the composition based on word suggestions, optimizing intuitiveness and usability:\n(i) We define the last word in a partially composed text as all characters after the last space character, if there is at least one space character in the text. Otherwise, the last word is the text.\nFor example, for the text \u201cI\u201d, the last word is \u201cI\u201d; for the text \u201cI-WANT-TO-B\", the last word is \u201cB\u201d; for the text \u201cI-WOULD\u201d, the last word is \u201cWOULD\u201d; and the last word for the text \u201cI-WOULD-\u201d is empty.\n(ii) Whenever a candidate word on the GUI is selected, it always replaces the last word in the partially-composed text, and adds a space character to the end of the text.\nFor example, the result of selecting word \"BUY\" for the text \u201cI-WANT-TO-B\" is \u201cI-WANT-TO-BUY-\u201d (word completion); and the result of selecting word \u201cLIKE\u201d for the text \u201cI-WOULD-\u201dis\u201cI-WOULD-LIKE-\u201d (word prediction).\nEach word selected from the suggestions automatically adds a space to the end of the text and triggers a new GPT query to update the suggestions based on the current text with the newly added word. This allows users to chain new words with just one key selection per word, thereby, significantly reducing keystrokes and speeding up the sentence composition process. Thus, the proposed ChatBCI can continuously provide flexible, dynamic predictive spelling suggestions, offering an effective means for composing long sentences.\n2.3. P300 Detection and Target Character Recognition\nTo achieve P300-based spelling, the P300 component should be first detected from the acquired EEG signals, and then the corresponding target key needs to be identified. Determining the presence or absence of a P300 evoked potential from EEG signals can be treated as a binary classification problem, whereas target character recognition is a multi-class classification problem involving 40 classes corresponding to the 5 \u00d7 8 grid keyboard (see Figure 2). Here, we describe the algorithms used for these two tasks.\nP300 Detection: We considered the Stepwise Linear Discriminant Analysis (SWLDA) classifier for detecting the P300 evoked potential from the EEG signals. SWLDA has shown strong performance in prior P300 speller BCIs [5, 24, 15, 16, 18, 19, 20, 22, 21, 32, 33], making it a good candidate for ChatBCI.\nSWLDA utilizes a stepwise feature selection process, iteratively adding or removing features based on their statistical significance and each feature's individual contribution to the model. This process involves ordinary least squares (OLS) regression for evaluating features. The feature selection begins with forward regression, where OLS estimates model parameters and assesses the significance of each feature. The feature with the lowest p-value is added to the model first. In the forward selection phase, additional features demonstrating a significant and unique contribution to the variance (here with p-values less than 0.1) are included in the model. The subsequent backward elimination phase evaluates whether each included feature continues to contribute significantly, removing those with p-values greater than 0.25. This iterative process of forward selection and backward elimination continues until no further features meet the inclusion or exclusion criteria. The details of the Stepwise Feature Selection algorithm utilized in the SWLDA classifier are provided in the Appendix (Algorithm 1). Features selected by the algorithm are used to calculate the Linear Discriminant Analysis (LDA) score to differentiate between the two classes, corresponding to the presence or absence of P300. We used the LinearDiscriminantAnalysis class implemented in scikit-learn version 1.3.1 [34], with the default Singular Value Decomposition (SVD) solver.\nTarget Character Recognition: As illustrated in Figure 2, the keyboard panel of the GUI consists of 8 columns and 5 rows. We associated each column (left to right) with stimulus codes 1 to 8, and each row (top to bottom) to stimulus codes 9 to 13. The combination of column and row codes is used to identify the target character. For instance, [6, 11] corresponds to the character \u201cQ\u201d. Due to low SNR, we required 8 repetitions to recognize the target character. Each repetition involved 13 flashes, each corresponding to one of the 13 stimulus codes.\nLet $s_i^{(r)}$ denote the LDA score for the stimulus code i (i = 1,\u2026, 13) from the r-th repetition of total of n repetitions. Si is defined as the cumulative sum of classifier scores for stimulus code i from the first to the n-th repetition. A cumulative score Si for each stimulus code across all repetitions (n) is obtained as\n$S_i = \\Sigma_{r=1}^{n} s_i^{(r)}$\nThe P300 response is triggered when the user focuses on a particular column or row flash. Therefore, the target character can be identified from the row and column with the highest cumulative scores as\ntarget column = arg max $S_i$, target row = arg max $S_i$\ni\u2208{1:8}\ni\u2208{9:13}\nThe intersection of the highest-scoring column and row gives the most likely target character, which is then selected.\n2.4. Evaluation Metrics\nThe performance of the proposed ChatBCI was evaluated using a series of metrics. The accuracy was assessed using two measures of \u201cSelection Accuracy\u201d and \u201cSuccess Rate (SR)", "Time to Complete\". The performance was also evaluated using \"Information Transfer Rate\" which takes into account both speed and accuracy. The spelling efficiency was evaluated based on a few measures involving \u201ckeystroke\" analysis. In what follows, we describe these metrics.\nTime to Complete": "This metric represents the time that it takes to complete writing the intended sentence using the speller. A shorter time to complete writing", "Speed": "This metric is defined as the ratio of total characters in the sentence over the time it takes to complete writing the sentence.\nSelection Accuracy: Selection accuracy is calculated as the ratio of correct selections to the total number of selections used to type the sentence. High selection accuracy indicates that fewer attempts were needed for correct spelling.\nSuccess Rate (SR): This metric is calculated as the ratio of correctly-typed characters to the total number of characters in the sentence. A high SR for a BCI", "ITR)": "In letter-by-letter speller BCIs", "35": "nITR = $\\frac{B"}, {"ITR*": "nITR*-1: We consider N = 28", "ChatBCI.\nITR*-2": "We consider N = 28 + M", "Analysis": "To assess the predictive typing capability of ChatBCI", "36": ".", "39": "nKS(%) =$\\frac{\\text{Normal Keystrokes"}, "text{Actual Keystrokes}}{\\text{Normal Keystrokes}} X 100$,\nwhere \u201cNormal Keystrokes\" refers to the number of keystrokes needed to spell the sentence letter-by-letter, while \u201cActual Keystrokes\" represents the keystrokes actually used by the speller system to complete the sentence. If corrective selections like deletions or re-selections are made, they are not included in the \"Actual Keystrokes\u201d count. This approach assesses the predictive efficiency of the speller, by assuming an ideal spelling process without typing mistakes or corrections [40, 41"], "I-WOULD-LIKE-TO-HAVE-WATER\\\". This sentence has 6 words, and 26 characters. For an ideal speller with word completion capability, the KS-WCmax given by (9) is $\\frac{26-2*6}{26}X 100$ = 53.85%, while the KS-WPmax given by (10) is $\\frac{26-6}{26} X 100$ = 76.92%.\nNow, assume a user spelled the same sentence using a speller BCI with predictive typing capabilities, through 10 selections as\nSelection 1": "I\"\nSelection 2: \"I-\"\nSelection 3: \"I-W\"\nSelection 4: \"I-WOULD-\"\nSelection 5: \"I-WOULD-LIKE-\"\nSelection 6: \"I-WOULD-LIKE-TO-\"\nSelection 7: \"I-WOULD-LIKE-TO-H\"\nSelection 8: \"I-WOULD-LIKE-TO-HAVE-\"\nSelection 9: \"I-WOULD-LIKE-TO-HAVE-W\"\nSelection 10: \"I-WOULD-LIKE-TO-HAVE-WATER\"\nWith 10 selections", "sessions": "a calibration (training) session", "included": 1, "Session": "The validation session assesses the usability of the personalized model and helps the user become familiar with focusing on keys to make selections. A maximum of 10 trials was allowed for each subject. During each trial", "A": "En", "B": "Sp", "E": "Z", "F": "Y\u201d", "M": "R", "H": "utilizing the words that appear in the GUI.\n4.2.1. Speed and Accuracy: The improvised sentences composed by each subject in the online Task 2", "K": "T", "I": "V", "U": "J", "N": "Q", "1": "Task 1 was a copy-spelling task", "Apple": "Water", "Restroom\", and \"Phone\", without any descriptions or labels (Figure 5). Prior to the experiment, subjects were asked to select one image and compose a coherent sentence relevant to the selected image, excluding punctuation. This sentence was used as their target sentence, which was then entered into the \u201cTarget\" block of the GUI, as reference (Figure 2). Subjects were then asked to copy-spell the target sentence using ChatBCI, and were encouraged to employ the GPT-based word suggestions shown on the GUI during this process. Subjects were instructed to correct mistakes, if they occurred, using as few key selections as possible by utilizing the \u201cDC\u201d (Delete Character) and \u201cDW\u201d (Delete Word) function keys.\nOnline Session-Task 1 (Control)": "To establish a control case", "2": "Task 2 involved improvising a sentence using ChatBCI and represents a more realistic scenario for communication. Subjects were instructed to create a meaningful sentence starting with the letter \u201cH\u201d . They were encouraged to avoid typing words letter by letter", "Accuracy": "Table 2 summarizes the sentences each subject created from their selected image and used in the copy-spelling task (Task 1) and its control letter-by-letter version. The number of characters (# of Char.) in each sentence", "Rate": "Table 5 also summarizes the information transfer rate results for the ChatBCI from Task 2. For this task", "Analysis": "Table 6 presents results from keystroke analysis for Task 2 where subjects used ChatBCI to write improvised sentences. For each improvised sentence, the number of keystrokes used to write the sentence, the number of words, the number of occurrences of word completion (WCs) and word prediction (WPs) events along with KS, theoretical KS-WCmax and KS-WPmax, and KS-DR are provided. Averages across subjects are also reported for each metric.\nThe averaged achieved keystroke savings was 80.68%, indicating that ChatBCI effectively reduced the number of keystrokes required to compose sentences. Indeed, for all subjects, KS exceeded the theoretical KS-WCmax limit and approached or exceeded\n5. Discussions\nTo the best of our knowledge, ChatBCI is the first P300 speller BCI to effectively and efficiently incorporate LLMs (here GPT), and leverage their predictive power and contextual understanding to enhance the speller's speed, performance, and overall user experience. Additionally, by offering three typing options (word prediction, word completion, and letter-by-letter), the new GUI in ChatBCI allows users to employ the speller based on their personal preferences when composing and typing sentences.\nAs demonstrated in Section 4, compared to a traditional letter-by-letter P300 speller, ChatBCI offers significantly improved performance, including faster communication speeds, as evidenced by significantly shorter time to complete writing sentences, higher selection accuracy, and an significantly improved information transfer rate. These features reduce user fatigue and enhance user experience, making ChatBCI a desirable assistive technology candidate for individuals with motor and communication disabilities.\nAnother contribution of this paper is the introduction of keystroke analysis to assess the predictive typing capability of speller BCIs. Typically, the performance of P300 speller BCIs is assessed using metrics such as ITR, selection accuracy, and success rate. However, the integration of LLMs into speller BCIs introduces new capabilities, necessitating the development of additional metrics for performance evaluation. Here, we proposed to employ keystroke analysis to specifically assess the predictive typing capability of speller BCIs equipped with word prediction functionality. We also introduced a new metric, the keystroke savings deficit ratio (KS-DR), to evaluate how far a speller BCI with predictive features is from achieving its theoretical maximum keystroke savings limit. This metric can set targets for designing efficient speller BCIs. Moreover, the predictive typing features and the new keyboard GUI, which includes word-level suggestions as selectable keys, necessitate revisiting the usual way of calculating the information transfer rate. We introduced two approaches, one to estimate the lower bound (ITR*-1), and another, as the observed measure of information transfer rate (ITR*-2).\nAdditionally, we evaluated the performance of ChatBCI using a novel experiment of asking subjects to improvise sentences (online Task 2). Traditionally, speller BCIs have been evaluated by asking users to copy-spell pre-determined sentences. However, by\n6. Conclusion\nIn this paper, we introduced ChatBCI, the first demonstration of a P300 speller BCI that incorporates LLMs (here GPT-3.5) to improve typing speed, performance, and user experience by offering predictive typing features. By enabling word and multi-word predictions through the integration of LLMs, ChatBCI reduces the need for letter-by-letter selection, cutting down on keystrokes, task time, and cognitive load. ChatBCI also features a new keyboard GUI that offers users three typing options (word prediction, word completion, and letter-by-letter) allowing them to use the speller according to their personal preferences when composing sentences. To assess ChatBCI's real-world applicability, we evaluated it not only through a traditional copy-spelling task but also through a new task, where users improvised sentences. This approach assesses speller's efficiency in practical, spontaneous communication scenarios, and its potential for real-time communication. Additionally, we introduced keystroke analysis and a new metric, the keystroke savings deficit ratio (KS-DR), to quantify speller's predictive typing performance. Results from two online tasks demonstrate that ChatBCI significantly outperforms traditional letter-by-letter P300 spellers. In the composition task"}