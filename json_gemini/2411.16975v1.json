[{"title": "ExpTest: Automating Learning Rate Searching and Tuning with Insights from Linearized Neural Networks", "authors": ["Zan Chaudhry", "Naoko Mizuno"], "abstract": "Hyperparameter tuning remains a significant chal- lenge for the training of deep neural networks (DNNs), requiring manual and/or time-intensive grid searches, increasing resource costs and presenting a barrier to the democratization of machine learning. The global initial learning rate for DNN training is particularly important. Several techniques have been proposed for automated learning rate tuning during training; however, they still require manual searching for the global initial learning rate. Though methods exist that do not require this initial selection, they suffer from poor performance. Here, we present ExpTest, a sophisticated method for initial learning rate searching and subsequent learning rate tuning for the training of DNNs. ExpTest draws on insights from linearized neural networks and the form of the loss curve, which we treat as a real-time signal upon which we perform hypothesis testing. We mathematically justify ExpTest and provide empirical support. ExpTest requires minimal overhead, is robust to hyperparameter choice, and achieves state-of-the-art performance on a variety of tasks and architectures, without initial learning rate selection or learning rate scheduling.", "sections": [{"title": "I. INTRODUCTION", "content": "MACHINE learning (ML) and artificial intelligence (AI) are experiencing tremendous growth, particularly the development of deep neural networks (DNNs), which have become ubiquitous tools, achieving state-of-the-art performance in applications from computer vision to natural language processing, in fields from consumer devices to scientific research.\nDespite these successes, the training of DNNs remains an open problem. Hyper-parameter tuning has earned comparisons with alchemy, and requires trial and error, expertise, and luck [1]. Offline methods exist, predominantly grid/random searches; however, these are time and resource intensive. Hyper-parameter tuning remains a major barrier preventing end-users of DNNs from fine-tuning models, ultimately delaying the diffusion of AI/ML innovations [2].\nDNNs are typically trained with gradient descent-based optimizations, iteratively minimizing an objective function. In its most basic form, the gradient descent rule is given by:\n$\\theta_{t+1} = \\theta_t - \\eta \\nabla L_{\\theta}(t)$\nThe model parameters are adjusted by taking the derivative of the objective function (L) with respect to the parameters ($\\theta$) at each time point (t), and moving towards minima by stepping in the negative gradient direction, as controlled by the step size parameter ($\\eta$). This step size is comonly referred to as the learning rate.\nThe learning rate is considered the most important hyper- parameter, arising in all forms of gradient-descent based optimization [3]. Learning rate adjustment techniques have become popular in DNN training, including momentum, annealing, cycling, and adaptive algorithms such as Adam [4]- [7]. However, these methods still require selection of an initial global learning rate. Currently, heuristic methods are used to choose the initial global learning rate by interpreting the loss curves acquired during DNN training [8]. The aim is to identify the highest learning rate that produces a converging loss curve to minimize training time. Empirical work supports power-law or exponential decay-like loss curve behavior to be indicative of convergence [9].\nRecently, linearized networks have seen increasing use as tools for studying the evolution of DNNs under training [10]\u2013 [13]. Additionally, signal processing approaches to learning rate tuning have been explored, treating the loss curve data as a time series signal [14]. Here, we utilize insights from linearized networks to design a lightweight algorithm for learning rate selection that does not require a choice of initial global learning rate. Our algorithm, ExpTest, involves estimation of an upper bound learning rate based on linear models, followed by hypothesis testing on the loss curve time series to detect the previously mentioned exponential decay behavior, which we show is a hallmark of DNN convergence. ExpTest introduces two new hyper-parameters (similar to momentum values or annealing decay rates), but they are interpretable and training results are robust to hyper-parameter choice. We provide a mathematical justification for ExpTest and validate it experimentally on several datasets and architectures, achieving state- of-the-art performance without initial learning rate selection."}, {"title": "II. MATHEMATICAL MOTIVATION", "content": "We aim to demonstrate that the loss function, treated as a real-time signal, is well-approximated by exponential decay in convergence conditions during training of a neural network, such that we can treat this behavior as a metric for model convergence. Additionally, we aim to define an upper bound on the learning rate with which to begin learning rate searching. We start with the linear case, re-establishing and extending"}, {"title": "A. The Linear Case", "content": "some classic results [15]. Consider a single linear layer T$\\in$ R^{mxn} with input vector $Z\\in$ R^{n} and its corresponding true output vector $\\tilde{y}\\in$ R^{m}. We define the predicted output vector for this input as:\n$T\\vec{x} = \\tilde{y}$\n$\\mathcal{L} = \\frac{1}{2m} \\sum_{i=1}^{m} (y_i - \\tilde{y}_i)^2 = \\frac{1}{2m} (\\vec{y} - \\vec{\\tilde{y}})^T (\\vec{y} - \\vec{\\tilde{y}})$ We then generalize these definitions to matrix-valued in- puts/outputs. We define each input vector $x_i \\in X$ as a member of the set of input vectors constituting the training data $X \\subset R^{nxs}$ with size: $|X| = s$. We assemble all of these vectors as the columns of a matrix, $X \\in R^{nxs}$. Then, we assemble all of the true output vectors similarly into a matrix, $Y\\in R^{mxs}$. Thus, we redefine our objective, with:\n$TX = Y$\nWhere we aim to find the T that minimizes a new loss function defined using the trace operator, or tr:\n$\\mathcal{L} = \\frac{1}{2ms} tr((Y - \\tilde{Y})^T (Y - \\tilde{Y}))$\nWe consider the tuning of T by gradient descent:\n$T[k + 1] = T[k] - \\eta \\frac{\\partial \\mathcal{L}}{\\partial T}_{T=T[k]}$\nHere, T[k] is defined as a discrete, matrix-valued function in terms of iteration number, k, and \u03b7 is the step size taken in the direction opposite the gradient (the learning rate). We set the condition that span{X} = R^{n}. The condition ensures two facts: there exists a unique T that minimizes L, and the sample covariance matrix for X is invertible (which will soon become important). Differentiating:\n$\\frac{\\partial \\mathcal{L}}{\\partial T} = \\frac{1}{ms}(TXX^T - YX^T)$ Thus:\n$T[k + 1] = T[k]A + B$\nWhere we have made the substitution, A = I_n - $\\frac{\\eta}{ms} XX^T$, and B = $\\frac{\\eta}{ms} YX^T$. Given the recursive form, we extrapolate the explicit:\n$\\vec{x} = \\tilde{y}$\nWe define the mean-squared-error (MSE) loss function:\n$T[k] = T[0]A^k + \\sum_{i=0}^{k-1} BA^i$\nWe then rewrite the series (having previously guaranteed the invertibility of $XX^T$):\n$\\sum_{i=0}^{k-1} BA^i = B \\sum_{i=0}^{k-1} A^i = B \\sum_{i=0}^{k-1} A^i$\n$= B (I_n - A)^{-1} (I_n - A^k)$\n$= B (\\frac{\\eta}{ms}XX^T)^{-1} (I_n - A^k)$"}, {"title": "Returning:", "content": "$= YX^T (XX^T)^{-1} (I_n - A^k)$\n$T[k] = T[0]A^k + YX^T (XX^T)^{-1} (I_n - A^k)$\n$= (T[0] - YX^T (XX^T)^{-1}) A^k + YX^T (XX^T)^{-1}$\nNotice that $YX^T (XX^T)^{-1}$ is the exact solution for the T that minimizes L, obtained by equating the derivative to the zero matrix. We thus substitute with $T_\\infty = YX^T (XX^T)^{-1}$:\n$T[k] = (T[0] - T_\\infty) A^k + T_\\infty$\nWe note that the function only converges in the case that $A^k$ converges as $k \\rightarrow \\infty$. Thus, in convergence conditions, the eigenvalues of A must have magnitude less than one. We now introduce the condition that the input data is normalized, such that the mean of the vectors in X is $\\vec{\\mu} = 0$ and the variance along each dimension of the vectors in X is $\\sigma^2 = 1$. This simplifies the analysis and is common practice in machine learning. By definition, the sample covariance matrix of A is:\n$\\sum_{\\vec{x}\\vec{x}} = \\frac{1}{s - 1} \\sum_{i=1}^{s} (\\vec{x}_i - \\vec{\\mu})(\\vec{x}_i - \\vec{\\mu})^T$\nGiven the normalization, this becomes:\n$\\sum_{\\vec{x}\\vec{x}} = \\frac{1}{s - 1} \\sum_{i=1}^{s} \\vec{x}_i \\vec{x}_i^T = \\frac{1}{s - 1} XX^T$\nThus, we rewrite A as A = $I_n - \\frac{\\eta (s - 1)}{ms} \\sum_{\\vec{x}\\vec{x}}$. By construction, the eigenvalues of $\\sum_{\\vec{x}\\vec{x}}$ are all in the range: (0, tr($\\sum_{\\vec{x}\\vec{x}}$)) = (0, n). We could define a more exact bound by calculating the eigenvalues and finding the maximum to give a revised range of: (0, $\\lambda_{max}$). We have that the eigenvalues of A are given in terms of the corresponding eigenvalues of $\\sum_{\\vec{x}\\vec{x}}$ by:\n$\\lambda_A = 1 - \\frac{\\eta (s - 1)}{ms} \\lambda_{\\sum_{\\vec{x}\\vec{x}}}$\nNow we arrive at a classic result [15]. We can define two boundaries on the learning rate: one that requires no additional computation but is in general smaller than the optimal learning rate for the fastest convergence, and one that requires computing the eigenvalues of the sample covariance matrix but will guarantee the fastest possible convergence. In practice, if n and s are very large, it may be preferable to use the first bound to reduce computations. Starting with the first bound, we have the range of $\\lambda_A$ as: (1 - $\\frac{\\eta n(s - 1)}{ms}$, 1). To guarantee convergence, the magnitude of the eigenvalues of A must be less than one; thus, the lower bound must be greater than negative one:\n$1 - \\frac{\\eta n (s - 1)}{ms} > -1$\n$\\Rightarrow \\frac{2ms}{n (s - 1)} > \\eta$"}, {"title": "Previous work studying the distribution of the maximum", "content": "eigenvalue of Wishart random matrices has shown that it is unlikely for $\\lambda_{max} \\approx tr(\\sum_{\\vec{x}\\vec{x}})$ [16]. Thus, this bound is likely much lower than the true maximum learning rate, which is given by the second bound:\n$\\frac{2ms}{\\lambda_{max} n (s - 1)} > \\eta$\nNow we proceed to diagonalize A, with A = PDP-1, where D is a diagonal matrix containing the eigenvalues of A along the diagonal. Substituting:\n$T[k] = (T[0] - T_\\infty)PD^kP^{-1} + T_\\infty$\nNow we consider the elements of T[k]. First, we rewrite the eigenvalues of A as: $\\lambda_i = \\beta_i e^{-\\alpha_i k}$, where $\\beta_i$ can only be 1 or -1. Then we note that $D^k$ has the form:\n$\\begin{bmatrix}\n\\beta_1 e^{-\\alpha_1 k} & 0 & 0 & ... & 0 \\\\\n0 & \\beta_2 e^{-\\alpha_2 k} & 0 & ... & 0 \\\\\n:\\\\n0 & 0 & 0 & ... & \\beta_n e^{-\\alpha_n k}\n\\end{bmatrix}$\nWe compute $V = (T[0] - T_\\infty)P$. Then (absorbing the $\\beta$'s into the v constants):\n$\\begin{bmatrix}\nv_{11} e^{-\\alpha_1 k} & v_{12} e^{-\\alpha_2 k} & ... & v_{1n} e^{-\\alpha_n k} \\\\\nv_{21} e^{-\\alpha_1 k} & v_{22} e^{-\\alpha_2 k} & ... & v_{2n} e^{-\\alpha_n k} \\\\\n:\\\\n\\left[\\begin{array}{cccc}v_{m 1} e^{-\\alpha_1 k} & v_{m 2} e^{-\\alpha_2 k} & ... & v_{m n} e^{-\\alpha_n k}\\end{array}\\right]\n\\end{bmatrix}$\nFinally, we define Q = $P^{-1}$, giving:\n$\\begin{bmatrix}\\sum_{i=1}^{n} v_{1i} q_{i1} e^{-\\alpha_i k} & \\sum_{i=1}^{n} v_{1i} q_{i n} e^{-\\alpha_i k} \\\\\n\\left[\\begin{array}{c}\\vdots\\\\\n\\sum_{i=1}^{n} v_{m i} q_{i 1} e^{-\\alpha_i k}\\end{array}\\right] & \\sum_{i=1}^{n} v_{m i} q_{i n} e^{-\\alpha_i k}\n\\end{bmatrix}$\nThus, the function T[k] is defined at each element of T by a linear combination of decaying exponentials plus the constant term, $T_\\infty$. Now we can characterize the behavior of the discretized loss function with a Taylor approximation:\n$\\Delta C \\approx \\frac{\\partial \\mathcal{L}}{\\partial T} \\Delta T = \\frac{\\partial \\mathcal{L}}{\\partial T} (-\\eta \\frac{\\partial \\mathcal{L}}{\\partial T}) = -\\eta ||\\nabla L||^2$\nThe learning rate discretizes the loss function, so we can parameterize in terms of time: $\\eta = \\Delta t$. Returning:\n$\\Delta C \\approx - \\Delta t ||\\nabla L||^2$\n$\\frac{\\Delta C}{\\Delta t} \\approx - ||\\nabla L||^2$\nFor analysis purposes, we adopt the limit of infinitesimal step size, arriving at continuous time dynamics (\"gradient flow\"):\n$\\frac{dC}{dt} = - ||\\nabla L||^2$\nNow let us consider the form of $f(t) = ||\\nabla L(t)||^2$. We substitute the diagonalized expression of T(t) into the gradient, omitting the constant factors (they can be absorbed into V):"}, {"title": null, "content": "$\\nabla L(t) = T(t) XX^T - YX^T$\n$= (VD^kQ + T_\\infty) XX^T - YX^T$\n$= VD^kQ XX^T + YX^T (XX^T)^{-1} XX^T - YX^T$\n$= VD^kQ XX^T$\nClearly, the elements of $\\nabla L(t)$ will be linear combinations of decaying exponentials of the form:\n$\\sum_{i=1}^{n} C_i e^{-\\alpha_i t}$\nThus:\n$||\\nabla L||^2 = \\left( \\sum_{i=1}^{m} \\sum_{j=1}^{n} C_{i j} e^{-\\alpha_{i} t} \\right)^2$\nReturning to the differential equation:\n$\\frac{d L}{dt} = - \\left( \\sum_{i=1}^{m} \\sum_{j=1}^{n} C_{i j} e^{-\\alpha_{i} t} \\right)^2$\n$L = C_{int} + \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\sum_{h=1}^{n} \\frac{C_{h j} C_{i j} e^{-(\\alpha_i + \\alpha_h)t}}{\\alpha_i + \\alpha_h}$\nNow we aim to approximate this sum of exponentials as a single exponential. In practice, we can solve numerically for a single exponential decay that fits the loss curve (a least- squares regression). However, we also provide two illustrative, analytical approximations. Consider the Taylor polynomial of a sum of exponentials, $A_1 e^{a_1 x} + A_2 e^{a_2 x} + ...$ For each ($A_i$, $a_i$):\n$A_i e^{a_i x} = \\sum_{n=0}^{\\infty} A_i a_i^n \\frac{x^n}{n!}$\nThen for the sum:\n$\\sum_{i=1}^{N} A_i e^{a_i x} = \\sum_{i=1}^{N} \\sum_{n=0}^{\\infty} A_i a_i^n \\frac{x^n}{n!}$\nApproximated with the first two terms as:\n$\\sum_{i=1}^{N} A_i e^{a_i x} = \\sum_{i=1}^{N} A_i + x \\sum_{i=1}^{N} A_i a_i + R_1(x)$\nWe can use the first two terms to fit a single exponential, $C e^{c x}$.\n$C e^{c x} = C + x C c + R_1(x)$\nEquating the first two terms of each, we have:\n$C = \\sum_{i=1}^{N} A_i$\n$C = \\sum_{i=1}^{N} A_i a_i \\Rightarrow c = \\frac{\\sum_{i=1}^{N} A_i a_i}{\\sum_{i=1}^{N} A_i}$"}, {"title": "The error in the remainders is given by:", "content": "$\\epsilon = \\sum_{i=1}^{N} (\\sum_{n=2}^{\\infty} A_i a_i^n \\frac{x^n}{n!}) = \\sum_{n=2}^{\\infty} (\\sum_{i=1}^{N} A_i a_i^n ) \\frac{x^n}{n!}$\nIn the case that $a_1 = a_2 = ... = a_n$, we note that the error completely disappears. Thus, this is a good approximation in the case that the $a_i$ are similar. If they are dissimilar, we can assess the dominant term(s), noting that for $\\sum_{i=1}^{N} A_i e^{-a_i x}$ if some $a_d < a_i \\forall d \\neq i$ then $e^{-a_d x} >> e^{-a_i x}$ as $x \\rightarrow \\infty$, and thus for large x:\n$\\sum_{i=1}^{N} A_d e^{-a_d x} = Ce^{-cx} \\approx A_i e^{-a_i x}$\nIf some subset of the $a_i$'s, {$a_{d1}, a_{d2}, ...$} are similar in magnitude to each other but less than the remaining $a_i$'s, we can calculate the dominant term using our Taylor polynomial method for the $a_d$'s and then use this computed dominant term to approximate the entire sum. Therefore, L has the form:\n$L = C_{int} + \\sum_{i=1}^{m} \\sum_{j=1}^{n} \\sum_{h=1}^{n} \\frac{C_{h j} C_{i j} e^{-(\\alpha_i + \\alpha_h)t}}{\\alpha_i + \\alpha_h} \\approx C_{int} + Ce^{-ct}$\nWe have demonstrated two approximation methods, depending on the values of the given ($A_i, a_i$) pairs. In practice, a least-squares estimator of ($C_{int}, C, c$) will balance these two extremes. Furthermore, any approximation of this sum as a single exponential is significantly improved by the guarantee of monotonicity of L (Equations 19 and 20)."}, {"title": "B. Extension to Stochastic Gradient Descent", "content": "We have shown that in the linear case, where all of the input data is assembled as a matrix, X, the discrete loss function is approximately exponential. However, in practice, X is a prohibitively large matrix, and thus the method of stochastic gradient descent (SGD) is used. We now present some classic results to extend the argument to SGD, where at each step, k, a random input vector z is sampled from the set X (and its associated output vector, y) and used for the gradient descent calculation. It is trivial to further extend the proof for mini-batch SGD by considering the expected value of the mini-batch mean gradient. We now demonstrate the same exponential decay-like behavior in the discrete loss function for SGD by considering the behavior of the expected value of T: E(T[k]). We rewrite the gradient as:\n$\\frac{\\partial T}{\\partial T} = \\frac{\\eta}{ms} (y z^T - T zz^T)$"}, {"title": null, "content": "Thus, following the same logic as before, but this time with A = $I_n - \\frac{\\eta}{ms} zz^T$ and B = $\\frac{\\eta}{ms} y z^T$:\n$T[k + 1] = T[k]A + B$\nSince at each time step, we sample a different z, A and B change as well. We denote the k-th A and B as $A_k$ and $B_k$.\nThen we can write T[k] explicitly as:\n$T[k] = T[0] (\\prod_{i=1}^{k} A_i) + \\sum_{i=1}^{k-1} B_i (\\prod_{j=i+1}^{k} A_j)$\nHowever, if we assume the dataset is large ($s \\rightarrow \\infty$), then the sampling is independent, and if we apply expectation, we have:\n$E(\\prod_{i=1}^{k} A_i) = [E(A_i)]^k = E(A)^k$\nFor B, we have similarly:\n$E (B_i (\\prod_{j=i+1}^{k} A_j)) = E(B_i) \\prod_{j=i+1}^{k} E(A_j) = E(B) E(A)^{k-i}$\nThus, we begin by calculating E(A):\n$E(A) = E(I_n - \\frac{\\eta}{ms} zz^T)$\n$= I_n - \\frac{\\eta}{ms} E(\\sum_{xx})$\nWhere we have used the definition of the covariance matrix, $\\sum_{xx} = E(zz^T)$. Then for E(B):\n$E(B) = E(\\frac{\\eta}{ms} y z^T)$\n$= \\frac{\\eta}{ms} \\sum_{yx}$\nWhere $\\sum_{yx}$ is the cross-covariance matrix of y with respect to z. Thus (omitting some of the steps used in subsection A):\n$E(T[k]) = (T[0] - T_\\infty) (I_n - \\frac{\\eta}{ms} \\sum_{xx})^k + T_\\infty$\nFrom here, it clearly follows that the exponential decay behavior will be preserved in expectation, though with revised bounds on the learning rate."}, {"title": "C. Nonlinearities and Classification Loss Functions", "content": "A significant body of previous work has demonstrated that the early-time training dynamics of multilayer networks with nonlinearities can be well-approximated by linear models [10]-[13]. Thus, we continue in this vein to justify extending the findings from linear theory to neural networks with non- linearities. We begin by considering the arbitrary depth neural network, f($\\theta$, x), where $\\theta$ refers to the vectorized parameters of the network (we omit the vector arrow for f and $\\theta$ for clarity). Each layer of the network is defined as:\n$\\vec{y}_l = \\sigma (W_l \\vec{y}_{l-1} + \\vec{b}_l)$\nHere, the input $\\vec{y}_{l-1}$ is the output of the previous layer (where l refers to layer number), $W_l$ and $\\vec{b}_l$ are the weight matrix and bias term, respectively, of the current layer, and $\\sigma$ is a coordinate-wise nonlinear function (an \"activation\" function). We assume $\\sigma$ to be Lipschitz-continuous. We now consider gradient descent:"}, {"title": null, "content": "$\\Delta \\theta = -\\eta \\frac{\\partial \\mathcal{L}}{\\partial \\theta}$\n$\\frac{d \\theta}{dt} = - \\frac{\\partial \\mathcal{L}}{\\partial \\theta}$\nIn the continuous limit:\n$\\frac{d \\theta}{dt} = - \\frac{\\partial \\mathcal{L}}{\\partial \\theta}$\n$\\frac{d f}{dt} = \\nabla_{\\theta} f \\frac{d \\theta}{dt}$\n$= -\\nabla_{\\theta} f \\frac{d \\theta}{dt}$\n$= -\\nabla_{\\theta} f \\nabla_{\\theta}^T f \\frac{d \\theta}{dt}$\nIf we adopt a linear approximation of f around $\\theta = \\theta(0)$, then we arrive at the neural tangent kernel (NTK) description, which is an exact solution for infinite-width networks [11]. We substitute, $K = \\nabla_{\\theta} f \\nabla_{\\theta}^T f$ (notice that this is a positive semi-definite matrix):\n$\\frac{d f}{dt} = -K \\nabla_{\\theta} f \\mathcal{L}$\nIn general, this is a nonlinear equation without a simple solution. However, in the cases of MSE-loss and cross-entropy loss (up to a second order approximation), which cover the majority of regression and classification tasks, respectively, the solution can easily be found. Cross-entropy loss (CE-loss) is defined as:\n$\\mathcal{L} = - \\sum_{i=1}^{m} y_i log \\tilde{y}_i$\nFor both MSE-loss and CE-loss (up to second order; see Appendices A and B for full derivations), we have for some constant vector, c, and some positive semi-definite constant matrix, C:\n$\\frac{d f}{dt} = -K C(f - c)$\nThe solution is:\nf(t) = e^{-tKc} (f_0 - c) + c\nThis is familiar as each element of f is a sum of exponential decays plus a constant term, as in the linear case. Thus, for MSE-loss and CE-loss (up to second order), L(t) will clearly have the form:\n$L(t) = C + \\sum A_i e^{-B_i t} \\approx c + ae^{-bt}$"}, {"title": "III. ALGORITHM DEVELOPMENT", "content": "The exponential loss curve behavior established thus far is already used as part of heuristic manual learning rate tuning methods [9]. Here, we propose a simple algorithm to automate this process. We wish to measure the \u201cexponential-ness\" of the real-time loss curve during early training to determine whether the model displays convergent behavior. We draw from signal processing to propose our method, ExpTest, which entails 1) estimating an upper bound on the learning rate to begin training; 2) computing a least-squares linear regression over some window of the real-time loss curve data; 3) computing a least-squares exponential regression on the loss curve data over that window; 4) performing a statistical test (F-test) on the residuals to see whether the exponential model describes the loss data significantly better than the linear model; and 5) decreasing the learning rate if this is not the case.\nWe can incorporate additional heuristics as well, for example, decreasing the learning rate upon a plateau, by 6) performing a statistical test (t-test) to determine whether the slope of the linear model is significantly less than zero, and decreasing the learning rate if this is not the case. Additionally, we can combine this method with existing techniques such as momentum [4].\nFor the first part of the algorithm, empirical work has shown that as model complexity (layer number, layer width, etc.) increases, the learning rate of convergence decreases, likely due to increasing complexity of the loss surface [17]. Theoretical analyses of the convergence properties of multilayer linear networks have similarly demonstrated inverse power relationships between learning rate and network depth [18]. Thus, in general, the learning rate that guarantees convergence of a given multilayer nonlinear network is less than the learning rate that guarantees convergence for a single layer linear network on the same problem (discounting certain regularization properties of nonlinear networks, which we address in the Discussion). So we can set upper bounds using single-layer linear models, as described for MSE-loss in Equations 15 and 16 and for CE-loss in Appendix C.\nA question naturally arises regarding the window size for performing the regression steps of this algorithm. We propose a method based around the NTK description. We begin by considering the natural distinguishing feature of exponential decay relative to linear decay: curvature. Curvature is defined by:\n$\\kappa (t) = \\frac{|f''(t)|}{(1 + f'(t)^2)^{3/2}}$\nNow consider once more the NTK description:\nf(t) = e^{-tKc} (f_0 - c) + c\nThere is some decay rate within the eigenvalues of the matrix KC that maximizes the time-point at which the point of maximum curvature exists (i.e. an exponential possessing this decay rate achieves maximum curvature at the latest point in time and thus provides a reasonable upper bound for the time-point at which the loss curve achieves maximum curvature). Let's call it $\\lambda_{t,max}$. Furthermore, when dealing with the discretized case, we consider a step-size in time of \u03b7. Thus, we are interested in the function (for some constant $C_{exp}$):"}, {"title": null, "content": "$f_{\\lambda t", "at": "n$t_{max"}, "frac{\\sqrt{2C_{exp}}}{e}$\nWe can set $C_{exp}$ as the loss value at time zero, $L_0$, giving the window size for point collection, w, as:\n$w = \\frac{2\\sqrt{2}L_0}{\\eta e} + \\frac{1}{2}$\nWe adopt nearest-integer rounding conventions to avoid a fractional window size.\nThus far, this analysis has been independent of batch size, which will naturally impact the window of convergence. We propose a simple correction factor for differing batch sizes. In mini-batch SGD, we approximate the full gradient at each step by the mean gradient over some batch of size B. Thus, there will be some amount of noise in the gradient prediction, deviating from the optimal path. We can view the mini-batch gradient steps as oscillating around some true path, which we can approximately recover by averaging the direction over several steps (as in the exponential moving averages of momentum) [4"], "misstepping,\" we can measure the path length over some window (the sum of the magnitudes of the gradient step vectors), and we can divide this by the displacement over this window (the magnitude of the sum of the gradient step vectors). Explicitly, we define the window correction, $C_w$ over some window of gradient descent steps, w, as": "n$C_w = \\frac{\\sum_{i=t-w+1}^{t} ||\\nabla L(x_i)||}{\\sqrt{(\\sum_{i=t-w+1}^{t} \\nabla L(x_i))^2"}, {}]