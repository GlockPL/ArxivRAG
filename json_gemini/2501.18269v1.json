{"title": "MAMS: Model-Agnostic Module Selection Framework for Video Captioning", "authors": ["Sangho Lee", "Il Yong Chun", "Hogun Park"], "abstract": "Multi-modal transformers are rapidly gaining attention in video captioning tasks. Existing multi-modal video captioning methods typically extract a fixed number of frames, which raises critical challenges. When a limited number of frames are extracted, important frames with essential information for caption generation may be missed. Conversely, extracting an excessive number of frames includes consecutive frames, potentially causing redundancy in visual tokens extracted from consecutive video frames. To extract an appropriate number of frames for each video, this paper proposes the first model-agnostic module selection framework in video captioning that has two main functions: (1) selecting a caption generation module with an appropriate size based on visual tokens extracted from video frames, and (2) constructing subsets of visual tokens for the selected caption generation module. Furthermore, we propose a new adaptive attention masking scheme that enhances attention on important visual tokens. Our experiments on three different benchmark datasets demonstrate that the proposed framework significantly improves the performance of three recent video captioning models.", "sections": [{"title": "Introduction", "content": "The video captioning task generates descriptions for provided videos in natural language (Li et al. 2021; Wang et al. 2019). It has been rapidly gaining attention in blind navigation, video event commentary, human-computer interaction, etc. To improve video captioning performances, it is pivotal to introduce multi-modal transformers (Sun et al. 2019). Many recent studies extract an identical number of frames for different videos to use a consistent input size for transformer-based models; see, e.g., (Chen et al. 2023; Yang et al. 2023).\nSelecting a fixed number of frames in existing captioning models has critical limitations. For videos with abundant information, e.g., videos with large dynamics, if we extract a limited number of frames, caption generation performances can degrade (Lin et al. 2022). It may omit frames that encapsulate essential information for caption generation, potentially compromising the accuracy and completeness of caption generation (Gao et al. 2023). Conversely, for videos with little information, e.g., videos with little dynamics, if frames are densely extracted, it could result in extracting similar frames. Numerous studies have demonstrated that redundant visual tokens generated by a large number of consecutive frames can adversely affect performance (Liu et al. 2023; Liang et al. 2022). Our observations indicate that the performance of existing video captioning methods stagnates or even declines as the number of frames increases. We conjecture that this phenomenon is caused by the fact that existing methods rely on a fixed number of frames across all videos. This study assumes in caption generation that it could be more reasonable to vary the number of frames or visual tokens for each video, rather than to fix it for all videos.\nTo address the aforementioned issues, this paper proposes the first Model-Agnostic Module Selection (MAMS) framework in video captioning that adaptively selects a caption generation module for each video, where each module extracts a different number of frames. The proposed framework consists of an existing caption generator that uses all frames, a smaller caption generator module that uses a subset of frames, and a module & token selector that selects the appropriate generation module and tokens for each video, respectively. The process of the proposed framework is given as follows. (1) We extract visual tokens from video with a"}, {"title": "Methods", "content": "This section discusses the ablation study for the proposed MAMS framework. The second row in Table 3 demonstrates that MAMS can significantly improve the stand-alone counterpart, even without the adaptive attention masking scheme.\nIt suggests that by adaptively varying the number of frames or visual tokens used for each video, the caption generation performance improves. The third row in Table 3 demonstrates the effectiveness of the proposed adaptive attention masking scheme. The adaptive attention masking scheme that is designed for each module of MAMS, can be applied to the stand-alone counterpart and significantly improve its captioning performance. By using the adaptive attention mask, we focus more on visual tokens with a higher contribution in caption generation, resulting in performance improvements. See the details of implementing the independent integration of an adaptive attention mask into existing models in the supplementary material. Additionally, the last row in Table 3 implies that MAMS significantly improves the captioning performance by focusing more on essential visual tokens at both the token and frame levels."}, {"title": "The Overall Architecture of MAMS Framework", "content": "In video captioning, many popular architectures based on multi-modal transformers consist of three major modules: 1) a video encoder that transforms a video into visual tokens; 2) a text encoder that transforms a caption into textual tokens; and 3) a caption generation module that creates captions. In a nutshell, the proposed MAMS framework augments the aforementioned architecture by introducing a smaller caption generation module in parallel. We differentiate two generation modules with the terms, a large and a small module, which are tailored for inputs of sizes $T_{large}$ and $T_{small}$, respectively. Furthermore, our MAMS framework includes a score calculator for calculating the importance of each visual token, referred to as a token significance score. Based on this score, module and token selectors within the MAMS framework strategically choose between two modules for optimal training and inference. Additionally, a mask generator of the MAMS framework creates an adaptive attention mask for"}, {"title": "Token Significance Score", "content": "In video captioning models, a video encoder transforms frames into visual tokens. A caption generation module then takes these visual tokens to produce captions. As adjacent frames are similar, it is natural that their visual tokens have similar values. We assume, however, that their contributions to caption generation are different.\nTo quantify the contribution of each token to caption generation, we define a token significance score inspired by (Cao et al. 2023). Specifically, we define the token significance score of the $p$th token at the $i$th frame as follows:\n$t_{i,p} = \\frac{a_{i,p} \\cdot ||x_p||}{\\sum_{i,p} a_{i,p} \\cdot ||x_p||}, i = 1,..., T_{large}, p = 1, . . ., P,$\nwhere $a_{i,p}$ denotes the attention value between a special classification (CLS) token and a $p$th visual token at the $i$th frame, $x_p$ denotes a $p$th visual token at a $i$th frame, $T_{large}$ is the number of total frames, and $P$ is the number of visual tokens per video frame. We calculate {$t_{i,p} : \\forall i, p$} from the first attention layer of a caption generation module. Considering a CLS token as representing the starting point of a caption, Attention values between a CLS token and visual tokens can quantify the contribution of visual tokens to the entire caption (Cao et al. 2023). In Eq. (1), we additionally assume that not only the attention values but also the visual tokens themselves influence caption generation and use the norm values of each visual token in computing the token significance scores."}, {"title": "Module and Token Selector", "content": "Module Selector Using calculated {$t_{i,p} : \\forall i, p$} in Eq. (1), we define a frame significance score for the $i$th frame as follows:\n$f_i = \\sum_{p=1}^{P} t_{i,p}, i = 1,..., T_{large},$\nwhere by default, we consider that a video encoder generates multiple visual tokens from a single frame. Calculating the defined quantities Eqs. (1)\u2013(2), we use them to select important frames for caption generation module selection. We first select important frames, applying a for loop algorithm based on the Gumbel-Softmax operator (Jang et al. 2017) to {$f_i : i = 1,...,T_{large}$}. As we run the Gumbel-Softmax operator $T_{large}$ times, the same frame may be selected multiple times so the number of selected frame indices can vary for different videos. In choosing between small and large caption generation modules, we apply the following selection rule using the set of selected frame indices $S_{frm}$:\n$\\begin{cases} Select \\\\ a \\\\ small \\\\ module, \\\\ if \\\\ |S_{frm}| < T_{small}, \\\\ Select \\\\ a \\\\ large \\\\ module, \\\\ if \\\\ |S_{frm} |> T_{small}, \\end{cases}$\nwhere $|S_{frm}|$ denotes the number of selected frame indices. The decision rule in Eq. (3) implies the following. The condition $|S_{frm} > T_{small}$ implies that a small module may miss"}, {"title": "Adaptive Attention Mask", "content": "A token selector in the proposed MAMS framework selects appropriate visual tokens at the frame level based on their contribution to caption generation. Even though visual tokens are generated from the identical frame, their contributions to caption generation may vary. To better select important visual tokens from selected or all frames, we propose a new adaptive attention masking scheme, where the corresponding binary mask for each frame of a small and large module is denoted as $M_{small}$ and $M_{large}$, respectively. We use the adaptive attention mask to enable each module in the proposed MAMS framework to focus more on visual tokens with a higher contribution in caption generation. First, we extract the indices of important tokens based on their contribution to caption generation and use those indices to generate $M_{large}$. We apply the for loop algorithm based on the Gumbel-Softmax operator to {$t_{i,p} : \\forall i,p$} in (1) to generate a set of indices of selected tokens in the form of $(i, p)$, denoted as $S_{tk}$. We run the Gumbel-Softmax operator for $T_{large} \\cdot P$ times that represents the total number of visual tokens. As a result, some token(s) may be selected multiple times, leading to a variable number of selected token indices across different videos, with an adaptive attention mask varying from video to video. The elements of $M_{large}$ are determined using $S_{tk}$ through the following process:\n$M_{large}^{(x,y)} = \\begin{cases} 1, \\\\ if \\\\ x=y, \\\\ 1, \\\\ ytk, \\\\ y \\\\ \\in \\\\ S'_{tk}, \\\\ 0, \\\\ if \\\\ x \\\\ \\neq \\\\ y, \\\\ x \\\\ \\in \\\\ S'_{tk}, \\\\ y \\\\ otherwise, \\\\ \\end{cases}$\nwhere $x, y \\in { (i,p) : i = 1,..., T_{large}, p = 1,..., P }$. We apply this masking scheme to all attention layers of a large caption generation module (if selected). If a small caption generation module is selected, we construct $M_{small}$ by using a subset of {$M_{large}^{(x,y)} :  x, y \\in S'_{tk}$} in Eq. (4), with the indices of the visual tokens selected in the earlier token selector. We explain further details in the supplementary material.\nWhile a module and token selector in the MAMS framework selects an appropriate number of visual tokens at the frame level, the adaptive attention masking scheme focuses on more important visual tokens at the token level. Combined with the adaptive attention masking scheme, the proposed MAMS framework selects/focuses on essential visual tokens at both the frame and token levels."}, {"title": "Training Phase", "content": "For the training phase, we train both a large and small module using two loss functions in the following form:\n$L = \\lambda_{large} L_{large} + \\lambda_{small}L_{small}, \\\\ (\\lambda_{large}, \\lambda_{small}) = \\begin{cases} (1,0), \\\\ if \\\\ |S > T_{small}, \\\\ (0,1), \\\\ if \\\\ |S| \\\\leq \\\\ T_{small} \\\\ \\end{cases}$\nwhere $L_{large}$ and $L_{small}$ are losses for training a large and small caption generation module, respectively. By setting"}, {"title": "Experimental Setups", "content": "We ran experiments with three different datasets: MSVD (Chen et al. 2011), MSRVTT (Xu et al. 2016), and YOUCOOKII datasets (Zhou et al. 2018). We incorporated the following video captioning models into the proposed MAMS framework:\n\u2022 Two representative models: SwinBERT (Lin et al. 2022) and UniVL (Luo et al. 2020)\n\u2022 The state-of-the-art model, mPLUG-2 (Xu et al. 2023). The mPLUG-2 model is the state-of-the-art video captioning model, particularly for the MSVD and MSRVTT datasets.\n\u2022 While the SwinBERT and M-PLUG-2 models extract visual features directly from raw videos for training, the UniVL model trains on pre-extracted features. The UniVL model, unlike the other two models, is a modular (i.e., non-end-to-end) approach.\nWe evaluated the generated captions using four different evaluation metrics: BLEU-4 (Papineni et al. 2002), METEOR (Banerjee et al. 2005), ROUGE (Lin et al. 2004), CIDEr (Vedantam et al. 2015). Throughout the tables, we denote the above metrics as B4, M, R, and C, respectively. For our experiments, we used PyTorch (Paszke et al. 2019) and NVIDIA A100 GPUs. See details of experiments and implementation in the supplementary material."}, {"title": "Ablation Study for the Proposed Framework", "content": "This section discusses the ablation study for the proposed MAMS framework. The second row in Table 3 demonstrates that MAMS can significantly improve the stand-alone counterpart, even without the adaptive attention masking scheme. It suggests that by adaptively varying the number of frames or visual tokens used for each video, the caption generation performance improves. The third row in Table 3 demonstrates the effectiveness of the proposed adaptive attention masking scheme. The adaptive attention masking scheme that is designed for each module of MAMS, can be applied to the stand-alone counterpart and significantly improve its captioning performance. By using the adaptive attention mask, we focus more on visual tokens with a higher contribution in caption generation, resulting in performance improvements. See the details of implementing the independent integration of an adaptive attention mask into existing models in the supplementary material. Additionally, the last row in Table 3 implies that MAMS significantly improves the captioning performance by focusing more on essential visual tokens at both the token and frame levels."}, {"title": "Sanity Tests of Module and Token Selectors in MAMS Framework", "content": "Table 5a demonstrates that the proposed module selector in Eq. (3) works appropriately to improve the caption generation performances. Comparing the performances between MAMS with the inappropriate module selection design (see the second row in Table 5a) and the stand-alone counterpart (see the first row in Table 3) show that MAMS with the inappropriate module selection design significantly degrades the performances of the stand-alone counterpart.\nTable 5b shows that the proposed token selector using the score defined in Eq. (1) works appropriately to improve the caption generation performances. Comparing the performances between MAMS with the inappropriate token selector design (see the second row in Table 5b) and the stand-alone counterpart (see the first row in Table 3) show that MAMS with the inappropriate token selector design significantly degrades the stand-alone counterpart. The results can justify the token significance score in Eq. (1) in selecting essential tokens in the MAMS framework for accurate caption generations."}, {"title": "Comparisons with Different Numbers of Generation Modules", "content": "The default configuration in our MAMS framework selects between two caption generation modules: a large module and a small module. This section compares the performance of MAMS with different numbers of generation modules. Table 7 compares captioning performances of MAMS incorporating SwinBERT by varying the number of generation module candidates. It shows that the default setup selecting between two generation modules outperforms configurations that select among three or four generation modules, each handling different numbers of frames. Increasing the number of candidate modules can divide the training data into a larger number of groups, potentially resulting in some generation modules having a limited number of training samples, which leads to performance degradation. Unless the training data contains a large number of samples sufficient to adequately train all modules, the default setup is likely to be preferred."}, {"title": "Conclusion", "content": "In this paper, we propose the first model-agnostic framework in video captioning, that selects a caption generation module of appropriate size for each video. To further enhance the video captioning performance, we propose a new adaptive attention masking scheme for the MAMS framework by focusing on more significant visual tokens, which can guide in identifying the main words. Our numerical experiments across different datasets demonstrate that the proposed MAMS framework significantly and consistently improves the recent video captioning models.\nFor future work, we plan to further improve captioning performances and gain further insights by focusing more on important textual tokens, as well as important visual tokens. Additionally, we aim to extend the underlying principles of the MAMS framework to other video understanding tasks, such as video summarization and video question answering, to broaden its applicability and impact in the field of video analysis."}, {"title": "Analysis of Generated Sentences by the Proposed Framework", "content": "This section analyzes the generated sentences by the proposed framework. Table 4 compares the caption generation performances with different combinations of MAMS, adaptive attention mask, and SwinBERT. Comparing the first and fourth rows in Table 4, we explain why does the proposed MAMS framework combined with the proposed adaptive attention masking scheme improve the captioning quality. The comparisons suggest that the proposed framework allows an existing model to better focus on important visual tokens in generating core words more accurately."}, {"title": "Analyses for the Proposed Adaptive Attention Masks", "content": "The proposed adaptive attention masking scheme focuses more on the visual tokens with higher contributions to caption generation, which can be located at the edges, center, or anywhere within the frame, varying for each video. This section analyzes its effectiveness compared to the fixed learnable mask (Lin et al. 2022), from both the qualitative and quantitative perspectives.\nComparing results in Figure 6 show that the proposed adaptive attention masking scheme is more reasonable than the fixed learnable attention mask. In Figure 6c, SwinBERT, with the fixed learnable attention mask, fails to understand the man in white clothing at the edges of each frame in Figure 6a and generates an incorrect word, 'woman'. In Figure 6b, the adaptive attention masks correctly select the parts with the man in white clothing and the man in black clothing as important visual tokens for caption generation, resulting in accurate captions as shown in Figure 6c.\nTable 6 shows that the adaptive attention masking scheme outperforms the fixed learnable attention mask in captioning performance, both with the MAMS framework+SwinBERT and the stand-alone counterpart. These results imply that the proposed adaptive attention masks, while overcoming the limitations of the fixed learnable attention mask, better focus on essential tokens for caption generation."}]}