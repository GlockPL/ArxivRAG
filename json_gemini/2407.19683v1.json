{"title": "Revisiting the robustness of post-hoc interpretability methods", "authors": ["Jiawen Wei", "Hugues Turb\u00e9", "Gianmarco Mengaldo"], "abstract": "Post-hoc interpretability methods play a critical role in explainable artificial intelligence (XAI), as they pinpoint portions of data that a trained deep learning model deemed important to make a decision. However, different post-hoc interpretability methods often provide different results, casting doubts on their accuracy. For this reason, several evaluation strategies have been proposed to understand the accuracy of post-hoc interpretability. Many of these evaluation strategies provide a coarse-grained assessment \u2013 i.e., they evaluate how the performance of the model degrades on average by corrupting different data points across multiple samples. While these strategies are effective in selecting the post-hoc interpretability method that is most reliable on average, they fail to provide a sample-level, also referred to as fine-grained, assessment. In other words, they do not measure the robustness of post-hoc interpretability methods. We propose an approach and two new metrics to provide a fine-grained assessment of post-hoc interpretability methods. We show that the robustness is generally linked to its coarse-grained performance.", "sections": [{"title": "Introduction", "content": "The increasingly widespread adoption of deep learning models across different fields is pushing an increasing demand for artificial intelligence (AI) transparency European Commission, Directorate-General for Com-munications Networks, Content and Technology [2021]. Transparency can be defined at many levels of the Al workflow, ranging from AI awareness, to AI model interpretability, and AI outcome interpretability. The focus of this work is on the latter \u2013 AI outcome interpretability. This aspect of transparency is concerned with explaining the input contributions to a given output. To achieve AI outcome transparency, several approaches have been proposed. These are usually categorized in ante-hoc (also referred to as self-explainable) and post-hoc interpretability. The former try to build AI models that are interpretable by construction \u2013 e.g., Li et al. [2018], Turb\u00e9 et al. [2024]. The latter refer to methods that allow probing black-box AI models and grasp an approximate view of what inputs where used for a certain output \u2013 e.g., Ribeiro et al. [2016]. Both ante-hoc and post-hoc interpretability methods can be used to assess whether decisions made by an AI model are trustworthy - i.e., the converge to human domain experts understanding of a given problem Mengaldo [2024]. This aspect may be useful for regulatory purposes in critical sectors such as the healthcare. They may also be used to shape AI model behaviour Nickl et al. [2024], by e.g., completing the dataset with relevant adversarial samples. More recently, it was additionally proposed that interpretability methods can also be useful for knowledge discovery, especially when the AI model provides a divergent view from human domain experts, where the latter can try to explain why the machine used certain inputs to reach a certain outcome Mengaldo [2024]. In this work, we focus on post-hoc interpretability methods. These methods were initially proposed for image classification tasks Zeiler and Fergus [2014], and they were later applied to text and time-series classification tasks Samek et al. [2017], Ismail et al. [2020]. In recent years, this field of interpretability has gained significant traction, leading to several model-agnostic methods to understand the contribution of input features to a given AI output Zhang et al. [2021]. An example of such traction is the open-source library Captum Kokhlikyan et al. [2020], an effort brought forward by Meta to encapsulate post-hoc interpretability methods under a unique software framework. However, despite the significant interest in this field, a critical issue remains: the discrepancy that different post-hoc interpretability methods provide when interpreting the same trained model on the same sample of a given dataset. For such task, all the different interpretability methods available should provide the same result, yet they frequently provide different relevance maps (maps of the input data deemed important by the AI model). This critical issue should be solved or at least mitigated, before moving towards reliable and useful post-hoc interpretations of deep learning models Lipton [2018], Rudin et al. [2022]. To mitigate and possibly address this issue, a few evaluation strategies of interpretability methods have been proposed in the literature. These are commonly based on the notion of corrupting (also referred to as occluding in computer vision) data points deemed relevant by the interpretability method, and evaluate how the output of the model changes Turb\u00e9 et al. [2023]. This allows understanding how accurately a post-hoc interpretability method captures the input data that was actually used by the deep learning model. Hence, they provide a way to rank post-hoc interpretability methods by the accuracy of their explanations. Yet, these evaluation methods usually take into account average performance across several samples within the dataset of interest. This coarse-grained evaluation is helpful to assess whether an interpretability method is providing explanations that are accurate on average. However, it does not provide a fine-grained view of how an interpretability method performs at sample level, and the sensitivity of the explanations for different samples. In other words, they do not assess the robustness of post-hoc interpretability methods. One may ask why this is important. Consider Figure 1. Here, we show the relevance map (i.e., the data portions deemed most important for the output of the trained model) produced by a post-hoc interpretability method, namely DeepSHAP Lundberg and Lee [2017], for a time series classification task. The red dots pinpoint the top-15% most important data portions for four different samples. On top of each subfigure, we report the normalized score drop \u2013 i.e., how much the output changed after corrupting the red dots. We note that for sample 1 and 2, the score drop is significant, while for sample 3 and 4, is extremely small. We also note a different clustering of red dots in the top two plots vs the bottom two plots. In the top two plots, the red dots are clustered in the high-frequency wave-pocket that constitutes the discriminative part for the learning task at hand. In the bottom two plots the red dots are distributed across the entire time series. If we were to compute an average score drop for DeepSHAP, we would obtain a value of 0.66. Yet, the performance of DeepSHAP for the top two vs the bottom two plots is remarkably different. Indeed, for sample 3 and 4, DeepSHAP seem to fail capturing the relevant portions of the data used by the deep learning model to classify the time series. In this work, we propose a new methodology for deep learning classification that provides a fine-grained understanding of post-hoc interpretability methods performance, thereby assessing their robustness. We frame robustness as the ability of post-hoc interpretability methods to pinpoint the relevant portion of data across all (or at least the majority) of the samples in a given dataset. This means having score drops that do not vary widely across samples, like in Figure 1. We note that the score drop depends on two factors: (i) the ability of the interpretability method to capture the most relevant portion of the data, and (ii) the sensitivity of the sample in question to data corruption or in other words, to how close the sample is to the decision boundary of the classification task. The first point provide a fine-grained quantification of how different post-hoc interpretability methods perform across samples, and it is the main target of this study. The second point might be used to grasp the sensitivity of each sample to corruption of the input data. To this end, we performed a calibration study and some tests in the appendix, that sheds light into this aspect. We show that we can use ridge-line plots (see Figure 4) as a fine-grained tool to visualize the distribution of the score drop across samples. The properties of the distribution provide quantitative metrics for assessing the robustness of different interpretability methods. In the literature, one commonly takes the average of the distribution to"}, {"title": "Relevance and related work", "content": "In this work, we rethink post-hoc interpretability robustness. In particular, we look at how robust post-hoc explanations are across samples, probing the properties of the score drop distribution arising of all samples in the dataset of interest. While the idea seems relatively trivial, this aspect has not been considered in the literature. Yet, it is extremely important to understand the robustness properties of interpretability methods, especially if we are interested in single samples of our data. Indeed, this is often the case when demanding interpretability results. Consider the case of a patient, who might be affected by a certain heart disease. If we use electrocardiogram (ECG) data to assess the potential disease of the patient, we can setup a classification task that probes the ECG time series and classifies them as belonging to a disease type. The subsequent task is to pass the results to a clinician (human-in-the-loop) who needs to make a diagnosis based on the AI results and the ECG data. At this point, the clinician may ask: what data the AI model used to take its decision for this particular patient? Different post-hoc interpretability methods will provide different answers to this question. Hence evaluating the accuracy of the explanations is as important as interpretability per se. However, this evaluation is commonly done considering the average accuracy of post-hoc interpretability on the entire patient population. This approach discounts other properties of the distribution, in addition to the average. We argue that these other properties are equally important, especially if we want to evaluate how consistent the interpretability results are across different samples (e.g., patients). In other words, they provide a measure of uncertainty regarding the explanations provided. Going back to the heart disease example, this means that we are now able to choose the interpretability method that gives the best results on average but that also provides robustly accurate (or less uncertain) explanations across different patients. This is of course critically important if we want to provide an accurate answer to the original question asked by the clinician. More specifically, it allows the clinician to view the interpretability results with associated uncertainty, aspect that is of paramount importance in critical sectors. Indeed, in high-risk application sectors, such as medicine, and finance, non-robust explanations may limit the use of deep learning solutions. This because non-robust explanations may undermine the pillars of deep learning deployment commonly accepted by humans, including: trust, informativeness, and fair and ethical decision-making Lipton [2018]. Pillars that are now being increasingly regulated European Commission, Directorate-General for Communications Networks, Content and Technology [2021]. In addition, having a granular understanding of the correct portions of input data used by deep learning models across samples can help to improve our knowledge of a given problem. It can, for instance, equip domain-expert with new ways of looking at a problem, thereby possibly uncovering causal relationships that were previously not understood."}, {"title": "Related work", "content": "The importance of evaluating interpretability has recently received increasing attention, leading to various attempts at evaluating post-hoc interpretability methods. A common strategy for evaluation is to remove portions of the input data deemed important for the prediction task to see the degradation of model performance Samek et al. [2016] \u2013 i.e., to see the change in probability (also referred to as normalized score drop) associated to the class identified by the model. If taken as is, without further considerations, this approach violates the i.i.d. assumption since the training and testing datasets generally come from different distributions. Hence, the decrease in model accuracy might not be due to the removal of relevant portions of input data, but because of the distribution shift between the training and the test dataset. The ROAR approach (RemOve And Retrain) Hooker et al. [2019] was introduced to address this issue. This approach retrains the deep learning model and evaluates the drop in score after occluding relevant portions of the input data. While ROAR retains a similar distribution between training and testing, it evaluates the retrained model rather than the initial one used for the prediction task. To address these limitations of existing evaluation methods, Turb\u00e9 et al. [2023] recently introduced data augmentation into the training dataset. The augmentation accounts for possible distribution shifts between training and testing, thereby solving the distribution shift and retraining issues. The same work also proposes two quantitative metrics $AUC_{Stop}$ and $F1_S$ to evaluate post-hoc interpretability methods. The works mentioned aim to evaluate the average performance of interpretability methods for a given testing dataset. Yet, as motivated in section 2.1, it is also critical to understand the behaviour of interpretability methods across samples \u2013 i.e., their robustness (or uncertainty). There are very few works that attempt to understand robustness of interpretability methods at the sample level. One such work introduces metrics to quantify the robustness (the term robustness is often used interchangeably to refer to stability/sensitivity Jyoti et al. [2022]) of local explanation methods, and argue that \u201csimilar inputs should give rise to similar explanations\" Alvarez-Melis and Jaakkola [2018], Alvarez Melis and Jaakkola [2018]. Built on this assumption, a few researchers measure sensitivity by calculating the difference in explanations with respect to the change in the input (i.e., infinitesimally small noise to the original instances). For instance, Yeh et al. [2019] and Bhatt et al. [2020] explore max sensitivity to measure the maximum change in explanations with a small perturbation of the input, while Fel et al. [2022] investigate the dissimilarity between two explanations to evaluate the generalizability and consistency. However, these works carry the risk that deep learning models have the same behavior on original and perturbed inputs. To overcome this limitation, Agarwal et al. [2022] propose new relative stability metrics to evaluate explanations via three aspects, including change in input, model representation, and output of the predictor. Another family of studies bridges adversarial robustness and interpretability evaluation by means of adversarial training or adversarial attacks on the input to measure the deviations from the original explanations Kim et al. [2019], Dombrowski et al. [2019], Heo et al. [2019], Sinha et al. [2021]. In this work, we rethink robustness from a different angle. We evaluate higher order moments of the score-drop distributions, proposing new analytics and metrics to probe and understand post-hoc interpretability performance."}, {"title": "Methodology", "content": "We consider a time series classification dataset $D = {X, c}$, where X is the time series input and c is the associated classification label. In this setting, each time series is defined as $X = (x_{m,t}) \\in R^{M \\times T}$, where M is the number of features, T is the number of time steps per feature, and $x_{m,t}$ represents the input feature m at time step t. Taking the time series as the input, neural networks produce a score S(X) for the predicted class c. Given class c, post-hoc interpretability methods assign relevance scores $R = (r_{m,t}) \\in R^{M \\times T}$ for the input feature m at time step t, which can be positive or negative. For simplicity, we drop the class c that post-hoc methods aims to interpret for the rest of the paper, and denote the score as S(X). Those time steps with positive scores (red) are identified to be in favor of the prediction, while the one with negative scores (blue) are against it (see contour map in Figure 2). Here, we focus on positive scores $R_+ := {r_{m,t}|r_{m,t} > 0}$ as we are interested in relevant time steps that neural networks use for learning. The methodology proposed here consists of two steps (described in section 3.1), a coarse evaluation of the average performance, followed by the evaluation of robustness or uncertainty of these average performance. For the latter step, we devised"}, {"title": "Coarse-to-fine-grained level evaluation", "content": "Coarse-grained evaluation aims to globally assess the ability of post-hoc interpretability methods in capturing relevant time steps in a dataset on average. To perform this evaluation step, we corrupt portions of the input time series associated to positive relevance, $R_+$, yielding to a corrupted input $\\bar{X}$. In particular, we corrupt different numbers of time steps based on $R_+ k$-percentile levels, where k represents the percentile level. Once the time series is corrupted for a certain k-percentile, the classifier produces a score, $S(\\bar{X})$. For more details on the corruption strategy, see Appendix A.1. Once the k-percentile corruption is performed for all samples X in the testing set at different values of k, we can assess the coarse-grained performance of interpretability methods, looking at the average of normalized score drop $\\bar{S(X)}$ (see Appendix A.1 for more details). To this end, we use the two metrics designed in Turb\u00e9 et al. [2023], namely $AUC_{STOP}^m$ and $F1_S^m$. The first measures the ability of post-hoc methods to identify the most relevant time steps see Appendix A.2. The second calculates an harmonic mean that evaluates the ability of the method to recognize both the most and least relevant time steps see Appendix A.2. We note that we use a slightly different notation from Turb\u00e9 et al. [2023], adding subscript m to emphasize that these metrics refer to mean (average) values. Indeed, these two metrics provide only a view of the average performance of post-hoc interpretability methods across an entire dataset. To understand the robustness of post-hoc interpretability methods across samples we propose to compute the distribution of the normalized score drop $\\bar{S(X)}$ via kernel density estimation (KDE). We perform this computation for each k-percentile corruption. In Figure 3, we show an example of the distribution obtained when corrupting the top 55% time steps identified by DeepSHAP for a trained Transformer model on our synthetic dataset (for more details on the dataset see Appendix B.1). The right plot of Figure 3 depicts four different distributions that can be associated to the robustness of post-hoc interpretability methods. A distribution similar to shape A indicates that the post-hoc method is robust for the majority of samples in the testing set. In other words, the post-hoc method can correctly capture relevant time steps for most samples, and corrupting these time steps will substantially impact the prediction. Conversely, shape B shows the poorest robustness where the post-hoc method hardly identifies relevant information for most samples (it provide more uncertain results). Shape C suggests less robustness, with a relatively mediocre drop for some samples compared to shape A. The bimodal shape D presents polarized performance across all samples, capturing relevant information in some samples but failing in others. Using k-percentile corruption from 5% to 95%, with 10% intervals, we obtain 10 different distributions of normalized score drops. We propose the use of ridge-line plots to visualize these distributions for a qualitative evaluation of post-hoc methods robustness. A ridge-line plot example is depicted in Figure 4, accompanied by detailed illustrations and discussions in section 4.1. Around the distributions provided in the ridge-line plots, we can define quantitative metrics that can better capture the robustness of post-hoc interpretability methods, and that are introduced next."}, {"title": "Fine-grained evaluation metrics", "content": "To evaluate robustness, we adopt two statistical metrics, skewness and kurtosis, to quantitatively characterize the distribution shapes introduced in section 3.1. On the one hand, skewness measures the asymmetry of the score drop distribution, and helps quantifying how much the normalized score drop is skewed to a certain side. On the other hand, kurtosis quantifies the tailedness and peakedness of a score drop distribution compared to a normal distribution. We use sample skewness, which can be computed as the Fisher-Pearson coefficient of skewness:\n$skew = \\frac{K_3}{K_2^{3/2}},$ where $K_i = \\frac{1}{L}\\sum_{n=1}^{L} (\\bar{S(X_n)}- \\bar{S}^m)^i ,$ (1)\nwhere, $k_i$ is the i-th biased sample central moment, $\\bar{S(X_n)}$ is the normalized score drop of the $n^{th}$ time series sample, $\\bar{S}^m$ is the mean normalized score drop under the top-k corruption strategy (i.e., when corrupting time steps from higher to lower k-percentile \u2013 see also Appendix A.1) across all L samples in the testing set. We use top-k as it provides information on whether post-hoc methods capture the most relevant time steps associated to the classification decision. The skewness coefficient, skew, can be zero, negative, or positive. Symmetric distributions similar to Shape B and D (bimodal distribution) in Figure 3 can both result in zero skewness. Negative skewness indicates a left-skewed (longer tail on the left side) distribution, while positive skewness indicates a right-skewed (longer tail on the right side) distribution. In this sense, the more negative the skewness, the more robust the post-hoc method is at each k-percentile corruption.\nFor kurtosis, we use the excess kurtosis, which is defined as Pearson's kurtosis (standard measure of kurtosis) minus 3:\n$(E)kurt = \\frac{K_4}{K_2^2}-3.$ (2)\nExcess kurtosis is a practically adopted measurement, allowing for easy comparison to the normal distribution with excess kurtosis of zero. Negative excess kurtosis represents a \u201cplatykurtic\" distribution (e.g., uniform distribution), while a positive excess kurtosis represents \u201cleptokurtic\" distribution (e.g., Laplace distribution). In terms of post-hoc interpretability robustness, the higher the positive value of kurtosis the less spread the distribution. Hence, having a large positive value of kurtosis, with a large negative value of skewness means that a post-hoc method is robust across the dataset.\nBy calculating skewness and kurtosis of score drop distributions, as in eq. (1) and eq. (2), we can quantitatively assess the robustness of post-hoc interpretability methods. For instance, distribution A in Figure 3 has a skewness of -6.00 and a kurtosis of 36.75, indicating its superior robustness (compared to the other distributions in the figure) in capturing informative time steps for most samples.\nIn order to evaluate post-hoc interpretability methods, we are interested in the skewness and kurtosis results across different k-percentile corruptions. To this end, we rescale both metrics to [0, 1], and create skew-k and (E)kurt-k curves. One example of these two curvesis shown in Figure 4, top-right panel, obtained for the synthetic dataset (see Appendix B.1 for more details on the dataset). From these plots, we can compute the area under the skew-k and (E)kurt-k curves, and define two quantitative metrics, namely $AUC_{Skew}$ and $AUC_{Kurt}$:\n$AUC_{Skew} = 1 - \\frac{1}{0.95}\\int_{0.05}^{1} skew dk,$ (3)\n$AUC_{Kurt} = \\frac{1}{0.95}\\int_{0.05}^{1} (E)kurt dk.$ (4)\nRecall that a post-hoc method is more robust when the skewness is more negative and the kurtosis is more positive. Accordingly, a method with better robustness is expected to have smaller scaled skewness and larger scaled kurtosis. $AUC_{Skew}$ is designed to be the inverse of the area under the skew-k curve $(1 \u2013 AUC_{Skew})$ in eq. (3). Thereby, regarding fine-grained quantitative evaluation, a post-hoc interpretability method is considered more robust when the two metrics, $AUC_{Skew}$ and $AUC_{Kurt}$, are larger."}, {"title": "Results", "content": "Datasets. We apply our evaluation framework to one synthetic dataset and 20 public datasets. The synthetic dataset is composed of four features, and was first proposed in Turb\u00e9 et al. [2023]. The 20 public datasets"}, {"title": "Qualitative evaluation: Ridge-line visualization", "content": "We consider the synthetic dataset case as an example to illustrate the proposed ridge-line visualization for a qualitative fine-grained post-hoc interpretability evaluation. In Figure 4, we show the ridge-line plots (left panel) of six post-hoc interpretability methods for the trained Transformer model applied to the synthetic dataset. The Transformer model achieves 92% accuracy on the testing set, composed of 1500 samples. Six post-hoc methods are deployed to generate relevance scores for the testing set. For each top-k percentile corruption from 5% to 95%, we show the score drop distribution, obtained through the KDE of the normalized score drop $\\bar{S(X)}$ for all 1500 samples. By visually inspecting the ridge-line plots, we can grasp a qualitative understanding of post-hoc methods robustness. A post-hoc method has better robustness if its distribution is left-skewed (negative skewness) and \u201cleptokurtic\" (positive kurtosis), starting at a relatively low k-percentile corruption. For instance, the ridge-line plot of DS in Figure 4, left panel, shows that for small k-percentile value, the distribution shifts significantly to the right, thereby showing that DS is able to capture the most informative time steps used by the classifier to make its decision across all samples in the testing set. From Figure 4, left panel, we can infer that DS, DL and SVS exhibit similar and superior robustness than e.g., KS, which demonstrates the poorest robustness among the six interpretability methods. These results are confirmed by the skew-k and (E)kurt-k curves shown in Figure 4, top-right panel, where the curves associated to the three methods are the lowest and highest for skewness and kurtosis, respectively. The peak of the distribution is located near 1.0 for all three methods, indicating that they identify relevant time steps for the majority of samples in the testing set. If we look at the worst performing method, namely KS, the score drop distribution shows that it can hardly identify informative points for almost all samples."}, {"title": "Quantitative evaluation: AUCSkew and AUCKurt", "content": "We consider the synthetic dataset to illustrate the use of the fine-grained quantitative metrics proposed, namely $AUCSkew^*$ and $AUCKurt^*$. Figure 4, bottom-right panel (table), shows the two fine-grained metrics for the Transformer model trained on the synthetic dataset and for all six post-hoc interpretability methods. In the same table we also show the coarse-grained metrics, $AUC_{STOP}^m$ and $F1_S^m$. For each experiment, we perform 5 repetitions, to get the mean and standard deviation of all metrics shown. We also rescale the metrics to [0, 1] across the six post-hoc methods for a comparative analysis. The standardized metrics are marked with superscript (*). The table in Figure 4, bottom-right panel, shows that DS is the best performing method for both coarse-grained and fine-grained metrics. This means that it provides the best coarse-grained (average) interpretations as well as the most robust ones (fine-grained). The second and third best performing methods in terms of coarse-grained metrics, namely SVS and DL, follow closely DS. Yet, when it comes to fine grained metrics, they seem to fall well behind DS, with scores that are significantly lower. Indeed, if we inspect the ridge-line plots (left panel in Figure 4), we observe that the distributions associated to SVS and DL start becoming skewed toward a score drop of 1 for higher k-percentile, as highlighted by the top-right panel in Figure 4. We also note that KS provides the worst coarse- and fine-grained interpretations. The quantitative assessment through coarse- and fine-grained metrics allows selecting the post-hoc inter-pretability method that performs best for the task at hand, in terms of average performance, and robustness (i.e., uncertainty). We note that robustness is generally linked to coarse-grained performance, yet, there can be differences in terms of methods ranking using coarse- vs fine-grained metrics. Depending on the specific application, the post-hoc method selection may not be trivial, and could value average performance vs robustness differently. In addition, practitioners may also take into account the computational costs associated to the interpretability methods, that may play a factor in the selection procedure."}, {"title": "Sensitivity to dataset and model", "content": "To thoroughly test the proposed evaluation framework, we use 3 different neural network architectures and 20 public datasets. Figure 5a and 5b show the the coarse-grained and fine-grained metrics of the six post-hoc interpretability methods (x-axis of each plot), where each plot-column represents a different neural network, i.e., the left plots depict CNN, the middle plots BiLSTM, and the right plots Transformer. For each post-hoc interpretability method applied to a neural network architecture, we display metrics of 20 datasets using circle dots. Additionally, box-plots visualize the statistical distribution of different post-hoc interpretability methods. DS is the best (coarse-grained) and the most robust (fine-grained) method for CNN. SVS consistently performs best, both coarse- and fine-grained for BiLSTM and Transformers models. For BiLSTM and Transformers IG, DL, DS, and GS have longer boxes, which means these methods have a larger interquartile range of metrics. It indicates these four methods successfully identify important information used by BiLSTM and Transformer models in some datasets but fail in others. As shown in Figure 5a, KS ranks last in all datasets across all three neural network architectures, indicating that it is unable to capture essential time steps that neural networks use for the learning task. Overall, SVS provide reliable results for the majority of architectures and datasets, both in terms of coarse- and fine-grained performance."}, {"title": "Sensitivity to noise", "content": "We investigate the sensitivity of the six interpretability methods to noise. We keep the same hyperparameters of CNN, BILSTM, and Transformer models trained on the synthetic dataset without noise, and train corresponding models for the dataset with three different amplitudes of Gaussian noise. The signal-to-noise ratio of these three versions of noise are, $SNR_{dB} = 20dB, 15dB, 10dB$. The detailed results can be found in Appendix E. Table E. I shows the sensitivity to noise of different neural network architectures and post-hoc methods. Note that KS ranks last among six methods based on coarse-grained metrics, and its high value of $AUC_{Kurt}^*$ is caused by the peak shape of score drop distribution around zero (that means poor fine-grained performance). After introducing noise in the synthetic dataset, SVS is the most robust post-hoc interpretability method for interpreting different classifiers, according to both coarse- and fine-grained metrics presented in table E. I."}, {"title": "Concluding remarks", "content": "We proposed a new way of thinking about the robustness and evaluation of post-hoc interpretability methods. In particular, we provided a new framework, namely coarse-to-fine-grained evaluation, as well as new metrics to evaluate the robustness of post-hoc interpretability methods. This entails understanding both the average performance of interpretability methods through a coarse-grained evaluation, and the uncertainty associated to these average performance through a fine-grained evaluation. The latter aspect is achieved by looking at the properties of the statistical distributions associated to the score drops obtained after corrupting relevant inputs. To this end, we developed two metrics that measure the skewness and the kurtosis of the distribution across different k-percentile corruptions, and allow for a quantitative understanding of the robustness (or uncertainty) of the coarse-grained evaluation. We tested the framework and metrics across 20 different public datasets, as well as 1 synthetic dataset for 3 different neural network architectures. The results show consistent results across neural networks and datasets, and are able to rank the methods in terms of average performance and robustness. The two aspects are frequently related, albeit not always, and practitioners may value average performance vs robustness differently depending of the specific applications. Yet, our framework allows for quantifying these two key aspects, and can make stakeholders, especially in critical application areas, more prone to use deep learning solutions. Our work is in line with increasing regulations of machine learning, and demand for transparency European Commission, Directorate-General for Communications Networks, Content and Technology [2021]."}, {"title": "Broader impact and ethical considerations", "content": "This work aims to improve the transparency of deep learning models, focusing on robustness, a key aspect for trustworthy machine learning. We do not envision any negative ethical implications of the current work, while we expect positive future societal outcomes, as we address trust issues towards deep learning models."}, {"title": "Corruption-based evaluation", "content": "Assuming that time steps with higher relevance scores are more important than others, we sort the positive relevance $R_+$ to form an ordered set ${R_e = (r_{m,t})}_{e=[MXT]^+}$, where $R_e$ is the $e^{th}$ element and $[M \u00d7 T]^+$ is the total number of points with positive relevance. Given that different post-hoc methods may recognize varying numbers of positive scores for one sample, we use k-percentile corruption rather than corrupting a constant number of time steps: $P_k = [M \u00d7 T]^+ \u00d7 k$, where $k \u2208 {0.05, 0.15, ..., 1.00}$ Meanwhile, to comprehensively understand the capability of post-hoc methods to identify the most and least relevant points, we conduct top-k and bot-k corruption strategies. Specifically, we sort the positive relevance in a descending order ${R_{top}^{d}}_{d=1}^{[MXT]^+}$ for the top-k corruption schema, while ascending order ${R_{bot}^{d}}_{d=1}^{[MXT]^+}$ for the bot-k schema. For each time series sample, we replace the first k% points with the points drawn from a Gaussian distribution, for both top-k and bot-k corruption schemes:\n$\\bar{X}_{m,t}^{top} := \\begin{cases} \\mathcal{N}(0,1) & \\text{if } r_{m,t} \\in {R_{top}^{d}}_{d=1}^{\\mathcal{R}_{top^{P_k}}} \\\\ X_{m,t} & \\text{otherwise} \\end{cases}$ $\\bar{X}_{m,t}^{bot} := \\begin{cases} \\mathcal{N}(0,1) & \\text{if } r_{m,t} \\in {R_{bot}^{d}}_{d=1}^{\\mathcal{R}_{bot^{P_k}}} \\\\ X_{m,t} & \\text{otherwise} \\end{cases}$ (A.1.2)\n(A.1.3)\nGiven two different inputs, X and $\\bar{X}$, the trained neural network has two different outputs, S(X) and $S(\\bar{X})$. Corrupting time steps can result in significant differences in the output of the neural network if one post-hoc method identifies the necessary time steps for a correct prediction. We introduce the normalized score drop to build quantitative metrics:\n$\\bar{S(X)} = \\frac{S(X) \u2013 S(\\bar{X})}{S(X)}$ (A.1.4)\nInterpretability evaluation in this work conduct on testing set $\\mathcal{T}$, thus all quantitative metrics aim to analyze the normalized score drop $\\bar{S(X)}$ for all testing samples X."}, {"title": "Coarse-grained metrics", "content": "After implementing top-k and bot-k corruption, we can calculate the mean score drop $\\bar{S}_{top}(X)$ and $\\bar{S}_{bot}(X)$ at each k-percentile corruption, respectively. Let $N \u2208 \\mathbb{Z}$ represent the total number of time steps for each sample, we obtain the mean ratio of corruption $\\tilde{N} = \\frac{\\bar{N}}{N}$, where $\\bar{N}$ is the mean number of time steps corrupted across all samples. This in mind, it is possible to build $\\bar{S}_{top}$ \u2013 $\\tilde{N}$ and $\\bar{S}_{bot}$ \u2013 $\\tilde{N}$ curves to track the mean normalized score drop as corruption increases, as shown in the example at the top of Figure 3.\nWith $\\bar{S_m}$ - $\\tilde{N}$ curve, we adopt two metrics from Turb\u00e9 et al. [2023"}]}