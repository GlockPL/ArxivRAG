{"title": "BONSAI: GRADIENT-FREE GRAPH DISTILLATION FOR NODE CLASSIFICATION", "authors": ["Mridul Gupta", "Samyak Jain", "Vansh Ramani", "Hariprasad Kodamana", "Sayan Ranu"], "abstract": "Graph distillation has emerged as a promising avenue to enable scalable training of GNNS by compressing the training dataset while preserving essential graph characteristics. Our study uncovers significant shortcomings in current graph distillation techniques. First, the majority of the algorithms paradoxically require training on the full dataset to perform distillation. Second, due to their gradient-emulating approach, these methods require fresh distillation for any change in hyperparameters or GNN architecture, limiting their flexibility and reusability. Finally, they fail to achieve substantial size reduction due to synthesizing fully-connected, edge-weighted graphs. To address these challenges, we present Bonsai, a novel graph distillation method empowered by the observation that computation trees form the fundamental processing units of message-passing Gnns. Bonsai distills datasets by encoding a careful selection of exemplar trees that maximize the representation of all computation trees in the training set. This unique approach imparts BONSAI as the first linear-time, model-agnostic graph distillation algorithm for node classification that outperforms existing baselines across 6 real-world datasets on accuracy, while being 22 times faster on average. BONSAI is grounded in rigorous mathematical guarantees on the adopted approximation strategies making it robust to GNN architectures, datasets, and parameters.", "sections": [{"title": "1 INTRODUCTION AND RELATED WORKS", "content": "Graph Neural Networks (Gnns) have shown remarkable success in various predictive tasks on graph data (Veli\u010dkovi\u0107 et al., 2018; Kipf & Welling, 2017; Hamilton et al., 2017). However, real world graphs often contain millions of nodes and edges making the training pipeline slow and computationally demanding (Hu et al., 2021). This limitation hinders their adoption in resource-constrained environments (Miao et al., 2021) and applications dealing with massive datasets (Hu et al., 2021). Graph distillation (also called condensation) has emerged as a promising way to bypass this bottleneck (Jin et al., 2021; Liu et al., 2024a;b; Zheng et al., 2023). The objective in graph distillation is to synthesize a significantly smaller distilled data set that retains the essential information of the original data. By training GNNS on these distilled datasets, we can achieve comparable performance while reducing computational overhead and storage costs. This makes GnNS more accessible and practical for a wider range of applications, including those with limited computational resources and large-scale datasets. In this work, we study the problem of graph distillation for node classification."}, {"title": "1.1 EXISTING WORKS AND THEIR LIMITATIONS", "content": "Table 1 presents existing graph distillation algorithms proposed for node classification and their characterization across various dimensions. We omit listing DosCOND (Jin et al., 2022), MIRAGE (Gupta et al., 2024), and KIDD (Xu et al., 2023) in Table 1 since they are designed for graph classification,"}, {"title": "1.2 CONTRIBUTIONS", "content": "To address the limitations outlined in Table 1, we present BONSAI\u00b9.\n\u2022 Gradient-free distillation: Instead of replicating the gradient trajectory, Bonsai emulates the distribution of input data processed by message-passing Guns. By shifting the computation task to the pre-learning phase, BONSAI achieves independence from hyper-parameters and model architectures as long as it adheres to a message-passing GNN framework like GAT (Veli\u010dkovi\u0107 et al., 2018), GCN (Kipf & Welling, 2017), GRAPHSAGE (Hamilton et al., 2017), GIN (Xu et al., 2019), etc. Moreover, this addresses a critical limitation of existing graph distillation algorithms that necessitates training on the entire training dataset.\n\u2022 Novel algorithm design: BONSAI is empowered by the observation that any message-passing GNN decomposes a graph of n nodes into n rooted computation trees. Furthermore, topologically similar computation trees generate similar embeddings regardless of the GNN being used (Xu et al.,"}, {"title": "2 PROBLEM FORMULATION AND PRELIMINARIES", "content": "Definition 1 (Graph). $G = (V,E, X)$ denotes a graph over a finite, non-empty node set V and edge set $E = \\{(u, v) | u, v \\in V\\}$. $X \\in \\mathbb{R}^{|V|\\times|F|}$ denotes node attributes encoded using F-dimensional feature vectors. We denote the attributes of node v as $x_v$.\nTwo graphs are identical if they are isomorphic to each other.\nDefinition 2 (Graph Isomorphism). Graph $G_1$ is isomorphic to graph $G_2$ if there exists a bijective mapping between their node sets that preserves both edge connectivity and node features. Specifically, $G_1$ is isomorphic to $G_2 \\Leftrightarrow \\exists f : V_1 \\rightarrow V_2$ such that: (1) f is a bijection, (2) $x_v = X_{f(v)}$, where $v \\in V_1, f(v) \\in V_2$ and (3) $(u, v) \\in E_1$ if and only if $(f(u), f(v)) \\in E_2$.\nThe problem of graph distillation for node classification is defined as follows.\nProblem 1 (Graph Distillation). Given train and validation graphs, $G_{tr}$ and $G_{val}$, respectively, and a memory budget b in bytes, synthesize a graph $G_s$ from $G_{tr}$ within the budget, while minimizing the error gap between $G_s$ and $G_{tr}$ on the validation set, i.e., minimize $\\{|E_{Gs} \u2013 E_{G_{tr}}|\\}$. Eg represents the node classification error on the validation set when trained on graph G."}, {"title": "2.1 COMPUTATION STRUCTURE OF GNNS", "content": "GNNS operate through an iterative process of information exchange between nodes. Let $x_v \\in \\mathbb{R}^{IFI}$ represent the initial feature vector of node $v \\in V$. The propagation mechanism proceeds as follows:\nInitialization: Set $h_v^0 = x_v, \\forall v \\in V$.\nMessage creation: In layer l, collect and aggregate messages for each neighbor.\n$m_v^l(u) = MESSAGE^l(h_u^{l-1}, h_v^{l-1}), \\forall u \\in N_v = \\{u | (u, v) \\in E\\}$\n$m_v^l = AGGREGATE^l(\\{{m_v^l(u) : u \\in N_v\\}})$\nUpdate embedding: $h_v^l = UPDATE^l(h_v^{l-1}, m_v^l)$\nHere, $MESSAGE^l$, $AGGREGATE^l$, and $UPDATE^l$ may be predefined operations (e.g., mean pooling) or learnable neural networks. $\\{\u00b7\\}$ denotes a multiset, accommodating potential message repetition from different neighbors. This process repeats for L layers, yielding the final node representations $h_v^L$.\nA computation tree, also known as a receptive field, describes how information propagates through the graph during the neural network's operation (Xu et al., 2019). Formally, it is defined as follows."}, {"title": "Definition 3 (Computation Tree)", "content": "Given a graph $G = (V,E,X)$, a node $v \\in V$ and the number of layers L in the Gnn, the computation tree $T_v^L$ rooted at v is constructed as follows:\n\u2022 Enumerate all paths of length L (including those with repeated vertices) starting from v.\n\u2022 Merge these paths into a tree structure where:\n  1. The root is always v, and\n  2. Two nodes from different paths are merged if: (i) They are at the same depth in their respective paths, and (ii) All their ancestors in the paths have already been merged."}, {"title": "Properties of computation trees", "content": "We now highlight two key properties of computation trees, formally established in Xu et al. (2019), that form the core of our algorithm design. These properties hold regardless of the underlying message-passing Gnn (GCN, Gat, GraphSAGE, GIN, etc.).\nProperty 1 (Sufficiency). In an L-layered Gnn, the computation tree $T_v^L$ is sufficient to compute node embedding $h_v^L, \\forall v \\in V$. Hence, given a graph $G = (V,E, X)$, we may treat it as a (multi)set of computation trees $T = \\{T_v^L | \\forall v \\in V\\}$. Clearly, $|T| = |V|$.\nProperty 2 (Equivalence). If $T_v^L$ is isomorphic to $T_{v'}^L$, then $h_v^L = h_{v'}^L$. This follows from the property that the expressive power of a message-passing Gnn is upper bounded by the Weisfeiler-Lehman test (1-WL) (Xu et al., 2019). This implies that if the L-hop neighborhoods of two nodes are indistinguishable by 1-WL, then their representations will be the same. The 1-WL test cannot differentiate between isomorphic computation trees (Shervashidze et al., 2011).\nMotivated with the above observations, we explore a relaxation of the Equivalence property: do nodes rooted at topologically similar computation trees generate similar node embeddings? To explore this hypothesis, we need a method to quantify the distance between computation trees. Given that the Equivalence property is derived from the 1-WL test, the Weisfeiler-Lehman kernel (Togninalli et al., 2019) presents itself as the natural choice for this distance metric."}, {"title": "Definition 4 (Weisfeiler-Lehman (WL) Kernel (Togninalli et al., 2019))", "content": "Given graph $G = (V,E,X)$, WL-kernel constructs unsupervised embeddings over each node in the graph via a message-passing aggregation. Like in Gnns, the initial embedding $a_v^0$ in layer 0 is initialized to $x_v$. Subsequently, the embeddings in any layer l is defined as:\n$a_v^l = \\frac{1}{2} (a_v^{l-1} + \\frac{1}{deg(v)} \\sum_{u \\in N(v)} \\omega((v, u)) a_u^{l-1})$   (1)\nHere, $N(v)$ denotes the neighbors of v, $deg(v) = |N(v)|$ and $\\omega((v, u)) = 1$ for unweighted graphs.\nBy emulating the same message-passing framework employed by GNNS, the unsupervised embedding $a_v^L$ jointly encapsulates the topology and node attributes within the computation tree $T_v^L$. Hence, the distance between two computation trees $T_v^L$ and $T_{v'}^L$ is defined to be:\n$d(T_v^L, T_{v'}^L) = ||a_v^L - a_{v'}^L||_2$   (2)\nWhile we use the L2 norm, one may use other distance functions over vectors as well. The following corollary follows from the Eq. 2.\nCorollary 1. If $T_v^L$ is isomorphic to $T_{v'}^L$, then $a_v^L = a_{v'}^L$.\nHypothesis 1 (Relaxed Equivalence). If $T_v^L \\approx T_{v'}^L$, then $a_v^L \\approx a_{v'}^L$, and, therefore $h_v^L \\approx h_{v'}^L$ irrespective of the specific message-passing architecture employed."}, {"title": "3 BONSAI: PROPOSED METHODOLOGY", "content": "Fig. 3 presents the pipeline of BONSAI, which is founded on the following logical progression:\n1. Similar computation trees produce similar GNN embeddings (Hypotheses 1).\n2. Similar GNN embeddings generate comparable outputs, resulting in similar impacts on the loss function and, consequently, on the gradients.\nBuilding on this reasoning, Bonsai aims to identify a set of b exemplar computation trees that optimally represent the full training set\u00b2. These exemplars are selected based on two critical criteria:\n\u2022 Representativeness: Each exemplar should be similar to a large number of other computation trees from the input training set, ensuring it captures common structural patterns. We quantify the representative power of a computation tree using the idea of reverse k nearest neighbors (\u00a7 3.1).\n\u2022 Diversity: The set of exemplars should be diverse, maximizing the coverage of different structural patterns present in the original graph (\u00a7 3.2).\nBy optimizing for both representativeness and diversity, BONSAI ensures that the union of computation trees similar to the chosen exemplars encompasses a broad and representative spectrum of the original graph's structural properties. The induced subgraph spanned by the nodes in these exemplar trees forms the initial distilled set. This initial version undergoes further refinement through an iterative process of edge sparsification and enrichment to reduce O(|E|) complexity of GNNS."}, {"title": "3.1 QUANTIFYING REPRESENTATIVENESS THROUGH REVERSE K-NN", "content": "Utilizing Sufficiency (Property 1), we decompose the input graph $G = (V,E,X) into a set of |V| computation trees T and then embed them into a feature space using WL-kernel (See Fig. 3). We now identify representative exemplars by analyzing this space.\nThe k nearest neighbors of a computation tree $T_v^L \\in T$, denoted as $k-NN(T_v^L)$, are its k nearest computations trees from T in the WL-kernel space (Def. 4).\nDefinition 5 (Reverse k-NN and Representative Power). The reverse k-NN of $T_v^L$ denotes the set of trees that contains $T_v^L$ in their k-NNs. Formally, $Rev-k-NN(T_v^L) = \\{T_{v'}^L | T_{v'}^L \\in k-NN (T_v^L)\\}$.\nThe representative power of $T_v^L$ is:\n$\\Pi (T_v^L) = \\frac{|Rev-k-NN(T_v^L)|}{|T|}$   (3)\nIf $T_{v'}^L \\in k-NN(T_v^L)$ for a small k, it suggests that $h_v^L \\approx h_{v'}^L$ (Hypothesis 1). Consequently, a high $\\Pi (T_v^L)$ indicates that $T_v^L$ frequently appears in the k-NN lists of many other trees and is therefore"}, {"title": "3.1.1 SAMPLING FOR SCALABLE COMPUTATION OF REVERSE K-NN", "content": "Computing k-NN for each tree consumes $O(n logk) \\approx O(n)$ time (since $k \\ll n$), where n = |V|. Since |T| = n, computing k-NN for all trees consumes O($n^2$) time. Hence, computing reverse k-NN for all trees consumes O($n^2$) time as well, which may be prohibitively expensive for graphs containing millions of nodes. To address this challenge, we employ a sampling technique that offers a provable approximation guarantee on accuracy. This is achieved as follows. Let us sample z\u226an trees uniformly at random from T, which we denote as S. Now, we compute the k-NN of only trees in S, which incurs O(zn) computation cost. We next approximate reverse k-NN of all trees in T based only on S. Specifically, $Rev-k-NN (T_v^L) = \\{T_{v'}^L \\in S | T_{v'}^L \\in k-NN (T_v^L)\\}$. The approximated representative power is therefore:\n$\\hat{\\Pi} (T_v^L) = \\frac{|Rev-k-NN(T_v^L)|}{|S|}$   (4)\nThe sample size z balances the trade-off between accuracy and computational efficiency. By applying Chernoff bounds, we demonstrate in Lemma 1 that z can be precisely determined to maintain the error within a specified error threshold 0, with a confidence level of 1 \u03b4.\nLemma 1. Given a desirable error upper bound 0 and a confidence interval of 1 \u2013 8, if we sample at least $z = \\frac{In (\\frac{3}{\\delta}) (2 + \\theta)}{\\theta^2}$ trees in S, then for any $T_v^L \\in T$:\n$P((\\hat{\\Pi}(T_v^L) \u2013 \\Pi (T_v^L) | \\leq \\theta) \\geq 1 \u2013 \\delta$  (5)\nPROOF. For the formal proof, see App. A.1. Intuitively, if a tree participates in the k-NN of a large percentage of trees from the full dataset, then it will continue to do so in the sampled set as well. Lemma 1 establishes this formally resulting in the following positive implications.\n\u2022 The number of samples needed is independent of the size of T.\n\u2022 Because z grows logarithmically with $In(\\frac{3}{\\delta})$, even a small sample size provides high confidence. Hence, the computation cost of Rev-k-NN reduces to O(zn) since z < n."}, {"title": "3.2 COVERAGE MAXIMIZATION", "content": "We aim to find the set of exemplar computation trees with the maximum representative power.\nDefinition 6 (The exemplars). Let the representative power of a set of trees A be denoted by:\n$\\Pi (A) = \\frac{| \\cup_{T_v^L \\in A} Rev-k-NN (T_v^L) |}{|T|}$   (6)\nThen, given the set of computation trees T and the budget b, we seek to identify the subset of computations trees, termed exemplars, A*, by maximizing the following objective:\n$A^* = \\underset{ \\forall A \\subset T, |A|=b }{ max } \\Pi(A)$   (7)\nFor brevity of discourse, we proceed assuming the true reverse k-NN set.\nTheorem 1. Maximizing the representative power of exemplars (Eq. 7) is NP-hard.\nPROOF. App. A.2 presents the formal proof by reducing to the Set Cover problem (Cormen et al., 2009).\nFortunately, Eq. 6 is monotone and submodular, which allows the greedy hill-climbing algorithm (Alg. 1) to approximate Eq. 7 within a provable error bound.\nTheorem 2. The exemplars, $A_{greedy}$, selected by Alg. 1 provides an 1 \u2013 1/e approximation, i.e., $\\Pi(A_{greedy}) \\geq (1 \u2013 \\frac{1}{e}) \\Pi(A^*)$."}, {"title": "3.3 SPARSIFICATION AND ENRICHMENT", "content": "Recall that the complexity of a GNN forward pass is linear to the number of edges in the graph. In the final step, we seek to sparsify the graph induced by the exemplar trees. Furthermore, in the additional space created due to sparsfication, we include more exemplar trees, resulting in further magnification of the representative power. This process is formalized as follows.\nLet $V_s$ be the node set in the initial distilled graph, which can be partitioned into two disjoint subsets: $V_{root}$ and $V_{ego}$, where $V_{root} = \\{v | \\exists T_v^L \\in A\\}$, the root nodes, that serve as the root node of some exemplar in $A_{greedy}$, and $V_{ego} = V_s \\backslash V_{root}$, the ego nodes, that are included because they fall within the L-hop neighborhood of a root node. We propose to employ Personalized PageRank (PPR) (Page et al., 1998) with teleportation only to root nodes to identify and prune ego nodes with minimal impact on the root node embeddings. Starting with timestamp t = 0, we proceed as follows:\n1. Compute the PPR distribution $\u03c0_t$ for t \u2265 0 defined by:\n$\\pi_t = (1 \u2013 \\beta) A \\pi_{t-1} + \\beta e$   (8)\nwhere: $\\pi_0[i] = \\frac{1}{|V_s|}$ for all i, e[i] = 1 if $V_i \\in V_{root}$, otherwise 0, A is the normalized adjacency matrix, \u03b2 is the teleportation probability.\n2. Obtain the PPR vector \u03c0 after convergence where $\u03c0_t = \u03c0_{t\u22121}$.\n3. Sort the nodes in \u03c0 by their PPR scores in descending order such that \u2200i, \u03c0[i - 1] \u2265 \u03c0[\u03af].\n4. Define the knee point $i_{knee}$ as:\n$i_{knee} = \\underset{i}{arg \\underset{i}{max}} \\{[i + 1] + \\pi[i - 1] \u2212 2 \\cdot \\pi[i]\\}$   (9)\n5. Prune all ego nodes with PPR scores below $\u03c0[i_{knee}]$.\n6. Use the freed-up space to include additional exemplar trees using Algorithm 1.\n7. Repeat steps 1\u20136 iteratively until $i_{knee}$ \u2265 0, where 0 is a threshold setting the minimum number of nodes to be removed.\nThe rationale for this pruning process can be outlined as follows. The PPR distribution $\u03c0_t$ iteratively updates based on the transition matrix A and teleportation vector e. Given that $\u03c0_0$ is initialized uniformly and e assigns higher probabilities to root nodes, $\u03c0_t$ converges to a distribution where nodes more connected to root nodes have higher scores (See Fig. 6 in Appendix). After convergence, the vector \u03c0 represents the steady-state probabilities of nodes in the graph. Sorting \u03c0 by descending scores ensures that nodes with the highest influence on root nodes are prioritized. The knee point $i_{knee}$ is identified as the maximum curvature in the sorted PPR scores. This point is where the rate of change in scores shifts most significantly, indicating a transition between nodes of high and low influence. Nodes with PPR scores below $\u03c0[i_{knee}]$ have minimal impact on root node embeddings and removing these nodes and edges incident on them effectively sparsifies the graph without substantial loss of information. Additional exemplar trees can then be incorporated into the available space using the greedy algorithm as given by Algorithm 1."}, {"title": "3.4 PROPERTIES OF BONSAI", "content": "Complexity analysis: The computation compelxity of Bonsai is O(|V| + |E|). The detailed derivation is provided in App. \u0410.4.\nCPU-bound and Parallelizable: BONSAI does not involve learning any parameters through gradient descent or its variants. Hence, the entire process is CPU-bound. Furthermore, all steps are embarrass-ingly parallelizable leading distillation of datasets with hundreds of millions of edges within minutes.\nIndependence from model-architecture and hyper-parameters: Unlike majority of existing dis-tillation algorithms that require knowledge of the GNN architecture and all training hyper-parameters,"}, {"title": "4 EXPERIMENTS", "content": "In this section, we benchmark BONSAI and establish:\n\u2022 Superior Accuracy: Bonsai consistently outperforms existing baselines in terms of accuracy across various compression factors, datasets, and Gnn architectures.\n\u2022 Enhanced Efficiency: On average, BONSAI is 350, 22, and 1500 times faster than the state-of-the-art baselines GCOND, GDEM and Gcsr, respectively, in distilling datasets.\n\u2022 Increased Robustness: Unlike existing methods that require tuning distillation-specific hyper-parameters for each combination of Gnn architecture, dataset, and compression ratio, our approach surpasses all baselines using a single set of parameters across all scenarios.\nOur implementation is available at https://anonymous.4open.science/r/bonsai."}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "The specifics of our experimental setup, including hardware and software environment, and hyper-parameters are detailed in App. B. For the baseline algorithms, we use the code shared by their respective authors. We conduct each experiment 5 times and report the means and standard deviations. Across all datasets, we maintain a train-validation-test split ratio of 60: 20:20."}, {"title": "4.2 PREDICTION ACCURACY", "content": "Table 4 presents the accuracies obtained by BONSAI and the baselines on GCN. Several insights emerge from this experiment. Firstly, BONSAI achieves the best accuracy in 14 out of 18 scenarios, with substantial improvements (\u2265 5%) over the second-best performer in several cases. This demonstrates that the unsupervised gradient-agnostic approach adopted by BONSAI does not come at the cost of accuracy. GDEM emerges as the second best perfomer. Gcsr fails to complete in the two largest datasets of Ogbn-arxiv and Reddit since it performs training on the full dataset 100 times to distill, which exceeds 5 hours.\nSecond, even at extreme compression ratio of 0.5%, it is possible to achieve reasonable accuracy levels. We highlight that past studies have denoted compression ratio in terms of number of nodes (recollect Table 2), which is misleading, since they generate fully connected weighted graphs, and thus real-compression in terms of bytes consumption, is lower."}, {"title": "4.3 CROSS-ARCHITECTURE GENERALIZATION", "content": "GDEM and BONSAI are both model-agnostic, meaning their distilled datasets are independent of the downstream GNN architecture used. In contrast, GCOND and GCSR produce architecture-specific distillation datasets, requiring separate training for each architecture. This distinction raises two im-portant questions: First, how well do the distilled datasets generated by GDEM and Bonsai generalize across different architectures? Second, if we apply the distillation datasets produced by GCOND and GCSR for GCN to other architectures, how significantly does it impact performance? We explore these questions in Table 5.\nConsistent with the earlier trend, BONSAI continues to outperform all baselines. We further observe that the performance gap between BONSAI and the baselines is wider in Gat and GIN compared to GCN (Table 4). For GCOND and GCSR, this wider gap is not surprising since they are trained on the gradients of GCN, which are expected to differ from those of GIN and GAT. GIN uses a Sum-Pool aggregation, while GAT employs attention to effectively dampen unimportant edges, with a potentially different eigen spectrum - the property that GDEM attempts to preserve. The approach of BONSAI, on the other hand, aligns more directly with the computational structure of GNNS. It focuses on analyzing the input space of computation trees and aims to represent as much of this space as possible within the given budget. This approach of retaining essential information, rather than mimicking a pre-determined proxy function assumed to be important for GNNS, leads to superior results."}, {"title": "4.4 DISTILLATION AND TRAINING EFFICIENCY", "content": "Distillation efficiency: Fig. 4a presents the time consumed by BONSAI and other baselines. We note that while BONSAI is CPU-bound, all other algorithms are GPU-bound. Despite being CPU-bound,"}, {"title": "4.5 ABLATION STUDY AND IMPACT OF PARAMETERS", "content": "Fig. 4b presents an ablation study comparing the full BONSAI method against two variants: BonsAI-PPR, which omits the use of Personalized PageRank, and BONSAI-Rev-k-NN, which substitutes the Rev-k-NN-based coverage maximization with random exemplar selection. The results reveal that the Rev-k-NN component generally has a more substantial impact on performance, with the exception of the Reddit dataset where PPR marginally dominates.\nThis observation aligns with expectations, as random exemplar selection would fail to comprehen-sively represent the input space. The PPR component, while important, typically has a lesser impact since its primary function is to sparsify the initially distilled graph by eliminating message passing paths that minimally affect the embeddings of selected exemplars.\nThe Reddit dataset presents a unique case where PPR has a more significant impact. This is attributed to the dataset's high average node degree of 100, in contrast to less than 14 in other networks. Consequently, within the given budget, very few exemplars can be selected due to their substantially larger L-hop neighborhoods. In this scenario, PPR plays a crucial role by pruning these expansive neighborhoods, thereby creating space for additional exemplars and improving overall performance."}, {"title": "5 CONCLUSIONS AND FUTURE WORKS", "content": "In this work, we have developed BONSAI, a novel graph distillation method that addresses critical limitations in existing approaches. By leveraging the fundamental role of computation trees in message-passing Guns, BonsAI achieves superior performance in node classification tasks across multiple real-world datasets. Our method stands out as the first linear-time, model-agnostic graph distillation algorithm, offering significant improvements in both accuracy and computational ef-"}, {"title": "Limitations and Future Works", "content": "While existing research on graph distillation has primarily focused on node and graph classification tasks, Guns have demonstrated their versatility across a broader spectrum of applications. In our future work, we aim to expand the scope of graph distillation by developing task-agnostic data distillation algorithms."}, {"title": "A APPENDIX", "content": "A.1 PROOF. OF THEOREM 1.\nRecall the notation that S \u2282 T denotes the subset of trees from which the representative power of all trees in T is being estimated. Let |S| = z.\nCorresponding to each $T_v^L$ \u2208 S, let us define an indicator random variable $X_v^L$ denoting whether $T_{v'}^L$ \u2208 k-NN($T_v^L$). Therefore, we have:\n$X = \\sum_{T_v^L \\in S} X_v^L = z\\hat{\\Pi} (T_v^L)$   (10)\nSince each tree in S is sampled uniformly at random from T, we have E($X_v^L$) = z\u03a0 ($T_v^L$). We also note that $X_v^L$ ~ Binomial(z, \u03a0 ($T_v^L$)). Given any \u0454 \u2265 0, from Chernoff bounds (Motwani & Raghavan, 1995), we have:\nUpper tail bound:\n$P (X_v^L \\geq (1+e)z\\Pi (T_v^L))\\leq exp(-\\frac{\\epsilon^2}{2+\\epsilon}z\\Pi (T_v^L))$   (11)\nLower tail bound:\n$P(X_v^L \\leq (1-e)z\\Pi (T_v^L)) \\leq exp(-\\frac{\\epsilon^2}{2}z\\Pi (T_v^L))$   (12)\nCombining the upper and tail bounds, we obtain:\n$P (|X_v^L \u2013 z\\Pi (T_v^L)| \\geq \\epsilon z\\Pi (T_v^L))$   (13)\n\u2194 P (|z\\hat{\\Pi}(T_v^L) \u2013 z\\Pi (T_v^L) | \\geq \\epsilon z\\Pi (T_v^L))$   (14)\n$P (\\hat{\\Pi}(T_v^L) \u2013 \\Pi (T_v^L) | \\geq \\epsilon\\Pi (T_v^L)) $   (15)\nPlugging 0 from Eq. 5 in Eq. 15, we obtain 0 = \u0454\u03a0 ($T_v^L$). Hence,\n$P (|(T) \u2013 II (T) | \u2265 0) \u2264 2 exp \\\\)   (16)\n$\\leq 2 exp(\\frac{\\theta^2}{2II(T)+\\theta})$   (17)\nFor any $T_v^L \\in T$, \u03a0 ($T_v^L$) \u2264 1. Thus,\n$P((\\hat{\\Pi}(T_v^L) \u2013 \\Pi (T_v^L) | \\geq \\theta) \\leq 2 exp(-\\frac{\\theta^2}{2 + \\theta})$   (18)"}, {"title": "Now, if we want a confidence interval of 1 \u03b4 (as stated in Eq. 5)", "content": "we have:\n$\\frac{\\delta}{2} \\geq exp(-\\frac{\\theta^2}{2+\\theta})$   (19)\n$\\frac{\\delta}{2} \\geq exp(-\\frac{\\theta^2}{2})$   (20)\n$\\ln{\\frac{\\delta}{2}} \\leq -\\frac{\\theta^2}{2+\\theta}$   (21)\n$\\ln{\\frac{\\delta}{2}} \\leq -\\frac{\\theta^2}{2}$   (22)\n$\\ln{\\frac{\\delta}{2}} \\leq -\\frac{\\theta^2}{2}$   (23)\n$z \\geq \\frac{\\ln{\\frac{3}{\\delta}} (2 + \\theta)}{\\theta^2}$   (24)\nHence, if we sample at least $\\frac{\\ln{\\frac{3}{\\delta}} (2 + \\theta)}{\\theta^2}$ trees in S, then for any $T_v^L$ \u2208 T:\n$\\hat{\\Pi}(T_v^L) \u2208 [\\Pi (T_v^L) \u2013 \\theta, \\Pi (T_v^L) + \\theta]$ with probability at least 1 \u2013 \u03b4.   (25)\nA.2 NP-HARDNESS: PROOF OF THEOREM 1"}, {"title": "A.3 PROOFS OF MONOTONICITY, SUBMODULARITY", "content": "Theorem 3 (Monotonicity). \u2200A' \u2283 \u0410, \u03a0(A') \u2013 \u03a0(A) \u2265 0, where A and A' are sets of computation trees.\nProof. Since the denominator in \u03a0(A) is constant, it suffices to prove that the numerator is monotonic. This reduces to establishing that:\n$\\bigcup_{T_v^L \\in A'} Rev-k-NN (T_v^L) \u2287 \\bigcup_{T_v^L \\in A} Rev-k-NN (T_v^L)$   (26)\nThis inequality holds true because the union operation is a monotonic function. As A' is a superset of A, the union over A' will always include at least as many elements as the union over A.\nTheorem 4 (Submodularity). \u2200A' \u2283 A, \u03a0(A' \u222a \\{T\\}) \u2013 \u03a0(A') \u2264 \u03a0(A \u222a \\{T\\}) \u2013 \u03a0(A), where A and A' are sets of computation trees and T is a computation tree."}, {"title": "A.4 COMPLEXITY ANALYSIS", "content": "For this analysis, we assume |V| = n and |E| = m. In sync with Fig. 3, the computation structure of BONSAI can be decomposed into four components.\n1. Embed trees into WL-"}]}