{"title": "A Transparency Paradox? Investigating the Impact of Explanation Specificity and Autonomous Vehicle Perceptual Inaccuracies on Passengers", "authors": ["Daniel Omeiza", "Raunak Bhattacharyya", "Marina Jirotka", "Nick Hawes", "Lars Kunze"], "abstract": "Transparency in automated systems could be afforded through the provision of intelligible expla-\nnations. While transparency is desirable, might it lead to catastrophic outcomes (such as anxiety),\nthat could outweigh its benefits? It's quite unclear how the specificity of explanations (level of\ntransparency) influences recipients, especially in autonomous driving (AD). In this work, we\nexamined the effects of transparency mediated through varying levels of explanation specificity\nin AD. We first extended a data-driven explainer model by adding a rule-based option for\nexplanation generation in AD, and then conducted a within-subject lab study with 39 participants\nin an immersive driving simulator to study the effect of the resulting explanations. Specifically,\nour investigation focused on: (1) how different types of explanations (specific vs. abstract)\naffect passengers' perceived safety, anxiety, and willingness to take control of the vehicle when\nthe vehicle perception system makes erroneous predictions; and (2) the relationship between\npassengers' behavioural cues and their feelings during the autonomous drives. Our findings\nshowed that passengers felt safer with specific explanations when the vehicle's perception system\nhad minimal errors, while abstract explanations that hid perception errors led to lower feelings\nof safety. Anxiety levels increased when specific explanations revealed perception system errors\n(high transparency). We found no significant link between passengers' visual patterns and their\nanxiety levels. Our study suggests that passengers prefer clear and specific explanations (high\ntransparency) when they originate from autonomous vehicles (AVs) with optimal perceptual\naccuracy.", "sections": [{"title": "1. Introduction", "content": "The automotive industry is witnessing an increasing level of development in the past decades, from manufacturing\nmanually operated vehicles to manufacturing vehicles with a high level of automation. Despite these technological\nstrides, accidents involving AVs continue to undermine public trust [59, 5, 38, 62, 44]. As highly automated vehicles\nmake high-stake decisions that can significantly affect end-users, the vehicles should explain or justify their decisions\nto meet set transparency guidelines or regulations, e.g., GDPR Article 12 [63] and the [31].\nAccompanying AVs' driving decisions with natural language explanations is one promising approach for better\nvehicle transparency [51, 26, 34]. This transparency, obtained through intelligible explanations, can help to reassure\npassengers of safety and also assist them in effectively calibrating their trust in an AV [32]. The specificity level of\nexplanations is, however, an important factor in achieving the aforementioned benefits. In real-world deployments,\nAVs may not always achieve perfect scene understanding due to the limitations of their perception systems. Depending\non the specificity level, explanations might reflect these flaws even when they are inconsequential. Informing operators\nabout these inherent imperfections could enhance safety by helping them recognise when to remotely take control of the\nvehicle [36] and it would equally help developers optimise for higher accuracy. While this transparency is helpful for\nthese groups, it remains uncertain whether in-vehicle passengers would prefer high level transparency (if at all useful)\nthat reveals such minor errors (such as mistaking a van for a bus). This is what we refer to the transparency paradox\nin this paper. It is therefore important to determine the appropriate level of transparency for in-vehicle passengers,\nmediated through the specificity of the explanations.\nFurthermore, as passengers are likely to engage in other activities during their ride, relying solely on visual cues\nto communicate awareness may be ineffective when passengers' attention is desired. Hence, auditory and vibrotactile\nfeedback[37] are also necessary to ensure passengers are adequately informed and can respond appropriately in critical\nsituations.\nIn this study, we investigate the effects of explanation specificity on AV passengers' perceived safety, feeling of\nanxiety, and desire to takeover control. We test two levels of explanation specificity: abstract and specific. We use the\nterm abstract to describe the provision of vague auditory explanations that conceal details about a driving situation\n(including perception system errors). In contrast, specific refers to the provision of very detailed and fine-grained\nexplanations about a situation. We found non-significant effect of explanation specificity on perceived safety. However,\nwe noticed a significant influence with respect to the amount of perception errors unveiled in the explanations (i.e.,\nbetween Specific (5) \u2014 Specific (50). Perception of safety dropped in the Specific (50) scenario compared to Specific\n(5). The feeling of anxiety also significantly increased with the increase in perception errors. There was a significant\neffect of explanations specificity on takeover feeling (Abstract \u2014 Specific (50)), but no significant difference noticed\nbetween Specific (5) and Specific (50). Lastly, no strong correlation was observed between the participants' behavioural\ncues and the aforementioned factors."}, {"title": "1.1. Contribution Statement", "content": "Overall, this research makes three contributions to the fields of explainable autonomous driving and human-\nmachine interaction:\n1. It sets out a new case study of explanation specificity in the presence of perception systems errors in the\nautonomous driving context;\n2. It provides an enhanced interpretable technique for generating textual and auditory natural language explanations\nfor AV navigation actions;\n3. It reveals experimental findings on whether high AV transparency, though critical to other stakeholders, is helpful\nto AV passengers."}, {"title": "2. Background", "content": "This section provides a background on explanations for automated decisions and in autonomous driving from the\nliterature."}, {"title": "2.1. Explanations for Automated Decisions", "content": "The concept of explanations has been studied extensively by a multidisciplinary group of scholars, ranging from\nPhilosophy to Psychology. Each adopts an idiosyncratic lens, characterising explanations in terms relevant to the goals\nof their respective disciplines. Therefore, to guide our study, we align with a definition proposed in an extensive survey\nof explanations in autonomous driving [51]: explanations are a piece of information presented in an intelligible way\nas a reason or part of a reason for an outcome, event or an effect in text, speech, or visual forms.\nRecent efforts around explanations have been mainly channelled towards complex Al systems (explainable\nAI [25, 1, 16]) to understand and communicate the reasons for the systems' decisions. Techniques developed for\nthis purpose fall under different categories based on their mode of operation. Some are model-specific [7] in that\nthey investigate the underlying AI algorithm in detail to support debugging tasks [43]. These types of explainers are\nintrinsic and model specific meaning that they are inherently coupled with the underlying algorithm. Other explanation\napproaches are classified as model-agnostic as they can assess the properties of an output independent of the algorithms\nused to realise the output [55, 41]. These explainers aim to help enhance user's knowledge of a system and potentially\nfoster trust."}, {"title": "2.2. Explanations in Autonomous Driving", "content": "Explanations have been found useful in enhancing user experience [56], trust [34, 26], and improved situational\nawareness [49, 40] in automated driving. Recent works have explored human factors in the application of explainable\nAI in AD. For instance, in [50, 48], a socio-technical approach to explainability was proposed. An interpretable\nrepresentation and algorithms for explanations based on a combination of actions, observations, and road rules were\ndesigned. Regarding explanations depths, the notion that explanations with higher levels of abstraction and correctness\nare superior has been argued in the literature [6, 24]. Additionally, Ramon et al. (2021) argued that the specificity\nof explanations should be tailored to the application context, noting that low-level specificity is often preferred by\nindividuals with a more deliberative cognitive style."}, {"title": "2.3. Research Questions", "content": "From the preceding literature review, we find the need to gain a deeper understanding of the effects of transparency,\nbrought about by natural language explanations of varying specificity, especially under imperfect AV perception\nsystems.\n1. Given varying levels of perception system errors, how do natural language explanations influence passengers'\nperceived safety?\n\u2022 H1.1 - Perceived Safety. Low transparency yields a higher perception of safety in an AV with perception\nsystem errors. We hypothesise that passengers feel safer in a low transparency AV, despite receiving\nabstract explanations. While individuals often seek the truth, many prefer information that aligns with\ntheir expectations [27]. Consequently, specific explanations may reveal perception system errors that\ncontradict passenger expectations. Additionally, research has shown that placebo explanations can have\nsimilar positive effects on people as real explanations [18].\n\u2022 H1.2 - Feeling of Anxiety. Passengers' feeling of anxiety increases with increasing perception system\nerrors in a highly transparent AV. We posit that there is a connection between perceived safety and the\nfeeling of anxiety [13, 53]. Therefore, explanations that frequently reference misclassified actors are likely\nto create a sense of insecurity, leading to increased anxiety.\n\u2022 H1.3 - Takeover Feeling. In highly transparent AVs, passengers are more likely to develop the feeling to\ntake over navigation control from the AV with higher perception system errors. Although passengers are\nnot able to take control in this study, we anticipated that they might nurse the thought to do so if they\nrepeatedly received illogical explanations from the AV.\n2. Do passengers' behavioural cues correlate with their feelings?\n\u2022 H2.1 - Visual Feedback Visual feedback from participants correlates with their feeling of anxiety.\nIndividuals with the feeling of anxiety might be usually hyper-aroused and sensitive to environmental\nstimuli. They may have difficulties concentrating, performing tasks efficiently, and inhibiting unwanted\nthoughts and distractions [28, 8]. Participants' fixation points and saccades should correlate with anxiety."}, {"title": "3. Passenger Study", "content": "In this section, we describe the participants' demographic, experiment apparatus setup, experiment design, and the\nprocedure of the experiment. The necessary approval to conduct the study was obtained from our University's Research\nEthics Committee."}, {"title": "3.1. Participants", "content": "We conducted a power analysis to estimate the number of subjects required for the study. Afterward, calls for\nparticipants were placed on various online platforms, such as the callforparticipants platform, university mailing\ngroups, university Slack channels, the research group website, and social media to recruit subjects. Upon screening, the\nfinal sample consisted of N = 39 participants (28 male, 11 female) ranging in age from 18 to 59 years. The participants\ncomprised students, university employees, and members of the callforparticipants platform. Although prior driving\nexperiences were not required, 28 (71.79 %) of the participants were licensed drivers. Only 2 of the 39 participants\n(5.13%) had experience with autonomous drives, however, in a research context. 6 (15.38%) of the participants had\nused a virtual reality headset for a driving game or driving experiment in the past."}, {"title": "3.2. Apparatus", "content": "3.2.1. Hardware\nThe hardware setup is shown in Fig. 1. We conducted the experiment in a driving simulator that comprised a GTR\narcade seat, Logitech G29 steering wheel with force feedback, turn signal paddles, brake and accelerator pedals, and\nan ultra-wide LG curved screen to display the experiment. A state-of-the-art virtual reality (VR) headset (with an\nimmersive 360\u00b0 FoV and an eye tracker) was also used to provide an immersive experience and high visual fidelity.\n3.2.2. Driving Software\nSoftware architecture is illustrated in ??. We adapted the DReyeVR [57], an open-source VR-based driving\nsimulation platform for behavioural and interaction research involving human drivers. DReyeVR was built atop\nCARLA [17], an open-source driving simulator for AD and Unreal Engine 4. DReyeVR provides a very realistic\nexperience with naturalistic visuals (e.g., in-vehicle mirrors) and auditory (e.g. vehicular and ambient sounds)\ninterfaces allowing for an ecologically valid setup. It also provides an experimental monitoring and logging system to\nrecord and replay scenarios, as well as a sign-based navigation system.\n3.2.3. Explainer Software\nAs shown in Fig. 2, we developed an explainer system based on previous work [48]. This system utilises a tree-based\nmodel trained on an annotated AV driving dataset that we collected in a prior project. While the original algorithm\nin [48] is primarily data-driven, we incorporated a rule-based technique to serve as a fallback when the data-driven\nmethod fails or makes an incorrect ego action prediction. The data-driven method employs a trained tree-based model\nto predict and generate explanations from detections obtained from CARLA, a driving simulator. In contrast, the\nrule-based approach relies on CARLA's ground truth data and follows predefined rules to determine which agents to\nreference in the explanations. By comparing predictions from the data-driven method with ground truth observations\nfrom CARLA, we can identify incorrect predictions. This enhanced explainer system, combining both data-driven\nand rule-based approaches, was used to generate preliminary explanations for our created scenarios. Wintersberger\net al. (2020) suggested types of traffic elements to be included in visual explanations based on user preferences. Our\nproposed explainer, however, selects traffic elements deemed important (feature importance [2]) by the driving model\nfor its decisions (see Algorithm 1).\nWe performed post-processing operations on the generated explanations, including fine-tuning some of the content\nand adjusting timestamps to ensure the explanations were delivered at the appropriate moments."}, {"title": "4. Experiment Design", "content": "Before the start of the trials, participants were asked to manually drive a vehicle for about two minutes in CARLA\nTown03-a complex town, with a 5-lane junction, a roundabout, unevenness, and a tunnel. 30 vehicles and 10\npedestrians were spawned in this town. This preliminary drive aimed to familiarise participants with the driving\nsimulation environment and to allow them to experience manual driving within the simulation.\nWe employed a within-subject design due to the limited sample size, which was insufficient for a between-subject\nstudy. Additionally, this design helped mitigate the potential co-founding factor of between-individual differences in a\nbetween-subject design."}, {"title": "4.1. Independent Variable", "content": "Combinations of transparency level (low and high) and AV perception errors (low and high) were done to obtain\nthe independent variable Scenarios. The first scenario (Abstract scenario) comprises abstract explanations indicating\nlow transparency and an undefined amount of perception system errors. The second scenario (Specific(5) scenario)\ncomprises specific explanations indicating high transparency and 5% amount of perception system errors indicating low\nerror degree. The third scenario (Specific(50) scenario) comprises specific explanations indicating high transparency\nand 50% amount of perception system errors indicating high error degree. The driving events that made up the different\nscenarios were carefully designed to include different driving conditions that are obtainable in the real world (See\nTable 1). Note that scenarios The scenario routes are shown in Figure 3.\ni. Abstract A scenario in CARLA Town10HD, which is about 4 minutes long (330 secs). Town10HD is an urban city\nenvironment with different infrastructures, such as an avenue or promenade, and realistic textures. Driving conditions\nare a combination of the events in Table 1. The perception system in this scenario might contain some errors, but\nthe explanations provided in this scenario were post-processed to always provide surface information which is vague\nenough to conceal perception errors. The rules governing explanations for this scenario were:\n\u2022 all traffic lights are referred to as \u2018traffic sign' without specifying the state (e.g., red, green, amber, off) of the\ntraffic light;\n\u2022 pedestrians are referred to as 'road users';\n\u2022 all non-human moving actors are referred to as 'vehicle'. This includes cycles, motorbikes, cars, etc.\nAn example explanation is 'stopping because of the traffic sign on my lane'. This obfuscates the type and colour of the\ntraffic sign.\nii. Specific(5) A scenario in CARLA Town10HD, which was about 4 minutes in length (256 seconds). Driving\nconditions in this scenario were a combination of the events in Table 1. The explanations generated in this scenario were\nspecific and detailed, exposing all errors. The perception system of the AV in this scenario was about 5% inaccurate.\nThis error value was estimated following the dynamic traffic agent classification model and confusion matrix provided\nby [4] and the traffic light classification model and confusion matrix by [45]. We were only interested in the confusion\nmatrices (and not the models). The confusion matrices helped us to systematically introduce the 5% perception system\nerrors during the post-processing stage of the explanations. In this scenario, the 5% error resulted in one explanation\n(1 out of the 22) being erroneous as the explanation exposed the misclassification errors from the perception system.\nAn example of an erroneous explanation is: 'van ahead on my lane'. Here, a car was misclassified as a van.\niii. Specific(50) A scenario in CARLA Town10HD, which was 4 minutes in length (274 seconds). Driving conditions\nwere a combination of the events in Table 1. The explanations generated in this scenario were as fine-grained/specific\nand detailed as those in the Specific(5) scenario. The perception system error of the AV in scenario Specific(5)\nwas significantly noised to reach a reduced accuracy of 50%. We assumed that this reduction in accuracy might be\nsufficient to influence peoples' behaviour. Therefore, half of the explanations in this scenario (12 out of 24) reflected\nmisclassification of actors or actor states. An example of an erroneous explanation is 'moving because traffic light is\nswitched off on my lane'. In this case, the perception system failed to identify a green light accurately.\nNote that all three scenarios were designed so that the AV perception errors were insignificant to the AV's navigation\nactions. Hence, the AV respected all road rules and avoided collisions. This was important as the state-of-the-art AVs\nwould likely not make obvious navigation errors. Moreover, we were interested in the effects of the awareness of\ninconsequential perceptual errors in AVs. Hence, it was necessary to introduce artificial errors of varying degrees\n(low and high). The non-influence of AV perception errors on navigation control also helped to avoid the confounding\nfactors of route navigation problems. Further, we counterbalanced the routes across scenarios. That is, the AV's route\nwas different in each scenario. This design decision was made to reduce carry-over effects on the participants. With\nthis setup, the scenarios were still comparable as they were all within the same town, and the routes shared similar\nfeatures. Each scenario also had a balanced combination of the events listed in Table 1. In all the scenarios, the AV\nmaintained a speed below 30mph, the recommended speed limit in urban areas in the UK. See Figure 4 for sample\nscenes from each scenario and their corresponding explanations."}, {"title": "4.2. Dependent Variables", "content": "There were six dependent variables: Perceived Safety, Feeling of Anxiety, Takeover Feeling, Fixation Divergence,\nSaccade Difference, and Button Presses. These variables were categorised into two (psychological factors and\nbehavioural cues) for easy analysis and reporting.\nPsychological Factors These factors include Perceived Safety, Feeling of Anxiety, and Takeover Feeling. They\nwere mainly measured using items from the Autonomous Vehicle Acceptance Model Questionnaire (AVAM) [29].\nAVAM is a user acceptance model for autonomous vehicles, adapted from existing user acceptance models for generic\ntechnologies. It comprises a 26-item questionnaire on a 7-point Likert scale, developed after a survey conducted to\nevaluate six different autonomy scenarios.\nItems 24-26 were used to assess the Perceived Safety factor, while items 19\u201421 were used to assess the Feeling of\nAnxiety factor. Similar to [56], we introduced a new item to assess participants' feelings to takeover navigation control\nfrom the AV during the ride (Takeover Feeling). Specifically, participants were asked to rate the statement 'During the\nride, I had the feeling to take over control from the vehicle' on a 7-point Likert scale. Actual navigation takeover by\nparticipants was not permitted because we wanted to be able to control the entire experiment and have all participants\nexperience the same scenarios. Moreover, we were dealing with L4 automation. Though participants were not expected\nto drive or take over control, they might have nursed the thought to do so. This is what the Takeover Feeling variable\nmeasures.\nWe added a free-response question related to explanations with the aim of obtaining qualitative data for triangu-\nlating quantitative results. Participants were asked the following question: 'What is your thought on the explanations\nprovided by the vehicle, e.g., made you less/more anxious, safe, feeling to take over control?'. We refer to the resulting\nquestionnaire as the APT Questionnaire (i.e., A-Anxiety, P-Perceived Safety, T-Takeover Feeling)."}, {"title": "4.3. Procedure", "content": "The procedure of the experiment is illustrated in Fig. 5. After all preliminary form completions and briefings, we\nintroduced the physical driving simulator and explained the subsequent steps, which involved a pre-experiment manual\ndriving session in VR mode lasting for 2 minutes. Participants were informed that the purpose of this pre-experiment\nexercise was to familiarise them with the simulation environment and to identify individuals prone to motion sickness,\nwho would then be excluded from the main experiment.\nUpon completion of the manual driving exercise, the researcher removed the VR headset from the participant\nand explained the aim and procedure of the main experiment. The instructions included the following statements:\n'you would experience 3 autonomous rides by different vehicles, [...] and after each ride, you would complete a short\nsurvey. The vehicle drives along a predefined path for about 4 minutes and provides explanations for its planned driving\ndecisions and announces relevant objects in its environment. [...]. The vehicle tells you its next direction at a junction\nor an intersection using its right or left red light indicators on its dashboard accordingly.[...] Simply click any of these\nbuttons if the decision or the explanation of the vehicle makes you feel confused, anxious or unsafe [...]'. The researcher\nthen placed the VR headset back on the participant and initiated the scenarios. Complete counterbalancing was applied\nto the scenario treatments, resulting in six different orders of scenarios. Each participant experienced the scenarios in\none of these six orders, with approximately six participants per order.\nParticipants were encouraged to rest briefly after each driving experience, with the VR headset removed. A short\ndebriefing session followed the study, and participants were given a \u00a310 Amazon gift card. The entire experiment lasted\napproximately 50 minutes.\nThe researcher also participated in the experiment, experiencing all three scenarios. Throughout, the researcher\nfocused on the lane ahead and the actors referenced by the explanations. Neither erroneous nor abstract explanations\ninfluenced the researcher's focus, as the researcher remained attentive to the lane and the actors or obstacles impacting\nthe AV's actions, regardless of the explanations. This consistency was due to the researcher's familiarity with all\nscenarios. The data from the researcher served as a reference or ground truth. Notably, the researcher's eye movements\nmatched normal human saccadic velocity, which reaches 300-400\u00b0/seconds [54, 65]."}, {"title": "5. Quantitative Results", "content": "5.1. Psychological Factors Analysis\nTo test our hypotheses listed in Section 2.3, we analyzed data from the three APT questionnaires. We created a latent\nvariable (Feeling of Anxiety) by averaging responses to AVAM Items 19\u201421, and another latent variable (Perceived\nSafety) by averaging responses to AVAM Items 24\u201326. We calculated Cronbach's Alpha (\u03b1) for the independent\nvariables that formed the latent dependent variables to ensure they had adequate internal consistency. Results with\nan adjusted p-value less than 0.05 (p < .05) were considered significant. P-values were adjusted using Bonferroni\ncorrections, where the calculated p-values were multiplied by the number of scenarios, to reduce the likelihood of Type\nI errors (false positives). Normality tests, including the Kolmogorov-Smirnov, Shapiro-Wilk, and Anderson-Darling\ntests, indicated a violation of normality in the Feeling of Anxiety, Perceived Safety, and Takeover Feeling factors. Hence,\nwe performed a Friedman test for these dependent variables, see Table 3 and Figure 6.\nH1.1 - Perceived Safety Low transparency yields a higher perception of safety in an AV with perception system\nerrors.\nA Friedman test was conducted. No significant difference was found in the scenario pair: Abstract - Specific(5),\nand the pair: Abstract - Specific(50). In fact, the perceived safety mean rank in the Specific(5) scenario (2.22) was\nhigher than that in the Abstract scenario (2.15), see Table 3. Therefore, there was no sufficient evidence in support of\nhypothesis H1.1.\nH1.2 - Feeling of Anxiety Passengers' feeling of anxiety increases with increasing perception system errors in a\nhighly transparent AV. A Friedman test indicated a significant difference in the Feeling of Anxiety across scenarios,\nH(2) = 13.32, p = .001. The pairwise scenario comparisons of Abstract - Specific(50) and Specific(5) - Specific(50)\nresulted in an adjusted p-value of .003 and .01 respectively (see Table 3). Hence, there is strong evidence in support of\nhypothesis H1.2.\nH1.3 - Takeover Feeling In highly transparent AVs, passengers are more likely to develop the feeling to take over\nnavigation control from the AV with higher perception system errors. A Friedman test showed a significant difference\nin Takeover Feeling across scenarios, H(2) = 6.27,p = .044. While the pairwise scenario comparison of Abstract\nSpecific(50) resulted in an adjusted p-value of .017, the pairwise comparison of Specific(5) - Specific(50) resulted\nin an adjusted p-value of 0.61. Hence, there is no significant difference in Takeover Feeling between Specific(5) and\nSpecific(50) scenarios, and therefore, no evidence in support of hypothesis H1.3 (see Table 3).\n5.2. Behavioural Cues Analysis\nH2.1 - Visual Responses Visual feedback from passengers correlates with passengers' anxiety. At this stage, we\nutilised the reference data from the researcher. We estimated the Euclidean distances between participants' fixation\npoints and the reference fixation points over time for each participant.\nResults from Spearman correlation showed that there was no significant association between the Feeling of Anxiety\nand Fixation Divergence, r(115) = \u22120.07,p = .442. See the fixation divergence plot in Figure 7. Results from\nSpearman correlation showed that there was no significant association between the Feeling of Anxiety and saccade\ndifference, r(115) = 0.1, p = .281. However, there was a significant association between perceived safety and saccade\ndifference, r(115) = \u22120.25, p = .007., indicating a weak negative correlation between perceived safety and saccade\ndifference. Hypothesis H2.1, therefore, has no sufficient support. See the saccade difference plot in Figure 8.\nIn addition to correlation, we checked for significant differences. There was a significant difference in Fixation\nDivergence between Abstract and Specific(5) with an adjusted p-value of .028, and between Specific(5) and Specific(50)"}, {"title": "5.2.1. Haptic Response", "content": "Participants were asked to press a button on the Logitech wheel when they felt confused, anxious, or unsafe by\nthe explanations or the decision of the AV during the ride. Spearman rank correlation was used as a measure to\ninvestigate monotonic associations. There was a weak negative correlation between the variables Perceived Safety\nand ButtonPress (r(115) = -0.31, p = .001), a weak positive correlation between the Feeling of Anxiety and\nButtonPress (r(115) = 0.31, p = .001), and insignificant correlation between the Feelings to Takeover and ButtonPress\n(r(115) = 0.15, p = .099).\nWe also checked for statistically significant differences in Button Presses across scenarios. There was a significant\ndifference in ButtonPresses, H(2) = 15.44, p < .001. This was specifically in the pairs: Abstract - Specific(50) with\nadjusted p-value .002, and Specific(5) - Specific(50) with adjusted p-value .005. See Figure 9 for behavioural cues\nresults."}, {"title": "6. Qualitative Results: Themes and Reflections", "content": "We obtained qualitative data from the APT questionnaire administered after every scenario. Participants were asked\nto describe their feelings regarding the explanations they received during the ride. Table 5 and Figure 10 describe the\nthemes obtained from the inductive thematic analysis of the comments. Themes are broadly categorised based on the\nparticipants' feelings, their assessment of the explanations, and the vehicle dynamics.\nPerceptual errors in the Specific(50) scenario evoked negative emotions of anxiety, feeling to takeover navigation\ncontrol and distrust. CAND1 expressed a feeling of anxiety: 'The explanations made me feel a bit anxious, it says many\nthings that were not right and misleading. I had the urge to look at the buildings and the environment but could not\nreally do that because I wanted to be sure the vehicle is taking the right decision.'. CAND39 expressed the urge to\ntakeover navigation control: \u2018When the explanations are false, e.g. 'a cyclist is crossing my lane', and it is actually\na pedestrian, it made me slightly anxious and likely to want to take over. But nevertheless, I felt safe in the vehicle'.\nCAND5 expressed distrust in the AV: \u2018anxious as the vehicle did not correctly understand the environment and the\ntypes of vehicles around it, which made me trust its judgement less'. More participants expressed a feeling of safety in\nthe Specific(5) scenario: 'felt safe that the vehicle understood the road and what was going on around us'. About the\nsame number of participants expressed a decline in their feeling of anxiety in the Abstract and Specific(5) scenarios.\nAn example is CAND34's comment about the abstract scenario: \u2018When the explanations provided are more general,\ne.g. 'vehicle' instead of 'van' and 'road user' instead of 'cyclist', it feels like the vehicle has a better understanding of\nthe surroundings because it gives a correct explanation, so I felt less anxious and unsafe'. The abstract explanations\nmight have concealed some errors, in turn, reducing the feeling of anxiety.\nThere were specific comments about the explanations across the three scenarios. Many participants thought that\nthe explanations in the Specific(5) were plausible in that they sounded correct and aligned with what the participants\nsaw. For example: \u2018Explanations were clear and made sense. Still don't feel some of the reactions were as quick as\nI might have made them\u2019\u2014CAND14. There were a good number of comments around the implausible nature of the\nexplanations in the Specific(50) scenario. For example, CAND20 said, \u2018The vehicle this time had difficulty giving\nthe correct reason for stopping/going. Couldn't tell the difference between a pedestrian and a cyclist sometime or\nthought that traffic lights were off instead of green. I feel that this time I would have wanted more control over the car,\nparticularly at traffic lights as I could determine better if a traffic light was 'working' or not'.\nA couple of candidates thought that the explanations in the Abstract scenarios were either too early or late. For\nexample, 'The explanations should have arrived a bit earlier, like a few meters before the vehicle actually stops so that\nI will know that it is planning to stop. Also, I would be more comfortable if the explanation 'traffic sign' was 'traffic\nlight is red/green'. when referring to a traffic light.'\u2014CAND19.\nSome interesting comments were made about the vehicle's driving style and its interior. For example, CAND31\nmade a comment about the careful manoeuvre of the vehicle in the Specific(50) scenario: \u2018I was calm throughout the\njourney. There was no feeling of anxiety as the vehicle did not speed too much to make me feel that way.\u2019\u2014CAND31.\nThere was a comment relating to aggressive manoeuvre in the Abstract scenario: \u2018Seemed like oncoming vehicles\nwere going to collide with me. It seems to sometime drive on pavements when negotiating corners.'\u2014CAND35. The\nrotating steering wheel of the vehicle made some of the participants uncomfortable: \u2018The steering wheel moving\nabruptly startled me sometimes.\u2019\u2014CAND21 (Specific(5) scenario). Some participants liked the vehicle indicators and\nthe sound they made when indicating the next directions. \u2018The indicator sound was nice to hear. [...]'\u2014CAND6\n(Specific(50) scenario)."}, {"title": "7. General Discussion", "content": "We examined the effects of explanation specificity (abstract and specific) in AD, while accounting for varying\ndegrees of perception system errors (low and high). We focused on how this setup would impact passengers'\nperceived safety and related factors, such as the feeling of anxiety and the thought to takeover control. Our results\nnot only corroborate but also extend previous findings in the field, among others, demonstrating that while intelligible\nexplanations generally create positive experiences for AV users [50, 26, 56, 42", "52": ".", "14": "."}]}