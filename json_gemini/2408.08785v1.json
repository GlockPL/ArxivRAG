{"title": "A Transparency Paradox? Investigating the Impact of Explanation Specificity and Autonomous Vehicle Perceptual Inaccuracies on Passengers", "authors": ["Daniel Omeiza", "Raunak Bhattacharyya", "Marina Jirotka", "Nick Hawes", "Lars Kunze"], "abstract": "Transparency in automated systems could be afforded through the provision of intelligible explanations. While transparency is desirable, might it lead to catastrophic outcomes (such as anxiety), that could outweigh its benefits? It's quite unclear how the specificity of explanations (level of transparency) influences recipients, especially in autonomous driving (AD). In this work, we examined the effects of transparency mediated through varying levels of explanation specificity in AD. We first extended a data-driven explainer model by adding a rule-based option for explanation generation in AD, and then conducted a within-subject lab study with 39 participants in an immersive driving simulator to study the effect of the resulting explanations. Specifically, our investigation focused on: (1) how different types of explanations (specific vs. abstract) affect passengers' perceived safety, anxiety, and willingness to take control of the vehicle when the vehicle perception system makes erroneous predictions; and (2) the relationship between passengers' behavioural cues and their feelings during the autonomous drives. Our findings showed that passengers felt safer with specific explanations when the vehicle's perception system had minimal errors, while abstract explanations that hid perception errors led to lower feelings of safety. Anxiety levels increased when specific explanations revealed perception system errors (high transparency). We found no significant link between passengers' visual patterns and their anxiety levels. Our study suggests that passengers prefer clear and specific explanations (high transparency) when they originate from autonomous vehicles (AVs) with optimal perceptual accuracy.", "sections": [{"title": "1. Introduction", "content": "The automotive industry is witnessing an increasing level of development in the past decades, from manufacturing manually operated vehicles to manufacturing vehicles with a high level of automation. Despite these technological strides, accidents involving AVs continue to undermine public trust [59, 5, 38, 62, 44]. As highly automated vehicles make high-stake decisions that can significantly affect end-users, the vehicles should explain or justify their decisions to meet set transparency guidelines or regulations, e.g., GDPR Article 12 [63] and the [31].\nAccompanying AVs' driving decisions with natural language explanations is one promising approach for better vehicle transparency [51, 26, 34]. This transparency, obtained through intelligible explanations, can help to reassure passengers of safety and also assist them in effectively calibrating their trust in an AV [32]. The specificity level of explanations is, however, an important factor in achieving the aforementioned benefits. In real-world deployments, AVs may not always achieve perfect scene understanding due to the limitations of their perception systems. Depending on the specificity level, explanations might reflect these flaws even when they are inconsequential. Informing operators about these inherent imperfections could enhance safety by helping them recognise when to remotely take control of the vehicle [36] and it would equally help developers optimise for higher accuracy. While this transparency is helpful for these groups, it remains uncertain whether in-vehicle passengers would prefer high level transparency (if at all useful) that reveals such minor errors (such as mistaking a van for a bus). This is what we refer to the transparency paradox in this paper. It is therefore important to determine the appropriate level of transparency for in-vehicle passengers, mediated through the specificity of the explanations.\nFurthermore, as passengers are likely to engage in other activities during their ride, relying solely on visual cues to communicate awareness may be ineffective when passengers' attention is desired. Hence, auditory and vibrotactile feedback[37] are also necessary to ensure passengers are adequately informed and can respond appropriately in critical situations.\nIn this study, we investigate the effects of explanation specificity on AV passengers' perceived safety, feeling of anxiety, and desire to takeover control. We test two levels of explanation specificity: abstract and specific. We use the term abstract to describe the provision of vague auditory explanations that conceal details about a driving situation (including perception system errors). In contrast, specific refers to the provision of very detailed and fine-grained explanations about a situation. We found non-significant effect of explanation specificity on perceived safety. However, we noticed a significant influence with respect to the amount of perception errors unveiled in the explanations (i.e., between Specific (5) \u2014 Specific (50). Perception of safety dropped in the Specific (50) scenario compared to Specific (5). The feeling of anxiety also significantly increased with the increase in perception errors. There was a significant effect of explanations specificity on takeover feeling (Abstract \u2014 Specific (50)), but no significant difference noticed between Specific (5) and Specific (50). Lastly, no strong correlation was observed between the participants' behavioural cues and the aforementioned factors."}, {"title": "1.1. Contribution Statement", "content": "Overall, this research makes three contributions to the fields of explainable autonomous driving and human-machine interaction:\n1. It sets out a new case study of explanation specificity in the presence of perception systems errors in the autonomous driving context;\n2. It provides an enhanced interpretable technique for generating textual and auditory natural language explanations for AV navigation actions;\n3. It reveals experimental findings on whether high AV transparency, though critical to other stakeholders, is helpful to AV passengers."}, {"title": "2. Background", "content": "This section provides a background on explanations for automated decisions and in autonomous driving from the literature."}, {"title": "2.1. Explanations for Automated Decisions", "content": "The concept of explanations has been studied extensively by a multidisciplinary group of scholars, ranging from Philosophy to Psychology. Each adopts an idiosyncratic lens, characterising explanations in terms relevant to the goals of their respective disciplines. Therefore, to guide our study, we align with a definition proposed in an extensive survey of explanations in autonomous driving [51]: explanations are a piece of information presented in an intelligible way as a reason or part of a reason for an outcome, event or an effect in text, speech, or visual forms.\nRecent efforts around explanations have been mainly channelled towards complex Al systems (explainable AI [25, 1, 16]) to understand and communicate the reasons for the systems' decisions. Techniques developed for this purpose fall under different categories based on their mode of operation. Some are model-specific [7] in that they investigate the underlying AI algorithm in detail to support debugging tasks [43]. These types of explainers are intrinsic and model specific meaning that they are inherently coupled with the underlying algorithm. Other explanation approaches are classified as model-agnostic as they can assess the properties of an output independent of the algorithms used to realise the output [55, 41]. These explainers aim to help enhance user's knowledge of a system and potentially foster trust.\nExplanations must possess certain properties to be effective to their intended recipients. Mittelstadt et al. [47] argued that the risk of conflicts in communicating explanations when the explainer (explanation provider) and the explainee (explanation recipient) have different motives may be mitigated through social, selective, and contrastive explanations. Social in the sense that the explanation process involves different parties and the explainer is able to model the expectations of the explainee. The explanation is selective if it can select explanations from several competing hypotheses. It is considered contrastive if it can differentiate the properties of two competing hypotheses. Kment [33] further emphasised the value of counterfactual explanations in enhancing understanding. These explanations describe how changes in input can lead to a shift from one fact to a competing hypothesis (foil) [46]. Explanations should be intelligible [50] and strike a delicate balance between providing sufficient detail and respecting the cognitive capacity of the explainee. This balance presents a significant challenge, as accurately gauging an individual's real-time cognitive capacity remains difficult.\nDifferent methodologies have been adopted in the XAI literature. Wang et al. [64] categorised these research methodologies into three groups: First, the existence of unvalidated guidelines for the design and evaluations of explanations was highlighted. The authors claimed that these kinds of guidelines are based on authors' experiences with no further substantial justification. Second, researchers suggested (in [66]) that understanding users' requirements is helpful in XAI research. It is on this premise that previous research on explanation design has been thought to be empirically derived. This type of XAI research elicits explanation requirements from user surveys to determine the right explanation for a use case with explanation interfaces. Third, some explanation design methods are derived from psychological constructs from formal theories in the academic literature. Some of these methods (e.g., in [30]) draw on theories from cognitive psychology to inform explanation design for explanation frameworks. Our work is heavily grounded on empirical studies."}, {"title": "2.2. Explanations in Autonomous Driving", "content": "Explanations have been found useful in enhancing user experience [56], trust [34, 26], and improved situational awareness [49, 40] in automated driving. Recent works have explored human factors in the application of explainable AI in AD. For instance, in [50, 48], a socio-technical approach to explainability was proposed. An interpretable representation and algorithms for explanations based on a combination of actions, observations, and road rules were designed. Regarding explanations depths, the notion that explanations with higher levels of abstraction and correctness are superior has been argued in the literature [6, 24]. Additionally, Ramon et al. (2021) argued that the specificity of explanations should be tailored to the application context, noting that low-level specificity is often preferred by individuals with a more deliberative cognitive style.\nIn this paper, the term explanation specificity is used to refer to two specificity levels of explanations, abstract (low transparency) and specific (high transparency). Explanations can be used to convey different information in AD, e.g., vehicle uncertainties and intentions, and communicated through different modalities. For example, Kunze et al. [36] conveyed visual uncertainties with multiple levels to operators using heartbeat animation. This information helped operators calibrate their trust in automation and increased their situation awareness. Similarly, Kunze et al. [37] used peripheral awareness display to communicate uncertainties to alleviate the workload on operators simultaneously observing the instrument cluster and focusing on the road. This uncertainty communication style decreased workload and improved takeover performance. In addition, the effects of augmented reality visualisation methods on trust, situation awareness, and cognitive load have been investigated in previous studies using semantic segmentation [11], scene detection and prediction [12], and pedestrian detection and prediction [10]. These deep vision-based techniques applied to automated driving videos and rendered in augmented reality mode were a way of calling the attention of operators to risky traffic agents in order to enhance safety. While under-explored, auditory means of communicating explanations are important to calling in-vehicle participants' attention to critical situations in AD. We thus used an auditory communication style in this study to convey explanations to passengers. Some existing works around human-machine interaction [40] have leveraged theoretical models (e.g., mental and situational models [21]) to study explanations. We based our work on behavioural cues and subjective feedback from participants while drawing connections to such existing works."}, {"title": "2.3. Research Questions", "content": "From the preceding literature review, we find the need to gain a deeper understanding of the effects of transparency, brought about by natural language explanations of varying specificity, especially under imperfect AV perception systems.\n1. Given varying levels of perception system errors, how do natural language explanations influence passengers' perceived safety?\n\u2022 H1.1 - Perceived Safety. Low transparency yields a higher perception of safety in an AV with perception system errors. We hypothesise that passengers feel safer in a low transparency AV, despite receiving abstract explanations. While individuals often seek the truth, many prefer information that aligns with their expectations [27]. Consequently, specific explanations may reveal perception system errors that contradict passenger expectations. Additionally, research has shown that placebo explanations can have similar positive effects on people as real explanations [18].\n\u2022 H1.2 - Feeling of Anxiety. Passengers' feeling of anxiety increases with increasing perception system errors in a highly transparent AV. We posit that there is a connection between perceived safety and the feeling of anxiety [13, 53]. Therefore, explanations that frequently reference misclassified actors are likely to create a sense of insecurity, leading to increased anxiety.\n\u2022 H1.3 - Takeover Feeling. In highly transparent AVs, passengers are more likely to develop the feeling to take over navigation control from the AV with higher perception system errors. Although passengers are not able to take control in this study, we anticipated that they might nurse the thought to do so if they repeatedly received illogical explanations from the AV.\n2. Do passengers' behavioural cues correlate with their feelings?\n\u2022 H2.1 - Visual Feedback Visual feedback from participants correlates with their feeling of anxiety. Individuals with the feeling of anxiety might be usually hyper-aroused and sensitive to environmental stimuli. They may have difficulties concentrating, performing tasks efficiently, and inhibiting unwanted thoughts and distractions [28, 8]. Participants' fixation points and saccades should correlate with anxiety."}, {"title": "3. Passenger Study", "content": "In this section, we describe the participants' demographic, experiment apparatus setup, experiment design, and the procedure of the experiment. The necessary approval to conduct the study was obtained from our University's Research Ethics Committee."}, {"title": "3.1. Participants", "content": "We conducted a power analysis to estimate the number of subjects required for the study. Afterward, calls for participants were placed on various online platforms, such as the callforparticipants platform, university mailing groups, university Slack channels, the research group website, and social media to recruit subjects. Upon screening, the final sample consisted of N = 39 participants (28 male, 11 female) ranging in age from 18 to 59 years. The participants comprised students, university employees, and members of the callforparticipants platform. Although prior driving experiences were not required, 28 (71.79 %) of the participants were licensed drivers. Only 2 of the 39 participants (5.13%) had experience with autonomous drives, however, in a research context. 6 (15.38%) of the participants had used a virtual reality headset for a driving game or driving experiment in the past."}, {"title": "3.2. Apparatus", "content": ""}, {"title": "3.2.1. Hardware", "content": "The hardware setup is shown in Fig. 1. We conducted the experiment in a driving simulator that comprised a GTR arcade seat, Logitech G29 steering wheel with force feedback, turn signal paddles, brake and accelerator pedals, and an ultra-wide LG curved screen to display the experiment. A state-of-the-art virtual reality (VR) headset (with an immersive 360\u00b0 FoV and an eye tracker) was also used to provide an immersive experience and high visual fidelity."}, {"title": "3.2.2. Driving Software", "content": "Software architecture is illustrated in ??. We adapted the DReyeVR [57], an open-source VR-based driving simulation platform for behavioural and interaction research involving human drivers. DReyeVR was built atop CARLA [17], an open-source driving simulator for AD and Unreal Engine 4. DReyeVR provides a very realistic experience with naturalistic visuals (e.g., in-vehicle mirrors) and auditory (e.g. vehicular and ambient sounds) interfaces allowing for an ecologically valid setup. It also provides an experimental monitoring and logging system to record and replay scenarios, as well as a sign-based navigation system."}, {"title": "3.2.3. Explainer Software", "content": "As shown in Fig. 2, we developed an explainer system based on previous work [48]. This system utilises a tree-based model trained on an annotated AV driving dataset that we collected in a prior project. While the original algorithm in [48] is primarily data-driven, we incorporated a rule-based technique to serve as a fallback when the data-driven method fails or makes an incorrect ego action prediction. The data-driven method employs a trained tree-based model to predict and generate explanations from detections obtained from CARLA, a driving simulator. In contrast, the rule-based approach relies on CARLA's ground truth data and follows predefined rules to determine which agents to reference in the explanations. By comparing predictions from the data-driven method with ground truth observations from CARLA, we can identify incorrect predictions. This enhanced explainer system, combining both data-driven and rule-based approaches, was used to generate preliminary explanations for our created scenarios. Wintersberger et al. (2020) suggested types of traffic elements to be included in visual explanations based on user preferences. Our proposed explainer, however, selects traffic elements deemed important (feature importance [2]) by the driving model for its decisions (see Algorithm 1).\nWe performed post-processing operations on the generated explanations, including fine-tuning some of the content and adjusting timestamps to ensure the explanations were delivered at the appropriate moments."}, {"title": "Algorithm 1: Intelligible Explanation Generation", "content": "Input: tree model M for ego's action prediction, input vector X describing ego's environment\nOutput: intelligible auditory explanation\n1 Select a representative tree $m\\in M$ from tree model M.\n2 Predict action $y \\in Y$ given X.\n3 Compare prediction y with CARLA ground truth $Y_{GT}$.\n4 if $y = Y_{GT}$ then\n5\nTrace the decision path $P_y$ for the prediction y in tree m.\n6\nCompute the importance score $I(X_i)$ of the attributes $X_i$ in each node i along the decision path $P_y$.\n7\nSelect attributes $X_i$ with importance scores $I(X_i) \\geq k$, where k is a predefined threshold.\n8\nMerge the conditions/inequalities in the selected attributes $X_i$.\n9\nTranslate merged attributes $X_i$ to natural language following the template in Tab. 1.\n10 else\n11\nUse CARLA ground truth information $y_{GT}$ and predefined rules to generate explanation following the template in Tab. 1."}, {"title": "4. Experiment Design", "content": "Before the start of the trials, participants were asked to manually drive a vehicle for about two minutes in CARLA Town03-a complex town, with a 5-lane junction, a roundabout, unevenness, and a tunnel. 30 vehicles and 10 pedestrians were spawned in this town. This preliminary drive aimed to familiarise participants with the driving simulation environment and to allow them to experience manual driving within the simulation.\nWe employed a within-subject design due to the limited sample size, which was insufficient for a between-subject study. Additionally, this design helped mitigate the potential co-founding factor of between-individual differences in a between-subject design."}, {"title": "4.1. Independent Variable", "content": "Combinations of transparency level (low and high) and AV perception errors (low and high) were done to obtain the independent variable Scenarios. The first scenario (Abstract scenario) comprises abstract explanations indicating low transparency and an undefined amount of perception system errors. The second scenario (Specific(5) scenario) comprises specific explanations indicating high transparency and 5% amount of perception system errors indicating low error degree. The third scenario (Specific(50) scenario) comprises specific explanations indicating high transparency and 50% amount of perception system errors indicating high error degree. The driving events that made up the different scenarios were carefully designed to include different driving conditions that are obtainable in the real world (See Table 1). Note that scenarios The scenario routes are shown in Figure 3.\ni. Abstract A scenario in CARLA Town10HD, which is about 4 minutes long (330 secs). Town10HD is an urban city environment with different infrastructures, such as an avenue or promenade, and realistic textures. Driving conditions are a combination of the events in Table 1. The perception system in this scenario might contain some errors, but the explanations provided in this scenario were post-processed to always provide surface information which is vague enough to conceal perception errors. The rules governing explanations for this scenario were:\n\u2022 all traffic lights are referred to as \u2018traffic sign' without specifying the state (e.g., red, green, amber, off) of the traffic light;\n\u2022 pedestrians are referred to as 'road users';\n\u2022 all non-human moving actors are referred to as 'vehicle'. This includes cycles, motorbikes, cars, etc.\nAn example explanation is 'stopping because of the traffic sign on my lane'. This obfuscates the type and colour of the traffic sign.\nii. Specific(5) A scenario in CARLA Town10HD, which was about 4 minutes in length (256 seconds). Driving conditions in this scenario were a combination of the events in Table 1. The explanations generated in this scenario were specific and detailed, exposing all errors. The perception system of the AV in this scenario was about 5% inaccurate. This error value was estimated following the dynamic traffic agent classification model and confusion matrix provided by [4] and the traffic light classification model and confusion matrix by [45]. We were only interested in the confusion matrices (and not the models). The confusion matrices helped us to systematically introduce the 5% perception system errors during the post-processing stage of the explanations. In this scenario, the 5% error resulted in one explanation (1 out of the 22) being erroneous as the explanation exposed the misclassification errors from the perception system. An example of an erroneous explanation is: 'van ahead on my lane'. Here, a car was misclassified as a van.\niii. Specific(50) A scenario in CARLA Town10HD, which was 4 minutes in length (274 seconds). Driving conditions were a combination of the events in Table 1. The explanations generated in this scenario were as fine-grained/specific and detailed as those in the Specific(5) scenario. The perception system error of the AV in scenario Specific(5) was significantly noised to reach a reduced accuracy of 50%. We assumed that this reduction in accuracy might be sufficient to influence peoples' behaviour. Therefore, half of the explanations in this scenario (12 out of 24) reflected misclassification of actors or actor states. An example of an erroneous explanation is 'moving because traffic light is switched off on my lane'. In this case, the perception system failed to identify a green light accurately.\nNote that all three scenarios were designed so that the AV perception errors were insignificant to the AV's navigation actions. Hence, the AV respected all road rules and avoided collisions. This was important as the state-of-the-art AVs would likely not make obvious navigation errors. Moreover, we were interested in the effects of the awareness of inconsequential perceptual errors in AVs. Hence, it was necessary to introduce artificial errors of varying degrees (low and high). The non-influence of AV perception errors on navigation control also helped to avoid the confounding factors of route navigation problems. Further, we counterbalanced the routes across scenarios. That is, the AV's route was different in each scenario. This design decision was made to reduce carry-over effects on the participants. With this setup, the scenarios were still comparable as they were all within the same town, and the routes shared similar features. Each scenario also had a balanced combination of the events listed in Table 1. In all the scenarios, the AV maintained a speed below 30mph, the recommended speed limit in urban areas in the UK. See Figure 4 for sample scenes from each scenario and their corresponding explanations."}, {"title": "4.2. Dependent Variables", "content": "There were six dependent variables: Perceived Safety, Feeling of Anxiety, Takeover Feeling, Fixation Divergence, Saccade Difference, and Button Presses. These variables were categorised into two (psychological factors and behavioural cues) for easy analysis and reporting.\nPsychological Factors These factors include Perceived Safety, Feeling of Anxiety, and Takeover Feeling. They were mainly measured using items from the Autonomous Vehicle Acceptance Model Questionnaire (AVAM) [29]. AVAM is a user acceptance model for autonomous vehicles, adapted from existing user acceptance models for generic technologies. It comprises a 26-item questionnaire on a 7-point Likert scale, developed after a survey conducted to evaluate six different autonomy scenarios.\nItems 24-26 were used to assess the Perceived Safety factor, while items 19\u201421 were used to assess the Feeling of Anxiety factor. Similar to [56], we introduced a new item to assess participants' feelings to takeover navigation control from the AV during the ride (Takeover Feeling). Specifically, participants were asked to rate the statement 'During the ride, I had the feeling to take over control from the vehicle' on a 7-point Likert scale. Actual navigation takeover by participants was not permitted because we wanted to be able to control the entire experiment and have all participants experience the same scenarios. Moreover, we were dealing with L4 automation. Though participants were not expected to drive or take over control, they might have nursed the thought to do so. This is what the Takeover Feeling variable measures.\nWe added a free-response question related to explanations with the aim of obtaining qualitative data for triangulating quantitative results. Participants were asked the following question: 'What is your thought on the explanations provided by the vehicle, e.g., made you less/more anxious, safe, feeling to take over control?'. We refer to the resulting questionnaire as the APT Questionnaire (i.e., A-Anxiety, P-Perceived Safety, T-Takeover Feeling)."}, {"title": "5. Quantitative Results", "content": "To test our hypotheses listed in Section 2.3, we analyzed data from the three APT questionnaires. We created a latent variable (Feeling of Anxiety) by averaging responses to AVAM Items 19\u201421, and another latent variable (Perceived Safety) by averaging responses to AVAM Items 24\u201326. We calculated Cronbach's Alpha ($\\alpha$) for the independent variables that formed the latent dependent variables to ensure they had adequate internal consistency. Results with an adjusted p-value less than 0.05 ($p < .05$) were considered significant. P-values were adjusted using Bonferroni corrections, where the calculated p-values were multiplied by the number of scenarios, to reduce the likelihood of Type I errors (false positives). Normality tests, including the Kolmogorov-Smirnov, Shapiro-Wilk, and Anderson-Darling tests, indicated a violation of normality in the Feeling of Anxiety, Perceived Safety, and Takeover Feeling factors. Hence, we performed a Friedman test for these dependent variables, see Table 3 and Figure 6."}, {"title": "H1.1 - Perceived Safety", "content": "Low transparency yields a higher perception of safety in an AV with perception system errors.\nA Friedman test was conducted. No significant difference was found in the scenario pair: Abstract - Specific(5), and the pair: Abstract - Specific(50). In fact, the perceived safety mean rank in the Specific(5) scenario (2.22) was higher than that in the Abstract scenario (2.15), see Table 3. Therefore, there was no sufficient evidence in support of hypothesis H1.1."}, {"title": "H1.2 - Feeling of Anxiety", "content": "Passengers' feeling of anxiety increases with increasing perception system errors in a highly transparent AV. A Friedman test indicated a significant difference in the Feeling of Anxiety across scenarios, H(2) = 13.32, p = .001. The pairwise scenario comparisons of Abstract - Specific(50) and Specific(5) - Specific(50) resulted in an adjusted p-value of .003 and .01 respectively (see Table 3). Hence, there is strong evidence in support of hypothesis H1.2."}, {"title": "H1.3 - Takeover Feeling", "content": "In highly transparent AVs, passengers are more likely to develop the feeling to take over navigation control from the AV with higher perception system errors. A Friedman test showed a significant difference in Takeover Feeling across scenarios, H(2) = 6.27,p = .044. While the pairwise scenario comparison of Abstract - Specific(50) resulted in an adjusted p-value of .017, the pairwise comparison of Specific(5) - Specific(50) resulted in an adjusted p-value of 0.61. Hence, there is no significant difference in Takeover Feeling between Specific(5) and Specific(50) scenarios, and therefore, no evidence in support of hypothesis H1.3 (see Table 3)."}, {"title": "5.2. Behavioural Cues Analysis", "content": "H2.1 - Visual Responses Visual feedback from passengers correlates with passengers' anxiety. At this stage, we utilised the reference data from the researcher. We estimated the Euclidean distances between participants' fixation points and the reference fixation points over time for each participant.\nResults from Spearman correlation showed that there was no significant association between the Feeling of Anxiety and Fixation Divergence, r(115) = \u22120.07,p = .442. See the fixation divergence plot in Figure 7. Results from Spearman correlation showed that there was no significant association between the Feeling of Anxiety and saccade difference, r(115) = 0.1, p = .281. However, there was a significant association between perceived safety and saccade difference, r(115) = \u22120.25, p = .007., indicating a weak negative correlation between perceived safety and saccade difference. Hypothesis H2.1, therefore, has no sufficient support. See the saccade difference plot in Figure 8.\nIn addition to correlation, we checked for significant differences. There was a significant difference in Fixation Divergence between Abstract and Specific(5) with an adjusted p-value of .028, and between Specific(5) and Specific(50) with an adjusted p-value < .001. See Table 4 for descriptive statistics. Also, there was a significant difference between Abstract and Specific(5) with respect to Saccade Difference (adjusted p-value of < .001). See Figure 4 for sample scenes from each scenario with the generated explanations. All the participants' gaze points are plotted as heatmaps over the screenshots."}, {"title": "5.2.1. Haptic Response", "content": "Participants were asked to press a button on the Logitech wheel when they felt confused, anxious, or unsafe by the explanations or the decision of the AV during the ride. Spearman rank correlation was used as a measure to investigate monotonic associations. There was a weak negative correlation between the variables Perceived Safety and ButtonPress (r(115) = -0.31, p = .001), a weak positive correlation between the Feeling of Anxiety and ButtonPress (r(115) = 0.31, p = .001), and insignificant correlation between the Feelings to Takeover and ButtonPress (r(115) = 0.15, p = .099).\nWe also checked for statistically significant differences in Button Presses across scenarios. There was a significant difference in ButtonPresses, H(2) = 15.44, p < .001. This was specifically in the pairs: Abstract - Specific(50) with adjusted p-value .002, and Specific(5) - Specific(50) with adjusted p-value .005. See Figure 9 for behavioural cues results."}, {"title": "6. Qualitative Results: Themes and Reflections", "content": "We obtained qualitative data from the APT questionnaire administered after every scenario. Participants were asked to describe their feelings regarding the explanations they received during the ride. Table 5 and Figure 10 describe the themes obtained from the inductive thematic analysis of the comments. Themes are broadly categorised based on the participants' feelings, their assessment of the explanations, and the vehicle dynamics.\nPerceptual errors in the Specific(50) scenario evoked negative emotions of anxiety, feeling to takeover navigation control and distrust. CAND1 expressed a feeling of anxiety: 'The explanations made me feel a bit anxious, it says many things that were not right and misleading. I had the urge to look at the buildings and the environment but could not really do that because I wanted to be sure the vehicle is taking the right decision.'. CAND39 expressed the urge to takeover navigation control: \u2018When the explanations are false, e.g. 'a cyclist is crossing my lane', and it is actually a pedestrian, it made me slightly anxious and likely to want to take over. But nevertheless, I felt safe in the vehicle'. CAND5 expressed distrust in the AV: \u2018anxious as the vehicle did not correctly understand the environment and the types of vehicles around it, which made me trust its judgement less'. More participants expressed a feeling of safety in the Specific(5) scenario: 'felt safe that the vehicle understood the road and what was going on around us'. About the same number of participants expressed a decline in their feeling of anxiety in the Abstract and Specific(5) scenarios. An example is CAND34's comment about the abstract scenario: \u2018When the explanations provided are more general, e.g. 'vehicle' instead of 'van' and 'road user' instead of 'cyclist', it feels like the vehicle has a better understanding of the surroundings because it gives a correct explanation, so I felt less anxious and unsafe'. The abstract explanations might have concealed some errors, in turn, reducing the feeling of anxiety.\nThere were specific comments about the explanations across the three scenarios. Many participants thought that the explanations in the Specific(5) were plausible in that they sounded correct and aligned with what the participants saw. For example: \u2018Explanations were clear and made sense. Still don't feel some of the reactions were as quick as I might have made them\u2019\u2014CAND14. There were a good number of comments around the implausible nature of the explanations in the Specific(50) scenario. For example, CAND20 said, \u2018The vehicle this time had difficulty giving the correct reason for stopping/going. Couldn't tell the difference between a pedestrian and a cyclist sometime or thought that traffic lights were off instead of green. I feel that this time I would have wanted more control over the car, particularly at traffic lights as I could determine better if a traffic light was 'working' or not'.\nA couple of candidates thought that the explanations in the Abstract scenarios were either too early or late. For example, 'The explanations should have arrived a bit earlier, like a few meters before the vehicle actually stops so that I will know that it is planning to stop. Also, I would be more comfortable if the explanation 'traffic sign' was 'traffic light is red/green'. when referring to a traffic light.'\u2014CAND19.\nSome interesting comments were made about the vehicle's driving style and its interior. For example, CAND31 made a comment about the careful manoeuvre of the vehicle in the Specific(50) scenario: \u2018I was calm throughout the journey. There was no feeling of anxiety as the vehicle did not speed too much to make me feel that way.\u2019\u2014CAND31. There was a comment relating to aggressive manoeuvre in the Abstract scenario: \u2018Seemed like oncoming vehicles were going to collide with me. It seems to sometime drive on pavements when negotiating corners.'\u2014CAND35. The rotating steering wheel of the vehicle made some of the participants uncomfortable: \u2018The steering wheel moving abruptly startled me sometimes.\u2019\u2014CAND21 (Specific(5) scenario). Some participants liked the vehicle indicators and the sound they made when indicating the next directions. \u2018The indicator sound was nice to hear. [...]'\u2014CAND6 (Specific(50) scenario)."}, {"title": "7. General Discussion", "content": "We examined the effects of explanation specificity (abstract and specific) in AD, while accounting for varying degrees of perception system errors (low and high). We focused on how this setup would impact passengers' perceived safety and related factors, such as the feeling of anxiety and the thought to takeover control. Our results not only corroborate but also extend previous findings in the field, among others, demonstrating that while intelligible explanations generally create positive experiences for AV users [50, 26, 56, 42], this effect is predominantly observed when the AV's perception system errors are low."}, {"title": "7.1. Psychological Effects", "content": "Hypothesis 1.1 - Low transparency yields higher perception of safety Contrary to expectations, participants expressed a greater sense of safety in the Specific(5) scenario, indicating a preference for specific explanations in an AV with notably reduced perception system errors. This finding challenges our initial hypothesis and suggests a more nuanced relationship between transparency and perceived safety. It appears that users may prefer more detailed information when the system demonstrates high reliability. However, overly detailed explanations were perceived as verbose and repetitive by some participants in the specific scenarios"}]}