{"title": "Training-Free Bayesianization for Low-Rank Adapters of Large Language Models", "authors": ["Haizhou Shi", "Yibin Wang", "Ligong Han", "Huan Zhang", "Hao Wang"], "abstract": "Estimating the uncertainty of responses of Large Language Models (LLMs) remains a critical challenge. While recent Bayesian methods have demonstrated effectiveness in quantifying uncertainty through low-rank weight updates, they typically require complex fine-tuning or post-training procedures. In this paper, we propose Training-Free Bayesianization (TFB), a novel framework that transforms existing off-the-shelf trained LoRA adapters into Bayesian ones without additional training. TFB systematically searches for the maximally acceptable level of variance in the weight posterior, constrained within a family of low-rank isotropic Gaussian distributions. We theoretically demonstrate that under mild conditions, this search process is equivalent to variational inference for the weights. Through comprehensive experiments, we show that TFB achieves superior uncertainty estimation and generalization compared to existing methods while eliminating the need for complex training procedures.", "sections": [{"title": "1 Introduction", "content": "Despite recent advances in Large Language Models (LLMs) showing great capacity for generating responsive answers to human instructions (Biderman et al., 2023; Wei et al., 2022; 2021; Min et al., 2022; Chowdhery et al., 2023; Anil et al., 2023; Touvron et al., 2023a;b; Radford et al., 2019; Brown et al., 2020; Achiam et al., 2023; OpenAI, 2022), the reliability of such large models remains a critical concern for researchers (Wang et al., 2024b;a), as untruthful yet confident answers could cause significant damage to individuals and society (Gupta et al., 2024; Nikitin et al., 2024; Yadkori et al., 2024; Kapoor et al., 2024). The accurate estimation of uncertainty in LLMs has thus emerged as a crucial challenge. Current research approaches mainly follow two paths: one focuses on eliciting models' internal (verbalized) uncertainty through direct questioning (Xiong et al., 2023; Tian et al., 2023; Kapoor et al., 2024), while the other employs fine-tuning (Kapoor et al., 2024; Wang et al., 2024d) or post-training procedures (Yang et al., 2023).\nBoth approaches face inherent limitations. Verbalized uncertainty, while simple to implement, remains controversial in terms of its empirical reliability and theoretical soundness (Kadavath et al., 2022; Kuhn et al., 2023). On the other hand, low-rank adapters (LoRA (Hu et al., 2022)), which offer a parameter-efficient way to adapt LLMs by adding a small set of low-rank weight matrices, have emerged as a promising direction for fine-tuning models. However, while LoRA efficiently adapts large models to new tasks, it does not itself provide a mechanism for principled uncertainty estimation. Recent Bayesianization attempts, such as BLOB (Wang et al., 2024d), integrate Bayesian methods with LoRA but still require complex training procedures and delicate hyperparameter tuning, limiting their practicality. These constraints motivate the following research question:\nCan we \u201cBayesianize\u201d a low-rank adapter in a theoretically sound and empirically simple way?\nIn this paper, leveraging the existing abundance of trained LLM adapters across various domains, we diverge from conventional fine-tuning and post-training approaches. Instead, we develop a Training-Free Bayesianization (TFB) technique applicable to any given low-rank LLM adapter. TFB constrains the family of full-weight posteriors produced by LORA adapters to low-rank isotropic Gaussian distributions. Given a trained LoRA adapter, it systematically searches for the maximally acceptable variance of the variational distribution of the weight posterior. TFB's search range and stopping criteria can be determined using any additional in-distribution \"anchor dataset,\" independent of its supervision status or prior use in LoRA training.\nDespite its straightforward approach, we theoretically demonstrate that under mild conditions, TFB's process of finding the maximal variance of the isotropic Gaussian"}, {"title": "2 Related Work", "content": "LLM Uncertainty Estimation. To estimate the uncertainty of large language models (LLMs), the models themselves are often employed to generate and evaluate their own uncertainty (Lin et al., 2022; Kadavath et al., 2022). However, such approaches typically rely on task-specific labels and require additional training. Semantic entropy (Kuhn et al., 2023) leverages the invariance of language stemming from shared meanings to estimate uncertainty, while mutual information is used to compute a lower bound on model uncertainty by sampling from the model's output distribution (Yadkori et al., 2024). Despite their contributions, these methods fail to accurately capture true model uncertainty, as they do not model the probability distribution over the parameter space (H\u00fcllermeier & Waegeman, 2021; Abdar et al., 2021; Gawlikowski et al., 2023).\nBayesian Low-Rank Adaptation. The Bayesian framework provides a powerful approach for capturing and es-timating uncertainty by defining prior distributions and approximating posterior distributions over the parameter space (Neal, 2012; Hern\u00e1ndez-Lobato & Adams, 2015; Gal & Ghahramani, 2016; Wang & Yeung, 2016; Gustafsson et al., 2020). However, modeling parameter distributions across the entire parameter space during fine-tuning introduces significant computational overhead (Fan et al., 2020; Zhang et al., 2021). To address this challenge, recent research has explored combining Bayesian methods with Parameter-Efficient Fine-Tuning (PEFT) techniques to improve the efficiency of uncertainty estimation. Several notable approaches have emerged in this direction. Wang et al. (2023) and Balabanov & Linander (2024) demonstrate improved performance by training multiple LoRA modules and ensemble their predictions during inference. Taking a different approach, Yang et al. (2023) applies a Kronecker-factorized Laplace approximation to fine-tuned LoRA parameters. More recently, BLOB (Wang et al., 2024d) advances the field by simultaneously estimating both the mean and covariance of LLM parameters within a single fine-tuning stage, leading to substantial performance improvements. Our proposed training-free Bayesianization represents a significant departure from these existing methods. Unlike approaches that require re-training (Gal & Ghahramani, 2016; Wang et al., 2023; Balabanov & Linander, 2024; Wang et al., 2024d) or rely on continued training and gradient estimation (Yang et al., 2023), our method achieves uncertainty estimation without any additional training steps, substantially improving the simplicity and efficiency for Bayesian learning of LLMs."}, {"title": "3 Methodology", "content": "In this section, we present Training-Free Bayesianization (TFB), detailing our approach across four subsections. Sec. 3.1 establishes the foundational problem setting and preliminaries. Sec. 3.2 introduces our low-rank isotropic Gaussian posterior formulation, while Sec. 3.3 presents our core contribution: a novel approach for converting deterministic weights to probabilistic distributions without training. Sec. 3.4 provides the complete algorithmic implementation. The theoretical foundations supporting our method are separately addressed in Sec. 4.\n3.1 Preliminaries\nLow-Rank Adapation (LoRA). Given a pre-trained neural network layer with weight matrix Wo, Low-Rank Adaptation (LORA) (Hu et al., 2022) confines weight updates to a low-rank subspace during fine-tuning, expressing the update as \\(\\Delta W = BA\\), where \\(\\Delta W \\in \\mathbb{R}^{m\\times n}\\), \\(B \\in \\mathbb{R}^{m\\times r}\\), and \\(A \\in \\mathbb{R}^{r\\times n}\\). For input h and output z of the LoRA layer, the forward pass computation is given by:\n\\(z = W_0 h + \\Delta W h = W_0 h + BAh.\\) (1)"}, {"title": "3.2 Low-Rank Isotropic Gaussian Posterior", "content": "Variational Distribution Family. In TFB, we constrain the variational distributions of the weight posterior to a more compact family of Gaussians than BLoB: specifically, we employ full-space isotropic Gaussian distributions projected onto the low-rank space:\n\\(q(\\text{vec}(W)|B, \\Omega) = \\mathcal{N}(\\text{vec}(W)|\\mu_q, \\text{proj}(\\sigma_q I)),\\) (3)\nwhere \\(\\mu_q\\) is defined as in Eqn. 2. Here, \\(I \\in \\mathbb{R}^{mn\\times mn}\\) represents a full-rank isotropic covariance matrix with standard deviation \\(\\sigma_q\\), and \\(\\text{proj}\\) denotes a linear projection operator that maps the full-space covariance matrix onto the low-rank space jointly defined by B and A.\nThe choice of low-rank isotropic Gaussian posteriors serves both theoretical and empirical purposes: it provides a single-parameter family that enables converting the VI objective into a maximization problem (Sec. 3.3 and Theorem 4.2), and empirically outperforms alternative distribution families (Sec. 5.3). We now present a practically efficient implementation for Bayesianizing LoRA under the constraint specified in Eqn. 3, with detailed theoretical analysis provided in Theorem 4.1.\nTFB in Practice. Consider a LoRA layer with weight updates \\(B \\in \\mathbb{R}^{m\\times r}\\), \\(A \\in \\mathbb{R}^{r\\times n}\\) and a standard deviation scale \\(\\sigma_q > 0\\). We begin by computing the compact Singular Value Decomposition (SVD) (Klema & Laub, 1980) of B:\n\\(B = U D V^T,\\) (4)"}, {"title": "3.3 Training-Free Bayesianization (TFB)", "content": "The previous section presents a straightforward Bayesianization scheme for cases where the variational distribution of the full-weight matrix \\(W_0 + \\Delta W\\) is constrained to the isotropic Gaussian family, assuming a predetermined value of \\(\\sigma_q\\). In this section, we describe a practical method for determining \\(\\sigma_q\\).\nA General Bayesianization Framework. Consider an in-distribution \"anchor\" dataset \\(\\mathcal{D}\\), an associated evaluation metric \\(l\\), and a permissible performance degradation threshold \\(\\epsilon\\). Training-Free Bayesianization (TFB) formulates this as a constrained optimization problem:\n\\(\\begin{aligned} \\max_{\\sigma_q} & \\quad \\sigma_q \\\\ \\text{s.t.} & \\quad |l(\\mathcal{D}|B', M, \\Omega(\\sigma_q)) - l(\\mathcal{D}|B, A)| \\leq \\epsilon, \\end{aligned}\\) (7)\nwhere \\(l(\\mathcal{D}|B', M, \\Omega(\\sigma_q)) = \\mathbb{E}_{E \\sim \\mathcal{N}(0,\\Omega)}[l(\\mathcal{D}|B', M + E)]\\) represents the post-Bayesianization performance. This optimization seeks to maximize the noise scale \\(\\sigma_q\\) applied to model parameters M while ensuring that the resulting performance degradation (or loss increase) remains within the acceptable threshold \\(\\epsilon\\).\nAnchor Dataset \\(\\mathcal{D}\\) and Evaluation Metric \\(l\\). The framework of TFB accommodates various choices of anchor dataset \\(\\mathcal{D}\\) and evaluation metric \\(l\\) based on practical re-quirements. We consider two key scenarios (with \\(N\\) being slightly overloaded in its notation):\nFor supervised dataset \\(\\mathcal{D} = \\{\\mathbf{x}_n, y_n\\}_{n=1}^N\\): The Negative-Log Likelihood (NLL) serves as a natural evaluation metric: \\(l_{nll}(\\mathcal{D}) = - \\sum_{n=1}^N \\log P_{\\theta}(y_n|\\mathbf{x}_n)\\), as it directly corresponds to minimizing the variational free energy, as we demonstrate in Sec. 4. The anchor dataset \\(\\mathcal{D}\\) can be either the original training set used for the LORA model or an independent calibration dataset, as commonly employed in calibration-based methods (Guo et al., 2017; Zhao et al., 2021). Alternative evaluation metrics such as accuracy or F1 score are also readily applicable. In our experimental setup, to ensure fair comparisons across uncertainty estimation baselines, we use the original training data as \\(\\mathcal{D}\\) (maintaining the same information access as baselines) and employ accuracy as the evaluation metric.\nFor unsupervised dataset \\(\\mathcal{D} = \\{\\mathbf{x}_n\\}_{n=1}^N\\): Our framework offers substantial flexibility compared to pure calibration methods. One approach is to generate pseudo-labels \\(y\\) using the model before Bayesianization, effectively converting the problem to the supervised case with \\(\\mathcal{D} = \\{\\mathbf{x}_n, y_n\\}_{n=1}^N\\). Additionally, the framework supports alternative evaluation metrics and statistical measures specifically designed for unsupervised data.\nPerformance Change Tolerance \\(\\epsilon\\). The selection of per-formance change tolerance \\(\\epsilon\\) is critical in TFB. While our experiments demonstrate that a fixed \\(\\epsilon = 1\\%\\) for the accuracy metric achieves effective uncertainty estimation across various datasets and LoRA checkpoints (Table 8), an adaptive \\(\\epsilon\\) can further enhance the performance of TFB (Table 1).\n3.4 TFB: Final Algorithm"}, {"title": "4 Theoretical Analysis", "content": "We present our theoretical analysis in this section (with com-plete proofs provided in Appendix A). First, we demonstrate that our Bayesianization scheme, defined in Equations 4, 5, and 6, projects a full-rank isotropic Gaussian distribution onto the low-rank space. We then prove that optimizing Eqn. 7 is equivalent to performing variational inference for LLM's weights under specific, achievable conditions.\nAssumption 4.1. The evaluation metric \\(l_\\mathcal{D} : \\mathbb{R}^+ \\rightarrow \\mathbb{R}^+\\) is the Negative Log Likelihood (NLL) evaluated on the data distribution \\(\\mathcal{D}\\) for the given posterior standard deviation"}, {"title": "5 Experiments", "content": "In this section, we evaluat TFB through comprehensive experiments on various settings. Sec. 5.1 details our experimental setup, including baselines and evaluation protocols. We then present our main results on in- and out-of-distribution performance in Sec. 5.2, followed by an ablation study analyzing key components of our method in Sec. 5.3. To demonstrate the broad applicability of our approach, we conduct additional experiments on different LLM architectures (Sec. 5.4) and explore its compatibility with other LORA-like PEFT methods (Sec. 5.5).\n5.1 Settings\nModels, Datasets, and Evaluation. Following prior work on Bayesian low-rank adaptation (Yang et al., 2023; Wang et al., 2023; 2024d), we use llama-2-7b-hf as our primary LLM backbone. We provide additional results on recent LLM architectures in Sec. 5.4, including Meta-Llama-3-8B, Meta-Llama-3.1-8B, and Mistral-7B-v0.3 from the Llama (Dubey et al., 2024) and Mistral (Jiang et al., 2023) families."}, {"title": "5.2 TFB Improves Accuracy and Uncertainty Estimation Across Distributional Shifts", "content": "Table 1 presents comprehensive performance metrics (ACC, ECE, and NLL) for various methods applied to LoRA on Llama2-7B pre-trained weights. Due to space limitations, we report the full empirical results of TFB variants (with different \\(\\epsilon\\)s) in Appendix C.1.\nIn-Distribution Results. Looking at the IND Datasets results, several key patterns emerge. The MLE baseline shows relatively strong accuracy but suffers from high ECE values (e.g., 29.83 on WG-S), indicating significant over-confidence. This aligns with the common challenge of LLM overconfidence during conventional fine-tuning.\nTFB applied to BLOB-Mean demonstrates strong overall performance across the IND datasets, achieving the highest accuracy on several datasets (69.94 on WG-S, 70.72 on ARC-C, and 86.74 on ARC-E). More importantly, it achieves this while maintaining lower ECE values compared to methods like MCD and ENS, suggesting better calibrated predictions. The method also shows strong NLL performance, with values consistently among the lowest across datasets (0.62 for WG-S, 0.86 for ARC-C).\nOut-of-Distribution Results. The OOD evaluation reveals interesting patterns across both smaller and larger distribution shifts. For smaller shifts (ARC-C and ARC-E), BLOB-Mean with TFB maintains strong performance, achieving 70.38 and 80.16 accuracy respectively, while keeping ECE values low (12.28 and 8.07). This suggests robust generalization under moderate distribution shifts.\n5.3 TFB Beyond Low-Rank Isotropic Gaussians"}, {"title": "5.4 TFB Beyond the Llama2-7B Backbone", "content": "We conduct comprehensive experiments across multiple LLM backbones to validate our approach. Our experiments span several models from the Llama architecture (Touvron et al., 2023b; Dubey et al., 2024), including llama-2-7b-hf, Meta-Llama-3-8B, and Meta-Llama-3.1-8B.  We also extend our analysis to include Mistral-7B-v0.3 (Jiang et al., 2023).\n5.5 TFB Beyond the Naive LoRA"}, {"title": "6 Conclusion", "content": "In this paper, we introduce Training-Free Bayesianization (TFB), a novel framework that transforms trained LoRA adapters into Bayesian ones without additional training. By systematically searching for the maximally acceptable variance in the weight posterior within a family of low-rank isotropic Gaussian distributions, TFB provides a practical solution to uncertainty estimation in LLMs. We theoretically demonstrate that TFB's variance maximization process is equivalent to variational inference under mild conditions, while empirically showing its superior performance across various settings and model configurations. Our framework's simplicity and effectiveness, requiring only an anchor dataset for search, makes it widely applicable across different domains. As LLMs continue to evolve, TFB represents a significant step toward more reliable and uncertainty-aware AI systems, paving the way for future research in adaptive and trustworthy machine learning."}, {"title": "7 Limitations", "content": "TFB is subject to several limitations. First, our approach relies on the availability of an anchor dataset for determining search range and stopping criteria. Although this dataset doesn't require supervision or prior use in LoRA training, its quality and representativeness could impact the effectiveness of uncertainty estimation. Second, by constraining"}, {"title": "A Proof of Theorems", "content": "Theorem 4.1. With the pre-trained weight matrix \\(W_0 \\in \\mathbb{R}^{m\\times n}\\), the low-rank weight update matrix \\(\\{B' \\in \\mathbb{R}^{m\\times r}, A' \\in \\mathbb{R}^{r\\times n}\\}\\) transformed from the given matrices \\(\\{B, A\\}\\) following Eqn. 4 and 5, suppose that the variational distribution of A' is Gaussian \\(q(A'|\\Omega) = \\prod_{ij} \\mathcal{N}(A'_{ij}|M_{ij}, \\Omega_{ij})\\), where \\(M = [M_{ij} = A'_{ij}] \\in \\mathbb{R}^{r\\times n}\\) is its mean and \\(\\Omega = [\\Omega_{ij}] \\in \\mathbb{R}^{r\\times n}\\) is the standard deviation calculated as in Eqn. 6. The equivalent variational distribution \\(q(\\text{vec}(W)|\\sigma_q)\\) defined on the full weight matrix W is\n\\(\\begin{aligned} q(\\text{vec}(W)|\\sigma_q) = \\mathcal{N}(\\text{vec}(W)|\\mu_q, \\Sigma_q), \\\\ \\text{where} \\quad \\begin{aligned} \\mu_q &= \\text{vec}(W_0 + B'M), \\\\ \\Sigma_q &= \\sigma_q \\cdot I_n \\otimes \\begin{bmatrix} I_r & \\\\\\ \\Omega_{m-r} \\end{bmatrix}. \\end{aligned} \\end{aligned}\\) (13)\nTheorem 4.2. Suppose the evaluation metric \\(l_{\\mathcal{D}}(\\sigma_q)\\) defined following Assumption 4.1 is locally convex within the range of \\(0 < \\sigma_q < \\epsilon_0\\). Suppose the posterior distribution of W given \\(\\sigma_q\\) is defined following Theorem 4.2. Suppose we have the prior distribution \\(P(\\text{vec}(W)) = \\mathcal{N}(\\text{vec}(W)|\\mu_p, \\Sigma_p)\\), where \\(\\mu_p = \\mu_q = \\text{vec}(W_0 + B'M)\\), and \\(\\Sigma_p = \\sigma_p^2 I\\) with \\(\\sigma_p >> \\epsilon_0\\). Then \\(\\exists\\epsilon > 0\\), s.t. the two following optimization problems"}, {"title": "B Implementation Details", "content": "B.1 Datasets\nWe provide details of the datasets used in this work, as shown in Table 6. The combined dataset consisting of the six commonsense reasoning tasks contains the label set of \"[A, B, C, D, E, True, False]\"."}, {"title": "C Additional Experimental Results", "content": "C.1 TFB Variants: \\(\\epsilon\\)s and Optimal Calibration\nC.2 TFB Beyond Low-Rank Isotropic Gaussians\nIn Table 2 (Sec. 5.3), we compare TFB with two alternative Gaussian distribution families that are controlled by a single parameter \\(\\sigma_q\\):\n\u2022 Full-Rank Isotropic Gaussian (FR): given \\(\\sigma_q\\), the FR's variational distribution of the weight matrix \\(q(\\text{vec}(W)) = \\mathcal{N}(\\text{vec}(W)|\\mu_q, \\Sigma_q)\\) where \\(\\mu_q = W_0 + BA\\) (same as TFB) and \\(\\Sigma_q = \\sigma_q I_{mn}\\) is full-rank.\n\u2022 Constant Standard Deviation Matrix (C-STD): given \\(\\sigma_q\\), the C-STD's variational distribution of the weight matrix \\(q(\\text{vec}(W)) = \\mathcal{N}(\\text{vec}(W)|\\mu_q, \\Sigma_q)\\) where \\(\\mu_q = W_0 + BA\\) (same as TFB) and \\(\\Sigma_q = \\sigma_q^2 \\cdot I_n \\otimes [BB^T]\\).\nC-STD's covariance matrix \\(\\Sigma_q\\) is derived through Lemma A.1:\n\\(\\begin{aligned} \\Sigma_q &= [I_n \\otimes B] \\cdot [\\text{diag}(\\text{vec}(\\Omega)^2)] \\cdot [I_n \\otimes B^T] \\\\ &= [I_n \\otimes B] \\cdot [\\sigma_q^2 \\cdot I_{rn}] \\cdot [I_n \\otimes B^T] \\\\ &= \\sigma_q^2 \\cdot [I_n \\otimes B] \\cdot [I_n \\otimes B^T] \\\\ &= \\sigma_q^2 \\cdot I_n \\otimes [BB^T]. \\end{aligned}\\) (33) (34) (35) (36)\nC.3 TFB Beyond the Llama2-7B Backbone\nC.4 TFB Beyond the Naive LoRA"}]}