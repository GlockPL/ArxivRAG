{"title": "Enhancing Large Language Models with Domain-specific Retrieval Augment Generation: A Case Study on Long-form Consumer Health Question Answering in Ophthalmology", "authors": ["Aidan Gilson", "Xuguang Ai", "Thilaka Arunachalam", "Ziyou Chen", "Ki Xiong Cheong", "Amisha Dave", "Cameron Duic", "Mercy Kibe", "Annette Kaminaka", "Minali Prasad", "Fares Siddig", "Maxwell Singer", "Wendy Wong", "Qiao Jin", "Tiarnan D.L. Keenan", "Xia Hu", "Emily Y. Chew", "Zhiyong Lu", "Hua Xu", "Ron A. Adelman", "Yih-Chung Tham", "Qingyu Chen"], "abstract": "Despite the potential of Large Language Models (LLMs) in medicine, they may generate responses lacking supporting evidence or based on hallucinated evidence. While Retrieval Augment Generation (RAG) is popular to address this issue, few studies implemented and evaluated RAG in downstream domain-specific applications.\nWe developed a RAG pipeline with ~70,000 ophthalmology-specific documents that retrieve relevant documents to augment LLMs during inference time. In a case study on long-form consumer health questions, we systematically evaluated the responses \u2013 including over 500 references - of LLMs with and without RAG on 100 questions with 10 healthcare professionals. The evaluation focuses on factuality of evidence, selection and ranking of evidence, attribution of evidence, and answer accuracy and completeness.\nLLMs without RAG provided 252 references in total. Of which, 45.3% hallucinated, 34.1% consisted of minor errors, and 20.6% were correct. In contrast, LLMs with RAG significantly improved accuracy (54.5% being correct) and reduced error rates (18.8% with minor hallucinations and 26.7% with errors). 62.5% of the top 10 documents retrieved by RAG were selected as the top references in the LLM response, with an average ranking of 4.9. The use of RAG also improved evidence attribution (increasing from 1.85 to 2.49 on a 5-point scale, P<0.001), albeit with slight decreases in accuracy (from 3.52 to 3.23, P=0.03) and completeness (from 3.47 to 3.27, P=0.17).\nThe results demonstrate that LLMs frequently exhibited hallucinated and erroneous evidence in the responses, raising concerns for downstream applications in the medical domain. RAG substantially reduced the proportion of such evidence but encountered challenges. In contrast to existing studies, the results highlight that (1) LLMs may not select top-ranked documents by RAG, which results in hallucinated evidence remaining, (2) LLMs may miss top-ranked", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) represent one of the latest advancements in Al systems designed for language modeling1,2. Compared with early language models, LLMs demonstrate notable capabilities in natural language generation and reasoning tasks such as reading comprehension\u00b3, translation, and question-answering5. Studies also demonstrate that LLMs exhibit in-context learning abilities, enabling them to effectively interpret and generate text even when provided with minimal prompts (zero-shot learning) or a limited number of example demonstrations (few-shot learning). In the medical domain, LLMs also demonstrate potential across a range of applications7-12. We conducted a systematic evaluation of the effectiveness of LLMs across 12 biomedical natural language processing benchmarks demonstrating that LLMs already surpassed previous state-of-the-art methods in generative applications under zero/few-shot scenarios. Additionally, other studies have shown promising performance of LLMs in disease diagnosis13, impression generation14, and medication education15.\nDespite this promise, studies also highlight the issue of hallucination in medical applications of LLMs, where these models may produce responses that are linguistically fluent and semantically coherent but may deviate from factual accuracy or contain fabricated information16-18. For instance, Hou et al. manually examined over 10,000 LLM-generated responses to 600"}, {"title": "Data and Methods", "content": "Figure 1 demonstrates the overview of the study. The details are below.\nRetrieval Augment Generation Pipeline\nCreation of Domain-Specific Corpus. Initially, we curated a corpus of approximately 70,000 ophthalmology-specific documents from three primary resources, as outlined in Table 1."}, {"title": "Ophthalmology Journal Articles.", "content": "We sourced articles from ten primary ophthalmology journals, including Ophthalmology, JAMA Ophthalmology, American Journal of Ophthalmology, British Journal of Ophthalmology, Retina, Ophthalmology Glaucoma, Journal of Cataract and Refractive Surgery, Asia-Pacific Journal of Ophthalmology, Investigative Ophthalmology and Visual Science, and Survey of Ophthalmology, published since 1990. Abstracts and related metadata such as journal names, publication years, and DOls were extracted using e-utils. Further, we conducted quality control and removed abstracts with invalid metadata. Each abstract was treated as a single document in the corpus."}, {"title": "Preferred Practice Patterns.", "content": "We further collected Preferred Practice Patterns in Ophthalmology from the American Academy of Ophthalmology (AAO): These guidelines are publicly accessible via the AAO website and offer expert panel-developed recommendations for high quality eye care. The guidelines are updated every five years and may span hundreds of pages. Each page was processed as a single document in the corpus."}, {"title": "EyeWiki.", "content": "This collection comprises publicly accessible articles written by ophthalmologists. Aimed at providing introductory and educational materials using simpler language, the articles cover various topics on eye diseases, diagnoses, and treatments. Each page was treated as a single document in the corpus."}, {"title": "Indexing, Embedding, and Querying.", "content": "The documents underwent further segmentation into 1024-token snippets for indexing. Text snippet embeddings (semantic representations) were generated using text-embedding-ada-00232 and stored in the database. Given a natural language query, the RAG pipeline generates its embedding, performs dense retrieval, identifies the top similar candidates based on cosine similarity of embeddings, and augments those candidates to an LLM to generate a response."}, {"title": "Case Investigation and Evaluations", "content": "Task. We evaluated the effectiveness of RAG in augmenting LLMs for long-form question answering in ophthalmology. In contrast to binary or multiple-choice questions, long-form question answering involves providing a free-text passage with reasoning and supporting evidence to justify the answer. We chose 100 publicly accessible question-answer pairs from the Ask An Ophthalmologist forum by AAO, by sampling 20 each from the following five topics: Retina, Glaucoma, Cataract, Dry Eye, and Uveitis. These questions cover various aspects of eye health, vision problems, ophthalmic conditions, and eye care.\nEach question prompted the LLM as follows: \"Answer this question and provide references at the end of your response. The references should adhere to the AMA format,\" Same prompts were used for the LLMs with and without RAG. When using RAG, it retrieves the top 10 most relevant documents in the corpus to augment the LLM. We used GPT-3.5 (gpt-3.5-turbo-0613) as the representative LLM. We chose GPT-3.5 as the representative LLM for its reasonable accuracy and cost efficiency10. Note that RAG can be integrated into any LLMs. Each LLM with and without RAG will provide different references and answers, which adds substantially to the manual evaluation task. The focus of the paper is to assess the evidence factuality, evidence selection, and evidence attribution, not the performance of LLM itself. The temperature of the LLM was set to 0 to minimize the variance of generated responses. We also manually verified that providing top 10 most relevant documents did not reach the token limit of GPT-3.5.\nEvaluation of Factuality of Evidence. We manually examined the top three references in the LLM responses, categorizing them as (1) correct references, where the references are real with correct metadata, (2) references with minor errors, where the references are real but have minor metadata errors, or (3) hallucinated references, where the references do not exist.\nEvaluation of the Selection and Ranking of Retrieved Documents. For the top 10 most relevant documents retrieved by RAG, we quantified how many were selected as the top three references in the LLM responses and the average rankings of the selected documents. For"}, {"title": "Evaluation of Response Accuracy, Completeness, and Evidence Attribution.", "content": "We further sub-sampled 30 questions, 10 each for retina, glaucoma, and cataract, representing three major ophthalmology subspecialties. Ten healthcare professionals (six medical students, three residents, and one attending specialist) manually evaluated the responses, with and without RAG, in a blinded manner with shuffled orders. Each response was rated on three axes: accuracy, completeness, and evidence attribution, on a scale from 1 (poor) to 5 (perfect). The details of the healthcare professionals and evaluation guidelines are provided in Supplementary Materials."}, {"title": "Results", "content": "Evaluation of Factuality of Evidence\nFigure 2(A) shows the evaluation results. For the 100 questions, the LLM without RAG and with RAG provided 252 and 277 references in the responses, in total, respectively. Of the 252 references of the LLM without RAG, 20.6% (52) were correct references (i.e., real references with correct metadata), 34.1% (86) were references with minor errors (i.e., real references but with minor metadata errors), and 45.3% (114) were hallucinated references.\nIn contrast, out of the 277 references in the responses the LLM with RAG, 54.5% (151) were correct references, 26.7% (74) were references with minor errors, and 18.8% (52) were"}, {"title": "Evaluation of the Selection and Ranking of Retrieved Documents", "content": "In total, of the 277 references provided in the LLM + RAG responses, 173 references were from the top 10 most relevant documents retrieved by RAG. In other words, only 62.5% of the references identified by RAG were selected as top references in the final LLM responses. In addition, the average ranking of those top references was 4.89 (standard deviation 2.40), with median ranking of 4.67. This implies that the top-ranked documents by RAG may not be selected by LLM in the final response; the selected documents in the final responses are at the median rankings by RAG."}, {"title": "Discussions", "content": "Main Findings and Interpretations\nIn this study, we developed a domain-specific RAG pipeline consisting of about 70,000 documents, including biomedical literature, clinical practice guidelines, and relevant wiki articles in ophthalmology, and performed a systematic evaluation on the case study of long-form consumer health question answering with 10 healthcare professionals. This study contributes three main findings.\nFirst, it demonstrates that LLMs frequently include hallucinated and erroneous evidence while they may generate reasonable answers. The results quantify that almost half of the references are hallucinated and about 30% of references contain errors in the responses of LLMs. This pressing issue needs to be addressed as it concerns the downstream accountability of LLMs in the medical domain.\nSecond, through a systematic evaluation on LLMs with RAG, the results highlight that RAG could improve factuality and evidence attributions; however, there are three primary challenges to address: (1) LLMs may not always select the documents provided by RAG, resulting in the"}, {"title": "Comparison with Literature", "content": "As mentioned, most studies focused on assessing the content level of LLM-generated responses, such as content accuracy, rather than on the evidence level16. In the medical domain, however, evidence is arguably more crucial33; biomedical researchers and healthcare professionals need to verify evidence and justify claims rather than focusing solely on responses. Pioneering studies found that up to 90% of LLM responses are not supported by the sources they provide34. Our case evaluation on real consumer health questions systematically quantifies the factuality of evidence, selection and ranking of evidence, and evidence attribution, in addition to response accuracy and completeness. Additionally, only a few studies have implemented RAG in specific downstream applications. We are aware of only one existing study that applied RAG to improve the accuracy of multiple-choice questions in LLMs within ophthalmology31. We implemented by far the largest ophthalmology-specific RAG pipeline.\nFurthermore, studies in the medical domain have reported higher accuracy when using RAG, but this is often based on simpler tasks such as multiple-choice question answering23,27,31. Our results reveal important challenges in long-form question answering for real consumer health questions, particularly where there are no candidate answer options and LLMs may need to synthesize information from multiple documents. The evaluation highlights the three challenges of using RAG to augment LLM responses as detailed above."}, {"title": "Limitations", "content": "Our study also has several potential limitations. First, as manual annotations (540 annotations per participant) are costly, we evaluated only GPT-3.5 as the representative LLM and default RAG configurations as the representative RAG. Each representation will require re-annotation. However, this is arguably the most common choice for downstream users. In the future, we will also evaluate other representative LLMs including GPT-4, LLaMA35, and PMC-LLaMA36, as well as explore different RAG setups, such as using domain-specific embeddings for semantic search 37. Second, we hypothesized that Ophthalmology domain-specific resources are more effective and curated about 70K Ophthalmology domain-specific resources ranging from biomedical literature, clinical guidelines, and educational materials for RAG. However, it might be possible that general domain or medical domain resources also contain relevant documents. We will evaluate the trade-offs for selection of corpora (e.g., using the entire PubMed vs Ophthalmology-specific literature) in the future."}]}