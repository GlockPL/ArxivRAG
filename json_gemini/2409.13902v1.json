[{"article_text": "Enhancing Large Language Models with Domain-specific Retrieval ...  Ron A. Adelman, MD, MPH\u00b9 ...   ..."}, {"title": "Enhancing Large Language Models with Domain-specific Retrieval Augment Generation: A Case Study on Long-form Consumer Health Question Answering in Ophthalmology", "authors": ["Aidan Gilson", "Xuguang Ai", "Thilaka Arunachalam", "Ziyou Chen", "Ki Xiong Cheong", "Amisha Dave", "Cameron Duic", "Mercy Kibe", "Annette Kaminaka", "Minali Prasad", "Fares Siddig", "Maxwell Singer", "Wendy Wong", "Qiao Jin", "Tiarnan D.L. Keenan", "Xia Hu", "Emily Y. Chew", "Zhiyong Lu", "Hua Xu", "Ron A. Adelman", "Yih-Chung Tham", "Qingyu Chen"], "abstract": "Despite the potential of Large Language Models (LLMs) in medicine, they may generate responses lacking supporting evidence or based on hallucinated evidence. While Retrieval Augment Generation (RAG) is popular to address this issue, few studies implemented and evaluated RAG in downstream domain-specific applications.  We developed a RAG pipeline with ~70,000 ophthalmology-specific documents that retrieve relevant documents to augment LLMs during inference time. ... and completeness.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) represent one of the latest advancements in Al systems designed for language modeling1,2. Compared with early language models, LLMs demonstrate notable capabilities in natural language generation and reasoning tasks such as reading comprehension\u00b3, translation4, and question-answering5. ... RAG techniques."}, {"title": "Discussions", "content": "Main Findings and Interpretations In this study, we developed a domain-specific RAG pipeline consisting of about 70,000 documents, including biomedical literature, clinical practice guidelines, and relevant wiki articles in ophthalmology, and performed a systematic evaluation on the case study of long-form consumer health question answering with 10 healthcare professionals. This study contributes three main findings. First, it demonstrates that LLMs frequently include hallucinated and erroneous evidence while they may generate reasonable answers. The results quantify that almost half of the references are hallucinated and about 30% of references contain errors in the responses of LLMs. This pressing issue needs to be addressed as it concerns the downstream accountability of LLMs in the medical domain. Second, through a systematic evaluation on LLMs with RAG, the results highlight that RAG could improve factuality and evidence attributions; however, there are three primary challenges to address: (1) LLMs may not always select the documents provided by RAG, resulting in the"}]}]