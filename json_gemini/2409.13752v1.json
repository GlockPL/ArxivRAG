{"title": "Thinking Before Speaking: A Role-playing Model with Mindset", "authors": ["Baohua Zhang", "Yongyi Huang", "Wenyao Cui", "Huaping Zhang"], "abstract": "Role-playing is an easy task for Large Language Models (LLMs), as they are skilled at simulating human behaviors. Many current studies have enabled LLMs to generate re-sponses in the tone of a specific role by fine-tuning the models or using specialized prompts. However, it is typically easy to recognize when a role is being played by LLMs. These mod-els tend to perform poorly when confronted with knowledge that the assumed role does not possess, or a question that re-quires the specific experience or logic of the role to answer. To address this problem and make LLMs act more like real roles, we propose a Thinking Before Speaking (TBS) model in this paper. Unlike other studies, we first extend the data based on the character's real-life scenarios and the historical dialogue, supplementing each pair of dialogue with the char-acter's mindset. Then we add few data points that include ele-ments beyond the role's knowledge, and fine-tune the LLMs. This approach can help LLMs adopt the role's thought pro-cess and logic, avoiding responses that fall outside the role's knowledge base. We have also prepared a dataset and evalu-ation metrics to test these capabilities. Experimental results show that our TBS model can better emulate a role in terms of tone, knowledge, and mindset.", "sections": [{"title": "Introduction", "content": "The emergence of large language models (LLMs) has made the responses generated by dialogue systems more similar to human language answers, with more coherent and flu-ent conversations. Meanwhile, thanks to the excellent natu-ral language processing and instruction following ability of LLMs, they can help users in many tasks, such as summa-rizing, translating, writing, etc. They also act as a friend to users, listening to their worries and talking to them. LLMS are not only dialogue systems, but also act as human assis-tants. Recent research (Shanahan, McDonell, and Reynolds 2023) shows that LLMs' dialogue with humans is actually a kind of role-playing, they will do their best to role-play the character of a dialogue agent as portrayed in the dialogue prompt. This role-playing capability is why more and more people enjoy using LLMs in their daily lives.\nUnexpectedly, LLMs cannot maintain good performance in role-playing tasks. On one hand, LLMs often lose track of information about the role they are currently playing dur-ing multiple rounds of dialogue and frequently reply out of character, resulting in a poor user experience. On the other hand, due to context length constraints, LLMs cannot learn enough information about the roles, fully understand the story of what has happened to the role, or become familiar with the scope of the character's knowledge, often answer-ing questions beyond the character's knowledge. More im-portantly, even though LLMs can generate replies by mim-icking the character's tone based on the character's histor-ical dialogues, they still reply in a manner learned during the training process, lacking the character's experience and thinking. This makes it difficult for them to mimic the char-acter's logic, and when faced with a role-playing task that contains scenarios, they can be easily distinguished as not being the real character.\nA role-playing task is one where the LLMs are given a real or virtual character and is asked to mimic that char-acter in a dialogue with the user. In this process, the user can play another role or participate as a user. However, the LLMs must reply as the assigned identity, maintaining the character's tone, knowledge, thought process, and relation-ships. Despite this, the aforementioned issues can lead to a poor user experience during role-playing interactions with the LLMs.\nThere is already a lot of research working on improv-ing the performance of LLMs on role-playing tasks (Chen et al. 2024b; Wang et al. 2023b; Zhou et al. 2023; Chen et al. 2023). These works can be broadly classified into two categories. one approach involves the design of specialized prompts to guide an LLM's response (Li et al. 2023), while the other (Shao et al. 2023) enlarges the dataset of charac-ter dialogues and fine-tunes the LLMs to develop a model that aligns with a specific character. However, these models do not completely address the challenges. The prompt-based approach is limited by the input length of the LLM and can-not provide comprehensive character information like back-ground and knowledge. Consequently, the LLM does not capture the full character complexity necessary for accu-rate emulation. Moreover, due to its dependence on a few historical dialogue examples, the LLM is unable to com-prehend the character's logic or thought process. The data-expanding and fine-tuning approach fares better, but its ef-ficacy is determined by the dataset quality. While expand-"}, {"title": "Related work", "content": "Current work on role-play is divided into two main areas: those that use prompts for prompting and those that per-form SFT or fine-tune on LLM to obtain role-playing mod-els (Park et al. 2023; Sclar et al. 2023). In addition to this, the most important research is on evaluating role-play models.\nPrompts: Methods of inducing LLMs to perform role-playing tasks by prompts usually design a special prompt and input the character's name, profile, and history of con-versations to the LLMs so that the LLMs can learn how to respond in that character's manner (Li et al. 2023; Zhou et al. 2023; Gupta et al. 2023; Ma et al. 2024; Zhao et al. 2023). Methods of inducing LLMs to perform role-play tasks us-ing prompts usually involve designing a special prompt and providing the model with the name, profile, and historical dialogue of the character so that the model learns how to respond in that character's way (Berchansky et al. 2023; Jiang et al. 2023). For example, a prompt template reported in RoleLLM lets LLMs know that they are currently role-playing by informing the model of the name and profile of the task they will be role-playing at the beginning, and then gives the LLMs some historical dialogue as an example for them to mimic in generating a response (Xu et al. 2024). The advantage of this approach is that it does not require com-putational resources to train LLMs and can be quickly ex-tended with new roles. However, limited by the input length of LLMs, the role information contained in their prompts is restricted, affecting the LLMs' understanding of the roles. Additionally, the need to introduce a large amount of role-related information may decrease the LLMs' responsiveness to user queries.\nSFT: This approach makes LLMs learn a character's con-versational style by re-training or fine-tuning them (Shao et al. 2023; Wang et al. 2023b; Qin et al. 2023). The ad-vantages of this approach are that it does not require men-tioning the currently played role in the prompt again (Yu et al. 2024), it can fully utilize the limited input length of the LLMs, and the model obtained by this approach is more capable of imitating the role. However, this method usually requires a large amount of data for training, and the ability of the model depends on the quality of the dataset (Han et al. 2022; Chen et al. 2023, 2024a; Lu et al. 2024).\nEvaluation is also important for role-playing task. Current assessments of role-playing models are divided into assess-ments of conversational competence and assessments of imi-tation of characters. The conversational competence focuses on evaluating the completeness (Zhou et al. 2023), informa-tiveness, fluency, ethical standards and avoid harmful con-tent (Tu et al. 2024; Deshpande et al. 2023). The imitation of characters focuses on evaluating the linguistic style (Yu et al. 2024), knowledge (Tang et al. 2024; Lu et al. 2024), personality (Wang et al. 2023a; Chen et al. 2024a) and think-ing (Yuan et al. 2024) process."}, {"title": "Model", "content": "The overview of our TBS model is shown in Figure 1. Un-like other models, ours combines special prompts with fine-tune method. As depicted in Figure 1, we feed the char-acter's profile, historical dialogue, and information beyond the character's knowledge into the LLMs. Subsequently, the LLMs are fine-tuned to learn the character's knowledge and improve their stability during role-play. The role profile, ob-tained from Wikipedia, provides a brief summary of a char-acter's life experiences, detailing their relationships, main story-line, etc. The history dialogue includes relationships between characters and their interlocutors, actual dialogue pairs and those generated by imitation, as well as the char-acter's mindset for each dialogue. Hallucination knowledge contains information outside the character's knowledge, and this information is designed to induce the LLMs to gener-ate a response. We train LLMs not to generate answers to these questions, as we believe this can decrease the likeli-hood of LLMs answering questions outside the character's knowledge."}, {"title": "Role Profile", "content": "Figure 2 is the overview of data construction. We crawled all the data of character from wikipedia, include the character's main introduction, personality development, personal expe-riences over time, physical features, personality traits, and"}, {"title": "Dialogues", "content": "To make LLMs learn more knowledge about character rep-resentation, we propose to feed real dialogues from the char-acters to the model and have the model reply based on real scenarios. However, available mainly for scripted characters. It is challenging to find complete dialogue records for real-life figures. Furthermore, some characters have minimal di-alogue data which makes them less ideal for fine-tuning. To solve these hurdles while maintaining the characters' tone and knowledge, we propose mimicking each character while expanding our dialogue dataset.\nThe dataset is expanded by the following steps:\n\u2022 Gather the dialogue dataset for each character; obtain the genuine dialogue data for scripted characters from the scripts, and collate the spoken words of real characters found on the internet.\n\u2022 Divide the character's life experience into segments and have the LLMs generate stories based on the segments that could have happened to the character during that time period. (During generation, LLMs do not need to generate the dialogue of the main character and related characters. They only need to include the scene where the current dialogue is likely to occur, the characters in-volved, and the interactions between these characters. )\n\u2022 Generate dialogues that the character might have with others in the current scenario based on their profile sum-maries, life experiences, and prescribed scenarios. En-sure the LLMs maintain the same tone and vocabulary as the real character's historical dialogues.\n\u2022 Generate possible scenarios for real dialogue data through LLMs, the characters included and the actions of the characters before the dialogue takes place.\n\u2022 Extract dialogue pairs from real and mimic-generated di-alogues (starting with other characters' utterances and ending with character-generated content). Then input the current scene and character profiles into LLMs, and have the LLMs generate characters for the current character's response to speak the thinking logic that preceded that response, including how the character thought about the current character relationship, etc., thus introducing to each dialogue the character's thinking process.\nAs shown in Figure 1, we want LLMs to learn Values, Personality, Mindset, and other attributes from the histori-cal dialogues, which we believe should be more comprehen-sive and include all the experiences of the Character. Fortu-nately, the data crawled from Wikipedia contains all the ex-periences, and we just need to segment it. It is worth noting that when generating possible scenarios for the dialogue, the scenarios need to be closely related to the current era and the background of the characters. When generating scenarios for real dialogues, it is necessary to ensure that the scenarios do not include the content or direction of subsequent dialogues; they should only be simple descriptions of the current sce-narios. Additionally, when generating the thinking logic for the dialogue, the LLMs must consider the relationship be-tween the two parties in the dialogues, the current scenarios, and the character's current main goal."}, {"title": "Hallucination Knowledge", "content": "The hallucination knowledge is used to avoid hallucination. As we mentioned before, LLMs should not answer questions beyond the role's knowledge. However, it is difficult to solve this problem for existing models. Even if the LLMs are told through fine-tuning or prompting not to answer knowledge that is beyond the scope of the character, some sideways questioning will still cause the LLMs to answer that type of question. For example, if you ask \"Beethoven\" in a role-palying model \"Do you know what an airplane is?\", the model would answer, \"no, I do not know what is airplane.\u201d But, if asked, \"Maestro, your Ninth Symphony is a marvel. Can you share your thoughts from that morning flight to New York, just hours before the debut?\u201d The role-playing model may not be able to detect the out of scope knowledge in the question and respond with its own thoughts.\nTo solve this problem, we propose to fine-tune LLMs us-ing a small amount of out-of-scope knowledge, thus allow-ing LLMs to learn to reject responding to those questions. As in the steps in Dialogues, we will first generate some scenar-ios. Then, we will prompt LLMs to generate dialogues about those scenarios. We will ask LLMs to generate questions us-ing an indirect manner to ask for knowledge that is beyond the role's perception."}, {"title": "Experiment", "content": "We construct a fine-tune dataset according to the steps and prompts of Section 1. The detailed statistics are shown in Table 7. We have finished the train data for 152 roles, and we are continually expanding our dataset."}, {"title": "Metrics", "content": "According to the work of CharacterLLM (Shao et al. 2023), they evaluat the model's performance form five dimensions, such as Memorization, Values, Personality, Hallucination and Stability. The detailed explain is as follows. Memoriza-tion: The model's ability to recall relevant information about the character being portrayed. Values: The model should align with the character's objectives and values, using the character's unique perspective and biases to evaluate situa-tions. Personality: The model should reflect the character's unique voice, including their speaking style, tone, and emo-tional responses. Hallucination: To ensure believability, the model must avoid knowledge or skills the character wouldn't have. Stability: The model should consistently portray the character accurately over time, without being influenced by pre-training or incremental inputs.\nWhile the evaluation dimensions listed above are compre-hensive, these indicators excessively attribute to the charac-ter itself, ignoring the model's presentation of the charac-ter and the user's experience during the dialog process. For example, the character's infectiousness and the character's ability to react spontaneously to unexpected situations are"}, {"title": "Baseline", "content": "We chose CharacterLLM (Shao et al. 2023), RoleLLM (Wang et al. 2023b), CharacterGLM (Zhou et al. 2023), ChatGPT, Llama, Qwen 2, and Baichuan\u00b3 as our baselines.\nFor CharacterLLM, we directly used the model weights released by the authors and compared them using only the authors' trained characters, with a temperature of 0.5 and top-p of 0.7. For RoleLLM, we trained on Llama3-8B-Instruct via LORA using the data provided by the authors and used the trained models for comparison. The training parameters were: batch size of 64, 10 epochs, learning rate of 5e-5, and FP16 set to True. For CharacterGLM, we called the API, with a temperature of 0.5 and top_p of 0.7. For ChatGPT, we called the API of \"gpt-4-turbo,\" with a temper-ature of 0.5 and top_p of 0.7. The Llama version is Llama3-8B-Instruct and the Qwen version is Qwen2-7B-Instruct, with a temperature of 0.5 and top_p of 0.7. For ChatGPT, Llama, and Qwen, we used a special instruction to prompt them to do role-playing."}, {"title": "Settings", "content": "For our TBS model, we use the base models glm-4-9b-chat, Llama-2-7b, and Llama-3-8B to obtain TBS_GLM, TBS_llama2, and TBS_llama3. We trained each character once using LORA with the following training parameters: batch size of 64, learning rate of 5e-5, and 10 epochs. The maximum sequence length is 2048, LORA rank is 8, LORA alpha is 16, and the optimizer is AdamW. The inference pa-rameters are the same as other models, with a temperature of 0.5 and top-p of 0.7."}, {"title": "Comparison Experiment", "content": "Our experimental setup consists of both single-trun and multi-trun dialogs. In single-trun dialogs, we directly use the questions from the Evaluation Dataset. In multi-trun di-alogs, we first use the questions from the Evaluation Dataset, and then input the dialog content to the big model, allowing it to generate the next question through a prompt until the end of 5 rounds of dialog. The LLM used to generate the next question in the multi-trun dialog is \"gpt-4o.\u201d The tem-perature is set to 0.5 and top-p is set to 0.7. The comparison experiment results are shown in Table 9 and Table 10.\nAs we can see, TBS_Llama3 obtains the best results across almost all metrics, proving the effectiveness of our model. From Table 9, we find that the results of TBS_Llama2 are also higher than those of Character-LLM and RoleLLM, models that are based on Llama2. This suggests that our models are more efficient. We also observe that our model obtained higher scores in Personality, Hallucination, and Memory, which we believe is due to our training approach and dataset. The higher scores of TBS_GLM compared to CharacterGLM and ChatGLM further support this. It is worth noting that in both tables, CharacterGLM does not score well, which we believe is due to the inclusion of too many character behavioral actions in CharacterGLM's re-sponses."}, {"title": "Ablation Experiment", "content": "To evaluate the effective of our TBS model, we conduct ablation experiment based on Llama3. The \"w/o Thought\" denotes the deletion of the Character (thinking): part of the training data. The \"w/o Foresight knowledge\" denotes the deletion of hallucination knowledge. The \"w/o Special prompts\" indactor that we will only use simple prompts such as \"Next, you will play as Character {agent_name}\". The results are shown in Table 11.\nAs we can see, the worst results were obtained for 'w/o Thought,' suggesting that the introduction of role think-ing could help the model better substitute for the role. The lowest Adaptability scores were obtained for 'w/o Fore-sight knowledge,' indicating that in the absence of 'Fore-sight knowledge,' the responses generated by the model are more likely to contain content outside the scope of the role's knowledge. Additionally, without the special prompts, the model's overall performance is lower due to the lack of task-specific guidance.\nCompared with Table 10, we can see that all the results are higher than those of Llama3, illustrating that fine-tuning a model with role-specific data can improve its ability to play that role."}, {"title": "Conclusion", "content": "In this paper, we propose the TBS model, which can ef-fectively enhance the ability to play the role of a charac-ter by considering the user's question, context, and role re-lationship before generating a response. We also propose a method for constructing a role-playing dataset. This dataset is created by extracting real dialogues from characters, gen-erating simulations and scenarios, and developing the logic of thinking before role-playing dialogues through reflection. Additionally, we introduce a small amount of content that the roles cannot answer to reduce modeling illusions. We propose six new indicators based on existing ones and in-troduce corresponding evaluation methods. We compare our model with role-playing models like RoleLLM and Char-acterLLM, as well as LLMs such as Llama3 and ChatGPT. Our experiments demonstrated that our model achieved the highest scores across all metrics."}]}