{"title": "A Survey of Camouflaged Object Detection and Beyond", "authors": ["Fengyang Xiao", "Sujie Hu", "Yuqi Shen", "Chengyu Fang", "Jinfa Huang", "Chunming He", "Longxiang Tang", "Ziyun Yang", "Xiu Li"], "abstract": "Camouflaged Object Detection (COD) refers to the task of identifying and segmenting objects that blend seamlessly into their surroundings, posing a significant challenge for computer vision systems. In recent years, COD has garnered widespread attention due to its potential applications in surveillance, wildlife conservation, autonomous systems, and more. While several surveys on COD exist, they often have limitations in terms of the number and scope of papers covered, particularly regarding the rapid advancements made in the field since mid-2023. To address this void, we present the most comprehensive review of COD to date, encompassing both theoretical frameworks and practical contributions to the field. This paper explores various COD methods across four domains, including both image-level and video-level solutions, from the perspectives of traditional and deep learning approaches. We thoroughly investigate the correlations between COD and other camouflaged scenario methods, thereby laying the theoretical foundation for subsequent analyses. Furthermore, we delve into novel tasks such as referring-based COD and collaborative COD, which have not been fully addressed in previous works. Beyond object-level detection, we also summarize extended methods for instance-level tasks, including camouflaged instance segmentation, counting, and ranking. Additionally, we provide an overview of commonly used benchmarks and evaluation metrics in COD tasks, conducting a comprehensive evaluation of deep learning-based techniques in both image and video domains, considering both qualitative and quantitative performance. Finally, we discuss the limitations of current COD models and propose 9 promising directions for future research, focusing on addressing inherent challenges and exploring novel, meaningful technologies. This comprehensive examination aims to deepen the understanding of COD models and related methods in camouflaged scenarios. For those interested, a curated list of COD-related techniques, datasets, and additional resources can be found at https://github.com/ChunmingHe/awesome-concealed-object-segmentation.", "sections": [{"title": "INTRODUCTION", "content": "BJECT detection, a fundamental task in computer vision, involves identifying and locating objects within images or videos. It comprises various fine-grained sub-fields: generic object detection (GOD), salient object detection (SOD), and camouflaged object detection (COD). GOD aims to detect general objects, while SOD identifies prominent objects that stand out from the background. In contrast, COD targets those objects that blend into their surroundings, making it an extremely challenging task. COD has recently garnered increasing attention and rapid development for its advantages in facilitating the development of visual perception for nuance discrimination and promoting various valuable real-life applications, ranging from concealed defect detection in industry and pest monitoring in agriculture to lesion segmentation in medical diagnosis and art, such as recreational art and photo-realistic blending.\nHowever, unlike GOD and SOD, COD involves detecting objects that are purposefully designed to be inconspicuous, like the Dalmatian hidden in the forest on the far right of Fig.1 which is difficult to detect due to its camouflage with the surroundings, thus requiring more sophisticated detection strategies. COD can be further classified into image and video tasks . Normal COD, i.e., image-level COD, to detect camouflaged objects in static images, whereas video-level COD, dubbed VCOD, deals with detecting these objects within video sequences. The latter one introduces additional complexity due to temporal continuity and dynamic changes, necessitating models capable of extracting both spatial and temporal features effectively.\nTraditional methods for COD and VCOD, including tex-ture, intensity, color, motion, optical flow, and multi-modal analysis , have demonstrated their strengths in specific scenarios but also exhibit notable shortcomings. These approaches, relying on manually designed operators, suffer from limited feature extraction capacity, thus struggling with complex backgrounds and varying object appearances, constraining the accuracy and robustness.\nIn contrast, deep learning-based COD method, e.g., convolutional neural network (CNN), transformer, and diffusion model, offer significant advantages by automatically learning rich feature representations . In addition, these methods utilize various strategies to address such challenging task, e.g., aggregating multi-scale features , bio-inspired mechanism-simulation , fusing multi-source information , learning multi-task , jointing SOD , and setting novel-task . Despite their advantages, these methods also face intractable challenges, including high computational demands and the requirement for large annotated, clean, and paired datasets .\nSeveral surveys have been conducted on COD, with three seminal works providing valuable overviews of the field. However, these surveys have limitations due to the narrow scope and limited number of papers they cover. For example, most of the methods discussed in these surveys are from before the first half of 2023, resulting in insufficient historical depth and domain breadth. As illustrated in Fig. 2, the COD field has seen rapid development in 2023. To address these gaps, we propose a more comprehensive survey that not only covers traditional and deep learning COD methods across both image and video domains but also benchmarks deep learning models in these areas. Furthermore, to the best of our knowledge, this survey is the first to deeply explore novel tasks such as referring-based COD and collaborative COD . We also provide a broader review of commonly used COD datasets and comprehensively cover recent advancements, challenges, and future trends.\nThe motivation for this paper stems from the critical importance of COD and the inadequacies of existing surveys. Our survey aims to provide a more thorough and detailed examination of COD, address gaps in the current literature, and highlight recent developments. We systematically categorize and analyze existing cutting-edge techniques, identify critical challenges, and suggest future research directions to advance the field.\nOur contributions are summarized as follows:\n\u2022 We provide a comprehensive review of existing COD methods and related tasks in camouflaged scenario understanding (CSU), along with commonly used datasets and evaluation metrics. To the best of our knowledge, this work represents the most extensive investigation to date, encompassing approximately 180 CSU-relevant cutting-edge studies.\n\u2022 We methodically benchmark 40 representative image-level models and 8 representative video-level models based deep features on 6 characteristic datasets and 6 typical evaluation metrics, providing the quantitative and qualitative analysis of them.\n\u2022 We systematically identify the limitations of existing COD methods and propose potential directions for future research. By shedding light on these challenges and opportunities, our work serves to guide and inspire further research efforts to advance the state-of-the-art in COD technology.\n\u2022 We create a repository that houses a carefully curated collection of COD methods, datasets, and relevant resources, which will be consistently updated to ensure the latest information is accessible.\nWe hope that this survey on COD will not only enhance understanding of the field but also stimulate greater interest within the computer vision community, fostering further research initiatives in related areas.\nNote. In developing our search strategy, we conducted a thorough investigation across a variety of databases, including DBLP, Google Scholar, and ArXiv Sanity Preserver. Our focus is particularly directed toward reputable sources, such as TPAMI and IJCV, as well as prominent conferences like CVPR, ICCV, and ECCV. We prioritized studies that provided official codes to enhance reproducibility, as well as those with higher citations and Github stars, indicative of significant recognition and adoption within the academic community. Following this initial screening, our literature selection process involved a rigorous evaluation of each paper's novelty, contribution, and significance, and an assessment of its status as seminal work in the field. While we acknowledge the possibility of omitting some noteworthy papers, our aim is to present a comprehensive overview of the most influential and impactful research, promoting re-search advancement and suggesting potential future trends and directions."}, {"title": "2 IMAGE-LEVEL COD MODELS", "content": "Image-level COD refers to the process of identifying and distinguishing objects that are designed to blend into their surroundings 8 within static images, garnering significant attention currently. In this section, we categorize these methods into two primary approaches based on the features they utilize: traditional COD methods and deep learning COD methods. Traditional approaches typically rely on handcrafted features, whereas deep learning methods leverage neural networks to automatically learn and extract discriminative features from data. Given the rapid advancements in technology, we will focus primarily on deep learning COD methods, which have recently become the predominant approach."}, {"title": "2.1 Traditional COD methods", "content": "COD is initially rooted in traditional image-level methods, leveraging hand-crafted low-level features tailored to capture nuances in textures, intensities, and colors. These approaches constituted the foundation of early efforts in this domain. Texture features capture the surface properties and distinctive patterns of images, usually manifested through grayscale distributions across pixels and their spatial neighborhoods. These features are utilized to distinguish camouflaged objects from their backgrounds based on texture differences.\nGalun et al. introduce a bottom-up approach, TSMA, for texture segmentation. This method leverages the adaptive identification and characterization of texture elements-such as size, aspect ratio, orientation, and brightness-combined with filter response statistics for enhanced differentiation and noise reduction. Unlike TSMA, which extracts global texture features, Bhajantri et al. employed a co-occurrence matrix for block-based local texture analysis. Their method uses Watershed segmentation for each defective block, processed through a dendrogram to distinguish the target, and concludes with cluster analysis to further confirm detection results.\nBuilding upon earlier works, Sengottuvelan et al. propose an unsupervised technique for decamouflaging that converts images to grayscale and then splits them into blocks for gray-level co-occurrence matrix analysis, capturing pixel-neighbor relationships. They use a dendrogram plot to identify camouflaged objects without requiring prior background knowledge. Traditional evaluation methods often rely on subjective assessments, which can be cumbersome and ambiguous processes. To address it, Song et al. leverage a weighted structural similarity index and intrinsic image feature analysis to assess camouflage textures. To be more objective, Feng et al. utilize human visual-based saliency maps to quantitatively assess texture differences.\nIntensity feature-based approaches. These methods progressively advance from basic intensity-based techniques to more sophisticated exploitation of 3D convexity. CBDCE detected regions of interest by processing intensity images directly, distinguishing between 3D convex and concave regions. This approach demonstrates robustness against variations in illumination, orientation, and scale. To enhance the detection of 3D objects camouflaged in complex scenes, CVCB enhanced the D-Arg operator in CBDCE. It maximizes the response to curved 3D objects against flat backgrounds, effectively mitigating visual camouflage in both natural and artificial environments. CTDDC employed another 3D convexity-based operator that exploits image gray levels and median filtering to eliminate background noise, enabling effective and robust detection.\nColor feature-based approaches. In certain scenarios, color contrast and distribution can provide significant distinctiveness for separating camouflaged objects from their surroundings. Siricharoen et al. optimized a statistical background subtraction and shadow detection algorithm by integrating color, edge, and intensity features for outdoor human segmentation, where strong shadows and low contrast are common. They employ vector median filtering to remove outlier pixels and combine color statistics with edge to generate initial coarse results, which are then refined using intensity features for accuracy. Kavitha et al. propose an image retrieval technique for COD, which involves segmenting images into blocks and extracting Hue, Saturation, Value (HSV) color, and gray-level co-occurrence matrix texture features from each block. They use the principle of matching images based on similarity and Euclidean distance to improve detection accuracy."}, {"title": "2.2 Deep learning COD methods", "content": "While traditional methods rely on hand-crafted low-level features to capture key attributes of an image, deep learning methods extract complex and deep features directly from the data through automatic learning representations, demonstrating superior performance across various computer vision tasks. According to the existing surveys , deep learning methods for COD can be broadly categorized based on three fundamental criteria: network architecture, learning paradigm, and supervision level. A detailed description of the three criteria is shown in Fig. 3. What's more,\nNetwork architecture delineates how the input and output configurations are structured within the models. The linear employs bottom-up/top-down network, where the data flows in a single feed-forward pass. Aggregative architecture combines features from multiple input streams, while branched architecture is characterized by multiple pathways for multiple outputs, associated with the multi-task learning paradigm. The hybrid integrates elements and strategies from the aforementioned ones to leverage their strengths.\nThe learning paradigm pertains to the approach adopted by the models to learn and adapt. This includes single-task and multi-task learning, specifically the former only involving COD, while the latter often importing auxiliary tasks, e.g., localization/ranking, reconstruction, and predicting associated cues, such as boundary , texture and uncertainty , leading to improved accuracy and performance.\nThe supervision level describes the degree and nature of supervision provided during the training phase. This can be classified into four categories: fully supervised, with complete ground truth data; weakly-supervised, which utilizes limited or imprecise annotations ; semi-supervised, which combines labeled and unlabeled data, generating pseudo labels for the latter ; and unsupervised, where no explicit labels are provided . Notably, the \"train-free\" mode refers to models that do not require a traditional training process.\nBeyond these categories, existing works also employ several strategies, such as aggregating multi-scale features, simulating bio-inspired mechanisms, fusing multi-source information, learning multiple tasks, jointing SOD, and establishing novel tasks, all aimed at enhancing COD performance. These strategies underscore the diverse and innovative approaches that researchers adopt to address the challenges in COD, revealing the underlying principles and techniques driving advancements. By focusing on these strategies, we can better understand the strengths and limitations of different methods, identify emerging trends, and provide a clearer roadmap for future research."}, {"title": "3 VIDEO-LEVEL COD MODELS", "content": "Unlike image-level Camouflaged Object Detection (COD) techniques, which focus on single static images, video-level COD requires greater emphasis on motion cues to identify and localize camouflaged objects within continuous video frames. Video COD (VCOD) typically leverages temporal information, such as motion and changes across frames, to reveal objects that are difficult to detect in individual frames. However, this task presents significant challenges, including complex background noise, lighting variations, occlusions, and diverse camouflage strategies. Additionally, the high-dimensional nature of video data necessitates algorithms that are not only spatially accurate but also temporally consistent and stable. In this section, we categorize existing methods into two main types: traditional approaches and deep learning-based approaches."}, {"title": "3.1 Traditional VCOD methods", "content": "Compared to image-level methods, the video-level ones include more techniques, like optical flow analysis and motion detection, making them practical in video surveillance and security.  this section will delve into 12 representative traditional VCOD models, and we also categorize and introduce these methods based on the types of features they rely on.\nTexture feature-based approaches. Malathi et al. employ multi-camera codebooks for the detection of foreground objects. In their approach, texture pixels resembling the background are extracted and quantized into distinct codebooks. These codebooks are then integrated into a weighted framework to guide the abstraction of the foreground. To enhance the accuracy of predictions, disparity maps generated from the codebooks are used as supplementary information, particularly in cases where the color contrast between the foreground and background is weak. However, there remains the potential for shadows to be misclassified as targets.\nIntensity feature-based approaches. When applied to video, these methods track dynamic changes between frames by leveraging temporal information, thereby improving the detection and localization of moving camouflaged objects. To tackle the challenge of foreground-background segmentation in video surveillance, particularly when complicated by camouflage, Guo et al. combine Bayesian classification with Gaussian mixture models and apply temporal averaging across multiple frames in video sequences to reduce the bias of background models. However, detecting subtle differences remains difficult when the foreground and background are highly similar. To address this issue, Li et al. extend COD to the wavelet domain, where subtle differences are emphasized in specific wavelet bands. They apply wavelet transforms to video sequences and estimate the probability of a wavelet coefficient belonging to the foreground by constructing foreground and background models within each individual wavelet band. This approach detects camouflaged moving foregrounds through wavelet-domain multi-scale fusion.\nColor feature-based approaches. Zhang et al. employs computer-assisted detection of camouflaged targets, which first globally models the background of the input image, with the modeling of the foreground divided into global and local models. The global model captures the overall color and texture information of the foreground, while the local model focuses on details or changes that may exist in the foreground. Based on the models of the background and foreground, a factor measuring the degree of camouflage is introduced, determining whether a pixel is camouflaged by comparing the color differences between the background and foreground at that pixel. By comparing color contrast measurement results, true camouflaged regions are identified. Finally, the camouflage and identification models are fused under a Bayesian framework to perform complete target detection.\nMotion feature-based approaches. These methods leverage the relative movement between the target object and the background between consecutive frames to identify camouflaged objects. Boult et al. propose a method for distinguishing background and foreground objects in visual surveillance systems using motion features. They introduce a conditional incremental model to update multiple background models in real-time and employed quasi-connected components to fill gaps caused by slight object movements, adapting to scene changes. However, background subtraction under camouflage conditions often results in fragmented, discontinuous object pieces, complicating subsequent classification or tracking processes. To address this, Conte et al. developed an algorithm for detecting camouflaged personnel by aggregating fragmented detection blocks to reconstruct the complete shape of the target object during post-processing. This algorithm relies on the consistent segmentation of parts of the human body, such as the head, torso, and legs, across video sequences. By defining specific parameters to model the bounding boxes of human figures, the algorithm merges two or more boxes according to a set of rules. Considering perspective effects, a semi-automatic calibration phase dynamically adjusts parameters"}, {"title": "3.2 Deep learning VCOD methods", "content": "Compared to traditional VCOD techniques, deep learning methods verify a significant advantage by automatically learning complex feature representations from large datasets. These methods have a unique ability to capture intricate and subtle patterns, which in turn enhances the understanding of object dynamics within video sequences. However, video data presents greater complexity compared to image-based approaches. This added complexity stems from factors such as higher data dimensions, temporal continuity across frames, and the constantly evolving nature of dynamic changes in video content. These factors require models to possess not only strong spatial feature extraction capabilities but also a robust mechanism for accurately capturing subtle temporal variations. Despite promising advancements, existing work has largely focused on leveraging motion cues between different frames, and these approaches remain in their early stages of development, with considerable room for growth before they reach maturity.\nIn recent years, researchers have increasingly adopted a two-step framework, where optical flow maps or pseudo masks are pre-generated to serve as motion cues for video object detection. However, due to the challenges associated with cumulative errors and weak generalization [193], there is a growing trend towards employing end-to-end universal models to enhance reliability."}, {"title": "3.2.1 Two-step VCOD framework", "content": "Since motion cues are crucial for distinguishing moving camouflaged objects from their backgrounds, researchers often incorporate a stage to pre-generate optical flow maps or pseudo masks for extracting motion information. Optical flow maps directly capture motion fields, i.e., the optical flow, between consecutive frames, providing compensation or registration to mitigate the camouflage effect. In contrast, pseudo masks implicitly learn temporal correspondences and motion patterns within the network, reducing reliance on external optical flow estimation and enabling the network to capture motion and maintain temporal consistency.\nExplicit motion-based methods. Lamdouar et al. adopt optical flow and a different image as inputs, and propose a differentiable registration module for background alignment and a motion segmentation module with memory for moving object discovery. Facing the challenge of massive human annotation, Yang et al. introduce a self-supervised method without any manual supervision, which groups motion with similar optical flow according to perceptual grouping principles. Meunier et al. utilize unsupervised CNN-based motion segmentation from optical flow, leveraging the Expectation-Maximization framework for loss design and training, as well as designing data augmentation on the optical flow field, enabling real-time segmentation without annotations or iterative motion model estimation. Xie et al. introduce object-centric layered representation and generate synthetic data for multi-object segmentation and tracking. However, reliance on external optical flow estimation can introduce errors that accumulate over time, potentially compromising the final mask prediction.\nImplicit motion-based methods. SLT-Net leverages short and long-term spatiotemporal relationships, specifically, utilizing the short-term motion capture between consecutive frames to produce pseudo masks and long-term temporal consistency to refine the former predictions, mitigating the flow estimation error. However, such implicit modeling may suffer from limited VCOD data."}, {"title": "3.2.2 End-to-end VCOD framework", "content": "This framework emerges as a solution to address the limitations of two-step approaches, particularly the cumulative errors stemming from intermediate stages and the weak generalization ability caused by limited training data. By integrating feature extraction, motion modeling, and segmentation into a unified network, this framework aims to minimize error propagation and maximize the utilization of scarce data, thereby enhancing robustness and performance. To extend the previous version which zooms in and out on images with a shared triplet feature encoder, ZoomNeXt implements image-video unified framework, and furtherly integrates multi-head scale integration and rich granularity perception, enhancing the structural representation and discrimination. IMEX unifies implicit and explicit motion learning in a cohesive framework, achieving inter-frame alignment and consistency preserving of camouflaged objects respectively. TSP-SAM and EMIP are both prompt learning-based methods for VCOD. The difference is that TSP-SAM utilizes frozen SAM embedded with temporal-spatial injection, motion-driven self-prompt learning and long-range consistency to learn reliable visual prompts, while EMIP adopts two-stream architecture, incorporating segmentation-to-motion and motion-to-segmentation prompts. Notably, although EMIP handles motion cues explicitly, it is not a two-step method, as it simultaneously conducts optical flow estimation and VCOD by interactive prompting. These methods demonstrate the evolving trend towards holistic, unified frameworks that not only address the shortcomings of traditional two-step approaches but also push the boundaries of performance and efficiency in VCOD."}, {"title": "4 OTHER CAMOUFLAGED SCENARIO TASKS", "content": "Beyond the fundamental tasks of COD and VCOD, the domain of concealed scene understanding has evolved to encompass a wider array of high-level semantic tasks. These tasks aim to provide a deeper comprehension of camouflaged objects, extending from their classification to the generation of new camouflaged images. This section delves into the following advanced tasks, each addressing unique aspects of concealed scene understanding.\nCamouflaged objects classification (COCls) focuses on distinguishing between different types of camouflaged objects. This task involves categorizing camouflaged objects into predefined or never seen classes, i.e., zero-shot and open-vocabulary learning, to handle the subtle differences and similarities among various camouflaged entities. Many COD methods integrate COCls , training with datasets that are labeled with specific categories , for better performance. Accurate classification of camouflaged objects can aid in better understanding and various ecological studies.\nCamouflaged objects localization (COL) aims to identify the most detectable regions of camouflaged objects, and further localize the discriminative regions that make the camouflaged object stand out. Lv et al. is the first to propose this task and leverage an eye tracker to record human gaze patterns, pinpointing salient discriminative regions. Simultaneously, they relabel existing datasets with fixation annotation, providing a valuable resource for training and evaluating models. This task enhances the ability to pinpoint critical areas within a camouflaged scene, which is crucial for applications in wildlife monitoring, and search and rescue operations.\nCamouflaged instance count (COCnt) is focused on quantifying the number of camouflaged objects within a given scene, even in complex environments where objects may overlap or partially obscure each other. Sun et al. introduce a correlated task, indiscernible object counting (IOC), to count objects that blend seamlessly with their surroundings. To tackle this challenge, they propose a unified framework, IOCFormer, integrating density-based and regression-based counting methods. Due to the scarcity of suitable datasets, they also created IOCfish5K, full of high-resolution images for underwater IOC, with dense annotations. This emerging and promising task helps in assessing population densities and ensuring comprehensive area surveillance.\nCamouflaged instance rank (CIR) addresses the challenge of ranking multiple camouflaged instances based on specific criteria such as visibility, detectability, or relevance. With the introduction of the COL task, Lv et al. propose the CIR task, which is based on the difficulty level of camouflage. To this end, the proposed CAM-FR and CAM-LDR datasets also include ranking labels. Furthermore, they devise a triple-task learning framework that simultaneously localizes, segments and ranks camouflaged objects, efficiently utilizing the inner correlation among COD, COL, and CIR.\nCamouflaged instance segmentation (CIS) is a more granular task that involves segmenting individual camouflaged objects, i.e., instances, from their background and from each other. CIS is proposed by Le et al. , who also introduce CIS dataset, i.e., CAMO++, by extending the previous CAMO dataset. They also conduct camouflage fusion learning, which fuses existing instance segmentation models, e.g., Cascade Mask RCNN , by learning to predict the best model per image. To better break the deceptive camouflage, Luo et al. propose a framework with a pixel-level camouflage decoupling module and an instance-level camouflage suppression module. Pei et al. introduce the first one-stage transformer in CIS by fusing local features and long-range context dependencies. Recently, Vu et al. leverage text-to-image diffusion and CLIP for CIS, while utilizing open-vocabulary capabilities to learn multi-scale textual-visual features. This level of detail allows for more precise identification and interaction with each object, which is essential for applications that require accurate object differentiation, such as advanced robotic vision, detailed ecological studies, and targeted medical imaging."}, {"title": "5 EXPERIMENTS", "content": "Datasets play a pivotal role in the development and evaluation of COD algorithms. In this subsection, we provide an overview of prominent datasets relevant to COD tasks, categorized into image-level and video-level datasets."}, {"title": "5.1.1 Characteristic COD datasets", "content": "CHAMELEON is a small-scale, unpeer-reviewed dataset consisting of 76 camouflaged images collected from the internet using the keyword \"camouflaged animals\". Each image is manually annotated with labels focusing on animals camouflaged within complex ecological backgrounds. This dataset is typically used as a test dataset for model performance evaluation.\nCAMO-COCO consists of 2,500 images across eight categories, created by merging the camouflaged dataset CAMO with the non-camouflaged dataset MS-COCO , each contributing 1,250 images. In CAMO-COCO, 80% of the images are designated for training and the remaining for testing. CAMO includes both natural camouflaged objects, such as animals, and artificial camouflaged objects, such as human beings, featuring seven challenging attributes that complicate detection and segmentation."}, {"title": "5.1.2 Characteristic VCOD Datasets", "content": "CAD2016 is composed of nine short video sequences sourced from YouTube, total having 836 frames. Each sequence is manually annotated every five frames. The camouflaged objects in these images are exclusively biological entities found in natural settings.\nMoCA is currently the largest dataset for camouflaged animal detection in video format. It consists of 141 video sequences, also sourced from YouTube, representing 67 different categories of animals found in natural scenarios. The dataset spans over 37250 frames and 26 minutes of video content. Annotations include PWC-Net optical flow data for each frame and bounding boxes with motion labels provided every five frames, with linear interpolation used for the intervening frames.\nMoCA-Mask is an extension of MoCA and includes 87 video sequences with a total of 22,939 frames, following the removal of irrelevant scenes. This dataset enhances MoCA by providing manually annotated masks every five frames, resulting in 4,691 bounding boxes and"}, {"title": "5.2 Evaluation metrics", "content": "We evaluate existing COD models using four common evaluation metrics as recommended in . These metrics, i.e., S-measure (Sa) , F-measure (FB) , Mean Absolute Error (MAE) , and E-measure (E$) , provide a comprehensive assessment of performance. Here, we detail these evaluation metrics:\nPrecision-Recall (PR) curve is generated by transforming the input into a binary mask $M$, which is segmented across a range of thresholds from 0 to 255. Precision (P) and Recall (R) are calculated by comparing the binary mask M with the ground truth mask (G) at each threshold, producing the PR curve. P and R can be calculated as\n$P = \\frac{|M(T) \\cap G|}{|M(T)|}, \\quad R = \\frac{|M(T) UG|}{|G|}$     (1)\nwhere M(T) is the mask obtained by thresholding the non-binary prediction map at threshold T.\nS-measure (Sa) quantifies the spatial structural similarity between the predicted map (C) and the ground truth (G). It combines both object-aware (S) and region-aware (Sr) assessments with the following definition:\n$S_a = \\alpha S_o + (1 - \\alpha)S_r$,       (2)\nwhere $\\alpha \\in [0,1]$ is a weighting factor, typically set at 0.5, that balances the contribution of So and Sp.\nF-measure (FB) is used to calculate the relationship between Precision (P) and Recall (R). Initially, the input is transformed into a binary mask, M, segmented over a range of thresholds from 0 to 255. P and R are calculated by comparing M with G across these thresholds. The formulas for P and R are defined as follows:\n$P = \\frac{|M(T) \\cap G|}{|M(T)|} \\quad R = \\frac{|M(T) UG|}{|G|}$     (3)\nwhere M(T) represents the binary mask obtained by thresholding the non-binary prediction map at threshold T, and || denotes the total area of the mask. But FB further demonstrates the average harmonic mean value between them. The formula for FB is defined as\n$F_{\\beta} = \\frac{(\\beta^2+1)PR}{\\beta^2P + R}$,      (4)\nwith \u03b22 typically set to 0.3. From the range of thresholds, three variants of F\u00df are computed: the maximum (Fax), the mean (Fmean) and the adaptive (Fadi). Besides, (F) is also a widely used metric where the P and R are both weighted averages. Fmean is adopted in this paper.\nE-measure (E) evaluates both the local and global similarity between C and G. It is defined as follows:\n$E = \\frac{1}{W \\times H} \\sum_{x=1}^{W} \\sum_{y=1}^{H} [\\phi(x,y), G(x, y)]$,   (5)\nwhere & represents an enhanced alignment matrix, and W and H are the width and height of the input image, respectively. E also provides three indicative values: maximum (Emax), mean (Emean), and adaptive (Eadi). Emean is adopted for evaluation in this survey.\nMean Absolute Error (M) quantifies the average absolute difference per pixel between the normalized predicted map C and the ground truth map G, where C, G\u2208 [0,1]. The mean absolute error M is formulated as follows:\n$M = \\frac{1}{HW} \\sum_{x=1}^{H} \\sum_{y=1}^{W} |C(x,y) \u2013 G(x,y)|$,    (6)\nwhere W and H denote the width and height of the input image, and (x, y) represents the pixel coordinates. Unlike FB, E and Sa, a lower M suggests a more accurate model. For VCOD, mean Dice (mDice) for similarity evaluation and mean IoU (mIoU) for overlap measurement are also used for evaluation. Notice that larger mDice and mIoU scores indicate better performance."}, {"title": "5.3 Quantitative analysis", "content": "Results of deep COD models. In this section, we conduct experiments on 39 cutting-edge techniques, covering six strategies mentioned earlier, and categorize them based on their backbones into two types: convolution-based and transformer-based. As shown in Tab.7.\nPopNet stands out as a top performer, achieving the highest Sa and FB scores across multiple datasets. Its success is attributed to its innovative integration of source-free depth estimation and object-popping techniques, followed by the precise separation of objects from their contact surfaces. Overall, methods utilizing transformer-based backbones, such as FSNet and HitNet , exhibit superior performance compared to those using convolution-based backbones. The self-attention mechanism inherent in transformers is highly effective in modeling long-range dependencies, which are crucial for accurate camouflaged object detection (COD). Furthermore, the results reveal that certain strategies are particularly effective for specific datasets. For example, methods incorporating multi-scale context information, such as C2F-Net-V2 , tend to perform better on datasets with varying object sizes, such as COD10K-test. Similarly, mechanism simulation strategies, such as LSR-V2 , are beneficial for datasets with complex backgrounds, like CAMO-test. Interestingly, Gen-SAM , despite incorporating advanced models like CLIP and BLIP2, does not consistently outperform other methods. This underscores that while powerful pre-trained models can be beneficial, their effectiveness also depends on their integration into the overall COD framework. Additionally, models designed for more challenging settings, such as UCOS-DA for UCOS and MLKG for RefCOD, do not always achieve outstanding performance. These models require more sophisticated approaches and larger datasets to generalize effectively. Consequently, some researchers are developing new datasets to train proposed models, such as R2CNet , OVCoser , and BBNet ."}, {"title": "5.4 Qualitative analysis", "content": "As depicted in Fig. 9, we present a comprehensive visual comparison of 10 cutting-edge image-level COD methods. We selected various challenging camouflaged images across 7 typical complex scenarios, including background matching, variable shape, multi-object environments, degraded scenarios, tiny objects, blurred boundaries, and severe occlusion. Additional, more challenging scenarios for COD are discussed in Section 6, and typical examples can also be seen in Fig. 10.\nIn the background-matching scenario, where insects blend seamlessly with their surroundings, we observe that SINet struggles to identify insects, often resulting in missed detections. Conversely, SegMaR demonstrates robustness by effectively outlining insect contours, even under low-contrast conditions. In the variable shape scenario, such as with the leafy sea dragon, whose appendages resemble leaves, the adaptability of models is tested. While ZoomNet misidentifies parts of these large appendages as separate objects, FPNet exhibits a superior ability to segment more complete instances, illustrating its robustness to shape variations.\nUnder multi-object configurations, where the scene is crowded with numerous camouflaged entities, the models generally succeed in locating all targets but struggle with accurate segmentation. However, PopNet performs commendably, achieving better coverage without excessive false positives. Degraded scenarios, including poor lighting or blurring, significantly impact the localization and identification processes of the models. The detection results across all models fall short of expectations, indicating considerable room for improvement in these challenging conditions.\nFor tiny objects, prediction precision degrades, with most methods failing to detect them accurately. However, ZoomNet , with its zoom-in-and-out operation, shows potential in highlighting even the smallest targets. In blurred boundary scenarios, defining precise contours becomes challenging. Here, FEDER stands out with its ODE-inspired edge reconstruction for complete edge prediction, whereas other methods often produce incomplete boundary predictions. Lastly, in severe occlusion scenarios, where two insects partially obscure each other, the results from all methods are generally acceptable. Nonetheless, BGNet displays an improved capacity through its"}, {"title": "6 FUTURE DIRECTIONS", "content": "Deep generative models for data scarcity. To mitigate the scarcity of data, leveraging deep generative models to synthesize diverse, realistic camouflaged images will bolster training effectiveness by dataset augmentation , enhancing model robustness in dealing with camouflaged scenarios. With the rise of image generation models represented by GANs and diffusion models , diverse and high-quality images can now be controlled and generated using other multimodal inputs such as text. This advancement can effectively address the longstanding issue of dataset scarcity in this field. By training with both generated camouflaged object data and the original datasets, performance can be significantly improved. Adopting an adversarial manner, where the camouflaged sample generator and the segmentation network are pitted against each other, may help unlock further potential . However, there are no quantitative metrics to evaluate the camouflage degree and sample quality in deep datasets, making the widespread use of generated samples for training a topic of debate. For VCOD datasets, the generation of such data still has a long way to go due to the current immaturity and lack of a general framework in video generation technology. Diffusion models have become a research hotspot in the field of computer vision due to their stability in algorithm"}]}