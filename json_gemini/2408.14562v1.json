{"title": "A Survey of Camouflaged Object Detection and Beyond", "authors": ["Fengyang Xiao", "Sujie Hu", "Yuqi Shen", "Chengyu Fang", "Jinfa Huang", "Chunming He", "Longxiang Tang", "Ziyun Yang", "Xiu Li"], "abstract": "Camouflaged Object Detection (COD) refers to the task of identifying and segmenting objects that blend seamlessly into their surroundings, posing a significant challenge for computer vision systems. In recent years, COD has garnered widespread attention due to its potential applications in surveillance, wildlife conservation, autonomous systems, and more. While several surveys on COD exist, they often have limitations in terms of the number and scope of papers covered, particularly regarding the rapid advancements made in the field since mid-2023. To address this void, we present the most comprehensive review of COD to date, encompassing both theoretical frameworks and practical contributions to the field. This paper explores various COD methods across four domains, including both image-level and video-level solutions, from the perspectives of traditional and deep learning approaches. We thoroughly investigate the correlations between COD and other camouflaged scenario methods, thereby laying the theoretical foundation for subsequent analyses. Furthermore, we delve into novel tasks such as referring-based COD and collaborative COD, which have not been fully addressed in previous works. Beyond object-level detection, we also summarize extended methods for instance-level tasks, including camouflaged instance segmentation, counting, and ranking. Additionally, we provide an overview of commonly used benchmarks and evaluation metrics in COD tasks, conducting a comprehensive evaluation of deep learning-based techniques in both image and video domains, considering both qualitative and quantitative performance. Finally, we discuss the limitations of current COD models and propose 9 promising directions for future research, focusing on addressing inherent challenges and exploring novel, meaningful technologies. This comprehensive examination aims to deepen the understanding of COD models and related methods in camouflaged scenarios. For those interested, a curated list of COD-related techniques, datasets, and additional resources can be found at https://github.com/ChunmingHe/awesome-concealed-object-segmentation.", "sections": [{"title": "1 INTRODUCTION", "content": "BJECT detection, a fundamental task in computer vi-sion, involves identifying and locating objects within images or videos. It comprises various fine-grained sub-fields: generic object detection (GOD) [1]\u2013[4], salient object detection (SOD) [5]\u2013[7], and camouflaged object detection (COD) [8]\u2013[10]. GOD aims to detect general objects, while SOD identifies prominent objects that stand out from the background. In contrast, COD targets those objects that blend into their surroundings, making it an extremely challenging task. Fig. 1 illustrates the relationship between the target dog and its background across GOD, SOD, and COD tasks, sourced from the classical datasets [11]-[13] of them.\nCOD has recently garnered increasing attention and rapid development for its advantages in facilitating the development of visual perception for nuance discrimination and promoting various valuable real-life applications, rang-"}, {"title": "2 IMAGE-LEVEL COD MODELS", "content": "Image-level COD refers to the process of identifying and distinguishing objects that are designed to blend into their surroundings 8 within static images, garnering sig-nificant attention currently. In this section, we categorize"}, {"title": "2.1 Traditional COD methods", "content": "COD is initially rooted in traditional image-level meth-ods, leveraging hand-crafted low-level features tailored to capture nuances in textures, intensities, and colors. These approaches constituted the foundation of early efforts in this domain. Tab. 1 summarizes the relevant models and characteristics of these methods.\nTexture feature-based approaches. Texture features capture the surface properties and distinctive patterns of images, usually manifested through grayscale distributions across pixels and their spatial neighborhoods. These features are utilized to distinguish camouflaged objects from their backgrounds based on texture differences.\nGalun et al. [22] introduce a bottom-up approach, TSMA, for texture segmentation. This method leverages the adap-tive identification and characterization of texture elements-such as size, aspect ratio, orientation, and brightness-combined with filter response statistics for enhanced differ-entiation and noise reduction. Unlike TSMA, which extracts global texture features, Bhajantri et al. [59] employed a co-occurrence matrix for block-based local texture analysis. Their method uses Watershed segmentation for each defec-tive block, processed through a dendrogram to distinguish the target, and concludes with cluster analysis to further confirm detection results.\nBuilding upon earlier works, Sengottuvelan et al. [60] propose an unsupervised technique for decamouflaging that converts images to grayscale and then splits them into blocks for gray-level co-occurrence matrix analysis, captur-ing pixel-neighbor relationships. They use a dendrogram plot to identify camouflaged objects without requiring prior background knowledge. Traditional evaluation methods of-ten rely on subjective assessments, which can be cumber-some and ambiguous processes. To address it, Song et al. [62] leverage a weighted structural similarity index and intrinsic image feature analysis to assess camouflage textures. To be more objective, Feng et al. [66] utilize human visual-based saliency maps to quantitatively assess texture differences.\nIntensity feature-based approaches. These methods pro-gressively advance from basic intensity-based techniques to more sophisticated exploitation of 3D convexity. CB-DCE [23] detected regions of interest by processing inten-sity images directly, distinguishing between 3D convex and concave regions. This approach demonstrates robustness against variations in illumination, orientation, and scale. To enhance the detection of 3D objects camouflaged in complex scenes, CVCB [57] enhanced the D-Arg operator in CBDCE. It maximizes the response to curved 3D objects against flat backgrounds, effectively mitigating visual camouflage in both natural and artificial environments. CTDDC [65] employed another 3D convexity-based operator that ex-ploits image gray levels and median filtering to eliminate background noise, enabling effective and robust detection.\nColor feature-based approaches. In certain scenarios, color contrast and distribution can provide significant distinc-tiveness for separating camouflaged objects from their sur-roundings. Siricharoen et al. [63] optimized a statistical background subtraction and shadow detection algorithm by integrating color, edge, and intensity features for out-door human segmentation, where strong shadows and low contrast are common. They employ vector median filtering to remove outlier pixels and combine color statistics with edge to generate initial coarse results, which are then refined using intensity features for accuracy. Kavitha et al. [64] propose an image retrieval technique for COD, which involves segmenting images into blocks and extracting Hue, Satura-tion, Value (HSV) color, and gray-level co-occurrence matrix texture features from each block. They use the principle of matching images based on similarity and Euclidean distance to improve detection accuracy."}, {"title": "2.2 Deep learning COD methods", "content": "While traditional methods rely on hand-crafted low-level features to capture key attributes of an image, deep learning methods extract complex and deep features directly from the data through automatic learning representations, demonstrating superior performance across various com-puter vision tasks. According to the existing surveys [55], [56], deep learning methods for COD can be broadly catego-rized based on three fundamental criteria: network architec-ture, learning paradigm, and supervision level. A detailed description of the three criteria is shown in Fig. 3. What's more, Tab. 2 and Tab. 3 outline the key characteristics of a total of 104 representative methods for image-level COD, published in 2019-2022 and 2023&2024, respectively.\nNetwork architecture delineates how the input and out-put configurations are structured within the models. The lin-ear [8], [13], [28], [29], [31], [32], [35], [36], [52], [68]\u2013[108] em-ploys bottom-up/top-down network, where the data flows in a single feed-forward pass. Aggregative architecture [15], [37], [38], [48], [49], [109]\u2013[121] combines features from multiple input streams, while branched architecture [9], [10], [33], [41]\u2013[44], [47], [53], [122]\u2013[148] is characterized by multiple pathways for multiple outputs, associated with the multi-task learning paradigm. The hybrid [20], [40], [45], [46], [50], [51], [149]\u2013[155] integrates elements and strategies from the aforementioned ones to leverage their strengths.\nThe learning paradigm pertains to the approach adopted by the models to learn and adapt. This includes single-task and multi-task learning, specifically the former only involv-ing COD, while the latter often importing auxiliary tasks, e.g., localization/ranking [33], [144], reconstruction [33], [144] and predicting associated cues, such as boundary [10], [20], [42], [44], [46], [53], [106], [123], [127], [129], [130], [132], [134], [136], [138]\u2013[141], [145], [146], [150], texture [106], [126], [127], [143] and uncertainty [9], [43], [45], [122], [125], [128], [131], [140], [152], leading to improved accuracy and performance.\nThe supervision level describes the degree and nature of supervision provided during the training phase. This can be classified into four categories: fully supervised, with complete ground truth data; weakly-supervised, which utilizes limited or imprecise annotations [29], [102]; semi-supervised, which combines labeled and unlabeled data, generating pseudo labels for the latter [145]; and unsuper-vised, where no explicit labels are provided [52]. Notably, the \"train-free\" mode [110], [121], [160] refers to models that"}, {"title": "3 VIDEO-LEVEL COD MODELS", "content": "Unlike image-level Camouflaged Object Detection (COD) techniques, which focus on single static images, video-level COD requires greater emphasis on motion cues to identify and localize camouflaged objects within contin-uous video frames. Video COD (VCOD) typically lever-ages temporal information, such as motion and changes across frames, to reveal objects that are difficult to detect in individual frames. However, this task presents significant"}, {"title": "3.1 Traditional VCOD methods", "content": "Compared to image-level methods, the video-level ones include more techniques, like optical flow analysis and mo-tion detection, making them practical in video surveillance and security. As illustrated in Tab. 4, this section will delve into 12 representative traditional VCOD models, and we also categorize and introduce these methods based on the types of features they rely on.\nTexture feature-based approaches. Malathi et al. [178] em-ploy multi-camera codebooks for the detection of fore-ground objects. In their approach, texture pixels resembling the background are extracted and quantized into distinct codebooks. These codebooks are then integrated into a weighted framework to guide the abstraction of the fore-ground. To enhance the accuracy of predictions, disparity maps generated from the codebooks are used as supple-mentary information, particularly in cases where the color contrast between the foreground and background is weak. However, there remains the potential for shadows to be misclassified as targets.\nIntensity feature-based approaches. When applied to video, these methods track dynamic changes between frames by leveraging temporal information, thereby im-proving the detection and localization of moving cam-ouflaged objects. To tackle the challenge of foreground-background segmentation in video surveillance, particularly when complicated by camouflage, Guo et al. [175] combine Bayesian classification with Gaussian mixture models and apply temporal averaging across multiple frames in video sequences to reduce the bias of background models. How-ever, detecting subtle differences remains difficult when the foreground and background are highly similar. To address this issue, Li et al. [21] extend COD to the wavelet domain, where subtle differences are emphasized in specific wavelet bands. They apply wavelet transforms to video sequences and estimate the probability of a wavelet coefficient be-longing to the foreground by constructing foreground and background models within each individual wavelet band. This approach detects camouflaged moving foregrounds through wavelet-domain multi-scale fusion.\nColor feature-based approaches. Zhang et al. [196] employs computer-assisted detection of camouflaged targets, which first globally models the background of the input image, with the modeling of the foreground divided into global and local models. The global model captures the overall color and texture information of the foreground, while the local model focuses on details or changes that may exist in the foreground. Based on the models of the background and foreground, a factor measuring the degree of camouflage is introduced, determining whether a pixel is camouflaged by comparing the color differences between the background and foreground at that pixel. By comparing color contrast measurement results, true camouflaged regions are identi-fied. Finally, the camouflage and identification models are fused under a Bayesian framework to perform complete target detection.\nMotion feature-based approaches. These methods leverage the relative movement between the target object and the background between consecutive frames to identify cam-ouflaged objects. Boult et al. [25] propose a method for distinguishing background and foreground objects in visual surveillance systems using motion features. They introduce a conditional incremental model to update multiple back-ground models in real-time and employed quasi-connected components to fill gaps caused by slight object move-ments, adapting to scene changes. However, background subtraction under camouflage conditions often results in fragmented, discontinuous object pieces, complicating sub-sequent classification or tracking processes. To address this, Conte et al. [176] developed an algorithm for detecting cam-ouflaged personnel by aggregating fragmented detection blocks to reconstruct the complete shape of the target object during post-processing. This algorithm relies on the consis-tent segmentation of parts of the human body, such as the head, torso, and legs, across video sequences. By defining specific parameters to model the bounding boxes of human figures, the algorithm merges two or more boxes according to a set of rules. Considering perspective effects, a semi-automatic calibration phase dynamically adjusts parameters"}, {"title": "3.2 Deep learning VCOD methods", "content": "Compared to traditional VCOD techniques, deep learn-ing methods verify a significant advantage by automati-cally learning complex feature representations from large datasets. These methods have a unique ability to capture intricate and subtle patterns, which in turn enhances the understanding of object dynamics within video sequences. However, video data presents greater complexity compared to image-based approaches. This added complexity stems from factors such as higher data dimensions, temporal con-tinuity across frames, and the constantly evolving nature of dynamic changes in video content. These factors require models to possess not only strong spatial feature extraction capabilities but also a robust mechanism for accurately capturing subtle temporal variations. Despite promising advancements, existing work has largely focused on lever-aging motion cues between different frames, and these ap-proaches remain in their early stages of development, with considerable room for growth before they reach maturity.\nIn recent years, researchers have increasingly adopted a two-step framework, where optical flow maps or pseudo masks are pre-generated to serve as motion cues for video object detection. However, due to the challenges associated with cumulative errors and weak generalization [193], there is a growing trend towards employing end-to-end universal models to enhance reliability. The key characteristics of a total of 16 representative methods for VCOD are detailed in Tab. 5. We categorize these methods based on various criteria, e.g., network architecture, the use of optical flow maps, supervision level, and synthetic dataset generation. Besides, some methods provide links to their open-source projects."}, {"title": "3.2.1 Two-step VCOD framework", "content": "Since motion cues are crucial for distinguishing moving camouflaged objects from their backgrounds, researchers often incorporate a stage to pre-generate optical flow maps or pseudo masks for extracting motion information. Optical flow maps directly capture motion fields, i.e., the optical flow, between consecutive frames, providing compensation or registration to mitigate the camouflage effect. In contrast, pseudo masks implicitly learn temporal correspondences and motion patterns within the network, reducing reliance on external optical flow estimation and enabling the net-work to capture motion and maintain temporal consistency.\nExplicit motion-based methods. Lamdouar et al. [182] adopt optical flow and a different image as inputs, and propose a differentiable registration module for background alignment and a motion segmentation module with mem-ory for moving object discovery. Facing the challenge of massive human annotation, Yang et al. [183] introduce a self-supervised method without any manual supervision, which groups motion with similar optical flow according to perceptual grouping principles. Meunier et al. [186] utilize unsupervised CNN-based motion segmentation from op-tical flow, leveraging the Expectation-Maximization frame-work for loss design and training, as well as designing data augmentation on the optical flow field, enabling real-time segmentation without annotations or iterative motion model estimation. Xie et al. [188] introduce object-centric layered representation and generate synthetic data for multi-object segmentation and tracking. However, reliance on external optical flow estimation can introduce errors that accumulate over time, potentially compromising the final mask predic-tion.\nImplicit motion-based methods. SLT-Net [185] leverages short and long-term spatiotemporal relationships, specifi-cally, utilizing the short-term motion capture between con-secutive frames to produce pseudo masks and long-term temporal consistency to refine the former predictions, mit-igating the flow estimation error. However, such implicit modeling may suffer from limited VCOD data."}, {"title": "3.2.2 End-to-end VCOD framework", "content": "This framework emerges as a solution to address the limitations of two-step approaches, particularly the cumu-lative errors stemming from intermediate stages and the weak generalization ability caused by limited training data. By integrating feature extraction, motion modeling, and segmentation into a unified network, this framework aims to minimize error propagation and maximize the utilization of scarce data, thereby enhancing robustness and perfor-mance. To extend the previous version [37] which zooms in and out on images with a shared triplet feature encoder, ZoomNeXt [114] implements image-video unified frame-work, and furtherly integrates multi-head scale integration and rich granularity perception, enhancing the structural representation and discrimination. IMEX [192] unifies im-plicit and explicit motion learning in a cohesive framework, achieving inter-frame alignment and consistency preserving of camouflaged objects respectively. TSP-SAM [193] and EMIP [195] are both prompt learning-based methods for VCOD. The difference is that TSP-SAM utilizes frozen SAM embedded with temporal-spatial injection, motion-driven self-prompt learning and long-range consistency to learn re-liable visual prompts, while EMIP adopts two-stream archi-tecture, incorporating segmentation-to-motion and motion-to-segmentation prompts. Notably, although EMIP handles motion cues explicitly, it is not a two-step method, as it simultaneously conducts optical flow estimation and VCOD by interactive prompting. These methods demonstrate the evolving trend towards holistic, unified frameworks that not only address the shortcomings of traditional two-step approaches but also push the boundaries of performance and efficiency in VCOD."}, {"title": "4 OTHER CAMOUFLAGED SCENARIO TASKS", "content": "Beyond the fundamental tasks of COD and VCOD, the domain of concealed scene understanding has evolved to encompass a wider array of high-level semantic tasks. These tasks aim to provide a deeper comprehension of camouflaged objects, extending from their classification to the generation of new camouflaged images. This section delves into the following advanced tasks, each addressing unique aspects of concealed scene understanding. The detail descriptions are illustrated in Fig. 6.\nCamouflaged objects classification (COCls) focuses on dis-tinguishing between different types of camouflaged objects. This task involves categorizing camouflaged objects into predefined or never seen classes, i.e., zero-shot [135] and"}, {"title": "5 EXPERIMENTS", "content": "Datasets play a pivotal role in the development and evaluation of COD algorithms. In this subsection, we pro-vide an overview of prominent datasets relevant to COD tasks, categorized into image-level and video-level datasets. Tab. 6 summarizes essential information along with their respective links for access. Additionally, Fig. 7 and Fig. 8 showcases exemplar images from these datasets, offering a visual insight into the challenges posed by COD."}, {"title": "5.1.1 Characteristic COD datasets", "content": "CHAMELEON [207] is a small-scale, unpeer-reviewed dataset consisting of 76 camouflaged images collected from the internet using the keyword \"camouflaged animals\". Each image is manually annotated with labels focusing on animals camouflaged within complex ecological back-grounds. This dataset is typically used as a test dataset for model performance evaluation.\nCAMO-COCO [41] consists of 2,500 images across eight categories, created by merging the camouflaged dataset CAMO with the non-camouflaged dataset MS-COCO [11], each contributing 1,250 images. In CAMO-COCO, 80% of the images are designated for training and the remaining for testing. CAMO includes both natural camouflaged objects, such as animals, and artificial camouflaged objects, such as human beings, featuring seven challenging attributes that complicate detection and segmentation."}, {"title": "4.1 Mitigating current issues", "content": "Deep generative models for data scarcity. To mitigate the scarcity of data, leveraging deep generative models to synthesize diverse, realistic camouflaged images will bol-ster training effectiveness by dataset augmentation [218], enhancing model robustness in dealing with camouflaged scenarios. With the rise of image generation models repre-sented by GANs [173] and diffusion models [205], diverse and high-quality images can now be controlled and gen-erated using other multimodal inputs such as text. This advancement can effectively address the longstanding issue of dataset scarcity in this field. By training with both gen-erated camouflaged object data and the original datasets, performance can be significantly improved. Adopting an adversarial manner, where the camouflaged sample genera-tor and the segmentation network are pitted against each other, may help unlock further potential [20]. However, there are no quantitative metrics to evaluate the camouflage degree and sample quality in deep datasets, making the widespread use of generated samples for training a topic of debate. For VCOD datasets, the generation of such data still has a long way to go due to the current immaturity and lack of a general framework in video generation technology. Diffusion models have become a research hotspot in the field of computer vision due to their stability in algorithm training and high-quality sample generation. CamDiff [219] innovates by synthesizing salient objects within camouflage scenes, mitigating the scarcity of multi-pattern training data and enhancing robustness to salient misclassifications. The authors also use CamDiff to propose Diff-COD dataset from the original COD datasets [13], [33], [41], [207] to enhance the robustness to saliency. Meanwhile, LAKE-RED [220] tackles the limitations of dataset diversity and expensive data collection by automatically generating camouflage im-ages without manual background specification, fostering scalability and interpretability. Both approaches emphasize the potential of diffusion models to address data-scarce vision tasks.\nTackling complex scenarios & challenging samples. Ad-dressing the intricacies of COD requires grappling with var-ious challenges posed by complex scenes and difficult sam-ples. Key issues include extremely complex backgrounds and advanced camouflage techniques that render objects nearly invisible. Fig. 10 depicts some extremely concealed scenarios with challenging samples. Multi-object, multi-scale scenarios present difficulties in capturing objects of varying sizes and scales, while severely occluded objects demand robust algorithms capable of disentangling over-lapping structures. Objects with fuzzy appearances, partic-ularly small ones, challenge the limits of detection frame-works due to their minimal visual cues. Disruptive patterns, transparency, and background matching further complicate detection by seamlessly blending objects into their sur-roundings. Blurred boundaries and structural ambiguity add to the difficulty of delineating object edges, while vari-able shapes require flexible recognition capabilities. More-over, detecting extremely rare camouflaged objects necessi-tates specialized methods to distinguish them from the vast majority of non-camouflaged instances. Objects concealed in darkness, affected by drastic illumination variations, pose unique challenges that require innovative approaches to handle varying light conditions. Overcoming these obstacles"}, {"title": "6.2 Exploring expansive potentials", "content": "Embracing Novel Tasks. CoCOD [51], a collaborative ap-proach to COD, holds the potential to significantly en-hance performance by leveraging multi-source data and cross-modal interactions. However, challenges persist in efficiently fusing heterogeneous information and ensuring ro-bust performance across diverse scenarios. Another promis-ing avenue, RefCOD [48], [49], demands advancements in techniques for multi-modal alignment and the comprehen-sion of complex textual or visual references, particularly for rare or ambiguous species. The primary difficulties in-clude bridging cross-modal gaps and extracting fine-grained visual cues that are relevant to the provided textual or visual descriptions. Beyond CoCOD and RefCOD, there are numerous unexplored opportunities for novel tasks that could further empower COD. For instance, Interactive COD (I-COD) could incorporate user feedback during de-tection, enabling iterative refinement and personalization. This would require the development of robust interaction mechanisms and ensuring the system's responsiveness to user inputs [223]. By continually exploring and innovat-ing in these novel tasks, we can substantially enhance the capabilities and applicability of COD, thereby pushing the boundaries of what is possible in this exciting field.\nDifferentialting SOD and COD. At the feature level, delv-ing into the nuances between SOD and COD is crucial, as both, though under the umbrella of abnormal segmenta-tion, target distinct abnormalities [224], [225]. Current COD techniques struggle with salient objects [10], [20], as they misinterpret prominence as camouflage, thereby necessi-tating robustness enhancement. Transferring salient objects to concealed scenarios, or vice versa, could alleviate data scarcity and boost model generalization. The partial positive correlation between SOD and COD highlights opportuni-ties for conversion strategies, increasing sample diversity. Integrating generative adversarial mechanisms between the two tasks fosters innovation. Ultimately, a multi-task uni-fied learning framework that encapsulates domain self-generalization for saliency and camouflage detection within the broader AI landscape represents a compelling frontier.\nMultimodal information fusion. Integrating multiple modalities, such as text [40], audio, video [114], optical"}, {"title": "7 CONCLUSION", "content": "We present a comprehensive and exhaustive overview of the rapidly evolving field of COD, encompassing both traditional and deep learning methods in image and video domains. By reviewing approximately 150 relevant COD studies, we offer the most extensive and detailed survey to date, providing a concise yet thorough perspective for both newcomers and established scholars. Additionally, we provide both quantitative and qualitative benchmarks for representative image and video models across 6 character-istic datasets and 6 evaluation metrics. Through rigorous benchmarking, we have identified key limitations and chal-lenges in current COD methods, thereby paving the way for future research directions. We hope this survey will inspire innovative solutions that push the boundaries of COD technology. Additionally, we have established a dedi-cated GitHub repository to house COD techniques, datasets, and resources, ensuring that the latest developments and insights are readily accessible to the research community for further exploration."}, {"title": "Evaluation metrics", "content": "We evaluate existing COD models using four common evaluation metrics as recommended in [13]. These metrics, i.e., S-measure (Sa) [212], F-measure (FB) [213], Mean Abso-lute Error (MAE) [214], and E-measure (E$) [215], provide a comprehensive assessment of performance. Here, we detail these evaluation metrics:\nPrecision-Recall (PR) curve is generated by transforming the input into a binary mask M, which is segmented across a range of thresholds from 0 to 255. Precision (P) and Recall (R) are calculated by comparing the binary mask M with the ground truth mask (G) at each threshold, producing the PR curve. P and R can be calculated as\n$P = \\frac{|M(T) \\cap G|}{|M(T)|}$,\n$R = \\frac{|M(T) \\cup G|}{|G|}$    (1)\nwhere M(T) is the mask obtained by thresholding the non-binary prediction map at threshold T.\nS-measure (Sa) quantifies the spatial structural similarity between the predicted map (C) and the ground truth (G). It combines both object-aware (So) and region-aware (Sr) assessments with the following definition:\n$S_a = \\alpha S_o + (1-\\alpha)S_r, $(2)\nwhere $\\alpha \\in [0,1]$ is a weighting factor, typically set at 0.5, that balances the contribution of So and Sr.\nF-measure (FB) is used to calculate the relationship between Precision (P) and Recall (R). Initially, the input is trans-formed into a binary mask, M, segmented over a range of thresholds from 0 to 255. P and R are calculated by comparing M with G across these thresholds. The formulas for P and R are defined as follows:\n$P = \\frac{|M(T) \\cap G|}{|M(T)|}$,\n$R = \\frac{|M(T) \\cup G|}{|G|}$ (3)\nwhere M(T) represents the binary mask obtained by thresh-olding the non-binary prediction map at threshold T, and || denotes the total area of the mask. But FB further demonstrates the average harmonic mean value between them. The formula for FB is defined as\n$F_B = \\frac{(\\beta^2+1)PR}{\\beta^2P + R}, $(4)\nwith $\\beta^2$ typically set to 0.3. From the range of thresholds, three variants of F\u00df are computed: the maximum (Fmax), the mean (Fmean) and the adaptive (Fadi). Besides, (F) is\nalso a widely used metric where the P and R are both weighted averages. Fmean is adopted in this paper.\nE-measure (E) evaluates both the local and global similar-ity between C and G. It is defined as follows:\n$E = \\frac{1}{W \\times H}\\sum_{x=1}^{W}\\sum_{y=1}^{H} \\Phi[C(x,y), G(x, y)], $(5)\nwhere \u03a6 represents an enhanced alignment matrix, and W and H are the width and height of the input image, respec-tively. E also provides three indicative values: maximum (Emax), mean (Emean), and adaptive (Eadi). Emean is adopted for evaluation in this survey.\nMean Absolute Error (M) quantifies the average absolute difference per pixel between the normalized predicted map C and the ground truth map G, where C, G\u2208 [0,1]. The mean absolute error M is formulated as follows:\n$M = \\frac{1}{H \\times W}\\sum_{x=1}^{H}\\sum_{y=1}^{W} |C(x,y) \u2013 G(x,y)|, $(6)\nwhere W and H denote the width and height of the input image, and (x, y) represents the pixel coordinates. Unlike FB, E and Sa, a lower M suggests a more accurate model. For VCOD, mean Dice (mDice) [20] for similarity eval-uation and mean IoU (mIoU) for overlap measurement are also used for evaluation. Notice that larger mDice and mIoU scores indicate better performance."}]}