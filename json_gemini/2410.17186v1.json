{"title": "DyPNIPP: Predicting Environment Dynamics for RL-based Robust Informative Path Planning", "authors": ["Srujan Deolasee", "Siva Kailas", "Wenhao Luo", "Katia Sycara", "Woojun Kim"], "abstract": "Informative path planning (IPP) is an important planning paradigm for various real-world robotic applications such as environment monitoring. IPP involves planning a path that can learn an accurate belief of the quantity of interest, while adhering to planning constraints. Traditional IPP methods typically require high computation time during execution, giving rise to reinforcement learning (RL) based IPP methods. However, the existing RL-based methods do not consider spatio-temporal environments which involve their own challenges due to variations in environment characteristics. In this paper, we propose DyPNIPP, a robust RL-based IPP frame-work, designed to operate effectively across spatio-temporal environments with varying dynamics. To achieve this, DYPNIPP incorporates domain randomization to train the agent across diverse environments and introduces a dynamics prediction model to capture and adapt the agent actions to specific environment dynamics. Our extensive experiments in a wildfire environment demonstrate that DyPNIPP outperforms existing RL-based IPP algorithms by significantly improving robustness and performing across diverse environment conditions.", "sections": [{"title": "I. INTRODUCTION", "content": "Informative path planning (IPP) has been actively studied for robotics deployments involving information acquisition, such as autonomous exploration of unknown areas [1], [2], environment monitoring [3], and target tracking [4]. IPP aims to find a path for autonomous robots that maximizes acquisition of interests (e.g., intensity of fire in fire moni-toring) while adhering to resource constraints. Conventional IPP methods typically involve sampling-based path planning using a graph for spatial environments [3], [5], [6]. Re-cently, IPP solvers for spatiotemporal environments, where the interest changes over time, have also been proposed [7], [8]. Despite their effectiveness, these methods require heavy computation time to determine the path, limiting their appli-cability for real-world deployment.\nWith the recent success of RL in various domains, RL-based IPP has been actively studied, demonstrating both superior performance and reduced computation time [10], [11], [12]. However, none of the prior works based on RL have considered spatiotemporal environments. Learning in such environments faces an inherent problem of RL: a lack of robustness against variations in environment dynamics [13]. That is, the agent demonstrates optimal performance only in the environment it was trained on, but not when environment dynamics vary. In addition, even if the agent is trained in an environment with various dynamics, it exhibits suboptimal performance across different environment variations due to over-regularized policies [14]. To understand such problems regarding robustness, let us consider a wildfire domain, where the interest is the fire that spreads over time. Here, the fire growth rate is determined by several characteristics, including fuel. Fig. 1 shows two environments with different fuel distributions: $F_c$ = 1 (first row) and $F_c$ = 10 (second row). These environments have varying environment dynam-ics due to difference in fuels (1: slow, 10: fast). Consequently, the agent trained in the $F_c$ = 1 environment has suboptimal performance in the $F_c$ = 10 environment, and vice versa. The corresponding result in terms of the RMSE error is shown in Table I. Specifically, CAtNIPP\u00b9 [10] trained on $F_c$ = 10 performs worse than that trained on $F_c$ = 1 in"}, {"title": "II. BACKGROUND AND RELATED WORKS", "content": "RL aims to train an agent by interacting with an en-vironment, and the decision making procedure is typically formulated as a Markov decision process (MDP) defined as the tuple < S, A, P, r, \u03b3 >, where S is the state space, A is the action space, P is the transition probability (dynamics), r is the reward function, and \u03b3\u2208 [0,1) is the discount factor. At each time step t, the agent generates an action $a_t \u2208 A$ from a policy \u03c0 : S \u00d7 A \u2192 [0,1] based on the given state $s_t \u2208 S$. The environment then yields the reward $r_t = r(s_t, a_t)$ and the next state $s_{t+1} ~ P(S_{t+1}|S_t, a_t)$. The goal of RL is to learn a policy that maximizes the expected discounted return, \u0395 [\u03a3\u03c4\u03bf $\u03b3^tr_t$]. One representative algo-rithm is proximal policy optimization (PPO). PPO consists of an actor and a value function (critic), which estimates expected returns. The agent collects experience, and the value function estimates advantages, indicating how much better or worse an action is compared to the current policy expectation. The actor is then updated to maximize these advantages. PPO uses a clipping method to limit how much the new policy can deviate from the previous one, ensuring stable and efficient learning.\nRL has been successfully applied to various robotics tasks, including manipulation [15], locomotion [16], and path planning [10] due to its decision-making capabilities. Our work focuses on informative path planning by leveraging the capabilities of RL and addressing its weaknesses in robustness, to achieve a solution that is both effective and robust."}, {"title": "III. METHODOLOGY", "content": "As described in Sec. II-A, the standard RL framework assumes a fixed, stationary transition probability, $p(s'|s, a)$, which captures the environment dynamics. Thus, the optimal RL policy in an environment with a specific transition prob-ability may be suboptimal in an environment with a different transition probability. In order to formalize this, we consider the distribution of MDPs, where the transition probability $p(s'|s, a, c)$ is conditioned on environment characteristics c. Here, the environment characteristics depend on the domain; for example, in the wildfire domain, c comprises several com-ponents that affect fire dynamics, including fuel/vegetation, and wind velocity.\nThe existing RL-based IPP algorithms consider the same environment dynamics for both training and testing, using a fixed value for c in both phases [10], [12]. This, in practice, limits the robustness of the trained RL policy in the face of varying environment dynamics. As we will discuss in Sec. IV-C.1, an RL policy trained on specific environment characteristics performs well only in the same environment but poorly in the environments with different characteristics. Since the agent can encounter various environments with different characteristics, it should be capable of handling these variations."}, {"title": "1) Domain Randomization", "content": "In order to allow the RL agent to encounter a diverse range of environment characteristics, we adopt domain randomization, which involves randomiz-ing the environment dynamics by sampling characteristics from a specified range. This can be interpreted as con-structing the environment dynamics by marginalizing the dynamics over the characteristics, written as $p(s'|s,a) = \\mathbb{E}_{c} [p(s'| s, a, c)]$. That is, we sample a characteristic from a prior distribution of characteristics (e.g., a uniform distri-bution Uniform[1,10] for the fuel/vegetation in the wildfire domain) for each episode and then train the RL policy. As we show in Sec. IV-C.1, this domain randomization alone is not enough to find the optimal policy across the characteristics. This is because such a policy is optimal for an environment with marginalized transitions, $\\mathbb{E}_{c} [p(s'|s, a, c)]$, implying it is optimal only for the averaged environment, and not for an individual environment with specific characteristics."}, {"title": "2) Dynamics Prediction Model", "content": "To address the afore-mentioned problem, we introduce a dynamics prediction model (DPM) capable of extracting features of environment dynamics. The proposed DPM uses an encoder consisting of convolutional layers and a LSTM layer, and a decoder involving a dense layer and transposed convolutional layers. The encoder and the decoder parameterized by $\u03a6_{enc}$ and $\u03a6_{dec}$ are given by $f(b(o_t), h_t; \u03a6_{enc})$ and $g(z_t; \u03a6_{dec})$ respectively. Here, $b(o_t)$ and $h_t$ are the belief of the observation and the hidden vector of the LSTM at time step t, respectively. $z_t$ is the output of the encoder referred to as the latent vector. We design the DPM to predict the belief of the next observation based on the belief of the current observation, which is given by GP regression. The corresponding loss function $\\mathcal{L}_{DPM}$ is written as:\n$\\mathcal{L}_{DPM}(\u03a6_{enc}, \u03a6_{dec}) = \\mathbb{E} [||Y_t - g(z_t; \u03a6_{dec})||^2] (2)$\nwhere $z_t = f(b(o_t), h_t; \u03a6_{enc})$ and $Y_t = b(o_{t+1})$. This learning process allows the hidden features of the DPM to represent the environment dynamics. We use the output of the DPM encoder, $z_t$, as an environment-context feature ($z_t$ is 16-dimensional feature)."}, {"title": "3) RL policy", "content": "Based on the two proposed components, we train an RL policy that selects the next waypoint to move to. Note that DPM can be combined with any RL-based IPP policy. In this work, we use CAtNIPP, which is described in Sec. II-C. The key difference is that the environment-context feature of DPM is injected into the RL policy, which is consequently represented as \u03c0($a_t|s_t, z_t$). The detailed architecture of the RL policy to generate action is as follows: The RL policy consists of an attention-based encoder and a attention and LSTM-based [38] decoder module. The encoder takes the belief-augmented graph as input to generate a spatially aware embedding for all nodes of the graph. An additional node-embedding is generated using the planning state, which comprises of the remaining budget, executed trajectory, and the augmented graph. We use our environment-context feature $z_t$ as an additional input with these current node-embeddings and pass them to the LSTM block. A decoder is then used to select the most informative neighbor of the robot as an action. The action is selected through a cross-attention network. This attention mechanism computes attention weights by comparing the current node's LSTM features with the neighboring nodes' features. These weights reflect the relevance of each neighbor in the context of the current planning state, and also act as the policy. Since this process uses the budget and the executed trajectory, along with all the spatial and environmental embeddings, the policy is able to suggest actions which are cognizant of the robot's current state, the environment dynamics, and the remaining operational budget. This ensures that the selected actions are informative, spatially-aware, also feasible within the constraints. To train this policy, we use PPO [39], following prior work [10], [12]."}, {"title": "IV. EXPERIMENTS", "content": "We evaluate the proposed framework on a wildfire domain given by the Fire Area Simulator (FARSITE) [40] model. We leverage the FireCommander [9] simulator to access the model for generating the spatio-temporal environment. The fire's growth rate (i.e. fire propagation velocity) is a function of fuel and vegetation coefficient ($F_c$), wind speed ($U_c$), and wind azimuth ($\u03b8_c$). The first-order firespot dynamics $q_t$ are estimated for each propagating spot $q_t$, where q is a 2-dimensional vector representing the X-Y coordinates, by $q_t = C(F_c, U_c)D(\u03b8_c)$, where $D(\u03b8_c) = [sin(\u03b8_c), cos(\u03b8_c)]$. Here, $C(F_c, U_c)$ can be calculated as:\n$C(F_c, U_c) = F_c (1 - L_B(U_c)/(L_B(U_c) + \\sqrt{G_B(U_c)}))$,\nwhere $L_B(U_c) = 0.936e^{0.256U_c} + 0.461e^{-0.154U_c}$ and $G_B(U_c) = L_B(U_c)^2 - 1$. Note that the fuel and vegeta-tion coefficient $F_c$ is the most dominant factor affecting the fire spread. To evaluate the algorithms in terms of robustness, we consider environments with varying the fuel/vegetation coefficient $F_c$ and the number of fire origins within the environment.\nIn the FireCommander simulator, both higher $F_c$ and $U_c$ result in a higher fire spread rate ($F_c, U_c > 0$). Since a higher $F_c$ implies highly flammable fuel, it also means that the fuel can get exhausted more quickly, resulting in a quicker decay of fire. Although theoretically both coefficients can be any positive numbers, 10 is given as the recommended limit for both. For all our experiments, we fix the wind speed $U_c$ (weaker factor) at 5 and vary the fuel and vegetation coef-ficient $F_c$ (dominant factor) to train and test for robustness. The wind direction is chosen randomly for each episode. We use domain randomization to make our policy robust to changes in these parameters which impact the fire spread dynamics. While training our policy, we randomly choose a $F_c$ in between [1,10] for every episode (unless otherwise stated). Based on these sampled environment parameters, we use FireCommander to generate the spatio-temporal wildfire environment and normalize the field dimensions to construct the true interest map of a unit square [0, 1]2 size. Since there are no initial observations of the environment, the robot's belief starts as a uniform distribution GP(0, P\u00ba), P\u00ba = 1. The start and destination positions are randomly generated in [0, 1]2. We train our policy with a fixed number (200) of nodes for our graph, where the number of neighboring nodes is fixed to k = 20, and the budget is randomized between [7, 9]. Note that the graph is reinitialized for each episode. A measurement is obtained every time the robot has traveled 0.2 units from the previous measurement. We set the max episode length to 256 time steps, and the batch size to 32. We use the Adam optimizer with learning rate 10-4, which decays every 32 steps by a factor of 0.96.\nDyPNIPP trains a RL agent on top of CAtNIPP [10] utilizing PPO [39] for the training. For each training episode, PPO runs 8 iterations. Our model is trained on a workstation using AMD EPYC 7713 CPU, and a single NVIDIA RTX 6000 Ada Gen GPU. We use Ray [41] to distribute the training process and run 16 IPP instances in parallel, thus requiring just around 2 hours to converge."}, {"title": "C. Experimental Results", "content": "In this subsection, we examine (1) whether DyPNIPP enhances the robustness against environment variations, in-cluding the fuel coefficient $F_c$ and the number of fires, (2) the effectiveness of our DPM design, and (3) how the environment-context latent feature varies with respect to the dynamics.\n\u2022 The prior RL-based IPP algorithm lacks robustness: CATNIPP trained on a specific $F_c$ exhibits suboptimal per-formance in environments with a different $F_c$. For example, in the case of a budget of 15, CAtNIPP trained on $F_c$ = 1 outperforms CATNIPP trained on $F_c$ = 5 and $F_c$ = 10 in an environment with $F_c$ = 1, whereas it performs poorly in environments with $F_c$ = 5 and $F_c$ = 10.\n\u2022 Domain randomization alone is not sufficient: CAt-NIPP+DR performs intermediate to the CATNIPP policies trained on $F_c$ = 1,5,10. This implies that DyPNIPP without environment dynamic prediction converges to the optimal policy for averaged environment dynamics, which is suboptimal for each individual environment dynamic.\n\u2022 DyPNIPP is robust against the variation in envi-ronment dynamics: DyPNIPP outperforms the baselines on both metrics. In addition, DyPNIPP shows similar co-variance trace performance regardless of the underlying fuel distribution. These are strong pieces of evidence supporting the robustness of our approach against variations in envi-ronment dynamics."}, {"title": "D. Analysis: Dynamics Prediction Model", "content": "We proposed to design the DPM to predict the belief of the next observation to capture the environment dynamics. To verify this, we conduct an ablation study comparing two modified versions of DyPNIPP that predict (1) the belief of the current observation and (2) the belief difference between the current observation and the next observation. The corresponding result is shown in Table V. We can see that predicting the belief of the current observation performs worse than DyPNIPP and the version of DyPNIPP that predicts the belief difference, both of which involve predicting the next observation. This is because predicting the belief of the current observation is not capable of capturing the environment dynamics. We observe that the version of DyPNIPP predicting the belief difference also performs well, even though its performance is slightly lower than the original DyPNIPP.\nAdditionally, to verify that the DPM implicitly learns the environment dynamics, we investigate the latent embeddings generated by the LSTM of our belief prediction network using t-SNE [42]. We collect the final 16-dimensional em-beddings at the end of the episode for three fuel/vegetation coefficients, $F_c$\u2208 {1,5,10}, with 200 seeds each. These embeddings are visualized in a 2-dimensional space using t-SNE. As shown in Fig. 3, we observe a clear progression of the latent embeddings as the fuel/vegetation coefficient increases from 1 (purple) to 5 (green), and then to 10 (yellow)."}, {"title": "E. Experimental Validation on Real Robot", "content": "We provide experimental validation on a real robot to ver-ify that our IPP model, trained in a simulator, can be applied in the real world. For this, we project the spatiotemporal wildfire environment onto a physical 1.5 m \u00d7 1.5 m arena, maintaining consistency in configuration between simulation and physical deployment. We used the Khepera-IV robot, equipped with a Raspberry Pi 3 and a camera module. In this experiment, we first trained a policy in simulation and then tested it using the Khepera-IV robot and the arena. Here, the robot can only observe the intensity of the fire, which is an environmental phenomenon, at its current location. It updates its belief via GP regression using the observation, and the updated belief is used as input for the next forward pass of our policy. We use a budget of 12 m for this experiment, which corresponds to 8 units in a unit grid. It is observed that the robot trained in the simulator successfully finds a path that maximizes the information gain, as shown in Fig. Figure 4. Through this experiment, we also demonstrate the fast decision-making time of our policy (less than 0.25 s on an Intel Xeon CPU). This experiment shows that DYPNIPP can be deployed on a robot in the spatiotemporal environment. The full video can be accessed here."}, {"title": "V. CONCLUSION", "content": "In this paper, we propose DyPNIPP, an RL-based frame-work for IPP that not only works in spatio-temporal environ-ments but is also robust against variations in environmental dynamics. DyPNIPP introduces two components on top of an RL-based IPP algorithm. First, DyPNIPP includes domain randomization during training to encourage the agent to encounter variations in environment dynamics. Second, DyPNIPP includes a dynamics prediction model that pre-dicts the belief of the next observation, allowing the agent to figure out the current environmental dynamics. We show that DYPNIPP is superior to existing IPP methods in terms of solution quality across diverse environmental dynamics, demonstrating its greater robustness. In addition, we provide a physical robot experiment to showcase real-time planning, highlighting its potential for real-life robotic applications."}, {"title": "Limitations", "content": "Our subroutine algorithm's reliance on a predefined graph for traversal presents two limitations: (1) uniform sampling may leave some areas of interest un-reachable due to insufficient coverage, and (2) the graph cannot adapt to unknown obstacles. We expect DyPNIPP to work with a dynamically sampled graph to enhance obstacle avoidance [12]. Additionally, the performance of the policy relies heavily on the robot's belief, which is influenced by GP hyperparameters. Different environmental dynamics may have distinct, optimal hyperparameters, but this mapping is unknown. Future work will explore integrating GP hyperpa-rameter optimization into the existing learning framework."}]}