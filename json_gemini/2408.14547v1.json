{"title": "Revisiting Image Captioning Training Paradigm via Direct CLIP-based Optimization", "authors": ["Nicholas Moratelli", "Davide Caffagni", "Marcella Cornia", "Lorenzo Baraldi", "Rita Cucchiara"], "abstract": "The conventional training approach for image captioning involves pre-training a network using teacher forcing and subsequent fine-tuning with Self-Critical Sequence Training to maximize hand-crafted captioning metrics. However, when attempting to optimize modern and higher-quality metrics like CLIP-Score and PAC-Score, this training method often encounters instability and fails to acquire the genuine descriptive capabilities needed to produce fluent and informative captions. In this paper, we propose a new training paradigm termed Direct CLIP-Based Optimization (DiCO). Our approach jointly learns and optimizes a reward model that is distilled from a learnable captioning evaluator with high human correlation. This is done by solving a weighted classification problem directly inside the captioner. At the same time, DiCO prevents divergence from the original model, ensuring that fluency is maintained. DiCO not only exhibits improved stability and enhanced quality in the generated captions but also aligns more closely with human preferences compared to existing methods, especially in modern metrics. Additionally, it maintains competitive performance in traditional metrics. Our source code and trained models are publicly available at https://github.com/aimagelab/DiCO.", "sections": [{"title": "Introduction", "content": "The task of image captioning [24, 56, 61, 67] requires an algorithm to describe a visual input in natural language. As a captioner should ideally match the level of detail and precision desired by the user, over time there has been an increasing interest in developing training strategies for aligning the behavior of a captioner to mimic a desired style and quality level."}, {"title": "Related Work", "content": "Standard image captioning. Early attempts in the field of image captioning were based on an encoder-decoder architecture, wherein the visual input content is encoded through a CNN, while the textual output is aptly generated by an RNN conditioned on the visual encoding [7, 24, 48, 61]. Subsequently, this approach witnessed refinement through the integration of different attention-based strategies [67], eventually applied to image regions [4] and enhanced with spatial and semantic graphs [68, 69]. More recently, an alternative trend encompasses Transformer-based architectures, where numerous works have been developed exploring varied directions [6, 16, 22, 32]. While the aforementioned approaches exploited the same fine-tuning strategy usually composed of a pre-training with cross-entropy loss followed by reinforcement learning, we explore a different perspective. Along this line, Cho et al. [14] stands out as the method that is closely related to our proposal, as it defines a CLIP-based fine-tuning scheme that, however, relies on reinforcement learning. Concurrently, large-scale vision-and-language pre-training has been used to perform several tasks requiring multimodal capabilities, such as image captioning. These models [21, 31, 62, 63, 71] are pre-trained on millions or even billions of image-text pairs, usually collected from the web, and fine-tuned for a target task.\nLLM-based image captioning. To leverage the power of LLMs demonstrated in different contexts, many attempts have emerged to bestow vision capabilities to a pre-trained LLM [17, 25, 26, 49, 73], resulting in impressive performance over various vision-and-language tasks like image captioning. In this context, ZeroCap [58] runs a few optimization steps for each new token, to align the text produced by GPT-2 [43] to the input image, using CLIP [44] as guidance. Other works [40, 46], instead, start from a pre-trained LLM and only learn cross-attention layers to mimic the interaction between textual and visual domains. Research efforts have also been dedicated to developing large-scale multimodal models [8], usually based on LLMs and trained on huge amounts of multimodal data [9, 12, 29, 30]. In this context, image captioning is employed as a pre-training task to help vision-and-language alignment, and eventually in the instruction-tuning stage [35, 36]. Thanks to the underlying LLM, all these solutions usually lead to image captioners with greater descriptive capabilities. In this work, we show how to increase the quality and descriptiveness of generated captions without relying on any pre-trained LLM.\nTraining strategies for LLMs. Aligning models with human judgment constitutes a well-known issue in both NLP and captioning literature. In this context, several strategies for fine-tuning LLMs have been explored. For example, a common research direction is to guide the model through a combination of input-output pairs and explicit instructions [13, 23, 64, 65]."}, {"title": "Proposed Method", "content": "Preliminaries. Self-critical sequence training (SCST [48]) is a traditional training paradigm for image captioning. It consists of a two-step training methodology which pre-trains a captioner using a time-wise cross-entropy loss with respect to ground-truth sequences, and fine-tunes the same network by maximizing the CIDEr score [60] using an reinforcement learning approach. Recently, it has been applied also with learnable metrics [19, 50, 51] such as CLIP-S [19], which employs a CLIP [44] embedding space trained to align the embeddings of 400M images and caption pairs. Consequently, a high similarity between a pair of visual-textual CLIP embeddings means that the image-caption pair is highly correlated as well. On the other hand, reinforcement learning from human feedback (RLHF [41]) has been shown to be effective in making LLMs behave more like humans. It starts with a self-supervised pre-trained LLM, then goes through a supervised training phase, and finally, a fine-tuning stage using reinforcement learning. This last step is focused on improving the quality of generated responses by maximizing the score given by a reward model trained to imitate human judgment when comparing two candidate answers. We refer to Appendix A for more details about SCST, RLHF, and image captioning metrics based on contrastive embedding spaces, i.e. CLIP-S [19] and PAC-S [50].\nMotivation. While adopting significantly different technical choices, there are striking conceptual similarities between the modern RLHF paradigm employed in LLMs and the traditional SCST approach employed in image captioning. Both approaches, indeed, employ reinforcement learning to optimize a reward function, which nevertheless in SCST is a hand-crafted metric, while in RLHF is a learned function from human data. While using RLHF in captioning is impracticable due to the insufficient amount of human preference data to train the reward model (see the comparison with RLHF in Appendix C), contrastive-based learnable metrics offer a compelling alternative to it, as they show a significant alignment with human judgment [50]. Our proposal solves this issue by distilling a reward model from a pre-trained captioning evaluator, considering pairwise relationships from candidate captions. In addition, it also avoids model collapse which is frequent in SCST (cf. Fig. 1)."}, {"title": "Deriving the fine-tuning objective", "content": "Following recent works on LLM alignment [41], we aim at fine-tuning a captioner $f_{\\theta}$ with a Proximal Policy Optimization (PPO) objective [52], where given an image $I$ and a caption $s'$ sampled from the model, the environment produces a reward $r(s', I)$ through a reward model. In addition, we add a per-token KL penalty with the output of the pre-trained model to mitigate overoptimization of the fine-tuned captioner to the reward model. Our objective is therefore defined as\n$\\max _{f_{\\theta}} \\mathbb{E}_{I \\sim \\mathcal{D}, s^{\\prime} \\sim f_{\\theta}(s \\mid I)}[r(s^{\\prime}, I)]-\\beta D_{K L}\\left[f_{\\theta}(s^{\\prime} \\mid I) \\| f_{*}(s^{\\prime} \\mid I)\\right],$\nwhere $\\beta$ controls the deviation from the pre-trained model, termed as $f_{*}$. As it can be seen, the second term has a crucial role, as it prevents the fine-tuned model $f_{\\theta}$ from deviating from the distribution on which the reward model is accurate, and prevents the captioner from hacking it, i.e. collapsing to high-rewarded answers.\nUnder this objective, it can be shown [45] that the optimal solution to the fine-tuning problem is given by a model $f$, defined as\n$f_{*}(s^{\\prime} \\mid I)=\\frac{1}{Z(I)} f_{*}(s^{\\prime} \\mid I) \\exp \\left(\\frac{1}{\\beta} r(s^{\\prime}, I)\\right),$\nwhere $Z(I)=\\mathbb{E}_{s} f_{*}(s \\mid I) \\exp \\left(\\frac{1}{\\beta} r(s, I)\\right)$ is the partition function over possible captions. Although the partition function is difficult to estimate, we can still manipulate Eq. 2 to express the reward function in terms of the optimal captioner, the pre-trained captioner, and the partition function, as follows:\n$r(s^{\\prime}, I)=\\beta \\log \\frac{f_{*}(s^{\\prime} \\mid I)}{f_{*}(s^{\\prime} \\mid I)}+\\beta \\log Z(I).$"}, {"title": "Defining a distilled reward model", "content": "Since we do not have access to sufficiently large human preference data, defining the reward model in a purely data-driven way would be cumbersome. Instead, we learn our reward model by distilling it from a contrastive-based captioning evaluator $\\mathcal{E}$. We assume that, given an image and a candidate sentence $(I, s^{\\prime})$, the evaluator returns a matching score $\\mathcal{E}(s^{\\prime}, I)$ proportional to the similarity between $s^{\\prime}$ and $I$.\nGiven a dataset $\\mathcal{D}$ comprising images, we let the captioner generate $k+1$ candidate captions (e.g. through beam search). Then, for each image, we select the caption with the highest score according to $\\mathcal{E}$ and denote it as $s_{w}$ (i.e. \"winner\"). The others, instead, are denoted as $\\{s_{l}^{i}\\}_{i=1}^{k}$ (i.e. \"losers\"). Based on the evaluator, we define a reward model which distinguishes between the winner caption $s_{w}$ and the loser captions $\\{s_{l}^{i}\\}$. To make the reward model more robust and accurate, we also impose that it can predict the relative quality distances between the winner and the loser captions. Formally, we define our reward model through the following objective:\n$\\mathcal{L}_{R}(r)=-\\mathbb{E}\\left[\\log \\sigma\\left(\\sum_{i=1}^{k} \\gamma_{i}\\left(r(I, s_{w})-r\\left(I, s_{l}^{i}\\right)\\right)\\right)\\right],$\nwhere the expectation is taken over images in the dataset and winner and loser captions. Also, $\\gamma_{i}$ weights the relative distance between the winner caption $s_{w}$ and the i-th loser caption $s_{l}^{i}$ according to the evaluator $\\mathcal{E}$. Specifically, it is computed as a normalized probability distribution between score distances, as follows:\n$\\gamma_{i}=\\operatorname{softmax}_{s_{l}^{i}} \\frac{\\mathcal{E}(I, s_{w})-\\mathcal{E}(I, s_{l}^{i})}{\\tau}.$"}, {"title": "Overall loss function", "content": "Following [45], we learn the reward model directly into the captioner. Recalling that the Bradley-Terry model depends only on the difference in rewards between two completions and that $\\gamma_{i}$ are a valid probability distribution, we replace the definition of $r(s^{\\prime}, I)$ as a function of the optimal fine-tuned and pre-trained captioner (Eq. 3) into the reward model objective (Eq. 6), and obtain the final fine-tuning loss of DiCO as\n$\\mathcal{L}\\left(f_{\\theta}, f_{*}\\right)=-\\mathbb{E}\\left[\\log \\sigma\\left(\\beta \\log \\frac{f_{\\theta}\\left(s_{w} \\mid I\\right)}{f_{*}\\left(s_{w} \\mid I\\right)}-\\sum_{i=1}^{k} \\beta \\gamma_{i} \\log \\frac{f_{\\theta}\\left(s_{l}^{i} \\mid I\\right)}{f_{*}\\left(s_{l}^{i} \\mid I\\right)}\\right)\\right],$\nwhere, noticeably, the unknown partition function $Z(I)$ has been cancelled out. Furthermore, the obtained fine-tuning loss, while being derived from the optimal solution to a PPO objective, can be directly optimized through gradient descent, without the need of employing reinforcement learning techniques.\nComparing DiCO with SCST and RLHF. DiCO fine-tunes a captioning model by aligning it to a contrastive-based evaluator while avoiding over-parametrization and model collapse. In comparison with SCST and RLHF, its unique feature is that of distilling a reward model from an external evaluator by learning it directly inside of the captioner. Further, this is done by avoiding the usage of reinforcement learning at fine-tuning time, which is common to both SCST and RLHF. Differently from RLHF, also, caption candidates are directly sampled from the model, so that a dataset of human-annotated preferences can be avoided. Finally, differently from SCST, DiCO embeds a regularizer to prevent the fine-tuned model from deviating too much from the pre-trained captioner."}, {"title": "Experiments", "content": "4.1 Experimental Setting\nDatasets. All experiments are performed on the COCO dataset [34], using the standard splits defined in [24] with 5,000 images for both test and validation and the rest for training. We report our experimental results on the test set of COCO. Further, we refer the reader to Appendix C for results on six additional datasets, namely nocaps [2], VizWiz [18], TextCaps [55], Conceptual Captions 3M (CC3M) [53], FineCapEval [14], and Flickr30k [70].\nEvaluation metrics. In addition to the standard image captioning metrics like BLEU [42], METEOR [5], and CIDEr [60], we employ two CLIP-based scores, namely CLIP-S [19] and PAC-S [50], in both their reference-free and reference-based versions, using the ViT-B/32 backbone for both metrics (also see Appendix A). Moreover, following recent works [11, 27], we measure the quality of generated captions in distinguishing images in a dataset and compute the percentage of the times the image corresponding to each generated caption is retrieved among the first $K$ retrieved items. This is done by ranking the images in terms of CLIP similarity between visual and textual embeddings, using the CLIP ViT-B/32 model, and computing recall at $K$ with $K=1,5,10$. We also compute the mean reciprocal rank"}, {"title": "Comparison with the State of the Art", "content": "Results on COCO test set. We compare our model trained with the proposed DiCO strategy with other state-of-the-art solutions. We restrain the comparison by only considering captioning models that use CLIP-based visual features to encode images, which have proven"}, {"title": "Conclusion", "content": "We presented DiCO, a novel fine-tuning strategy for image captioning which aligns a model to a learnable evaluator with high human correlation. Our approach optimizes a distilled reward model by solving a weighted classification problem directly inside the captioner, which allows it to capture fine-grained differences between multiple candidate captions. Experimental results on several datasets, conducted through automatic metrics and human evaluations, validate the effectiveness of our approach, which can generate more descriptive and detailed captions than competitors. At the same time, it achieves state-of-the-art results when trained to optimize traditional reference-based metrics."}, {"title": "Preliminaries", "content": "In this section, we first recap the definition of the SCST and Reinforcement Learning from Human Feedback (RLHF) training protocols [41, 48]. Then, we introduce captioning metrics based on contrastive embedding spaces [44].\nSelf-critical sequence training. SCST [48] is a two-step training methodology which (1) pre-trains a captioner $f_{\\theta}$ using a time-wise cross-entropy loss with respect to ground-truth sequences, and (2) fine-tunes the same network by maximizing the CIDEr score [60] using a reinforcement learning (RL) approach. We assume that the captioner takes as input an image $I$ described with a sequence of visual features $(V_{1}, V_{2},..., V_{R})$, and a ground-truth sequence $S = (W_{1},W_{2},..., W_{T})$, where $w_{i}$ is a token belonging to a pre-defined vocabulary. Noticeably, depending on the dataset there might be multiple ground-truth sequences associated with each image. During the first training stage, the network is conditioned on visual features and all ground-truth tokens up to the current prediction step $t$, and $f_{\\theta}$ is optimized using the cross-entropy loss (teacher forcing). In the second training stage, instead, the network is only conditioned on the input image and generates an entire caption $s' = (w'_{1}, w'_{2}, ..., w'_{T})$ by sampling input tokens from the output probability distribution generated at the previous time step. For instance, $w'_{t}$ might be chosen as $w'_{t} = \\operatorname{argmax} f_{\\theta}(w'_{t} \\mid w'_{t-1},..., W'_{1}, V_{1},..., V_{R})$, or multiple sentences can be sampled via beam search. The generated sentences are then employed to compute the CIDEr metric, which is later used as a reward to guide a policy-gradient RL update step (see [48] for details).\nReinforcement learning from human feedback. Recent NLP literature has employed techniques based on RLHF [41] to align the behavior of a large language model to human preferences. This approach is usually based on the collection of large-scale datasets of human preferences: the language model $f_{\\theta}$ is prompted with a prompt $x$ to produce pairs of answers $(s_{1}, s_{2}) \\sim f_{\\theta}$, which are then presented to human labelers who express preference for one answer, i.e. $s_{w} > s^{\\prime}_{l}$, where $s_{w}$ and $s^{\\prime}_{l}$ indicate, respectively, the preferred and dis-preferred completion. The resulting dataset of human preferences $\\mathcal{D} = \\{(x_{i}, s_{w_{i}}, s^{\\prime}_{l_{i}})\\}_{i=1}^{N}$"}, {"title": "Learnable contrastive captioning metrics", "content": "As pointed out by recent literature on captioning evaluation, a model learned with language-image pre-training [44] can be straightforwardly employed as a captioning metric. Given a caption $s'$ generated from $I$, indeed, its correctness score can be defined as a function of the similarity predicted by the image-text model, i.e. $\\operatorname{sim}(I, s')$. A popular choice [19] is to define the score to be proportional to the ReLU of the predicted similarity and to employ a scalar multiplier $w$ to stretch the resulting score within the range of [0, 1]:\n$\\operatorname{CLIP-S}(I, s') = w \\cdot \\operatorname{ReLU}(\\operatorname{sim}(I, s')).$\nIn the original formulation of [19] (termed CLIP-S), the backbone employed for computing similarities was pre-trained on 400M noisy (image, text) pairs collected from the internet. While CLIP-S shows a significantly higher alignment with human judgments compared to traditional metrics (e.g. BLEU, METEOR, CIDEr), the noisy nature of the training data limits the CLIP-S capability to distinguish fluent human-generated captions. To overcome this issue, a recent choice [50] is that of fine-tuning the backbone on cleaned data, which further boosts the correlation with human judgments. Specifically, the PAC-S score [50] trains on the basis of a similarity matrix built with human-collected captions and machine-generated ones, where the latter are obtained from a captioner trained to mimic the same distribution of human captions. In case a set of reference captions $R = \\{r_{i}\\}_{i=1}^{k}$ is given, there exists a version of the CLIP-based metrics accounting for them [19], which is defined as follows:\n$\\operatorname{RefCLIP-S}(I, s', R) = H-\\operatorname{Mean}\\left(\\operatorname{CLIP-S}(I, s'), \\operatorname{ReLU}\\left(\\max _{r \\in R} \\operatorname{cos}(s', r)\\right)\\right).$\nFollowing [50], the same formula can be applied to compute the reference-based version of PAC-S (RefPAC-S)."}, {"title": "Ablation Studies", "content": "Early stopping condition. When comparing multiple training strategies, we always employ an early stopping condition based on the validation value of the reference-based version of the metric used as a reward. In practice, when optimizing for CLIP-S, we early stop the training according to the validation RefCLIP-S, while when optimizing for PAC-S we early stop based on the validation RefPAC-S. We then take the model state corresponding to the epoch with the highest validation score and report its evaluation metrics. While this provides a reasonable evaluation strategy that equally promotes all compared approaches, evaluating a single model state does not capture the full training behavior of different fine-tuning strategies."}, {"title": "Additional Qualitative Results", "content": "Finally, we report additional qualitative results to qualitatively validate the effectiveness of our training strategy. In particular, Fig. 8 and Fig. 9 show sample images from the COCO dataset and captions predicted by DiCO in comparison to those generated by SCST, the model proposed in [14], and the large-scale model BLIP-2 [30]. As it can be seen, DiCO generates significantly more detailed captions than BLIP-2, while reducing repetitions typically present in SCST-generated sentences. To qualitative validate the generalization capabilities to out-of-domain images, we report sample captions predicted by DiCO and SCST using PAC-S as reward on nocaps [2] (Fig. 10), VizWiz [18] (Fig. 11), TextCaps [55] (Fig. 12), and CC3M [53] (Fig. 13).\nIn Fig. 14, we instead show some qualitative results when using CIDEr as reward. In this case, we compare DiCO with standard image captioning models, including a vanilla Transformers trained with the same visual features used in our approach, COS-Net [32], and M\u00b2 Transformer [16]. All competitors have been trained with a standard XE+SCST training protocol. Also in this setting, DiCO is able to generate high-quality captions compared to competitors, confirming that it can also be employed as a valid alternative to SCST for training standard image captioning models."}, {"title": "Limitations", "content": "As with all image captioning models, we acknowledge that our method might fail to provide informative captions in some rare contexts. To qualitatively evaluate the limitations of our approach, we report some failure cases in Fig. 15. As it can be seen, DiCO may produce factual errors, e.g. mistaking balloons for kites (first sample, first row) or a stuffed animal for a seal (first sample, second row). Additionally, DiCO may fail to recognize known entities, thus providing only a broad description of the scene (e.g. a white monument rather than the Taj Mahal mausoleum, or a black silver car rather than an Aston Martin). This can be conducted to the image-caption pairs contained in the COCO dataset, which lack open-world knowledge. Finally, when the main subject of the image is uncertain (second sample, third row), DiCO may overlook the picture and generate captions based on its learned priors, resulting in hallucinations."}]}