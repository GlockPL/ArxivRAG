{"title": "Revisiting Image Captioning Training Paradigm via Direct CLIP-based Optimization", "authors": ["Nicholas Moratelli", "Davide Caffagni", "Marcella Cornia", "Lorenzo Baraldi", "Rita Cucchiara"], "abstract": "The conventional training approach for image captioning involves pre-training a network using teacher forcing and subsequent fine-tuning with Self-Critical Sequence Training to maximize hand-crafted captioning metrics. However, when attempting to optimize modern and higher-quality metrics like CLIP-Score and PAC-Score, this training method often encounters instability and fails to acquire the genuine descriptive capabilities needed to produce fluent and informative captions. In this paper, we propose a new training paradigm termed Direct CLIP-Based Optimization (DiCO). Our approach jointly learns and optimizes a reward model that is distilled from a learnable captioning evaluator with high human correlation. This is done by solving a weighted classification problem directly inside the captioner. At the same time, DiCO prevents divergence from the original model, ensuring that fluency is maintained. DiCO not only exhibits improved stability and enhanced quality in the generated captions but also aligns more closely with human preferences compared to existing methods, especially in modern metrics. Additionally, it maintains competitive performance in traditional metrics. Our source code and trained models are publicly available at https://github.com/aimagelab/DiCO.", "sections": [{"title": "Introduction", "content": "The task of image captioning [24, 56, 61, 67] requires an algorithm to describe a visual input in natural language. As a captioner should ideally match the level of detail and precision desired by the user, over time there has been an increasing interest in developing training strategies for aligning the behavior of a captioner to mimic a desired style and quality level.\nTraditionally, the quality of captions has been measured with textual similarity metrics, so captioners have been trained to maximize a non-differentiable metric like CIDEr [60] during a fine-tuning stage based on reinforcement learning, i.e. Self-Critical Sequence Training (SCST) [37, 47, 48]. As this strategy requires the availability of multiple reference captions and tends to produce less distinctive descriptions that ignore the fine detailed aspects of an image, recently there have been preliminary attempts to optimize higher-quality image captioning metrics based on embedding spaces that do not require human references [14, 27, 72], like CLIP-Score [19] and PAC-Score [50]. Besides, these metrics also consider the actual multi-modal alignment between the generated text and the visual content of the input image rather than just comparing texts. Most importantly, they also showcase a superior alignment with human judgment, making them ideal candidates for tuning the behavior of captioners towards a higher quality of generation.\nUnfortunately, optimizing modern metrics with pre-existing strategies like SCST results in instability and model collapse [14]. We showcase this in Fig. 1, where we employ SCST for optimizing either PAC-S or CIDEr (light blue lines). When we try to optimize PAC-S, the fine-tuned captioner hacks the metric and deviates from a fluent and high-quality generation, resulting in a rapid decrease according to all other metrics and leading to repetitions and grammatical errors. To solve these issues, we propose DiCO, a novel training methodology that can align a captioner towards better quality captions by distilling from an external contrastive-based evaluator like CLIP-S or PAC-S, without incurring model collapse and without employing a reinforcement learning objective. Our approach achieves this goal by learning a reward model directly into the captioner and mimicking pairwise quality relations expressed by the external evaluator. This ensures a high degree of alignment with human preferences while avoiding reward hacking. This is visually represented in Fig. 1 (dark blue lines): DiCO can optimize both a modern metric like PAC-S and a traditional one like CIDEr by maintaining good scores across all metrics.\nWe assess the quality of the proposed training methodology by conducting extensive experiments on the COCO dataset [34]. Furthermore, in the supplementary materials, we prove the generalization capabilities of DiCO over other six image captioning benchmarks. Our experimental results demonstrate that DiCO features state-of-the-art quality in the generated captions and improved training stability. This also results in a better performance in terms of modern captioning metrics, while also balancing with competitive performances"}, {"title": "Related Work", "content": "Standard image captioning. Early attempts in the field of image captioning were based on an encoder-decoder architecture, wherein the visual input content is encoded through a CNN, while the textual output is aptly generated by an RNN conditioned on the visual encoding [7, 24, 48, 61]. Subsequently, this approach witnessed refinement through the integration of different attention-based strategies [67], eventually applied to image regions [4] and enhanced with spatial and semantic graphs [68, 69]. More recently, an alternative trend encompasses Transformer-based architectures, where numerous works have been developed exploring varied directions [6, 16, 22, 32]. While the aforementioned approaches exploited the same fine-tuning strategy usually composed of a pre-training with cross-entropy loss followed by reinforcement learning, we explore a different perspective. Along this line, Cho et al. [14] stands out as the method that is closely related to our proposal, as it defines a CLIP-based fine-tuning scheme that, however, relies on reinforcement learning. Concurrently, large-scale vision-and-language pre-training has been used to perform several tasks requiring multimodal capabilities, such as image captioning. These models [21, 31, 62, 63, 71] are pre-trained on millions or even billions of image-text pairs, usually collected from the web, and fine-tuned for a target task.\nLLM-based image captioning. To leverage the power of LLMs demonstrated in different contexts, many attempts have emerged to bestow vision capabilities to a pre-trained LLM [17, 25, 26, 49, 73], resulting in impressive performance over various vision-and-language tasks like image captioning. In this context, ZeroCap [58] runs a few optimization steps for each new token, to align the text produced by GPT-2 [43] to the input image, using CLIP [44] as guidance. Other works [40, 46], instead, start from a pre-trained LLM and only learn cross-attention layers to mimic the interaction between textual and visual domains. Research efforts have also been dedicated to developing large-scale multimodal models [8], usually based on LLMs and trained on huge amounts of multimodal data [9, 12, 29, 30]. In this context, image captioning is employed as a pre-training task to help vision-and-language alignment, and eventually in the instruction-tuning stage [35, 36]. Thanks to the underlying LLM, all these solutions usually lead to image captioners with greater descriptive capabilities. In this work, we show how to increase the quality and descriptiveness of generated captions without relying on any pre-trained LLM.\nTraining strategies for LLMs. Aligning models with human judgment constitutes a well-known issue in both NLP and captioning literature. In this context, several strategies for fine-tuning LLMs have been explored. For example, a common research direction is to guide the model through a combination of input-output pairs and explicit instructions [13, 23, 64, 65]."}, {"title": "Proposed Method", "content": "Preliminaries. Self-critical sequence training (SCST [48]) is a traditional training paradigm for image captioning. It consists of a two-step training methodology which pre-trains a captioner using a time-wise cross-entropy loss with respect to ground-truth sequences, and fine-tunes the same network by maximizing the CIDEr score [60] using an reinforcement learning approach. Recently, it has been applied also with learnable metrics [19, 50, 51] such as CLIP-S [19], which employs a CLIP [44] embedding space trained to align the embeddings of 400M images and caption pairs. Consequently, a high similarity between a pair of visual-textual CLIP embeddings means that the image-caption pair is highly correlated as well. On the other hand, reinforcement learning from human feedback (RLHF [41]) has been shown to be effective in making LLMs behave more like humans. It starts with a self-supervised pre-trained LLM, then goes through a supervised training phase, and finally, a fine-tuning stage using reinforcement learning. This last step is focused on improving the quality of generated responses by maximizing the score given by a reward model trained to imitate human judgment when comparing two candidate answers. We refer to Appendix A for more details about SCST, RLHF, and image captioning metrics based on contrastive embedding spaces, i.e. CLIP-S [19] and PAC-S [50].\nMotivation. While adopting significantly different technical choices, there are striking conceptual similarities between the modern RLHF paradigm employed in LLMs and the traditional SCST approach employed in image captioning. Both approaches, indeed, employ reinforcement learning to optimize a reward function, which nevertheless in SCST is a hand-crafted metric, while in RLHF is a learned function from human data. While using RLHF in captioning is impracticable due to the insufficient amount of human preference data to train the reward model (see the comparison with RLHF in Appendix C), contrastive-based learnable metrics offer a compelling alternative to it, as they show a significant alignment with human judgment [50]. Our proposal solves this issue by distilling a reward model from a pre-trained captioning evaluator, considering pairwise relationships from candidate captions. In addition, it also avoids model collapse which is frequent in SCST (cf. Fig. 1)."}, {"title": "Deriving the fine-tuning objective", "content": "Following recent works on LLM alignment [41], we aim at fine-tuning a captioner fe with a Proximal Policy Optimization (PPO) objective [52], where given an image I and a caption s' sampled from the model, the environment produces a reward r(s', I) through a reward model. In addition, we add a per-token KL penalty with the output of the pre-trained model to mitigate overoptimization of the fine-tuned captioner to the reward model. Our objective is therefore defined as\n$\\max_{f_e} E_{I \\sim D, s' \\sim f_e(\\cdot | I)} [r(s', I)] \u2013 \\beta D_{KL} [f_o(s' | I) || f_*(s' | I)],$ (1)\nwhere \u03b2 controls the deviation from the pre-trained model, termed as f*. As it can be seen, the second term has a crucial role, as it prevents the fine-tuned model fe from deviating from the distribution on which the reward model is accurate, and prevents the captioner from hacking it, i.e. collapsing to high-rewarded answers.\nUnder this objective, it can be shown [45] that the optimal solution to the fine-tuning problem is given by a model f, defined as\n$f_r(s' | I) = \\frac{1}{Z(I)} f_*(s' | I) exp (\\frac{1}{\\beta} r(s', I));$ (2)\nwhere Z(I) = Es f*(s|I) exp (\\frac{1}{\\beta} r(s, I))\nis the partition function over possible captions. Although the partition function is difficult to estimate, we can still manipulate Eq. 2 to express the reward function in terms of the optimal captioner, the pre-trained captioner, and the partition function, as follows:\n$r(s', I) = \\beta log \\frac{f_r(s' | I)}{f_*(s' | I)} + \\beta log Z(I).$ (3)"}, {"title": "Defining a distilled reward model", "content": "Since we do not have access to sufficiently large human preference data, defining the reward model in a purely data-driven way would be cumbersome. Instead, we learn our reward model by distilling it from a contrastive-based captioning evaluator E. We assume that, given an image and a candidate sentence (I,s'), the evaluator returns a matching score E(s', I) proportional to the similarity between s' and I.\nGiven a dataset D comprising images, we let the captioner generate k + 1 candidate captions (e.g. through beam search). Then, for each image, we select the caption with the highest score according to & and denote it as sw (i.e. \"winner\"). The others, instead, are denoted as {s\\}_{i=1}^{k} (i.e. \"losers\"). Based on the evaluator, we define a reward model which distinguishes between the winner caption sw and the loser captions {si}i. To make the reward model more robust and accurate, we also impose that it can predict the relative quality distances between the winner and the loser captions. Formally, we define our reward model through the following objective:\n$L_R(r) = -E \\Big[log \\sigma \\Big( \\sum_{i=1}^k \\gamma_i (r(I, s_w) - r(I, s_i))\\Big)\\Big],$ (4)\nwhere the expectation is taken over images in the dataset and winner and loser captions. Also, \u03b3i weights the relative distance between the winner captions, and the i-th loser caption si according to the evaluator E. Specifically, it is computed as a normalized probability distribution between score distances, as follows:\n$\\gamma_i = softmax_{\\gamma_1, ..., \\gamma_k}\\Big(\\frac{E(I, s_w) - E(I, s_i)}{\\tau}\\Big),$ (5)"}, {"title": "Overall loss function", "content": "Following [45], we learn the reward model directly into the captioner. Recalling that the Bradley-Terry model depends only on the difference in rewards between two completions and that \u03b3i are a valid probability distribution, we replace the definition of r(s',I) as a function of the optimal fine-tuned and pre-trained captioner (Eq. 3) into the reward model objective (Eq. 6), and obtain the final fine-tuning loss of DiCO as\n$L(f_e, f_*) = -E \\Big[log \\sigma \\Big( \\beta \\gamma_i log \\frac{f_e(s_w|I)}{f_*(s_w|I)} \u2013 \\sum_{i=1}^k \\beta \\gamma_i log \\frac{f_e(s_i|I)}{f_*(s_i|I)} \\Big)\\Big],$ (7)\nwhere, noticeably, the unknown partition function Z(I) has been cancelled out. Furthermore, the obtained fine-tuning loss, while being derived from the optimal solution to a PPO objective, can be directly optimized through gradient descent, without the need of employing reinforcement learning techniques."}, {"title": "Comparing DiCO with SCST and RLHF", "content": "DiCO fine-tunes a captioning model by aligning it to a contrastive-based evaluator while avoiding over-parametrization and model collapse. In comparison with SCST and RLHF, its unique feature is that of distilling a reward model from an external evaluator by learning it directly inside of the captioner. Further, this is done by avoiding the usage of reinforcement learning at fine-tuning time, which is common to both SCST and RLHF. Differently from RLHF, also, caption candidates are directly sampled from the model, so that a dataset of human-annotated preferences can be avoided. Finally, differently from SCST, DiCO embeds a regularizer to prevent the fine-tuned model from deviating too much from the pre-trained captioner."}, {"title": "Experiments", "content": "Experimental Setting\nDatasets. All experiments are performed on the COCO dataset [34], using the standard splits defined in [24] with 5,000 images for both test and validation and the rest for training. We report our experimental results on the test set of COCO. Further, we refer the reader to Appendix C for results on six additional datasets, namely nocaps [2], VizWiz [18], TextCaps [55], Conceptual Captions 3M (CC3M) [53], FineCapEval [14], and Flickr30k [70].\nEvaluation metrics. In addition to the standard image captioning metrics like BLEU [42], METEOR [5], and CIDEr [60], we employ two CLIP-based scores, namely CLIP-S [19] and PAC-S [50], in both their reference-free and reference-based versions, using the ViT-B/32 backbone for both metrics (also see Appendix A). Moreover, following recent works [11, 27], we measure the quality of generated captions in distinguishing images in a dataset and compute the percentage of the times the image corresponding to each generated caption is retrieved among the first K retrieved items. This is done by ranking the images in terms of CLIP similarity between visual and textual embeddings, using the CLIP ViT-B/32 model, and computing recall at K with K = 1,5,10. We also compute the mean reciprocal rank"}, {"title": "Comparison with the State of the Art", "content": "Results on COCO test set. We compare our model trained with the proposed DiCO strategy with other state-of-the-art solutions. We restrain the comparison by only considering captioning models that use CLIP-based visual features to encode images, which have proven"}, {"title": "Conclusion", "content": "We presented DiCO, a novel fine-tuning strategy for image captioning which aligns a model to a learnable evaluator with high human correlation. Our approach optimizes a distilled reward model by solving a weighted classification problem directly inside the captioner, which allows it to capture fine-grained differences between multiple candidate captions. Experimental results on several datasets, conducted through automatic metrics and human evaluations, validate the effectiveness of our approach, which can generate more descriptive and detailed captions than competitors. At the same time, it achieves state-of-the-art results when trained to optimize traditional reference-based metrics."}, {"title": "Preliminaries", "content": "In this section, we first recap the definition of the SCST and Reinforcement Learning from Human Feedback (RLHF) training protocols [41, 48]. Then, we introduce captioning metrics based on contrastive embedding spaces [44].\nSelf-critical sequence training. SCST [48] is a two-step training methodology which (1) pre-trains a captioner fe using a time-wise cross-entropy loss with respect to ground-truth sequences, and (2) fine-tunes the same network by maximizing the CIDEr score [60] using a reinforcement learning (RL) approach. We assume that the captioner takes as input an image I described with a sequence of visual features (V1, V2,..., VR), and a ground-truth sequence S = (W1,W2,..., wr), where w\u2081 is a token belonging to a pre-defined vocabulary. Noticeably, depending on the dataset there might be multiple ground-truth sequences associated with each image. During the first training stage, the network is conditioned on visual features and all ground-truth tokens up to the current prediction step t, and fe is optimized using the cross-entropy loss (teacher forcing). In the second training stage, instead, the network is only conditioned on the input image and generates an entire caption s' = (w\u2081, W2, ..., Wp) by sampling input tokens from the output probability distribution generated at the previous time step. For instance, w' might be chosen as w\u2081 = argmaxfo(w\u2081|w-1,..., W1, V1, ..., VR), or multiple sentences can be sampled via beam search. The generated sentences are then employed to compute the CIDEr metric, which is later used as a reward to guide a policy-gradient RL update step (see [48] for details).\nReinforcement learning from human feedback. Recent NLP literature has employed techniques based on RLHF [41] to align the behavior of a large language model to human preferences. This approach is usually based on the collection of large-scale datasets of human preferences: the language model fe\u00b3 is prompted with a prompt x to produce pairs of answers (s1,s2) ~ fe, which are then presented to human labelers who express preference for one answer, i.e. sw > s', where s and si indicate, respectively, the preferred and dis-preferred completion. The resulting dataset of human preferences D = {xi, Swili 1 is"}, {"title": "Deriving the fine-tuning objective", "content": "CLIP-S(I, s') = w\u00b7 ReLU(sim(I, s')). (8)\nIn the original formulation of [19] (termed CLIP-S), the backbone employed for computing similarities was pre-trained on 400M noisy (image, text) pairs collected from the internet. While CLIP-S shows a significantly higher alignment with human judgments compared to traditional metrics (e.g. BLEU, METEOR, CIDEr), the noisy nature of the training data limits the CLIP-S capability to distinguish fluent human-generated captions. To overcome this issue, a recent choice [50] is that of fine-tuning the backbone on cleaned data, which further boosts the correlation with human judgments. Specifically, the PAC-S score [50] trains on the basis of a similarity matrix built with human-collected captions and machine-generated ones, where the latter are obtained from a captioner trained to mimic the same distribution of human captions. In case a set of reference captions R = {ri}\u2081 is given, there exists a version of the CLIP-based metrics accounting for them [19], which is defined as follows:\nRefCLIP-S(I, s', R) = H-Mean(CLIP-S(I, s'), ReLU(maxcos(s',r))). (9)\nFollowing [50], the same formula can be applied to compute the reference-based version of PAC-S (RefPAC-S)."}, {"title": "Ablation Studies", "content": "Early stopping condition. When comparing multiple training strategies, we always employ an early stopping condition based on the validation value of the reference-based version of the metric used as a reward. In practice, when optimizing for CLIP-S, we early stop the training according to the validation RefCLIP-S, while when optimizing for PAC-S we early stop based on the validation RefPAC-S. We then take the model state corresponding to the epoch with the highest validation score and report its evaluation metrics. While this provides a reasonable evaluation strategy that equally promotes all compared approaches, evaluating a single model state does not capture the full training behavior of different fine-tuning strategies."}, {"title": "Additional Details", "content": "Additional Implementation and training details. During cross-entropy pre-training, we accumulate gradients for 8 training steps over 2 GPUs, resulting in 1,024 samples per batch. For this training stage, the learning rate is linearly increased up to 2.5\u00b710-4. Each fine-tuning experiment starts from the XE checkpoint with the highest CIDEr, leveraging 2 GPUs and a global batch size of 16. Training the reward models for RLHF follows the same settings as the fine-tuning phase.\nCIDEr-based optimization. In computing quality distances with CIDEr metric as reward"}, {"title": "Additional Qualitative Results", "content": "Finally, we report additional qualitative results to qualitatively validate the effectiveness of our training strategy. In particular, Fig. 8 and Fig. 9 show sample images from the COCO dataset and captions predicted by DiCO in comparison to those generated by SCST, the model proposed in [14], and the large-scale model BLIP-2 [30]. As it can be seen, DiCO generates significantly more detailed captions than BLIP-2, while reducing repetitions typically present in SCST-generated sentences. To qualitative validate the generalization capabilities to out-of-domain images, we report sample captions predicted by DiCO and SCST using PAC-S as reward on nocaps [2] (Fig. 10), VizWiz [18] (Fig. 11), TextCaps [55] (Fig. 12), and CC3M [53] (Fig. 13).\nIn Fig. 14, we instead show some qualitative results when using CIDEr as reward. In this case, we compare DiCO with standard image captioning models, including a vanilla Transformers trained with the same visual features used in our approach, COS-Net [32], and M\u00b2 Transformer [16]. All competitors have been trained with a standard XE+SCST training protocol. Also in this setting, DiCO is able to generate high-quality captions compared to competitors, confirming that it can also be employed as a valid alternative to SCST for training standard image captioning models."}, {"title": "Limitations", "content": "As with all image captioning models, we acknowledge that our method might fail to provide informative captions in some rare contexts. To qualitatively evaluate the limitations of our approach, we report some failure cases in Fig. 15. As it can be seen, DiCO may produce factual errors, e.g. mistaking balloons for kites (first sample, first row) or a stuffed animal for a seal (first sample, second row). Additionally, DiCO may fail to recognize known entities, thus providing only a broad description of the scene (e.g. a white monument rather than the Taj Mahal mausoleum, or a black silver car rather than an Aston Martin). This can be conducted to the image-caption pairs contained in the COCO dataset, which lack open-world knowledge. Finally, when the main subject of the image is uncertain (second sample, third row), DiCO may overlook the picture and generate captions based on its learned priors, resulting in hallucinations."}]}