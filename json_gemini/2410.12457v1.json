{"title": "SHARPNESS-AWARE BLACK-BOX OPTIMIZATION", "authors": ["Feiyang Ye", "Yueming Lyu", "Xuehao Wang", "Masashi Sugiyama", "Yu Zhang", "Ivor Tsang"], "abstract": "Black-box optimization algorithms have been widely used in various machine learning problems, including reinforcement learning and prompt fine-tuning. However, directly optimizing the training loss value, as commonly done in existing black-box optimization methods, could lead to suboptimal model quality and generalization performance. To address those problems in black-box optimization, we propose a novel Sharpness-Aware Black-box Optimization (SABO) algorithm, which applies a sharpness-aware minimization strategy to improve the model generalization. Specifically, the proposed SABO method first reparameterizes the objective function by its expectation over a Gaussian distribution. Then it iteratively updates the parameterized distribution by approximated stochastic gradients of the maximum objective value within a small neighborhood around the current solution in the Gaussian distribution space. Theoretically, we prove the convergence rate and generalization bound of the proposed SABO algorithm. Empirically, extensive experiments on the black-box prompt fine-tuning tasks demonstrate the effectiveness of the proposed SABO method in improving model generalization performance.", "sections": [{"title": "Introduction", "content": "Black-box optimization involves optimizing one objective function by using function queries only. In this work, we study the black-box optimization problem [23], which is formulated as\n\nmin F(x), s.t. x \u2208 X,\n\nwhere X Rd, and d represents the parameter dimension. The objective function F : Rd \u2192 R, which satisfies F(x) \u2265\u221e (lower bounded), can only be queried to obtain function values and we cannot get the gradient of F w.r.t. x. In this work, we focus on the online setting for black-box optimization, where different from the offline setting [8, 39], we do not have a prior dataset containing the variable \u00e6 and its corresponding objective value.\nBlack-box optimization has drawn intensive attention in a wide range of applications, such as deep reinforcement learning [41, 9], black-box adversarial attacks of deep neural networks [21, 12], etc. Recently, black-box optimization has shown increasing power on real-world natural language processing tasks, especially with the emergence of large language models (LLMs). Since a common practice is to release LLMs as a service and allow users to access it through their inference APIs. In such a scenario, called Languaged-Model-as-a-Service (LMaaS) [48, 46, 47], users cannot access or tune model parameters but can only tune their prompts without model backpropagation to accomplish language tasks of interest, which directly increase the demand of black-box optimization methods.\nAlthough black-box optimization algorithms have been successfully applied to various learning tasks, most existing works directly optimize the training loss value [48], which may lead to suboptimal model quality and generalization performance. Since the training loss landscape is complex and has many local minima with different generalization abilities [55], the learned model may suffer from the overfitting problem, causing poor generalization performance"}, {"title": "Background", "content": "Stochastic Gradient Approximation The stochastic gradient approximation method [51, 29, 53] is a representative strategy for solving black-box optimization problems, which instead of maintaining a population of searching points, iteratively updates a search distribution by stochastic gradient approximation. The general procedure of stochastic gradient approximation methods is to first generate a batch of sample points by a parameterized search distribution. Then the sample points allow the algorithm to capture the local structure of the fitness function and appropriately estimate the stochastic gradient to update the distribution.\nSpecifically, the stochastic gradient approximation method reparameterizes F(x) as\n\nJ(0) = Epo(x) [F(x)] = F(x)p(x;0)dx,"}, {"title": "Methodology", "content": "In this section, we introduce the proposed SABO algorithm. Firstly, we formulate the sharpness-aware black-box optimization as a min-max optimization problem and solve it in Section 3.1, and in Section 3.2, we derive the update formula of parameters in the search distribution. The detailed derivations in this section are put in Appendix A."}, {"title": "Sharpness-Aware Black-box Optimization", "content": "Suppose we are given a training set D with i.i.d. samples {(Xi, Yi)}. The main objective is defined as\n\nF(x; D) =  \u2211l(x; (Xi, Yi)),\n\nwhere x denotes model parameters, |D| denotes the number of data in the dataset D, and l(x; (X, y)) denotes the loss function (e.g., the cross-entropy loss for classification). To simplify the notation, we define F(x) := F(x; D).\nIn black-box optimization, we aim at minimizing the objective function F(x), with only function queries. Due to the lack of gradient information, we first apply the stochastic gradient approximation method [51, 29]. We denote by 0 the parameters of the search distribution pe and define the expected fitness of F(x) under the parametric search distribution po(x) as J(0) = Epe(x) [F(x)]. Then the optimal parameter 0 can be found by minimizing the reparameterized objective J(0).\nInspired by SAM [16], we attempt to improve generalization by finding flat minima of 0. However, for the reparameter-ized objective J(\u03b8), the geometry of the corresponding distribution space is not Euclidean but a statistical manifold, where the distance between two probability distributions is defined by some statistical distance, e.g., Kullback-Leibler (KL) divergence. Therefore, instead of restricting the perturbation in an Euclidean ball, we restrict the perturbation distribution to be inside a small neighborhood of the unperturbed distribution w.r.t. the KL divergence [1]. The proposed optimization problem for black-box optimization is formulated as\n\nmin max J(0 + \u03b4),"}, {"title": "Update Formulations for SABO", "content": "The gradients of the reparameterized objective J(0) w.r.t. \u00b5 and \u2211 rely on the expectations of the black-box function and can be obtained with only function queries [51] (see Theorem A.1 in Appendix A.4). Hence, we estimate them by Monte Carlo sampling. Specifically, the stochastic approximation of the gradients \u2207\u00b5J(0t) and \u2207\u2211J(0+) are given as\n\ng=\n\nG="}, {"title": "Analysis", "content": "In this section, we provide comprehensive theoretical analyses for the proposed SABO method with all the detailed proofs in Appendix D."}, {"title": "Convergence Analysis of SABO", "content": "Firstly, we make an assumption for the reparameterized objective function.\nAssumption 4.1. The function J(0) is H-Lipschitz and L-smoothness w.r.t. 0 = {\u03bc, \u03a3} \u2208 \u0398, where \u0398 := {\u03bc, \u03a3 |\n\u03bc\u2208Rd, \u03a3\u2208S+}.\nThe smoothness assumption in Assumption 4.1 has been widely adopted in optimization literature [6]. Note that the proposed SABO algorithm approximates the gradients of the reparameterized objective function. It is necessary to study the relation between the optimal solutions of the original objective functions F(x) and J(0), and we put the results in the following proposition.\nProposition 4.2. [29] Suppose \u0440\u04e9(x) is a Gaussian distribution with 0 = {\u03bc, \u03a3} and F(x) is a convex function. Let J(0) = Epo [F(x)], and J(\u03bc*, 0) := F(\u03bc*). Then we have\n\nF(\u03bc) \u2013 F(\u03bc*) \u2264 J(\u03bc, \u03a3) \u2013 J(\u03bc*, 0),\n\nwhere 0 denotes a zero matrix with appropriate size.\nThe convexity assumption of the objective function in Theorem 4.3 has been widely adopted in the area of stochastic gradient approximation black-box optimization [3, 51, 29, 54]. Since the Gaussian-smooth approximation function is always an upper bound of the true target function in convex cases, i.e., F(\u03bc) \u2264 \u0395\u039d(\u03bc,\u03a3) [F(x)]. When \u03bc* is an optimal solution of minimization problem min F(x), Proposition 4.2 implies that the difference between the objective value at u and the optimal objective value of the original problem is upper-bounded by that of the expected objective function.\nThen for Algorithm 1, the following theorem captures the convergence of \u00b5 for a convex objective function.\nTheorem 4.3. Suppose that F(x) is a convex function, J(0) is c-strongly convex w.r.t. \u03bc, the gradient estimator Gt (w.r.t. the covariance matrix) is positive semi-definite matrix such that \u0121I < G+ < = with \u00a7 \u2265 0, \u03a3\u03bf \u2208 S+, and \u03a3\u03bf < RI where R > 0. Suppose the sequence {\u00b5t} generated by Algorithm 1 satisfies that the distance between the sequence {\u00b5t} and the optimal solution of F(x) is bounded, i.e., ||\u03bc\u03b5 \u2013 \u03bc* || \u2264 D, \u03b2\u2081 = O(1), and p < Vasatisfies\nva p = O(), then with Assumption 4.1, we have\n\n\u0395 [J(\u03bct+1, 2+) \u2013 J(\u03bc*,0)] = O\n\nBased on Theorem 4.3 and Proposition 4.2, when \u1e9et = O(1) and p = O(\u30c6), we have\n\nE [F(t+1) \u2013 F(\u03bc*)] = O(log).\n\nTherefore, the proposed SABO algorithm with full-batch function query possesses a convergence rate O(log for the convex objective function. Additionally, in Theorem 4.3, when \u03b2 = O(1) and p = O(1), the proposed SABO\nalgorithm still maintains a convergence rate O(T\u00af\u00bd). The detailed discussion is provided in Remark D.1.\nFor the mini-batch setting, we make an additional assumption for the objective function F(x; D).\nAssumption 4.4. It is assumed that the datasets D and B are i.i.d. sampled from a data distribution P(X, y), and the variance of the mini-batch estimation of the objective function is bounded, i.e., ||F(x; B) \u2013 EF(x; B)||32 \u2264 & and ||F(x; D) \u2013 EF(x; D)||3 \u2264 \u03b5\u03be.\nNote that for the standard stochastic gradient descent (SGD) method [42], the unbiased estimation and bounded variance assumptions were made for the approximated gradient. However, in black-box optimization, the gradient of the objective function F(x) w.r.t. \u00e6 is unavailable. Hence we can only make assumptions for the batch estimations F(x; B) and F(x; D).\nThen we have the following result for the mini-batch estimation.\nProposition 4.5. Suppose Assumption 4.4 holds, then we have EF(x; B) = EF(x; D), and ||F(x; B) \u2013 F(x; D) || < \u20ac2, where \u20ac\u00b2 = 2(\u03b5\u03be + \u03b5\u03be).\nThen with Assumption 4.4, the following theorem shows the convergence of \u03bc for Algorithm 2.\nTheorem 4.6. Suppose that F(x) is a convex function, J(0) is c-strongly convex w.r.t. \u03bc, the gradient estimator Gt (w.r.t. the covariance matrix) is positive semi-definite matrix such that &I < G+ < + with \u00a7 \u2265 0, \u03a3\u03bf \u2208 S+, and \u03a30 < RI where R > 0. Suppose the sequence {\u00b5t} generated by Algorithm 2 satisfies that the distance between the"}, {"title": "Generalization Error Analysis", "content": "In this subsection, we analyze the generalization bound of the proposed SABO algorithm. Specifically, we bound the expectation of the objective function over the Gaussian perturbation.\nWe denote by (X, y) a data pair drawn from a data distribution P(X, y) and by F(x; (X, y)) the corresponding loss of parameter \u00e6 on (X,y). So we have F(x; (X,y)) = l(x; (X, y)). We define the population loss over the data distribution P(X,y) as Ep(X,y)[F(x; (X,y))], and the empirical loss over a dataset S, which consists of M i.i.d. samples drawn from P(X, y), as F(x; S) = \u2211=1l(x; (Xi, yi)). Then we have following result.\nTheorem 4.8. Let the loss function F(x; (X,y)) be a convex function w.r.t. x, then for any \u03bc \u2208 Rd, with probability at least 1 k, we have\n\nEP(x,y) [F(\u03bc; (X,y))] \u2264 max Epets [F(x; S)] +\n\nwhere p\u0473 := N(\u03bc, \u03a3), C(0) = {\u03b4 | KL(p\u04e9+8||P\u04e9) \u2264 p\u00b2}, and S denotes the training set that consists of M\ni.i.d. samples drawn from data distribution P(X,y).\nTheorem 4.8 provides a generalization bound for the proposed SABO algorithm. Compared with the generalization bound of SAM presented in Appendix A.1 of [16], Theorem 4.8 has an asymptotically identical order in the complexity term. However, the expected generalization loss on the right-hand side of Eq. (28) is different in that we reparameterize the objective function and have perturbation of 0 in its neighborhood of a statistical manifold, i.e., \u03b4 \u2208 C(0), while SAM bounds the generalization loss averaged over a spherical Gaussian perturbation on parameters."}, {"title": "Related Works", "content": "Black-Box Optimization. Many methods have been proposed for black-box optimization, including Bayesian optimization (BO) methods [45, 18, 34], stochastic optimization methods such as evolution strategies (ES) [19, 51, 29], and genetic algorithms (GA) [44, 32]. Among those methods, BO achieves good performance for low-dimensional problems, but it often fails to handle high-dimensional problems through a global surrogate model, as shown in [14] and [37]. As a result, TuRBO [14] and GIBO [37] try to address this problem with the local BO approach. [59] further showed that decomposition is important for alleviating the high-dimensional problems in BO. Although BO is not our main focus, we further compare our method with these local Bayesian optimization methods on the black-box prompt fine-tuning problem, and the corresponding exploration is shown in Appendix F.2. GA method is computationally expensive for machine learning problems and usually lacks convergence analysis. The stochastic optimization methods such as CMA-ES [19] and INGO [29] can scale up to higher-dimensional problems compared with BO. Hence we mainly consider stochastic optimization methods as baseline methods in our experiments.\nSharpness-Aware Minimization. SAM has been widely studied for improving the model generalization. Among previous works on SAM [26, 58, 57, 22], the most relevant method to our approach is the FSAM method [24] which also finds the worst-case objective function via a statistical manifold instead of the Euclidean space. However, the loss function of the model studied in FSAM is a predictive distribution conditional on both model parameters and data, while in our case, we consider the parameter as a Gaussian distribution. The bSAM method [33] builds a connection between the SAM objective and Bayes objective by Fenchel biconjugate of the loss function. [33] shares a similarity with our work in developing SAM w.r.t. the expected loss. However, The bSAM method relies on the derivation of a convex lower bound of the expected loss by the Fenchel biconjugate and the perturbation is still w.r.t. each point inside the expected loss as standard SAM. Hence FSAM and bSAM are different from the proposed SABO method. Additionally,"}, {"title": "Empirical Study", "content": "In this section, we empirically evaluate the proposed SABO method, and compare it with four representative black-box methods, i.e., CMA-ES [19], MMES [20], BES [17], and INGO [29]. All the experiments are conducted on a single NVIDIA GeForce RTX 3090 GPU."}, {"title": "Synthetic Problems", "content": "To verify the convergent results of the proposed SABO method in Section 4. We compare the proposed SABO method with baseline methods on minimizing four d-dimensional synthetic benchmark test functions, i.e., ellipsoid function, la-ellipsoid function, different powers function, and Levy function. All the test functions are listed in Appendix F.1.\nThe results are evaluated by calculating the Euclidean distance between the solution \u00e6 and the optimal solution \u00e6*, i.e., E = |x - x* ||2. We then assess the baseline methods using varying dimensions, i.e., d \u2208 {200, 500, 1000}. Due to the page limitation, the implementation details and more detailed experimental results are put in Appendix F.1."}, {"title": "Black-box Prompt Fine-tuning", "content": "Black-box prompt fine-tuning of large language models [10, 48, 46, 47] is a promising direction to achieve expertise models efficiently for downstream tasks. In such an LMaaS setting, we cannot access the model parameter and can only tune their prompts without backpropagation. We evaluate the proposed SABO method in improving generalization performance on the black-box prompt fine-tuning task."}, {"title": "Conclusion", "content": "In this work, we have introduced SABO, a novel black-box optimization algorithm that improves generalization by utilizing a sharpness-aware minimization strategy. Theoretically, we provide a convergence guarantee for the"}, {"title": "A Additional Material for Section 3", "content": ""}, {"title": "A.1 Determine the perturbation", "content": "The Lagrangian of problem (9) is\n\nL(\u03b4, 1) = - (V0J(\u03b8), \u03b4) + \u03bb(KL(po+s||po) \u2013 p\u00b2)\n\n= -8\u00b5J(0) \u2013 tr(\u03b4\u00a3\u2207\u2211J(0+))\n+\n+\n\nTaking the derivative \u03b4\u03bc and \u03b4\u2211 and setting them to zero, we can obtain that\n\n\u221a\u03bcJ(0) \u2013 \u03bb\u03a3\u2212\u00b9\u03b4\u03bc = 0,\n\n\u2207J(0) \u2013 [(\u03a3 + \u03b4\u00a3)-1 \u2013 \u03a3\u2212\u00b9] = 0.\n\nNote that \u2211 is a diagonal matrix. Therefore, we can achieve that\n\n\u03b4\u03bc(0) = \u03a3\u03bcJ(0),\n\n\u03b4\u03b5(0) ="}, {"title": "A.2 Determine the optimal solution", "content": "Note that we have\n\nKL(Po+s||Po) = + \u03b4\u03a3\u2212\u00b9\u03b4\u03bc + log\n\n= +(1+V2J(0) (\u03a3+ \u03b4\u03b5)) + \u03b4\u03a3\u2212\u00b9\u03b4\u03bc + log\n\n=\n\nwhere\n\nQ = \u2013 2\u2207\u00a3J(0)).\n\nSince \u2211 and \u22072J(0) are both diagonal matrix, we denote diag(\u03a3\u2207\u2211J(0)) = (v\u00b9, . . ., vd), then we have\n\nQ =\n\nWe denote diag(\u2207\u2081J(0)) = (\u00f41, . . ., \u00eed), then we have \u1fe6\u1f30\u03c3\u00b2 = v\u00b2 and\n\n=\n\nThen substituting \u03b4\u03bc (\u03b8) and \u03b4\u2211(0) into the inequality KL(p\u04e9+8||P\u04e9\u2081) \u2264 p\u00b2, we can obtain that\n\nKL(po+8||Po) =+\u20ac \u2264 p\u00b2,\n\nwhere \u20ac = (). Let the equality holds and solve Eq. (41), we have\n\n\u203a\u2248\u221a||\u2211\u2207\u00a3J(0)||} + 0.5||\u2211\u2021\u2207\u201eJ(0)||1."}, {"title": "A.3 Update rule", "content": "Note that we have\n\n(\u03b8 \u2013 \u03b8\u03b5, VoJ (\u03b8\u03b5 + \u03b4\u03b5)) + KL(pe||Pe)\n\n1= (\u03bc \u2013 \u03bct)\u2207\u00b5J(\u03b8t + dt) + tr((\u03a3 \u2013 \u03a3t)\u2207\u00a3J(\u03b8t + \u03b4t))\n+"}]}