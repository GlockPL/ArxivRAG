{"title": "OD-Stega: LLM-Based Near-Imperceptible Steganography via Optimized Distributions", "authors": ["Yu-Shin Huang", "Peter Just", "Krishna Narayanan", "Chao Tian"], "abstract": "We consider coverless steganography where a Large Language Model (LLM) drives an arithmetic coding decoder to generate stego-texts. An efficient method should embed secret message bits in as few language tokens as possible, while still keeping the stego-text natural and fluent. We show that on the individual token level, this problem is mathematically equivalent to maximizing the entropy of a replacement probability distribution of the next token generation, subject to a constraint on the KL divergence between the chosen probability distribution and the original distribution given by the LLM. A closed-form solution is provided for the optimization problem, which can be computed efficiently. Several important practical issues are also tackled: 1) An often-overlooked tokenization mismatch issue is resolved with a simple prompt selection approach, 2) The combination of the optimized distribution and the vocabulary truncation technique is considered, and 3) The combination of the optimized distribution with other sequence-level selection heuristics to further enhance the efficiency and reliability is studied.", "sections": [{"title": "1 Introduction", "content": "In a steganography system, Alice, the sender, aims to convey a secret message to Bob, the receiver. The carrier signal can take the form of text, image, audio, or video (Anderson & Petitcolas, 1998; Cox et al., 2007; Provos & Honeyman, 2003). In this work, we focus on natural language text messages as the type of carrier signals, and in this case, the resultant signal with the secret message embedded is referred to as the stego-text. Alice transmits the stego-text to Bob via a public channel, which is being monitored by an eavesdropper Eve. Eve wishes to determine whether there is a hidden message in the stego-text. Alice must ensure that the stego-text can be decoded correctly by Bob, and at the same time, guarantee with a high probability that Eve cannot detect whether a secret message exists or not. A good analogy is that Bob is a prisoner, Alice is the family member outside the prison who has a letter for Bob, and Eve is the prison guard who may confiscate the letter if anything unusual is detected about the letter (Simmons, 1984).\nConventionally, steganography relies on an existing cover signal (cover text), and achieves steganography by making subtle changes imperceptible to Eve on the cover text. For example, Alice can replace certain words by their synonyms following pre-agreed patterns (Topkara et al., 2006; Chang & Clark, 2010; Safaka et al., 2016). Recently, as generative models, particularly large language models, become more and more powerful, coverless steganography has shown significant performance advantages. With this approach, the stego-text appears indistinguishable from natural languages, and more importantly, a large amount of the secret information can be hidden in shorter stego-texts than the traditional cover-text-based approaches (Fang et al., 2017; Yang et al., 2018; Ziegler et al., 2019; Xiang et al., 2017).\nThe underlying driver for LLM-based steganography is usually the arithmetic coding (AC) algorithm (Witten et al., 1987), which is an efficient data compression algorithm based on the idea that any finite-length finite-alphabet data sequence (e.g., text) can be mapped to a small interval in the range of [0, 1) based on the cumulative probability distribution function. Therefore, a binary representation that accurately specifies this"}, {"title": "2 Related Works", "content": "Linguistic Steganography (LS) can be divided into two main areas: modification-based (cover-based) and generation-based (coverless). The modification-based approach conceals secret messages by altering the cover text through synonyms, syntactic changes, and word substitutions (Topkara et al., 2006; Chang & Clark, 2010; Qi et al., 2013; Chang & Clark, 2014). In contrast, the generation-based approach creates stego-texts using methods like Markov chains (Dai et al., 2009, 2010; Moraldo, 2014) and deep learning techniques. With the advancement of generative language models, an increasing number of steganography research efforts now leverage neural networks to produce steganographic texts (Fang et al., 2017; Yang et al., 2018; Dai & Cai,"}, {"title": "3 Preliminary", "content": "A Large Language Model (LLM) can provide an estimate for the conditional probability distribution for the next token, given the sequence of tokens preceding it (Vaswani, 2017; Brown, 2020; Touvron et al., 2023). To generate a natural language sequence, one can sample the tokens from these distributions in an autoregressive manner. We next provide some notation for the rest of the paper.\nFollowing the work of Shen et al. (2020) for LLM-based steganography, we assume that the secret message bit sequence S is already encrypted, before Alice starts to embed it in the stego-text. Before encoding, Alice selects an initial prompt text Tp, independent of S, which typically determines the nature or semantic of the resulting stego-text. To encode S, Alice uses an encoding function f(Tp, S) to produce a sequence of tokens Xi>0 = (X1,X2,X3,...), which is then converted to the corresponding stego-text Ts via detokenizing. The prompt and the stego-text (Tp, Ts) are sent on the public channel. Bob first converts T, into the token form Xi>0, then uses a decoding function g() such that g(Tp, xi>0) = S.\nIn LLM-based steganography, both f and g rely on the same LLM. At time i, an LLM takes the tokenized input Xi-1 = (x-np-1, X-np-2, \u2026, xi\u22121) as the prompt, where xo = (X-np-1, X-np-2,\u2026\u2026, xo) represents the tokenized sequence of Tp and np is the number of tokens in Tp. This produces the probability distribution PLLM for the next token xi. We shall write it as PLLM(Y = Xi | Xi\u22121), or simply P\u00b2, which is the conditional probability for the next token, given the proceeding tokens (in the context window)."}, {"title": "3.1 LLM-based Steganography", "content": "A Large Language Model (LLM) can provide an estimate for the conditional probability distribution for the next token, given the sequence of tokens preceding it (Vaswani, 2017; Brown, 2020; Touvron et al., 2023). To generate a natural language sequence, one can sample the tokens from these distributions in an autoregressive manner. We next provide some notation for the rest of the paper.\nFollowing the work of Shen et al. (2020) for LLM-based steganography, we assume that the secret message bit sequence S is already encrypted, before Alice starts to embed it in the stego-text. Before encoding, Alice selects an initial prompt text Tp, independent of S, which typically determines the nature or semantic of the resulting stego-text. To encode S, Alice uses an encoding function f(Tp, S) to produce a sequence of tokens Xi>0 = (X1,X2,X3,...), which is then converted to the corresponding stego-text Ts via detokenizing. The prompt and the stego-text (Tp, Ts) are sent on the public channel. Bob first converts T, into the token form Xi>0, then uses a decoding function g() such that g(Tp, xi>0) = S.\nIn LLM-based steganography, both f and g rely on the same LLM. At time i, an LLM takes the tokenized input Xi-1 = (x-np-1, X-np-2, \u2026, xi\u22121) as the prompt, where xo = (X-np-1, X-np-2,\u2026\u2026, xo) represents the tokenized sequence of Tp and np is the number of tokens in Tp. This produces the probability distribution PLLM for the next token xi. We shall write it as PLLM(Y = Xi | Xi\u22121), or simply P\u00b2, which is the conditional probability for the next token, given the proceeding tokens (in the context window)."}, {"title": "3.2 Arithmetic Coding", "content": "Several authors have shown that Arithmetic Coding, or AC for short, can be used together with language models to perform steganography (Ziegler et al., 2019; Shen et al., 2020; Ivasenko et al., 2021). AC is a method for data compression that encodes a whole sequence of symbols as a single value, based on the probability distribution. Typically, AC compresses the character in the sequence sequentially into a sequence of bits at the transmitter, and converts the sequence back to text during decompressing. The main idea of using AC for steganography is that an AC decoder can be viewed as a sampler in the set of natural language paragraphs using the secret message as a random seed, and since the secret message is uniformly distributed on the message set, the sampled text would look like natural language. Note that the AC encoding procedure is the steganography decoding procedure, and the AC decoding procedure is the steganography encoding procedure.\nAn illustrative example is given in Figure 1. Initially, a secret bit string is transformed into a decimal fraction interval. For instance, the sequence 10111 can be represented as the interval\nI = [0.101110000\u00b7\u00b7\u00b72,0.1011111111\u00b7\u00b7\u00b72) ~ [0.718750.74902).\nNext, we identify the range where this interval falls in the probability distribution P\u00b2.\nAs illustrated in Figure 1, when we input the starting prompt \"What is the probability of\", the LLM generates a probability distribution for the most likely next tokens P\u00b9. Based on this distribution, we determine where the interval lies. In this example, the interval corresponds to the token \"winning\", so we select the first token of the stego-text as x1 = \"winning\".\nOnce the first token 21 is selected, the probability distribution P2 is obtained by the same procedure of prompting 2\u2081 into the LLM. The next token 22 is chosen based on where the interval lies within this distribution. This process is repeated iteratively until there is no ambiguity regarding where the interval I falls into. As Figure 1 illustrates, the interval is outside any Pn token interval range; thus, after choosing the n-th token, the stego-text generation is completed with a total of n tokens, which can be converted directly into the stego-text.\nDuring the decoding phase, Bob recognizes the starting token of the stego-text from the received text. Bob can then derive the identical distribution P\u00b9 from the same LLM with the starting prompt text. With those stego-text tokens he receives, Bob is then able to retrieve the probabilities Pi>0 and reconstruct the bit sequence, continuing this process until every bit is recovered."}, {"title": "4 Proposed Methodology", "content": "A well-known fact in data compression is that the expected minimum number of bits to represent a message symbol following a probability P is H(P), i.e., the entropy of symbol (Cover & Thomas, 1991), and AC is one algorithm that can compress at a rate close to the minimum value. The same relation holds for LLM-based steganography using AC, in the sense that the expected number of secret message bits that can be embedded for a given token position-i is the entropy of the conditional distribution H(P\u00b2). For example, if a token has a conditional distribution of {1,1 on four possible token values, then 2 bits of secret message can be embedded in the stego-text.\nIt is obvious that a slight modification to the probability distribution from the true natural language distribution is nearly imperceptible to a human reader, or even to a computer program for that matter. We can take advantage of such an opportunity to make the conditional distribution P more amicable for embedding secret message bits, i.e., choose a different distribution Q such that the entropy H(Q) is larger. As long as Q is similar enough to P, we expect the generated stego-text to be nearly imperceptible, which leads us to the formulation given next."}, {"title": "4.1 Problem Formulation: Sampling Strategy under Perception Constraint", "content": "We formulate the following optimization problem for each token at time instance-i.\n$\\begin{aligned}\n&\\underset{Q^i, \\forall j \\in [1:N_i]}{\\text{max}} & H(Q^i) = \\sum_{j=1}^{N_i} Q^i_j \\log Q^i_j  \\\\\n&\\text{subject to} & D_{KL}(Q^i || P^i) = \\sum_{j=1}^{N_i} Q^i_j \\log (\\frac{Q^i_j}{P^i_j}) \\leq \\delta \\\\\n& & Q^i_j \\geq 0, \\forall j \\in [1:N_i]  \\\\\n& & \\sum_{j=1}^{N_i} Q^i_j = 1 \\\\\n& & Q^i_j = 0 \\forall j \\in \\mathcal{A}_i = [N_i + 1 : N]\n\\end{aligned}$\nLet N = |V| be the total number of symbols in the vocabulary. The objective function H(Q\u00b2) in (1) represents the standard Shannon entropy, where we use the logarithm of base 2, implying we will measure the information in bits. We seek to replace the natural language distribution probability distribution P given by the LLMs with a new distribution Q towards a larger entropy value, which usually means a more uniform distribution. This would allow for embedding a greater number of secret bits within a single token. It is crucial for the new distribution to be indistinguishable from natural language, which is ensured by the constraint in (2), that the divergence between Q and P does not exceed a small threshold d. Note the problem above is a convex optimization problem.\nThere are in fact various other metrics to quantify the difference between P and Q, but we choose to use the KL divergence in this work, since it has a clear operational meaning and is well adopted in steganography, moreover, it is connected to the error exponent in hypothesis testing (Cover & Thomas, 1991).\nWithout loss of generality, we will assume throughout the rest of this paper that the elements in the vocabulary are already given in descending order according to the probabilities P\u00b2. The set A\u017c in the constraint (5) corresponds to the index set of elements in the alphabets with zero probability, i.e., P = 0. Clearly there is no need to adjust tokens with a zero probability, since otherwise, the resultant KL divergence will be unbounded; this consideration is reflected in (5). We denote the number of nonzero elements in Pi as Ni = N - Ai. As a result, the number of variables in this optimization problem is in fact Ni instead of \u039d."}, {"title": "4.2 The Optimal Probability Adjustment Strategy", "content": "The main theoretical contribution of the work is Theorem 1, which gives the solution to the optimization problem (1)-(5)."}, {"title": "5 Practical Considerations", "content": "In this section, we will discuss several practical issues in LLM-based steganography and when applying OD-Stega."}, {"title": "5.1 Tokenization Error", "content": "LLM-based steganography relies on several assumptions. Firstly, the underlying LLM and the parameters given to both Alice and Bob must be identical. Second, it is essential that Bob's tokenization process matches that intended by Alice. The second assumption is in fact quite subtle, and is complicated by the sub-word tokenizer used in modern pre-trained LLMs. These tokenizers can guarantee that after detokenizing, the original text can be recovered; however, it does not guarantee the tokenizer can always reproduce the same sequence of tokens from the detokenized text. For example, a token sequence Alice generated during the stego-text encoding process is { \"This\", \"mount\u201d, \u201cain\u201d, \u201cis\", \"high\"}, resulting in the stego-text containing \"This mountain is high\", which Bob might incorrectly tokenize to {\u201cThis\u201d, \u201cmountain\u201d, \u201cis\u201d, \u201chigh\u201d}. In order words, the tokenizer merged \"mountain\" into a single token rather than the two that the stego-text encoder intended. This issue exists in most of the previous LLM-based steganography approaches (Ziegler et al., 2019; Shen et al., 2020), though it has not been addressed explicitly so far.\nThis tokenization error leads Bob to decode a bit sequence different from the original secret bit sequence. After thorough testing, we found that the likelihood of such errors occurring is proportional to the length of the bit file. In mathematical terms, the relationship can be described as Etok = O(n), where n is the number of secret bits, and Etok represents the error rate, measuring the proportion of tests that fail due to tokenization errors relative to the total number of tests.\nSince LLMs are computationally demanding, it is not realistic to enumerate all such potential error cases to design strategies to prevent such errors from occurring. Instead, we observe that Alice can in fact verify whether the stego-text can be correctly decoded by Bob since both sides have a copy of the same tokenizer. Based on this observation, we propose the following strategy. We prepend a short sequence of additional B bits to the bit sequence S. Alice then iterates among all B-bits combinations, and uses f(Tp, (B, S)) to produce the stego-text, until she verifies Bob can indeed correctly decode the text. Bob simply discards the beginning B bits after decoding.\nNext we determine an appropriate choice for the length of B that guarantees the entire steganography process succeeds with high probability, which we set as 1 \u2013 10-8 in our work. Our experiments reveal that for LLAMA models, a single bit produces a tokenization error at a rate below 2 \u00d7 10-4. Since we are essentially making 2|B| independent attempts to find a successful embedding, we can ensure that at least one of these"}, {"title": "5.2 Reduce Computational Complexity via Vocabulary Truncation", "content": "To reduce the computational complexity when the vocabulary set is large, especially when there is a large number of tokens with probabilities near zero, a simple strategy is to truncate the vocabulary in the sub-sequent processing once a probability distribution has been generated. This strategy has been adopted in Shen et al. (2020). To leverage our optimization formulation, we consider a two-stage process: first, we truncate the vocabulary, and second, we optimize the probability adjustment on the truncated vocabulary as discussed in the previous section. For this two-stage approach, we establish the KL divergence between the original distribution and the eventual distribution on the truncated vocabulary, given below in Theorem 2.\nLet us make the two-stage strategy more precise. We first expand the zero-probability index set A\u00bf from [N\u00bf + 1 : N] to [N\u20ac + 1 : N], where Ne = min{n | \u2211j=1P > 1 \u2212 \u20ac}. This leaves us with a total of Ne variables. There may not exist an n such that E=1 P = 1 - \u20ac exactly, but for simplicity, we assume that this can be achieved exactly, meaning that 1 P = = 1-6. This assumption is reasonable, since in LLMs, the number of tokens is quite large, and the cutoff value e is small, therefore, this approximation is usually quite accurate. After the first stage, the variables in the optimization problem are reduced to [Q\u2081,\u2026\u2026, Qva].\nWe can now focus on the most likely symbols in the probability list P, j\u2208 [1 : N\u025b]. We define the re-normalized probability P(e) = P, which we refer to as an e cutoff probability of P\u00b2. The KL divergences between P\u00b2 and its cutoff Pi(e) are\n$\\begin{aligned}\n&D_{KL}(P^{i(\\epsilon)} || P^i) = \\sum_{j=1}^{N_{\\epsilon}} P_j^{(\\epsilon)} \\log \\frac{P_j^{(\\epsilon)}}{P_j} = \\frac{1}{1-\\epsilon} \\sum_{j=1}^{N_{\\epsilon}} P_j \\log \\frac{\\frac{P_j}{1-\\epsilon}}{P_j} \\\\\n&= \\frac{1}{1-\\epsilon} \\log (\\frac{1}{1-\\epsilon}) \\sum_{j=1}^{N_{\\epsilon}} P_j = - \\log(1-\\epsilon)\n\\end{aligned}$\nThe next theorem establishes the KL divergence between the original distribution Pi and the optimized distribution Q\u00b2, the latter of which is obtained by solving the optimization problem in (1)-(5), with P\u00b2(\u03b5) replacing Pi(e).\nTheorem 2. Let Pi(e) be the e cutoff probability distribution of Pi and Qi be the solution of the optimization problem (1)-(5) with the constraint DKL(Q'||Pi(e)) \u2264 \u2642(e), then it holds that\nDKL(Q||P\u00b2) = DKL(P\u00b2(\u20ac)||P\u00b2) + DKL(Q\u00b2||P\u00b2(\u20ac)).\nThe proof of Theorem 2 can be found in the Appendix. It is well known that the KL divergence is not a true metric since it is not symmetric and does not satisfy the triangular inequality in general. Theorem 2 indicates that, in the specific scenario involving the cutoff probability and optimized counterpart, the KL divergence is in fact additive. Given a total KL budget \u03b4, it is clear that we can determine \u03b4(\u20ac) = 8+log(1-\u20ac), where - log(1 - \u20ac) represents the KL divergence between Pi(e) and Pi as given in (12). Since the KL divergence is positive, it is essential to select e within the range 0 < \u20ac < 1 - e-d to guarantee that 8(e) represents a valid KL divergence value."}, {"title": "5.3 8 Selection on the Sequence Level", "content": "Denote the divergence threshold in each time i as di. If di is set too large, the resulting adjustment to the probability distribution may lead to the selection of unusual tokens, negatively impacting the fluency of the stego-text. This issue is particularly noticeable when dealing with positions that have probability distributions with very low entropy values, i.e., most tokens have near-zero probability and the choices of tokens are almost deterministic. To address this issue, we need to choose di at the sequence level adaptively to the entropy H(P\u00b2), \u0456.\u0435. \u0431\u2081 = h(H(P\u00b2)). A simple approach is to set d\u2081 = C\u00b7H(Pi) where C is a constant. Furthermore, we introduce another threshold a,\n$\\delta_i =\\begin{cases}\nC \\cdot H(P^i), & \\text{if } H(P^i) \\geq \\alpha \\\\\n0, & \\text{if } H(P^i) < \\alpha\n\\end{cases}$\nwhich means that for the position where H(Pi) falls below this threshold, we set di to zero."}, {"title": "6 Experimental Results", "content": "In our experiment, we chose the LLAMA2-7B pretrained model (Touvron et al. (2023)) as our main Large Language Model, employing the SentencePiece tokenizer. This LLM features a vocabulary of 32,000 tokens, facilitating efficient tokenization and diverse text representation.\nWe performed experiments using a range of starting prompts on different topics of interest. Examples topics include the Olympics, news, technology, and blogs, among others. The prompts usually have 10 to 20 words. Despite their brevity, we demonstrate that OD-Stega can still generate stego-texts that remains relevant to the initial prompt with the assistance of contemporary LLMs.\nIn our two-stage optimization framework, we typically select a cutoff value e within the range (0,0.05], and adjust the constant C\u2208 [0, 0.2) in (14) to control the di values. Additionally, the threshold a is adjusted within the interval [0, 2] to enhance the optimization procedure. Setting the cutoff e at its maximum of 0.05 results in the effective elimination of roughly 2000 variables. Moreover, by adjusting the range of di and a values, we can assess how these values influence both the performance of the generated stego-text and the number of embedded bits."}, {"title": "6.1 Experimental Setup", "content": "In our experiment, we chose the LLAMA2-7B pretrained model (Touvron et al. (2023)) as our main Large Language Model, employing the SentencePiece tokenizer. This LLM features a vocabulary of 32,000 tokens, facilitating efficient tokenization and diverse text representation.\nWe performed experiments using a range of starting prompts on different topics of interest. Examples topics include the Olympics, news, technology, and blogs, among others. The prompts usually have 10 to 20 words. Despite their brevity, we demonstrate that OD-Stega can still generate stego-texts that remains relevant to the initial prompt with the assistance of contemporary LLMs.\nIn our two-stage optimization framework, we typically select a cutoff value e within the range (0,0.05], and adjust the constant C\u2208 [0, 0.2) in (14) to control the di values. Additionally, the threshold a is adjusted within the interval [0, 2] to enhance the optimization procedure. Setting the cutoff e at its maximum of 0.05 results in the effective elimination of roughly 2000 variables. Moreover, by adjusting the range of di and a values, we can assess how these values influence both the performance of the generated stego-text and the number of embedded bits."}, {"title": "6.2 Bits/Token vs. KL Tradeoff", "content": "In this experiment, we keep the number of tokens in the stego-text to be 25, but attempt to embed more secret bits than can be embedded with 25 tokens in order to test the limits of the method. By varying parameter C from 0 to 0.075 and adjusting parameters e and a, we obtained various pairs consisting of number of bytes embedded and the corresponding KL divergence, shown in Figure 4. Different shapes of the data points in this plot correspond to different levels of truncation cutoff value. The highest contour curve predominantly consists of the square points, representing the smallest cutoff category in our experiments, ranging from 0.005 to 0.015. This behavior suggests that, given a fixed KL divergence budget, allocating a larger proportion of the probability distance to the optimization process, rather than to the truncated portion, results in more effective bit embedding.\nWe observe that as C increases (which corresponds to an increase in di), the data points move linearly towards the upper right, meaning more secret bits are embedded, but the stego-text becomes less natural. On the other hand, the black points representing the truncation-based method (Shen et al., 2020) shifts toward lower right, meaning losses in the embedding capability. From this plot, it is clear that the proposed method has the ability to embed more than 20 bytes while maintaining a KL divergence below 0.25. In comparison to the truncation-based method, at a KL divergence of 0.02, our approach achieves a 1.25-times improvement in bit embedding capacity. At a KL divergence close to 0.06, our method shows an even greater enhancement, achieving a 1.5-times increase in embedding efficiency over the truncated method."}, {"title": "6.3 GPT Evaluation", "content": "We use the GPT-4 model to evaluate whether our stego-text appears natural and can avoid detection by the human eavesdropper Eve. We instructed GPT to mimic a human evaluator to assess the text and determine if it was likely written by a human, responding with either \"yes\" or \"no\". In this experiment, we examined hundreds of generated stego-texts with GPT-4 under various parameters outlined in Section 6.2, with the results displayed in Figure 5. The horizontal axis represents the GPT evaluation score, i.e., the ratio of test cases marked \"yes\" by GPT in the total number of files evaluated.\nSince a higher GPT score indicates a better result, the upper right direction means better performance in this point cloud plot. We first observe that the KL divergence is a relatively accurate measure of human perception. The red data points represent the truncation-based method, which again under-performs. At a GPT evaluation score of approximately 0.77, OD-Stega can attain an embedding rate 1.4 times higher; at around 0.85, OD-Stega even achieves 1.5 times the number of embedded bits than the truncation-based approach. We observe that a cutoff value between 0.01 and 0.3 appears to be suitable for OD-Stega.\nInterestingly, at the extreme high GPT score regime, OD-Stega with a small truncation can achieve above 0.9, which the approach with essentially unadjusted LLM distributions (extremely small truncation values and no optimization of the distribution) cannot achieve. In other words, at the extreme regime, the OD-Stega approach can achieve more natural stego-texts than those directly generated from the LLAMA model, viewed from the point of a GPT surrogate."}, {"title": "6.4 Examples of Generated Stego-Texts", "content": "Figure 6 presents examples of stego-texts generated using our proposed method. Given a secret message S and an initial prompt Tp, two text outputs were generated by varying the parameter values. The prompt, which discusses animals in the Amazon rainforest, yielded distinct results based on the chosen parameter settings. The green text represents coherent and logical content, while the red text deviates from the given topic. The green OD-Stega text, generated using the parameter pair (C = 0.025, \u0454 = 0.05), demonstrates fluency and maintains consistency with the prompt's topic. In contrast, the red OD-Stega text, produced with a larger parameter C = 0.05 and higher KL divergence, shows a significant departure from the natural language distribution. Specifically, in the second sentence, the text becomes incoherent, leading to the"}, {"title": "7 Conclusion", "content": "To embed more secret messages in stego-texts while reducing computational complexity and maintaining near-imperceptibility, we propose the OD-Stega method. This approach optimizes the probability distribu-tion towards a more uniform structure under a perception constraint. Additionally, we address the tokeniza-tion errors that often arise in LLM-based steganography due to the use of sub-word tokenizers in modern LLMs. Together with the vocabulary truncation technique, our two-stage embedding process significantly increases the embedding efficiency under the KL divergence constraint, and demonstrates strong impercep-tibility performance. We conducted extensive tests and evaluate the outputs both using the KL divergence"}, {"title": "A Proof of Theorem 1", "content": "The Lagrangian function of the problem is\n$\\begin{aligned}\n\\mathcal{L} = \\sum_{j=1}^{N_i} Q_j^i \\log Q_j^i + \\mu \\left( \\sum_{j=1}^{N_i} Q_j^i \\log \\left(\\frac{Q_j^i}{P_j^i} \\right) - \\delta \\right) - \\sum_{j=1}^{N_i} \\lambda_j (-Q_j^i) + \\omega \\left(\\sum_{j=1}^{N_i} Q_j^i - 1\\right)\n\\end{aligned}$\nwhere \u03bc, \u03bb, \u03c9 are the Lagrangian multipliers of constraint (2), (3) and (4), respectively. Then the KKT condition can be derived as follows:\n1. Stationarity:\n$\\frac{\\partial \\mathcal{L}}{\\partial Q_j^i} = \\log Q_j^i + 1 + \\mu \\left( \\log \\frac{Q_j^i}{P_j^i} + 1 \\right) - \\lambda_j + \\omega = 0, \\forall j \\in [1 : N_i]$\n2. Primal feasibility:\n$\\begin{aligned}\n& \\sum_{j=1}^{N_i} Q_j^i \\log \\frac{Q_j^i}{P_j^i} - \\delta \\leq 0 \\\\\n& Q_j^i \\geq 0, \\forall j \\in [1 : N_i] \\\\\n& \\sum_{j=1}^{N_i} Q_j^i - 1 = 0\n\\end{aligned}$\n3. Dual feasibility:\n$\\begin{cases}\n\\mu \\geq 0 \\\\\n\\lambda_j \\geq 0, \\forall j \\in [1: N_i]\n\\end{cases}$\n4. Complementary slackness:\n$\\begin{cases}\n\\mu \\left( \\sum_{j=1}^{N_i} Q_j^i \\log \\frac{Q_j^i}{P_j^i} - \\delta \\right) = 0 \\\\\n\\lambda_j (-Q_j^i) = 0, \\forall j \\in [1 : N_i] \\\\\n\\omega \\left(\\sum_{j=1}^{N_i} Q_j^i - 1\\right) = 0\n\\end{cases}$\nSince the optimization problem is convex and clearly feasible, a solution to the KKT condition is also a global optimal solution. We claim the following is a solution to the KKT conditions:"}, {"title": "B Proof of Lemma 1", "content": "First", "1": "N_i"}]}